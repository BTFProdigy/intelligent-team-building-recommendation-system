Proceedings of NAACL HLT 2007, Companion Volume, pages 185?188,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Kernel Regression Based Machine Translation
Zhuoran Wang and John Shawe-Taylor
Department of Computer Science
University College London
London, WC1E 6BT
United Kingdom
{z.wang, jst}@cs.ucl.ac.uk
Sandor Szedmak
School of Electronics and Computer Science
University of Southampton
Southampton, SO17 1BJ
United Kingdom
ss03v@ecs.soton.ac.uk
Abstract
We present a novel machine translation
framework based on kernel regression
techniques. In our model, the translation
task is viewed as a string-to-string map-
ping, for which a regression type learning
is employed with both the source and the
target sentences embedded into their ker-
nel induced feature spaces. We report the
experiments on a French-English transla-
tion task showing encouraging results.
1 Introduction
Fig. 1 illustrates an example of phrase alignment
for statistical machine translation (SMT). A rough
linear relation is shown by the co-occurences of
phrases in bilingual sentence pairs, which motivates
us to introduce a novel study on the SMT task:
If we define the feature space Hx of our source
language X as all its possible phrases (i.e. informa-
tive blended word n-grams), and define the mapping
?x : X ? Hx, then a sentence x ? X can be ex-
pressed by its feature vector ?x(x) ? Hx. Each
component of ?x(x) is indexed by a phrase with the
value being the frequency of it in x. The definition
of the feature space Hy of our target language Y can
be made in a similar way, with corresponding map-
ping ?y : Y ? Hy. Now in the machine translation
task, given S = {(xi, yi) : xi ? X , yi ? Y, i =
1, . . . ,m}, a set of sample sentence pairs where yi
is the translation of xi, we are trying to learn W a
matrix represented linear operator, such that:
?y(y) = f(x) = W?x(x) (1)
we return
to marked
questions
marqu?es
nous
revenous
aux
questions
Figure 1: Phrase alignment in SMT
to predict the translation y for a new sentence x.
Comparing with traditional methods, this model
gives us a theoretical framework to capture higher-
dimensional dependencies within the sentences. To
solve the multi-output regression problem, we inves-
tigate two models, least squares regression (LSR)
similar to the technique presented in (Cortes et al,
2005), and maximum margin regression (MMR) in-
troduced in (Szedmak et al, 2006).
The rest of the paper is organized as follows. Sec-
tion 2 gives a brief review of the regression models.
Section 3 details the solution to the pre-image prob-
lem. We report the experimental results in Section
4, with discussions in Section 5.
2 Kernel Regression with Vector Outputs
2.1 Kernel Induced Feature Space
In the practical learning process, only the inner prod-
ucts of the feature vectors are needed (see Section
2.2, 2.3 and 3), so we can perform the so-called
kernel trick to avoid dealing with the very high-
dimensional feature vectors explicitly. That is, for
x, z ? X , a kernel function is defined as:
?x(x, z) = ??x(x),?x(z)? = ?x(x)??x(z) (2)
185
Similarly, a kernel function ?y(?, ?) is defined in Hy.
In our case, the blended n-spectrum string ker-
nel (Lodhi et al, 2002) that compares two strings
by counting how many (contiguous) substrings of
length from 1 up to n they have in common, is a good
choice for the kernel function to induce our feature
spaces Hx and Hy implicitly, even though it brings
in some uninformative features (word n-grams) as
well, when compared to our original definition.
2.2 Least Squares Regression
A basic method to solve the problem in Eq. 1 is least
squares regression that seeks the matrix W mini-
mizing the squared loss in Hy on the training set S:
min ?WMx ?My?2F (3)
where Mx = [?x(x1), ...,?x(xm)], My =
[?y(y1), ...,?y(ym)], and ? ? ?F denotes the Frobe-
nius norm.
Differentiating the expression and setting it to
zero gives:
2WMxM?x ? 2MyM?x = 0
? W = MyK?1x M?x (4)
where Kx = M?x Mx = (?x(xi, xj)1?i,j?m) is the
Gram matrix.
2.3 Maximum Margin Regression
An alternative solution to our regression learn-
ing problem is proposed in (Szedmak et al,
2006), called maximum margin regression. If L2-
normalized feature vectors are used in Eq. 1, de-
noted by ??x(?) and ??y(?), MMR solves the follow-
ing optimization:
min 12?W?
2
F + C
m
?
i=1
?i (5)
s.t. ???y(yi),W??x(xi)?Hy ? 1? ?i,
?i > 0, i = 1, . . . ,m.
where C > 0 is the regularization coefficient, and
?i are the slack variables. The Lagrange dual form
with dual variables ?i gives:
min
m
?
i,j=1
?i?j ??x(xi, xj)??y(yi, yj)?
m
?
i=1
?i
s.t. 0 ? ?i ? C, i = 1, . . . ,m. (6)
where ??x(?, ?) and ??y(?, ?) denote the kernel func-
tions associated to the respective normalized feature
vectors.
This dual problem can be solved efficiently with
a perceptron algorithm based on an incremental
subgradient method, of which the bounds on the
complexity and achievable margin can be found in
(Szedmak et al, 2006).
Then according to Karush-Kuhn-Tucker theory,
W is expressed as:
W =
m
?
i=1
?i??y(yi)??x(xi)? (7)
In practice, MMR works better when the distribu-
tion of the training points are symmetrical. So we
center the data before normalizing them. If ?Sx =
1
m
?m
i=1 ?x(xi) is the centre of mass of the source
sentence sample set {xi} in the feature space, the
new feature map is given by ??x(?) = ?x(?) ? ?Sx .
The similar operation is performed on ?y(?) to ob-
tain ??y(?). Then the L2-normalizations of ??x(?) and
??y(?) yield our final feature vectors ??x(?) and ??y(?).
3 Pre-image Solution
To find the pre-image sentence y = f?1(x) can be
achieved by seeking yt that has the minimum loss
between its feature vector ?y(yt) and our prediction
f(x). That is (Eq. 8: LSR, Eq. 9: MMR):
yt = arg min
y?Y(x)
?W?x(x)? ?y(y)?2
= arg min
y?Y(x)
?y(y, y)? 2ky(y)K?1x kx(x) (8)
yt = arg min
y?Y(x)
1? ???y(y),W??x(x)?Hy
= arg max
y?Y(x)
m
?
i=1
?i??y(yi, y)??x(xi, x) (9)
where Y(x) ? Y is a finite set covering all po-
tential translations for the given source sentence
x, and kx(?) = (?x(?, xi)1?i?m) and ky(?) =
(?y(?, yi)1?i?m) are m? 1 column matrices.
A proper Y(x) can be generated according to a
lexicon that contains possible translations for every
component (word or phrase) in x. But the size of it
will grow exponentially with the length of x, which
poses implementation problem for a decoding algo-
rithm.
186
In earlier systems, several heuristic search meth-
ods were developed, of which a typical example
is Koehn (2004)?s beam search decoder for phrase-
based models. However, in our case, because of the
?y(y, y) item in Eq. 8 and the normalization opera-
tion in MMR, neither the expression in Eq. 8 nor
the one in Eq. 9 can be decomposed into a sum
of subfunctions each involving feature components
in a local area only. It means we cannot estimate
exactly how well a part of the source sentence is
translated, until we obtain a translation for the entire
sentence, which prevents us doing a straightforward
beam search similar to (Koehn, 2004).
To simplify the situation, we restrict the reorder-
ing (distortion) of phrases that yield the output sen-
tences by only allowing adjacent phrases to ex-
change their positions. (The discussion of this strat-
egy can be found in (Tillmann, 2004).) We use x[i:j]
and y[i:j] to denote the substrings of x and y that be-
gin with the ith word and end with the jth. Now, if
we go back to the implementation of a beam search,
the current distortion restriction guarantees that in
each expansion of the search states (hypotheses) we
have x[1:lx] translated to a y[1:ly], either like state (a)
or like state (b) in Fig. 2, where lx is the number of
words translated in the source sentence, and ly is the
number of words obtained in the translation.
We assume that if y is a good translation of x,
then y[1:ly] is a good translation of x[1:lx] as well. So
we can expect that the squared loss ?W?x(x[1:lx])?
?y(y[1:ly])?2 in the LSR is small, or the inner prod-
uct ???y(y[1:ly]),W??x(x[1:lx])?Hy in the MMR is
large, for the hypothesis yielding a good translation.
According to Eq. 8 and Eq. 9, the hypotheses in the
search stacks can thus be reranked with the follow-
ing score functions (Eq. 10: LSR, Eq. 11: MMR):
Score(x[1:lx], y[1:ly]) = (10)
?y(y[1:ly], y[1:ly])? 2ky(y[1:ly])K?1x kx(x[1:lx])
Score(x[1:lx], y[1:ly]) =
m
?
i=1
?i??y(yi, y[1:ly])??x(xi, x[1:lx]) (11)
Therefore, to solve the pre-image problem, we
just employ the same beam search algorithm as
(Koehn, 2004), except we limit the derivation of new
hypotheses with the distortion restriction mentioned
nous revenous aux questions
we return to questions
marqu?es ?(a)
(b)
marked
?nous revenous aux questions
we return to questions
marqu?es
Figure 2: Search states with the limited distortion.
above. However, our score functions will bring
more runtime complexities when compared with tra-
ditional probabilistic methods. The time complexity
of a naive implementation of the blended n-spectrum
string kernel between two sentences si and sj is
O(n|si||sj|), where |?| denotes the length of the sen-
tence. So the score function in Eq. 11 results in an
average runtime complexity of O(mnlyl), where l is
the average length of the sentences yi in the training
set. Note here ??x(x[1:lx], xi) can be pre-computed
for lx from 1 to |x| before the beam search, which
calls for O(m|x|) space. The average runtime com-
plexity of the score function in Eq. 10 will be the
same if we pre-compute K?1x kx(x[1:lx]).
4 Experimental Results
4.1 Resource Description
Baseline System To compare with previous work,
we take Pharaoh (Koehn, 2004) as a baseline system,
with its default settings (translation table size 10,
beam size 100). We train a trigram language model
with the SRILM toolkit (Stocke, 2002). Whilst, the
parameters for the maximum entropy model are de-
veloped based on the minimum error rate training
method (Och, 2003).
In the following experiments, to facilitate com-
parison, each time we train our regression models
and the language model and translation model for
Pharaoh on a common corpus, and use the same
phrase translation table as Pharaoh?s to decode our
systems. According to our preliminary experiments,
with the beam size of 100, the search errors of our
systems can be limited within 1.5%.
Corpora To evaluate our models, we randomly
take 12,000 sentences from the French-English por-
tion of the 1996?2003 Europarl corpus (Koehn,
2005) for scaling-up training, 300 for test (Test), and
300 for the development of Pharaoh (Dev). Some
187
Vocabulary Words Perplexity
Fr En Fr En Dev Test
4k 5084 4039 43k 39k 32.25 31.92
6k 6426 5058 64k 59k 30.81 29.03
8k 7377 5716 85k 79k 29.91 28.94
10k 8252 6339 106k 98k 27.55 27.09
12k 9006 6861 127k 118k 27.19 26.41
Table 1: Statistics of the corpora.
characteristics of the corpora are summarized in Ta-
ble 1.
4.2 Results
Based on the 4k training corpus, we test the per-
formance of the blended n-spectrum string kernel in
LSR and MMR using BLEU score, with n increas-
ing from 2 to 7. Fig. 3 shows the results. It can be
found that the performance becomes stable when n
reaches a certain value. Finally, we choose the 3-
spectrum for LSR, and the 5-spectrum for MMR.
Then we scale up the training set, and compare the
performance of our models with Pharaoh in Fig. 4.
We can see that the LSR model performs almost as
well as Pharaoh, whose differences of BLEU score
are within 0.5% when the training set is larger than
6k. But MMR model performs worse than the base-
line. With the training set of 12k, it is outperformed
by Pharaoh by 3.5%.
5 Discussions
Although at this stage the main contribution is
still conceptual, the capability of our approach to
be applied to machine translation is still demon-
strated. Comparable performance to previous work
is achieved by the LSR model.
But a main problem we face is to scale-up the
training set, as in practice the training set for SMT
will be much larger than several thousand sentences.
A method to speed up the training is proposed in
(Cortes et al, 2005). By approximating the Gram
matrix with a n ? m (n ? m) low-rank matrix,
the time complexity of the matrix inversion opera-
tion can be reduced from O(m3) to O(n2m). But
the space complexity of O(nm) in their algorithm is
still too expensive for SMT tasks. Subset selection
techniques could give a solution to this problem, of
2 3 4 5 6 7
26
28
30
32
34
36
38
40
42
MMR
LSR
Figure 3: BLEU(%) versus n-spectrum
4000 6000 8000 10000 12000
30
32
34
36
38
40
42
44
Pharaoh
LSR
MMR
Figure 4: BLEU(%) versus training set size
which we will leave the further exploration to future
work.
Acknowledgements
The authors acknowledge the support of the EU un-
der the IST project No. FP6-033917.
References
C. Cortes, M. Mohri, and J. Weston. 2005. A general re-
gression technique for learning transductions. In Proc.
of ICML?05.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. of AMTA 2004.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit X.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. J. Mach. Learn. Res., 2:419?444.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL?03.
A. Stocke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. of ICSLP?02.
S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.
2006. Learning via linear operators: Maximum mar-
gin regression; multiclass and multiview learning at
one-class complexity. Technical report, PASCAL,
Southampton, UK.
C. Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
NAACL?04.
188
Proceedings of the Third Workshop on Statistical Machine Translation, pages 155?158,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Kernel Regression Framework for Machine Translation: UCL System
Description for WMT 2008 Shared Translation Task
Zhuoran Wang
University College London
Dept. of Computer Science
Gower Street, London, WC1E 6BT
United Kingdom
z.wang@cs.ucl.ac.uk
John Shawe-Taylor
University College London
Dept. of Computer Science
Gower Street, London, WC1E 6BT
United Kingdom
jst@cs.ucl.ac.uk
Abstract
The novel kernel regression model for SMT
only demonstrated encouraging results on
small-scale toy data sets in previous works due
to the complexities of kernel methods. It is
the first time results based on the real-world
data from the shared translation task will be
reported at ACL 2008 Workshop on Statisti-
cal Machine Translation. This paper presents
the key modules of our system, including the
kernel ridge regression model, retrieval-based
sparse approximation, the decoding algorithm,
as well as language modeling issues under this
framework.
1 Introduction
This paper follows the work in (Wang et al, 2007;
Wang and Shawe-Taylor, 2008) which applied the
kernel regression method with high-dimensional
outputs proposed originally in (Cortes et al, 2005)
to statistical machine translation (SMT) tasks. In our
approach, the machine translation problem is viewed
as a string-to-string mapping, where both the source
and the target strings are embedded into their re-
spective kernel induced feature spaces. Then ker-
nel ridge regression is employed to learn the map-
ping from the input feature space to the output one.
As a kernel method, this model offers the potential
advantages of capturing very high-dimensional cor-
respondences among the features of the source and
target languages as well as easy integration of ad-
ditional linguistic knowledge via selecting particu-
lar kernels. However, unlike the sequence labeling
tasks such as optical character recognition in (Cortes
et al, 2005), the complexity of the SMT problem it-
self together with the computational complexities of
kernel methods significantly complicate the imple-
mentation of the regression technique in this field.
Our system is actually designed as a hybrid of
the classic phrase-based SMT model (Koehn et al,
2003) and the kernel regression model as follows:
First, for each source sentence a small relevant set of
sentence pairs are retrieved from the large-scale par-
allel corpus. Then, the regression model is trained
on this small relevant set only as a sparse approx-
imation of the regression hyperplane trained on the
entire training set, as proposed in (Wang and Shawe-
Taylor, 2008). Finally, a beam search algorithm is
utilized to decode the target sentence from the very
noisy output feature vector we predicted, with the
support of a pre-trained phrase table to generate pos-
sible hypotheses (candidate translations). In addi-
tion, a language model trained on a monolingual cor-
pus can be integrated either directly into the regres-
sion model or during the decoding procedure as an
extra scoring function.
Before describing each key component of our sys-
tem in detail, we give a block diagram overview in
Figure 1.
2 Problem Formulation
Concretely, the machine translation problem in our
method is formulated as follows. If we define a fea-
ture space Hx of our source language X , and define
the mapping ? : X ? Hx, then a sentence x ? X
can be expressed by its feature vector ?(x) ? Hx.
The definition of the feature space Hy of our target
language Y can be made in a similar way, with cor-
155
AlignmentParallel
Corpus
Retriever
Phrase Table
Phrase
Extraction
Kernel
Regression Decoder
Monolingual
Corpus
Language
Modeling
N-gram
Model
Target Text
Relevant Set
Source Text
Figure 1: System overview. The processes in gray blocks
are pre-performed for the whole system, while the white
blocks are online processes for each input sentence. The
two dash-line arrows represent two possible ways of lan-
guage model integration in our system described in Sec-
tion 6.
responding mapping ? : Y ? Hy. Now in the ma-
chine translation task, we are trying to seek a matrix
represented linear operator W, such that:
?(y) = W?(x) (1)
to predict the translation y for an arbitrary source
sentence x.
3 Kernel Ridge Regression
Based on a set of training samples, i.e. bilingual
sentence pairs S = {(xi,yi) : xi ? X ,yi ? Y, i =
1, . . . ,m.}, we use ridge regression to learn the W
in Equation (1), as:
min ?WM? ? M??2F + ??W?2F (2)
where M? = [?(x1), ...,?(xm)], M? =
[?(y1), ...,?(ym)], ? ? ?F denotes the Frobenius
norm that is a matrix norm defined as the square root
of the sum of the absolute squares of the elements in
that matrix, and ? is a regularization coefficient.
Differentiating the expression and setting it to
zero gives the explicit solution of the ridge regres-
sion problem:
W = M?(K? + ?I)?1M?? (3)
where I is the identity matrix, and K? =
M??M? = (??(xi,xj)1?i,j?m). Note here, we use
the kernel function:
??(xi,xj) = ??(xi),?(xj)? = ?(xi)??(xj) (4)
to denote the inner product between two feature vec-
tors. If the feature spaces are properly defined, the
?kernel trick? will allow us to avoid dealing with
the very high-dimensional feature vectors explicitly
(Shawe-Taylor and Cristianini, 2004).
Inserting Equation (3) into Equation (1), we ob-
tain our prediction as:
?(y) = M?(K? + ?I)?1k?(x) (5)
where k?(x) = (??(x,xi)1?i?m) is an m ? 1 col-
umn matrix. Note here, we will use the exact matrix
inversion instead of iterative approximations.
3.1 N -gram String Kernel
In the practical learning and prediction processes,
only the inner products of feature vectors are re-
quired, which can be computed with the kernel func-
tion implicitly without evaluating the explicit coor-
dinates of points in the feature spaces. Here, we de-
fine our features of a sentence as its word n-gram
counts, so that a blended n-gram string kernel can
be used. That is, if we denote by xi:j a substring
of sentence x starting with the ith word and ending
with the jth, then for two sentences x and z, the
blended n-gram string kernel is computed as:
?(x, z) =
n
?
p=1
|x|?p+1
?
i=1
|z|?p+1
?
j=1
[[xi:i+p?1 = zj:j+p?1]]
(6)
Here, | ? | denotes the length of the sentence, and
[[?]] is the indicator function for the predicate. In our
system, the blended tri-gram kernel is used, which
means we count the n-grams of length up to 3.
4 Retrieval-based Sparse Approximation
For SMT, we are not able to use the entire training
set that contains millions of sentences to train our
regression model. Fortunately, it is not necessary ei-
ther. Wang and Shawe-Taylor (2008) suggested that
a small set of sentences whose source is relevant to
the input can be retrieved, and the regression model
can be trained on this small-scale relevant set only.
156
Src n? y a-t-il pas ici deux poids , deux mesures
Rlv pourquoi y a-t-il deux poids , deux mesures
pourquoi deux poids et deux mesures
peut-e?tre n? y a-t-il pas d? e?pide?mie non
plus
pourquoi n? y a-t-il pas urgence
cette directive doit exister d? ici deux mois
Table 1: A sample input (Src) and some of the retrieved
relevant examples (Rlv).
In our system, we take each sentence as a docu-
ment and use the tf-idf metric that is frequently used
in information retrieval tasks to retrieve the relevant
set. Preliminary experiments show that the size of
the relevant set should be properly controlled, as if
many sentences that are not very close to the source
text are involved, they will correspond to adding
noise. Hence, we use a threshold of the tf-idf score
to filter the relevant set. On average, around 1500
sentence pairs are extracted for each source sen-
tence. Table 1 shows a sample input and some of
its top relevant sentences retrieved.
5 Decoding
After the regression, we have a prediction of the
target feature vector as in Equation (1). To ob-
tain the target sentence, a decoding algorithm is still
required to solve the pre-image problem. This is
achieved in our system by seeking the sentence y?
whose feature vector has the minimum Euclidean
distance to the prediction, as:
y? = arg min
y?Y(x)
?W?(x) ? ?(y)? (7)
where Y(x) ? Y denotes a finite set covering all
potential translations for the given source sentence
x. To obtain a smaller search space and more re-
liable translations, Y(x) is generated with the sup-
port of a phrase table extracted from the whole train-
ing set. Then a modified beam search algorithm
is employed, in which we restricted the distortion
of the phrases by only allowing adjacent phrases to
exchange their positions, and rank the search states
in the beams according to Equation (7) but applied
directly to the partial translations and their corre-
sponding source parts. A more detailed explanation
of the decoding algorithm can be found in (Wang
et al, 2007). In addition, Wang and Shawe-Taylor
(2008) further showed that the search error rate of
this algorithm is acceptable.
6 Language Model Integration
In previous works (Wang et al, 2007; Wang and
Shawe-Taylor, 2008), there was no language model
utilized in the regression framework for SMT, as
similar function can be achieved by the correspon-
dences among the n-gram features. It was demon-
strated to work well on small-scale toy data, how-
ever, real-world data are much more sparse and
noisy, where a language model will help signifi-
cantly.
There are two ways to integrate a language model
in our framework. First, the most straightforward so-
lution is to add a weight to adjust the strength of the
regression based translation scores and the language
model score during the decoding procedure. Alter-
natively, as language model is n-gram-based which
matches the definition of our feature space, we can
add a langauge model loss to the objective function
of our regression model as follows. We define our
language score for a target sentence y as:
LM(y) = V??(y) (8)
where V is a vector whose components Vy??y?y will
typically be log-probabilities logP (y|y??y?), and y,
y? and y?? are arbitrary words. Note here, in or-
der to match our blended tri-gram induced feature
space, we can make V of the same dimension as
?(y), while zero the components corresponding to
uni-grams and bi-grams. Then the regression prob-
lem can be defined as:
min ?WM??M??2F +?1?W?2F ??2V?WM?1
(9)
where ?2 is a coefficient balancing between the pre-
diction being close to the target feature vector and
being a fluent target sentence, and 1 denotes a vec-
tor with components 1. By differentiating the ex-
pression with respect to W and setting the result to
zero, we can obtain the explicit solution as:
W = (M? + ?2V1?)(K? + ?1I)?1M?? (10)
7 Experimental Results
Preliminary experiments are carried out on the
French-English portion of the Europarl corpus. We
157
System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER (%)
Kernel Regression 26.59 7.00 52.63 55.98 60.52 43.20
Moses 31.15 7.48 56.80 55.14 59.85 42.79
Table 3: Evaluations based on different metrics with comparison to Moses.
train our regression model on the training set, and
test the effects of different language models on the
development set (test2007). The results evaluated
by BLEU score (Papineni et al, 2002) is shown in
Table 2.
It can be found that integrating the language
model into the regression framework works slightly
better than just using it as an additional score com-
ponent during decoding. But language models of
higher-order than the n-gram kernel cannot be for-
mulated to the regression problem, which would be
a drawback of our system. Furthermore, the BLEU
score performance suggests that our model is not
very powerful, but some interesting hints can be
found in Table 3 when we compare our method with
a 5-gram language model to a state-of-the-art system
Moses (Koehn and Hoang, 2007) based on various
evaluation metrics, including BLEU score, NIST
score (Doddington, 2002), METEOR (Banerjee and
Lavie, 2005), TER (Snover et al, 2006), WER and
PER. It is shown that our system?s TER, WER and
PER scores are very close to Moses, though the
gaps in BLEU, NIST and METEOR are significant,
which suggests that we would be able to produce ac-
curate translations but might not be good at making
fluent sentences.
8 Conclusion
This work is a novel attempt to apply the advanced
kernel method to SMT tasks. The contribution at this
stage is still preliminary. When applied to real-world
data, this approach is not as powerful as the state-of-
the-art phrase-based log-linear model. However, in-
teresting prospects can be expected from the shared
translation task.
Acknowledgements
This work is supported by the European Commis-
sion under the IST Project SMART (FP6-033917).
no-LM LM13gram LM
2
3gram LM
1
5gram
BLEU 23.27 25.19 25.66 26.59
Table 2: BLEU score performance of different language
models. LM1 denotes adding the language model dur-
ing decoding process, while LM2 represents integrating
the language model into the regression framework as de-
scribed in Problem (9).
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2005. A general regression technique for learning
transductions. In Proc. of ICML?05.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. of HLT?02, pages 138?145.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proc. of EMNLP-CoNLL?07.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HAACL-HLT?03, pages 48?54.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proc. of ACL?02.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA?06.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel-
based machine translation. In Cyril Goutte, Nicola
Cancedda, Marc Dymetman, and George Foster, edi-
tors, Learning Machine Translation. MIT Press, to ap-
pear.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine transla-
tion. In Proc. of NAACL-HLT?07, Short Paper Volume,
pages 185?188.
158
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57?67,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Policy Learning for Domain Selection in an Extensible Multi-domain
Spoken Dialogue System
Zhuoran Wang
Mathematical & Computer Sciences
Heriot-Watt University
Edinburgh, UK
zhuoran.wang@hw.ac.uk
Hongliang Chen, Guanchun Wang
Hao Tian, Hua Wu
?
, Haifeng Wang
Baidu Inc., Beijing, P. R. China
SurnameForename@baidu.com
?
wu hua@baidu.com
Abstract
This paper proposes a Markov Decision
Process and reinforcement learning based
approach for domain selection in a multi-
domain Spoken Dialogue System built on
a distributed architecture. In the proposed
framework, the domain selection prob-
lem is treated as sequential planning in-
stead of classification, such that confir-
mation and clarification interaction mech-
anisms are supported. In addition, it is
shown that by using a model parameter ty-
ing trick, the extensibility of the system
can be preserved, where dialogue com-
ponents in new domains can be easily
plugged in, without re-training the domain
selection policy. The experimental results
based on human subjects suggest that the
proposed model marginally outperforms a
non-trivial baseline.
1 Introduction
Due to growing demand for natural human-
machine interaction, over the last decade Spo-
ken Dialogue Systems (SDS) have been increas-
ingly deployed in various commercial applications
ranging from traditional call centre automation
(e.g. AT&T ?Lets Go!? bus information sys-
tem (Williams et al., 2010)) to mobile personal
assistants and knowledge navigators (e.g. Ap-
ple?s Siri
R
?
, Google Now
TM
, Microsoft Cortana,
etc.) or voice interaction for smart household ap-
pliance control (e.g. Samsung Evolution Kit for
Smart TVs). Furthermore, latest progress in open-
vocabulary Automatic Speech Recognition (ASR)
is pushing SDS from traditional single-domain in-
formation systems towards more complex multi-
domain speech applications, of which typical ex-
amples are those voice assistant mobile applica-
tions.
Recent advances in SDS have shown that sta-
tistical approaches to dialogue management can
result in marginal improvement in both the nat-
uralness and the task success rate for domain-
specific dialogues (Lemon and Pietquin, 2012;
Young et al., 2013). State-of-the-art statistical
SDS treat the dialogue problem as a sequential
decision making process, and employ established
planning models, such as Markov Decision Pro-
cesses (MDPs) (Singh et al., 2002) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Thomson and Young, 2010; Young et al., 2010;
Williams and Young, 2007), in conjunction with
reinforcement learning techniques (Jur?c???cek et al.,
2011; Jur?c???cek et al., 2012; Ga?si?c et al., 2013a)
to seek optimal dialogue policies that maximise
long-term expected (discounted) rewards and are
robust to ASR errors.
However, to the best of our knowledge, most of
the existing multi-domain SDS in public use are
rule-based (e.g. (Gruber et al., 2012; Mirkovic
and Cavedon, 2006)). The application of statistical
models in multi-domain dialogue systems is still
preliminary. Komatani et al. (2006) and Nakano
et al. (2011) utilised a distributed architecture (Lin
et al., 1999) to integrate expert dialogue systems in
different domains into a unified framework, where
a central controller trained as a data-driven clas-
sifier selects a domain expert at each turn to ad-
dress user?s query. Alternatively, Hakkani-T?ur et
al. (2012) adopted the well-known Information
State mechanism (Traum and Larsson, 2003) to
construct a multi-domain SDS and proposed a dis-
criminative classification model for more accurate
state updates. More recently, Ga?si?c et al. (2013b)
proposed that by a simple expansion of the kernel
function in Gaussian Process (GP) reinforcement
learning (Engel et al., 2005; Ga?si?c et al., 2013a),
one can adapt pre-trained dialogue policies to han-
dle unseen slots for SDS in extended domains.
In this paper, we use a voice assistant applica-
57
User Interface Manager
ASR
User Intention
Identier Central Controller
SLU NLG
Domain Expert
(Travel Info)
SLU NLG
Domain Expert
(Restaurant Search)
SLU NLG
Domain Expert
(Movie Search)
SLU NLG
Domain Expert
(etc.)
Web 
Search
Weather
Report
QA
etc.
Ou
t-o
f-d
om
ain
 Se
rvi
ce
s
Service
Ranker
Mobile Devices
Flight Ticket 
Booking
Train Ticket 
Booking
Hotel  
Booking
speech
text
text, clicks
query,
intention label,
condence
TTS Web PageRendering etc.
Figure 1: The distributed architecture of the voice assistant system (a simplified illustration).
tion (similar to Apple?s Siri but in Chinese lan-
guage) as an example to demonstrate a novel
MDP-based approach for central interaction man-
agement in a complex multi-domain dialogue sys-
tem. The voice assistant employs a distributed ar-
chitecture similar to (Lin et al., 1999; Komatani et
al., 2006; Nakano et al., 2011), and handles mixed
interactions of multi-turn dialogues across differ-
ent domains and single-turn queries powered by
a collection of information access services (such
as web search, Question Answering (QA), etc.).
In our system, the dialogues in each domain are
managed by an individual domain expert SDS, and
the single-turn services are used to handle those
so-called out-of-domain requests. We use fea-
turised representations to summarise the current
dialogue states in each domain (see Section 3 for
more details), and let the central controller (the
MDP model) choose one of the following system
actions at each turn: (1) addressing user?s query
based on a domain expert, (2) treating it as an
out-of-domain request, (3) asking user to confirm
whether he/she wants to continue a domain ex-
pert?s dialogue or to switch to out-of-domain ser-
vices, and (4) clarifying user?s intention between
two domains. The Gaussian Process Temporal
Difference (GPTD) algorithm (Engel et al., 2005;
Ga?si?c et al., 2013a) is adopted here for policy op-
timisation based on human subjects, where a pa-
rameter tying trick is applied to preserve the ex-
tensibility of the system, such that new domain
experts (dialogue systems) can be flexibly plugged
in without the need of re-training the central con-
troller.
Comparing to the previous classification-based
methods (Komatani et al., 2006; Nakano et al.,
2011), the proposed approach not only has the
advantage of action selection in consideration of
long-term rewards, it can also yield more robust
policies that allow clarifications and confirmations
to mitigate ASR and Spoken Language Under-
standing (SLU) errors. Our human evaluation re-
sults show that the proposed system with a trained
MDP policy achieves significantly better natural-
ness in domain switching tasks than a non-trivial
baseline with a hand-crafted policy.
The remainder of this paper is organised as
follows. Section 2 defines the terminology used
throughout the paper. Section 3 briefly overviews
the distributed architecture of our system. The
MDP model and the policy optimisation algorithm
are introduced in Section 4 and Section 5, respec-
tively. After this, experimental settings and eval-
uation results are described in Section 6. Finally,
we discuss some possible improvements in Sec-
tion 7 and conclude ourselves in Section 8.
2 Terminology
A voice assistant application provides a unified
speech interface to a collection of individual infor-
mation access systems. It aims to collect and sat-
isfy user requests in an interactive manner, where
58
different types of interactions can be involved.
Here we focus ourselves on two interaction scenar-
ios, i.e. task-oriented (multi-turn) dialogues and
single-turn queries.
According to user intentions, the dialogue inter-
actions in our voice assistant system can further be
categorised into different domains, of which each
is handled by a separate dialogue manager, namely
a domain expert. Example domains include travel
information, restaurant search, etc. In addition,
some domains in our system can be further de-
composed into sub-domains, e.g. the travel in-
formation domain consists of three sub-domains:
flight ticket booking, train ticket booking and hotel
reservation. We use an integrated domain expert to
address queries in all its sub-domains, so that rel-
evant information can be shared across those sub-
domains to allow intelligent induction in the dia-
logue flow.
For convenience of future reference, we call
those single-turn information access systems out-
of-domain services or simply services for short.
The services integrated in our system include web
search, semantic search, QA, system command ex-
ecution, weather report, chat-bot, and many more.
3 System Architecture
The voice assistant system introduced in this pa-
per is built on a distributed architecture (Lin et al.,
1999), as shown in Figure 1, where the dialogue
flow is processed as follows. Firstly, a user?s query
(either an ASR utterance or directly typed in text)
is passed to a user intention identifier, which la-
bels the raw query with a list of intention hypothe-
ses with confidence scores. Here an intention label
could be either a domain name or a service name.
After this, the central controller distributes the raw
query together with its intention labels and confi-
dence scores to all the domain experts and the ser-
vice modules, which will attempt to process the
query and return their results to the central con-
troller.
The domain experts in the current implementa-
tion of our system are all rule-based SDS follow-
ing the RavenClaw framework proposed in (Bo-
hus and Rudnicky, 2009). When receiving a query,
a domain expert will use its own SLU module to
parse the utterance or text input and try to update
its dialogue state in consideration of both the SLU
output and the intention labels. If the dialogue
state in the domain expert can be updated given
the query, it will return its output, internal ses-
sion record and a confidence score to the central
controller, where the output can be either a natu-
ral language utterance realised by its Natural Lan-
guage Generation (NLG) module or a set of data
records obtained from its database (if a database
search operation is triggered), or both. If the do-
main expert cannot update its state using the cur-
rent query, it will just return an empty result with
a low confidence score. Similar procedures ap-
ply to those out-of-domain services as well, but
there are no session records or confidence scores
returned. Finally, given all the returned informa-
tion, the central controller chooses, according to
its policy, the module (either a domain expert or a
service) whose results will be provided to the user.
When the central controller decides to pass a
domain expert?s output to the user, we regard the
domain expert as being activated. Also note here,
the updated state of a domain expert in a turn will
not be physically stored, unless the domain expert
is activated in that turn. This is a necessary mech-
anism to prevent an inactive domain expert being
misled by ambiguous queries in other domains.
In addition, we use a well-engineered priority
ranker to rank the services based on the num-
bers of results they returned as well as some prior
knowledge about the quality of their data sources.
When the central controller decides to show user
the results from an out-of-domain service, it will
choose the top one from the ranked list.
4 MDP Modelling of the Central Control
Process
The main focus of this paper is to seek a policy for
robustly switching the control flow among those
domain experts and services (the service ranker in
practice) during a dialogue, where the user may
have multiple or compound goals (e.g. booking a
flight ticket, booking a restaurant in the destina-
tion city and checking the weather report of the
departure or destination city).
In order to make the system robust to ASR er-
rors or ambiguous queries, the central controller
should also have basic dialogue abilities for confir-
mation and clarification purposes. Here we define
the confirmation as an action of asking whether a
user wants to continue the dialogue in a certain do-
main. If the system receives a negative response at
this point, it will switch to out-of-domain services.
On the other hand, the clarification action is de-
59
fined between domains, in which case, the system
will explicitly ask the user to choose between two
domain candidates before continuing the dialogue.
Due to the confirmation and clarification mech-
anisms defined above, the central controller be-
comes a sequential decision maker that must take
the overall smoothness of the dialogue into ac-
count. Therefore, we propose an MDP-based ap-
proach for learning an optimal central control pol-
icy in this section.
The potential state space of our MDP is huge,
which in principle consists of the combinations of
all possible situations of the domain experts and
the out-of-domain services, therefore function ap-
proximation techniques must be employed to en-
able tractable computations. However, when de-
veloping such a complex application as the voice
assistant here, one also needs to take the extensi-
bility of the system into account, so that new do-
main experts can be easily integrated into the sys-
tem without major re-training or re-engineering of
the existing components. Essentially, it requires
the state featurisation and the central control pol-
icy learnt here to be independent of the number of
domain experts. In Section 4.3, we show that such
a property can be achieved by a parameter tying
trick in the definition of the MDP.
4.1 MDP Preliminaries
Let P
X
denote the set of probability distributions
over a set X . An MDP is defined as a five tuple
?S,A, T,R, ??, where the components are defined
as follows. S and A are the sets of system states
and actions, respectively. T : S ? A ? P
S
is the
transition function, and T (s
?
|s, a) defines the con-
ditional probability of the system transiting from
state s ? S to state s
?
? S after taking action
a ? A. R : S ? A ? P
R
is the reward function
with R(s, a) specifying the distribution of the im-
mediate rewards for the system taking action a at
state s. In addition, 0 ? ? ? 1 is the discount
factor on the summed sequence of rewards.
A finite-horizon MDP operates as follows. The
system occupies a state s and takes an action a,
which then will make it transit to a next state s
?
?
T (?|s, a) and receive a reward r ? R(s, a). This
process repeats until a terminal state is reached.
For a given policy pi : S ? A, the value
function V
pi
is defined to be the expected cumula-
tive reward, as V
pi
(s
0
) = E
[
?
n
t=0
?
t
r
t
|
s
t
,pi(s
t
)
]
,
where s
0
is the starting state and n is the plan-
ning horizon. The aim of policy optimisation is
to seek an optimal policy pi
?
that maximises the
value function. If T and R are given, in conjunc-
tion with a Q-function, the optimal value V
?
can
be expressed by recursive equations as Q(s, a) =
R(s, a) + ?
?
s
?
?S
T (s
?
|s, a)V
?
(s
?
) and V
?
(s) =
max
a?A
Q(s, a) (here we assume R(s, a) is de-
terministic), which can be solved by dynamic pro-
gramming (Bellman, 1957). For problems with
unknown T or R, such as dialogue systems, the
Q-values are usually estimated via reinforcement
learning (Sutton and Barto, 1998).
4.2 Problem Definition
Let D denote the set of the domain experts in our
voice assistant system, and s
d
be the current di-
alogue state of domain expert d ? D at a certain
timestamp. We also define s
o
as an abstract state to
describe the current status of those out-of-domain
services. Then mathematically we can represent
the central control process as an MDP, where its
state s is a joint set of the states of all the domain
experts and the services, as s = {s
d
}
d?D
? {s
o
}.
Four types of system actions are defined as fol-
lows.
? present(d): presenting the output of do-
main expert d to user;
? present ood(null): presenting the re-
sults of the top-ranked out-of-domain service
given by the service ranker;
? confirm(d): confirming whether user
wants to continue with domain expert d (or
to switch to out-of-domain services);
? clarify(d,d
?
): asking user to clarify
his/her intention between domains d and d
?
.
For convenience of notations, we use a(x) to
denote a system action of our MDP, where a ?
{present,present ood,confirm,clarify},
x ? {d,null, (d, d
?
)}
d,d
?
?D,d6=d
?
, x = null
only applies to present ood, and x = (d, d
?
)
only applies to clarify actions.
4.3 Function Approximation
Function approximation is a commonly used tech-
nique to estimate the Q-values when the state
space of the MDP is huge. Concretely, in our case,
we assume that:
Q(s, a(x)) = f(?(s, a(x)); ?) (1)
60
where ? : S ? A ? R
K
is a feature function
that maps a state-action pair to an K-dimensional
feature vector, and f : R
K
? R is a function of
?(s, a(x)) parameterised by ?. A frequent choice
of f is the linear function, as:
Q(s, a(x)) = ?
>
?(s, a(x)) (2)
After this, the policy optimisation problem be-
comes learning the parameter ? to approximate the
Q-values based on example dialogue trajectories.
However, a crucial problem with the standard
formulation in Eq. (2) is that the feature function
? is defined over the entire state and action spaces.
In this case, when a new domain expert is inte-
grated into the system, both the state space and the
action space will be changed, therefore one will
have to re-define the feature function and conse-
quentially re-train the model. In order to achieve
an extensible system, we make some simplifica-
tion assumptions and decompose the feature func-
tion as follows. Firstly, we let:
?(s, a(x)) = ?
a
(s
x
) (3)
=
?
?
?
?
?
?
?
?
pr
(s
d
) if a(x) =present(d)
?
ood
(s
o
) if a(x) =present ood()
?
cf
(s
d
) if a(x) =confirm(d)
?
cl
(s
d
, s
d
?
) if a(x) =clarify(d,d
?
)
where the feature function is reduced to only de-
pend on the state of the action?s operand, instead
of the entire system state. Then, we make those ac-
tions a(x) that have a same action type (a) but op-
erate different domain experts (x) share the same
parameter, i.e.:
Q(s, a(x)) = ?
>
a
?
a
(s
x
) (4)
This decomposition and parameter tying trick pre-
serves the extensibility of the system, because both
?
>
a
and ?
a
are independent of x, when there is a
new domain expert
?
d, we can directly substitute
its state s
?
d
into Eq. (3) and (4) to compute its cor-
responding Q-values.
4.4 Features
Based on the problem formulation in Eq. (3) and
(4), we shall only select high-level summary fea-
tures to sketch the dialogue state and dialogue his-
tory of each domain expert, which must be ap-
plicable to all domain experts, regardless of their
domain-specific characteristics or implementation
differences. Suppose that the dialogue states of the
# Feature Range
1
the number of unfilled
required slots of a domain
expert
{0, . . . ,M}
2
the number of filled required
slots of a domain expert
{0, . . . ,M}
3
the number of filled optional
slots of a domain expert
{0, . . . , L}
4
whether a domain expert has
executed a database search
{0, 1}
5
the confidence score
returned by a domain expert
[0, 1.2]
6
the total number of turns that
a domain expert has been
activated during a dialogue
Z
+
7
e
?t
a
where t
a
denotes the
relative turn of a domain
expert being last activated,
or 0 if not applicable
[0, 1]
8
e
?t
c
where t
c
denotes the
relative turn of a domain
expert being last confirmed,
or 0 if not applicable
[0, 1]
9
the summed confidence
score from the user intention
identifier of a query being
for out-of-domain services
[0, 1.2N ]
Table 1: A list of all features used in our model.
M and L respectively denote the maximum num-
bers of required and optional slots for the domain
experts. N is the maximum number of hypotheses
that the intention identifier can return. Z
+
stands
for the non-negative integer set.
domain experts can be represented as slot-value
pairs
1
, and for each domain there are required slots
and optional slots, where all required slots must
be filled before the domain expert can execute a
database search operation. The features investi-
gated in the proposed framework are listed in Ta-
ble 1.
Detailed featurisation in Eq. (3) is explained
as follows. For ?
pr
, we choose the first 8 fea-
tures plus a bias dimension that is always set to
1
This is a rather general assumption. Informally speak-
ing, for most task-oriented SDS, one can extract a slot-value
representation from their dialogue models, of which exam-
ples include the RavenClaw architecture (Bohus and Rud-
nicky, 2009), the Information State dialogue engine (Traum
and Larsson, 2003), MDP-SDS (Singh et al., 2002) or
POMDP-SDS (Thomson and Young, 2010; Young et al.,
2010; Williams and Young, 2007).
61
?1. Whilst, feature #9 plus a bias is used to de-
fine ?
ood
. All the features are used in ?
cf
, as to
do a confirmation, one needs to consider the joint
situation in and out of the domain. Finally, the
feature function for a clarification action between
two domains d and d
?
is defined as ?
cl
(s
d
, s
d
?
) =
exp{?|?
pr
(s
d
) ? ?
pr
(s
d
?
)|}, where we use | ? |
to denote the element-wise absolute of a vector
operand. The intuition here is that the more dis-
tinguishable the (featurised) states of two domain
experts are, the less we tend to clarify them.
For those domain experts that have multiple
sub-domains with different numbers of required
and optional slots, the feature extraction procedure
only applies to the latest active sub-domain.
In addition, note that, the confidence scores pro-
vided by the user intention identifier are only used
as features for out-of-domain services. This is be-
cause in the current version of our system, the con-
fidence estimation of the intention identifier for
domain-dependent dialogue queries is less reliable
due to the lack of context information. In contrast,
the confidence scores returned by the domain ex-
perts will be more informative at this point.
5 Policy Learning with GPTD
In traditional statistical SDS, dialogue policies are
usually trained using reinforcement learning based
on simulated dialogue trajectories (Schatzmann
et al., 2007; Keizer et al., 2010; Thomson and
Young, 2010; Young et al., 2010). Although the
evaluation of the simulators themselves could be
an arguable issue, there are various advantages,
e.g. hundreds of thousands of data examples can
be easily generated for training and initial policy
evaluation purposes, and different reinforcement
learning models can be compared without incur-
ring notable extra costs.
However, for more complex multi-domain SDS,
particularly a voice assistant application like ours
that aims at handling very complicated (ideally
open-domain) dialogue scenarios, it would be dif-
ficult to develop a proper simulator that can rea-
sonably mimic real human behaviours. There-
fore, in this work, we learn the central control
policy directly with human subjects, for which
the following properties of the learning algorithm
are required. Firstly and most importantly, the
learner must be sample-efficient as the data collec-
tion procedure is costly. Secondly, the algorithm
should support batch reinforcement learning. This
is because when using function approximation, the
learning process may not strictly converge, and the
quality of the sequence of generated policies tends
to oscillate after a certain number of improving
steps at the beginning (Bertsekas and Tsitsiklis,
1996). If online reinforcement learning is used,
we will be unable to evaluate the generated policy
after each update, and hence will not know which
policy to keep for the final evaluation. Therefore,
we do a batch policy update and iterate the learn-
ing process for a number of batches, such that the
data collection phase in a new iteration yields an
evaluation of the policy obtained from the previ-
ous iteration at the same time.
To fulfill the above two requirements, the Gaus-
sian Process Temporal Difference (GPTD) algo-
rithm (Engel et al., 2005) is a proper choice, due to
its sample efficiency (Fard et al., 2011) and batch
learning ability (Engel et al., 2005), as well as its
previous success in dialogue policy learning with
human subjects (Ga?si?c et al., 2013a). Note that,
GPTD can also admit recursive (online) compu-
tations, but here we focus ourselves on the batch
version.
A Gaussian Process (GP) is a generative model
of Bayesian inference that can be used for func-
tion regression, and has the superiority of obtain-
ing good posterior estimates with just a few obser-
vations (Rasmussen and Williams, 2006). GPTD
models the Q-function as a zero mean GP which
defines correlations in different parts of the fea-
turised state and action spaces through a kernel
function ?, as:
Q(s, a(x)) ? GP(0, ?((s
x
, a), (s
x
, a))) (5)
Given a sequence of t state-action pairs X
t
=
[(s
0
, a
0
(x
0
)), . . . , (s
t
, a
t
(x
t
))] from a collection
of dialogues and their corresponding immedi-
ate rewards r
t
= [r
0
, . . . , r
t
], the posterior of
Q(s, a(x)) for an arbitrary new state-action pair
(s, a(x)) can be computed as:
Q(s, a(x))|
X
t
,r
t
? N
(
?
Q(s, a(x)), cov (s, a(x))
)
(6)
?
Q(s, a(x)) = k
t
(s
x
, a)
>
H
>
t
G
?1
t
r
t
(7)
cov (s, a(x)) = ?((s
x
, a), (s
x
, a))
? k
t
(s
x
, a)
>
H
>
t
G
?1
t
H
t
k
t
(s
x
, a) (8)
G
t
= H
t
K
t
H
>
t
+ ?
2
H
t
H
>
t
(9)
62
Ht
=
?
?
?
?
?
1 ?? ? ? ? 0 0
0 1 ? ? ? 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 ? ? ? 0 1 ??
?
?
?
?
?
(10)
where K
t
is the Gram matrix with elements
K
t
(i, j) = ?((s
i
x
i
, a
i
), (s
j
x
j
, a
j
)), k
t
(s
x
, a) =
[?((s
i
x
i
, a
i
), (s
x
, a))]
t
i=0
is a vector, and ? is a
hyperparameter specifying the diagonal covari-
ance values of the zero-mean Gaussian noise. In
addition, we use cov (s, a(x)) to denote (for short)
the self-covariance cov (s, a(x), s, a(x)).
In our case, as different feature functions ?
a
are
defined for different action types, the kernel func-
tion is defined to be:
?((s
x
, a), (s
?
x
?
, a
?
)) = [[a = a
?
]]?
a
(s
x
, s
?
x
?
) (11)
where [[?]] is an indicator function and ?
a
is the ker-
nel function defined corresponding to the feature
function ?
a
.
Given a state, a most straightforward policy is
to select the action that corresponds to the max-
imum mean Q-value estimated by the GP. How-
ever, since the objective is to learn the Q-function
associated with the optimal policy by interacting
directly with users, the policy must exhibit some
form of stochastic behaviour in order to explore
alternatives during the process of learning. In this
work, the strategy employed for the exploration-
exploitation trade-off is that, during exploration,
actions are chosen according to the variance of
the GP estimate for the Q-function, and during
exploitation, actions are chosen according to the
mean. That is:
pi(s) =
{
arg max
a(x)
?
Q(s, a(x)) : w.p. 1? 
arg max
a(x)
cov (s, a(x)) : w.p. 
(12)
where 0 <  < 1 is a pre-defined exploration rate,
and will be exponentially reduced at each batch
iteration during our learning process.
Note that, in practice, not all the actions are
valid at every possible state. For example, if a do-
main expert d has never been activated during a
dialogue and can neither process the user?s current
query, the actions with an operand d will be re-
garded as invalid at this state. When executing the
policy, we only consider those valid actions for a
given state.
Score Interpretation
5
The domain selections are totally
correct, and the entire dialogue flow
is fluent.
4
The domain selections are totally
correct, but the dialogue flow is
slightly redundant.
3
There are accidental domain
selections errors, or the dialogue
flow is perceptually redundant.
2
There are frequent domain selections
errors, or the dialogue flow is
intolerably redundant.
1
Most domain selections are
incorrect, or the dialogue is
incompletable.
Table 2: The scoring standard in our experiments.
6 Experimental Results
6.1 Training
We use the batch version of GPTD as described
in Section 5 to learn the central control policy
with human subjects. There are three domain ex-
perts available in our current system, but during
the training only two domains are used, which are
the travel information domain and the restaurant
search domain. We reserve a movie search domain
for evaluating the generalisation property of the
learnt policy (see Section 6.2). The learning pro-
cess started from a hand-crafted policy. Then 15
experienced users
2
volunteered to contribute dia-
logue examples with multiple or compound goals
(see Figure 4 for an instance), from whom we
collected around 50?70 dialogues per day for 5
days
3
. After each dialogue, the users were asked
to score the system from 5 to 1 according to a scor-
ing standard shown in Table 2. The scores were
taken as the (delayed) rewards to train the GPTD
model, where we set the rewards for intermediate
turns to 0. The working policy was updated daily
based on the data obtained up to that day. The
data collected on the first day was used for pre-
liminary experiments to choose the hyperparame-
2
Overall user satisfactions may rely on various aspects of
the entire system, e.g. the data source quality of the services,
the performance of each domain expert, etc. It will be diffi-
cult to make non-experienced users to score the central con-
troller isolatedly.
3
Not all the users participated the experiments everyday.
There were 311 valid dialogues received in total, with an av-
erage length of 9 turns.
63
2	 ?
3	 ?
4	 ?
5	 ?
Figure 2: Average scores and standard deviations
during policy iteration.
0.7	 ?
0.72	 ?
0.74	 ?
0.76	 ?
0.78	 ?
0.8	 ?
0.82	 ?
0.84	 ?
0.86	 ?
0.88	 ?
0.9	 ?
Figure 3: Domain selection accuracies during pol-
icy iteration.
ters of the model, such as the kernel function, the
kernel parameters (if applicable), and ? and ? in
the GPTD model. We initially experimented with
linear, polynomial and Gaussian kernels, with dif-
ferent configurations of ? and ? values, as well
as kernel parameter values. It was found that
the linear kernel in combination with ? = 5 and
? = 0.99 works more appropriate than the other
settings. This configuration was then fixed for the
rest iterations.
The learning process was iterated for 4 days af-
ter the first one. On each day, we computed the
mean and standard deviation of the user scores
as an evaluation of the policy learnt on the pre-
vious day. The learning curve is illustrated in Fig-
ure 2. Note here, as we were actually executing a
stochastic policy according to Eq. (12), to calcu-
late the values in Figure 2 we ignored those dia-
logues that contain any actions selected due to the
exploration. We also manually labelled the cor-
rectness of domain selection at every turn of the
dialogues. The domain selection accuracies of the
obtained policy sequence are shown in Figure 3,
where similarly, those exploration actions as well
Policy
Scenario
Baseline GPTD
p-value
(i) 4.5?0.8 4.2?0.8 0.387
(ii) 3.4?0.9 4.2?0.8 0.018
(iii) 4.1?1.0 4.3?1.0 0.0821
(iv) 3.9?1.1 4.5?0.8 0.0440
Table 3: Paired comparison experiments between
the system with a trained GPTD policy and the
rule-based baseline.
as the clarification and confirmation actions were
excluded from the calculations. Although the do-
main selection accuracy is not the target that our
learning algorithm aims to optimise, it reflects the
quality of the learnt policies from a different angle
of view.
It can be found in Figure 2 that the best policy
was obtained in the third iteration, and after that
the policy quality oscillated. The same finding is
indicated in Figure 3 as well, when we use the do-
main selection accuracy as the evaluation metric.
Therefore, we kept the policy corresponding to the
peak point of the learning curve for the compari-
son experiments below.
6.2 Comparison Experiments
We conducted paired comparison experiments in
four scenarios to compare between the system
with the GPTD-learnt central control policy and a
non-trivial baseline. The baseline is a publicly de-
ployed version of the voice assistant application.
The central control policy of the baseline system is
handcrafted, which has a separate list of semantic
matching rules for each domain to enable domain
switching.
The first two scenarios aim to test the perfor-
mance of the two systems on (i) switching between
a domain expert and out-of-domain services, and
(ii) switching between two domain experts, where
only the two training domains (travel information
and restaurant search) were considered. Scenar-
ios (iii) and (iv) are similar to scenarios (i) and (ii)
respectively, but at this time, the users were re-
quired to carry out the tests surrounding the movie
search domain (which is addressed by a new do-
main expert not used in the training phase). There
were 13 users who participated this experiment.
In each scenario, every user was required to test
the two systems with an identical goal and similar
queries. After each test, the users were asked to
64
score the two systems separately according to the
scoring standard in Table 2.
The average scores received by the two systems
are shown in Table 3, where we also compute the
statistical significance (the p-values) of the results
based on paired t-tests. It can be found that the
learnt policy works significantly better than the
rule-based policy in scenarios (ii) and (iv), but in
scenarios (i) and (iii) the differences between two
systems are statistically insignificant. Moreover,
the learnt policy preserves the extensibility of the
entire system as expected, of which strong evi-
dences are given by the results in scenarios (iii)
and (iv).
6.3 Policy Analysis
To better understand the policy learnt by the
GPTD model, we look into the obtained weight
vectors, as shown in Table 4. It can be found that
confidence score (#5) is an informative feature for
all the system actions, while the relative turn of a
domain being last activated (#7) is an additional
strong evidence for a confirmation decision. In
addition, the similarity between the dialogue com-
pletion status (#1 & #2) of two ambiguous domain
experts and the relative turns of them being last
confirmed (#8) tend to be extra dominating fea-
tures for clarification decisions, besides the close-
ness of the confidence scores returned by the two
domain experts.
A less noticeable but important phenomenon is
observed for feature #6, i.e. the total number of
active turns of a domain expert during a dialogue.
Concretely, feature #6 has a small negative effect
on presentation actions but a small positive con-
tribution to confirmation actions. Such weights
could correspond to the discount factor?s penalty
to long dialogues in the value function. How-
ever, it implies an unexpected effect in extreme
cases, which we explain in detail as follows. Al-
though the absolute weights for feature #6 are tiny
for both presentation and confirmation actions, the
feature value will grow linearly during a dialogue.
Therefore, when a dialogue in a certain domain
last rather long, there tend to be very frequent con-
firmations. A possible solution to this problem
could be either ignoring feature #6 or twisting it to
some nonlinear function, such that its value stops
increasing at a certain threshold point. In addition,
to cover sufficient amount of those ?extreme? ex-
amples in the training phase could also be an alter-
Feature Weights
#
present confirm clarify
1 0.09 0.02 0.60
p
r
e
s
e
n
t
o
o
d
2 0.20 0.29 0.53
3 0.18 0.29 0.16
4 -0.10 0.16 0.25
5 0.75 0.57 0.54
6 -0.02 0.11 0.13
7 0.25 1.19 0.36
8 -0.22 -0.19 0.69
9 ? 0.20 ? 0.47
Bias -1.79 ? ? -2.42
Table 4: Feature weights learnt by GPTD. See Ta-
ble 1 for the meanings of the features.
native solution, as our current training set contains
very few examples that exhibit extraordinary long
domain persistence.
7 Further Discussions
The proposed approach is a rather general frame-
work to learn extensible central control policies
for multi-domain SDS based on distributed archi-
tectures. It does not rely on any internal represen-
tations of those individual domain experts, as long
as a unified featurisation of their dialogue states
can be achieved.
However, from the entire system point of view,
the current implementation is still preliminary.
Particularly, the confirmation and clarification
mechanisms are isolated, for which the surface re-
alisations sometimes may sound stiff. This phe-
nomenon explains one of the reasons that make
the proposed system slightly less preferred by the
users than the baseline in scenario (i), when the
interaction flows are relative simple. A possi-
ble improvement here could be associating the
confirmation and clarification actions in the cen-
tral controller to the error handling mechanisms
within each domain expert, and letting domain ex-
perts generate their own utterances on receiving a
confirmation/clarification request from the central
controller.
Online reinforcement learning with real user
cases will be another undoubted direction of fur-
ther improvement of our system. The key chal-
lenge here is to automatically estimate user?s satis-
factions, which will be transformed to the rewards
for the reinforcement learners. Strong feedbacks
such as clicks or actual order placements can be
65
collected. But to regress user?s true satisfaction,
other environment features must also be taken into
account. Practical solutions are still an open issue
at this stage, and are left to our future work.
8 Conclusion
In this paper, we introduce an MDP framework
for learning domain selection policies in a com-
plex multi-domain SDS. Standard problem for-
mulation is modified with tied model parameters,
so that the entire system is extensible and new
domain experts can be easily integrated without
re-training the policy. This expectation is con-
firmed by empirical experiments with human sub-
jects, where the proposed system marginally beats
a non-trivial baseline and demonstrates proper ex-
tensibility. Several possible improvements are dis-
cussed, which will be the central arc of our future
research.
Acknowledgements
The research in this paper is supported by China?s
973 Programme no. 2014CB340505. The first au-
thor is partially funded by the EC FP7 programme
under grant agreement no. 287615 (PARLANCE)
and a SICSA PECE grant. The authors would also
like to thank Qiaoqiao She, Duo Cai and the HCI-
APP group at Baidu for volunteering to participate
in the human subject experiments.
References
Richard Ernest Bellman. 1957. Dynamic Program-
ming. Princeton University Press, Princeton, NJ.
Dimitri P. Bertsekas and John N. Tsitsiklis. 1996.
Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Dan Bohus and Alexander I. Rudnicky. 2009. The
RavenClaw dialog management framework: Archi-
tecture and systems. Computer Speech and Lan-
guage, 23(3):332?361.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proceedings of the 22nd International Conference
on Machine Learning (ICML), pages 201?208.
Mahdi Milani Fard, Joelle Pineau, and Csaba
Szepesv?ari. 2011. PAC-Bayesian policy evaluation
for reinforcement learning. In Proceedings of the
27th Conference on Uncertainty in Artificial Intelli-
gence (UAI), pages 195?202.
USER/ASR:      ?????? (Flight ticket to Qingdao.)
SYSTEM: ?????????????????????        ???????        (Okay, flight ticket to Qingdao. When are you        departing, and where are you departing from?)     [Travel Info. Domain Expert]
USER/ASR:  ???? (I?m leaving tomorrow.)
SYSTEM: ???????????????????????     (Okay, flight ticket to Qingdao, leaving         tomorrow. Where are you departing from?)      [Travel Info. Domain Expert]
USER/ASR:  ?? (Beijing)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER:    ??????????        (Are there any good Sichuan cuisines in Qingdao?)ASR:       ???????????        (ungrammatical utterance, partially with     ?delicious? and ?Sichuan food?.)
SYSTEM: ?????????    (Do you want to continue with flight ticket booking?)    [Confirmation - Central Controller]
USER:      ?? (No)
SYSTEM: [Web Search Service: ?????????????     ]
USER/ASR:  ????? (I?m going to Qingdao tomorrow.)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER/ASR:  ????????? (Restaurants near Qingdao Airport)
SYSTEM: ????????   (I found the following restaurants for you.)        [Results Displayed     - Restaurant Search Domain Expert]
USER/ASR:  ????????? (Will it rain in Qingdao tomorrow?)
SYSTEM: ?????????16??20????5-6?     (Qingdao is rainy tomorrow. The temperature is 16 to 20         degree centigrade with moderate east wind.)     [Results Displayed     - Weather Report Service]
Figure 4: An example dialogue containing multi-
ple user goals. The icons stand for graphical user
interface based nonverbal outputs.
Milica Ga?si?c, Catherine Breslin, Matthew Henderson,
Dongho Kim, Martin Szummer, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2013a. On-
line policy optimisation of Bayesian spoken dia-
logue systems via human interaction. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
8367?8371.
Milica Ga?si?c, Catherine Breslin, Matthew Hender-
son, Dongho Kim, Martin Szummer, Blaise Thom-
son, Pirros Tsiakoulis, and Steve Young. 2013b.
POMDP-based dialogue manager adaptation to ex-
tended domains. In Proceedings of the 14th annual
SIGdial Meeting on Discourse and Dialogue, pages
214?222.
Thomas Robert Gruber, Adam John Cheyer, Dag
66
Kittlaus, Didier Rene Guzzoni, Christopher Dean
Brigham, Richard Donald Giuli, Marcello Bastea-
Forte, and Harry Joseph Saddler. 2012. Intelligent
automated assistant. United States Patent No. US
20120245944 A1.
Dilek Z. Hakkani-T?ur, Gokhan T?ur, Larry P. Heck,
Ashley Fidler, and Asli C?elikyilmaz. 2012. A dis-
criminative classification-based approach to infor-
mation state updates for a multi-domain dialog sys-
tem. In Proceedings of the 13th Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH).
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2011. Natural actor and belief critic: Reinforcement
algorithm for learning parameters of dialogue sys-
tems modelled as POMDPs. ACM Transactions on
Speech and Language Processing, 7(3):6:1?6:25.
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech & Language, 26(3):168?192.
Simon Keizer, Milica Ga?si?c, Filip Jur?c???cek, Franc?ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Parameter estimation for agenda-
based user simulation. In Proceedings of the 11th
annual SIGdial Meeting on Discourse and Dialogue,
pages 116?123.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proceed-
ings of the 7th SIGdial Workshop on Discourse and
Dialogue, pages 9?17.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-Driven Methods for Adaptive Spoken Dia-
logue Systems: Computational Learning for Conver-
sational Interfaces. Springer.
Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee.
1999. A distributed architecture for cooperative
spoken dialogue agents with coherent dialogue state
and history. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop
(ASRU).
Danilo Mirkovic and Lawrence Cavedon. 2006. Di-
alogue management using scripts. United States
Patent No. US 20060271351 A1.
Mikio Nakano, Shun Sato, Kazunori Komatani, Kyoko
Matsuyama, Kotaro Funakoshi, and Hiroshi G.
Okuno. 2011. A two-stage domain selection frame-
work for extensible multi-domain spoken dialogue
systems. In Proceedings of the 12th annual SIGdial
Meeting on Discourse and Dialogue, pages 18?29.
Carl Edward Rasmussen and Christopher K. I.
Williams, editors. 2006. Gaussian Processes for
Machine Learning. MIT Press.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In Proceedings of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 149?152.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16(1):105?133.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
David R. Traum and Staffan Larsson. 2003. The In-
formation State approach to dialogue management.
In Jan van Kuppevelt and Ronnie W. Smith, editors,
Current and New Directions in Discourse and Dia-
logue, pages 325?353. Springer.
Jason D. Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. 2010. Demonstration of AT&T ?Let?s Go?:
A production-grade statistical spoken dialog system.
In Proceedings of the 3rd IEEE Workshop on Spoken
Language Technology (SLT).
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Ga?si?c, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialogue systems: a review. Proceedings of
the IEEE, PP(99):1?20.
67
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 46?50,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Statistical Spoken Dialogue System using Complex User Goals and
Value Directed Compression
Paul A. Crook, Zhuoran Wang, Xingkun Liu and Oliver Lemon
Interaction Lab
School of Mathematical and Computer Sciences (MACS)
Heriot-Watt University, Edinburgh, UK
{p.a.crook, zhuoran.wang, x.liu, o.lemon}@hw.ac.uk
Abstract
This paper presents the first demonstration
of a statistical spoken dialogue system that
uses automatic belief compression to rea-
son over complex user goal sets. Reasoning
over the power set of possible user goals al-
lows complex sets of user goals to be rep-
resented, which leads to more natural dia-
logues. The use of the power set results in a
massive expansion in the number of belief
states maintained by the Partially Observ-
able Markov Decision Process (POMDP)
spoken dialogue manager. A modified form
of Value Directed Compression (VDC) is
applied to the POMDP belief states produc-
ing a near-lossless compression which re-
duces the number of bases required to rep-
resent the belief distribution.
1 Introduction
One of the main problems for a spoken dialogue
system (SDS) is to determine the user?s goal (e.g.
plan suitable meeting times or find a good Indian
restaurant nearby) under uncertainty, and thereby
to compute the optimal next system dialogue ac-
tion (e.g. offer a restaurant, ask for clarification).
Recent research in statistical SDSs has success-
fully addressed aspects of these problems through
the application of Partially Observable Markov
Decision Process (POMDP) approaches (Thom-
son and Young, 2010; Young et al 2010). How-
ever POMDP SDSs are currently limited by the
representation of user goals adopted to make sys-
tems computationally tractable.
Work in dialogue system evaluation, e.g.
Walker et al(2004) and Lemon et al(2006),
shows that real user goals are generally sets of
items, rather than a single item. People like to
explore possible trade offs between the attributes
of items.
Crook and Lemon (2010) identified this as a
central challenge for the field of spoken dialogue
systems, proposing the use of automatic compres-
sion techniques to allow for extended accurate
representations of user goals. This paper presents
a proof of concept of these ideas in the form of a
complete, working spoken dialogue system. The
POMDP dialogue manager (DM) of this demon-
stration system uses a compressed belief space
that was generated using a modified version of the
Value Directed Compression (VDC) algorithm
as originally proposed by Poupart (2005). This
demonstration system extends work presented by
Crook and Lemon (2011) in that it embeds the
compressed complex user goal belief space into a
working system and demonstrates planning (and
acting) in the compressed space.
2 Complex User Goals
The type of SDS task that we focus on is a limited-
domain query-dialogue, also known as a ?slot fill-
ing? task. The spoken dialogue system has knowl-
edge about some set of objects where these ob-
jects have attributes and these attributes can take
several values. An object can thus be described
by a conjunction of attribute-value pairs. A di-
alogue progresses with the system obtaining re-
quirements from the user which are specified in
terms of attribute values. The system should even-
tually present objects (search results) based upon
its understanding of the user?s requirement. The
dialogue ends when the user accepts one of the
domain objects.
Prior work on POMDP SDSs has assumed that
a user has a narrowly constrained goal (as speci-
46
User goal: cheap central Thai or expensive French
S: Hello, how can I help you?
U: I?m looking for a cheap central Thai restaurant.
S: You?re looking for budget, city centre, Thai
restaurants. ?
U: Or an expensive French restaurant anywhere.
S: You?re looking for non-budget French restaurants.
What ?
U: And cheap, city centre, Thai.
S: You?re looking for budget, city centre, Thai
restaurants. What about ?
U: (SIGH) And French.
S: You?re looking for budget, city centre, French
restaurants. What about ?
U: (HANG UP)
Table 1: Example dialogue using simple user goals.
S=System, U=User. ??? indicates the user barged in.
fied in terms of the domain objects) and thus the
role of the DM is one of reducing uncertainty un-
til its belief is strongly focused on a particular do-
main object. This has the unfortunate effect of
forcing users to select one domain object in order
to progress the dialogue, see Table 1. Note that the
example given is problematic not only because the
user wants two different food types but because
they have different requirements associated with
each, i.e. Thai restaurants should be in the centre
and cheap, while any French restaurants should be
expensive1 and can be located anywhere. To our
knowledge such a combination of goals with dif-
ferent attribute values cannot be straightforwardly
handled by comparable state-of-the-art statistical
SDSs which appear in the literature.
Crook and Lemon (2011) suggest that rather
than the DM assuming that the user has a single
narrowly constrained goal in mind, it should as-
sume that they want any possible sub-set of the
complete set of domain objects. Thus, instead
of maintaining the POMDP belief over individual
domain objects, it should be maintained over the
power set of domain objects. As an example see
Table 3 which is the power set for a domain with
three objects that can take two attributes u, v with
associated values u1, u2, u3 and v1 respectively.
The power set representation allows the
demonstration system to straightforwardly handle
previously problematic dialogues. See Table 2.
Of course this approach significantly expands the
1Interpreted as non-budget by the system since its
database only classifies restaurants as budget or non-budget.
User goal: cheap central Thai or expensive French
S: Hello, how can I help you?
U: I?m looking for a cheap central Thai restaurant.
S: You?re looking for budget, city centre, Thai
restaurants. ?
U: Or an expensive French restaurant anywhere.
S: You?re looking for budget, city centre, Thai
restaurants or non-budget French. What about
Chang Thai in Old Town?? Chang Thai?s set lunch
price is ? 7.95 ?
U: What alternatives do you have?
S: What about the Plumed Horse in Leith?? The
Plumed Horse is a Michelin starred French ...
Table 2: Example dialogue using complex user goals.
S=System, U=User. ?Old Town is part of the city cen-
tre. ?Leith is outside the centre.
state space of possible user goals, with the num-
ber of goal sets being equal to 2|domain objects| .
2.1 Automatic Compression
Even considering limited domains, POMDP state
spaces for SDSs grow very quickly. Thus the cur-
rent state-of-the-art in POMDP SDSs uses a vari-
ety of handcrafted compression techniques, such
as making several types of independence assump-
tion as discussed above.
Crook and Lemon (2010) propose replacing
handcrafted compressions with automatic com-
pression techniques. The idea is to use princi-
pled statistical methods for automatically reduc-
ing the dimensionality of belief spaces, but which
preserve useful distributions from the full space,
and thus can more accurately represent real user?s
goals.
2.2 VDC Algorithm
The VDC algorithm (Poupart, 2005) uses Krylov
iteration to compute a reduced state space. It finds
a set of linear basis vectors that can reproduce the
value2 of being in any of the original POMDP
states. Where, if a lossless VDC compression is
possible, the number of basis vectors is less than
the original number of POMDP states.
The intuition here is that if the value of taking
an action in a given state has been preserved then
planning is equally as reliable in the compressed
space as the in full space.
The VDC algorithm requires a fully specified
POMDP, i.e. ?S,A,O, T,?,R? where S is the set
2The sum of discounted future rewards obtained through
following some series of actions.
47
state goal set meaning: user?s goal is
s1 ? (empty set) none of the domain objects
s2 u=u1?v=v1 domain object 1
s3 u=u2?v=v1 domain object 2
s4 u=u3?v=v1 domain object 3
s5 (u=u1?v=v1) ? (u=u2?v=v1) domain objects 1 or 2
s6 (u=u1?v=v1) ? (u=u3?v=v1) domain objects 1 or 3
s7 (u=u2?v=v1) ? (u=u3?v=v1) domain objects 2 or 3
s8 (u=u1?v=v1) ? (u=u2?v=v1) ? (u=u3?v=v1) any of the domain objects
Table 3: Example of complex user goal sets.
of states, A is the set of actions, O is the set of ob-
servations, T conditional transition probabilities,
? conditional observation probabilities, and R is
the reward function. Since it iteratively projects
the rewards associated with each state and action
using the state transition and observation proba-
bilities, the compression found is dependent on
structures and regularities in the POMDP model.
The set of basis vectors found can be used to
project the POMDP reward, transition, and obser-
vation probabilities into the reduced state space
allowing the policy to be learnt and executed in
this state space.
Although the VDC algorithm (Poupart, 2005)
produces compressions that are lossless in terms
of the states? values, the set of basis vectors found
(when viewed as a transformation matrix) can be
ill-conditioned. This results in numerical instabil-
ity and errors in the belief estimation. The com-
pression used in this demonstration was produced
using a modified VDC algorithm that improves
the matrix condition by approximately selecting
the most independent basis vectors, thus improv-
ing numerical stability. It achieves near-lossless
state value compression while allowing belief es-
timation errors to be minimised and traded-off
against the amount of compression. Details of this
algorithm are to appear in a forthcoming publica-
tion.
3 System Description
3.1 Components
Input and output to the demonstration system is
using standard open source and commercial com-
ponents. FreeSWITCH (Minessale II, 2012) pro-
vides a platform for accepting incoming Voice
over IP calls, routing them (using the Media Re-
source Control Protocol (MRCP)) to a Nuance 9.0
Automatic Speech Recogniser (Nuance, 2012).
Output is similarly handled by FreeSWITCH
routing system responses via a CereProc Text-to-
Speech MRCP server (CereProc, 2012) in order
to respond to the user.
The heart of the demonstration system consists
of a State-Estimator server which estimates the
current dialogue state using the compressed state
space previously produced by VDC, a Policy-
Executor server that selects actions based on
the compressed estimated state, and a template
based Natural Language Generator server. These
servers, along with FreeSWITCH, use ZeroC?s
Internet Communications Engine (Ice) middle-
ware (ZeroC, 2012) as a common communica-
tions platform.
3.2 SDS Domain
The demonstration system provides a restaurant
finder system for the city of Edinburgh (Scot-
land, UK). It presents search results from a real
database of over 600 restaurants. The search
results are based on the attributes specified by
the user, currently; location, food type and
budget/non-budget.
3.3 Interface
The demonstration SDS is typically accessed over
the phone network. For debugging and demon-
stration purposes it is possible to visualise the
belief distribution maintained by the DM as dia-
logues progress. The compressed version of the
belief distribution is not a conventional proba-
bility distribution3 and its visualisation is unin-
formative. Instead we take advantage of the re-
versibility of the VDC compression and project
the distribution back onto the full state space. For
an example of the evolution of the belief distribu-
tion during a dialogue see Figure 1.
3The values associated with the basis vectors are not con-
fined to the range [0? 1].
48
#4096
10?7 10?6 10?5 0.0001 0.001
(a) Initial uniform distribution over the power set.
#2048
#2048
10?7 10?6 10?5 0.0001 0.001
(b) Distribution after user responds to greet.
#512
#3584
10?11 10?9 10?7 10?5 0.001
(c) Distribution after second user utterance.
Figure 1: Evolution of the belief distribution for the
example dialogue in Table 2. The horizontal length of
each bar corresponds to the probability of that com-
plex user goal state. Note that the x-axis uses a log-
arithmic scale to allow low probability values to be
seen. The y-axis is the set of complex user goals or-
dered by probability. Lighter shaded (green) bars indi-
cate complex user goal states corresponding to ?cheap,
central Thai? and ?cheap, central Thai or expensive
French anywhere? in figures (b) and (c) respectively.
The count ?#? indicates the number of states in those
groups.
4 Conclusions
We present a demonstration of a statistical SDS
that uses automatic belief compression to reason
over complex user goal sets. Using the power set
of domain objects as the states of the POMDP
DM allows complex sets of user goals to be rep-
resented, which leads to more natural dialogues.
To address the massive expansion in the number
of belief states, a modified form of VDC is used
to generate a compression. It is this compressed
space which is used by the DM for planning and
acting in response to user utterances. This is the
first demonstration of a statistical SDS that uses
automatic belief compression to reason over com-
plex user goal sets.
VDC and other automated compression tech-
niques reduce the human design load by automat-
ing part of the current POMDP SDS design pro-
cess. This reduces the knowledge required when
building such statistical systems and should make
them easier for industry to deploy.
Such compression approaches are not only ap-
plicable to SDSs but should be equally relevant
for multi-modal interaction systems where sev-
eral modalities are being combined in user-goal
or state estimation.
5 Future Work
The current demonstration system is a proof
of concept and is limited to a small number
of attributes and attribute-values. Part of our
ongoing work involves investigation of scaling.
For example, increasing the number of attribute-
values should produce more regularities across
the POMDP space. Does VDC successfully ex-
ploit these?
We are in the process of collecting corpora
for the Edinburgh restaurant domain mentioned
above with the aim that the POMDP observation
and transition statistics can be derived from data.
As part of this work we have launched a long
term, public facing outlet for testing and data col-
lection, see http:\\www.edinburghinfo.
co.uk. It is planned to make future versions of
the demonstration system discussed in this paper
available via this public outlet.
Finally we are investigating the applicability
of other automatic belief (and state) compression
techniques for SDSs, e.g. E-PCA (Roy and Gor-
don, 2002).
49
Acknowledgments
The research leading to these results was funded
by the Engineering and Physical Sciences Re-
search Council, UK (EPSRC) under project no.
EP/G069840/1 and was partially supported by the
EC FP7 projects Spacebook (ref. 270019) and
JAMES (ref. 270435).
References
CereProc. 2012. http://www.cereproc.com/.
Paul A. Crook and Oliver Lemon. 2010. Representing
uncertainty about complex user goals in statistical
dialogue systems. In proceedings of SIGdial.
Paul A. Crook and Oliver Lemon. 2011. Lossless
Value Directed Compression of Complex User Goal
States for Statistical Spoken Dialogue Systems. In
Proceedings of the Twelfth Annual Conference of
the International Speech Communication Associa-
tion (Interspeech).
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating Effectiveness and Portabil-
ity of Reinforcement Learned Dialogue Strategies
with real users: the TALK TownInfo Evaluation. In
IEEE/ACL Spoken Language Technology.
Anthony Minessale II. 2012. FreeSWITCH. http:
//www.freeswitch.org/.
Nuance. 2012. Nuance Recognizer. http://www.
nuance.com.
P. Poupart. 2005. Exploiting Structure to Efficiently
Solve Large Scale Partially Observable Markov De-
cision Processes. Ph.D. thesis, Dept. Computer Sci-
ence, University of Toronto.
N. Roy and G. Gordon. 2002. Exponential Family
PCA for Belief Compression in POMDPs. In NIPS.
B. Thomson and S. Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
Marilyn Walker, S. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
User tailored generation in the match multimodal
dialogue system. Cognitive Science, 28:811?840.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2010. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language, 24(2):150?174.
ZeroC. 2012. The Internet Communications Engine
(Ice). http://www.zeroc.com/ice.html.
50
Proceedings of the SIGDIAL 2013 Conference, pages 423?432,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A Simple and Generic Belief Tracking Mechanism for the Dialog State
Tracking Challenge: On the believability of observed information
Zhuoran Wang and Oliver Lemon
Interaction Lab, MACS, Heriot-Watt University
Edinburgh, EH14 4AS, United Kingdom
{zhuoran.wang; o.lemon}@hw.ac.uk
Abstract
This paper presents a generic dialogue
state tracker that maintains beliefs over
user goals based on a few simple domain-
independent rules, using basic probability
operations. The rules apply to observed
system actions and partially observable
user acts, without using any knowledge
obtained from external resources (i.e.
without requiring training data). The core
insight is to maximise the amount of in-
formation directly gainable from an error-
prone dialogue itself, so as to better lower-
bound one?s expectations on the perfor-
mance of more advanced statistical tech-
niques for the task. The proposed method
is evaluated in the Dialog State Track-
ing Challenge, where it achieves compara-
ble performance in hypothesis accuracy to
machine learning based systems. Conse-
quently, with respect to different scenarios
for the belief tracking problem, the poten-
tial superiority and weakness of machine
learning approaches in general are investi-
gated.
1 Introduction
Spoken dialogue system (SDS) can be modelled
as a decision process, in which one of the main
problems researchers try to overcome is the un-
certainty in tracking dialogue states due to error-
prone outputs from automatic speech recognition
(ASR) and spoken language understanding (SLU)
components (Williams, 2012). Recent advances
in SDS have demonstrated that maintaining a dis-
tribution over a set of possible (hidden) dialogue
states and optimising dialogue policies with re-
spect to long term expected rewards can signifi-
cantly improve the interaction performance (Roy
et al, 2000; Williams and Young, 2007a). Such
methods are usually developed under a partially
observable Markov decision process (POMDP)
framework (Young et al, 2010; Thomson and
Young, 2010; Williams, 2010), where the distribu-
tion over dialogue states is called a ?belief? and is
modelled as a posterior updated every turn given
an observation. Furthermore, instead of simply
taking the most probable (or highest confidence
score) hypothesis of the user act as in ?traditional?
handcrafted systems, the observation here may
consist of an n-best list of the SLU hypotheses (di-
alogue acts) with (normalised) confidence scores.
See (Henderson and Lemon, 2008; Williams and
Young, 2007b; Thomson et al, 2010; Young et al,
2013) for more details of POMDP-based SDS.
It is understandable that beliefs more accurately
estimating the true dialogue states will ease the
tuning of dialogue policies, and hence can result
in better overall system performance. The accu-
racy of belief tracking has been studied in depth
by Williams (2012) based on two SDS in public
use. Here the effects of several mechanisms are
analysed, which can alter the ?most-believed? dia-
logue state hypothesis (computed using a genera-
tive POMDP model) from the one derived directly
from an observed top SLU hypothesis. Williams?s
work comprehensively explores how and why a
machine learning approach (more specifically the
generative model proposed in (Williams, 2010))
functions in comparison with a naive baseline.
However, we target a missing intermediate anal-
ysis in this work: how much information one
can gain purely from the SLU n-best lists (and
the corresponding confidence scores), without any
prior knowledge either being externally learned
(using data-driven methods) or designed (based on
domain-specific strategies), but beyond only con-
sidering the top SLU hypotheses. We explain this
idea in greater detail as follows.
Firstly, we can view the belief update procedure
in previous models as re-constructing the hidden
423
dialogue states (or user goals) based on the previ-
ous belief, a current observation (normally an SLU
n-best list), and some prior knowledge. The prior
knowledge can be observation probabilities given
a hidden state, the previous system action and/or
dialogue histories (Young et al, 2010; Thom-
son and Young, 2010; Williams, 2010), or prob-
abilistic domain-specific ontologies (Mehta et al,
2010), where the probabilities can be either trained
on a collection of dialogue examples or manually
assigned by human experts. In such models, a
common strategy is to use the confidence scores in
the observed n-best list as immediate information
substituted into the model for belief computation,
which implies that the performance of such belief
tracking methods to a large extent depends on the
reliability of the confidence scores. On the other
hand, since the confidence scores may reflect the
probabilities of the occurrences of corresponding
user acts (SLU hypotheses), a belief can also be
maintained based on basic probability operations
on those events (as introduced in this paper). Such
a belief will advance the estimation obtained from
top SLU hypotheses only, and can serve as a base-
line to justify how much further improvement is
actually contributed by the use of prior knowledge.
Note that the fundamental method in this paper re-
lies on the assumption that confidence scores carry
some useful information, and their informative-
ness will affect the performance of the proposed
method as will be seen in our experiments (Sec-
tion 5).
Therefore, this paper presents a generic belief
tracker that maintains beliefs over user goals only
using information directly observable from the di-
alogue itself, including SLU n-best list confidence
scores and user and system behaviours, such as
a user not disconfirming an implicit confirma-
tion of the system, or the system explicitly re-
jecting a query (since no matching item exists),
etc. The belief update is based on simple proba-
bility operations and a few very general domain-
independent rules. The proposed method was
evaluated in the Dialog State Tracking Challenge
(DSTC) (Williams et al, 2013). A systematic
analysis is then conducted to investigate the ex-
tent to which machine learning can advance this
naive strategy. Moreover, the results show the per-
formance of the proposed method to be compara-
ble to other machine learning based approaches,
which, in consideration of the simplicity of its im-
plementation, suggests that another practical use
of the proposed method could be as a module
in an initial system installation to collect training
data for machine learning techniques, in addition
to functioning as a baseline for further analysing
them.
The remainder of this paper is organised as fol-
lows. Section 2 reviews some basic mathematical
background, based on which Section 3 introduces
the proposed belief tracker. Section 4 briefly de-
scribes the DSTC task. The evaluation results and
detailed analysis are illustrated in Section 5. Fi-
nally, we further discuss in Section 6 and conclude
in Section 7.
2 Basic Mathematics
We first review some basic mathematics, which
provide the fundamental principles for our be-
lief tracker. Let P (X) denote the probability of
the occurrence of an event X , then the proba-
bility of X not occurring is simply P (?X) =
1 ? P (X). Accordingly, if X occurs at a time
with probability P1(X), and at a second time, it
occurs with probability P2(X) independently of
the first time, then the overall probability of its
occurrence is P (X) = 1 ? P1(?X)P2(?X) =
1 ? (1 ? P1(X))(1 ? P2(X)). To generalise,
we can say that in a sequence of k independent
events, if the probability of X occurring at the ith
time is Pi(X), the overall probability of X hav-
ing occurred at least once among the k chances
is P (X) = 1 ??ki=1 Pi(?X) = 1 ?
?k
i=1(1 ?
Pi(X)). This quantity can also be computed re-
cursively as:
P t(X) = 1? (1? P t?1(X))(1? Pt(X)) (1)
where P t(X) denotes the value of P (X) after t
event occurring chances, and we let P 0(X) = 0.
Now we consider another situation. Let A be
a binary random variable. Suppose that we know
the prior probability of A being true is Pr(A). If
there is a chance where with probability P (B) we
will observe an event B independent of A, and we
assume that if B happens, we must set A to false,
then after this, the probability of A still being true
will become P (A = true) = Pr(A) ? P (?B) =
Pr(A)(1? P (B)).
3 A Generic Belief Tracker
In this section, we will take the semantics defined
in the bus information systems of DSTC as
424
examples to explain our belief tracker. Without
losing generality, the principle applies to other
domains and/or semantic representations. The
SDS we are interested in here is a turn-based
slot-filling task. In each turn, the system executes
an action and receives an observation. The
observation is an SLU n-best list, in which each
element could be either a dialogue act without
taking any slot-value arguments (e.g. affirm()
or negate()) or an act presenting one or more
slot-value pairs (e.g. deny(route=64a) or
inform(date.day=today, time.ampm=
am)), and normalised confidence scores are
assigned to those dialogue act hypotheses. In
addition, we follow a commonly used assumption
that the user?s goal does not change during a
dialogue unless an explicit restart action is
performed.
3.1 Tracking Marginal Beliefs
Since a confidence score reflects the probability
of the corresponding dialogue act occurring in the
current turn, we can apply the probability opera-
tions described in Section 2 plus some ?common
sense? rules to track the marginal probability of a
certain goal being stated by the user during a di-
alogue trajectory, which is then used to construct
our beliefs over user goals. Concretely, we start
from an initial belief b0 with zero probabilities for
all the slot-value hypotheses and track the beliefs
over individual slot-value pairs as follows.
3.1.1 Splitting-Merging Hypotheses
Firstly, in each turn, we split those dialogue acts
with more than one slot-value pairs into single
slot-value statements and merge those identical
statements among the n-best list by summing over
their confidence scores, to yield marginal confi-
dence scores for individual slot-value representa-
tions. For example, an n-best list observation:
inform(date.day=today, time.ampm=am) 0.7
inform(date.day=today) 0.3
after the splitting-merging procedure will become:
inform(date.day=today) 1
inform(time.ampm=am) 0.7
3.1.2 Applying Rules
Let Pt(u, s, v) denote the marginal confidence
score for a user dialogue act u(s = v) at turn
t. Then the belief bt(s, v) for the slot-value pair
(s, v) is updated as:
? Rule 1: If u = inform, then bt(s, v) =
1? (1? bt?1(s, v))(1? Pt(u, s, v)).
? Rule 2: If u = deny, then bt(s, v) =
bt?1(s, v)(1? Pt(u, s, v)).
In addition, motivated by some strategies com-
monly used in rule-based systems (Bohus and
Rudnicky, 2005), we consider the effects of cer-
tain system actions on the beliefs as well. Let a(h)
be one of the system actions performed in turn t,
where h stands for a set of n slot-value arguments
taken by a, i.e. h = {(s1, v1), . . . , (sn, vn)}. We
check:
? Rule 3: If a is an implicit or explicit confir-
mation action (denoted by impl-conf and
expl-conf, respectively) and an affirm
or negate user act u is observed with con-
fidence score Pt(u):
? Rule 3.1: If u = affirm, then
bt(si, vi) = 1 ? (1 ? bt?1(si, vi))(1 ?
Pt(u)), ?(si, vi) ? h.
? Rule 3.2: If u = negate, then
bt(si, vi) = bt?1(si, vi)(1 ? Pt(u)),
?(si, vi) ? h.
? Rule 4: Otherwise, if a is an impl-conf
action, and there are no affirm/negate
user acts observed, and no information pre-
sented in a is re-informed or denied in the
current turn, then we take all (si, vi) ? h as
being affirmed by the user with probability 1.
However, note that, the marginal probabilities
b(s, v) computed using the above rules do not nec-
essarily yield valid beliefs, because sometimes we
may have?v b(s, v) > 1 for a given slot s. When
this occurs, a reasonable solution is to seek a
multinomial vector b?(s, ?) that minimises the sym-
metrised Kullback-Leibler (KL) divergence be-
tween b(s, ?) and itself. It can be checked that
solving such an optimisation problem is actually
equivalent to simply normalising b(s, ?), for which
the proof is omitted here but can be found in Ap-
pendix B.
Finally, we consider an extra fact that normally
a user will not insist on a goal if he/she has been
notified by the system that it is impossible to sat-
isfy. (In the DSTC case, such notifications cor-
respond to those canthelp.* system actions.)
Therefore, we have:
425
? Rule 5: If the system has explicitly disabled
a hypothesis h, we will block the generation
of any hypotheses containing h in the be-
lief tracking procedure, until the dialogue fin-
ishes.
Note here, if h is a marginal hypothesis, elimi-
nating it from our marginal belief will result in
joint hypotheses (see Section 3.2) containing h
also being blocked, but if h is a joint representa-
tion, we will only block the generation of those
joint hypothesis containing h, without affecting
any marginal belief.
3.2 Constructing Joint Representations
Beliefs over joint hypotheses can then be con-
structed by probabilistic disjunctions of those
marginal representations. For example, given two
marginal hypotheses (s1, v1) and (s2, v2) (s1 6=
s2) with beliefs b(s1, v1) and b(s2, v2) respec-
tively, one can compute the beliefs of their joint
representations as:
bjoint(s1 = v1, s2 = v2) = b(s1, v1)b(s2, v2)
bjoint(s1 = v1, s2 = null) = b(s1, v1)b(s2,null)
bjoint(s1 = null, s2 = v2) = b(s1,null)b(s2, v2)
where null represents that none of the current
hypotheses for the corresponding slot is correct,
i.e. b(s,null) stands for the belief that the in-
formation for slot s has never been presented by
the user, and can be computed as b(s,null) =
1??v b(s, v).
3.3 Limitations
The insight of the proposed approach is to explore
the upper limit of the observability one can ex-
pect from an error-prone dialogue itself. Never-
theless, this method has two obvious deficiencies.
Firstly, the dialogue acts in an SLU n-best list
are assumed to be independent events, hence er-
ror correlations cannot be handled in this method
(which is also a common drawback of most ex-
isting models as discussed by Williams (2012)).
Modelling error correlations requires statistics on
a certain amount of data, which implies a poten-
tial space of improvement left for machine learn-
ing techniques. Secondly, the model is designed
to be biased on the accuracy of marginal be-
liefs rather than that of joint beliefs. The be-
liefs for joint hypotheses in this method can only
lower-bound the true probability, as the observ-
able dependencies among some slot-value pairs
are eliminated by the splitting-merging and re-
joining procedures described above. For exam-
ple, in the worst case, a multi-slot SLU hypoth-
esis inform(s1 = v1, s2 = v2) with a confi-
dence score p < 1 may yield two marginal be-
liefs b(s1, v1) = p and b(s2, v2) = p,1 then the
re-constructed joint hypothesis will have its belief
bjoint(s1 = v1, s2 = v2) = p2, which is exponen-
tially reduced compared to the originally observed
confidence score. However, the priority between
the marginal hypotheses and the joint representa-
tions to a greater extent depends on the action se-
lection strategy employed by the system.
4 Description of DSTC
DSTC (Williams et al, 2013) is a public eval-
uation of belief tracking (a.k.a. dialogue state
tracking) models based on the data collected
from different dialogue systems that provide bus
timetables for Pittsburgh, Pennsylvania, USA.
The dialogue systems here were fielded by three
anonymised groups (denoted as Group A, B, and
C).
There are 4 training sets (train1a,
train1b, train2 and train3) and 4
test sets (test1. . .4) provided, where all the
data logs are transcribed and labelled, except
train1b which is transcribed but not labelled
(and contains a much larger number of dialogues
than others). It is known in advance to partici-
pants that test1 was collected using the same
dialogue system from Group A as train1* and
train2, test2 was collected using a different
version of Group A?s dialogue manager but is
to a certain extent similar to the previous ones,
train3 and test3 were collected using the
same dialogue system from Group B (but the
training set for this scenario is relatively smaller
than that for test1), and test4 was collected
using Group C?s system totally different from any
of the training sets.
The evaluation is based on several different met-
rics2, but considering the nature of our system, we
will mainly focus on the hypothesis accuracy, i.e.
1The worst case happens when (s1, v1) and (s2, v2) are
stated for the first time in the dialogue and cannot merge with
any other marginal hypotheses in the current turn, as their
marginal beliefs will remain p without being either propa-
gated by the belief update rules, or increased by the merging
procedure.
2Detailed descriptions of these metrics can be found in the
DSTC handbook at http://research.microsoft.
com/en-us/events/dstc/
426
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 1
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 2
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 3
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 4
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 21
2
3
4
5
6
7
8
9
10
 
 
Baseline Our system Team 1 Team 3 Team 4 Team 5 Team 6 Team 7 Team 8 Team 9
Figure 1: Hypothesis accuracy on the four test sets: the columns in each schedule, from left to right,
stand for the ensemble, mixed-domain, in-domain and out-of-domain system groups, except for test4
where the last three groups are merged into the right-hand side column.
percentage of turns in which the tracker?s 1-best
hypothesis is correct, but with the receiver operat-
ing characteristic (ROC) performance briefly dis-
cussed as well. In addition, there are 3 ?sched-
ules? for determining which turns to include when
measuring a metric: schedule 1 ? including
all turns, schedule 2 ? including a turn for a
given concept only if that concept either appears
on the SLU n-best list in that turn, or if the sys-
tem action references that concept in that turn, and
schedule 3 ? including only the turn before the
restart system action (if there is one), and the
last turn of the dialogue.
5 Evaluation and Analysis
The method proposed in this paper corresponds to
Team 2, Entry 1 in the DSTC submissions. In
the following analysis, we will compare it with
the 26 machine learning models submitted by the
other 8 anonymised participant teams plus a base-
line system (Team 0, Entry 1) that only con-
siders the top SLU result.
Each team can submit up to 5 systems, whilst
the systems from a same team may differ from
each other in either the statistical model or the
training data selection (or both of them). There is
a brief description of each system available after
the challenge. For the convenience of analysis and
illustration, on each test set we categorise these
systems into the following groups: in-domain ?
systems trained only using the data sets which
are similar (including the ?to-some-extent-similar?
ones) to the particular test set, out-of-domain ?
systems trained on the data sets which are to-
tally different from the particular test set, mixed-
domain ? systems trained on a mixture of the in-
domain and out-of-domain data, and ensemble ?
systems combining multiple models to generate
their final output. (The ensemble systems here are
all trained on the mixed-domain data.) Note that,
427
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7 TEST 1
 
 CorrectIncorrect
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7 TEST 2
 
 CorrectIncorrect
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9 TEST 3
 
 CorrectIncorrect
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7 TEST 4
 
 CorrectIncorrect
Figure 2: Distributions of SLU confidence scores on the four test sets: The x-axis stands for the confi-
dence score interval, and the y-axis stands for the occurrence rate.
for test4 there are no in-domain data available,
so all those non-ensemble systems are merged into
one group. Detailed system categorisation on each
test set can be found in Appendix A.
5.1 Hypothesis Accuracy
We plot the hypothesis accuracy of our method
(red dashed line) on the 4 test sets in compari-
son with the baseline system (blue dotted line) and
other systems in Figure 1, where different mark-
ers are used to identify the systems from different
teams. Here we use the overall accuracy of the
marginal hypotheses (all) and the accuracy of
the joint hypotheses (joint) to sketch the gen-
eral performance of the systems, without looking
into the result for each individual slot.
It can be seen that the proposed method pro-
duces more accurate marginal and joint hypothe-
ses than the baseline on all the test sets and in
all the schedules. Moreover, generally speak-
ing, further improvement can be achieved by prop-
erly designed machine learning techniques. For
example, some systems from Team 6, especially
their in-domain and ensemble ones, almost consis-
tently outperform our approach (as well as most of
the models from the other teams) in all the above
tasks. In addition, the following detailed trends
can be found.
Firstly, and surprisingly, our method tends
to be more competitive when measured using
schedule 1 and schedule 3 than using
schedule 2. As schedule 2 is supposed to
measure system performance on the concepts that
are in focus, and to prevent a belief tracker receiv-
ing credit for new guesses about those concepts
not in focus, the results disagree with our origi-
nal expectation of the proposed method. A possi-
ble explanation here is that some machine learning
models tend to give a better belief estimation when
a concept is in focus, however their correct top hy-
potheses might more easily be replaced by other
incorrect ones when the focus on the concepts in
those correct hypotheses are lost (possibly due to
improperly assigned correlations among the con-
cepts). In this sense, our method is more robust,
as the beliefs will not change if their correspond-
ing concepts are not in focus.
428
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 1
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 2
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 3
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 4
0 0.2 0.4 0.6 0.8 1 1.2 .4 1.6 1.8 21
2
3
4
5
6
7
8
9
10
 
 
Baseline Our system Team 1 Team 3 Team 4 Team 5 Team 6 Team 7 Team 8 Team 9
Figure 3: ROC equal error rate on the four test sets: The columns in each schedule, from left to right,
stand for the ensemble, mixed-domain, in-domain and out-of-domain system groups, except for test4
where the last three groups are merged into the right-hand side column.
Secondly, the proposed method had been sup-
posed to be more preferable when there are no (or
not sufficient amount of) in-domain training data
available for those statistical methods. Initial evi-
dence to support this point of view can be observed
from the results on test1, test2 and test3.
More concretely, when the test data distribution
becomes less identical to the training data distri-
bution on test2, out system outperforms most
of the other systems except those from Team 6
(and a few others in the schedule 2/all task
only), compared to its middle-level performance
on test1. Similarly, on test3 when the amount
of available in-domain training data is small, our
approach gives more accurate beliefs than most of
the others with only a few exceptions in each sce-
nario, even if extra out-of-domain data are used to
enlarge the training set for many systems. How-
ever, the results on test4 entirely contradicts the
previous trend, where a significant number of ma-
chine learning techniques perform better than our
domain-independent rules without using any in-
domain training data at all. We analyse such re-
sults in detail as follows.
To explain the unexpected outcome on test4,
our first concern is the influence of Rule 4, which
is relatively ?stronger? and more artificial than
the other rules. Hence, for the four test sets,
we compute the percentage of dialogues where a
impl-conf system action occurs. The statistics
show that the occurrence rates of the implicit con-
firmation system actions in test1. . .4 are 0.01,
0, 0.94 and 0.67, respectively. This means that
the two very extreme cases happen in test3 and
test2 (the situation in test1 is very similar to
test2), and the result for test4 is roughly right
in the middle of them, which suggests that Rule
4 will not be the main factor to affect our per-
formance on test4. Therefore, we further look
into the distributions of the SLU confidence scores
across these different test sets. A normalised his-
togram of the confidence scores for correct and
incorrect SLU hypotheses observed in each test
set is plotted in Figure 2. Here we only consider
429
the SLU hypotheses that will actually contribute
during our belief tracking processes, i.e. only the
inform, deny, affirm and negate user dia-
logue acts. It can be found that the dialogue sys-
tem used to collect the data in test4 tends to
produce significantly more ?very confident? SLU
hypotheses (those with confidence scores greater
than 0.8) than the dialogue systems used for col-
lecting the other test sets, where, however, a con-
siderable proportion of its highly confident hy-
potheses are incorrect. In such a case, our system
would be less capable in revising those incorrect
hypotheses with high confidence scores than many
machine learning techniques, since it to a greater
extent relies on the confidence scores to update the
beliefs. This finding indicates that statistical ap-
proaches will be helpful when observed informa-
tion is less reliable.
5.2 Discussions on the ROC Performance
Besides the hypothesis accuracy, another impor-
tant issue will be the ability of the beliefs to dis-
criminate between correct and incorrect hypothe-
ses. Williams (2012) suggests that a metric to
measure such performance of a system is the ROC
curve. Note that, in the DSTC task, most of the
systems from the other teams are based on dis-
criminative models (except two systems, a simple
generative model from Team 3 and a deep neural
network method from Team 1), which are opti-
mised specifically for discrimination. Unsurpris-
ingly, our approach becomes much less competi-
tive when evaluated based on the ROC curve met-
rics, as illustrated in Figure 3 using the ROC equal
error rate (EER) for the all and joint scenar-
ios. (ERR stands for the intersection of the ROC
curve with the diagonal, i.e. where the false ac-
cept rate equals the false reject rate. The smaller
the ERR value, the better a system?s performance
is.) However, our argument on this point is that
since an optimised POMDP policy is not a linear
classifier but has a manifold decision surface (Cas-
sandra, 1998), the ROC curves may not be able to
accurately reflect the influence of beliefs on a sys-
tem?s decision quality, for which further investiga-
tions will be needed in our future work.
6 Further Discussions
In this paper, we made the rules for our belief
tracker as generic as possible, in order to ensure
the generality of the proposed mechanism. How-
ever, in practice, it is extendable by using more
detailed rules to address additional phenomena if
those phenomena are deterministically identifiable
in a particular system. For example, when the sys-
tem confirms a joint hypothesis (s1 = v1, s2 =
v2) and the user negates it and only re-informs one
of the two slot-values (e.g. inform(s1 = v?1)),
one may consider that it is more reasonable to only
degrade the belief on s1 = v1 instead of reducing
the beliefs on both s1 = v1 and s2 = v2 syn-
chronously as we currently do in Rule 3.2. How-
ever, the applicability of this strategy will depend
on whether it is possible to effectively determine
such a compact user intention from an observed
SLU n-best list without ambiguities.
7 Conclusions
This paper introduces a simple rule-based belief
tracker for dialogue systems, which can maintain
beliefs over both marginal and joint representa-
tions of user goals using only the information ob-
served within the dialogue itself (i.e. without need-
ing training data). Based on its performance in
the DSTC task, potential advantages and disad-
vantages of machine learning techniques are anal-
ysed. The analysis here is more focused on general
performance of those statistical approaches, where
our concerns include the similarity of distributions
between the training and test data, the adequacy of
available training corpus, as well as the SLU confi-
dence score distributions. Model-specific features
for different machine learning systems are not ad-
dressed at this stage. Considering its competitive-
ness and simplicity of implementation, we suggest
that the proposed method can serve either as a rea-
sonable baseline for future research on dialogue
state tracking problems, or a module in an ini-
tial system installation to collect training data for
those machine learning techniques.
Acknowledgments
The research leading to these results was sup-
ported by the EC FP7 projects JAMES (ref.
270435) and Spacebook (ref. 270019). We thank
Jason D. Williams for fruitful comments on an ear-
lier version of this paper. We also acknowledge
helpful discussions with Simon Keizer and Herib-
erto Cuaya?huitl.
430
References
Dan Bohus and Alexander I. Rudnicky. 2005. Con-
structing accurate beliefs in spoken dialog systems.
In Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 272?
277.
Anthony R. Cassandra. 1998. Exact and Approxi-
mate Algorithms for Partially Observable Markov
Decision Processes. Ph.D. thesis, Brown University,
Providence, RI, USA.
James Henderson and Oliver Lemon. 2008. Mixture
model POMDPs for efficient handling of uncertainty
in dialogue management. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 73?76.
Neville Mehta, Rakesh Gupta, Antoine Raux, Deepak
Ramachandran, and Stefan Krawczyk. 2010. Prob-
abilistic ontology trees for belief tracking in dialog
systems. In Proceedings of the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 37?46.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 93?100.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Blaise Thomson, Filip Jurc?ic?ek, Milica Gas?ic?, Simon
Keizer, Francois Mairesse, Kai Yu, and Steve Young.
2010. Parameter learning for POMDP spoken dia-
logue models. In Proceedings of IEEE Workshop on
Spoken Language Technology.
Jason D. Williams and Steve Young. 2007a. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams and Steve Young. 2007b. Scal-
ing POMDPs for spoken dialog management. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(7):2116?2129.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan W. Black. 2013. The Dialog State
Tracking Challenge. In Proceedings of the 14th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue.
Jason D. Williams. 2010. Incremental partition re-
combination for efficient tracking of multiple dia-
log states. In Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 5382?5385.
Jason D. Williams. 2012. Challenges and oppor-
tunities for state tracking in statistical spoken dia-
log systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Process-
ing, 6(8):959?970.
Steve Young, Milica Gas?ic?, Simon Keizer, Francois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Gas?ic?, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160?1179.
A System Categorisation
Table 1 shows detailed categorisation of the sys-
tems submitted to DSTC, where TiEj stands for
Team i, Entry j.
ensemble
T6E3, T6E4, T9E1, T9E2, T9E3
T9E4, T9E5
mixed-domain non-ensemblefortest4
T1E1, T3E1, T3E2, T3E3, T4E1
T5E2, T5E4, T5E5, T8E4, T8E5
in-domain
test1 T5E1
test2 T6E1, T8E1, T8E2 T5E3
test3 T6E2, T6E5, T8E3 T7E1
out-of-domain
test1
test2 T6E2, T6E5, T8E3
test3 T6E1, T8E1, T8E2
Table 1: Categorisation of the systems submitted
to DSTC.
B Symmetrised KL-divergence
Minimisation
We prove the following proposition to support our
discussions in the end of Section 3.1.
Proposition 1 Let p ? RN be an arbitrary N -
dimensional non-negative vector (i.e. p ? 0).
Let p? = p?p?1 , where ? ? ?1 stands for the `1-norm of a vector. Then p? is the solution of the
optimisation problem. minq?0,?q?1=1 DSKL(p?q),
where DSKL(p?q) denotes the symmetrised KL-
divergence between p and q, defined as:
DSKL(p?q) = DKL(p?q) + DKL(q?p) (2)
=
?
i
pi log
pi
qi
+
?
i
qi log
qi
pi
431
and pi and qi denote the ith element in p and q
respectively.
Proof Let q? = arg minq?0,?q?1=1 DSKL(p?q).
Firstly, using the facts that limx?0 x log xy ? 0
and limx?0 y log yx ? +?, ?y > 0, one can eas-ily prove that if pi = 0 then q?i = 0, and pi 6= 0
then q?i 6= 0, because otherwise the objective value
of Eq. (2) will become unbounded.
Therefore, we only consider the case p > 0 and
q > 0. By substituting pi = p?i?p?1 into Eq. (2),
we obtain:
DSKL(p?q) = ?p?1
?
i
p?i log
?p?1p?i
qi
+
?
i
qi log
qi
?p?1p?i
= ?p?1
(?
i
p?i log
p?i
qi
+
?
i
p?i log ?p?1
)
+
?
i
qi log
qi
p?i
?
?
i
qi log ?p?1
= ?p?1
?
i
p?i log
p?i
qi
+
?
i
qi log
qi
p?i
+(?p?1 ? 1) log ?p?1
= ?p?1DKL(p??q) + DKL(q?p?)
+(?p?1 ? 1) log ?p?1
? (?p?1 ? 1) log ?p?1
where we use the facts that?i p?i = 1,
?
i qi = 1,
DKL(p??q) ? 0 and DKL(q?p?) ? 0, since p? and
q are valid distributions. It can be found that the
minimum (?p?1 ? 1) log ?p?1 is only achievable
when DKL(p??q) = 0 and DKL(q?p?) = 0, i.e. q =
p?, which proves Proposition 1.
432
