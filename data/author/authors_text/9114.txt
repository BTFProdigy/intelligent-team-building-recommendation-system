Proceedings of NAACL HLT 2007, Companion Volume, pages 173?176,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Effects of Word Prediction on Communication Rate for AAC
Keith Trnka, Debra Yarrington, John McCaw,
and Kathleen F. McCoy
Department of Computer and Information Sciences
University of Delaware Newark, DE 19716
trnka,yarringt,mccaw,mccoy@cis.udel.edu
Christopher Pennington
AgoraNet, Inc.
314 East Main Street, Suite 1
Newark, DE 19711
penningt@agora-net.com
Abstract
Individuals using an Augmentative and
Alternative Communication (AAC) de-
vice communicate at less than 10% of
the speed of ?traditional? speech, creat-
ing a large communication gap. In this
user study, we compare the communica-
tion rate of pseudo-impaired individuals
using two different word prediction algo-
rithms and a system without word pre-
diction. Our results show that word pre-
diction can increase AAC communication
rate and that more accurate predictions
significantly improve communication rate.
1 Introduction
Communication is a significant quality-of-life issue
for individuals with severe speech impairments. The
field of Augmentative and Alternative Communica-
tion (AAC) is concerned with mitigating commu-
nication barriers that would otherwise isolate indi-
viduals from society. Most high-tech AAC devices
provide the user with an electronic letter and word
board to input messages which are output via speech
synthesis. However, even with substantial user inter-
face optimization, communication rate is often less
than 10 words per minute (Newell et al, 1998) as
compared to about 150-200 words per minute for
unimpaired speech.
One way to improve communication rate is to de-
crease the number of keys entered to form a mes-
sage. Word prediction is an application of language
modeling to allowing the user to access words they
may be spelling at a cost of one keystroke. Many
commercial AAC devices use word prediction, such
as PRC?s PathfinderTM, Dynavox Technology?s Dy-
navox 4TM, and Saltillo?s ChatPCTM.
Although word prediction is used in AAC de-
vices, researchers have questioned whether it ac-
tually increases communication rate (Venkatagiri,
1993; Koester and Levine, 1997; Anson et al,
2004). These works note the additional cognitive
demands and cost of using word prediction in con-
junction with a letter-by-letter interface, such as the
need to shift the focus of attention to the prediction
list, the time to scan the prediction list, and the cog-
nitive effort required for making decisions about the
predicted words. Obviously the design of the par-
ticular interface (e.g., the ease of using word pre-
diction) will affect these results. In addition, these
studies used a single, simplistic method of generat-
ing predictions, and this may also be responsible for
some of their results.
In contrast, other researchers (Lesher and Hig-
ginbotham, 2005; Li and Hirst, 2005; Trnka et
al., 2006) have continued to investigate various im-
provements to language modeling for word pre-
diction in order to save the user more keystrokes.
Newer methods such as topic modeling yield sta-
tistically significant keystroke savings over previ-
ous methods. However, the question remains as to
whether improvements in prediction methods trans-
late to an enhanced communication rate. We hypoth-
esize that it will.
In this paper we study (1) whether a word pre-
diction interface increases communication rate over
173
letter-by-letter typing when a reasonable prediction
method is employed and (2) whether an advanced
word prediction method increases communication
rate over a basic word prediction method to a degree
greater than that afforded by the difference in theo-
retical keystroke savings between the two methods.
We expect that the communication rate gain due to
the better word prediction method will exceed the
gains from the poorer system. Our reasons for this
expectation has to do with not only users wasting
time scanning lists that do not contain the desired
word, but also the tendency for a user to give up on
such a system (i.e., choosing to ignore the predic-
tions) and thus missing the predicted word even if it
does appear in the list. Validating these hypotheses
will motivate continued improvements in word pre-
diction methods for increased communication rate.
The target population of our research is adult
AAC users without significant cognitive impair-
ments. Including actual AAC users in the study
poses several significant complications, perhaps the
largest of which concerns the user interface. AAC
devices vary significantly in the physical interfaces
available, in accordance with the variety of physi-
cal abilities of AAC users. This diversity has caused
different word prediction interfaces to be developed
for each physical interface. Moreover, it would be
impossible to mimic our word prediction layout in a
consistent fashion on all of the major AAC devices
used. Because of this, we conducted this pilot study
using subjects that are pseudo-impaired: the subjects
have no motor impairments but we have simulated
a motor impairment by providing an interface that
emulates the communication rate of a typical AAC
user. Future work includes the verification of the re-
sults using a smaller number of actual AAC users.
2 Approach
The purpose of the study was to measure the effects
of word prediction methods on communication rate.
To this end, the interface used for text entry was opti-
mized for ease-of-use and kept constant across trials.
Subjects were asked to enter text on a touchscreen
monitor using WivikTM, an on-screen keyboard. Be-
cause we wanted to simulate AAC users with mo-
tor impairments, we programmed a 1.5 second de-
lay between a key press and its registration in the
system. The artificial impairment gave the subjects
the same incentive to use word prediction that AAC
users face every day, whereas users with fine motor
control tend to ignore word prediction (e.g., in com-
mon word processing software). The delay slows the
input rate of our subjects down to a rate more typical
of AAC users (about 8-10 words per minute).
Seventeen adult, native speakers of English with
no visual, cognitive, or motor impairments partic-
ipated in the study. These subjects were asked to
type in three different excerpts from held-out data of
the Switchboard corpus on three different days.1 In
each of these sessions, a different prediction method
was used and the order of prediction methods was
randomized across subjects. Keystrokes and pre-
dictions were logged and then post-processed to
compute the words produced per minute, seconds
per keystroke, and keystroke savings, among other
statistics.
2.1 Independent variable: prediction methods
The independent variable in our study is the method
of text entry used: (1) letter-by-letter typing using
the Wivik keyboard with no word prediction, (2)
letter-by-letter typing augmented with word predic-
tions produced by a basic prediction method, (3)
letter-by-letter typing augmented with word predic-
tions produced by an advanced prediction method.
Basic prediction generates predictions from the
combination of a recency model of the text entered
so far in conjunction with a large word list. The
recency model is given priority in generating pre-
dictions. This model is similar to language models
used in AAC devices with the exception that many
devices use a unigram model in lieu of a word list.
Advanced prediction generates predictions on
the basis of a trigram model with backoff. A spe-
cial unigram model is used for the first word in
each sentence. This language model is constructed
from the transcribed telephone conversations of the
Switchboard corpus. If the prediction list isn?t filled
from this model?s predictions, then predictions are
selected from a recency model and then a word list,
as in the basic prediction method.
1Switchboard was chosen because our prediction models
were trained using another portion of the corpus. A copy task
was chosen for more controlled experimental conditions.
174
Adv. prediction Basic prediction No prediction
Words per minute (wpm) 8.09 5.50 5.06
Time (seconds) 1316s 1808s 2030s
Seconds per keystroke (spk) 2.92s 2.58s 2.28s
Keystroke savings (ks) 50.3% 18.2% -
Potential keystroke savings (pks) 55.2% 25.0% -
Prediction utilization (pru) 90.9% 73.3% -
Figure 1: Average statistics for each method.
3 Results
Once the data was collected, we post-processed the
logs and accumulated statistics. Average values for
each method are shown in Figure 1 and comparative
values are shown in Figure 2.
3.1 Communication rate (output rate)
The overall average words per minute and task com-
pletion time for each method is shown in Figure 1,
and Figure 2 shows comparative data for the three
methods. As hypothesized, advanced prediction was
found to be significantly faster than basic prediction
and basic prediction was found to be significantly
faster than no prediction (? = 0.01). For example,
users produced 59.9% more words per minute using
advanced prediction compared to no prediction. Ad-
vanced prediction was 44.4% faster than basic pre-
diction but basic prediction was only 10.1% faster
than no prediction.
Additionally, the relative task completion time is
shown in Figure 2. The copy tasks with advanced
prediction were completed in 64.5% of the time it
took to complete without word prediction. The trend
shown with relative task completion time reinforces
the trends shown with words per minute ? advanced
prediction offers a large speedup over no prediction
and basic prediction, but basic prediction offers a
much smaller increase over no prediction.
Our results show that basic word prediction sig-
nificantly boosts communication rate and that ad-
vanced word prediction substantially increases com-
munication rate beyond basic prediction.
3.2 Input rate (seconds per keystroke)
Figures 1 and 2 indicate that there were significant
differences (at ? = 0.01) in the methods in terms
of the rate at which keys were pressed. In partic-
ular, while overall communication rate was signif-
icantly faster with advanced prediction, users took
0.641 seconds longer for each key press from us-
ing advanced prediction compared to entry without
prediction. Similarly, users spent 0.345s longer to
enter each key using advanced as opposed to basic
prediction and basic prediction required more time
per keystroke than no prediction. The slower input
rate can be attributed to the additional demands of
searching through a prediction list and making a de-
cision about selecting a word from that list over con-
tinuing to type letters.
3.3 Keystroke savings / prediction utilization
The difference between the potential keystroke sav-
ings offered by advanced and basic prediction is sub-
stantial: 55.2% vs. 25.0%, as shown in Figure 1.
Accordingly, the actual keystroke savings that users
realized under each prediction method shows a wide
separation: 50.3% for advanced and 18.2% for ba-
sic. The keystroke savings that users of basic predic-
tion achieved seems quite a bit lower than the poten-
tial keystroke savings offered by the predictions. In
other words, the prediction utilization of basic pre-
diction was much lower than that of advanced pre-
diction. Comparative analysis shows a 17.1% im-
provement in prediction utilization from advanced
over basic prediction.
4 Discussion
The results show that communication rate increased
despite the decreased input rate due to a large reduc-
tion in the amount of input required (high keystroke
savings). In the past, researchers have noted that the
cognitive load of using word prediction was consid-
erable, so that the keystroke savings of word pre-
175
Adv. over None Adv. over Basic Basic over None
Relative task completion time 0.6451 0.7011 0.9191
Words per minute (wpm) 59.9% faster2 44.4% faster2 10.1% faster2
Seconds per keystroke (spk) 0.641s2 0.345s2 0.286s2
Prediction utilization (pru) 17.1%2
Figure 2: Average per-subject improvements. (1 Significance not tested. 2 Significant at ? = 0.01.)
diction was outweighed by the overhead of using
it. However, we have shown that despite significant
cognitive load, the reduction in keystroke savings
dominates the effect on output rate.
In contrast to earlier studies, our basic method
showed a significantly improved communication
rate over no prediction. One reason for this could
be the intuitiveness of our user interface. A second
reason could be related to the consistency of the ba-
sic prediction method. In particular, at least some
subjects using the basic prediction method learned
to scan the prediction list when the desired word was
recently used and mentioned it in the exit survey. At
other times they simply ignored the prediction list
and proceeded with letter-by-letter typing. This be-
havior would also explain why the input was sig-
nificantly slower with the advanced method over the
basic method ? users found that scanning the predic-
tion list more often was worth the added effort. This
also explains the significant difference in prediction
utilization between the methods.
The relationship between keystroke savings and
communication rate is a trend of increasing rate
enhancement with increasingly accurate prediction
methods. Improved prediction methods offer greater
potential keystroke savings to users and users see
increased keystroke savings in practice. Addition-
ally, users rely on better predictions more and thus
lose less of the potential keystroke savings offered
by the method. We expect that keystroke savings
will see substantial increases from improved poten-
tial keystroke savings until prediction utilization is
closer to 100%.
5 Conclusions
Word prediction in an experimental AAC device
with simulated AAC users significantly enhances
communication rate. The difference between an ad-
vanced and basic prediction method demonstrates
that further improvements in language modeling for
word prediction are likely to appreciably increase
communication rate. Therefore, further research in
improving word prediction is likely to have an im-
portant impact on quality-of-life for AAC users. We
plan to improve word prediction and validate these
results using AAC users as future work.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051.
References
Denis Anson, Penni Moist, Mary Przywars, Heather
Wells, Heather Saylor, and Hantz Maxime. 2004.
The effects of word completion and word prediction
on typing rates using on-screen keyboards. Assistive
Technology, 18.
Heidi Horstmann Koester and Simon P. Levine. 1997.
Keystroke-level models for user performance with
word prediction. Augmentative and Alternative Com-
munication, 13:239?257, December.
Gregory W. Lesher and D. Jeffery Higginbotham. 2005.
Using web content to enhance augmentative commu-
nication. In Proceedings of CSUN 2005.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS ?05, pages 121?
128.
Alan Newell, Stefan Langer, andMarianne Hickey. 1998.
The ro?le of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):1?16.
Keith Trnka, Debra Yarrington, Kathleen F. McCoy, and
Christopher A. Pennington. 2006. Topic modeling in
fringe word prediction for aac. In IUI ?06, pages 276?
278.
Horabail S. Venkatagiri. 1993. Efficiency of lexical
prediction as a communication acceleration technique.
Augmentative and Alternative Communication, 9:161?
167, September.
176
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 28?31,
Columbus, June 2008. c?2008 Association for Computational Linguistics
ModelTalker Voice Recorder ? An Interface System for Recording a 
Corpus of Speech for Synthesis 
 
 
Debra Yarrington, John Gray,  
Chris Pennington  
 
H. Timothy Bunnell, Allegra Cornaglia, 
Jason Lilley, Kyoko Nagao,  
James Polikoff,  
AgoraNet, Inc. Speech Research Laboratory 
Newark, DE  19711 A.I. DuPont Hospital for Children 
USA Wilmington, DE  19803, USA 
{yarringt, gray, penningt} 
@agora-net.com 
{bunnell, cornagli, lilley,  
nagao, polikoff}@asel.udel.edu 
 
 
 
 
Abstract 
We will demonstrate the ModelTalker Voice 
Recorder (MT Voice Recorder) ? an interface 
system that lets individuals record and bank a 
speech database for the creation of a synthetic 
voice. The system guides users through an au-
tomatic calibration process that sets pitch, 
amplitude, and silence. The system then 
prompts users with both visual (text-based) 
and auditory prompts. Each recording is 
screened for pitch, amplitude and pronuncia-
tion and users are given immediate feedback 
on the acceptability of each recording. Users 
can then rerecord an unacceptable utterance. 
Recordings are automatically labeled and 
saved and a speech database is created from 
these recordings. The system?s intention is to 
make the process of recording a corpus of ut-
terances relatively easy for those inexpe-
rienced in linguistic analysis. Ultimately, the 
recorded corpus and the resulting speech da-
tabase is used for concatenative synthetic 
speech, thus allowing individuals at home or 
in clinics to create a synthetic voice in their 
own voice. The interface may prove useful  
for other purposes as well. The system facili-
tates the recording and labeling of large cor-
pora of speech, making it useful for speech 
and linguistic research, and it provides imme-
diate feedback on pronunciation, thus making 
it useful as a clinical learning tool.  
 
1 Demonstration 
1.1 MT Voice Recorder Background 
While most of us are familiar with the highly intel-
ligible but somewhat robotic sound of synthetic 
speech, for the approximately 2 million people in 
the United States with a limited ability to commu-
nicate vocally (Matas et al, 1985), these synthetic 
voices are inadequate. The restricted number of 
available voices lack the personalization they de-
sire. While intelligibility is a priority for these in-
dividuals, almost equally important is the 
naturalness and individuality one associates with 
one?s own voice. Individuals with difficulty speak-
ing can be any age, gender, and from any part of 
the country, with regional dialects and idiosyncrat-
ic variations. Each individual deserves to speak 
with a voice that is not only intelligible, but uni-
quely his or her own. For those with degenerative 
diseases such as Amyotrophic Lateral Sclerosis 
(ALS), knowing they will be losing the voice that 
has become intricately associated with their identi-
ty is not only traumatic to the individual but to 
family and friends as well.  
A form of synthesis that incorporates the quali-
ties of individual voices is concatenative synthesis. 
In this type of synthesis, units of recorded speech 
are appended. By using recorded speech, many of 
the voice qualities of the person recording the 
speech remain in the resulting synthetic voice. Dif-
ferent synthesis systems append different sized 
28
segments of speech. Appending larger the units of 
speech results in smoother, more natural sounding 
synthesis, but requires many hours of recording, 
often by a trained professional. The recording 
process is usually supervised, and the recordings 
are often hand-polished. Because appending small-
er units requires less recording on the part of the 
speaker, this is the approach the ModelTalker Syn-
thesizer has taken. However using smaller units 
may result in noticeable auditory glitches at conca-
tenative junctures that are a result of variations (in 
pitch, amplitude, pronunciation, etc.) between the 
speech units being appended. Thus the speech rec-
orded must be more uniform in pitch and ampli-
tude. In addition, the units cannot be 
mispronounced because each unit is crucial to the 
resulting synthetic speech. In a smaller database 
there may not be a second example of a specific 
phoneme sequence.  
MT Voice Recorder expects that the individuals 
recording will be untrained and unsupervised, and 
may lack strength and endurance because of the 
presence of a degenerative disease. Thus the sys-
tem is user-friendly enough for untrained, unsu-
pervised individuals to record a corpus of speech. 
The system provides the user with feedback on the 
quality of each utterance they record in terms of 
pronunciation accuracy, relative uniformity of 
pitch, and relative uniformity of amplitude. Confe-
rence attendees will be able to experience this in-
terface system and test all its different features. 
1.2 Feature Demonstration 
At the conference, attendees will be able to try out 
the different features of ModelTalker Voice Re-
corder. These features include automatic micro-
phone calibration, pitch, amplitude, and 
pronunciation detection and feedback, and auto-
matic phoneme labeling of speech recordings. 
 
1.2.1 Microphone calibration 
One important new feature of the MT Voice Re-
corder is the automatic microphone calibration 
procedure. In InvTool, a predecessor software of 
MT Voice Recorder, users had to set the micro-
phone?s amplitude. The system now calibrates the 
signal to noise ratio automatically through a step-
by-step process (see Figure 1, below). 
 
 
Using the automatic calibration procedure, the 
optimal signal to noise ratio is set for the recording 
session. These measurements are retained for fu-
ture recording sessions in cases in which an indi-
29
vidual is unable to record the entire corpus in one 
sitting. 
Once the user has completed the automatic cali-
bration procedure, he will be able to start recording 
a corpus of speech. The interface has been de-
signed with the assumption that individuals will be 
recording without supervision. Thus the interface 
incorporates a number of feedback mechanisms to 
aid individuals in making a high quality corpus for 
synthesis (see Figure 2, below). 
 
1.2.2 Recording Utterances 
The corpus was carefully chosen so that all fre-
quently used phoneme combinations are included 
at least once. Thus it is critical that users pro-
nounce prompted sentences in the manner in which 
the system expects. Alterations in pronunciation as 
small as saying /i/ versus /?/ for ?the,? for example, 
can negatively affect the resulting synthetic voice. 
To reduce the incidence of alternate pronunciation, 
the user is prompted with both a text and an audito-
ry version of the utterance.  
 
1.2.3 Recording Feedback 
Once an utterance has been recorded, the user rece-
ives feedback on the overall quality of the utter-
ance. Specifically, the user receives feedback on 
the pitch, the overall amplitude, and the pronuncia-
tion of the recording. 
Pitch: The user receives feedback on whether 
the utterance?s average pitch is within range of the 
user?s base pitch determined during the calibration 
process. Collecting all recordings within a relative-
ly small pitch range minimizes concatenation costs 
during the synthesis process. MT Voice Recorder 
determines the average pitch of each utterance and 
gives the user feedback on whether the pitch is 
within an acceptable range. This feedback mechan-
ism also helps to eliminate cases in which the sys-
tem is unable to accurately track the pitch of an 
utterance. In these cases, the utterance will be 
marked unacceptable and the user should rerecord, 
hopefully yielding an utterance with more accurate 
pitch tracking. 
 
 
Figure 2: MT Voice Recorder User Interface 
30
Amplitude: The user is also given feedback on 
the overall amplitude of an utterance. If the ampli-
tude is either too low or too high, the user must 
rerecord the utterance. 
Pronunciation: Each recorded utterance is eva-
luated for pronunciation. Each utterance within the 
corpus is associated with a string of phonemes 
representing its transcription. When an utterance is 
recorded, the phoneme string associated with the 
utterance is force-aligned with the recorded 
speech. If the alignment does not fall within an 
acceptable range, the user is given feedback that 
the recording?s pronunciation may not be accepta-
ble and the user is given the option of rerecording 
the utterance. 
 
1.2.4 Automatic Phoneme Labeling  
During the process of pronunciation evaluation, an 
associated phoneme transcription is aligned with 
the utterance. This alignment is retained so that 
each utterance is automatically labeled. Once the 
entire corpus has been recorded, alignments are 
automatically refined based on specific individual 
voice characteristics. 
 
1.2.5 Other Features 
The MT Voice Recorder also allows users to add 
utterances of their choice to the corpus of speech 
for the synthetic voice. These utterances are those 
the user wants to be synthesized clearly and will 
automatically be included in their entirety in the 
speech database. These utterances are also auto-
matically labeled before being stored. 
In addition, for those with more speech and lin-
guistic experience, the system has a number of 
other features that can be explored. For example, 
the MT Voice Recorder also allows one to change 
settings so that the phoneme string, peak ampli-
tude, RMS range, average F0, F0 range, and pro-
nunciation score can be viewed. Users may use this 
information to more precisely adjust their utter-
ances. 
1.3 Synthetic Voice Demonstration 
Those attending the demonstration will also be 
able to listen to a sampling of synthetic voices 
created using the ModelTalker system. While one 
of the synthetic voices was created by a profes-
sional speaker and manually polished, all other 
voices were created by untrained individuals, most 
of whom have ALS, in an untrained setting, with 
the recordings having no manual polishing. 
2 Other Applications 
Although the MTVR was designed specifically to 
record speech for the creation of a database that 
will be used in speech synthesis, it can also be used 
as a digital audio recording tool for speech re-
search. For example, the MT Voice Recorder of-
fers useful features for language documentation. 
An immediate warning about a poor quality re-
cording will alert a researcher to rerecord the utter-
ance. MT Voice Recorder employs file formats 
that are recommended for digital language docu-
mentation (e.g., XML, WAV, and TXT) (Bird & 
Simons, 2003). The recorded files are automatical-
ly stored with broad phonetic labels. The automatic 
saving function will reduce the time of recordings 
and the potential risk for miscataloging the files. 
Currently, the automatic phonetic labeling feature 
is only available for English, but it could be appli-
cable to different languages in the future.  
For more information about the ModelTalker 
System and to experience an interactive demo as 
well as listen to sample synthetic voices,  
visit http://www.modeltalker.com.  
Acknowledgments 
This work was supported by STTR grants 
R41/R42-DC006193 from NIH/NIDCD and from 
Nemours Biomedical Research. We are especially 
indebted to the many people with ALS, the AAC 
specialists in clinics, and other interested individu-
als who have invested a great deal of time and ef-
fort into this project and have provided valuable 
feedback. 
References  
Bird, S. and Simons, G.F. (2003). Seven dimensions of 
portability for language documentation and descrip-
tion. Language, 79(3): 557-582. 
Matas, J., Mathy-Laikko, P., Beaukelman, D. and Le-
gresley. K. (1985). Identifying the nonspeaking 
population: a demographic study, Augmentative & 
Alternative Communication, 1: 17-31. 
  
31
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 98?106,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automated Skimming in Response to Questions for NonVisual Readers 
?
?
Debra Yarrington Kathleen F. McCoy 
Dept. of Computer and Information Science Dept. of Computer and Information Science 
University of Delaware University of Delaware 
Newark, DE, 19716, USA Newark, DE, 19716, USA 
yarringt@eecis.udel.edu mccoy@cis.udel.edu 
 
  
Abstract 
This paper presents factors in designing a sys-
tem for automatically skimming text docu-
ments in response to a question. The system 
will take a potentially complex question and a 
single document and return a Web page con-
taining links to text related to the question. 
The goal is that these text areas be those that 
visual readers would spend the most time on 
when skimming for the answer to a question. 
To identify these areas, we had visual readers 
skim for an answer to a complex question 
while being tracked by an eye-tracking sys-
tem. Analysis of these results indicates that 
text with semantic connections to the question 
are of interest, but these connections are much 
looser than can be identified with traditional 
Question-Answering or Information Retrieval 
techniques. Instead, we are expanding tradi-
tional semantic treatments by using a Web 
search. The goal of this system is to give non-
visual readers information similar to what vis-
ual readers get when skimming through a 
document in response to a question. 
1 Introduction 
This paper describes semantic considerations in 
developing a system for giving nonvisual readers 
information similar to what visual readers glean 
when skimming through a document in response to 
a question. Our eventual system will be unique in 
that it takes both simple and complex questions, 
will work in an unrestricted domain, will locate 
answers within a single document, and will return 
not just an answer to a question, but the informa-
tion visual skimmers acquire when skimming 
through a document.  
1.1 Goals 
Production of our skimming system will require 
the attainment of three major goals: 
1. Achieving an understanding of what information 
in the document visual skimmers pay attention 
to when skimming in response to a question 
2. Developing Natural Language Processing (NLP) 
techniques to automatically identify areas of text 
visual readers focus on as determined in 1. 
3. Developing a user interface to be used in con-
junction with screen reading software to deliver 
the visual skimming experience. 
In this paper we focus on the first two of these 
goals. Section 2 will discuss experiments analyzing 
visual skimmers skimming for answers to ques-
tions. Section 3 will discuss developing NLP tech-
niques to replicate the results of Section 2. Section 
4 will discuss future work. 
1.2 Impetus 
The impetus for this system was work done by the 
author with college students with visual impair-
ments who took significantly longer to complete 
homework problems than their visually reading 
counterparts. Students used both ScreenReaders, 
which read electronic text aloud, and screen mag-
nifiers, which increase the size of text on a screen. 
While these students were comfortable listening to 
the screenreader reading at rates of up to 500 
words per minute, their experience was quite dif-
ferent from their visual-reading peers. Even after 
listening to an entire chapter, when they wanted to 
return to areas of text that contained text relevant 
to the answer, they had to start listening from the 
beginning and traverse the document again. Doing 
98
homework was a tedious, time-consuming task 
which placed these students at a serious disadvan-
tage. It is clear that individuals with visual im-
pairments struggle in terms of education. By 
developing a system that levels the playing field in 
at least one area, we may make it easier for at least 
some individuals to succeed. 
2 Visual Skimming  
If our intention is to convey to nonvisual readers 
information similar to what visual readers acquire 
when skimming for answers to questions, we first 
must determine what information visual readers get 
when skimming. For our purposes, we were inter-
ested in what text readers focused on in connection 
to a question. While many systems exist that focus 
on answering simple, fact-based questions, we 
were more interested in more complex questions in 
which the answer could not be found using pattern 
matching and in which the answer would require at 
least a few sentences, not necessarily contiguous 
within a document. From an NLP standpoint, lo-
cating longer answers with relevant information 
occuring in more than one place that may or may 
not have words or word sequences in common with 
the question poses an interesting and difficult prob-
lem. The problem becomes making semantic con-
nections within any domain that are more loosely 
associated than the synonyms, hypernyms, hypo-
nyms, etc. provided by WordNet (Felbaum, 1998). 
Indeed, the questions that students had the most 
difficulty with were more complex in nature. Thus 
we needed to find out whether visual skimmers 
were able to locate text in documents relevant to 
complex questions and, if so, what connections 
visual skimmers are making in terms of the text 
they choose to focus on.  
2.1 Task Description 
To identify how visual readers skim documents to 
answer questions, we collected 14 questions ob-
tained from students? homework assignments, 
along with an accompanying document per ques-
tion from which the answer could be obtained. The 
questions chosen were on a wide variety of topics 
and were complex in nature. An example of a typi-
cal question is, ?According to Piaget, what tech-
niques do children use to adjust to their 
environment as they grow?? Documents largely 
consisted of plain text, although each had a title on 
the first page. They held no images and few sub-
titles or other areas users might find visually inter-
esting. Twelve of the documents were two pages in 
length, one was eight pages in length, and one was 
nine pages long. In each case, the answer to the 
question was judged by the researchers to be found 
within a single paragraph in the document.  
Forty-three visual reading subjects skimmed for 
the answer to between 6 ? 13 questions. The sub-
jects sat in front of a computer screen to which the 
Eye Tracker 1750 by Tobii Technologies was in-
stalled. The questions and accompanying docu-
ments were displayed on the computer screen and, 
after being calibrated, subjects were tracked as 
they skimmed for the answer. For the two-page 
documents, the question appeared at the top of the 
first page. For the longer documents, the question 
appeared at the top of each page. Subjects had no 
time limit for skimming and switched pages by 
pressing the space bar. When done skimming each 
document, subjects were asked to select a best an-
swer in multiple choice form (to give them a rea-
son to take the skimming task seriously).  
2.2 Results 
Results showed that subjects were reliably able to 
correctly answer the multiple choice question after 
skimming the document. Of the 510 questions, 423 
(about 86%) were answered correctly. The two 
questions from longer documents were the least 
likely to be answered correctly (one had 10 correct 
answers of 21 total answers, and the other had 10 
incorrect answers and only one correct answer).  
Clearly for the shorter documents, subjects were 
able to get appropriate information out of the doc-
ument to successfully answer the question. With 
that established, we were interested in analyzing 
the eye tracking data to see if there was a connec-
tion between where subjects spent the most time in 
the document and the question. If there was an un-
derstandable connection, the goal then became to 
automatically replicate those connections and thus 
automatically locate places in the text where sub-
jects were most likely to spend the most time. 
The Tobii Eye Tracking System tracks the path 
and length of time a subject gazes at a particular 
point as a subject skims through a document. The 
system allows us to define Areas of Interest (AOIs) 
and then track the number of prolonged gaze points 
99
within those areas of interest. For our analysis, we 
defined areas of interest as being individual para-
graphs. While we purposely chose documents that 
were predominantly text, each had a title as well. 
Titles and the few subtitles and lists that occurred 
in the documents were also defined as separate 
AOIs. For each skimming activity, the eye tracking 
system gave us a gaze plot showing the order in 
which individuals focused on particular areas, and 
a hot spot image showing the gaze points, with 
duration indicated with color intensity, that oc-
curred in each AOI (see Figure 1).  
In looking at the hot spot images, we found that 
subjects used three techniques to peruse a docu-
ment. One technique subjects used was to move 
their gaze slowly throughout the entire document, 
indicating that they were most likely reading the 
document. A second technique used was to move 
randomly and quickly from top to bottom of the 
document (described as ?fixations distributed in a 
rough zig-zag down the page? by McLaughlin in 
reference to speed reading (1969)), without ever 
focusing on one particular area for a longer period 
of time. This technique was the least useful to us 
because it gave very little information A third 
technique was a combination of the first two, in 
which the subject?s gaze darted quickly and ran-
domly around the page, and then appeared to focus 
on a particular area for an extended period of time. 
Figure 1 is a good example of this technique. The 
data from this group was clearly relevant to our 
task since their fixation points clearly showed what 
areas subjects found most interesting while skim-
ming for an answer to a question.  
2.3 Analysis of Skimming Data 
To determine exactly which AOIs subjects focused 
on most frequently, we counted the number of gaze 
points (or focus points) in each AOI (defined as 
paragraphs, titles, subtitles) across all subjects. In 
looking at what information individuals focused on 
while skimming, we found that individuals did fo-
cus on the title and subtitles that occurred in the 
documents. Subjects frequently focused on the first 
paragraph or paragraphs of a document. There was 
less of a tendency, but still a trend for focusing on 
the first paragraph on each page. Interestingly, al-
though a few subjects focused on the first line of 
each paragraph, this was not a common practice. 
This is significant because it is a technique availa-
ble to users of screenreaders, yet it clearly does not 
give these users the same information that visual 
skimmers get when skimming through a document.  
We also wanted to look at AOIs that did not 
have physical features that may have attracted at-
tention. Our conjecture was that these AOIs were 
focused on by subjects because of their semantic 
?
Figure 1. Hot spot image results of skimming for the answer to the question, ?What are two dietary factors 
thought to raise and lower cholesterol?? using the Tobii Eye Tracking System 
100
relationship to the question. Indeed, we did find 
evidence of this. Results indicated that subjects did 
focus on the areas of text containing the answer to 
the question. As an example, one of the questions 
used in the study was, 
?How do people catch the West Nile Vi-
rus?? 
The paragraph with the most gaze points for the 
most subjects was: 
?In the United States, wild birds, especial-
ly crows and jays, are the main reservoir 
of West Nile virus, but the virus is actually 
spread by certain species of mosquitoes. 
Transmission happens when a mosquito 
bites a bird infected with the West Nile vi-
rus and the virus enters the mosquito's 
bloodstream. It circulates for a few days 
before settling in the salivary glands. Then 
the infected mosquito bites an animal or a 
human and the virus enters the host's 
bloodstream, where it may cause serious 
illness. The virus then probably multiplies 
and moves on to the brain, crossing the 
blood-brain barrier. Once the virus 
crosses that barrier and infects the brain 
or its linings, the brain tissue becomes in-
flamed and symptoms arise.? 
This paragraph contains the answer to the ques-
tion, yet it has very few words in common with the 
question. The word it does have in common with 
the question, ?West Nile Virus?, is the topic of the 
document and occurs fairly frequently throughout 
the document, and thus cannot account for sub-
jects' focusing on this particular paragraph. 
 The subjects must have made semantic connec-
tions between the question and the answer that 
cannot be explained by simple word matching or 
even synonyms, hypernyms and hyponyms. In the 
above example, the ability of the user to locate the 
answer hinged on their ability to make a connec-
tion between the word ?catch? in the question and 
its meaning ?to be infected by?. Clearly simple 
keyword matching won?t suffice in this case, yet 
equally clearly subjects successfully identified this 
paragraph as being relevant to the question. This 
suggests that when skimming subjects were able to 
make the semantic connections necessary to locate 
question answers, even when the answer was of a 
very different lexical form than the question. 
Other areas of text focused on also appear to 
have a semantic relationship with the question. For 
example, with the question, 
?Why was Monet?s work criticized by the 
public?? 
the second most frequently focused on paragraph 
was: 
?In 1874, Manet, Degas, Cezanne, Renoir, 
Pissarro, Sisley and Monet put together an 
exhibition, which resulted in a large finan-
cial loss for Monet and his friends and 
marked a return to financial insecurity for 
Monet. It was only through the help of 
Manet that Monet was able to remain in 
Argenteuil. In an attempt to recoup some 
of his losses, Monet tried to sell some of 
his paintings at the Hotel Drouot. This, 
too, was a failure. Despite the financial 
uncertainty, Monet?s paintings never be-
came morose or even all that sombre. In-
stead, Monet immersed himself in the task 
of perfecting a style which still had not 
been accepted by the world at large. Mo-
net?s compositions from this time were ex-
tremely loosely structured, with color 
applied in strong, distinct strokes as if no 
reworking of the pigment had been at-
tempted. This technique was calculated to 
suggest that the artist had indeed captured 
a spontaneous impression of nature.? 
Of the 30 subjects who skimmed this document, 
15 focused on this paragraph, making it the second 
most focused on AOI in the document, second only 
to the paragraph that contained the answer (fo-
cused on by 21 of the subjects). The above para-
graph occurred within the middle of the second 
page of the document, with no notable physical 
attributes that would have attracted attention. Upon 
closer inspection of the paragraph, there are refer-
ences to ?financial loss,? ?financial insecurity,? 
?losses,? ?failure,? and ?financial uncertainty.? 
The paragraph also includes ?morose? and ?somb-
er? and even ?had not been accepted by the world 
at large.? Subjects appeared to be making a con-
nection between the question topic, Monet?s work 
being criticized by the public, and the above terms. 
Intuitively, we do seem to make this connection. 
Yet the connection being made is not straightfor-
ward and cannot be replicated using the direct se-
101
mantic connections that are available via WordNet. 
Indeed, the relationships made are more similar to 
Hovy and Lin?s (1997) Concept Signatures created 
by clustering words in articles with the same edi-
tor-defined classification from the Wall Street 
Journal. Our system must be able to replicate these 
connections automatically. 
 Upon further examination, we found other pa-
ragraphs that were focused on by subjects for rea-
sons other than their physical appearance or 
location, yet their semantic connection to the ques-
tion was even more tenuous. For instance, when 
skimming for the answer to the question, 
?How does marijuana affect the brain?? 
the second most frequently focused on paragraph 
(second to the paragraph with the answer) was,  
?The main active chemical in marijuana is 
THC (delta-9-tetrahydrocannabinol). The 
protein receptors in the membranes of cer-
tain cells bind to THC. Once securely in 
place, THC kicks off a series of cellular 
reactions that ultimately lead to the high 
that users experience when they smoke 
marijuana.? 
While this paragraph does appear to have loose 
semantic connections with the question, the con-
nections are less obvious than paragraphs that fol-
low it, yet it was this paragraph that subjects chose 
to focus on. The paragraph is the third to last para-
graph on the first page, so its physical location 
could not explain its attraction to subjects. Howev-
er, when we looked more closely at the previous 
paragraphs, we saw that the first paragraph deals 
with definitions and alternate names for marijuana 
(with no semantic links to the question), and the 
second and third paragraph deal with statistics on 
people who use marijuana (again, with no semantic 
connection to the question). The fourth paragraph, 
the one focused on, represents a dramatic semantic 
shift towards the topic of the question. Intuitively it 
makes sense that individuals skimming through the 
document would pay more attention to this para-
graph because it seems to represent the start of the 
area that may contain the answer, not to mention 
conveying topological information about the layout 
of the document and general content information 
as well.  
Data collected from these experiments suggest 
that subjects do make and skim for semantic con-
nections. Subjects not only glean information that 
directly answers the question, but also on content 
within the document that is semantically related to 
the question. While physical attributes of text do 
attract the attention of skimmers, and thus we must 
include methods for accessing this data as well, it 
is clear that in order to create a successful skim-
ming device that conveys information similar to 
what visual skimmers get when skimming for the 
answer to a question, we must come up with a me-
thod for automatically generating loose semantic 
connections and then using those semantic connec-
tions to locate text skimmers considered relevant 
within the document. 
3 NLP Techniques  
In order to automatically generate the semantic 
connections identified above as being those visual 
skimmers make, we want to explore Natural Lan-
guage Processing (NLP) techniques. 
3.1 Related Research 
Potentially relevant methodologies may be found 
in Open Domain Question Answering Systems. 
Open Domain Question Answering Systems in-
volve connecting questions within any domain and 
potential answers. These systems usually do not 
rely on external knowledge sources and are limited 
in the amount of ontological information that can 
be included in the system. The questions are usual-
ly fact-based in form (e.g., ?How tall is Mt. Ever-
est??). These systems take a question and query a 
potentially large set of documents (e.g., the World 
Wide Web) to find the answer. A common tech-
nique is to determine a question type (e.g., ?How 
many ??? would be classified as ?numerical?, 
whereas ?Who was ??? would be classified as 
?person?, etc.) and then locate answers of the cor-
rect type (Abney et al, 2000; Kwok et al, 2001; 
Srihari and Li, 2000; Galea, 2003). Questions are 
also frequently reformulated for pattern matching 
(e.g., ?Who was the first American Astronaut in 
space?? becomes, ?The first American Astronaut 
in space was? (Kwok et al, 2001; Brill et al, 
2002)). Many systems submit multiple queries to a 
document corpus, relying on redundancy of the 
answer to handle incorrect answers, poorly con-
structed answers or documents that don?t contain 
the answer (e.g., Brill et al, 2002; Kwok et al, 
102
2001). For these queries, systems often include 
synonyms, hypernyms, hyponyms, etc. in the query 
terms used for document and text retrieval (Hovy 
et al,2000; Katz et al, 2005). In an attempt to an-
swer more complex relational queries, Banko et al 
(2007) parsed training data into relational tuples 
for use in classifying text tagged for part of speech, 
chunked into noun phrases, and then tagged the 
relations for probability. Soricut and Brill (2006) 
trained data on FAQ knowledge bases from the 
World Wide Web, resulting in approximately 1 
million question-answer pairs. This system related 
potential answers to questions using probability 
models computed using the FAQ knowledge base. 
Another area of research that may lend useful 
techniques for connecting and retrieving relevant 
text to a question is query-biased text summariza-
tion. With many summarization schemes, a good 
deal of effort has been placed on identifying the 
main topic or topics of the document. In query bi-
ased text summarization, however, the topic is 
identified a priori, and the task is to locate relevant 
text within a document or set of documents. In 
multidocument summarization systems, redundan-
cy may be indicative of relevance, but should be 
eliminated from the resulting summary. Thus a 
concern is measuring relevance versus redundancy 
(Carbonell and Goldstein, 1998; Hovy et al, 2005; 
Otterbacher et al, 2006). Like Question Answering 
systems, many summarization systems simply 
match the query terms, expanded to include syn-
onyms, hypernyms, hyponyms, etc., to text in the 
document or documents (Varadarajan and Hristi-
dis, 2006; Chali, 2002)  
Our system is unique in that it has as its goal not 
just to answer a question or create a summary, but 
to return information visual skimmers glean while 
skimming through a document. Questions posed to 
the system will range from simple to complex in 
nature, and the answer must be found within a sin-
gle document, regardless of the form the answer 
takes. Questions can be on any topic. With com-
plex questions, it is rarely possible to categorize 
the type of question (and thus the expected answer 
type). Intuitively, it appears equally useless to at-
tempt reformulation of the query for pattern match-
ing. This intuition is born out by Soricut and Brill 
(2006) who stated that in their study reformulating 
complex questions more often hurt performance 
than improved it. Answering complex questions 
within a single document when the answer may not 
be straightforward in nature poses a challenging 
problem. 
3.2 Baseline Processing 
Our baseline system attempted to identify areas of 
interest by matching against the query in the tradi-
tion of Open Domain Question Answering. For our 
baseline, we used the nonfunction words in each 
question as our query terms. The terms were 
weighted with a variant of TF/IDF (Salton and 
Buckley, 1988) in which terms were weighted by 
the inverse of the number of paragraphs they oc-
curred in within the document. This weighting 
scheme was designed to give lower weight to 
words associated with the document topic and thus 
conveying less information about relevance to the 
question. Each query term was matched to text in 
each paragraph, and paragraphs were ranked for 
matching using the summation of, for each query 
term, the number of times it occurred in the para-
graph multiplied by its weight.  
Results of this baseline ranking were poor. In 
none of the 14 documents did this method connect 
the question to the text relevant to the answer. This 
was expected. This original set of questions was 
purposely chosen because of the complex relation-
ship between the question and answer text. 
Next we expanded the set of query terms to in-
clude synonyms, hypernyms, and hyponyms as 
defined in WordNet (Felbaum, 1998). We included 
all senses of each word (query term). Irrelevant 
senses resulted in the inclusion of terms that were 
no more likely to occur frequently than any other 
random word, and thus had no effect on the result-
ing ranking of paragraphs. Again, each of the 
words in the expanded set of query terms was 
weighted as described above, and paragraphs were 
ranked accordingly. 
Again, results were poor. Paragraphs ranked 
highly were no more likely to contain the answer, 
nor were they likely to be areas focused on by the 
visual skimmers in our collected skimming data.  
Clearly, for complex questions, we need to ex-
pand on these basic techniques to replicate the se-
mantic connections individuals make when 
skimming. As our system must work across a vast 
array of domains, our system must make these 
connections ?on the fly? without relying on pre-
viously defined ontological or other general know-
ledge. And our system must work quickly: asking 
103
individuals to wait long periods of time while the 
system creates semantic connections and locates 
appropriate areas of text would defeat the purpose 
of a system designed to save its users time. 
3.3 Semantically-Related Word Clusters 
Our solution is to use the World Wide Web to form 
clusters of topically-related words, with the topic 
being the question. The cluster of words will be 
used as query terms and matched to paragraphs as 
described above for ranking relevant text.  
Using the World Wide Web as our corpus has a 
number of advantages. Because of the vast number 
of documents that make up the World Wide Web, 
we can rely on the redundancy that has proved so 
useful for Question Answering and Text Summari-
zation systems. By creating the word clusters from 
documents returned from a search using question 
words, the words that occur most frequently in the 
related document text will most likely be related in 
some way to the question words. Even relatively 
infrequently occurring word correlations can most 
likely be found in some document existing on the 
Web, and thus strangely-phrased questions or 
questions with odd terms will still most likely 
bring up some documents that can be used to form 
a cluster. The Web covers virtually all domains. 
Somewhere on the Web there is almost certainly an 
answer to questions on even the most obscure top-
ics. Thus questions containing words unique to 
uncommon domains or questions containing un-
usual word senses will return documents with ap-
propriate cluster words. Finally, the Web is 
constantly being updated. Terms that might not 
have existed even a year ago will now be found on 
the Web. 
Our approach is to use the nonstop words in a 
question as query terms for a Web search. The 
search engine we are using is Google 
(www.google.com). For each search engine query, 
Google returns an ranked list of URLs it considers 
relevant, along with a snippet of text it considers 
most relevant to the query (usually because of 
words in the snippet that exactly match the query 
terms). To create the cluster of words related se-
mantically to the question, we are taking the top 50 
URLs, going to their correlating Web page, locat-
ing the snippet of text within the page, and creating 
a cluster of words using a 100-word window sur-
rounding the snippet. We are using only nonstop 
words in the cluster, and weighting the words 
based on their total number of occurrences in the 
windows. These word clusters, along with the ex-
panded baseline words, are used to locate and rank 
paragraphs in our question document. 
Our approach is similar in spirit to other re-
searchers using the Web to identify semantic rela-
tions. Matsuo et al (2006) looked at the number of 
hits of each of two words as a single keyword ver-
sus the number of hits using both words as key-
words to rate the semantic similarity of two words. 
Chen et al (2006) used a similar approach to de-
termine the semantic similarity between two 
words: with a Web search using word P as the 
query term, they counted the number of times word 
Q occurred in the snippet of text returned, and vice 
versa. Bollegala et al (2007) determined semantic 
relationships by extracting lexico-syntactic patterns 
from the snippets returned from a search on two 
keywords (e.g.,??x? is a ?y??) and extracting the 
relationship of the two words based on the pattern. 
Sahami and Heilman (2006) used the snippets from 
a word search to form a set of words weighted us-
ing TF/IDF, and then determined the semantic si-
milarity of two keywords by the similarity of two 
word sets returned in those snippets.  
Preliminary results from our approach have been 
encouraging. For example, with the question, 
?How does Marijuana affect the brain??, the ex-
panded set of keywords included, ?hippocampus, 
receptors, THC, memory, neuron?. These words 
were present in both the paragraph containing the 
answer and the second-most commonly focused on 
paragraph in our study. While neither our baseline 
nor our expanded baseline identified either para-
graph as an area of interest, the semantically-
related word clusters did. 
4 Future Work 
This system is a work in progress. There are many 
facets still under development, including a finer 
analysis of visual skimming data, a refinement of 
the ranking system for locating areas of interest 
within a document, and the development of the 
system?s user interface. 
4.1 Skimming Data Analysis 
For our initial analysis, we focused on the length of 
time users spent gazing at text areas. In future 
104
analysis, we will look at the order of the gaze 
points to determine exactly where the subjects first 
gazed before choosing to focus on a particular 
area. This may give us even more information 
about the type of semantic connection subjects 
made before choosing to focus on a particular area. 
In addition, in our initial analysis, we defined AOIs 
to be paragraphs. We may want to look at smaller 
AOIs. For example, with longer paragraphs, the 
text that actually caught the subject?s eye may have 
occurred only in one portion of the paragraph, yet 
as the analysis stands now the entire content of the 
paragraph is considered relevant and thus we are 
trying to generate semantic relationships between 
the question and potentially unrelated text. While 
the system only allows us to define AOIs as rec-
tangular areas (and thus we can?t do a sentence-by-
sentence analysis), we may wish to define AOIs as 
small as 2 lines of text to narrow in on exactly 
where subjects chose to focus.  
4.2 Ranking System Refinement 
It is worth mentioning that, while a good deal of 
research has been done on evaluating the goodness 
of automatically generated text summaries (Mani 
et al,2002; Lin and Hovy, 2003; Santos et al, 
2004) our system is intended to mimic the actions 
of skimmers when answering questions, and thus 
our measure of goodness will be our system?s 
ability to recreate the retrieval of text focused on 
by our visual skimmers. This gives us a distinct 
advantage over other systems in measuring good-
ness, as defining a measure of goodness can prove 
difficult. In future work, we will be exploring dif-
ferent methods of ranking text such that the system 
returns results most similar to the results obtained 
from the visual skimming studies. The system will 
then be used on other questions and documents and 
compared to data to be collected of visual skim-
mers skimming for answers to those questions. 
Many variations on the ranking system are poss-
ible. These will be explored to find the best 
matches with our collected visual skimming data. 
Possibilities include weighting keywords different-
ly according to where they came from (e.g., direct-
ly from the question, from the text in retrieved 
Web pages, from text from a Web page ranked 
high on the returned URL list or lower, etc.), or 
considering how a diversity of documents might 
affect results. For instance, if keywords include 
?falcon? and ?hawk? the highest ranking URLs will 
most likely be related to birds. However, in G.I. 
Joe, there are two characters, Lieutenant Falcon 
and General Hawk. To get the less common con-
nection between falcon and hawk and G.I. Joe, one 
may have to look for diversity in the topics of the 
returned URLs. Another area to be explored will 
be the effect of varying the window size surround-
ing the snippet of text to form the bag of words.  
4.3 User Interface 
The user interface for our system poses some inter-
esting questions. It is important that the output of 
the system provide the user with information about 
(1) document topology, (2) document semantics, 
and (3) information most relevant to answering the 
question. At the same time, it is important that us-
ing the output be relatively fast. The output of the 
system is envisioned as a Web page with ranked 
links at the top pointing to sections of the text like-
ly to be relevant to answering the question.  
An important issue that must be explored in 
depth with potential users of the system is the ex-
act form of the output web page. We need to ex-
plore the best method for indicating text areas of 
interest and the overall topology. The goal is that 
reading the links simulate what a visual skimmer 
gets from lightly skimming. The user would actual-
ly follow the links that appeared to be ?worth read-
ing? in more detail in the same way that skimmers 
focus in on particular text segments that appear 
worth reading.  
5 Conclusion 
This system attempts to correlate NLP techniques 
for creating semantic connections with the seman-
tic connections individuals make. Using the World 
Wide Web, we may be able to make those seman-
tic connections across any topic in a reasonable 
amount of time without any previously defined 
knowledge. We have ascertained that people can 
and do make semantic links when skimming for 
answers to questions, and we are currently explor-
ing the best use of the World Wide Web in repli-
cating those connections. In the long run, we 
envision a system that is user-friendly to nonvisual 
and low vision readers that will give them an intel-
ligent way to skim through documents for answers 
to questions.  
105
References  
S. Abney, M. Collins, and A. Singhal. 2000. Answer 
Extraction. In Proceedings of ANLP 2000, 296-301. 
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open information extraction 
from the Web. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence, 
2670-2676. 
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using Web 
search engines. In Proceedings of WWW 2007. 757-
766. 
Brill, E., Lin, J., Banko, M., Domais, S. and Ng, A. 
2001. Data-Intensive Question Answering. In Pro-
ceedings of the TREC-10 Conference, NIST, Gai-
thersburg, MD, 183-189. 
J. Carbonell and J. Goldstein. 1998. The use of MMR, 
diversity-based reranking for reordering documents 
and producing summaries. In Proceedings of SIGIR 
?98, New York, NY, USA, 335-336. 
Y. Chali. 2002. Generic and query-based text summari-
zation using lexical cohesion, In Proceedings of the 
Fifteenth Canadian Conference on Artificial Intelli-
gence, Calgary, May, 293-303. 
H. Chen, M. Lin, and Y. Wei. 2006. Novel association 
measures using web search with double checking. In 
Proceedings of the COLING/ACL 2006. 1009-1016. 
C. Felbaum. 1998. WordNet an Electronic Database, 
Boston/Cambridge: MIT Press. 
A. Galea.2003. Open-domain Surface-Based Question 
Answering System. In Proceedings of the Computer 
Science Annual Workshop (CSAW), University of 
Malta. 
E. H. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C.-
Y. Lin. 2000. Question Answering in Webclopedia. 
In Proceedings of the TREC-9 Conference. NIST, 
Gaithersburg, MD. November 2000. 655-664. 
E. Hovy and C.Y. Lin. 1997. Automated Text Summa-
rization in SUMMARIST. In Proceedings of the 
Workshop on Intelligent Scalable Text Summariza-
tion, Madrid, Spain, 18-24. 
Boris Katz, Gregory Marton, Gary Borchardt, Alexis 
Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-
Rosenberg, Ben Lu, Federico Mora, Stephan Stiller, 
Ozlem Uzuner, and Angela Wilcox. 2005. External 
Knowledge Sources for Question Answering Pro-
ceedings of the 14th Annual Text REtrieval Confe-
rence (TREC2005), November 2005, Gaithersburg, 
MD. 
C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling 
Question Answering to the Web. In Proceedings of 
the 10th World Wide Web Conference, Hong Kong, 
150-161. 
M. Sahami and T. Heilman. 2006. A Web-based kernel 
function for measuring the similarity of short text 
snippets. In Proceedings of 15th International World 
Wide Web Conference. 377-386. 
Chin-Yew Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence Sta-
tistics, In Proceedings of HLT-NAACL, 71?78. 
I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, 
and B. Sundheim. 2002. SUMMAC: a text summari-
zation evaluation, Natural Language Engineering, 8 
(1):43-68. 
Y, Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka. 
2006. Graph-based word clustering using Web search 
engine. In Proceedings of EMNLP 2006, 542-550. 
G. Harry McLaughlin. 1969. Reading at ?Impossible? 
Speeds. Journal of Reading, 12(6):449-454,502-510. 
Radu Soricut and Eric Brill. 2006. Automatic question 
answering using the web: Beyond the factoid. Jour-
nal of Information Retrieval - Special Issue on Web 
Information Retrieval, 9:191?206. 
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing & Management, 24, 5, 513-523. 
E. J. Santos, A. A. Mohamed, and Q. Zhao. 2004. "Au-
tomatic Evaluation of Summaries Using Document 
Graphs," Text Summarization Branches Out. Pro-
ceedings of the ACL-04 Workshop, Barcelona, Spain, 
66-73. 
R. Srihari and W.A. Li. 2000. Question Answering Sys-
tem Supported by Information Extraction. In Pro-
ceedings of the 1st Meeting of the North American 
Chapter of the Association for Computational Lin-
guistics (NAACL-00), 166-172. 
R. Varadarajan and V. Hristidis. 2006. A system for 
query-specific document summarization, ACM 15th 
Conference on Information and Knowledge Man-
agement (CIKM), Arlington, VA, 622-631. 
 
106
