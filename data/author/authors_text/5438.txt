	

		
	
Scalability and Portability of a Belief Network-based
Dialog Model for Different Application Domains
Carmen Wai
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
SAR, China
Tel:  +852 2609 8327
cmwai@se.cuhk.edu.hk
Helen M. Meng
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
SAR, China
Tel:  +852 2609 8327
hmmeng@se.cuhk.edu.hk
Roberto Pieraccini
SpeechWorks International Ltd
17 State Street
New York, NY 1004
Tel: +1.212.425.7200
roberto.pieraccini@speechworks.com
ABSTRACT
This paper describes the scalability and portability of a Belief
Network (BN)-based mixed initiative dialog model across
application domains. The Belief Networks (BNs) are used to
automatically govern the transitions between a system-initiative
and a user-initiative dialog model, in order to produce mixed-
initiative interactions. We have migrated our dialog model from
a simpler domain of foreign exchange to a more complex
domain of air travel information service.  The adapted processes
include: (i) automatic selection of specified concepts in the
user?s query, for the purpose of informational goal inference; (ii)
automatic detection of missing / spurious concepts based on
backward inference using the BN.  We have also enhanced our
dialog model with the capability of discourse context
inheritance.  To ease portability across domains, which often
implies the lack of training data for the new domain, we have
developed a set of principles for hand-assigning BN
probabilities, based on the ?degree of belief? in the
relationships between concepts and goals.  Application of our
model to the ATIS data gave promising results.
1. INTRODUCTION
Spoken dialog systems demonstrate a high degree of usability in
many restricted domains, and dialog modeling in such systems
plays an important role in assisting users to achieve their goals.
The system-initiative dialog model assumes complete control in
guiding the user through an interaction towards task completion.
This model often attains high task completion rates, but the user
is bound by many constraints throughout the interaction.
Conversely, the user-initiative model offers maximum flexibility
to the user in determining the preferred course of interaction.
However this model often has lower task completion rates
relative to the system-initiative model, especially when the
user?s request falls beyond the system's competence level. To
strike a balance between these two models, the mixed-initiative
dialog model allows both the user and the system to influence
the course of interaction.  It is possible to handcraft a
sophisticated mixed-initiative dialog flow, but the task is
expensive, and may become intractable for complex application
domains.
  
We strive to reduce handcrafting in the design of mixed-
initiative dialogs. We propose to use Belief Networks (BN) to
automatically govern the transitions between a system-initiative
and a user-initiative dialog model, in order to produce mixed-
initiative interactions.  Previous work includes the use of
semantic interpretation rules for natural language understanding,
where the rules are learnt by decision trees known as Semantic
Classification Trees (SCTs) [6]. Moreover, there is also
previous effort that explores the use of machine learning
techniques to automatically determine the optimal dialog
strategy. A dialog system can be described as a sequential
decision process that has states and actions. An optimal strategy
can be obtained by reinforcement learning [7, 8].  While the
system is interacting with users, it can explore the state space
and thus learn different actions.
Our BN framework was previously used for natural
language understanding [1,2].  We have extended this model for
dialog modeling, and demonstrated feasibility in the CU
FOREX (foreign exchange) [3,4] system, whose domain has
low complexity.  This work explores the scalability and
portability of our BN-based dialog model to a more complex
application.  We have chosen the ATIS (Air Travel Information
Service) domain due to data availability.1
2. BELIEF NETWORKS FOR MIXED-
INITIATIVE DIALOG MODELING ?
THE CU FOREX DOAMIN
We have devised an approach that utilizes BNs for mixed-
initiative dialog modeling, and demonstrated its feasibility in
the CU FOREX domain.  Details can be found in [4].  We
provide a brief description here for the sake of continuity.
CU FOREX is a bilingual (English and Cantonese)
conversational hotline that supports inquiries regarding foreign
exchange.  The domain is relatively simple, and can be
characterized by two query types (or informational goals ?
Exchange Rate or Interest Rate); and five domain-specific
concepts (a CURRENCY pair, TIME DURATION, EXCHANGE RATE
and INTEREST RATE).  Our approach involves two processes:
2.1 Informational Goal Inference
A BN is trained for each informational goal.  Each BN receives
as input the concepts that are related to its corresponding goal.
In CU FOREX, there are two BNs, each with five input
concepts. The pre-defined BN topology shown in Figure 1
(without dotted arrow) incorporates the simplifying assumption
that all concepts are dependent only on the goal, but are
independent of one another.  This topology can be enhanced by
                                                                
1 The ATIS data can be licensed from the Linguistic Data
Consortium (www.ldc.upenn.edu).
learning the inter-concept dependencies from training data
according to the Minimum Description Length (MDL) principle
[2]. The resultant topology is illustrated in Figure 1.
Figure 1. The predefined topology of our BNs is enhanced by
the linkage (dotted arrow) learnt to capture dependencies
among concepts. The arrows of the acyclic graph are drawn
from cause to effect.
Given an input query, each trained BN will make a binary
decision (using pre-set threshold of 0.5)2 regarding the presence
or absence of its corresponding informational goal, based on the
presence or absence of its input concepts in the query.  The
decisions across all BNs are combined to identify the
informational goal of the input query.  We labeled the query to
a goal if the corresponding BN votes positive with the
maximum aposteriori probability.  Alternatively, we may label
the query with all goals for which the BNs vote positive.
Should all BNs vote negative, the query is rejected as out-of-
domain (OOD).
2.2 Detection of Missing / Spurious
Concepts
Automatic detection of missing or spurious concepts is
achieved by backward inference in the BN.  Given an identified
goal from the previous process, the goal node of the
corresponding BN is instantiated (i.e. P(Gi) set to 1), and
backward inference updates the probability of each concept
P(Ci).  Comparison between P(Ci)  and a pre-set threshold ?
(=0.5) determines whether the concept should be present or
absent; and further comparison with the actual occurrence(s)
determines whether the concept is missing or spurious. In this
way, domain-specific constraints for database access is captured
and enforced in the BN, i.e. an Exchange Rate inquiry
requires a currency pair, and an Interest Rate inquiry requires
specifications of the currency and the duration.  A missing
concept will cause the dialog model to automatically trigger a
system prompt.  A spurious concept will cause automatically
trigger a request for clarification.
Table 1 provides an illustrative example from the CU
FOREX domain.   The first process infers that the query ?Can I
have the interest rate of the yen?? has the informational goal of
Interest Rate.  The second process of backward inference
indicates that the concept <DURATION> should be present, but is
absent from the query.  Hence <DURATION> is a missing concept
and the dialog model prompts for the information.
                                                                
2  We choose threshold at 0.5 since P(G=1|C)+P(G=0|C)=1
Table 1. This table steps through our dialog modeling process.
The input query is ?Can I have the interest rate of the yen?.
Process 1 (informational goal inference) identifies that this is an
interest rate inquiry.  Process 2 performs backward inference to
compute the concept probabilities. Thresholding with ?=0.5
indicates whether the concept should be present or absent.
Comparison between this binary decision and the actual
occurrence detects that the concept <DURATION> is missing.
Hence the dialog model prompts for the missing information.
Query:  Can I have the interest rate of the yen?
Process 1:  Informational Goal Inference
BN for Interest Rate
P(Goal = Interest Rate | Query) = 0.801 ? goal present
BN for Exchange Rate
P(Goal = Exchange Rate | Query) = 0.156 ? goal absent
Hence, inferred goal is Interest Rate.
Process 2:  Detection of Missing / Spurious Concepts
Concept Cj P(Cj) Binary
Decision
for Cj
Actual
Occurrence
of Cj
CURRENCY1 0.91 present present
CURRENCY2 0.058 absent absent
DURATION 0.77 present absent
EXCHANGE_RATE 0.011 absent absent
INTEREST RATE 0.867 present present
Response:  How long would you like to deposit?
3. MIGRATION TO THE ATIS DOMAIN
Our experiments are based on the training and test sets of the
Air Travel Information Service (ATIS) domain. ATIS is a
common task in the ARPA (Advanced Research Projects
Agency) Speech and Language Program in the US. We used the
Class A (context-independent) as well as Class D (context-
dependent) queries of the ATIS-3 corpus. The disjoint training
and test sets consist of 2820, 773 (1993 test), 732 (1994 test)
transcribed utterances respectively.  Each utterance is
accompanied with its corresponding SQL query for retrieving
the relevant information.
We derive the informational goal for each utterance from
the main attribute label of its SQL query. Inspection of the
Class A training data reveals that out of the 32 query types (or
informational goals, e.g. flight identification, fare identification,
etc.), only 11 have ten or more occurrences. These 11 goals
cover over 95% of the training set, and 94.7% of the testing set
(1993 test).  Consequently, we have developed 11 BNs to
capture the domain-specific constraints for each informational
goal. Also, with the reference to the attribute labels identified as
key semantic concepts from the SQL query, we have designed
our semantic tags for labeling the input utterance. We have a
total of 60 hand-designed semantic tags, where both syntactic
(e.g. <PREPOSITION> <SUPERLATIVE>) and semantic concepts
(e.g. <DAY_NAME>, <FLIGHT_NUMBER>) are present. Hence,
ATIS presents increased domain complexity, which is
characterized by 11 query types and total 60 domain-specific
concepts.
? represents goal node
? represents concept node
Interest Rates
CURR1
CURR2 DURATION EX_RATE
IN_RATE
4. SCALABILITY OF A BN-BASED
DIALOG MODEL
4.1 Informational Goal Inference
There is a total of 60 hand-designed3 semantic concepts in the
ATIS domain. In order to constrain computation time for goal
inference, we have limited the number of semantic concepts (N)
that are indicative of each goal Gj.  The parameter N (=20) has
been selected using the Information Gain criterion to optimize
on overall goal identification accuracy on the Class A training
utterances [1].
We have also refined the pre-defined topology using
Minimum Description Length (MDL) principle to model
concept dependencies. Example of the BN is shown in Figure 2.
Their inclusion brought performance improvements in goal
identification [2].
Figure 2. Topology of the BNs for the informational goal
Flight_ID.
Consequently, each BN has a classification-based network
topology ? there are N (=20) input concept nodes (e.g. airline,
flight_number, etc.) and a single output node.  To avoid the use
of sparsely trained BNs, we have developed 11 BNs to capture
the domain-specific constraints for each informational goal
using Class A training data. The remaining goals are then
treated as out-of-domain.
A trained BN is then used to infer the presence / absence
of its corresponding informational goal, based on the input
concepts. According to the topology shown in Figure 1, the
learnt network is divided into sub-networks: {Flight_ID,
CITY_1, CITY_2}, {Flight_ID, AIRLINE, CLASS}, {Flight_ID,
TIME}, etc. The updated joint probabilities are iteratively
computed according to the Equation (1) by each sub-network,
the aposteriori probability P*(Gi) is computed by the
marginalization of the updated joint probability P*(Gi,C). P*(Gi)
is then compared to a threshold (?) to make the binary decision.
where P*(C) is instantiated according the presence or absence
of the concepts; P(Gi,C) is the joint probability obtained from
training and P*(Gi,C) is the updated joint probability
The binary decisions across all BNs are combined to
identify the informational goal of the input query.  We may
label the query to a goal if the corresponding BN votes positive
with the highest aposteriori probability. Alternatively, we may
label the query with all the goals for which the BNs votes
positive. Should all BNs vote negative, the input query is
rejected as out-of-domain (OOD).
                                                                
3 We have included the concepts/attributes needed for database
access, as well as others that play a syntactic role for natural
language understanding.
4.2 Detection of Missing / Spurious
Concepts
Having inferred the informational goal of the query, the
corresponding node (goal node) is instantiated, and we perform
backward inference to test the networks' confidence in each
input concept. In this way, we can test for cases of spurious and
missing concepts, and generate the appropriate systems
response.
When the goal node is instantiated for backward inference,
the joint probability of P(C, Gi) will be updated for each sub-
network by Equation 2:
where P*(G) is updated and instantiated to 1, P(C|Gi) is the
conditional probability obtained from training data and
P*(C,Gi) is the updated joint probability
By marginalization, we can get P(Cj). We have pre-set
threshold 0.5 for the CU FOREX domain to determine whether
the concept should be present or absent. However, when the
dialog modeling using single threshold scheme is applied to the
ATIS domain, we often obtained several missing / spurious
concepts for an input query. For example, consider the query.
Query: What type of aircraft is used in American airlines
flight number seventeen twenty three?
 Concepts: <WHAT> <TYPE> <AIRCRAFT> <AIRLINE_NAME>
<FLIGHT_NUMBER>
 Goal: Aircraft_Code
Our BN for AIRCRAFT_CODE performed backward
inference and the results in Table 2 using single threshold
scheme indicated that the concepts <ORIGIN> and
<DESTINATION> are missing, while <FLIGHT_NUMBER> is
spurious.  One reason is because in the training data, most
queries with the goal Aircraft_Code provided the city pair
instead of the flight number, but both serve equally well as an
additional specification for database access.  If our dialog
model followed through with these detected missing and
spurious concepts, it would prompt the user for the city of
origin, then the city of destination; and then clarify that the
flight number is spurious.  In order to avoid such redundancies,
we defined two thresholds for backward inferencing, as follows:
Hence concepts whose probabilities (from backward
inference) scores between ?upper and ?lower will not take effect in
response generation (i.e. prompting / clarification).  Concepts
whose scores exceed ?upper, and also correspond to an SQL
attribute will be prompted if missing; and concepts whose
scores scant ?lower, and correspond to an SQL attribute will be
clarified if spurious. By minimizing number of dialog turns
interacting with the users in the training data, we have
empirically adopted 0.7 and 0.2 for ?upper and ?lower respectively.
The double threshold scheme enables the dialog model to
prompt for missing concepts that are truly needed, and clarify
for spurious concepts that may confuse the query?s
interpretation.
 (1)
 (2)
<?upper and >=?lower? Cj is optional in the given Gi query
>=?upper ? Cj  should be present in the given Gi query
P(Cj)
< ?lower ? Cj should be absent in the given Gi query
)()(
),(),()()|(),( **** CPCP
CGPCGPCPCGPCGP iiii
v
v
vvvvv
=?=
)()|(),( ** iii GPGCPGCP
vv
=
?
 ?
? represents  a goal node
? represents a concept node
CITY_1
Flight ID
CITY_2
    AIRLINE
TIME     CLASS
Table 2. Aposteriori probabilities obtained from backward
inferencing using 0.5 as threshold for the query ?What type of
aircraft is used in american airlines flight number seventeen
twenty three??
Conceptj (Cj)
(Part of concepts)
P(Cj ) Binary
Decision
For Cj
Actual
Occurrence
for Cj
AIRCRAFT 1.000 present present
CITY_NAME1 0.645 present absent
CITY_NAME2 0.615 present absent
DAY_NAME 0.077 absent absent
FLIGHT_NUMBER 0.420 absent present
4.3 Context Inheritance
We attempt to test our framework using ATIS-3 Class A and D
queries.  As the Class D queries involve referencing discourse
context derived from previous dialog turns, we have enhanced
our BN-based dialog model with the capability of context
inheritance. Since the additional concepts may affect our goal
inference, we choose to invoke goal inference again (after
context inheritance) only if query was previously (prior to
context inheritance) classified as OOD.  Otherwise, the original
inferred goal of the query is maintained.  This is illustrated in
Table 3.  Context inheritance serves to fill in the concepts
detected missing from the original query.  This is illustrated in
Tables 4 and 5.
Table 3. Examples of ATIS dialogs produced by the BN-based
dialog model. It indicates that the OOD query is inferred again
as Flight_ID query after the inheritance of discourse context.
System What kind of flight information are you interested
in?
User I'd like to fly from miami to chicago on american
airlines. (Class A query)
System Goal Inference: Flight_ID (Concepts pass the
domain constraints)
User Which ones arrive around five p.m.?
(Class D)
System Goal Inference: Flight_ID. (System first infers this
query as OOD, but it retrieves the concepts from
the discourse context and infers again to get
Flight _ ID.)
Table 4. Examples of ATIS dialogs produced by the BN-based
dialog model with the capability of inheritance for the missing
concepts.
System What kind of flight information are you
interested in?
User Please list all the flights from Chicago to Kansas
city on June seventeenth. (Class A query)
System Goal Inference: Flight_ID (Concepts pass the
domain constraints)
User For this flight how much would a first class
fare cost.  (Class D)
System Goal Inference: Fare_ID. (The missing concepts
<CITY_NAME1> <CITY_NAME2> are automatically
retrieved from the discourse context.)
Table 5. Aposteriori probabilities obtained from backward
inferencing using double threshold scheme for the Class D
query ?For this flight how much would a first class fare cost.?
in Table 4. It indicates that the cities of origin and destination
are missing.
Conceptj (Cj)
(Part of concepts)
P(Cj ) Decision
for Cj
Actual
Occurrence
for Cj
AIRPORT_NAME 0.0000 absent absent
CITY_NAME1 0.9629 present absent
CITY_NAME2 0.9629 present absent
CLASS_NAME 0.2716 optional present
FARE 0.8765 present present
We inherit discourse context for all the Class D queries.
Based on the training data, we have designed a few context
refresh rules to ?undo? context inheritance for several query
types. For example, if the goal of the Class D query is
<Airline_Code>, it is obviously asking about an airline, hence
the concept <AIRLINE_NAME> will not be inherited.
5. PORTABILITY OF A BN-BASED
DIALOG MODEL
In addition to scalability, this work conducts a preliminary
examination of the portability of our BN-based dialog models
across different application domains.   Migration to a new
application often implies the lack of domain-specific data to
train our BN probabilities.  At this stage, BN probabilities can
be hand-assigned to reflect the "degree of belief" of the
knowledge domain expert.
5.1 General Principles for Probability
Assignment
For each informational goal, we have to identify the concepts
that are related to the goal. For example, the informational goal
Ground_Transportation is usually associated with the key
concepts of <AIRPORT_NAME> <CITY_NAME> and
<TRANSPORT_TYPE>. After the identification of all concepts for
the 11 goals, 23 key concepts (more details below) are extracted
from the total 60 concepts. Each of the 11 handcrafted BNs
hence receives as input of the identical set of 23 concepts.
13 semantic concepts (out of 23) (e.g. <CITY_NAME>,
<AIRPORT_NAME>, <AIRLINE_NAME>) correspond to the SQL
attributes for database access, while the reminding 10
correspond to syntactic/semantic concepts (e.g. <AIRCRAFT>,
<FARE>, <FROM>). For the sake of simplicity, we assumed
independence among concepts in the BN (pre-defined topology),
and we then hand-assigned the four probabilities for each of the
11 BNs, namely P(Cj=1|Gi=1), P(Cj=0|Gi=1), P(Cj=1|Gi=0),
P(Cj=0|Gi=0). We avoid assigning the probabilities of 1 or 0
since they are not supportive of probabilistic inference.  In the
following we describe the general principles for assigning
P(Cj=1|Gi=1) and P(Cj=1|Gi=0). The remaining P(Cj=0|Gi=1)
and P(Cj=0|Gi=0) can be derived by the complement of the
former two probabilities.
5.1.1 Probability Assignment for P(Cj=1|Gi=1)
We assign the probabilities of P(Cj=1|Gi=1) based on the
occurrence of the concept Cj with the corresponding Gi query as
shown in Table 6.
Case 1. Cj must occur given Gi
If we identify a concept that is mandatory for a query of goal Gi,
we will hand-assign a high probability  (0.95-0.99) for
P(Cj=1|Gi=1). For example, concept <FARE> (for words e.g.
fare, price, etc.) must occur in Fare_ID query.  (?what is the
first class fare from detroit to las vegas? and ?show me the first
class and coach price").
Case 2. Cj often occurs given Gi
If the concept often occurs with the Gi query, then we will lower
the probabilities of P(Cj=1|Gi=1) to the range of 0.7-0.8. For
example, the Fare_ID query often comes with the concepts of
<CITY_ORIGIN> and <CITY_DESTINATION>.
Case 3. Cj may occur given Gi
This applies to the concepts that act as additional constraints for
database access. Examples are <TIME_VALUE>, <DAY_NAME>,
<PERIOD>specified in the user query.
Case 4. Cj seldom occurs given Gi
The occurrence of this kind of concepts in the user query is
infrequent. Example includes the concept <STOPS> which
specify the nonstop flight for the Fare_ID query.
Case 5. Cj never occurs given Gi
This kind of concepts usually provides negative evidence for
goal inference. Examples include the concept
<FLIGHT_NUMBER> in the Flight_ID query. The presence of
<FLIGHT_NUMBER> in the input query implies that the goal
Flight_ID  is unlikely, because the aposteriori probability for
the BN Flight_ID is lowered.
Table 6. Conditions for assigning the probabilities
P(Cj=1|Gi=1).
Condition Probability of P(Cj=1|Gi=1)
1. Cj must occur given GI 0.95 ? 0.99
2. Cj often occur given Gi 0.7 ? 0.8
3. Cj may occur given GI 0.4 ? 0.6
4. Cj seldom occur given Gi 0.2 ? 0.3
5. Cj must not occur given Gi 0.01 ? 0.1
5.1.2 Probability Assignment for P(Cj=1|Gi=0)
For assignment the probabilities of P(Cj=1|Gi=0) for BNi, we
have to consider the occurrence of  the concepts for goals other
than Gi, i.e. for goal Gm (where m ranges between 1 and 11  but
is not equal to i). The scheme for assigning P(Cj=1|Gi=0), i.e.
probability of concept Cj being present while goal Gi is absent,
is shown in Table 7.
Case 1. Cj always occurs for goals other than Gi
Consider the relationship between the concept <CITY> and the
goal Aircraft_Code. Since <CITY> always occur for other
informational goals, (e.g. Flight_ID, Fare_ID, etc.), we assign
P(C<CITY>=1|G<Aircraft_Code>=0) in the range of 0.7-0.9.
Case 2. Cj sometimes occurs for goals other than Gi
Consider the relationship between the concept <CLASS> and the
goal Aircraft_Code. Since <CLASS> sometimes occurs in the
informational goals other than Aircraft_Code, and acts as the
additional constraints for database access, we assign
P(C<CLASS>=1|G<Aircraft_Code>=0) in the range of 0.2-0.5.
Case 3. Cj seldom occurs for goals other than Gi
This applies to the concepts that are strongly dependent on a
specific goal and hence seldom appear for other goals. For
example, the concept <TRANSPORTATION> usually accompanies
the goal Ground_Transportation only. Hence
P(C<TRANSPORTATION>=1|G<Ground_Transportation>=0)
is set closed to 0.
Table 7 Conditions for assigning the probabilities P(Cj=1|Gi=0)
Condition Probability of
P(Cj=1|Gi=0)
1. Cj always occurs for goals other than Gi 0.7 ? 0.9
2. Cj sometimes occurs for goals other than Gi 0.2 ? 0.5
3. Cj seldom occurs for goals other than Gi 0.01 ? 0.1
5.2 Evaluation
BNs with hand-assigned probabilities achieved a goal
identification accuracy of 80.9% for the ATIS-3 1993 test set
(Class A and D sentences included). This compares to 84.6%
when they have been automatically trained on the training data.
The availability of training data for the BNs enhances
performance in goal identification. Queries whose goals are not
covered by our 11 BNs are treated as OOD, and are considered
to be identified correctly if there are classified as such.
We have compared the handcrafted probabilities with the
trained probabilities based on natural language understanding,
where the evaluation metric is the sentence error rate. A
sentence is considered correct only if the inferred goal and
extracted concepts in the generated semantic frame agrees with
those in the reference semantic frame (derived from the SQL in
the ATIS corpora). The goal identification accuracies and the
sentence error rates for the ATIS-3 1993 test set are
summarized in Table 8. When we compare the our results with
the NL understanding results from the 10 ATIS evaluation sites
shown in Table 9, our performance falls within a reasonable
range.
Table 8 Goal identification accuracies and the sentence error
rates of Class A and D queries of ATIS test 93 data for the
handcrafted probabilities and automatically trained probabilities
respectively.
Class
BNs
(handcrafted
probabilities)
BNs
(trained
probabilities)
A (448) 90.18% 91.74%
D (325) 68.31% 74.78%Goal IDAccuracy A+D  80.98%  84.61%
A (448) 12.05% 9.15%
D (325) 40.92% 33.85%
Sentence
Error
Rate A+D 24.19% 19.53%
Table 9 Benchmark NL  results from the  10 ATIS evaluation
sites [6].
Class Sentence Error Rate
A (448) 6.0 ? 28.6%
D (325) 13.8 ? 63.1%
A+D (773) 9.3 ? 43.1%
We observed that our strategy for context inheritance may
be too aggressive, which leads to concept insertion errors in the
generated semantic frame.  This is illustrated in the example in
Table 10.
Table 10 The case frame for query 3 indicates our context
inheritance strategy may be too aggressive which leads to a
concept insertion error in the generated semantic frame.
Query 1: List flights from oakland to salt lake city before
six a m Thursday morning
(Our system generates a correct semantic frame.)
Query 2: List delta flights before six a m (Class D)
(Our system generates a correct semantic frame.)
Query 3: List all flights from twelve oh one a m until six a
m (Class D)
(Our system detects missing concepts of
<CITY_NAME>, which are inherited from
discourse)
Case Frame SQL Reference
Goal: Flight_ID Flight_ID
CITY_NAME = oakland CITY_NAME = oaklandConcepts:
CITY_NAME = salt lake
city
DEPARTURE_TIME =
twelve oh one a m until
six a m
AIRLINE_NAME = delta (a
concept insertion error)
CITY_NAME = salt lake city
DEPARTURE_TIME = >=1
&& <= 600
6. SUMMARY AND CONCLUSIONS
This paper describes the scalability and portability of the BN-
based dialog model as we migrate from the foreign exchange
domain (CU FOREX) to the relatively more complex air travel
domain (ATIS).  The complexity of an application domain is
characterized by the number of in-domain informational goals
and concepts.  The presence / absence of concepts are used to
infer the presence/absence of each goal, by means of the BN.
When a large number of in-domain concepts are available, we
used an information-theoretic criterion (Information Gain) to
automatically select the small set of concepts most indicative of
a goal, and do so for every in-domain goal.   Automatic
detection of missing / spurious concepts is achieved by
backward inference using the BN corresponding to the inferred
goal.  This detection procedure drives our mixed-initiative
dialog model ? the system prompts the user for missing
concepts, and asks for clarification if spurious concepts are
detected.   For the simpler CU FOREX domain, detection of
missing / spurious concepts was based on a single probability
threshold.  However, scaling up to ATIS (which has many more
concepts) shows that some concepts need to be present, others
should be absent, but still others should be optional.  Hence we
need to use two levels of thresholding to decide if a concept
should be present, optional or absent in the query.  We have
also enhanced our BN-based dialog model with the capability
of context-inheritance, in order to handle the context-dependent
user queries in the ATIS domain.  Discourse context is inherited
for the Class D queries, and we invoke goal inference again
after context inheritance if a query was previously classified as
OOD.
As regards portability, migration to a new application
domain often implies the lack of domain-specific training data.
Hence we have proposed a set of general principles for
probability assignment to the BNs, as a reflection of our
?degree of belief? in the relationships between concepts and
goals.   We compared the goal identification performance, as
well as concept error rates between the use of hand-assigned
probabilities, and the probabilities trained from the ATIS
training set.  Results show that the hand-assigned probabilities
offer a decent starting performance to ease portability to a new
domain.  The system performance can be further improved if
data is available to train the probabilities.
7. REFERENCES
[1] Meng, H., W. Lam and C. Wai, ?To Believe is to
Understand,? Proceedings of Eurospeech, 1999.
[2] Meng, H., W. Lam and K. F. Low, ?Learning Belief
Networks for Language Understanding,?  Proceedings of
ASRU, 1999.
[3] Meng, H., S. Lee and C. Wai, ?CU FOREX:  A Bilingual
Spoken Dialog System for the Foreign Exchange Domain,?
Proceedings of ICASSP, 2000.
[4] Meng, H., C. Wai, R. Pieraccini, ?The Use of Belief
Networks for Mixed-Initiative Dialog Modeling,?
Proceeding of ICSLP, 2000.
[5] Kuhn, R., and R. De Mori, ?The Application of Semantic
Classification Trees for Natural Language Understanding,?
IEEE Trans. PAMI, Vol. 17, No. 5, pp. 449-460, May
1995.
[6] Pallet, D., J. Fiscus, W. Fisher, J. Garofolo, B. Lund, and
M. Przybocki, ?1993 Benchmark Tests for the ARPA
Spoken Language Program,? Proceedings of the Spoken
Language Technology Workshop, 1994.
[7] Levin, E., Pieraccini, R., and Eckert, W., ?A Stochastic
Model of Human-Machine Interaction for Learning
Dialogue Strategies?, Speech and Audio Processing, IEEE
Transactions, Vol 8, pp. 11-23, Jan 2000.
[8] Walker, M., Fromer, J., Narayanan, S., ?Learning Optimal
Dialogue Strategies: A Case Study of a Spoken Dialogue
Agent for Email?, in Proceedings of ACL/COLING 98 ,
1998.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 604?611, Vancouver, October 2005. c?2005 Association for Computational Linguistics
  
The Use of Metadata, Web-derived Answer Patterns and Passage 
Context to Improve Reading Comprehension Performance
Yongping Du 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
ypdu@fudan.edu.cn 
 
 
Helen Meng 
Human-Computer 
Communication Laboratory 
The Chinese University of 
Hong Kong 
 HongKong. SAR. China 
hmmeng@se.cuhk.edu.hk 
 
Xuanjing Huang 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
xjhuang@fudan.edu.cn 
 
 
Lide Wu 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
ldwu@fudan.edu.cn 
Abstract 
A reading comprehension (RC) system 
attempts to understand a document and returns 
an answer sentence when posed with a 
question.  RC resembles the ad hoc question 
answering (QA) task that aims to extract an 
answer from a collection of documents when 
posed with a question.  However, since RC 
focuses only on a single document, the system 
needs to draw upon external knowledge 
sources to achieve deep analysis of passage 
sentences for answer sentence extraction.  
This paper proposes an approach towards RC 
that attempts to utilize external knowledge to 
improve performance beyond the baseline set 
by the bag-of-words (BOW) approach.  Our 
approach emphasizes matching of metadata 
(i.e. verbs, named entities and base noun 
phrases) in passage context utilization and 
answer sentence extraction. We have also 
devised an automatic acquisition process for 
Web-derived answer patterns (AP) which 
utilizes question-answer pairs from TREC QA, 
the Google search engine and the Web.  This 
approach gave improved RC performances for 
both the Remedia and ChungHwa corpora, 
attaining HumSent accuracies of 42% and 
69% respectively.  In particular, performance 
analysis based on Remedia shows that relative 
performances of 20.7% is due to metadata 
matching and a further 10.9% is due to the 
application of Web-derived answer patterns. 
1. Introduction 
A reading comprehension (RC) system attempts to 
understand a document and returns an answer 
sentence when posed with a question.  The RC 
task was first proposed by the MITRE 
Corporation which developed the Deep Read 
reading comprehension system (Hirschman et al, 
1999).  Deep Read was evaluated on the Remedia 
Corpus that contains a set of stories, each with an 
average of 20 sentences and five questions (of 
types who, where, when, what and why). The 
MITRE group also defined the HumSent scoring 
metric, i.e. the percentage of test questions for 
which the system has chosen a correct sentence as 
the answer.  HumSent answers were compiled by a 
human annotator, who examined the stories and 
chose the sentence(s) that best answered the 
questions.  It was judged that for 11% of the 
Remedia test questions, there is no single sentence 
in the story that is judged to be an appropriate 
answer sentence.  Hence the upper bound for RC 
on Remedia should by 89% HumSent accuracy.  
(Hirschman et al 1999) reported a HumSent 
accuracy of 36.6% on the Remedia test set.  
Subsequently, (Ng et al, 2000) used a machine 
learning approach of decision tree and achieved 
the accuracy of 39.3%.   Then (Riloff and Thelen, 
2000) and (Charniak et al, 2000) reported 
improvements to 39.7% and 41%, respectively.  
They made use of handcrafted heuristics such as 
the WHEN rule: 
if contain(S, TIME), then Score(S)+=4 
i.e. WHEN questions reward candidate answer 
sentences with four extra points if they contain a 
name entity TIME.  
RC resembles the ad hoc question answering 
(QA) task in TREC.1  The QA task finds answers 
to a set of questions from a collection of 
documents, while RC focuses on a single 
                                                                 
1 http://www.nist.gov. 
604
  
document.  (Light et al 1998) conducted a 
detailed compared between the two tasks.  They 
found that the answers of most questions in the 
TREC QA task appear more than once within the 
document collection.  However, over 80% of the 
questions in the Remedia corpus correspond to 
answer sentences that have a single occurrence 
only.  Therefore an RC system often has only one 
shot at finding the answer. The system is in dire 
need of extensive knowledge sources to help with 
deep text analysis in order to find the correct 
answer sentence.   
Recently, many QA systems have exploited 
the Web as a gigantic data repository in order to 
help question answering (Clarke et al, 2001; 
Kwok et al, 2001; Radev et al, 2002).  Our 
current work attempts to incorporate a similar idea 
in exploiting Web-derived knowledge to aid RC.  
In particular, we have devised an automatic 
acquisition process for Web-derived answer 
patterns. Additionally we propose to emphasize 
the importance of metadata matching in our 
approach to RC.  By metadata, we are referring to 
automatically labeled verbs, named entities as well 
as base noun phrases in the passage.  It is 
important to achieve a metadata match between 
the question and a candidate answer sentence 
before the candidate is selected as the final answer.  
The candidate answer sentence may be one with a 
high degree of word overlap with the posed 
question, or it may come from other sentences in 
the neighboring context. We apply these different 
techniques step by step and obtain better results 
than have ever previously been reported. 
Especially, we give experiment analysis for 
understanding the results. 
    In the rest of this paper, we will first describe 
three main aspects of our approach towards RC ? 
(i) metadata matching, (ii)automatic acquisition of 
Web-derived answer patterns and (iii) the use of 
passage context.  This will be followed by a 
description of our experiments, analysis of results 
and conclusions. 
2. Metadata Matching 
A popular approach in reading comprehension is 
to represent the information content of each 
question or passage sentence as a bag of words 
(BOW).  This approach incorporates stopword 
removal and stemming.  Thereafter, two words are 
considered a match if they share the same 
morphological root.  Given a question, the BOW 
approach selects the passage sentence with the 
maximum number of matching words as the 
answer.  However, the BOW approach does not 
capture the fact that the informativeness of a word 
about a passage sentence varies from one word to 
another.  For example, it has been pointed out by 
(Charniak et al 2000) that the verb seems to be 
especially important for recognizing that a passage 
sentence is related to a specific question.  In view 
of this, we propose a representation for questions 
and answer sentences that emphasizes three types 
of metadata:  
(i) Main Verbs (MVerb), identified by the link 
parser (Sleator and Temperley 1993);  
(ii) Named Entities (NE), including names of 
locations (LCN), persons (PRN) and organizations 
(ORG), identified by a home-grown named entity 
identification tool; and  
(iii) Base Noun Phrases (BNP), identified by a 
home-grown base noun phrase parser respectively. 
We attempt to quantify the relative importance 
of such metadata through corpus statistics 
obtained only from the training set of the Remedia 
corpus, which has 55 stories. The Remedia test set, 
which contains 60 stories, is set aside for 
evaluation. On average, each training story has 20 
sentences and five questions. There are 274 
questions in all in the entire training set.  Each 
question corresponds to a marked answer sentence 
within the story text.  We analyzed all the 
questions and divided them into three question 
sets (Q_SETS) based on the occurrences of 
MVerb, NE and BNP identified with the tools 
mentioned above.  The following are illustrative 
examples of the Q_SETS as well as their sizes: 
Q_SETMverb  
(Count:169) 
Who helped the Pilgrims? 
Q_SETNE    
 (Count:62) 
When was the first merry-go-
round built in the United States? 
Q_SETBNP   
(Count:232) 
Where are the northern lights? 
Table 1.  Examples and sizes of question sets (Q_SETS) 
with different metadata ? main  verb (MVerb), named 
entity (NE) and base noun phrase (BNP). 
   It may also occur that a question belongs to 
multiple Q_SETS.  For example:  
605
  
Q_SETMVerb 
 
When was the first merry-go-round built 
in the United States? 
Q_SETNE 
 
When was the first merry-go-round built 
in the United States? 
Q_SETBNP 
 
When was the first merry-go-round built 
in the United States? 
Table 2.  An example sentence that belongs to multiple 
Q_SETS. 
As mentioned earlier, each question 
corresponds to an answer sentence, which is 
annotated in the story text by MITRE.  Hence we 
can follow the Q_SETS to divide the answer 
sentences into three answer sets (A_SETS).  
Examples of A_SETS that correspond to Table 1 
include: 
A_SETMVerb 
 
An Indian named Squanto came 
to help them. 
A_SETNE 
 
The first merry-go-round in the 
United States was built in 1799.
A_SETBNP 
 
Then these specks reach the air 
high above the earth. 
Table 3.  Examples of the answer sets (A_SETS) 
corresponding to the different metadata categories, 
namely, main verb (MVerb), named entity (NE) and 
base noun phrase) (BNP). 
    In order to quantify the relative importance of 
matching the three kinds of metadata between 
Q_SET and A_SET for reading comprehension, 
we compute the following relative weights based 
on corpus statistics: 
|_|
||
Metadata
Metadata
Metadata SETA
SWeight =  ?..Eqn (1) 
where SMetadata is the set of answer sentences in 
|A_SETMetadata| that contain the metadata of its 
corresponding question.  For example, referring to 
Tables 2 and 3, the question in Q_SETNE ?When 
was the first merry-go-round built in the United 
Sates?? contains the named entity (underlined) 
which is also found in the associated answer 
sentence from A_SETNE, ?The first merry-go-
round in the United States was built in 1799.?  
Hence this answer sentence belongs to the set SNE.   
Contrarily, the question in Q_SETBNP ?Where are 
the northern lights?? contains the base noun 
phrase (underlined) but it is not found in the 
associated answer sentence from A_SETBNP, 
?Then these specks reach the air high above the 
earth.?  Hence this answer sentence does not 
belong to the set SBNP.  Based on the three sets, we 
obtain the metadata weights: 
WeightMVerb=0.64, WeightNE=0.38, WeightBNP=0.21 
To illustrate how these metadata weights are 
utilized in the RC task, consider again the 
question, ?Who helped the Pilgrims?? together 
with three candidate answers that are ?equally 
good? with a single word match when the BOW 
approach is applied.  We further search for 
matching metadata among these candidate 
answers and use the metadata weights for scoring.   
Question Who helped the Pilgrims? 
MVerb identified: ?help? 
BNP identified: ?the Pilgrams? 
Candidate 
Sentence 1 
 
An Indian named Squanto came to help. 
Matched MVerb (underlined) 
Score= WeightMVerb=0.64 
Candidate 
Sentence 2 
 
By fall, the Pilgrims had enough food for 
the winter. 
Matched BNP (underlined) 
Score= WeightBNP=0.21 
Candidate 
Sentence 3 
 
Then the Pilgrims and the Indians ate and 
played games. 
Matched BNP (underlined) 
Score= WeightBNP=0.21 
Table 4.  The use of metadata matching to extend the 
bag-of-words approach in reading comprehension.  
3. Web-derived Answer Patterns 
In addition to using metadata for RC, the proposed 
approach also leverages knowledge sources that 
are external to the core RC resources ? primarily 
the Web and other available corpora.  This section 
describes our approach that attempts to 
automatically derive answer patterns from the 
Web as well as score useful answer patterns to aid 
RC.  We utilize the open domain question-answer 
pairs (2393 in all) from the Question Answering 
track of TREC (TREC8-TREC12) as a basis for 
automatic answer pattern acquisition.   
3.1 Deriving Question Patterns 
We define a set of question tags (Q_TAGS) that 
extend the metadata above in order to represent 
question patterns.  The tags include one for main 
verbs (Q_MVerb), three for named entities 
(Q_LCN, Q_PRN and Q_ORG) and one for base 
noun phrases (Q_BNP). We are also careful to 
ensure that noun phrases tagged as named entities 
are not further tagged as base noun phrases. 
606
  
    A question pattern is expressed in terms of 
Q_TAGS.  A question pattern can be used to 
represent multiple questions in the TREC QA 
resource.  An example is shown in Table 5.  
Tagging the TREC QA resource provides us with 
a set of question patterns {QPi} and for each 
pattern, up to mi example questions. 
Question Pattern (QPi): 
When do Q_PRN Q_MVerb Q_BNP? 
Represented questions: 
Q1: When did Alexander Graham Bell invent the 
telephone? 
Q2: When did Maytag make Magic Chef 
refrigerators? 
Q3: When did Amumdsen reach the South Pole? 
(mi example questions in all) 
Table 5.  A question pattern and some example 
questions that it represents. 
3.2  Deriving Answer Patterns 
For each question pattern, we aim to derive 
answer patterns for it automatically from the Web. 
The set of answer patterns capture possible ways 
of embedding a specific answer in an answer 
sentence.  We will describe the algorithm for 
deriving answer patterns as following and 
illustrate with the following question answer pair 
from TREC QA:  
Q: When did Alexander Graham Bell invent the 
telephone? 
A: 1876 
1. Formulate the Web Query 
The question is tagged and the Web query is 
formulated as ?Q_TAG?+ ?ANSWER?, i.e. 
Question: ?When did Alexander Graham Bell 
invent the telephone?? 
QP:            When do Q_PRN Q_MVerb Q_BNP ? 
where Q_PRN= ?Alexander Graham Bell?, 
Q_MVerb= ?invent?, and  Q_BNP=  ?the 
telephone? 
hence Web query:  ?Alexander Graham Bell?+ 
?invent? + ?the telephone? + ?1876? 
2. Web Search and Snippet Selection 
The Web query is submitted to the search 
engine Google using the GoogleAPI and the top 
100 snippets are downloaded.  From each 
snippet, we select up to ten contiguous words to 
the left as well as to the right of the ?ANSWER? 
for answer pattern extraction.  The selected 
words must be continuous and do not cross the 
snippet boundary that Google denotes with ???. 
3. Answer Pattern Selection 
We label the terms in each selected snippet with 
the Q_TAGs from the question as well as the 
answer tag <A>.  The shortest string containing 
all these tags (underlined below) is extracted as 
the answer pattern (AP).  For example:  
Snippet 1: 1876, Alexander Graham Bell 
invented the telephone in the United States? 
AP 1:   <A>, Q_PRN Q_MVerb Q_BNP. 
(N.B.  The answer tag <A> denotes ?1876? in this 
example). 
Snippet 2: ?which has been invented by 
Alexander Graham Bell in 1876? 
AP 2:    Q_MVerb by Q_PRN in <A>. 
    As may be seen in above, the acquisition 
algorithm for Web-derived answer questions calls 
for specific answers, such as a factoid in a word or 
phrase.  Hence the question-answer pairs from 
TREC QA are suitable for use.  On the other hand, 
Remedia is less suitable here because it contains 
labelled answer sentences instead of factoids.  
Inclusion of whole answer sentences in Web 
query formulation generally does not return the 
answer pattern that we seek in this work. 
3.3 Scoring the Acquired Answer Patterns 
The answer pattern acquisition algorithm returns 
multiple answer patterns for every question-
answer pair submitted to the Web.   In this 
subsection we present an algorithm for deriving 
scores for these answer patterns.  The 
methodology is motivated by the concept of 
confidence level, similar to that used in data 
mining.  The algorithm is as follows: 
1. Formulate the Web Query 
For each question pattern QPi (see Table 5) 
obtained previously, randomly select an example 
question among the mi options that belongs to this 
pattern.  The question is tagged and the Web 
query is formulated in terms of the Q_TAGs only.  
(Please note that the corresponding answer is 
excluded from Web query formulation here, 
which differs from the answer pattern acquisition 
algorithm).  E.g., 
Question: ?When did Alexander Graham Bell 
invent the telephone? 
Q_TAGs: Q_PRN Q_MVerb Q_BNP 
Web query:  ?Alexander Graham Bell?+ 
?invent? + ?the telephone? 
2.   Web Search and Snippet Selection 
The Web query is submitted to the search engine 
607
  
Google and the top 100 snippets are downloaded. 
3.   Scoring each Answer Pattern APij relating to 
QPi 
Based on the question, its pattern QPi, the answer 
and the retrieved snippets, totally the following 
counts for each answer pattern APij relating to 
QPi . 
cij ? # snippets matching APij and for which the 
tag <A> matches the correct answer. 
nij ? #  snippets matching APij and for which the 
tag <A> matches any term 
Compute the ratio rij= cij / nij..........Eqn(2) 
Repeat steps 1-3 above for another example 
question randomly selected from the pool of mi 
example under QPi.  We arbitrarily set the 
maximum number of iterations to be ki = ??
???
?
im3
2  
in order to achieve decent coverage of the 
available examples.  The confidence for APij.is 
computed as          
k
r
APConfidence
k
i
ij
ij
?
== 1)( ??Eqn(3) 
Equation (3) tries to assign high confidence 
values to answer patterns APij that choose the 
correct answers, while other answer patterns are 
assigned low confidence values.  E.g.: 
<A>, Q_PRN Q_MVerb Q_BNP     (Confidence=0.8) 
Q_MVerb by Q_PRN in <A>.         (Confidence=0.76) 
3.4 Answer Pattern Matching in RC 
The Web-derived answer patterns are used in the 
RC task.  Based on the question and its QP, we 
select the related AP to match among the answer 
sentence candidates.  The candidate that matches 
the highest-scoring AP will be selected.  We find 
that this technique is very effective for RC as it 
can discriminate among candidate answer 
sentences that are rated ?equally good? by the 
BOW or metadata matching approaches, e.g.: 
Q:   When is the Chinese New Year? 
QP: When is the Q_BNP? 
where Q_BNP=Chinese New Year 
Related AP:  Q_BNP is <A> (Confidence=0.82) 
Candidate answer sentences 1: you must wait a few more 
weeks for the Chinese New Year. 
Candidate answer sentences 2: Chinese New Year is most 
often between January 20 and February 20. 
Both candidate answer sentences have the same 
number of matching terms ? ?Chinese?, ?New? 
and ?Year? and the same metadata, i.e. 
Q_BNP=Chinese New Year. The term ?is? is 
excluded by stopword removal. However the 
Web-derived answer pattern is able to select the 
second candidate as the correct answer sentence. 
Hence our system gives high priority to the 
Web-derived AP ? if a candidate answer sentence 
can match an answer pattern with confidence > 
0.6, the candidate is taken as the final answer.  No 
further knowledge constraints will be enforced. 
4. Context Assistance 
During RC, the initial application of the BOW 
approach focuses the system?s attention on a small 
set of answer sentence candidates.  However, it 
may occur the true answer sentence is not 
contained in this set.  As was observed by (Riloff 
and Thelen, 2000) and (Charniak et al, 2000), the 
correct answer sentence often precedes/follows the 
sentence with the highest number of matching 
words.  Hence both the preceding and following 
context sentences are searched in their work to 
find the answer sentence especially for why 
questions. 
Our proposed approach references this idea in 
leveraging contextual knowledge for RC.  
Incorporation of contextual knowledge is very 
effective when used in conjunction with named 
entity (NE) identification.  For instance, who 
questions should be answered with words tagged 
with Q_PRN (for persons).  If the candidate 
sentence with the highest number of matching 
words does not contain the appropriate NE, it will 
not be selected as the answer sentence.  Instead, 
our system searches among the two preceding and 
two following context sentences for the 
appropriate NE.  Table 6 offers an illustration. 
Data analysis Remedia training set shows that the 
context window size selected is appropriate for 
when, who and where questions.   
Football Catches On Fast 
(LATROBE, PA., September 4, 1895) - The new 
game of football is catching on fast, and each month new 
teams are being formed. 
Last night was the first time that a football player was 
paid.  The man's name is John Brallier, and he was paid 
$10 to take the place of someone who was hurt.? 
Question: Who was the first football player to be paid? 
Sentence with maximum # matching words: Last night 
was the first time that a football player was paid. 
Correct answer sentence: The man's name is John 
Brallier, and he was paid $10 to take the place of 
someone who was hurt. 
Table 6.  An example illustrating the use of contextual 
knowledge in RC. 
608
  
As for why questions, a candidate answer 
sentence is selected from the context window if its 
first word is one of ?this?, ?that?, ?these?, 
?those?, ?so? or ?because?.  We did not utilize 
contextual constraints for what questions. 
5. Experiments 
RC experiments are run on the Remedia corpus as 
well as the ChungHwa corpus.  The Remedia 
training set has 55 stories, each with about five 
questions.  The Remedia test set has 60 stories and 
5 questions per story.  The ChungHwa corpus is 
derived from the book, ?English Reading 
Comprehension in 100 days,? published by 
Chung Hwa Book Co., (H.K.) Ltd.  The 
ChungHwa training set includes 100 English 
stories and each has four questions on average.  
The ChungHwa testing set includes 50 stories and 
their questions.  We use HumSent as the prime 
evaluation metric for reading comprehension.   
The three kinds of knowledge sources are used 
incrementally in our experimental setup and 
results are labeled as follows: 
Result Technique 
Result_1 BOW 
Result_2 BOW+MD 
Result_3 BOW+MD+AP 
Result_4 BOW+MD+AP+Context 
Table 7.  Experimental setup in RC evaluations.  
Abbrievations are: bag-of-words (BOW), metadata 
(MD), Web-derived answer patterns (AP), contextual 
knowledge (Context). 
5.1 Results on Remedia 
Table 8 shows the RC results for various question 
types in the Remedia test set.  
 When Who What Where Why 
Result_1 32.0% 30.0% 31.8% 29.6% 18.6%
Result_2 40.0% 28.0% 39.0% 38.0% 20.0%
Result_3 52.6% 42.8% 40.6% 38.4% 21.0%
Result_4 55.0% 48.0% 40.6% 36.4% 27.6%
 Table 8.  HumSent accuracies for the Remedia test set. 
We observe that the HumSent accuracies vary 
substantially across different interrogatives. The 
system performs best for when questions and 
worst for why questions. The use of Web-derived 
answer patterns brought improvements to all the 
different interrogatives.  The other knowledge 
sources, namely, meta data and context, bring 
improvements for some question types but 
degraded others.  
Figure 1 shows the overall RC results of our 
system.  The relative incremental gains due to the 
use of metadata, Web-derived answer patterns and 
context are 20.7%, 10.9% and 8.2% respectively.  
We also ran pairwise t-tests to test the statistical 
significance of these improvements and results are 
shown in Table 9.  The improvements due to 
metadata matching and Web-derived answer 
patterns are statistically significant (p<0.05) but 
the improvement due to context is not. 
29%
35%
38.80% 42%
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
Result_1 Result_2 Result_3 Result_4
H
um
Se
nt
 P
re
ci
sio
n
 
Figure 1.  HumSent accuracies for Remedia. 
Pairwise 
Comparison 
Result_1 & 
Result_2 
Result_2 & 
Result_3 
Result_3 & 
Result_4 
t-test Results t(4)=2.207, 
p=0.046 
t(4)=2.168, 
p=0.048 
t(4)=1.5, 
p=0.104 
Table 9.  Tests of statistical significance in the 
incremental improvements over BOW among the use 
of metadata, Web-derived answer patterns and context.   
We also compared our results across various 
interrogatives with those previously reported in 
(Riloff and Thelen, 2000).  Their system is based 
on handcrafted rules with deterministic algorithms.  
The comparison (see Table 10) shows that our 
approach which is based on data-driven patterns 
and statistics can achieve comparable performance. 
Question Type Riloff &Thelen 2000 Result_4 
When 55% 55.0% 
Who 41% 48.0% 
What 28% 40.6% 
Where 47% 36.4% 
Why 28% 27.6% 
Overall 40% 42.0% 
Table 10.  Comparison of HumSent results with a 
heuristic based RC system (Riloff & Thelen 00).  
5.2 Results on ChungHwa 
Experimental results for the ChungHwa corpus are 
presented in Figure 2.  The HumSent accuracies 
obtained are generally higher than those with 
609
  
Remedia.  We observe similar trends as before, i.e. 
our approach in the use of metadata, Web-derived 
answer patterns and context bring incremental 
gains to RC performance.  However, the actual 
gain levels are much reduced. 
65%
66%
68%
69%
63%
64%
65%
66%
67%
68%
69%
70%
Result_1 Result_2 Result_3 Result_4
H
um
Se
nt
 P
re
ci
sio
n
 
Figure 2.  HumSent accuracies for ChungHwa. 
5.3. Analyses of Results 
In order to understand the underlying reason for 
reduced performance gains as we migrated from 
Remedia to Chunghwa, we analyzed the question 
lengths as well as the degree of word match 
between questions and answers among the two 
corpora.  Figure 3 shows that the average length 
of questions in Chunghwa are longer than 
Remedia.  Longer questions contain more 
information which is beneficial to the BOW 
approach in finding the correct answer. 
32.6
7.5
32.5
60
13.3
54.1
0
10
20
30
40
50
60
70
?4 5,6,7 ?8
Question Length
Pe
rc
en
t o
f Q
ue
st
io
ns
 (%
)
Remedia ChungHwa
 
Figure 3.  Distribution of question lengths among the 
Remedia and ChungHwa corpora. 
The degree of word match between questions 
and answers among the two corpora is depicted in 
Figure 4.  We observe that ChungHwa has a larger 
proportion of questions that have a match- size (i.e. 
number of matching words between a question 
and its answer) larger than 2.  This presents an 
advantage for the BOW approach in RC.  It is also 
observed that approximately 10% of the Remedia 
questions have no correct answers (i.e. match-
size=-1) and about 25% have no matching words 
with the correct answer sentence.  This explains 
the overall discrepancies in HumSent accuracies 
between Remedia and ChungHwa. 
0
5
10
15
20
25
30
35
-1 0 1 2 3 4 5 ?6
Match Size
Pe
rc
en
t o
f Q
ue
st
io
ns
 (%
)
Remedia ChungHwa
 
Figure 4.  Distribution of match-sizes (i.e. the number 
of matching words between questions and their 
answers) in the two corpora. 
While our approach has leveraged a variety of 
knowledge sources in RC, we still observe that 
our system is unable to correctly answer 58% of 
the questions in Remedia. An example of such 
elusive questions is:  
Question: When do the French celebrate their 
freedom? 
Answer Sentence: To the French, July 14 has the 
same meaning as July 4th does to the United 
States.  
6. Conclusions 
A reading comprehension (RC) system aims to 
understand a single document (i.e. story or passage) 
in order to be able to automatically answer questions 
about it.   The task presents an information retrieval 
paradigm that differs significantly from that found in 
Web search engines.  RC resembles the question 
answering (QA) task in TREC which returns an 
answer for a given question from a collection of 
documents.  However, while a QA system can 
utilize the knowledge and information in a collection 
of documents, RC systems focuses only on a single 
document only.  Consequently there is a dire need to 
draw upon a variety of knowledge sources to aid 
deep analysis of the document for answer generation.  
This paper presents our initial effort in designing an 
approach for RC that leverages a variety of 
knowledge sources beyond the context of the 
passage, in an attempt to improve RC performance 
beyond the baseline set by the bag-of-words (BOW) 
approach.  The knowledge sources include the use of 
metadata (i.e. verbs, named entities and base noun 
phrases).  Metadata matching is applied in our 
approach in answer sentence extraction as well as 
use of contextual sentences.  We also devised an 
610
  
automatic acquisition algorithm for Web-derived 
answer patterns.  The acquisition process utilizes 
question-answer pairs from TREC QA, the Google 
search engine and the Web.  These answer patterns 
capture important structures for answer sentence 
extraction in RC.  The use of metadata matching and 
Web-derived answer patterns improved reading 
comprehension performances for the both Remedia 
and ChungHwa corpora. We obtain improvements 
over previously reported results for Remedia, with 
an overall HumSet accuracy of 42%.  In particular, a 
relative gain of 20.7% is due to metadata matching 
and a further 10.9% is due to application of Web-
derived answer patterns. 
Acknowledgement 
This work is partially supported by the Direct 
Grant from The Chinese University of Hong Kong 
(CUHK) and conducted while the first author was 
visiting CUHK. This work is supported by Natural 
Science Foundation of China under Grant 
No.60435020. 
References 
Charles L.A. Clarke, Gordon V. Cormack, Thomas R. 
Lynam. 2001. Exploiting Redundancy in Question 
Answering. In Proceedings of the 24th ACM 
Conference on Research and Development in 
Information Retrieval (SIGIR-2001, New Orleans, 
LA). ACM Press. New York, 358?365. 
Cody C. T. Kwok, Oren Etzioni, Daniel S. Weld. 2001. 
Scaling Question Answering to the Web. In 
Proceedings of the 10th World Wide Web 
Conference (WWW?2001). 150-161. 
Daniel Sleator and Davy Temperley. 1993. Parsing 
English with a Link Grammar. Third International 
Workshop on Parsing Technologies. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question 
Answering System. In Proceedings of the 40th 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2002). 41-47.  
Dell Zhang, Wee Sun Lee. 2002. Web Based Pattern 
Mining and Matching Approach to Question 
Answering. In Proceedings of the TREC-11 
Conference. 2002. NIST, Gaithersburg, MD, 505- 
512. 
Dragomir Radev, Weiguo Fan, Hong Qi, Harris Wu, 
Amardeep Grewal. 2002. Probabilistic Question 
Answering on the Web. In Proceedings of the 11th 
World Wide Web Conference (WWW?2002). 
Ellen Riloff and Michael Thelen. 2000. A Rule-based 
Question Answering System for Reading 
Comprehension Test. ANLP/NAACL-2000 
Workshop on Reading Comprehension Tests as 
Evaluation for Computer-Based Language 
Understanding Systems. 
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo 
Braz, Benjamin Garrett, Margaret Kosmala, Tomer 
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun, 
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa 
Zorn. 2000. Reading Comprehension Programs in a 
Statistical-Language-Processing Class. ANLP-
NAACL 2000 Workshop: Reading Comprehension 
Tests as Evaluation for Computer-Based Language 
Understanding Systems. 
Hwee Tou Ng, Leong Hwee Teo, Jennifer Lai Pheng 
Kwan. 2000. A Machine Learning Approach to 
Answering Questions for Reading Comprehension 
Tests.  Proceedings of the 2000 Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora 2000. 
Lynette Hirschman, Marc Light, Eric Breck, and John 
Burger. 1999. Deep Read: A Reading 
Comprehension System. Proceedings of the 37th 
Annual Meeting of the Association for 
Computational Linguistics. 
Marc Light, Gideon S. Mann, Ellen Riloff and Eric 
Break. 1998. Analyses for Elucidating Current 
Question Answering Technology.  Natural 
Language Engineering. Vol. 7, No. 4.  
Martin M. Soubbotin, Sergei M. Soubbotin. 2002. Use 
of Patterns for Detection of Likely Answer Strings: 
A Systematic Approach. In Proceedings of the 
TREC-11 Conference. 2002. NIST, Gaithersburg, 
MD, 134-143. 
611
Proceedings of NAACL HLT 2007, Companion Volume, pages 193?196,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combined Use of Speaker- and Tone-Normalized Pitch Reset with Pause
Duration for Automatic Story Segmentation in Mandarin Broadcast News
Lei Xie, Chuan Liu and Helen Meng
Human-Computer Communications Laboratory
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong, Hong Kong SAR of China
{lxie, cliu3, hmmeng}se.cuhk.edu.hk
Abstract
This paper investigates the combined use of
pause duration and pitch reset for automatic
story segmentation in Mandarin broadcast
news. Analysis shows that story boundaries
cannot be clearly discriminated from utterance
boundaries by speaker-normalized pitch reset
due to its large variations across different syl-
lable tone pairs. Instead, speaker- and tone-
normalized pitch reset can provide a clear sep-
aration between utterance and story bound-
aries. Experiments using decision trees for
story boundary detection reinforce that raw and
speaker-normalized pitch resets are not effec-
tive for Mandarin Chinese story segmentation.
Speaker- and tone-normalized pitch reset is a
good story boundary indicator. When it is com-
bined with pause duration, a high F-measure
of 86.7% is achieved. Analysis of the decision
tree uncovered four major heuristics that show
how speakers jointly utilize pause duration and
pitch reset to separate speech into stories.
1 Introduction
Pitch reset refers to the speaker?s general pitch declina-
tion through the course of a speech unit, followed by a re-
set to a high pitch at the start of next speech unit, as shown
in Figure 1(a). The speech unit may be of different lev-
els of granularity (Tseng et. al., 2005), such as a speech
segment that conveys a central topic (e.g. a news story), a
prosodic phrase group (PG) or an utterance. These units
are often separated by pauses. Pauses and pitch resets
were shown to be effective story boundary indicators in
English broadcast news segmentation (Shriberg et. al.,
2000; Tu?r et. al., 2001). These previous efforts specifi-
cally point out that pause durations are longer and pitch
resets are more pronounced at story boundaries, when
compared to utterance boundaries in English broadcast
news. However, such story segmentation approaches may
be different for a tonal language such as Mandarin Chi-
nese. The use of similar prosodic features for Chinese
news story segmentation deserves further investigation.
The main reason is that Chinese tonal syllables may com-
plicate the expressions of pitch resets. Chinese syllable
tones are expressed acoustically in pitch trajectories, i.e,
different tones show different pitch value ranges and tra-
jectory patterns,1 as shown in Figure 1(b). Initial work in
(Levow, 2004) has shown that Mandarin words at story
ending positions show a lower pitch as compared with
words at non-story-ending positions. In this paper, we
present a data-oriented study to investigate how the tonal-
ity of Mandarin syllables affects pitch resets at utterance
and story boundaries. To alleviate the effects from tonal-
ity, we propose to use speaker- and tone-normalized pitch
reset with pause duration to separate Mandarin broadcast
audio stream into distinct news stories.
F0 PitchReset
t
SpeechU nit SpeechU nit
(a)
5 5
3
5
2
1
4
5
1
5
Tone 1 (high) Tone 2 (rising) Tone 3 (low) Tone 4 (falling)
4
3
2
1(b)
Figure 1: (a) Pitch reset phenomenon between speech
units; (b) Pitch trajectories for the four Mandarin basic
syllable tones. The speaker pitch range is segmented to
five zones from high to low. The pitch trajectories of the
four tones are 5-5, 3-5, 2-1-4 and 5-1, respectively.
2 Task and Corpus
In a continuous audio stream of broadcast news, there are
programs that consist of speaker changes among anchors,
reporters and interviewees. Other programs may contain
a sequence of news stories reported by a single speaker.
We focus on the latter kind in this investigation, because
the combined use of pause duration and pitch reset to
punctuate the end of a story and the beginning of the next
carries many speaker-dependent characteristics.
We select a subset of TDT2 VOA Mandarin broadcast
news corpus (LDC, 1998) and manually extract the news
sessions reported by a single speaker. We also annotate
1http://www.mandarinbook.net/pronunciation/
193
Table 1: The TDT2 subset used in this study.
Nature
Mandarin news sessions reported
by a single speaker (13.4 hours)
# of News
Sessions
175 (Training: 74, Development:
50, Testing: 51)
Mean Session
Duration
276 seconds, 1071 Mandarin char-
acters
# of Story
Boundaries
1085 (Training: 442, Develop-
ment: 316, Testing: 327)
# of Speakers 11 (7 females and 4 males)
Mean Story
Duration
36 seconds, 105 Mandarin charac-
ters
Transcriptions Dragon ASR recognizer, GB-
encoded word-level transcriptions
in XML format
the news story boundaries in this subset. These single-
speaker sessions typically contain between 3 to 9 short
news stories separated by pauses and constitute about
30% of the entire TDT2 Mandarin corpus (by time du-
ration). The selected subset is divided into training, de-
velopment and testing sets. Details are shown in Table 1.
3 Region of Interest and Pitch Extraction
Previous work on English news segmentation (Shriberg
et. al., 2000) measured pitch resets at inter-word bound-
aries. Since Chinese news transcripts come as a charac-
ter stream and each character is pronounced as a tonal
syllable, it is more reasonable to investigate the pitch re-
set phenomenon at the syllable level. We assume that a
story boundary must occur at an utterance boundary. The
utterances are separated by labeled pauses in the VOA
transcriptions ([P] in Figure 2) and a story may contain
various utterances (between 2 to 38 in the corpus). There-
fore, we only investigate pitch resets in inter-syllable re-
gions across two consecutive utterances as shown in Fig-
ure 2. This is reasonable because there are only 6 story
boundaries (out of 1085) that are not signaled by pause
breaks in the corpus. The region of interest (ROI) is lim-
ited to only two tonal syllables, i.e., the last tonal syllable
of the previous utterance and the first tonal syllable of the
following utterance. We have performed experiments on
window length selection and results have shown a wider
window does not bring a noticeable improvement.
Raw pitch values are extracted by the YIN pitch
tracker (Cheveigne? et. al., 2002). The output pitch tra-
jectories are ranked as ?good? and ?best? by the pitch
tracker. Pitch values for unvoiced and pause segments are
assigned to be zero. We keep the ?best? pitch trajectories
for pitch reset measurements. We focus on pitch resets in
the ROIs and thus obtain pitch contours for the left and
right tonal syllables for each ROIs. However, the corpus
transcription does not provide time annotations for those
tonal syllables. Therefore, in the pitch trajectory of an
! " # $ % & ' ( ) * + , *[P]Character
TonalS yllable lian2h e2g uo2??m i4s hu1z hang3??a n1?n an2 di3??d a2??b a1?g e2?d a2[P]
Translation UnitedN ations Secretary-general Annan Arriveda t Bagdad
[P]Utterance Utterance Utterance Utterance
Story StoryBoundaryUtteranceBoundary
Utterance Utterance
Tone 2???2????2???4???1????3????1????2??????????3???2???1???2???2
Story
UtteranceFinal UtteranceInitial
ROI
[P] [P]
ROI
ROI ROI
Figure 2: Region of interest(ROI) for pitch reset measure.
audio stream, we search forwards and backwards on both
sides of the pause segment for the nearest non-zero pitch
measurement sequences. The two pitch sequences found
are used as the pitch contours for the left and right tonal
syllables of the ROI, respectively. This approximation is
reasonable because a Mandarin tonal syllable usually ex-
hibits a continuous pitch contour within its time duration.
4 Speaker- and Tone-Normalized Pitch
Reset Analysis in Mandarin Broadcast
News
We investigate the pitch reset behavior in the ROIs, i.e.,
the pitch jump between the left and right tonal syllables
at utterance and story boundaries across all corpus audio.
Since pitch is a speaker-related feature, we adopt speaker-
normalized pitch reset, defined as
PR = F0r ? F0l, (1)
where F0l and F0r are the speaker-normalized pitch for
the left and right tonal syllables in the ROIs, which are
calculated using
F0 = (f0 ? ?sf0)/?sf0 . (2)f0 denotes the mean value of the pitch contour of a tonal
syllable uttered by speaker s. ?sf0 and ?sf0 are the pitch
mean and standard deviation calculated for speaker s over
all the ROIs of speaker s in the corpus.
We measure the speaker-normalized pitch resets in all
ROIs, and categorize them into two boundary types, i.e.
utterance boundary and story boundary. To show the ef-
fects of tonality in pitch movement, we also categorize
the pitch resets by different tone combinations (16 com-
binations for 4 Mandarin tones2). Figure 3 plots the mean
PR of each tone combinations for the two boundary
types calculated on the corpus data. We see that the pitch
reset phenomenon holds for all tone combinations, even
for the tone pair (1,3) (i.e. high, low) that has a very small
reset. We perform t-tests (p < 0.0025, one-tailed), which
show that for a given tone pair across a boundary, there
is a significant difference in PR between an utterance
boundary and a story boundary. However, the PR val-
ues vary greatly across different tone pairs. For example,
2The neutral tone is not considered here since its pitch pat-
tern depends heavily on its neighboring tonal syllables.
194
(1,1) (1,2) (1,3) (1,4)(2,1) (2,2) (2,3) (2,4) (3,1)(3,2) (3,3) (3,4) (4,1) (4,2)(4,3) (4,4)0
0.5
1
1.5
2
2.5
StoryB oundary
UtteranceB oundary
ToneP air
Me
an
PR
OverallM ean forStoryB oundaryPR
OverallM ean forUtteranceB oundaryPR
Figure 3: Mean speaker-normalized pitch reset of the 16
tone pairs for story and utterance boundaries.
pitch resets are reduced for the tone pairs (1,3) and (4,3),
but are pronounced for the tone pairs (3,1) and (2,1). The
t-test (p < 0.0025, one-tailed) shows that the PR differ-
ence between utterance boundaries and story boundaries
are not significant. This motivates us to formulate a defi-
nition for speaker- and tone-normalized pitch reset.
The speaker- and tone-normalized pitch reset is defined
as:
PR = F0r ?F0l, (3)
where F0l and F0r are the speaker- & tone-normalized
pitch for the left and right tonal syllables in the ROIs,
respectively, defined as
F0 = (F0 ? ??F0)/??F0 , (4)
where F0 is the speaker-normalized pitch in Equation (2)
of a tonal syllable with tone ? . ??F0 and ??F0 are the pitch
mean and standard deviation calculated for the tonal syl-
lables with tone ? over all ROIs in the corpus. Figure 4
plots the mean PR of each tone combinations for the two
boundary types calculated on the corpus data.
Figure 4 shows a clear separation in speaker- and tone-
normalized pitch reset (PR) between utterance and story
boundaries (shade area in Figure 4). This result is sta-
tistically significant based on a t-test (p < 0.0025, one-
tailed). This observation suggests that speaker- and tone-
normalized pitch reset may be an effective story boundary
indicator for Mandarin broadcast news.
5 Experiments on Story Boundary
Detection
We perform experiments on story boundary detection at
the ROIs in the corpus. Since all ROIs are utterance
boundaries, of which only some are story boundaries, we
take a ?hypothesize and classify? approach in order to
strike a good balance between recall and precision. We
first hypothesize the occurrence of a story boundary if
the ROI has a pause duration that exceeds a threshold.
This is followed by a decision tree classifier that decides
on the existence of a story boundary. We used Quinlan?s
C4.5-style decision tree (Quinlan, 1992) as the classifier,
0
0.5
1
1.5
2
2.5
StoryB oundary
UtteranceB oundary
Me
an
ToneP air
OverallM ean???f orStoryB oundary
OverallM ean????f orUtteranceB oundary
(1,1) (1,2) (1,3) (1,4)(2,1) (2,2) (2,3) (2,4) (3,1)(3,2) (3,3) (3,4) (4,1) (4,2)(4,3) (4,4)
Figure 4: Mean speaker- and tone-normalized pitch reset
of the 16 tone pairs for story and utterance boundaries.
implemented by the IND toolkit.3 The pause duration
threshold was selected by a heuristic search procedure
described as follows: We experimented with pause du-
rations ranging from 0.1 to 4 seconds with step size of
0.1 second. In each case, we hypothesized raw bound-
aries in the training and development sets. A decision tree
was then grown using the raw boundary hypotheses of the
training set, and tested on the raw boundary hypotheses
of the development set. The pause duration leading to the
highest F-measure on the development set was selected
as the optimal threshold for the further experiments on
the testing set.
We develop seven story boundary detectors according
to the features used (see Table 2). The boundary de-
tection results on the testing set are shown in Table 2.
From Table 2, we can see that the detector using pause
duration achieves a high F-measure of 82.2%. This re-
sult is reasonable since VOA Mandarin news broadcast
makes large use of long pauses at story boundaries, es-
pecially at news sessions reported by a single speaker.
The detector using raw pitch reset (pr = f0r?f0l) only
gets a F-measure of 50.8% and the speaker-normalized
pitch reset (PR) achieves a slightly better F-measure of
55.3%. Speaker- and tone-normalized pitch reset (PR)
achieves a superior performance with an F-measure of
71.1%. This result is consistent with the observations
in Section 4. The story boundary indicative ability of
speaker-normalized pitch reset is affected by the tonal-
ity of Mandarin syllable. Speaker- and tone-normalized
pitch reset can alleviate the effects, thus leading to a bet-
ter discrimination. Based on Table 2, when pause is
combined with raw pitch reset, the F-measure degrades
from 82.2% to 68.3%. The F-measure reaches 77.4%
when we combine pause with speaker-normalized pitch
reset. When pause is combined with speaker- and tone-
normalized pitch reset (Pause+PR), the best F-measure
is achieved at 86.7%.
3http://ic.arc.nasa.gov/projects/bayes-group/ind/
195
Table 2: Story boundary detection experiment results(%)
Feature Recall Precision F-Measure
Pause 77.1 88.1 82.2
pr 52.0 49.7 50.8
PR 56.6 54.1 55.3
PR 70.3 72.0 71.1
Pause+pr 66.4 70.3 68.3
Pause+PR 72.2 83.5 77.4
Pause+PR 82.6 91.3 86.7
Table 3: Heuristics for story boundary decision
No. Description StoryBoundary?
1
Pause duration is short (P <
1.475) and pitch reset is small
(PR < 0.401)
No
2
Pause duration is short (P <
1.475) and pitch reset is huge
(PR > 1.112)
Yes
3
Pause duration is long (2.315?
P<4.915) and pitch reset is big
(PR>0.715)
Yes
4
Pause duration is long (P ?
4.915) and pitch reset is low
(PR < 0.3513)
No
Figure 5 shows the top levels of the decision tree ob-
tained using the Pause+PR set. We can observe the com-
plementarity between pause duration and pitch reset in
story boundary detection. This may be summarized in
terms of four major heuristics shown on the tree (labeled
as 1 to 4 in Figure 5). These heuristics cover about 83%
decisions made on the testing set, as described in Table 3.
Heuristics 2 is mainly used to detect possibly miss-
ing story boundaries with short pauses caused by speaker
speaking style, e.g., reporters Li Weiqing and Yang Chen
tend to use short pauses to separate news stories, but they
tend to offset the reduced pauses with pronounced pitch
resets to signify story boundaries. Heuristics 4 detects
possibly false alarms due to broadcast interruptions in
boundary detection. These interruptions (i.e. silences)
usually occur within a news story and may last for sev-
eral seconds (usually > 5 seconds).
6 Summary and Future Work
This paper investigated the combined use of pause dura-
tion and pitch reset for automatic story segmentation in
Mandarin broadcast news. Pitch reset analysis on Man-
darin broadcast news shows that story boundaries cannot
be discriminated from utterance boundaries by speaker-
normalized pitch reset, because speaker-normalized pitch
reset varies greatly across different tone pairs of boundary
syllables. This motivates us to investigate the speaker-
and tone-normalized pitch reset. Analysis shows that
speaker- and tone-normalized pitch reset can clearly sep-
NOT_BND
<1.805 >=1.805
1
>=1.475<1.475
<0.4012
>=2.315<2.315
>=4.915<4.915
P
PP
P
BNDBND
BNDBND
BND
NOT_BND
>=0.4012
>=1.112<1.112
>=0.8803<0.8803
<0.3513>=0.3513>=0.7150 <0.7150
>=0.811<0.811
2 3 4
Sub-treeSub-tree
Sub-tree Sub-tree
Figure 5: Decision tree for story boundary classification
based on the Pause+PR feature set. B denotes story
boundary, and NOT BND denotes not story boundary.
arate utterance boundaries from story boundaries across
all tone pairs. This result shows the difference be-
tween English and Chinese. Previous work for En-
glish (Shriberg et. al., 2000; Tu?r et. al., 2001) shows
that speaker-normalized pitch reset is effective. This
work shows that the same measurement is not sufficient
for Chinese; instead we need to use speaker- and tone-
normalized pitch reset in Chinese story segmentation.
When pause duration is combined with speaker- and tone-
normalized pitch reset, the best performance is achieved
with a high F-measure of 86.7%. Analysis of the deci-
sion tree uncovered four major heuristics that show how
speakers jointly utilize pause and pitch reset to separate
speech into stories.
Future work will investigate the pitch reset phe-
nomenon in Cantonese broadcast news, because Can-
tonese is another major Chinese dialect with more com-
plicated tonal characteristics. We also plan to incorporate
prosodic cues with lexical cues to further improve perfor-
mance in Chinese story segmentation.
References
Shriberg E., Stolcke A., Hakkani-Tu?r D. and Tu?r G. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Comm., 32(1-2):127?154.
Tu?r G. and Hakkani-Tu?r D. 2001. Integrating Prosodic and
Lexical Cues for Automatic Topic Segmentation. Computa-
tional Linguistics, 27(1):31?57.
Levow G. A. 2004. Prosody-based Topic Segmentation for
Mandarin Broadcast News. Proc. of HLT-NAACL, 137?140.
The Linguistic Data Consortium. 1998.
http://projects.ldc.upenn.edu/TDT2/.
de Cheveigne? A. and Kawahara H. 2002. Yin, a fundamental
frequency estimator for speech and music. Journal of the
Acoustic Society of America, 111(4):1917?1930.
Tseng C. Y., Pin S. H., Lee Y., Wang H. M. and Chen Y. C.
2005. Fluent speech prosody: Framework and modeling.
Speech Comm., 46:284?309.
Quinlan J. R. 1992. C4.5: Programs for Machine Learning.
Morgan Kaufmann.
196
Mandarin-English Information (MEI): 
Investigating Translingual Speech Retrieval 
Helen Meng, 1 Sanjeev Khudanpur, ~ Gina Levow, 3 Douglas W. Oard, 3 Hsin-Min Wang' 
1The Chinese University of Hong Kong, 2Johns Hopkins University, 
3University of Maryland and 4Academia Sinica (Taiwan) 
{hmmeng@se.cuhk.edu.hk, sanjeev@clsp.jhu.edu, gina@umiacs.umd.edu, 
oard@, glue.umd.edu, whm@ iis.sinica.edu.tw } 
Abstract 
We describe a system which supports 
English text queries searching for 
Mandarin Chinese spoken documents. 
This is one of the first attempts to tightly 
couple speech recognition with machine 
translation technologies for cross-media 
and cross-language retrieval. The 
Mandarin Chinese news audio are indexed 
with word and subword units by speech 
recognition. Translation of these multi- 
scale units can effect cross-language 
information retrieval. The integrated 
technologies will be evaluated based on 
the performance of translingnal speech 
retrieval. 
1. Introduction 
Massive quantities of audio and multimedia 
programs are becoming available. For example, 
in mid-February 2000, www.real.com listed 
1432 radio stations, 381 Internet-only 
broadcasters, and 86 television stations with 
Internet-accessible content, with 529 
broadcasting in languages other than English. 
Monolingual speech retrieval is now practical, as 
evidenced by services such as SpeechBot 
(speechbot.research.compaq.com), and it is clear 
that there is a potential demand for translingual 
speech retrieval if effective techniques can be 
developed. The Mandarin-English Information 
(MEI) project represents one of the first efforts 
in that direction. 
MEI is one of the four projects elected for 
the Johns Hopkins University (JHU) Summer 
Workshop 2000.1 Our research focus is on the 
integration of speech recognition and embedded 
translation technologies in the context of 
translingual speech retrieval. Possible 
applications of this work include audio and 
video browsing, spoken document retrieval, 
automated routing of information, and 
automatically alerting the user when special 
events occur. 
At the time of this writing, most of the MEI 
team members have been identified. This paper 
provides an update beyond our first proposal 
\[Meng et al, 2000\]. We present some ongoing 
work of our current eam members, as well as 
our ideas on an evolving plan for the upcoming 
JHU Summer Workshop 2000. We believe the 
input from the research community will benefit 
us greatly in formulating ourfinal plan. 
2. Background 
2.1 Translingual Information Retrieval 
The earliest work on large-vocabulary cross- 
language information retrieval from free-text 
(i,e., without manual topic indexing) was 
reported in 1990 \[Landauer and Littman, 1990\], 
and the topic has received increasing attention 
over the last five years \[Oard and Diekema, 
1998\]. Work on large-vocabulary retrieval from 
recorded speech is more recent, with some initial 
work reported in 1995 using subword indexing 
\[Wechsler and Schauble, 1995\], followed by the 
first TREC 2 Spoken Document Retrieval (SDR) 
I http://www.clsp,jhu.edu/ws2000/ 
2 Text REtrieval Conference, http://trec.nist.gov 
23 
evaluation \[Garofolo et al, 2000\]. The Topic 
Detection and Tracking (TDT) evaluations, 
which started in 1998, fall within our definition 
of speech retrieval for this purpose, differing 
from other evaluations principally in the nature 
of the criteria that human assessors use when 
assessing the relevance of a news stow to an 
information eed. In TDT, stories are assessed 
for relevance to an event, while in TREC stories 
are assessed for relevance to an explicitly stated 
information eed that is often subject- rather 
than event-oriented. 
The TDT-33 evaluation marked the first 
case of translingual speech retrieval - the task of 
finding information in a collection of recorded 
speech based on evidence of the information 
need that might be expressed (at least partially) 
in a different language. Translingual speech 
retrieval thus merges two lines of research that 
have developed separately until now. In the 
TDT-3 topic tracking evaluation, recognizer 
transcripts which have recognition errors were 
available, and it appears that every team made 
use of them. This provides a valuable point of 
reference for investigation of techniques that 
more tightly couple speech recognition with 
translingual retrieval. We plan to explore one 
way of doing this in the Mandarin-English 
Information (MEI) project. 
2.2 The Chinese Language 
In order to retrieve Mandarin audio documents, 
we should consider a number of linguistic 
characteristics of the Chinese language: 
The Chinese language has many dialects. 
Different dialects are characterized by their 
differences in the phonetics, vocabularies and 
syntax. Mandarin, also known as Putonglma 
("the common language"), is the most widely 
used dialect. Another major dialect is Cantonese, 
predominant in Hong Kong, Macau, South 
China and many overseas Chinese communities. 
Chinese is a syllable-based language, 
where each syllable carries a lexical tone. 
Mandarin has about 400 base syllables and four 
lexical tones, plus a "light" tone for reduced 
syllables. There are about 1,200 distinct, tonal 
syllables for Mandarin. Certain syllable-tone 
3 http://morph.ldc.upenn.edu/Projects/TDT3/ 
combinations are non-existent in the language. 
The acoustic correlates of the lexical tone 
include the syllable's fundamental frequency 
(pitch contour) and duration. However, these 
acoustic features are also highly dependent on 
prosodic variations of spoken utterances. 
The structure of Mandarin (base) syllables 
is (CG)V(X), where (CG) the syllable onset - C 
the initial consonant, G is the optional medial 
glide, V is the nuclear vowel, and X is the coda 
(which may be a glide, alveolar nasal or velar 
nasal). Syllable onsets and codas are optional. 
Generally C is known as the syllable initial, and 
the rest (GVX) syllable final. 4 Mandarin has 
approximately 21 initials and 39 finals. 5 
In its written form, Chinese is a sequence 
of characters. A word may contain one or more 
characters. Each character is pronounced as a 
tonal syllable. The character-syllable mapping is 
degenerate. On one hand, a given character may 
have multiple syllable pronunciations - for 
example, the character/d" may be pronounced as 
/hang2/, 6/hang4/, or/xing2/. On the other hand, 
a given tonal syllable may correspond to 
multiple characters. Consider the two-syllable 
pronunciation/fu4 shu4/, which corresponds toa 
two-character word. Possible homophones 
include ~, ,  (meaning "rich"), ~ ~tR, ("negative 
number"), ~1~1~, ("complex number" or 
"plural"), ~1~ ("repeat"). 7 
Aside from homographs and homophones, 
another source of ambiguity in the Chinese 
language is the definition of a Chinese word. 
The word has no delimiters, and the distinction 
between a word and a phrase is often vague. The 
lexical structure of the Chinese word is very 
different compared to English. Inflectional 
forms are minimal, while morphology and word 
derivations abide by a different set of rules. A 
word may inherit the syntax and semantics of 
(some of) its compositional characters, for 
4 http://m?rph'ldc'upenn'edu/Pr?jects/Chinese/intr?'html 
5 The corresponding linguistic haracteristics of Cantonese 
are very similar. 
6 These are Mandarin pinyin, the number encodes the tone 
of the syllable. 
7 Example drawn from \[Leung, 1999\]. 
24 
example, 8 ~ means red (a noun or an 
adjective), ~., means color (a noun), and ~. ,  
together means "the color red"(a noun) or 
simply "red" (an adjective). Alternatively, a 
word may take on totally different 
characteristics of its own, e.g. ~. means east (a 
noun or an adjective), ~ means west (a noun or 
an adjective), and .~.~ together means thing (a 
noun). Yet another case is where the 
compositional characters of a word do not form 
independent lexical entries in isolation, e.g. D~ 
means fancy (a verb), but its characters do not 
occur individually. Possible ways of deriving 
new words from characters are legion. The 
problem of identifying the words string in a 
character sequence is known as the segmentation 
/ tokenization problem. Consider the syllable 
string: 
/zhe4 yil wan3 hui4 ru2 chang2 ju3 xing2/ 
The corresponding character string has three 
possible segmentations - all are correct, but each 
involves a distinct set of words: 
(Meaning: It will be take place tonight as usual.) 
(Meaning: The evening banquet will take place 
as usual.) 
(Meaning: If this evening banquet akes place 
frequently...) 
The above considerations lead to a number 
of techniques we plan to use for our task. We 
concentrate on three equally critical problems 
related to our theme of translingual speech 
retrieval: (i) indexing Mandarin Chinese audio 
with word and subword units, (ii) translating 
variable-size units for cross-language 
information retrieval, and (iii) devising effective 
retrieval strategies for English text queries and 
Mandarin Chinese news audio. 
3. Multiscale Audio Indexing 
A popular approach to spoken document 
retrieval is to apply Large-Vocabulary 
s Examples drawn from \[Meng and Ip, 1999\]. 
Continuous Speech Recognition (LVCSR) 9 for 
audio indexing, followed by text retrieval 
techniques. Mandarin Chinese presents a 
challenge for word-level indexing by LVCSR, 
because of the ambiguity in tokenizing a 
sentence into words (as mentioned earlier). 
Furthermore, LVCSR with a static vocabulary is
hampered by the out-of-vocabulary (OOV) 
problem, especially when searching sources with 
topical coverage as diverse as that found in 
broadcast news. 
By virtue of the monosyllabic nature of the 
Chinese language and its dialects, the syllable 
inventory can provide a complete phonological 
coverage for spoken documents, and circumvent 
the OOV problem in news audio indexing, 
offering the potential for greater recall in 
subsequent retrieval. The approach thus supports 
searches for previously unknown query terms in 
the indexed audio. 
The pros and cons of subword indexing for 
an English spoken document retrieval task was 
studied in \[Ng, 2000\]. Ng pointed out that the  
exclusion of lexical knowledge when subword 
indexing is performed in isolation may adversely 
impact discrimination power for retrieval, but 
that some of that impact can be mitigated by 
modeling sequential constraints among subword 
units. We plan to investigate the efficacy of 
using both word and subword units for 
Mandarin audio indexing \[Meng et al, 2000\]. 
Although Ng found that such an approach 
produced little gain over words alone for 
English, the structure of Mandarin Chinese may 
produce more useful subword features. 
3.1 Modeling Syllable Sequence Constraints 
We have thus far used overlapping syllable N- 
grams for spoken document retrieval for two 
Chinese dialects - Mandarin and Cantonese. 
Results on a known-item retrieval task with over 
1,800 error-free news transcripts \[Meng et al, 
1999\] indicate that constraints from overlapping 
bigrams can yield significant improvements in 
retrieval performance over syllable unigrams, 
producing retrieval performance competitive 
9 The lexicon size of a typical large-vocabulary 
continuous speech recognizer can range from 10,000 
to 100,000 word forms. 
25 
with that obtained using automatically tokenized 
Chinese words. 
The study in \[Chen, Wang and Lee, 2000\] 
also used syllable pairs with skipped syllables in 
between. This is because many Chinese 
abbreviations are derived from skipping 
characters, e.g. J .~:~.~t:~  ~ National 
Science Council" can be abbreviated as l~r~ 
(including only the first, third and the last 
characters). Moreover, synonyms often differ by 
one or two characters, e.g. both ~ ' /~4~ and 
~.~,,Ag mean "Chinese culture". Inclusion o f  
these "skipped syllable pairs" also contributed to
retrieval performance. 
When modeling sequential syllable 
constraints, lexical constraints on recognized 
words may be helpful. We thus plan to exp\]Iore 
the potential for integrated sequential model\]ling 
of both words and syllables \[Meng et al, 20013\]. 
4. Multiseale Embedded Translation 
Figures 1 and 2 illustrate two translingual 
retrieval strategies. In query translation, English 
text queries are transformed into Mandarin and 
then used to retrieve Mandarin documents. For 
document translation, Mandarin documents are 
translated into English before they are indexed 
and then matched with English queries. 
McCarley has reported improved effectiveness 
from techniques that couple the two techniques 
\[McCarley, 1999\], but time constraints may 
limit us to explonng only the query translation 
strategy dunng the six-week Workshop. 
4,1 Word  Translat ion 
While we make use of sub-word 
transcription tosmooth out-of-vocabulary(OOV) 
problems in speech recognition as described 
above, and to alleviate the OOV problem :for 
translation as we discuss in the next section, 
accurate translation generally relies on the 
additional information available at the word and 
phrase levels. Since the "bag of words" 
information retrieval techniques do not 
incorporate any meaningful degree of language 
understanding to assess similarity between 
queries and documents, a word-for-word (or, 
more generally, term-for-term) embedded 
translation approach can achieve a useful level 
of effectiveness for many translingual retrieval 
applications \[Oard and Diekema, 1998\]. 
We have developed such a technique for the 
TDT-3 topic tracking evaluation \[Levow and 
Oard, 2000\]. For that work we extracted an 
enriched bilingual Mandarin-English term list by 
combining two term lists: (i) A list assembled 
by the Linguistic Data Consortium from freely 
available on-line resources; and (ii) entries from 
the CETA file (sometimes referred to as 
"Optilex"). This is a Chinese to English 
translation resource that was manually compiled 
by a team of linguists from more than 250 text 
sources, including special and general-purpose 
print dictionaries, and other text sources uch as 
newspapers. The CETA file contains over 
250,000 entries, but for our lexical work we 
extracted a subset of those entries drawn from 
contemporary general-purpose sources. We also 
excluded efinitions uch as "particle indicating 
a yes/no question." Our resulting Chinese to 
English merged bilingual term list contains 
translations for almost 200,000 Chinese terms, 
with average of almost two translation 
alternatives per term. We have also used the 
same resources to construct an initial English to 
Chinese bilingual term list that we plan to refine 
before the Workshop. 
Three significant challenges faced by term- 
to-term translation systems are term selection in 
the source language, the source language 
coverage of the bilingual term list, and 
translation selection in the target language when 
more than one alternative translation is known. 
Word segmentation is a natural by-product of 
large vocabulary Mandarin speech recognition, 
and white space provides word boundaries for 
the English queries. We thus plan to choose 
words as our basic term set, perhaps augmenting 
this with the multiword expressions found in the 
bilingual term list. 
Achieving adequate source language 
coverage is challenging in news retrieval 
applications of the type modelled by TDT, 
because proper names and technical terms that 
may not be present in general-purpose lexical 
resources often provide important retrieval cues. 
Parallel (translation equivalent) corpora have 
proven to be a useful source of translation 
26 
equivalent terms, but obtaining appropriate 
domain-specific parallel corpora in electronic 
form may not be practical in some applications. 
We therefore plan to investigate the use of 
comparable corpora to learn translation 
equivalents, based on techniques in \[Fung, 
1998\]. Subword translation, described below, 
provides a complementary way of handling 
terms for which translation equivalents cannot 
be reliably extracted from the available 
comparable corpora. 
One way of dealing with multiple 
translations is to weight the alternative 
translations using either a statistical translation 
model trained on parallel or comparable corpora 
to estimate translation probability conditioned 
on the source language term. When such 
resources are not sufficiently informative, it is 
generally possible to back off to an 
unconditioned preference statistic based on 
usage frequency of each possible translation i  a 
representative monolingual corpus in the target 
language. In retrospective r trieval applications 
the collection being searched can be used for 
this purpose. We have applied simple versions 
of this approach with good results \[Levow and 
Oard, 2000\]. 
We have recently observed that a simpler 
technique introduced by \[Pirkola, 1998\] can 
produce xcellent results. The key idea is to use 
the structure of the lexicon, in which several 
target language terms can represent a single 
source language term, to induce structure in the 
translated query that the retrieval system can 
automatically exploit. In essence, the translated 
query becomes a bag of bags of terms, where 
each smaller bag corresponds to the set of 
possible translations for one source-language 
term. We plan to implement his structured 
query translation approach using the Inquery 
\[Callan, 1992\] "synonym" operator in the same 
manner as \[Pirkola, 1998\], and to the potential to 
extend the technique to accommodate alternative 
recognition hypothesis and subword units as 
well: 
4.2 Subword  Translat ion 
Since Mandarin spoken documents can be 
indexed with both words and subwords, the 
translation (or "phonetic transliteration") of 
subword units is of particular interest. We plan 
to make use of cross-language phonetic 
mappings derived from English and Mandarin 
pronunciation rules for this purpose. This should 
be especially useful for handling named entities 
in the queries, e.g. names of people, places and 
organizations, etc. which are generally important 
for retrieval, but may not be easily translated. 
Chinese translations of English proper nouns 
may involve semantic as well as phonetic 
mappings. For example, "Northern Ireland" is 
translated as :~b~ttlM - -  where the first 
character ~ means 'north', and the remaining 
characters ~tllllll are pronounced as /ai4-er3- 
lan2L Hence the translation is both semantic 
and phonetic. When Chinese translations strive 
to attain phonetic similarity, the mapping may 
be inconsistent. For example, consider the 
translation of "Kosovo" - sampling Chinese 
newspapers in China, Taiwan and Hong Kong 
produces the following translations: 
~-~r~ /kel-suo3-wo4?, ~-~ /kel-suo3-fo2/, 
~'~&/kel-suo3-ful/f l4"~dt/kel-suo3-fu2/, or 
~/ke  1-suo3-fo2/. 
As can be seen, there is no systematic 
mapping to the Chinese character sequences, but 
the translated Chinese pronunciations bear some 
resemblance to the English pronunciation (/k ow 
s ax vow/). In order to support retrieval under 
these circumstances, the approach should 
involve approximate matches between the 
English pronunciation and the Chinese 
pronunciation. The matching algorithm should 
also accommodate phonological variations. 
Pronunciation dictionaries, or pronunciation 
generation tools for both English words and 
Chinese words / characters will be useful for the 
matching algorithm. We can probably leverage 
off of ideas in the development of universal 
speech recognizers \[Cohen et al, 1997\]. 
5. Mulfiscale Retrieval 
5.1 Coupling Words and Subwords 
We intend to use both words and subwords for 
retrieval. Loose coupling would involve separate 
retrieval runs using words and subwords, 
producing two ranked lists, followed by list 
merging using techniques such as those explored 
by \[Voorhees, 1995\]. Tight coupling, by 
27 
contrast, would require creation of a unified 
index containing both word and subword units, 
resulting in a single ranked list. We hope to 
explore both techniques during the Workshop. 
5.2 Imperfect Indexing and Translat ion 
It should be noted that speech recognition 
exacerbates uncertainty when indexing audio, 
and that translation or transliteration exacerbates 
uncertainty when translating queries and/or 
documents. To achieve robustness for retrieval, 
we have tried three techniques that we have 
found useful: (i) Syllable lattices were used in 
\[Wang, 1999\] and \[Chien et al, 2000\] for 
monolingual Chinese retrieval experiments. The 
lattices were pruned to constrain the search 
space, but were able to achieve robust retrieval 
based on imperfect recognized transcripts. (ii) 
Query expansion, in which syllable transcription 
were expanded to include possibly confusable 
syllable sequences based on a syllable confusion 
matrix derived from recognition errors, was used 
in \[Meng et al, 1999\]. (iii) We have expanded 
the document representation using terms 
extracted from similar documents in a 
comparable collection \[Levow and Oard, 2000\], 
and similar techniques are known to work well 
in the case of query translation (Ballesteros and 
Croft, 1997). We hope to add to this set: of 
techniques by exploring the potential for query 
expansion based on cross-language phonetic 
mapping. 
6. Using the TDT-3 Collection 
We plan to use the TDT-2 collection for 
development testing and the TDT-3 collection 
for evaluation. Both collections provide 
documents from two English newswire sources, 
six English broadcast news audio sources, two 
Mandarin Chinese newswire sources, and one 
Mandarin broadcast news source (Voice of 
America). Manually established story 
boundaries are available for all audio 
collections, and we plan to exploit that 
information to simplify our experiment design. 
The TDT-2 collection includes complete 
relevance assessments for 20 topics, and the 
TDT-3 collection provides the same for 60 
additional topics, 56 of which have at least one 
relevant audio story. For each topic, at least four 
English stories and four Chinese stories are 
known. 
We plan to automatically derive text queries 
based on one or more English stories that are 
presented as exemplars, and to use those queries 
to search the Mandarin audio collection. 
Manually constructed queries will provide a 
contrastive condition. Unlike the TDT "topic 
tracking" task in which stories must be declared 
relevant or not relevant in the order of their 
arrival, we plan to perform retrospective 
retrieval experiments in which all documents are 
known when the query is issued. By relaxing 
the temporal ordering of the TDT topic tracking 
task, we can meaningfully search for Mandarin 
Chinese stories that may have arrived before the 
exemplar story or stories. We thus plan to report 
ranked retrieval measures of effectiveness uch 
as average precision in addition to the detection 
statistics (miss and false alarm) typically 
reported in TDT. 
7.  Summary 
This paper presents our current ideas and 
evolving plan for the MEI project, to take place 
at the Johns Hopkins University Summer 
Workshop 2000. Translingual speech retrieval is 
a long-term research direction, and our team 
looks forward to jointly taking an initial step to 
tackle the problem. The authors welcome all 
comments and suggestions, aswe strive to better 
define the problem in preparation for the six- 
week Workshop. 
Acknowledgments 
The authors wish to thank Patrick Schone, Erika 
Grams, Fred Jelinek, Charles Wayne, Kenney 
? Ng, John Garofolo, and the participants in the 
December 1999 WS2000 planning meeting and 
the TDT-3 workshop for their many helpful 
suggestions. The Hopkins Summer Workshop 
series is supported by grants from the National 
Science Foundation. Our results reported in this 
paper eference thesis work in progress of Wai- 
Kit Lo (Ph.D. candidate, The Chinese Unversity 
of Hong Kong) and Berlin Chen (Ph.D. 
candidate, National Taiwan University). 
28 
References 
Ballesteros and W. B. Croft, "Phrasal 
Translation and Query Expansion Techniques 
for Cross-Language Information Retrieval," 
Proceedings ofACM SIGIR, 1997. 
Callan, J. P., W. B. Croft, and S. M. Harding, 
"The INQUERY Retrieval System," 
Proceedings of the 3rd International Conference 
on Database and Expert Systems Applications, 
1992. 
Carbonnell, J., Y. Yang, R. Frederking and R.D. 
Brown, "Translingual Information Retrieval: A 
Comparative Evaluation," Proceedings ofIJCAI, 
1997. 
Chen, B., H.M. Wang, and L.S. Lee, "Retrieval 
of Broadcast News Speech in Mandarin Chinese 
Collected in Taiwan using Syllable-Level 
Statistical Characteristics," Proceedings of 
ICASSP, 2000. 
Chien, L. F., H. M. Wang, B. R. Bai, and S. C. 
Lin, "A Spoken-Access Approach for Chinese 
Text and Speech Information Retrieval," Journal 
of the American Society for Information 
Science, 51 (4), pp. 313-323, 2000. 
Choy, C. Y., "Acoustic Units for Mandarin 
Chinese Speech Recognition," M.Phil. Thesis, 
The Chinese University of Hong Kong, Hong 
Kong SAR, China, 1999. 
Cohen, P., S. Dharanipragada, J. Gros, M. 
Mondowski, C. Neti, S. Roukos and T. Ward, 
"Towards a Universal Speech Recognizer for 
Multiple Languages," Proceedings of ASRU, 
1997. 
Fung, P., "A Statistical View on Bilingual 
Lexicon Extraction: From parallel corpora to 
non-parallel corpora," Proceedings of AMTA, 
1998. 
Garofolo, J.S., Auzanne, G.P., Voorhees, E.M., 
"The TREC Spoken Document Retrieval Track: 
A Success Story," Proceedings of the Recherche 
d'informations A sistre par Ordinateur: Content- 
Based Multimedia Information Access 
Conference, April 12-14, 2000,to be published. 
Knight, K. and J. Graehl, "Machine 
Transliteration," Proceedings ofACL, 1997. 
Landauer, T. K. and M.L. Littman, "Fully 
Automatic Cross-Language Document Retrieval 
Using Latent Semantic Indexing," Proceedings 
of the 6 th Annual Conference of the UW Centre 
for the New Oxford English Dictionary, 1990. 
Leung, R., "Lexical Access for Large 
Vocabulary Chinese Speech Recognition," M. 
Phil. Thesis, The Chinese University of Hong 
Kong, Hong Kong SAR, China 1999. 
Levow, G. and D.W. Oard, "Translingual Topic 
Tracking with PRISE," Working notes of the 
DARPA TDT-3 Workshop, 2000. 
Lin, C. H., L. S. Lee, and P. Y. Ting, "A New 
Framework for Recognition of Mandarin 
Syllables with Tones using Sub-Syllabic Units," 
Proceedings ofICASSP, 1993. 
Liu~ F. H., M. Picheny, P. Srinivasa, M. 
Monkowski and J. Chen, "Speech Recognition 
on Mandarin Call Home: A Large-Vocabulary, 
Conversational, nd Telephone Speech Corpus," 
Proceedings ofICASSP, 1996. 
McCarley, S., "Should we Translate the 
Documents or the Queries in Cross-Language 
Information Retrieval," Proceedings of ACL, 
1999. 
Meng, H. and C. W. Ip, "An Analytical Study o f  
Transformational Tagging of Chinese Text," 
Proceedings of the Research On Computational 
Lingustics (ROCLING) Conference, 1999. 
Meng, H., W. K. Lo, Y. C. Li and P. C. Ching, 
"A Study on the Use of Syllables for Chinese 
Spoken Document Retrieval," Technical Report 
SEEM1999-11, The Chinese University of Hong 
Kong, 1999. 
Meng, H., Khudanpur, S., Oard, D. W. and 
Wang, H. M., "Mandarin-English Information 
(MEI)," Working notes of the DARPA TDT-3 
Workshop, 2000. 
Ng, K., "Subword-based Approaches for Spoken 
Document Retrieval," Ph.D. Thesis, MIT, 
February 2000. 
Oard, D. W. and A.R. Diekema, "Cross- 
Language Information Retrieval," Annual 
Review of Information Science and Technology, 
vol.33, 1998. 
Pirkola, A., "The effects of query structure and 
dictionary setups in dictionary-based cross- 
language information retrieval," Proceedings of 
ACM SIGIR, 1998. 
Sheridan P. and J. P. Ballerini, "Experiments in
Multilingual Information Retrieval using the 
29 
SPIDER System," Proceedings of ACM SIGIR, 
1996. 
Voorhees, E., "Learning Collection Fusion 
Strategies," Proceedings of SIGIR, 1995. 
Wang, H. M., "Retrieval of Mandarin Spoken 
Documents Based on Syllable Lattice 
Matching," Proceedings of the Fourth 
International Workshop on Information 
Retrieval in Asian Languages, 1999. 
Wechsler, M. and P. Schaiible, "Speech 
Retrieval Based on Automatic Indexing," 
Proceedings of MIRO- 1995. 
English Text Queries 
(words) 
Words that are present entities and unknown words 
translation dictionary 
\[ Trans'a~on I I Transliteration 
Mand~Sn Queries (with words and syllables) 
Mandarin Spoken Documents \[ 
(indexed with word and subword units) 
"7 
Information Retrieval 
Engine 
I Evaluate Retrieval 
Performance 
Figure 1. Query translation strategy. 
Mandarin Spoken Documents 
(indexed with word and subword units) 
l 
Translation 
Documents in 
English 
English Text Queries 
(words) 
I Information Retrieval 
Engine 
Evaluate 
Retrieval 
Performance 
Figure 2. Document translation strategy. 
3O 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 185?188,
New York, June 2006. c?2006 Association for Computational Linguistics
A Maximum Entropy Framework that Integrates Word Dependencies and
Grammatical Relations for Reading Comprehension
Kui Xu1,2 and Helen Meng1
1Human-Computer Communications Laboratory
Dept. of Systems Engineering and
Engineering Management
The Chinese University of Hong Kong
Hong Kong SAR, China
{kxu, hmmeng}@se.cuhk.edu.hk
Fuliang Weng2
2Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
Fuliang.weng@rtc.bosch.com
Abstract
Automatic reading comprehension (RC)
systems can analyze a given passage and
generate/extract answers in response to
questions about the passage. The RC
passages are often constrained in their
lengths and the target answer sentence
usually occurs very few times. In order
to generate/extract a specific precise an-
swer, this paper proposes the integration
of two types of ?deep? linguistic features,
namely word dependencies and grammati-
cal relations, in a maximum entropy (ME)
framework to handle the RC task. The
proposed approach achieves 44.7% and
73.2% HumSent accuracy on the Reme-
dia and ChungHwa corpora respectively.
This result is competitive with other re-
sults reported thus far.
1 Introduction
Automatic reading comprehension (RC) systems
can analyze a given passage and generate/extract
answers in response to questions about the pas-
sage. The RC passages are often constrained in
their lengths and the target answer sentence usu-
ally occurs only once (or very few times). This
differentiates the RC task from other tasks such as
open-domain question answering (QA) in the Text
Retrieval Conference (Light et al, 2001). In order
to generate/extract a specific precise answer to a
given question from a short passage, ?deep? linguis-
tic analysis of sentences in a passage is needed.
Previous efforts in RC often use the bag-of-words
(BOW) approach as the baseline, which is further
augmented with techniques such as shallow syn-
tactic analysis, the use of named entities (NE) and
pronoun references. For example, Hirschman et
al. (1999) have augmented the BOW approach
with stemming, NE recognition, NE filtering, se-
mantic class identification and pronoun resolution
to achieve 36% HumSent1 accuracy in the Reme-
dia test set. Based on these technologies, Riloff
and Thelen (2000) improved the HumSent accuracy
to 40% by applying a set of heuristic rules that as-
sign handcrafted weights to matching words and NE.
Charniak et al (2000) used additional strategies for
different question types to achieve 41%. An exam-
ple strategy for why questions is that if the first word
of the matching sentence is ?this,? ?that,? ?these? or
?those,? the system should select the previous sen-
tence as an answer. Light et al (2001) also intro-
duced an approach to estimate the performance up-
per bound of the BOW approach. When we apply
the same approach to the Remedia test set, we ob-
tained the upper bound of 48.3% HumSent accuracy.
The state-of-art performance reached 42% with an-
swer patterns derived from web (Du et al, 2005).
This paper investigates the possibility of enhanc-
ing RC performance by applying ?deep? linguistic
analysis for every sentence in the passage. We
refer to the use of two types of features, namely
word dependencies and grammatical relations, that
1If the system?s answer sentence is identical to the corre-
sponding human marked answer sentence, the question scores
one point. Otherwise, the question scores no point. HumSent
accuracy is the average score across all questions.
185
are integrated in a maximum entropy framework.
Word dependencies refer to the headword depen-
dencies in lexicalized syntactic parse trees, together
with part-of-speech (POS) information. Grammat-
ical relations (GR) refer to linkages such as sub-
ject, object, modifier, etc. The ME framework
has shown its effectiveness in solving QA tasks (It-
tycheriah et al, 1994). In comparison with previ-
ous approaches mentioned earlier, the current ap-
proach involves richer syntactic information that
cover longer-distance relationships.
2 Corpora
We used the Remedia corpus (Hirschman et al,
1999) and ChungHwa corpus (Xu and Meng, 2005)
in our experiments. The Remedia corpus contains
55 training stories and 60 testing stories (about 20K
words). Each story contains 20 sentences on aver-
age and is accompanied by five types of questions:
who, what, when, where and why. The ChungHwa
corpus contains 50 training stories and 50 test stories
(about 18K words). Each story contains 9 sentences
and is accompanied by four questions on average.
Both the Remedia and ChungHwa corpora contain
the annotation of NE, anaphor referents and answer
sentences.
3 The Maximum Entropy Framework
Suppose a story S contains n sentences, C0, . . . , Cn,
the objective of an RC system can be described as:
A = arg maxCi?S P (Ci answers Q|Q). (1)
Let ?x? be the question (Q) and ?y? be the answer
sentence Ci that answers ?x?. Equation 1 can be
computed by the ME method (Zhou et al, 2003):
p(y|x) = 1Z(x) exp
?
j ?jfj(x,y), (2)
where Z(x) = ?y exp
?
j
?jfj(x,y) is a normalization
factor, fj(x, y) is the indicator function for feature
fj; fj occurs in the context x, ?j is the weight of
fj . For a given question Q, the Ci with the highest
probability is selected. If multiple sentences have
the maximum probability, the one that occurs
the earliest in the passage is returned. We used
the selective gain computation (SGC) algorithm
(Zhou et al, 2003) to select features and estimate
parameters for its fast performance.
Question: Who wrote the "Pledge of Allegiance"
Answer sentence: The pledge was written by Frances Bellamy.
PP(by)
by/IN
Frances/NNP Bellamy/NNP
was/VBD
NPB(Bellamy)
PP(of)
NP(Pledge)
VP(wrote)
Who/WP of/IN
WHNP(Who)
SBARQ(wrote)
wrote/VBD NP(Allegiance)
Allegiance/NNP "/??
NP(Pledge)
the/DT "/?? Pledge/NN
The/DT
NPB(pledge)
written/VBN
VP(written)
S(written)
VP(written)
pledge/NN
Figure 1. The lexicalized syntactic parse trees of a
question and a candidate answer sentence.
4 Features Used in the ?Deep? Linguistic
Analysis
A feature in the ME approach typically has binary
values: fj(x, y) = 1 if the feature j occurs; other-
wise fj(x, y) = 0. This section describes two types
of ?deep? linguistic features to be integrated in the
ME framework in two subsections.
4.1 POS Tags of Matching Words and
Dependencies
Consider the following question Q and sentence C ,
Q: Who wrote the ?Pledge of Allegiance?
C: The pledge was written by Frances Bellamy.
The set of words and POS tags2 are:
Q: {write/VB, pledge/NN, allegiance/NNP}
C: {write/VB, pledge/NN, by/IN, Frances/NNP,
Bellamy/NNP}.
Two matching words between Q and C (i.e. ?write?
and ?pledge?) activate two POS tag features:
fV B(x, y)=1 and fNN (x, y)=1.
We extracted dependencies from lexicalized
syntactic parse trees, which can be obtained accord-
ing to the head-rules in (Collins, 1999) (e.g. see
Figure 1). In a lexicalized syntactic parse tree, a
dependency can be defined as:
< hc ? hp > or < hr ? TOP >,
where hc is the headword of the child node, hp
is the headword of the parent node (hc 6= hp),
hr is the headword of the root node. Sample
2We used the MXPOST toolkit downloaded from
ftp://ftp.cis.upenn.edu/pub/adwait/jmx/ to generate POS
tags. Stop words including who, what, when, where, why,
be, the, a, an, and of are removed in all questions and story
sentences. All plural noun POS tags are replaced by their
single forms (e.g. NNS?NN); all verb POS tags are replaced
by their base forms (e.g. VBN?VB) due to stemming.
186
mod
be
write/V
subj
Question: Who wrote the "Pledge of Allegiance"
the/Det be/be
by/Prep
pcomp?n
Frances Bellamy/N
pledge/N
obj
det
write/V subj
Answer sentence: The pledge was written by Frances Bellamy.
Who/N the/Det
Pledge/N
det
punc
"/U of/Prep
Allegiance/N
punc
"/U
mod
obj
Figure 2. The dependency trees produced by MINI-
PAR for a question and a candidate answer sentence.
dependencies in C (see Figure 1) are:
<write?TOP> and <pledge?write>.
The dependency features are represented by the
combined POS tags of the modifiers and headwords
of (identical) matching dependencies3 . A matching
dependency between Q and C , <pledge?write>
activates a dependency feature: fNN?V B(x, y)=1.
In total, we obtained 169 and 180 word dependency
features from the Remedia and ChungHwa training
sets respectively.
4.2 Matching Grammatical Relationships (GR)
We extracted grammatical relationships from the de-
pendency trees produced by MINIPAR (Lin, 1998),
which covers 79% of the dependency relationships
in the SUSANNE corpus with 89% precision4 . IN
a MINIPAR dependency relationship:
(word1 CATE1:RELATION:CATE2 word2),
CATE1 and CATE2 represent such grammatical cat-
egories as nouns, verbs, adjectives, etc.; RELA-
TION represents the grammatical relationships such
as subject, objects, modifiers, etc.5 Figure 2 shows
dependency trees of Q and C produced by MINI-
PAR. Sample grammatical relationships in C are
pledge N:det:Det the, and write V:by-subj:Prep by.
GR features are extracted from identical matching
relationships between questions and candidate sen-
tences. The only identical matching relationship be-
tween Q and C , ?write V:obj:N pledge? activates a
grammatical relationship feature: fobj(x, y)=1. In
total, we extracted 44 and 45 GR features from the
Remedia and ChungHwa training sets respectively.
3We extracted dependencies from parse trees generated by
Collins? parser (Collins, 1999).
4MINIPAR outputs GR directly, while Collins? parser gives
better result for dependencies.
5Refer to the readme file of MINIPAR downloaded from
http://www.cs.ualberta.ca/ lindek/minipar.htm
5 Experimental Results
We selected the features used in Quarc (Riloff and
Thelen, 2000) to establish the reference performance
level. In our experiments, the 24 rules in Quarc are
transferred6 to ME features:
?If contains(Q,{start, begin}) and contains(S,{start,
begin, since, year}) Then Score(S)+=20? ?
fj(x, y) = 1 (0< j <25) if Q is a when question that
contains ?start? or ?begin? and C contains ?start,?
?begin,? ?since? or ?year?; fj(x, y) = 0 otherwise.
In addition to the Quarc features, we resolved five
pronouns (he, him, his, she and her) in the stories
based on the annotation in the corpora. The result
of using Quarc features in the ME framework is
38.3% HumSent accuracy on the Remedia test set.
This is lower than the result (40%) obtained by our
re-implementation of Quarc that uses handcrafted
scores. A possible explanation is that handcrafted
scores are more reliable than ME, since humans
can generalize the score even for sparse data.
Therefore, we refined our reference performance
level by combining the ME models (MEM) and
handcrafted models (HCM). Suppose the score of a
question-answer pair is score(Q,Ci), the conditional
probability that Ci answers Q in HCM is:
HCM(Q,Ci) = P (Ci answers Q|Q) = score(Q,Ci)?j?nscore(Q,Cj) .We combined the probabilities from MEM and
HCM in the following manner:
score?(Q, Ci) = ?MEM(Q, Ci) + (1 ? ?)HCM(Q, Ci).
To obtain the optimal ?, we partitioned the training
set into four bins. The ME models are trained on
three different bins; the optimal ? is determined
on the other bins. By trying different bins com-
binations and different ? such that 0 < ? < 1
with interval 0.1, we obtained the average optimal
? = 0.15 and 0.9 from the Remedia and ChungHwa
training sets respectively7 . Our baseline used the
combined ME models and handcrafted models to
achieve 40.3% and 70.6% HumSent accuracy in the
Remedia and ChungHwa test sets respectively.
We set up our experiments such that the linguistic
features are applied incrementally - (i) First , we use
only POS tags of matching words among questions
6The features in (Charniak et al, 2000) and (Du et al, 2005)
could have been included similarly if they were available.
7HCM are tuned by hand on Remedia, thus a bigger weight,
0.85 represents their reliability. For ChungHwa, a weight, 0.1
means that HCM are less reliable.
187
and candidate answer sentences. (ii) Then we add
POS tags of the matching dependencies. (iii) We ap-
ply only GR features from MINIPAR. (iv) All fea-
tures are used. These four feature sets are denoted
as ?+wp,? ?+wp+dp,? ?+mini? and ?+wp+dp+mini?
respectively. The results are shown in Figure 3 for
the Remedia and ChungHwa test sets.
With the significance level 0.05, the pairwise t-
test (for every question) to the statistical significance
of the improvements shows that the p-value is 0.009
and 0.025 for the Remedia and ChungHwa test sets
respectively. The ?deep? syntactic features signif-
icantly improve the performance over the baseline
system on the Remedia and ChungHwa test sets8.
Baseline +wp +wp+dp +mini +wp+dp+mini
Combinations of different features
H
um
Se
nt
 A
cc
ur
ac
y(%
)
30
40
50
60
70
80
90
Remedia
ChungHwa
40.3 41.7
43.3 43 44.7
70.6 71.1 72.7 72.2
73.2
Figure 3. Baseline and proposed feature results on
the Remedia and ChungHwa test sets.
6 Conclusions
This paper proposes the integration of two types of
?deep? linguistic features, namely word dependen-
cies and grammatical relations, in a ME framework
to handle the RC task. Our system leverages
linguistic information such as POS, word depen-
dencies and grammatical relationships in order to
extract the appropriate answer sentence for a given
question from all available sentences in the passage.
Our system achieves 44.7% and 73.2% HumSent
accuracy on the Remedia and ChungHwa test sets
respectively. This shows a statistically significant
improvement over the reference performance levels,
40.3% and 70.6% on the same test sets.
Acknowledgements
This work is done during the first author?s internship
8Our previous work about developing the ChungHwa corpus
(Xu and Meng, 2005) shows that most errors can only be solved
by reasoning with domain ontologies and world knowledge.
at RTC Bosch Corp. The work is also affiliated with
the CUHK Shun Hing Institute of Advanced Engi-
neering and partially supported by CUHK4237/03E
from RGC of HKSAR Government.
References
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. Workshop on the Evaluation of Parsing
Systems 1998.
Ellen Riloff and Michael Thelen. 2000. A Rule-based
Question Answering System for Reading Comprehen-
sion Test. ANLP/NAACL-2000 Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
Eugene Charniak, Yasemin Altun, Rofrigo D. Braz, Ben-
jamin Garrett, Margaret Kosmala, Tomer Moscovich,
Lixin Pang, Changhee Pyo, Ye Sun, Wei Wy, Zhongfa
Yang, Shawn Zeller, and Lisa Zorn. 2000. Reading
Comprehension Programs In a Statistical-Language-
Processing Class. ANLP-NAACL 2000 Work-
shop: Reading Comprehension Tests as Evaluation for
Computer-Based Language Understanding Systems.
Kui Xu and Helen Meng. 2005. Design and Develop-
ment of a Bilingual Reading Comprehension Corpus.
International Journal of Computational Linguistics &
Chinese Language Processing, Vol. 10, No. 2.
Lynette Hirschman, Marc Light, Eric Breck, and John D.
Burger. 1999. Deep Read: A Reading Comprehension
System. Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Marc Light, Gideon S. Mann, Ellen Riloff, and Eric
Breck. 2001. Analyses for Elucidating Current Ques-
tion Answering Technology. Journal of Natural Lan-
guage Engineering, No. 4 Vol. 7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD thesis, University
of Pennsylvania.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu and
Adwait Ratnaparkhi. 2001. Question Answering Us-
ing Maximum-Entropy Components. Proceedings of
NAACL 2001.
Yaqian Zhou, Fuliang Weng, Lide Wu, Hauke Schmidt.
2003. A Fast Algorithm for Feature Selection in Con-
ditional Maximum Entropy Modeling. Proceedings of
EMNLP 2003.
Yongping Du, Helen Meng, Xuanjing Huang, Lide
Wu. 2005. The Use of Metadata, Web-derived An-
swer Patterns and Passage Context to Improve Read-
ing Comprehension Performance. Proceedings of
HLT/EMNLP 2005.
188
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 265?268,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Automatic Story Segmentation using a Bayesian Decision Framework 
for Statistical Models of Lexical Chain Features 
 
 Wai-Kit Lo Wenying Xiong Helen Meng 
 The Chinese University The Chinese University The Chinese University 
 of Hong Kong, of Hong Kong, of Hong Kong, 
 Hong Kong, China Hong Kong, China Hong Kong, China  
 wklo@se.cuhk.edu.hk wyxiong@se.cuhk.edu.hk hmmeng@se.cuhk.edu.hk 
  
 
Abstract 
This paper presents a Bayesian decision 
framework that performs automatic story 
segmentation based on statistical model-
ing of one or more lexical chain features. 
Automatic story segmentation aims to lo-
cate the instances in time where a story 
ends and another begins. A lexical chain 
is formed by linking coherent lexical 
items chronologically. A story boundary 
is often associated with a significant 
number of lexical chains ending before it, 
starting after it, as well as a low count of 
chains continuing through it. We devise a 
Bayesian framework to capture such be-
havior, using the lexical chain features of 
start, continuation and end. In the scoring 
criteria, lexical chain starts/ends are 
modeled statistically with the Weibull 
and uniform distributions at story boun-
daries and non-boundaries respectively. 
The normal distribution is used for lexi-
cal chain continuations. Full combination 
of all lexical chain features gave the best 
performance (F1=0.6356). We found that 
modeling chain continuations contributes 
significantly towards segmentation per-
formance. 
1 Introduction 
Automatic story segmentation is an important 
precursor in processing audio or video streams in 
large information repositories. Very often, these 
continuous streams of data do not come with 
boundaries that segment them into semantically 
coherent units, or stories. The story unit is 
needed for a wide range of spoken language in-
formation retrieval tasks, such as topic tracking, 
clustering, indexing and retrieval. To perform 
automatic story segmentation, there are three 
categories of cues available: lexical cues from 
transcriptions, prosodic cues from the audio 
stream and video cues such as anchor face and 
color histograms. Among the three types of cues, 
lexical cues are the most generic since they can 
work on text and multimedia sources. Previous 
approaches include TextTiling (Hearst 1997) that 
monitors changes in sentence similarity, use of 
cue phrases (Reynar 1999) and Hidden Markov 
Models (Yamron 1998). In addition, the ap-
proach based on lexical chaining captures the 
content coherence by linking coherent lexical 
items (Morris and Hirst 1991, Hirst and St-Onge 
1998). Stokes (2004) discovers boundaries by 
chaining up terms and locating instances of time 
where the count of chain starts and ends (boun-
dary strength) achieves local maxima. Chan et al 
(2007) enhanced this approach through statistical 
modeling of lexical chain starts and ends. We 
further extend this approach in two aspects: 1) a 
Bayesian decision framework is used; 2) chain 
continuations straddling across boundaries are 
taken into consideration and statistically modeled. 
2 Experimental Setup 
Experiments are conducted using data from the 
TDT-2 Voice of America Mandarin broadcast. 
In particular, we only use the data from the long 
programs (40 programs, 1458 stories in total), 
each of which is about one hour in duration.  The 
average number of words per story is 297. The 
news programs are further divided chronologi-
cally into training (for parameter estimation of 
the statistical models), development (for tuning 
decision thresholds) and test (for performance 
evaluation) sets, as shown in Figure 1. Automatic 
speech recognition (ASR) outputs that are pro-
vided in the TDT-2 corpus are used for lexical 
chain formation. 
265
The story segmentation task in this work is to 
decide whether a hypothesized utterance boun-
dary (provided in the TDT-2 data based on the 
speech recognition result) is a story boundary. 
Segmentation performance is evaluated using the 
F1-measure. 
20 hour 10 hour 10 hour
Feb.20th,1998 Mar.4th,1998 Mar.17th,1998 Apr.4th,1998
Training Set Development Set Test Set
697 stories 385 stories 376 stories
 
Figure 1: Organization of the long programs in TDT-2 
VOA Mandarin for our experiments. 
3 Approach 
Our approach considers utterance boundaries that 
are labeled in the TDT-2 corpus and classifies 
them either as a story boundary or non-boundary. 
We form lexical chains from the TDT-2 ASR 
outputs by linking repeated words. Since words 
may also repeat across different stories, we limit 
the maximum distance between consecutive 
words within the lexical chain. This limit is op-
timized according to the approach in (Chan et al 
2007) based on the training data. The optimal 
value is found to be 130.9sec for long programs. 
We make use of three lexical chain features: 
chain starts, continuations and ends. At the be-
ginning of a story, new words are introduced 
more frequently and hence we observe many lex-
ical chain starts. There is also tendency of many 
lexical chains ending before a story ends. As a 
result, there is a higher density of chain starts and 
ends in the proximity of a story boundary. Fur-
thermore, there tends to be fewer chains strad-
dling across a story boundary. Based on these 
characteristics of lexical chains, we devise a sta-
tistical framework for story segmentation by 
modeling the distribution of these lexical chain 
features near the story boundaries. 
3.1 Story Segmentation based on a Single 
Lexical Chain Feature 
Given an utterance boundary with the lexical 
chain feature, X, we compare the conditional 
probabilities of observing a boundary, B, or non-
boundary, B , as  
 <> )|()|( XBPXBP . (1) 
where X is a single chain feature, which may be 
the chain start (S), chain continuation (C), or 
chain end (E). 
By applying the Bayes? theorem, this can be 
rewritten as a likelihood ratio test, 
 
B
x
XP
BXP ?
)|(
)|( <>
 
(2) 
for which the decision threshold 
is )(/)( BPBPx =? , dependent on the a priori 
probability of observing boundary or a non-
boundary. 
3.2 Story Segmentation based on Combined 
Chain Features 
When multiple features are used in combination, 
we formulate the problem as  
 ),,|(),,|( CESBPCESBP <> . (3) 
By assuming that the chain features are condi-
tionally independent of one another (i.e., 
P(S,C,E|B) = P(S|B) P(C|B) P(E|B)), the formu-
lation can be rewritten as a likelihood ratio test 
 
<> SECBCPBEPBSP
BCPBEPBSP ?
)|()|()|(
)|()|()|(
.
 
(4) 
4 Modeling of Lexical Chain Features 
4.1 Chain starts and ends 
We follow (Chan et al 2007) to model the lexi-
cal chain starts and ends at a story boundary with 
a statistical distribution. We apply a window 
around the candidate boundaries (same window 
size for both chain starts and ends) in our work. 
Chain features falling outside the window are 
excluded from the model. Figure 2 shows the 
distribution when a window size of 20 seconds is 
used. This is the optimal window size when 
chain start and end features are combined. 
0-2-4-6-8-10-12-14-16-18-20 2 4 6 8 10 12 14 16 18 20
10
20
30
40
50
Offset from story boundary in second
Number of lexical chain features
Fitted Weibull dist. for 
lexical chain ends
Frequency of lexical 
chain features
Fitted Weibull dist. for 
lexical chain starts
x
 
Figure 2: Distribution of chain starts and ends at 
known story boundaries. The Weibull distribution is 
used to model these distributions. 
We also assume that the probability of seeing 
a lexical chain start / end at a particular instance 
is independent of the starts / ends of other chains. 
As a result, the probability of seeing a sequence 
of chain starts at a story boundary is given by the 
product of a sequence of Weibull distributions 
 
?
=
??
???
???
??
???
?=
s
k
iN
i
tk
i e
tk
BSP
1
1
)|( ??? , (5) 
266
where S is the sequence of time with chain starts 
(S=[t1, t2, ? ti, ? tNs]), ks is the shape, ?s is the 
scale for the fitted Weibull distribution for chain 
starts, Ns is the number of chain starts. The same 
formulation is similarly applied to chain ends. 
Figure 3 shows the frequency of raw feature 
points for lexical chain starts and ends near utter-
ance boundaries that are non-story boundaries. 
Since there is no obvious distribution pattern for 
these lexical chain features near a non-story 
boundary, we model these characteristics with a 
uniform distribution. 
 
2 4 6 8 10 12 14 16
0.02
0.04
0.06
0.08
0-2-4-6-8-10-12-14-16
0.1
Relative frequency of chain starts / ends
Offset from utterance boundary in seconds
(non-story boundaries only)
Lexical chain starts / ends
Fitted uniform dist. for 
lexical chain starts
x
Fitted uniform dist. for 
lexical chain ends
 
Figure 3: Distribution of chain starts and ends at ut-
terance boundaries that are non-story boundaries. 
4.2 Chain continuations 
Figure 4 shows the distributions of chain contin-
uations near story boundary and non-story boun-
dary. As one may expect, there are fewer lexical 
chains that straddle across a story boundary (the 
curve of )|( BCP ) when compared to a non-story 
boundary (the curve of )|( BCP ). Based on the 
observations, we model the probability of occur-
rence of lexical chains straddling across a given 
story boundary or non-story boundary by a nor-
mal distribution. 
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
P
ro
ba
bi
lit
y
0 5 10 15 20 25
Number of chain continuations straddling across an 
utterance boundary
Story boundary, )|( BCP
Non-story boundary, )|( BCP
Relative frequency of lexical chain 
continuation at an utterance boundaryx
Fitted distribution at story boundary
Fitted distribution at non-story boundary
P
ro
ba
bi
lit
y
 
Figure 4: Distributions of chain continuations at story 
boundaries and non-story boundaries. 
5 Story Segmentation based on Combi-
nation of Lexical Chain Features 
We trained the parameters of the Weibull distri-
bution for lexical chain starts and ends at story 
boundaries, the uniform distribution for lexical 
chain start / end at non-story boundary, and the 
normal distribution for lexical chain continua-
tions. Instead of directly using a threshold as 
shown in Equation (2), we optimize on the para-
meter n, which is the optimal number of top scor-
ing utterance boundaries that are classified as 
story boundaries in the development set. 
5.1 Using Bayesian decision framework 
We compare the performance of the Bayesian 
decision framework to the use of likelihood only 
P(X|B) as shown in Figure 5. The results demon-
strate consistent improvement in F1-measure 
when using the Bayesian decision framework. 
0
0.2
0.4
0.6
F1
-m
ea
su
re
)|( BSP )|( BEP
)|(
)|(
BSP
BSP
)|(
)|(
BEP
BEP
F1
-m
ea
su
re
 
Figure 5: Story segmentation performance in F1-
measure when using single lexical chain features. 
5.2 Modeling multiple features jointly 
0
0.2
0.4
0.6
0.8
F1
-m
ea
su
re
(a) (b) (c) (d) (e) (f) (g) (h)
)|(
)|(
(c)
BEP
BEP
)|(
)|(
(d)
BCP
BCP
)|()|(
)|()|(
(e)
BEPBSP
BEPBSP
)|()|(
)|()|(
(f)
BCPBSP
BCPBSP
)|()|(
)|()|(
(g)
BCPBEP
BCPBEP
)|()|()|(
)|()|()|(
(h)
BCPBEPBSP
BCPBEPBSP
)|(
)|(
(b)
BSP
BSP
]2007[),(core (a) ChanESS
F1
-m
ea
su
re
 
Figure 6: Results of F1-measure comparing the seg-
mentation results using different statistical models of 
lexical chain features. 
We further compare the performance of various 
scoring methods including single and combined 
lexical chain features. The baseline result is ob-
tained using a scoring function based on the like-
lihoods of seeing a chain start or end at a story 
boundary (Chan et al 2007) which is denoted as 
Score(S, E). Performance from other methods 
based on the same dataset can be referenced from 
Chan et al 2007 and will not be repeated here. 
The best story segmentation performance is 
achieved by combining all lexical chain features 
which achieves an F1-measure of 0.6356. All 
improvements have been verified to be statisti-
cally significant (?=0.05). By comparing the re-
sults of (e) to (h), (c) to (g), and (b) to (f), we can 
see that lexical chain continuation feature contri-
butes significantly and consistently towards story 
segmentation performance. 
267
5.3 Analysis 
Utterance boundary
(occurs at 664 second in document VOM19980317_0900_1000, 
which is not a story boundary)
time
5 10-5-10
11 chain continuations:
W1[??], W2[??], W3[??], W4[???], W5[??],
W6[??], W7[??], W8[??], W9[??], W10[??], W11[??]
15-15
W 1
5
[??
]
W 1
6
[??
]
W 1
7
[??
?]
W 1
8
[??
]
W 1
9
[??
]
W 2
0
[??
]
W 2
1
[??
]
W 1
2
[??
]
W 1
3
[??
]
W 1
4
[??
]
ts1 ts2 ts3 ts4 ts5 ts6 ts7te1te2te3
 
Figure 7: Lexical chain starts, ends and continuations 
in the proximity of a non-story boundary. Wi[xxxx] 
denotes the i-th Chinese word ?xxxx?. 
Figure 7 shows an utterance boundary that is a 
non-story boundary. There is a high concentra-
tion of chain starts and ends near the boundary 
which leads to a misclassification if we only 
combine chain starts and ends for segmentation. 
However, there are also a large number of chain 
continuations across the utterance boundary, 
which implies that a story boundary is less likely. 
The full combination gives the correct decision. 
Utterance boundary
(occurs at 2014 second in document 
VOM19980319_0900_1000, which is a story boundary)
time
10 201020
ts1 ts3te4te5te6 te1te2te3 ts2
6 chain continuations:
W1[???], W2[??], W3[???],
W4[??], W5[??, W6[??]
W 1
3
[?
??
??
]
W 1
4
[?
??
?]
W 1
5
[?
?]
W 1
2
[?
?]
W 1
1
[?
?]
W 1
0
[?
??
]
W 9
[?
?]
W 8
[?
?]
W 7
[?
?]
 
Figure 8: Lexical chain starts, ends and continuations 
in the proximity of a story boundary. 
Figure 8 shows another example where an ut-
terance boundary is misclassified as a non-story 
boundary when only the combination of lexical 
chain starts and ends are used. Incorporation of 
the chain continuation feature helps rectify the 
classification. 
From these two examples, we can see that the 
incorporation of chain continuation in our story 
segmentation framework can complement the 
features of chain starts and ends.  In both exam-
ples above, the number of chain continuations 
plays a crucial role in correct identification of a 
story boundary. 
6 Conclusions 
We have presented a Bayesian decision frame-
work that performs automatic story segmentation 
based on statistical modeling of one or more lex-
ical chain features, including lexical chain starts, 
continuations and ends. Experimentation shows 
that the Bayesian decision framework is superior 
to the use of likelihoods for segmentation. We 
also experimented with a variety of scoring crite-
ria, involving likelihood ratio tests of a single 
feature (i.e. lexical chain starts, continuations or 
ends), their pair-wise combinations, as well as 
the full combination of all three features. Lexical 
chain starts/ends are modeled statistically with 
the Weibull and normal distributions for story 
boundaries and non-boundaries. The normal dis-
tribution is used for lexical chain continuations. 
Full combination of all lexical chain features 
gave the best performance (F1=0.6356). Model-
ing chain continuations contribute significantly 
towards segmentation performance. 
Acknowledgments 
This work is affiliated with the CUHK MoE-
Microsoft Key Laboratory of Human-centric Compu-
ting and Interface Technologies. We would also like 
to thank Professor Mari Ostendorf for suggesting the 
use of continuing chains and Mr. Kelvin Chan for 
providing information about his previous work. 
References  
Chan, S. K. et al 2007. ?Modeling the Statistical Be-
haviour of Lexical Chains to Capture Word Cohe-
siveness for Automatic Story Segmentation?, Proc. 
of INTERSPEECH-2007.  
Hearst, M. A. 1997. ?TextTiling: Segmenting Text 
into Multiparagraph Subtopic Passages?, Computa-
tional Linguistics, 23(1), pp. 33?64. 
Hirst, G. and St-Onge, D. 1998. ?Lexical chains as 
representations of context for the detection and 
correction of malapropisms?, WordNet: An Elec-
tronic Lexical Database, pp. 305?332. 
Morris, J. and Hirst, G. 1991. ?Lexical cohesion com-
puted by thesaural relations as an indicator of the 
structure of text?, Computational Linguistics, 
17(1), pp. 21?48. 
Reynar, J.C. 1999, ?Statistical models for topic seg-
mentation?, Proc. 37th annual meeting of the ACL, 
pp. 357?364. 
Stokes, N. 2004. Applications of Lexical Cohesion 
Analysis in the Topic Detection and Tracking Do-
main, PhD thesis, University College Dublin.  
Yamron, J.P. et al 1998, ?A hidden Markov model 
approach to text segmentation and event tracking?, 
Proc. ICASSP 1998, pp. 333?336. 
268
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 527?531,
Dublin, Ireland, August 23-24, 2014.
SeemGo: Conditional Random Fields Labeling and Maximum Entropy
Classification for Aspect Based Sentiment Analysis
Pengfei Liu and Helen Meng
Human-Computer Communications Laboratory
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong, Hong Kong SAR, China
{pfliu,hmmeng}@se.cuhk.edu.hk
Abstract
This paper describes our SeemGo sys-
tem for the task of Aspect Based Sen-
timent Analysis in SemEval-2014. The
subtask of aspect term extraction is cast
as a sequence labeling problem modeled
with Conditional Random Fields that ob-
tains the F-score of 0.683 for Laptops and
0.791 for Restaurants by exploiting both
word-based features and context features.
The other three subtasks are solved by the
Maximum Entropy model, with the occur-
rence counts of unigram and bigram words
of each sentence as features. The sub-
task of aspect category detection obtains
the best result when applying the Boosting
method on the Maximum Entropy model,
with the precision of 0.869 for Restau-
rants. The Maximum Entropy model also
shows good performance in the subtasks
of both aspect term and aspect category
polarity classification.
1 Introduction
In this paper, we present the SeemGo system de-
veloped for the task of Aspect Based Sentiment
Analysis in SemEval-2014. The task consists of
four subtasks: (1) aspect term extraction (iden-
tify particular aspects of a given entity, e.g., lap-
top, restaurant, etc.); (2) aspect category detection
(detect the category of a given sentence, e.g., food,
service for a restaurant, etc.), (3) aspect term po-
larity, and (4) aspect category polarity. The po-
larity of each aspect term or aspect category in-
cludes positive, negative, neutral or conflict (i.e.,
both positive and negative).
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
In the SeemGo system, the subtask of aspect
term extraction is implemented with the CRF
model that shows good performance by integrat-
ing both word-based features and context features.
The other subtasks of aspect category detection,
aspect term/category polarity classification are all
developed with the MaxEnt model with the occur-
rence counts of unigram and bigram words of each
sentence as features. Experimental results show
that MaxEnt obtains good performance in all the
three subtasks. For the subtask of aspect cate-
gory detection, MaxEnt obtains even better perfor-
mance when combined with the Boosting method.
The rest of this paper is organized as fol-
lows: Section 2 discusses related work; Section 3
presents the architecture and the underlying mod-
els of the SeemGo system as well as the experi-
mental results. We summarize the paper and pro-
pose future work in Section 4.
2 Related Work
The subtask of aspect term extraction is quite
similar with Noun Phrase Chunking (NPC) (Sha
and Pereira, 2003) and Named Entity Recognition
(NER) (Finkel et al., 2005). NPC recognizes noun
phrases from sentences, while NER extracts a set
of entities such as Person, Place, and Organiza-
tion. Both NPC and NER are sequential learn-
ing problems and they are typically modelled by
sequence models such as Hidden Markov Model
(HMM) and CRF (Finkel et al., 2005).
For the task of aspect term extraction, some re-
lated papers also model it with sequence models.
Jin et al. (2009) proposed an HMM-based frame-
work to extract product entities and associated
opinion orientations by integrating linguistic fea-
tures such as part-of-speech tag, lexical patterns
and surrounding words/phrases. Choi et al. (2005)
proposed a hybrid approach using both CRF and
extraction patterns to identify sources of opinions
in text. Jakob and Gurevych (2010) described a
527
CRF-based approach for the opinion target extrac-
tion problem in both single- and cross-domain set-
tings. Shariaty and Moghaddam (2011) used CRF
for the task of identifying aspects, aspect usages
and opinions in review sentences by making use
of labeled dataset on aspects, opinions as well as
background words in the sentences.
The task of aspect category detection is essen-
tially a text classification problem, for which many
techniques exist. Joachims (1998) explored the
use of Support Vector Machines (SVM) for text
categorization and obtained good performance
due to their ability to generalize well in high-
dimensional feature spaces. Nigam et al. (1999)
proposed the MaxEnt model for document clas-
sification by estimating the conditional distribu-
tion of the class variable give the document, and
showed that MaxEnt is significantly better that
Naive Bayes on some datasets.
For polarity classification, Pang et al. (2002)
conducted experiments on movie reviews and
showed that standard machine learning techniques
(e.g., Naive Bayes, SVM and MaxEnt) outperform
human-produced baselines.
3 The SeemGo System
We use the CRF model (Lafferty et al., 2001) for
the subtask of aspect term extraction, and adopt
the MaxEnt model for the other three subtasks
with the vectors of word count as features. Each
entry in the vector represents the occurrence count
of each unigram or bigram words in the sentence.
Figure 1 shows the architecture and the MaxEnt
and CRF models of the SeemGo system. The la-
bel is denoted in lowercase (e.g. y for sentiment),
while word count, label sequence and word se-
quence are vectors, denoted in bold lowercase (e.g.
y for label sequence). We developed the SeemGo
system in Java based on the MALLET Toolkit
(McCallum, 2002) for MaxEnt and the Stanford
CRFClassifier(Finkel et al., 2005) for CRF.
3.1 Background
3.1.1 Maximum Entropy Classifier
The MaxEnt model defines the conditional distri-
bution of the class (y) given an observation vector
x as the exponential form in Formula 1:
P(y|x) =
1
Z(x)
exp
(
K?
k=1
?
k
f
k
(x, y)
)
(1)
?? 
?1 word count  
?1 label MaxEnt P(?|?) ? label 
x word count 
Train Predict ?? word count 
??  label 
Transform 
(a) MaxEnt model for label classification  
I?ve been to several places for Dim Sum and this has got to be the WORST. 
Test sentence: 
Training Set 
?? 
?1 word sequence 
?1 label sequence CRF P(?|x) ? label sequence 
? word sequence 
Train Predict ?? word sequence 
?? label sequence 
Transform 
(b) CRF model for sequence labeling  
I?ve been to several places for Dim Sum and this has got to be the WORST. 
Test sentence: 
Training Set 
Figure 1: The Architecture, the MaxEnt and CRF
Models of the SeemGo System.
where ?
k
is a weight parameter to be estimated for
the corresponding feature function f
k
(x, y), and
Z(x) is a normalizing factor over all classes to en-
sure a proper probability. K is the total number of
feature functions.
3.1.2 Conditional Random Fields
CRF is an extension to the MaxEnt model for han-
dling sequence data. The linear-chain CRF is a
special case of CRF that obeys the Markov prop-
erty between its neighbouring labels. Following
McCallum and Li (2003), Formula 2 defines the
linear-chain CRF: y = {y
t
}
T
t=1
, x = {x
t
}
T
t=1
are
label sequence and observation sequence respec-
tively, and there are K arbitrary feature functions
{f
k
}
1?k?K
and the corresponding weight param-
eters {?
k
}
1?k?K
. Z(x) is a normalizing factor
over all label sequences.
P (y|x) =
1
Z(x)
exp
(
T?
t=1
K?
k=1
?
k
f
k
(y
t
, y
t?1
,x, t)
)
(2)
In the labeling phase, the Viterbi decoding algo-
rithm is applied to find the best label sequence y?
for the observation sequence x.
3.2 Subtask 1: Aspect Term Extraction
The datasets (Laptops and Restaurants) are pro-
vided in XML format, with each sentence and its
annotations consisting of a training instance. For
each instance, SeemGo first transform the sen-
tence into a word sequence x, and converts the cor-
responding annotations into the label sequence y.
SeemGo then learns a CRF model P (y|x) based
on the N the training instances {(x
n
,y
n
)}
N
n=1
.
528
3.2.1 IOB Labeling
Since an aspect term can contain multiple words
(e.g., hard disk), we define the label B-TERM
for the beginning of an aspect term, the label I-
TERM for the subsequent inside words or end
word of an aspect term and the label O for all other
words. This definition follows the Inside, Out-
side, Beginning (IOB) labeling scheme (Ramshaw
and Marcus, 1999). The subtask 1 can be viewed
as a sequence labeling problem by labeling each
word either as B-TERM, I-TERM or O. Figure
2 shows two example sentences labeled with the
IOB2 scheme
1
.
The hard disk is very noisy. 
O B-TERM I-TERM O O O 
I liked the service and  the staff. 
O O O B-TERM O O B-TERM 
Figure 2: Example Sentences with IOB2 Labels.
3.2.2 Features for the CRF Model
In CRF, features typically refer to feature func-
tions {f
k
}, which can be arbitrary functions. In
text applications, CRF features are typically bi-
nary (Sutton and McCallum, 2012). As an exam-
ple for ?virus protection?, a binary feature func-
tion may have value 1 if and only if the label for
?virus? is B-TERM and the current word ?protec-
tion? has the suffix of ?tion?, and otherwise 0.
Similar to the features used in Finkel et al. (2005)
for the NER task, Table 1 summarizes the features
for the aspect term extraction task. We call the fea-
tures derived from the current word word-based
features such as w
id
, w
character
, and the features
from the surrounding words and the previous label
the contex features (context).
We consider the sentence ?I?ve been to several
places for Dim Sum and this has got to be the
WORST.? as an example to explain why we choose
these features: (a) word-based features: the word
?Sum? is located in the middle of the sentence,
with the first character capitalized. (b) context fea-
tures: the previous word ?Dim? is also capitalized
in the first character and the label of ?Dim? is as-
sumed to be ?B-TERM?. By combining the word-
based features and the context features, the Viterbi
decoding algorithm will then label ?Sum? as ?I-
TERM? with high degree of confidence, which is
1
With IOB2, every aspect term begins with the B label.
a part of the multi-word term ?Dim Sum?, instead
of a mathematical function in some other context.
Table 1: Features for the CRF Model.
Feature Description
w
id
word identity
w
character
whether the word characters are capital-
ized, hyphenated, numeric, e.g., built-in
camera, BIOS, Dim Sum, Windows 7
w
location
word index in the word sequence x
w
ngram
n-gram character sequences of each
word with maximum length of 6, includ-
ing prefixes and suffixes, e.g., ?tion? in
specification, navigation
context
current wordw
t
, its neighbouring words
(w
t?2
,...,w
t+2
) and previous label y
t?1
w
pos
part-of-speech tag of each word
3.2.3 Experimental Results
We trained the CRF model with different fea-
ture set on the training set provided by the Se-
mEval2014 organizers, and reported the experi-
mental results on the testing set by the evaluation
tool eval.jar. The detailed experimental results are
listed in Table 2. The basic feature set consists of
w
id
,w
character
andw
location
. The results from one
of the best systems on each dataset are also listed,
marked with the star (*).
Table 2: Experimental Results on Different Fea-
ture Set for Aspect Term Extraction.
Feature Set Precision Recall F-score
Lap
basic
0.780
(263/337)
0.402
(263/654)
0.531
basic+ w
ngram
0.781
(375/480)
0.573
(375/654)
0.661
(+0.13)
basic+ w
context
0.827
(296/358)
0.453
(296/654)
0.585
(+0.054)
basic+w
ngram
+
context
0.830
(380/458)
0.581
(380/654)
0.683
(+0.152)
basic+w
ngram
+
context+ w
pos
0.837
(365/436)
0.558
(365/654)
0.670
(-0.013)
IHS RD Belarus* 0.848 0.665 0.746
Res
basic
0.862
(692/803)
0.610
(692/1134)
0.715
basic+ w
ngram
0.838
(804/959)
0.709
(804/1134)
0.768
(+0.053)
basic+ w
context
0.856
(704/822)
0.621
(704/1134)
0.720
(+0.05)
basic+w
ngram
+
context
0.865
(827/956)
0.729
(827/1134)
0.791
(+0.076)
basic+w
ngram
+
context+ w
pos
0.870
(806/926)
0.711
(806/1134)
0.783
(-0.08)
XRCE* 0.909 0.818 0.840
We have the following observations:
(1) Compared with using only the basic features,
adding the feature of w
n?gram
contributes the
529
greatest performance improvement, with the
absolute increase of F-score by 13% for Lap-
tops and 5.3% for Restaurants; while adding
the w
context
feature improves the F-score by
around 5% for both datasets.
(2) Combining the word-based features (basic
and w
ngram
) and the context-based features
(w
context
) lead to the best performance for
both datasets in terms of recall and F-score.
(3) The POS tags lead to a decrease in both re-
call and F-score, with the absolute decrease
of F-score by 1.3% for Laptops and 8% for
Restaurants. The same observation is also re-
ported by Tkachenko and Simanovsky (2012)
for NER.
3.3 Subtask 3: Aspect Category Detection
We encode each sentence as a feature vector x
with each entry representing occurrence count of
each unigram word and bigram words (i.e., word
count). All words are lowercased, while keeping
the stopwords as most sentences in the datasets are
short. Using the provided training set, We trained
a MaxEnt classifier (ME) P (y|x) with a Gaussian
prior variance of 20 to prevent overfitting.
We also tried the Bagging (Breiman, 1996) on
MaxEnt (BaggingME) and the Boosting (Freund
and Schapire, 1996) on MaxEnt (BoostME). Table
3 shows the experimental results on the provided
testing set. It shows that the Boosting method on
MaxEnt improves both precision and recall as well
as the F-score by 1.1%. The best evaluation result
is by the NRC-Canada team.
Table 3: Performance of Different Classifiers for
Aspect Category Detection.
Classifier Precision Recall F-score
ME
0.858
(686/800)
0.669
(686/1025)
0.752
BagME
0.843
(674/800)
0.658
(674/1025)
0.739
BoostME
0.869
(695/800)
0.678
(695/1025)
0.762
Best* 0.910 0.862 0.886
3.4 Subtask 2 & 4: Aspect Term & Category
Polarity Classification
Similar to subtask-3, we also used MaxEnt for the
subtasks of 2 and 4, with word count as features.
For category polarity classification, we count the
words from both the sentence and the category
name. For example, we count the sentence ?The
Dim Sum is delicious.? and its category ?Food?
as features. This improves performance compared
with counting the sentence only.
Table 4 shows the accuracy of each classifier for
the subtasks of 2 and 4 on Laptops and Restau-
rants, including the best results from NRC-Canada
(a) and DCU (b). In both datasets, the distributions
of aspect term/category polarities are very imbal-
anced with very few sentences on conflict but with
most sentences on positive or negative. This leads
to very low classification performance for the con-
flict class, with the F-score less than 0.2. In this
case, the Boosting method does not necessarily
improve the performance.
Table 4: Accuracy of Different Classifiers for As-
pect Term & Category Polarity Classification.
Classifier
Term
Category
Laptops Restaurants (Restaurants)
ME
0.648
(424/654)
0.729
(827/1134)
0.752
(771/1025)
BagME
0.635
(415/654)
0.732
(830/1134)
0.752
(771/1025)
BoostME
0.642
(420/654)
0.730
(828/1134)
0.747
(766/1025)
Best*
0.705 (a,b)
(461/654)
0.810 (b)
(918/1134)
0.829 (a)
(850/1025)
3.5 Evaluation Ranks
Table 5 shows the official ranks (and the new ranks
in braces of the revised version after evaluation) of
the SeemGo system on the two datasets. The eval-
uation metrics are Precision, Recall and F-score
for the subtasks of 1 and 3, and Accuracy (Acc)
for the subtasks of 2 and 4.
Table 5: Ranks of SeemGo on the Constrained
Run (Using only the Provided Datasets).
Subtask Precision Recall F-score Acc
Lap
1 4 12 (8) 8 (4) -
2 - - - 12 (6)
Res
1 3 11 (7) 5 -
2 - - - 8 (6)
3 3 (2) 12 8 (7) -
4 - - - 4
4 Conclusions
This paper presents the architecture, the CRF
and MaxEnt models of our SeemGo system for
the task of Aspect Based Sentiment Analysis in
530
SemEval-2014. For the subtask of aspect term ex-
traction, CRF is trained with both the word-based
features and the context features. For the other
three subtasks, MaxEnt is trained with the fea-
tures of the occurrence counts of unigram and bi-
gram words in the sentence. The subtask of aspect
category detection obtains the best performance
when applying the Boosting method on MaxEnt.
MaxEnt also shows good average accuracy for po-
larity classification, but obtains low performance
for the conflict class due to very few training sen-
tences.This leaves us the future work to improve
classification performance for imbalanced datasets
(He and Garcia, 2009).
Acknowledgements
We thank the organizers for their hard work in or-
ganizing this evaluation, and the two anonymous
reviewers for their helpful comments.
References
Leo Breiman. 1996. Bagging predictors. Machine
learning, 24(2):123?140.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 355?362.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370.
Yoav Freund and Robert E Schapire. 1996. Experi-
ments with a new boosting algorithm. In Interna-
tional Conference on Machine Learning, volume 96,
pages 148?156.
Haibo He and Edwardo A Garcia. 2009. Learning
from imbalanced data. Knowledge and Data Engi-
neering, IEEE Transactions on, 21(9):1263?1284.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 1035?1045.
Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009. A
novel lexicalized HMM-based learning framework
for web opinion mining. In Proceedings of the In-
ternational Conference on Machine Learning, pages
465?472. Citeseer.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. Springer.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 188?191.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
Kamal Nigam, John Lafferty, and Andrew McCallum.
1999. Using maximum entropy for text classifica-
tion. In IJCAI-99 workshop on machine learning
for information filtering, volume 1, pages 61?67.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157?176. Springer.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 134?141.
Shabnam Shariaty and Samaneh Moghaddam. 2011.
Fine-grained opinion mining using conditional ran-
dom fields. In Data Mining Workshops (ICDMW),
2011 IEEE 11th International Conference on, pages
109?114. IEEE.
Charles Sutton and Andrew McCallum. 2012. An in-
troduction to conditional random fields. Founda-
tions and Trends in Machine Learning, 4(4):267?
373.
Maksim Tkachenko and Andrey Simanovsky. 2012.
Named entity recognition: Exploring features. In
Proceedings of KONVENS, volume 2012, pages
118?127.
531
