Supervised and unsupervised PCFG adaptation to novel domains
Brian Roark and Michiel Bacchiani
AT&T Labs - Research
{roark,michiel}@research.att.com
Abstract
This paper investigates adapting a lexicalized
probabilistic context-free grammar (PCFG) to
a novel domain, using maximum a posteriori
(MAP) estimation. The MAP framework is gen-
eral enough to include some previous model
adaptation approaches, such as corpus mixing in
Gildea (2001), for example. Other approaches
falling within this framework are more effec-
tive. In contrast to the results in Gildea (2001),
we show F-measure parsing accuracy gains of as
much as 2.5% for high accuracy lexicalized pars-
ing through the use of out-of-domain treebanks,
with the largest gains when the amount of in-
domain data is small. MAP adaptation can also be
based on either supervised or unsupervised adap-
tation data. Even when no in-domain treebank is
available, unsupervised techniques provide a sub-
stantial accuracy gain over unadapted grammars,
as much as nearly 5% F-measure improvement.
1 Introduction
A fundamental concern for nearly all data-driven ap-
proaches to language processing is the sparsity of la-
beled training data. The sparsity of syntactically anno-
tated corpora is widely remarked upon, and some recent
papers present approaches to improving performance in
the absence of large amounts of annotated training data.
Johnson and Riezler (2000) looked at adding features to a
maximum entropy model for stochastic unification-based
grammars (SUBG), from corpora that are not annotated
with the SUBG, but rather with simpler treebank annota-
tions for which there are much larger treebanks. Hwa (2001)
demonstrated how active learning techniques can reduce
the amount of annotated data required to converge on the
best performance, by selecting from among the candidate
strings to be annotated in ways which promote more in-
formative examples for earlier annotation. Hwa (1999) and
Gildea (2001) looked at adapting parsing models trained on
large amounts of annotated data from outside of the domain
of interest (out-of-domain), through the use of a relatively
small amount of in-domain annotated data. Hwa (1999)
used a variant of the inside-outside algorithm presented
in Pereira and Schabes (1992) to exploit a partially labeled
out-of-domain treebank, and found an advantage to adapta-
tion over direct grammar induction. Gildea (2001) simply
added the out-of-domain treebank to his in-domain training
data, and derived a very small benefit for his high accuracy,
lexicalized parser, concluding that even a large amount of
out-of-domain data is of little use for lexicalized parsing.
Statistical model adaptation based on sparse in-domain
data, however, is neither a new problem nor unique to pars-
ing. It has been studied extensively by researchers work-
ing on acoustic modeling for automatic speech recognition
(ASR) (Legetter and Woodland, 1995; Gauvain and Lee,
1994; Gales, 1998; Lamel et al, 2002). One of the meth-
ods that has received much attention in the ASR literature is
maximum a posteriori (MAP) estimation (Gauvain and Lee,
1994). In MAP estimation, the parameters of the model are
considered to be random variables themselves with a known
distribution (the prior). The prior distribution and the max-
imum likelihood distribution based on the in-domain obser-
vations then give a posterior distribution over the parame-
ters, from which the mode is selected. If the amount of in-
domain (adaptation) data is large, the mode of the posterior
distribution is mostly defined by the adaptation sample; if
the amount of adaptation data is small, the mode will nearly
coincide with the mode of the prior distribution. The intu-
ition behind MAP estimation is that once there are sufficient
observations, the prior model need no longer be relied upon.
Bacchiani and Roark (2003) investigated MAP adapta-
tion of n-gram language models, in a way that is straight-
forwardly applicable to probabilistic context-free grammars
(PCFGs). Indeed, this approach can be used for any gen-
erative probabilistic model, such as part-of-speech taggers.
In their language modeling approach, in-domain counts are
mixed with the out-of-domain model, so that, if the num-
ber of observations within the domain is small, the out-
of-domain model is relied upon, whereas if the number of
observations in the domain is high, the model will move
toward a Maximum Likelihood (ML) estimate on the in-
domain data alone. The case of a parsing model trained via
relative frequency estimation is identical: in-domain counts
can be combined with the out-of-domain model in just such
a way. We will show below that weighted count merging
is a special case of MAP adaptation; hence the approach
of Gildea (2001) cited above is also a special case of MAP
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 126-133
                                                         Proceedings of HLT-NAACL 2003
adaptation, with a particular parameterization of the prior.
This parameterization is not necessarily the one that opti-
mizes performance.
In the next section, MAP estimation for PCFGs is pre-
sented. This is followed by a brief presentation of the PCFG
model that is being learned, and the parser that is used
for the empirical trials. We will present empirical results
for multiple MAP adaptation schema, both starting from
the Penn Wall St. Journal treebank and adapting to the
Brown corpus, and vice versa. We will compare our su-
pervised adaptation performance with the results presented
in Gildea (2001). In addition to supervised adaptation, i.e.
with a manually annotated treebank, we will present results
for unsupervised adaptation, i.e. with an automatically an-
notated treebank. We investigate a number of unsupervised
approaches, including multiple iterations, increased sample
sizes, and self-adaptation.
2 MAP estimation
In the maximum a posteriori estimation framework de-
scribed in detail in Gauvain and Lee (1994), the model pa-
rameters ? are assumed to be a random vector in the space
?. Given an observation sample x, the MAP estimate is ob-
tained as the mode of the posterior distribution of ? denoted
as g(. | x)
?MAP = argmax
?
g(? | x) = argmax
?
f(x | ?)g(?) (1)
In the case of n-gram model adaptation, as discussed in
Bacchiani and Roark (2003), the objective is to estimate
probabilities for a discrete distribution across words, en-
tirely analogous to the distribution across mixture compo-
nents within a mixture density, which is a common use for
MAP estimation in ASR. A practical candidate for the prior
distribution of the weights ?1, ?2, ? ? ? , ?K , is its conjugate
prior, the Dirichlet density,
g(?1, ?2, ? ? ? , ?K | ?1, ?2, ? ? ? , ?K) ?
K?
i=1
??i?1i (2)
where ?i > 0 are the parameters of the Dirichlet distribu-
tion. With such a prior, if the expected counts for the i-th
component is denoted as ci, the mode of the posterior distri-
bution is obtained as
??i =
(?i ? 1) + ci
?K
k=1(?k ? 1) +
?K
k=1 ck
1 ? i ? K. (3)
We can use this formulation to estimate the posterior, but we
must still choose the parameters of the Dirichlet. First, let
us introduce some notation. A context-free grammar (CFG)
G = (V, T, P, S?), consists of a set of non-terminal symbols
V , a set of terminal symbols T , a start symbol S? ? V , and
a set of rule productions P of the form: A ? ?, where
A ? V and ? ? (V ? T )?. A probabilistic context-free
grammar (PCFG) is a CFG with a probability assigned to
each rule, such that the probabilities of all rules expanding a
given non-terminal sum to one; specifically, each right-hand
side has a probability given the left-hand side of the rule1.
LetA denote the left-hand side of a production, and ?i the
i-th possible expansion of A. Let the probability estimate
for the production A ? ?i according to the out-of-domain
model be denoted as P?(?i | A) and let the expected adapta-
tion counts be denoted as c(A ? ?i). Then the parameters
of the prior distribution for left-hand side A are chosen as
?Ai = ?AP?(?i | A) + 1 1 ? i ? K. (4)
where ?A is the left-hand side dependent prior weighting pa-
rameter. This choice of prior parameters defines the MAP
estimate of the probability of expansion ?i from the left-
hand side A as
P?(?i | A) =
?AP?(?i | A) + c(A? ?i)
?A +
?K
k=1 c(A? ?k)
1 ? i ? K. (5)
Note that the MAP estimates with this parameterization re-
duce to the out-of-domain model parameters in the absence
of adaptation data.
Each left-hand side A has its own prior distribution, pa-
rameterized with ?A. This presents an over-parameterization
problem. We follow Gauvain and Lee (1994) in adopt-
ing a parameter tying approach. As pointed out in
Bacchiani and Roark (2003), two methods of parameter ty-
ing, in fact, correspond to two well known model mixing
approaches, namely count merging and model interpolation.
Let P? and c? denote the probabilities and counts from the
out-of-domain model, and let P and c denote the probabili-
ties and counts from the adaptation model (i.e. in-domain).
2.1 Count Merging
If the left-hand side dependent prior weighting parameter is
chosen as
?A = c?(A)
?
?
, (6)
the MAP adaptation reduces to count merging, scaling the
out-of-domain counts with a factor ? and the in-domain
counts with a factor ?:
P?(?i | A) =
c?(A)?? P?(?i | A) + c(A? ?i)
c?(A)?? + c(A)
=
?c?(A? ?i) + ?c(A? ?i)
?c?(A) + ?c(A)
(7)
1An additional condition for well-formedness is that the PCFG
is consistent or tight, i.e. there is no probability mass lost to in-
finitely large trees. Chi and Geman (1998) proved that this con-
dition is met if the rule probabilities are estimated using relative
frequency estimation from a corpus.
2.2 Model Interpolation
If the left-hand side dependent prior weighting parameter is
chosen as
?A =
{
c(A) ?1?? , 0 < ? < 1 if c(A) > 0
1 otherwise
(8)
the MAP adaptation reduces to model interpolation using
interpolation parameter ?:
P?(?i | A) =
c(A) ?1?? P?(?i | A) + c(A? ?i)
c(A) ?1?? + c(A)
=
?
1?? P?(?i | A) + P(?i | A)
?
1?? + 1
= ?P?(?i | A) + (1? ?)P(?i | A) (9)
2.3 Other Tying Candidates
While we will not be presenting empirical results for other
parameter tying approaches in this paper, we should point
out that the MAP framework is general enough to allow
for other schema, which could potentially improve perfor-
mance over simple count merging and model interpolation
approaches. For example, one may choose a more com-
plicated left-hand side dependent prior weighting parameter
such as
?A =
{
c(A) ?1?? , 0 < ? < 1 if c?(A) c(A) > ?
c?(A)?? otherwise
(10)
for some threshold ?. Such a schema may do a better job
of managing how quickly the model moves away from the
prior, particularly if there is a large difference in the respec-
tive sizes of the in-domain and out-of domain corpora. We
leave the investigation of such approaches to future research.
Before providing empirical results on the count merging
and model interpolation approaches, we will introduce the
parser and parsing models that were used.
3 Grammar and parser
For the empirical trials, we used a top-down, left-to-right
(incremental) statistical beam-search parser (Roark, 2001a;
Roark, 2003). We refer readers to the cited papers for de-
tails on this parsing algorithm. Briefly, the parser maintains
a set of candidate analyses, each of which is extended to
attempt to incorporate the next word into a fully connected
partial parse. As soon as ?enough? candidate parses have
been extended to the next word, all parses that have not
yet attached the word are discarded, and the parser moves
on to the next word. This beam search is parameterized
with a base beam parameter ?, which controls how many
or how few parses constitute ?enough?. Candidate parses
are ranked by a figure-of-merit, which promotes better can-
didates, so that they are worked on earlier. The figure-of-
merit consists of the probability of the parse to that point
times a look-ahead statistic, which is an estimate of how
much probability mass it will take to connect the parse with
the next word. It is a generative parser that does not require
any pre-processing, such as POS tagging or chunking. It has
been demonstrated in the above papers to perform compet-
itively on standard statistical parsing tasks with full cover-
age. Baseline results below will provide a comparison with
other well known statistical parsers.
The PCFG is a Markov grammar (Collins, 1997; Char-
niak, 2000), i.e. the production probabilities are estimated
by decomposing the joint probability of the categories on the
right-hand side into a product of conditionals via the chain
rule, and making a Markov assumption. Thus, for example,
a first order Markov grammar conditions the probability of
the category of the i-th child of the left-hand side on the cat-
egory of the left-hand side and the category of the (i-1)-th
child of the left-hand side. The benefits of Markov gram-
mars for a top-down parser of the sort we are using is de-
tailed in Roark (2003). Further, as in Roark (2001a; 2003),
the production probabilities are conditioned on the label of
the left-hand side of the production, as well as on features
from the left-context. The model is smoothed using standard
deleted interpolation, wherein a mixing parameter ? is esti-
mated using EM on a held out corpus, such that probability
of a production A ? ?, conditioned on j features from the
left context, Xj1 = X1 . . . Xj , is defined recursively as
P(A? ? | Xj1) = P(? | A,X
j
1) (11)
= (1? ?)P?(? | A,Xj1) + ?P(? | A,X
j?1
1 )
where P? is the maximum likelihood estimate of the condi-
tional probability. These conditional probabilities decom-
pose via the chain rule as mentioned above, and a Markov
assumption limits the number of previous children already
emitted from the left-hand side that are conditioned upon.
These previous children are treated exactly as other con-
ditioning features from the left context. Table 1 gives the
conditioning features that were used for all empirical trials
in this paper. There are different conditioning features for
parts-of-speech (POS) and non-POS non-terminals. Deleted
interpolation leaves out one feature at a time, in the reverse
order as they are presented in the table 1.
The grammar that is used for these trials is a PCFG that
is induced using relative frequency estimation from a trans-
formed treebank. The trees are transformed with a selec-
tive left-corner transformation (Johnson and Roark, 2000)
that has been flattened as presented in Roark (2001b). This
transform is only applied to left-recursive productions, i.e.
productions of the form A ? A?. The transformed trees
look as in figure 1. The transform has the benefit for a top-
down incremental parser of this sort of delaying many of
the parsing decisions until later in the string, without un-
duly disrupting the immediate dominance relationships that
provide conditioning features for the probabilistic model.
(a)
NP

NP

NP

NNP
Jim
bb
POS
?s
HHH
NN
dog
PPPP
PP
,
IN
with . . .
l
NP
(b)
NP

NNP
Jim
POS
?s
XXXXX
NP/NP

NN
dog
HHH
NP/NP
PP

IN
with . . .
l
NP
(c)
NP
      
NNP
Jim
!!!
POS
?s
l
NP/NP
NN
dog
``````
NP/NP
PP
,
IN
with . . .
l
NP
Figure 1: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner
representation; and (c) a flat structure that is unambiguously equivalent to (b)
Features for non-POS left-hand sides
0 Left-hand side (LHS)
1 Last child of LHS
2 2nd last child of LHS
3 3rd last child of LHS
4 Parent of LHS (PAR)
5 Last child of PAR
6 Parent of PAR (GPAR)
7 Last child of GPAR
8 First child of conjoined category
9 Lexical head of current constituent
Features for POS left-hand sides
0 Left-hand side (LHS)
1 Parent of LHS (PAR)
2 Last child of PAR
3 Parent of PAR (GPAR)
4 POS of C-Commanding head
5 C-Commanding lexical head
6 Next C-Commanding lexical head
Table 1: Conditioning features for the probabilistic CFG
used in the reported empirical trials
The parse trees that are returned by the parser are then de-
transformed to the original form of the grammar for evalua-
tion2.
For the trials reported in the next section, the base beam
parameter is set at ? = 10. In order to avoid being pruned, a
parse must be within a probability range of the best scoring
parse that has incorporated the next word. Let k be the num-
ber of parses that have incorporated the next word, and let p?
be the best probability from among that set. Then the prob-
ability of a parse must be above p?k
3
10? to avoid being pruned.
2See Johnson (1998) for a presentation of the transform/de-
transform paradigm in parsing.
4 Empirical trials
The parsing models were trained and tested on treebanks
from the Penn Treebank II. For the Wall St. Journal portion,
we used the standard breakdown: sections 2-21 were kept
training data; section 24 was held-out development data; and
section 23 was for evaluation. For the Brown corpus por-
tion, we obtained the training and evaluation sections used
in Gildea (2001). In that paper, no held-out section was used
for parameter tuning3, so we further partitioned the training
data into kept and held-out data. The sizes of the corpora
are given in table 2, as well as labels that are used to refer to
the corpora in subsequent tables.
4.1 Baseline performance
The first results are for parsing the Brown corpus. Table
3 presents our baseline performance, compared with the
Gildea (2001) results. Our system is labeled as ?MAP?. All
parsing results are presented as labeled precision and recall.
Whereas Gildea (2001) reported parsing results just for sen-
tences of length less than or equal to 40, our results are for
all sentences. The goal is not to improve upon Gildea?s
parsing performance, but rather to try to get more benefit
from the out-of-domain data. While our performance is 0.5-
1.5 percent better than Gildea?s, the same trends hold ? low
eighties in accuracy when using the Wall St. Journal (out-of-
domain) training; mid eighties when using the Brown corpus
training. Notice that using the Brown held out data with the
Wall St. Journal training improved precision substantially.
Tuning the parameters on in-domain data can make a big
difference in parser performance. Choosing the smoothing
parameters as Gildea did, based on the distribution within
the corpus itself, may be effective when parsing within the
same distribution, but appears less so when using the tree-
bank for parsing outside of the domain.
3According to the author, smoothing parameters for his parser
were based on the formula from Collins (1999).
Corpus;Sect Used for Sentences Words
WSJ;2-21 Training 39,832 950,028
WSJ;24 Held out 1,346 32,853
WSJ;23 Eval 2,416 56,684
Brown;T Training 19,740 373,152
Brown;H Held out 2,078 40,046
Brown;E Eval 2,425 45,950
Table 2: Corpus sizes
System Training Heldout LR LP
Gildea WSJ;2-21 80.3 81.0
MAP WSJ;2-21 WSJ;24 81.3 80.9
MAP WSJ;2-21 Brown;H 81.6 82.3
Gildea Brown;T,H 83.6 84.6
MAP Brown;T Brown;H 84.4 85.0
Table 3: Parser performance on Brown;E, baselines. Note
that the Gildea results are for sentences ? 40 words in
length.
Table 4 gives the baseline performance on section 23 of
the WSJ Treebank. Note, again, that the Gildea results are
for sentences ? 40 words in length, while all others are for
all sentences in the test set. Also, Gildea did not report per-
formance of a Brown corpus trained parser on the WSJ. Our
performance under that condition is not particularly good,
but again using an in-domain held out set for parameter tun-
ing provided a substantial increase in accuracy, somewhat
more in terms of precision than recall. Our baseline results
for a WSJ section 2-21 trained parser are slightly better than
the Gildea parser, at more-or-less the same level of perfor-
mance as Charniak (1997) and Ratnaparkhi (1999), but sev-
eral points below the best reported results on this task.
4.2 Supervised adaptation
Table 5 presents parsing results on the Brown;E test set for
models using both in-domain and out-of-domain training
data. The table gives the adaptation (in-domain) treebank
that was used, and the ?A that was used to combine the adap-
tation counts with the model built from the out-of-domain
treebank. Recall that ?c?(A) times the out-of-domain model
yields count merging, with ? the ratio of out-of-domain
to in-domain counts; and ?c(A) times the out-of-domain
model yields model interpolation, with ? the ratio of out-of-
domain to in-domain probabilities. Gildea (2001) merged
the two corpora, which just adds the counts from the out-of-
domain treebank to the in-domain treebank, i.e. ? = 1.
This resulted in a 0.25 improvement in the F-measure. In
our case, combining the counts in this way yielded a half
a point, perhaps because of the in-domain tuning of the
smoothing parameters. However, when we optimize ? em-
pirically on the held-out corpus, we can get nearly a full
point improvement. Model interpolation in this case per-
System Training Heldout LR LP
MAP Brown;T Brown;H 76.0 75.4
MAP Brown;T WSJ;24 76.9 77.1
Gildea WSJ;2-21 86.1 86.6
MAP WSJ;2-21 WSJ;24 86.9 87.1
Charniak (1997) WSJ;2-21 WSJ;24 86.7 86.6
Ratnaparkhi (1999) WSJ;2-21 86.3 87.5
Collins (1999) WSJ;2-21 88.1 88.3
Charniak (2000) WSJ;2-21 WSJ;24 89.6 89.5
Collins (2000) WSJ;2-21 89.6 89.9
Table 4: Parser performance on WSJ;23, baselines. Note
that the Gildea results are for sentences ? 40 words in
length. All others include all sentences.
forms nearly identically to count merging.
Adaptation to the Brown corpus, however, does not ad-
equately represent what is likely to be the most common
adaptation scenario, i.e. adaptation to a consistent domain
with limited in-domain training data. The Brown corpus is
not really a domain; it was built as a balanced corpus, and
hence is the aggregation of multiple domains. The reverse
scenario ? Brown corpus as out-of-domain parsing model
and Wall St. Journal as novel domain ? is perhaps a more
natural one. In this direction, Gildea (2001) also reported
very small improvements when adding in the out-of-domain
treebank. This may be because of the same issue as with the
Brown corpus, namely that the optimal ratio of in-domain to
out-of-domain is not 1 and the smoothing parameters need
to be tuned to the new domain; or it may be because the new
domain has a million words of training data, and hence has
less use for out-of-domain data. To tease these apart, we par-
titioned the WSJ training data (sections 2-21) into smaller
treebanks, and looked at the gain provided by adaptation as
the in-domain observations grow. These smaller treebanks
provide a more realistic scenario: rapid adaptation to a novel
domain will likely occur with far less manual annotation of
trees within the new domain than can be had in the full Penn
Treebank.
Table 6 gives the baseline performance on WSJ;23, with
models trained on fractions of the entire 2-21 test set. Sec-
tions 2-21 contain approximately 40,000 sentences, and we
partitioned them by percentage of total sentences. From ta-
ble 6 we can see that parser performance degrades quite dra-
matically when there is less than 20,000 sentences in the
training set, but that even with just 2000 sentences, the sys-
tem outperforms one trained on the Brown corpus.
Table 7 presents parsing accuracy when a model trained
on the Brown corpus is adapted with part or all of the WSJ
training corpus. From this point forward, we only present
results for count merging, since model interpolation con-
sistently performed 0.2-0.5 points below the count merging
System Training Heldout Adapt ?A Baseline Adapted ?F
LR LP F LR LP F
Gildea WSJ;2-21 Brown;T,H c?(A) 83.6 84.6 84.1 83.9 84.8 84.35 0.25
MAP WSJ;2-21 Brown;H Brown;T c?(A) 84.4 85.0 84.7 84.9 85.6 85.25 0.55
MAP WSJ;2-21 Brown;H Brown;T 0.25?c(A) 84.4 85.0 84.7 85.4 85.9 85.65 0.95
MAP WSJ;2-21 Brown;H Brown;T 0.20c(A) 84.4 85.0 84.7 85.3 85.9 85.60 0.90
Table 5: Parser performance on Brown;E, supervised adaptation
System Training % Heldout LR LP
MAP WSJ;2-21 100 WSJ;24 86.9 87.1
MAP WSJ;2-21 75 WSJ;24 86.6 86.8
MAP WSJ;2-21 50 WSJ;24 86.3 86.4
MAP WSJ;2-21 25 WSJ;24 84.8 85.0
MAP WSJ;2-21 10 WSJ;24 82.6 82.6
MAP WSJ;2-21 5 WSJ;24 80.4 80.6
Table 6: Parser performance on WSJ;23, baselines
approach4. The ?A mixing parameter was empirically opti-
mized on the held out set when the in-domain training was
just 10% of the total; this optimization makes over a point
difference in accuracy. Like Gildea, with large amounts of
in-domain data, adaptation improved our performance by
half a point or less. When the amount of in-domain data
is small, however, the impact of adaptation is much greater.
4.3 Unsupervised adaptation
Bacchiani and Roark (2003) presented unsupervised MAP
adaptation results for n-gram models, which use the same
methods outlined above, but rather than using a manually
annotated corpus as input to adaptation, instead use an auto-
matically annotated corpus. Their automatically annotated
corpus was the output of a speech recognizer which used the
out-of-domain n-gram model. In our case, we use the pars-
ing model trained on out-of-domain data, and output a set
of candidate parse trees for the strings in the in-domain cor-
pus, with their normalized scores. These normalized scores
(posterior probabilities) are then used to give weights to the
features extracted from each candidate parse, in just the way
that they provide expected counts for an expectation maxi-
mization algorithm.
For the unsupervised trials that we report, we collected
up to 20 candidate parses per string5. We were interested in
investigating the effects of adaptation, not in optimizing per-
formance, hence we did not empirically optimize the mixing
parameter ?A for the new trials, so as to avoid obscuring the
effects due to adaptation alone. Rather, we used the best
4This is consistent with the results presented in
Bacchiani and Roark (2003), which found a small but con-
sistent improvement in performance with count merging versus
model interpolation for n-gram modeling.
5Because of the left-to-right, heuristic beam-search, the parser
does not produce a chart, rather a set of completed parses.
performing parameter from the supervised trials, namely
0.20c?(A). Since we are no longer limited to manually anno-
tated data, the amount of in-domain WSJ data that we can
include is essentially unlimited. Hence the trials reported go
beyond the 40,000 sentences in the Penn WSJ Treebank, to
include up to 5 times that number of sentences from other
years of the WSJ.
Table 8 shows the results of unsupervised adaptation as
we have described it. Note that these improvements are had
without seeing any manually annotated Wall St. Journal
treebank data. Using the approximately 40,000 sentences
in f2-21, we derived a 3.8 percent F-measure improvement
over using just the out of domain data. Going beyond the
size of the Penn Treebank, we continued to gain in accuracy,
reaching a total F-measure improvement of 4.2 percent with
200 thousand sentences, approximately 5 million words. A
second iteration with this best model, i.e. re-parsing the 200
thousand sentences with the adapted model and re-training,
yielded an additional 0.65 percent F-measure improvement,
for a total F-measure improvement of 4.85 percent over the
baseline model.
A final unsupervised adaptation scenario that we inves-
tigated is self-adaptation, i.e. adaptation on the test set it-
self. Because this adaptation is completely unsupervised,
thus does not involve looking at the manual annotations at
all, it can be equally well applied using the test set as the un-
supervised adaptation set. Using the same adaptation proce-
dure presented above on the test set itself, i.e. producing the
top 20 candidates from WSJ;23 with normalized posterior
probabilities and re-estimating, we produced a self-adapted
parsing model. This yielded an F-measure accuracy of 76.8,
which is a 1.1 percent improvement over the baseline.
5 Conclusion
What we have demonstrated in this paper is that maximum a
posteriori (MAP) estimation can make out-of-domain train-
ing data beneficial for statistical parsing. In the most likely
scenario ? porting a parser to a novel domain for which there
is little or no annotated data ? the improvements can be quite
large. Like active learning, model adaptation can reduce the
amount of annotation required to converge to a best level
of performance. In fact, MAP coupled with active learning
may reduce the required amount of annotation further.
There are a couple of interesting future directions for this
System % of ?A Baseline Adapted ?F
WSJ;2-21 LR LP F LR LP F
Gildea 100 c?(A) 86.1 86.6 86.35 86.3 86.9 86.60 0.25
MAP 100 0.20?c(A) 86.9 87.1 87.00 87.2 87.5 87.35 0.35
MAP 75 0.20?c(A) 86.6 86.8 86.70 87.1 87.3 87.20 0.50
MAP 50 0.20?c(A) 86.3 86.4 86.35 86.7 86.9 86.80 0.45
MAP 25 0.20?c(A) 84.8 85.0 84.90 85.3 85.5 85.40 0.50
MAP 10 0.20?c(A) 82.6 82.6 82.60 84.3 84.4 84.35 1.75
MAP 10 c?(A) 82.6 82.6 82.60 83.2 83.4 83.30 0.70
MAP 5 0.20?c(A) 80.4 80.6 80.50 83.0 83.1 83.05 2.55
Table 7: Parser performance on WSJ;23, supervised adaptation. All models use Brown;T,H as the out-of-domain treebank.
Baseline models are built from the fractions of WSJ;2-21, with no out-of-domain treebank.
Adaptation Iter- LR LP F- ?F
Sentences ation measure
0 0 76.0 75.4 75.70
4000 1 78.6 77.9 78.25 2.55
10000 1 78.9 78.0 78.45 2.75
20000 1 79.3 78.5 78.90 3.20
30000 1 79.7 78.9 79.30 3.60
39832 1 79.9 79.1 79.50 3.80
100000 1 79.7 79.2 79.45 3.75
200000 1 80.2 79.6 79.90 4.20
200000 2 80.6 80.5 80.55 4.85
Table 8: Parser performance on WSJ;23, unsupervised
adaptation. For all trials, the base training is Brown;T, the
held out is Brown;H plus the parser output for WSJ;24, and
the mixing parameter ?A is 0.20c?(A).
research. First, a question that is not addressed in this paper
is how to best combine both supervised and unsupervised
adaptation data. Since each in-domain resource is likely to
have a different optimal mixing parameter, since the super-
vised data is more reliable than the unsupervised data, this
becomes a more difficult, multi-dimensional parameter op-
timization problem. Hence, we would like to investigate au-
tomatic methods for choosing mixing parameters, such as
EM. Also, an interesting question has to do with choosing
which treebank to use for out-of-domain data. For a new
domain, is it better to choose as prior the balanced Brown
corpus, or rather the more robust Wall St. Journal treebank?
Perhaps one could use several out-of-domain treebanks as
priors. Most generally, one can imagine using k treebanks,
some in-domain, some out-of-domain, and trying to find the
best mixture to suit the particular task.
The conclusion in Gildea (2001), that out-of-domain tree-
banks are not particularly useful in novel domains, was pre-
mature. Instead, we can conclude that, just as in other sta-
tistical estimation problems, there are generalizations to be
had from these out-of-domain trees, providing more robust
estimates, especially in the face of sparse training data.
References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
language model adaptation. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the Fourteenth National Conference on Artificial Intelli-
gence, pages 598?603.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Conference of the North
American Chapter of the Association for Computational
Linguistics, pages 132?139.
Zhiyi Chi and Stuart Geman. 1998. Estimation of proba-
bilistic context-free grammars. Computational Linguis-
tics, 24(2):299?305.
Michael J. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23.
Michael J. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Michael J. Collins. 2000. Discriminative reranking for nat-
ural language parsing. In The Proceedings of the 17th
International Conference on Machine Learning.
M. J. F. Gales. 1998. Maximum likelihood linear transfor-
mations for hmm-based speech recognition. Computer
Speech and Language, pages 75?98.
Jean-Luc Gauvain and Chin-Hui Lee. 1994. Maximum
a posteriori estimation for multivariate gaussian mixture
observations of markov chains. IEEE Transactions on
Speech and Audio Processing, 2(2):291?298.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the Sixth Conference on Empir-
ical Methods in Natural Language Processing (EMNLP-
01).
Rebecca Hwa. 1999. Supervised grammar induction us-
ing training data with limited constituent information. In
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.
Rebecca Hwa. 2001. On minimizing training corpus for
parser acquisition. In Proceedings of the Fifth Computa-
tional Natural Language Learning Workshop.
Mark Johnson and Stefan Riezler. 2000. Exploiting aux-
iliary distributions in stochastic unification-based gram-
mars. In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics.
Mark Johnson and Brian Roark. 2000. Compact non-left-
recursive grammars using the selective left-corner trans-
form and factoring. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics (COL-
ING), pages 355?361.
Mark Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):617?636.
L. Lamel, J.-L. Gauvain, and G. Adda. 2002. Unsupervised
acoustic model training. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), pages 877?880.
C. J. Legetter and P.C. Woodland. 1995. Maximum like-
lihood linear regression for speaker adaptation of contin-
uous density hidden markov models. Computer Speech
and Language, pages 171?185.
Fernando C.N. Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora. In
Proceedings of the 30th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 128?135.
Adwait Ratnaparkhi. 1999. Learning to parse natural lan-
guage with maximum entropy models. Machine Learn-
ing, 34:151?175.
Brian Roark. 2001a. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Brian Roark. 2003. Robust garden path parsing. Natural
Language Engineering, 9(2):1?24.
Language model adaptation with MAP estimation
and the perceptron algorithm
Michiel Bacchiani, Brian Roark and Murat Saraclar
AT&T Labs-Research, 180 Park Ave., Florham Park, NJ 07932, USA
{michiel,roark,murat}@research.att.com
Abstract
In this paper, we contrast two language model
adaptation approaches: MAP estimation and
the perceptron algorithm. Used in isolation, we
show that MAP estimation outperforms the lat-
ter approach, for reasons which argue for com-
bining the two approaches. When combined,
the resulting system provides a 0.7 percent ab-
solute reduction in word error rate over MAP
estimation alone. In addition, we demonstrate
that, in a multi-pass recognition scenario, it is
better to use the perceptron algorithm on early
pass word lattices, since the improved error rate
improves acoustic model adaptation.
1 Introduction
Most common approaches to language model adapta-
tion, such as count merging and model interpolation, are
special cases of maximum a posteriori (MAP) estima-
tion (Bacchiani and Roark, 2003). In essence, these ap-
proaches involve beginning from a smoothed language
model trained on out-of-domain observations, and adjust-
ing the model parameters based on in-domain observa-
tions. The approach ensures convergence, in the limit, to
the maximum likelihood model of the in-domain obser-
vations. The more in-domain observations, the less the
out-of-domain model is relied upon. In this approach, the
main idea is to change the out-of-domain model parame-
ters to match the in-domain distribution.
Another approach to language model adaptation would
be to change model parameters to correct the errors
made by the out-of-domain model on the in-domain data
through discriminative training. In such an approach,
the baseline recognizer would be used to recognize in-
domain utterances, and the parameters of the model ad-
justed to minimize recognition errors. Discriminative
training has been used for language modeling, using vari-
ous estimation techniques (Stolcke and Weintraub, 1998;
Roark et al, 2004), but language model adaptation to
novel domains is a particularly attractive scenario for dis-
criminative training, for reasons we discuss next.
A key requirement for discriminative modeling ap-
proaches is training data produced under conditions that
are close to testing conditions. For example, (Roark et al,
2004) showed that excluding an utterance from the lan-
guage model training corpus of the baseline model used
to recognize that utterance is essential to getting word
error rate (WER) improvements with the perceptron al-
gorithm in the Switchboard domain. In that paper, 28
different language models were built, each omitting one
of 28 sections, for use in generating word lattices for the
omitted section. Without removing the section, no benefit
was had from models built with the perceptron algorithm;
with removal, the approach yielded a solid improvement.
More time consuming is controlling acoustic model train-
ing. For a task such as Switchboard, on which the above
citation was evaluated, acoustic model estimation is ex-
pensive. Hence building multiple models, omitting var-
ious subsections is a substantial undertaking, especially
when discriminative estimation techniques are used.
Language model adaptation to a new domain, how-
ever, can dramatically simplify the issue of controlling
the baseline model for producing discriminative training
data, since the in-domain training data is not used for
building the baseline models. The purpose of this paper is
to compare a particular discriminative approach, the per-
ceptron algorithm, which has been successfully applied
in the Switchboard domain, with MAP estimation, for
adapting a language model to a novel domain. In addi-
tion, since the MAP and perceptron approaches optimize
different objectives, we investigate the benefit from com-
bination of these approaches within a multi-pass recogni-
tion system.
The task that we focus upon, adaptation of a general
voicemail recognition language model to a customer ser-
vice domain, has been shown to benefit greatly from
MAP estimation (Bacchiani and Roark, 2003). It is an
attractive test for studying language model adaptation,
since the out-of-domain acoustic model is matched to
the new domain, and the domain shift does not raise the
OOV rate significantly. Using 17 hours of in-domain
observations, versus 100 hours of out-of-domain utter-
ances, (Bacchiani and Roark, 2003) reported a reduction
in WER from 28.0% using the baseline system to 20.3%
with the best performing MAP adapted model. In this pa-
per, our best scenario, which uses MAP adaptation and
the perceptron algorithm in combination, achieves an ad-
ditional 0.7% reduction, to 19.6% WER.
The rest of the paper is structured as follows. In the
next section, we provide a brief background for both
MAP estimation and the perceptron algorithm. This is
followed by an experimental results section, in which we
present the performance of each approach in isolation, as
well as several ways of combining them.
2 Background
2.1 MAP language model adaptation
To build an adapted n-gram model, we use a count
merging approach, much as presented in (Bacchiani and
Roark, 2003), which is shown to be a special case of max-
imum a posteriori (MAP) adaptation. Let wO be the out-
of-domain corpus, and wI be the in-domain sample. Let
h represent an n-gram history of zero or more words. Let
ck(hw) denote the raw count of an n-gram hw in wk,
for k ? {O, I}. Let p?k(hw) denote the standard Katz
backoff model estimate of hw given wk. We define the
corrected count of an n-gram hw as:
c?k(hw) = |wk| p?k(hw) (1)
where |wk| denotes the size of the sample wk. Then:
p?(w | h) =
?hc?O(hw) + c?I(hw)
?h
?
w? c?O(hw
?) +
?
w? c?I(hw
?)
(2)
where ?h is a state dependent parameter that dictates how
much the out-of-domain prior counts should be relied
upon. The model is then defined as:
p?(w | h) =
{
p?(w | h) if cO(hw) + cI(hw) > 0
?p?(w | h?) otherwise
(3)
where ? is the backoff weight and h? the backoff history
for history h.
The principal difficulty in MAP adaptation of this sort
is determining the mixing parameters ?h in Eq. 2. Follow-
ing (Bacchiani and Roark, 2003), we chose a single mix-
ing parameter for each model that we built, i.e. ?h = ?
for all states h in the model.
2.2 Perceptron algorithm
Our discriminative n-gram model training approach uses
the perceptron algorithm, as presented in (Roark et al,
2004), which follows the general approach presented in
(Collins, 2002). For brevity, we present the algorithm,
not in full generality, but for the specific case of n-gram
model training.
The training set consists of N weighted word lattices
produced by the baseline recognizer, and a gold-standard
transcription for each of the N lattices. Following (Roark
et al, 2004), we use the lowest WER hypothesis in the
lattice as the gold-standard, rather than the reference tran-
scription. The perceptron model is a linear model with k
feature weights, all of which are initialized to 0. The al-
gorithm is incremental, i.e. the parameters are updated at
each example utterance in the training set in turn, and the
updated parameters are used for the next utterance. Af-
ter each pass over the training set, the model is evaluated
on a held-out set, and the best performing model on this
held-out set is the model used for testing.
For a given path pi in a weighted word lattice L, let
w[pi] be the cost of that path as given by the baseline rec-
ognizer. Let GL be the gold-standard transcription for
L. Let ?(pi) be the K-dimensional feature vector for pi,
which contains the count within the path pi of each fea-
ture. In our case, these are unigram, bigram and trigram
feature counts. Let ??t ? RK be the K-dimensional fea-
ture weight vector of the perceptron model at time t. The
perceptron model feature weights are updated as follows
1. For the example lattice L at time t, find p?it such that
p?it = argmin
pi?L
(w[pi] + ??(pi) ? ??t) (4)
where ? is a scaling constant.
2. For the 0 ? k ? K features in the feature weight
vector ??t,
??t+1[k] = ??t[k] + ?(p?it)[k] ? ?(GL)[k] (5)
Note that if p?it = GL, then the features are left un-
changed.
As shown in (Roark et al, 2004), the perceptron fea-
ture weight vector can be encoded in a deterministic
weighted finite state automaton (FSA), so that much of
the feature weight update involves basic FSA operations,
making the training relatively efficient in practice. As
suggested in (Collins, 2002), we use the averaged per-
ceptron when applying the model to held-out or test data.
After each pass over the training data, the averaged per-
ceptron model is output as a weighted FSA, which can be
used by intersecting with a lattice output from the base-
line system.
3 Experimental Results
We evaluated the language model adaptation algorithms
by measuring the transcription accuracy of an adapted
voicemail transcription system on voicemail messages re-
ceived at a customer care line of a telecommunications
network center. The initial voicemail system, named
Scanmail, was trained on general voicemail messages
collected from the mailboxes of people at our research
site in Florham Park, NJ. The target domain is also com-
posed of voicemail messages, but for a mailbox that re-
ceives messages from customer care agents regarding
network outages. In contrast to the general voicemail
messages from the training corpus of the Scanmail sys-
tem, the messages from the target domain, named SS-
NIFR, will be focused solely on network related prob-
lems. It contains frequent mention of various network
related acronyms and trouble ticket numbers, rarely (if at
all) found in the training corpus of the Scanmail system.
To evaluate the transcription accuracy, we used a multi-
pass speech recognition system that employs various
unsupervised speaker and channel normalization tech-
niques. An initial search pass produces word-lattice out-
put that is used as the grammar in subsequent search
passes. The system is almost identical to the one de-
scribed in detail in (Bacchiani, 2001). The main differ-
ences in terms of the acoustic model of the system are
the use of linear discriminant analysis features; use of a
100 hour training set as opposed to a 60 hour training set;
and the modeling of the speaker gender which in this sys-
tem is identical to that described in (Woodland and Hain,
1998). Note that the acoustic model is appropriate for ei-
ther domain as the messages are collected on a voicemail
system of the same type. This parallels the experiments
in (Lamel et al, 2002), where the focus was on AM adap-
tation in the case where the LM was deemed appropriate
for either domain.
The language model of the Scanmail system is a Katz
backoff trigram, trained on hand-transcribed messages of
approximately 100 hours of voicemail (1 million words).
The model contains 13460 unigram, 175777 bigram, and
495629 trigram probabilities. The lexicon of the Scan-
mail system contains 13460 words and was compiled
from all the unique words found in the 100 hours of tran-
scripts of the Scanmail training set.
For every experiment, we report the accuracy of the
one-best transcripts obtained at 2 stages of the recog-
nition process: after the first pass lattice construction
(FP), and after vocal tract length normalization and gen-
der modeling (VTLN), Constrained Model-space Adap-
tation (CMA), and Maximum Likelihood Linear regres-
sion adaptation (MLLR). Results after FP will be denoted
FP; results after VTLN, CMA and MLLR will be denoted
MP.
For the SSNIFR domain we have available a 1 hour
manually transcribed test set (10819 words) and approx-
imately 17 hours of manually transcribed adaptation data
(163343 words). In all experiments, the vocabulary of
the system is left unchanged. Generally, for a domain
shift this can raise the error rate significantly due to an
increase in the OOV rate. However, this increase in error
rate is limited in these experiments, because the majority
of the new domain-dependent vocabulary are acronyms
System FP MP
Baseline 32.7 28.0
MAP estimation 23.7 20.3
Perceptron (FP) 26.8 23.0
Perceptron (MP) ? 23.9
Table 1: Recognition on the 1 hour SSNIFR test set us-
ing systems obtained by supervised LM adaptation on the
17 hour adaptation set using the two methods, versus the
baseline out-of-domain system.
which are covered by the Scanmail vocabulary through
individual letters. The OOV rate of the SSNIFR test set,
using the Scanmail vocabulary is 2%.
Following (Bacchiani and Roark, 2003), ?h in Eq. 2 is
set to 0.2 for all reported MAP estimation trials. Follow-
ing (Roark et al, 2004), ? in Eq. 4 is also (coincidentally)
set to 0.2 for all reported perceptron trials. For the percep-
tron algorithm, approximately 10 percent of the training
data is reserved as a held-out set, for deciding when to
stop the algorithm.
Table 1 shows the results using MAP estimation and
the perceptron algorithm independently. For the percep-
tron algorithm, the baseline Scanmail system was used to
produce the word lattices used in estimating the feature
weights. There are two ways to do this. One is to use the
lattices produced after FP; the other is to use the lattices
produced after MP.
These results show two things. First, MAP estimation
on its own is clearly better than the perceptron algorithm
on its own. Since the MAP model is used in the ini-
tial search pass that produces the lattices, it can consider
all possible hypotheses. In contrast, the perceptron algo-
rithm is limited to the hypotheses available in the lattice
produced with the unadapted model.
Second, training the perceptron model on FP lattices
and applying that perceptron at each decoding step out-
performed training on MP lattices and only applying the
perceptron on that decoding step. This demonstrates the
benefit of better transcripts for the unsupervised adapta-
tion steps.
The benefit of MAP adaptation that leads to its supe-
rior performance in Table 1 suggests a hybrid approach,
that uses MAP estimation to ensure that good hypotheses
are present in the lattices, and the perceptron algorithm
to further reduce the WER. Within the multi-pass recog-
nition approach, several scenarios could be considered to
implement this combination. We investigate two here.
For each scenario, we split the 17 hour adaptation set
into four roughly equi-sized sets. In a first scenario, we
produced a MAP estimated model on the first 4.25 hour
subset, and produced word lattices on the other three sub-
sets, for use with the perceptron algorithm. Table 2 shows
System MAP Pct. FP MP
Baseline 0 32.7 28.0
MAP estimation 100 23.7 20.3
MAP estimation 25 25.6 21.5
Perceptron (FP) 25 23.8 20.5
Perceptron (MP) 25 ? 20.8
Table 2: Recognition on the 1 hour SSNIFR test set using
systems obtained by supervised LM adaptation on the 17
hour adaptation set using the first method of combination
of the two methods, versus the baseline out-of-domain
system.
the results for this training scenario.
A second scenario involves making use of all of the
adaptation data for both MAP estimation and the percep-
tron algorithm. As a result, it requires a more compli-
cated control of the baseline models used for producing
the word lattices for perceptron training. For each of the
four sub-sections of the adaptation data, we produced a
baseline MAP estimated model using the other three sub-
sections. Using these models, we produced training lat-
tices for the perceptron algorithm for the entire adaptation
data set. At test time, we used the MAP estimated model
trained on the entire adaptation set, as well as the percep-
tron model trained on the entire set. The results for this
training scenario are shown in table 3.
Both of these hybrid training scenarios demonstrate a
small improvement by using the perceptron algorithm on
FP lattices rather than MP lattices. Closely matching the
testing condition for perceptron training is important: ap-
plying a perceptron trained on MP lattices to FP lattices
hurts performance. Iterative training did not produce fur-
ther improvements: training a perceptron on MP lattices
produced by using both MAP estimation and a perceptron
trained on FP lattices, achieved no improvement over the
19.6 percent WER shown above.
4 Discussion
This paper has presented a series of experimental re-
sults that compare using MAP estimation for language
model domain adaptation to a discriminative modeling
approach for correcting errors produced by an out-of-
domain model when applied to the novel domain. Be-
cause the MAP estimation produces a model that is used
during first pass search, it has an advantage over the
perceptron algorithm, which simply re-weights paths al-
ready in the word lattice. In support of this argument, we
showed that, by using a subset of the in-domain adapta-
tion data for MAP estimation, and the rest for use in the
perceptron algorithm, we achieved results at nearly the
same level as MAP estimation on the entire adaptation
set.
System MAP Pct. FP MP
Baseline 0 32.7 28.0
MAP estimation 100 23.7 20.3
Perceptron (FP) 100 22.9 19.6
Perceptron (MP) 100 ? 19.9
Table 3: Recognition on the 1 hour SSNIFR test set us-
ing systems obtained by supervised LM adaptation on the
17 hour adaptation set using the second method of com-
bination of the two methods, versus the baseline out-of-
domain system.
With a more complicated training scenario, which used
all of the in-domain adaptation data for both methods
jointly, we were able to improve WER over MAP estima-
tion alone by 0.7 percent, for a total improvement over
the baseline of 8.4 percent.
Studying the various options for incorporating the per-
ceptron algorithm within the multi-pass rescoring frame-
work, our results show that there is a benefit from incor-
porating the perceptron at an early search pass, as it pro-
duces more accurate transcripts for unsupervised adapta-
tion. Furthermore, it is important to closely match testing
conditions for perceptron training.
References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
language model adaptation. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP), pages 224?227.
Michiel Bacchiani. 2001. Automatic transcription of
voicemail at AT&T. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?8.
L. Lamel, J.-L. Gauvain, and G. Adda. 2002. Unsuper-
vised acoustic model training. In Proceedings of the
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pages 877?880.
Brian Roark, Murat Saraclar, and Michael Collins. 2004.
Corrective language modeling for large vocabulary
ASR with the perceptron algorithm. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
A. Stolcke and M. Weintraub. 1998. Discriminitive lan-
guage modeling. In Proceedings of the 9th Hub-5
Conversational Speech Recog nition Workshop.
P.C. Woodland and T. Hain. 1998. The September 1998
HTK Hub 5E System. In The Proceedings of the 9th
Hub-5 Conversational Speech Recognition Workshop.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 287-294.
                         Proceedings of the 40th Annual Meeting of the Association for







Generalized Algorithms for Constructing Statistical Language Models
Cyril Allauzen, Mehryar Mohri, Brian Roark
AT&T Labs ? Research
180 Park Avenue
Florham Park, NJ 07932, USA
 
allauzen,mohri,roark  @research.att.com
Abstract
Recent text and speech processing applications such as
speech mining raise new and more general problems re-
lated to the construction of language models. We present
and describe in detail several new and efficient algorithms
to address these more general problems and report ex-
perimental results demonstrating their usefulness. We
give an algorithm for computing efficiently the expected
counts of any sequence in a word lattice output by a
speech recognizer or any arbitrary weighted automaton;
describe a new technique for creating exact representa-
tions of  -gram language models by weighted automata
whose size is practical for offline use even for a vocab-
ulary size of about 500,000 words and an  -gram order
 ; and present a simple and more general technique
for constructing class-based language models that allows
each class to represent an arbitrary weighted automaton.
An efficient implementation of our algorithms and tech-
niques has been incorporated in a general software library
for language modeling, the GRM Library, that includes
many other text and grammar processing functionalities.
1 Motivation
Statistical language models are crucial components of
many modern natural language processing systems such
as speech recognition, information extraction, machine
translation, or document classification. In all cases, a
language model is used in combination with other in-
formation sources to rank alternative hypotheses by as-
signing them some probabilities. There are classical
techniques for constructing language models such as  -
gram models with various smoothing techniques (see
Chen and Goodman (1998) and the references therein for
a survey and comparison of these techniques).
In some recent text and speech processing applications,
several new and more general problems arise that are re-
lated to the construction of language models. We present
new and efficient algorithms to address these more gen-
eral problems.
Counting. Classical language models are constructed
by deriving statistics from large input texts. In speech
mining applications or for adaptation purposes, one often
needs to construct a language model based on the out-
put of a speech recognition system. But, the output of a
recognition system is not just text. Indeed, the word er-
ror rate of conversational speech recognition systems is
still too high in many tasks to rely only on the one-best
output of the recognizer. Thus, the word lattice output
by speech recognition systems is used instead because it
contains the correct transcription in most cases.
A word lattice is a weighted finite automaton (WFA)
output by the recognizer for a particular utterance. It
contains typically a very large set of alternative transcrip-
tion sentences for that utterance with the corresponding
weights or probabilities. A necessary step for construct-
ing a language model based on a word lattice is to derive
the statistics for any given sequence from the lattices or
WFAs output by the recognizer. This cannot be done by
simply enumerating each path of the lattice and counting
the number of occurrences of the sequence considered in
each path since the number of paths of even a small au-
tomaton may be more than four billion. We present a
simple and efficient algorithm for computing the expected
count of any given sequence in a WFA and report experi-
mental results demonstrating its efficiency.
Representation of language models by WFAs. Clas-
sical  -gram language models admit a natural representa-
tion by WFAs in which each state encodes a left context
of width less than  . However, the size of that represen-
tation makes it impractical for offline optimizations such
as those used in large-vocabulary speech recognition or
general information extraction systems. Most offline rep-
resentations of these models are based instead on an ap-
proximation to limit their size. We describe a new tech-
nique for creating an exact representation of  -gram lan-
guage models by WFAs whose size is practical for offline
use even in tasks with a vocabulary size of about 500,000
words and for  .
Class-based models. In many applications, it is nat-
ural and convenient to construct class-based language
models, that is models based on classes of words (Brown
et al, 1992). Such models are also often more robust
since they may include words that belong to a class but
that were not found in the corpus. Classical class-based
models are based on simple classes such as a list of
words. But new clustering algorithms allow one to create
more general and more complex classes that may be reg-
ular languages. Very large and complex classes can also
be defined using regular expressions. We present a simple
and more general approach to class-based language mod-
els based on general weighted context-dependent rules
(Kaplan and Kay, 1994; Mohri and Sproat, 1996). Our
approach allows us to deal efficiently with more complex
classes such as weighted regular languages.
We have fully implemented the algorithms just men-
tioned and incorporated them in a general software li-
brary for language modeling, the GRM Library, that in-
cludes many other text and grammar processing function-
alities (Allauzen et al, 2003). In the following, we will
present in detail these algorithms and briefly describe the
corresponding GRM utilities.
2 Preliminaries
Definition 1 A system 	
   is a semiring
(Kuich and Salomaa, 1986) if: 	
  is a commuta-
tive monoid with identity element  ; 	
Discriminative Language Modeling with
Conditional Random Fields and the Perceptron Algorithm
Brian Roark Murat Saraclar
AT&T Labs - Research
{roark,murat}@research.att.com
Michael Collins Mark Johnson
MIT CSAIL Brown University
mcollins@csail.mit.edu Mark Johnson@Brown.edu
Abstract
This paper describes discriminative language modeling
for a large vocabulary speech recognition task. We con-
trast two parameter estimation methods: the perceptron
algorithm, and a method based on conditional random
fields (CRFs). The models are encoded as determin-
istic weighted finite state automata, and are applied by
intersecting the automata with word-lattices that are the
output from a baseline recognizer. The perceptron algo-
rithm has the benefit of automatically selecting a rela-
tively small feature set in just a couple of passes over the
training data. However, using the feature set output from
the perceptron algorithm (initialized with their weights),
CRF training provides an additional 0.5% reduction in
word error rate, for a total 1.8% absolute reduction from
the baseline of 39.2%.
1 Introduction
A crucial component of any speech recognizer is the lan-
guage model (LM), which assigns scores or probabilities
to candidate output strings in a speech recognizer. The
language model is used in combination with an acous-
tic model, to give an overall score to candidate word se-
quences that ranks them in order of probability or plau-
sibility.
A dominant approach in speech recognition has been
to use a ?source-channel?, or ?noisy-channel? model. In
this approach, language modeling is effectively framed
as density estimation: the language model?s task is to
define a distribution over the source ? i.e., the possible
strings in the language. Markov (n-gram) models are of-
ten used for this task, whose parameters are optimized
to maximize the likelihood of a large amount of training
text. Recognition performance is a direct measure of the
effectiveness of a language model; an indirect measure
which is frequently proposed within these approaches is
the perplexity of the LM (i.e., the log probability it as-
signs to some held-out data set).
This paper explores alternative methods for language
modeling, which complement the source-channel ap-
proach through discriminatively trained models. The lan-
guage models we describe do not attempt to estimate a
generative model P (w) over strings. Instead, they are
trained on acoustic sequences with their transcriptions,
in an attempt to directly optimize error-rate. Our work
builds on previous work on language modeling using the
perceptron algorithm, described in Roark et al (2004).
In particular, we explore conditional random field meth-
ods, as an alternative training method to the perceptron.
We describe how these models can be trained over lat-
tices that are the output from a baseline recognizer. We
also give a number of experiments comparing the two ap-
proaches. The perceptron method gave a 1.3% absolute
improvement in recognition error on the Switchboard do-
main; the CRF methods we describe give a further gain,
the final absolute improvement being 1.8%.
A central issue we focus on concerns feature selection.
The number of distinct n-grams in our training data is
close to 45 million, and we show that CRF training con-
verges very slowly even when trained with a subset (of
size 12 million) of these features. Because of this, we ex-
plore methods for picking a small subset of the available
features.1 The perceptron algorithm can be used as one
method for feature selection, selecting around 1.5 million
features in total. The CRF trained with this feature set,
and initialized with parameters from perceptron training,
converges much more quickly than other approaches, and
also gives the optimal performance on the held-out set.
We explore other approaches to feature selection, but find
that the perceptron-based approach gives the best results
in our experiments.
While we focus on n-gram models, we stress that our
methods are applicable to more general language mod-
eling features ? for example, syntactic features, as ex-
plored in, e.g., Khudanpur and Wu (2000). We intend
to explore methods with new features in the future. Ex-
perimental results with n-gram models on 1000-best lists
show a very small drop in accuracy compared to the use
of lattices. This is encouraging, in that it suggests that
models with more flexible features than n-gram models,
which therefore cannot be efficiently used with lattices,
may not be unduly harmed by their restriction to n-best
lists.
1.1 Related Work
Large vocabulary ASR has benefitted from discrimina-
tive estimation of Hidden Markov Model (HMM) param-
eters in the form of Maximum Mutual Information Es-
timation (MMIE) or Conditional Maximum Likelihood
Estimation (CMLE). Woodland and Povey (2000) have
shown the effectiveness of lattice-based MMIE/CMLE in
challenging large scale ASR tasks such as Switchboard.
In fact, state-of-the-art acoustic modeling, as seen, for
example, at annual Switchboard evaluations, invariably
includes some kind of discriminative training.
Discriminative estimation of language models has also
been proposed in recent years. Jelinek (1995) suggested
an acoustic sensitive language model whose parameters
1Note also that in addition to concerns about training time, a lan-
guage model with fewer features is likely to be considerably more effi-
cient when decoding new utterances.
are estimated by minimizing H(W |A), the expected un-
certainty of the spoken text W, given the acoustic se-
quence A. Stolcke and Weintraub (1998) experimented
with various discriminative approaches including MMIE
with mixed results. This work was followed up with
some success by Stolcke et al (2000) where an ?anti-
LM?, estimated from weighted N-best hypotheses of a
baseline ASR system, was used with a negative weight
in combination with the baseline LM. Chen et al (2000)
presented a method based on changing the trigram counts
discriminatively, together with changing the lexicon to
add new words. Kuo et al (2002) used the generalized
probabilistic descent algorithm to train relatively small
language models which attempt to minimize string error
rate on the DARPA Communicator task. Banerjee et al
(2003) used a language model modification algorithm in
the context of a reading tutor that listens. Their algorithm
first uses a classifier to predict what effect each parame-
ter has on the error rate, and then modifies the parameters
to reduce the error rate based on this prediction.
2 Linear Models, the Perceptron
Algorithm, and Conditional Random
Fields
This section describes a general framework, global linear
models, and two parameter estimation methods within
the framework, the perceptron algorithm and a method
based on conditional random fields. The linear models
we describe are general enough to be applicable to a di-
verse range of NLP and speech tasks ? this section gives
a general description of the approach. In the next section
of the paper we describe how global linear models can
be applied to speech recognition. In particular, we focus
on how the decoding and parameter estimation problems
can be implemented over lattices using finite-state tech-
niques.
2.1 Global linear models
We follow the framework outlined in Collins (2002;
2004). The task is to learn a mapping from inputs x ? X
to outputs y ? Y . We assume the following compo-
nents: (1) Training examples (xi, yi) for i = 1 . . . N .
(2) A function GEN which enumerates a set of candi-
dates GEN(x) for an input x. (3) A representation
? mapping each (x, y) ? X ? Y to a feature vector
?(x, y) ? Rd. (4) A parameter vector ?? ? Rd.
The components GEN,? and ?? define a mapping
from an input x to an output F (x) through
F (x) = argmax
y?GEN(x)
?(x, y) ? ?? (1)
where ?(x, y) ? ?? is the inner product
?
s ?s?s(x, y).
The learning task is to set the parameter values ?? using
the training examples as evidence. The decoding algo-
rithm is a method for searching for the y that maximizes
Eq. 1.
2.2 The Perceptron algorithm
We now turn to methods for training the parameters
?? of the model, given a set of training examples
Inputs: Training examples (xi, yi)
Initialization: Set ?? = 0
Algorithm:
For t = 1 . . . T , i = 1 . . . N
Calculate zi = argmaxz?GEN(xi) ?(xi, z) ? ??
If(zi 6= yi) then ?? = ??+ ?(xi, yi)? ?(xi, zi)
Output: Parameters ??
Figure 1: A variant of the perceptron algorithm.
(x1, y1) . . . (xN , yN ). This section describes the per-
ceptron algorithm, which was previously applied to lan-
guage modeling in Roark et al (2004). The next section
describes an alternative method, based on conditional
random fields.
The perceptron algorithm is shown in figure 1. At
each training example (xi, yi), the current best-scoring
hypothesis zi is found, and if it differs from the refer-
ence yi , then the cost of each feature2 is increased by
the count of that feature in zi and decreased by the count
of that feature in yi. The features in the model are up-
dated, and the algorithm moves to the next utterance.
After each pass over the training data, performance on
a held-out data set is evaluated, and the parameterization
with the best performance on the held out set is what is
ultimately produced by the algorithm.
Following Collins (2002), we used the averaged pa-
rameters from the training algorithm in decoding held-
out and test examples in our experiments. Say ??ti is the
parameter vector after the i?th example is processed on
the t?th pass through the data in the algorithm in fig-
ure 1. Then the averaged parameters ??AVG are defined
as ??AVG =
?
i,t ??
t
i/NT . Freund and Schapire (1999)
originally proposed the averaged parameter method; it
was shown to give substantial improvements in accuracy
for tagging tasks in Collins (2002).
2.3 Conditional Random Fields
Conditional Random Fields have been applied to NLP
tasks such as parsing (Ratnaparkhi et al, 1994; Johnson
et al, 1999), and tagging or segmentation tasks (Lafferty
et al, 2001; Sha and Pereira, 2003; McCallum and Li,
2003; Pinto et al, 2003). CRFs use the parameters ??
to define a conditional distribution over the members of
GEN(x) for a given input x:
p??(y|x) =
1
Z(x, ??)
exp (?(x, y) ? ??)
where Z(x, ??) =
?
y?GEN(x) exp (?(x, y) ? ??) is a
normalization constant that depends on x and ??.
Given these definitions, the log-likelihood of the train-
ing data under parameters ?? is
LL(??) =
N?
i=1
log p??(yi|xi)
=
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)] (2)
2Note that here lattice weights are interpreted as costs, which
changes the sign in the algorithm presented in figure 1.
Following Johnson et al (1999) and Lafferty et al
(2001), we use a zero-mean Gaussian prior on the pa-
rameters resulting in the regularized objective function:
LLR(??) =
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)]?
||??||2
2?2
(3)
The value ? dictates the relative influence of the log-
likelihood term vs. the prior, and is typically estimated
using held-out data. The optimal parameters under this
criterion are ??? = argmax?? LLR(??).
We use a limited memory variable metric method
(Benson and More?, 2002) to optimize LLR. There is a
general implementation of this method in the Tao/PETSc
software libraries (Balay et al, 2002; Benson et al,
2002). This technique has been shown to be very effec-
tive in a variety of NLP tasks (Malouf, 2002; Wallach,
2002). The main interface between the optimizer and the
training data is a procedure which takes a parameter vec-
tor ?? as input, and in turn returns LLR(??) as well as
the gradient of LLR at ??. The derivative of the objec-
tive function with respect to a parameter ?s at parameter
values ?? is
?LLR
??s
=
N?
i=1
?
??s(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?s(xi, y)
?
??
?s
?2
(4)
Note that LLR(??) is a convex function, so that there is
a globally optimal solution and the optimization method
will find it. The use of the Gaussian prior term ||??||2/2?2
in the objective function has been found to be useful in
several NLP settings. It effectively ensures that there is a
large penalty for parameter values in the model becoming
too large ? as such, it tends to control over-training. The
choice ofLLR as an objective function can be justified as
maximum a-posteriori (MAP) training within a Bayesian
approach. An alternative justification comes through a
connection to support vector machines and other large
margin approaches. SVM-based approaches use an op-
timization criterion that is closely related to LLR ? see
Collins (2004) for more discussion.
3 Linear models for speech recognition
We now describe how the formalism and algorithms in
section 2 can be applied to language modeling for speech
recognition.
3.1 The basic approach
As described in the previous section, linear models re-
quire definitions of X , Y , xi, yi, GEN, ? and a param-
eter estimation method. In the language modeling setting
we take X to be the set of all possible acoustic inputs; Y
is the set of all possible strings, ??, for some vocabu-
lary ?. Each xi is an utterance (a sequence of acous-
tic feature-vectors), and GEN(xi) is the set of possible
transcriptions under a first pass recognizer. (GEN(xi)
is a huge set, but will be represented compactly using a
lattice ? we will discuss this in detail shortly). We take
yi to be the member of GEN(xi) with lowest error rate
with respect to the reference transcription of xi.
All that remains is to define the feature-vector repre-
sentation, ?(x, y). In the general case, each component
?i(x, y) could be essentially any function of the acous-
tic input x and the candidate transcription y. The first
feature we define is ?0(x, y) as the log-probability of y
given x under the lattice produced by the baseline recog-
nizer. Thus this feature will include contributions from
the acoustic model and the original language model. The
remaining features are restricted to be functions over the
transcription y alone and they track all n-grams up to
some length (say n = 3), for example:
?1(x, y) = Number of times ?the the of? is seen in y
At an abstract level, features of this form are introduced
for all n-grams up to length 3 seen in some training data
lattice, i.e., n-grams seen in any word sequence within
the lattices. In practice, we consider methods that search
for sparse parameter vectors ??, thus assigning many n-
grams 0 weight. This will lead to more efficient algo-
rithms that avoid dealing explicitly with the entire set of
n-grams seen in training data.
3.2 Implementation using WFA
We now give a brief sketch of how weighted finite-state
automata (WFA) can be used to implement linear mod-
els for speech recognition. There are several papers de-
scribing the use of weighted automata and transducers
for speech in detail, e.g., Mohri et al (2002), but for clar-
ity and completeness this section gives a brief description
of the operations which we use.
For our purpose, a WFA A = (?, Q, qs, F, E, ?),
where ? is the vocabulary, Q is a (finite) set of states,
qs ? Q is a unique start state, F ? Q is a set of final
states, E is a (finite) set of transitions, and ? : F ? R
is a function from final states to final weights. Each tran-
sition e ? E is a tuple e = (l[e], p[e], n[e], w[e]), where
l[e] ? ? is a label (in our case, words), p[e] ? Q is the
origin state of e, n[e] ? Q is the destination state of e,
and w[e] ? R is the weight of the transition. A suc-
cessful path pi = e1 . . . ej is a sequence of transitions,
such that p[e1] = qs, n[ej ] ? F , and for 1 < k ? j,
n[ek?1] = p[ek]. Let ?A be the set of successful paths pi
in a WFA A. For any pi = e1 . . . ej , l[pi] = l[e1] . . . l[ej ].
The weights of the WFA in our case are always in the
log semiring, which means that the weight of a path pi =
e1 . . . ej ? ?A is defined as:
wA[pi] =
(
j?
k=1
w[ek]
)
+ ?(ej) (5)
By convention, we use negative log probabilities as
weights, so lower weights are better. All WFA that we
will discuss in this paper are deterministic, i.e. there are
no  transitions, and for any two transitions e, e? ? E,
if p[e] = p[e?], then l[e] 6= l[e?]. Thus, for any string
w = w1 . . . wj , there is at most one successful path
pi ? ?A, such that pi = e1 . . . ej and for 1 ? k ? j,
l[ek] = wk, i.e. l[pi] = w. The set of strings w such that
there exists a pi ? ?A with l[pi] = w define a regular
language LA ? ?.
We can now define some operations that will be used
in this paper.
? ?A. For a set of transitions E and ? ? R, define
?E = {(l[e], p[e], n[e], ?w[e]) : e ? E}. Then, for
any WFA A = (?, Q, qs, F, E, ?), define ?A for ? ? R
as follows: ?A = (?, Q, qs, F, ?E, ??).
? A ?A?. The intersection of two deterministic WFAs
A ? A? in the log semiring is a deterministic WFA
such that LA?A? = LA
?
LA? . For any pi ? ?A?A? ,
wA?A? [pi] = wA[pi1] + wA? [pi2], where l[pi] = l[pi1] =
l[pi2].
?BestPath(A). This operation takes a WFA A, and
returns the best scoring path p?i = argminpi??A wA[pi].
? MinErr(A, y). Given a WFA A, a string y, and
an error-function E(y,w), this operation returns p?i =
argminpi??A E(y, l[pi]). This operation will generally be
used with y as the reference transcription for a particular
training example, and E(y,w) as some measure of the
number of errors in w when compared to y. In this case,
the MinErr operation returns the path pi ? ?A such
l[pi] has the smallest number of errors when compared to
y.
? Norm(A). Given a WFA A, this operation yields
a WFA A? such that LA = LA? and for every pi ? ?A
there is a pi? ? ?A? such that l[pi] = l[pi?] and
wA? [pi
?] = wA[pi] + log
(
?
p?i??A
exp(?wA[p?i])
)
(6)
Note that
?
pi?Norm(A)
exp(?wNorm(A)[pi]) = 1 (7)
In other words the weights define a probability distribu-
tion over the paths.
? ExpCount(A,w). Given a WFA A and an n-gram
w, we define the expected count of w in A as
ExpCount(A,w) =
?
pi??A
wNorm(A)[pi]C(w, l[pi])
where C(w, l[pi]) is defined to be the number of times
the n-gram w appears in a string l[pi].
Given an acoustic input x, let Lx be a deterministic
word-lattice produced by the baseline recognizer. The
lattice Lx is an acyclic WFA, representing a weighted set
of possible transcriptions of x under the baseline recog-
nizer. The weights represent the combination of acoustic
and language model scores in the original recognizer.
The new, discriminative language model constructed
during training consists of a deterministic WFA which
we will denote D, together with a single parameter ?0.
The parameter ?0 is the weight for the log probability
feature ?0 given by the baseline recognizer. The WFA
D is constructed so that LD = ?? and for all pi ? ?D
wD[pi] =
d?
j=1
?j(x, l[pi])?j
Recall that ?j(x,w) for j > 0 is the count of the j?th n-
gram in w, and ?j is the parameter associated with that
w  wi-2     i-1 w   wi-1     iwi
wi-1
?
wi
?wi
?
? wi
Figure 2: Representation of a trigram model with failure transitions.
n-gram. Then, by definition, ?0L ? D accepts the same
set of strings as L, but
w?0L?D[pi] =
d?
j=0
?j(x, l[pi])?j
and argmin
pi?L
?(x, l[pi]) ? ?? = BestPath(?0L ? D).
Thus decoding under our new model involves first pro-
ducing a lattice L from the baseline recognizer; second,
scaling L with ?0 and intersecting it with the discrimi-
native language model D; third, finding the best scoring
path in the new WFA.
We now turn to training a model, or more explicitly,
deriving a discriminative language model (D, ?0) from a
set of training examples. Given a training set (xi, ri) for
i = 1 . . . N , where xi is an acoustic sequence, and ri is
a reference transcription, we can construct lattices Li for
i = 1 . . . N using the baseline recognizer. We can also
derive target transcriptions yi = MinErr(Li, ri). The
training algorithm is then a mapping from (Li, yi) for
i = 1 . . . N to a pair (D, ?0). Note that the construction
of the language model requires two choices. The first
concerns the choice of the set of n-gram features ?i for
i = 1 . . . d implemented by D. The second concerns
the choice of parameters ?i for i = 0 . . . d which assign
weights to the n-gram features as well as the baseline
feature ?0.
Before describing methods for training a discrimina-
tive language model using perceptron and CRF algo-
rithms, we give a little more detail about the structure
of D, focusing on how n-gram language models can be
implemented with finite-state techniques.
3.3 Representation of n-gram language models
An n-gram model can be efficiently represented in a de-
terministic WFA, through the use of failure transitions
(Allauzen et al, 2003). Every string accepted by such an
automaton has a single path through the automaton, and
the weight of the string is the sum of the weights of the
transitions in that path. In such a representation, every
state in the automaton represents an n-gram history h,
e.g. wi?2wi?1, and there are transitions leaving the state
for every word wi such that the feature hwi has a weight.
There is also a failure transition leaving the state, labeled
with some reserved symbol ?, which can only be tra-
versed if the next symbol in the input does not match any
transition leaving the state. This failure transition points
to the backoff state h?, i.e. the n-gram history h minus
its initial word. Figure 2 shows how a trigram model can
be represented in such an automaton. See Allauzen et al
(2003) for more details.
Note that in such a deterministic representation, the
entire weight of all features associated with the word
wi following history h must be assigned to the transi-
tion labeled with wi leaving the state h in the automa-
ton. For example, if h = wi?2wi?1, then the trigram
wi?2wi?1wi is a feature, as is the bigram wi?1wi and
the unigram wi. In this case, the weight on the transi-
tion wi leaving state h must be the sum of the trigram,
bigram and unigram feature weights. If only the trigram
feature weight were assigned to the transition, neither the
unigram nor the bigram feature contribution would be in-
cluded in the path weight. In order to ensure that the cor-
rect weights are assigned to each string, every transition
encoding an order k n-gram must carry the sum of the
weights for all n-gram features of orders ? k. To ensure
that every string in ?? receives the correct weight, for
any n-gram hw represented explicitly in the automaton,
h?w must also be represented explicitly in the automaton,
even if its weight is 0.
3.4 The perceptron algorithm
The perceptron algorithm is incremental, meaning that
the language model D is built one training example at
a time, during several passes over the training set. Ini-
tially, we build D to accept all strings in ?? with weight
0. For the perceptron experiments, we chose the param-
eter ?0 to be a fixed constant, chosen by optimization on
the held-out set. The loop in the algorithm in figure 1 is
implemented as:
For t = 1 . . . T, i = 1 . . . N :
? Calculate zi = argmaxy?GEN(x) ?(x, y) ? ??
= BestPath(?0Li ? D)
? If zi 6= MinErr(Li, ri), then update the feature
weights as in figure 1 (modulo the sign, because of
the use of costs), and modify D so as to assign the
correct weight to all strings.
In addition, averaged parameters need to be stored
(see section 2.2). These parameters will replace the un-
averaged parameters in D once training is completed.
Note that the only n-gram features to be included in
D at the end of the training process are those that oc-
cur in either a best scoring path zi or a minimum error
path yi at some point during training. Thus the percep-
tron algorithm is in effect doing feature selection as a
by-product of training. Given N training examples, and
T passes over the training set,O(NT ) n-grams will have
non-zero weight after training. Experiments in Roark et
al. (2004) suggest that the perceptron reaches optimal
performance after a small number of training iterations,
for example T = 1 or T = 2. Thus O(NT ) can be very
small compared to the full number of n-grams seen in
all training lattices. In our experiments, the perceptron
method chose around 1.4 million n-grams with non-zero
weight. This compares to 43.65 million possible n-grams
seen in the training data.
This is a key contrast with conditional random fields,
which optimize the parameters of a fixed feature set. Fea-
ture selection can be critical in our domain, as training
and applying a discriminative language model over all
n-grams seen in the training data (in either correct or in-
correct transcriptions) may be computationally very de-
manding. One training scenario that we will consider
will be using the output of the perceptron algorithm (the
averaged parameters) to provide the feature set and the
initial feature weights for use in the CRF algorithm. This
leads to a model which is reasonably sparse, but has the
benefit of CRF training, which as we will see gives gains
in performance.
3.5 Conditional Random Fields
The CRF methods that we use assume a fixed definition
of the n-gram features ?i for i = 1 . . . d in the model.
In the experimental section we will describe a number of
ways of defining the feature set. The optimization meth-
ods we use begin at some initial setting for ??, and then
search for the parameters ??? which maximize LLR(??)
as defined in Eq. 3.
The optimization method requires calculation of
LLR(??) and the gradient of LLR(??) for a series of val-
ues for ??. The first step in calculating these quantities is
to take the parameter values ??, and to construct an ac-
ceptor D which accepts all strings in ??, such that
wD[pi] =
d?
j=1
?j(x, l[pi])?j
For each training lattice Li, we then construct a new lat-
tice L?i = Norm(?0Li ? D). The lattice L?i represents
(in the log domain) the distribution p??(y|xi) over strings
y ? GEN(xi). The value of log p??(yi|xi) for any i can
be computed by simply taking the path weight of pi such
that l[pi] = yi in the new lattice L?i. Hence computation
of LLR(??) in Eq. 3 is straightforward.
Calculating the n-gram feature gradients for the CRF
optimization is also relatively simple, once L?i has been
constructed. From the derivative in Eq. 4, for each i =
1 . . . N, j = 1 . . . d the quantity
?j(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?j(xi, y) (8)
must be computed. The first term is simply the num-
ber of times the j?th n-gram feature is seen in yi. The
second term is the expected number of times that the
j?th n-gram is seen in the acceptor L?i. If the j?th
n-gram is w1 . . . wn, then this can be computed as
ExpCount(L?i, w1 . . . wn). The GRM library, which
was presented in Allauzen et al (2003), has a direct im-
plementation of the function ExpCount, which simul-
taneously calculates the expected value of all n-grams of
order less than or equal to a given n in a lattice L.
The one non-ngram feature weight that is being esti-
mated is the weight ?0 given to the baseline ASR nega-
tive log probability. Calculation of the gradient of LLR
with respect to this parameter again requires calculation
of the term in Eq. 8 for j = 0 and i = 1 . . . N . Com-
putation of
?
y?GEN(xi)
p??(y|xi)?0(xi, y) turns out to
be not as straightforward as calculating n-gram expec-
tations. To do so, we rely upon the fact that ?0(xi, y),
the negative log probability of the path, decomposes to
the sum of negative log probabilities of each transition
in the path. We index each transition in the lattice Li,
and store its negative log probability under the baseline
model. We can then calculate the required gradient from
L?i, by calculating the expected value in L?i of each in-
dexed transition in Li.
We found that an approximation to the gradient of
?0, however, performed nearly identically to this exact
gradient, while requiring substantially less computation.
Let wn1 be a string of n words, labeling a path in word-
lattice L?i. For brevity, let Pi(wn1 ) = p??(wn1 |xi) be the
conditional probability under the current model, and let
Qi(wn1 ) be the probability of wn1 in the normalized base-
line ASR lattice Norm(Li). Let Li be the set of strings
in the language defined by Li. Then we wish to compute
Ei for i = 1 . . . N , where
Ei =
?
wn1 ?Li
Pi(w
n
1 ) log Qi(w
n
1 )
=
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
1 ) (9)
The approximation is to make the following Markov
assumption:
Ei ?
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
k?2)
=
?
xyz?Si
ExpCount(L?i, xyz) log Qi(z|xy)(10)
where Si is the set of all trigrams seen in Li. The term
log Qi(z|xy) can be calculated once before training for
every lattice in the training set; the ExpCount term is
calculated as before using the GRM library. We have
found this approximation to be effective in practice, and
it was used for the trials reported below.
When the gradients and conditional likelihoods are
collected from all of the utterances in the training set, the
contributions from the regularizer are combined to give
an overall gradient and objective function value. These
values are provided to the parameter estimation routine,
which then returns the parameters for use in the next it-
eration. The accumulation of gradients for the feature set
is the most time consuming part of the approach, but this
is parallelizable, so that the computation can be divided
among many processors.
4 Empirical Results
We present empirical results on the Rich Transcription
2002 evaluation test set (rt02), which we used as our de-
velopment set, as well as on the Rich Transcription 2003
Spring evaluation CTS test set (rt03). The rt02 set con-
sists of 6081 sentences (63804 words) and has three sub-
sets: Switchboard 1, Switchboard 2, Switchboard Cel-
lular. The rt03 set consists of 9050 sentences (76083
words) and has two subsets: Switchboard and Fisher.
We used the same training set as that used in Roark
et al (2004). The training set consists of 276726 tran-
scribed utterances (3047805 words), with an additional
20854 utterances (249774 words) as held out data. For
0 500 100037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticePerceptron, Feat=PN, N=1000CRF, ? = ?, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=PN, N=1000
Figure 3: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
each utterance, a weighted word-lattice was produced,
representing alternative transcriptions, from the ASR
system. From each word-lattice, the oracle best path
was extracted, which gives the best word-error rate from
among all of the hypotheses in the lattice. The oracle
word-error rate for the training set lattices was 12.2%.
We also performed trials with 1000-best lists for the same
training set, rather than lattices. The oracle score for the
1000-best lists was 16.7%.
To produce the word-lattices, each training utterance
was processed by the baseline ASR system. However,
these same utterances are what the acoustic and language
models are built from, which leads to better performance
on the training utterances than can be expected when the
ASR system processes unseen utterances. To somewhat
control for this, the training set was partitioned into 28
sets, and baseline Katz backoff trigram models were built
for each set by including only transcripts from the other
27 sets. Since language models are generally far more
prone to overtrain than standard acoustic models, this
goes a long way toward making the training conditions
similar to testing conditions.
There are three baselines against which we are com-
paring. The first is the ASR baseline, with no reweight-
ing from a discriminatively trained n-gram model. The
other two baselines are with perceptron-trained n-gram
model re-weighting, and were reported in Roark et al
(2004). The first of these is for a pruned-lattice trained
trigram model, which showed a reduction in word er-
ror rate (WER) of 1.3%, from 39.2% to 37.9% on rt02.
The second is for a 1000-best list trained trigram model,
which performed only marginally worse than the lattice-
trained perceptron, at 38.0% on rt02.
4.1 Perceptron feature set
We use the perceptron-trained models as the starting
point for our CRF algorithm: the feature set given to
the CRF algorithm is the feature set selected by the per-
ceptron algorithm; the feature weights are initialized to
those of the averaged perceptron. Figure 3 shows the
performance of our three baselines versus three trials of
0 500 1000 1500 2000 250037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=E,  ?=0.01CRF, ? = 0.5, Feat=E,  ?=0.9
Figure 4: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
the CRF algorithm. In the first two trials, the training
set consists of the pruned lattices, and the feature set
is from the perceptron algorithm trained on pruned lat-
tices. There were 1.4 million features in this feature set.
The first trial set the regularizer constant ? =?, so that
the algorithm was optimizing raw conditional likelihood.
The second trial is with the regularizer constant ? = 0.5,
which we found empirically to be a good parameteriza-
tion on the held-out set. As can be seen from these re-
sults, regularization is critical.
The third trial in this set uses the feature set from the
perceptron algorithm trained on 1000-best lists, and uses
CRF optimization on these on these same 1000-best lists.
There were 0.9 million features in this feature set. For
this trial, we also used ? = 0.5. As with the percep-
tron baselines, the n-best trial performs nearly identically
with the pruned lattices, here also resulting in 37.4%
WER. This may be useful for techniques that would be
more expensive to extend to lattices versus n-best lists
(e.g. models with unbounded dependencies).
These trials demonstrate that the CRF algorithm can
do a better job of estimating feature weights than the per-
ceptron algorithm for the same feature set. As mentioned
in the earlier section, feature selection is a by-product of
the perceptron algorithm, but the CRF algorithm is given
a set of features. The next two trials looked at selecting
feature sets other than those provided by the perceptron
algorithm.
4.2 Other feature sets
In order for the feature weights to be non-zero in this ap-
proach, they must be observed in the training set. The
number of unigram, bigram and trigram features with
non-zero observations in the training set lattices is 43.65
million, or roughly 30 times the size of the perceptron
feature set. Many of these features occur only rarely
with very low conditional probabilities, and hence cannot
meaningfully impact system performance. We pruned
this feature set to include all unigrams and bigrams, but
only those trigrams with an expected count of greater
than 0.01 in the training set. That is, to be included, a
Trial Iter rt02 rt03
ASR Baseline - 39.2 38.2
Perceptron, Lattice - 37.9 36.9
Perceptron, N-best - 38.0 37.2
CRF, Lattice, Percep Feats (1.4M) 769 37.4 36.5
CRF, N-best, Percep Feats (0.9M) 946 37.4 36.6
CRF, Lattice, ? = 0.01 (12M) 2714 37.6 36.5
CRF, Lattice, ? = 0.9 (1.5M) 1679 37.5 36.6
Table 1: Word-error rate results at convergence iteration for
various trials, on both Switchboard 2002 test set (rt02), which
was used as the dev set, and Switchboard 2003 test set (rt03).
trigram must occur in a set of paths, the sum of the con-
ditional probabilities of which must be greater than our
threshold ? = 0.01. This threshold resulted in a feature
set of roughly 12 million features, nearly 10 times the
size of the perceptron feature set. For better comparabil-
ity with that feature set, we set our thresholds higher, so
that trigrams were pruned if their expected count fell be-
low ? = 0.9, and bigrams were pruned if their expected
count fell below ? = 0.1. We were concerned that this
may leave out some of the features on the oracle paths, so
we added back in all bigram and trigram features that oc-
curred on oracle paths, giving a feature set of 1.5 million
features, roughly the same size as the perceptron feature
set.
Figure 4 shows the results for three CRF trials versus
our ASR baseline and the perceptron algorithm baseline
trained on lattices. First, the result using the perceptron
feature set provides us with a WER of 37.4%, as pre-
viously shown. The WER at convergence for the big
feature set (12 million features) is 37.6%; the WER at
convergence for the smaller feature set (1.5 million fea-
tures) is 37.5%. While both of these other feature sets
converge to performance close to that using the percep-
tron features, the number of iterations over the training
data that are required to reach that level of performance
are many more than for the perceptron-initialized feature
set.
Table 1 shows the word-error rate at the convergence
iteration for the various trials, on both rt02 and rt03. All
of the CRF trials are significantly better than the percep-
tron performance, using the Matched Pair Sentence Seg-
ment test for WER included with SCTK (NIST, 2000).
On rt02, the N-best and perceptron initialized CRF trials
were were significantly better than the lattice perceptron
at p < 0.001; the other two CRF trials were significantly
better than the lattice perceptron at p < 0.01. On rt03,
the N-best CRF trial was significantly better than the lat-
tice perceptron at p < 0.002; the other three CRF tri-
als were significantly better than the lattice perceptron at
p < 0.001.
Finally, we measured the time of a single iteration over
the training data on a single machine for the perceptron
algorithm, the CRF algorithm using the approximation to
the gradient of ?0, and the CRF algorithm using an exact
gradient of ?0. Table 2 shows these times in hours. Be-
cause of the frequent update of the weights in the model,
the perceptron algorithm is more expensive than the CRF
algorithm for a single iteration. Further, the CRF algo-
rithm is parallelizable, so that most of the work of an
CRF
Features Percep approx exact
Lattice, Percep Feats (1.4M) 7.10 1.69 3.61
N-best, Percep Feats (0.9M) 3.40 0.96 1.40
Lattice, ? = 0.01 (12M) - 2.24 4.75
Table 2: Time (in hours) for one iteration on a single Intel
Xeon 2.4Ghz processor with 4GB RAM.
iteration can be shared among multiple processors. Our
most common training setup for the CRF algorithm was
parallelized between 20 processors, using the approxi-
mation to the gradient. In that setup, using the 1.4M fea-
ture set, one iteration of the perceptron algorithm took
the same amount of real time as approximately 80 itera-
tions of CRF.
5 Conclusion
We have contrasted two approaches to discriminative
language model estimation on a difficult large vocabu-
lary task, showing that they can indeed scale effectively
to handle this size of a problem. Both algorithms have
their benefits. The perceptron algorithm selects a rela-
tively small subset of the total feature set, and requires
just a couple of passes over the training data. The CRF
algorithm does a better job of parameter estimation for
the same feature set, and is parallelizable, so that each
pass over the training set can require just a fraction of
the real time of the perceptron algorithm.
The best scenario from among those that we investi-
gated was a combination of both approaches, with the
output of the perceptron algorithm taken as the starting
point for CRF estimation.
As a final point, note that the methods we describe do
not replace an existing language model, but rather com-
plement it. The existing language model has the benefit
that it can be trained on a large amount of text that does
not have speech transcriptions. It has the disadvantage
of not being a discriminative model. The new language
model is trained on the speech transcriptions, meaning
that it has less training data, but that it has the advan-
tage of discriminative training ? and in particular, the ad-
vantage of being able to learn negative evidence in the
form of negative weights on n-grams which are rarely
or never seen in natural language text (e.g., ?the of?),
but are produced too frequently by the recognizer. The
methods we describe combines the two language models,
allowing them to complement each other.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized
algorithms for constructing language models. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F.
Smith. 2002. Petsc users manual. Technical Report ANL-95/11-
Revision 2.1.2, Argonne National Laboratory.
Satanjeev Banerjee, Jack Mostow, Joseph Beck, and Wilson Tam.
2003. Improving language models by learning from speech recog-
nition errors in a reading tutor that listens. In Proceedings of the
Second International Conference on Applied Artificial Intelligence,
Fort Panhala, Kolhapur, India.
Steven J. Benson and Jorge J. More?. 2002. A limited memory vari-
able metric method for bound constrained minimization. Preprint
ANL/ACSP909-0901, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge J. More?, and Jason
Sarich. 2002. Tao users manual. Technical Report ANL/MCS-TM-
242-Revision 1.4, Argonne National Laboratory.
Zheng Chen, Kai-Fu Lee, and Ming Jing Li. 2000. Discriminative
training on language model. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing (ICSLP), Bei-
jing, China.
Michael Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron algo-
rithms. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1?8.
Michael Collins. 2004. Parameter estimation for statistical parsing
models: Theory and practice of distribution-free methods. In Harry
Bunt, John Carroll, and Giorgio Satta, editors, New Developments
in Parsing Technology. Kluwer.
Yoav Freund and Robert Schapire. 1999. Large margin classification
using the perceptron algorithm. Machine Learning, 3(37):277?296.
Frederick Jelinek. 1995. Acoustic sensitive language modeling. Tech-
nical report, Center for Language and Speech Processing, Johns
Hopkins University, Baltimore, MD.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan
Riezler. 1999. Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 535?541.
Sanjeev Khudanpur and Jun Wu. 2000. Maximum entropy techniques
for exploiting syntactic, semantic and collocational dependencies in
language modeling. Computer Speech and Language, 14(4):355?
372.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-
Hui Lee. 2002. Discriminative training of language models for
speech recognition. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing (ICASSP), Orlando,
Florida.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Con-
ditional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282?289, Williams
College, Williamstown, MA, USA.
Robert Malouf. 2002. A comparison of algorithms for maximum en-
tropy parameter estimation. In Proc. CoNLL, pages 49?55.
Andrew McCallum and Wei Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction and
web-enhanced lexicons. In Proc. CoNLL.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition. Computer
Speech and Language, 16(1):69?88.
NIST. 2000. Speech recognition scoring toolkit (sctk) version 1.2c.
Available at http://www.nist.gov/speech/tools.
David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003.
Table extraction using conditional random fields. In Proc. ACM SI-
GIR.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994. A max-
imum entropy model for parsing. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (ICSLP), pages
803?806.
Brian Roark, Murat Saraclar, and Michael Collins. 2004. Corrective
language modeling for large vocabulary ASR with the perceptron al-
gorithm. In Proceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages 749?752.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL, Edmonton, Canada.
A. Stolcke and M. Weintraub. 1998. Discriminitive language model-
ing. In Proceedings of the 9th Hub-5 Conversational Speech Recog-
nition Workshop.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, F. Weng, and
J. Zheng. 2000. The SRI March 2000 Hub-5 conversational speech
transcription system. In Proceedings of the NIST Speech Transcrip-
tion Workshop.
Hanna Wallach. 2002. Efficient training of conditional random fields.
Master?s thesis, University of Edinburgh.
P.C. Woodland and D. Povey. 2000. Large scale discriminative training
for speech recognition. In Proc. ISCA ITRW ASR2000, pages 7?16.
Incremental Parsing with the Perceptron Algorithm
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Brian Roark
AT&T Labs - Research
roark@research.att.com
Abstract
This paper describes an incremental parsing approach
where parameters are estimated using a variant of the
perceptron algorithm. A beam-search algorithm is used
during both training and decoding phases of the method.
The perceptron approach was implemented with the
same feature set as that of an existing generative model
(Roark, 2001a), and experimental results show that it
gives competitive performance to the generative model
on parsing the Penn treebank. We demonstrate that train-
ing a perceptron model to combine with the generative
model during search provides a 2.1 percent F-measure
improvement over the generative model alone, to 88.8
percent.
1 Introduction
In statistical approaches to NLP problems such as tag-
ging or parsing, it seems clear that the representation
used as input to a learning algorithm is central to the ac-
curacy of an approach. In an ideal world, the designer
of a parser or tagger would be free to choose any fea-
tures which might be useful in discriminating good from
bad structures, without concerns about how the features
interact with the problems of training (parameter estima-
tion) or decoding (search for the most plausible candidate
under the model). To this end, a number of recently pro-
posed methods allow a model to incorporate ?arbitrary?
global features of candidate analyses or parses. Exam-
ples of such techniques are Markov Random Fields (Rat-
naparkhi et al, 1994; Abney, 1997; Della Pietra et al,
1997; Johnson et al, 1999), and boosting or perceptron
approaches to reranking (Freund et al, 1998; Collins,
2000; Collins and Duffy, 2002).
A drawback of these approaches is that in the general
case, they can require exhaustive enumeration of the set
of candidates for each input sentence in both the train-
ing and decoding phases1. For example, Johnson et al
(1999) and Riezler et al (2002) use all parses generated
by an LFG parser as input to an MRF approach ? given
the level of ambiguity in natural language, this set can
presumably become extremely large. Collins (2000) and
Collins and Duffy (2002) rerank the top N parses from
an existing generative parser, but this kind of approach
1Dynamic programming methods (Geman and Johnson, 2002; Laf-
ferty et al, 2001) can sometimes be used for both training and decod-
ing, but this requires fairly strong restrictions on the features in the
model.
presupposes that there is an existing baseline model with
reasonable performance. Many of these baseline models
are themselves used with heuristic search techniques, so
that the potential gain through the use of discriminative
re-ranking techniques is further dependent on effective
search.
This paper explores an alternative approach to pars-
ing, based on the perceptron training algorithm intro-
duced in Collins (2002). In this approach the training
and decoding problems are very closely related ? the
training method decodes training examples in sequence,
and makes simple corrective updates to the parameters
when errors are made. Thus the main complexity of the
method is isolated to the decoding problem. We describe
an approach that uses an incremental, left-to-right parser,
with beam search, to find the highest scoring analysis un-
der the model. The same search method is used in both
training and decoding. We implemented the perceptron
approach with the same feature set as that of an existing
generative model (Roark, 2001a), and show that the per-
ceptron model gives performance competitive to that of
the generative model on parsing the Penn treebank, thus
demonstrating that an unnormalized discriminative pars-
ing model can be applied with heuristic search. We also
describe several refinements to the training algorithm,
and demonstrate their impact on convergence properties
of the method.
Finally, we describe training the perceptron model
with the negative log probability given by the generative
model as another feature. This provides the perceptron
algorithm with a better starting point, leading to large
improvements over using either the generative model or
the perceptron algorithm in isolation (the hybrid model
achieves 88.8% f-measure on the WSJ treebank, com-
pared to figures of 86.7% and 86.6% for the separate
generative and perceptron models). The approach is an
extremely simple method for integrating new features
into the generative model: essentially all that is needed
is a definition of feature-vector representations of entire
parse trees, and then the existing parsing algorithms can
be used for both training and decoding with the models.
2 The General Framework
In this section we describe a general framework ? linear
models for NLP ? that could be applied to a diverse range
of tasks, including parsing and tagging. We then describe
a particular method for parameter estimation, which is a
generalization of the perceptron algorithm. Finally, we
give an abstract description of an incremental parser, and
describe how it can be used with the perceptron algo-
rithm.
2.1 Linear Models for NLP
We follow the framework outlined in Collins (2002;
2004). The task is to learn a mapping from inputs x ? X
to outputs y ? Y . For example, X might be a set of sen-
tences, with Y being a set of possible parse trees. We
assume:
. Training examples (xi, yi) for i = 1 . . . n.
. A function GEN which enumerates a set of candi-
dates GEN(x) for an input x.
. A representation ? mapping each (x, y) ? X ?Y
to a feature vector ?(x, y) ? Rd.
. A parameter vector ?? ? Rd.
The components GEN,? and ?? define a mapping from
an input x to an output F (x) through
F (x) = arg max
y?GEN(x)
?(x, y) ? ?? (1)
where ?(x, y) ? ?? is the inner product
?
s ?s?s(x, y).
The learning task is to set the parameter values ?? using
the training examples as evidence. The decoding algo-
rithm is a method for searching for the arg max in Eq. 1.
This framework is general enough to encompass sev-
eral tasks in NLP. In this paper we are interested in pars-
ing, where (xi, yi), GEN, and ? can be defined as fol-
lows:
? Each training example (xi, yi) is a pair where xi is
a sentence, and yi is the gold-standard parse for that
sentence.
? Given an input sentence x, GEN(x) is a set of
possible parses for that sentence. For example,
GEN(x) could be defined as the set of possible
parses for x under some context-free grammar, per-
haps a context-free grammar induced from the train-
ing examples.
? The representation ?(x, y) could track arbitrary
features of parse trees. As one example, suppose
that there are m rules in a context-free grammar
(CFG) that defines GEN(x). Then we could define
the i?th component of the representation, ?i(x, y),
to be the number of times the i?th context-free rule
appears in the parse tree (x, y). This is implicitly
the representation used in probabilistic or weighted
CFGs.
Note that the difficulty of finding the arg max in Eq. 1
is dependent on the interaction of GEN and ?. In many
cases GEN(x) could grow exponentially with the size
of x, making brute force enumeration of the members
of GEN(x) intractable. For example, a context-free
grammar could easily produce an exponentially growing
number of analyses with sentence length. For some rep-
resentations, such as the ?rule-based? representation de-
scribed above, the arg max in the set enumerated by the
CFG can be found efficiently, using dynamic program-
ming algorithms, without having to explicitly enumer-
ate all members of GEN(x). However in many cases
we may be interested in representations which do not al-
low efficient dynamic programming solutions. One way
around this problem is to adopt a two-pass approach,
where GEN(x) is the top N analyses under some initial
model, as in the reranking approach of Collins (2000).
In the current paper we explore alternatives to rerank-
ing approaches, namely heuristic methods for finding the
arg max, specifically incremental beam-search strategies
related to the parsers of Roark (2001a) and Ratnaparkhi
(1999).
2.2 The Perceptron Algorithm for Parameter
Estimation
We now consider the problem of setting the parameters,
??, given training examples (xi, yi). We will briefly re-
view the perceptron algorithm, and its convergence prop-
erties ? see Collins (2002) for a full description. The
algorithm and theorems are based on the approach to
classification problems described in Freund and Schapire
(1999).
Figure 1 shows the algorithm. Note that the
most complex step of the method is finding zi =
arg maxz?GEN(xi) ?(xi, z)??? ? and this is precisely the
decoding problem. Thus the training algorithm is in prin-
ciple a simple part of the parser: any system will need
a decoding method, and once the decoding algorithm is
implemented the training algorithm is relatively straight-
forward.
We will now give a first theorem regarding the con-
vergence of this algorithm. First, we need the following
definition:
Definition 1 Let GEN(xi) = GEN(xi) ? {yi}. In
other words GEN(xi) is the set of incorrect candidates
for an example xi. We will say that a training sequence
(xi, yi) for i = 1 . . . n is separable with margin ? > 0
if there exists some vector U with ||U|| = 1 such that
?i, ?z ? GEN(xi), U ? ?(xi, yi)?U ? ?(xi, z) ? ?
(2)
(||U|| is the 2-norm of U, i.e., ||U|| = ??s U2s.)
Next, define Ne to be the number of times an error is
made by the algorithm in figure 1 ? that is, the number of
times that zi 6= yi for some (t, i) pair. We can then state
the following theorem (see (Collins, 2002) for a proof):
Theorem 1 For any training sequence (xi, yi) that is
separable with margin ?, for any value of T , then for
the perceptron algorithm in figure 1
Ne ?
R2
?2
where R is a constant such that ?i, ?z ?
GEN(xi) ||?(xi, yi)? ?(xi, z)|| ? R.
This theorem implies that if there is a parameter vec-
tor U which makes zero errors on the training set, then
after a finite number of iterations the training algorithm
will converge to parameter values with zero training er-
ror. A crucial point is that the number of mistakes is in-
dependent of the number of candidates for each example
Inputs: Training examples (xi, yi) Algorithm:
Initialization: Set ?? = 0 For t = 1 . . . T , i = 1 . . . n
Output: Parameters ?? Calculate zi = arg maxz?GEN(xi) ?(xi, z) ? ??
If(zi 6= yi) then ?? = ??+ ?(xi, yi)? ?(xi, zi)
Figure 1: A variant of the perceptron algorithm.
(i.e. the size of GEN(xi) for each i), depending only
on the separation of the training data, where separation
is defined above. This is important because in many NLP
problems GEN(x) can be exponential in the size of the
inputs. All of the convergence and generalization results
in Collins (2002) depend on notions of separability rather
than the size of GEN.
Two questions come to mind. First, are there guar-
antees for the algorithm if the training data is not sepa-
rable? Second, performance on a training sample is all
very well, but what does this guarantee about how well
the algorithm generalizes to newly drawn test examples?
Freund and Schapire (1999) discuss how the theory for
classification problems can be extended to deal with both
of these questions; Collins (2002) describes how these
results apply to NLP problems.
As a final note, following Collins (2002), we used the
averaged parameters from the training algorithm in de-
coding test examples in our experiments. Say ??ti is the
parameter vector after the i?th example is processed on
the t?th pass through the data in the algorithm in fig-
ure 1. Then the averaged parameters ??AVG are defined
as ??AVG =
?
i,t ??
t
i/NT . Freund and Schapire (1999)
originally proposed the averaged parameter method; it
was shown to give substantial improvements in accuracy
for tagging tasks in Collins (2002).
2.3 An Abstract Description of Incremental
Parsing
This section gives a description of the basic incremental
parsing approach. The input to the parser is a sentence
x with length n. A hypothesis is a triple ?x, t, i? such
that x is the sentence being parsed, t is a partial or full
analysis of that sentence, and i is an integer specifying
the number of words of the sentence which have been
processed. Each full parse for a sentence will have the
form ?x, t, n?. The initial state is ?x, ?, 0? where ? is a
?null? or empty analysis.
We assume an ?advance? function ADV which takes
a hypothesis triple as input, and returns a set of new hy-
potheses as output. The advance function will absorb
another word in the sentence: this means that if the input
to ADV is ?x, t, i?, then each member of ADV(?x, t, i?)
will have the form ?x, t?,i+1?. Each new analysis t? will
be formed by somehow incorporating the i+1?th word
into the previous analysis t.
With these definitions in place, we can iteratively de-
fine the full set of partial analysesHi for the first i words
of the sentence as H0(x) = {?x, ?, 0?}, and Hi(x) =
?h??Hi?1(x)ADV(h?) for i = 1 . . . n. The full set of
parses for a sentence x is then GEN(x) = Hn(x) where
n is the length of x.
Under this definition GEN(x) can include a huge
number of parses, and searching for the highest scor-
ing parse, arg maxh?Hn(x) ?(h) ? ??, will be intractable.
For this reason we introduce one additional function,
FILTER(H), which takes a set of hypotheses H, and re-
turns a much smaller set of ?filtered? hypotheses. Typi-
cally, FILTER will calculate the score ?(h) ? ?? for each
h ? H, and then eliminate partial analyses which have
low scores under this criterion. For example, a simple
version of FILTER would take the top N highest scoring
members of H for some constant N . We can then rede-
fine the set of partial analyses as follows (we use Fi(x)
to denote the set of filtered partial analyses for the first i
words of the sentence):
F0(x) = {?x, ?, 0?}
Fi(x) = FILTER
(
?h??Fi?1(x)ADV(h?)
)
for i=1 . . . n
The parsing algorithm returns arg maxh?Fn ?(h) ? ??.
Note that this is a heuristic, in that there is no guar-
antee that this procedure will find the highest scoring
parse, arg maxh?Hn ?(h) ? ??. Search errors, where
arg maxh?Fn ?(h) ? ?? 6= arg maxh?Hn ?(h) ? ??, will
create errors in decoding test sentences, and also errors in
implementing the perceptron training algorithm in Fig-
ure 1. In this paper we give empirical results that suggest
that FILTER can be chosen in such a way as to give ef-
ficient parsing performance together with high parsing
accuracy.
The exact implementation of the parser will depend on
the definition of partial analyses, of ADV and FILTER,
and of the representation ?. The next section describes
our instantiation of these choices.
3 A full description of the parsing
approach
The parser is an incremental beam-search parser very
similar to the sort described in Roark (2001a; 2004), with
some changes in the search strategy to accommodate the
perceptron feature weights. We first describe the parsing
algorithm, and then move on to the baseline feature set
for the perceptron model.
3.1 Parser control
The input to the parser is a string wn0 , a grammar G, a
mapping ? from derivations to feature vectors, and a pa-
rameter vector ??. The grammar G = (V, T,S?, S?, C,B)
consists of a set of non-terminal symbols V , a set of ter-
minal symbols T , a start symbol S? ? V , an end-of-
constituent symbol S? ? V , a set of ?allowable chains?C,
and a set of ?allowable triples? B. S? is a special empty
non-terminal that marks the end of a constituent. Each
chain is a sequence of non-terminals followed by a ter-
minal symbol, for example ?S? ? S ? NP ? NN ?
S?
S
!!
NP
NN
Trash
. . . . . . . . . . . . .
NN
can
. . . . . . . . . . . . . . . VP
MD
can
. . . . . . . . . . . . . . . . . . VP
VP
MD
can
Figure 2: Left child chains and connection paths. Dotted
lines represent potential attachments
Trash?. Each ?allowable triple? is a tuple ?X,Y, Z?
where X,Y, Z ? V . The triples specify which non-
terminals Z are allowed to follow a non-terminal Y un-
der a parent X . For example, the triple ?S,NP,VP?
specifies that a VP can follow an NP under an S. The
triple ?NP,NN,S?? would specify that the S? symbol can
follow an NN under an NP ? i.e., that the symbol NN is
allowed to be the final child of a rule with parent NP
The initial state of the parser is the input string alone,
wn0 . In absorbing the first word, we add all chains of the
form S? . . . ? w0. For example, in figure 2 the chain
?S? ? S ? NP ? NN ? Trash? is used to construct
an analysis for the first word alone. Other chains which
start with S? and end with Trash would give competing
analyses for the first word of the string.
Figure 2 shows an example of how the next word in
a sentence can be incorporated into a partial analysis for
the previous words. For any partial analysis there will
be a set of potential attachment sites: in the example, the
attachment sites are under the NP or the S. There will
also be a set of possible chains terminating in the next
word ? there are three in the example. Each chain could
potentially be attached at each attachment site, giving
6 ways of incorporating the next word in the example.
For illustration, assume that the set B is {?S,NP,VP?,
?NP,NN,NN?, ?NP,NN,S??, ?S,NP,VP?}. Then some
of the 6 possible attachments may be disallowed because
they create triples that are not in the set B. For example,
in figure 2 attaching either of the VP chains under the
NP is disallowed because the triple ?NP,NN,VP? is not
in B. Similarly, attaching the NN chain under the S will
be disallowed if the triple ?S,NP,NN? is not in B. In
contrast, adjoining ?NN ? can? under the NP creates a
single triple, ?NP,NN,NN?, which is allowed. Adjoining
either of the VP chains under the S creates two triples,
?S,NP,VP? and ?NP,NN,S??, which are both in the set
B.
Note that the ?allowable chains? in our grammar are
what Costa et al (2001) call ?connection paths? from
the partial parse to the next word. It can be shown that
the method is equivalent to parsing with a transformed
context-free grammar (a first-order ?Markov? grammar)
? for brevity we omit the details here.
In this way, given a set of candidatesFi(x) for the first
i words of the string, we can generate a set of candidates
Tree POS f24 f2-21 f2-21, # > 1
transform tags Type Type OOV Type OOV
None Gold 386 1680 0.1% 1013 0.1%
None Tagged 401 1776 0.1% 1043 0.2%
FSLC Gold 289 1214 0.1% 746 0.1%
FSLC Tagged 300 1294 0.1% 781 0.1%
Table 1: Left-child chain type counts (of length > 2) for
sections of the Wall St. Journal Treebank, and out-of-
vocabulary (OOV) rate on the held-out corpus.
for the first i + 1 words, ?h??Fi(x)ADV(h?), where the
ADV function uses the grammar as described above. We
then calculate ?(h) ? ?? for all of these partial hypotheses,
and rank the set from best to worst. A FILTER function is
then applied to this ranked set to giveFi+1. Let hk be the
kth ranked hypothesis in Hi+1(x). Then hk ? Fi+1 if
and only if ?(hk) ? ?? ? ?k. In our case, we parameterize
the calculation of ?k with ? as follows:
?k = ?(h0) ? ???
?
k3
. (3)
The problem with using left-child chains is limiting
them in number. With a left-recursive grammar, of
course, the set of all possible left-child chains is infinite.
We use two techniques to reduce the number of left-child
chains: first, we remove some (but not all) of the recur-
sion from the grammar through a tree transform; next,
we limit the left-child chains consisting of more than
two non-terminal categories to those actually observed
in the training data more than once. Left-child chains of
length less than or equal to two are all those observed
in training data. As a practical matter, the set of left-
child chains for a terminal x is taken to be the union of
the sets of left-child chains for all pre-terminal part-of-
speech (POS) tags T for x.
Before inducing the left-child chains and allowable
triples from the treebank, the trees are transformed with a
selective left-corner transformation (Johnson and Roark,
2000) that has been flattened as presented in Roark
(2001b). This transform is only applied to left-recursive
productions, i.e. productions of the form A ? A?.
The transformed trees look as in figure 3. The transform
has the benefit of dramatically reducing the number of
left-child chains, without unduly disrupting the immedi-
ate dominance relationships that provide features for the
model. The parse trees that are returned by the parser are
then de-transformed to the original form of the grammar
for evaluation2.
Table 1 presents the number of left-child chains of
length greater than 2 in sections 2-21 and 24 of the Penn
Wall St. Journal Treebank, both with and without the
flattened selective left-corner transformation (FSLC), for
gold-standard part-of-speech (POS) tags and automati-
cally tagged POS tags. When the FSLC has been applied
and the set is restricted to those occurring more than once
2See Johnson (1998) for a presentation of the transform/de-
transform paradigm in parsing.
(a)
NP

NP

NP

NNP
Jim
bb
POS
?s
HHH
NN
dog
PPPP
PP
,
IN
with . . .
l
NP
(b)
NP

NNP
Jim
POS
?s
XXXXX
NP/NP

NN
dog
HHH
NP/NP
PP

IN
with . . .
l
NP
(c)
NP
      
NNP
Jim
!!!
POS
?s
l
NP/NP
NN
dog
``````
NP/NP
PP
,
IN
with . . .
l
NP
Figure 3: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner
representation; and (c) a flat structure that is unambiguously equivalent to (b)
F0 = {L00, L10} F4 = F3 ? {L03} F8 = F7 ? {L21} F12 = F11 ? {L11}
F1 = F0 ? {LKP} F5 = F4 ? {L20} F9 = F8 ? {CL} F13 = F12 ? {L30}
F2 = F1 ? {L01} F6 = F5 ? {L11} F10 = F9 ? {LK} F14 = F13 ? {CCP}
F3 = F2 ? {L02} F7 = F6 ? {L30} F11 = F0 ? {L20} F15 = F14 ? {CC}
Table 2: Baseline feature set. Features F0 ? F10 fire at non-terminal nodes. Features F0, F11 ? F15 fire at terminal
nodes.
in the training corpus, we can reduce the total number of
left-child chains of length greater than 2 by half, while
leaving the number of words in the held-out corpus with
an unobserved left-child chain (out-of-vocabulary rate ?
OOV) to just one in every thousand words.
3.2 Features
For this paper, we wanted to compare the results of a
perceptron model with a generative model for a compa-
rable feature set. Unlike in Roark (2001a; 2004), there
is no look-ahead statistic, so we modified the feature set
from those papers to explicitly include the lexical item
and POS tag of the next word. Otherwise the features
are basically the same as in those papers. We then built
a generative model with this feature set and the same
tree transform, for use with the beam-search parser from
Roark (2004) to compare against our baseline perceptron
model.
To concisely present the baseline feature set, let us
establish a notation. Features will fire whenever a new
node is built in the tree. The features are labels from the
left-context, i.e. the already built part of the tree. All
of the labels that we will include in our feature sets are
i levels above the current node in the tree, and j nodes
to the left, which we will denote Lij . Hence, L00 is the
node label itself; L10 is the label of parent of the current
node; L01 is the label of the sibling of the node, imme-
diately to its left; L11 is the label of the sibling of the
parent node, etc. We also include: the lexical head of the
current constituent (CL); the c-commanding lexical head
(CC) and its POS (CCP); and the look-ahead word (LK)
and its POS (LKP). All of these features are discussed at
more length in the citations above. Table 2 presents the
baseline feature set.
In addition to the baseline feature set, we will also
present results using features that would be more dif-
ficult to embed in a generative model. We included
some punctuation-oriented features, which included (i)
a Boolean feature indicating whether the final punctua-
tion is a question mark or not; (ii) the POS label of the
word after the current look-ahead, if the current look-
ahead is punctuation or a coordinating conjunction; and
(iii) a Boolean feature indicating whether the look-ahead
is punctuation or not, that fires when the category imme-
diately to the left of the current position is immediately
preceded by punctuation.
4 Refinements to the Training Algorithm
This section describes two modifications to the ?basic?
training algorithm in figure 1.
4.1 Making Repeated Use of Hypotheses
Figure 4 shows a modified algorithm for parameter es-
timation. The input to the function is a gold standard
parse, together with a set of candidates F generated
by the incremental parser. There are two steps. First,
the model is updated as usual with the current example,
which is then added to a cache of examples. Second, the
method repeatedly iterates over the cache, updating the
model at each cached example if the gold standard parse
is not the best scoring parse from among the stored can-
didates for that example. In our experiments, the cache
was restricted to contain the parses from up to N pre-
viously processed sentences, where N was set to be the
size of the training set.
The motivation for these changes is primarily effi-
ciency. One way to think about the algorithms in this
paper is as methods for finding parameter values that sat-
isfy a set of linear constraints ? one constraint for each
incorrect parse in training data. The incremental parser is
Input: A gold-standard parse = g for sentence k of N . A set of candidate parses F . Current parameters
??. A Cache of triples ?gj ,Fj , cj? for j = 1 . . . N where each gj is a previously generated gold standard
parse, Fj is a previously generated set of candidate parses, and cj is a counter of the number of times that ??
has been updated due to this particular triple. Parameters T1 and T2 controlling the number of iterations be-
low. In our experiments, T1 = 5 and T2 = 50. Initialize the Cache to include, for j = 1 . . . N , ?gj , ?, T2?.
Step 1: Step 2:
Calculate z = arg maxt?F ?(t) ? ?? For t = 1 . . . T1, j = 1 . . . N
If (z 6= g) then ?? = ??+ ?(g)? ?(z) If cj < T2 then
Set the kth triple in the Cache to ?g,F , 0? Calculate z = arg maxt?Fj ?(t) ? ??
If (z 6= gj) then
?? = ??+ ?(gj)? ?(z)
cj = cj + 1
Figure 4: The refined parameter update method makes repeated use of hypotheses
a method for dynamically generating constraints (i.e. in-
correct parses) which are violated, or close to being vio-
lated, under the current parameter settings. The basic al-
gorithm in Figure 1 is extremely wasteful with the gener-
ated constraints, in that it only looks at one constraint on
each sentence (the arg max), and it ignores constraints
implied by previously parsed sentences. This is ineffi-
cient because the generation of constraints (i.e., parsing
an input sentence), is computationally quite demanding.
More formally, it can be shown that the algorithm in
figure 4 also has the upper bound in theorem 1 on the
number of parameter updates performed. If the cost of
steps 1 and 2 of the method are negligible compared to
the cost of parsing a sentence, then the refined algorithm
will certainly converge no more slowly than the basic al-
gorithm, and may well converge more quickly.
As a final note, we used the parameters T1 and T2 to
limit the number of passes over examples, the aim being
to prevent repeated updates based on outlier examples
which are not separable.
4.2 Early Update During Training
As before, define yi to be the gold standard parse for the
i?th sentence, and also define yji to be the partial analy-
sis under the gold-standard parse for the first j words of
the i?th sentence. Then if yji /? Fj(xi) a search error has
been made, and there is no possibility of the gold stan-
dard parse yi being in the final set of parses, Fn(xi). We
call the following modification to the parsing algorithm
during training ?early update?: if yji /? Fj(xi), exit the
parsing process, pass yji , Fj(xi) to the parameter estima-
tion method, and move on to the next string in the train-
ing set. Intuitively, the motivation behind this is clear. It
makes sense to make a correction to the parameter values
at the point that a search error has been made, rather than
allowing the parser to continue to the end of the sentence.
This is likely to lead to less noisy input to the parameter
estimation algorithm; and early update will also improve
efficiency, as at the early stages of training the parser will
frequently give up after a small proportion of each sen-
tence is processed. It is more difficult to justify from a
formal point of view, we leave this to future work.
Figure 5 shows the convergence of the training algo-
rithm with neither of the two refinements presented; with
just early update; and with both. Early update makes
1 2 3 4 5 682
83
84
85
86
87
88
Number of passes over training data
F?m
eas
ure 
pars
ing a
ccur
acy
No early update, no repeated use of examplesEarly update, no repeated use of examplesEarly update, repeated use of examples
Figure 5: Performance on development data (section f24)
after each pass over the training data, with and without
repeated use of examples and early update.
an enormous difference in the quality of the resulting
model; repeated use of examples gives a small improve-
ment, mainly in recall.
5 Empirical results
The parsing models were trained and tested on treebanks
from the Penn Wall St. Journal Treebank: sections 2-21
were kept training data; section 24 was held-out devel-
opment data; and section 23 was for evaluation. After
each pass over the training data, the averaged perceptron
model was scored on the development data, and the best
performing model was used for test evaluation. For this
paper, we used POS tags that were provided either by
the Treebank itself (gold standard tags) or by the per-
ceptron POS tagger3 presented in Collins (2002). The
former gives us an upper bound on the improvement that
we might expect if we integrated the POS tagging with
the parsing.
3For trials when the generative or perceptron parser was given POS
tagger output, the models were trained on POS tagged sections 2-21,
which in both cases helped performance slightly.
Model Gold-standard tags POS-tagger tags
LP LR F LP LR F
Generative 88.1 87.6 87.8 86.8 86.5 86.7
Perceptron (baseline) 87.5 86.9 87.2 86.2 85.5 85.8
Perceptron (w/ punctuation features) 88.1 87.6 87.8 87.0 86.3 86.6
Table 3: Parsing results, section 23, all sentences, including labeled precision (LP), labeled recall (LR), and F-measure
Table 3 shows results on section 23, when either gold-
standard or POS-tagger tags are provided to the parser4.
With the base features, the generative model outperforms
the perceptron parser by between a half and one point,
but with the additional punctuation features, the percep-
tron model matches the generative model performance.
Of course, using the generative model and using the
perceptron algorithm are not necessarily mutually ex-
clusive. Another training scenario would be to include
the generative model score as another feature, with some
weight in the linear model learned by the perceptron al-
gorithm. This sort of scenario was used in Roark et al
(2004) for training an n-gram language model using the
perceptron algorithm. We follow that paper in fixing the
weight of the generative model, rather than learning the
weight along the the weights of the other perceptron fea-
tures. The value of the weight was empirically optimized
on the held-out set by performing trials with several val-
ues. Our optimal value was 10.
In order to train this model, we had to provide gen-
erative model scores for strings in the training set. Of
course, to be similar to the testing conditions, we can-
not use the standard generative model trained on every
sentence, since then the generative score would be from
a model that had already seen that string in the training
data. To control for this, we built ten generative models,
each trained on 90 percent of the training data, and used
each of the ten to score the remaining 10 percent that was
not seen in that training set. For the held-out and testing
conditions, we used the generative model trained on all
of sections 2-21.
In table 4 we present the results of including the gen-
erative model score along with the other perceptron fea-
tures, just for the run with POS-tagger tags. The gen-
erative model score (negative log probability) effectively
provides a much better initial starting point for the per-
ceptron algorithm. The resulting F-measure on section
23 is 2.1 percent higher than either the generative model
or perceptron-trained model used in isolation.
6 Conclusions
In this paper we have presented a discriminative train-
ing approach, based on the perceptron algorithm with
a couple of effective refinements, that provides a model
capable of effective heuristic search over a very difficult
search space. In such an approach, the unnormalized dis-
criminative parsing model can be applied without either
4When POS tagging is integrated directly into the generative pars-
ing process, the baseline performance is 87.0. For comparison with the
perceptron model, results are shown with pre-tagged input.
Model POS-tagger tags
LP LR F
Generative baseline 86.8 86.5 86.7
Perceptron (w/ punctuation features) 87.0 86.3 86.6
Generative + Perceptron (w/ punct) 89.1 88.4 88.8
Table 4: Parsing results, section 23, all sentences, in-
cluding labeled precision (LP), labeled recall (LR), and
F-measure
an external model to present it with candidates, or poten-
tially expensive dynamic programming. When the train-
ing algorithm is provided the generative model scores as
an additional feature, the resulting parser is quite com-
petitive on this task. The improvement that was derived
from the additional punctuation features demonstrates
the flexibility of the approach in incorporating novel fea-
tures in the model.
Future research will look in two directions. First, we
will look to include more useful features that are diffi-
cult for a generative model to include. This paper was
intended to compare search with the generative model
and the perceptron model with roughly similar feature
sets. Much improvement could potentially be had by
looking for other features that could improve the mod-
els. Secondly, combining with the generative model can
be done in several ways. Some of the constraints on the
search technique that were required in the absence of the
generative model can be relaxed if the generative model
score is included as another feature. In the current paper,
the generative score was simply added as another feature.
Another approach might be to use the generative model
to produce candidates at a word, then assign perceptron
features for those candidates. Such variants deserve in-
vestigation.
Overall, these results show much promise in the use of
discriminative learning techniques such as the perceptron
algorithm to help perform heuristic search in difficult do-
mains such as statistical parsing.
Acknowledgements
The work by Michael Collins was supported by the Na-
tional Science Foundation under Grant No. 0347631.
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?617.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 263?270.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In The Proceedings of the
17th International Conference on Machine Learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?8.
Michael Collins. 2004. Parameter estimation for sta-
tistical parsing models: Theory and practice of
distribution-free methods. In Harry Bunt, John Car-
roll, and Giorgio Satta, editors, New Developments in
Parsing Technology. Kluwer.
Fabrizio Costa, Vincenzo Lombardo, Paolo Frasconi,
and Giovanni Soda. 2001. Wide coverage incremental
parsing by learning attachment preferences. In Con-
ference of the Italian Association for Artificial Intelli-
gence (AIIA), pages 297?307.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19:380?393.
Yoav Freund and Robert Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 3(37):277?296.
Yoav Freund, Raj Iyer, Robert Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In Proc. of the 15th Intl. Con-
ference on Machine Learning.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 279?286.
Mark Johnson and Brian Roark. 2000. Compact non-
left-recursive grammars using the selective left-corner
transform and factoring. In Proceedings of the 18th
International Conference on Computational Linguis-
tics (COLING), pages 355?361.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 535?541.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):617?636.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward.
1994. A maximum entropy model for parsing. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP), pages 803?806.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151?175.
Stefan Riezler, Tracy King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson.
2002. Parsing the wall street journal using a lexical-
functional grammar and discriminative estimation
techniques. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 271?278.
Brian Roark, Murat Saraclar, and Michael Collins. 2004.
Corrective language modeling for large vocabulary
ASR with the perceptron algorithm. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 749?752.
Brian Roark. 2001a. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Brian Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
Compact non-left-recursive grammars using the selective 
left-corner transform and factoring* 
Mark  Johnson  Br ian  Roark  
C()gnitiv(; and  L ingu is t ic  Sci('.n(:('.s, Box  1978 
Brown Un ivers i ty  
Mark_Johnson@\[3rown.ed u Bria n_Roark@grown.edu 
Abst ract  
Tim lcft-c.orner transibrm reiIloves left-r(;cursion 
fl'om (l)rol)al)ilisti(') (:ontext-free granunars and uni- 
t|cation grammars, i)ermitting siml)l(~ tol)-down 
parsing te(:hniques to l)e used. Unforl.unately the 
grammars l)roduced by the stal~dard \]etTt-('orner 
transform are usually much larger than |;he original. 
The select, lye left-corner i;ransform (lescril)ed in this 
l)aI)er 1)rodu(:es a transformed grammar which simu- 
lates left-corner ecognition of a user-st)coiffed set of 
tim original productions, and tOl)-down r(~cognition 
of the, others. C()mbined with tw() factorizations, it;
1)rOdll(;es l lOl l - lef l ; - reci l rs ive grallll l l~ll'S |;lilt|; ~/re 11o|; 
much larger than the original. 
1 In t roduct ion  
TOl)-down i)arsing techniques are al;tl'a(:tiv(! because 
of their simt)licity, and can often a(:hi(~ve good 1)er- 
formance in 1)racti(:e (l{oark and .\]()hns(m, 1999). 
However, with a left-re(:ursive grammar such l)ars('as 
tyl)i(:ally fail to termim~te. Tim left>corner gram- 
l t lar  l ; ra l l s for ln  eo l iver ts  a le f l ; - recurs ive  ~l'iillllll~lr 
into a non-lefl;-recursive one: a top-down t)arser 
using a left-corner transformed grammar simulates 
a lefl;-(:orner parser using the original granllnar 
(l/os(mkrantz and Lewis II, 1970; Aho and Ulhnan, 
1972). Ih)wever, the left-corner transformed gram- 
mar can 1)e significantly larger than the original 
grammar, ca.using mlmero~ls l)rol)lelns. For exam- 
ple, we show 1)clew that a probat)ilistic ontext-fr(.~e 
grammm: (PCFG) estimated froln left-corner trans- 
formed Petal WSJ tree-bank trees exhil)its consid- 
erably greater st)arse data prol)lems than a PCFG 
estimal, e(t in the usual manner, siint)ly because the 
left-corner transformed grammar contains approxi- 
mately 20 times more 1)reductions. The transform 
described in this paper t)roduees a grammar al)prox- 
imately the same size as the inlmt grmmnar, which 
is not as adversely at\[ected by sparse data. 
* This  research was slli)i)orl;ed t)y NSF awards !1720368, 
9870676 and 98121(19. We would like to t lmnk o1|1" (:olleagues 
in I~I,IAP (Brown l ,aboratory for Linguistic Information Pro- 
ccssing) and Bet) Moore tbr their hcll)ful comments  on this 
pal)Or. 
Left-corner transforms a.re particularly useflll be- 
cause they can i)reserve annotations on productions 
(n:ore on this 1)(flow) and are thereibre apt)lieable to 
more COml)Iex graminar formalisms as well its CFGs; 
a t)roI)erty which other al)l)roaehes to lefl;-recursion 
elimination tyl)ically lack. For examl)le , they al)l)ly 
to l(~ft-r(~cursive unification-based granmmrs (Mat;- 
sumoto et al, 1983; Pereira and Shieber, 1987; .h)hn- 
son, 1998a). Because the emission 1)robabilit;y of a 
PCFG 1)ro(hm(;ion ca15 be regarded as an anllotatioll 
on a CFG 1)reduction, the left-corner transform can 
t)rodue(', a CFG with weighted l)roductions which 
assigns the same l)robal)iliti(~s to strings an(l trans- 
tbrmed trees its the original grammar (Abney et al, 
11999). Ilowever, the transibrmed grammars (:an be 
much larger than the original, which is unac('el)table 
tbr many aI)t)lieations involving large grammars. 
The selective left-corner transform reduces the 
transl'ornm(l grammar size because only those l)ro- 
(lu(:tions which apt)ear in a left-recto'sire (:y(:le llee(l 
1)e recognized left-(:orner in order to remove left- 
recurs|on. A tOl)-down parser using a grammar pro- 
dueed by the selective left-(:orner |;ranst.'orm simu- 
lates a generalized left-corner parser (Demers, 1977; 
Nijholt, 1980) wlfich recognizes t user-specified sul)- 
set; of the original productions in a left-corner fash- 
ion, and the other productions tol)-down. 
Although we do not investigate it in this 1)al)er, 
the selective left-(:orner transform should usually 
lmve a slnaller sear(:h sl)ace relative, to tim standard 
left-corner transform, all else being equal. The par- 
tial l)arses t)roduced uring a tot)-down parse consist 
of a single connected tree fragment, while the par- 
tial parses l)rodueed produced during a let't-corner 
t)arse generally consist of several discommcted tree 
fragments. Since these fragments arc only weakly re- 
lated (via the "link" constraint descril)ed below), the 
search for each fragment ix relatively independent. 
This lllay l)e rest)onsil)le for the ol)servation that 
exhaustive left-corner 1)arsing is less efficient titan 
top-down l)arsing (Covington, 1994). Intbrmally, be- 
cause the selective left-corner transforln recognizes 
only a sul)set of 1)reductions in a lefl;-corner fashion, 
its partial parses contain fewer tree discontiguous 
355 
fl'agnlents and the search inay be more efficient. 
While this paper focuses oil reducing grammar 
size to nlinimize sparse data problems in PCFG 
estilnation, tile modified left-corner transforms de- 
scribed here are generally api)licable wherever the 
original left-conler transform is. For example, tile 
selective left-corner transform can be used in place 
of the standard left-comer transform in the con- 
struction of finite-state approximations (Johnson, 
1998a), often reducing the size of the intermedi- 
ate automata constructed. The selective left-corner 
transform can be generalized to head-corner parsing 
(vail Noord, 1997), yielding a selective head-corner 
parser. (This follows from generalizing the selective 
left-corner transform to Horn clauses). 
After this paI)er was accepted for publication we 
learnt of Moore (2000), which addresses the issue 
of grammar size using very similar techniques to 
those proposed here. The goals of tile two pat)ers 
are slightly different: Moore's approach is designed 
to reduce the total grammar size (i.e., the sunl of 
the lengths of the productions), wtfile our approach 
minimizes the number of productions. Moore (2000) 
does not address left-corner tree-transforms, or ques- 
tions of sparse data and parsing accuracy that are 
covered ill section 3. 
2 The  se lec t ive  le f t - corner  and  
re la ted  t rans forms 
This section introduces the selective left-corner 
transform and two additional factorization trans- 
forms which apply to its output. These transfbrnm 
are used ill tile experiInents described in tile follow- 
ing section. As Moore (2000) observes, in general 
the transforms produce a non-left-recursive output 
grammar only if tile input grammar G does not con- 
tain unary cycles, i.e., there is no nonterminal A 
such that A -~+ A. 
2.1 The selective left-corner t rans form 
The selective left-corner transform takes as input a 
CFG G = (V, T, P, S) and a set of left-corner produc- 
tions L C_ P, which contains no epsilon t)roductions; 
the non-left-corner prodnctions P - L are called top- 
down productions. The standard left-corner tr'ans- 
form is obtained by setting L to the set of all 
non-epsilon productions in P. The selective left- 
corner trnnsform of G with respect o L is the CFG 
?.Cn (G) = (V1, T, P1, S), where: 
V1 = VU{D X:DEV,  XcVUT} 
and P1 contains all instances of tile schemata 1. In 
these schemata, D E V, w E T, and lower case 
greek letters range over (V tO T)*. The D-X are 
new nont;ernlinals; informally they encode a parse 
state in which an D is predicted top-down and an X 
? - -D  . . . . . .  D . . -  
/31 f~o a D A 
: ~ ~ ~  
? " f in  D /3n  
CC '.. 
A ~,~ D-Bj 
a rio D-D 
Figure 1: Schematic parse trees generated by the origi- 
nal grmnmar G and the selective left-corner transformed 
grammar gCL(G). The shaded local trees in the orig- 
inal parse tree correspond to left-corner productions; 
the corresponding local trees (generated by instances of 
schema lc) in the selective leff-conler transfornled tree 
are also shown shaded. The local tree colored black is 
generated by an instance of schema lb. 
has been found left-corner, so D X ~cr . (c , )  7 only 
if D ~b XT. 
D ~ w D-w (la) 
D -~ (.~D-A whereA-+aEP-L  (lb) 
D-/3 -+ \[3 D C whereC+/3 /3EL  (lc) 
D-D ---> e (ld) 
Tile schemata flmction as follows. The productions 
introduced by schema 1~ start a left-corner parse of 
a predicted nonterminal D with its let'mlost ermi- 
nal w, while those introduced by schenla lb start; a 
left-corner parse of D with a left>corner A, which is 
itself found by the top-down recognition of produc- 
tion A -+ (t E P -  L. Scheina lc extends the current 
left-corner B tit) to a C with tile left>corner recogni- 
tion of production C ~ /3 ft. Finally, scheina ld 
inatches tile top-down prediction with tile recog- 
nized left-corner category. 
Figure 1 schematically depicts the relationship be- 
tween a chain of left-comer t)roductions in a parse 
tree generated by G and the chain of correst)onding 
instances of schema le. The left-comer ecognition 
of the chain starts with the recognition of (t, tile 
right-hand side of a top-down production A --+ ~, 
using an instance of schema lb. Tile left-branching 
chain of left-corner productions corresponds to a 
right-branching chain of instances of schema lc; the 
left-corner transforln in effect converts left recursion 
into right recursion. Notice that tile top-down pre- 
dicted category D is passed own this right-recursive 
chain, effectively multiplying each left-conler pro- 
ductions by the possible top-down predicted cate- 
gories. Tile right recursion terininates with an ill- 
stance of schema ld when tile left-comer and top- 
down categories match. 
Figure 2 shows how tot)-down productions from 
G are recognized using ?CL(G). When the se- 
356 
. . .A . . .  - . .A - - .  . . .A . . .  
(&, ?C O: A--A r:-re, nmval (t: 
.K2ZX LECx 
Figure 2: The recognition of a top-down produc, tion A --+ 
a: by ?CL(G) involves a left-corner category A-A, which 
immediately rewrites to e. One-step e-removal applied 
to ?CL(G) l)roduces a grmnmar in which each top-down 
production A -+ ct corresponds to a production A --+ tt 
in the transformed grammar. 
lective left-corner tra,nsform is tbllowed by a one- 
step c-renlowd transfornl (i.e., coml)osition or partial 
evaluation of schema 1t) with respect o schema ld 
(Johnson, 1998a; Abney and 3oMson, 1991; Resnik, 
1992)), each top-down production f'rolll G appears 
uilclmnged in tile tinal grammar. Full e-relnoval 
yields the grannnar giwm 1) 3, the schemata below. 
.D -~ w D-w 
D -~ 'w where. D ~ j  w 
D ~ ~DA whereA-+(~c l  ) -L  
.D -+ a where D =>* A P L A, -+ ~ G -- L 
D-B --+ fl D C whereC- ->Bf lcL  
D-B -} fl wl le reD~},C ,C~Bf l6L  
Moore (2000) introduces a version of the left- 
corner transform called LCLIt, which al)plies only to 
productions with left-recursive parent and left clfihl 
categories. \]n the~ (:ontext of the other transforms 
that Moore introduces, it seems to have the, sallle 
effect in his system as the s(Je(;tive lefl;-corll(W trails- 
form does lmre. 
2.2  Select ive le f t -corner  t ree  transfor l l lS 
There is a 1.-to-1 correspondence b tween the 1)arse 
trees generated by G and ?CL(G). A tree t is gener- 
ated by G iff there is a corresponding t' generated by 
?CL(G), where each occurrence of a top-down pro- 
duction in the derivation of t corresponds to exactly 
one local l, ree gelmrated by occurrence of the cor- 
responding instance of schema 11) ill the derivation 
of t', and each occurrence of a M't-corner produc- 
tion in 1 corresponds to exactly one occurrence of 
the corresponding instance of schema le in t'. It; is 
straightforward to detine a 14o-1 tree l;ransform TL 
mapping parse trees of G into parse trees of ?dL (G) 
(.Johnson, 1998a; Roark and Johnson, 1999). In the 
empirical evaluation below, we estinmte a PCFG 
Dora the trees obtained by applying 7}, to the trees 
in the Petal WSJ tree-lmnk, and compare it to tile 
PCFG estinmted from the original tree-bank trees. 
A stochastic top-down parser using the I 'CFG es- 
timated from the trees produced by ~,  simulates 
a stochastic generalized left-corner Imrser, wlfich is 
a generalization of a standard stochastic lefl;-corner 
1)arser that pernfits productions to t)e ret;ognize, d 
top-down as well as left-corner (Manning and Car- 
penter, 1997). Thus investigating the 1)roperties of 
PCFG estimated from trees transformed with "YL is 
an easy way of studying stochastic trash-down au- 
tomata performing eneralized lefi;-corner parses. 
2.3  Prun ing  useless product ions  
We turn now to the problmn of reducing the size of 
tile grmnmars produced by left-corner transforms. 
Many of the productions generated by schemata 1
art: useless, i.e., they never appear in any termi- 
nating deriw~tion. Wtfile they can be removed by 
standard methods for deleting useless productions 
(Ilopcroft and Ulhnan, 1979), the relationship be- 
tween the parse trees of G and ?CL(G) depicted in 
Figure 1 shows how to determine ahead of time the 
new nonterminals D X that can at)pear in useful 
productions of ECL (G). This is known as a link con- 
straint. 
D)r (P)CFGs there is a particularly simple link 
constrainl;: \]) X apt)ears in useflfl productions of 
?CL(G) only if ~7 < ( 17 U T)*.D =>* XT. If ? L 
epsilon removal is applied to the resulting gram- 
mar, D X appears in usefill productions only if 
H7 C (17 U T) + .D ~}, X7. Thus one only need 
ge.nerate instances of the left-corner schemata which 
satist~y the corresponding link constraints. 
Moore (2000) suggests all additional constraint on 
nonte.rminals D X that can al)l)ear in useflll 1)roduc - 
l;iolts of ?CL(G): D lllllsl; eitller be th(! start synJ)ol 
of G or else al)pear in a production A --+ o'D/3 of G, 
for .,,;, A c- V, c {Vu T}+ c Tp .  
It is easy to see that the l}roducl,ions that Moore's 
constraint prohibits are useless. There is one non- 
ternfinal in the tree-bank gramnmr investigated be- 
low that has this property, namely LST. However, 
ill the tree-lmnk granmmr none of the productions 
exlmnding LST are left-recursive (in fact, the first; 
dfild is ahvays a pretermiiml), so Moore's constraint 
does not atgect he size of the transformed grammars 
investigated below. 
While these constraints can dramatically reduce 
both the number of productions and the size of the 
1)arsing search space of the 1;ransformed grmnmar, 
in general the transfl)rmed grammar ?CL (G) can 1)e 
quadratically larger than G. There are two causes 
for the explosion ill grmnmar size. First, ?CL(G) 
contains an instance of sdmma lb tbr each top- 
down production A --+ a and each D such that 
37. D ~}, A 7. Second, ?CI,(G) contains an in- 
stance of schema lc for each left-corner production 
C -~ fi and each D such that BT.D ~,  C7. In 
etDct, ?CL(G) contains one copy of each production 
for each possible left-comer ancestor. Section 2.5 
describes filrther factorizations of the l)roductions 
of ?CL (G) which mitigate these causes. 
357 
2.4 Optimal choice of L 
Because ::>~, increases monotonically with =>L and 
hence L, we typically reduce the size of ?CL(G) by 
making the left-corner production set L as small as 
possit)le. This section shows how to find the unique 
minimal set of left-corner productions L such that 
?CL(G) is not left-recursive. 
Assume G = (V,T, P, S) is wuned (i.e., P con- 
tains no useless productions) and that there is no 
A 6 V such that A --++ A (i.e., G does not gen- 
erate recursive unary branching chains). For rea- 
sons of space we also assume that P contains no 
e-productions, but this approach can be extended to 
deal with them if desired. A production A -+/3fl C 
P is left-rccursive iff ~3' C (V U T)*. \]3 ~,  AT, i.e., 
P rewrites B into a string beginning with A. Let L0 
be the set of left-recursive prodtlctious in G. Then 
we claim (1) that ?CLo (G) is not left-recursive, and 
(2) that for all L C Lo, ?CL(G) is leff-recursive. 
Claim 1 follows t?om the fact, that if A ~s,0 B7 
then A =:>,, /37 and tile constraints ill section 2.3 
on useful productions of ?CLo(G). Claim 2 tbllows 
from the fact that if L C L0 then there is a chain of 
left-recursive productions that includes a top-down 
production; a simple induction on tile length of the 
chain shows that gCL (G) is left-recursive. 
This result justifies the common practice in natu- 
ral language lefl;-corner t)arsing of taking tile termi- 
nals to be the preterminal t)art-of-speech tags, rather 
than the lexical items themselves. (We did not at- 
tempt to calculate tile size of such a left-comer gram- 
mar in tilt empirical evaluation below, lint it would 
be much larger than any of the grammars described 
there). In fact, if the preterminals are distinct from 
the other nonterminals (as they are ill the tree-bank 
grammars investigated below) then L0 does not in- 
clude any productions beginning with a preterminal, 
and ?CLo (G) contains no instances of schema la at 
all. We now turn our attention to tlm other sclmmata 
of the selective left-corner grammar transform. 
2.5 Factoring the output of ?CL 
This section defines two factorizations of the outtmt 
of the selective left-corner grammar transform that 
can dramatically reduce its size. These factoriza- 
tions are most effective if the number of t)roductions 
is much larger than the number of nonterminals, as 
is usually the case with tree-bank grmnmars. 
Tilt top-down factorization decomposes 
schema lb by introducing new imnterminals 
D t, where D C V, that have the stone expansions 
that D does in G. Using the same interpretation for 
variables as in schemata 1, if G = (I~ T, P, S) then 
(? (a) = T, S), where: 
14a = Iq tO{D' :DEV} 
and Ptd contains all instances of the schemata la, 
3a, 3b, lc and 1(t. 
D --+ A'D-A whereA-+aEP-L  (3a) 
A' -+ a, whereA- ->creP-L  (3b) 
Notice that the number of instances of schema 3a is 
less than the square of tile number of nonterminals 
and that the number of instances of sdmma 31) is the 
number of top-down productions; the sum of these 
numbers is usually much less than tile mlmber of 
instances of schema lb. 
Top-down factoring p lws approximately tile same 
role as "non-left-recursion grouping" (NLRG) does 
in Moore's (2000) approach. The meier difl!erence 
is that NLRG applies to all productions A ~ /3/9 
in wtfich /3 is not left-recm'sive, i.e., ~7./7 =>~ /3% 
while in our system toll-down factorization applies to 
those productions tbr which ~7. B ~,  AO', i.e., the 
productions not directly involved in left recursion. 
Tim left-corner factorization decomposes 
schema lc in a similar way using new nonter- 
minals D\X, where D e V and X ~ V U T. 
c)(c) = T, S), where: 
I'}o = ~qU{D\X:D6V,  X6VUT} 
and Plc contains all instances of tile schemata la, 
ib, 4a., 4b and id. 
D /3 -+ C \BD C whereC-+B\ [9?L  (4a) 
CxB --+ fl whereC- -+Bf lEL  (4b) 
The number of instances of schema 4a is bounded 
by the numtmr of instances of schema lc and is typ- 
ically nmch smaller, while the number of instances 
of schema 41) is precisely the munber of left-corner 
productions L. 
Left-corner factoring seems to correspond to one 
step of Moore's (2000) "left factor" (LF) operation. 
Tile left; factor operation constructs new nontermi- 
nals corresponding to common prefixes of" arbitrary 
length, while left-corner factoring effectively only 
factors the f rst  nonterminal symbol on the right 
hand side of left-corner productions. While we have 
not done experiments, Moore's left factor operation 
would seem to reduce the total number of symbols 
in the transformed grammar at tile expense of pos- 
sibly introducing additional productions, while our 
left-corner factoring reduces the number of produc- 
tions. 
These two factorizations can be used together 
in the obvious way to define a grmnmar trans- ~__.C(ld,le) form "L , whose productions are defined by 
schemata la, 3a, 3b, 4a, 4b and ld. There are corre- 
spondiug tree transtbrms, which we refer to as TI! td) , 
etc., below. Of course, the pruning constraints de- 
scribed in section 2.3 are applicable with these fac- 
torizations, and corresponding invertible tree trans- 
forms can be constructed. 
358 
3 Empi r i ca l  Resu l t s  
To examine the effect of the tra.nsforms outlined 
above, we experimented with vm'ious PCFGs in- 
dueed from sections 2--21 of a modified Pcml WSJ 
tree-bank as described in Johnson (19981)) (i.e., 
labels simplifiecl to grammatical ca.tegorics, R.OOT 
lu)des added, empty nodes and vacuous unary 
bra.nehcs deleted, and auxiliaries retagged as AUX 
or AUX('). \~,Ze. ignored lexic.al items, and treated 
the part-of-speech tags as terminals. As Bob Moore 
pointed out Lo us, the left-corner transform may pro- 
duc.e left-recursive grmnmars if its inlmt grammar 
contains mmry cycles, so we removed them using the 
a transforln that Moore suggested. Given an iifitial 
set of (non-epsihm) productions P,  the transtbrmed 
grammar contains the following in:odu(:tions, wherc~ 
l;he A ~ are 1lew llOll-terlilillals: 
A "-~ (t where A--} (t G P, A 75~; A 
A~D~ whereA=>~,D~iA  
./1 h -~ ~: where A -~ (~ G P, A ~;  .,1, (t ~>~, A
This transform can be extended t,o one on PCFGs 
which preserves derivation probabilities. In this sec- 
tic)n, we fix P to) be the produeticms l;lmt re.sult; afl;er 
al)plying this unary t:yc:le removal transforma.tion to
the tree-l)ank 1)roductions, and G to \])e the ('orre- 
st)onding grammm'. 
Tables 1 and 2 give the sizes of selective left;- 
(:orner grmnlnar trmlsforms of G for various wthles 
of l;he left-et)rner set L and fa(:torizal;ions, without 
and with epsilon-remowfl respectively. In l;he ta- 
bles, L/j is the st'./; of hd't-rc.cm'siv(' 1)roductions in 
P,  as detined in set:lion 2.4. N is the sel of 1)roclu(: - 
l;ions in 1~ whose hfft-ha\]M sides do not begin with 
a part-ofspee(:h (P()S) tag; 1)ecause I 'OS tags are 
distinct front other nontermimtls in l;he tree-lmnk, 
N is an easily identified set of I)roductions guaran- 
teed to include L0. The tables also gives the sizes 
of maximum-likelihood PCFGs estimated from the 
tr(;es resulting fl:om applying the sele(:tive left-corner 
tree transforms 7- 1,(} the tree-bank, l)reaking mmry 
t:yeles as clescribed above. For the I)arsing exl)eri- 
ments below we always deleted empty nodes in the 
outl)ut of these tree transforms; this corresponds to 
el)silon removal in the grammar transform. 
First, note that/2Cv(G),  the result of al)plying the 
standard left-corner g lmmnar transform to G, has 
al)proximately 20 times the number of t)roductions 
?C (m't~)(G), the result of aI)- tha.t G has. Itowever "co 
plying the selective left-corner grammar transforma- 
tion with factorization, has approximately 11.4 times 
the munber of productions that G has. Thus the. 
methods described in this paper cml in fact dramati- 
cally reduce the. size of left-corner transformed gram- 
mars. Second, note that ?C(~t'I")(G) is not much 
th.,,  :his t,et:.,,se N larger is l lOt  IJO \ \] 
G 
?C 1' 
?CN 
?CLo 
T,\, 
,,o,~e (ta) (z~) (t(z, z(..) 
1.5,040 
346,344 30,716 
345,272 113,616 254,067 22,4:11 
314,555 103,504 232,41.5 21,364 
20,087 17,146 
19,61.9 16,349 19,002 15,732 
18,945 16,126 18,437 15,618 
Table \] : Sizes of PCFGs inferred using vm'ious grammar 
and tree transtbrms after pruning with link constraints 
without epsihm removal. Cohmms indicate thctorization. 
In the grammar and tree transfl)rms, P is the set, of pro- 
ductions in G (i.e., the standard M't-corner transform), 
N is the set of all productions in P which do not be- 
gin with a POS tag, mM L0 is the set of left-recursive 
t)roclu(:tions. 
?C1' 
?CN 
?CL, 
"I-N 
"~\])o 
,,o,,e (?,~z) (l~) (~< >) 
564,430 38,489 
563,295 1.76,644: 411,986 25,335 
505,435 157,899 371,102 23,566 
22,035 17,398 
2:1,58!) 16,688 20,696 15,795 
21,061 16,566 20,168 15,673 
'.l'alfle 2: Sizc's of PCFGs inferred using various grammm: 
and tree trmtsforms aftc.r pruning with link constraints 
with epsihm removM, using the same notation as Table 1. 
much larger than L0, which in turn is be(:ausc, most 
pairs of non-P()S nonternfinals A, B are nmt;ually 
left-recursive. 
'l)lrning now to the PCFGs estimated after at)- 
plying tree transtbrms, we notice that grammar size 
(Loes l l ( )t  Jll(;Fe}Lqe. Ile}llJ\]y St) dramatically. These 
PCFGs encode a. maximum-likelihood estimate of 
the state transiti(m probabilities for vmious stochas- 
tic generalized h;t't-(-orner t)m'sers, since a tol).-clt)wn 
parser using these, grammars simulates a general- 
ized left-corner 1)arser. The fact that ?Cp(G) is 
17 timc.s larger than the. PCFG infe.rred a.fter apply- 
ing "T}, to the tree-lmnk means that most of tile l}OS -
sible transitions of a standard stochastic left-corner 
parser are not observc.d in the tree-bank la"'ammg" 
data. The state of a left-corner parser does capture 
some linguistic generalizations (Mmming an<l Car- 
penter, 1997; Roark a.nd Johnson, 1999), but one 
might still expect sparse-data problems. Note that 
"Lo is only 1.4 times larger than T, (t~'z~) Lo , SO We 
expect less serious sp~rse data problems with the 
fat:toted selective left-corner transibrm. 
We quantii~ these sparse data prol)lems in two 
ways using a held-out test eorIms, viz., all sentences 
in section 23 of the trce-lmnk. First, table 3 lists the 
mmfl)er of sentences in the test corpus that fail to 
receive a parse with the wwious PCFGs  mentioned 
359 
TransfBrln 
I lone 
%, 
TN 
%.0 
none (t(0 (t~) (td, lc) 
0 
2 0 
2 0 2 
0 0 0 
Table 3: The number of sentences 
not receive a parse using various 
fl'om sections 2-21. 
in section 23 that do 
grammars estimated 
Transforin 
none 
7?, 
TN 
Tp~ 
TN~ 
7I~0 e 
nolle (td) (lc) (td, Ic) 
514 
665 535 
664 543 639 518 
640 547 615 522 
719 539 
718 554 685 521 
706 561 666 521 
Table 4: The lmml)er of productions found in the trans- 
formed trees of sentences in section 23 that do not appear 
in the corresponding transforined trccs f,'om sections 2 
21. (The subscript epsilon indicates epsilon remowfl was 
applied). 
above. This is a relatively crude lneasure, but cor- 
relates roughly with the ratios of gralnlnar sizes, as 
expected. 
Second, table 4 lists the number of productions 
found in the tree-transformed test cortms that (lo 
not at)pear in the correspondingly transformed trees 
of sections 2 2t. What is striking here is tlmt the 
number of missing I)roductions aft;er either of the 
l ;ransforlns , Lo or , N is apl)roxil l lal;ely the 
sa, ine as t im number  of in iss ing 1)reductions us ing 
the untransformed trees, indicating that the factored 
selective left-corner transfl)rms cause little or no ad- 
ditional sparse data problem. (The relationship be- 
tween local trees ill the parse trees of G and ?dc(G) 
mentioned earlier implies that left-corner tree trans- 
tbrmations wilt not decrease the number of missing 
productions). 
We also investigate the accuracy of the maximum- 
likelihood parses (MLPs) obtained using the PCFGs 
estimated from tile output of the various left-corner 
tree transforms. 1 We searched for these parses us- 
ing all exhaustive CKY parser. Because the parse 
trees of these PCFGs are isomorphic to the deriva- 
tions of the corresponding stochastic generalized 
left-corner parsers, we are in fact evaluating different 
kinds of stochastic generalized left-corner parsers in- 
ferred from sections 2-21 of the tree-bank. We used 
1\?e (lid not investigate the grammars produced by the 
various left-corner grammar transforms. Because a left-corner 
grammar transform ECL preserves production probal)ilities, 
the highest scoring parses obtained using the weighted CFG 
EeL(G) should be the highest scoring parses obtained using 
G transformed by TL. 
nolle 
%, 
TN 
7}:0 
,,one (t~Z) (Z~) (ta, to) 
70.8,75.3 
75.8,77.7 74.8,76.9 
75.8,77.6 73.8,75.8 75.5,77.8 72.8,75.4 
75.8,77.4 73.0,74.7 75.6,77.8 72.9,75.4 
Table 5: Labelled recall and precision scores of PCFGs 
estimated using various tree-transforms ill a transform- 
detransform framework using test data from section 23. 
tile transforn>detransfornl franmwork described in 
Johnson (1998b) to evaluate the parses, i.e., we ap- 
plied tile at)propriate inverse tree transfornl ,\]---1 
to detransform the parse trees produced using the 
PCFG estimated froul trees transtbrnmd by T. By 
calculating the labelled precision and recall scores 
tbr the detransformed trees in the usual rammer, we 
can systematically compare the parsing accuracy of 
diflbrent kinds of stochastic generalized left-corner 
parsers. 
Table 5 presents the results of this comparison. As 
reported previously, the standard left-corner grmn- 
inar embeds sufficient non-local infornlation in its 
productions to significantly improve the labelled pre- 
cision and recall of its MLPs with respect o MLPs of 
the PCFG estimated from the untransfornmd trees 
(Maiming and Carpenter, 1997; ll.oark and John- 
son, 1999). Parsing accuracy drops off as granunar 
size decreases, presuntably because smaller PCFGs 
have fewer adjustatfle parameters with which to de- 
scribe this non-local information. There are other 
kinds of non-local information which can be incor- 
porated into a PCFG using a transforln-detransform 
approacll that result in an eve.n greater improvement 
of lml'sing accuracy (3ohnson, 1998b). Ultinmtely, 
however, it seems that a more complex ai)t)roach in- 
corporating back-off and smoothing is necessary ill 
order to achieve the parsing accuracy achieved by 
Charniak (1997) and Collins (1997). 
4 Conc lus ion  
This paper presented factored selective left-corner 
grammar transtbrms. These transtbrlns preserve the 
priinary benefits of the left-conmr grammar trans- 
form (i.e., elimination of left-recursion and preserva- 
tion of annotations on tlroductions) while dranmti- 
tally ameliorating its 1)rincipal problems (gramnmr 
size and sparse data problelns). This should extend 
the applicability of left-conmr techniques to situa- 
tions involving large grammars. We showed how to 
identif~y the nfinimal set L0 of productions of a gram- 
mar that must be recognized left-corner ill order for 
the transformed grammar not to be left-recursive. 
We also proposed two factorizations of tile output of 
the selective left-corner grmnmar t, ransfbrm which 
fllrther reduce grammar size, and showed that there 
is only a nfinor increase in gralnmar size when the 
360 
B~ctored sele(:tive left-corner transform is apl)lied to 
a large tre('A)a.nk grmnmar.  Finally, we exploited 
the tree trm~sforms that, correspond to these gram- 
mar trmlsforms to formulate and study a class of 
sto(:hastie general ized left-corner t)arsers. 
This work could be extended in a. nmnber of ways. 
D)r examl)le, in this t)al)er we assumed that  one 
would always choose a left-corner l)ro(lut'tioll set 
that  inehtdes the nfinimal set L0 required to ensure 
that  the transfbrmed grammar  is not left-recursive. 
However, Roark mid Johnson (1999) report  good 
perR)rmance from a stochast ical ly-guided top-down 
parser, suggesting that lefl;-recm'sion is not; always 
fatal. It might be possible to judic iously choose 
a M't-cor,mr product ion set smaller than L0 which 
el imiimtes t)erni(:ious left-r(;cursion, so that  the re- 
maining lefl;-reeursive cycles llav(', su(:h low t)rol)a- 
1)ility that  tlmy will efl'(~(:t;ively never l)e used and 
a stochast ical ly-guided top-down l)arser will II(~,Vel ' 
sea.reh l;h(un. 
References 
Stephen Almey and Mark Johnson. 1991. Memory re- 
quirements and local mnbiguities of parsing strategies. 
Journal of 1Lsycholinflui.stic R.e.scavch, 20(3):233 250. 
Steven Abney, David MeAlles,x~r, and D;rnando Pereira. 
1999. Relating 1)r()|)al)ilisl;i(: grammars and automata. 
Ill Procccdinfls of tile ,?Tth Annual Mcefinfl of the Asso- 
ciation for Computational Linipdstics, pages 542 549, 
San Francisco. Morgan Kauflmmn. 
Alfred V. Aho mM .leth;ry D. 1Jllman. 1!172. The 5lTtc- 
ory oJ'Parsing, Translation and Compiling; Volume. I: 
Parsing. Prentice-Hall, Englewood Cliffs, New Jersey. 
Eugene Charniak. 1997. St;atist:i(:al parsing wit;h a 
colL|;ex1;-ll'ee ~lalll l l lar and word sl;atisl;i(:s. In \])~Y)('ccd,- 
ings of the Fourteenth Nationo, l Covj):rcnce on Artifi- 
cial hLtcdliflcnce, Menlo Park. AAAI Press/MIT Press. 
Michael Collins. 1997. Three generative, lexicalised 
models for st;atistical parsing. In The Proceedings of 
the 35th Annual Meeting of tile: Association jbr Com- 
p'utational Linguistics, San Francisco. M~orgall Kallf- 
I l I~U l l l .  
Michael A. Covington. 1994. Natural Lanfluage Pro- 
cessin9 for Prolo 9 Programmers. Prentice Ilall, En- 
glewood Clitli% New Jersey. 
A. Demers. 1977. Generalized left>corner parsing. In 
Cot@fence R.ccord of the Fourth ACM Symposium 
on Principles of Programming Lanfluagcs, 1977 A C'M 
SIGA CT/SIGPLAN, pages 170 -182. 
,John E. Hopcroft ml.d JeIfrey D. Ulhmm. 1979. Intro- 
duction to Automata Theory, Languages and Compu- 
tation. Addison-\Vesley. 
Mark Johnson. 1998a. Finite state apl)roximation 
of unification grammars using lefl;-eorner grmmnar 
l\[,,rallsforlllS. In ~'hc \])rocccdiltga of tlt('. 3O'th dtrt- 
nual Gin@fence of the Association J'or Computational 
Linguistics (COLING-ACL), pages 619 623. Morgan 
Kaufmann. 
Marl{ Johnson. 19981). PCFG mode, Is of linguis- 
tic tree representations. Computational Linguistics, 
24(4):613 632. 
Christol)her D. Manning mM Bob Carl)enter. 1997. 
1)robal)ilistic parsing using left>corner models. In Pro- 
cecdings of flu: 5th hdcrnational Workshop on Parsing 
Technologies, 1)ages 147- 158, Massachusetts Institut, e 
of Technology. 
guji Matsumoto, IIozumi Tanaka, Ilideki Ilirakawa, 
IIideo Miyoshi, and Hideki Yasukawa. 1983. BUP: 
A 1)otl;oIn-ll I) t)arser embedded in Prolog. New Gen- 
eration Computing, 1(2):145 158. 
l{.obert C. Moore. 2000. Removing left reeursion from 
context-flee grmnmars. In Proceedings of 1st Annual 
Conference of tile North American Chapter of the As- 
sociation for Computational Linguistics, San ~'an- 
cisco. Morgan Kauflnann. 
Anton Nijholt. 1980. Context-free Grammars: Co'rots, 
Normal Forms, and Par,sing. Springer Verlag, Berlin. 
Fermmdo C.N. Pereira and Stuart M. Shieber. 1987. 
P'~wlofl and Natural Language Analysis. Numbex l 0 in 
CSLI lx~eture Notes Series. Chicago University Press, 
Chicago. 
Philip l{esnik. 1.992. l,eft-corner parsing and psychologi- 
cal plausibility. In The Proceedings of th(', JlJ'tec, nth h~- 
tcrnational Conference on Computational Linfluistics, 
COLING-92, vohmw, 1, pages 191 197. 
\]3rian l/.oark and Mark Johnson. 1999. Efficient proba- 
bilistic top-down and left-corner parsing. In P~wcccd- 
ings of the 37th Annual Mcctinfl of the ACL, pages 
421 428. 
Stanley J. Rosenkrantz and Philip M. Lewis II. 197{). 
Deterministic M't corner parser. In IEEE CmLfcrcnce 
Record of the l lth Annual Symposium on Switchinq 
and Automata, pages 139 152. 
Gertjan van Noord. 1997. An efficient implenmnl;al;ion 
of the head-(:orn(!r l)arser. Computational Linguistics, 
23(3):425 q56. 
361 
Probabilistic Top-Down Parsing and 
Language Modeling 
Brian Roark* 
Brown University 
This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its 
application to the problem of language modeling for speech recognition. The paper first introduces 
key notions in language modeling and probabilistic parsing, and briefly reviews ome previous 
approaches tousing syntactic structure for language modeling. A lexicalized probabilistic top- 
down parser is then presented, which performs very well, in terms of both the accuracy of returned 
parses and the efficiency with which they are found, relative to the best broad-coverage statistical 
parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and 
empirical results how that it improves upon previous work in test corpus perplexity. Interpolation 
with a trigram model yields an exceptional improvement relative to the improvement observed 
by other models, demonstrating the degree to which the information captured by our parsing 
model is orthogonal to that captured by a trigram model. A small recognition experiment also 
demonstrates the utility of the model. 
1. Introduction 
With certain exceptions, computational linguists have in the past generally formed 
a separate research community from speech recognition researchers, despite some 
obvious overlap of interest. Perhaps one reason for this is that, until relatively re- 
cently, few methods have come out of the natural language processing community 
that were shown to improve upon the very simple language models still standardly 
in use in speech recognition systems. In the past few years, however, some improve- 
ments have been made over these language models through the use of statistical meth- 
ods of natural anguage processing, and the development of innovative, linguistically 
well-motivated techniques for improving language models for speech recognition is 
generating more interest among computational linguists. While language models built 
around shallow local dependencies are still the standard in state-of-the-art speech 
recognition systems, there is reason to hope that better language models can and will 
be developed by computational linguists for this task. 
This paper will examine language modeling for speech recognition from a nat- 
ural language processing point of view. Some of the recent literature investigating 
approaches that use syntactic structure in an attempt o capture long-distance depen- 
dencies for language modeling will be reviewed. A new language model, based on 
probabilistic top-down parsing, will be outlined and compared with the previous liter- 
ature, and extensive mpirical results will be presented which demonstrate its utility. 
Two features of our top-down parsing approach will emerge as key to its success. 
First, the top-down parsing algorithm builds a set of rooted candidate parse trees from 
left to right over the string, which allows it to calculate a generative probability for 
* Department of Cognitive and Linguistic Sciences, Box 1978, Brown University, Providence, RI 02912 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 2 
each prefix string from the probabilistic grammar, and hence a conditional probability 
for each word given the previous words and the probabilistic grammar. A left-to- 
right parser whose derivations are not rooted, i.e., with derivations that can consist of 
disconnected tree fragments, uch as an LR or shift-reduce parser, cannot incrementally 
calculate the probability of each prefix string being generated by the probabilistic 
grammar, because their derivations include probability mass from unrooted structures. 
Only at the point when their derivations become rooted (at the end of the string) can 
generative string probabilities be calculated from the grammar. These parsers can 
calculate word probabilities based upon the parser state--as in Chelba and Jelinek 
(1998a)--but such a distribution is not generative from the probabilistic grammar. 
A parser that is not left to right, but which has rooted derivations, e.g., a head- 
first parser, will be able to calculate generative joint probabilities for entire strings; 
however, it will not be able to calculate probabilities for each word conditioned on 
previously generated words, unless each derivation generates the words in the string 
in exactly the same order. For example, suppose that there are two possible verbs that 
could be the head of a sentence. For a head-first parser, some derivations will have the 
first verb as the head of the sentence, and the second verb will be generated after the 
first; hence the second verb's probability will be conditioned on the first verb. Other 
derivations will have the second verb as the head of the sentence, and the first verb's 
probability will be conditioned on the second verb. In such a scenario, there is no 
way to decompose the joint probability calculated from the set of derivations into the 
product of conditional probabilities using the chain rule. Of course, the joint probability 
can be used as a language model, but it cannot be interpolated on a word-by-word 
basis with, say, a trigram model, which we will demonstrate is a useful thing to do. 
Thus, our top-down parser allows for the incremental calculation of generative 
conditional word probabilities, a property it shares with other left-to-right parsers 
with rooted derivations uch as Earley parsers (Earley 1970) or left-corner parsers 
(Rosenkrantz and Lewis II 1970). 
A second key feature of our approach is that top-down guidance improves the 
efficiency of the search as more and more conditioning events are extracted from the 
derivation for use in the probabilistic model. Because the rooted partial derivation is 
fully connected, all of the conditioning information that might be extracted from the 
top-down left context has already been specified, and a conditional probability model 
built on this information will not impose any additional burden on the search. In 
contrast, an Earley or left-corner parser will underspecify certain connections between 
constituents in the left context, and if some of the underspecified information is used 
in the conditional probability model,- it will have to become specified. Of course, this 
can be done, but at the expense of search efficiency; the more that this is done, the 
less benefit here is from the underspecification. A top-down parser will, in contrast, 
derive an efficiency benefit from precisely the information that is underspecified in 
these other approaches. 
Thus, our top-down parser makes it very easy to condition the probabilistic gram- 
mar on an arbitrary number of values extracted from the rooted, fully specified eriva- 
tion. This has lead us to a formulation of the conditional probability model in terms 
of values returned from tree-walking functions that themselves are contextually sen- 
sitive. The top-down guidance that is provided makes this approach quite efficient in 
practice. 
The following section will provide some background in probabilistic ontext-free 
grammars and language modeling for speech recognition. There will also be a brief 
review of previous work using syntactic information for language modeling, before 
we introduce our model in Section 4. 
250 
Roark Top-Down Parsing 
(a) (b) (c) 
S ~ S t S t 
S S STOP S STOP 
NP VP NP VP (/s) NP VP 
Spot VBD NP Spot VBD NP Spot 
chased DT NN chased DT NN 
I I f J 
the ball the ball 
Figure 1 
Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop 
symbol; and (c) a partial parse tree. 
2. Background 
2.1 Grammars and Trees 
This section will introduce probabilistic (or stochastic) context-flee grammars  (PCFGs), 
as well as such notions as complete and partial parse trees, which will be important 
in defining our language model later in the paper. 1In addition, we will explain some 
simple grammar  transformations that will be used. Finally, we will explain the notion 
of c-command, which will be used extensively later as well. 
PCFGs model the syntactic ombinatorics of a language by extending conventional 
context-free grammars (CFGs). A CFG G = (V, T, P, St), consists of a set of nonterminal 
symbols V, a set of terminal symbols T, a start symbol St E V, and a set of rule 
productions P of the form: A ~ a, where a c (V U T)*. These context-free rules 
can be interpreted as saying that a nonterminal symbol A expands into one or more 
either nonterminal or terminal symbols, a = Xo . . .  Xk}  A sequence of context-free rule 
expansions can be represented in a tree, with parents expanding into one or more 
children below them in the tree. Each of the individual local expansions in the tree 
is a rule in the CFG. Nodes in the tree with no children are called leaves. A tree 
whose leaves consist entirely of terminal symbols is complete. Consider, for example, 
the parse tree shown in (a) in Figure 1: the start symbol is St, which expands into an 
S. The S node expands into an NP followed by a VP. These nonterminal nodes each 
in turn expand, and this process of expansion continues until the tree generates the 
terminal string, "Spot chased the ba l l " ,  as leaves. 
A CFG G defines a language Lc, which is a subset of the set of strings of terminal 
symbols, including only those that are leaves of complete trees rooted at St, built 
with rules from the grammar  G. We will denote strings either as w or as WoW1 . . .  wn, 
where wn is understood to be the last terminal symbol in the string. For simplicity in 
displaying equations, from this point forward let w/be  the substring wi . . .  wj. Let Twg 
1 For a detailed introduction to PCFGs, see Manning and Sch~itze (1999), for example. 
2 For ease of exposition, we will ignore epsilon productions for now. An epsilon production has the 
empty string (e) on the right-hand side, and can be written A --~ c. Everything that is said here can be 
straightforwardly extended to include such productions. 
251 
Computational Linguistics Volume 27, Number 2 
be the set of all complete trees rooted at the start symbol, with the string of terminals 
w~ as leaves. We call T G the set of complete parses of w~. 
A PCFG is a CFG with a probability assigned to each rule; specifically, each right- 
hand side has a probability given the left-hand side of the rule. The probability of a 
parse tree is the product of the probabilities of each rule in the tree. Provided a PCFG 
is consistent (or tight), which it always will be in the approach we will be advocating, 
this defines a proper probability distribution over completed trees. 3
A PCFG also defines a probability distribution over strings of words (terminals) 
in the following way: 
P(w~) = ~ P(t) (1) 
tETw~ 
The intuition behind Equation 1 is that, if a string is generated by the PCFG, then it 
will be produced if and only if one of the trees in the set Twg generated it. Thus the 
probability of the string is the probability of the set T G, i.e., the sum of its members' 
probabilities. 
Up to this point, we have been discussing strings of words without specifying 
whether they are "complete" strings or not. We will adopt the convention that an 
explicit beginning of string symbol, (s/, and an explicit end symbol, </s), are part of 
the vocabulary, and a string wg is a complete string if and only if w 0 is (s) and wn 
is (/s). Since the beginning of string symbol is not predicted by language models, 
but rather is axiomatic in the same way that S ~f is for a parser, we can safely omit it 
from the current discussion, and simply assume that it is there. See Figure l(b) for the 
explicit representation. 
While a complete string of words must contain the end symbol as its final word, 
a string prefix does not have this restriction. For example, "Spot chased the ba l l  
(/s)" is a complete string, and the following is the set of prefix strings of this com- 
plete string: "Spot"; "Spot chased"; "Spot chased the"; "Spot chased the bal l" ;  
and "Spot chased the ba l l  (/s>'. A PCFG also defines a probability distribution 
over string prefixes, and we will present his in terms of partial derivations. A partial 
derivation (or parse) d is defined with respect o a prefix string w~ as follows: it is the 
leftmost derivation of the string, with wj on the right-hand side of the last expansion 
in the derivation) Let Dw0J be the set of all partial derivations for a prefix string w0 J.
Then 
P(wg) = ~ P(d) (2) 
dCDwJ ? 
We left-factor the PCFG, so that all productions are binary, except hose with a 
single terminal on the right-hand side and epsilon productions,  We do this because 
it delays predictions about what nonterminals we expect later in the string until we 
have seen more of the string. In effect, this is an underspecification f some of the 
predictions that our top-down parser is making about the rest of the string. The left- 
factorization transform that we use is identical to what is called right binarization 
in Roark and Johnson (1999). See that paper for more discussion of the benefits of 
3 A PCFG is consistent or tight if there is no probability mass  reserved for infinite trees. Chi and Geman 
(1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight. 
All of the PCFGs that are used in this paper are estimated using the relative frequency estimator. 
4 A leftmost derivation is a derivation in which the leftmost nonterminal  is always expanded. 
5 The only c-productions that we will use in this paper are those introduced by left-factorization. 
252 
Roark Top-Down Parsing 
(a) (b) 
S t 
f ~  
S ~-S 
I 
NP S-NP (/s) 
Spot VP S-NP, VP 
VBD VP-VBD e 
chased NP VP-VBD,NP 
DT NP-DT e 
the NN NP-DT,NN 
I I 
ball e 
S t 
S "~-S 
NP S-NP 
I 
Spot 
Figure 2 
Two parse trees: (a) a complete left-factored parse tree with epsilon productions and an 
explicit stop symbol; and (b) a partial eft-factored parse tree. 
factorization for top-down and left-corner parsing. For a grammar G, we define a 
factored grammar Gf as follows: 
i. (A--* B A-B) E Gf iff (A ~ Bfl) E G, s.t. B E V and fl E V* 
ii. (A-a ~ B A-aB) E Gf iff (A ~ aBfl) E G, s.t. B E V, a E V +,andf lEV*  
iii. (A-aB ---, c) E @ iff (A ---* c~B) E G, s.t. B E V and a E V* 
iv. (A --* a) E Gf iff (A ---* a) E G, s.t. a E T 
We can see the effect of this transform on our example parse trees in Figure 2. This 
underspecification f the nonterminal predictions (e.g., VP-VBD in the example in 
Figure 2, as opposed to NP), allows lexical items to become part of the left context, 
and so be used to condition production probabilities, even the production probabil- 
ities of constituents that dominate them in the unfactored tree. It also brings words 
further downstream into the look-ahead at the point of specification. Note that partial 
trees are defined in exactly the same way (Figure 2b), but that the nonterminal yields 
are made up exclusively of the composite nonterminals introduced by the grammar 
transform. 
This transform has a couple of very nice properties. First, it is easily reversible, 
i.e., every parse tree built with Gf corresponds to a unique parse tree built with G. 
Second, if we use the relative frequency estimator for our production probabilities, the 
probability of a tree built with @ is identical to the probability of the corresponding 
tree built with G. 
Finally, let us introduce the term c-command. We will use this notion in our condi- 
tional probability model, and it is also useful for understanding some of the previous 
work in this area. The simple definition of c-command that we will be using in this 
paper is the following: a node A c-commands a node B if and only if (i) A does not 
dominate B; and (ii) the lowest branching node (i.e., non-unary node) that dominates 
253 
Computational Linguistics Volume 27, Number 2 
A also dominates B. 6 Thus in Figure l(a), the subject NP and the VP each c-command 
the other, because neither dominates the other and the lowest branching node above 
both (the S) dominates the other. Notice that the subject NP c-commands the object 
NP, but not vice versa, since the lowest branching node that dominates the object NP 
is the VP, which does not dominate the subject NP. 
2.2 Language Modeling for Speech Recognition 
This section will briefly introduce language modeling for statistical speech recognition. 7 
In language modeling, we assign probabilities to strings of words. To assign a 
probability, the chain rule is generally invoked. The chain rule states, for a string of 
k+l words: 
k 
P(w k) = P(w0) I-I P(wi \[ Woi-1) (3) 
i=1 
A Markov language model of order n truncates the conditioning information in the 
chain rule to include only the previous n words. 
k 
P(w k) = P(w0)P(Wl \[ w0)... P(wn-1 \]wg-2)IIP(wi I Wi_n )i-1 
i=n 
(4) 
These models are commonly called n-gram models, s The standard language model 
used in many speech recognition systems is the trigram model, i.e., a Markov model 
of order 2, which can be characterized by the following equation: 
n- t  
P(wg -1) = P(w0)P(w l  \[ WO) I I  P(wi I Wi_2 )i-1 (5) 
i=2 
To smooth the trigram models that are used in this paper, we interpolate the 
probability estimates of higher-order Markov models with lower-order Markov models 
(Jelinek and Mercer 1980). The idea behind interpolation is simple, and it has been 
shown to be very effective. For an interpolated (n + 1)-gram: 
P(w i \ ]  i -1 i -1 A i -1 i -1 wi_ ) +  n(wi_ ))P(wi I /~n(Wi_n)P(wi I (1 i -1 Wi_n) = - (6) Wi-n+l) 
Here 13 is the empirically observed relative frequency, and /~n is a function from V n to 
\[0,1\]. This interpolation is recursively applied to the smaller-order n-grams until the 
bigram is finally interpolated with the unigram, i.e., A0 = 1. 
3. Previous Work 
There have been attempts to jump over adjacent words to words farther back in the 
left context, without the use of dependency links or syntactic structure, for example 
Saul and Pereira (1997) and Rosenfeld (1996, 1997). We will focus our very brief review, 
however, on those that use grammars or parsing for their language models. These can 
be divided into two rough groups: those that use the grammar as a language model, 
6 A node A dominates a node B in a tree if and only if either (i) A is the parent of B; or (ii) A is the 
parent of a node C that dominates B. 
7 For a detailed introduction to statistical speech recognition, see Jelinek (1997). 
8 The n in n-gram is one more than the order of the Markov model, since the n-gram includes the word 
being conditioned. 
254 
Roark Top-Down Parsing 
and those that use a parser to uncover phrasal heads tanding in an important relation 
(c-command) tothe current word. The approach that we will subsequently present uses 
the probabilistic grammar as its language model, but only includes probability mass 
from those parses that are found, that is, it uses the parser to find a subset of the total 
set of parses (hopefully most of the high-probability parses) and uses the sum of their 
probabilities as an estimate of the true probability given the grammar. 
3.1 Grammar Models 
As mentioned in Section 2.1, a PCFG defines a probability distribution over strings 
of words. One approach to syntactic language modeling is to use this distribution 
directly as a language model. There are efficient algorithms in the literature (Jelinek 
and Lafferty 1991; Stolcke 1995) for calculating exact string prefix probabilities given 
a PCFG. The algorithms both utilize a left-corner matrix, which can be calculated in 
closed form through matrix inversion. They are limited, therefore, to grammars where 
the nonterminal set is small enough to permit inversion. String prefix probabilities can 
be straightforwardly used to compute conditional word probabilities by definition: 
P(Wj+l I wg) = P(wg+l) 
P(wg) (7) 
Stolcke and Segal (1994) and Jurafsky et al (1995) used these basic ideas to es- 
timate bigram probabilities from hand-written PCFGs, which were then used in lan- 
guage models. Interpolating the observed bigram probabilities with these calculated 
bigrams led, in both cases, to improvements in word error rate over using the observed 
bigrams alone, demonstrating that there is some benefit o using these syntactic lan- 
guage models to generalize beyond observed n-grams. 
3.2 Finding Phrasal Heads 
Another approach that uses syntactic structure for language modeling has been to use 
a shift-reduce parser to "surface" c-commanding phrasal headwords or part-of-speech 
(POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model. 
A shift-reduce parser operates from left to right using a stack and a pointer to the 
next word in the input string. 9Each stack entry consists minimally of a nonterminal 
label. The parser performs two basic operations: (i) shifting, which involves pushing 
the POS label of the next word onto the stack and moving the pointer to the following 
word in the input string; and (ii) reducing, which takes the top k stack entries and 
replaces them with a single new entry, the nonterminal label of which is the left-hand 
side of a rule in the grammar that has the k top stack entry labels on the right-hand 
side. For example, if there is a rule NP -~ DT NN, and the top two stack entries are 
NN and DT, then those two entries can be popped off of the stack and an entry with 
the label NP pushed onto the stack. 
Goddeau (1992) used a robust deterministic shift-reduce parser to condition word 
probabilities by extracting a specified number of stack entries from the top of the cur- 
rent state, and conditioning on those entries in a way similar to an n-gram. In empirical 
trials, Goddeau sed the top two stack entries to condition the word probability. He 
was able to reduce both sentence and word error rates on the ATIS corpus using this 
method. 
9 For details, ee Hopcroft and Ullman (1979), for example. 
255 
Computational Linguistics Volume 27, Number 2 
Np\[do,a\] 
DT NN 
L I 
the dog 
Figure 3 
VBD \[chased\] Np\[cat\] 
DT NN 
I I 
chased tile cat with spots 
Tree representation f a derivation state. 
The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 
1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, 
except hat (i) their shift-reduce parser follows a nondeterministic beam search, and 
(ii) each stack entry contains, in addition to the nonterminal node label, the headword 
of the constituent. The SLM is like a trigram, except hat the conditioning words are 
taken from the tops of the stacks of candidate parses in the beam, rather than from 
the linear order of the string. 
Their parser functions in three stages. The first stage assigns a probability to the 
word given the left context (represented by the stack state). The second stage predicts 
the POS given the word and the left context. The last stage performs all possible parser 
operations (reducing stack entries and shifting the new word). When there is no more 
parser work to be done (or, in their case, when the beam is full), the following word 
is predicted. And so on until the end of the string. 
Each different POS assignment or parser operation is a step in a derivation. Each 
distinct derivation path within the beam has a probability and a stack state associated 
with it. Every stack entry has a nonterminal node label and a designated headword of 
the constituent. When all of the parser operations have finished at a particular point in 
the string, the next word is predicted as follows: For each derivation in the beam, the 
headwords of the two topmost stack entries form a trigram with the conditioned word. 
This interpolated trigram probability is then multiplied by the normalized probability 
of the derivation, to provide that derivation's contribution to the probability of the 
word. More precisely, for a beam of derivations Di 
P(Wi+l \] Wg) -- ~d6Di P(wi+l l hOd, hld)P(d) 
E~cD, P(d) (8) 
where hod and hld are  the lexical heads of the top two entries on the stack of d. 
Figure 3 gives a partial tree representation f a potential derivation state for the 
string "the dog chased the cat with spots", at the point when the word "with" is 
to be predicted. The shift-reduce parser will have, perhaps, built the structure shown, 
and the stack state will have an NP entry with the head "cat" at the top of the stack, 
and a VBD entry with the head "chased" second on the stack. In the Chelba and 
Jelinek model, the probability of "with" is conditioned on these two headwords, for 
this derivation. 
Since the specific results of the SLM will be compared in detail with our model 
when the empirical results are presented, at this point we will simply state that they 
have achieved a reduction in both perplexity and word error rate over a standard 
trigram using this model. 
The rest of this paper will present our parsing model, its application to language 
modeling for speech recognition, and empirical results. 
256 
Roark Top-Down Parsing 
4. Top-Down Parsing and Language Modeling 
Statistically based heuristic best-first or beam-search strategies (Caraballo and Char- 
niak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an 
enormous improvement in the quality and speed of parsers, even without any guaran- 
tee that the parse returned is, in fact, that with the maximum likelihood for the proba- 
bility model. The parsers with the highest published broad-coverage parsing accuracy, 
which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), 
all utilize simple and straightforward statistically based search euristics, pruning the 
search-space quite dramatically. 1? Such methods are nearly always used in conjunction 
with some form of dynamic programming (henceforth DP). That is, search efficiency 
for these parsers is improved by both statistical search heuristics and DP. Here we 
will present a parser that uses simple search heuristics of this sort without DE Our 
approach is found to yield very accurate parses efficiently, and, in addition, to lend 
itself straightforwardly to estimating word probabilities on-line, that is, in a single 
pass from left to right. This on-line characteristic allows our language model to be in- 
terpolated on a word-by-word basis with other models, such as the trigram, yielding 
further improvements. 
Next we will outline our conditional probability model over rules in the PCFG, 
followed by a presentation of the top-down parsing algorithm. We will then present 
empirical results in two domains: one to compare with previous work in the parsing 
literature, and the other to compare with previous work using parsing for language 
modeling for speech recognition, in particular with the Chelba and Jelinek results 
mentioned above. 
4.1 Conditional Probability Model 
A simple PCFG conditions rule probabilities on the left-hand side of the rule. It 
has been shown repeatedly--e.g., Briscoe and Carroll (1993), Charniak (1997), Collins 
(1997), Inui et al (1997), Johnson (1998)--that conditioning the probabilities of struc- 
tures on the context within which they appear, for example on the lexical head of a 
constituent (Charniak 1997; Collins 1997), on the label of its parent nonterrninal (John- 
son 1998), or, ideally, on both and many other things besides, leads to a much better 
parsing model and results in higher parsing accuracies. 
One way of thinking about conditioning the probabilities of productions on con- 
textual information (e.g., the label of the parent of a constituent or the lexical heads of 
constituents), is as annotating the extra conditioning information onto the labels in the 
context-free rules. Examples of this are bilexical grammars--such as Eisner and Satta 
(1999), Charniak (1997), Collins (1997)--where the lexical heads of each constituent 
are annotated on both the right- and left-hand sides of the context-free rules, under 
the constraint that every constituent inherits the lexical head from exactly one of its 
children, and the lexical head of a POS is its terminal item. Thus the rule S -* NP VP 
becomes, for instance, S\[barks\] ---* NP\[dog\] VP\[barks\]. One way to estimate the probabil- 
ities of these rules is to annotate the heads onto the constituent labels in the training 
corpus and simply count the number of times particular productions occur (relative 
frequency estimation). This procedure yields conditional probability distributions of 
10 Johnson et al (1999), Henderson a d Brill (1999), and Collins (2000) demonstrate m thods for choosing 
the best complete parse tree from among aset of complete parse trees, and the latter two show 
accuracy improvements over some of the parsers cited above, from which they generated their 
candidate s ts. Here we will be comparing our work with parsing algorithms, i.e., algorithms that 
build parses for strings of words. 
257 
Computational Linguistics Volume 27, Number 2 
constituents on the right-hand side with their lexical heads, given the left-hand side 
constituent and its lexical head. The same procedure works if we annotate parent infor- 
mation onto constituents. This is how Johnson (1998) conditioned the probabilities of 
productions: the left-hand side is no longer, for example, S, but rather STSBAR, i.e., an 
S with SBAR as parent. Notice, however, that in this case the annotations on the right- 
hand side are predictable from the annotation on the left-hand side (unlike, for ex- 
ample, bilexical grammars), so that the relative frequency estimator yields conditional 
probability distributions of the original rules, given the parent of the left-hand side. 
All of the conditioning information that we will be considering will be of this 
latter sort: the only novel predictions being made by rule expansions are the node 
labels of the constituents on the right-hand side. Everything else is already specified 
by the left context. We use the relative frequency estimator, and smooth our production 
probabilities by interpolating the relative frequency estimates with those obtained by 
"annotating" less contextual information. 
This perspective on conditioning production probabilities makes it easy to see that, 
in essence, by conditioning these probabilities, we are growing the state space. That 
is, the number of distinct nonterminals grows to include the composite labels; so does 
the number of distinct productions in the grammar. In a top-down parser, each rule 
expansion is made for a particular candidate parse, which carries with it the entire 
rooted derivation to that point; in a sense, the left-hand side of the rule is annotated 
with the entire left context, and the rule probabilities can be conditioned on any aspect 
of this derivation. 
We do not use the entire left context to condition the rule probabilities, but rather 
"pick-and-choose" which events in the left context we would like to condition on. 
One can think of the conditioning events as functions, which take the partial tree 
structure as an argument and return a value, upon which the rule probability can be 
conditioned. Each of these functions is an algorithm for walking the provided tree and 
returning a value. For example, suppose that we want to condition the probability of 
the rule A --* ~. We might write a function that takes the partial tree, finds the parent 
of the left-hand side of the rule and returns its node label. If the left-hand side has no 
parent (i.e., it is at the root of the tree), the function returns the null value (NULL). We 
might write another function that returns the nonterminal label of the closest sibling to 
the left of A, and NULL if no such node exists. We can then condition the probability 
of the production on the values that were returned by the set of functions. 
Recall that we are working with a factored grammar, so some of the nodes in the 
factored tree have nonterminal labels that were created by the factorization, and may 
not be precisely what we want for conditioning purposes. In order to avoid any con- 
fusions in identifying the nonterminal label of a particular rule production in either its 
factored or nonfactored version, we introduce the function constLtuent (A) for every 
nonterminal in the factored grammar GI, which is simply the label of the constituent 
whose factorization results in A. For example, in Figure 2, constLtuent (NP-DT-NN) 
is simply NP. 
Note that a function can return different values depending upon the location in 
the tree of the nonterminal that is being expanded. For example, suppose that we have 
a function that returns the label of the closest sibling to the left of const i tuent(A)  
or NULL if no such node exists. Then a subsequent function could be defined as 
follows: return the parent of the parent (the grandparent) of const i tuent  (A) only if 
const i tuent(A) has no sibling to the left--in other words, if the previous function 
returns NULL; otherwise return the second closest sibling to the left of constLtuent (A), 
or, as always, NULL if no such node exists. If the function returns, for example, NP, 
this could either mean that the grandparent is NP or the second closest sibling is 
258 
Roark Top-Down Parsing 
For all rules A -+ ct ? A ) 
O the parent, Yp, of const i tuent  (A) in the derivation I 
(~ the closest sibling, Ys, to the left of const i tuent (A)  in the derivation 
S ,  G NULL  
@ the parent, ~ ,  of Yp in the derivation the closest c-commanding 
lexical head to A 
s I 
the next closest c-commanding 
the closest sibling, the POS of the closest lexical head to A 
@ }'ps, to the left of }'~ c-commanding lexical head to .4 
If Y is CC the leftmost child 8 
? of the conjoining category; else NULL the closest c-commanding lexical head to A 
! 
the lexical t!ead of consl:ituent (.4) if already seen; 
? otherwise the lexical head of the closest the next closest c-commanding 
constituent to the left of A within constituent(A) lexical head to A 
Figure 4 
Conditional probability model represented asa decision tree, identifying the location in the 
partial parse tree of the conditioning information. 
NP; yet there is no ambiguity in the meaning of the function, since the result of the 
previous function disambiguates between the two possibilities. 
The functions that were used for the present study to condition the probability 
of the rule, A ---, a, are presented in Figure 4, in a tree structure. This is a sort of 
decision tree for a tree-walking algorithm to decide what value to return, for a given 
partial tree and a given depth. For example, if the algoritt~rn is asked for the value 
at level 0, it will return A, the left-hand side of the rule being expanded. 1~ Suppose 
the algorithm is asked for the value at level 4. After level 2 there is a branch in the 
decision tree. If the left-hand side of the rule is a POS, and there is no sibling to the left 
of const i tuent  (A) in the derivation, then the algorithm takes the right branch of the 
decision tree to decide what value to return; otherwise the left branch. Suppose it takes 
the left branch. Then after level 3, there is another branch in the decision tree. If the 
left-hand side of the production is a POS, then the algorithm takes the right branch of 
the decision tree, and returns (at level 4) the POS of the closest c-commanding lexical 
head to A, which it finds by walking the parse tree; if the left-hand side of the rule is 
not a POS, then the algorithm returns (at level 4) the closest sibling to the left of the 
parent of constituent (A). 
The functions that we have chosen for this paper follow from the intuition (and 
experience) that what helps parsing is different depending on the constituent that is 
being expanded. POS nodes have lexical items on the right-hand side, and hence can 
bring into the model some of the head-head ependencies that have been shown to 
be so effective. If the POS is leftmost within its constituent, hen very often the lexical 
11 Recall that A can be a composite nonterminal introduced by grammar factorization. When the function 
is defined in terms of constJ_tuent (A), the values returned are obtained by moving through the 
nonfactored tree. 
259 
Computational Linguistics Volume 27, Number 2 
Table 1 
Levels of conditioning information, mnemonic labels, and a brief description of the 
information level for empirical results. 
Conditioning Mnemonic Label Information Level 
0,0,0 none 
2,2,2 par+sib 
5,2,2 NT struct 
6,2,2 NT head 
6,3,2 POS struct 
6,5,2 attach 
6,6,4 all 
Simple PCFG 
Small amount of structural context 
All structural (non-lexical) context for non-POS 
Everything for non-POS expansions 
More structural info for leftmost POS expansions 
All attachment info for leftmost POS expansions 
Everything 
item is sensitive to the governing category to which it is attaching. For example, if the 
POS is a preposition, then its probability of expanding to a particular word is very 
different if it is attaching to a noun phrase than if it is attaching to a verb phrase, 
and perhaps quite different depending on the head of the constituent to which it is 
attaching. Subsequent POSs within a constituent are likely to be open-class words, and 
less dependent on these sorts of attachment preferences. 
Conditioning on parents and siblings of the left-hand side has proven to be very 
useful. To understand why this is the case, one need merely to think of VP expansions. 
If the parent of a VP is another VP (i.e., if an auxiliary or modal verb is used), then the 
distribution over productions is different han if the parent is an S. Conditioning on 
head information, both POS of the head and the lexical item itself, has proven useful 
as well, although given our parser's left-to-right orientation, in many cases the head 
has not been encountered within the particular constituent. In such a case, the head 
of the last child within the constituent is used as a proxy for the constituent head. All 
of our conditioning functions, with one exception, return either parent or sibling node 
labels at some specific distance from the left-hand side, or head information from c- 
commanding constituents. The exception is the function at level 5 along the left branch 
of the tree in Figure 4. Suppose that the node being expanded is being conjoined with 
another node, which we can tell by the presence or absence of a CC node. In that case, 
we want to condition the expansion on how the conjoining constituent expanded. In 
other words, this attempts to capture a certain amount of parallelism between the 
expansions of conjoined categories. 
In presenting the parsing results, we will systematically vary the amount of con- 
ditioning information, so as to get an idea of the behavior of the parser. We will refer 
to the amount of conditioning by specifying the deepest level from which a value 
is returned for each branching path in the decision tree, from left to right in Fig- 
ure 4: the first number is for left contexts where the left branch of the decision tree 
is always followed (non-POS nonterminals on the left-hand side); the second number 
is for a left branch followed by a right branch (POS nodes that are leftmost within 
their constituent); and the third number is for the contexts where the right branch is 
always followed (POS nodes that are not leftmost within their constituent). For exam- 
ple, (4,3,2) would represent a conditional probability model that (i) returns NULL for 
all functions below level 4 in all contexts; (ii) returns NULL for all functions below 
level 3 if the left-hand side is a POS; and (iii) returns NULL for all functions below 
level 2 for nonleftmost POS expansions. 
Table 1 gives a breakdown of the different levels of conditioning information used 
in the empirical trials, with a mnemonic label that will be used when presenting results. 
These different levels were chosen as somewhat natural points at which to observe 
260 
Roark Top-Down Parsing 
how much of an effect increasing the conditioning information has. We first include 
structural information from the context, namely, node labels from constituents in the 
left context. Then we add lexical information, first for non-POS expansions, then for 
leftmost POS expansions, then for all expansions. 
All of the conditional probabilities are linearly interpolated. For example, the prob- 
ability of a rule conditioned on six events is the linear interpolation of two probabilities: 
(i) the empirically observed relative frequency of the rule when the six events co-occur; 
and (ii) the probability of the rule conditioned on the first five events (which is in turn 
interpolated). The interpolation coefficients are a function of the frequency of the set 
of conditioning events, and are estimated by iteratively adjusting the coefficients o 
as to maximize the likelihood of a held-out corpus. 
This was an outline of the conditional probability model that we used for the 
PCFG. The model allows us to assign probabilities to derivations, which can be used 
by the parsing algorithm to decide heuristically which candidates are promising and 
should be expanded, and which are less promising and should be pruned. We now 
outline the top-down parsing algorithm. 
4.2 Top-Down Probabilistic Parsing 
This parser is essentially a stochastic version of the top-down parser described in Aho, 
Sethi, and Ullman (1986). It uses a PCFG with a conditional probability model of the 
sort defined in the previous section. We will first define candidate analysis (i.e., a 
partial parse), and then a derives relation between candidate analyses. We will then 
present he algorithm in terms of this relation. 
The parser takes an input string w~, a PCFG G, and a priority queue of candi- 
date analyses. A candidate analysis C = (D, S, Po, F, w n) consists of a derivation D, a 
stack S, a derivation probability Po, a figure of merit F, and a string w n remaining 
to be parsed. The first word in the string remaining to be parsed, wi, we will call the 
look-ahead word. The derivation D consists of a sequence of rules used from G. The 
stack $ contains a sequence of nonterminal symbols, and an end-of-stack marker $ at 
the bottom. The probability Po is the product of the probabilities of all rules in the 
derivation D. F is the product of PD and a look-ahead probability, LAP($,wi), which 
is a measure of the likelihood of the stack $ rewriting with wi at its left corner. 
We can define a derives relation, denoted 3 ,  between two candidate analyses as 
follows. (D,S,  PD, F,w~/) ~ (D' ,S ' ,PD, ,F ' ,w~) if and only if 12 
i. DI = D + A --~ Xo . . .Xk  
ii. $ = Ac~$; 
iii. either S'  = Xo . . .  Xko~$ and j = i 
or k = 0, X0 = wi, j = i+  1, and $'  = c~$; 
iv. PD, = PoP(A --~ Xo . . .  Xk); and 
v. F' = PD, LAP(S ' ,w / )  
The parse begins with a single candidate analysis on the priority queue: ((), St$, 
1, 1, w~). Next, the top ranked candidate analysis, C = (D, $, PD, F, w~), is popped from 
the priority queue. If $ = $ and wi = (/s), then the analysis is complete. Otherwise, 
all C' such that C ~ C t are pushed onto the priority queue. 
12 Again, for ease of exposition, we will ignore e-productions. Everything presented here can be 
straightforwardly extended to include them. The + in (i) denotes concatenation. To avoid confusion 
between sets and sequences, 0 will not be used for empty strings or sequences, rather the symbol () 
will be used. Note that the script $ is used to denote stacks, while St is the start symbol. 
261 
Computational Linguistics Volume 27, Number 2 
We implement this as a beam search. For each word position i, we have a separate 
priority queue Hi of analyses with look-ahead wi. When there are "enough" analyses 
by some criteria (which we will discuss below) on priority queue Hi+l, all candidate 
analyses remaining on Hi are discarded. Since Wn = (/s), all parses that are pushed 
onto Hn+l are complete. The parse on Hn+l with the highest probability is returned 
for evaluation. In the case that no complete parse is found, a partial parse is returned 
and evaluated. 
The LAP is the probability of a particular terminal being the next left-corner of a 
particular analysis. The terminal may be the left corner of the topmost nonterminal 
on the stack of the analysis or it might be the left corner of the nth nonterminal, after 
the top n - 1 nonterminals have rewritten to ~. Of course, we cannot expect o have 
adequate statistics for each nonterminal /word pair that we encounter, so we smooth 
to the POS. Since we do not know the POS for the word, we must sum the LAP for 
all POS labels. 13 
For a PCFG G, a stack $ = A0.. .  A,$ (which we will write Ag$) and a look-ahead 
terminal item wi, we define the look-ahead probability as follows: 
LAP($,wi) - ~ PG(A~ -G wio 0 (9) 
o~E (VuT)* 
We recursively estimate this with two empirically observed conditional probabilities 
for every nonterminal Ai: P(Ai -- wio~) and P(Ai * c). The same empirical probability, 
P(Ai -G Xc~), is collected for every preterminal X as well. The LAP approximation for 
a given stack state and look-ahead terminal is: 
Pc(A\] L wick) ~ PG(Aj --** wick) + P(Aj Z~ e)PG(A\]+\] L wick) (10) 
where 
PG(Aj -- wio~) ~, A&P(Aj -G wick) + (1 - AAj) ~ P(Aj Z~ Xc~)P(X --* wi) 
xcv 
(11) 
The lambdas are a function of the frequency of the nonterminal Aj, in the standard 
way (Jelinek and Mercer 1980). 
The beam threshold at word wi is a function of the probability of the top-ranked 
candidate analysis on priority queue Hi+1 and the number of candidates on Hi+l. The 
basic idea is that we want the beam to be very wide if there are few analyses that have 
been advanced, but relatively narrow if many analyses have been advanced. If ~ is the 
probability of the highest-ranked analysis on Hi+l, then another analysis is discarded 
if its probability falls below Pf('7, IH/+ll), where 3' is an initial parameter, which we 
call the base beam factor. For the current study, 3' was 10 -11 , unless otherwise noted, 
and f('y, IHi+l I) = 'TIHi+I\]3" ThUS, if 100 analyses have already been pushed onto/-//+1, 
then a candidate analysis must have a probability above 10-5~ to avoid being pruned. 
After 1,000 candidates, the beam has narrowed to 10-2p. There is also a maximum 
number of allowed analyses on Hi, in case the parse fails to advance an analysis to 
Hi+\]. This was typically 10,000. 
As mentioned in Section 2.1, we left-factor the grammar, so that all productions are 
binary, except hose with a single terminal on the right-hand side and epsilon produc- 
tions. The only e-productions are those introduced by left-factorization. Our factored 
13 Equivalently, we can split the analyses at this point, so that there is one POS per analysis. 
262 
Roark Top-Down Parsing 
grammar was produced by factoring the trees in the training corpus before grammar 
induction, which proceeded in the standard way, by counting rule frequencies. 
5. Empirical Results 
The empirical results will be presented in three stages: (i) trials to examine the accuracy 
and efficiency of the parser; (ii) trials to examine its effect on test corpus perplexity 
and recognition performance; and (iii) trials to examine the effect of beam variation 
on these performance measures. Before presenting the results, we will introduce the 
methods of evaluation. 
5.1 Evaluation 
Perplexity is a standard measure within the speech recognition community for com- 
paring language models. In principle, if two models are tested on the same test corpus, 
the model that assigns the lower perplexity to the test corpus is the model closest o 
the true distribution of the language, and thus better as a prior model for speech 
recognition. Perplexity is the exponential of the cross entropy, which we will define 
next. 
Given a random variable X with distribution p and a probability model q, the cross 
entropy, H(p, q) is defined as follows: 
H(p, q) = - ~ p(x) log q(x) (12) 
xcX 
Let p be the true distribution of the language. Then, under certain assumptions, given 
a large enough sample, the sample mean of the negative log probability of a model 
will converge to its cross entropy with the true model. 14 That is 
H(p,q) = - lira 1_ logq(w~) (13) 
r/--+ Oo n 
where w~ is a string of the language L. In practice, one takes a large sample of the 
language, and calculates the negative log probability of the sample, normalized by its 
size. 15 The lower the cross entropy (i.e., the higher the probability the model assigns 
to the sample), the better the model. Usually this is reported in terms of perplexity, 
which we will do as well. 16 
Some of the trials discussed below will report results in terms of word and/or 
sentence rror rate, which are obtained when the language model is embedded in a 
speech recognition system. Word error rate is the number of deletion, insertion, or 
substitution errors per 100 words. Sentence rror rate is the number of sentences with 
one or more errors per 100 sentences. 
Statistical parsers are typically evaluated for accuracy at the constituent level, 
rather than simply whether or not the parse that the parser found is completely correct 
or not. A constituent for evaluation purposes consists of a label (e.g., NP) and a span 
(beginning and ending word positions). For example, in Figure l(a), there is a VP that 
spans the words "chased the bal l".  Evaluation is carried out on a hand-parsed test 
corpus, and the manual parses are treated as correct. We will call the manual parse 
14 See Cover and Thomas (1991) for a discussion of the Shannon-McMillan-Breiman theorem, under the 
assumptions of which this convergence holds. 
15 It is important to remember to include the end marker in the strings of the sample. 
16 When assessing the magnitude of a perplexity improvement, i  is often better to look at the reduction 
in cross entropy, by taking the log of the perplexity. It will be left to the reader to do so. 
263 
Computational Linguistics Volume 27, Number 2 
GOLD and the parse that the parser eturns TEST. Precision is the number of common 
constituents in GOLD and TEST divided by the number of constituents in TEST. Recall 
is the number of common constituents in GOLD and TEST divided by the number of 
constituents in GOLD. Following standard practice, we will be reporting scores only 
for non-part-of-speech constituents, which are called labeled recall (LR) and labeled 
precision (LP). Sometimes in figures we will plot their average, and also what can be 
termed the parse error, which is one minus their average. 
LR and LP are part of the standard set of PARSEVAL measures of parser qual- 
ity (Black et al 1991). From this set of measures, we will also include the crossing 
bracket scores: average crossing brackets (CB), percentage of sentences with no cross- 
ing brackets (0 CB), and the percentage of sentences with two crossing brackets or 
fewer (< 2 CB). In addition, we show the average number of rule expansions con- 
sidered per word, that is, the number of rule expansions for which a probability was 
calculated (see Roark and Charniak \[2000\]), and the average number of analyses ad- 
vanced to the next priority queue per word. 
This is an incremental parser with a pruning strategy and no backtracking. In such 
a model, it is possible to commit o a set of partial analyses at a particular point that 
cannot be completed given the rest of the input string (i.e., the parser can "garden 
path"). In such a case, the parser fails to return a complete parse. In the event that 
no complete parse is found, the highest initially ranked parse on the last nonempty 
priority queue is returned. All unattached words are then attached at the highest 
level in the tree. In such a way we predict no new constituents and all incomplete 
constituents are closed. This structure is evaluated for precision and recall, which is 
entirely appropriate for these incomplete as well as complete parses. If we fail to 
identify nodes later in the parse, recall will suffer, and if our early predictions were 
bad, both precision and recall will suffer. Of course, the percentage of these failures 
are reported as well. 
5.2 Parser Accuracy and Efficiency 
The first set of results looks at the performance of the parser on the standard corpora 
for statistical parsing trials: Sections 2-21 (989,860 words, 39,832 sentences) of the 
Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training 
data, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter 
estimation, and Section 23 (59,100 words, 2,416 sentences) as the test data. Section 
22 (41,817 words, 1,700 sentences) erved as the development corpus, on which the 
parser was tested until stable versions were ready to run on the test data, to avoid 
developing the parser to fit the specific test data. 
Table 2 shows trials with increasing amounts of conditioning information from the 
left context. There are a couple of things to notice from these results. First, and least 
surprising, is that the accuracy of the parses improved as we conditioned on more 
and more information. Like the nonlexicalized parser in Roark and Johnson (1999), 
we found that the search efficiency, in terms of number of rule expansions consid- 
ered or number of analyses advanced, also improved as we increased the amount of 
conditioning. Unlike the Roark and Johnson parser, however, our coverage did not 
substantially drop as the amount of conditioning information i creased, and in some 
cases, coverage improved slightly. They did not smooth their conditional probability 
estimates, and blamed sparse data for their decrease in coverage as they increased the 
conditioning information. These results appear to support his, since our smoothed 
model showed no such tendency. 
Figure 5 shows the reduction in parser error, 1 - LR+LP and the reduction in 
2 ' 
rule expansions considered as the conditioning information increased. The bulk of 
264 
Roark Top-Down Parsing 
Table 2 
Results conditioning on various contextual events, standard training and testing corpora. 
Conditioning LR LP CB 0 CB G 2 CB Percent Average Rule Average 
Failed Expansions Analyses 
Considered ~ Advanced ~
Section 23:2245 sentences of length G 40 
none 71.1 75.3 2.48 37.3 62.9 0.9 14,369 516.5 
par+sib 82.8 83.6 1 .55 54.3 76.2 1.1 9,615 324.4 
NT struct 84.3 84.9 1 .38 56.7 79.5 1.0 8,617 284.9 
NT head 85.6 85.7 1.27 59.2 81.3 0.9 7,600 251.6 
POS struct 86.1 86.2 1 .23 60.9 82.0 1.0 7,327 237.9 
attach 86.7 86.6 1.17 61.7 83.2 1.2 6,834 216.8 
all 86.6 86.5 1 .19 62.0 82.7 1.3 6,379 198.4 
Section 23:2416 sentences of length < 100 
attach 85.8 85.8 1 .40 58.9 80.3 1.5 7,210 227.9 
all 85.7 85.7 1 .41 59.0 79.9 1.7 6,709 207.6 
~per word 
-1 
i \[ ~9-- Parse error ? .0. Rule expansions 
-20  
-30  
c -  
o 
g. 
-40  0 . .  
-50  
~.. 
-60  ? 
(0,0,0) (2,2,2) (5,2,2) (6,2,2) (6,3,2) (6,5,2) (6,6,4) 
Conditioning information 
Figure 5 
Reduction in average precision/recall error and in number of rule expansions per word as 
conditioning increases, for sentences of length < 40. 
the improvement  comes from simply conditioning on the labels of the parent and 
the closest sibling to the node being expanded. Interestingly, conditioning all POS 
expansions on two c-commanding heads made no difference in accuracy compared to 
conditioning only leftmost POS expansions on a single c-commanding head; but it did 
improve the efficiency. 
These results, achieved using very straightforward conditioning events and con- 
sidering only the left context, are within one to four points of the best publ ished 
265 
Computational Linguistics Volume 27, Number 2 
30 
25 
20 
o Individual parses 
~, -  Ratnaparkhi observed time 
o 
o ?~ 
o 
o 0 
0 ?0 
i i 
0 
o 
0 0 o 
oo 
og 8 # 
go 
0 
g 
"~ 0 0 O0 O0 O0 0 0 
O~ 0 0 0 0 ~15 o o 0 O0 0 0 0 
0 v~O~ ~^~:~oo~ 0 ~ v O^ o 0 
o % ~o~A~ ~G~?~oooo^ o ~ . o 
o o ,  o 
5 ?0 ? ~ o o,,,,,,ililiiiiiiilt 
? ~ o ? ' ??? ? ~ * o o o 
0 
0 10 20 30 40 50 60 70 
Sentence Length 
Figure 6 
Observed running time on Section 23 of the Penn Treebank, with the full conditional 
probability model and beam of 10 -:1, using one 300 MHz UltraSPARC processor and 256MB 
of RAM of a Sun Enterprise 450. 
accuracies cited above. :7 Of the 2,416 sentences in the section, 728 had the totally cor- 
rect parse, 30.1 percent ree accuracy. Also, the parser returns a set of candidate parses, 
from which we have been choosing the top ranked; if we use an oracle to choose the 
parse with the highest accuracy from among the candidates (which averaged 70.0 in 
number  per sentence), we find an average labeled precision/recal l  of 94.1, for sentences 
of length G 100. The parser, thus, could be used as a front end to some other model,  
with the hopes of selecting a more accurate parse from among the final candidates. 
While we have shown that the conditioning information improves the efficiency in 
terms of rule expansions considered and analyses advanced, what does the efficiency 
of such a parser look like in practice? Figure 6 shows the observed time at our standard 
base beam of 10 -11 with the full conditioning regimen, alongside an approximation 
of the reported observed (linear) time in Ratnaparkhi (1997). Our observed times look 
polynomial,  which is to be expected given our pruning strategy: the denser the com- 
petitors within a narrow probabil ity range of the best analysis, the more time will be 
spent working on these competitors; and the farther along in the sentence, the more 
chance for ambiguities that can lead to such a situation. While our observed times are 
not linear, and are clearly slower than his times (even with a faster machine), they are 
quite respectably fast. The differences between a k-best and a beam-search parser (not 
to mention the use of dynamic programming)  make a running time difference unsur- 
17 Our score of 85.8 average labeled precision and recall for sentences less than or equal to 100 on 
Section 23 compares to: 86.7 in Charniak (1997), 86.9 in Ratnaparkhi (1997), 88.2 in Collins (1999), 89.6 
in Charniak (2000), and 89.75 in Collins (2000). 
266 
Roark Top-Down Parsing 
prising. What is perhaps urprising is that the difference is not greater. Furthermore, 
this is quite a large beam (see discussion below), so that very large improvements in
efficiency can be had at the expense of the number of analyses that are retained. 
5.3 Perp lex i ty  Resu l ts  
The next set of results will highlight what recommends this approach most: the ease 
with which one can estimate string probabilities in a single pass from left to right across 
the string. By definition, a PCFG's estimate of a string's probability is the sum of the 
probabilities of all trees that produce the string as terminal eaves (see Equation 1). 
In the beam search approach outlined above, we can estimate the string's probability 
in the same manner, by summing the probabilities of the parses that the algorithm 
finds. Since this is not an exhaustive search, the parses that are returned will be a 
subset of the total set of trees that would be used in the exact PCFG estimate; hence 
the estimate thus arrived at will be bounded above by the probability that would be 
generated from an exhaustive search. The hope is that a large amount of the probability 
mass will be accounted for by the parses in the beam. The method cannot overestimate 
the probability of the string. 
Recall the discussion of the grammar models above, and our definition of the set 
of partial derivations Dwd with respect o a prefix string w0 j (see Equations 2 and 7). 
By definition, 
P(wj+I \]wg) P(wg+l) ~a~Dw?J+l 
P(d) 
- - (14)  
P(w0 j) ~d~Dwd P(d) 
Note that the numerator at word wj is the denominator at word wj+l, so that the 
product of all of the word probabilities i the numerator at the final word, namely, the 
string prefix probability. 
We can make a consistent estimate of the string probability by similarly summing 
over all of the trees within our beam. Let H~ nit be the priority queue Hi before any 
processing has begun with word Wi in the look-ahead. This is a subset of the possi- 
ble leftmost partial derivations with respect o the prefix string w 0i-1. Since ,/4init~i+l s
produced by expanding only analyses on priority queue HI '~it, the set of complete 
trees consistent with the partial derivations on priority queue ~4i,,~t is a subset of the "i+1 
set of complete trees consistent with the partial derivations on priority queue H~ nit, 
that is, the total probability mass represented by the priority queues is monotoni- 
cally decreasing. Thus conditional word probabilities defined in a way consistent with 
Equation 14 will always be between zero and one. Our conditional word probabilities 
are calculated as follows: 
P(wi I Wio -1) = EdcH~-iit P(d) 
~,acH~nit P(d) (15) 
As mentioned above, the model cannot overestimate he probability of a string, 
because the string probability is simply the sum over the beam, which is a subset of 
the possible derivations. By utilizing a figure of merit to identify promising analyses, 
we are simply focusing our attention on those parses that are likely to have a high 
probability, and thus we are increasing the amount of probability mass that we do 
capture, of the total possible. It is not part of the probability model itself. 
Since each word is (almost certainly, because of our pruning strategy) losing some 
probability mass, the probability model is not "proper"--the sum of the probabilities 
over the vocabulary is less than one. In order to have a proper probability distribution, 
267 
Computational Linguistics Volume 27, Number 2 
we would need to renormalize by dividing by some factor. Note, however, that this 
renormalization factor is necessarily less than one, and thus would uniformly increase 
each word's probability under the model, that is, any perplexity results reported below 
will be higher than the "true" perplexity that would be assigned with a properly 
normalized distribution. In other words, renormalizing would make our perplexity 
measure lower still. The hope, however, is that the improved parsing model provided 
by our conditional probability model will cause the distribution over structures to 
be more peaked, thus enabling us to capture more of the total probability mass, and 
making this a fairly snug upper bound on the perplexity. 
One final note on assigning probabilities to strings: because this parser does gar- 
den path on a small percentage of sentences, this must be interpolated with another 
estimate, to ensure that every word receives a probability estimate. In our trials, we 
used the unigram, with a very small mixing coefficient: 
P(wi l Wlo-1) = ~(w,- , ,  ~ i+, _ ? ~, 0 /v - ,  q-  (1  ~(w~-l))P(wi) (16) La~H~i* P(d) 
If ~dcH~i t  P(d) = 0 in our model, then our model provides no distribution over 
following words since the denominator is zero. Thus, 
,~(Wio_l) = ~0 if ~dcH~,~it P(d) = 0 (17) 
t .999 otherwise 
Chelba and Jelinek (1998a, 1998b) also used a parser to help assign word probabili- 
ties, via the structured language model outlined in Section 3.2. They trained and tested 
the SLM on a modified, more "speech-like" version of the Penn Treebank. Their mod- 
ifications included: (i) removing orthographic ues to structure (e.g., punctuation); 
(ii) replacing all numbers with the single token N; and (iii) closing the vocabulary 
at 10,000, replacing all other words with the UNK token. They used Sections 00-20 
(929,564 words) as the development set, Sections 21-22 (73,760 words) as the check set 
(for interpolation coefficient estimation), and tested on Sections 23-24 (82,430 words). 
We obtained the training and testing corpora from them (which we will denote C&J 
corpus), and also created intermediate corpora, upon which only the first two modifi- 
cations were carried out (which we will denote no punct). Differences in performance 
will give an indication of the impact on parser performance of the different modifica- 
tions to the corpora. All trials in this section used Sections 00-20 for counts, held out 
21-22, and tested on 23-24. 
Table 3 shows several things. First, it shows relative performance for unmodified, 
no punct, and C&J corpora with the full set of conditioning information. We can see 
that removing the punctuation causes (unsurprisingly) a dramatic drop in the accuracy 
and efficiency of the parser. Interestingly, it also causes coverage to become nearly total, 
with failure on just two sentences per thousand on average. 
We see the familiar pattern, in the C&J corpus results, of improving performance 
as the amount of conditioning information grows. In this case we have perplexity 
results as well, and Figure 7 shows the reduction in parser error, rule expansions, and 
perplexity as the amount of conditioning information grows. While all three seem to be 
similarly improved by the addition of structural context (e.g., parents and siblings), the 
addition of c-commanding heads has only a moderate ffect on the parser accuracy, 
but a very large effect on the perplexity. The fact that the efficiency was improved 
more than the accuracy in this case (as was also seen in Figure 5), seems to indicate 
that this additional information is causing the distribution to become more peaked, so 
that fewer analyses are making it into the beam. 
268 
Roark Top-Down Parsing 
Table 3 
Results conditioning on various contextual events, Sections 23-24, modifications following 
Chelba and Jelinek. 
Corpora Conditioning LR LP Percent Perplexity Average Rule Average 
Failed Expansions Analyses 
Considered t Advanced t 
Sections 23-24:3761 sentences G 120 
unmodified all 85.2 85.1 1.7 7,206 213.5 
no punct all 82.4 82.9 0.2 9,717 251.8 
C&J corpus par+sib 75.2 77.4 0.1 310.04 17,418 457.2 
C&J corpus NT struct 77.3 79.2 0.1 290.29 15,948 408.8 
C&J corpus NT head 79.2 80.4 0.1 255.85 14,239 363.2 
C&J corpus POS struct 80.5 81.6 0.1 240.37 13,591 341.3 
C&J corpus all 81.7 82.1 0.2 152.26 11,667 279.7 
tper word 
-10 
-2O 
~-30 
~5 
--4O 
-50 
, ' ' ~ Parse rror 
" '~"  - I <) Rule expansions 
" ~ \  _-. I --cy- Perplexity 
X . .  
\ 
X 
X 
\ 
\ 
X 
\ 
\ 
\ 
\ 
\ 
\ 
-60 I I I 
(2,2,2) (5,2,2) (6,2,2) (6,3,2) (6,6,4) 
Conditioning information 
Figure 7 
Reduction in average precision/recall error, number of rule expansions, and perplexity as 
conditioning increases. 
Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a, 
1998b) on the same training and testing corpora. We built an interpolated tr igram 
model to serve as a baseline (as they did), and also interpolated our model 's  perplexity 
with the trigram, using the same mixing coefficient as they did in their trials (taking 
36 percent of the estimate from the trigram), is The tr igram model was also trained 
on Sections 00-20 of the C&J corpus. Trigrams and bigrams were binned by the total 
18 Our optimal mixture level was closer to 40 percent, but the difference was negligible. 
269 
Computational Linguistics Volume 27, Number 2 
Table 4 
Comparison with previous perplexity results. 
Paper 
Perplexity 
Trigram Baseline Model Interpolation, A = .36 
Chelba and Jelinek (1998a) 167.14 158.28 
Chelba and Jelinek (1998b) 167.14 153.76 
Current results 167.02 152.26 
148.90 
147.70 
137.26 
count of the conditioning words in the training corpus, and maximum likelihood 
mixing coefficients were calculated for each bin, to mix the trigram with bigram and 
unigram estimates. Our trigram model performs at almost exactly the same level as 
theirs does, which is what we would expect. Our parsing model's perplexity improves 
upon their first result fairly substantially, but is only slightly better than their second 
result. 19 However, when we interpolate with the trigram, we see that the additional 
improvement is greater than the one they experienced. This is not surprising, since our 
conditioning information is in many ways orthogonal to that of the trigram, insofar 
as it includes the probability mass of the derivations; in contrast, their model in some 
instances is very close to the trigram, by conditioning on two words in the prefix 
string, which may happen to be the two adjacent words. 
These results are particularly remarkable, given that we did not build our model as 
a language model per se, but rather as a parsing model. The perplexity improvement 
was achieved by simply taking the existing parsing model and applying it, with no 
extra training beyond that done for parsing. 
The hope was expressed above that our reported perplexity would be fairly close 
to the "true" perplexity that we would achieve if the model were properly normal- 
ized, i.e., that the amount of probability mass that we lose by pruning is small. One 
way to test this is the following: at each point in the sentence, calculate the condi- 
tional probability of each word in the vocabulary given the previous words, and sum 
them. 2? If there is little loss of probability mass, the sum should be close to one. We 
did this for the first 10 sentences in the test corpus, a total of 213 words (including 
the end-of-sentence markers). One of the sentences was a failure, so that 12 of the 
word probabilities (all of the words after the point of the failure) were not estimated 
by our model. Of the remaining 201 words, the average sum of the probabilities over 
the 10,000-word vocabulary was 0.9821, with a minimum of 0.7960 and a maximum 
of 0.9997. Interestingly, at the word where the failure occurred, the sum of the proba- 
bilities was 0.9301. 
5.4 Word Error Rate 
In order to get a sense of whether these perplexity reduction results can translate to 
improvement in a speech recognition task, we performed a very small preliminary 
experiment on n-best lists. The DARPA '93 HUB1 test setup consists of 213 utter- 
ances read from the Wall Street Journal, a total of 3,446 words. The corpus comes with 
a baseline trigram model, using a 20,000-word open vocabulary, and trained on ap- 
proximately 40 million words. We used Ciprian Chelba's A* decoder to find the 50 
best hypotheses from each lattice, along with the acoustic and trigram scores. 21 Given 
19 Recall that our perplexity measure should, ideally, be even lower still. 
20 Thanks to Ciprian Chelba for this suggestion. 
21 See Chelba (2000) for details on the decoder. 
270 
Roark Top-Down Parsing 
Table 5 
Word and sentence rror rate results for various models, with differing training and 
vocabulary sizes, for the best language model factor for that particular model. 
Percentage Percentage 
Training Vocabulary LM Word Error Sentence 
Model Size Size Weight Rate Error Rate 
Lattice trigram 40M 20K 16 13.7 69.0 
Chelba (2000) (;~ = .4) 20M 20K 16 13.0 
Current model 1M 10K 15 15.1 73.2 
Treebank trigram 1M 10K 5 16.5 79.8 
No language model 0 16.8 84.0 
the idealized circumstances of the production (text read in a lab), the lattices are rel- 
atively sparse, and in many cases 50 distinct string hypotheses were not found in 
a lattice. We reranked an average of 22.9 hypotheses with our language model per 
utterance. 
One complicating issue has to do with the tokenization in the Penn Treebank 
versus that in the HUB1 lattices. In particular, contractions (e.g., he'  s) are split in the 
Penn Treebank (he 's) but not in the HUB1 lattices. Splitting of the contractions i
critical for parsing, since the two parts oftentimes (as in the previous example) fall 
in different constituents. We follow Chelba (2000) in dealing with this problem: for 
parsing purposes, we use the Penn Treebank tokenization; for interpolation with the 
provided trigram model, and for evaluation, the lattice tokenization is used. If we are to 
interpolate our model with the lattice trigram, we must wait until we have our model's 
estimate for the probability of both parts of the contraction; their product can then be 
interpolated with the trigram estimate. In fact, interpolation in these trials made no 
improvement over the better of the uninterpolated models, but simply resulted in 
performance somewhere between the better and the worse of the two models, so we 
will not present interpolated trials here. 
Table 5 reports the word and sentence rror rates for five different models: (i) the 
trigram model that comes with the lattices, trained on approximately 40M words, with 
a vocabulary of 20,000; (ii) the best-performing model from Chelba (2000), which was 
interpolated with the lattice trigram at ,~ = 0.4; (iii) our parsing model, with the same 
training and vocabulary as the perplexity trials above; (iv) a trigram model with the 
same training and vocabulary as the parsing model; and (v) no language model at all. 
This last model shows the performance from the acoustic model alone, without the 
influence of the language model. The log of the language model score is multiplied 
by the language model (LM) weight when summing the logs of the language and 
acoustic scores, as a way of increasing the relative contribution of the language model 
to the composite score. We followed Chelba (2000) in using an LM weight of 16 for 
the lattice trigram. For our model and the Treebank trigram model, the LM weight 
that resulted in the lowest error rates is given. 
The small size of our training data, as well as the fact that we are rescoring n-best 
lists, rather than working directly on lattices, makes comparison with the other models 
not particularly informative. What is more informative is the difference between our 
model and the trigram trained on the same amount of data. We achieved an 8.5 percent 
relative improvement in word error rate, and an 8.3 percent relative improvement 
in sentence rror rate over the Treebank trigram. Interestingly, as mentioned above, 
interpolating two models together gave no improvement over the better of the two, 
whether our model was interpolated with the lattice or the Treebank trigram. This 
271 
Computational Linguistics Volume 27, Number 2 
Table 6 
Results with full conditioning on the C&J corpus at various base beam factors. 
Base LR LP Percentage Perplexity Perplexity Average Rule Words Per 
Beam Failed )~ = 0 ), = .36 Expansions Second 
Factor Considered t 
Sections 23-24:3761 sentences ~ 120 
10 -11 81.7 82.1 0.2 152.26 137.26 11,667 3.1 
10 -l? 81.5 81.9 0.3 154.25 137.88 6,982 5.2 
10 -9  80.9 81.3 0.4 156.83 138.69 4,154 8.9 
10 -s 80.2 80.6 0.6 160.63 139.80 2,372 15.3 
10 -7  78.8 79.2 1.2 166.91 141.30 1,468 25.5 
10 -6  77.4 77.9 1.5 174.44 143.05 871 43.8 
10 -s 75.8 76.3 2.6 187.11 145.76 517 71.6 
10 -4  72.9 73.9 4.5 210.28 148.41 306 115.5 
10 -3  68.4 70.6 8.0 253.77 152.33 182 179.6 
tper word 
contrasts with our perplexity results reported above, as well as with the recognition 
experiments in Chelba (2000), where the best results resulted from interpolated models. 
The point of this small experiment was to see if our parsing model could provide 
useful information even in the case that recognition errors occur, as opposed to the 
(generally) fully grammatical strings upon which the perplexity results were obtained. 
As one reviewer pointed out, given that our model relies so heavily on context, it may 
have difficulty recovering from even one recognition error, perhaps more difficulty 
than a more locally oriented trigram. While the improvements over the trigram model 
in these trials are modest, they do indicate that our model is robust enough to provide 
good information even in the face of noisy input. Future work will include more 
substantial word recognition experiments. 
5.5 Beam Variation 
The last set of results that we will present addresses the question of how wide the beam 
must be for adequate results. The base beam factor that we have used to this point 
is 10 -11 , which is quite wide. It was selected with the goal of high parser accuracy; 
but in this new domain, parser accuracy is a secondary measure of performance. To 
determine the effect on perplexity, we varied the base beam factor in trials on the 
Chelba and Jelinek corpora, keeping the level of conditioning information constant, 
and Table 6 shows the results across a variety of factors. 
The parser error, parser coverage, and the uninterpolated model perplexity ()~ = 1) 
all suffered substantially from a narrower search, but the interpolated perplexity re- 
mained quite good even at the extremes. Figure 8 plots the percentage increase in 
parser error, model perplexity, interpolated perplexity, and efficiency (i.e., decrease in 
rule expansions per word) as the base beam factor decreased. Note that the model per- 
plexity and parser accuracy are quite similarly affected, but that the interpolated per- 
plexity remained far below the trigram baseline, even with extremely narrow beams. 
6. Conclusion and Future Directions 
The empirical results presented above are quite encouraging, and the potential of this 
kind of approach both for parsing and language modeling seems very promising. 
272 
Roark Top-Down Parsing 
100 
90 
80 
70 
0) 
,~ 60 
50 
30 
20 
10 
- l i  
-o -  Parse error ~.  -~r~ ? - - 
Mode l  Perp lex i ty  ~ ~ - 
? O Interpolated Perplexity ~ 
? ~" Efficiency (decrease in / /~"  
/ rule expansions) ~ i 
/ 
/ 
/ 
/ 
/" / 
. /  
/I / / /  
I / 
/ /  I I 
/ ~ -?3/ 
O" .... .  ' , '  . . . . . . .  . . . . .  
: - -  ...... , .......... ? .......... , , , 
- 10 -9  -8  -7  -6  -5  -4  -3  
lOgjo of base beam factor 
Figure 8 
Increase in average precision/recall error, model perplexity, interpolated perplexity, and 
efficiency (i.e., decrease in rule expansions per word) as base beam factor decreases. 
With a simple conditional probability model, and simple statistical search heuristics, 
we were able to find very accurate parses efficiently, and, as a side effect, were able to 
assign word probabilities that yield a perplexity improvement over previous results. 
These perplexity improvements are particularly promising, because the parser is pro- 
viding information that is, in some sense, orthogonal to the information provided by 
a trigram model, as evidenced by the robust improvements o the baseline trigram 
when the two models are interpolated. 
There are several important future directions that will be taken in this area. First, 
there is reason to believe that some of the conditioning information is not uniformly 
useful, and we would benefit from finer distinctions. For example, the probability 
of a preposition is presumably more dependent on a c-commanding head than the 
probability of a determiner is. Yet in the current model they are both conditioned 
on that head, as leftmost constituents of their respective phrases. Second, there are 
advantages to top-down parsing that have not been examined to date, e.g., empty 
categories. A top-down parser, in contrast to a standard bottom-up chart parser, has 
enough information to predict empty categories only where they are likely to occur. 
By including these nodes (which are in the original annotation of the Penn Treebank), 
we may be able to bring certain long-distance dependencies into a local focus. In 
addition, as mentioned above, we would like to further test our language model in 
speech recognition tasks, to see if the perplexity improvement that we have seen can 
lead to significant reductions in word error rate. 
Other parsing approaches might also be used in the way that we have used a top- 
down parser. Earley and left-corner parsers, as mentioned in the introduction, also 
have rooted derivations that can be used to calculated generative string prefix proba- 
273 
Computational Linguistics Volume 27, Number 2 
bilities incrementally. In fact, left-corner parsing can be simulated by a top-down parser 
by transforming the grammar, as was done in Roark and Johnson (1999), and so an 
approach very similar to the one outlined here could be used in that case. Perhaps 
some compromise between the fully connected structures and extreme underspecifica- 
tion will yield an efficiency improvement. Also, the advantages of head-driven parsers 
may outweigh their inability to interpolate with a trigram, and lead to better off-line 
language models than those that we have presented here. 
Does a parsing model capture exactly what we need for informed language mod- 
eling? The answer to that is no. Some information is simply not structural in nature 
(e.g., topic), and we might expect other kinds of models to be able to better handle 
nonstructural dependencies. The improvement that we derived from interpolating the 
different models above indicates that using multiple models may be the most fruitful 
path in the future. In any case, a parsing model of the sort that we have presented 
here should be viewed as an important potential source of key information for speech 
recognition. Future research will show if this early promise can be fully realized. 
Acknowledgments 
The author wishes to thank Mark Johnson 
for invaluable discussion, guidance, and 
moral support over the course of this 
project. Many thanks also to Eugene 
Charniak for the use of certain grammar 
training routines, and for an enthusiastic 
interest in the project. Thanks also to four 
anonymous reviewers for valuable and 
insightful comments, and to Ciprian Chelba, 
Sanjeev Khudanpur, and Frederick Jelinek 
for comments and suggestions. Finally, the 
author would like to express his 
appreciation to the participants of 
discussions during meetings of the Brown 
Laboratory for Linguistic Information 
Processing (BLLIP); in addition to Mark and 
Eugene: Yasemin Altun, Don Blaheta, 
Sharon Caraballo, Massimiliano Ciaramita, 
Heidi Fox, Niyu Ge, and Keith Hall. This 
research was supported in part by NSF 
IGERT Grant #DGE-9870676. 
References 
Aho, Alfred V., Ravi Sethi, and Jeffrey D. 
Ullman. 1986. Compilers, Principles, 
Techniques, and Tools. Addison-Wesley, 
Reading, MA. 
Black, Ezra, Steven Abney, Dan Flickenger, 
Claudia Gdaniec, Ralph Grishman, Philip 
Harrison, Donald Hindle, Robert Ingria, 
Frederick Jelinek, Judith Klavans, Mark 
Liberman, Mitchell P. Marcus, Salim 
Roukos, Beatrice Santorini, and Tomek 
Strzalkowski. 1991. A procedure for 
quantitatively comparing the syntactic 
coverage of english grammars. In DARPA 
Speech and Natural Language Workshop, 
pages 306-311. 
Briscoe, Ted and John Carroll. 1993. 
Generalized probabilistic LR parsing of 
natural language (corpora) with 
unification-based grammars. 
Computational Linguistics, 19(1):25-60. 
Caraballo, Sharon and Eugene Charniak. 
1998. New figures of merit for best-first 
probabilistic chart parsing. Computational 
Linguistics, 24(2):275-298. 
Charniak, Eugene. 1997. Statistical parsing 
with a context-flee grammar and word 
statistics. In Proceedings ofthe Fourteenth 
National Conference on Arti~'cial Intelligence, 
pages 598-603, Menlo Park. AAAI 
Press/MIT Press. 
Charniak, Eugene. 2000. A 
maximum-entropy-inspired parser. In 
Proceedings ofthe ls t Conference ofthe North 
American Chapter of the Association for 
Computational Linguistics, pages 132-139. 
Charniak, Eugene, Sharon Goldwater, and 
Mark Johnson. 1998. Edge-based best-first 
chart parsing. In Proceedings ofthe Sixth 
Workshop on Very Large Corpora, 
pages 127-133. 
Chelba, Ciprian. 2000. Exploiting Syntactic 
Structure for Natural Language Modeling. 
Ph.D. thesis, The Johns Hopkins 
University. 
Chelba, Ciprian and Frederick Jelinek. 
1998a. Exploiting syntactic structure for 
language modeling. In Proceedings ofthe 
36th Annual Meeting of the Association for 
Computational Linguistics and 17 th 
International Conference on Computational 
Linguistics, pages 225-231. 
Chelba, Ciprian and Frederick Jelinek. 
1998b. Refinement of a structured 
language model. In International 
Conference on Advances in Pattern 
274 
Roark Top-Down Parsing 
Recognition, pages 275-284. 
Chelba, Ciprian and Frederick Jelinek. 1999. 
Recognition performance of a structured 
language model. In Proceedings ofthe 6th 
European Conference on Speech 
Communication a d Technology (Eurospeech), 
pages 1,567-1,570. 
Chi, Zhiyi and Stuart Geman. 1998. 
Estimation of probabilistic context-free 
grammars. Computational Linguistics, 
24(2):299-305. 
Collins, Michael J. 1997. Three generative, 
lexicalised models for statistical parsing. 
In Proceedings ofthe 35th Annual Meeting, 
pages 16-23. Association for 
Computational Linguistics. 
Collins, Michael J. 1999. Head-Driven 
Statistical Models for Natural Language 
Parsing. Ph.D. thesis, University of 
Pennsylvania. 
Collins, Michael J. 2000. Discriminative 
reranking for natural language parsing. In 
The Proceedings ofthe 17th International 
Conference on Machine Learning, 
pages 175-182. 
Cover, Thomas M. and Joy A. Thomas. 
1991. Elements of Information Theory. John 
Wiley and Sons, Inc., New York. 
Earley, Jay. 1970. An efficient context-free 
parsing algorithm. Communications of the 
ACM, 6(8):451-455. 
Eisner, J. and G. Satta. 1999. Efficient 
parsing for bilexical context-free 
grammars and head automaton 
grammars. In Proceedings ofthe 37th Annual 
Meeting, pages 457-464. Association for 
Computational Linguistics. 
Goddeau, David. 1992. Using probabilistic 
shift-reduce parsing in speech recognition 
systems. In Proceedings ofthe 2nd 
International Conference on Spoken Language 
Processing, pages 321-324. 
Goodman, Joshua. 1997. Global 
thresholding and multiple-pass parsing. 
In Proceedings ofthe Second Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-97), pages 11-25. 
Henderson, John C. and Eric Brill. 1999. 
Exploiting diversity in natural anguage 
processing: Combining parsers. In 
Proceedings ofthe Fourth Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-99), pages 187-194. 
Hopcroft, John E. and Jeffrey D. Ullman. 
1979. Introduction to Automata Theory, 
Languages and Computation. 
Addison-Wesley. 
Inui, Kentaro, Virach Sornlertlarnvanich, 
Hozurnmi Tanaka, and Takenobu 
Tokunaga. 1997. A new formalization of 
probabilistic GLR parsing. In Proceedings of
the 5th International Workshop on Parsing 
Technologies, pages 123-134. 
Jelinek, Frederick. 1997. Statistical Methods 
for Speech Recognition. MIT Press, 
Cambridge, MA. 
Jelinek, Frederick and Ciprian Chelba. 1999. 
Putting language into language modeling. 
In Proceedings ofthe 6th European Conference 
on Speech Communication a d Technology 
(Eurospeech), pages KN1-6. 
Jelinek, Frederick and John D. Lafferty. 1991. 
Computation of the probability of initial 
substring eneration by stochastic 
context-free grammars. Computational 
Linguistics, 17(3):315-323. 
Jelinek, Frederick and Robert L. Mercer. 
1980. Interpolated estimation of Markov 
source parameters from sparse data. In 
Proceedings ofthe Workshop on Pattern 
Recognition i  Practice, pages 381-397. 
Johnson, Mark. 1998. PCFG models of 
linguistic tree representations. 
Computational Linguistics, 24(4):617-636. 
Johnson, Mark, Stuart Geman, Stephen 
Canon, Zhiyi Chi, and Stefan Riezler. 
1999. Estimators for stochastic 
"unification-based" grammars. In 
Proceedings ofthe 37th Annual Meeting, 
pages 535-541. Association for 
Computational Linguistics. 
Jurafsky, Daniel, Chuck Wooters, Jonathan 
Segal, Andreas Stolcke, Eric Fosler, Gary 
Tajchman, and Nelson Morgan. 1995. 
Using a stochastic context-free grammar 
as a language model for speech 
recognition. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, pages 189-192. 
Manning, Christopher D. and Hinrich 
Schiitze. 1999. Foundations of Statistical 
Natural Language Processing. MIT Press, 
Cambridge, MA. 
Marcus, Mitchell P., Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building 
a large annotated corpus of English: The 
Penn Treebank. Computational Linguistics, 
19(2):313-330. 
Ratnaparkhi, Adwait. 1997. A linear 
observed time statistical parser based on 
maximum entropy models. In Proceedings 
of the Second Conference on Empirical 
Methods in Natural Language Processing 
(EMNLP-97), pages 1-10. 
Roark, Brian and Eugene Charniak. 2000. 
Measuring efficiency in high-accuracy, 
broad-coverage statistical parsing. In 
Proceedings ofthe COLING-2000 Workshop 
on Efficiency in Large-scale Parsing Systems, 
pages 29-36. 
Roark, Brian and Mark Johnson. 1999. 
Efficient probabilistic top-down and 
275 
Computational Linguistics Volume 27, Number 2 
left-corner parsing. In Proceedings ofthe 
37th Annual Meeting, pages 421--428. 
Association for Computational 
Linguistics. 
Rosenfeld, Ronald. 1996. A maximum 
entropy approach to adaptive statistical 
language modeling. Computer, Speech and 
Language, 10:187-228. 
Rosenfeld, Ronald. 1997. A whole sentence 
maximum entropy language model. In 
Proceedings oflEEE Workshop on Speech 
Recognition and Understanding, 
pages 230-237, 
Rosenkrantz, Daniel J. and Philip M. 
Lewis II. 1970. Deterministic left corner 
parsing. In IEEE Conference Record of the 
11th Annual Symposium on Switching and 
Automata, pages 139-152. 
Saul, Lawrence and Fernando C. N. Pereira. 
1997. Aggregate and mixed-order Markov 
models for statistical language processing. 
In Proceedings ofthe Second Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-97), pages 81-89. 
Stolcke, Andreas. 1995. An efficient 
probabilistic context-free parsing 
algorithm that computes prefix 
probabilities. Computational Linguistics, 
21(2):165-202. 
Stolcke, Andreas and Jonathan Segal. 1994. 
Precise n-gram probabilities from 
stochastic context-free grammars. In 
Proceedings ofthe 32nd Annual Meeting, 
pages 74-79. Association for 
Computational Linguistics. 
276 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 745?752
Manchester, August 2008
Classifying chart cells for quadratic complexity context-free inference
Brian Roark and Kristy Hollingshead
Center for Spoken Language Understanding
Oregon Health & Science University, Beaverton, Oregon, 97006 USA
{roark,hollingk}@cslu.ogi.edu
Abstract
In this paper, we consider classifying word
positions by whether or not they can either
start or end multi-word constituents. This
provides a mechanism for ?closing? chart
cells during context-free inference, which
is demonstrated to improve efficiency and
accuracy when used to constrain the well-
known Charniak parser. Additionally, we
present a method for ?closing? a sufficient
number of chart cells to ensure quadratic
worst-case complexity of context-free in-
ference. Empirical results show that this
O(n
2
) bound can be achieved without im-
pacting parsing accuracy.
1 Introduction
While there have been great advances in the statis-
tical modeling of hierarchical syntactic structure in
the past 15 years, exact inference with such mod-
els remains very costly, so that most rich syntac-
tic modeling approaches involve heavy pruning,
pipelining or both. Pipeline systems make use of
simpler models with more efficient inference to re-
duce the search space of the full model. For ex-
ample, the well-known Ratnaparkhi (1999) parser
used a POS-tagger and a finite-state NP chunker as
initial stages of a multi-stage Maximum Entropy
parser. The Charniak (2000) parser uses a simple
PCFG to prune the chart for a richer model; and
Charniak and Johnson (2005) added a discrimina-
tively trained reranker to the end of that pipeline.
Recent results making use of finite-state chun-
kers early in a syntactic parsing pipeline have
shown both an efficiency (Glaysher andMoldovan,
2006) and an accuracy (Hollingshead and Roark,
2007) benefit to the use of such constraints in a
parsing system. Glaysher and Moldovan (2006)
demonstrated an efficiency gain by explicitly dis-
allowing entries in chart cells that would result in
constituents that cross chunk boundaries. Holling-
shead and Roark (2007) demonstrated that high
precision constraints on early stages of the Char-
niak and Johnson (2005) pipeline?in the form of
base phrase constraints derived either from a chun-
ker or from later stages of an earlier iteration of the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
same pipeline?achieved significant accuracy im-
provements, by moving the pipeline search away
from unlikely areas of the search space. Both
of these approaches (as with Ratnaparkhi earlier)
achieve their improvements by ruling out parts of
the search space for downstream processes, and the
gain can either be realized in efficiency (same ac-
curacy, less time) or accuracy (same time, greater
accuracy). Parts of the search space are ruled out
precisely when they are inconsistent with the gen-
erally reliable output of the chunker, i.e., the con-
straints are a by-product of chunking.
In this paper, we consider building classifiers
that more directly address the problem of ?closing?
chart cells to entries, rather than extracting this in-
formation from taggers or chunkers built for a dif-
ferent purpose. We build two classifiers, which tag
each word in the sequence with a binary class la-
bel. The first classifier decides if the word can be-
gin a constituent of span greater than one word; the
second classifier decides if the word can end a con-
stituent of span greater than 1. Given a chart cell
(i, j) with start word w
i
and end word w
j
, where
j>i, that cell can be ?closed? to entries if the first
classifier decides thatw
i
cannot be the first word of
a multi-word constituent or if the second classifier
decides that w
j
cannot be the last word in a multi-
word constituent. In such a way, we can optimize
classifiers specifically for the task of constrain-
ing chart parsers. Note that such classifier output
would be relatively straightforward to incorporate
into most existing context-free constituent parsers.
We demonstrate the baseline accuracies of such
classifiers, and their impact when the constraints
are placed on the Charniak and Johnson (2005)
parsing pipeline. Various ways of using classifier
output are investigated, including one method for
guaranteeing quadratic complexity of the context-
free parser. A proof of the quadratic complexity is
included, along with a detailed performance evalu-
ation when constraining the Charniak parser to be
worst-case quadratic.
2 Background
Dynamic programming for context-free inference
generally makes use of a chart structure, as shown
in Fig. 1. Each cell in the chart represents a pos-
sible constituent spanning a substring, which is
identified by the indices of the first and last words
of the substring. Thus, the cell identified with
745
i, j
i, j?1 i+1, j
i, j?2 i+1, j?1 i+2, j
i, j?3 i+1, j?2 i+2, j?1 i+3, j
i, j?4 i+1, j?3 i+2, j?2 i+3, j?1 i+4, j
Figure 1: Fragment of a chart structure. Each cell is indexed
with start and end word indices.
i, j will contain possible constituents spanning the
substring w
i
. . . w
j
. Context-free inference has cu-
bic complexity in the length of the string n, due
to the O(n
2
) chart cells and O(n) possible child
configurations at each cell. For example, the CYK
algorithm, which assumes a grammar in Chomsky
Normal Form (hence exactly 2 non-terminal chil-
dren for each constituent of span greater than 1),
must consider the O(n) possible midpoints for the
two children of constituents at each cell.
In a parsing pipeline, some decisions about the
hidden structure are made at an earlier stage. For
example, base phrase chunking involves identify-
ing a span as a base phrase of some category,
often NP. A base phrase constituent has no chil-
dren other than pre-terminal POS-tags, which all
have a single terminal child, i.e., there is no in-
ternal structure in the base phrase involving non-
POS non-terminals. This has a number of implica-
tions for the context-free parser. First, there is no
need to build internal structure within the identi-
fied base phrase constituent. Second, constituents
which cross brackets with the base phrase can-
not be part of the final tree structure. This sec-
ond constraint on possible trees can be thought
of as a constraint on chart cells, as pointed out
in Glaysher and Moldovan (2006): no multi-word
spanning constituent can begin at a word falling
within a base-phrase chunk, other than the first
word of that chunk. Similarly, no multi-word span-
ning constituent can end at a word falling within a
base-phrase chunk, other than the last word of that
chunk. These constraints rule out many possible
structures that the full context-free parser would
have to otherwise consider.
These start and end constraints can be extracted
from the output of the chunker, but the chunker is
not trained to optimize the accuracy (or the pre-
cision) of these particular constraints, rather typi-
cally to optimize chunking accuracy. Further, these
constraints can apply even for words which fall
outside of typical chunks. For example, in En-
glish, verbs and prepositions tend to occur before
their arguments, hence are often unlikely to end
constituents, despite not being inside a typically
defined base phrase. If we can build a classifier
specifically for this task (determining whether a
Strings in corpus 39832
Word tokens in corpus 950028
Tokens neither first nor last in string 870399
Word tokens in S
1
439558 50.5%
Word tokens in E
1
646855 74.3%
Table 1: Statistics on word classes from sections 2-21 of the
Penn Wall St. Journal Treebank
word can start or end a multi-word constituent),
we can more directly optimize the classifier for use
within a pipeline.
3 Starting and ending constituents
To better understand the particular task that we
propose, and its likely utility, we first look at the
distribution of classes and our ability to build sim-
ple classifiers to predict these classes. First, let
us introduce notation. Given a string of n words
w
1
. . . w
n
, we will say that a word w
i
(1<i<n) is
in the class S
>1
if there is a constituent spanning
w
i
. . . w
j
for some j>i; and w
i
? S
1
otherwise.
Similarly, we will say that a word w
j
(1<j<n) is
in the class E
>1
if there is a constituent spanning
w
i
. . . w
j
for some i<j; and w
j
? E
1
otherwise.
These are two separate binary classification tasks.
Note that the first word w
1
and the last word
w
n
are unambiguous in terms of whether they start
or end constituents of length greater than 1. The
first word w
1
must start a constituent spanning the
whole string, and the last word w
n
must end that
same constituent. The first word w
1
cannot end a
constituent of length greater than 1; similarly, the
last word w
n
cannot start a constituent of length
greater than 1. Hence our classifier evaluation
omits those two word positions, leading to n?2
classifications for a string of length n.
Table 1 shows statistics from sections 2-21 of
the Penn WSJ Treebank (Marcus et al, 1993).
From the nearly 1 million words in approximately
40 thousand sentences, just over 870 thousand are
neither the first nor the last word in the string,
hence possible members of the sets S
1
or E
1
, i.e.,
not beginning a multi-word constituent (S
1
) or not
ending a multi-word constituent (E
1
). Of these,
over half (50.5%) do not begin multi-word con-
stituents, and nearly three quarters (74.3%) do not
end multi-word constituents. This high latter per-
centage reflects English right-branching structure.
How well can we perform these binary classifi-
cation tasks, using simple (linear complexity) clas-
sifiers? To investigate this question, we used sec-
tions 2-21 of the Penn WSJ Treebank as training
data, section 00 as heldout, and section 24 as de-
velopment. Word classes are straightforwardly ex-
tracted from the treebank trees, by measuring the
span of constituents starting and ending at each
word position. We trained log linear models with
the perceptron algorithm (Collins, 2002) using fea-
746
Markov order
Classification Task 0 1 2
S
1
(no multi-word constituent start) 96.7 96.9 96.9
E
1
(no multi-word constituent end) 97.3 97.3 97.3
Table 2: Classification accuracy on development set for bi-
nary classes S
1
and E
1
, for various Markov orders.
tures similar to those used for NP chunking in Sha
and Pereira (2003), including surrounding POS-
tags (provided by a separately trained log linear
POS-tagger) and surrounding words, up to 2 be-
fore and 2 after the current word position.
Table 2 presents classification accuracy on the
development set for both of these classification
tasks. We trained models with Markov order 0
(each word classified independently), order 1 (fea-
tures with class pairs) and order 2 (features with
class triples). This did not change performance
for the E
1
classification, but Markov order 1 was
slightly (but significantly) better than order 0 for
S
1
classification. Hence, from this point forward,
all classification will be Markov order 1.
We can see from these results that simple classi-
fication approaches yield very high classification
accuracy. The question now becomes, how can
classifier output be used to constrain a context-free
parser, and what is the impact on parser perfor-
mance of using such a classifier in the pipeline.
4 Closing chart cells
Before moving on to an empirical investigation of
constraining context-free parsers with the methods
we propose, we first need to take a fairly detailed
look at representations internal to these parsers. In
particular, while we can rule out multi-word con-
stituents with particular start and end positions,
there may be intermediate or incomplete structures
within the parser that should not be ruled out at
these same start and end positions. Hence the no-
tion of ?closing? a chart cell is slightly more com-
plicated than it may seem initially.
Consider the chart representation in Fig. 1. Sup-
pose that w
i
is in class S
1
and w
j
is in class E
1
,
for i<j. We can ?close? all cells (i, k) such that
i<k and all cells (l, j) such that l<j, based on
the fact that multi-word constituents cannot begin
with word w
i
and cannot end with w
j
. A closed
cell will not take complete entries, and, depending
on the constraint used to close the cell, will have
restrictions on incomplete entries. To make this
more explicit, let us precisely define complete and
incomplete entries.
Context-free inference using dynamic program-
ming over a chart structure builds longer-span con-
stituents by combining smaller span constituents,
guided by rules in a context-free grammar. A
context-free grammar G = (V, T, S
?
, P ) consists
of: a set of non-terminal symbols V , including a
special start symbol S
?
; a set of terminal symbols
T ; and a set of rule productions P of the form
A ? ? for A ? V and ? ? (V ? T )
?
, i.e.,
a single non-terminal on the left-hand side of the
rule production, and a sequence of 0 or more ter-
minals or non-terminals on the right-hand side of
the rule. If we have a rule production A ? B C
in P , a completed B entry in chart cell (i, j) and
a completed C entry in chart cell (j, k), then we
can place a completed A entry in chart cell (i, k),
typically with some indication that the A was built
from the B and C entries. Such a chart cell entry
is sometimes called an ?edge?.
The issue with incomplete edges arises when
there are rule productions in P with more than two
children on the right-hand side of the rule. Rather
than trying to combine an arbitrarily large num-
ber of smaller cell entries, a more efficient ap-
proach, which exploits shared structure between
rules, is to only perform pairwise combination,
and store incomplete edges to represent combina-
tions that require further combination to achieve
a complete edge. This can either be performed
in advance, e.g., by factoring a grammar to be in
Chomsky Normal Form, as required by the CYK
algorithm (Cocke and Schwartz, 1970; Younger,
1967; Kasami, 1965), resulting in ?incomplete?
non-terminals created by the factorization; or in-
complete edges can be represented through so-
called dotted rules, as with the Earley (1970) al-
gorithm, in which factorization is essentially per-
formed on the fly. For example, if we have a rule
production A ? B C D in P , a completed B en-
try in chart cell (i, j) and a completed C entry in
chart cell (j, k), then we can place an incomplete
edgeA ? B C ?D in chart cell (i, k). The dot sig-
nifies the division between what has already been
combined (to the left of the dot), and what remains
to be combined.
1
Then, if we have an incomplete
edge A ? B C ?D in chart cell (i, k) and a com-
plete D in cell (k, l), we can place a completed A
entry in chart cell (i, l).
If a chart cell (i, j) has been ?closed? due to
constraints limiting multi-word constituents with
that span ? either w
i
? S
1
or w
j
? E
1
(and i<j) ?
then it is clear that ?complete? edges should not be
entered in the cell, since these represent precisely
the multi-word constituents that are being ruled
out. How about incomplete edges? To the extent
that an incomplete edge can be extended to a valid
complete edge, it should be allowed. There are
two cases. If w
i
? S
1
, then under the assumption
that incomplete edges are extended from left-to-
right (see footnote 1), the incomplete edge should
1
Without loss of generality, we will assume that edges are
extended from left-to-right.
747
Parsing accuracy % of Cells
Parsing constraints LR LP F Closed
None (baseline) 88.6 89.2 88.9 ?
S
1
positions 87.6 89.1 88.3 44.6
E
1
positions 87.4 88.5 87.9 66.4
Both S
1
and E
1
86.5 88.6 87.4 80.3
Table 3: Charniak parsing accuracy on section 24 under var-
ious constraint conditions, using word labels extracted using
Markov order 1 model.
be discarded, because any completed edges that
could result from extending that incomplete edge
would have the same start position, i.e., the chart
cell would be (i, k) for some k>i, which is closed
to the completed edge. However, if w
i
6? S
1
, then
w
j
? E
1
. A complete edge achieved by extending
the incomplete edge will end at w
k
for k>j, and
cell (i, k) may be open, hence the incomplete edge
should be allowed in cell (i, j). See ?6 for limita-
tions on how such incomplete edges arise in closed
cells, which has consequences for the worst-case
complexity under certain conditions.
5 Constraining the Charniak parser
5.1 Parser overview and constraint methods
The Charniak (2000) parser is a multi-stage,
agenda-driven, edge-based parser, that can be con-
strained by precluding edges from being placed on
the agenda. Here we will briefly describe the over-
all architecture of that parser, and our method for
constraining its search.
The first stage of the Charniak parser uses an
agenda and a simple PCFG to build a sparse chart,
which is used in later stages with the full model.
We will focus on this first stage, since it is here
that we will be constraining the parser. The edges
on the agenda and in the chart are dotted rules, as
described in ?4. When edges are created, they are
pushed onto the agenda. Edges that are popped
from the agenda are placed in the chart, and then
combined with other chart entries to create new
edges that are pushed onto the agenda. When a
complete edge spanning the whole string is placed
in the chart, at least one full solution exists in the
chart. After this happens, the parser continues
adding edges to the chart and agenda until reaching
some parameterized target number of additional
edges in the chart, at which point the next stage
of the pipeline receives the chart as input and any
edges remaining on the agenda are discarded.
We constrain the first stage of the Charniak
parser as follows. Using classifiers, a subset of
word positions are assigned to class S
1
, and a sub-
set are assigned to class E
1
. (Words can be as-
signed to both.) When an edge is created for cell
(i, j), where i < j, it is not placed on the agenda
if either of the following two conditions hold: 1)
w
i
? S
1
; or 2) the edge is complete and w
j
? E
1
.
0.5 0.6 0.7 0.8 0.9 10.95
0.96
0.97
0.98
0.99
1
Recall
Pre
cis
ion
Start classification
End classification
Figure 2: Precision/recall tradeoff of S
1
and E
1
tags on the
development set.
Of course, the output of our classifier is not per-
fect, hence imposing these constraints will some-
times rule out the true parse, and parser accuracy
may degrade. Furthermore, because of the agenda-
based heuristic search, the efficiency of search may
not be impacted as straightforwardly as one might
expect for an exact inference algorithm. For these
reasons, we have performed extensive empirical
trials under a variety of conditions to try to clearly
understand the best practices for using these sorts
of constraints for this sort of parser.
5.2 Experimental trials
We begin by simply taking the output of the
Markov order 1 taggers, whose accuracies are re-
ported in Table 2, and using word positions labeled
as S
1
or E
1
to ?close? cells in the Charniak parser,
as described above. Table 3 presents parser accu-
racy on the development set (section 24) under four
conditions: the unconstrained baseline; using just
S
1
words to close cells; using just E
1
word posi-
tions to close cells; and using both S
1
and E
1
po-
sitions to close cells. As can be seen from these
results, all of these trials result in a decrease in
accuracy from the baseline, with larger decreases
associated with higher percentages of closed cells.
These results indicate that, despite the relatively
high accuracy of classification, the precision of our
classifier in producing the S
1
and E
1
tags is too
low. To remedy this, we traded some recall for pre-
cision as follows. We used the forward-backward
algorithm with our Markov order 1 tagging model
to assign a conditional probability at each word po-
sition of the tags S
1
and E
1
given the string. At
each word position w
i
for 1<i<n, we took the log
likelihood ratio of tag S
1
as follows:
LLR(w
i
? S
1
) = log
P(w
i
? S
1
| w
1
. . . w
n
)
P(w
i
6? S
1
| w
1
. . . w
n
)
(1)
and the same for tag E
1
. A default classification
threshold is to label S
1
or E
1
if the above log like-
lihood is greater than zero, i.e., if the S
1
tag is more
likely than not. To improve the precision, we can
move this threshold to some greater value.
748
0 0.2 0.4 0.6 0.8 187.5
88
88.5
89
89.5
Fraction of constraints preserved
Ch
arn
iak
 pa
rse
r F
?m
eas
ure
Start position constraints
End position constraints
Baseline performance
Figure 3: Charniak parser F-measure at various operating
points of the fraction c of total constraints kept.
Each word position in a string was ranked with
respect to these log likelihood ratios for each
tag.
2
If the total number of words w
i
with
LLR(w
i
? S
1
) > 0 is k, then we defined multi-
ple operating points by setting the threshold such
that ck words remained above threshold, for some
constant c between 0 and 1. Fig. 2 shows the pre-
cision/recall tradeoff at these operating points for
both S
1
and E
1
tags. Note that for both tags, we
can achieve over 99% precision with recall above
70%, and for theE
1
tag (a more frequent class than
S
1
) that level of precision is achieved with recall
greater than 90%.
Constraints were derived at each of these oper-
ating points and used within the Charniak parsing
pipeline. Fig. 3 shows the F-measure parsing per-
formance using either S
1
or E
1
constraints at vari-
ous values of c for preserving ck of the original k
constraints. As can be seen from that graph, with
improved precision both types of constraints have
operating points that achieve accuracy improve-
ments over the baseline parser on the dev set under
default parser settings.
This accuracy improvement is similar to results
obtained in Hollingshead and Roark (2007), where
base phrase constraints from a finite-state chun-
ker were used to achieve improved parse accuracy.
Their explanation for the accuracy improvement,
which seems to apply in this case as well, is that
the first stage of the Charniak parser is still pass-
ing the same number of edges in the chart to the
second stage, but that the edges now come from
more promising parts of the search space, i.e., the
parser does a better job of exploring good parts of
the search space. Hence the constraints seem to be
doing what they should do, which is constrain the
search without unduly excluding good solutions.
Note that these results are all achieved with
the default parsing parameterizations, so that ac-
curacy gains are achieved, but not necessarily ef-
ficiency gains. The Charniak parser allows for
2
Perceptron weights were interpreted in the log domain
and conditionally normalized appropriately.
0 200 400 600 800 1000 120086
87
88
89
90
Seconds to parse development set
F?m
eas
ure
 pa
rse
 ac
cur
acy
Constrained parser
Unconstrained parser
Figure 4: Speed/accuracy tradeoff for both the uncon-
strained Charniak parser and when constrained with high pre-
cision start/end constraints.
narrow search parameterizations, whereby fewer
edges are added to the chart in the initial stage.
Given the improved search using these constraints,
high accuracy may be achieved at far narrower
search parameterizations than the default setting of
the parser. To look at potential efficiency gains
to be had from these constraints, we chose the
most constrained operating points for both start
and end constraints that do not hurt accuracy rel-
ative to the baseline parser (c = 0.7 for S
1
and
c = 0.8 for E
1
) and used both kinds of constraints
in the parser. We then ran the Charniak parser with
varying search parameters, to observe performance
when search is narrower than the default. Fig. 4
presents F-measure accuracy for both constrained
and unconstrained parser configurations at various
search parameterizations. The times for the con-
strained parser configurations include the approx-
imately 20 seconds required for POS-tagging and
word-boundary classification of the dev set.
These results demonstrate a sharper knee of the
curve for the constrained runs, with parser accu-
racy that is above that achieved by the uncon-
strained parser under the default search parameter-
ization, even after a nearly 5 times speedup.
5.3 Analysis of constraints on 1-best parses
There are two ways in which the constraints could
be improving parser performance: by helping the
parser to find higher probability parses that it was
formerly losing because of search errors; or by
not allowing the parser to select high probability
parses that violate the constraints. To get a sense
of whether the constraints on the parser are sim-
ply fixing search errors or are imposing constraints
on the model itself, we examined the 1-best parses
from both constrained and unconstrained scenar-
ios. First, we calculated the geometric mean of
the 1-best parse probabilities under both scenarios,
which were (in logs) ?207.99 for unconstrained
and ?208.09 for constrained. Thus, the con-
strained 1-best parses had very slightly less proba-
bility than the unconstrained parses, indicating that
the constraints were not simply fixing search er-
749
rors, but also eliminated some MAP parses.
To get a sense of how often search errors
were corrected versus ruling out of MAP parses,
we compared the constrained and unconstrained
parses at each string, and tallied when the uncon-
strained parse probabilities were greater (or less)
than the constrained parse probabilities, as well as
when they were equal. At the default search pa-
rameterization (210), 84.8 percent of the strings
had the same parses; in 9.2 percent of the cases the
unconstrained parses had higher probability; and
in 5.9 percent of the cases the constrained parses
had higher probability. The narrower search pa-
rameterization at the knee of the curve in Fig. 4 had
similar results: 84.6 percent were the same; in 8.6
percent of the cases the unconstrained probability
was higher; and in 6.8 percent of the cases the con-
strained probability was higher. Hence, when the
1-best parse differs, the parse found via constraints
has a higher probability in approximately 40 per-
cent of the cases.
6 O(n
2
) complexity context-free parsing
Using sufficient S
1
and E
1
constraints of the sort
we have been investigating, we can achieve worst-
case quadratic (instead of cubic) complexity. A
proof, based on the CYK algorithm, is given in
Appendix A, but we can make the key points here.
First, cubic complexity of context-free inference
is due to O(n
2
) chart cells and O(n) possible
child configurations per cell. If we ?close? all but
O(n) cells, the ?open? cells will be processed with
worst-case quadratic complexity (O(n) cells with
O(n) possible child configurations per cell). If we
can show that the remaining O(n
2
) ?closed? cells
each can be processed within constant time, then
the overall complexity is quadratic. The proof in
Appendix A shows that this is the case if closing a
cell is such that: when a cell (i, j) is closed, then
either all cells (i, k) for k>i are closed or all cells
(k, j) for k<j are closed. These conditions are
achieved when we select sets S
1
and E
1
and close
cells accordingly.
Just as we were able to order word position log
likelihood scores for classes S
1
and E
1
to im-
prove precision in the previous section, here we
will order them so that we can continue select-
ing positions until we have guaranteed less than
some threshold of ?open? cells. If the threshold
is linear in the length of the string, we will be
able to parse the string with worst-case quadratic
complexity, as shown in Appendix A. We will set
our threshold to kn for some constant k (in our
experiments, k ranges from 2 to 10). Table 4
presents the percentage of cells closed, class (S
1
and E
1
) precision and parser accuracy when the
number of ?open? cells is bounded to be less than
Open % cells Class Parse accuracy
cells closed Prec LR LP F
all ? ? 88.6 89.2 88.9
10n 39.1 99.9 88.6 89.2 88.9
8n 50.4 99.9 88.6 89.2 88.9
6n 62.8 99.9 88.6 89.2 88.9
4n 75.7 99.8 88.8 89.4 89.1
2n 88.8 99.8 88.8 89.5 89.1
Table 4: Varying constant k for kn ?open? cells, yielding
O(n
2
) parsing complexity guarantees
the threshold. These results clearly demonstrate
that such constraints can be placed on real context-
free parsing problems without significant impact to
accuracy?in fact, with small improvements.
We were quite surprised by these trials, fully ex-
pecting these limits to negatively impact accuracy.
The likely explanation is that the existing Char-
niak search strategy itself is bounding processing
in such a way that the additional constraints placed
on the process do not interfere with standard pro-
cessing. Note that our approach closes a higher
percentage of cells in longer strings, which the
Charniak pipeline already more severely prunes
than shorter strings. Further, this approach appears
to be relying very heavily on E
1
constraints, hence
has very high precision of classification.
While the Charniak parser may not be the ideal
framework within which to illustrate these worst-
case complexity improvements, the lack of impair-
ment to the parser provides strong evidence that
other parsers could make use of the resulting charts
to achieve significant efficiency gains.
7 Conclusion & Future Work
In this paper, we have presented a very simple ap-
proach to constraining context-free chart parsing
pipelines that has several nice properties. First,
it is based on a simple classification task that
can achieve very high accuracy using very sim-
ple models. Second, the classifier output can
be straightforwardly used to constrain any chart-
based context-free parser. Finally, we have shown
(in Appendix A) that ?closing? sufficient cells
with these techniques leads to quadratic worst-case
complexity bounds. Our empirical results with the
Charniak parser demonstrated that our classifiers
were sufficiently accurate to allow for such bounds
to be placed on the parser without hurting parsing
accuracy.
Future work in this direction will involve trying
different methods for defining effective operating
points, such as more heavily constraining longer
strings, in an attempt to further improve the search
in the Charniak parser. We would also like to in-
vestigate performance when using other chart pars-
ing strategies, such as when using cell pruning in-
stead of an agenda.
750
CYK(w
1
. . . w
n
, G = (V, T, S
?
, P, ?))  PCFG G must be in CNF
1 for t = 1 to n do  scan in words/POS-tags (span=1)
2 for j = 1 to |V | do
3 ?
j
(t, t)? P(A
j
? w
t
)
4 for s = 2 to n do  all spans > 1
5 for t = 1 to n?s+1 do
6 e? t+s?1  end word position for this span
7 for i = 1 to |V | do
8
?
i
(t, e)? argmax
t<m?e
?
argmax
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
9
?
i
(t, e)? max
t<m?e
?
max
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
Figure 5: Pseudocode of a basic CYK algorithm for PCFG in Chomsky Normal Form (CNF).
References
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 173?180.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics,
pages 132?139.
Cocke, J. and J.T. Schwartz. 1970. Programming languages
and their compilers: Preliminary notes. Technical report,
Courant Institute of Mathematical Sciences, NYU.
Collins, M.J. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 1?8.
Earley, J. 1970. An efficient context-free parsing algorithm.
Communications of the ACM, 6(8):451?455.
Glaysher, E. and D. Moldovan. 2006. Speeding up full syn-
tactic parsing by leveraging partial parsing decisions. In
Proceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 295?300.
Hollingshead, K. and B. Roark. 2007. Pipeline iteration. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 952?959.
Kasami, T. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Technical
report, AFCRL-65-758, Air Force Cambridge Research
Lab., Bedford, MA.
Marcus, M.P., M.A. Marcinkiewicz, and B. Santorini. 1993.
Building a large annotated corpus of English: The Penn
treebank. Computational Linguistics, 19:313?330.
Ratnaparkhi, A. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-
3):151?175.
Sha, F. and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of HLT-NAACL,
pages 134?141.
Younger, D.H. 1967. Recognition and parsing of context-
free languages in time n
3
. Information and Control,
10(2):189?208.
Appendix A Proof of quadratic
complexity parsing with constraints
For this proof, we will use the well-known CYK
parsing algorithm, which makes use of grammars
in Chomsky Normal Form (CNF). To achieve
CNF, among other things, rules with more than 2
children on the right-hand side must be factored
into multiple binary rules. To do this, compos-
ite non-terminals are created in the factorizations,
which represent incomplete constituents, i.e., those
edges that require further combination to be made
complete.
3
For example, if we have a rule pro-
duction A ? B C D in the context-free grammar
G, then a new composite non-terminal would be
created, e.g., B-C, and two binary rules would re-
place the previous ternary rule: A ? B-C D and
B-C ? B C. The B-C non-terminal represents
part of a rule expansion that needs to be combined
with something else to produce a complete non-
terminal from the original set of non-terminals.
Let V
?
be the set of non-terminals that are cre-
ated through factorization, which hence represent
incomplete edges.
Fig. 5 shows pseudocode of a basic CYK algo-
rithm for use with a probabilistic CFG in CNF,
G = (V, T, S
?
, P, ?). The function ? maps from
rules in P to probabilities. Lines 1-3 of the algo-
rithm in Fig. 5 initialize the span 1 cells. Lines 4-9
are where the cubic complexity comes in: O(n)
loops in line 4, each of which include O(n) loops
in line 5, each of which requires finding a max-
imum over O(n) midpoints m in lines 8-9. For
each non-terminal A
i
? V at each cell (t, e), the
algorithm stores a backpointer ?
i
(t, e) in line 8, for
efficiently extracting the maximum likelihood so-
lution at the end of inference; and maximum prob-
abilities ?
i
(t, e) in line 9, for use in the dynamic
program.
Given a set of word positions in the classes S
1
and E
1
, as defined in the main part of this paper,
we can designate all cells (i, j) in the chart where
either w
i
? S
1
or w
j
? E
1
to be ?closed?. Chart
cells that are not closed will be called ?open?. The
total number of cells in the chart is (n
2
+ n)/2,
and if we set a threshold on the maximum number
of open cells to be kn, the number of closed cells
must be at least (n
2
+n)/2?kn. Given an ordering
of words (see ?6 for one approach), we can add
words to these sets one word at a time and close the
3
As before, we assume that edges are extended from left-
to-right, which requires a left-factorization of the grammar.
751
QUADCYK(w
1
. . . w
n
, G = (V, T, S
?
, P, ?), V
?
, S
1
, E
1
)  PCFG G must be in CNF
1 for t = 1 to n do  scan in words/POS-tags (span=1)
2 for j = 1 to |V | do
3 ?
j
(t, t)? P(A
j
? w
t
)
4 for s = 2 to n do  all spans > 1
5 for t = 1 to n?s+1 do
6 e? t+s?1  end word position for this span
7 if w
t
? S
1
CONTINUE  start position t ?closed?
8 else if w
e
? E
1
 end position e ?closed?
9 for i = 1 to |V | do
10 if A
i
6? V
?
CONTINUE  only ?incomplete? factored non-terminals (V
?
)
11
?
i
(t, e)? argmax
j,k
P(A
i
? A
j
A
k
) ?
j
(t, e? 1) ?
k
(e, e)
12
?
i
(t, e)? max
j,k
P(A
i
? A
j
A
k
) ?
j
(t, e? 1) ?
k
(e, e)
13 else  chart cell (t, e) ?open?
14 for i = 1 to |V | do
15
?
i
(t, e)? argmax
t<m?e
?
argmax
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
16
?
i
(t, e)? max
t<m?e
?
max
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
Figure 6: Pseudocode of a modified CYK algorithm, with quadratic worst case complexity with O(n) ?open? cells. In
addition to string and grammar, it requires specification of factored non-terminal set V
?
and position constraints (S
1
, E
1
).
related cells, until the requisite number of closures
are achieved. Then the resulting sets of S
1
word
positions andE
1
word positions can be provided to
the parsing algorithm, in addition to the grammar
G and the set of factored non-terminals V
?
.
Fig. 6 shows pseudocode of a modified CYK al-
gorithm that takes into account S
1
and E
1
word
classes. Lines 1-6 of the algorithm in Fig. 6 are
identical to those in the algorithm in Fig. 5. At
line 7, we have identified the chart cell being pro-
cessed, which is (t, e). If w
t
? S
1
then the cell is
completely closed, and there is nothing to do. Oth-
erwise, if w
e
? E
1
(lines 8-12), then factored non-
terminals from V
?
can be created in that cell by
finding legal combinations of children categories.
If neither of these conditions hold, then the cell is
open (lines 13-16) and processing occurs as in the
standard CYK algorithm (lines 14-16 of the algo-
rithm in Fig. 6 are identical to lines 7-9 in Fig. 5).
If the number of ?open? cells is less than kn for
some constant k, then we can prove that the algo-
rithm in Fig. 6 is O(n
2
) when given a left-factored
grammar in CNF. A key part of the proof rests on
two lemmas:
Lemma 1: Let V
?
be the set of composite non-
terminals created when left-factoring a CFG
to be in CNF, as described earlier. Then, for
any production A
i
? A
j
A
k
in the grammar,
A
k
6? V
?
.
Proof: With left-factoring, any k-ary production
A ? A
1
. . . A
k?1
A
k
results in new non-terminals
that concatenate the first k ? 1 non-terminals on
the right-hand side. These factored non-terminals
are always the leftmost child in the new produc-
tion, hence no second child in the resulting CNF
grammar can be a factored non-terminal.2
Lemma 2: For a cell (t, e) in the chart, if
w
e
? E
1
, then the only possible midpoint m
for creating an entry in the cell is e.
Proof: Placing an entry in cell (t, e) requires a rule
A
i
? A
j
A
k
, an A
j
entry in cell (t,m?1) and an
A
k
entry in cell (m, e). Suppose there is an A
k
en-
try in cell (m, e) for m < e. Recall that w
e
? E
1
,
hence the cell (m, e) is closed to non-terminals not
in V
?
. By Lemma 1, A
k
6? V
?
, therefore the cell
(m, e) is closed to A
k
entries. This is a contradic-
tion. Therefore, the lemma is proved.2
Theorem: Let O be the set of cells (t, e) such
that w
t
6? S
1
and w
e
6? E
1
(?open? cells).
If |O| < kn for some constant k, where n is
the length of the string, then the algorithm in
Fig. 6 has worst case complexity O(n
2
).
Proof: Lines 4 and 5 of the algorithm in Fig. 6
loop throughO(n
2
) cells (t, e), for which there are
three cases: w
t
? S
1
(line 7 of Fig. 6); w
e
? E
1
(lines 8-12); and (t, e) ? O (lines 13-16).
Case 1: w
t
? S
1
. No further work to be done.
Case 2: w
e
? E
1
. There is a constant amount of
work to be done, for the reason that there is only
one possible midpoint m for binary children com-
binations (namely e, as proved in Lemma 2), hence
no need to perform the maximization over O(n)
midpoints.
Case 3: (t, e) ? O. As with standard CYK pro-
cessing, there are O(n) possible midpoints m over
which to maximize, hence O(n) work required.
Only O(n) cells fall in case 3, hence the to-
tal amount of work associated with the cells in O
is O(n
2
). There are O(n
2
) cells associated with
cases 1 and 2, each of which has a total amount
of work bounded by a constant, hence the total
amount of work associated with the cells not in
O is also O(n
2
). Therefore the overall worst-case
complexity of the algorithm under these conditions
is O(n
2
). 2
752
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324?333,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Deriving lexical and syntactic expectation-based measures
for psycholinguistic modeling via incremental top-down parsing
Brian Roark
?
Asaf Bachrach
?
Carlos Cardenas
?
and Christophe Pallier
?
?
Center for Spoken Language Understanding, Oregon Health & Science University
?
INSERM-CEA Cognitive Neuroimaging Unit, Gif sur Yvette, France
?
MIT
roark@cslu.ogi.edu asafbac@gmail.com cardenas@mit.edu christophe@pallier.org
Abstract
A number of recent publications have
made use of the incremental output of
stochastic parsers to derive measures of
high utility for psycholinguistic modeling,
following the work of Hale (2001; 2003;
2006). In this paper, we present novel
methods for calculating separate lexical
and syntactic surprisal measures from a
single incremental parser using a lexical-
ized PCFG. We also present an approx-
imation to entropy measures that would
otherwise be intractable to calculate for a
grammar of that size. Empirical results
demonstrate the utility of our methods in
predicting human reading times.
1 Introduction
Assessment of linguistic complexity has played
an important role in psycholinguistics and neu-
rolinguistics for a long time, from the use of
mean length of utterance and related scores in
child language development (Klee and Fitzgerald,
1985), to complexity scores related to reading dif-
ficulty in human sentence processing studies (Yn-
gve, 1960; Frazier, 1985; Gibson, 1998). Opera-
tionally, such linguistic complexity scores are de-
rived via deterministic manual (human) annotation
and scoring algorithms of language samples. Nat-
ural language processing has been employed to
automate the extraction of such measures (Sagae
et al, 2005; Roark et al, 2007), which can have
high utility in terms of reduction of time required
to annotate and score samples. More interest-
ingly, however, novel data driven methods are be-
ing increasingly employed in this sphere, yield-
ing language sample characterizations that require
NLP in their derivation. For example, scores
derived from variously estimated language mod-
els have been used to evaluate and classify lan-
guage samples associated with neurodevelopmen-
tal or neurodegenerative disorders (Roark et al,
2007; Solorio and Liu, 2008; Gabani et al, 2009),
as well as within general studies of human sen-
tence processing (Hale, 2001; 2003; 2006). These
scores cannot feasibly be derived by hand, but
rather rely on large-scale statistical models and
structured inference algorithms to be derived. This
is quickly becoming an important application of
NLP, making possible new methods in the study
of human language processing in both typical and
impaired populations.
The use of broad-coverage parsing for psy-
cholinguistic modeling has become very popular
recently. Hale (2001) suggested a measure (sur-
prisal) derived from an Earley (1970) parser us-
ing a probabilistic context-free grammar (PCFG)
for psycholinguistic modeling; and in later work
(Hale, 2003; 2006) he suggested an alternate
parser-derived measure (entropy reduction) that
may also account for some human sentence pro-
cessing performance. Recent work continues to
advocate surprisal in particular as a very use-
ful measure for predicting processing difficulty
(Boston et al, 2008a; Boston et al, 2008b; Dem-
berg and Keller, 2008; Levy, 2008), and the mea-
sure has been derived using a variety of incre-
mental (left-to-right) parsing strategies, includ-
ing an Earley parser (Boston et al, 2008a), the
Roark (2001) incremental top-down parser (Dem-
berg and Keller, 2008), and an n-best version of
the Nivre et al (2007) incremental dependency
parser (Boston et al, 2008a; 2008b). Deriving
such measures by hand, even for a relatively lim-
ited set of stimuli, is not feasible, hence parsing
plays a critical role in this developing psycholin-
guistic enterprise.
There is no single measure that can account for
all of the factors influencing human sentence pro-
cessing performance, and some of the most recent
work on using parser-derived measures for psy-
cholinguistic modeling has looked to try to de-
rive multiple, complementary measures. One of
324
the key distinctions being looked at is syntactic
versus lexical expectations (Gibson, 2006). For
example, in Demberg and Keller (2008), trials
were run deriving surprisal from the Roark (2001)
parser under two different conditions: fully lex-
icalized parsing, and fully unlexicalized parsing
(to pre-terminal part-of-speech tags). Boston et
al. (2008a) capture a similar distinction by mak-
ing use of an unlexicalized PCFG within an Ear-
ley parser and a fully lexicalized unlabeled depen-
dency parser (Nivre et al, 2007). As Demberg and
Keller (2008) point out, fully unlexicalized gram-
mars ignore important lexico-syntactic informa-
tion when deriving the ?syntactic? expectations,
such as subcategorization preferences of particular
verbs, which are generally accepted to impact syn-
tactic expectations in human sentence processing
(Garnsey et al, 1997). Demberg and Keller argue,
based on their results, for unlexicalized surprisal
instead of lexicalized surprisal. Here we present a
novel method for deriving separate syntactic and
lexical surprisal measures from a fully lexicalized
incremental parser, to allow for rich probabilistic
grammars to be used to derive either measure, and
demonstrate the utility of this method versus that
of Demberg and Keller in empirical trials.
The use of large-scale lexicalized grammars
presents a problem for using an Earley parser to
derive surprisal or for the calculation of entropy as
Hale (2003; 2006) defines it, because both meth-
ods require matrix inversion of a matrix with di-
mensionality the size of the non-terminal set. With
very large lexicalized PCFGs, the size of the non-
terminal set is too large for tractable matrix in-
version. The use of an incremental, beam-search
parser provides a tractable approximation to both
measures. Incremental top-down and left-corner
parsers have been shown to effectively (and effi-
ciently) make use of non-local features from the
left-context to yield very high accuracy syntactic
parses (Roark, 2001; Henderson, 2003; Collins
and Roark, 2004), and we will use such rich mod-
els to derive our scores.
In addition to teasing apart syntactic and lexical
surprisal (defined explicitly in ?3), we present an
approximation to the full entropy that Hale (2003;
2006) used to define the entropy reduction hypoth-
esis. Such an entropy measure is derived via a pre-
dictive step, advancing the parses independently
of the input, as described in ?3.3. We also present
syntactic and lexical alternatives for this measure,
and demonstrate the utility of making such a dis-
tinction for entropy as well as surprisal.
The purpose of this paper is threefold. First,
to present a careful and well-motivated decompo-
sition of lexical and syntactic expectation-based
measures from a given lexicalized PCFG. Sec-
ond, to explicitly document methods for calculat-
ing these and other measures from a specific in-
cremental parser. And finally, to present some em-
pirical validation of the novel measures from real
reading time trials. We modified the Roark (2001)
parser to calculate the discussed measures
1
, and
the empirical results in ?4 show several things,
including: 1) using a fully lexicalized parser to
calculate syntactic surprisal and entropy provides
higher predictive utility for reading times than
these measures calculated via unlexicalized pars-
ing (as in Demberg and Keller); and 2) syntactic
entropy is a useful predictor of reading time.
2 Notation and preliminaries
A probabilistic context-free grammar (PCFG)
G = (V, T, S
?
, P, ?) consists of a set of non-
terminal variables V ; a set of terminal items
(words) T ; a special start non-terminal S
?
? V ;
a set of rule productions P of the form A ? ?
for A ? V , ? ? (V ? T )
?
; and a function ?
that assigns probabilities to each rule in P such
that for any given non-terminal symbol X ? V ,
?
?
?(X ? ?) = 1.
For a given rule A ? ? ? P , let the func-
tion RHS return the right-hand side of the rule, i.e.,
RHS(A ? ?) = ?. Without loss of generality, we
will assume that for every rule A ? ? ? P , one
of two cases holds: either RHS(A ? ?) ? T or
RHS(A ? ?) ? V
?
. That is, the right-hand side
sequences consist of either (1) exactly one termi-
nal item, or (2) zero or more non-terminals.
Let W ? T
n
be a terminal string of length n,
i.e., W = W
1
. . .W
n
and |W | = n. Let W [i, j]
denote the substring beginning at word W
i
and
ending at word W
j
of the string. Then W
|W |
is the
last word in the string, and W [1, |W |] is the string
as a whole. Adjacent strings represent concate-
nation, i.e., W [1, i]W [i+1, j] = W [1, j]. Thus
W [1, i]w represents the string where W
i+1
= w.
We can define a ?derives? relation (denoted?
G
for a given PCFG G) as follows: ?A? ?
G
???
if and only if A ? ? ? P . A string W ? T
?
is in the language of a grammar G if and only
if S
?
+
?
G
W , i.e., a sequence of one or more
derivation steps yields the string from the start
1
The parser version will be made publicly available.
325
non-terminal. A leftmost derivation begins with
S
?
and each derivation step replaces the leftmost
non-terminal A in the yield with some ? such that
A ? ? ? P . For a leftmost derivation S
?
?
?
G
?,
where ? ? (V ? T )
?
, the sequence of deriva-
tion steps that yield ? can be represented as a
tree, with the start symbol S
?
at the root, and the
?yield? sequence ? at the leaves of the tree. A
complete tree has only terminal items in the yield,
i.e., ? ? T
?
; a partial tree has some non-terminal
items in the yield. With a leftmost derivation, the
yield ? = ?? partitions into an initial sequence
of terminals ? ? T
?
followed by a sequence of
non-terminals ? ? V
?
. For a complete derivation,
? = ; for a partial derivation ? ? V
+
, i.e., one or
more non-terminals. Let T (G,W [1, i]) be the set
of complete trees with W [1, i] as the yield of the
tree, given PCFG G.
A leftmost derivation D consists of a sequence
of |D| steps. Let D
i
represent the i
th
step in
the derivation D, and D[i, j] represent the subse-
quence of steps in D beginning with D
i
and end-
ing with D
j
. Note that D
|D|
is the last step in
the derivation, and D[1, |D|] is the derivation as
a whole. Each step D
i
in the derivation is a rule
in G, i.e., D
i
? P for all i. The probability of the
derivation and the corresponding tree is:
?(D) =
m
?
i=1
?(D
i
) (1)
Let D(G,W [1, i]) be the set of all possible left-
most derivations D (with respect to G) such that
RHS(D
|D|
) = W
i
. These are the set of partial left-
most derivations whose last step used a production
with terminal W
i
on the right-hand side. The pre-
fix probability of W [1, i] with respect to G is
PrefixProb
G
(W [1, i]) =
?
D?D(G,W [1,i])
?(D) (2)
From this prefix probability, we can calculate the
conditional probability of each word w ? T in the
terminal vocabulary, given the preceding sequence
W [1, i] as follows:
P
G
(w | W [1, i]) =
PrefixProb
G
(W [1, i]w)
P
w
?
?T
PrefixProb
G
(W [1, i]w
?
)
=
PrefixProb
G
(W [1, i]w)
PrefixProb
G
(W [1, i])
(3)
This, in fact, is precisely the conditional proba-
bility that is used for language modeling for such
applications as speech recognition and machine
translation, which was the motivation for various
syntactic language modeling approaches (Jelinek
and Lafferty, 1991; Stolcke, 1995; Chelba and Je-
linek, 1998; Roark, 2001).
As with language modeling, it is important to
model the end of the string as well, usually with
an explicit end symbol, e.g., </s>. For a string
W [1, i], we can calculate its prefix probability as
shown above. To calculate its complete probabil-
ity, we must sum the probabilities over the set of
complete trees T (G,W [1, i]). In such a way, we
can calculate the conditional probability of ending
the string with </s> given W [1, i] as follows:
P
G
(</s> | W [1, i]) =
?
D?T (G,W [1,i])
?(D)
PrefixProb
G
(W [1, i])
(4)
2.1 Incremental top-down parsing
In this section, we review relevant details of
the Roark (2001) incremental top-down parser,
as configured for use here. As presented in
Roark (2004), the probabilities in the PCFG are
smoothed so that the parser is guaranteed not to
fail due to garden pathing, despite following a
beam search strategy. Hence there is always a non-
zero prefix probability as defined in Eq. 2.
The parser follows a top-down leftmost deriva-
tion strategy. The grammar is factored so that ev-
ery production has either a single terminal item on
the right-hand side or is of the form A ? B A-B,
where A,B ? V and the factored A-B category
can expand to any sequence of children categories
of A that can follow B. This factorization of n-
ary productions continues to nullary factored pro-
ductions, i.e., the end of the original production
A ? B
1
. . . B
n
is signaled with an empty produc-
tion A-B
1
-. . . -B
n
? .
The parser maintains a set of possible connected
derivations, weighted via the PCFG. It uses a beam
search, whereby the highest scoring derivations
are worked on first, and derivations that fall out-
side of the beam are discarded. The reader is re-
ferred to Roark (2001; 2004) for specifics about
the beam search.
The model conditions the probability of each
production on features extracted from the par-
tial tree, including non-local node labels such as
parents, grandparents and siblings from the left-
context, as well as c-commanding lexical items.
Hence this is a lexicalized grammar, though the
incremental nature precludes a general head-first
strategy, rather one that looks to the left-context
for c-commanding lexical items.
To avoid some of the early prediction of struc-
ture, the version of the Roark parser that we used
326
performs an additional grammar transformation
beyond the simple factorization already described
? a selective left-corner transform of left-recursive
productions (Johnson and Roark, 2000). In the
transformed structure, slash categories are used to
avoid predicting left-recursive structure until some
explicit indication of modification is present, e.g.,
a preposition.
The final step in parsing, following the last word
in the string, is to ?complete? all non-terminals
in the yield of the tree. All of these open non-
terminals are composite factored categories, such
as S-NP-VP, which are ?completed? by rewriting
to . The probability of these  productions is what
allows for the calculation of the conditional prob-
ability of ending the string, shown in Eq. 4.
One final note about the size of the non-terminal
set and the intractability of exact inference for
such a scenario. The non-terminal set not only
includes the original atomic non-terminals of the
grammar, but also any categories created by gram-
mar factorization (S-NP) or the left-corner trans-
form (NP/NP). Additionally, however, to remain
context-free, the non-terminal set must include
categories that incorporate non-local features used
by the statistical model into their label, includ-
ing parents, grandparents and sibling categories in
the left-context, as well as c-commanding lexical
heads. These non-local features must be made lo-
cal by encoding them in the non-terminal labels,
leading to a very large non-terminal set and in-
tractable exact inference. Heavy smoothing is re-
quired when estimating the resulting PCFG. The
benefit of such a non-terminal set is a rich model,
which enables a more peaked statistical distribu-
tion around high quality syntactic structures and
thus more effective pruning of the search space.
The fully connected left-context produced by top-
down derivation strategies provides very rich fea-
tures for the stochastic parsing models. See Roark
(2001; 2004) for discussion of these issues.
We now turn to measures that can be derived
from the parser which may be of use for psycholin-
guistic modeling.
3 Parser and grammar derived measures
3.1 Surprisal
The surprisal at word W
i
is the negative log prob-
ability of W
i
given the preceding words. Using
prefix probabilities, this can be calculated as:
S
G
(W
i
) = ? log
PrefixProb
G
(W [1, i])
PrefixProb
G
(W [1, i? 1])
(5)
Substituting equation 2 into this, we get
S
G
(W
i
) = ? log
?
D?D(G,W [1,i])
?(D)
?
D?D(G,W [1,i?1])
?(D)
(6)
If we are using a beam-search parser, some of the
derivations are pruned away. Let B(G,W [1, i]) ?
D(G,W [1, i]) be the set of derivations in the
beam. Then the surprisal can be approximated as
S
G
(W
i
) ? ? log
?
D?B(G,W [1,i])
?(D)
?
D?B(G,W [1,i?1])
?(D)
(7)
Any pruning in the beam search will result in a de-
ficient probability distribution, i.e., a distribution
that sums to less than 1. Roark?s thesis (2001)
showed that the amount of probability mass lost
for this particular approach is very low, hence this
provides a very tight bound on the actual surprisal
given the model.
3.2 Lexical and Syntactic surprisal
High surprisal scores result when the prefix proba-
bility at word W
i
is low relative to the prefix prob-
ability at word W
i?1
. Sometimes this is due to the
identity of W
i
, i.e., it is a surprising word given
the context. Other times, it may not be the lexical
identity of the word so much as the syntactic struc-
ture that must be created to integrate the word into
the derivations. One would like to tease surprisal
apart into ?syntactic surprisal? versus ?lexical sur-
prisal?, which would capture this intuition of the
lexical versus syntactic dimensions to the score.
Our solution to this has the beneficial property of
producing two scores whose sum equals the origi-
nal surprisal score.
The original surprisal score is calculated via
sets of partial derivations at the point when each
word W
i
is integrated into the syntactic structure,
D(G,W [1, i]). We then calculate the ratio from
point to point in sequence. To tease apart the lexi-
cal and syntactic surprisal, we will consider sets of
partial derivations immediately before each word
W
i
is integrated into the syntactic structure, i.e.,
D[1, |D|?1] for D ? D(G,W [1, i]). Recall that
the last derivation move for every derivation in the
set is from the POS-tag to the lexical item. Hence
the sequence of derivation moves that excludes the
last one includes all structure except the word W
i
.
Then the syntactic surprisal is calculated as:
SynS
G
(W
i
) = ? log
P
D?D(G,W [1,i])
?(D[1, |D|?1])
P
D?D(G,W [1,i?1])
?(D)
(8)
327
and the lexical surprisal is calculated as:
LexS
G
(W
i
) = ? log
P
D?D(G,W [1,i])
?(D)
P
D?D(G,W [1,i])
?(D[1, |D|?1])
(9)
Note that the numerator of SynS
G
(W
i
) is the de-
nominator of LexS
G
(W
i
), hence they sum to form
total surprisal S
G
(W
i
). As with total surprisal,
these measures can be defined either for the full
set D(G,W [1, i]) or for a pruned beam of deriva-
tions B(G,W [1, i]) ? D(G,W [1, i]).
Finally, we replicated the Demberg and Keller
(2008) ?unlexicalized? surprisal by replacing ev-
ery lexical item in the training corpus with its
POS-tag, and then parsing the POS-tags of the lan-
guage samples rather than the words. This differs
from our syntactic surprisal by having no lexical
conditioning events for rule probabilities, and by
having no ambiguity about the POS-tag of the lex-
ical items in the string. We will refer to the result-
ing surprisal measure as ?POS surprisal? to distin-
guish it from our syntactic surprisal measure.
3.3 Entropy
Entropy scores of the sort advocated by Hale
(2003; 2006) involve calculation over the set of
complete derivations consistent with the set of par-
tial derivations. Hale performs this calculation
efficiently via matrix inversion, which explains
the use of relatively small-scale grammars with
tractably sized non-terminal sets. Such methods
are not tractable for the kinds of richly condi-
tioned, large-scale PCFGs that we advocate using
here. At each word in the string, the Roark (2001)
top-down parser provides access to the weighted
set of partial analyses in the beam; the set of com-
plete derivations consistent with these is not im-
mediately accessible, hence additional work is re-
quired to calculate such measures.
Let H(D) be the entropy over a set of deriva-
tions D, calculated as follows:
H(D) = ?
X
D?D
?(D)
P
D
?
?D
?(D
?
)
log
?(D)
P
D
?
?D
?(D
?
)
(10)
If the set of derivations D = D(G,W [1, i])
is a set of partial derivations for string W [1, i],
then H(D) is a measure of uncertainty over the
partial derivations, i.e., the uncertainty regarding
the correct analysis of what has already been pro-
cessed. This can be calculated directly from the
existing parser operations. If the set of derivations
are the complete derivations consistent with the set
of partial derivations ? complete derivations that
could occur over the set of possible continuations
of the string ? then this is a measure of the un-
certainty about what is yet to come. We would
like measures that can capture this distinction be-
tween (a) uncertainty of what has already been
processed (?current ambiguity?) versus (b) uncer-
tainty of what is yet to be processed (?predictive
entropy?). In addition, as with surprisal, we would
like to tease apart the syntactic uncertainty versus
lexical uncertainty.
To calculate the predictive entropy after word
sequence W [1, i], we modify the parser as fol-
lows: the parser extends the set of partial deriva-
tions to include all possible next words (the entire
vocabulary plus </s>), and calculates the entropy
over that set. This measure is calculated from just
one additional word beyond the current word, and
hence is an approximation to Hale?s conditional
entropy of grammatical continuations, which is
over complete derivations. We will denote this as
H
1
G
(W [1, i]) and calculate it as follows:
H
1
G
(W [1, i]) = H(
?
w?T?{</s>}
D(G,W [1, i]w)) (11)
This is performing a predictive step that the base-
line parser does not perform, extending the parses
to all possible next words.
Unlike surprisal, entropy does not decompose
straightforwardly into syntactic and lexical com-
ponents that sum to the original composite mea-
sure. To tease apart entropy due to syntactic un-
certainty versus that due to lexical uncertainty, we
can define the set of derivations up to the pre-
terminal (POS-tag) non-terminals as follows. Let
S(D) = {D[1, |D|?1] : D ? D}, i.e., the set of
derivations achieved by removing the last step of
all derivations inD. Then we can calculate a ?syn-
tactic? H
1
G
as follows:
SynH
1
G
(W [1, i]) = H(
[
w?T?{</s>}
S(D(G,W [1, i]w))) (12)
Finally, ?lexical? H
1
G
is defined in terms of the
conditional probabilities derived from prefix prob-
abilities as defined in Eq. 3.
LexH
1
G
(W [1, i]) =
?
X
w?T?{</s>}
P
G
(w | W [1, i]) logP
G
(w | W [1, i]) (13)
As a practical matter, these values are calculated
within the Roark parser as follows. A ?dummy?
word is created that can be assigned every POS-
tag, and the parser extends from the current state to
this dummy word. (The beam threshold is greatly
328
expanded to allow for many possible extensions.)
Then every word in the vocabulary is substituted
for the word, and the appropriate probabilities cal-
culated over the beam. Finally, the actual next
word is substituted, the beam threshold is reduced
to the actual working threshold, and the requisite
number of analyses are advanced to continue pars-
ing the string. This represents a significant amount
of additional work for the parser ? particularly for
vocabulary sizes that we currently use, on the or-
der of tens of thousands of words.
As with surprisal, we can calculate an ?unlex-
icalized? version of the measure by training and
parsing just to POS-tags. We will refer to this sort
of entropy as ?POS entropy?.
4 Empirical validation
4.1 Subjects and stimuli
In order to test the psycholinguistic relevance of
the different measures produced by the parser, we
conducted a word by word reading experiment.
23 native speakers of English read 4 short texts
(mean length: 883.5 words, 49.25 sentences). The
texts were the written versions of narratives used
in a parallel fMRI experiment making use of the
same parser derived measures and whose results
will be published in a different paper (Bachrach et
al., 2009). The narratives contained a high density
of syntactically complex structures (in the form of
sentential embeddings, relative clauses and other
non-local dependencies) but were constructed so
as to appear highly natural. The modified version
of the Roark parser, trained on the Brown Cor-
pus section of the Penn Treebank (Marcus et al,
1993), was used to parse the different narratives
and produce the word by word measures.
4.2 Procedure
Each narrative was presented line by line (cer-
tain sentences required more than one line) on a
computer screen (Dell Optiplex 755 running Win-
dows XP Professional) using Linger 2.88
2
. Each
line contained 11.5 words on average. Each word
would appear in its relative position on the screen.
The subject would then be required to push a key-
board button to advance to the next word. The
original word would then disappear and the fol-
lowing word appear in the subsequent position on
the screen. After certain sentences a comprehen-
sion question would appear on the screen (10 per
narrative). This was done in order to encourage
2
http://tedlab.mit.edu/?dr/Linger/readme.html
subjects to pay attention and to provide data for a
post-hoc evaluation of comprehension. After each
narrative, subjects were instructed to take a short
break (2 minutes on average).
4.3 Data analysis
The log (base 10) of the reaction times were ana-
lyzed using a linear mixed effects regression anal-
ysis implemented in the language R (Bates et al,
2008). Reaction times longer than 1500 ms and
shorter than 150 ms (raw) were excluded from the
analysis (4.8% of total data). Since button press la-
tencies inferior to 150 ms must have been planned
prior to the presentation of the word, we consid-
ered that they could not reflect stimulus driven ef-
fects. Data from the first and last words on each
line were discarded.
The combined data from the 4 narratives was
first modeled using a model which included or-
der of word in the narrative
3
, word length, parser-
derived lexical surprisal, unigram frequency, bi-
gram probability, syntactic surprisal, lexical en-
tropy, syntactic entropy and mean number of
parser derivation steps as numeric regressors. We
also included the unlexicalized POS variants of
syntactic surprisal and entropy, along the lines of
Demberg and Keller (2008), as detailed in ? 3.
Table 1 presents the correlations between these
mean-centered measures.
In addition, we modeled word class
(open/closed) as a categorical factor in order
to assess interaction between class and the vari-
ables of interest, since such an interaction has
been observed in the case of frequency (Bradley,
1983). Finally, the random effect part of the
model included intercepts for subjects, words and
sentences. We report significant effects at the
threshold p < .05.
Given the presence of significant interactions
between lexical class (open/closed) and a number
of the variables of interests, we decided to split
the data set into open and closed class words and
model these separately (linear mixed effects with
the same numeric variables as in the full model).
In order to evaluate the usefulness of splitting
total surprisal into lexical and syntactic compo-
nents we compared, using a likelihood ratio test,
a model where lexical and syntactic surprisal are
modeled as distinct regressors to a model where a
single regressor equal to their sum (total surprisal)
3
This is a regressor to control for the trend of subjects to
read faster later in the narrative.
329
Predictor SynH LexH SynS LexS Freq Bgrm PosS PosH Step WLen
Syntactic Entropy (SynH) 1.00 -0.26 0.00 0.24 -0.24 0.20 0.02 0.55 -0.05 0.18
Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29
Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03
Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64
Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72
Bigram Probability (Bgrm) 0.20 -0.38 0.18 0.87 -0.69 1.00 0.11 -0.11 -0.16 0.56
POS Surprisal (PosS) 0.02 -0.03 0.77 -0.10 0.02 0.11 1.00 0.22 0.32 0.02
POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11
Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24
Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00
Table 1: Correlations between (mean-centered) predictors. Note that unigram frequencies were represented as logs, other
scores as negative logs, hence the sign of the correlations.
was included. If the larger model provides a sig-
nificantly better fit than the smaller model, this
provides evidence that distinguishing between lex-
ical and syntactic contributions to surprisal is rel-
evant. Since total entropy is not a sum of syntactic
and lexical entropy, an analogous test would not
be valid in that case.
4.4 Results
All subjects successfully answered the com-
prehension questions (92.8% correct responses,
S.D.=5.1). In the full model, we observed signifi-
cant main effects of word class as well as of lexical
surprisal, bigram probability, unigram frequency,
syntactic entropy, POS entropy and of order in the
narrative. Syntactic surprisal, lexical entropy and
number of steps had no significant effect. Word
length also had no significant main effect but inter-
acted significantly with word class (open/closed).
Word class also interacted significantly with lexi-
cal surprisal, unigram frequency and syntactic sur-
prisal.
The presence of these interactions led us to
construct models restricted to open and closed
class items respectively. The estimated parame-
ters are reported in Table 2. Reading time for open
class words showed significant effects of unigram
frequency, syntactic surprisal, syntactic entropy,
POS entropy and order within the narrative. The
positive effect of length approached significance.
Reading time for closed class words exhibited sig-
nificant effects of lexical surprisal, bigram prob-
ability, syntactic entropy and order in the narra-
tive. Length had a non-significant negative effect,
thus explaining the interaction observed in the full
model.
The models with separate lexical and syntac-
tic surprisal performed better than models includ-
ing combined surprisal. For open class words, the
Akaike?s information criterion (AIC) was -54810
for the combined model and -54819 for the inde-
pendent model (likelihood ratio test comparing the
Estimate Std. Error t-value
Open-class
(Intercept) 2.40?10
+00
2.39?10
?02
100.4*
Lexical Surprisal -1.99?10
?04
7.28?10
?04
-0.3
Word Length 8.97?10
?04
4.62?10
?04
1.9
Bigram 4.18?10
?04
5.27?10
?04
0.8
Unigram Freq -2.43?10
?03
1.20?10
?03
-2.0*
Derivation Steps -1.17?10
?03
9.02?10
?04
-1.3
Syntactic Entropy 2.55?10
?03
6.19?10
?04
4.1*
Lexical Entropy 3.96?10
?04
6.68?10
?04
0.6
Syntactic Surprisal 3.28?10
?03
9.71?10
?04
3.4*
Order in narrative -1.43?10
?05
4.34?10
?06
-3.3*
POS Surprisal -6.84?10
?04
8.11?10
?04
-0.8
POS Entropy 1.47?10
?03
6.05?10
?04
2.4*
Closed-class
(Intercept) 2.42?10
+00
2.32?10
?02
104.3*
Lexical Surprisal 2.02?10
?03
7.84?10
?04
2.6*
Word Length -1.87?10
?03
1.13?10
?03
-1.7
Bigram 1.19?10
?03
4.94?10
?04
2.4*
Unigram Freq 1.69?10
?03
2.67?10
?03
0.6
Derivation Steps 3.01?10
?04
5.09?10
?04
0.6
Syntactic Entropy 3.15?10
?03
5.05?10
?04
6.2*
Lexical Entropy 1.83?10
?04
8.63?10
?04
0.2
Syntactic Surprisal 3.00?10
?04
8.35?10
?04
0.4
Order in narrative -1.33?10
?05
3.99?10
?06
-3.3*
POS Surprisal -6.46?10
?04
6.81?10
?04
-0.9
POS Entropy 6.63?10
?04
5.04?10
?04
1.3
Table 2: Estimated effects from mixed effects models on
open and closed items (stars denote significance at p<.05)
two, nested, models: ?
2
(1)=10.7, p<.001). For
closed class items, combined model?s AIC was -
61467 and full model?s AIC was -61469 (likeli-
hood ratio test: ?
2
(1)=3.54, p=0.06).
4.5 Discussion
Our results demonstrate the relevance of model-
ing psycholinguistic processes using an incremen-
tal probabilistic parser, and the utility of the novel
measures presented here. Of particular interest
are: the significant effects of our syntactic en-
tropy measure; the independent contributions of
lexical surprisal, bigram probability and unigram
frequency; and the differences between the pre-
dictions of the lexicalized parsing model and the
unlexicalized (POS) parsing model.
The effect of entropy, or uncertainty regarding
330
the upcoming input independent of the surprise
of that input, has been observed in non-linguistic
tasks (Hyman, 1953; Bestmann et al, 2008) but
to our knowledge has not been quantified before
in the context of sentence processing. The use-
fulness of computational modeling is particularly
evident in the case of entropy given the absence of
any subjective procedure for its evaluation
4
. The
results argue in favor of a predictive parsing archi-
tecture (Van Berkum et al, 2005). The approach
to entropy here differs from the one described in
Hale (2006) in a couple of ways. First, as dis-
cussed above, the calculation procedure is differ-
ent ? we focus on extending the derivations with
just one word, rather than to all possible complete
derivations. Second, and most importantly, Hale
emphasizes entropy reduction (or the gain in in-
formation, given an input, regarding the rest of the
sentence) as the correlate of cognitive cost while
here we are interested in the amount of entropy it-
self (and not the size of change).
Interestingly, we observed only an effect of syn-
tactic entropy, not lexical entropy. Recent ERP
work has demonstrated that subjects do form spe-
cific lexical predictions in the context of sentence
processing (Van Berkum et al, 2005; DeLong et
al., 2005) and so we suspect that the absence of
lexical entropy effect might be partly due to sparse
data. Lexical surprisal and entropy were calcu-
lated using the internal state of a parser trained
on the relatively small Brown corpus. Lexical en-
tropy showed no significant effect while lexical
surprisal affected only closed class words. This
pattern of results might be due to the sparseness
of the relevant information in such a small corpus
(e.g., verb/object preferences) and the relevance of
extra-textual dimensions (world knowledge, con-
textual information) to lexical-specific prediction.
Closed class words are both more frequent (and
hence better sampled) and are less sensitive to
world knowledge, yet are often determined by the
grammatical context.
Demberg and Keller (2008) made use of the
same parsing architecture used here to compute a
syntactic surprisal measure, but used an unlexical-
ized parser (down to POS-tags rather than words)
for this score. Their ?lexicalized? surprisal is
equivalent to our total surprisal (lexical surprisal
+ syntactic surprisal), while their POS surprisal is
4
The Cloze procedure (Taylor, 1953) is one way to derive
probabilities that could be used to calculate entropy, though
this procedure is usually conducted with lexical elicitation,
which would make syntactic entropy calculations difficult.
derived from a completely different model. In con-
trast, our approach achieves lexical and syntactic
measures from the same model. In order to eval-
uate the difference between the two approaches
we added unlexicalized POS surprisal calculated
along the lines of that paper to our model, along
with an unlexicalized POS entropy from the same
model. We found no effect of unlexicalized POS
surprisal
5
and a significant (but relatively small)
effect of unlexicalized POS entropy. While syn-
tactic surprisal was correlated with POS surprisal
(see Table 1) and syntactic entropy correlated with
POS entropy, the fact that our syntactic measures
still had a significant effect suggests that lexical
information contributes towards the formation of
syntactic expectations.
While the effect of surprisal calculated by an
incremental top down parser has been already
demonstrated (Demberg and Keller, 2008), our re-
sults argue for a distinction between the effect
of lexical surprisal and that of syntactic surprisal
without requiring unlexicalized parsing of the sort
that Demberg and Keller advocate. It is important
to keep in mind that this distinction between types
of prediction (and as a consequence, prediction er-
ror) is not equivalent to the one drawn in the tradi-
tional cognitive science modularity debate, which
has focused on the source of these predictions. We
found a positive effect of syntactic surprisal in the
case of open class words. The absence of an effect
for closed class words remains to be explained.
We quantified word specific surprisal using 3
sources: the parser?s internal state (lexical sur-
prisal); probability given the preceding word (neg-
ative log bigram probability); and the unigram fre-
quency of the word in a large corpus
6
. As can
be observed in Table 1, these three measures are
highly correlated
7
. This is the consequence of the
smoothing in the estimation procedure but also re-
lates to a more general fact about language use:
overall, more frequent words are also words more
expected to appear in a specific context (Anderson
and Schooler, 1991). Despite these strong corre-
lations, the three measures produced independent
5
We also ran the model including unlexicalized POS sur-
prisal without our syntactic surprisal or syntactic entropy, and
in this condition the unlexicalized POS surprisal measure had
a nearly significant effect (t = 1.85), which is consistent with
the results in Boston et al (2008a) and Demberg and Keller
(2008).
6
The unigram frequencies came from the HAL corpus
(Lund and Burgess, 1996). All other statistical models were
estimated from the Brown Corpus.
7
Unigram frequencies were represented as logs, the others
as negative logs, hence the sign of the correlations.
331
effects. Unigram frequency had a significant effect
for open class words while bigram probability and
lexical surprisal each had an effect on reading time
of closed class items. Bigram probability has been
often found to affect reading time using eye move-
ment measures. This is the first study to demon-
strate an additional effect of contextual surprisal
given the preceding sentential context (lexical sur-
prisal). Demberg and Keller found no effect for
surprisal once bigram and unigram probabilities
were included in the model but, importantly, they
did not distinguish lexical and syntactic surprisal,
rather ?lexicalized? and ?unlexicalized? surprisal.
5 Summary
We have presented novel methods for teasing apart
syntactic and lexical surprisal from a fully lexi-
calized parser, as well as for extending the oper-
ation of a predictive parser to capture novel en-
tropy measures that are also shown to be rele-
vant to psycholinguistic modeling. Such auto-
matic methods provide psycholinguistically rele-
vant measures that are intractable to calculate by
hand. The empirical validation presented here
demonstrated that the new measures ? particularly
syntactic entropy and syntactic surprisal ? have
high utility for modeling human reading time data.
Our approach to calculating syntactic surprisal,
based on fully lexicalized parsing, provided sig-
nificant effects, while the POS-tag based (unlexi-
calized) surprisal ? of the sort used in Boston et
al. (2008a) and Demberg and Keller (2008) ? did
not provide a significant effect in our trials. Fur-
ther, we showed an effect of lexical surprisal for
closed class words even when combined with uni-
gram and bigram probabilities in the same model.
This work contributes to the important, develop-
ing enterprise of leveraging data-driven NLP ap-
proaches to derive new measures of high utility for
psycholinguistic and neuropsychological studies.
Acknowledgments
Thanks to Michael Collins, John Hale and Shravan
Vasishth for valuable discussions about this work.
This research was supported in part by NSF Grant
#BCS-0826654. Any opinions, findings, conclu-
sions or recommendations expressed in this publi-
cation are those of the authors and do not neces-
sarily reflect the views of the NSF.
References
J.R. Anderson and L.J. Schooler. 1991. Reflections of
the environment in memory. Psychological Science,
2(6):396?408.
A. Bachrach, B. Roark, A. Marantz, S. Whitfield-
Gabrieli, C. Cardenas, and J.D.E. Gabrieli. 2009.
Incremental prediction in naturalistic language pro-
cessing: An fMRI study. In preparation.
D. Bates, M. Maechler, and B. Dai, 2008. lme4: Linear
mixed-effects models using S4 classes. R package
version 0.999375-20.
S. Bestmann, L.M. Harrison, F. Blankenburg, R.B.
Mars, P. Haggard, and K.J. Friston. 2008. Influence
of uncertainty and surprise on human corticospinal
excitability during preparation for action. Current
Biology, 18:775?780.
M. Ferrara Boston, J.T. Hale, R. Kliegl, U. Patil, and
S. Vasishth. 2008a. Parsing costs as predictors
of reading difficulty: An evaluation using the Pots-
dam sentence corpus. Journal of Eye Movement Re-
search, 2(1):1?12.
M. Ferrara Boston, J.T. Hale, R. Kliegl, and S. Va-
sishth. 2008b. Surprising parser actions and read-
ing difficulty. In Proceedings of ACL-08:HLT, Short
Papers, pages 5?8.
D.C. Bradley. 1983. Computational Distinctions of
Vocabulary Type. Indiana University Linguistics
Club, Bloomington.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. In Proceedings of
ACL-COLING, pages 225?231.
M.J. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of
ACL, pages 111?118.
K.A. DeLong, T.P. Urbach, and M. Kutas. 2005. Prob-
abilistic word pre-activation during language com-
prehension inferred from electrical brain activity.
Nature Neuroscience, 8(8):1117?1121.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
L. Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen, and A.M. Zwicky, editors,
Natural Language Parsing. Cambridge University
Press, Cambridge, UK.
K. Gabani, M. Sherman, T. Solorio, and Y. Liu.
2009. A corpus-based approach for the prediction
of language impairment in monolingual English and
Spanish-English bilingual children. In Proceedings
of NAACL-HLT.
S.M. Garnsey, N.J. Pearlmutter, E. Myers, and M.A.
Lotocky. 1997. The contributions of verb bias and
plausibility to the comprehension of temporarily am-
biguous sentences. Journal of Memory and Lan-
guage, 37(1):58?93.
332
E. Gibson. 1998. Linguistic complexity: locality of
syntactic dependencies. Cognition, 68(1):1?76.
E. Gibson. 2006. The interaction of top-down and
bottom-up statistics in the resolution of syntactic
category ambiguity. Journal of Memory and Lan-
guage, 54(3):363?388.
J.T. Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the 2nd
meeting of NAACL.
J.T. Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101?123.
J.T. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643?672.
J. Henderson. 2003. Inducing history representations
for broad coverage statistical parsing. In Proceed-
ings of HLT-NAACL, pages 24?31.
R. Hyman. 1953. Stimulus information as a determi-
nant of reaction time. Journal of Experimental Psy-
chology: General, 45(3):188?96.
F. Jelinek and J. Lafferty. 1991. Computation of
the probability of initial substring generation by
stochastic context-free grammars. Computational
Linguistics, 17(3):315?323.
M. Johnson and B. Roark. 2000. Compact non-left-
recursive grammars using the selective left-corner
transform and factoring. In Proceedings of COL-
ING, pages 355?361.
T. Klee and M.D. Fitzgerald. 1985. The relation be-
tween grammatical development and mean length
of utterance in morphemes. Journal of Child Lan-
guage, 12:251?269.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers, 28:203?208.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K?ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
B. Roark, M. Mitchell, and K. Hollingshead. 2007.
Syntactic complexity measures for detecting mild
cognitive impairment. In Proceedings of BioNLP
Workshop at ACL, pages 1?8.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Au-
tomatic measurement of syntactic development in
child langugage. In Proceedings of ACL, pages 197?
204.
T. Solorio and Y. Liu. 2008. Using language models
to identify language impairment in Spanish-English
bilingual children. In Proceedings of BioNLP Work-
shop at ACL, pages 116?117.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2):165?202.
W.L. Taylor. 1953. Cloze procedure: A new tool
for measuring readability. Journalism Quarterly,
30:415?433.
J.J.A. Van Berkum, C.M. Brown, P. Zwitserlood,
V.Kooijman, and P. Hagoort. 2005. Anticipat-
ing upcoming words in discourse: Evidence from
ERPs and reading times. Learning and Memory,
31(3):443?467.
V.H. Yngve. 1960. A model and an hypothesis for lan-
guage structure. Proceedings of the American Philo-
sophical Society, 104:444?466.
333
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 787?794, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Comparing and Combining Finite-State and Context-Free Parsers
Kristy Hollingshead and Seeger Fisher and Brian Roark
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
Beaverton, Oregon, 97006
{hollingk,fishers,roark}@cslu.ogi.edu
Abstract
In this paper, we look at comparing high-
accuracy context-free parsers with high-
accuracy finite-state (shallow) parsers on
several shallow parsing tasks. We
show that previously reported compar-
isons greatly under-estimated the perfor-
mance of context-free parsers for these
tasks. We also demonstrate that context-
free parsers can train effectively on rel-
atively little training data, and are more
robust to domain shift for shallow pars-
ing tasks than has been previously re-
ported. Finally, we establish that combin-
ing the output of context-free and finite-
state parsers gives much higher results
than the previous-best published results,
on several common tasks. While the
efficiency benefit of finite-state models
is inarguable, the results presented here
show that the corresponding cost in accu-
racy is higher than previously thought.
1 Introduction
Finite-state parsing (also called chunking or shallow
parsing) has typically been motivated as a fast first-
pass for ? or approximation to ? more expensive
context-free parsing (Abney, 1991; Ramshaw and
Marcus, 1995; Abney, 1996). For many very-large-
scale natural language processing tasks (e.g. open-
domain question answering from the web), context-
free parsing may be too expensive, whereas finite-
state parsing is many orders of magnitude faster and
can also provide very useful syntactic annotations
for large amounts of text. For this reason, finite-state
parsing (hereafter referred to as shallow parsing) has
received increasing attention in recent years.
In addition to the clear efficiency benefit of
shallow parsing, Li and Roth (2001) have further
claimed both an accuracy and a robustness benefit
versus context-free parsing. The output of a context-
free parser, such as that of Collins (1997) or Char-
niak (2000), can be transformed into a sequence of
shallow constituents for comparison with the output
of a shallow parser. Li and Roth demonstrated that
their shallow parser, trained to label shallow con-
stituents along the lines of the well-known CoNLL-
2000 task (Sang and Buchholz, 2000), outperformed
the Collins parser in correctly identifying these con-
stituents in the Penn Wall Street Journal (WSJ) Tree-
bank (Marcus et al, 1993). They argued that their
superior performance was due to optimizing directly
for the local sequence labeling objective, rather than
for obtaining a hierarchical analysis over the entire
string. They further showed that their shallow parser
trained on the Penn WSJ Treebank did a far better
job of annotating out-of-domain sentences (e.g. con-
versational speech) than the Collins parser.
This paper re-examines the comparison of shal-
low parsers with context-free parsers, beginning
with a critical examination of how their outputs
are compared. We demonstrate that changes to the
conversion routine, which take into account differ-
ences between the original treebank trees and the
trees output by context-free parsers, eliminate the
previously-reported accuracy differences. Second,
we show that a convention that is widely accepted
for evaluation of context-free parses ? ignoring
punctuation when setting the span of a constituent ?
results in improved shallow parsing performance by
certain context-free parsers across a variety of shal-
low parsing tasks. We also demonstrate that context-
free parsers perform competitively when applied to
out-of-domain data. Finally, we show that large im-
provements can be obtained in several shallow pars-
ing tasks by using simple strategies to incorporate
context-free parser output into shallow parsing mod-
els. Our results demonstrate that a rich context-free
787
parsing model is, time permitting, worth applying,
even if only shallow parsing output is needed. In
addition, our best results, which greatly improve on
the previous-best published results on several tasks,
shed light on how much accuracy is sacrificed in
shallow parsing to get finite-state efficiency.
2 Evaluating Heterogeneous Parser Output
Two commonly reported shallow parsing tasks are
Noun-Phrase (NP) Chunking (Ramshaw and Mar-
cus, 1995) and the CoNLL-2000 Chunking task
(Sang and Buchholz, 2000), which extends the NP-
Chunking task to recognition of 11 phrase types1
annotated in the Penn Treebank. Reference shal-
low parses for this latter task were derived from
treebank trees via a conversion script known as
chunklink2. We follow Li and Roth (2001) in
using chunklink to also convert trees output by a
context-free parser into a flat representation of shal-
low constituents. Figure 1(a) shows a Penn Tree-
bank tree and Figure 1(c) its corresponding shallow
parse constituents, according to the CoNLL-2000
guidelines. Note that consecutive verb phrase (VP)
nodes result in a single VP shallow constituent.
Just as the original treebank trees are converted
for training shallow parsers, they are also typ-
ically modified for training context-free parsers.
This modification includes removal of empty nodes
(nodes tagged with ?-NONE-? in the treebank), and
removal of function tags on non-terminals; e.g., NP-
SBJ (subject NP) and NP-TMP (temporal NP) are
both mapped to NP. The output of the context-free
parser is, of course, in the same format as the train-
ing input, so empty nodes and function tags are not
present. This type of modified tree is what is shown
in Figure 1(b); note that the original treebank tree,
shown in Figure 1(a), had an empty subject NP in
the embedded clause which has been removed for
the modified tree.
To compare the output of their shallow parser with
the output of the well-known Collins (1997) parser,
Li and Roth applied the chunklink conversion
script to extract the shallow constituents from the
output of the Collins parser on WSJ section 00. Un-
1These include: ADJP, ADVP, CONJP, INTJ, LST, NP, PP,
PRT, SBAR, UCP and VP. Anything not in one of these base
phrases is designated as ?outside?.
2Downloaded from http://ilk.kub.nl/?sabine/chunklink/.
(a) S


H
HH
NP-SBJ-1
They
VP
 HH
are VP
 HH
starting S


HH
H
NP-SBJ
-NONE-
*-1
VP


H
HH
to VP


HH
H
buy NP
 HH
growth stocks
(b) S


HH
H
NP
They
VP


HH
H
are VP


HH
H
starting S
VP


HH
H
to VP


H
HH
buy NP
 HH
growth stocks
(c) [NP They] [VP are starting to buy] [NP growth stocks]
Figure 1: (a) Penn WSJ treebank tree, (b) modified treebank
tree, and (c) CoNLL-2000 style shallow bracketing, all of the
same string.
fortunately, the script was built to be applied to the
original treebank trees, complete with empty nodes,
which are not present in the output of the Collins
parser, or any well-known context-free parser. The
chunklink script searches for empty nodes in the
parse tree to perform some of its operations. In par-
ticular, any S node that contains an empty subject
NP and a VP is reduced to just a VP node, and
then combined with any immediately-preceding VP
nodes to create a single VP constituent. If the S
node does not contain an empty subject NP, as in
Figure 1(b), the chunklink script creates two VP
constituents: [VP are starting] [VP to buy], which
in this case results in a bracketing error. However,
it is a simple matter to insert an empty subject NP
into unary S?VP productions so that these nodes
are processed correctly by the script.
Various conventions have become standard in
evaluating parser output over the past decade. Per-
haps the most widely accepted convention is that
of ignoring punctuation for the purposes of assign-
ing constituent span, under the perspective that, fun-
788
Phrase Evaluation Scenario
System Type (a) (b) (c)
?Modified? All 98.37 99.72 99.72
Truth VP 92.14 98.70 98.70
Li and Roth All 94.64 - -
(2001) VP 95.28 - -
Collins (1997) All 92.16 93.42 94.28
VP 88.15 94.31 94.42
Charniak All 93.88 95.15 95.32
(2000) VP 88.92 95.11 95.19
Table 1: F-measure shallow bracketing accuracy under three
different evaluation scenarios: (a) baseline, used in Li and Roth
(2001), with original chunklink script converting treebank
trees and context-free parser output; (b) same as (a), except that
empty subject NPs are inserted into every unary S?VP produc-
tion; and (c) same as (b), except that punctuation is ignored for
setting constituent span. Results for Li and Roth are reported
from their paper. The Collins parser is provided with part-of-
speech tags output by the Brill tagger (Brill, 1995).
damentally, constituents are groupings of words.
Interestingly, this convention was not followed in
the CoNLL-2000 task (Sang and Buchholz, 2000),
which as we will see has a variable effect on context-
free parsers, presumably depending on the degree to
which punctuation is moved in training.
2.1 Evaluation Analysis
To determine the effects of the conversion routine
and different evaluation conventions, we compare
the performance of several different models on one
of the tasks presented in Li and Roth (2001). For
this task, which we label the Li & Roth task, sec-
tions 2-21 of the Penn WSJ Treebank are used as
training data, section 24 is held out, and section 00
is for evaluation.
For all trials in this paper, we report F-measure
labeled bracketing accuracy, which is the harmonic
mean of the labeled precision (P ) and labeled recall
(R), as they are defined in the widely used PARSE-
VAL metrics; i.e. the F-measure accuracy is 2PRP+R .
Table 1 shows baseline results for the Li and
Roth3 shallow parser, two well-known, high-
accuracy context-free parsers, and the reference
(true) parses after being modified as described
3We were unable to obtain the exact model used in Li and
Roth (2001), and so we use their reported results here. Note
that they used reference part-of-speech (POS) tags for their re-
sults on this task. All other results reported in this paper, unless
otherwise noted, were obtained using Brill-tagger POS tags.
above (by removing empty nodes and function
tags). Evaluation scenario (a) in Table 1 corre-
sponds to what was used in Li and Roth (2001) fol-
lowing CoNLL-2000 guidelines, with the original
chunklink script used to transform the context-
free parser output into shallow constituents. We
can see from the performance of the modified truth
in this scenario that there are serious problems
with this conversion, due to the way in which
it handles unary S?VP productions. If we de-
terministically insert empty subject NP nodes for
all such unary productions prior to the use of the
chunklink script, which we do in evaluation sce-
nario (b) of Table 1, this repairs the bulk of the
errors. Some small number of errors remain, due
largely to the fact that if the S node has been an-
notated with a function tag (e.g. S-PRP, S-PRD, S-
CLR), then chunklink will not perform its re-
duction operation on that node. However, for our
purposes, this insertion repair sufficiently corrects
the error to perform meaningful comparisons. Fi-
nally, evaluation scenario (c) follows the context-
free parsing evaluation convention of ignoring punc-
tuation when assigning constituent span. This af-
fects some parsers more than others, depending on
how the parser treats punctuation internally; for
example, Bikel (2004) documents that the Collins
parser raises punctuation nodes within the parse
tree. Since ignoring punctuation cannot hurt perfor-
mance, only improve it, even the smallest of these
differences are statistically significant.
Note that after inserting empty nodes and ignor-
ing punctuation, the accuracy advantage of Li and
Roth over Collins is reduced to a dead heat. Of
the two parsers we evaluated, the Charniak (2000)
parser gave the best performance, which is consis-
tent with its higher reported performance on the
context-free parsing task versus other context-free
parsers. Collins (2000) reported a reranking model
that improved his parser output to roughly the same
level of accuracy as Charniak (2000), and Charniak
and Johnson (2005) report an improvement using
reranking over Charniak (2000). For the purposes
of this paper, we needed an available parser that
was (a) trainable on different subsets of the data to
be applied to various tasks; and (b) capable of pro-
ducing n-best candidates, for potential combination
with a shallow parser. Both the Bikel (2004) imple-
789
System NP-Chunking CoNLL-2000 Li & Roth task
SPRep averaged perceptron 94.21 93.54 95.12
Kudo and Matsumoto (2001) 94.22 93.91 -
Sha and Pereira (2003) CRF 94.38 - -
Voted perceptron 94.09 - -
Zhang et al (2002) - 94.17 -
Li and Roth (2001) - 93.02 94.64
Table 2: Baseline results on three shallow parsing tasks: the NP-Chunking task (Ramshaw and Marcus, 1995); the CoNLL-2000
Chunking task (Sang and Buchholz, 2000); and the Li & Roth task (Li and Roth, 2001), which is the same as CoNLL-2000 but
with more training data and a different test section. The results reported in this table include the best published results on each of
these tasks.
mentation of the Collins parser and the n-best ver-
sion of the Charniak (2000) parser, documented in
Charniak and Johnson (2005), fit the requirements.
Since we observed higher accuracy from the Char-
niak parser, from this point forward we report just
Charniak parser results4.
2.2 Shallow Parser
In addition to the trainable n-best context-free parser
from Charniak (2000), we needed a trainable shal-
low parser to apply to the variety of tasks we were
interested in investigating. To this end, we repli-
cated the NP-chunker described in Sha and Pereira
(2003) and trained it as either an NP-chunker or with
the tagset extended to classify all 11 phrase types
included in the CoNLL-2000 task (Sang and Buch-
holz, 2000). Our shallow parser uses exactly the fea-
ture set delineated by Sha and Pereira, and performs
the decoding process using a Viterbi search with a
second-order Markov assumption as they described.
These features include unigram and bigram words
up to two positions to either side of the current word;
unigram, bigram, and trigram part-of-speech (POS)
tags up to two positions to either side of the current
word; and unigram, bigram, and trigram shallow
constituent tags. We use the averaged perceptron al-
gorithm, as presented in Collins (2002), to train the
parser. See (Sha and Pereira, 2003) for more details
on this approach.
To demonstrate the competitiveness of our base-
line shallow parser, which we label the SPRep av-
eraged perceptron, Table 2 shows results on three
different shallow parsing tasks. The NP-Chunking
4The parser is available for research purposes at
ftp://ftp.cs.brown.edu/pub/nlparser/ and can be run in n-
best mode. The one-best performance of the parser is the same
as what was presented in Charniak (2000).
task, originally introduced in Ramshaw and Marcus
(1995) and also described in (Collins, 2002; Sha and
Pereira, 2003), brackets just base NP constituents5.
The CoNLL-2000 task, introduced as a shared task
at the CoNLL workshop in 2000 (Sang and Buch-
holz, 2000), extends the NP-Chunking task to label
11 different base phrase constituents. For both of
these tasks, the training set was sections 15-18 of
the Penn WSJ Treebank and the test set was section
20. We follow Collins (2002) and Sha and Pereira
(2003) in using section 21 as a heldout set. The third
task, introduced by Li and Roth (2001), performs the
same labeling as in the CoNLL-2000 task, but with
more training data and different testing sets: training
was WSJ sections 2-21 and test was section 00. We
used section 24 as a heldout set; this section is often
used as heldout for training context-free parsers.
Training and testing data for the CoNLL-2000
task is available online6. For the heldout sets for
each of these tasks, as well as for all data sets
needed for the Li & Roth task, reference shallow
parses were generated using the chunklink script
on the original treebank trees. All data was tagged
with the Brill POS tagger (Brill, 1995) after the
chunklink conversion. We verified that using
this method on the original treebank trees in sections
15-18 and 20 generated data that is identical to the
CoNLL-2000 data sets online. Replacing the POS
tags in the input text with Brill POS tags before the
5We follow Sha and Pereira (2003) in deriving the NP con-
stituents from the CoNLL-2000 data sets, by replacing all non-
NP shallow tags with the ?outside? (?O?) tag. They mention
that the resulting shallow parse tags are somewhat different than
those used by Ramshaw and Marcus (1995), but that they found
no significant accuracy differences in training on either set.
6Downloaded from the CoNLL-2000 Shared Task website
http://www.cnts.ua.ac.be/conll2000/chunking/.
790
chunklink conversion results in slightly different
shallow parses.
From Table 2 we can see that our shallow parser
is competitive on all three tasks7. Sha and Pereira
(2003) noted that the difference between their per-
ceptron and CRF results was not significant, and
our performance falls between the two, thus repli-
cating their result within noise. Our performance
falls 0.6 percentage points below the best published
result on the CoNLL-2000 task, and 0.5 percentage
points above the performance by Li and Roth (2001)
on their task. Overall, ours is a competitive approach
for shallow parsing.
3 Experimental Results
3.1 Comparing Finite-State and
Context-Free Parsers
The first two rows of Table 3 present a comparison
between the SPRep shallow parser and the Charniak
(2000) context-free parser detailed in Charniak and
Johnson (2005). We can see that the performance
of the two models is virtually indistinguishable for
all three of these tasks, with or without ignoring of
punctuation. As mentioned earlier, we used the ver-
sion of this parser with improved n-best extraction,
as documented in Charniak and Johnson (2005), al-
though without the reranking of the candidates that
they also report in that paper. For these trials, we
used just the one-best output of that model, which is
the same as in Charniak (2000).
Note that the standard training set for context-free
parsing (sections 2-21) is only used for the Li &
Roth task; for the other two tasks, both the SPRep
and the Charniak parsers were trained on sections
15-18, with section 21 as heldout. This demonstrates
that the context-free parser, even when trained on a
small fraction of the total treebank, is able to learn a
competitive model for this task.
3.2 Combining Finite-State and
Context-Free Parsers
It is likely true that a context-free parser which has
been optimized for global parse accuracy will, on
occasion, lose some shallow parse accuracy to sat-
isfy global structure constraints that do not constrain
7Sha and Pereira (2003) reported the Kudo and Matsumoto
(2001) performance on the NP-Chunking task to be 94.39 and
to be the best reported result on this task. In the cited paper,
however, the result is as reported in our table.
a shallow parser. However, it is also likely true
that these longer distance constraints will on occa-
sion enable the context-free parser to better identify
the shallow constituent structure. In other words,
despite having very similar performance, our shal-
low parser and the Charniak context-free parser are
likely making complementary predictions about the
shallow structure that can be exploited for further
improvements. In this section, we explore two sim-
ple methods for combining the system outputs.
The first combination of the system outputs,
which we call unweighted intersection, is the sim-
plest kind of ?rovered? system, which restricts the
set of shallow parse candidates to the intersection
of the sets output by each system, but does not
combine the scores. Since the Viterbi search of
the SPRep model provides a score for all possi-
ble shallow parses, the intersection of the two sets
is simply the set of shallow-parse sequences in the
50-best candidates output by the Charniak parser.
We then use the SPRep perceptron-model scores to
choose from among just these candidates. We con-
verted the 50-best lists returned by the Charniak
parser into k-best lists of shallow parses by using
chunklink to convert each candidate context-free
parse into a shallow parse. Many of the context-free
parses map to the same shallow parse, so the size of
this list is typically much less than 50, with an aver-
age of around 7. Each of the unique shallow-parse
candidates is given a score by the SPRep percep-
tron, and the best-scoring candidate is selected. Ef-
fectively, we used the Charniak parser?s k-best shal-
low parses to limit the search space for our shallow
parser.
The second combination of the system outputs,
which we call weighted intersection, extends the un-
weighted intersection by including the scores from
the Charniak parser, which are log probabilities.
The score for a shallow parse output by the Char-
niak parser is the log of the sum of the probabili-
ties of all context-free parses mapping to that shal-
low parse. We normalize across all candidates for
a given string, hence these are conditional log prob-
abilities. We multiply these conditional log proba-
bilities by a scaling factor ? before adding them to
the SPRep perceptron score for a particular candi-
date. Again, the best-scoring candidate using this
composite score is selected from among the shallow
791
NP-Chunking CoNLL-2000 Li & Roth task
Punctuation Punctuation Punctuation
System Leave Ignore Leave Ignore Leave Ignore
SPRep averaged perceptron 94.21 94.25 93.54 93.70 95.12 95.27
Charniak (2000) 94.17 94.20 93.77 93.92 95.15 95.32
Unweighted intersection 95.13 95.16 94.52 94.64 95.77 95.92
Weighted intersection 95.57 95.58 95.03 95.16 96.20 96.33
Table 3: F-measure shallow bracketing accuracy on three shallow parsing tasks, for the SPRep perceptron shallow parser, the
Charniak (2000) context-free parser, and for systems combining the SPRep and Charniak system outputs.
parse candidates output by the Charniak parser. We
used the heldout data to empirically estimate an op-
timal scaling factor for the Charniak scores, which
is 15 for all trials reported here. This factor com-
pensates for differences in the dynamic range of the
scores of the two parsers.
Both of these intersections are done at test-time,
i.e. the models are trained independently. To remain
consistent with task-specific training and testing sec-
tion conventions, the individual models were always
trained on the appropriate sections for the given task,
i.e. WSJ sections 15-18 for NP-Chunking and the
CoNLL-2000 tasks, and sections 2-21 for the Li &
Roth task.
Results from these methods of combination are
shown in the bottom two rows of Table 3. Even
the simple unweighted intersection gives quite large
improvements over each of the independent systems
for all three tasks. All of these improvements are
significant at p < 0.001 using the Matched Pair
Sentence Segment test (Gillick and Cox, 1989). The
weighted intersection gives further improvements
over the unweighted intersection for all tasks, and
this improvement is also significant at p < 0.001,
using the same test.
3.3 Robustness to Domain Shift
Our final shallow parsing task was also proposed in
Li and Roth (2001). The purpose of this task was
to examine the degradation in performance when
parsers, trained on one relatively clean domain such
as WSJ, are tested on another, mismatched domain
such as Switchboard. The systems that are reported
in this section are trained on sections 2-21 of the
WSJ Treebank, with section 24 as heldout, and
tested on section 4 of the Switchboard Treebank.
Note that the systems used here are exactly the ones
presented for the original Li & Roth task, in Sec-
Punctuation
System Leave Ignore
Li & Roth (reference tags) 88.47 -
SPRep avg perceptron
Reference tags 91.37 91.86
Brill tags 87.94 88.42
Charniak (2000) 87.94 88.44
Unweighted intersection 88.66 89.16
Weighted intersection 89.22 89.69
Table 4: Shallow bracketing accuracy of several different sys-
tems, trained on sections 2-21 of WSJ Treebank and applied
to section 4 of the Switchboard Treebank. Li and Roth (2001)
results are as reported in their paper, with reference POS tags
rather than Brill-tagger POS tags.
tions 3.1 and 3.2; only the test set has changed, train-
ing and heldout sets remain exactly the same, as do
the mixing parameters for the weighted intersection.
In the trials reported in Li and Roth (2001), both of
the evaluated systems were provided with reference
POS tags from the Switchboard Treebank. In the
current results, we show our SPRep averaged per-
ceptron system provided both with reference POS
tags for comparison with the Li and Roth results,
and provided with Brill-tagger POS tags for com-
parison with other systems. Table 4 shows our re-
sults for this task. Whereas Li and Roth reported
a more marked degradation in performance when
using a context-free parser as compared to a shal-
low parser, we again show virtually indistinguish-
able performance between our SPRep shallow parser
and the Charniak context-free parser. Again, using a
weighted combined model gave us large improve-
ments over each independent model, even in this
mismatched domain.
3.4 Reranked n-best List
Just prior to the publication of this paper, we were
able to obtain the trained reranker from Charniak
792
WSJ Sect. 00 SWBD Sect. 4
Punctuation Punctuation
System Leave Ignore Leave Ignore
SPRep 95.12 95.27 87.94 88.43
C & J one-best 95.15 95.32 87.94 88.44
(2005) reranked 95.81 96.04 88.64 89.17
Weighted intersection 96.32 96.47 89.32 89.80
Table 5: F-measure shallow bracketing accuracy when trained
on WSJ sections 2-21 and applied to either WSJ section 00 or
SWBD section 4. Systems include our shallow parser (SPRep);
the Charniak and Johnson (2005) system (C & J), both initial
one-best and reranked-best; and the weighted intersection be-
tween the reranked 50-best list and the SPRep system.
and Johnson (2005), which allows a comparison of
the shallow parsing gains that they obtain from that
system with those documented here. The reranker is
a discriminatively trained Maximum Entropy model
with an F-measure parsing accuracy objective. It
uses a large number of features, and is applied to the
50-best output from the generative Charniak parsing
model. The reranking model was trained on sections
2-21, with section 24 used as heldout. This allows us
to compare its shallow parsing accuracy with other
systems on the tasks that use this training setup: the
Li & Roth task (testing on WSJ section 00) and the
domain shift task (testing on Switchboard section
4). Table 5 shows two new trials making use of this
reranking model.
The Charniak and Johnson (2005) system out-
put (denoted C & J in the table) before rerank-
ing (denoted one-best) is identical to the Charniak
(2000) results that have been reported in the other
tables. After reranking (denoted reranked), the per-
formance improves by roughly 0.7 percentage points
for both tasks, nearly reaching the performance
that we obtained with weighted intersection of the
SPRep model and the n-best list before reranking.
Weighted intersection between the reranked list and
the shallow parser as described earlier, with a newly
estimated scaling factor (?=30), provides a roughly
0.5 percentage point increase over the result ob-
tained by the reranker. The difference between the
Charniak output before and after reranking is statis-
tically significant at p < 0.001, as is the difference
between the reranked output and the weighted inter-
section, using the same test reported earlier.
3.5 Discussion
While it may be seen to be overkill to apply a
context-free parser for these shallow parsing tasks,
we feel that these results are very interesting for
a couple of reasons. First, they go some way to-
ward correcting the misperception that context-free
parsers are less applicable in real-world scenarios
than finite-state sequence models. Finite-state mod-
els are undeniably more efficient; however, it is
important to have a clear idea of how much ac-
curacy is being sacrificed to reach that efficiency.
Any given application will need to examine the ef-
ficiency/accuracy trade-off with different objectives
for optimality. For those willing to trade efficiency
for accuracy, it is worthwhile knowing that it is pos-
sible to do much better on these tasks than what has
been reported in the past.
4 Conclusion and Future Work
In summary, we have demonstrated in this paper that
there is no accuracy or robustness benefit to shal-
low parsing with finite-state models over using high-
accuracy context-free models. Even more, there is a
large benefit to be had in combining the output of
high-accuracy context-free parsers with the output
of shallow parsers. We have demonstrated a large
improvement over the previous-best reported re-
sults on several tasks, including the well-known NP-
Chunking and CoNLL-2000 shallow parsing tasks.
Part of the misperception of the relative benefits
of finite-state and context-free models is due to dif-
ficulty evaluating across these differing annotation
styles. Mapping from context-free parser output
to the shallow constituents defined in the CoNLL-
2000 task depends on many construction-specific
operations that have unfairly penalized context-free
parsers in previous comparisons.
While the results of combining system outputs
show one benefit of combining systems, as presented
in this paper, they hardly exhaust the possibilities
of exploiting the differences between these models.
Making use of the scores for the shallow parses out-
put by the Charniak parser is a demonstrably ef-
fective way to improve performance. Yet there are
other possible features explicit in the context-free
parse candidates, such as head-to-head dependen-
cies, which might be exploited to further improve
performance. We intend to explore including fea-
tures from the context-free parser output in our per-
ceptron model to improve shallow parsing accuracy.
Another possibility is to look at improving
793
context-free parsing accuracy. Within a multi-pass
parsing strategy, the high-accuracy shallow parses
that result from system combination could be used
to restrict the search within yet another pass of a
context-free parser. That parser could then search
for the best global analysis from within just the
space of parses consistent with the provided shallow
parse. Also, features of the sort used in our shallow
parser could be included in a reranker, such as that
in Charniak and Johnson (2005), with a context-free
parsing accuracy objective.
A third possibility is to optimize the definition of
the shallow-parse phrase types themselves, for use
in other applications. The composition of the set of
phrase types put forth by Sang and Buchholz (2000)
may not be optimal for certain applications. One
such application is discourse parsing, which relies
on accurate detection of clausal boundaries. Shal-
low parsing could provide reliable information on
the location of these boundaries, but the current set
of phrase types may be too general for such use. For
example, consider infinitival verb phrases, which of-
ten indicate the start of a clause whereas other types
of verb phrases do not. Unfortunately, with only one
VP category in the CoNLL-2000 set of phrase types,
this distinction is lost. Expanding the defined set of
phrase types could benefit many applications.
Future work will also include continued explo-
ration of possible features that can be of use for ei-
ther shallow parsing models or context-free parsing
models. In addition, we intend to investigate ways
in which to encode approximations to context-free
parser derived features that can be used within finite-
state models, thus perhaps preserving finite-state ef-
ficiency while capturing at least some of the accu-
racy gain that was observed in this paper.
Acknowledgments
We would like to thank Eugene Charniak and Mark
Johnson for help with the parser and reranker doc-
umented in their paper. The first author of this pa-
per was supported under an NSF Graduate Research
Fellowship. In addition, this research was supported
in part by NSF Grant #IIS-0447214. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
References
Steven Abney. 1991. Parsing by chunks. In Robert Berwick,
Steven Abney, and Carol Tenny, editors, Principle-Based
Parsing. Kluwer Academic Publishers, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cascades.
Natural Language Engineering, 2(4):337?344.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4).
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of NAACL, pages
132?139.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th Annual
Meeting of ACL, pages 16?23.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of the 17th ICML Confer-
ence.
Michael Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the Conference on
EMNLP, pages 1?8.
L. Gillick and S. Cox. 1989. Some statistical issues in the com-
parison of speech recognition algorithms. In Proceedings of
ICASSP, pages 532?535.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support
vector machines. In Proceedings of the 2nd Annual Meeting
of NAACL.
Xin Li and Dan Roth. 2001. Exploring evidence for shallow
parsing. In Proceedings of the 5th Conference on CoNLL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceedings of
the 3rd Workshop on Very Large Corpora, pages 82?94.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proceed-
ings of the 4th Conference on CoNLL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the HLT-NAACL
Annual Meeting.
Tong Zhang, Fred Damerau, and David Johnson. 2002. Text
chunking based on a generalization of Winnow. Journal of
Machine Learning Research, 2:615?637.
794
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 312?319,
New York, June 2006. c?2006 Association for Computational Linguistics
Probabilistic Context-Free Grammar Induction
Based on Structural Zeros
Mehryar Mohri
Courant Institute of Mathematical Sciences
and Google Research
251 Mercer Street
New York, NY 10012
mohri@cs.nyu.edu
Brian Roark
Center for Spoken Language Understanding
OGI at Oregon Health & Science University
20000 NW Walker Road
Beaverton, Oregon 97006
roark@cslu.ogi.edu
Abstract
We present a method for induction of con-
cise and accurate probabilistic context-
free grammars for efficient use in early
stages of a multi-stage parsing technique.
The method is based on the use of statis-
tical tests to determine if a non-terminal
combination is unobserved due to sparse
data or hard syntactic constraints. Ex-
perimental results show that, using this
method, high accuracies can be achieved
with a non-terminal set that is orders
of magnitude smaller than in typically
induced probabilistic context-free gram-
mars, leading to substantial speed-ups in
parsing. The approach is further used in
combination with an existing reranker to
provide competitive WSJ parsing results.
1 Introduction
There is a very severe speed vs. accuracy tradeoff
in stochastic context-free parsing, which can be ex-
plained by the grammar factor in the running-time
complexity of standard parsing algorithms such as
the CYK algorithm (Kasami, 1965; Younger, 1967).
That algorithm has complexity O(n3|P |), where n is
the length in words of the sentence parsed, and |P | is
the number of grammar productions. Grammar non-
terminals can be split to encode richer dependen-
cies in a stochastic model and improve parsing ac-
curacy. For example, the parent of the left-hand side
(LHS) can be annotated onto the label of the LHS
category (Johnson, 1998), hence differentiating, for
instance, between expansions of a VP with parent S
and parent VP. Such annotations, however, tend to
substantially increase the number of grammar pro-
ductions as well as the ambiguity of the grammar,
thereby significantly slowing down the parsing algo-
rithm. In the case of bilexical grammars, where cat-
egories in binary grammars are annotated with their
lexical heads, the grammar factor contributes an ad-
ditional O(n2|VD|3) complexity, leading to an over-
all O(n5|VD|3) parsing complexity, where |VD| is
the number of delexicalized non-terminals (Eisner,
1997). Even with special modifications to the ba-
sic CYK algorithm, such as those presented by Eis-
ner and Satta (1999), improvements to the stochastic
model are obtained at the expense of efficiency.
In addition to the significant cost in efficiency,
increasing the non-terminal set impacts parame-
ter estimation for the stochastic model. With
more productions, much fewer observations per
production are available and one is left with the
hope that a subsequent smoothing technique can
effectively deal with this problem, regardless of
the number of non-terminals created. Klein and
Manning (2003b) showed that, by making certain
linguistically-motivated node label annotations, but
avoiding certain other kinds of state splits (mainly
lexical annotations) models of relatively high accu-
racy can be built without resorting to smoothing.
The resulting grammars were small enough to al-
low for exhaustive CYK parsing; even so, parsing
speed was significantly impacted by the state splits:
the test-set parsing time reported was about 3s for
average length sentences, with a memory usage of
1GB.
This paper presents an automatic method for de-
ciding which state to split in order to create concise
and accurate unsmoothed probabilistic context-free
grammars (PCFGs) for efficient use in early stages
of a multi-stage parsing technique. The method is
based on the use of statistical tests to determine if
a non-terminal combination is unobserved due to
the limited size of the sample (sampling zero) or
because it is grammatically impossible (structural
zero). This helps introduce a relatively small number
of new non-terminals with little additional parsing
312
NP


 @ PP
P
DT JJ NN NNS
NP


HH
H
DT NP:JJ+NN+NNS


H
H
JJ NP:NN+NNS
 HH
NN NNS
NP
 HH
DT NP:JJ+NN


H
H
JJ NP:NN+NNS
 HH
NN NNS
NP
 HH
DT NP:JJ
 HH
JJ NP:NN
 HH
NN NNS
NP
 HH
DT NP:
 HH
JJ NP:
 HH
NN NNS
(a) (b) (c) (d) (e)
Figure 1: Five representations of an n-ary production, n = 4. (a) Original production (b) Right-factored production (c) Right-
factored Markov order-2 (d) Right-factored Markov order-1 (e) Right-factored Markov order-0
overhead. Experimental results show that, using this
method, high accuracies can be achieved with orders
of magnitude fewer non-terminals than in typically
induced PCFGs, leading to substantial speed-ups in
parsing. The approach can further be used in combi-
nation with an existing reranker to provide competi-
tive WSJ parsing results.
The remainder of the paper is structured as fol-
lows. Section 2 gives a brief description of PCFG
induction from treebanks, including non-terminal
label-splitting, factorization, and relative frequency
estimation. Section 3 discusses the statistical criteria
that we explored to determine structural zeros and
thus select non-terminals for the factored PCFG. Fi-
nally, Section 4 reports the results of parsing experi-
ments using our exhaustive k-best CYK parser with
the concise PCFGs induced from the Penn WSJ tree-
bank (Marcus et al, 1993).
2 Grammar induction
A context-free grammar G = (V, T, S?, P ), or CFG
in short, consists of a set of non-terminal symbols V ,
a set of terminal symbols T , a start symbol S? ? V ,
and a set of production P of the form: A ? ?,
where A ? V and ? ? (V ? T )?. A PCFG is a
CFG with a probability assigned to each production.
Thus, the probabilities of the productions expanding
a given non-terminal sum to one.
2.1 Smoothing and factorization
PCFGs induced from the Penn Treebank have many
productions with long sequences of non-terminals
on the RHS. Probability estimates of the RHS given
the LHS are often smoothed by making a Markov
assumption regarding the conditional independence
of a category on those more than k categories away
(Collins, 1997; Charniak, 2000):
P(X ? Y1...Yn)= P(Y1|X)
nY
i=2
P(Yi|X,Y1 ? ? ?Yi?1)
? P(Y1|X)
nY
i=2
P(Yi|X,Yi?k ? ? ?Yi?1).
Making such a Markov assumption is closely re-
lated to grammar transformations required for cer-
tain efficient parsing algorithms. For example, the
CYK parsing algorithm takes as input a Chomsky
Normal Form PCFG, i.e., a grammar where all pro-
ductions are of the form X ? Y Z or X ? a,
where X , Y , and Z are non-terminals and a a ter-
minal symbol.1. Binarized PCFGs are induced from
a treebank whose trees have been factored so that
n-ary productions with n>2 become sequences of
n?1 binary productions. Full right-factorization in-
volves concatenating the final n?1 categories from
the RHS of an n-ary production to form a new com-
posite non-terminal. For example, the original pro-
duction NP ? DT JJ NN NNS shown in Figure 1(a)
is factored into three binary rules, as shown in Fig-
ure 1(b). Note that a PCFG induced from such right-
factored trees is weakly equivalent to a PCFG in-
duced from the original treebank, i.e., it describes
the same language.
From such a factorization, one can make a
Markov assumption for estimating the production
probabilities by simply recording only the labels of
the first k children dominated by the composite fac-
tored label. Figure 1 (c), (d), and (e) show right-
factored trees of Markov orders 2, 1 and 0 respec-
tively.2 In addition to being used for smoothing
1Our implementation of the CYK algorithm has been ex-
tended to allow for unary productions with non-terminals on
the RHS in the PCFG.
2Note that these factorizations do not provide exactly the
stated Markov order for all dependencies in the productions,
because we are restricting factorization to only produce binary
productions. For example, in Figure 1(e), the probability of the
313
PCFG Time (s) Words/s |V | |P | LR LP F
Right-factored 4848 6.7 10105 23220 69.2 73.8 71.5
Right-factored, Markov order-2 1302 24.9 2492 11659 68.8 73.8 71.3
Right-factored, Markov order-1 445 72.7 564 6354 68.0 73.0 70.5
Right-factored, Markov order-0 206 157.1 99 3803 61.2 65.5 63.3
Parent-annotated, Right-factored, Markov order-2 7510 4.3 5876 22444 76.2 78.3 77.2
Table 1: Baseline results of exhaustive CYK parsing using different probabilistic context-free grammars. Grammars are trained
from sections 2-21 of the Penn WSJ Treebank and tested on all sentences of section 24 (no length limit), given weighted k-best
POS-tagger output. The second and third columns report the total parsing time in seconds and the number of words parsed per
second. The number of non-terminals, |V |, is indicated in the next column. The last three columns show the labeled recall (LR),
labeled precision (LP), and F-measure (F).
as mentioned above, these factorizations reduce the
size of the non-terminal set, which in turn improves
CYK efficiency. The efficiency benefit of making a
Markov assumption in factorization can be substan-
tial, given the reduction of both non-terminals and
productions, which improves the grammar constant.
With standard right-factorization, as in Figure 1(b),
the non-terminal set for the PCFG induced from sec-
tions 2-21 of the Penn WSJ Treebank grows from
its original size of 72 to 10105, with 23220 produc-
tions. With a Markov factorization of orders 2, 1 and
0 we get non-terminal sets of size 2492, 564, and 99,
and rule production sets of 11659, 6354, and 3803,
respectively.
These reductions in the size of the non-terminal
set from the original factored grammar result in an
order of magnitude reduction in complexity of the
CYK algorithm. One common strategy in statisti-
cal parsing is what can be termed an approximate
coarse-to-fine approach: a simple PCFG is used to
prune the search space to which richer and more
complex models are applied subsequently (Char-
niak, 2000; Charniak and Johnson, 2005). Produc-
ing a ?coarse? chart as efficiently as possible is thus
crucial (Charniak et al, 1998; Blaheta and Charniak,
1999), making these factorizations particularly use-
ful.
2.2 CYK parser and baselines
To illustrate the importance of this reduction in non-
terminals for efficient parsing, we will present base-
line parsing results for a development set. For
these baseline trials, we trained a PCFG on sec-
tions 2-21 of the Penn WSJ Treebank (40k sen-
tences, 936k words), and evaluated on section 24
(1346 sentences, 32k words). The parser takes as
input the weighted k-best POS-tag sequences of a
final NNS depends on the preceding NN, despite the Markov
order-0 factorization. Because of our focus on efficient CYK,
we accept these higher order dependencies rather than produc-
ing unary productions. Only n-ary rules n>2 are factored.
perceptron-trained tagger, using the tagger docu-
mented in Hollingshead et al (2005). The number
of tagger candidates k for all trials reported in this
paper was 0.2n, where n is the length of the string.
From the weighted k-best list, we derive a condi-
tional probability of each tag at position i by taking
the sum of the exponential of the weights of all can-
didates with that tag at position i (softmax).
The parser is an exhaustive CYK parser that takes
advantage of the fact that, with the grammar fac-
torization method described, factored non-terminals
can only occur as the second child of a binary pro-
duction. Since the bulk of the non-terminals result
from factorization, this greatly reduces the number
of possible combinations given any two cells. When
parsing with a parent-annotated grammar, we use a
version of the parser that also takes advantage of the
partitioning of the non-terminal set, i.e., the fact that
any given non-terminal has already its parent indi-
cated in its label, precluding combination with any
non-terminal that does not have the same parent an-
notated.
Table 1 shows baseline results for standard right-
factorization and factorization with Markov orders
0-2. Training consists of applying a particular gram-
mar factorization to the treebank prior to inducing
a PCFG using maximum likelihood (relative fre-
quency) estimation. Testing consists of exhaustive
CYK parsing of all sentences in the development set
(no length limit) with the induced grammar, then de-
transforming the maximum likelihood parse back to
the original format for evaluation against the refer-
ence parse. Evaluation includes the standard PAR-
SEVAL measures labeled precision (LP) and labeled
recall (LR), plus the harmonic mean (F-measure) of
these two scores. We also present a result using
parent annotation (Johnson, 1998) with a 2nd-order
Markov assumption. Parent annotation occurs prior
to treebank factorization. This condition is roughly
equivalent to the h = 1, v = 2 in Klein and Manning
314
(2003b)3.
From these results, we can see the large efficiency
benefit of the Markov assumption, as the size of the
non-terminal and production sets shrink. However,
the efficiency gains come at a cost, with the Markov
order-0 factored grammar resulting in a loss of a full
8 percentage points of F-measure accuracy. Parent
annotation provides a significant accuracy improve-
ment over the other baselines, but at a substantial
efficiency cost.
Note that the efficiency impact is not a strict func-
tion of either the number of non-terminals or pro-
ductions. Rather, it has to do with the number of
competing non-terminals in cells of the chart. Some
grammars may be very large, but less ambiguous in
a way that reduces the number of cell entries, so that
only a very small fraction of the productions need to
be applied for any pair of cells. Parent annotation
does just the opposite ? it increases the number of
cell entries for the same span, by creating entries for
the same constituent with different parents. Some
non-terminal annotations, e.g., splitting POS-tags by
annotating their lexical items, result in a large gram-
mar, but one where the number of productions that
will apply for any pair of cells is greatly reduced.
Ideally, one would obtain the efficiency benefit
of the small non-terminal set demonstrated with the
Markov order-0 results, while encoding key gram-
matical constraints whose absence results in an ac-
curacy loss. The method we present attempts to
achieve this by using a statistical test to determine
structural zeros and modifying the factorization to
remove the probability mass assigned to them.
3 Detecting Structural Zeros
The main idea behind our method for detecting
structural zeros is to search for events that are in-
dividually very frequent but that do not co-occur.
For example, consider the Markov order-0 bi-
nary rule production in Figure 2. The produc-
tion NP?NP NP: may be very frequent, as is the
NP:?CC NN production, but they never co-occur
together, because NP does not conjoin with NN
in the Penn Treebank. If the counts of two such
events a and b, e.g., NP?NP NP: and NP:?CC NN
are very large, but the count of their co-occurrence
3Their Markov order-2 factorization does not follow the lin-
ear order of the children, but rather includes the head-child plus
one other, whereas our factorization does not involve identifica-
tion of the head child.
NP


HH
H
NP
 PP
?
NP:
 H
CC NN
Figure 2: Markov order-0 local tree, with possible non-local
?state-split information.
is zero, then the co-occurrence of a and b can be
viewed as a candidate for the list of events that
are structurally inadmissible. The probability mass
for the co-occurrence of a and b can be removed
by replacing the factored non-terminal NP: with
NP:CC:NN whenever there is a CC and an NN com-
bining to form a factored NP non-terminal.
The expansion of the factored non-terminals is not
the only event that we might consider. For exam-
ple, a frequent left-most child of the first child of the
production, or a common left-corner POS or lexi-
cal item, might never occur with certain productions.
For example, ?SBAR?IN S? and ?IN?of? are both
common productions, but they never co-occur. We
focus on left-most children and left-corners because
of the factorization that we have selected, but the
same idea could be applied to other possible state
splits.
Different statistical criteria can be used to com-
pare the counts of two events with that of their co-
occurrence. This section examines several possible
criteria that are presented, for ease of exposition,
with general sequences of events. For our specific
purpose, these sequences of events would be two
rule productions.
3.1 Notation
This section describes several statistical criteria to
determine if a sequence of two events should be
viewed as a structural zero. These tests can be gen-
eralized to longer and more complex sequences, and
to various types of events, e.g., word, word class, or
rule production sequences.
Given a corpus C, and a vocabulary ?, we denote
by ca the number of occurrences of a in C. Let n
be the total number of observations in C. We will
denote by a? the set {b ? ? : b 6= a}. Hence ca? =
n? ca. Let P(a) = can , and for b ? ?, let P(a|b) =cab
cb
. Note that ca?b = cb ? cab.
315
3.2 Mutual information
The mutual information between two random vari-
ables X and Y is defined as
I(X;Y ) =
?
x,y
P(x, y) log
P(x, y)
P(x)P(y)
. (1)
For a particular event sequence of length two ab, this
suggests the following statistic:
I(ab) = log P(ab)? log P(a)? log P(b)
= log cab ? log ca ? log cb + log n
Unfortunately, for cab = 0, I(ab) is not finite. If we
assume, however, that all unobserved sequences are
given some ? count, then when cab = 0,
I(ab) = K ? log ca ? log cb, (2)
where K is a constant. Since we need these statistics
only for ranking purposes, we can ignore the con-
stant factor.
3.3 Log odds ratio
Another statistic that, like mutual information, is ill-
defined with zeros, is the log odds ratio:
log(??) = log cab + log ca?b? ? log ca?b ? log cab?.
Here again, if cab = 0, log(??) is not finite. But, if we
assign to all unobserved pairs a small count ?, when
cab = 0, ca?b = cb, and the expression becomes
log(??) = K + log ca?b? ? log cb ? log ca. (3)
3.4 Pearson chi-squared
For any i, j ? ?, define ??ij = cicjn . The Pearson
chi-squared test of independence is then defined as
follows:
X 2 =
?
i ? {a, a?}
j ?
?
b, b?
?
(cij???ij)2
??ij
=
?
i ? {a, a?}
j ?
?
b, b?
?
(ncij?cicj)2
ncicj
.
In the case of interest for us, cab = 0 and the statistic
simplifies to:
X 2 = cacbn +
c2acb
nca?
+
cac2b
ncb?
+
c2ac
2
b
nca?cb?
= ncacbca?cb? .
(4)
3.5 Log likelihood ratio
Pearson?s chi-squared statistic assumes a normal or
approximately normal distribution, but that assump-
tion typically does not hold for the occurrences of
rare events (Dunning, 1994). It is then preferable to
use the likelihood ratio statistic which allows us to
compare the null hypothesis, that P(b) = P(b|a) =
P(b|a?) = cbn , with the hypothesis that P(b|a) =
cab
ca
and P(b|a?) = ca?bca? . In words, the null hypothesis
is that the context of event a does not change the
probability of seeing b. These discrete conditional
probabilities follow a binomial distribution, hence
the likelihood ratio is
? =
B[P(b), cab, ca] B[P(b), ca?b, ca?]
B[P(b|a), cab, ca] B[P(b|a?), ca?b, ca?]
, (5)
where B[p, x, y] = px(1 ? p)y?x( y
x
). In the spe-
cial case where cab = 0, P(b|a?) = P(b), and this
expression can be simplified as follows:
? =
(1? P(b))caP(b)ca?b(1? P(b))ca??ca?b
P(b|a?)ca?b(1? P(b|a?))ca??ca?b
= (1? P(b))ca . (6)
The log-likelihood ratio, denoted by G2, is known to
be asymptotically X 2-distributed. In this case,
G2 = ?2ca log(1? P(b)), (7)
and with the binomial distribution, it has has one
degree of freedom, thus the distribution will have
asymptotically a mean of one and a standard devia-
tion of
?
2.
We experimented with all of these statistics.
While they measure different ratios, empirically they
seem to produce very similar rankings. For the
experiments reported in the next section, we used
the log-likelihood ratio because this statistic is well-
defined with zeros and is preferable to the Pearson
chi-squared when dealing with rare events.
4 Experimental results
We used the log-likelihood ratio statistic G2 to rank
unobserved events ab, where a ? P and b ? V . Let
Vo be the original, unfactored non-terminal set, and
let ? ? (Vo :)? be a sequence of zero or more non-
terminal/colon symbol pairs. Suppose we have a fre-
quent factored non-terminal X :?B for X,B ? Vo.
Then, if the set of productions X ? Y X :?A with
316
A ? Vo is also frequent, but X ? Y X:?B is un-
observed, this is a candidate structural zero. Simi-
lar splits can be considered with non-factored non-
terminals.
There are two state split scenarios we consider in
this paper. Scenario 1 is for factored non-terminals,
which are always the second child of a binary pro-
duction. For use in Equation 7,
ca =
?
A?Vo
c(X ? Y X:?A)
cb = c(X:?B) for B ? Vo
cab = c(X ? Y X:?B)
P(b) =
c(X:?B)
?
A?Vo c(X:?A)
.
Scenario 2 is for non-factored non-terminals, which
we will split using the leftmost child, the left-corner
POS-tag, and the left-corner lexical item, which are
easily incorporated into our grammar factorization
approach. In this scenario, the non-terminal to be
split can be either the left or right child in the binary
production. Here we show the counts for the left
child case for use in Equation 7:
ca =
?
A
c(X ? Y [?A]Z)
cb = c(Y[?B])
cab = c(X ? Y [?B]Z)
P(b) =
c(Y [?B])
?
A c(Y [?A])
In this case, the possible splits are more compli-
cated than just non-terminals as used in factoring.
Here, the first possible split is the left child cat-
egory, along with an indication of whether it is
a unary production. One can further split by in-
cluding the left-corner tag, and even further by
including the left-corner word. For example, a
unary S category might be split as follows: first to
S[1:VP] if the single child of the S is a VP; next
to S[1:VP:VBD] if the left-corner POS-tag is VBD;
finally to S[1:VP:VBD:went] if the VBD verb was
?went?.
Note that, once non-terminals are split by anno-
tating such information, the base non-terminals, e.g.,
S, implicitly encode contexts other than the ones that
were split.
Table 2 shows the unobserved rules with the
largest G2 score, along with the ten non-terminals
Unobserved production G2
(added NT(s) in bold) score
PP ? IN[that] NP 7153.1
SBAR ? IN[that] S[1:VP] 5712.1
SBAR ? IN[of] S 5270.5
SBAR ? WHNP[1:WDT] S[1:VP:TO] 4299.9
VP ? AUX VP[MD] 3972.1
SBAR ? IN[in] S 3652.1
NP ? NP VP[VB] 3236.2
NP ? NN NP:CC:NP 2796.3
SBAR ? WHNP S[1:VP:VBG] 2684.9
Table 2: Top ten non-terminals to add, and the unobserved
productions leading to their addition to the non-terminal set.
that these productions suggest for inclusion in
our non-terminal set. The highest scoring un-
observed production is PP ? IN[that] NP. It re-
ceives such a high score because the base production
(PP ? IN NP) is very frequent, and so is ?IN?that?,
but they jointly never occur, since ?IN?that? is a
complementizer. This split non-terminal also shows
up in the second-highest ranked zero, an SBAR with
?that? complementizer and an S child that consists
of a unary VP. The unary S?VP production is very
common, but never with a ?that? complementizer in
an SBAR.
Note that the fourth-ranked production uses two
split non-terminals. The fifth ranked rule presum-
ably does not add much information to aid parsing
disambiguation, since the AUX MD tag sequence is
unlikely4. The eighth ranked production is the first
with a factored category, ruling out coordination be-
tween NN and NP.
Before presenting experimental results, we will
mention some practical issues related to the ap-
proach described. First, we independently parame-
terized the number of factored categories to select
and the number of non-factored categories to se-
lect. This was done to allow for finer control of the
amount of splitting of non-terminals of each type.
To choose 100 of each, every non-terminal was as-
signed the score of the highest scoring unobserved
production within which it occurred. Then the 100
highest scoring non-terminals of each type were
added to the base non-terminal list, which originally
consisted of the atomic treebank non-terminals and
Markov order-0 factored non-terminals.
Once the desired non-terminals are selected, the
training corpus is factored, and non-terminals are
split if they were among the selected set. Note, how-
4In fact, we do not consider splits when both siblings are
POS-tags, because these are unlikely to carry any syntactic dis-
ambiguation.
317
0 250 500 750 1000 1250 150060
6570
7580
8590
Number of non?factored splits
F?me
asure 
accura
cy
Figure 3: F-measure accuracy on development set versus the
number of non-factored splits for the given run. Points represent
different numbers of factored splits.
ever, that some of the information in a selected non-
terminal may not be fully available, requiring some
number of additional splits. Any non-terminal that is
required by a selected non-terminal will be selected
itself. For example, suppose that NP:CC:NP was
chosen as a factored non-terminal. Then the sec-
ond child of any local tree with that non-terminal
on the LHS must either be an NP or a factored
non-terminal with at least the first child identified
as an NP, i.e., NP:NP. If that factored non-terminal
was not selected to be in the set, it must be added.
The same situation occurs with left-corner tags and
words, which may be arbitrarily far below the cate-
gory.
After factoring and selective splitting of non-
terminals, the resulting treebank corpus is used to
train a PCFG. Recall that we use the k-best output of
a POS-tagger to parse. For each POS-tag and lexical
item pair from the output of the tagger, we reduce
the word to lower case and check to see if the com-
bination is in the set of split POS-tags, in which case
we split the tag, e.g., IN[that].
Figure 3 shows the F-measure accuracy for our
trials on the development set versus the number of
non-factored splits parameterized for the trial. From
this plot, we can see that 500 non-factored splits
provides the best F-measure accuracy on the dev
set. Presumably, as more than 500 splits are made,
sparse data becomes more problematic. Figure 4
shows the development set F-measure accuracy ver-
sus the number of words-per-second it takes to parse
the development set, for non-factored splits of 0 and
500, at a range of factored split parameterizations.
With 0 non-factored splits, efficiency is substantially
impacted by increasing the factored splits, whereas
it can be seen that with 500 non-factored splits, that
impact is much less, so that the best performance
0 20 40 60 80 100 120 140 160 18060
65
70
75
80
85
90
Words per second
F?me
asure 
accura
cy
non?fact. splits=0non?fact. splits=500Markov order?0Markov order?1Markov order?2PA, Markov order?2
Figure 4: F-measure accuracy versus words-per-second for
(1) no non-factored splits (i.e., only factored categories se-
lected); (2) 500 non-factored splits, which was the best perform-
ing; and (3) four baseline results.
is reached with both relatively few factored non-
terminal splits, and a relatively small efficiency im-
pact. The non-factored splits provide substantial ac-
curacy improvements at relatively small efficiency
cost.
Table 3 shows the 1-best and reranked 50-best re-
sults for the baseline Markov order-2 model, and
the best-performing model using factored and non-
factored non-terminal splits. We present the effi-
ciency of the model in terms of words-per-second
over the entire dev set, including the longer strings
(maximum length 116 words)5. We used the k-best
decoding algorithm of Huang and Chiang (2005)
with our CYK parser, using on-demand k-best back-
pointer calculation. We then trained a MaxEnt
reranker on sections 2-21, using the approach out-
lined in Charniak and Johnson (2005), via the pub-
licly available reranking code from that paper.6 We
used the default features that come with that pack-
age. The processing time in the table includes the
time to parse and rerank. As can be seen from the
trials, there is some overhead to these processes, but
the time is still dominated by the base parsing.
We present the k-best results to demonstrate the
benefits of using a better model, such as the one we
have presented, for producing candidates for down-
stream processing. Even with severe pruning to only
the top 50 candidate parses per string, which re-
sults in low oracle and reranked accuracy for the
Markov order-2 model, the best-performing model
based on structural zeros achieves a relatively high
oracle accuracy, and reaches 88.0 and 87.5 percent
F-measure accuracy on the dev (f24) and eval (f23)
sets respectively. Note that the well-known Char-
5The parsing time with our model for average length sen-
tences (23-25 words) is 0.16 seconds per sentence.
6http://www.cog.brown.edu/?mj/code.
318
No. of Development (f24) Eval (f23)
Technique Cands Time(s) Words/s Oracle F LR LP F LR LP F
Baseline, Markov order-2 1 1302 24.9 71.3 68.8 73.8 71.3 68.9 73.9 71.4
50 1665 19.4 86.2 79.7 83.3 81.5 80.5 84.0 82.2
NT splits: factored=200 1 491 65.9 83.7 83.1 84.3 83.7 82.4 83.4 82.9
non-factored=500 50 628 51.5 93.8 87.4 88.7 88.0 87.1 88.0 87.5
Table 3: Parsing results on the development set (f24) and the evaluation set (f23) for the baseline Markov order-2 model and the
best-performing structural zero model, with 200 factored and 500 non-factored non-terminal splits. 1-best results, plus reranking
using a trained version of an existing reranker with 50 candidates.
niak parser (Charniak, 2000; Charniak and Johnson,
2005) uses a Markov order-3 baseline PCFG in the
initial pass, with a best-first algorithm that is run
past the first parse to populate the chart for use by
the richer model. While we have demonstrated ex-
haustive parsing efficiency, our model could be used
with any of the efficient search best-first approaches
documented in the literature, from those used in the
Charniak parser (Charniak et al, 1998; Blaheta and
Charniak, 1999) to A? parsing (Klein and Manning,
2003a). By using a richer grammar of the sort we
present, far fewer edges would be required in the
chart to include sufficient quality candidates for the
richer model, leading to further downstream savings
of processing time.
5 Conclusion
We described a method for creating concise PCFGs
by detecting structural zeros. The resulting un-
smoothed PCFGs have far higher accuracy than sim-
ple induced PCFGs and yet are very efficient to use.
While we focused on a small number of simple non-
terminal splits that fit the factorization we had se-
lected, the technique presented is applicable to a
wider range of possible non-terminal annotations,
including head or parent annotations. More gener-
ally, the ideas and method for determining structural
zeros (vs. sampling zeros) can be used in other con-
texts for a variety of other learning tasks.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant IIS-
0447214. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the NSF. The first author?s work was
partially funded by the New York State Office of
Science Technology and Academic Research (NYS-
TAR).
References
D. Blaheta and E. Charniak. 1999. Automatic compensation
for parser figure-of-merit flaws. In Proceedings of ACL,
pages 513?518.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proceedings of
ACL, pages 173?188.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-based
best-first chart parsing. In Proceedings of the 6th Workshop
on Very Large Corpora, pages 127?133.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of NAACL, pages 132?139.
M.J. Collins. 1997. Three generative, lexicalised models for
statistical parsing. In Proceedings of ACL, pages 16?23.
T. Dunning. 1994. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguistics,
19(1):61?74.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head automaton grammars. In
Proceedings of ACL, pages 457?464.
J. Eisner. 1997. Bilexical grammars and a cubic-time proba-
bilistic parser. In Proceedings of the International Workshop
on Parsing Technologies, pages 54?65.
K. Hollingshead, S. Fisher, and B. Roark. 2005. Comparing
and combining finite-state and context-free parsers. In Pro-
ceedings of HLT-EMNLP, pages 787?794.
L. Huang and D. Chiang. 2005. Better k-best parsing. In Pro-
ceedings of the 9th International Workshop on Parsing Tech-
nologies (IWPT), pages 53?64.
M. Johnson. 1998. PCFG models of linguistic tree representa-
tions. Computational Linguistics, 24(4):617?636.
T. Kasami. 1965. An efficient recognition and syntax analy-
sis algorithm for context-free languages. Technical Report,
AFCRL-65-758, Air Force Cambridge Research Lab., Bed-
ford, MA.
D. Klein and C. Manning. 2003a. A* parsing: Fast exact
Viterbi parse selection. In Proceedings of HLT-NAACL.
D. Klein and C. Manning. 2003b. Accurate unlexicalized pars-
ing. In Proceedings of ACL.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
D.H. Younger. 1967. Recognition and parsing of context-free
languages in time n3. Information and Control, 10(2):189?
208.
319
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 647?655,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Linear Complexity Context-Free Parsing Pipelines via Chart Constraints
Brian Roark and Kristy Hollingshead
Center for Spoken Language Understanding
Division of Biomedical Computer Science
Oregon Health & Science University
{roark,hollingk}@cslu.ogi.edu
Abstract
In this paper, we extend methods from Roark
and Hollingshead (2008) for reducing the
worst-case complexity of a context-free pars-
ing pipeline via hard constraints derived from
finite-state tagging pre-processing. Methods
from our previous paper achieved quadratic
worst-case complexity. We prove here that al-
ternate methods for choosing constraints can
achieve either linear orO(N log2N) complex-
ity. These worst-case bounds on processing
are demonstrated to be achieved without re-
ducing the parsing accuracy, in fact in some
cases improving the accuracy. The new meth-
ods achieve observed performance compara-
ble to the previously published quadratic com-
plexity method. Finally, we demonstrate im-
proved performance by combining complexity
bounding methods with additional high preci-
sion constraints.
1 Introduction
Finite-state pre-processing for context-free parsing
is very common as a means of reducing the amount
of search required in the later stage. For ex-
ample, the well-known Ratnaparkhi parser (Ratna-
parkhi, 1999) used a finite-state POS-tagger and NP-
chunker to reduce the search space for his Maxi-
mum Entropy parsing model, and achieved linear
observed-time performance. Other recent examples
of the utility of finite-state constraints for parsing
pipelines include Glaysher and Moldovan (2006),
Djordjevic et al (2007), Hollingshead and Roark
(2007), and Roark and Hollingshead (2008). Note
that by making use of constraints derived from pre-
processing, they are no longer performing full exact
inference?these are approximate inference meth-
ods, as are the methods presented in this paper. Most
of these parsing pipeline papers show empirically
that these techniques can improve pipeline efficiency
for well-known parsing tasks. In contrast, in Roark
and Hollingshead (2008), we derived and applied the
finite-state constraints so as to guarantee a reduc-
tion in the worst-case complexity of the context-free
parsing pipeline from O(N3) in the length of the
string N to O(N2) by closing chart cells to entries.
We demonstrated the application of such constraints
to the well-known Charniak parsing pipeline (Char-
niak, 2000), which resulted in no accuracy loss when
the constraints were applied.
While it is important to demonstrate that these
sorts of complexity-reducing chart constraints do not
interfere with the operation of high-accuracy, state-
of-the-art parsing approaches, existing pruning tech-
niques used within such parsers can obscure the im-
pact of these constraints on search. For example, us-
ing the default search parameterization of the Char-
niak parser, the Roark and Hollingshead (2008) re-
sults demonstrated no parser speedup using the tech-
niques, rather an accuracy improvement, which we
attributed to a better use of the amount of search per-
mitted by that default parameterization. We only
demonstrated efficiency improvements by reducing
the amount of search via the Charniak search param-
eterization. There we showed a nice speedup of the
parser versus the default, while maintaining accu-
racy levels. However, internal heuristics of the Char-
niak search, such as attention shifting (Blaheta and
Charniak, 1999; Hall and Johnson, 2004), can make
this accuracy/efficiency tradeoff somewhat difficult
to interpret.
Furthermore, one might ask whetherO(N2) com-
plexity is as good as can be achieved through the
paradigm of using finite-state constraints to close
chart cells. What methods of constraint would be
required to achieve O(N logN) or linear complex-
647
ity? Would such constraints degrade performance,
or can the finite-state models be applied with suffi-
cient precision to allow for such constraints without
significant loss of accuracy?
In this paper, we adopt the same paradigm pur-
sued in Roark and Hollingshead (2008), but apply
it to an exact inference CYK parser (Cocke and
Schwartz, 1970; Younger, 1967; Kasami, 1965). We
demonstrate that imposing constraints sufficient to
achieve quadratic complexity in fact yields observed
linear parsing time, suggesting that tighter complex-
ity bounds are possible. We prove that a differ-
ent method of imposing constraints on words be-
ginning or ending multi-word constituents can give
O(N log2N) or O(N) worst-case complexity, and
we empirically evaluate the impact of such an ap-
proach.
The rest of the paper is structured as follows. We
begin with a summary of the chart cell constraint
techniques from Roark and Hollingshead (2008),
and some initial empirical trials applying these tech-
niques to an exact inference CYK parser. Complex-
ity bounding approaches are contrasted (and com-
bined) with high precision constraint selection meth-
ods from that paper. We then present a new approach
to making use of the same sort of finite-state tag-
ger output to achieve linear or N log2N complexity.
This is followed with an empirical validation of the
new approach.
2 Background: Chart Cell Constraints
The basic algorithm from Roark and Hollingshead
(2008) is as follows. Let B be the set of words in a
string w1 . . . wk that begin a multi-word constituent,
and let E be the set of words in the string that end a
multi-word constituent. For chart parsing with, say,
the CYK algorithm, cells in the chart represent sub-
strings wi . . . wj of the string, and can be indexed
with (i, j), the beginning and ending words of the
substring. If wi 6? B, then we can close any cell
(i, j) where i < j, i.e., no complete constituents
need be stored in that cell. Similarly, if wj 6? E,
then we can close any cell (i, j) where i < j. A dis-
criminatively trained finite-state tagger can be used
to classify words as being in or out of these sets
with relatively high tagging accuracy, around 97%
for both sets (B and E). The output of the tagger is
then used to close cells, thus reducing the work for
the chart parser.
An important caveat must be made about these
closed cells, related to incomplete constituents. For
simplicity of exposition, we will describe incom-
plete constituents in terms of factored categories in
a Chomsky Normal Form grammar, e.g., the new
non-terminal Z:X+W that results when the ternary
rule production Z ? Y X W is factored into
the two binary productions Z ? Y Z:X+W and
Z:X+W ? X W . A factored category such
as Z:X+W should be permitted in cell (i, j) if
wj ? E, even if wi 6? B, because the category could
subsequently combine with an Y category to create
a Z constituent that begins at some word wp ? B.
Hence there are three possible conditions for cell
(i, j) in the chart:
1. wj 6? E: closing the cell affects all con-
stituents, both complete and incomplete
2. wi 6? B and wj ? E: closing the cell affects
only complete constituents
3. wi ? B and wj ? E: cell is not closed, i.e., it
is ?open?
In Roark and Hollingshead (2008), we proved
that, for the CYK algorithm, there is no work neces-
sary for case 1 cells, a constant amount of work for
case 2 cells, and a linear amount of work for case
3 cells. Therefore, if the number of cells allowed
to fall in case 3 is linear, the overall complexity of
search is O(N2).
The amount of work for each case is related
to how the CYK algorithm performs its search.
Each cell in the chart (i, j) represents a substring
wi . . . wj , and building non-terminal categories in
that cell involves combining non-terminal categories
(via rules in the context-free grammar) found in cells
of adjacent substrings wi . . . wm and wm+1 . . . wj .
The length of substrings can be up to order N
(length of the whole string), hence there are O(N)
midpoint words wm in the standard algorithm, and
in the case 3 cells above. This accounts for the lin-
ear amount of work for those cells. Case 2 cells
have constant work because there is only one pos-
sible midpoint, and that is wi, i.e., the first child of
any incomplete constituent placed in a case 2 cell
must be span 1, since wi 6? B. This is a very con-
cise recap of the proof, and we refer the reader to
our previous paper for more details.
648
3 Constraining Exact-Inference CYK
Despite referring to the CYK algorithm in the proof,
in Roark and Hollingshead (2008) we demonstrated
our approach by constraining the Charniak parser
(Charniak, 2000), and achieved an improvement in
the accuracy/efficiency tradeoff curve. However, as
mentioned earlier, the existing complicated system
of search heuristics in the Charniak parser makes in-
terpretation of the results more difficult. What can
be said from the previous results is that constraining
parsers in this way can improve performance of even
the highest accuracy parsers. Yet those results do not
provide much of an indication of how performance
is impacted for general context-free inference.
For this paper, we use an exact inference (exhaus-
tive search) CYK parser, using a simple probabilis-
tic context-free grammar (PCFG) induced from the
Penn WSJ Treebank (Marcus et al, 1993). The
PCFG is transformed to Chomsky Normal Form
through right-factorization, and is smoothed with a
Markov (order-2) transform. Thus a production such
as Z ? Y X W V becomes three rules: (1)
Z ? Y Z:X+W ; (2) Z:X+W ? X Z:W+V ;
and (3) Z:W+V ? W V . Note that only two child
categories are encoded within the new factored cate-
gories, instead of all of the remaining children as in
our previous factorization example. This so-called
?Markov? grammar provides some smoothing of the
PCFG; the resulting grammar is also smoothed us-
ing lower order Markov grammars.
We trained on sections 2-21 of the treebank, and
all results except for the final table are on the devel-
opment section (24). The final table is on the test
section (23). All results report F-measure labeled
bracketing accuracy for all sentences in the section.
To close cells, we use a discriminatively trained
finite-state tagger to tag words as being either in B
or not, and also (in a separate pass) either in E or
not. Note that the reference tags for each word can
be derived directly from the treebank, based on the
spans of constituents beginning (or ending) at each
word. Note also that these reference tags are based
on a non-factored grammar.
For example, consider the chart in Figure 1 for the
five symbol string ?abcde?. Each cell in the chart is
labeled with the substring that the cell spans, along
with the begin and end indices of the substring, e.g.,
(3, 5) spans the third symbol to the fifth symbol:
abcde
(1, 5)
abcd
(1, 4)
bcde
(2, 5)
abc
(1, 3)
bcd
(2, 4)
cde
(3, 5)
ab
(1, 2)
bc
(2, 3)
cd
(3, 4)
de
(4, 5)
a
(1, 1)
b
(2, 2)
c
(3, 3)
d
(4, 4)
e
(5, 5)
Figure 1: Fragment of a chart structure. Each cell is labeled
with the substring spanned by that cell, along with the start and
end word indices. Cell shading reflects b 6? E and d 6? E
constraints: black denotes ?closed? cells; white and gray are
?open?; gray cells have ?closed? children cells, reducing the
number of midpoints requiring processing.
cde. If our tagger output is such that b 6? E and
d 6? E, then four cells will be closed: (1, 2), (1, 4),
(2, 4) and (3, 4). The gray shaded cells in the figure
have some midpoints that require no work, because
they involve closed children cells.
4 Constraint Selection
4.1 High Precision vs Complexity Bounding
The chart constraints that are extracted from the
finite-state tagger come in the form of set exclu-
sions, e.g., d 6? E. Rather than selecting constraints
from the single, best-scoring tag sequence output by
the tagger, we instead rely on the whole distribu-
tion over possible tag strings to select constraints.
We have two separate tagging tasks, each with two
possible tags of each word wi in each string: (1) B
or ?B; and (2) E or ?E, where ?X signifies that
wi 6? X for X ? {B,E}. The tagger (Holling-
shead et al, 2005) uses log linear models trained
with the perceptron algorithm, and derives, via the
forward-backward algorithm, the posterior probabil-
ity of each of the two tags at each word, so that
Pr(B) + Pr(?B) = 1. Then, for every word wi
in the string, the tags B and E are associated with a
posterior probability that gives us a score forwi ? B
and wi ? E. All possible set memberships wi ? X
in the string can be ranked by this score. From this
ranking, a decision boundary can be set, such that
all word/set pairs wi ? B or wj ? E with above-
threshold probability are accepted, and all pairs be-
low threshold are excluded from the set.
The default decision boundary for this tagging
649
task is 0.5 posterior probability (more likely than
not), and tagging performance at that threshold is
good (around 97% accuracy, as mentioned previ-
ously). However, since this is a pre-processing step,
we may want to reduce possible cascading errors by
allowing more words into the sets B and E. In
other words, we may want more precision in our
set exclusion constraints. One method for this is to
count the number c of word/set pairs below poste-
rior probability of 0.5, then set the threshold so that
only kc word/set pairs fall below threshold, where
0 < k ? 1. Note that the closer the parameter k
is to 0, the fewer constraints will be applied to the
chart. We refer to the resulting constraints as ?high
precision?, since the selected constraints (set exclu-
sions) have high precision. This technique was also
used in the previous paper.
We also make use of the ranked list of word/set
pairs to impose quadratic bounds on context-free
parsing. Starting from the top of the list (high-
est posterior probability for set inclusion), word/set
pairs are selected and the number of open cells (case
3 in Section 2) calculated. When the accumulated
number of open cells reaches kN for sentence length
N , the decision threshold is set. In such a way, there
are only a linear number of open, case 3 cells, hence
the parsing has quadratic worst-case complexity.
For both of these methods, the parameter k can
vary, allowing for more or less set inclusion. Fig-
ure 2 shows parse time versus F-measure parse ac-
curacy on the development set for the baseline (un-
constrained) exact-inference CYK parser, and for
various parameterizations of both the high preci-
sion constraints and the quadratic bound constraints.
Note that accuracy actually improves with the im-
position of these constraints. This is not surpris-
ing, since the finite-state tagger deriving the con-
straints made use of lexical information that the sim-
ple PCFG did not, hence there is complementary in-
formation improving the model. The best operating
points?fast parsing and relatively high accuracy?
are achieved with 90% of the high precision con-
straints, and 5N cells left open. These achieve a
roughly 20 times speedup over the baseline uncon-
strained parser and achieve between 1.5 and 3 per-
cent accuracy gains over the baseline.
We can get a better picture of what is going on by
considering the scatter plots in Figure 3, which plot
0 500 1000 1500 2000 2500 3000 350065
70
75
80
Seconds to parse section
F?m
easu
re a
ccur
acy
 
 
Baseline exact inferenceHigh precision constraintsO(N2) complexity bounds
Figure 2: Time to parse (seconds) versus accuracy (F-measure)
for the baseline of exact inference (no constraints) versus
two methods of imposing constraints with varying parameters:
(1) High precision constraints; (2) Sufficient constraints to im-
pose O(N2) complexity (the number of open cells ? kN ).
each sentence according to its length versus the pars-
ing time for that sentence at three operating points:
baseline (unconstrained); high precision at 90%; and
quadratic with 5N open cells. The top plot shows up
to 120 words in the sentence, and up to 5 seconds of
parsing time. The middle graph zooms in to under
1 second and up to 60 words; and the lowest graph
zooms in further to under 0.1 seconds and up to 20
words. It can be seen in each graph that the uncon-
strained CYK parsing quickly leaves the graph via a
steep cubic curve.
Three points can be taken away from these plots.
First, the high precision constraints are better for
the shorter strings than the quadratic bound con-
straints (see bottom plot); yet with the longer strings,
the quadratic constraints better control parsing time
than the high precision constraints (see top plot).
Second, the quadratic bound constraints appear to
actually result in roughly linear parsing time, not
quadratic. Finally, at the ?crossover? point, where
quadratic constraints start out-performing the high
precision constraints (roughly 40-60 words, see mid-
dle plot), there is quite high variance in high preci-
sion constraints versus the quadratic bounds: some
sentences process more quickly than the quadratic
bounds, some quite a bit worse. This illustrates
the difference between the two methods of select-
ing constraints: the high precision constraints can
provide very strong gains, but there is no guarantee
for the worst case. In such a way, the high preci-
sion constraints are similar to other tagging-derived
650
0 20 40 60 80 100 1200
1
2
3
4
5
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsHigh precision constraints (90%)O(N2) parsing (open cells ? 5N)
0 10 20 30 40 50 600
0.2
0.4
0.6
0.8
1
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsHigh precision constraints (90%)O(N2) parsing (open cells ? 5N)
0 5 10 15 200
0.02
0.04
0.06
0.08
0.1
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsHigh precision constraints (90%)O(N2) parsing (open cells ? 5N)
Figure 3: Scatter plots of sentence length versus parsing time
for (1) baseline exact inference (no constraints); (2) high pre-
cision begin- and end-constituent constraints (90% level); and
(3) O(N2) constraints (5N open cells).
constraints like POS-tags or chunks.
4.2 Combining Constraints
Depending on the length of the string, the quadratic
constraints may close more or fewer chart cells
than the high precision constraints?more for long
strings, fewer for short strings. We can achieve
F-measure time
Constraints accuracy (seconds)
None (baseline CYK) 74.1 3646
High Precision (90%) 77.0 181
Quadratic (5N ) 75.7 317
Quad (5N ) + HiPrec (90%) 76.9 166
Table 1: Speed and accuracy of exact-inference CYK parser
on WSJ section 24 under various constraint conditions, includ-
ing combining quadratic bound constraints and high precision
constraints.
worst-case bounds, along with superior typical case
speedups, by combining both methods as follows:
first apply the quadratic bounds; then, if there are
any high precision constraints that remain unap-
plied, add them. Table 1 shows F-measure accuracy
and parsing time (in seconds) for four trials on the
development set: the baseline CYK with no con-
straints; high precision constraints at the 90% level;
quadratic bound constraints at the 5N level; and a
combination of the quadratic bound and high preci-
sion constraints. We can see that, indeed, the com-
bination of the two yield speedups over both inde-
pendently, with no significant drop in accuracy from
the high precision constraints alone. Further results
with worst-case complexity bounds will be com-
bined with high precision constraints in this way.
The observed linear parsing time in Figure 3 with
the quadratic constraints raises the following ques-
tion: can we apply these constraints in a way that
guarantees linear complexity? The answer is yes,
and this is the subject of the next section.
5 Linear andN log2N Complexity Bounds
Given the two setsB and E, recall the three cases of
chart cells (i, j) presented in Section 2: 1) wj 6? E
(cell completely closed); 2) wj ? E and wi 6? B
(cell open only for incomplete constituents); and 3)
wi ? B and wj ? E (cell open for all constituents).
Quadratic worst-case complexity is achieved with
these sets by limiting case 3 to hold for only O(N)
cells?each with linear work?and the remaining
O(N2) cells (cases 1 and 2) have none or constant
work, hence overall quadratic (Roark and Holling-
shead, 2008).
One might ask: why would imposing constraints
to achieve a quadratic bound give us linear observed
parsing time? One possibility is that the linear num-
ber of case 3 cells don?t have a linear amount of
work, but rather a constant bounded amount of work.
651
If there were a constant bounded number of mid-
points, then the amount of work associated with case
3 would be linear. Note that a linear complexity
bound would have to guarantee a linear number of
case 2 cells as well since there is a constant amount
of work associated with case 2 cells.
To provide some intuition as to why the quadratic
bound method resulted in linear observed parsing
time, consider again the chart structure in Figure 1.
The black cells in the chart represent the cells that
have been closed when wj 6? E (case 1 cells). In
our example, w2 6? E caused the cell spanning ab
to be closed, and w4 6? E caused the cells span-
ning abcd, bcd and cd to be closed. Since there is
no work required for these cells, the amount of work
required to parse the sentence is reduced. However,
the quadratic bound does not include any potential
reduced work in the remaining open cells. The gray
cells in the chart are cells with a reduced number of
possible midpoints, as effected by the closed cells
in the chart. For example, categories populating the
cell spanning abc in position (1, 3) can be built in
two ways: either by combining entries in cell (1, 1)
with entries in (2, 3) at midpoint m = 1; or by com-
bining entries in (1, 2) and (3, 3) at midpointm = 2.
However, cell (1, 2) is closed, hence there is only
one midpoint at which (1, 3) can be built (m = 1).
Thus the amount of work to parse the sentence will
be less than the worst-case quadratic bound based on
this processing savings in open cells.
While imposition of the quadratic bound may
have resulted (fortuitously) in constant bounded
work for case 3 cells and a linear number of case
2 cells, there is no guarantee that this will be the
case. One method to guarantee that both conditions
are met is the following: if |E| ? k for some con-
stant k, then both conditions will be met and parsing
complexity will be linear. We prove here that con-
straining E to contain a constant number of words
results in linear complexity.
Lemma 1: If |E| ? k for some k, then the
amount of work for any cell is bounded by ck
for some constant c (grammar constant).
Proof: Recall from Section 2 that for each cell
(i, j), there are j?i midpoints m that require com-
bining entries in cells (i,m) and (m+1, j) to create
entries in cell (i, j). If m > i, then cell (i,m) is
empty unless wm ? E. If cell (i,m) is empty, there
is no work to be done at that midpoint. If |E| ? k,
then there are a maximum of k midpoints for any
cell, hence the amount of work is bounded by ck for
some constant c.2
Lemma 2: If |E| ? k for some k, then the num-
ber of cells (i, j) such that wj ? E is no more
than kN where N is the length of the string.
Proof: For a string of length N , each word wj in
the string has at most N cells such that wj is the
last word in the substring spanned by that cell, since
each such cell must begin with a distinct word wi in
the string where i ? j, of which there are at mostN .
Therefore, if |E| ? k for some k, then the number
of cells (i, j) such that wj ? E would be no more
than kN .2
Theorem: If |E| ? k, then the parsing complex-
ity is O(k2N).
Proof: As stated earlier, each cell (i, j) falls in one
of three cases: 1) wj 6? E; 2) wj ? E and wi 6? B;
and 3) wi ? B and wj ? E. Case 1 cells are com-
pletely closed, there is no work to be done in those
cells. By Lemma 2, there are at maximum kN cells
that fall in either case 2 or case 3. By Lemma 1, the
amount of work for each of these cells is bounded
by ck for some constant c. Therefore, the theorem is
proved.2
If |E| ? k for a constant k, the theorem proves
the complexity will be O(N). If |E| ? k logN ,
then parsing complexity will be O(N log2N). Fig-
ure 4 shows sentence length versus parsing time
under three different conditions1: baseline (uncon-
strained); O(N log2N) at |E| ? 3 logN ; and linear
at |E| ? 16. The bottom graph zooms in to demon-
strate that the O(N log2N) constraints can outper-
form the linear constraints for shorter strings (see
around 20 words). As the length of the string in-
creases, though, the performance lines cross, and the
linear constraints demonstrate higher efficiency for
the longer strings, as expected.
Unlike the method for imposing quadratic
bounds, this method only makes use of set E, not
B. To select the constraints, we rank the word/E
posterior probabilities, and choose the top k (either
constant or scaled with a logN factor); the rest of
the words fall outside of the set. In this approach,
1Selection of these particular operating points for the
N log2N and linear methods is discussed in Section 6.
652
0 20 40 60 80 100 1200
2
4
6
8
10
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsO(Nlog2N) parsing (k=3)O(N) parsing (k=16)
0 10 20 30 400
0.2
0.4
0.6
0.8
1
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsO(Nlog2N) parsing (k=3)O(N) parsing (k=16)
Figure 4: Scatter plots of sentence length versus pars-
ing time for (1) baseline exact inference (no constraints);
(2) O(N log2N) constraints; and (3) O(N) constraints.
every word falls in the B set, hence no constraints
on words beginning multi-word constituents are im-
posed.
6 Results
Figure 5 plots F-measure accuracy versus time to
parse the development set for four methods of
imposing constraints: the previously plotted high
precision and quadratic bound constraints, along
with O(N log2N) and linear bound constraints us-
ing methods described in this paper. All meth-
ods are employed at various parameterizations, from
very lightly constrained to very heavily constrained.
The complexity-bound constraints are not combined
with the high-precision constraints for this plot.
As can be seen from the plot, the linear and
O(N log2N) methods do not, as applied, achieve as
favorable of an accuracy/efficiency tradeoff curve as
the quadratic bound method. This is not surprising,
0 200 400 600 80055
60
65
70
75
80
Seconds to parse section
F?m
easu
re a
ccur
acy
 
 
High precision constraintsO(N2) complexity boundsO(Nlog2N) complexity boundsO(N) complexity bounds
Figure 5: Time to parse (seconds) versus accuracy (F-
measure) for high precision constraints of various thresholds
versus three methods of imposing constraints with complexity
bounds: (1) O(N2) complexity (number of open cells ? kN );
(2) O(N log2N) complexity (|E| ? k logN ); and (3) linear
complexity (|E| ? k).
given that no words are excluded from the set B
for these methods, hence far fewer constraints over-
all are applied with the new method than with the
quadratic bound method.
Of course, the high precision constraints can be
applied together with the complexity bound con-
straints, as described in Section 4.2. For combining
complexity-bound constraints with high-precision
constraints, we first chose operating points for both
the linear and O(N log2N) complexity bound meth-
ods at the points before accuracy begins to de-
grade with over-constraint. For the linear complex-
ity method, the operating point is to constrain the
set size of E to a maximum of 16 members, i.e.,
|E| ? 16. For the N log2N complexity method,
|E| ? 3 logN .
Table 2 presents results for these operating points
used in conjunction with the 90% high precision
constraints. For these methods, this combination
is particularly important, since it includes all of the
high precision constraints from the set B, which are
completely ignored by both of the new methods. We
can see from the results in the table that the com-
bination brings the new constraint methods to very
similar accuracy levels as the quadratic constraints,
yet with the guarantee of scaling linearly to longer
and longer sentences.
The efficiency benefits of combining constraints,
shown in Table 2, are relatively small here because
the dataset contains mostly shorter sentences. Space
653
F-measure time
Constraints accuracy (seconds)
None (baseline CYK) 74.1 3646
High Precision (90%) 77.0 181
Quad (5N ) + HiPrec (90%) 76.9 166
N log2N (3logN ) + HP (90) 76.9 170
Linear (16) + HiPrec (90%) 76.8 167
Table 2: Speed and accuracy of exact-inference CYK parser
on WSJ section 24 under various constraint conditions, includ-
ing combining various complexity bound constraints and high
precision constraints.
limitations prevent us from including scatter plots
similar to those in Figure 3 for the constraint combi-
nation trials, which show that the observed parsing
time of shorter sentences is typically identical under
each constraint set, while the parsing time of longer
sentences tends to differ more under each condition
and exhibit characteristics of the complexity bounds.
Thus by combining high-precision and complexity
constraints, we combine typical-case efficiency ben-
efits with worst-case complexity bounds.
Note that these speedups are achieved with no
additional techniques for speeding up search, i.e.,
modulo the cell closing mechanism, the CYK pars-
ing is exhaustive?it explores all possible category
combinations from the open cells. Techniques such
as coarse-to-fine or A? parsing, the use of an agenda,
or setting of probability thresholds on entries in
cells?these are all orthogonal to the current ap-
proach, and could be applied together with them
to achieve additional speedups. However, none
of these other techniques provide what the current
methods do: a complexity bound that will hold even
in the worst case.
To validate the selected operating points on a dif-
ferent section, Table 3 presents speed and accuracy
results on the test set (WSJ section 23) for the exact-
inference CYK parser.
We also conducted similar preliminary trials for
parsing the Penn Chinese Treebank (Xue et al,
2004), which contains longer sentences and differ-
ent branching characteristics in the induced gram-
mar. Results are similar to those shown here, with
chart constraints providing both efficiency and ac-
curacy gains.
7 Conclusion
We have presented a method for constraining a
context-free parsing pipeline that provably achieves
F-measure time
Constraints accuracy (seconds)
None (baseline CYK) 73.8 5122
High Precision (90%) 76.8 272
Quad (5N ) + HiPrec (90%) 76.8 263
N log2N (3logN ) + HP (90) 76.8 266
Linear (16) + HiPrec (90%) 76.8 264
Table 3: Speed and accuracy of exact-inference CYK parser
on WSJ section 23 under various constraint conditions, includ-
ing combining various complexity bound constraints and high
precision constraints.
linear worst case complexity. Our method achieves
comparable observed performance to the quadratic
complexity method previously published in Roark
and Hollingshead (2008). We were motivated to
pursue this method by the observed linear parsing
time achieved with the quadratic bound constraints,
which suggested that a tighter complexity bound
could be achieved without hurting performance.
We have also shown that combining methods for
achieving complexity bounds?which are of pri-
mary utility for longer strings?with methods for
achieving strong observed typical case speedups can
be profitable, even for shorter strings. The result-
ing combination achieves both typical speedups and
worst-case bounds on processing.
The presented methods may not be the only way
to achieve these bounds using tagger pre-processing
of this sort, though they do have the virtue of
very simple constraint selection. More complicated
methods that track, in fine detail, how many cells
are open versus closed, run the risk of a constraint
selection process that is itself quadratic in the length
of the string, given that there are a quadratic number
of chart cells. Even so, the presented methods criti-
cally control midpoints for all cells only via the set
E (words that can end a multi-word constituent) and
ignoreB. More complicated methods for using both
sets that also achieve linear complexity (perhaps
with a smaller constant), or that achieve O(N logN)
complexity rather than O(N log2N), may exist.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0447214 and DARPA grant #HR0011-08-1-
0016. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
654
References
D. Blaheta and E. Charniak. 1999. Automatic compen-
sation for parser figure-of-merit flaws. In Proceedings
of the 37th annual meeting of the Association for Com-
putational Linguistics (ACL), pages 513?518.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 132?139.
J. Cocke and J.T. Schwartz. 1970. Programming lan-
guages and their compilers: Preliminary notes. Tech-
nical report, Courant Institute of Mathematical Sci-
ences, NYU.
B. Djordjevic, J.R. Curran, and S. Clark. 2007. Im-
proving the efficiency of a wide-coverage CCG parser.
In Proceedings of the 10th International Workshop on
Parsing Technologies (IWPT), pages 39?47.
E. Glaysher and D. Moldovan. 2006. Speeding up full
syntactic parsing by leveraging partial parsing deci-
sions. In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 295?300.
K. Hall and M. Johnson. 2004. Attention shifting for
parsing speech. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 40?46.
K. Hollingshead and B. Roark. 2007. Pipeline iteration.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
952?959.
K. Hollingshead, S. Fisher, and B. Roark. 2005.
Comparing and combining finite-state and context-
free parsers. In Proceedings of the Human Lan-
guage Technology Conference and the Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 787?794.
T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal report, AFCRL-65-758, Air Force Cambridge Re-
search Lab., Bedford, MA.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING), pages 745?
752.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2004. The
Penn Chinese treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1?30.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
655
Proceedings of the 43rd Annual Meeting of the ACL, pages 507?514,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Discriminative Syntactic Language Modeling for Speech Recognition
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Brian Roark
OGI/OHSU
roark@cslu.ogi.edu
Murat Saraclar
Bogazici University
murat.saraclar@boun.edu.tr
Abstract
We describe a method for discriminative
training of a language model that makes
use of syntactic features. We follow
a reranking approach, where a baseline
recogniser is used to produce 1000-best
output for each acoustic input, and a sec-
ond ?reranking? model is then used to
choose an utterance from these 1000-best
lists. The reranking model makes use of
syntactic features together with a parame-
ter estimation method that is based on the
perceptron algorithm. We describe exper-
iments on the Switchboard speech recog-
nition task. The syntactic features provide
an additional 0.3% reduction in test?set
error rate beyond the model of (Roark et
al., 2004a; Roark et al, 2004b) (signifi-
cant at p < 0.001), which makes use of
a discriminatively trained n-gram model,
giving a total reduction of 1.2% over the
baseline Switchboard system.
1 Introduction
The predominant approach within language model-
ing for speech recognition has been to use an n-
gram language model, within the ?source-channel?
or ?noisy-channel? paradigm. The language model
assigns a probability Pl(w) to each string w in the
language; the acoustic model assigns a conditional
probability Pa(a|w) to each pair (a,w) where a is a
sequence of acoustic vectors, and w is a string. For
a given acoustic input a, the highest scoring string
under the model is
w? = arg max
w
(? log Pl(w) + log Pa(a|w)) (1)
where ? > 0 is some value that reflects the rela-
tive importance of the language model; ? is typi-
cally chosen by optimization on held-out data. In
an n-gram language model, a Markov assumption
is made, namely that each word depends only on
the previous (n ? 1) words. The parameters of the
language model are usually estimated from a large
quantity of text data. See (Chen and Goodman,
1998) for an overview of estimation techniques for
n-gram models.
This paper describes a method for incorporating
syntactic features into the language model, using
discriminative parameter estimation techniques. We
build on the work in Roark et al (2004a; 2004b),
which was summarized and extended in Roark et al
(2005). These papers used discriminative methods
for n-gram language models. Our approach reranks
the 1000-best output from the Switchboard recog-
nizer of Ljolje et al (2003).1 Each candidate string
w is parsed using the statistical parser of Collins
(1999) to give a parse tree T (w). Information from
the parse tree is incorporated in the model using
a feature-vector approach: we define ?(a,w) to
be a d-dimensional feature vector which in princi-
ple could track arbitrary features of the string w
together with the acoustic input a. In this paper
we restrict ?(a,w) to only consider the string w
and/or the parse tree T (w) for w. For example,
?(a,w) might track counts of context-free rule pro-
ductions in T (w), or bigram lexical dependencies
within T (w). The optimal string under our new
model is defined as
w? = arg max
w
(? log Pl(w) + ???, ?(a,w)?+
log Pa(a|w)) (2)
where the arg max is taken over all strings in the
1000-best list, and where ?? ? Rd is a parameter
vector specifying the ?weight? for each feature in
? (note that we define ?x, y? to be the inner, or dot
1Note that (Roark et al, 2004a; Roark et al, 2004b) give
results for an n-gram approach on this data which makes use of
both lattices and 1000-best lists. The results on 1000-best lists
were very close to results on lattices for this domain, suggesting
that the 1000-best approximation is a reasonable one.
507
product, between vectors x and y). For this paper,
we train the parameter vector ?? using the perceptron
algorithm (Collins, 2004; Collins, 2002). The per-
ceptron algorithm is a very fast training method, in
practice requiring only a few passes over the train-
ing set, allowing for a detailed comparison of a wide
variety of feature sets.
A number of researchers have described work
that incorporates syntactic language models into a
speech recognizer. These methods have almost ex-
clusively worked within the noisy channel paradigm,
where the syntactic language model has the task
of modeling a distribution over strings in the lan-
guage, in a very similar way to traditional n-gram
language models. The Structured Language Model
(Chelba and Jelinek, 1998; Chelba and Jelinek,
2000; Chelba, 2000; Xu et al, 2002; Xu et al, 2003)
makes use of an incremental shift-reduce parser to
enable the probability of words to be conditioned on
k previous c-commanding lexical heads, rather than
simply on the previous k words. Incremental top-
down and left-corner parsing (Roark, 2001a; Roark,
2001b) and head-driven parsing (Charniak, 2001)
approaches have directly used generative PCFG
models as language models. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al, 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar were used to exploit syn-
tactic dependencies.
Our approach differs from previous work in a cou-
ple of important respects. First, through the feature-
vector representations ?(a,w) we can essentially
incorporate arbitrary sources of information from
the string or parse tree into the model. We would ar-
gue that our method allows considerably more flexi-
bility in terms of the choice of features in the model;
in previous work features were incorporated in the
model through modification of the underlying gen-
erative parsing or tagging model, and modifying a
generative model is a rather indirect way of chang-
ing the features used by a model. In this respect, our
approach is similar to that advocated in Rosenfeld et
al. (2001), which used Maximum Entropy modeling
to allow for the use of shallow syntactic features for
language modeling.
A second contrast between our work and previ-
ous work, including that of Rosenfeld et al (2001),
is in the use of discriminative parameter estimation
techniques. The criterion we use to optimize the pa-
rameter vector ?? is closely related to the end goal
in speech recognition, i.e., word error rate. Previ-
ous work (Roark et al, 2004a; Roark et al, 2004b)
has shown that discriminative methods within an n-
gram approach can lead to significant reductions in
WER, in spite of the features being of the same type
as the original language model. In this paper we ex-
tend this approach, by including syntactic features
that were not in the baseline speech recognizer.
This paper describe experiments using a variety
of syntactic features within this approach. We tested
the model on the Switchboard (SWB) domain, using
the recognizer of Ljolje et al (2003). The discrim-
inative approach for n-gram modeling gave a 0.9%
reduction in WER on this domain; the syntactic fea-
tures we describe give a further 0.3% reduction.
In the remainder of this paper, section 2 describes
previous work, including the parameter estimation
methods we use, and section 3 describes the feature-
vector representations of parse trees that we used in
our experiments. Section 4 describes experiments
using the approach.
2 Background
2.1 Previous Work
Techniques for exploiting stochastic context-free
grammars for language modeling have been ex-
plored for more than a decade. Early approaches
included algorithms for efficiently calculating string
prefix probabilities (Jelinek and Lafferty, 1991; Stol-
cke, 1995) and approaches to exploit such algo-
rithms to produce n-gram models (Stolcke and Se-
gal, 1994; Jurafsky et al, 1995). The work of Chelba
and Jelinek (Chelba and Jelinek, 1998; Chelba and
Jelinek, 2000; Chelba, 2000) involved the use of a
shift-reduce parser trained on Penn treebank style
annotations, that maintains a weighted set of parses
as it traverses the string from left-to-right. Each
word is predicted by each candidate parse in this set
at the point when the word is shifted, and the con-
ditional probability of the word given the previous
words is taken as the weighted sum of the condi-
tional probabilities provided by each parse. In this
approach, the probability of a word is conditioned
by the top two lexical heads on the stack of the par-
508
ticular parse. Enhancements in the feature set and
improved parameter estimation techniques have ex-
tended this approach in recent years (Xu et al, 2002;
Xu et al, 2003).
Roark (2001a; 2001b) pursued a different deriva-
tion strategy from Chelba and Jelinek, and used the
parse probabilities directly to calculate the string
probabilities. This work made use of a left-to-right,
top-down, beam-search parser, which exploits rich
lexico-syntactic features from the left context of
each derivation to condition derivation move proba-
bilities, leading to a very peaked distribution. Rather
than normalizing a prediction of the next word over
the beam of candidates, as in Chelba and Jelinek,
in this approach the string probability is derived by
simply summing the probabilities of all derivations
for that string in the beam.
Other work on syntactic language modeling in-
cludes that of Charniak (2001), which made use of
a non-incremental, head-driven statistical parser to
produce string probabilities. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al, 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar, were used to exploit syn-
tactic dependencies. The processing advantages of
the finite-state encoding of the model has allowed
for the use of probabilities calculated off-line from
this model to be used in the first pass of decoding,
which has provided additional benefits. Finally, Och
et al (2004) use a reranking approach with syntactic
information within a machine translation system.
Rosenfeld et al (2001) investigated the use of
syntactic features in a Maximum Entropy approach.
In their paper, they used a shallow parser to anno-
tate base constituents, and derived features from se-
quences of base constituents. The features were in-
dicator features that were either (1) exact matches
between a set or sequence of base constituents with
those annotated on the hypothesis transcription; or
(2) tri-tag features from the constituent sequence.
The generative model that resulted from their fea-
ture set resulted in only a very small improvement
in either perplexity or word-error-rate.
2.2 Global Linear Models
We follow the framework of Collins (2002; 2004),
recently applied to language modeling in Roark et
al. (2004a; 2004b). The model we propose consists
of the following components:
? GEN(a) is a set of candidate strings for an
acoustic input a. In our case, GEN(a) is a set of
1000-best strings from a first-pass recognizer.
? T (w) is the parse tree for string w.
? ?(a,w) ? Rd is a feature-vector representation
of an acoustic input a together with a string w.
? ?? ? Rd is a parameter vector.
? The output of the recognizer for an input a is
defined as
F (a) = argmax
w?GEN(a)
??(a,w), ??? (3)
In principle, the feature vector ?(a,w) could take
into account any features of the acoustic input a to-
gether with the utterance w. In this paper we make
a couple of restrictions. First, we define the first fea-
ture to be
?1(a,w) = ? log Pl(w) + log Pa(a|w)
where Pl(w) and Pa(a|w) are language and acous-
tic model scores from the baseline speech recog-
nizer. In our experiments we kept ? fixed at the
value used in the baseline recogniser. It can then
be seen that our model is equivalent to the model
in Eq. 2. Second, we restrict the remaining features
?2(a,w) . . . ?d(a,w) to be sensitive to the string
w alone.2 In this sense, the scope of this paper is
limited to the language modeling problem. As one
example, the language modeling features might take
into account n-grams, for example through defini-
tions such as
?2(a,w) = Count of the the in w
Previous work (Roark et al, 2004a; Roark et al,
2004b) considered features of this type. In this pa-
per, we introduce syntactic features, which may be
sensitive to the parse tree for w, for example
?3(a,w) = Count of S ? NP VP in T (w)
where S ? NP VP is a context-free rule produc-
tion. Section 3 describes the full set of features used
in the empirical results presented in this paper.
2Future work may consider features of the acoustic sequence
a together with the string w, allowing the approach to be ap-
plied to acoustic modeling.
509
2.2.1 Parameter Estimation
We now describe how the parameter vector ?? is
estimated from a set of training utterances. The
training set consists of examples (ai,wi) for i =
1 . . .m, where ai is the i?th acoustic input, and wi
is the transcription of this input. We briefly review
the two training algorithms described in Roark et al
(2004b), the perceptron algorithm and global condi-
tional log-linear models (GCLMs).
Figure 1 shows the perceptron algorithm. It is an
online algorithm, which makes several passes over
the training set, updating the parameter vector after
each training example. For a full description of the
algorithm, see Collins (2004; 2002).
A second parameter estimation method, which
was used in (Roark et al, 2004b), is to optimize
the log-likelihood under a log-linear model. Sim-
ilar approaches have been described in Johnson et
al. (1999) and Lafferty et al (2001). The objective
function used in optimizing the parameters is
L(??) =
?
i
log P (si|ai, ??) ? C
?
j
?2j (4)
where P (si|ai, ??) = e
??(ai,si),???
?
w?GEN(ai) e
??(ai,w),??? .
Here, each si is the member of GEN(ai) which
has lowest WER with respect to the target transcrip-
tion wi. The first term in L(??) is the log-likelihood
of the training data under a conditional log-linear
model. The second term is a regularization term
which penalizes large parameter values. C is a con-
stant that dictates the relative weighting given to the
two terms. The optimal parameters are defined as
??? = arg max
??
L(??)
We refer to these models as global conditional log-
linear models (GCLMs).
Each of these algorithms has advantages. A num-
ber of results?e.g., in Sha and Pereira (2003) and
Roark et al (2004b)?suggest that the GCLM ap-
proach leads to slightly higher accuracy than the per-
ceptron training method. However the perceptron
converges very quickly, often in just a few passes
over the training set?in comparison GCLM?s can
take tens or hundreds of gradient calculations before
convergence. In addition, the perceptron can be used
as an effective feature selection technique, in that
Input: A parameter specifying the number of iterations over
the training set, T . A value for the first parameter, ?. A
feature-vector representation ?(a,w) ? Rd. Training exam-
ples (ai,wi) for i = 1 . . . m. An n-best list GEN(ai) for each
training utterance. We take si to be the member of GEN(ai)
which has the lowest WER when compared to wi.
Initialization: Set ?1 = ?, and ?j = 0 for j =
2 . . . d.
Algorithm: For t = 1 . . . T, i = 1 . . . m
?Calculate yi = arg maxw?GEN(ai) ??(ai,w), ???
? For j = 2 . . .m, set ??j = ??j + ?j(ai, si) ?
?j(ai,yi)
Output: Either the final parameters ??, or the averaged pa-
rameters ??avg defined as ??avg =
?
t,i ??t,i/mT where ??t,i is
the parameter vector after training on the i?th training example
on the t?th pass through the training data.
Figure 1: The perceptron training algorithm. Following
Roark et al (2004a), the parameter ?1 is set to be some con-
stant ? that is typically chosen through optimization over the
development set. Recall that ?1 dictates the weight given to the
baseline recognizer score.
at each training example it only increments features
seen on si or yi, effectively ignoring all other fea-
tures seen on members of GEN(ai). For example,
in the experiments in Roark et al (2004a), the per-
ceptron converged in around 3 passes over the train-
ing set, while picking non-zero values for around 1.4
million n-gram features out of a possible 41 million
n-gram features seen in the training set.
For the present paper, to get a sense of the relative
effectiveness of various kinds of syntactic features
that can be derived from the output of a parser, we
are reporting results using just the perceptron algo-
rithm. This has allowed us to explore more of the po-
tential feature space than we would have been able
to do using the more costly GCLM estimation tech-
niques. In future we plan to apply GLCM parameter
estimation methods to the task.
3 Parse Tree Features
We tagged each candidate transcription with (1)
part-of-speech tags, using the tagger documented in
Collins (2002); and (2) a full parse tree, using the
parser documented in Collins (1999). The models
for both of these were trained on the Switchboard
510
SNP
PRP
we
VP
VBD
helped
NP
PRP
her
VP
VB
paint
NP
DT
the
NN
house
Figure 2: An example parse tree
treebank, and applied to candidate transcriptions in
both the training and test sets. Each transcription
received one POS-tag annotation and one parse tree
annotation, from which features were extracted.
Figure 2 shows a Penn Treebank style parse tree
that is of the sort produced by the parser. Given such
a structure, there is a tremendous amount of flexibil-
ity in selecting features. The first approach that we
follow is to map each parse tree to sequences encod-
ing part-of-speech (POS) decisions, and ?shallow?
parsing decisions. Similar representations have been
used by (Rosenfeld et al, 2001; Wang and Harper,
2002). Figure 3 shows the sequential representations
that we used. The first simply makes use of the POS
tags for each word. The latter representations make
use of sequences of non-terminals associated with
lexical items. In 3(b), each word in the string is asso-
ciated with the beginning or continuation of a shal-
low phrase or ?chunk? in the tree. We include any
non-terminals above the level of POS tags as poten-
tial chunks: a new ?chunk? (VP, NP, PP etc.) begins
whenever we see the initial word of the phrase dom-
inated by the non-terminal. In 3(c), we show how
POS tags can be added to these sequences. The final
type of sequence mapping, shown in 3(d), makes a
similar use of chunks, but preserves only the head-
word seen with each chunk.3
From these sequences of categories, various fea-
tures can be extracted, to go along with the n-gram
features used in the baseline. These include n-tag
features, e.g. ti?2ti?1ti (where ti represents the
3It should be noted that for a very small percentage of hy-
potheses, the parser failed to return a full parse tree. At the
end of every shallow tag or category sequence, a special end of
sequence tag/word pair ?</parse> </parse>? was emit-
ted. In contrast, when a parse failed, the sequence consisted of
solely ?<noparse> <noparse>?.
(a)
we/PRP helped/VBD her/PRP paint/VB the/DT
house/NN
(b)
we/NPb helped/VPb her/NPb paint/VPb the/NPb
house/NPc
(c)
we/PRP-NPb helped/VBD-VPb her/PRP-NPb
paint/VB-VPb the/DT-NPb house/NN-NPc
(d)
we/NP helped/VP her/NP paint/VP house/NP
Figure 3: Sequences derived from a parse tree: (a) POS-tag
sequence; (b) Shallow parse tag sequence?the superscripts b
and c refer to the beginning and continuation of a phrase re-
spectively; (c) Shallow parse tag plus POS tag sequence; and
(d) Shallow category with lexical head sequence
tag in position i); and composite tag/word features,
e.g. tiwi (where wi represents the word in posi-
tion i) or, more complicated configurations, such as
ti?2ti?1wi?1tiwi. These features can be extracted
from whatever sort of tag/word sequence we pro-
vide for feature extraction, e.g. POS-tag sequences
or shallow parse tag sequences.
One variant that we performed in feature extrac-
tion had to do with how speech repairs (identified as
EDITED constituents in the Switchboard style parse
trees) and filled pauses or interjections (labeled with
the INTJ label) were dealt with. In the simplest ver-
sion, these are simply treated like other constituents
in the parse tree. However, these can disrupt what
may be termed the intended sequence of syntactic
categories in the utterance, so we also tried skipping
these constituents when mapping from the parse tree
to shallow parse sequences.
The second set of features we employed made
use of the full parse tree when extracting features.
For this paper, we examined several features tem-
plates of this type. First, we considered context-free
rule instances, extracted from each local node in the
tree. Second, we considered features based on lex-
ical heads within the tree. Let us first distinguish
between POS-tags and non-POS non-terminal cate-
gories by calling these latter constituents NTs. For
each constituent NT in the tree, there is an associ-
ated lexical head (HNT) and the POS-tag of that lex-
ical head (HPNT). Two simple features are NT/HNT
and NT/HPNT for every NT constituent in the tree.
511
Feature Examples from figure 2
(P,HCP,Ci,{+,-}{1,2},HP,HCi ) (VP,VB,NP,1,paint,house)
(S,VP,NP,-1,helped,we)
(P,HCP,Ci,{+,-}{1,2},HP,HPCi ) (VP,VB,NP,1,paint,NN)
(S,VP,NP,-1,helped,PRP)
(P,HCP,Ci,{+,-}{1,2},HPP,HCi ) (VP,VB,NP,1,VB,house)
(S,VP,NP,-1,VBD,we)
(P,HCP,Ci,{+,-}{1,2},HPP,HPCi ) (VP,VB,NP,1,VB,NN)
(S,VP,NP,-1,VBD,PRP)
Table 1: Examples of head-to-head features. The examples
are derived from the tree in figure 2.
Using the heads as identified in the parser, example
features from the tree in figure 2 would be S/VBD,
S/helped, NP/NN, and NP/house.
Beyond these constituent/head features, we can
look at the head-to-head dependencies of the sort
used by the parser. Consider each local tree, con-
sisting of a parent node (P), a head child (HCP), and
k non-head children (C1 . . . Ck). For each non-head
child Ci, it is either to the left or right of HCP, and is
either adjacent or non-adjacent to HCP. We denote
these positional features as an integer, positive if to
the right, negative if to the left, 1 if adjacent, and 2 if
non-adjacent. Table 1 shows four head-to-head fea-
tures that can be extracted for each non-head child
Ci. These features include dependencies between
pairs of lexical items, between a single lexical item
and the part-of-speech of another item, and between
pairs of part-of-speech tags in the parse.
4 Experiments
The experimental set-up we use is very similar to
that of Roark et al (2004a; 2004b), and the exten-
sions to that work in Roark et al (2005). We make
use of the Rich Transcription 2002 evaluation test
set (rt02) as our development set, and use the Rich
Transcription 2003 Spring evaluation CTS test set
(rt03) as test set. The rt02 set consists of 6081 sen-
tences (63804 words) and has three subsets: Switch-
board 1, Switchboard 2, Switchboard Cellular. The
rt03 set consists of 9050 sentences (76083 words)
and has two subsets: Switchboard and Fisher.
The training set consists of 297580 transcribed
utterances (3297579 words)4. For each utterance,
4Note that Roark et al (2004a; 2004b; 2005) used 20854 of
these utterances (249774 words) as held out data. In this work
we simply use the rt02 test set as held out and development data.
a weighted word-lattice was produced, represent-
ing alternative transcriptions, from the ASR system.
The baseline ASR system that we are comparing
against then performed a rescoring pass on these first
pass lattices, allowing for better silence modeling,
and replaces the trigram language model score with
a 6-gram model. 1000-best lists were then extracted
from these lattices. For each candidate in the 1000-
best lists, we identified the number of edits (inser-
tions, deletions or substitutions) for that candidate,
relative to the ?target? transcribed utterance. The or-
acle score for the 1000-best lists was 16.7%.
To produce the word-lattices, each training utter-
ance was processed by the baseline ASR system. In
a naive approach, we would simply train the base-
line system (i.e., an acoustic model and language
model) on the entire training set, and then decode
the training utterances with this system to produce
lattices. We would then use these lattices with the
perceptron algorithm. Unfortunately, this approach
is likely to produce a set of training lattices that are
very different from test lattices, in that they will have
very low word-error rates, given that the lattice for
each utterance was produced by a model that was
trained on that utterance. To somewhat control for
this, the training set was partitioned into 28 sets, and
baseline Katz backoff trigram models were built for
each set by including only transcripts from the other
27 sets. Lattices for each utterance were produced
with an acoustic model that had been trained on the
entire training set, but with a language model that
was trained on the 27 data portions that did not in-
clude the current utterance. Since language mod-
els are generally far more prone to overtraining than
standard acoustic models, this goes a long way to-
ward making the training conditions similar to test-
ing conditions. Similar procedures were used to
train the parsing and tagging models for the training
set, since the Switchboard treebank overlaps exten-
sively with the ASR training utterances.
Table 2 presents the word-error rates on rt02 and
rt03 of the baseline ASR system, 1000-best percep-
tron and GCLM results from Roark et al (2005)
under this condition, and our 1000-best perceptron
results. Note that our n-best result, using just n-
gram features, improves upon the perceptron result
of (Roark et al, 2005) by 0.2 percent, putting us
within 0.1 percent of their GCLM result for that
512
WER
Trial rt02 rt03
ASR system output 37.1 36.4
Roark et al (2005) perceptron 36.6 35.7
Roark et al (2005) GCLM 36.3 35.4
n-gram perceptron 36.4 35.5
Table 2: Baseline word-error rates versus Roark et al (2005)
rt02
Trial WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS (1) perceptron 36.1
n-gram + POS (1,2) perceptron 36.1
n-gram + POS (1,3) perceptron 36.1
Table 3: Use of POS-tag sequence derived features
condition. (Note that the perceptron?trained n-gram
features were trigrams (i.e., n = 3).) This is due to
a larger training set being used in our experiments;
we have added data that was used as held-out data in
(Roark et al, 2005) to the training set that we use.
The first additional features that we experimented
with were POS-tag sequence derived features. Let
ti and wi be the POS tag and word at position i,
respectively. We experimented with the following
three feature definitions:
1. (ti?2ti?1ti), (ti?1ti), (ti), (tiwi)
2. (ti?2ti?1wi)
3. (ti?2wi?2ti?1wi?1tiwi), (ti?2ti?1wi?1tiwi),
(ti?1wi?1tiwi), (ti?1tiwi)
Table 3 summarizes the results of these trials on
the held out set. Using the simple features (num-
ber 1 above) yielded an improvement beyond just
n-grams, but additional, more complicated features
failed to yield additional improvements.
Next, we considered features derived from shal-
low parsing sequences. Given the results from the
POS-tag sequence derived features, for any given se-
quence, we simply use n-tag and tag/word features
(number 1 above). The first sequence type from
which we extracted features was the shallow parse
tag sequence (S1), as shown in figure 3(b). Next,
we tried the composite shallow/POS tag sequence
(S2), as in figure 3(c). Finally, we tried extract-
ing features from the shallow constituent sequence
(S3), as shown in figure 3(d). When EDITED and
rt02
Trial WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS perceptron 36.1
n-gram + POS + S1 perceptron 36.1
n-gram + POS + S2 perceptron 36.0
n-gram + POS + S3 perceptron 36.0
n-gram + POS + S3-E perceptron 36.0
n-gram + POS + CF perceptron 36.1
n-gram + POS + H2H perceptron 36.0
Table 4: Use of shallow parse sequence and full parse derived
features
INTJ nodes are ignored, we refer to this condition
as S3-E. For full-parse feature extraction, we tried
context-free rule features (CF) and head-to-head fea-
tures (H2H), of the kind shown in table 1. Table 4
shows the results of these trials on rt02.
Although the single digit precision in the table
does not show it, the H2H trial, using features ex-
tracted from the full parses along with n-grams and
POS-tag sequence features, was the best performing
model on the held out data, so we selected it for ap-
plication to the rt03 test data. This yielded 35.2%
WER, a reduction of 0.3% absolute over what was
achieved with just n-grams, which is significant at
p < 0.001,5 reaching a total reduction of 1.2% over
the baseline recognizer.
5 Conclusion
The results presented in this paper are a first step in
examining the potential utility of syntactic features
for discriminative language modeling for speech
recognition. We tried two possible sets of features
derived from the full annotation, as well as a va-
riety of possible feature sets derived from shallow
parse and POS tag sequences, the best of which
gave a small but significant improvement beyond
what was provided by the n-gram features. Future
work will include a further investigation of parser?
derived features. In addition, we plan to explore the
alternative parameter estimation methods described
in (Roark et al, 2004a; Roark et al, 2004b), which
were shown in this previous work to give further im-
provements over the perceptron.
5We use the Matched Pair Sentence Segment test for WER,
a standard measure of significance, to calculate this p-value.
513
References
Eugene Charniak. 2001. Immediate-head parsing for language
models. In Proc. ACL.
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntac-
tic structure for language modeling. In Proceedings of the
36th Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Computa-
tional Linguistics, pages 225?231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for Nat-
ural Language Modeling. Ph.D. thesis, The Johns Hopkins
University.
Stanley Chen and Joshua Goodman. 1998. An empirical study
of smoothing techniques for language modeling. Technical
Report, TR-10-98, Harvard University.
Michael J. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proc. EMNLP, pages 1?8.
Michael Collins. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-free
methods. In Harry Bunt, John Carroll, and Giorgio Satta,
editors, New Developments in Parsing Technology. Kluwer
Academic Publishers, Dordrecht.
Frederick Jelinek and John Lafferty. 1991. Computation of
the probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315?323.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proc. ACL, pages 535?541.
Daniel Jurafsky, Chuck Wooters, Jonathan Segal, Andreas
Stolcke, Eric Fosler, Gary Tajchman, and Nelson Morgan.
1995. Using a stochastic context-free grammar as a lan-
guage model for speech recognition. In Proceedings of the
IEEE Conference on Acoustics, Speech, and Signal Process-
ing, pages 189?192.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML, pages
282?289, Williams College, Williamstown, MA, USA.
Andrej Ljolje, Enrico Bocchieri, Michael Riley, Brian Roark,
Murat Saraclar, and Izhak Shafran. 2003. The AT&T 1xRT
CTS system. In Rich Transcription Workshop.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In Proceedings of HLT-NAACL
2004.
Brian Roark, Murat Saraclar, and Michael Collins. 2004a. Cor-
rective language modeling for large vocabulary ASR with the
perceptron algorithm. In Proc. ICASSP, pages 749?752.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004b. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In Proc.
ACL.
Brian Roark, Murat Saraclar, and Michael Collins. 2005. Dis-
criminative n-gram language modeling. Computer Speech
and Language. submitted.
Brian Roark. 2001a. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27(2):249?
276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. 2001.
Whole-sentence exponential language models: a vehicle for
linguistic-statistical integration. In Computer Speech and
Language.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of the Human
Language Technology Conference and Meeting of the North
American Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), Edmonton, Canada.
Andreas Stolcke and Jonathan Segal. 1994. Precise n-gram
probabilities from stochastic context-free grammars. In Pro-
ceedings of the 32nd Annual Meeting of the Association for
Computational Linguistics, pages 74?79.
Andreas Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities. Com-
putational Linguistics, 21(2):165?202.
Wen Wang and Mary P. Harper. 2002. The superARV language
model: Investigating the effectiveness of tightly integrating
multiple knowledge sources. In Proc. EMNLP, pages 238?
247.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004. The
use of a linguistically motivated language model in conver-
sational speech recognition. In Proc. ICASSP.
Wen Wang. 2003. Statistical parsing and language model-
ing based on constraint dependency grammar. Ph.D. thesis,
Purdue University.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A
study on richer syntactic dependencies for structured lan-
guage modeling. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics, pages
191?198.
Peng Xu, Ahmad Emami, and Frederick Jelinek. 2003. Train-
ing connectionist models for the structured language model.
In Proc. EMNLP, pages 160?167.
514
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 488?495,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The utility of parse-derived features for automatic discourse segmentation
Seeger Fisher and Brian Roark
Center for Spoken Language Understanding, OGI School of Science & Engineering
Oregon Health & Science University, Beaverton, Oregon, 97006 USA
{fishers,roark}@cslu.ogi.edu
Abstract
We investigate different feature sets for
performing automatic sentence-level dis-
course segmentation within a general ma-
chine learning approach, including features
derived from either finite-state or context-
free annotations. We achieve the best re-
ported performance on this task, and demon-
strate that our SPADE-inspired context-free
features are critical to achieving this level of
accuracy. This counters recent results sug-
gesting that purely finite-state approaches
can perform competitively.
1 Introduction
Discourse structure annotations have been demon-
strated to be of high utility for a number of NLP
applications, including automatic text summariza-
tion (Marcu, 1998; Marcu, 1999; Cristea et al,
2005), sentence compression (Sporleder and Lap-
ata, 2005), natural language generation (Prasad et
al., 2005) and question answering (Verberne et al,
2006). These annotations include sentence segmen-
tation into discourse units along with the linking
of discourse units, both within and across sentence
boundaries, into a labeled hierarchical structure. For
example, the tree in Figure 1 shows a sentence-level
discourse tree for the string ?Prices have dropped but
remain quite high, according to CEO Smith,? which
has three discourse segments, each labeled with ei-
ther ?Nucleus? or ?Satellite? depending on how cen-
tral the segment is to the coherence of the text.
There are a number of corpora annotated with
discourse structure, including the well-known RST
Treebank (Carlson et al, 2002); the Discourse
GraphBank (Wolf and Gibson, 2005); and the Penn
Discourse Treebank (Miltsakaki et al, 2004). While
the annotation approaches differ across these cor-
pora, the requirement of sentence segmentation into
Root





H
H
H
H
H
Nucleus


HH
HH
Nucleus


PP
PP
Prices have dropped
Satellite


PP
PP
but remain quite high
Satellite



PP
PP
P
according to CEO Smith
Figure 1: Example Nucleus/Satellite labeled sentence-level
discourse tree.
sub-sentential discourse units is shared across all ap-
proaches. These resources have facilitated research
into stochastic models and algorithms for automatic
discourse structure annotation in recent years.
Using the RST Treebank as training and evalua-
tion data, Soricut and Marcu (2003) demonstrated
that their automatic sentence-level discourse pars-
ing system could achieve near-human levels of ac-
curacy, if it was provided with manual segmenta-
tions and manual parse trees. Manual segmenta-
tion was primarily responsible for this performance
boost over their fully automatic system, thus mak-
ing the case that automatic discourse segmentation is
the primary impediment to high accuracy automatic
sentence-level discourse structure annotation. Their
models and algorithm ? subsequently packaged to-
gether into the publicly available SPADE discourse
parser1 ? make use of the output of the Charniak
(2000) parser to derive syntactic indicator features
for segmentation and discourse parsing.
Sporleder and Lapata (2005) also used the RST
Treebank as training data for data-driven discourse
parsing algorithms, though their focus, in contrast
to Soricut and Marcu (2003), was to avoid context-
free parsing and rely exclusively on features in their
model that could be derived via finite-state chunkers
and taggers. The annotations that they derive are dis-
1
http://www.isi.edu/publications/licensed-sw/spade/
488
course ?chunks?, i.e., sentence-level segmentation
and non-hierarchical nucleus/span labeling of seg-
ments. They demonstrate that their models achieve
comparable results to SPADE without the use of any
context-free features. Once again, segmentation is
the part of the process where the automatic algo-
rithms most seriously underperform.
In this paper we take up the question posed by
the results of Sporleder and Lapata (2005): how
much, if any, accuracy reduction should we expect
if we choose to use only finite-state derived fea-
tures, rather than those derived from full context-
free parses? If little accuracy is lost, as their re-
sults suggest, then it would make sense to avoid rel-
atively expensive context-free parsing, particularly
if the amount of text to be processed is large or if
there are real-time processing constraints on the sys-
tem. If, however, the accuracy loss is substantial,
one might choose to avoid context-free parsing only
in the most time-constrained scenarios.
While Sporleder and Lapata (2005) demonstrated
that their finite-state system could perform as well as
the SPADE system, which uses context-free parse
trees, this does not directly answer the question of
the utility of context-free derived features for this
task. SPADE makes use of a particular kind of fea-
ture from the parse trees, and does not train a gen-
eral classifier making use of other features beyond
the parse-derived indicator features. As we shall
show, its performance is not the highest that can be
achieved via context-free parser derived features.
In this paper, we train a classifier using a gen-
eral machine learning approach and a range of finite-
state and context-free derived features. We investi-
gate the impact on discourse segmentation perfor-
mance when one feature set is used versus another,
in such a way establishing the utility of features de-
rived from context-free parses. In the course of so
doing, we achieve the best reported performance on
this task, an absolute F-score improvement of 5.0%
over SPADE, which represents a more than 34% rel-
ative error rate reduction.
By focusing on segmentation, we provide an ap-
proach that is generally applicable to all of the
various annotation approaches, given the similari-
ties between the various sentence-level segmenta-
tion guidelines. Given that segmentation has been
shown to be a primary impediment to high accu-
racy sentence-level discourse structure annotation,
this represents a large step forward in our ability to
automatically parse the discourse structure of text,
whatever annotation approach we choose.
2 Methods
2.1 Data
For our experiments we use the Rhetorical Structure
Theory Discourse Treebank (Carlson et al, 2002),
which we will denote RST-DT, a corpus annotated
with discourse segmentation and relations according
to Rhetorical Structure Theory (Mann and Thomp-
son, 1988). The RST-DT consists of 385 docu-
ments from the Wall Street Journal, about 176,000
words, which overlaps with the Penn Wall St. Jour-
nal (WSJ) Treebank (Marcus et al, 1993).
The segmentation of sentences in the RST-DT
is into clause-like units, known as elementary dis-
course units, or edus. We will use the two terms
?edu? and ?segment? interchangeably throughout the
rest of the paper. Human agreement for this segmen-
tation task is quite high, with agreement between
two annotators at an F-score of 98.3 for unlabeled
segmentation (Soricut and Marcu, 2003).
The RST-DT corpus annotates edu breaks, which
typically include sentence boundaries, but sentence
boundaries are not explicitly annotated in the corpus.
To perform sentence-level processing and evalua-
tion, we aligned the RST-DT documents to the same
documents in the Penn WSJ Treebank, and used the
sentence boundaries from that corpus.2 An addi-
tional benefit of this alignment is that the Penn WSJ
Treebank tokenization is then available for parsing
purposes. Simple minimum edit distance alignment
effectively allowed for differences in punctuation
representation (e.g., double quotes) and tokenization
when deriving the optimal alignment.
The RST-DT corpus is partitioned into a train-
ing set of 347 documents and a test set of 38 doc-
uments. This test set consists of 991 sentences with
2,346 segments. For training purposes, we created
a held-out development set by selecting every tenth
sentence of the training set. This development set
was used for feature development and for selecting
the number of iterations used when training models.
2.2 Evaluation
Previous research into RST-DT segmentation and
parsing has focused on subsets of the 991 sentence
test set during evaluation. Soricut and Marcu (2003)
2A small number of document final parentheticals are in the
RST-DT and not in the Penn WSJ Treebank, which our align-
ment approach takes into account.
489
omitted sentences that were not exactly spanned by
a subtree of the treebank, so that they could fo-
cus on sentence-level discourse parsing. By our
count, this eliminates 40 of the 991 sentences in the
test set from consideration. Sporleder and Lapata
(2005) went further and established a smaller sub-
set of 608 sentences, which omitted sentences with
only one segment, i.e., sentences which themselves
are atomic edus.
Since the primary focus of this paper is on seg-
mentation, there is no strong reason to omit any sen-
tences from the test set, hence our results will eval-
uate on all 991 test sentences, with two exceptions.
First, in Section 2.3, we compare SPADE results un-
der our configuration with results from Sporleder
and Lapata (2005) in order to establish compara-
bility, and this is done on their 608 sentence sub-
set. Second, in Section 3.2, we investigate feed-
ing our segmentation into the SPADE system, in or-
der to evaluate the impact of segmentation improve-
ments on their sentence-level discourse parsing per-
formance. For those trials, the 951 sentence subset
from Soricut and Marcu (2003) is used. All other
trials use the full 991 sentence test set.
Segmentation evaluation is done with precision,
recall and F1-score of segmentation boundaries.
Given a word string w1 . . . wk, we can index word
boundaries from 0 to k, so that each word wi falls
between boundaries i?1 and i. For sentence-based
segmentation, indices 0 and k, representing the be-
ginning and end of the string, are known to be seg-
ment boundaries. Hence Soricut and Marcu (2003)
evaluate with respect to sentence internal segmenta-
tion boundaries, i.e., with indices j such that 0<j<k
for a sentence of length k. Let g be the number
of sentence-internal segmentation boundaries in the
gold standard, t the number of sentence-internal seg-
mentation boundaries in the system output, and m
the number of correct sentence-internal segmenta-
tion boundaries in the system output. Then
P = mt R =
m
g and F1 =
2PR
P+R =
2m
g+t
In Sporleder and Lapata (2005), they were pri-
marily interested in labeled segmentation, where the
segment initial boundary was labeled with the seg-
ment type. In such a scenario, the boundary at in-
dex 0 is no longer known, hence their evaluation in-
cluded those boundaries, even when reporting un-
labeled results. Thus, in section 2.3, for compar-
ison with reported results in Sporleder and Lapata
(2005), our F1-score is defined accordingly, i.e., seg-
Segmentation system F1
Sporleder and Lapata best (reported) 88.40
SPADE
Sporleder and Lapata configuration (reported): 87.06
current configuration: 91.04
Table 1: Segmentation results on the Sporleder and Lapata
(2005) data set, with accuracy defined to include sentence initial
segmentation boundaries.
mentation boundaries j such that 0 ? j < k.
In addition, we will report unlabeled bracketing
precision, recall and F1-score, as defined in the
PARSEVAL metrics (Black et al, 1991) and eval-
uated via the widely used evalb package. We also
use evalb when reporting labeled and unlabeled dis-
course parsing results in Section 3.2.
2.3 Baseline SPADE setup
The publicly available SPADE package, which en-
codes the approach in Soricut and Marcu (2003),
is taken as the baseline for this paper. We made
several modifications to the script from the default,
which account for better baseline performance than
is achieved with the default configuration. First, we
modified the script to take given parse trees as input,
rather than running the Charniak parser itself. This
allowed us to make two modifications that improved
performance: turning off tokenization in the Char-
niak parser, and reranking. The default script that
comes with SPADE does not turn off tokenization
inside of the parser, which leads to degraded perfor-
mance when the input has already been tokenized in
the Penn Treebank style. Secondly, Charniak and
Johnson (2005) showed how reranking of the 50-
best output of the Charniak (2000) parser gives sub-
stantial improvements in parsing accuracy. These
two modifications to the Charniak parsing output
used by the SPADE system lead to improvements
in its performance compared to previously reported
results.
Table 1 compares segmentation results of three
systems on the Sporleder and Lapata (2005) 608
sentence subset of the evaluation data: (1) their best
reported system; (2) the SPADE system results re-
ported in that paper; and (3) the SPADE system re-
sults with our current configuration. The evaluation
uses the unlabeled F1 measure as defined in that pa-
per, which counts sentence initial boundaries in the
scoring, as discussed in the previous section. As can
be seen from these results, our improved configu-
ration of SPADE gives us large improvements over
the previously reported SPADE performance on this
subset. As a result, we feel that we can use SPADE
490
as a very strong baseline for evaluation on the entire
test set.
Additionally, we modified the SPADE script to al-
low us to provide our segmentations to the full dis-
course parsing that it performs, in order to evalu-
ate the improvements to discourse parsing yielded
by any improvements to segmentation.
2.4 Segmentation classifier
For this paper, we trained a binary classifier, which
was applied independently at each word wi in the
string w1 . . . wk, to decide whether that word is the
last in a segment. Note that wk is the last word in
the string, and is hence ignored. We used a log-
linear model with no Markov dependency between
adjacent tags,3 and trained the parameters of the
model with the perceptron algorithm, with averag-
ing to control for over-training (Collins, 2002).
Let C={E, I} be the set of classes: seg-
mentation boundary (E) or non-boundary (I). Let
f(c, i, w1 . . . wk) be a function that takes as in-
put a class value c, a word index i and the word
string w1 . . . wk and returns a d-dimensional vector
of feature values for that word index in that string
with that class. For example, one feature might be
(c = E,wi = the), which returns the value 1 when
c = E and the current word is ?the?, and returns
0 otherwise. Given a d-dimensional parameter vec-
tor ?, the output of the classifier is that class which
maximizes the dot product between the feature and
parameter vectors:
c?(i, w1 . . . wk) = argmax
c?C
? ? f(c, i, w1 . . . wk) (1)
In training, the weights in ? are initialized to 0.
For m epochs (passes over the training data), for
each word in the training data (except sentence final
words), the model is updated. Let i be the current
word position in string w1 . . . wk and suppose that
there have been j?1 previous updates to the model
parameters. Let c?i be the true class label, and let c?i
be shorthand for c?(i, w1 . . . wk) in equation 1. Then
the parameter vector ?j at step j is updated as fol-
lows:
?j = ?j?1 ? f(c?, i, w1 . . . wk) + f(c?, i, w1 . . . wk) (2)
As stated in Section 2.1, we reserved every tenth
sentence as held-out data. After each pass over the
training data, we evaluated the system performance
3Because of the sparsity of boundary tags, Markov depen-
dencies between tags buy no additional system accuracy.
on this held-out data, and chose the model that op-
timized accuracy on that set. The averaged percep-
tron was used on held-out and evaluation sets. See
Collins (2002) for more details on this approach.
2.5 Features
To tease apart the utility of finite-state derived fea-
tures and context-free derived features, we consider
three feature sets: (1) basic finite-state features; (2)
context-free features; and (3) finite-state approxima-
tion to context-free features. Note that every feature
must include exactly one class label c in order to
discriminate between classes in equation 1. Hence
when presenting features, it can be assumed that the
class label is part of the feature, even if it is not ex-
plicitly mentioned.
The three feature sets are not completely disjoint.
We include simple position-based features in every
system, defined as follows. Because edus are typi-
cally multi-word strings, it is less likely for a word
near the beginning or end of a sentence to be at an
edu boundary. Thus it is reasonable to expect the
position within a sentence of a token to be a helpful
feature. We created 101 indicator features, repre-
senting percentages from 0 to 100. For a string of
length k, at position i, we round i/k to two decimal
places and provide a value of 1 for the corresponding
quantized position feature and 0 for the other posi-
tion features.
2.5.1 Basic finite-state features
Our baseline finite-state feature set includes simple
tagger derived features, as well as features based on
position in the string and n-grams4. We annotate
tag sequences onto the word sequence via a compet-
itive discriminatively trained tagger (Hollingshead
et al, 2005), trained for each of two kinds of tag
sequences: part-of-speech (POS) tags and shallow
parse tags. The shallow parse tags define non-
hierarchical base constituents (?chunks?), as defined
for the CoNLL-2000 shared task (Tjong Kim Sang
and Buchholz, 2000). These can either be used
as tag or chunk sequences. For example, the tree
in Figure 2 represents a shallow (non-hierarchical)
parse tree, with four base constituents. Each base
constituent X begins with a word labeled with BX ,
which signifies that this word begins the constituent.
All other words within a constituent X are labeled
4We tried using a list of 311 cue phrases from Knott (1996)
to define features, but did not derive any system improvement
through this list, presumably because our simple n-gram fea-
tures already capture many such lexical cues.
491
ROOT





 
 
 
@
@
@
PP
PP
PP
PP
P
NP
 HH
BNP
DT
the
INP
NN
broker
VP
 HH
BVP
MD
will
IVP
VBD
sell
NP
 HH
BNP
DT
the
INP
NNS
stocks
NP
BNP
NN
tomorrow
Figure 2: Tree representation of shallow parses, with B(egin)
and I(nside) tags
IX , and words outside of any base constituent are la-
beled O. In such a way, each word is labeled with
both a POS-tag and a B/I/O tag.
For our three sequences (lexical, POS-tag and
shallow tag), we define n-gram features surround-
ing the potential discourse boundary. If the current
word is wi, the hypothesized boundary will occur
between wi and wi+1. For this boundary position,
the 6-gram including the three words before and the
three words after the boundary is included as a fea-
ture; additionally, all n-grams for n < 6 such that
either wi or wi+1 (or both) is in the n-gram are in-
cluded as features. In other words, all n-grams in a
six word window of boundary position i are included
as features, except those that include neither wi nor
wi+1 in the n-gram. The identical feature templates
are used with POS-tag and shallow tag sequences as
well, to define tag n-gram features.
This feature set is very close to that used in
Sporleder and Lapata (2005), but not identical.
Their n-gram feature definitions were different
(though similar), and they made use of cue phrases
from Knott (1996). In addition, they used a rule-
based clauser that we did not. Despite such differ-
ences, this feature set is quite close to what is de-
scribed in that paper.
2.5.2 Context-free features
To describe our context-free features, we first
present how SPADE made use of context-free parse
trees within their segmentation algorithm, since this
forms the basis of our features. The SPADE features
are based on productions extracted from full syntac-
tic parses of the given sentence. The primary feature
for a discourse boundary after word wi is based on
the lowest constituent in the tree that spans words
wm . . . wn such that m ? i < n. For example, in
the parse tree schematic in Figure 3, the constituent
labeled with A is the lowest constituent in the tree
whose span crosses the potential discourse bound-
ary after wi. The primary feature is the production
A




 
 
 
@
@
@
PP
PP
PP
PP
B1 . . . Bj?1


H
H
C1 . . . Cn
H
. . . Ti
wi
Bj . . . Bm
Figure 3: Parse tree schematic for describing context-free seg-
mentation features
that expands this constituent in the tree, with the
proposed segmentation boundary marked, which in
this case is: A ? B1 . . . Bj?1||Bj . . . Bm, where
|| denotes the segmentation boundary. In SPADE,
the production is lexicalized by the head words of
each constituent, which are determined using stan-
dard head-percolation techniques. This feature is
used to predict a boundary as follows: if the relative
frequency estimate of a boundary given the produc-
tion feature in the corpus is greater than 0.5, then a
boundary is predicted; otherwise not. If the produc-
tion has not been observed frequently enough, the
lexicalization is removed and the relative frequency
of a boundary given the unlexicalized production is
used for prediction. If the observations of the unlex-
icalized production are also too sparse, then only the
children adjacent to the boundary are maintained in
the feature, e.g., A ? ?Bj?1||Bj? where ? repre-
sents zero or more categories. Further smoothing is
used when even this is unobserved.
We use these features as the starting point for our
context-free feature set: the lexicalized production
A ? B1 . . . Bj?1||Bj . . . Bm, as defined above for
SPADE, is a feature in our model, as is the unlexi-
calized version of the production. As with the other
features that we have described, this feature is used
as an indicator feature in the classifier applied at the
word wi preceding the hypothesized boundary. In
addition to these full production features, we use the
production with only children adjacent to the bound-
ary, denoted by A ? ?Bj?1||Bj?. This production
is used in four ways: fully lexicalized; unlexicalized;
only category Bj?1 lexicalized; and only category
Bj lexicalized. We also use A ? ?Bj?2Bj?1||?
and A ? ?||BjBj+1? features, both unlexicalized
and with the boundary-adjacent category lexical-
ized. If there is no category Bj?2 or Bj+1, they are
replaced with ?N/A?.
In addition to these features, we fire the same fea-
tures for all productions on the path from A down
492
Segment Boundary accuracy Bracketing accuracy
Segmentation system Recall Precision F1 Recall Precision F1
SPADE 85.4 85.5 85.5 77.7 77.9 77.8
Classifier: Basic finite-state 81.5 83.3 82.4 73.6 74.5 74.0
Classifier: Full finite-state 84.1 87.9 86.0 78.0 80.0 79.0
Classifier: Context-free 84.7 91.1 87.8 80.3 83.7 82.0
Classifier: All features 89.7 91.3 90.5 84.9 85.8 85.3
Table 2: Segmentation results on all 991 sentences in the RST-DT test set. Segment boundary accuracy is for sentence internal
boundaries only, following Soricut and Marcu (2003). Bracketing accuracy is for unlabeled flat bracketing of the same segments.
While boundary accuracy correctly depicts segmentation results, the harsher flat bracketing metric better predicts discourse parsing
performance.
to the word wi. For these productions, the seg-
mentation boundary || will occur after all children
in the production, e.g., Bj?1 ? C1 . . . Cn||, which
is then used in both lexicalized and unlexicalized
forms. For the feature with only categories adja-
cent to the boundary, we again use ?N/A? to denote
the fact that no category occurs to the right of the
boundary: Bj?1 ? ?Cn||N/A. Once again, these
are lexicalized as described above.
2.5.3 Finite-state approximation features
An approximation to our context-free features can
be made by using the shallow parse tree, as shown
in Figure 2, in lieu of the full hierarchical parse
tree. For example, if the current word was ?sell?
in the tree in Figure 2, the primary feature would
be ROOT ? NP VP||NP NP, and it would have an
unlexicalized version and three lexicalized versions:
the category immediately prior to the boundary lex-
icalized; the category immediately after the bound-
ary lexicalized; and both lexicalized. For lexicaliza-
tion, we choose the final word in the constituent as
the lexical head for the constituent. This is a rea-
sonable first approximation, because such typically
left-headed categories as PP and VP lose their argu-
ments in the shallow parse.
As a practical matter, we limit the number of cat-
egories in the flat production to 8 to the left and 8 to
the right of the boundary. In a manner similar to the
n-gram features that we defined in Section 2.5.1, we
allow all combinations with less than 8 contiguous
categories on each side, provided that at least one
of the adjacent categories is included in the feature.
Each feature has an unlexicalized and three lexical-
ized versions, as described above.
3 Experiments
We performed a number of experiments to deter-
mine the relative utility of features derived from
full context-free syntactic parses and those derived
solely from shallow finite-state tagging. Our pri-
mary concern is with intra-sentential discourse seg-
mentation, but we are also interested in how much
the improved segmentation helps discourse parsing.
The syntactic parser we use for all context-free
syntactic parses used in either SPADE or our clas-
sifier is the Charniak parser with reranking, as de-
scribed in Charniak and Johnson (2005). The Char-
niak parser and reranker were trained on the sections
of the Penn Treebank not included in the RST-DT
test set.
All statistical significance testing is done via the
stratified shuffling test (Yeh, 2000).
3.1 Segmentation
Table 2 presents segmentation results for SPADE
and four versions of our classifier. The ?Basic finite-
state? system uses only finite-state sequence fea-
tures as defined in Section 2.5.1, while the ?Full
finite-state? also includes the finite-state approxima-
tion features from Section 2.5.3. The ?Context-free?
system uses the SPADE-inspired features detailed in
Section 2.5.2, but none of the features from Sections
2.5.1 or 2.5.3. Finally, the ?All features? section in-
cludes features from all three sections.5
Note that the full finite-state system is consider-
ably better than the basic finite-state system, demon-
strating the utility of these approximations of the
SPADE-like context-free features. The performance
of the resulting ?Full? finite-state system is not sta-
tistically significantly different from that of SPADE
(p=0.193), despite no reliance on features derived
from context-free parses.
The context-free features, however, even without
any of the finite-state sequence features (even lex-
ical n-grams) outperforms the best finite-state sys-
tem by almost two percent absolute, and the sys-
tem with all features improves on the best finite-state
system by over four percent absolute. The system
5In the ?All features? condition, the finite-state approxima-
tion features defined in Section 2.5.3 only include a maximum
of 3 children to the left and right of the boundary, versus a max-
imum of 8 for the ?Full finite-state? system. This was found to
be optimal on the development set.
493
Segmentation Unlabeled Nuc/Sat
SPADE 76.9 70.2
Classifier: Full finite state 78.1 71.1
Classifier: All features 83.5 76.1
Table 3: Discourse parsing results on the 951 sentence Sori-
cut and Marcu (2003) evaluation set, using SPADE for parsing,
and various methods for segmentation. Scores are unlabeled
and labeled (Nucleus/Satellite) bracketing accuracy (F1). The
first line shows SPADE performing both segmentation and dis-
course parsing. The other two lines show SPADE performing
discourse parsing with segmentations produced by our classi-
fier using different combinations of features.
with all features is statistically significantly better
than both SPADE and the ?Full finite-state? classi-
fier system, at p < 0.001. This large improvement
demonstrates that the context-free features can pro-
vide a very large system improvement.
3.2 Discourse parsing
It has been shown that accurate discourse segmen-
tation within a sentence greatly improves the over-
all parsing accuracy to near human levels (Sori-
cut and Marcu, 2003). Given our improved seg-
mentation results presented in the previous section,
improvements would be expected in full sentence-
level discourse parsing. To achieve this, we modi-
fied the SPADE script to accept our segmentations
when building the fully hierarchical discourse tree.
The results for three systems are presented in Ta-
ble 3: SPADE, our ?Full finite-state? system, and
our system with all features. Results for unlabeled
bracketing are presented, along with results for la-
beled bracketing, where the label is either Nucleus
or Satellite, depending upon whether or not the node
is more central (Nucleus) to the coherence of the text
than its sibling(s) (Satellite). This label set has been
shown to be of particular utility for indicating which
segments are more important to include in an auto-
matically created summary or compressed sentence
(Sporleder and Lapata, 2005; Marcu, 1998; Marcu,
1999; Cristea et al, 2005).
Once again, the finite-state system does not
perform statistically significantly different from
SPADE on either labeled or unlabeled discourse
parsing. Using all features, however, results in
greater than 5% absolute accuracy improvement
over both of these systems, which is significant, in
all cases, at p < 0.001.
4 Discussion and future directions
Our results show that context-free parse derived fea-
tures are critical for achieving the highest level of
accuracy in sentence-level discourse segmentation.
Given that edus are by definition clause-like units,
it is not surprising that accurate full syntactic parse
trees provide highly relevant information unavail-
able from finite-state approaches. Adding context-
free features to our best finite-state feature model
reduces error in segmentation by 32.1%, an in-
crease in absolute F-score of 4.5%. These increases
are against a finite-state segmentation model that is
powerful enough to be statistically indistinguishable
from SPADE.
Our experiments also confirm that increased seg-
mentation accuracy yields significantly better dis-
course parsing accuracy, as previously shown to be
the case when providing reference segmentations to
a parser (Soricut and Marcu, 2003). The segmen-
tation reduction in error of 34.5% propagates to a
28.6% reduction in error for unlabeled discourse
parse trees, and a 19.8% reduction in error for trees
labeled with Nucleus and Satellite.
We have several key directions in which to con-
tinue this work. First, given that a general ma-
chine learning approach allowed us to improve upon
SPADE?s segmentation performance, we also be-
lieve that it will prove useful for improving full
discourse parsing, both at the sentence level and
beyond. For efficient inter-sentential discourse
parsing, we see the need for an additional level
of segmentation at the paragraph level. Whereas
most sentences correspond to a well-formed subtree,
Sporleder and Lascarides (2004) report that over
20% of the paragraph boundaries in the RST-DT do
not correspond to a well-formed subtree in the hu-
man annotated discourse parse for that document.
Therefore, to perform accurate and efficient pars-
ing of the RST-DT at the paragraph level, the text
should be segmented into paragraph-like segments
that conform to the human-annotated subtree bound-
aries, just as sentences are segmented into edus.
We also intend to begin work on the other dis-
course annotated corpora. While most work on tex-
tual discourse parsing has made use of the RST-DT
corpus, the Discourse GraphBank corpus provides a
competing annotation that is not constrained to tree
structures (Wolf and Gibson, 2005). Once accurate
levels of segmentation and parsing for both corpora
are attained, it will be possible to perform extrinsic
evaluations to determine their relative utility for dif-
ferent NLP tasks. Recent work has shown promis-
ing preliminary results for recognizing and labeling
relations of GraphBank structures (Wellner et al,
2006), in the case that the algorithm is provided with
494
manually segmented sentences. Sentence-level seg-
mentation in the GraphBank is very similar to that in
the RST-DT, so our segmentation approach should
work well for Discourse GraphBank style parsing.
The Penn Discourse Treebank (Miltsakaki et al,
2004), or PDTB, uses a relatively flat annotation of
discourse structure, in contrast to the hierarchical
structures found in the other two corpora. It contains
annotations for discourse connectives and their argu-
ments, where an argument can be as small as a nom-
inalization or as large as several sentences. This ap-
proach obviates the need to create a set of discourse
relations, but sentence internal segmentation is still
a necessary step. Though segmentation in the PDTB
tends to larger units than edus, our approach to seg-
mentation should be straightforwardly applicable to
their segmentation style.
Acknowledgments
Thanks to Caroline Sporleder and Mirella Lapata for
their test data and helpful comments. Thanks also to
Radu Soricut for helpful input. This research was
supported in part by NSF Grant #IIS-0447214. Any
opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grish-
man, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M.P. Marcus, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quantita-
tively comparing the syntactic coverage of english gram-
mars. In DARPA Speech and Natural Language Workshop,
pages 306?311.
L. Carlson, D. Marcu, and M.E. Okurowski. 2002. RST dis-
course treebank. Linguistic Data Consortium, Catalog #
LDC2002T07. ISBN LDC2002T07.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proceedings of
the 43rd Annual Meeting of ACL, pages 173?180.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics,
pages 132?139.
M.J. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages
1?8.
D. Cristea, O. Postolache, and I. Pistol. 2005. Summarisation
through discourse structure. In 6th International Conference
on Computational Linguistics and Intelligent Text Process-
ing (CICLing).
K. Hollingshead, S. Fisher, and B. Roark. 2005. Comparing
and combining finite-state and context-free parsers. In Pro-
ceedings of HLT-EMNLP, pages 787?794.
A. Knott. 1996. A Data-Driven Methodology for Motivating
a Set of Coherence Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of Edinburgh.
W.C. Mann and S.A. Thompson. 1988. Rhetorical structure
theory: Toward a functional theory of text organization. Text,
8(3):243?281.
D. Marcu. 1998. Improving summarization through rhetorical
parsing tuning. In The 6th Workshop on Very Large Corpora.
D. Marcu. 1999. Discourse trees are good indicators of im-
portance in text. In I. Mani and M. Maybury, editors, Ad-
vances in Automatic Text Summarization, pages 123?136.
MIT Press, Cambridge, MA.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004. The
Penn Discourse TreeBank. In Proceedings of the Language
Resources and Evaluation Conference.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and
B. Webber. 2005. The Penn Discourse TreeBank as a re-
source for natural language generation. In Proceedings of
the Corpus Linguistics Workshop on Using Corpora for Nat-
ural Language Generation.
R. Soricut and D. Marcu. 2003. Sentence level discourse pars-
ing using syntactic and lexical information. In Human Lan-
guage Technology Conference of the North American Asso-
ciation for Computational Linguistics (HLT-NAACL).
C. Sporleder and M. Lapata. 2005. Discourse chunking and its
application to sentence compression. In Human Language
Technology Conference and the Conference on Empirical
Methods in Natural Language Processing (HLT-EMNLP),
pages 257?264.
C. Sporleder and A. Lascarides. 2004. Combining hierarchi-
cal clustering and machine learning to predict high-level dis-
course structure. In Proceedings of the International Confer-
ence in Computational Linguistics (COLING), pages 43?49.
E.F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proceedings of
CoNLL, pages 127?132.
S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen. 2006.
Discourse-based answering of why-questions. Traitement
Automatique des Langues (TAL).
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and
R. Sauri. 2006. Classification of discourse coherence re-
lations: An exploratory study using multiple knowledge
sources. In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue.
F. Wolf and E. Gibson. 2005. Representing discourse coher-
ence: A corpus-based analysis. Computational Linguistics,
31(2):249?288.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of the 18th Inter-
national COLING, pages 947?953.
495
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 952?959,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Pipeline Iteration
Kristy Hollingshead and Brian Roark
Center for Spoken Language Understanding, OGI School of Science & Engineering
Oregon Health & Science University, Beaverton, Oregon, 97006 USA
{hollingk,roark}@cslu.ogi.edu
Abstract
This paper presents pipeline iteration, an ap-
proach that uses output from later stages
of a pipeline to constrain earlier stages of
the same pipeline. We demonstrate sig-
nificant improvements in a state-of-the-art
PCFG parsing pipeline using base-phrase
constraints, derived either from later stages
of the parsing pipeline or from a finite-
state shallow parser. The best performance
is achieved by reranking the union of un-
constrained parses and relatively heavily-
constrained parses.
1 Introduction
A ?pipeline? system consists of a sequence of pro-
cessing stages such that the output from one stage
provides the input to the next. Each stage in such a
pipeline identifies a subset of the possible solutions,
and later stages are constrained to find solutions
within that subset. For example, a part-of-speech
tagger could constrain a ?base phrase? chunker (Rat-
naparkhi, 1999), or the n-best output of a parser
could constrain a reranker (Charniak and Johnson,
2005). A pipeline is typically used to reduce search
complexity for rich models used in later stages, usu-
ally at the risk that the best solutions may be pruned
in early stages.
Pipeline systems are ubiquitous in natural lan-
guage processing, used not only in parsing (Rat-
naparkhi, 1999; Charniak, 2000), but also machine
translation (Och and Ney, 2003) and speech recogni-
tion (Fiscus, 1997; Goel et al, 2000), among others.
Despite the widespread use of pipelines, they have
been understudied, with very little work on gen-
eral techniques for designing and improving pipeline
systems (although cf. Finkel et al (2006)). This pa-
per presents one such general technique, here ap-
plied to stochastic parsing, whereby output from
later stages of a pipeline is used to constrain earlier
stages of the same pipeline. To our knowledge, this
is the first time such a pipeline architecture has been
proposed.
It may seem surprising that later stages of a
pipeline, already constrained to be consistent with
the output of earlier stages, can profitably inform
the earlier stages in a second pass. However, the
richer models used in later stages of a pipeline pro-
vide a better distribution over the subset of possible
solutions produced by the early stages, effectively
resolving some of the ambiguities that account for
much of the original variation. If an earlier stage is
then constrained in a second pass not to vary with re-
spect to these resolved ambiguities, it will be forced
to find other variations, which may include better so-
lutions than were originally provided.
To give a rough illustration, consider the Venn di-
agram in Fig. 1(i). Set A represents the original sub-
set of possible solutions passed along by the earlier
stage, and the dark shaded region represents high-
probability solutions according to later stages. If
some constraints are then extracted from these high-
probability solutions, defining a subset of solutions
(S) that rule out some of A, the early stage will be
forced to produce a different set (B). Constraints
derived from later stages of the pipeline focus the
search in an area believed to contain high-quality
candidates.
Another scenario is to use a different model al-
together to constrain the pipeline. In this scenario,
(i) (ii)
A
B
S A
B
S
Figure 1: Two Venn diagrams, representing (i) constraints
derived from later stages of an iterated pipelined system; and
(ii) constraints derived from a different model.
952
represented in Fig. 1(ii), the other model constrains
the early stage to be consistent with some subset of
solutions (S), which may be largely or completely
disjoint from the original set A. Again, a different set
(B) results, which may include better results than A.
Whereas when iterating we are guaranteed that the
new subset S will overlap at least partially with the
original subset A, that is not the case when making
use of constraints from a separately trained model.
In this paper, we investigate pipeline iteration
within the context of the Charniak and Johnson
(2005) parsing pipeline, by constraining parses to
be consistent with a base-phrase tree. We derive
these base-phrase constraints from three sources: the
reranking stage of the parsing pipeline; a finite-state
shallow parser (Hollingshead et al, 2005); and a
combination of the output from these two sources.
We compare the relative performance of these three
sources and find the best performance improvements
using constraints derived from a weighted combina-
tion of shallow parser output and reranker output.
The Charniak parsing pipeline has been exten-
sively studied over the past decade, with a num-
ber of papers focused on improving early stages of
the pipeline (Charniak et al, 1998; Caraballo and
Charniak, 1998; Blaheta and Charniak, 1999; Hall
and Johnson, 2004; Charniak et al, 2006) as well
as many focused on optimizing final parse accuracy
(Charniak, 2000; Charniak and Johnson, 2005; Mc-
Closky et al, 2006). This focus on optimization has
made system improvements very difficult to achieve;
yet our relatively simple architecture yields statisti-
cally significant improvements, making pipeline it-
eration a promising approach for other tasks.
2 Approach
Our approach uses the Charniak state-of-the-art
parsing pipeline. The well-known Charniak (2000)
coarse-to-fine parser is a two-stage parsing pipeline,
in which the first stage uses a vanilla PCFG to pop-
ulate a chart of parse constituents. The second
stage, constrained to only those items in the first-
stage chart, uses a refined grammar to generate an
n-best list of parse candidates. Charniak and John-
son (2005) extended this pipeline with a discrimina-
tive maximum entropy model to rerank the n-best
parse candidates, deriving a significant benefit from
the richer model employed by the reranker.
For our experiments, we modified the parser1 to
1ftp://ftp.cs.brown.edu/pub/nlparser/
Base Shallow
Parser Phrases Phrases
Charniak parser-best 91.9 94.4
reranker-best 92.8 94.8
Finite-state shallow parser 91.7 94.3
Table 1: F-scores on WSJ section 24 of output from two
parsers on the similar tasks of base-phrase parsing and shallow-
phrase parsing. For evaluation, base and shallow phrases are
extracted from the Charniak/Johnson full-parse output.
allow us to optionally provide base-phrase trees to
constrain the first stage of parsing.
2.1 Base Phrases
Following Ratnaparkhi (1999), we define a base
phrase as any parse node with only preterminal chil-
dren. Unlike the shallow phrases defined for the
CoNLL-2000 Shared Task (Tjong Kim Sang and
Buchholz, 2000), base phrases correspond directly
to constituents that appear in full parses, and hence
can provide a straightforward constraint on edges
within a chart parser. In contrast, shallow phrases
collapse certain non-constituents?such as auxiliary
chains?into a single phrase, and hence are not di-
rectly applicable as constraints on a chart parser.
We have two methods for deriving base-phrase
annotations for a string. First, we trained a finite-
state shallow parser on base phrases extracted from
the Penn Wall St. Journal (WSJ) Treebank (Marcus
et al, 1993). The treebank trees are pre-processed
identically to the procedure for training the Charniak
parser, e.g., empty nodes and function tags are re-
moved. The shallow parser is trained using the per-
ceptron algorithm, with a feature set nearly identical
to that from Sha and Pereira (2003), and achieves
comparable performance to that paper. See Holling-
shead et al (2005) for more details. Second, base
phrases can be extracted from the full-parse output
of the Charniak and Johnson (2005) reranker, via a
simple script to extract nodes with only preterminal
children.
Table 1 shows these systems? bracketing accu-
racy on both the base-phrase and shallow parsing
tasks for WSJ section 24; each system was trained
on WSJ sections 02-21. From this table we can
see that base phrases are substantially more difficult
than shallow phrases to annotate. Output from the
finite-state shallow parser is roughly as accurate as
output extracted from the Charniak parser-best trees,
though a fair amount below output extracted from
the reranker-best trees.
In addition to using base phrase constraints from
these two sources independently, we also looked at
953
combining the predictions of both to obtain more re-
liable constraints. We next present a method of com-
bining output from multiple parsers based on com-
bined precision and recall optimization.
2.2 Combining Parser n-best Lists
In order to select high-likelihood constraints for the
pipeline, we may want to extract annotations with
high levels of agreement (?consensus hypotheses?)
between candidates. In addition, we may want to
favor precision over recall to avoid erroneous con-
straints within the pipeline as much as possible.
Here we discuss how a technique presented in Good-
man?s thesis (1998) can be applied to do this.
We will first present this within a general chart
parsing approach, then move to how we use it for n-
best lists. Let T be the set of trees for a particular
input, and let a parse T ? T be considered as a set
of labeled spans. Then, for all labeled spans X ? T ,
we can calculate the posterior probability ?(X) as
follows:
?(X) =
?
T?T
P(T )JX ? T K
?
T ??T P(T ?)
(1)
where JX ? T K =
{
1 if X ? T
0 otherwise.
Goodman (1996; 1998) presents a method for us-
ing the posterior probability of constituents to maxi-
mize the expected labeled recall of binary branching
trees, as follows:
T? = argmax
T?T
?
X?T
?(X) (2)
Essentially, find the tree with the maximum sum of
the posterior probabilities of its constituents. This
is done by computing the posterior probabilities
of constituents in a chart, typically via the Inside-
Outside algorithm (Baker, 1979; Lari and Young,
1990), followed by a final CYK-like pass to find the
tree maximizing the sum.
For non-binary branching trees, where precision
and recall may differ, Goodman (1998, Ch.3) pro-
poses the following combined metric for balancing
precision and recall:
T? = argmax
T?T
?
X?T
(?(X)? ?) (3)
where ? ranges from 0 to 1. Setting ?=0 is equiv-
alent to Eq. 2 and thus optimizes recall, and setting
?=1 optimizes precision; Appendix 5 at the end of
this paper presents brief derivations of these met-
rics.2 Thus, ? functions as a mixing factor to balance
recall and precision.
This approach also gives us a straightforward way
to combine n-best outputs of multiple systems. To
do this, we construct a chart of the constituents in the
trees from the n-best lists, and allow any combina-
tion of constituents that results in a tree ? even one
with no internal structure. In such a way, we can
produce trees that only include a small number of
high-certainty constituents, and leave the remainder
of the string unconstrained, even if such trees were
not candidates in the original n-best lists.
For simplicity, we will here discuss the combina-
tion of two n-best lists, though it generalizes in the
obvious way to an arbitrary number of lists. Let T
be the union of the two n-best lists. For all trees
T ? T , let P1(T ) be the probability of T in the first
n-best list, andP2(T ) the probability of T in the sec-
ond n-best list. Then, we define P(T ) as follows:
P(T ) = ?
P1(T )
?
T ??T P1(T ?)
+
P2(T )
?
T ??T P2(T ?)
(4)
where the parameter ? dictates the relative weight of
P1 versus P2 in the combination.3
For this paper, we combined two n-best lists of
base-phrase trees. Although there is no hierarchi-
cal structure in base-phrase annotations, they can be
represented as flat trees, as shown in Fig. 2(a). We
constructed a chart from the two lists being com-
bined, using Eq. 4 to define P(T ) in Eq. 1. We wish
to consider every possible combination of the base
phrases, so for the final CYK-like pass to find the
argmax tree, we included rules for attaching each
preterminal directly to the root of the tree, in addi-
tion to rules permitting any combination of hypoth-
esized base phrases.
Consider the trees in Fig. 2. Figure 2(a) is a
shallow parse with three NP base phrases; Figure
2(b) is the same parse where the ROOT produc-
tion has been binarized for the final CYK-like pass,
which requires binary productions. If we include
productions of the form ?ROOT ? X ROOT? and
?ROOT ? X Y? for all non-terminals X and Y (in-
cluding POS tags), then any tree-structured com-
bination of base phrases hypothesized in either n-
2Our notation differs slightly from that in Goodman (1998),
though the approaches are formally equivalent.
3Note that P1 and P2 are normalized in eq. 4, and thus are
not required to be true probabilities. In turn, P is normalized
when used in eq. 1, such that the posterior probability ? is a
true probability. Hence P need not be normalized in eq. 4.
954
(a)
ROOT




 
 
@
@
PP
PP
PP
P
NP
 HH
DT
the
NN
broker
VBD
sold
NP
 HH
DT
the
NNS
stocks
NP
NN
yesterday
(b)
ROOT


H
HH
NP
 HH
DT
the
NN
broker
ROOT
 HH
VBD
sold
ROOT

 H
H
NP
 HH
DT
the
NNS
stocks
NP
NN
yesterday
(c)
S



H
H
HH
NP
 HH
DT
the
NN
broker
VP




H
H
H
H
VBD
sold
NP
 HH
DT
the
NNS
stocks
NP
NN
yesterday
Figure 2: Base-phrase trees (a) as produced for an n-best list and (b) after root-binarization for n-best list combination. Full-parse
tree (c) consistent with constraining base-phrase tree (a).
87 88 89 90 91 92 93 94 95 96 9786
87
88
89
90
91
92
93
94
95
96
precision
reca
ll
Charniak ? reranked (solid viterbi)
Finite?state shallow parser (solid viterbi)Charniak reranked + Finite?state
Figure 3: The tradeoff between recall and precision using a
range of ? values (Eq. 3) to select high-probability annotations
from an n-best list. Results are shown on 50-best lists of base-
phrase parses from two parsers, and on the combination of the
two lists.
best list is allowed, including the one with no base
phrases at all. Note that, for the purpose of finding
the argmax tree in Eq. 3, we only sum the posterior
probabilities of base-phrase constituents, and not the
ROOT symbol or POS tags.
Figure 3 shows the results of performing this com-
bined precision/recall optimization on three separate
n-best lists: the 50-best list of base-phrase trees ex-
tracted from the full-parse output of the Charniak
and Johnson (2005) reranker; the 50-best list output
by the Hollingshead et al (2005) finite-state shallow
parser; and the weighted combination of the two lists
at various values of ? in Eq. 3. For the combination,
we set ?=2 in Eq. 4, with the Charniak and Johnson
(2005) reranker providing P1, effectively giving the
reranker twice the weight of the shallow parser in
determining the posteriors. The shallow parser has
perceptron scores as weights, and the distribution of
these scores after a softmax normalization was too
peaked to be of utility, so we used the normalized
reciprocal rank of each candidate as P2 in Eq. 4.
We point out several details in these results.
First, using this method does not result in an F-
measure improvement over the Viterbi-best base-
phrase parses (shown as solid symbols in the graph)
for either the reranker or shallow parser. Also, us-
ing this model effects a greater improvement in pre-
cision than in recall, which is unsurprising with
these non-hierarchical annotations; unlike full pars-
ing (where long sequences of unary productions can
improve recall arbitrarily), in base-phrase parsing,
any given span can have only one non-terminal. Fi-
nally, we see that the combination of the two n-best
lists improves over either list in isolation.
3 Experimental Setup
For our experiments we constructed a simple parsing
pipeline, shown in Fig. 4. At the core of the pipeline
is the Charniak and Johnson (2005) coarse-to-fine
parser and MaxEnt reranker, described in Sec. 2.
The parser constitutes the first and second stages of
our pipeline, and the reranker the final stage. Fol-
lowing Charniak and Johnson (2005), we set the
parser to output 50-best parses for all experiments
described here. We constrain only the first stage of
the parser: during chart construction, we disallow
any constituents that conflict with the constraints, as
described in detail in the next section.
3.1 Parser Constraints
We use base phrases, as defined in Sec. 2.1, to con-
strain the first stage of our parsing pipeline. Under
these constraints, full parses must be consistent with
the base-phrase tree provided as input to the parser,
i.e., any valid parse must contain all of the base-
phrase constituents in the constraining tree. The
full-parse tree in Fig. 2(c), for example, is consis-
tent with the base-phrase tree in Fig. 2(a).
Implementing these constraints in a parser is
straightforward, one of the advantages of using base
phrases as constraints. Since the internal structure
of base phrases is, by definition, limited to preter-
minal children, we can constrain the entire parse by
constraining the parents of the appropriate pretermi-
nal nodes. For any preterminal that occurs within
the span of a constraining base phrase, the only
valid parent is a node matching both the span (start
and end points) and the label of the provided base
955
A3
Shallow
Parser
Coarse
Parser
Fine
Parser Reranker
D
CB
extracted
base phrases
A1
A2
+
Figure 4: The iterated parsing pipeline. In the first iteration,
the coarse parser may be either unconstrained, or constrained
by base phrases from the shallow parser (A1). In the second
iteration, base phrase constraints may be extracted either from
reranker output (A2) or from a weighted combination of shal-
low parser output and reranker output (A3). Multiple sets of
n-best parses, as output by the coarse-to-fine parser under dif-
ferent constraint conditions, may be joined in a set union (C).
phrase. All other proposed parent-nodes are re-
jected. In such a way, for any parse to cover the
entire string, it would have to be consistent with the
constraining base-phrase tree.
Words that fall outside of any base-phrase con-
straint are unconstrained in how they attach within
the parse; hence, a base-phrase tree with few words
covered by base-phrase constraints will result in a
larger search space than one with many words cov-
ered by base phrases. We also put no restrictions on
the preterminal labels, even within the base phrases.
We normalized for punctuation. If the parser fails to
find a valid parse with the constraints, then we lift
the constraints and allow any parse constituent orig-
inally proposed by the first-stage parser.
3.2 Experimental Conditions
Our experiments will demonstrate the effects of con-
straining the Charniak parser under several differ-
ent conditions. The baseline system places no con-
straints on the parser. The remaining experimen-
tal conditions each consider one of three possible
sources of the base phrase constraints: (1) the base
phrases output by the finite-state shallow parser;
(2) the base phrases extracted from output of the
reranker; and (3) a combination of the output from
the shallow parser and the reranker, which is pro-
duced using the techniques outlined in Sec. 2.2.
Constraints are enforced as described in Sec. 3.1.
Unconstrained For our baseline system, we
run the Charniak and Johnson (2005) parser and
reranker with default parameters. The parser is pro-
vided with treebank-tokenized text and, as men-
tioned previously, outputs 50-best parse candidates
to the reranker.
FS-constrained The FS-constrained condition
provides a comparison point of non-iterated con-
straints. Under this condition, the one-best base-
System LR LP F
Finite-state shallow parser 91.3 92.0 91.7
Charniak reranker-best 92.2 93.3 92.8
Combination (?=0.5) 92.2 94.1 93.2
Combination (?=0.9) 81.0 97.4 88.4
Table 2: Labeled recall (LR), precision (LP), and F-scores
on WSJ section 24 of base-phrase trees produced by the three
possible sources of constraints.
phrase tree output by the finite-state shallow parser
is input as a constraint to the Charniak parser. We
run the parser and reranker as before, under con-
straints from the shallow parser. The accuracy of
the constraints used under this condition is shown in
the first row of Table 2. Note that this condition is
not an instance of pipeline iteration, but is included
to show the performance levels that can be achieved
without iteration.
Reranker-constrained We will use the
reranker-constrained condition to examine the ef-
fects of pipeline iteration, with no input from other
models outside the pipeline. We take the reranker-
best full-parse output under the condition of uncon-
strained search, and extract the corresponding base-
phrase tree. We run the parser and reranker as be-
fore, now with constraints from the reranker. The
accuracy of the constraints used under this condition
is shown in the second row of Table 2.
Combo-constrained The combo-constrained
conditions are designed to compare the effects of
generating constraints with different combination
parameterizations, i.e., different ? parameters in Eq.
3. For this experimental condition, we extract base-
phrase trees from the n-best full-parse trees output
by the reranker. We combine this list with the n-best
list output by the finite-state shallow parser, exactly
as described in Sec. 2.2, again with the reranker pro-
viding P1 and ?=2 in Eq. 4. We examined a range
of operating points from ?=0.4 to ?=0.9, and re-
port two points here (?=0.5 and ?=0.9), which rep-
resent the highest overall accuracy and the highest
precision, respectively, as shown in Table 2.
Constrained and Unconstrained Union When
iterating this pipeline, the original n-best list of full
parses output from the unconstrained parser is avail-
able at no additional cost, and our final set of ex-
perimental conditions investigate taking the union
of constrained and unconstrained n-best lists. The
imposed constraints can result in candidate sets that
are largely (or completely) disjoint from the uncon-
strained sets, and it may be that the unconstrained
set is in many cases superior to the constrained set.
956
Constraints Parser-best Reranker-best Oracle-best # Candidates
Baseline (Unconstrained, 50-best) 88.92 90.24 95.95 47.9
FS-constrained 88.44 89.50 94.10 46.2
Reranker-constrained 89.60 90.46 95.07 46.9
Combo-constrained (?=0.5) 89.81 90.74 95.41 46.3
Combo-constrained (?=0.9) 89.34 90.43 95.91 47.5
Table 3: Full-parse F-scores on WSJ section 24. The unconstrained search (first row) provides a baseline comparison for the
effects of constraining the search space. The last four rows demonstrate the effect of various constraint conditions.
Even our high-precision constraints did not reach
100% precision, attesting to the fact that there was
some error in all constrained conditions. By con-
structing the union of the two n-best lists, we can
take advantage of the new constrained candidate set
without running the risk that the constraints have re-
sulted in a worse n-best list. Note that the parser
probabilities are produced from the same model in
both passes, and are hence directly comparable.
The output of the second pass of the pipeline
could be used to constrain a third pass, for multiple
pipeline iterations. However, we found that further
iterations provided no additional improvements.
3.3 Data
Unless stated otherwise, all reported results will be
F-scores on WSJ section 24 of the Penn WSJ Tree-
bank, which was our development set. Training data
was WSJ sections 02-21, with section 00 as held-
out data. Crossfold validation (20-fold with 2,000
sentences per fold) was used to train the reranker
for every condition. Evaluation was performed us-
ing evalb under standard parameterizations. WSJ
section 23 was used only for final testing.
4 Results & Discussion
We evaluate the one-best parse candidates before
and after reranking (parser-best and reranker-best,
respectively). We additionally provide the best-
possible F-score in the n-best list (oracle-best) and
the number of unique candidates in the list.
Table 3 presents trials showing the effect of con-
straining the parser under various conditions. Con-
straining the parser to the base phrases produced
by the finite-state shallow parser (FS-constrained)
hurts performance by half a point. Constraining the
parser to the base phrases produced by the reranker,
however, provides a 0.7 percent improvement in the
parser-best accuracy, and a 0.2 percent improvement
after reranking. Combining the two base-phrase n-
best lists to derive the constraints provides further
improvements when ?=0.5, to a total improvement
of 0.9 and 0.5 percent over parser-best and reranker-
best accuracy, respectively. Performance degrades
at ?=0.9 relative to ?=0.5, indicating that, even at
a lower precision, more constraints are beneficial.
The oracle rate decreases under all of the con-
strained conditions as compared to the baseline,
demonstrating that the parser was prevented from
finding some of the best solutions that were orig-
inally found. However, the improvement in F-
score shows that the constraints assisted the parser
in achieving high-quality solutions despite this de-
graded oracle accuracy of the lists.
Table 4 shows the results when taking the union
of the constrained and unconstrained lists prior to
reranking. Several interesting points can be noted
in this table. First, despite the fact that the FS-
constrained condition hurts performance in Table
3, the union provides a 0.5 percent improvement
over the baseline in the parser-best performance.
This indicates that, in some cases, the Charniak
parser is scoring parses in the constrained set higher
than in the unconstrained set, which is evidence of
search errors in the unconstrained condition. One
can see from the number of candidates that the FS-
constrained condition provides the set of candidates
most disjoint from the original unconstrained parser,
leading to the largest number of candidates in the
union. Surprisingly, even though this set provided
the highest parser-best F-score of all of the union
sets, it did not lead to significant overall improve-
ments after reranking.
In all other conditions, taking the union de-
creases the parser-best accuracy when compared to
the corresponding constrained output, but improves
the reranker-best accuracy in all but the combo-
constrained ?=0.9 condition. One explanation for
the lower performance at ?=0.9 versus ?=0.5 is
seen in the number of candidates, about 7.5 fewer
than in the ?=0.5 condition. There are fewer con-
straints in the high-precision condition, so the re-
sulting n-best lists do not diverge as much from the
original lists, leading to less diversity in their union.
The gains in performance should not be attributed
to increasing the number of candidates nor to allow-
957
Constraints Parser-best Reranker-best Oracle-best # Candidates
Baseline (Unconstrained, 50-best) 88.92 90.24 95.95 47.9
Unconstrained ? FS-constrained 89.39 90.27 96.61 74.9
Unconstrained ? Reranker-constrained 89.23 90.59 96.48 70.3
Unconstrained ? Combo (?=0.5) 89.28 90.78 96.53 69.7
Unconstrained ? Combo (?=0.9) 89.03 90.44 96.40 62.1
Unconstrained (100-best) 88.82 90.13 96.38 95.2
Unconstrained (50-best, beam?2) 89.01 90.45 96.13 48.1
Table 4: Full-parse F-scores on WSJ section 24 after taking the set union of unconstrained and constrained parser output under
the 4 different constraint conditions. Also, F-score for 100-best parses, and 50-best parses with an increased beam threshold, output
by the Charniak parser under the unconstrained condition.
Constraints F-score
Baseline (Unconstrained, 50-best) 91.06
Unconstrained ? Combo (?=0.5) 91.48
Table 5: Full-parse F-scores on WSJ section 23 for our best-
performing system on WSJ section 24. The 0.4 percent F-score
improvement is significant at p < 0.001.
ing the parser more time to generate the parses. The
penultimate row in Table 4 shows the results with
100-best lists output in the unconstrained condition,
which does not improve upon the 50-best perfor-
mance, despite an improved oracle F-score. Since
the second iteration through the parsing pipeline
clearly increases the overall processing time by a
factor of two, we also compare against output ob-
tained by doubling the coarse-parser?s beam thresh-
old. The last row in Table 4 shows that the increased
threshold yields an insignificant improvement over
the baseline, despite a very large processing burden.
We applied our best-performing model (Uncon-
strained ? Combo, ?=0.5) to the test set, WSJ sec-
tion 23, for comparison against the baseline system.
Table 5 shows a 0.4 percent F-score improvement
over the baseline for that section, which is statisti-
cally significant at p < 0.001, using the stratified
shuffling test (Yeh, 2000).
5 Conclusion & Future Work
In summary, we have demonstrated that pipeline it-
eration can be useful in improving system perfor-
mance, by constraining early stages of the pipeline
with output derived from later stages. While the
current work made use of a particular kind of
constraint?base phrases?many others could be ex-
tracted as well. Preliminary results extending the
work presented in this paper show parser accuracy
improvements from pipeline iteration when using
constraints based on an unlabeled partial bracketing
of the string. Higher-level phrase segmentations or
fully specified trees over portions of the string might
also prove to be effective constraints. The tech-
niques shown here are by no means limited to pars-
ing pipelines, and could easily be applied to other
tasks making use of pipeline architectures.
Acknowledgments
Thanks to Martin Jansche for useful discussions on
topics related to this paper. The first author of this
paper was supported under an NSF Graduate Re-
search Fellowship. In addition, this research was
supported in part by NSF Grant #IIS-0447214. Any
opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
References
J.K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication papers for the 97th Meeting of the
Acoustical Society of America.
D. Blaheta and E. Charniak. 1999. Automatic compensation
for parser figure-of-merit flaws. In Proceedings of the 37th
Annual Meeting of ACL, pages 513?518.
S. Caraballo and E. Charniak. 1998. New figures of merit
for best-first probabilistic chart parsing. Computational Lin-
guistics, 24(2):275?298.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proceedings of
the 43rd Annual Meeting of ACL, pages 173?180.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-based
best-first chart parsing. In Proceedings of the 6th Workshop
for Very Large Corpora, pages 127?133.
E. Charniak, M. Johnson, M. Elsner, J.L. Austerweil, D. Ellis,
S.R. Iyangar, J. Moore, M.T. Pozar, C. Hill, T.Q. Vu, and
I. Haxton. 2006. Multi-level course-to-fine PCFG parsing.
In Proceedings of the HLT-NAACL Annual Meeting, pages
168?175.
E. Charniak. 2000. A Maximum-Entropy-inspired parser. In
Proceedings of the 1st Annual Meeting of NAACL and 6th
Conference on ANLP, pages 132?139.
J.R. Finkel, C.D. Manning, and A.Y. Ng. 2006. Solving the
problem of cascading errors: Approximate Bayesian infer-
ence for linguistic annotation pipelines. In Proceedings of
EMNLP, pages 618?626.
J. Fiscus. 1997. A post-processing system to yield reduced
word error rates: Recognizer output voting error reduction
(ROVER). In Proceedings of the IEEE Workshop on Auto-
matic Speech Recognition and Understanding.
V. Goel, S. Kumar, and W. Byrne. 2000. Segmental minimum
Bayes-risk ASR voting strategies. In Proceedings of ICSLP,
pages 139?142.
958
J. Goodman. 1996. Parsing algorithms and metrics. In Pro-
ceedings of the 34th Annual Meeting of ACL, pages 177?183.
J. Goodman. 1998. Parsing inside-out. Ph.D. thesis, Harvard
University.
K. Hall and M. Johnson. 2004. Attention shifting for parsing
speech. In Proceedings of the 42nd Annual Meeting of ACL,
pages 40?46.
K. Hollingshead, S. Fisher, and B. Roark. 2005. Comparing
and combining finite-state and context-free parsers. In Pro-
ceedings of HLT-EMNLP, pages 787?794.
K. Lari and S.J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer Speech and Language, 4(1):35?56.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993.
Building a large annotated corpus of English: The Penn tree-
bank. Computational Linguistics, 19:314?330.
D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking
and self-training for parser adaptation. In Proceedings of
COLING-ACL, pages 337?344.
F.J. Och and H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29.
A. Ratnaparkhi. 1999. Learning to parse natural language with
maximum entropy models. Machine Learning, 34(1-3):151?
175.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proceedings of the HLT-NAACL Annual
Meeting, pages 134?141.
E.F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proceedings of
CoNLL, pages 127?132.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of the 18th Inter-
national COLING, pages 947?953.
Appendix A Combined Precision/Recall
Decoding
Recall that T is the set of trees for a particular input,
and each T ? T is considered as a set of labeled
spans. For all labeled spans X ? T , we can calcu-
late the posterior probability ?(X) as follows:
?(X) =
?
T?T
P(T )JX ? T K
?
T ??T P(T ?)
where JX ? T K =
{
1 if X ? T
0 otherwise.
If ? is the reference tree, the labeled precision
(LP) and labeled recall (LR) of a T relative to ? are
defined as
LP =
|T ? ? |
|T |
LR =
|T ? ? |
|? |
where |T | denotes the size of the set T .
A metric very close to LR is |T ? ? |, the number
of nodes in common between the tree and the ref-
erence tree. To maximize the expected value (E) of
this metric, we want to find the tree T? as follows:
T? = argmax
T?T
E
[
|T
?
? |
]
= argmax
T?T
?
T ??T
P(T ?)
[
|T
?
T ?|
]
?
T ???T P(T
??)
= argmax
T?T
?
T ??T
P(T ?)
?
X?T JX ? T
?K
?
T ???T P(T
??)
= argmax
T?T
?
X?T
?
T ??T
P(T ?)JX ? T ?K
?
T ???T P(T
??)
= argmax
T?T
?
X?T
?(X) (5)
This exactly maximizes the expected LR in the
case of binary branching trees, and is closely re-
lated to LR for non-binary branching trees. Simi-
lar to maximizing the expected number of match-
ing nodes, we can minimize the expected number of
non-matching nodes, for a metric related to LP:
T? = argmin
T?T
E
[
|T | ? |T
?
? |
]
= argmax
T?T
E
[
|T
?
? | ? |T |
]
= argmax
T?T
?
T ??T
P(T ?)
[
|T
?
T ?| ? |T |
]
?
T ???T P(T
??)
= argmax
T?T
?
T ??T
P(T ?)
?
X?T (JX ? T
?K ? 1)
?
T ???T P(T
??)
= argmax
T?T
?
X?T
?
T ??T
P(T ?)(JX ? T ?K ? 1)
?
T ???T P(T
??)
= argmax
T?T
?
X?T
(?(X)? 1) (6)
Finally, we can combine these two metrics in a
linear combination. Let ? be a mixing factor from 0
to 1. Then we can optimize the weighted sum:
T? = argmax
T?T
E
[
(1? ?)|T
?
? |+ ?(|T
?
? | ? |T |)
]
= argmax
T?T
(1? ?)E
[
|T
?
? |
]
+ ?E
[
|T
?
? | ? |T |
]
= argmax
T?T
[
(1? ?)
?
X?T
?(X)
]
+
[
?
?
X?T
(?(X)? 1)
]
= argmax
T?T
?
X?T
(?(X)? ?) (7)
The result is a combined metric for balancing preci-
sion and recall. Note that, if ?=0, Eq. 7 is the same
as Eq. 5 and thus maximizes the LR metric; and if
?=1, Eq. 7 is the same as Eq. 6 and thus maximizes
the LP metric.
959
Efficient incremental beam-search parsing
with generative and discriminative models
Brian Roark
Center for Spoken Language Understanding
OGI School of Science & Engineering, Oregon Health & Science University
Extended Abstract:
This talk will present several issues related to incre-
mental (left-to-right) beam-search parsing of natu-
ral language using generative or discriminative mod-
els, either individually or in combination. The first
part of the talk will provide background in incre-
mental top-down and (selective) left-corner beam-
search parsing algorithms, and in stochastic models
for such derivation strategies. Next, the relative ben-
efits and drawbacks of generative and discriminative
models with respect to heuristic pruning and search
will be discussed. A range of methods for using mul-
tiple models during incremental parsing will be de-
tailed. Finally, we will discuss the potential for ef-
fective use of fast, finite-state processing, e.g. part-
of-speech tagging, to reduce the parsing search space
without accuracy loss. POS-tagging is shown to im-
prove efficiency by as much as 20-25 percent with
the same accuracy, largely due to the treatment of un-
known words. In contrast, an ?islands-of-certainty?
approach, which quickly annotates labeled bracket-
ing over low-ambiguity word sequences, is shown to
provide little or no efficiency gain over the existing
beam-search.
The basic parsing approach that will be described
in this talk is stochastic incremental top-down pars-
ing, using a beam-search to prune the search space.
Grammar induction occurs from an annotated tree-
bank, and non-local features are extracted from each
derivation to enrich the stochastic model. Left-corner
grammar and tree transforms can be applied to the
treebank or the induced grammar, either fully or se-
lectively, to change the derivation order while retain-
ing the same underlying parsing algorithm. This ap-
proach has been shown to be accurate, relatively effi-
cient, and robust using both generative and discrim-
inative models (Roark, 2001; Roark, 2004; Collins
and Roark, 2004).
The key to effective beam-search parsing is com-
parability of analyses when the pruning is done. If
two competing parses are at different points in their
respective derivations, e.g. one is near the end of the
derivation and another is near the beginning, then it
will be difficult to evaluate which of the two is likely
to result in a better parse. With a generative model,
comparability can be accomplished by the use of a
look-ahead statistic, which estimates the amount of
probability mass required to extend a given deriva-
tion to include the word(s) in the look-ahead. Ev-
ery step in the derivation decreases the probability
of the derivation, but also takes the derivation one
step closer to attaching to the look-ahead. For good
parses, the look-ahead statistic should increase with
each step of the derivation, ensuring a certain degree
of comparability among competing parses with the
same look-ahead.
Beam-search parsing using an unnormalized dis-
criminative model, as in Collins and Roark (2004),
requires a slightly different search strategy than the
original generative model described in Roark (2001;
2004). This alternate search strategy is closer to the
approach taken in Costa et al (2001; 2003), in that
it enumerates a set of possible ways of attaching the
next word before evaluating with the model. This en-
sures comparability for models that do not have the
sort of behavior described above for the generative
models, rendering look-ahead statistics difficult to
estimate. This approach is effective, although some-
what less so than when a look-ahead statistic is used.
A generative parsing model can be used on its
own, and it was shown in Collins and Roark (2004)
that a discriminative parsing model can be used on
its own. Most discriminative parsing approaches,
e.g. (Johnson et al, 1999; Collins, 2000; Collins
and Duffy, 2002), are re-ranking approaches, in
which another model (typically a generative model)
presents a relatively small set of candidates, which
are then re-scored using a second, discriminatively
trained model. There are other ways to combine a
generative and discriminative model apart from wait-
ing for the former to provide a set of completed can-
didates to the latter. For example, the scores can
be used simultaneously; or the generative model can
present candidates to the discriminative model at in-
termediate points in the string, rather than simply at
the end. We discuss these options and their potential
benefits.
Finally, we discuss and present a preliminary eval-
uation of the use of rapid finite-state tagging to re-
duce the parsing search space, as was done in (Rat-
naparkhi, 1997; Ratnaparkhi, 1999). When the pars-
ing algorithm is integrated with model training, such
efficiency improvements can be particularly impor-
tant. POS-tagging using a simple bi-tag model im-
proved parsing efficiency by nearly 25 percent with-
out a loss in accuracy, when 1.2 tags per word were
produced on average by the tagger. Producing a sin-
gle tag sequence for each string resulted in further
speedups, but at the loss of 1-2 points of accuracy.
We show that much, but not all, of the speedup from
POS-tagging is due to more constrained tagging of
unknown words.
In a second set of trials, we make use of what
we are calling ?syntactic collocations?, i.e. collo-
cations that are (nearly) unambiguously associated
with a particular syntactic configuration. For ex-
ample, a chain of auxiliaries in English will always
combine in a particular syntactic configuration, mod-
ulo noise in the annotation. In our approach, the la-
beled bracketing spanning the sub-string is treated
as a tag for the sequence. A simple, finite-state
method for finding such collocations, and an effi-
cient longest match algorithm for labeling strings
will be presented. The labeled-bracketing ?tags? are
integrated with the parse search as follows: when a
derivation reaches the first word of such a colloca-
tion, the remaining words are attached in the given
configuration. This has the effect of extending the
look-ahead beyond the collocation, as well as po-
tentially reducing the amount of search required to
extend the derivations to include the words in the
collocation. However, while POS-tagging improved
efficiency, we find that using syntactic collocations
does not, indicating that ?islands-of-certainty? ap-
proaches are not what is needed from shallow pro-
cessing; rather genuine dis-ambiguation of the sort
provided by the POS-tagger.
Acknowledgments
Most of this work was done while the author was at
AT&T Labs - Research. Some of it was in collabora-
tion with Michael Collins.
References
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures and the voted perceptron.
In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, pages
263?270.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics. To appear.
Michael J. Collins. 2000. Discriminative reranking
for natural language parsing. In The Proceedings
of the 17th International Conference on Machine
Learning.
Fabrizio Costa, Vincenzo Lombardo, Paolo Frasconi,
and Giovanni Soda. 2001. Wide coverage in-
cremental parsing by learning attachment prefer-
ences. In Conference of the Italian Association for
Artificial Intelligence (AIIA), pages 297?307.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
and Giovanni Soda. 2003. Towards incremental
parsing of natural language using recursive neural
networks. Applied Intelligence. to appear.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 535?
541.
Adwait Ratnaparkhi. 1997. A linear observed time
statistical parser based on maximum entropy mod-
els. In Proceedings of the Second Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-97), pages 1?10.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Ma-
chine Learning, 34:151?175.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249?276.
Brian Roark. 2004. Robust garden path parsing.
Natural Language Engineering, 10(1):1?24.
BioNLP 2007: Biological, translational, and clinical language processing, pages 1?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
Syntactic complexity measures for detecting Mild Cognitive Impairment
Brian Roark, Margaret Mitchell and Kristy Hollingshead
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
Beaverton, Oregon, 97006 USA
{roark,meg.mitchell,hollingk}@cslu.ogi.edu
Abstract
We consider the diagnostic utility of vari-
ous syntactic complexity measures when ex-
tracted from spoken language samples of
healthy and cognitively impaired subjects.
We examine measures calculated from man-
ually built parse trees, as well as the same
measures calculated from automatic parses.
We show statistically significant differences
between clinical subject groups for a num-
ber of syntactic complexity measures, and
these differences are preserved with auto-
matic parsing. Different measures show dif-
ferent patterns for our data set, indicating
that using multiple, complementary mea-
sures is important for such an application.
1 Introduction
Natural language processing (NLP) techniques are
often applied to electronic health records and other
clinical datasets. Another potential clinical use of
NLP is for processing patient language samples,
which can be used to assess language development
(Sagae et al, 2005) or the impact of neurodegenera-
tive impairments on speech and language (Roark et
al., 2007). In this paper, we present methods for au-
tomatically measuring syntactic complexity of spo-
ken language samples elicited during neuropsycho-
logical exams of elderly subjects, and examine the
utility of these measures for discriminating between
clinically defined groups.
Mild Cognitive Impairment (MCI), and in par-
ticular amnestic MCI, the earliest clinically de-
fined stage of Alzheimer?s-related dementia, often
goes undiagnosed due to the inadequacy of com-
mon screening tests such as the Mini-Mental State
Examination (MMSE) for reliably detecting rela-
tively subtle impairments. Linguistic memory tests,
such as word list and narrative recall, are more ef-
fective than the MMSE in detecting MCI, yet are
still individually insufficient for adequate discrimi-
nation between healthy and impaired subjects. Be-
cause of this, a battery of examinations is typically
used to improve psychometric classification. Yet the
summary recall scores derived from these linguistic
memory tests (total correctly recalled) ignore poten-
tially useful information in the characteristics of the
spoken language itself.
Narrative retellings provide a natural, conversa-
tional speech sample that can be analyzed for many
of the characteristics of speech and language that
have been shown to discriminate between healthy
and impaired subjects, including syntactic complex-
ity (Kemper et al, 1993; Lyons et al, 1994) and
mean pause duration (Singh et al, 2001). These
measures go beyond simply measuring fidelity to
the narrative, thus providing key additional dimen-
sions for improved diagnosis of impairment. Recent
work (Roark et al, 2007) has shown significant dif-
ferences between healthy and MCI groups for both
pause related and syntactic complexity measures de-
rived from transcripts and audio of narrative recall
tests. In this paper, we look more closely at syntac-
tic complexity measures.
There are two key considerations when choos-
ing how to measure syntactic complexity of spoken
language samples for the purpose of psychometric
evaluation. First and most importantly, the syntactic
complexity measures will be used for discrimination
between groups, hence high discriminative utility is
desired. It has been demonstrated in past studies
(Cheung and Kemper, 1992) that many competing
measures are in fact very highly correlated, so it may
be the case that many measures are equally discrimi-
native. For this reason, previous results (Roark et al,
2007) have focused on a single syntactic complexity
metric, that of Yngve (1960).
A second key consideration, however, is the fi-
delity of the measure when derived from transcripts
via automatic parsing. Different syntactic complex-
ity measures rely on varying levels of detail from
1
the parse tree. Some syntactic complexity measures,
such as that of Yngve (1960), make use of unla-
beled tree structures to derive their scores; others,
such as that of Frazier (1985), rely on labels within
the tree, in addition to the tree structure, to pro-
vide the scores. Given these different uses of detail,
some measures may be less reliable with automa-
tion, hence dis-preferred in the context of automated
evaluation. Ideally, simple, easy-to-automate mea-
sures with high discriminative utility are preferred.
In the current paper, we demonstrate that various
syntactic complexity measures capture complemen-
tary systematic differences between subject groups,
suggesting that the best approach to discriminating
between healthy and impaired subjects is to collect
various measures, as a way of capturing language
?signatures? of the impairment.
For many measures of syntactic complexity, the
nature of the syntactic annotation is critical ? differ-
ent conventions of structural annotation will yield
different scores. We will thus spend the next sec-
tion briefly detailing the syntactic annotation con-
ventions that were followed for this work. This is
followed by a section describing a range of complex-
ity measures to be derived from these annotations.
Finally, we present empirical results on the samples
of spoken narrative retellings.
2 Syntactic annotation
For manual syntactic annotation of collected data
(see Section 4), we followed the syntactic annota-
tion conventions of the well-known Penn Treebank
(Marcus et al, 1993). This provides several key ben-
efits. First, there is an extensive annotation guide
that has been developed, not just for written but also
for spoken language, so that consistent annotation
was facilitated. Second, the large out-of-domain
corpora, in particular the 1 million words of syn-
tactically annotated Switchboard telephone conver-
sations, provide a good starting point for training
domain adapted parsing models. Finally, we can use
multiple domains for evaluating the correlations be-
tween various syntactic complexity measures.
There are characteristics of Penn Treebank anno-
tation that can impact syntactic complexity scoring.
First, prenominal modifiers are typically grouped in
a flat constituent with no internal structure. This an-
notation choice can result in very long noun phrases
(NPs) which pose very little difficulty in terms of
human processing performance, but can inflate com-
plexity measures that measure deviation from right-
branching structures, such as that of Yngve (1960).
Second, in spoken language annotations, a reparan-
dum1 is denoted with a special non-terminal cate-
gory EDITED. For this paper, we remove from the
tree these non-terminals, and the structures under-
neath them, prior to evaluating syntactic complexity.
3 Syntactic complexity
There is no single agreed-upon measurement of
syntactic complexity. A range of measures have
been proposed, with different primary considera-
tions driving the notion of complexity for each.
Many measures focus on the order in which vari-
ous constructions are acquired by children learning
the syntax of their native language ? later acquisi-
tions being taken as higher complexity. Examples
of this sort of complexity measure are: mean length
of utterance (MLU), which is typically measured
in morphemes (Miller and Chapman, 1981); the
Index of Productive Syntax (Scarborough, 1990),
a multi-point scale which has recently been auto-
mated for child-language transcript analysis (Sagae
et al, 2005); and Developmental Level (Rosenberg
and Abbeduto, 1987), a 7-point scale of complex-
ity based on the presence of specific grammatical
constructions. Other approaches have relied upon
the right-branching nature of English syntactic trees
(Yngve, 1960; Frazier, 1985), under the assump-
tion that deviations from that correspond to more
complexity in the language. Finally, there are ap-
proaches focused on the memory demands imposed
by ?distance? between dependent words (Lin, 1996;
Gibson, 1998).
3.1 Yngve scoring
The scoring approach taken in Yngve (1960) is re-
lated to the size of a ?first in/last out? stack at each
word in a top-down, left-to-right parse derivation.
Consider the tree in Figure 1. If we knew exactly
which productions to use, the parse would begin
with an S category on the stack and advance as
follows: pop the S and push VP and NP onto the
stack; pop NP and push PRP onto the stack; pop
PRP from the stack; pop VP from the stack and
push NP and VBD onto the stack; and so on. At
the point when the word ?she? is encountered, only
VP remains on the stack of the parser. When ?was?
1A reparandum is a sequence of words that are aborted by
the speaker, then repaired within the same utterance.
2
S
1 0
NP
0
VP
1 0
PRP VBD NP
1 0
NP
1 0
PP
1 0
DT NN IN NP
2 1 0
DT NN NN
Yngve score:she1 was1 a2 cook1 in1 a2 school1 cafeteria0
1
Figure 1: Parse tree with branch scores for Yngve scoring.
is reached, just NP is on the stack. Thus, the Yn-
gve score for these two words is 1. When the next
word ?a? is reached, however, there are two cate-
gories on the stack: PP and NN, so this word re-
ceives an Yngve score of 2. Stack size has been re-
lated by some (Resnik, 1992) to working memory
demands, although it most directly measures devia-
tion from right-branching trees.
To calculate the size of the stack at each word,
we can use the following simple algorithm. At each
node in the tree, label the branches from that node
to each of its children, beginning with zero at the
rightmost child and continuing to the leftmost child,
incrementing the score by one for each child. Hence,
each rightmost branch in the tree of Figure 1 is la-
beled with 0, the leftmost branch in all binary nodes
is labeled with 1, and the leftmost branch in the
ternary node is labeled with 2. Then the score for
each word is the sum of the branch scores from the
root of the tree to the word.
Given the score for each word, we can then de-
rive an overall complexity score by summing them
or taking the maximum or mean. For this paper,
we report mean scores for this and other word-based
measures, since we have found these means to pro-
vide better performing scores than either total sum
or maximum. For the tree in Figure 1, the maximum
is 2, the total is 9 and the mean over 8 words is 118 .
3.2 Frazier scoring
Frazier (1985) proposed an approach to scoring syn-
tactic complexity that traces a path from a word up
the tree until reaching either the root of the tree or
the lowest node which is not the leftmost child of its
parent.2 For example, Figure 2 shows the tree from
2An exception is made for empty subject NPs, in which case
S
1.5
NP
1
VP
1
PRP VBD NP
1
NP
1
PP
1
DT NN IN NP
1
DT NN NN
Frazier score:
she
2.5
was
1
a
2
cook
0
in
1
a
1
school
0
cafeteria
0
1
Figure 2: Parse tree fragments with scores for Frazier scoring.
Figure 1 broken into distinct paths for each word
in the string. The first word has a path up to the
root, while the second word just up to the VP, since
the VP has an NP sibling to its left. The word is
then scored, as in the Yngve measure, by summing
the scores on the links along the path. Each non-
terminal node in the path contributes a score of 1,
except for sentence nodes and sentence-complement
nodes,3 which score 1.5 rather than 1. Thus em-
bedded clauses contribute more to the complexity
measure than other embedded categories, as an ex-
plicit acknowledgment of sentence embeddings as a
source of syntactic complexity.
As with the Yngve score, we can calculate the
total and the mean of these word scores. In con-
trast to the maximum score calculated for the Yngve
measure, Frazier proposed summing the word scores
for each 3-word sequence in the sentence, then tak-
ing the maximum of these sums as a measure of
highly-localized concentrations of grammatical con-
stituents. For the example in Figure 2, the maximum
is 2.5, the maximum 3-word sum is 5.5, and the total
is 7.5, yielding a mean of 1516 .
3.3 Dependency distance
Rather than examining the tree structure itself, one
might also extract measures from lexical depen-
dency structures. These dependencies can be de-
rived from the tree using standard rules for estab-
lishing head children for constituents, originally at-
the succeeding verb receives an additional score of 1 (for the
deleted NP), and its path continues up the tree. Empty NPs are
annotated in our manual parse trees but not in the automatic
parses, which may result in a small disagreement in the Frazier
scores for manual and automatic trees.
3Every non-terminal node beginning with an S, including
SQ and SINV, were counted as sentence nodes. Sequences of
sentence nodes, i.e. an SBAR appearing directly under an S
node, were only counted as a single sentence node and thus only
contributed to the score once.
3
she was a cook in a school cafeteria
1
Figure 3: Dependency graph for the example string.
tributed to Magerman (1995), to percolate lexical
heads up the tree. Figure 3 shows the dependency
graph that results from this head percolation ap-
proach, where each link in the graph represents a de-
pendency relation from the modifier to the head. For
example, conventional head percolation rules spec-
ify the VP as the head of the S, so ?was?, as the head
of the VP, is thus the lexical head of the entire sen-
tence. The lexical heads of the other children of the
S node are called modifiers of the head of the S node;
thus, since ?she? is the head of the subject NP, there
is a dependency relation between ?she? and ?was?.
Lin (1996) argued for the use of this sort of depen-
dency structure to measure the difficulty in process-
ing, given the memory overhead of very long dis-
tance dependencies. Both Lin (1996) and Gibson
(1998) showed that human performance on sentence
processing tasks could be predicted with measures
of this sort. While details may differ ? e.g., how
to measure distance, what counts as a dependency ?
we can make use of the general approach given Tree-
bank style parses and head percolation, resulting in
graphs of the sort in Figure 3. For the current paper,
we count the distance between words for each de-
pendency link. For Figure 3, there are 7 dependency
links, a distance total of 11, and a mean of 147 .
3.4 Developmental level (D-Level)
D-Level defines eight levels of sentence complex-
ity, from 0-7, based on the development of complex
sentences in normal-development children. Each
level is defined by the presence of specific grammat-
ical constructions (Rosenberg and Abbeduto, 1987);
we follow Cheung and Kemper (1992) in assigning
scores equivalent to the defined level of complex-
ity. A score of zero corresponds to simple, single-
clause sentences; embedded infinitival clauses get
a score of 1 (She needs to pay the rent); conjoined
clauses (She worked all day and worried all night),
compound subjects (The woman and her four chil-
dren had not eaten for two days), and wh-predicate
complements score 2. Object noun phrase rela-
tive clauses or complements score 3 (The police
caught the man who robbed the woman), whereas
the same constructs in subject noun phrases score
5 (The woman who worked in the cafeteria was
robbed). Gerundive complements and comparatives
(They were hungrier than her) receive a score of 4;
subordinating conjunctions (if, before, as soon as)
score 6. Finally, a score of 7 is used as a catch-all
category for sentences containing more than one of
any of these grammatical constructions.
3.5 POS-tag sequence cross entropy
One possible approach for detecting rich syntactic
structure is to look for infrequent or surprising com-
binations of parts-of-speech (POS). We can measure
this over an utterance by building a simple bi-gram
model over POS tags, then measuring the cross en-
tropy of each utterance.4
Given a bi-gram model over POS-tags, we can
calculate the probability of the sequence as a whole.
Let ?i be the POS-tag of word wi in a sequence of
wordsw1 . . . wn, and assume that ?0 is a special start
symbol, and that ?n+1 is a special stop symbol. Then
the probability of the POS-tag sequence is
P(?1 . . . ?n) =
n+1?
i=1
P(?i | ?i?1) (1)
The cross entropy is then calculated as
H(?1 . . . ?n) = ?
1
n
log P(?1 . . . ?n) (2)
With this formulation, this basically boils down to
the mean negative log probability of each tag given
the previous tag.
4 Data
4.1 Subjects
We collected audio recordings of 55 neuropsycho-
logical examinations administered at the Layton Ag-
ing & Alzheimer?s Disease Center, an NIA-funded
Alzheimer?s center for research at OHSU. For this
study, we partitioned subjects into two groups: those
who were assigned a Clinical Dementia Rating
(CDR) of 0 (healthy) and those who were assigned
a CDR of 0.5 (Mild Cognitive Impairment; MCI).
The CDR (Morris, 1993) is assigned with access to
clinical and cognitive test information, independent
of performance on the battery of neuropsychologi-
cal tests used for this research study, and has been
shown to have high expert inter-annotator reliability
(Morris et al, 1997).
4For each test domain, we used cross-validation techniques
to build POS-tag bi-gram models and evaluate with them in that
domain.
4
CDR = 0 CDR = 0.5
(n=29) (n=18)
Measure M SD M SD t(45)
Age 88.1 9.0 91.9 4.4 ?1.65
Education (Y) 15.0 2.2 14.3 2.8 1.04
MMSE 28.4 1.4 25.9 2.6 4.29***
Word List (A) 20.0 4.0 15.4 3.3 4.06***
Word List (R) 6.8 2.0 3.9 1.7 5.12***
Wechsler LM I 17.2 4.0 10.9 4.2 5.20***
Wechsler LM II 15.8 4.3 9.5 5.4 4.45***
Cat.Fluency (A) 17.2 4.1 13.9 4.2 2.59*
Cat.Fluency (V) 12.8 4.5 9.6 3.6 2.57*
Digits (F) 6.6 1.4 6.1 1.2 1.11
Digits (B) 4.7 1.0 4.7 1.1 ?0.04
Table 1: Neuropsychological test results for subjects.
***p < 0.001; **p < 0.01 ; *p < 0.05
Of the collected recordings, three subjects were
recorded twice; for the current study only one
recording was used for each subject. Three subjects
were assigned a CDR of 1.0 and were excluded from
the study; two further subjects were excluded for er-
rors in the recording that resulted in missing audio.
Of the remaining 47 subjects, 29 had CDR = 0, and
18 had CDR = 0.5.
4.2 Neuropsychological tests
Table 1 presents means and standard deviations for
age, years of education and the manually-calculated
scores of a number of standard neuropsychological
tests that were administered during the recorded ses-
sion. These tests include: the Mini Mental State Ex-
amination (MMSE); the CERAD Word List Acqui-
sition (A) and Recall (R) tests; the Wechsler Logical
Memory (LM) I (immediate) and II (delayed) narra-
tive recall tests; Category Fluency, Animals (A) and
Vegetables (V); and Digit Span (WAIS-R) forward
(F) and backward (B).
The Wechsler Logical Memory I/II tests are the
basis of our study on syntactic complexity measures.
The original narrative is a short, 3 sentence story:
Anna Thompson of South Boston, employed as a cook in a
school cafeteria, reported at the police station that she had
been held up on State Street the night before and robbed of
fifty-six dollars. She had four small children, the rent was
due, and they had not eaten for two days. The police, touched
by the woman?s story, took up a collection for her.
Subjects are asked to re-tell this story immediately
after it is told to them (LM I), as well as after ap-
proximately 30 minutes of unrelated activities (LM
II). We transcribed each retelling, and manually an-
notated syntactic parse trees according to the Penn
Treebank annotation guidelines. Algorithms for au-
tomatically extracting syntactic complexity markers
from parse trees were written to accept either man-
System LR LP F-measure
Out-of-domain (WSJ) 77.7 80.1 78.9
Out-of-domain (SWBD) 84.0 86.2 85.1
Domain adapted from SWBD 87.9 88.3 88.1
Table 2: Parser accuracy on Wechsler Logical Memory re-
sponses using just out-of-domain data (either from the Wall St.
Journal (WSJ) or Switchboard (SWBD) treebanks) versus using
a domain adapted system.
ually annotated trees or trees output from an auto-
matic parser, to demonstrate the plausibility of using
automatically generated parse trees.
4.3 Parsing
For automatic parsing, we made use of the well-
known Charniak parser (Charniak, 2000). Following
best practices (Charniak and Johnson, 2001), we re-
moved sequences covered by EDITED nodes in the
tree from the strings prior to parsing. For this pa-
per, EDITED nodes were identified from the manual
parse, not automatically. Table 2 shows parsing ac-
curacy of our annotated retellings under three pars-
ing model training conditions: 1) trained on approx-
imately 1 million words of Wall St. Journal (WSJ)
text; 2) trained on approximately 1 million words
of Switchboard (SWBD) corpus telephone conver-
sations; and 3) using domain adaptation techniques
starting from the SWBD Treebank. The SWBD out-
of-domain system reaches quite respectable accura-
cies, and domain adaptation achieves 3 percent ab-
solute improvement over that.
For domain adaptation, we used MAP adapta-
tion techniques (Bacchiani et al, 2006) via cross-
validation over the entire set of retellings. For
each subject, we trained a model using the SWBD
treebank as the out-of-domain treebank, and the
retellings of the other 46 subjects as in-domain train-
ing. We used a count merging approach, with the
in-domain counts scaled by 1000 relative to the out-
of-domain counts. See Bacchiani et al (2006) for
more information on stochastic grammar adaptation
using these techniques.
5 Experimental results
5.1 Correlations
Our first set of experimental results regard correla-
tions between measures. Table 3 shows results for
five of our measures over all three treebanks that we
have been considering: Penn WSJ Treebank, Penn
SWBD Treebank, and the Wechsler LM retellings.
The correlations along the diagonal are between the
same measure when extracted from manually an-
notated trees and when extracted from automatic
5
Penn WSJ Treebank Penn SWBD Treebank Wechsler LM Retellings
(a) (b) (c) (d) (e) (a) (b) (c) (d) (e) (a) (b) (c) (d) (e)
(a) Frazier 0.89 0.96 0.94
(b) Yngve -0.31 0.96 -0.72 0.96 -0.69 0.95
(c) Tree nodes 0.91 -0.16 0.92 0.58 -0.06 0.93 0.93 -0.48 0.85
(d) Dep len -0.29 0.75 -0.13 0.93 -0.74 0.97 -0.08 0.96 -0.72 0.96 -0.51 0.96
(e) Cross Ent 0.17 0.18 0.15 0.19 0.93 -0.55 0.76 0.09 0.76 0.98 -0.13 0.45 0.05 0.41 0.97
Table 3: Correlation matrices for several measures on an utterance-by-utterance basis. Correlations along the diagonal are between
the manual measures and the measures when automatically parsed. All other correlations are between measures when derived from
manual parse trees.
parses. All other correlations are between mea-
sures derived from manual trees. All correlations
are taken per utterance.
From this table, we can see that all of the mea-
sures derived from automatic parses have a high
correlation with the manually derived measures, in-
dicating that they may preserve any discriminative
utility of these markers. Interestingly, the num-
ber of nodes in the tree per word tends to corre-
late well with the Frazier score, while the depen-
dency length tends to correlate well with the Yngve
score. Cross entropy correlates with Yngve and de-
pendency length for the SWBD and Wechsler tree-
banks, but not for the WSJ treebank.
5.2 Manually derived measures
Table 4 presents means and standard deviations
for measures derived from the LM I and LM II
retellings, along with the t-value and level of sig-
nificance. The first three measures presented in the
table are available without syntactic annotation: to-
tal number of words, total number of utterances, and
words per utterance in the retelling. None of these
three measures on either retelling show statistically
significant differences between the groups.
The first measure to rely upon syntactic annota-
tions is words per clause. The number of clauses are
automatically extracted from the parses by counting
the number of S nodes in the tree.5 Normalizing the
number of words by the number of clauses rather
than the number of utterances (as in words per ut-
terance) results in statistically significant differences
between the groups for LM I though not for LM II.
The other measures are as described in Section
3. Interestingly, Frazier score per word, the number
of tree nodes per word, and POS-tag cross entropy
all show a significant negative t-value on the LM I
retellings, meaning the CDR 0.5 subjects had sig-
nificantly higher scores than the CDR 0 subjects for
5For coordinated S nodes, the root of the coordination,
which in Penn Treebank style annotation also has an S label,
does not count as an additional clause.
these measures on this task. These measures showed
no significant difference on the LM II retellings.
The Yngve score per word and the dependency
length per word showed no significant difference on
LM I retellings but a statistically significant differ-
ence on LM II, with the expected outcome of higher
scores for the CDR 0 subjects. The D-Level measure
showed no significant differences.
5.3 Automatically derived measures
In addition to manual-parse derived measures, Table
4 also presents the same measures when automatic,
rather than manual, parses are used. Given the rela-
tively high quality of the automatic parses, most of
the means and standard deviations are quite close,
and all of the patterns observed in the upper half of
Table 4 are preserved, except that the Yngve score
per word no longer shows a statistically significant
difference for the LM II retelling.
5.4 Left-corner trees
For the tree-based complexity metrics (Frazier and
Yngve), we also investigated alternative imple-
mentations that make use of the left-corner trans-
formation (Rosenkrantz and Lewis II, 1970) of
the tree from which the measures were extracted.
This transformation is widely known for remov-
ing left-recursion from a context-free grammar, and
it changes the tree shape by transforming left-
branching structures into right-branching structures,
while leaving center-embedded structures center-
embedded. This property led Resnik (1992) to pro-
pose left-corner processing as a plausible mecha-
nism for human sentence processing, since it is pre-
cisely these center-embedded structures, and not the
left- or right-branching structures, that are problem-
atic for humans to process.
Table 5 presents results using either manually an-
notated trees or automatic parses to extract the Yn-
gve and Frazier measures after a left-corner trans-
form has been applied to the tree. The Frazier
scores are very similar to those without the left-
6
Logical Memory I Logical Memory II
CDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5
Measure M SD M SD t(45) M SD M SD t(45)
Total words in retelling 71.0 26.0 58.1 31.9 1.49 70.6 21.5 58.5 36.7 1.43
Total utterances in retelling 8.86 4.16 7.72 3.28 0.99 8.17 2.77 7.06 4.86 1.01
Words per utterance in retelling 8.57 2.44 7.78 3.67 0.89 9.16 3.06 7.82 4.76 1.18
Manually extracted: Words per clause 6.33 1.39 5.25 1.25 2.68* 6.12 1.20 5.48 3.37 0.93
Frazier score per word 1.19 0.09 1.26 0.11 ?2.68* 1.19 0.09 1.13 0.43 0.67
Tree nodes per word 1.96 0.07 2.01 0.10 ?2.08* 1.96 0.07 1.80 0.66 1.36
Yngve score per word 1.44 0.23 1.39 0.30 0.61 1.53 0.27 1.26 0.62 2.01*
Dependency length per word 1.54 0.25 1.47 0.27 0.90 1.63 0.30 1.34 0.60 2.19*
POS-tag Cross Entropy 1.83 0.16 1.96 0.26 ?2.18* 1.93 0.14 1.86 0.59 0.54
D-Level 1.07 0.75 1.03 1.23 0.14 1.23 0.81 1.68 1.41 ?1.42
Auto extracted: Words per clause 6.42 1.53 5.10 1.16 3.13** 6.04 1.25 5.61 3.67 0.59
Frazier score per word 1.16 0.10 1.24 0.10 ?2.92** 1.15 0.10 1.09 0.41 0.69
Tree nodes per word 1.96 0.07 2.03 0.10 ?2.55* 1.96 0.08 1.79 0.66 1.38
Yngve score per word 1.41 0.23 1.37 0.29 0.54 1.50 0.27 1.28 0.64 1.70
Dependency length per word 1.51 0.25 1.47 0.28 0.54 1.61 0.28 1.35 0.61 2.04*
POS-tag Cross Entropy 1.83 0.17 1.96 0.26 ?2.12* 1.92 0.14 1.86 0.58 0.53
D-Level 1.09 0.73 1.11 1.20 ?0.08 1.28 0.77 1.61 1.22 ?1.15
Table 4: Syntactic complexity measure group differences when measures are derived from either manual or automatic parse trees.
**p < 0.01 ; *p < 0.05
corner transform, while the Yngve scores are re-
duced across the board. With the left-corner trans-
formed tree, the automatically derived Yngve mea-
sure retains the statistically significant difference
shown by the manually derived measure.
6 Discussion and future directions
The results presented in the last section demonstrate
that NLP techniques applied to clinically elicited
spoken language samples can be used to automat-
ically derive measures that may be useful for dis-
criminating between healthy and MCI subjects. In
addition, we see that different measures show differ-
ent patterns when applied to these language samples,
with Frazier scores and tree nodes per word giving
quite different results than Yngve scores and depen-
dency length. It would thus appear that, for Penn
Treebank style annotations at least, these measures
are quite complementary.
There are two surprising aspects of these results:
the significantly higher means of three measures on
LM I samples for MCI subjects, and the fact that one
set of measures show significant differences on LM
I while another shows differences on LM II. We do
not have definitive explanations for these phenom-
ena, but we can speculate about why such results
were obtained.
First, there is an important difference between the
manner of elicitation for LM I versus LM II. LM I
is an immediate recall, so there will likely be, for
unimpaired subjects, much higher verbatim recall of
the story than in the delayed recall of LM II. For
the MCI group, which exhibits memory impairment,
there will be little in the way of verbatim recall, and
potentially much more in the way of spoken lan-
guage phenomena such as filled pauses, parenthet-
icals and off-topic utterances. This may account for
the higher Frazier score per word for the MCI group
on LM I. Such differences will likely be lessened in
the delayed recall.
Second, the Frazier and Yngve metrics differ in
how they score long, flat phrases, such as typical
base NPs. Consider the ternary NP in Figure 1. The
first word in that NP (?a?) receives an Yngve score
of 2, but a Frazier score of only 1 (Figure 2), while
the second word in the NP receives an Yngve score
of 1 and a Frazier score of 0. For a flat NP with
5 children, that difference would be 4 to 1 for the
first child, 3 to 0 for the second child, and so forth.
This difference in scoring relatively common syn-
tactic constructions, even those which may not affect
human memory load, may account for such different
scores achieved with these different measures.
In summary, we have demonstrated an important
clinical use for NLP techniques, where automatic
syntactic annotation provides sufficiently accurate
parse trees for use in automatic extraction of syntac-
tic complexity measures. Different syntactic com-
plexity measures appear to be measuring quite com-
plementary characteristics of the retellings, yielding
statistically significant differences from both imme-
diate and delayed retellings.
There are quite a number of questions that we will
7
Logical Memory I Logical Memory II
CDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5
Measure M SD M SD t(45) M SD M SD t(45)
Manually extracted: Left-corner Frazier 1.20 0.10 1.28 0.12 ?2.60* 1.20 0.11 1.18 0.45 0.29
Left-corner Yngve 1.33 0.20 1.25 0.23 1.20 1.37 0.21 1.14 0.52 2.14*
Auto extracted: Left-corner Frazier 1.16 0.10 1.27 0.13 ?3.02** 1.15 0.11 1.10 0.42 0.64
Left-corner Yngve 1.31 0.19 1.23 0.21 1.33 1.36 0.21 1.13 0.51 2.11*
Table 5: Syntactic complexity measure group differences when measures are derived from left-corner parse trees.
**p < 0.01 ; *p < 0.05
continue to pursue. Most importantly, we will con-
tinue to examine this data, to try to determine what
characteristics of the spoken language are leading to
the unexpected patterns in the results. In addition,
we will begin to explore composite measures, such
as differences in measures between LM I and LM II,
which promise to better capture some of the patterns
we have observed. Ultimately, we would like to
build classifiers making use of a range of measures
as features, although in order to demonstrate statisti-
cally significant differences between classifiers, we
will need much more data than we currently have.
Eventually, longitudinal tracking of subjects may be
the best application of such measures on clinically
elicited spoken language samples.
Acknowledgments
This research was supported in part by NSF Grant #IIS-
0447214 and pilot grants from the Oregon Center for Aging
& Technology (ORCATECH, NIH #1P30AG024978-01) and
the Oregon Partnership for Alzheimer?s Research. Also, the
third author of this paper was supported under an NSF Grad-
uate Research Fellowship. Any opinions, findings, conclusions
or recommendations expressed in this publication are those of
the authors and do not necessarily reflect the views of the NSF.
Thanks to Jeff Kaye, John-Paul Hosom, Jan van Santen, Tracy
Zitzelberger, Jessica Payne-Murphy and Robin Guariglia for
help with the project.
References
M. Bacchiani, M. Riley, B. Roark, and R. Sproat. 2006. MAP
adaptation of stochastic grammars. Computer Speech and
Language, 20(1):41?68.
E. Charniak and M. Johnson. 2001. Edit detection and parsing
for transcribed speech. In Proceedings of the 2nd Confer-
ence of the North American Chapter of the ACL.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the ACL, pages 132?139.
H. Cheung and S. Kemper. 1992. Competing complexity met-
rics and adults? production of complex sentences. Applied
Psycholinguistics, 13:53?76.
L. Frazier. 1985. Syntactic complexity. In D.R. Dowty,
L. Karttunen, and A.M. Zwicky, editors, Natural Language
Parsing. Cambridge University Press, Cambridge, UK.
E. Gibson. 1998. Linguistic complexity: locality of syntactic
dependencies. Cognition, 68(1):1?76.
S. Kemper, E. LaBarge, F.R. Ferraro, H. Cheung, H. Cheung,
and M. Storandt. 1993. On the preservation of syntax in
Alzheimer?s disease. Archives of Neurology, 50:81?86.
D. Lin. 1996. On structural complexity. In Proceedings of
COLING-96.
K. Lyons, S. Kemper, E. LaBarge, F.R. Ferraro, D. Balota, and
M. Storandt. 1994. Oral language and Alzheimer?s disease:
A reduction in syntactic complexity. Aging and Cognition,
1(4):271?281.
D.M. Magerman. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of the
ACL, pages 276?283.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
J.F. Miller and R.S. Chapman. 1981. The relation between
age and mean length of utterance in morphemes. Journal of
Speech and Hearing Research, 24:154?161.
J. Morris, C. Ernesto, K. Schafer, M. Coats, S. Leon, M. Sano,
L. Thal, and P. Woodbury. 1997. Clinical dementia
rating training and reliability in multicenter studies: The
Alzheimer?s disease cooperative study experience. Neurol-
ogy, 48(6):1508?1510.
J. Morris. 1993. The clinical dementia rating (CDR): Current
version and scoring rules. Neurology, 43:2412?2414.
P. Resnik. 1992. Left-corner parsing and psychological plausi-
bility. In Proceedings of COLING-92, pages 191?197.
B. Roark, J.P. Hosom, M. Mitchell, and J.A. Kaye. 2007. Au-
tomatically derived spoken language markers for detecting
mild cognitive impairment. In Proceedings of the 2nd Inter-
national Conference on Technology and Aging (ICTA).
S. Rosenberg and L. Abbeduto. 1987. Indicators of linguis-
tic competence in the peer group conversational behavior of
mildly retarded adults. Applied Psycholinguistics, 8:19?32.
S.J. Rosenkrantz and P.M. Lewis II. 1970. Deterministic left
corner parsing. In IEEE Conference Record of the 11th An-
nual Symposium on Switching and Automata, pages 139?
152.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Automatic
measurement of syntactic development in child langugage.
In Proceedings of the 43rd Annual Meeting of the ACL.
H.S. Scarborough. 1990. Index of productive syntax. Applied
Psycholinguistics, 11:1?22.
S. Singh, R.S. Bucks, and J.M. Cuerden. 2001. Evaluation of
an objective technique for analysing temporal variables in
DAT spontaneous speech. Aphasiology, 15(6):571?584.
V.H. Yngve. 1960. A model and an hypothesis for language
structure. Proceedings of the American Philosophical Soci-
ety, 104:444?466.
8
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 920?929,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimum Imputed Risk: Unsupervised Discriminative Training for
Machine Translation
Zhifei Li?
Google Research
Mountain View, CA 94043, USA
zhifei.work@gmail.com
Ziyuan Wang, Sanjeev Khudanpur
Johns Hopkins University
Baltimore, MD 21218, USA
zwang40,khudanpur@jhu.edu
Jason Eisner
Johns Hopkins University
Baltimore, MD 21218, USA
eisner@jhu.edu
Brian Roark
Oregon Health & Science University
Beaverton, Oregon 97006, USA
roark@cslu.ogi.edu
Abstract
Discriminative training for machine transla-
tion has been well studied in the recent past.
A limitation of the work to date is that it relies
on the availability of high-quality in-domain
bilingual text for supervised training. We
present an unsupervised discriminative train-
ing framework to incorporate the usually plen-
tiful target-language monolingual data by us-
ing a rough ?reverse? translation system. Intu-
itively, our method strives to ensure that prob-
abilistic ?round-trip? translation from a target-
language sentence to the source-language and
back will have low expected loss. Theoret-
ically, this may be justified as (discrimina-
tively) minimizing an imputed empirical risk.
Empirically, we demonstrate that augment-
ing supervised training with unsupervised data
improves translation performance over the su-
pervised case for both IWSLT and NIST tasks.
1 Introduction
Missing data is a common problem in statistics when
fitting the parameters ? of a model. A common strat-
egy is to attempt to impute, or ?fill in,? the missing
data (Little and Rubin, 1987), as typified by the EM
algorithm. In this paper we develop imputation tech-
niques when ? is to be trained discriminatively.
We focus on machine translation (MT) as our ex-
ample application. A Chinese-to-English machine
translation system is given a Chinese sentence x and
? Zhifei Li is currently working at Google Research, and
this work was done while he was a PHD student at Johns Hop-
kins University.
asked to predict its English translation y. This sys-
tem employs statistical models p?(y | x) whose pa-
rameters ? are discriminatively trained using bilin-
gual sentence pairs (x, y). But bilingual data for
such supervised training may be relatively scarce for
a particular language pair (e.g., Urdu-English), es-
pecially for some topics (e.g., technical manuals) or
genres (e.g., blogs). So systems seek to exploit ad-
ditional monolingual data, i.e., a corpus of English
sentences y with no corresponding source-language
sentences x, to improve estimation of ?. This is our
missing data scenario.1
Discriminative training of the parameters ? of
p?(y | x) using monolingual English data is a cu-
rious idea, since there is no Chinese input x to trans-
late. We propose an unsupervised training approach,
called minimum imputed risk training, which is con-
ceptually straightforward: First guess x (probabilis-
tically) from the observed y using a reverse English-
to-Chinese translation model p?(x | y). Then train
the discriminative Chinese-to-English model p?(y |
x) to do a good job at translating this imputed x
back to y, as measured by a given performance met-
ric. Intuitively, our method strives to ensure that
probabilistic ?round-trip? translation from a target-
language sentence to the source-language and back
again will have low expected loss.
Our approach can be applied in an application
scenario where we have (1) enough out-of-domain
bilingual data to build two baseline translation sys-
tems, with parameters ? for the forward direction,
and ? for the reverse direction; (2) a small amount
1Contrast this with traditional semi-supervised training that
looks to exploit ?unlabeled? inputs x, with missing outputs y.
920
of in-domain bilingual development data to discrim-
inatively tune a small number of parameters in ?;
and (3) a large amount of in-domain English mono-
lingual data.
The novelty here is to exploit (3) to discrimina-
tively tune the parameters ? of all translation model
components,2 p?(y|x) and p?(y), not merely train a
generative language model p?(y), as is the norm.
Following the theoretical development below, the
empirical effectiveness of our approach is demon-
strated by replacing a key supervised discriminative
training step in the development of large MT sys-
tems ? learning the log-linear combination of sev-
eral component model scores (viewed as features) to
optimize a performance metric (e.g. BLEU) on a set
of (x, y) pairs ? with our unsupervised discrimina-
tive training using only y. One may hence contrast
our approach with the traditional supervised meth-
ods applied to the MT task such as minimum error
rate training (Och, 2003; Macherey et al, 2008), the
averaged Perceptron (Liang et al, 2006), maximum
conditional likelihood (Blunsom et al, 2008), min-
imum risk (Smith and Eisner, 2006; Li and Eisner,
2009), and MIRA (Watanabe et al, 2007; Chiang et
al., 2009).
We perform experiments using the open-source
MT toolkit Joshua (Li et al, 2009a), and show that
adding unsupervised data to the traditional super-
vised training setup improves performance.
2 Supervised Discriminative Training via
Minimization of Empirical Risk
Let us first review discriminative training in the su-
pervised setting?as used in MERT (Och, 2003) and
subsequent work.
One wishes to tune the parameters ? of some
complex translation system ??(x). The function ??,
which translates Chinese x to English y = ??(x)
need not be probabilistic. For example, ? may be
the parameters of a scoring function used by ?, along
with pruning and decoding heuristics, for extracting
a high-scoring translation of x.
The goal of discriminative training is to mini-
mize the expected loss of ??(?), under a given task-
specific loss function L(y?, y) that measures how
2Note that the extra monolingual data is used only for tuning
the model weights, but not for inducing new phrases or rules.
bad it would be to output y? when the correct output
is y. For an MT system that is judged by the BLEU
metric (Papineni et al, 2001), for instance, L(y?, y)
may be the negated BLEU score of y? w.r.t. y. To be
precise, the goal3 is to find ? with low Bayes risk,
?? = argmin
?
?
x,y
p(x, y)L(??(x), y) (1)
where p(x, y) is the joint distribution of the input-
output pairs.4
The true p(x, y) is, of course, not known and,
in practice, one typically minimizes empirical risk
by replacing p(x, y) above with the empirical dis-
tribution p?(x, y) given by a supervised training set
{(xi, yi), i = 1, . . . , N}. Therefore,
?? = argmin
?
?
x,y
p?(x, y)L(??(x), y)
= argmin
?
1
N
N?
i=1
L(??(xi), yi). (2)
The search for ?? typically requires the use of nu-
merical methods and some regularization.5
3 Unsupervised Discriminative Training
with Missing Inputs
3.1 Minimization of Imputed Risk
We now turn to the unsupervised case, where we
have training examples {yi} but not their corre-
sponding inputs {xi}. We cannot compute the sum-
mand L(??(xi), yi) for such i in (2), since ??(xi)
requires to know xi. So we propose to replace
3This goal is different from the minimum risk training of
Li and Eisner (2009) in a subtle but important way. In both
cases, ?? minimizes risk or expected loss, but the expectation
is w.r.t. different distributions: the expectation in Li and Eisner
(2009) is under the conditional distribution p(y |x), while the
expectation in (1) is under the joint distribution p(x, y).
4In the terminology of statistical decision theory, p(x, y) is
a distribution over states of nature. We seek a decision rule
??(x) that will incur low expected loss on observations x that
are generated from unseen states of nature.
5To compensate for the shortcut of using the unsmoothed
empirical distribution rather than a posterior estimate of p(x, y)
(Minka, 2000), it is common to add a regularization term ||?||22
in the objective of (2). The regularization term can prevent over-
fitting to a training set that is not large enough to learn all pa-
rameters.
921
L(??(xi), yi) with the expectation
?
x
p?(x | yi)L(??(x), yi), (3)
where p?(? | ?) is a ?reverse prediction model? that
attempts to impute the missing xi data. We call the
resulting variant of (2) the minimization of imputed
empirical risk, and say that
?? = argmin
?
1
N
N?
i=1
?
x
p?(x | yi)L(??(x), yi) (4)
is the estimate with the minimum imputed risk6.
The minimum imputed risk objective of (4) could
be evaluated by brute force as follows.
1. For each unsupervised example yi, use the re-
verse prediction model p?(? | yi) to impute pos-
sible reverse translations Xi = {xi1, xi2, . . .},
and add each (xij , yi) pair, weighted by
p?(xij | yi) ? 1, to an imputed training set .
2. Perform the supervised training of (2) on the
imputed and weighted training data.
The second step means that we must use ?? to
forward-translate each imputed xij , evaluate the loss
of the translations y?ij against the corresponding true
translation yi, and choose the ? that minimizes the
weighted sum of these losses (i.e., the empirical risk
when the empirical distribution p?(x, y) is derived
from the imputed training set). Specific to our MT
task, this tries to ensure that probabilistic ?round-
trip? translation, from the target-language sentence
yi to the source-language and back again, will have
a low expected loss.7
The trouble with this method is that the reverse
model p? generates a weighted lattice or hyper-
graph Xi encoding exponentially many translations
of yi, and it is computationally infeasible to forward-
translate each xij ? Xi. We therefore investigate
several approximations to (4) in Section 3.4.
6One may exploit both supervised data {(xi, yi)} and unsu-
pervised data {yj} to perform semi-supervised training via an
interpolation of (2) and (4). We will do so in our experiments.
7Our approach may be applied to other tasks as well. For
example, in a speech recognition task, ?? is a speech recognizer
that produces text, whereas p? is a speech synthesizer that must
produce a distribution over audio (or at least over acoustic fea-
tures or phone sequences) (Huang et al, 2010).
3.2 The Reverse Prediction Model p?
A crucial ingredient in (4) is the reverse prediction
model p?(?|?) that attempts to impute the missing xi.
We will train this model in advance, doing the best
job we can from available data, including any out-
of-domain bilingual data as well as any in-domain
monolingual data8 x.
In the MT setting, ?? and p? may have similar pa-
rameterization. One translates Chinese to English;
the other translates English to Chinese.
Yet the setup is not quite symmetric. Whereas ??
is a translation system that aims to produce a single,
low-loss translation, the reverse version p? is rather
a probabilistic model. It is supposed to give an accu-
rate probability distribution over possible values xij
of the missing input sentence xi. All of these val-
ues are taken into account in (4), regardless of the
loss that they would incur if they were evaluated for
translation quality relative to the missing xi.
Thus, ? does not need to be trained to minimize
the risk itself (so there is no circularity). Ideally,
it should be trained to match the underlying condi-
tional distribution of x given y, by achieving a low
conditional cross-entropy
H(X |Y ) = ?
?
x,y
p(x, y) log p?(x | y). (5)
In practice, ? is trained by (empirically) minimiz-
ing ? 1M
?N
j=1 log p?(xj | yj) + 12?2 ???22 on some
bilingual data, with the regularization coefficient ?2
tuned on held out data.
It may be tolerable for p? to impute mediocre
translations xij . All that is necessary is that the (for-
ward) translations generated from the imputed xij
?simulate? the competing hypotheses that we would
see when translating the correct Chinese input xi.
3.3 The Forward Translation System ?? and
The Loss Function L(??(xi), yi)
The minimum empirical risk objective of (2) is
quite general and various popular supervised train-
ing methods (Lafferty et al, 2001; Collins, 2002;
Och, 2003; Crammer et al, 2006; Smith and Eisner,
8In a translation task from x to y, one usually does not make
use of in-domain monolingual data x. But we can exploit x to
train a language model p?(x) for the reverse translation system,
which will make the imputed xij look like true Chinese inputs.
922
2006) can be formalized in this framework by choos-
ing different functions for ?? and L(??(xi), yi). The
generality of (2) extends to our minimum imputed
risk objective of (4). Below, we specify the ?? and
L(??(xi), yi) we considered in our investigation.
3.3.1 Deterministic Decoding
A simple translation rule would define
??(x) = argmax
y
p?(y |x) (6)
If this ??(x) is used together with a loss function
L(??(xi), yi) that is the negated BLEU score9, our
minimum imputed risk objective of (4) is equivalent
to MERT (Och, 2003) on the imputed training data.
However, this would not yield a differentiable ob-
jective function. Infinitesimal changes to ? could re-
sult in discrete changes to the winning output string
??(x) in (6), and hence to the loss L(??(x), yi). Och
(2003) developed a specialized line search to per-
form the optimization, which is not scalable when
the number of model parameters ? is large.
3.3.2 Randomized Decoding
Instead of using the argmax of (6), we assume
during training that ??(x) is itself random, i.e. the
MT system randomly outputs a translation y with
probability p?(y |x). As a result, we will modify
our objective function of (4) to take yet another ex-
pectation over the unknown y. Specifically, we will
replace L(??(x), yi) in (4) with
?
y
p?(y |x)L(y, yi). (7)
Now, the minimum imputed empirical risk objective
of (4) becomes
?? = argmin
?
1
N
N?
i=1
?
x,y
p?(x | yi) p?(y |x)L(y, yi)
(8)
If the loss function L(y, yi) is a negated BLEU, this
is equivalent to performing minimum-risk training
described by (Smith and Eisner, 2006; Li and Eisner,
2009) on the imputed data.10
9One can manipulate the loss function to support other
methods that use deterministic decoding, such as Perceptron
(Collins, 2002) and MIRA (Crammer et al, 2006).
10Again, one may manipulate the loss function to support
other probabilistic methods that use randomized decoding, such
as CRFs (Lafferty et al, 2001).
The objective function in (8) is now differentiable,
since each coefficient p?(y |x) is a differentiable
function of ?, and thus amenable to optimization
by gradient-based methods; we use the L-BFGS al-
gorithm (Liu et al, 1989) in our experiments. We
perform experiments with the syntax-based MT sys-
tem Joshua (Li et al, 2009a), which implements
dynamic programming algorithms for second-order
expectation semirings (Li and Eisner, 2009) to effi-
ciently compute the gradients needed for optimizing
(8).
3.4 Approximating p?(x | yi)
As mentioned at the end of Section 3.1, it is com-
putationally infeasible to forward-translate each of
the imputed reverse translations xij . We propose
four approximations that are computationally feasi-
ble. Each may be regarded as a different approxima-
tion of p?(x | yi) in equations (4) or (8).
k-best. For each yi, add to the imputed training set
only the k most probable translations {xi1, . . . xik}
according to p?(x | yi). (These can be extracted
from Xi using standard algorithms (Huang and Chi-
ang, 2005).) Rescale their probabilities to sum to 1.
Sampling. For each yi, add to the training set k in-
dependent samples {xi1, . . . xik} from the distribu-
tion p?(x | yi), each with weight 1/k. (These can be
sampled from Xi using standard algorithms (John-
son et al, 2007).) This method is known in the liter-
ature as multiple imputation (Rubin, 1987).
Lattice. 11 Under certain special cases it is be pos-
sible to compute the expected loss in (3) exactly
via dynamic programming. Although Xi does con-
tain exponentially many translations, it may use a
?packed? representation in which these translations
share structure. This representation may further-
more enable sharing work in forward-translation, so
as to efficiently translate the entire set Xi and ob-
tain a distribution over translations y. Finally, the
expected loss under that distribution, as required by
equation (3), may also be efficiently computable.
All this turns out to be possible if (a) the poste-
rior distribution p?(x | yi) is represented by an un-
11The lattice approximation is presented here as a theoreti-
cal contribution, and we do not empirically evaluate it since its
implementation requires extensive engineering effort that is be-
yond the main scope of this paper.
923
ambiguous weighted finite-state automaton Xi, (b)
the forward translation system ?? is structured in a
certain way as a weighted synchronous context-free
grammar, and (c) the loss function decomposes in a
certain way. We omit the details of the construction
as beyond the scope of this paper.
In our experimental setting described below, (b) is
true (using Joshua), and (c) is true (since we use a
loss function presented by Tromble et al (2008) that
is an approximation to BLEU and is decomposable).
While (a) is not true in our setting because Xi is a
hypergraph (which is ambiguous), Li et al (2009b)
show how to approximate a hypergraph representa-
tion of p?(x | yi) by an unambiguous WFSA. One
could then apply the construction to this WFSA12,
obtaining an approximation to (3).
Rule-level Composition. Intuitively, the reason
why the structure-sharing in the hypergraphXi (gen-
erated by the reverse system) cannot be exploited
during forward translating is that when the forward
Hiero system translates a string xi ? Xi, it must
parse it into recursive phrases.
But the structure-sharing within the hypergraph of
Xi has already parsed xi into recursive phrases, in a
way determined by the reverse Hiero system; each
translation phrase (or rule) corresponding to a hy-
peredge. To exploit structure-sharing, we can use
a forward translation system that decomposes ac-
cording to that existing parse of xi. We can do that
by considering only forward translations that respect
the hypergraph structure of Xi. The simplest way to
do this is to require complete isomorphism of the
SCFG trees used for the reverse and forward trans-
lations. In other words, this does round-trip impu-
tation (i.e., from y to x, and then to y?) at the rule
level. This is essentially the approach taken by Li et
al. (2010).
3.5 The Log-Linear Model p?
We have not yet specified the form of p?. Following
much work in MT, we begin with a linear model
score(x, y) = ? ? f(x, y) =
?
k
?kfk(x, y) (9)
where f(x, y) is a feature vector indexed by k. Our
deterministic test-time translation system ?? simply
12Note that the forward translation of a WFSA is tractable by
using a lattice-based decoder such as that by Dyer et al (2008).
outputs the highest-scoring y for fixed x. At training
time, our randomized decoder (Section 3.3.2) uses
the Boltzmann distribution (here a log-linear model)
p?(y |x) =
e??score(x,y)
Z(x) =
e??score(x,y)?
y? e??score(x,y
?) (10)
The scaling factor ? controls the sharpness of the
training-time distribution, i.e., the degree to which
the randomized decoder favors the highest-scoring
y. For large ?, our training objective approaches
the imputed risk of the deterministic test-time sys-
tem while remaining differentiable.
In a task like MT, in addition to the input x and
output y, we often need to introduce a latent variable
d to represent the hidden derivation that relates x to
y. A derivation d represents a particular phrase seg-
mentation in a phrase-based MT system (Koehn et
al., 2003) and a derivation tree in a typical syntax-
based system (Galley et al, 2006; Chiang, 2007).
We change our model to assign scores not to an
(x, y) pair but to the detailed derivation d; in partic-
ular, now the function f that extracts a feature vector
can look at all of d. We replace y by d in (9)?(10),
and finally define p?(y|x) by marginalizing out d,
p?(y |x) =
?
d?D(x,y)
p?(d |x) (11)
where D(x, y) represents the set of derivations that
yield x and y.
4 Minimum Imputed Risk vs. EM
The notion of imputing missing data is familiar
from other settings (Little and Rubin, 1987), particu-
larly the expectation maximization (EM) algorithm,
a widely used generative approach. So it is instruc-
tive to compare EM with minimum imputed risk.
One can estimate ? by maximizing the log-
likelihood of the data {(xi, yi), i = 1, . . . , N} as
argmax
?
1
N
N?
i=1
log p?(xi, yi). (12)
If the xi?s are missing, EM tries to iteratively maxi-
mize the marginal probability:
argmax
?
1
N
N?
i=1
log
?
x
p?(x, yi). (13)
924
The E-step of each iteration comprises comput-
ing ?x p?t(x | yi) log p?(x, yi), the expected log-
likelihood of the complete data, where p?t(x | yi) is
the conditional part of p?t(x, yi) under the current
iterate ?t, and the M-step comprises maximizing it:
?t+1 = argmax
?
1
N
N?
i=1
?
x
p?t(x | yi) log p?(x, yi).
(14)
Notice that if we replace p?t(x|yi) with p?(x | yi)
in the equation above, and admit negated log-
likelihood as a loss function, then the EM update
(14) becomes identical to (4). In other words, the
minimum imputed risk approach of Section 3.1 dif-
fers from EM in (i) using an externally-provided and
static p?, instead of refining it at each iteration based
on the current p?t , and (ii) using a specific loss func-
tion, namely negated log-likelihood.
So why not simply use the maximum-likelihood
(EM) training procedure for MT? One reason is
that it is not discriminative: the loss function (e.g.
negated BLEU) is ignored during training.
A second reason is that training good joint models
p?(x, y) is computationally expensive. Contempo-
rary MT makes heavy use of log-linear probability
models, which allow the system designer to inject
phrase tables, linguistic intuitions, or prior knowl-
edge through a careful choice of features. Comput-
ing the objective function of (14) in closed form is
difficult if p? is an arbitrary log-linear model, be-
cause the joint probability p?(xi, yi) is then defined
as a ratio whose denominatorZ? involves a sum over
all possible sentence pairs (x, y) of any length.
By contrast, our discriminative framework will
only require us to work with conditional models.
While conditional probabilities such as p?(x | y) and
p?(y |x) are also ratios, computing their denomina-
tors only requires us to sum over a packed forest of
possible translations of a given y or x.13
In summary, EM would impute missing data us-
ing p?(x | y) and predict outputs using p?(y |x),
both being conditional forms of the same joint
model p?(x, y). Our minimum imputed risk train-
ing method is similar, but it instead uses a pair of
13Analogously, discriminative CRFs have become more pop-
ular than generative HMMs because they permit efficient train-
ing even with a wide variety of log-linear features (Lafferty et
al., 2001).
separately parameterized, separately trained mod-
els p?(x | y) and p?(y |x). By sticking to condi-
tional models, we can efficiently use more sophis-
ticated model features, and we can incorporate the
loss function when we train ?, which should improve
both efficiency and accuracy at test time.
5 Experimental Results
We report results on Chinese-to-English translation
tasks using Joshua (Li et al, 2009a), an open-source
implementation of Hiero (Chiang, 2007).
5.1 Baseline Systems
5.1.1 IWSLT Task
We train both reverse and forward baseline sys-
tems. The translation models are built using the cor-
pus for the IWSLT 2005 Chinese to English trans-
lation task (Eck and Hori, 2005), which comprises
40,000 pairs of transcribed utterances in the travel
domain. We use a 5-gram language model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998), trained on the English (resp. Chi-
nese) side of the bitext. We use a standard train-
ing pipeline and pruning settings recommended by
(Chiang, 2007).
5.1.2 NIST Task
For the NIST task, the TM is trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora dis-
tributed by LDC for the NIST MT evaluation using a
sampling method implemented in Joshua. We also
used a 5-gram language model, trained on a data set
consisting of a 130M words in English Gigaword
(LDC2007T07) and the bitext?s English side.
5.2 Feature Functions
We use two classes of features fk for discriminative
training of p? as defined in (9).
5.2.1 Regular Hiero Features
We include ten features that are standard in Hi-
ero (Chiang, 2007). In particular, these include
one baseline language model feature, three baseline
translation models, one word penalty feature, three
features to count how many rules with an arity of
925
zero/one/two are used in a derivation, and two fea-
tures to count how many times the unary and binary
glue rules in Hiero are used in a derivation.
5.2.2 Target-rule Bigram Features
In this paper, we do not attempt to discrimina-
tively tune a separate parameter for each bilingual
rule in the Hiero grammar. Instead, we train several
hundred features that generalize across these rules.
For each bilingual rule, we extract bigram fea-
tures over the target-side symbols (including non-
terminals and terminals). For example, if a bilingual
rule?s target-side is ?on the X1 issue of X2? where
X1 and X2 are non-terminals (with a position in-
dex), we extract the bigram features on the, the X ,
X issue, issue of, and of X . (Note that the posi-
tion index of a non-terminal is ignored in the fea-
ture.) Moreover, for the terminal symbols, we will
use their dominant POS tags (instead of the sym-
bol itself). For example, the feature the X becomes
DTX . We use 541 such bigram features for IWSLT
task (and 1023 such features for NIST task) that fire
frequently.
5.3 Data Sets for Discriminative Training
5.3.1 IWSLT Task
In addition to the 40,000 sentence pairs used to
train the baseline generative models (which are used
to compute the features fk), we use three bilingual
data sets listed in Table 1, also from IWSLT, for dis-
criminative training: one to train the reverse model
p? (which uses only the 10 standard Hiero features
as described in Section 5.2.1),14 one to train the for-
ward model ?? (which uses both classes of features
described in Section 5.2, i.e., 551 features in total),
and one for test.
Note that the reverse model ? is always trained us-
ing the supervised data of Dev ?, while the forward
model ? may be trained in a supervised or semi-
supervised manner, as we will show below.
In all three data sets, each Chinese sentence xi
has 16 English reference translations, so each yi is
actually a set of 16 translations. When we impute
data from yi (in the semi-supervised scenario), we
14Ideally, we should train ? to minimize the conditional
cross-entropy (5) as suggested in section 3.2. In the present
results, we trained ? discriminatively to minimize risk, purely
for ease of implementation using well versed steps.
Data set Purpose # of sentencesChinese English
Dev ? training ? 503 503?16
Dev ? training ? 503? 503?16
Eval ? testing 506 506?16
Table 1: IWSLT Data sets used for discriminative
training/test. Dev ? is used for discriminatively training
of the reverse model ?, Dev ? is for the forward model,
and Eval ? is for testing. The star ? for Dev ? empha-
sizes that some of its Chinese side will not be used in the
training (see Table 2 for details).
actually impute 16 different values of xi, by using
p? to separately reverse translate each sentence in
yi. This effectively adds 16 pairs of the form (xi, yi)
to the training set (see section 3.4), where each xi
is a different input sentence (imputed) in each case,
but yi is always the original set of 16 references.
5.3.2 NIST Task
For the NIST task, we use MT03 set (having 919
sentences) to tune the component parameters in both
the forward and reverse baseline systems. Addition-
ally, we use the English side of MT04 (having 1788
sentences) to perform semi-supervised tuning of the
forward model. The test sets are MT05 and MT06
(having 1082 and 1099 sentences, respectively). In
all the data sets, each source sentence has four refer-
ence translations.
5.4 Main Results
We compare two training scenarios: supervised and
semi-supervised. The supervised system (?Sup?)
carries out discriminative training on a bilingual data
set. The semi-supervised system (?+Unsup?) addi-
tionally uses some monolingual English text for dis-
criminative training (where we impute one Chinese
translation per English sentence).
Tables 2 and 3 report the results for the two tasks
under two training scenarios. Clearly, adding unsu-
pervised data improves over the supervised case, by
at least 1.3 BLEU points in IWSLT and 0.5 BLEU in
NIST.
5.5 Results for Analysis Purposes
Below, we will present more results on the IWSLT
data set to help us understand the behavior of the
926
Training scenario Test BLEU
Sup, (200, 200?16) 47.6
+Unsup, 101?16 Eng sentences 49.0
+Unsup, 202?16 Eng sentences 48.9
+Unsup, 303?16 Eng sentences 49.7?
Table 2: BLEU scores for semi-supervised training for
IWSLT task. The supervised system (?Sup?) is trained
on a subset of Dev ? containing 200 Chinese sentences
and 200?16 English translations. ?+Unsup? means that
we include additional (monolingual) English sentences
from Dev ? for semi-supervised training; for each En-
glish sentence, we impute the 1-best Chinese translation.
A star ? indicates a result that is signicantly better than
the ?Sup? baseline (paired permutation test, p < 0.05).
Training scenario Test BLEUMT05 MT06
Sup, (919, 919?4) 32.4 30.6
+Unsup, 1788 Eng sentences 33.0? 31.1?
Table 3: BLEU scores for semi-supervised training for
NIST task. The ?Sup? system is trained on MT03, while
the ?+Unsup? system is trained with additional 1788 En-
glish sentences from MT04. (Note that while MT04 has
1788?4 English sentences as it has four sets of refer-
ences, we only use one such set, for computational ef-
ficiency of discriminative training.) A star ? indicates a
result that is signicantly better than the ?Sup? baseline
(paired permutation test, p < 0.05).
methods proposed in this paper.
5.5.1 Imputation with Different Reverse
Models
A critical component of our unsupervised method
is the reverse translation model p?(x | y). We
wonder how the performance of our unsupervised
method changes when the quality of the reverse sys-
tem varies. To study this question, we used two dif-
ferent reverse translation systems, one with a lan-
guage model trained on the Chinese side of the bi-
text (?WLM?), and the other one without using such
a Chinese LM (?NLM?). Table 4 (in the fully unsu-
pervised case) shows that the imputed Chinese trans-
lations have a far lower BLEU score without the lan-
guage model,15 and that this costs us about 1 English
15The BLEU scores are low even with the language model
because only one Chinese reference is available for scoring.
Data size Imputed-CN BLEU Test-EN BLEUWLM NLM WLM NLM
101 11.8 3.0 48.5 46.7
202 11.7 3.2 48.9 47.6
303 13.4 3.5 48.8 47.9
Table 4: BLEU scores for unsupervised training
with/without using a language model in the reverse
system. A data size of 101 means that we use only
the English sentences from a subset of Dev ? containing
101 Chinese sentences and 101?16 English translations;
for each English sentence we impute the 1-best Chinese
translation. ?WLM? means a Chinese language model
is used in the reverse system, while ?NLM? means no
Chinese language model is used. In addition to reporting
the BLEU score on Eval ?, we also report ?Imputed-CN
BLEU?, the BLEU score of the imputed Chinese sentences
against their corresponding Chinese reference sentences.
BLEU point in the forward translations. Still, even
with the worse imputation (in the case of ?NLM?),
our forward translations improve as we add more
monolingual data.
5.5.2 Imputation with Different k-best Sizes
In all the experiments so far, we used the reverse
translation system to impute only a single Chinese
translation for each English monolingual sentence.
This is the 1-best approximation of section 3.4.
Table 5 shows (in the fully unsupervised case)
that the performance does not change much as k in-
creases.16 This may be because that the 5-best sen-
tences are likely to be quite similar to one another
(May and Knight, 2006). Imputing a longer k-best
list, a sample, or a lattice for xi (see section 3.4)
might achieve more diversity in the training inputs,
which might make the system more robust.
6 Conclusions
In this paper, we present an unsupervised discrimi-
native training method that works with missing in-
puts. The key idea in our method is to use a re-
verse model to impute the missing input from the ob-
served output. The training will then forward trans-
late the imputed input, and choose the parameters of
the forward model such that the imputed risk (i.e.,
16In the present experiments, however, we simply weighted
all k imputed translations equally, rather than in proportion to
their posterior probabilities as suggested in Section 3.4.
927
Training scenario Test BLEU
Unsup, k=1 48.5
Unsup, k=2 48.4
Unsup, k=3 48.9
Unsup, k=4 48.5
Unsup, k=5 48.4
Table 5: BLEU scores for unsupervised training with
different k-best sizes. We use 101?16 monolingual En-
glish sentences, and for each English sentence we impute
the k-best Chinese translations using the reverse system.
the expected loss of the forward translations with
respect to the observed output) is minimized. This
matches the intuition that the probabilistic ?round-
trip? translation from the target-language sentence
to the source-language and back should have low ex-
pected loss.
We applied our method to two Chinese to English
machine translation tasks (i.e. IWSLT and NIST).
We showed that augmenting supervised data with
unsupervised data improved performance over the
supervised case (for both tasks).
Our discriminative model used only a small
amount of training data and relatively few features.
In future work, we plan to test our method in settings
where there are large amounts of monolingual train-
ing data (enabling many discriminative features).
Also, our experiments here were performed on a lan-
guage pair (i.e., Chinese to English) that has quite
rich bilingual resources in the domain of the test
data. In future work, we plan to consider low-
resource test domains and language pairs like Urdu-
English, where bilingual data for novel domains is
sparse.
Acknowledgements
This work was partially supported by NSF Grants
No IIS-0963898 and No IIS-0964102 and the
DARPA GALE Program. The authors thank Markus
Dreyer, Damianos Karakos and Jason Smith for in-
sightful discussions.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In ACL,
pages 1012?1020.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In IWSLT.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53?64.
Jui-Ting Huang, Xiao Li, and Alex Acero. 2010. Dis-
criminative training methods for language models us-
ing conditional entropy criteria. In ICASSP.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In NAACL, pages 139?146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL,
pages 48?54.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40?51.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar. Zaidan. 2009a.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In WMT09, pages 26?30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and Jason
Eisner. 2010. Unsupervised discriminative language
928
model training for machine translation using simulated
confusion sets. In COLING, pages 556?664.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL, pages 761?
768.
R. J. A. Little and D. B. Rubin. 1987. Statistical Analysis
with Missing Data. J. Wiley & Sons, New York.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the
limited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503?528.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725?734.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Thomas Minka. 2000. Empirical risk minimization is
an incomplete inductive principle. In MIT Media Lab
note.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
D. B. Rubin. 1987. Multiple Imputation for Nonresponse
in Surveys. J. Wiley & Sons, New York.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In ACL,
pages 787?794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum-Bayes-risk de-
coding for statistical machine translation. In EMNLP,
pages 620?629.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764?773.
929
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1584?1589,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Pair Language Models for Deriving Alternative
Pronunciations and Spellings from Pronunciation Dictionaries
Russell Beckley
Oregon Health and Science University
beckleyr@ohsu.com
Brian Roark
Google Inc.
roarkbr@gmail.com
Abstract
Pronunciation dictionaries provide a readily
available parallel corpus for learning to trans-
duce between character strings and phoneme
strings or vice versa. Translation models can
be used to derive character-level paraphrases
on either side of this transduction, allowing
for the automatic derivation of alternative pro-
nunciations or spellings. We examine finite-
state and SMT-based methods for these related
tasks, and demonstrate that the tasks have
different characteristics ? finding alternative
spellings is harder than alternative pronunci-
ations and benefits from round-trip algorithms
when the other does not. We also show that
we can increase accuracy by modeling sylla-
ble stress.
1 Introduction
Robust processing of speech and language requires
dealing with variation in language production, ei-
ther in terms of pronunciation in the spoken domain
or spelling in the written domain. Predicting the
intended words of an acoustic or textual sequence
is an important recognition task, often required for
downstream processing such as spoken language un-
derstanding or knowledge extraction. Informal text
genres, such as those found in social media, share
some characteristics with speech; in fact such text is
often informed by pronunciation variation. For ex-
ample, consider the following tweet:
He aint gotta question my loyalty, cuz he knw
wen sh!t get real. Ill be right here!
where several tokens (e.g. ?cuz?, ?wen?) represent
spelling alternations related to pronunciation. Work
in text normalization and spelling correction ? e.g.,
Toutanova and Moore (2002); Li and Liu (2012) ?
has included pronunciation information to improve
recognition of the intended word, via grapheme to
phoneme (g2p) conversion modeling derived from
pronunciation dictionaries.
Pronunciation dictionaries provide natural par-
allel corpora, with strings of characters paired to
strings of phones. Thus, standard lexicons have
been used in recent years with machine transla-
tion systems such as Moses (Koehn et al, 2007),
to train g2p systems (Laurent et al, 2009; Gerosa
and Federico, 2009). Further, other algorithms us-
ing such dictionaries also use translation phrase
tables, but not for translation tasks. For exam-
ple, data-driven paraphrasing methods (Bannard and
Callison-Burch, 2005) use translation phrase-tables
as a ?pivot? to learn sets of phrases which trans-
lated to the same target phrase. In a similar manner,
with a pronunciation dictionary instead of a phrse-
table, pivoting can be used to learn alternative pro-
nunciations (Karanasou and Lamel, 2010), i.e., di-
rect phoneme-to-phoneme (p2p) ?translation? sys-
tems that yield alternative pronunciations. Alterna-
tively, round-trip translation could be used, e.g., to
map from letter strings to phone strings in one step,
then from the resulting phone strings to letter strings
in a second step, as the means to find alternative
spellings (Li and Liu, 2012).
In this study, we explore dictionary-derived mod-
els to find either alternative pronunciations or alter-
native spellings, using either direct (p2p or g2g) or
round-trip algorithms (p2g2p or g2p2g). We com-
pare methods based on weighted finite-state trans-
ducers (WFST) with phrase-based models trained
with Moses. Our main interest is to evaluate Karana-
sou and Lamel (2010) methods ? shown to be useful
for deriving alternative pronunciations ? for deriv-
ing alternative spellings, and thus to determine the
relative difficulty of these two tasks. We also exam-
ine when, if ever, round-trip processing yields ben-
efits over direct transduction. Our results indicate
that real alternative pronunciations are substantially
easier to find than real alternative spellings, partic-
1584
ularly when pronunciation features such as syllable
stress are available. Second, round trip translation
yields no gain (and some loss) over direct transduc-
tion for finding alternative pronunciations, yet yields
some modest gains for finding alternative spellings.
Further, WFST methods perform as well as or bet-
ter than Moses trained models. Finally, combining
the methods yields further gains, indicating that the
models are learning complementary sets of patterns.
The primary contribution of this work is to in-
troduce a competitive method of building and us-
ing pair language model WFSTs for generating al-
ternative spellings and pronunciations which reflect
real-world variability. This could improve results for
downstream processes, e.g., epidemiological studies
(Chew and Eysenbach, 2010) or sentiment analysis
(Barbosa and Feng, 2010) derived from social me-
dia text. Further, we present a controlled compari-
son between the two tasks, and demonstrate that they
differ in terms of task difficulty
2 Related work
Text normalization has been a major focus in text-
to-speech (TTS) research for many years. Notably,
Sproat et al (2001) deemed it a problem in itself,
rather than ad hoc preparatory work, and defined
many of the issues involved, as well as offering a va-
riety of initial solutions. Similar approaches apply to
automatic spelling correction, where Toutanova and
Moore (2002) extended the noisy channel spelling
correction method of Brill and Moore (2000), by
modeling pronunciation alternations to infer from
misspellings to correct spellings. Similarly, Li and
Liu (2012) extended the character-based translation
approach to text normalization of Pennell and Liu
(2011), by adding an additional round-trip trans-
lation to-and-from pronunciations. Karanasou and
Lamel (2010) used Moses to generate alternative
pronunciations from an English dictionary, using
both direct and round-trip methods. They validated
their systems on a set of words with multiple pro-
nunciations, measuring the degree to which alterna-
tive pronunciations are generated from one of the
given pronunciations. Our task and method of eval-
uation is similar to theirs, though we also look at
alternative spellings.
3 Methods
To generate alternative spellings and pronunciations,
we built phrase-based translation and finite-state
transduction models from a parallel corpus. When
pronunciations were part of the model ? i.e., not
direct grapheme-to-grapheme ? we included condi-
tions with and without vowel stress.
3.1 Corpus
Our training corpus is the CMU Pronouncing Dictio-
nary1, which contains nearly 130k entries. From this
corpus, we identified homophone sets, i.e., sets of
multiple spellings sharing the same pronunciation,
such as ?colonel? and ?kernel?. We found 9,977
such sets, and randomly selected 1000 for testing;
the rest we used for training. Each set had, on aver-
age, 2.46 members. We also identified homograph
sets, i.e., sets of multiple pronunciations all spelled
the same, such as potato (/potato/ and /p@teto/). We
found 8,216 such homograph sets, and randomly se-
lected 1000 for testing; the rest we used for training.
These sets averaged 2.13 members.
We construct seven parallel training corpora from
the lexicon, each disjoint from its relevant test
set. For round-trip models, the parallel corpus is
each grapheme string in the lexicon aligned with
its phoneme string, if neither the grapheme string
nor phoneme string appear in the test set. There
are four such corpora, corresponding to these op-
tions: stress or no stress, and g2p2g or p2g2p. The
g2p2g and p2g2g conditions require different cor-
pora because they are differently partitioned for test-
ing. For direct grapheme-to-grapheme training sets,
non-homophone words are self-aligned; for homo-
phones, from each homophone set, each possible
pair of spellings are aligned. For example, for a
pronunciation with four spellings?a, b, c, and d?
there would be six alignments: a:b, a:c, a:d, b:c, b:d,
c:d. Similarly for direct phoneme-to-phoneme train-
ing sets, non-homograph words are self-aligned;
words from the training homograph sets are pairwise
aligned in all pairings. There are two direct p2p cor-
pora: with and without stress.
3.2 Phrase-based translation models
As a baseline system, we used the Moses statisti-
cal machine translation package (Koehn et al, 2007)
to build grapheme-based and phoneme-based trans-
lation systems, using a bigram language model.2
These are trained on the parallel corpus resulting
from the homophone or homograph sets detailed in
1http://www.speech.cs.cmu.edu/cgi-bin/cmudict
2Higher order language models yielded no improvements.
1585
the previous section for the direct methods. For this
paper, we did not perform round-trip translation with
Moses, rather present it as a baseline for the direct
approach.
3.3 Pair language models
Our weighted finite-state transducer approach is
based on pair language models (Bisani and Ney,
2008; Deligne and Bimbot, 1997; Ghoshal et al,
2009), or, more recently, (Sagae et al, 2012).) The
basic idea in a pair LM is to align strings, then train a
language model over sequences whose symbols are
the input:output pairs of the alignment. This lan-
guage model can then be converted to transducers.
For a g2g example, homophones ?their? and ?there?
are aligned via the standard Levenshtein edit dis-
tance algorithm as ?t:t h:h e:e i: r:r :e?. A trigram
model over these x:y strings would use standard n-
gram modeling to estimate, for example, P(:e | i:
r:r); i.e., the probability of a silent ?r? in a given con-
text.
Building the pair language model transducers re-
quires two phases. In the first phase we create new
corpora by aligning the elements of the parallel cor-
pora outlined above. In the second phase we use
these corpora of string alignments to build a pair lan-
guage model.
3.3.1 Alignment and Corpora Building
We use extensions to the Levenshtein edit dis-
tance algorithm to align g2g, p2p and g2p strings,
with substitution matrices created to provide use-
ful alignments (Wagner and Fischer, 1974). As in
Brill and Moore (2000), we allow for certain multi-
symbol strings to be substituted with a single cost,
e.g., substituting ?th? with /?/ in g2p alignment. For
g2g alignment, our substitution cost is 0 for identity
and 2 for a few pairs of commonly interchangeable
graphemes, such as ?c? and ?k?. Other substitutions
are not permitted, and delete and insertion have cost
10. For p2p alignment there are two conditions, with
and without stress. Without vowel stress, no substi-
tutions other than identity are allowed; with vowel
stress, substitution cost is 2.5 for the same vowel
with differing stress; and 5.0 if substituting a vowel
with another vowel. Other substitutions are not per-
mitted, and, again, delete and insertion have cost 10.
For training round-trip models, we have to per-
form g2p and p2g alignment, with differing al-
phabets on the input and output of the alignment.
We begin with a basic substitution table that al-
lows graphemes and their most likely phonemes to
align. We then re-estimate the substitution costs
based on relative frequency estimation (-logP), and
also aggregate sequences of consecutively deleted
graphemes so that they collectively map to a single
phoneme. For example, given the alignment ?o:/a/
u:// g:// h:// t:/t/?, (?ought?, /at/), we make a new
rule: ough:/a/, and give it a cost based on its rela-
tive frequency. Grapheme strings that appear suffi-
ciently often with a given phoneme will thus accu-
mulate sufficient probability mass to compete.
Each alignment produced as described above is a
string in a training corpus for creating a pair lan-
guage model. As such, each alignment pair (e.g.
a:/@/) is a token.
3.3.2 From Corpora to WFSTs
We use the open source OpenGrm NGram library
(Roark et al, 2012) to build 5-gram language mod-
els from the strings of input:output pairs. These lan-
gauge models are encoded as weighted finite-state
acceptors in the OpenFst format (Allauzen et al,
2007). We shrink the models with the ngramshrink
command, using the relative entropy method (Stol-
cke, 1998), with the ?theta? threshold set at 1.0e?6.
These finite state acceptors are then converted into
transducers by modifying the arcs: split the labels
of each arc, x:y, making x the input label for that
arc, and y the output label. Thus traversing such
an arc will consume an x a return a y. Such pair
language models we use for all WFST methods dis-
cussed here.
3.4 Producing k-best output
Each tested input string, spelling or pronunciation,
is encoded as a cost-free linear chain WFST and
composed with a pair language model transducer de-
scribed in the previous section. The resulting lattice
is converted to an acceptor by projecting onto its out-
put labels, i.e., for each arc, the input label is set to
the value of the output label. Epsilons are then re-
moved and the result is determinized. The k-best
paths are extracted using the shortest path algorithm
in the OpenFst library.
For direct models (g2g and p2p), the k-best out-
put from this first transduction is our result, ranked
according the probability of each path. For round-
trip methods (e.g. g2p2g), however, we do a second
transduction in the other direction. For example, for
1586
g2p2g, the first transduction would have transduced
from a spelling to a set of candidate pronunciations;
the second transduction will transduce from pronun-
ciations to spellings. For this second transduction,
we take each string s from the k-best list from the
first transduction, and process them as we did in the
first transduction, now using the inverse transducer.
So, for each s in the first k-best list, we now have a k-
best list from the second transduction. Thus, for the
original input string, we have up to k2 alternatives.
Finally, we score each alternative by combining their
scores from both transductions.
Let p? represent a phoneme string, and g? a
grapheme string. If we perform a transduction from
p? to g?, the weights from the transducer provide the
(negative log) joint probability P(p?, g?). By perform-
ing a soft-max normalization over the k-best list out-
put, we obtain the (negative log) conditional proba-
bility P(g? | p?). For round-trip methods, we take the
product of the conditional probability in each direc-
tion, and marginalize out the intermediate grapheme
sequence, i.e.,
P(p?2 | p?1) =
?
g?
P(p?2 | g?) P(g? | p?1).
4 Experimental results
For evaluation purposes, we reserved a set of 1000
test homophone sets and 1000 test homograph sets,
as described in Section 3.1. From each set, we gen-
erate alternatives from the longest set member (ties
broken alphabetically) and examine the resulting k-
best list for presence of other members of the set.
Note that the input string itself is not a target, and,
before evaluation, is removed from the k-best list.
Recall is the proportion of the k-best list returned by
the system:
Recall({k-best}) =
| {k-best} ? {gold-list} |
| {gold-list} |
.
Results for generating alternative pronunciations
are listed in Table 1; those for generating alternative
spellings are in Table 2. For alternative spellings,
we also present results that combine the outputs of
direct, round-trip (no stress) and Moses into a sin-
gle list using a simple ranked voting scheme (simple
Borda count).
A noteworthy result is the apparent usefulness of
stress modeling for predicting pronunciation varia-
tion using WFSTs with the direct method; this is
Recall: Alternative Pronunciations
k- pair language model Moses
best Direct Roundtrip Direct
size stress none stress none stress none
1 0.43 0.54 0.38 0.37 0.44 0.46
3 0.77 0.71 0.59 0.58 0.60 0.62
5 0.82 0.77 0.66 0.66 0.64 0.65
10 0.86 0.80 0.73 0.76 0.68 0.69
Table 1: Recall for generating alternative pronunciations
seen in the first two data columns of 1. This sug-
gests that stress has an effect on phoneme alteration,
something we discuss in more detail in Section 5.
However, while providing a large gain in the p2p
condition, pronunciation modeling gives small or
negative effects elsewhere. In the round trip meth-
ods, the effects of stress are lost: stress has little
influence of how a particular phoneme is spelled.
Thus, graphemes do not retain much stress informa-
tion, hence any pass through the orthographic do-
main will shed it.
Recall is higher for alternative pronunciations
than for alternative spellings. One reason for this
is that spellings in our test set average eight let-
ters, whereas the pronunciations average around five
phonemes. Furthermore, the average Levenshtein
distance between original spellings and their tar-
get alernatives, is 2.6, while for pronunciations, it
is 2.2. Combining these factors, we see that, for
spellings, more edit operations are required, and
there are more symbols to which to apply them.
Therefore, for spellings, there are more incorrect
candidates.
The results also show gains resulting from the
roundtrip method when applied to finding alternative
spellings, but no such gains when roundtrip methods
are applied to alternative pronunciations. Suppose,
when seeking alternatives for some spelling, we al-
ter grapheme g1 to g2. With a direct method, we
must have instances of g1 mapping to g2 in the train-
ing set. The roundtrip method, however, is less con-
strained: there must exist some phoneme p1 in the
training set such that g1 maps to p1, and p1 maps to
g2; thus, the set of possible alternations at testing are
{g1 ? p1} ? {p1 ? g2}. This argument also ap-
plies to finding alternative pronunciations. Thus the
roundtrip method offers more possible mappings.
These extra possible mappings may be helpful or
harmful, depending on how likely they are compared
to the possible mappings they displace. Why are
they helpful for alternative spellings, but not for al-
1587
Recall: Alternative Spellings
k- pair language model Moses Comb.
best Direct Roundtrip Direct Direct
size none stress none none none
1 0.19 0.19 0.19 0.20 0.30
3 0.36 0.38 0.37 0.39 0.52
5 0.45 0.49 0.48 0.48 0.60
10 0.55 0.63 0.62 0.60 0.69
Table 2: Recall for generating alternative spellings
ternative pronunciations? We discuss one possible
explanation in Section 5.
Comparing Moses to the pair language model
methods, Moses does slightly better for smaller n
(n = 1, 3), and slightly worse for larger n (n = 10).
Our only partial explanation for this is that Moses
does well at weighing alternatives but, possibly, does
not generate a large number of viable alternatives.
System combination yields solid gains in finding al-
ternative spellings, demonstrating that these differ-
ent systems are coming up with diverse options.
Finally, we note that many of the false positive
pronunciations given by the WFST system are plau-
sibly correct although they are not included in the
CMU dictionary. For example, for the spelling, ad-
equate, the CMU dictionary provides two pronun-
ciations: /?d@kw@t/ and /?d@kwet/. Meanwhile,
the p2p WFST system (with stress modeling) pro-
duces /?d@kwIt/. This suggests that we can learn
from CMU dictionary to predict actual pronuncia-
tions that CMU dictionary does not itself list.
5 Discussion and Summary
The experimental results demonstrated the utility of
stress modeling for generating alternative pronunci-
ations, which we suggested was due to the impact of
stress on phoneme alternation. To examine this more
closely, we looked at each phoneme, stress class,
(ph, s)?e.g. (/@/, primary)?and determined how
likely is an occurrence of (ph, s) to have an alter-
native phoneme in a homograph set. We found that
primary and secondary stressed vowels had an alter-
ation probability of 0.017, while non-stressed vow-
els had an alteration probability of 0.036. This dif-
ference should be picked up in the transition proba-
bilities of our WFSTs, resulting in a preference for
alterations of unstressed vowels. This is analogous
to results found in (Greenberg et al, 2002) for spon-
taneous American English discourse. A further anal-
ysis of the system output might shed more light on
relationships between stress and phoneme choice.
Why are round-trip methods useful for finding al-
ternative spellings but not for finding alternative pro-
nunciations? One possible explanation is that the
variety of orthographic alternations is greater than
that of pronunciation alternations. Thus, the train-
ing set for spelling may provide less relative cover-
age of the alternations in its test set than the training
set for pronunciation provides for its test set. This
is supported by the fact that pronunciation recall ex-
ceeds spelling recall. The roundtrip method allows
for finding mappings not seen in training. These ex-
tra mappings might be no better for spelling than
they are for pronunciation, but for spelling, the map-
pings they replace in the k-best list are worse, so
they yield an improvement. For pronunciation, the
mappings they replace in the k-best list are better,
so they yield a loss. Further research is required to
validate this explanation.
Ultimately, we would like to apply these meth-
ods to the normalization of social media text, espe-
cially to find alternative spellings based on alterna-
tive pronunciations. To apply such methods to, say,
Twitter normalization requires a sizable corpus map-
ping canonical spellings to non-standard spellings.
To assess domain portability, we applied a model
built from the CMU dictionary to just over 100 al-
ternative spellings observed in a small Twitter col-
lection. Using the direct g2g method, we generated
alternative spellings from the canonical spelling of
each term, and measured the recall of the output, i.e.,
whether the observed alternatives were present in the
k-best list. Recall was extremely low (less than 5%),
suggesting that the type of orthographic alterations
that are found in dictionary pronunciations are very
different from the orthographic variations found on
Twitter, and that those differences have a profound
effect on our ability to recover alternatives.
In sum, we have presented a small study of
the utility of pronunciation dictionaries for finding
spelling and pronunciation alternatives, demonstrat-
ing key differences between these tasks.
Acknowledgments
This work was supported in part by NSF grant
#BCS-1049308. Any opinions, findings, conclu-
sions or recommendations expressed in this publica-
tion are those of the authors and do not necessarily
reflect the views of the NSF.
1588
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of Au-
tomata (CIAA 2007), Lecture Notes in Computer Sci-
ence, volume 4793, pages 11?23.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 597?604.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 36?44.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434 ? 451.
Eric Brill and Robert C Moore. 2000. An improved error
model for noisy channel spelling correction. In Pro-
ceedings of the 38th Annual Meeting on Association
for Computational Linguistics, pages 286?293.
Cynthia Chew and Gunther Eysenbach. 2010. Pan-
demics in the age of twitter: content analysis of
tweets during the 2009 h1n1 outbreak. PloS one,
5(11):e14118.
Sabine Deligne and Frdric Bimbot. 1997. Inference of
variable-length linguistic and acoustic units by multi-
grams. Speech Communication, 23(3):223 ? 241.
Matteo Gerosa and Marcello Federico. 2009. Coping
with out-of-vocabulary words: open versus huge vo-
cabulary asr. In Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP), pages 4313?4316.
A. Ghoshal, M. Jansche, S. Khudanpur, M. Riley, and
M. Ulinski. 2009. Web-derived pronunciations. In
Proc. ICASSP.
Steven Greenberg, Hannah Carvey, and Leah Hitchcock.
2002. The relation between stress accent and pro-
nunciation variation in spontaneous american english
discourse. In In Proceedings of ISCA Workshop on
Prosody in Speech Processing (Speech Prosody 2002),
Aix-enProvence.
Panagiota Karanasou and Lori Lamel. 2010. Comparing
SMT methods for automatic generation of pronuncia-
tion variants. In Advances in Natural Language Pro-
cessing, pages 167?178. Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180.
Antoine Laurent, Paul Dele?glise, Sylvain Meignier, and
France Spe?cinov-Tre?laze?. 2009. Grapheme to
phoneme conversion using an smt system. In Proceed-
ings of Interspeech, pages 708?711.
Chen Li and Yang Liu. 2012. Normalization of text mes-
sages using character-and phone-based machine trans-
lation approaches. In Proceedings of Interspeech.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of sms
abbreviations. In Proceedings of IJCNLP, pages 974?
982.
Brian Roark, Richard Sproat, Cyril Allauzen, Michael
Riley, Jeffrey Sorensen, and Terry Tai. 2012. The
OpenGrm open-source finite-state grammar software
libraries. In Proceedings of the ACL 2012 System
Demonstrations, pages 61?66.
Kenji Sagae, Maider Lehr, Emily Tucker
Prud?hommeaux, Puyang Xu, Nathan Glenn, Dami-
anos Karakos, Sanjeev Khudanpur, Brian Roark,
Murat Saraclar, Izhak Shafran, Daniel M. Bikel,
Chris Callison-Burch, Yuan Cao, Keith Hall, Eva
Hasler, Philipp Koehn, Adam Lopez, Matt Post, and
Darcey Riley. 2012. Hallucinated n-best lists for
discriminative language modeling. In ICASSP, pages
5001?5004. IEEE.
Richard Sproat, Alan W Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech & Language, 15(3):287?333.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
Kristina Toutanova and Robert C Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 144?151.
Robert A Wagner and Michael J Fischer. 1974. The
string-to-string correction problem. Journal of the
ACM (JACM), 21(1):168?173.
1589
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 980?989,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Data Driven Grammatical Error Detection
in Transcripts of Children?s Speech
Eric Morley
CSLU
OHSU
Portland, OR 97239
morleye@gmail.com
Anna Eva Hallin
Department of Communicative
Sciences and Disorders
New York University
New York, NY
ae.hallin@nyu.edu
Brian Roark
Google Research
New York, NY 10011
roarkbr@gmail.com
Abstract
We investigate grammatical error detec-
tion in spoken language, and present a
data-driven method to train a dependency
parser to automatically identify and label
grammatical errors. This method is ag-
nostic to the label set used, and the only
manual annotations needed for training are
grammatical error labels. We find that the
proposed system is robust to disfluencies,
so that a separate stage to elide disfluen-
cies is not required. The proposed system
outperforms two baseline systems on two
different corpora that use different sets of
error tags. It is able to identify utterances
with grammatical errors with an F1-score
as high as 0.623, as compared to a baseline
F1 of 0.350 on the same data.
1 Introduction
Research into automatic grammatical error detec-
tion has primarily been motivated by the task of
providing feedback to writers, whether they be na-
tive speakers of a language or second language
learners. Grammatical error detection, however, is
also useful in the clinical domain, for example, to
assess a child?s ability to produce grammatical lan-
guage. At present, clinicians and researchers into
child language must manually identify and clas-
sify particular kinds of grammatical errors in tran-
scripts of children?s speech if they wish to assess
particular aspects of the child?s linguistic ability
from a sample of spoken language. Such manual
annotation, which is called language sample anal-
ysis in the clinical field, is expensive, hindering
its widespread adoption. Manual annotations may
also be inconsistent, particularly between different
research groups, which may be investigating dif-
ferent phenomena. Automated grammatical error
detection has the potential to address both of these
issues, being both cheap and consistent.
Aside from performance, there are at least two
key requirements for a grammatical error detector
to be useful in a clinical setting: 1) it must be able
to handle spoken language, and 2) it must be train-
able. Clinical data typically consists of transcripts
of spoken language, rather than formal written lan-
guage. As a result, a system must be prepared
to handle disfluencies, utterance fragments, and
other phenomena that are entirely grammatical in
speech, but not in writing. On the other hand, a
system designed for transcripts of speech does not
need to identify errors specific to written language
such as punctuation or spelling mistakes. Further-
more, a system designed for clinical data must be
able to handle language produced by children who
may have atypical language due to a developmen-
tal disorder, and therefore may produce grammati-
cal errors that would be unexpected in written lan-
guage. A grammatical error detector appropriate
for a clinical setting must also be trainable be-
cause different groups of clinicians may wish to
investigate different phenomena, and will there-
fore prefer different annotation standards. This
is quite different from grammatical error detectors
for written language, which may have models for
different domains, but which are not typically de-
signed to enable the detection of novel error sets.
We examine two baseline techniques for gram-
matical error detection, then present a simple data-
driven technique to turn a dependency parser into a
grammatical error detector. Interestingly, we find
that the dependency parser-based approach mas-
sively outperforms the baseline systems in terms
of identifying ungrammatical utterances. Further-
more, the proposed system is able to identify spe-
cific error codes, which the baseline systems can-
not do. We find that disfluencies do not degrade
performance of the proposed detector, obviating
the need (for this task) for explicit disfluency de-
tection. We also analyze the output of our system
to see which errors it finds, and which it misses.
980
Code Description Example
[EO] Overgeneralization errors He falled [EO] .
[EW] Other word level errors He were [EW] looking .
[EU] Utterance level errors And they came to stopped .
[OM] Omitted bound morpheme He go [OM] .
[OW] Omitted word She [OW] running .
Table 1: Error codes proposed in the SALT manual. Note that in SALT annotated transcripts, [OM] and
[OW] are actually indicated by ?*? followed by the morpheme or word hypothesized to be omitted.
When treating codes (other than [EU]) as tags, they are attached to the previous word in the string.
Finally, we evaluate our detector on a second set
of data with a different label set and annotation
standards. Although our proposed system does not
perform as well on the second data set, it still out-
performs both baseline systems. One interesting
difference between the two data sets, which does
appear to impact performance, is that the latter set
more strictly follows SALT guidelines (see Sec-
tion 2.1) to collapse multiple errors into a single
label. This yields transcripts with a granularity of
labeling somewhat less amenable to automation,
to the extent that labels are fewer and can be re-
liant on non-local context for aggregation.
2 Background
2.1 Systematic Analysis of Language
Transcripts (SALT)
The Systematic Analysis of Language Transcripts
(SALT) is the de facto standard for clinicians look-
ing to analyze samples of natural language. The
SALT manual includes guidelines for transcrip-
tion, as well as three types of annotations, of
which two are relevant here: maze annotations,
and error codes.
1
Mazes are similar to what is referred to as ?dis-
fluencies? in the speech literature. The SALT
manual defines mazes as ?filled pauses, false
starts, repetitions, reformulations, and interjec-
tions? (Miller et al., 2011, p. 6), without defining
any of these terms. Partial words, which are in-
cluded and marked in SALT-annotated transcripts,
are also included in mazes. Mazes are delimited
by parentheses, and have no internal structure, un-
like disfluencies annotated following the Switch-
board guidelines (Meteer et al., 1995), which are
commonly followed by the speech and language
1
SALT also prescribes annotation of bound morphemes
and clitics, for example -ed in past tense verbs. We preprocess
all of the transcripts to remove bound morpheme and clitic
annotations.
processing communities. An example maze anno-
tation would be: ?He (can not) can not get up.?
The SALT manual proposes the set of error
codes shown (with examples) in Table 1, but re-
search groups may use a subset of these codes, or
augment them with additional codes. For example,
the SALT-annotated Edmonton Narrative Norms
Instrument (ENNI) corpus (Schneider et al., 2006)
rarely annotates omitted morphemes ([OM]), in-
stead using the [EW] code. Other SALT-annotated
corpora include errors that are not described in the
SALT manual. For example the CSLU ADOS cor-
pus (Van Santen et al., 2010) includes the [EX]
tag for extraneous words, and the Narrative Story
Retell corpus (SALT Software, 2014b) uses the
code [EP] to indicate pronominal errors (albeit
inconsistently, as many such errors are coded as
[EW] in this corpus). We note that the definitions
of certain SALT errors, notably [EW] and [EU],
are open to interpretation, and that these codes
capture a wide variety of errors. For example,
some of the errors captured by the [EW] code are:
pronominal case and gender errors; verb tense er-
rors; confusing ?a? and ?an?; and using the wrong
preposition.
The SALT guidelines specify as a general rule
that annotators should not mark utterances with
more than two omissions ([OM] or [OW]) and/or
word-level errors (ex [EW], [EP]) (SALT Soft-
ware, 2014a). Instead, annotators are instructed
to code such utterances with an utterance-level er-
ror ([EU]). How strictly annotators adhere to this
rule affects the distribution of errors, reducing the
number of word-level errors and increasing the
number of utterance-level errors. Following this
rule also increases the variety of errors captured
by the [EU] code. The annotations in different
corpora, including ENNI and NSR, vary in how
strictly they follow this rule, even though this is
not mentioned in the the published descriptions of
981
these corpora.
2.2 Grammatical Error Detection
The most visible fruits of research into grammati-
cal error detection are the spellchecking and gram-
mar checking tools commonly included with word
processors, for example Microsoft Word?s gram-
mar checker. Although developed for handling
written language, many of the techniques used
to address these tasks could still be applicable to
transcripts of speech because many of the same
errors can still occur. The earliest grammatical-
ity tools simply performed pattern matching (Mac-
donald et al., 1982), but this approach is not robust
enough to identify many types of errors, and pat-
tern matching systems are not trainable, and there-
fore cannot be adapted quickly to new label sets.
Subsequent efforts to create grammaticality classi-
fiers and detectors leveraged information extracted
from parsers (Heidorn et al., 1982) and language
models (Atwell, 1987). These systems, however,
were developed for formal written English pro-
duced by well-educated adults, as opposed to spo-
ken English produced by young children, partic-
ularly children with suspected developmental de-
lays.
There have been a few investigations into tech-
niques to automatically identify particular con-
structions in transcripts of spoken English. Bow-
den and Fox (2002) proposed a rule-based sys-
tem to classify many types of errors made by
learners of English. Although their system could
be used on either transcripts of speech, or on
written English, they did not evaluate their sys-
tem in any way. Caines and Buttery (2010) use
a logistic regression model to identify the zero-
auxiliary construction (e.g., ?you going home??)
with over 96% accuracy. Even though the zero-
auxilliary construction is not necessarily ungram-
matical, identifying such constructions may be
useful as a preprocessing step to a grammatical-
ity classifier. Caines and Buttery also demonstrate
that their detector can be integrated into a sta-
tistical parser yielding improved performance, al-
though they are vague about the nature of the parse
improvement (see Caines and Buttery, 2010, p. 6).
Hassanali and Liu (2011) conducted the first in-
vestigation into grammaticality detection and clas-
sification in both speech of children, and speech of
children with language impairments. They identi-
fied 11 types of errors, and compared three types
of systems designed to identify the presence of
each type of error: 1) rule based systems; 2) deci-
sion trees that use rules as features; and 3) naive
Bayes classifiers that use a variety of features.
They were able to identify all error types well
(F1 > 0.9 in all cases), and found that in general
the statistical systems outperformed the rule based
systems. Hassanali and Liu?s system was designed
for transcripts of spoken language collected from
children with impaired language, and is able to
detect the set of errors they defined very well.
However, it cannot be straightforwardly adapted
to novel error sets.
Morley et al. (2013) evaluated how well the
detectors proposed by Hassanali and Liu could
identify utterances with SALT error codes. They
found that a simplified version of one of Has-
sanali and Liu?s detectors was the most effective at
identifying utterances with any SALT error codes,
although performance was very low (F1=0.18).
Their system uses features extracted solely from
part of speech tags with the Bernoulli Naive Bayes
classifier in Scikit (Pedregosa et al., 2012). Their
detector may be adaptable to other annotation
standards, but it does not identify which errors are
in each utterance; it only identifies which utter-
ances have errors, and which do not.
2.3 Redshift Parser
We perform our experiments with the redshift
parser
2
, which is an arc-eager transition-based de-
pendency parser. We selected redshift because of
its ability to perform disfluency detection and de-
pendency parsing jointly. Honnibal and Johnson
(2014) demonstrate that this system achieves state-
of-the-art performance on disfluency detection,
even compared to single purpose systems such as
the one proposed by Qian and Liu (2013). Ra-
sooli and Tetreault (2014) have developed a sys-
tem that performs disfluency detection and depen-
dency parsing jointly, and with comparable perfor-
mance to redshift, but it is not publicly available as
of yet.
Redshift uses an averaged perceptron learner,
and implements several feature sets. The first fea-
ture set, which we will refer to as ZHANG is the
one proposed by Zhang and Nivre (2011). It in-
cludes 73 templates that capture various aspects
of: the word at the top of the stack, along with its
2
Redshift is available at https://github.com/
syllog1sm/redshift. We use the version in the
experiment branch from May 15, 2014.
982
leftmost and rightmost children, parent and grand-
parent; and the word on the buffer, along with
its leftmost children; and the second and third
words on the buffer. Redshift also includes fea-
tures extracted from the Brown clustering algo-
rithm (Brown et al., 1992). Finally, redshift in-
cludes features that are designed to help iden-
tify disfluencies; these capture rough copies, ex-
act copies, and whether neighboring words were
marked as disfluent. We will refer to the feature
set containing all of the features implemented in
redshift as FULL. We refer the reader to Honnibal
and Johnson (2014) for more details.
3 Data, Preprocessing, and Evaluation
Our investigation into using a dependency parser
to identify and label grammatical errors requires
training data with two types of annotations: de-
pendency labels, and grammatical error labels. We
are not aware of any corpora of speech with both
of these annotations. Therefore, we use two dif-
ferent sets of training data: the Switchboard cor-
pus, which contains syntactic parses; and SALT
annotated corpora, which have grammatical error
annotations.
3.1 Switchboard
The Switchboard treebank (Godfrey et al., 1992)
is a corpus of transcribed conversations that have
been manually parsed. These parses include
EDITED nodes, which span disfluencies. We pre-
process the Switchboard treebank by removing all
partial words as well as all words dominated by
EDITED nodes, and converting all words to lower-
case. We then convert the phrase-structure trees to
dependencies using the Stanford dependency con-
verter (De Marneffe et al., 2006) with the basic de-
pendency scheme, which produces dependencies
that are strictly projective.
3.2 SALT Annotated Corpora
We perform two sets of experiments on the two
SALT-annotated corpora described in Table 2. We
carry out the first set of experiments on on the Ed-
monton Narrative Norms Instrument (ENNI) cor-
pus, which contains 377 transcripts collected from
children between the ages of 3 years 11 months
and 10 years old. The children all lived in Edmon-
ton, Alberta, Canada, were typically developing,
and were native speakers of English.
After exploring various system configurations,
ENNI NSR
Words Utts Words Utts
Train 360,912 44,915 103,810 11,869
Dev. 45,504 5,614 12,860 1,483
Test 44,996 5,615 12,982 1,485
% with error 13.2 14.3
(a) Word and utterance counts
ENNI NSR
[EP] 0 20
[EO] 0 495
[EW] 4,916 1,506
[EU] 3,332 568
[OM] 10 297
[OW] 766 569
Total 9,024 3,455
(b) Error code counts
Table 2: Summary of ENNI and NSR Corpora.
There can be multiple errors per utterance. Word
counts include mazes.
we evaluate how well our method works when it
is applied to another corpus with different anno-
tation standards. Specifically, we train and test
our technique on the Narrative Story Retell (NSR)
corpus (SALT Software, 2014b), which contains
496 transcripts collected from typically develop-
ing children living in Wisconsin and California
who were between the ages of 4 years 4 months
and 12 years 8 months old. The ENNI and NSR
corpora were annotated by two different research
groups, and as Table 2 illustrates, they contain
a different distribution of errors. First, ENNI
uses the [EW] (other word-level error) tag to code
both overgeneralization errors instead of [EO], and
omitted morphemes instead of [OM]. The [EU]
code is also far more frequent in ENNI than NSR.
Finally, the NSR corpus includes an error code that
does not appear in the ENNI corpus: [EP], which
indicates a pronominal error, for example using
the wrong person or case. [EP], however, is rarely
used.
We preprocess the ENNI and NSR corpora to
reconstruct surface forms from bound morpheme
annotations (ex. ?go/3S? becomes ?goes?), partial
words, and non-speech sounds. We also either ex-
cise manually identified mazes or remove maze
annotations, depending upon the experiment.
3.3 Evaluation
Evaluating system performance in tagging tasks
on manually annotated data is typically straight-
983
Evaluation Level: ERROR UTTERANCE
Individual error codes Has error?
Gold error codes: [EW] [EW] Yes
Predicted error codes: [EW] [OW] Yes
Evaluation: TP FN FP TP
Figure 1: Illustration of UTTERANCE and ERROR level evaluation
TP = true positive; FP = false positive; FN = false negative
forward: we simply compare system output to the
gold standard. Such evaluation assumes that the
best system is the one that most faithfully repro-
duces the gold standard. This is not necessarily
the case with applying SALT error codes for three
reasons, and each of these reasons suggests a dif-
ferent form of evaluation.
First, automatically detecting SALT error codes
is an important task because it can aid clini-
cal investigations. As Morley et al. (2013) il-
lustrated, even extremely coarse features derived
from SALT annotations, for example a binary fea-
ture for each utterance indicating the presence of
any error codes, can be of immense utility for iden-
tifying language impairments. Therefore, we will
evaluate our system as a binary tagger: each ut-
terance, both in the manually annotated data and
system output either contains an error code, or it
does not. We will label this form of evaluation as
UTTERANCE level.
Second, clinicians are not only interested in
how many utterances have an error, but also which
particular errors appear in which utterances. To
address this issue, we will compute precision, re-
call, and F1 score from the counts of each er-
ror code in each utterance. We will label this
form of evaluation as ERROR level. Figure 1 illus-
trates both UTTERANCE and ERROR level evalua-
tion. Note that the utterance level error code [EU]
is only allowed to appear once per utterance. As
a result, we will ignore any predicted [EU] codes
beyond the first.
Third, the quality of the SALT annotations
themselves is unknown, and therefore evaluation
in which we treat the manually annotated data as a
gold standard may not yield informative metrics.
Morley et al. (2014) found that there are likely
inconsistencies in maze annotations both within
and across corpora. In light of that finding, it is
possible that error code annotations are somewhat
inconsistent as well. Furthermore, our approach
has a critical difference from manual annotation:
we perform classification one utterance at a time,
while manual annotators have access to the context
of an utterance. Therefore certain types of errors,
for example using a pronoun of the wrong gender,
or responding ungrammatically to a question (ex.
?What are you doing?? ?Eat.?) will appear gram-
matical to our system, but not to a human anno-
tator. We address both of these issues with an in-
depth analysis of the output of one of our systems,
which includes manually re-coding utterances out
of context.
4 Detecting Errors in ENNI
4.1 Baselines
We evaluate two existing systems to see how ef-
fectively they can identify utterances with SALT
error codes: 1) Microsoft Word 2010?s gram-
mar check, and 2) the simplified version of Has-
sanali and Liu?s grammaticality detector (2011)
proposed by Morley et al. (2013) (mentioned in
Section 2.2). We configured Microsoft Word
2010?s grammar check to look for the following
classes of errors: negation, noun phrases, subject-
verb agreement, and verb phrases (see http://
bit.ly/1kphUHa). Most error classes in gram-
mar check are not relevant for transcribed speech,
for example capitalization errors or confusing it?s
and its; we selected classes of errors that would
typically be indicated by SALT error codes.
Note that these baseline systems can only give
us an indication of whether there is an error in
the utterance or not; they do not provide the spe-
cific error tags that mimic the SALT guidelines.
Hence we evaluate just the UTTERANCE level per-
formance of the baseline systems on the ENNI de-
velopment and test sets. These results are given
in the top two rows of each section of Table 3.
We apply these systems to utterances in two condi-
tions: with mazes (i.e., disfluencies) excised; and
with unannotated mazes left in the utterances. As
can be seen in Table 3, the performance Microsoft
Word?s grammar checker degrades severely when
984
(a)
Him [EW] (can not) can not get up .
(b)
ROOT him can not can not get up .
nsubj+[EW]
aux
neg
aux
neg
ROOT
prt
P
Figure 2: (a) SALT annotated utterance; mazes indicated by parentheses; (b) Dependency parse of same
utterance parsed with a grammar trained on the Switchboard corpus and augmented dependency labels.
We use a corpus of parses with augmented labels to train our grammaticality detector.
mazes are not excised, but this is not the case for
the Morley et al. (2013) detector.
4.2 Proposed System
Using the ENNI corpus, we now explore various
configurations of a system for grammatical error
code detection. All of our systems use redshift
to learn grammars and to parse. First, we train
an initial grammar G
0
on the Switchboard tree-
bank (Godfrey et al., 1992) (preprocessed as de-
scribed in Section 3.1). Redshift learns a model for
part of speech tagging concurrently with G
0
. We
use G
0
to parse the training portion of the ENNI
corpus. Then, using the SALT annotations, we
append error codes to the dependency arc labels
in the parsed ENNI corpus, assigning each error
code to the word it follows in the SALT annotated
data. Figure 2 shows a SALT annotated utterance,
as well as its dependency parse augmented with
error codes. Finally, we train a grammar G
Err
on
the parse of the ENNI training fold that includes
the augmented arc labels. We can now use G
Err
to automatically apply SALT error codes: they are
simply encoded in the dependency labels. We also
apply the [EW] label to any word that is in a list of
overgeneralization errors
3
.
We modify three variables in our initial trials on
the ENNI development set. First, we change the
proportion of utterances in the training data that
contain an error by removing utterances.
4
Doing
so allows us to alter the operating point of our sys-
3
The list of overgeneralization errors was generously pro-
vided by Kyle Gorman
4
Of course, we never modify the development or test data.
tem in terms of precision and recall. Second, we
again train and test on two versions of the ENNI
corpus: one which has had mazes excised, and the
other which has them present (but not annotated).
Third, we evaluate two feature sets: ZHANG and
FULL.
The plots in Figure 3 show how the per-
formances of our systems at different operating
points vary, while Table 3 shows the performance
of our best system configurations on the ENNI de-
velopment and test sets. Surprisingly, we see that
neither the choice of feature set, nor the presence
of mazes has much of an effect on system per-
formance. This is in strong contrast to Microsoft
Word?s grammar check, which is minimally effec-
tive when mazes are included in the data. The
Morley et al. (2013) system is robust to mazes,
but still performs substantially worse than our pro-
posed system.
4.3 Error Analysis
We now examine the errors produced by our best
performing system for data in which mazes are
present. As shown in Table 3, when we apply our
system to ENNI-development, the UTTERANCE
P/R/F1 is 0.831 / 0.502 / 0.626 and the ERROR
P/R/F1is 0.759 / 0.434 / 0.552. This system?s per-
formance detecting specific error codes is shown
in Table 4. We see that the recall of [EU] errors is
quite low compared with the recall for [EW] and
[OW] errors. This is not surprising, as human an-
notators may need to leverage the context of an ut-
terance to identify [EU] errors, while our system
makes predictions for each utterance in isolation.
985
(a) UTTERANCE level evaluation (b) ERROR level evaluation
Figure 3: SALT error code detection performance at various operating points on ENNI development set
Eval Mazes Excised Mazes Present
System type P R F1 P R F1
Development
MS Word UTT 0.843 0.245 0.380 0.127 0.063 0.084
Morley et al. (2013) UTT 0.407 0.349 0.376 0.343 0.321 0.332
Current paper
UTT 0.943 0.470 0.627 0.831 0.502 0.626
ERR 0.895 0.412 0.564 0.759 0.434 0.552
Test
MS Word UTT 0.824 0.209 0.334 0.513 0.219 0.307
Morley et al. (2013) UTT 0.375 0.328 0.350 0.349 0.252 0.293
Current Paper
UTT 0.909 0.474 0.623 0.809 0.501 0.618
ERR 0.682 0.338 0.452 0.608 0.360 0.452
Table 3: Baseline and current paper systems? performance on ENNI. Evaluation is at the UTTERANCE
(UTT) level except for the current paper?s system, which also presents evaluation at the ERROR (ERR)
level.
Error Code P R F1
EU 0.639 0.193 0.297
EW 0.832 0.582 0.685
OW 0.680 0.548 0.607
Table 4: ERROR level detection performance for
each code (system trained on ENNI; 30% error
utterances; ZHANG feature set; with mazes)
We randomly sampled 200 utterances from the
development set that have a manually annotated
error, are predicted by our system to have an er-
ror, or both. A speech-language pathologist who
has extensive experience with using SALT for re-
search purposes in both clinical and typically de-
veloping populations annotated the errors in each
utterance. She annotated each utterance in isola-
tion so as to ignore contextual errors. We compare
our annotations to the original annotations, and
system performance using our annotations and the
original annotations as different gold standards.
The results of this comparison are shown in Table
5.
Comparing our manual annotations to the orig-
inal annotations, we notice some disagreements.
We suspect there are two reasons for this. First,
unlike the original annotators, we annotate these
utterances out of context. This may explain why
we identify far fewer utterance level error [EU]
codes than the original annotators (20 compared
with 67). Second, we may be using different cri-
teria for each error code than the original anno-
tators. This is an inevitable issue, as the SALT
guidelines do not provide detailed definitions of
the error codes, nor do individual groups of anno-
tators. To illustrate, the ?coding notes? section of
986
Tag Gold Gold Count Disagreement P R F1
[EU] Original 67 52 0.500 0.149 0.230
Revised 20 0.450 0.333 0.383
[EW] Original 137 27 0.859 0.533 0.658
Revised 126 0.800 0.540 0.645
[OW] Original 16 13 0.667 0.275 0.480
Revised 15 0.444 0.267 0.333
Table 5: System performance using ERROR level evaluation on 200 utterances selected from ENNI-dev
using original and revised annotations as gold standard
UTTERANCE level ERROR level
System P R F1 P R F1
ENNI-trained 0.310 0.124 0.178 0.157 0.057 0.084
NSR-trained 0.243 0.249 0.277 0.150 0.195 0.170
MS Word 0.561 0.171 0.261 ? ? ?
Morley et al. (2013) 0.250 0.281 0.264 ? ? ?
NSR ? MS Word 0.291 0.447 0.353 ? ? ?
NSR ? Morley et al. (2013) 0.297 0.387 0.336 ? ? ?
All 3 0.330 0.498 0.397 ? ? ?
Table 6: Error detection performance on NSR-development, mazes included
the description of the ENNI corpus
5
only lists the
error codes that were used consistently, but does
not describe how to apply them. These findings
illustrate the importance of having a rapidly train-
able error code detector: research groups will be
interested in different phenomena, and therefore
will likely have different annotation standards.
5 Detecting Errors in NSR
We apply our system directly to the NSR corpus
with mazes included. We use the same parameters
set on the ENNI corpus in Section 4.2. We apply
the model trained on ENNI to NSR, but find that it
does not perform very well as illustrated in Table
6. These results further underscore the need for
a trainable error code detector in this domain, as
opposed to the static error detectors that are more
common in the grammatical error detection litera-
ture.
We see in Table 6 that retraining our model
on NSR data improves performance substantially
(UTTERANCE F1 improves from 0.178 to 0.277),
but not to the level we observed on the ENNI cor-
pus. The Morley et al. (2013) system also per-
forms worse when trained and tested on NSR, as
compared with ENNI. When mazes are included,
5
http://www.saltsoftware.com/salt/
databases/ENNIRDBDoc.pdf
the performance of Microsoft Word?s grammar
check is higher on NSR than on ENNI (F1=0.261
vs 0.084), but it it still yields the lowest perfor-
mance of the three systems. We find that combin-
ing our proposed system with either or both of the
baseline systems further improves performance.
The NSR corpus differs from ENNI in several
ways: it is smaller, contains fewer errors, and uses
a different set of tags with a different distribution
from the ENNI corpus, as shown in Table 2. We
found that the smaller amount of training data is
not the only reason for the degradation in perfor-
mance; we trained a model for ENNI with a set of
training data that is the same size as the one for
NSR, but did not observe a major drop in perfor-
mance. We found that UTTERANCE F1 drops from
0.626 to 0.581, and ERROR F1 goes from 0.552 to
0.380, not nearly the magnitude drop in accuracy
observed for NSR.
We believe that a major reason for why our sys-
tem performs worse on NSR than ENNI may be
that the ENNI annotations adhere less strictly to
certain SALT recommendations than do the ones
in NSR. The SALT guidelines suggest that utter-
ances with two or more word-level [EW] and/or
omitted word [OW] errors should only be tagged
with an utterance-level [EU] error (SALT Soft-
ware, 2014a). ENNI, however, has many utter-
987
ances with multiple [EW] and [OW] error codes,
along with utterances containing all three error
codes. NSR has very few utterances with [EU] and
other codes, or multiple [EW] and [OW] codes.
The finer grained annotations in ENNI may sim-
ply be easier to learn.
6 Conclusion and Future Directions
We have proposed a very simple method to rapidly
train a grammatical error detector and classifier.
Our proposed system only requires training data
with error code annotations, and is agnostic as to
the nature of the specific error codes. Furthermore,
our system?s performance does not appear to be
affected by disfluencies, which reduces the burden
required to produce training data.
There are several key areas we plan to inves-
tigate in the future. First, we would like to ex-
plore different update functions for the parser; the
predicted error codes are a byproduct of parsing,
but we do not care what the parse itself looks like.
At present, the parser is updated whenever it pro-
duces a parse that diverges from the gold stan-
dard. It may be better to update only when the
error codes predicted for an utterance differ from
the gold standard. Second, we hope to explore fea-
tures that could be useful for identifying grammat-
ical errors in multiple data sets. Finally, we plan
to investigate why our system performed so much
better on ENNI than on NSR.
Acknowledgments
We would like to thank the following people for
valuable input into this study: Joel Tetreault,
Jan van Santen, Emily Prud?hommeaux, Kyle
Gorman, Steven Bedrick, Alison Presmanes Hill
and others in the CSLU Autism research group
at OHSU. This material is based upon work
supported by the National Institute on Deafness
and Other Communication Disorders of the Na-
tional Institutes of Health under award number
R21DC010033. The content is solely the respon-
sibility of the authors and does not necessarily rep-
resent the official views of the National Institutes
of Health.
References
Eric Atwell. 1987. How to detect grammatical er-
rors in a text without parsing it. In Bente Maegaard,
editor, EACL, pages 38?45, Copenhagen, Denmark,
April. The Association for Computational Linguis-
tics.
Mari I Bowden and Richard K Fox. 2002. A diagnos-
tic approach to the detection of syntactic errors in
english for non-native speakers. The University of
Texas?Pan American Department of Computer Sci-
ence Technical Report.
Peter F. Brown, Vincent J. Della Pietra, Peter V.
de Souza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Andrew Caines and Paula Buttery. 2010. You talking
to me?: A predictive model for zero auxiliary con-
structions. In Proceedings of the 2010 Workshop on
NLP and Linguistics: Finding the Common Ground,
pages 43?51.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, volume 1, pages 517?520.
Khairun-nisa Hassanali and Yang Liu. 2011. Measur-
ing language development in early childhood educa-
tion: a case study of grammar checking in child lan-
guage transcripts. In Proceedings of the 6th Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 87?95.
George E. Heidorn, Karen Jensen, Lance A. Miller,
Roy J. Byrd, and Martin S Chodorow. 1982.
The EPISTLE text-critiquing system. IBM Systems
Journal, 21(3):305?326.
Matthew Honnibal and Mark Johnson. 2014. Joint
incremental disfluency detection and dependency
parsing. TACL, 2:131?142.
Nina H Macdonald, Lawrence T Frase, Patricia S Gin-
grich, and Stacey A Keenan. 1982. The writer?s
workbench: Computer aids for text analysis. Edu-
cational psychologist, 17(3):172?179.
Marie W Meteer, Ann A Taylor, Robert MacIntyre,
and Rukmini Iyer. 1995. Dysfluency annotation
stylebook for the switchboard corpus. University of
Pennsylvania.
Jon F Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT
software: A clinician?s guide to language sample
analysis. SALT Software, LLC.
988
Eric Morley, Brian Roark, and Jan van Santen. 2013.
The utility of manual and automatic linguistic error
codes for identifying neurodevelopmental disorders.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 1?10, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Eric Morley, Anna Eva Hallin, and Brian Roark. 2014.
Challenges in automating maze detection. In Pro-
ceedings of the First Workshop on Computational
Linguistics and Clinical Psychology, pages 69?77,
Baltimore, Maryland, June.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2012.
Scikit-learn: Machine learning in python. CoRR,
abs/1201.0490.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Lucy Vander-
wende, Hal Daum?e III, and Katrin Kirchhoff, edi-
tors, HLT-NAACL, pages 820?825, Atlanta, Georgia,
USA, June. The Association for Computational Lin-
guistics.
Mohammad Sadegh Rasooli and Joel R. Tetreault.
2014. Non-monotonic parsing of fluent umm i mean
disfluent sentences. In Gosse Bouma and Yannick
Parmentier, editors, EACL, pages 48?53, Gothen-
burg, Sweden, April. The Association for Compu-
tational Linguistics.
LLC SALT Software. 2014a. Course
1306: Transcription - Conventions Part 3.
http://www.saltsoftware.com/
onlinetraining/section-page?
OnlineTrainingCourseSectionPageId=
76. [Online; accessed 29-May-2104].
LLC SALT Software. 2014b. Narrative
Story Retell Database. http://www.
saltsoftware.com/salt/databases/
NarStoryRetellRDBDoc.pdf. [Online;
accessed 29-May-2104].
Phyllis Schneider, Denyse Hayward, and Rita Vis
Dub?e. 2006. Storytelling from pictures using
the edmonton narrative norms instrument. Jour-
nal of Speech Language Pathology and Audiology,
30(4):224.
Jan PH Van Santen, Emily T Prud?hommeaux, Lois M
Black, and Margaret Mitchell. 2010. Com-
putational prosodic markers for autism. Autism,
14(3):215?236.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
ACL (Short Papers), pages 188?193, Portland, Ore-
gon, USA, June. The Association for Computational
Linguistics.
989
Finite-State Chart Constraints for Reduced
Complexity Context-Free Parsing Pipelines
Brian Roark?
Oregon Health & Science University
Kristy Hollingshead??
University of Maryland
Nathan Bodenstab?
Oregon Health & Science University
We present methods for reducing the worst-case and typical-case complexity of a context-free
parsing pipeline via hard constraints derived from finite-state pre-processing. We perform O(n)
predictions to determine if each word in the input sentence may begin or end a multi-word
constituent in chart cells spanning two or more words, or allow single-word constituents in
chart cells spanning the word itself. These pre-processing constraints prune the search space
for any chart-based parsing algorithm and significantly decrease decoding time. In many cases
cell population is reduced to zero, which we term chart cell ?closing.? We present methods for
closing a sufficient number of chart cells to ensure provably quadratic or even linear worst-case
complexity of context-free inference. In addition, we apply high precision constraints to achieve
large typical-case speedups and combine both high precision and worst-case bound constraints
to achieve superior performance on both short and long strings. These bounds on processing
are achieved without reducing the parsing accuracy, and in some cases accuracy improves.
We demonstrate that our method generalizes across multiple grammars and is complementary
to other pruning techniques by presenting empirical results for both exact and approximate
inference using the exhaustive CKY algorithm, the Charniak parser, and the Berkeley parser. We
also report results parsing Chinese, where we achieve the best reported results for an individual
model on the commonly reported data set.
1. Introduction
Although there have been great advances in the statistical modeling of hierarchical
syntactic structure over the past 15 years, exact inference with suchmodels remains very
costly andmost rich syntactic modeling approaches resort to heavy pruning, pipelining,
? Center for Spoken Language Understanding, Oregon Health & Science University, Beaverton, Oregon,
97006 USA. E-mails: roarkbr@gmail.com, bodenstab@gmail.com.
?? Some of the work in this paper was done while Kristy Hollingshead was at OHSU. She is currently at the
University of Maryland Institute for Advanced Computer Studies, College Park, Maryland, 20740 USA.
E-mail: hollingk@gmail.com.
Submission received: 9 August 2011; revised submission received: 30 November 2011; accepted for
publication: 4 January 2012.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 4
or both. Pipeline systems make use of simpler models with more efficient inference to
reduce the search space of the full model. For example, the well-known Ratnaparkhi
(1999) parser used a part-of-speech (POS)-tagger and a finite-state noun phrase (NP)
chunker as initial stages of a multi-stageMaximum Entropy parser. The Charniak (2000)
parser uses a simple probalistic context-free grammar (PCFG) to sparsely populate a
chart for a richer model, and Charniak and Johnson (2005) added a discriminatively
trained reranker to the end of that pipeline.
Finite-state pre-processing for context-free parsing is very common as a means
of reducing the amount of search required in the later stage. As mentioned earlier,
the Ratnaparkhi pipeline used a finite-state POS-tagger and a finite-state NP-chunker
to reduce the search space at the parsing stage, and achieved linear observed-time
performance. Other recent examples of the utility of finite-state constraints for parsing
pipelines include Glaysher and Moldovan (2006), Djordjevic, Curran, and Clark (2007),
and Hollingshead and Roark (2007). Similar hard constraints have been applied for
dependency parsing, as will be outlined in Section 2. Note that by making use of pre-
processing constraints, such approaches are no longer performing full exact inference?
these are approximate inference methods, as are the methods presented in this article.
Using finite-state chunkers early in a syntactic parsing pipeline has shown both an
efficiency (Glaysher and Moldovan 2006) and an accuracy (Hollingshead and Roark
2007) benefit for parsing systems. Glaysher and Moldovan (2006) demonstrated an
efficiency gain by explicitly disallowing constituents that cross chunk boundaries.
Hollingshead and Roark (2007) demonstrated that high-precision constraints on
early stages of the Charniak and Johnson (2005) pipeline (in the form of base phrase
constraints derived either from a chunker or from later stages of an earlier iteration
of the same pipeline) achieved significant accuracy improvements, by moving the
pipeline search away from unlikely areas of the search space. All of these approaches
(as with Ratnaparkhi earlier) achieve improvements by ruling out parts of the search
space, and the gain can either be realized in efficiency (same accuracy, less time) and/or
accuracy (same time, greater accuracy).
Rather than extracting constraints from taggers or chunkers built for different
purposes, in this study we have trained prediction models to more directly reduce the
number of entries stored in cells of a dynamic programming chart during parsing?even
to the point of ?closing? chart cells to all entries. We demonstrate results using three
finite-state taggers that assign each word position in the sequence with a binary class
label. The first tagger decides if the word can begin a constituent of span greater than
one word; the second tagger decides if the word can end a constituent of span greater
than oneword; and the third tagger decides if a chart cell spanning a single word should
contain phrase-level non-terminals, or only POS tags. Following the prediction of each
word, chart cells spanning multiple words can be completely closed as follows: Given a
chart cell (b, e) spanning words wb . . .we where b < e, we can ?close? cell (b, e) if the first
tagger decides that wb cannot be the first word of a multi-word constituent (MWC) or if
the second tagger decides thatwe cannot be the last word in aMWC. Completely closing
sufficient chart cells allows us to impose worst-case complexity bounds on the overall
pipeline, a bound that none of the other previously mentioned methods for finite-state
preprocessing can guarantee.
To complement closing multi-word constituent chart cells, our third tagger restricts
the population of span-1 chart cells. We note that all span-1 chart cells must contain
at least one POS tag and can therefore never be closed completely. Instead, our tagger
restricts unary productions with POS tags on their right-hand side that span a single
word. We term these single word constituents (SWCs). Disallowing SWCs alters span-1
720
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
cell population from potentially containing all non-terminals to just POS non-terminals.
In practice, this decreases the number of entries in span-1 chart cells by 70% during ex-
haustive parsing, significantly reducing the number of allowable constituents in larger
spans (Bodenstab, Hollingshead, and Roark 2011). Span-1 chart cells are also the most
frequently queried cells in the Cocke-Younger-Kasami (CKY) algorithm. The search over
possible midpoints will always include two cells spanning a single word?one as the
first left child and one as the last right child. It is therefore beneficial to minimize the
number of entries in these span-1 cells.
The pre-processing framework we have outlined is straightforward to incorporate
into most existing context-free constituent parsers, a task we have already done for sev-
eral state-of-the art parsers. In the following sections we formally define our approach
to finite-state chart constraints and analyze the accuracy of each of the three taggers
and their impact on parsing efficiency and accuracy when used to prune the search
space of a constituent parser. We apply our methods to exhaustive CYK parsing with
simple grammars, as well as to high-accuracy parsing approaches such as the Charniak
and Johnson (2005) parsing pipeline and the Berkeley parser (Petrov and Klein 2007a,
2007b). Various methods for applying finite-state chart constraints are investigated,
including methods that guarantee quadratic or linear complexity of the context-free
parser.
2. Related Work
Hard constraints are ubiquitous within parsing pipelines. One of the most basic and
standard techniques is the use of a POS-tag dictionary, whereby words are only allowed
to be assigned one of a subset of the POS-tag vocabulary, often based on what has been
observed in the training data. This will overly constrain polysemous word types that
happen not to have been observedwith one of their possible tags; yet the large efficiency
gain of so restricting the tags is typically seen as outweighing the loss in coverage. POS-
tag preprocessing has been used for both context-free constituent parsing (Ratnaparkhi
1999) and dependency parsing (McDonald et al 2005). Richer tag sets can also be used
to further constrain the parser, such as supertags (Bangalore and Joshi 1999), which
contain information about how the word will syntactically integrate with other words
in the sequence. Supertagging has been widely used to make parsing algorithms effi-
cient, particularly those making use of context-sensitive grammars (Clark and Curran
2004).
By applying finite-state chart constraints to constituent parsing, the approaches
pursued in this article constrain the possible shapes of unlabeled trees, eliminating from
consideration trees with constituents over specific spans. There is thus some similarity
with other tagging approaches (e.g., supertagging) that dictate how words combine
with the rest of the sentence via specific syntactic structures. Supertagging is generally
used to enumerate which sorts of structures are licensed, whereas the constraints in
this article indicate unlabeled tree structures that are proscribed. Along the same lines,
there is a very general similarity with coarse-to-fine search methods, such as those
used in the Berkeley (Petrov and Klein 2007a) and Charniak (2000) parsers, and more
general structured prediction cascades (Weiss, Sapp, and Taskar 2010; Weiss and Taskar
2010). Our approach also uses simpler models that reduce the search space for larger
downstream models.
Dependency parsing involves constructing a graph of head/dependent relations,
andmanymethods for constraining the space of possible dependency graphs have been
721
Computational Linguistics Volume 38, Number 4
investigated, such as requiring that each word have a single head or that the graph be
acyclic. Nivre (2006) investigated the impact of such constraints on coverage and the
number of candidate edges in the search space. Most interestingly, that paper found
that constraining the degree of non-projectivity that is allowed can greatly reduce the
number of arcs that must be considered during search, and, as long as some degree
of non-projectivity is allowed, coverage is minimally impacted. Of course, the total
absence of projectivity constraints allows for the use of spanning tree algorithms that
can be quadratic complexity for certain classes of statistical models (McDonald et al
2005), so the ultimate utility of such constraints varies depending on the model being
used.
Other hard constraints have been applied to dependency parsing, including con-
straints on the maximum length of dependencies (Eisner and Smith 2005; Dreyer, Smith,
and Smith 2006), which is known as vine parsing. Such vine parsers can be further
constrained using taggers to determine the directionality and distance of each word?s
head in a way similar to our use of taggers (Sogaard and Kuhn 2009). More general arc
filtering approaches, using a variety of features (including some inspired by previously
published results of methods presented in this article) have been proposed to reduce
the number of arcs considered for the dependency graph (Bergsma and Cherry 2010;
Cherry and Bergsma 2011), resulting in large parsing speedups.
In a context-free constituent parsing pipeline, constraints on the final parse struc-
ture can be made in stages preceding the CYK algorithm. For example, base phrase
chunking (Hollingshead and Roark 2007) involves identifying a span as a base phrase
of some category, often NP. A base phrase constituent has no children other than
pre-terminal POS-tags, which all have a single terminal child (i.e., there is no in-
ternal structure in the base phrase involving non-POS non-terminals). This has a
number of implications for the context-free parser. First, there is no need to build
internal structure within the identified base phrase constituent. Second, constituents
that cross brackets with the base phrase cannot be part of the final tree structure.
This second constraint on possible trees can be thought of as a constraint on chart
cells, as pointed out in Glaysher and Moldovan (2006): No multi-word constituent
can begin at a word falling within a base-phrase chunk, other than the first word
of that chunk. Similarly, no multi-word constituent can end at a word falling within
a base-phrase chunk, other than the last word of that chunk. These constraints rule
out many possible structures that the full context-free parser would have otherwise
considered.
These begin and end constraints can be extracted from the output of the chunker,
but the chunker is most often trained to optimize chunking accuracy, not parsing
accuracy (or parsing precision). Further, these constraints can apply even for words
that fall outside of typical chunks. For example, in English, verbs and prepositions tend
to occur before their arguments, hence are often unlikely to end constituents, yet verbs
and prepositions are rarely inside a typically defined base phrase. Instead of imposing
parsing constraints from NLP pre-processing steps such as chunking, we propose that
building specific prediction models to constrain the search space within the CYK chart
will more directly optimize efficiency within a parsing pipeline.
In this article, we focus on linear complexity finite-state methods for deriving
constraints on the chart. Recent work has also examined methods for constraining each
of theO(N2) chart cell independently (Bodenstab et al 2011), permitting a finer-grained
pruning (e.g., not just ?open? or ?closed? but an actual beam width prediction) and the
use of features beyond the scope of our tagger. We discuss this and other extensions of
the current methods in our concluding remarks.
722
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
3. Preliminaries
3.1 Dynamic Programming Chart
Dynamic programming for context-free inference generally makes use of a chart struc-
ture, as shown in Figure 1c for the left-binarized gold parse tree in Figure 1b. Each cell
in the chart represents a collection of possible constituents covering a substring, which
is identified by the indices of the first and last words of the substring. Let w1 . . .wn be
a string of n words to be parsed. The cell identified with (b, e) will contain possible
constituents spanning the substring wb . . .we, where the span of a constituent or a chart
cell is defined as the number of words it covers, or e? b+ 1. As an example, in this
Figure 1
Gold parse tree (a), left-binarized representation of the same tree (b), and corresponding
dynamic programming chart (c) for the sentence As usual, the real-estate market had overreacted.
(sentence 1094 in WSJ Section 24). Each cell in the chart spans a unique substring of the input
sentence. Non-terminals preceded with the symbol ?@? are created through binarization
(see Section 3.3).
723
Computational Linguistics Volume 38, Number 4
article we will occasionally refer to span-1 chart cells, meaning all chart cells covering a
single word.
Context-free inference using dynamic programming over a chart structure builds
longer-span constituents by combining smaller span constituents, guided by rules in a
context-free grammar. A context-free grammar G = (V,T,S?,P) consists of: a set of non-
terminal symbols V, including a special start symbol S?; a set of terminal symbols T;
and a set of rule productions P of the form A ? ? for A ? V and ? ? (V ? T)?, i.e., a
single non-terminal on the left-hand side of the rule production, and a sequence of 0 or
more terminals or non-terminals on the right-hand side of the rule. If we have a rule
production A ? B C ? P, a completed B entry in chart cell (b,m) and a completed C
entry in chart cell (m+1, e), we can place a completed A entry in chart cell (b, e). Such
a chart cell entry is sometimes called an ?edge? and can be represented by the tuple
(A ? B C, b,m, e).
Context-free inference has cubic complexity in the length of the string N, due to the
O(N2) number of chart cells and O(N) possible child configurations at each cell. As an
example, a cell spanning (b, e) must consider all possible configurations of two child
constituents (child cells) that span a proper prefix (b, m) and a proper suffix (m+1, e)
where b ? m < e, leading to O(N) possible midpoints.
3.2 The CYK Algorithm
Algorithm 1 contains pseudocode for the CYK algorithm (Kasami 1965; Younger 1967;
Cocke and Schwartz 1970), where the context-free grammar G is assumed to be bina-
rized. The function ? maps each grammar production in P to a probability. Lines 1?3
Algorithm 1 CYK
Pseudocode of the CYK algorithm using a binarized PCFG. Unary processing is sim-
plified to allow only chains of length one (excluding lexical unary productions). Back-
pointer storage is omitted.
Input:
w1 . . .wn: Input sentence
G: Left-binarized PCFG
Output:
?: Viterbi-max score for all non-terminals over every span
CYK(w1 . . .wn, G = (V,T,S
?,P,?))
1: for s = 1 to n do  Span width: bottom-up traversal
2: for b = 1 to n? s+ 1 do  Begin word position
3: e ? b+s?1
4: for Ai ? V do
5: if s = 1 then  Add lexical productions
6: ?i(b, e)? ?(Ai ? wb)
7: else
8: ?i(b, e)? max
b?m<e
(
max
j,k
?(Ai ? Aj Ak) ?j(b,m) ?k(m+ 1, e)
)
9: for Ai ? V do  Add unary productions
10: ?i(b, e)? max
(
?i(b, e) , max
j
?(Ai ? Aj) ?j(b, e)
)
11: ?(b, e)? ?(b, e)
12: return ?
724
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
iterate over all O(N2) chart cells in a bottom?up traversal. Line 6 initializes span-1 cells
with all possible part-of-speech tags, and line 8 introduces the cubic complexity of the
algorithm: maximization over all midpoints m, which is O(N). The variable ? stores the
Viterbi-max score for each non-terminalAi ? V at each cell (b, e), representing the span?s
most probable derivation rooted in Ai. Backpointers indicating which argument(s)
maximize each ?i(b, e) can be optionally recorded to efficiently extract the maximum
likelihood solution at the end of inference, but we omit these for clarity; they are easily
recoverable by storing the argmax for each max in lines 8 and 10.
We have also included pseudocode in Algorithm 1 to process unary productions in
lines 9 and 10. Unary processing is a necessary step to recover the gold-standard trees of
the Penn treebanks (Marcus, Marcinkiewicz, and Santorini 1993; Xue et al 2005), but is
often ignored in the presentation of the CYK algorithm. Because there is little discussion
about unary processing in the literature, implementation details often differ from parser
to parser. In Algorithm 1 we present a simplified version of unary processing that only
allows unary chains of length 1 per span (excluding lexical productions). This approach
is efficient and can be iterated as needed to represent the length of observed unary
chain in the treebank. Note that line 10 uses the temporary variable ? to store the
accumulated Viterbi-max scores ?. This temporary variable is necessary due to the
iterative nature in which we update ?. If we were to write the result of line 10 directly to
?i(b, e), then the subsequent maximization of ?i+1(b, e) would use an unstable version of
?, some of which would already be updated with unary productions, and some which
would not.
3.3 Incomplete Edges: Chomsky Normal Form and Dotted-Rules
A key feature to the efficiency of the CYK algorithm is that all productions in the
grammar G are assumed to have no more than two right-hand-side children. Rather
than trying to combine an arbitrary number of smaller substrings (child cells), the CYK
algorithm exploits shared structure between rules and only needs to consider pairwise
combination. To conform to this requirement, incomplete edges are needed to represent
that further combination is required to achieve a complete edge. This can either be
performed in advance, for example, by transforming a grammar into Chomsky Normal
Form resulting in ?incomplete? non-terminals created by the transform, or incomplete
edges can be represented through so-called dotted rules, as with the Earley (1970)
algorithm, in which transformation is essentially performed on the fly. For example,
if we have a rule production A ? B C D ? P, a completed B entry in chart cell (b,m1)
and a completed C entry in chart cell (m1+1,m2), then we can place an incomplete edge
A ? B C ?D in chart cell (b,m2). The dot signifies the division between what has already
been combined (left of the dot), and what remains to be combined. Then, if we have an
incomplete edge A ? B C ?D in chart cell (b,m2) and a complete D in cell (m2+1, e), we
can place a completed A entry in chart cell (b, e).
Transforming a grammar into ChomskyNormal Form (CNF) is an off-line operation
that converts rules with more than two children on the right-hand side into multiple
binary rules. To do this, composite non-terminals are created during the transformation,
which represent incomplete constituents (i.e., those edges that require further combina-
tion to be made complete).1 For example, if we have a rule production A ? B C D in
1 In this section we assume that edges are extended from left-to-right, which requires a left-binarization of
the grammar, but everything carries over straightforwardly to the right-binarized case.
725
Computational Linguistics Volume 38, Number 4
the context-free grammar G, then a new composite non-terminal would be created (e.g.,
@A:BC) and two binary rules would replace the previous ternary rule: A ? @A:BC D
and @A:BC ? B C. The @A:BC non-terminal represents part of a rule expansion that
needs to be combined with something else to produce a complete non-terminal from
the original set of non-terminals.2
In addition to binarization, one frequent modification to the grammar is to create
a Markov model of the dependencies on the right-hand side of the rule. One way to
do this is to reduce the number of children categories annotated on our new composite
non-terminals introduced by binarization. For example, if instead of @A:BC we assign
the label @A:C to our new non-terminal?with the semantics ?an incomplete A con-
stituent with rightmost child C??then the two rules that result from binarization are:
A ? @A:C D and @A:C ? B C. Probabilistically, the result of this is that the children
non-terminals B and D are conditionally independent of each other given A and C.
This approach will provide probability mass to combinations of children of the original
category A that may not have been observed together, hence it should be seen as a form
of smoothing. One can go further and remove all children categories from the new non-
terminals (i.e., replacing @A:BC with just @A). This is the binarization pursued in the
Berkeley parser, and is shown in Figure 1b.
In this article, we explicitly discuss unary productions of type A ? B where B is a
non-terminal, and include these productions in our grammar. These productions violate
the definition of a CNF grammar, and therefore we will use the term ?binarized gram-
mar? for the remainder of the article to indicate a grammar in CNF with the addition
of unary productions. We will assume that all of our grammars have been previously
binarized and we define V? to be the set of non-terminals that are created through
binarization, and denote edges where A ? V? as incomplete edges. Note that categories
A ? V? are only used in binary productions, not unary productions, a consideration that
will be used in our constrained parsing algorithm.
4. Finite-State Chart Constraints
In this section, we will explicitly define our chart constraints, and present methods for
using the constraints to constrain parsing. We begin with constraints on beginning or
ending multi-word constituents, then move to constraining span-1 chart cells.
4.1 Constituent Begin and End Constraints
Our task is to learn which words (in the appropriate context) can begin (B) or end (E)
multi-word constituents. We will treat this as a pre-processing step to parsing and use
these constraints to either completely or partially close chart cells during execution of
the CYK algorithm.
First, let us introduce notation. Given a set of labeled pairs (S,T) where S is a string
of n words w1 . . .wn and T is the target constituent parse tree for S, we say that word
wb ? B if there is a constituent spanning wb . . .we for some e > b and wb ? B otherwise.
Similarly, we say that word we ? E if there is a constituent spanning wb . . .we for some
b < e and we ? E otherwise. Recovery of these labels will be treated as two separate
binary tagging tasks (B/B and E/E).
2 In this example, the symbol ?@? indicates that the new non-terminal is incomplete, and the symbol ?:?
separates the original parent non-terminal from the children.
726
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Figure 2
Begin (a), End (b), Unary (c), and the combination of the three (d) type of constraints for the
dynamic programming chart used to parse As usual, the real-estate market had overreactedwith
a left-binarized grammar (see Figure 1). Black denotes ?closed? cells, white cells are ?open,?
and gray cells are only open to a restricted population (gray cells closed by E constraints only
allow incomplete edges; gray cells closed by U constraints only allow POS tags).
Although it may be obvious that we can rule out multi-word constituents with
particular begin and end positions, there may be incomplete structures within the parser
that should not be ruled out by these same constraints. Hence the notion of ?closing?
a chart cell is slightly more complicated than it may initially seem (which accounts
for our use of quotes around the term). Consider the chart representation in Figure 2
with the following constraints, where B is the set of words disallowed from beginning a
multi-word constituent and E is the set of words disallowed from ending a multi-word
constituent3
B : {?usual?, ?, ?, ?real-estate?, ?market?, ?overreacted?}
E : {?, ?, ?the?, ?real-estate?, ?had?}
Given our constraints, suppose that wb is in class B and we is in class E, for b < e.
We can ?close? all cells (b,m1) such that m1 > b and all cells (m2, e) such that m2 < e,
based on the fact that multi-word constituents cannot begin with word wb and cannot
end with we. In Figure 2 this is depicted by the black and gray diagonals through the
chart, ?closing? those chart cells.
3 In this example we list the actual words from the sentence for clarity with Figure 2, but in practice we
classify word positions as a specific word may occur multiple times in the same sentence with potentially
different B or E labels.
727
Computational Linguistics Volume 38, Number 4
If a chart cell (b, e) has been ?closed? due to begin or end constraints then it is clear
that complete edges should not be permitted in the cell since these represent precisely
the multi-word constituents that are being ruled out. But what about incomplete edges
that are introduced through grammar binarization or dotted rule parsing? To the extent
that an incomplete edge can be extended to a valid complete edge, it must be allowed.
There are two cases where this is possible. If wb ? B, then under the assumption that
incomplete edges are extended from left-to-right (see footnote 1), the incomplete edge
should be discarded, because any completed edges that could result from extending that
incomplete edge would have the same begin position. Stated another way, if wb ? B
for chart cell (b, e) then all chart cells (b, i) for i > b must also be closed. For example,
in Figure 2, the cell associated with the two-word substring real-estate market can be
closed to both complete and incomplete edges, since real-estate ? B, and any complete
edge built from entries in that cell would also have to start with the same word and
hence would be discarded. Thus, the whole diagonal is closed. If, however, wb ? B and
we ? E, such as the cell associated with the two-word substring the real-estate in Figure 2,
a complete edge?achieved by extending the incomplete edge?may end at wi for i > e,
and cell (b, i) may be open (the real-estate market), hence the incomplete edge should be
allowed in cell (b, e).
In Section 4.4 we discuss limitations on how such incomplete edges arise in closed
cells, which has consequences for the worst-case complexity under certain conditions.
4.2 Unary Constraints
In addition to begin and end constraints, we also introduce unary constraints in span-1
cells. Although we cannot close span-1 cells entirely because each of these cells must
contain at least one POS tag, we can reduce the population of these cells by restricting
the type of constituents they contain.We define a single-word constituent (SWC) as any
unary production A ? B in the grammar such that B is a non-terminal (not a lexicon
entry) and the production spans a single word. The productions ADJP ? JJ and VP ?
VBN in Figure 1a are examples of SWCs. Note that TOP ? S and JJ ?usual in Figure 1a
are also unary productions, but by definition they are not SWC unary productions. We
train a distinct tagger, as is done for B and E constraints, to label each word position as
either in U or U, indicating that the word position may or may not be extended by a
SWC, respectively.
Because the search over possible grammar extension from two child cells in the
CYK algorithm is analogous to a database JOIN operation, the efficiency of this cross-
product hinges on the population of the two child cells that are intersected. We focus on
constraining the population of span-1 chart cells for three reasons. First, the begin/end
constituent constraints only affect chart cells spanning more than one word and leave
span-1 chart cells completely unpruned. By pruning entries in these span-1 cells, we
complement multi-word constituent pruning so that all chart cells are now candidates
for finite-state tagging constraints. Second, span-1 chart cells are the most frequently
queried cells in the CYK algorithm. The search over possible midpoints will always
include two cells spanning a single word?one as the first left child and one as the
last right child. It is therefore important that the number of entries in these cells be
minimized to make bottom?up CYK processing efficient. Finally, as we will show in
Section 5, only 11.2% of words in the WSJ treebank are labeled with SWC productions.
With oracle unary constraints, the possibility of constraining nearly 90% of span-1 chart
cells has promising efficiency benefits to downstream processing.
728
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
To reiterate, unary constraints in span-1 chart cells never close the cell completely
because each span-1 cell must contain at least one POS non-terminal. Instead, if wi ? U
then we simply do not add phrase-level non-terminals to chart cell (i, i). Similar to chart
cells spanning multiple words that cannot be closed completely, these span-1 chart cells
partially restrict the population of the cell, which we will empirically show to reduce
processing time over unconstrained CYK parsing. These partially closed chart cells are
represented as gray in Figure 2.
4.3 The Constrained CYK Algorithm
Our discussion of cell closing in Section 4.1 highlighted different conditions for closing
cells for complete and incomplete edges. We will refer to specific conditions in the pseu-
docode, which we enumerate here. The constraints determine three possible conditions
for cell (b, e) spanning multiple words:
1. wb ? B: cell is closed to all constituents, both complete and incomplete
2. wb ? B and we ? E: cell is closed to complete constituents
3. wb ? B and we ? E: cell is open to all constituents
and two possible conditions for cell (i, i) spanning a single word:
4. wi ? U: cell is closed to unary phrase-level constituents
5. wi ? U: cell is open to all constituents
We will refer to a chart cell (b, e) that matches the above condition as ?case 1 cells,?
?case 2 cells,? and so on, throughout the remainder of this article. Figure 2 represents
these five cases pictorially, where case 1 cells are black, case 2 and 4 cells are gray, and
case 3 and 5 cells are white.
Algorithm 2 contains pseudocode of our modified CYK algorithm that takes into
account B, E, and U constraints. Line 4 of Algorithm 2 is the first modification from
the standard CYK processing of Algorithm 1: We now consider only words in set B to
begin multi-word constituents. Chart cells excluded from this criteria fall into case 1
and require no work. Line 5 determines if chart cell (b, e) is of case 2 (partially open)
or case 3 (completely open). If we ? E, then we skip to lines 16?17 and only incomplete
edges are permitted. Note that there is only one possible midpoint for case 2 chart cells,
which results in constant-time work (see proof in Section 4.4) and unary productions
are not considered because all entries in the cell are incomplete constituents, which only
participate in binary productions. Otherwise, if we ? E on Line 5, then the cell is open
to all constituents and processing occurs as in the standard CYK algorithm (lines 6?10
of Algorithm 2 are identical to lines 4?8 of Algorithm 1). Finally, unary productions are
added in lines 12?14, but restricted to multi-word spanning chart cells, or span-1 cells
where wb ? U.
4.4 Proofs to Bound Worst-Case Complexity
All of the proofs in this section rely on constraints imposed by B and E. Pruning
provided by the U constraints reduces decoding time in practice, but does not provide
additional reductions in complexity. Our proofs of complexity bounds will rest on
the number of cells that fall in cases 1?3 outlined in the Section 4.3, and the amount
729
Computational Linguistics Volume 38, Number 4
Algorithm 2 CONSTRAINEDCYK
Pseudocode of a modified CYK algorithm, with constituent begin/end and unary con-
straints. Unary processing is simplified to allow only chains of length one (excluding
lexical unary productions). Backpointer storage is omitted.
Input:
w1 . . .wn: Input sentence
G: Left-binarized PCFG
V?: Set of binarized non-terminals from V
B,E,U: Begin, End, and Unary chart constraints
Output:
?: Viterbi-max scores for all non-terminals over every span
CONSTRAINEDCYK(w1 . . .wn, G = (V,T, S
?,P,?),V?,B,E,U)
1: for s = 1 to n do  Span width: bottom-up traversal
2: for b = 1 to n?s+1 do  Begin word position
3: e ? b+s?1
4: if wb ? B or s = 1 then  Case 1 cells excluded
5: if we ? E or s = 1 then
6: for Ai ? V do
7: if s = 1 then  Add lexical productions
8: ?i(b, b)? ?(Ai ? wb)
9: else  Case 3: cell open
10: ?i(b, e)? max
b?m<e
(
max
j,k
?(Ai ? Aj Ak) ?j(b,m) ?k(m+ 1, e)
)
11: if s > 1 or wb ? U then  Case 5 for span-1 cells
12: for Ai ? V do  Add unary productions
13: ?i(b, e)? max
(
?i(b, e) , max
j
?(Ai ? Aj) ?j(b, e)
)
14: ?(b, e)? ?(b, e)
15: else  Case 2: closed to complete constituents
16: for Ai ? V? do  Only consider binarized non-terminals
17: ?i(b, e)? max
j,k
?(Ai ? Aj Ak) ?j(b, e? 1) ?k(e, e)
18: return ?
of work in each case. The amount of work for each case is related to how the CYK
and CONSTRAINEDCYK algorithms performs their search. Each cell (b, e) in the chart
spans the substring wb . . .we, and building non-terminal categories in that cell involves
combining non-terminal categories (via rules in the context-free grammar) found in cells
spanning adjacent substrings wb . . .wm and wm+1 . . .we. The substring wb . . .we can be
as large as the entire sentence, requiring a search overO(N) possiblemidpointwordswm.
This accounts for the linear amount of work in these case 3, open, cells.
More formally, we can define an upper bound on the work required in a chart cell
as W = |P| ? |M| where |P| is the number grammar productions bounded above by the
constant size of the grammar, and |M| is the number ofmidpoints considered.We denote
the upper bound on the amount of work done in case 1 cells byW1 and the total number
of case 1 cells considered by C1 (similarly for case 2 and 3). With this decomposition, the
complexity of the CONSTRAINEDCYK algorithm can be written as O(|C1|W1 + |C2|W2 +
|C3|W3). Because there is no work for case 1 cells (W1 = 0), these can be ignored. In fact,
we can reduce the complexity even further.
730
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Theorem 1
W2 is constant and CONSTRAINEDCYK has complexity O(|C2|+ |C3|W3).
Proof
Without loss of generality, we assume a left-binarized grammar. The upper bound on re-
quiredwork for cell (b, e) is defined asW = |P| ? |M|where |P| is the number of grammar
productions bounded above by the constant size of the grammar, and |M| is the num-
ber of midpoints considered. For some (b, e) where 1 ? b < e ? n, assume wb ? B and
we ? E, that is, (b, e) ? C2. The possible child cells of (b, e) are (b,m) and (m+ 1, e) where
b ? m < e. Because we assume a left-binarized grammar, the production?s right child
must be a complete non-terminal and (m+ 1, e) ? C3 must hold. But we ? E so (m+ 1, e)
cannot be in C3 unless m = e? 1 (span 1 cell). Therefore, the only valid midpoint is
m = e? 1 andW2 = |P|?1 is constant, hence O(|C2|W2 + |C3|W3) = O(|C2|+ |C3|W3). 
In summary, we find that case 2 chart cells (gray in Figure 2) are surprisingly cheap
to process because the right child cell must also be in case 2, resulting in only one
possible midpoint to consider. Next, we show that bounding the number of open cells
and applying Theorem 1 leads to quadratically bounded parsing.
Theorem 2
If |C3| < ?N for some constant ? then CONSTRAINEDCYK has complexity O(N2).
Proof
The total number of cells in the chart for a string of length N is N(N + 1)/2, therefore
|C2| < N2. Further, the number of midpoints for any cell is less than N, hence W3 < N.
If we bound |C3| < ?N, then it follows directly from Theorem 1 that CONSTRAINEDCYK
has complexity O(N2 + ?N ?N) = O(N2). 
We have just proved in Theorem 2 that we can reduce the complexity of constituent
parsing from O(N3) to O(N2) by restricting the number of open cells in the chart via
constraints B and E. Next, we will show that this bound can be reduced even further
to O(N) when the number of words in B is held constant. We call this approach ?linear
constraints.? There are two things to note when considering these constraints. First,
the proof relies only on the size of set B, leaving E potentially empty and limiting
the efficiency gains on short sentence compared to high-precision and quadratically
bounded constraints. Second, limiting the size of B to a constant value is a restrictive
criterion. When applied to a long sentence, this will over-constrain the search space and
impose a branching bias on possible trees. In Section 7.2 we show that if the branching
bias from linear constraints matches the branching bias of the treebank, then accuracy
is not severely affected and linear constraints can provide a useful bound on long
sentences that often dominate computational resources.
We first prove limits on the total number of open chart cells (case 3) and work
required in these cells, which directly leads to the proof bounding the complexity of
the algorithm.
Lemma 1
If |B| ? ? for some ?, thenW3 ? ?|P|.
731
Computational Linguistics Volume 38, Number 4
Proof
It has been defined thatW1 is zero and shown in Theorem 1 thatW2 is constant. For chart
cell (b, e) ? C3 there are e? b possiblemidpoints to consider. For somemidpointmwhere
b ? m < e, ifwm ? B then (m, e) ? C1 and contains no complete or incomplete constituent
entries. Therefore midpoint m is not a valid midpoint for cell (b, e). Because |B| ? ?,
there are at most ? midpoints to consider and the work required in (b, e) is bounded
above by the constant number of productions in the grammar (|P|) and ? midpoints,
thusW3 ? ?|P|. 
Lemma 2
If |B| ? ? for some ?, then |C2|+ |C3| ? ?N.
Proof
For a string of length N and some wb ? B, there are at most N substrings wb . . .we to
consider, hence O(N) chart cells for each word wb ? B. Because |B| ? ?, then there are
at most ?N cells (b, e) ? C1 and |C2|+ |C3| ? ?N. 
Theorem 3
If |B| ? ? for some ? then CONSTRAINEDCYK has complexity O(N).
Proof
From Theorem 1 we know that CONSTRAINEDCYK has complexity O(|C2|+ |C3|W3).
Given that |B| ? ?, it follows from Lemmas 1 and 2 that CONSTRAINEDCYK has com-
plexity O(?2N|P|) = O(N). 
In this section, we have been discussing worst-case complexity bounds, but there is
a strong expectation for large typical-case complexity savings that aren?t explicitly
accounted for here. To provide some intuition as to why this might be, consider again
the chart structure in Figure 2d. The black cells in the chart represent the cells that have
been closed when wj ? B (case 1 cells). Because there is no work required for these cells,
the total amount of work required to parse the sentence is reduced. The quadratic bound
does not include any potential reduction of work for the remaining open cells, however.
Thus the amount of work to parse the sentence will be less than the worst-case quadratic
bound because of this reduction in processing. In Section 7.2 we compute the empirical
complexity of each constraint method and compare that with its theoretical bound.
5. Tagging Chart Constraints
To better understand the proposed tagging tasks and their likely utility, we will first
look at the distribution of classes and our ability to automatically assign them correctly.
Note that we do not consider the first word w1 and the last word wN during the
begin-constituent and end-constituent prediction tasks because they are unambiguous
in terms of whether they begin or end constituents of span greater than one. The first
wordw1 must begin a constituent spanning the whole string, and the last wordwN must
end that same constituent. The first word w1 cannot end a constituent of length greater
than 1; similarly, the last word wN cannot begin a constituent of length greater than 1.
We therefore omit B and E at these two word positions from prediction, leading to
N ? 2 begin-constituent and N ? 2 end-constituent ambiguous predictions for a string
of length N.
732
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Table 1 displays word statistics from the training sections of the Penn English
treebank (Marcus, Marcinkiewicz, and Santorini 1993) and the Penn Chinese treebank
(Xue et al 2005), and the positive and negative class label frequency of all words in
the data.
From the nearly 1 million words in the English corpus, just over 870,000 are neither
the first nor the last word in the string, therefore possible members of the sets B or E (i.e.,
neither beginning a multi-word constituent (B) nor ending a multi-word constituent
(E)). Of these 870,000 words, over half (50.5%) do not begin multi-word constituents,
and nearly three quarters (74.3%) do not end multi-word constituents. The skewed dis-
tribution of E to E reflects the right-branching structure of English. Finally, almost 90%
of words are not labeled with a single-word constituent, demonstrating the infrequency
at which these productions occur in the English treebank.
The Chinese treebank is approximately half of the size of the English treebank
in terms of the number of sentences and word count. Again, we see a bias towards
right-branching trees (|B| > |E|), but the skew of the distributions is much smaller
than it is for English. The most significant difference between the two corpora is the
percentage of single-word constituents in Chinese compared with English. Due to the
annotation guidelines of the Chinese treebank, nearly 40% of words contain single-word
constituent unary productions. Because these occur with such high frequency, we can
assume that unary constraints may have a smaller impact on efficiency for Chinese than
for English, because an accurate tagger will constrain fewer cells. We still have the
expectation, though, that accurate tagging of SWC productions will increase parsing
accuracy for both English and Chinese.
To automatically predict the class of each word position, we train a binary predictor
from supervised data for each language/word-class pair, tuning performance on the
respective development sets (WSJ Section 24 for English and Penn Chinese Treebank
articles 301-325 for Chinese). Word classes are extracted from the treebank trees by
observing constituents beginning or ending at each word position, or by observing
single word constituents. We use the tagger from Hollingshead, Fisher, and Roark
(2005) to train six linear models (three for English, three for Chinese) with the averaged
perceptron algorithm (Collins 2002).
Table 2 summarizes the features implemented in our tagger for B, E, and U identifi-
cation. In the table, the? features are instantiated as POS-tags (provided by a separately
trained log-linear POS-tagger) and ? features are instantiated as constituent tags (B,
E, and U class labels). The feature set used in the tagger includes the n-grams of
surrounding words, the n-grams of surrounding POS-tags, and the constituent tags of
Table 1
Statistics on extracted word classes for English (Sections 2?21 of the Penn WSJ treebank) and
Chinese (articles 1?270 and 400?1151 of the Penn Chinese treebank).
Corpus totals Begin class End class Unary class
Strings Words B B E E U U
English
Count 39,832 950,028 430,841 439,558 223,544 646,855 105,973 844,055
Percent 49.5 50.5 25.7 74.3 11.2 88.8
Chinese
Count 18,086 493,708 188,612 269,000 165,591 292,021 196,732 296,976
Percent 41.2 58.8 36.2 63.8 39.9 60.1
733
Computational Linguistics Volume 38, Number 4
Table 2
Tagger features for B, E, and U.
LEX ORTHO POS
?i ?i,wi ?i,wi[0] ?i,?i
?i?1, ?i ?i,wi?1 ?i,wi[0..1] ?i,?i?1
?i?2, ?i ?i,wi+1 ?i,wi[0..2] ?i,?i?1,?i
?i?2, ?i?1, ?i ?i,wi?2 ?i,wi[0..3] ?i,?i+1
?i,wi+2 ?i,wi[n] ?i,?i,?i+1
?i,wi?1,wi ?i,wi[n-1..n] ?i,?i?1,?i,?i+1
?i,wi,wi+1 ?i,wi[n-2..n] ?i,?i?2
?i,wi[n-3..n] ?i,?i?2,?i?1
?i,wi ? Digit ?i,?i?2,?i?1,?i
?i,wi ? UpperCase ?i,?i+2
?i,wi ? Hyphen ?i,?i+1,?i+2
?i,?i,?i+1,?i+2
All lexical (LEX), orthographic (ORTHO), and part-of-speech (POS) features are duplicated to also occur
with ?i?1; e.g., {?i?1, ?i,wi} as a LEX feature.
the preceding words. The n-gram features are represented by the words within a three-
wordwindow of the current word. The tag features are represented as unigram, bigram,
and trigram tags (i.e., constituent tags from the current and two previous words). These
features are based on the feature set implemented by Sha and Pereira (2003) for NP
chunking. Additional orthographical features are used for unknown and rare words
(words that occur fewer than five times in the training data), such as the prefixes and
suffixes of the word (up to the first and last four characters of the word), and the pres-
ence of a hyphen, a digit, or a capitalized letter, following the features implemented by
Ratnaparkhi (1999). Note that the orthographic feature templates, including the prefix
(e.g., wi[0..1]) and suffix (e.g., wi[n-2..n]) templates, are only activated for unknown and
rare words. When applying our tagging model to Chinese data, all feature functions
were left in the model as-is, and not tailored to the specifics of the language.
We ran various tagging experiments on the development set and report accuracy
results in Table 3 for all three predictions tasks, using Viterbi decoding. We trained
Table 3
Tagging accuracy on the respective development sets (WSJ Section 24 for English and Penn
Chinese Treebank articles 301?325 for Chinese) for binary classes B, E, and U, for various
Markov orders.
Tagging Task Markov order
0 1 2
English
B (no multi-word constituent begin) 96.7 96.9 96.9
E (no multi-word constituent end) 97.3 97.3 97.3
U (no span-1 unary constituent) 98.3 98.3 98.3
Chinese
B (no multi-word constituent begin) 94.8 95.4 95.2
E (no multi-word constituent end) 96.2 96.4 96.6
U (no span-1 unary constituent) 95.9 96.2 96.3
734
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
models with Markov order-0 (constituent tags predicted for each word independently),
order-1 (features with constituent tag pairs), and order-2 (features with constituent tag
triples). In general, tagging accuracy for English is higher than for Chinese, especially
for the U and B tasks. Given the consistent improvement from Markov order-0 to
Markov order-1 (particularly on the Chinese data), and for the sake of consistency, we
have chosen to perform Markov order-1 prediction for all results in the remainder of
this article.
6. Experimental Set-up
In the sections that follow, we present empirical trials to examine the behavior of chart
constraints under a variety of conditions. First, we detail the data, evaluation, and
parsers used in these experiments.
6.1 Data Sets and Evaluation
For English, all stochastic grammars are induced from the Penn WSJ Treebank (Marcus,
Marcinkiewicz, and Santorini 1993). Sections 2?21 of the treebank are used as training,
Section 00 as held-out (for determining stopping criteria during training and some
parameter tuning), Section 24 as development, and Section 23 as test set. For Chinese,
we use the Penn Chinese Treebank (Xue et al 2005). Articles 1?270 and 400?1151 are
used for training, articles 301?325 for both held-out and development, and articles 271?
300 for testing. Supervised class labels are extracted from the non-binarized treebank
trees for B, E, and U (as well as their complements).
All results report F-measure labeled bracketing accuracy (harmonic mean of labeled
precision and labeled recall) for all sentences in the data set (Black et al 1991), and
timing is reported using an Intel 3.00GHz processor with 6MB of cache and 16GB of
memory. Timing results include both the pre-processing time to tag the chart constraints
as well as the subsequent context-free inference, but tagging time is relatively negligible
as it takes less than three seconds to tag the entire development corpus.
6.2 Tagging Methods and Closing Chart Cells
We have three separate tagging tasks, each with two possible tags for every word wi in
the input string: (1) B or B; (2) E or E; and (3) U or U. Our taggers are as described in
Section 5.
Within a pipeline system that leverages hard constraints, one may want to choose
a tagger operating point that favors precision of constraints over recall to avoid over-
constraining the downstream parser. We have two methods for trading recall for preci-
sion that will be detailed later in this section, both relying on calculating the cumulative
score Si for each of the binary tags at each word position wi. That is, (using B as the
example tag):
Si(B | w1 . . .wn) = log
?
?1...?n
?(?i,B) e
?(w1...wn,?1...?n )?w (1)
where
?
sums over all possible tag sequences for sentence w1 . . .wn; ?(?i,B) = 1 if ?i =
B and 0 otherwise; ?(w1 . . .wn, ?1 . . . ?n) maps the word string and particular tag string
to a d-dimensional (global) feature vector; andw is the d-dimensional parameter vector
735
Computational Linguistics Volume 38, Number 4
estimated by the averaged perceptron algorithm. Note that this cumulative score over
all tag sequences that have B in position i can be calculated efficiently using the forward?
backward algorithm. We can compare the cumulative scores Si(B) and Si(B) to decide
how to tag word wi, and define the cumulative score ratio (CSR) as follows:
CSR(wi) = Si(B)? Si(B) (2)
If we want to tag Bwith high precision, and thus avoid over-constraining the parser, we
can change our decision criterion to produce fewer such tags. We present two different
selection criteria in the next two subsections.
To show the effect of precision-oriented decision criteria, Figure 3 shows the preci-
sion/recall tradeoff at various operating points, using the global high-precision method
detailed herein, for all three tags on both the English and the Chinese development
sets. As expected, we see that the English B curve is significantly lower than the
English E and U curves. This is due to the near-uniform prior on B in the data (E and
U are much higher-frequency classes). Still, we can achieve 99% precision for B with
recall above 70%. We do much better with E and U and see that when precision is 99%
for these two tags, recall does not drop below 90%. For the Chinese tagging task, B, E,
and U all have similar performance as we trade precision for recall. Here, as with the
English B tag, we achieve 99% precision with recall still above 70%.
We can see from these results that our finite-state tagging approach yields very
high accuracy on these tasks, as well as the ability to provide high precision (above
99%) operating points with a tolerable loss in recall. In what follows, we present two
approaches to adjusting precision: first by adjusting the overall precision and recall of
the tagger directly, as shown here; and second, by adjusting the precision and recall
of tagging results on a per-sentence basis. We then discuss how complexity-bounded
constraints are implemented in our experiments. In Section 7.1 we discuss empirical
results showing how adjusting the tagger in these ways affects parsing performance.
Figure 3
Tagger precision/recall tradeoff of B, E, and U on the development set for English (a) and
Chinese (b).
736
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
6.2.1 Global High Precision. Global high precision constraints (GHP) are implemented
using the cumulative score ratios directly to determine if wb ? B or B.4 Given the
cumulative score ratio as defined herein, we define the set B under global high precision
constraints as:
GHP?(wi . . .wN ) = {wi ? B : CSR(wi) > ?} (3)
The default decision boundary ? = 0 provides high tagging accuracy, but as men-
tioned in Section 6.2 we may want to increase the precision of the tagger so that
the subsequent parser is not over constrained. To do this, we increase the threshold
parameter ? towards positive infinity, potentially moving some words in the corpus
from B to B. The same procedure is applied to E and U tags.
6.2.2 Sentence-Level High Precision. Global high precision constraints, as defined here,
affect the precision of the tagger over the entire corpus, but may or may not affect
individual sentences. Because parsing operates on a sentence-by-sentence basis, it may
be beneficial to modify the precision of the tagger based on the current sentence being
processed. We call this approach sentence-level high precision (HP).5
To increase tagging precision at the sentence level, we compute the cumulative
scores Si and the cumulative scores ratio (CSR) at all word positions as described in
this article. We next tag all word positions with a CSR(wi) score less than zero with
B, and rank the remaining word positions according to their CSR score. The lowest-
ranking ? fraction of this sorted list are tagged as B, and the rest default to B. When
? = 0, zero percent of words are tagged with B (i.e., no constraints are applied); and
when ? = 1, 100 percent of the words with CSR(wi) > 0 are tagged with B. This ensures
that the high-precision threshold is adapted to each sentence, even if its absolute CSR
scores are not similar to others in the corpus.
6.2.3 Quadratic Bounds.We impose quadratic bounds on context-free parsing by restrict-
ing the number of open cells (case 3 in Section 4.3) to be less than or equal to ?N for some
tuning parameter ? and sentence lengthN. This only involves the sets B and E, as unary
constraints are restricted to span-1 cells. Given gold parse trees t? and a function T(B,E)
to generate the set of all valid parse trees satisfying constraints B and E, the optimal
quadratically bounded constraints would be:
Quad?(wi . . .wN ) = argmax
B,E
( max
t?T(B,E)
F1(t
?, t) s.t. |C3| ? ?N) (4)
that is, the set of constraints that provides the optimal F-score of the best remaining
candidate in the chart while still imposing the required bound on the number of C3
cells. This equation is an instance of combinatorial optimization and solving it exactly
is NP-complete. Furthermore, we do not have access to gold parse trees t? during
parsing and must rely on the posterior scores of the tagger.
We instead use a greedy approach to approximate Equation (4). To accomplish this,
at every sentence we first sort both the B and E CSR scores for each word position
4 In our experiments these scores are in the range ?103.
5 As a historical note, we referred to this approach as simply ?high precision constraints? in Roark and
Hollingshead (2008) and Roark and Hollingshead (2009).
737
Computational Linguistics Volume 38, Number 4
into a single list. Next, we assume all word positions are in B and E, then starting
from the top of the sorted list (highest posterior probability for set inclusion), we
continue to add word positions to their respective open set and compute the number
of open cells with the given constraints while |C3| < ?N. By doing this, we guarantee
that only a linear number of case 3 cells are open in the chart, which leads to quadratic
worst-case parsing complexity.
6.2.4 Linear Bounds. Imposing O(N) complexity bounds requires constraining the
size of the set B such that |B| ? ? for some constant ?. As with quadratic complexity
bounds, we wish to find the optimal set B to fulfill these requirements:
Linear?(wi . . .wN ) = argmax
B
( max
t?T(B)
F1(t
?, t) such that |B| ? ?) (5)
We again resort to a greedy method to determine the set B, as this optimization
problem is still NP-complete and gold trees are not available. B is constructed by sorting
all word positions by their CSR scores for B, and then adding only the highest-scoring
? entries to the inclusion set. All other word positions are closed and in the set B.
Because this method does not consider the set E to impose limits on processing,
it will be shown in Section 7.2 that O(N) complexity bounding is not as effective as
a stand-alone method when compared to the quadratic complexity or high precision
constraints presented here.
6.3 Parsers
We will present results constraining several different parsers. We first present results
for exhaustive parsing using both the CYK and the CONSTRAINEDCYK algorithms. We
use a basic CYK exhaustive parser, termed the BUBS parser,6 to parse with a simple
PCFG model that uses non-terminal node-labels as provided by the Penn Treebank,
after removing empty nodes, node indices, and function tags. The results presented
here replicate and extend the results presented in Roark and Hollingshead (2009), using
a different CYK parser.7 The BUBS parser is an open-source high-efficiency parser
that is grammar agnostic and can be run in either exhaustive mode or with various
approximate inference options (detailed more fully in Section 8.1). It has been shown to
parse exhaustively with very competitive or superior efficiency compared with other
highly optimized CYK parsers (Dunlop, Bodenstab, and Roark 2011). In contrast to
the results in Roark and Hollingshead (2009), here we present results with both left-
and right-binarized PCFGs induced using a Markov order-2 transform, as detailed in
Section 3.3, and also present results for parsing Chinese.
We will then present results applying finite-state chart constraints to state-of-the-
art parsers, and evaluate the additional efficiency gains these constraints provide, even
when these parsers are already heavily pruned. To simplify the presentation of these
6 http://code.google.com/p/bubs-parser.
7 Note that there are several differences between the two parsers, including the way in which grammars
are induced, leading to different baseline accuracies and parsing times. Most notably, the parser from
Roark and Hollingshead (2009) relied upon POS-tagger output, and collapsed unary productions in a
way that effectively led to parent annotation in certain unary chain constructions. The current results
do not exploit those additional annotations, hence the baseline F-measure accuracy is a couple of
percentage points lower.
738
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
results, we concentrate in Section 7.1 on parsing with the simple Markov order-2
grammars, and then move to higher accuracy parsers in Section 8.
7. CYK Parsing with Finite-State Chart Constraints
7.1 High-Precision Constraints
As mentioned in Section 6.2, we can adjust the severity of pruning to favor preci-
sion over recall. Figure 4a shows parse time versus F-measure labeled parse accuracy
on the English development set for the baseline (unconstrained) exact-inference CYK
parser, and for various parameterizations of global high precision (GHP), sentence-level
high precision (HP), and unary constraints. Note that we see accuracy increasing over
the baseline in Figure 4a with the imposition of these constraints at some operating
points. This is not surprising, though, as the finite-state tagger makes use of lexical
Figure 4
English development set results (WSJ Section 24) applying global high precision (GHP),
sentence-level high precision (HP), and unary constraints with the CONSTRAINEDCYK
algorithm. We sweep over multiple values of ? in (a) and plot results in (b), (c), and (d) with the
optimal value for each constraint found in (a). F-measure accuracy in (b) is computed over
binned sentence lengths. Figure (d) plots the same data as (c), but zoomed in.
739
Computational Linguistics Volume 38, Number 4
information that the simple PCFG does not, hence there is complementary information
added that improves the model. The best operating points?fast parsing and relatively
high accuracy?are achieved for GHP constraints at ? = 40, for sentence-level HP at
? = 0.95, and for unary constraints at ? = 40. These high-precision parameterizations
achieve roughly an order of magnitude speed-up and between 0.2 absolute (for unary
constraints) and 3.7 absolute (for high-precision constraints) F-measure improvement
over the baseline unconstrained parser.
In order to analyze how these constraints affect accuracy with respect to sentence
length, we turn to Figure 4b. In this plot, F-measure accuracy is computed over binned
sentence lengths in increments of ten. All sentences of length greater than 60 are in-
cluded in the final bin. As one might expect, accuracy declines with sentence length
for all models because the number of possible trees is exponential in the sentence length
and errors in one portion of the tree can adversely affect prediction of other constituents
in nearby structures. We see that all constraints provide accuracy gains over the baseline
at all sentence lengths, but point out that the gains by GHP B and E constraints are larger
for longer sentences. As stated earlier, this is due to themodel-correcting behavior of cell
constraints: Lexical features are leveraged to prune trees that the PCFG may favor but
are not syntactically correct with respect to the entire sentence. Because longer sentences
are poorly modeled by the PCFG, cell constraints play a larger role in restricting the
space of possible trees considered and correcting modeling errors.
We can get a different perspective of how these constraints affect parsing time by
considering the scatter plots in Figures 4c and 4d, which plot each sentence according
to its length and parsing time at four operating points: baseline (unconstrained); global
high precision at ? = 40; sentence-level high precision at ? = 0.95; and unary at ? =
40. Figure 4c shows data points with up to 80 words and 400 msec of parsing time.
Figure 4d zooms in to under 200 msec and up to 40 words. It can be seen in each graph
that unconstrained CYK parsing quickly leaves the plotted area via a steep cubic curve
(least-squares fit is N2.9). Unary constraints operate at the same cubic complexity as
the baseline, but with a constant factor decrease in parsing time (least-squares fit is
also N2.9). The plots for GHP and HP show dramatic decreases in typical-case runtime
compared with the baseline. We again run a least-squares fit to the data and find that
both GHP and HP constraints follow a N2.5 trajectory.
The empirical complexity and run-time performance of GHP and HP are nearly
identical relative to all other chart constraints. But looking a little closer, we see in
Figures 4c and 4d that that GHP constraints are slightly faster than HP for some
sentences, and accumulated over the entire development set, this leads to both higher
accuracy (75.1 vs. 74.6 F-measure) and faster parsing time (46 vs. 50 seconds).8 This
leads to the conclusion that when optimizing parameters for high-precision constraints,
we can better tune the overall pipeline by choosing an operating point based on the
corpus-level tagger performance as opposed to tuning for sentence-specific precision
goals. Consequently, other than Table 4, we only apply GHP constraints on test set
results in the remainder of this section.
Although Figure 4a displays the constrained F-measure parse accuracy as a function
of parsing time, one may also be interested in how tagger precision directly affects parse
accuracy. To answer this question, we apply high precision constraints to sets B and E in
isolation and plot results in Figure 5. Note that when only constraints on E are applied,
no chart cells can be completely closed and parsing time does not significantly decrease.
8 See Table 4 for additional performance comparisons.
740
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Likewise, when we apply constraints on B, a chart cell is either open to all constituents
(case 3) or closed to all constituents (case 1). When constraining either B or E, we are
predicting the structure of the final parse tree, which (as can be seen in Figure 5) has
large effects on parse F-measure.
We point out in Figure 5 that to achieve optimal parsing F-measure, tagger precision
must be above 97% for both English and Chinese. For both languages, B constraints
are more forgiving and do not adversely affect parsing F-measure as quickly as E con-
straints. These results may lead one to tune two separate high-precision parameters?
one to constrain B and one to constrain E. We ran such experiments but found no
significant gains compared to tying these parameters together.
7.2 Complexity-Bounded Constraints
Figure 6a plots F-measure accuracy versus time to parse the entire development set for
two complexity bounded constraints: O(N2) and O(N). We also include the previously
plotted global high-precision constraints for comparison. The large asterisk in the plot
depicts the baseline accuracy and efficiency of standard CYK parsing without con-
straints. We sweep over various parameterizations for each method, from very lightly
constrained to very heavily constrained. The complexity-bounded constraints are not
combined with the high-precision constraints for this plot (but are later in the article).
As can be seen in Figure 6a, the linear-bounded method does not, as applied,
achieve a favorable accuracy/efficiency tradeoff curve compared with the quadratic
bound or high-precision constraints. This is not surprising, given that no words are
excluded from the set E for this method, hence far fewer constraints overall are applied
with the linear-bounded constraints.
In addition, we see in Figure 6b that unlike high precision and quadratic constraints,
the linear method hurts accuracy on longer sentences over the baseline unconstrained
algorithm, notably for sentences greater than 50 words. We attribute this to the fact
that for longer sentences, say length 65, only ? = 16 words are allowed in B for linear
Figure 5
The effects of tagger precision on parsing F-measure. B and E constraints are applied in isolation;
no other constraints are used during parsing. Results on the English development set in (a) and
Chinese in (b). Baseline unconstrained F-measure accuracy is indicated with the horizontal
black line.
741
Computational Linguistics Volume 38, Number 4
Figure 6
English development set results (WSJ Section 24), applying complexity-bounding constraints
with the CONSTRAINEDCYK algorithm. We sweep over multiple values of ? in (a) and plot
results in (b), (c), and (d) with the optimal value for each constraint found in (a). F-measure
accuracy in (b) is computed over binned sentence lengths. Figure (d) plots the same data as (c),
but zoomed in.
constraints, which severely limits the search space for these sentences to the point of
pruning gold constituents and decreasing overall accuracy.
Next we turn to the scatter plots of Figures 6c and 6d. Fitting an exponential curve
to the data for each constraint via least-squares, we find that global high-precision
constraints follow a N2.5 trajectory, quadratic at N1.6, and linear at N1.4. It is interesting
that quadratic constraints actually perform at sub-quadratic run-time complexity. This
is because the quadric complexity proof in Section 4.4 assumes that the linear number
of open cells each process O(N) midpoints. But in practice, many midpoints are not
considered due to the heavily constrained chart, decreasing the average-case runtime
of CONSTRAINEDCYK with quadratically bounded constraints.
Also interesting is that linear constraints perform worse than O(N) at N1.4. We
attribute this to the nature of the data set. When parsing with linear constraints, we
see that for short sentences parsing complexity is actually cubic. Because we allow
a constant number of word positions in the open set B, sentences with length less
742
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
than ? are completely unconstrained and all chart cells remain open. In Figure 6d it
looks as if parsing becomes linear after the sentence length notably exceeds ?, around
N = 20. Assuming our corpus contained a disproportionate number of long sentences,
empirical complexity of linear constraints should approach O(N), but due to the large
number of short sentences, we see an empirical complexity greater than O(N).
Three important points can be taken away from Figure 6. First, although we can
provably constrain parsing to linear speeds, in practice this method is inferior to
both quadratically bounded constraints and high-precision constraints. Second, high-
precision constraints are more accurate (see Figure 6b) and more efficient (see Fig-
ure 6d) for shorter strings than the quadratically bound constraints; yet with longer
strings the quadratic constraints better control parsing time than the high-precision
constraints (see Figure 6c). Finally, at the ?crossover? point, where quadratic constraints
start becoming more efficient than high-precision constraints (roughly 50?60 words,
see Figure 4c), there is a larger variance in the parsing time with high-precision con-
straints versus those with quadratic bounds. This illustrates the difference between
the two methods of selecting constraints: High-precision constraints can provide very
strong typical-case gains, but there is no guarantee of worst-case performance. In this
way, the high-precision constraints are similar to other tagging-derived constraints like
POS-tags or chunks.
7.3 Combining Constraints
Depending on the length of the string, the complexity-bound constraints may close
more or fewer chart cells than the high-precision constraints?more for long strings,
fewer for short strings. We can achieve worst-case bounds along with superior typical-
case speed-ups by combining both methods. This is accomplished by taking the union
of high-precision B, E, andU constraints with their respective complexity-bounded sets.
When combining complexity-bound constraints with high-precision constraints, we
first chose operating parameters for each complexity-bounded method at the point
where efficiency is greatest and accuracy has yet to decline. These operating points can
be seen as the ?knee? of the curves in Figure 6a. For the quadratic complexity method,
we set ? = 4, limiting the number of open cells to 4N. For the linear complexity method,
we set ? = 12, limiting the number of word positions in B to a maximum of 12 members.
Table 4 displays F-measure accuracy and parsing time (in seconds) for many indi-
vidual and combined constraints on the development set: unconstrained CYK parsing;
unary constraints; global high-precision (GHP) and sentence-level high-precision (HP)
constraints; and O(N2) and O(N) complexity-bounded constraints (Quad and Linear,
respectively). We present all parsing results in Table 4 using both a left- and right-
binarized Markov order-2 grammar so that the effects of grammar binarization on
finite-state constraints can be evaluated. Pre-processing the grammar with a right or
left binarization alters the nature and distribution of child non-terminals for grammar
productions. Because B and E constraints prune the chart differently depending on the
grammar binarization, we suspect that one method may outperform the other due to
the branching bias of the language being parsed.
We find three general trends in Table 4. First, the efficiency benefits of combin-
ing constraints are relatively small. We suspect this is because the data set contains
mostly shorter sentences. Global high-precision constraints outperform the complexity
bounded constraints on sentences of length 10 to 50, which makes up the majority of
the development set. It is not until we parse longer sentences that the trends start to
differ and exhibit characteristics of the complexity bounds. Thus by combining high-
743
Computational Linguistics Volume 38, Number 4
Table 4
English development set results (WSJ Section 24) for the CONSTRAINEDCYK algorithm with both
left- and right-binarized Markov order-2 grammars under various individual and combined
constraints.
Constraints F1 Precision Recall Seconds Speed-up
R
ig
h
t-
b
in
a
ri
z
e
d
g
ra
m
m
a
r
None (baseline CYK) 71.5 74.5 68.8 451
GHP(40) 75.3 78.6 72.3 46 9.8x
HP(0.95) 74.6 77.8 71.7 50 8.9x
Quad(4) 73.8 77.0 70.9 70 6.4x
Linear(12) 72.4 75.4 69.6 106 4.3x
Unary(40) 72.0 75.4 68.9 386 1.2x
HP(0.95) + Quad(4) 74.6 77.8 71.7 48 9.5x
HP(0.95) + Linear(12) 74.4 77.6 71.5 48 9.5x
GHP(40) + Quad(4) 75.3 78.5 72.3 45 10.0x
GHP(40) + Linear(12) 75.1 78.4 72.1 44 10.2x
GHP(40) + Unary(40) 75.7 79.4 72.4 34 13.4x
GHP(40) + Unary(40) + Quad(4) 75.8 79.5 72.4 33 13.9x
L
e
ft
-b
in
a
ri
z
e
d
g
ra
m
m
a
r
None (baseline CYK) 71.7 74.5 69.1 774
GHP(40) 75.4 78.5 72.5 66 11.2x
HP(0.95) 74.9 77.9 72.1 75 10.3x
Quad(4) 74.0 77.0 71.2 99 7.8x
Linear(24) 71.7 74.5 69.1 448 1.7x
Unary(40) 71.9 75.2 69.0 607 1.3x
HP(0.95) + Quad(4) 74.9 78.0 72.1 69 11.2x
HP(0.95) + Linear(24) 74.7 77.8 71.9 71 11.0x
GHP(40) + Quad(4) 75.4 78.5 72.5 62 12.6x
GHP(40) + Linear(24) 75.2 78.4 72.3 62 12.6x
GHP(40) + Unary(40) 75.7 79.3 72.5 37 20.9x
GHP(40) + Unary(40) + Quad(4) 75.7 79.3 72.5 37 20.9x
precision and complexity constraints, we attain the typical-case efficiency benefits of
high-precision constraints with worst-case complexity bounds.
Second, we see that the efficiency gain combining unary and high-precision con-
straints is more than additive. Unary constraints alone for the right-binarized grammar
decreased parsing time by 1.3x, but in conjunction with high-precision constraints,
parsing time is decreased from 66 seconds to 37 seconds, an additional 1.8x speed-up.
We suspect that this additional gain comes from cache efficiencies (due to the heavily
pruned nature of the chart, the population of commonly-queried span-1 chart cells have
a higher likelihood of remaining in high-speed cache, decreasing overall parsing time),
but we leave empirical verification of this theory to future work.
The third trend we see in Table 4 is that there are significant efficiency differences
when parsing with a right- or left-binarized grammar. The difference in baseline per-
formance has been previously studied (Song, Ding, and Lin 2008; Dunlop, Bodenstab,
and Roark 2010), and our results confirm that a right-binarized grammar is superior for
parsing the WSJ treebank due to the right-branching bias of parse trees in this corpus.
Furthermore, linear constraints were far less effective with a left-binarized grammar,
744
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Table 5
English test set results (WSJ Section 23) for the CONSTRAINEDCYK algorithm with both left- and
right-binarized Markov order-2 grammars.
Constraints F1 Precision Recall Seconds Speed-up
R
ig
h
t None (baseline CYK) 71.7 74.6 69.0 628
Unary(40) 72.1 75.5 69.0 525 1.2x
GHP(40) 75.8 79.0 72.8 75 8.4x
GHP(40) + Unary(40) 76.1 79.8 72.7 57 11.0x
L
e
ft
None (baseline CYK) 72.0 74.8 69.4 1,063
Unary(40) 72.4 75.8 69.4 910 1.2x
GHP(40) 76.1 79.3 73.2 106 10.1x
GHP(40) + Unary(40) 76.4 80.0 73.1 60 17.9x
Table 6
Chinese test set results (PCTB Sections 271?300) for the CONSTRAINEDCYK algorithm with both
left- and right-binarized Markov order-2 grammars.
Constraints F1 Precision Recall Seconds Speed-up
R
ig
h
t None (baseline CYK) 60.5 64.6 56.9 157
Unary(20) 62.3 67.6 57.8 141 1.1x
GHP(20) 66.9 71.4 63.0 17 9.1x
GHP(20) + Unary(20) 68.9 74.4 64.1 15 10.2x
L
e
ft
None (baseline CYK) 60.4 64.5 56.9 269
Unary(20) 62.1 67.2 57.8 234 1.2x
GHP(20) 66.0 70.5 62.1 33 8.2x
GHP(20) + Unary(20) 68.0 73.5 63.2 23 11.7x
requiring a tuning parameter of ? = 24 such that accuracy was not adversely affected
(compare with ? = 12 for right-binarized results). This is also caused by the branching
bias inherent in the treebank. For example, consider a left-binarized grammar and
linear constraints; in the extreme case where ? = 0, only w1 will be in the open set B,
forcing all constituents in the final tree to start at w1. This results in a completely left-
branching tree. With a right-binarized grammar, only the last word position will be in
E, resulting in a completely right-branching tree. Thus, an overly constrained linear-
bounded parse will favor one branching direction over the other. Because the treebank
is biased towards right-branching trees, a right-binarized grammar is more favorable
when linear constraints are applied.
Finally, we note that after all constraints have been applied, the accuracy and
efficiency differences between the two binarization strategies nearly disappear.
To validate the selected operating points on unseen data, we present results on the
test sets for English in Table 5 and Chinese in Table 6.9 We parse individually with global
high-precision and unary constraints, then present the combined result as this gave the
best performance on the development set. In all cases, we see efficiency improvements
greater than 10-fold and accuracy improvements of 4.4 absolute F-measure for English,
9 We used the development set (articles 301?325 of the Penn Chinese treebank) to tune chart constraint
parameters for Chinese, exactly as was done for English.
745
Computational Linguistics Volume 38, Number 4
and 8.4 absolute F-measure for Chinese. Note that these efficiency improvements are
achieved with no additional techniques for speeding up search; modulo the cell closing
mechanism, the CYK parsing is exhaustive?it explores all possible category combina-
tions from all open child cells. Techniques such as coarse-to-fine, A? parsing, or beam-
search are all orthogonal to the current approach, and could be applied in conjunction
to achieve additional speedups.
In the next section, we investigate the use of chart constraints with a number of
high-accuracy parsers, and empirically evaluate the combination of our chart constraint
methods with popular heuristic search methods for parsing. These parsers include the
Charniak parser, the Berkeley parser, and an in-house beam-search parser.
8. High-Accuracy Parsing with Finite-State Chart Constraints
In this section we evaluate the additive efficiency gains provided by finite-state con-
straints to state-of-the-art parsers. We apply B, E, and U constraints to three parsers, all
of which already prune the search space to make decoding time with large grammars
more practical. Each high-accuracy parser that we evaluate also prunes the search space
in a different way?agenda-based search, coarse-to-fine pruning, and beam-search.
Applying finite-state constraints to these three distinct parsers demonstrates how our
constraints interact with other well-known pruning algorithms. In what follows we
describe the pruning inherent in the three parsers and how we apply finite-state con-
straints within each framework.
8.1 Parsers
8.1.1 Charniak Parser. The Charniak (2000) parser is a multi-stage, agenda-driven parser
that can be constrained by pruning edges before they are placed on the agenda. The first
stage of the Charniak parser uses an agenda and a simple PCFG to build a sparse chart,
which is used to limit the search in later stages with the full model. We focus on this
first stage, because it is here that we will be constraining the parser. The edges on the
agenda and in the chart are dotted rules, as described in Section 3.3. When edges are
created, they are pushed onto the agenda. Edges that are popped from the agenda are
placed in the chart, and then combined with other chart entries to create new edges that
are pushed onto the agenda. Once a complete edge spanning the whole string is placed
in the chart, at least one full solution must exist. Instead of terminating the initial chart
population at this point, a technique called ?over-parsing? is used that continues adding
edges to the chart (and agenda) until a parameterized number of additional edges have
been added. A small over-parsing value will heavily constrain the search space of the
later stages within the pipeline, and a large value will often increase accuracy at the
expense of efficiency. Upon reaching the desired number of edges, the next stage of
the pipeline receives the chart as input and any edges remaining on the agenda are
discarded.
We constrain the first stage of the Charniak parser by restricting agenda edges.
When an edge is created for cell (b, e), where b < e, it is not placed on the agenda if
either of the following two conditions hold: 1) wb ? B; or 2) the edge is complete and
we ? E. With these constraints, a large number of edges that would have previously
been considered in the first stage of this pipeline will now be ignored. This allows us
to either reduce the amount of over-parsing, which will increase efficiency, or maintain
the over-parsing threshold and expand the search space in more promising directions
according to the chart constraints. In this article, we have chosen to do the latter. Note
that speed-ups are still observed, presumably due to the parser finding a complete edge
746
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
spanning the whole sentence more quickly, thus leading to slight reductions in total
edges added to the chart.
8.1.2 Berkeley Parser. The Berkeley parser (Petrov and Klein 2007a) is a multi-level coarse-
to-fine parser that operates over a set of coarse-to-fine grammars, G1 . . .Gt. At each
grammar level, the inside and outside constituent probabilities are computed with
coarse grammarGi and used to prune the subsequent search withinGi+1. This continues
until the sentence is parsed with the target grammar Gt. The initial grammar G1 is
a Markov order-0 grammar, and the target grammar Gt is a latent-variable grammar
induced through multiple iterations of splitting and merging non-terminals to maxi-
mize the likelihood of the training data (Petrov and Klein 2007b). This explicit target
grammar is very large, consisting of 4.3 million productions, 2.4 million of which are
lexical productions. We refer to Gt as the Berkeley grammar.
We apply chart constraints to the Berkeley parser during the initial inside pass
with grammar G1. Inside scores are computed via the CONSTRAINEDCYK algorithm
of Algorithm 2 modulo the fact that the standard inside/outside sum over scores is
used in lines 10, 13, and 17 instead of the Viterbi-max. It is unnecessary to constrain
either the subsequent outside pass or the search with the following grammars G2 . . .Gt,
as the coarse-to-fine algorithm only considers constituents remaining in the chart from
the previous coarse grammar. Reducing the population of chart cells during the initial
G1 inside pass consequently prunes the search space for all levels of the coarse-to-fine
search. We also note that even though there are t = 6 grammar levels in our experiments
with the Berkeley parser, exhaustive parsing with G1 consumes nearly 50% of the total
parse time (Petrov, personal communication). Applying chart constraints during this
initial pass is where we see the largest efficiency gain?much more than when chart
constraints supplement coarse-to-fine pruning in subsequent passes.
8.1.3 BUBS Parser. The bottom?up beam-search parser (BUBS) is a variation of the CYK
algorithm where, at each chart cell, all possible edges are sorted by a Figure-of-Merit
(FOM) and only the k-best edges are retained (Bodenstab et al 2011). We follow this set-
up and use the Boundary FOM (Caraballo and Charniak 1997), but do not apply beam-
width prediction in these experiments as chart constraints and beam-width prediction
prune the search space in similar ways (see Bodenstab et al [2011] for a comparison of
the two methods). The BUBS parser is grammar agnostic, so to achieve high accuracy
we parse with the Berkeley latent variable grammar (Gt described in the previous
subsection), yet only require a single pass over the chart. The BUBS parser performs
Viterbi decoding and does not marginalize over the latent variables or compute the
max-rule solution as is done in the Berkeley parser. This leads to a lower F-measure
score in the final results even though both parsers use the same grammar.
In this article, we apply finite-state constraints to the BUBS parser in a fashion
almost identical to the CONSTRAINEDCYK algorithm. Because the BUBS parser is a
beam-search parser, the difference is that instead of retaining the max score for all non-
terminals Ai at each chart cell, we only retain the max score for the k-best non-terminals.
Otherwise, B, E, and U constraints are used to prune the search space in the same way.
8.2 High-Accuracy Parsing Results
Figure 7 displays accuracy and efficiency results of applying three independent
constraints to the three parsers: high precision, quadratically bounded, and unary
constraints. We sweep over possible tuning parameters from unconstrained (baseline
747
Computational Linguistics Volume 38, Number 4
Figure 7
English accuracy and efficiency results of applying high precision, quadratic, and unary
constraints at multiple values of ? to the Charniak, Berkeley, and BUBS parsers, all of which
already heavily prune the search space.
asterisk) to overly constrained such that accuracy is adversely affected. We also plot the
optimal combination of high precision and quadratic constraints (diamond) and the
combination of all three constraints (circle) for each parser which was computed via a
grid search over the joint parameter space.
There are many interesting patterns in Figure 7. First, all three constraints inde-
pendently improve the accuracy and efficiency of all three parsers, with the exception
of accuracy in the Berkeley parser. This is a powerful result considering each of these
parsers is simultaneously performing various alternative forms of pruning, which were
(presumably) tuned for optimal accuracy and efficiency on this same data set. We also
note that the efficiency gains from all three constraints are not identical. In particular,
high precision and quadratic constraints outperforms unary constraints in isolation. But
this should be expected as unary constraints only partially closeO(n) chart cells whereas
both high precision and quadratic constraints affect O(n2) chart cells. Nevertheless,
looking at the optimal point combining all three constraints, we see that adding unary
constraints to begin/end constraints does provide additional gains (in both accuracy
and efficiency) for the BUBS and Charniak parsers.
The Berkeley parser appears to benefit from B and E constraints, but sees almost
no gain from unary constraints. The reason for this has to do with the implementation
details of combining (joining) two child cells within the inner loop of the CYK algorithm
(line 8 in Algorithm 1). In bottom?up CYK parsing, to extend derivations of adjacent
substrings into new constituents spanning the combined string, one can either iterate
over all binary productions in the grammar and test if the new derivation is valid (we
call this ?grammar loop?), or one can take the cross-product of active entries in the cells
spanning the substrings and poll the grammar for possible derivations (we call this
?cross-product?). With the cross-product approach, fewer active entries in either child
cell leads to fewer grammar access operations. Thus, pruning constituents in smaller-
span cells directly affects the overall efficiency of parsing. On the other hand, with the
748
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
grammar loop method there is a constant number of grammar access operations (i.e.,
the number of grammar productions) and the number of active entries in each child
cell has little impact on efficiency. Therefore, with the grammar loop implementation
of the CYK algorithm, pruning techniques such as unary constraints will have very
little impact on the final run-time efficiency of the parser unless the list of grammar
productions is modified given the constraints. For example, alternate iterators over
grammar productions could be created such that they only consider a subset of all
possible productions. If the left child is a span-1 chart cell in U, then only grammar
productions with a POS tag as the left child need to be considered. Looping over
this smaller list, opposed to all possible grammar productions, can reduce the overall
runtime. The Berkeley parser contains the grammar-loop implementation of the CYK
algorithm. Although all grammar productions are iterated over at each cell, the parser
maintains meta-information indicating where non-terminals have been placed in the
chart, allowing it to quickly skip over a subset of the productions that are incompatible
with state of the chart. This optimization improves efficiency, but does not take full
advantage of restricted cell population imposed by unary constraints, and thus does
not benefit as greatly when compared to the BUBS or Charniak parsers.
The second trend we see in Figure 7 is that accuracy actually improves with ad-
ditional constraints. This is expected with the Charniak parser as we keep the amount
of search space fixed in hopes of pursuing more accurate global paths, but both the
Berkeley and BUBS parsers are simply eliminating search paths they would have
otherwise considered. Although it is unusual that pruning leads to higher accuracy
during search, it is not wholly unexpected here as our finite-state tagger makes use of
lexical relationships that the PCFG does not (i.e., lexical relationships based on the linear
string rather than over the syntactic structure). By leveraging this new information to
constrain the search space, we are indirectly improving the quality of the model. We
also suspect that the Berkeley parser sees less of an accuracy improvement than the
BUBS parsers because the coarse-to-fine pruning within the Berkeley parser is more
?globally informed? than the beam-search within the BUBS parser. By leveraging the
coarse-grained inside/outside distribution of trees over the input sentence, the Berkeley
parser can more intelligently prune the search space with respect to the target grammar
and may not benefit from the additional information inherent in the finite-state tagging
model.
The third observation we point out in Figure 7 is that we see no additional gains
from combining high-precision constraints with quadratic complexity constraints. With
all three parsers, high-precision constraints are empirically superior to quadratic con-
straints, even though high-precision constraints come with no guarantee on worst-case
complexity reduction. It is our hypothesis that the additional pruning provided by
quadratic constraints for exhaustive CYK parsing is already removed by the internal
pruning of each of the three high-accuracy parsers. We therefore report testing results
using only high-precision and unary constraints for these high-accuracy parsers.
We apply models tuned on the development set to unseen English test data (WSJ
Section 23) in Table 7, and Chinese test data (PCTB articles 271?300) in Table 8. For
English, we see similar trends as we did on the development set results: Decoding time
is nearly halved when chart constraints are applied to these already heavily constrained
parsers, without any loss in accuracy. We also see independent gains from both unary
and high-precision constraints, and additive efficiency gains when combined.
Applying chart constraints to Chinese parsing in Table 8 gives substantially larger
accuracy and efficiency gains than English for both the BUBS and Berkeley parser. In
particular, the accuracy of the BUBS parser increases by 2.3 points absolute (p = 0.0002),
749
Computational Linguistics Volume 38, Number 4
Table 7
English test set results (WSJ Section 23) applying sentence-level high precision and unary
constraints to three parsers with parameter settings tuned on development data.
Parser F1 Precision Recall Seconds Speed-up
BUBS (2010) 88.4 88.5 88.3 586
+ Unary(100) 88.5 88.7 88.3 486 1.2x
+ HP(0.9) 88.7 88.9 88.6 349 1.7x
+ HP(0.9) + Unary(100) 88.7 89.0 88.4 283 2.1x
Charniak (2000) 89.7 89.7 89.6 1,116
+ Unary(100) 89.8 89.8 89.7 900 1.2x
+ HP(0.8) 89.8 90.0 89.6 716 1.6x
+ HP(0.8) + Unary(100) 89.7 90.0 89.5 679 1.6x
Berkeley (2007) 90.2 90.3 90.0 564
+ Unary(125) 90.1 90.3 89.9 495 1.1x
+ HP(0.7) 90.2 90.4 90.0 320 1.8x
+ HP(0.7) + Unary(125) 90.2 90.4 89.9 289 2.0x
Table 8
Chinese test set results (PCTB articles 271?300) applying sentence-level high-precision and unary
constraints to two parsers with parameter settings tuned on development data.
Parser F1 Precision Recall Seconds Speed-up
BUBS (2010) 79.5 79.5 79.1 169
+ Unary(50) 80.7 82.1 79.4 153 1.1x
+ HP(0.8) 81.1 81.5 80.7 75 2.3x
+ HP(0.8) + Unary(50) 81.8 83.1 80.5 44 3.8x
Berkeley (2007) 83.9 84.5 83.3 141
+ Unary(50) 84.5 85.9 83.0 125 1.1x
+ HP(0.7) 84.5 85.1 83.8 64 2.2x
+ HP(0.7) + Unary(50) 84.7 86.1 83.4 57 2.5x
and the Berkeley parser increases by 0.8 points absolute to 84.7 (p = 0.0119), the highest
accuracy we are aware of for an individual model on this data set.10,11 These increases
relative to English may be surprising as chart constraint tagging accuracy for Chinese
is worse than English (see Table 3). We attribute this large gain to the lower baseline
accuracy of parsing with the Chinese treebank, allowing our method to contribute
additional syntactic constraints that were otherwise unmodeled by the PCFG.
9. Conclusion and Future Work
We have presented finite-state pre-processing methods to constrain context-free
parsing that reduce both the worst-case complexity and overall run time. Four unique
10 Significance was tested using stratified shuffling.
11 Zhang et al (2009) report an F-measure of 85.5 with a k-best combination of parsers, and Burkett, Blitzer,
and Klein (2010) report an F-measure of 86.0 by leveraging parallel English data for training, but our
model is trained from the Chinese treebank alone and is integrated into the Berkeley parser, making it
very efficient.
750
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
bounding methods were presented, with high-precision constraints showing superior
performance in empirical trials. Applying these constraints to context-free parsing
increased efficiency by over 20 times for exhaustive CYK parsing, and nearly doubled
the speed of the Charniak and Berkeley parsers?both of which have been previously
tuned for optimal accuracy/efficiency performance. We have shown that our method
generalizes across multiple grammars and languages, and that constraining both
multi-word chart cells and single-word chart cells produces additive efficiency gains.
In future work we plan to investigate additional methods to make context-free
parsing more efficient. In particular, we believe the dynamic programming chart could
be constrained even further if we take into account the population and structure of
chart cells at a finer level. For example, the B and E constraints we have presented here
do not take into account the height of the constituent they are predicting. Instead, they
simply open or close the entire diagonal in the chart. Refining this structured prediction
to bins, as is done in Zhang et al (2010), or classifying chart cells directly, as is done
in Bodenstab et al (2011), has shown promise, but we believe that further research in
this area would yield addition efficiency gains. In particular, those two papers do not
differentiate between non-terminals when opening or closing cells, and we hypothesize
that learning separate finite-state classifiers for automatically derived clusters of non-
terminals may increase performance.
Acknowledgments
Portions of this paper have appeared in
conference papers: Roark and Hollingshead
(2008), Roark and Hollingshead (2009), and
Bodenstab, Hollingshead, and Roark (2011).
We thank three anonymous reviewers for
their insightful comments and suggestions.
Also thanks to Aaron Dunlop for being so
swell. This research was supported in part
by NSF grants IIS-0447214 and IIS-0811745,
and DARPA grant HR0011-09-1-0041.
Any opinions, findings, conclusions,
or recommendations expressed in this
publication are those of the authors and
do not necessarily reflect the views of the
NSF or DARPA.
References
Bangalore, Srinivas and Aravind K. Joshi.
1999. Supertagging: an approach to
almost parsing. Computational Linguistics,
25:237?265.
Bergsma, Shane and Colin Cherry. 2010.
Fast and accurate arc filtering for
dependency parsing. In Proceedings
of the 23rd International Conference on
Computational Linguistics (Coling 2010),
pages 53?61, Beijing.
Black, E., S. Abney, S. Flickenger,
C. Gdaniec, C. Grishman, P. Harrison,
D. Hindle, R. Ingria, F. Jelinek, J. Klavans,
M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991.
Procedure for quantitatively comparing
the syntactic coverage of English
grammars. In Proceedings of the Workshop
on Speech and Natural Language, HLT ?91,
pages 306?311, Pacific Grove, CA.
Bodenstab, Nathan, Aaron Dunlop, Keith
Hall, and Brian Roark. 2011. Beam-width
prediction for efficient context-free
parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics, pages 440?449, Portland, OR.
Bodenstab, Nathan, Kristy Hollingshead,
and Brian Roark. 2011. Unary constraints
for efficient context-free parsing. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics,
pages 676?681, Portland, OR.
Burkett, David, John Blitzer, and Dan Klein.
2010. Joint parsing and alignment with
weakly synchronized grammars. In Human
Language Technologies: The 2010 Annual
Conference of the North American Chapter
of the Association for Computational
Linguistics, HLT ?10, pages 127?135,
Los Angeles, CA.
Caraballo, Sharon A. and Eugene Charniak.
1997. New figures of merit for best-first
probabilistic chart parsing. Computational
Linguistics, 24:275?298.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 132?139, Seattle, WA.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
751
Computational Linguistics Volume 38, Number 4
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 173?180, Sydney.
Cherry, Colin and Shane Bergsma. 2011.
Joint training of dependency parsing
filters through latent support vector
machines. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 200?205, Toulouse.
Clark, Stephen and James R. Curran.
2004. The importance of supertagging
for wide-coverage CCG parsing. In
Proceedings of COLING, pages 282?288,
Geneva.
Cocke, John and Jacob T. Schwartz. 1970.
Programming languages and their
compilers: Preliminary notes. Courant
Institute of Mathematical Sciences,
New York University.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 1?8, Philadelphia, PA.
Djordjevic, Bojan, James R. Curran, and
Stephen Clark. 2007. Improving the
efficiency of a wide-coverage CCG parser.
In Proceedings of the 10th International
Conference on Parsing Technologies,
IWPT ?07, pages 39?47, Prague.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In Proceedings of the Tenth
Conference on Computational Natural
Language Learning (CoNLL-X),
pages 201?205, New York, NY.
Dunlop, Aaron, Nathan Bodenstab, and
Brian Roark. 2010. Reducing the grammar
constant: an analysis of CYK parsing
efficiency. Technical report CSLU-2010-02,
Oregon Health & Science University,
Beaverton, OR.
Dunlop, Aaron, Nathan Bodenstab,
and Brian Roark. 2011. Efficient
matrix-encoded grammars and low
latency parallelization strategies for CYK.
In Proceedings of the 12th International
Conference on Parsing Technologies (IWPT),
pages 163?174, Dublin.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. Communications of the
ACM, 6(8):451?455.
Eisner, Jason and Noah A. Smith. 2005.
Parsing with soft and hard constraints on
dependency length. In Proceedings of the
Ninth International Workshop on Parsing
Technology (IWPT), pages 30?41,
Vancouver.
Glaysher, Elliot and Dan Moldovan. 2006.
Speeding up full syntactic parsing by
leveraging partial parsing decisions. In
Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 295?300,
Sydney.
Hollingshead, Kristy, Seeger Fisher, and
Brian Roark. 2005. Comparing and
combining finite-state and context-free
parsers. In Proceedings of the Human
Language Technology Conference and the
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 787?794, Vancouver.
Hollingshead, Kristy and Brian Roark.
2007. Pipeline iteration. In Proceedings
of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 952?959, Prague.
Kasami, Tadao. 1965. An efficient
recognition and syntax analysis
algorithm for context-free languages.
Technical report AFCRL-65-758, Air Force
Cambridge Research Lab, Bedford, MA.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn treebank. Computational
Linguistics, 19:313?330.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 523?530, Vancouver.
Nivre, Joakim. 2006. Constraints on
non-projective dependency parsing.
In Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 73?80, Trento.
Petrov, Slav and Dan Klein. 2007a. Improved
inference for unlexicalized parsing. In
Proceedings of Human Language Technologies
2007: The Conference of the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 404?411,
Rochester, NY.
Petrov, Slav and Dan Klein. 2007b. Learning
and inference for hierarchically split
PCFGs. In Proceedings of the 22nd National
Conference on Artificial Intelligence -
Volume 2, pages 1663?1666, Vancouver.
752
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Ratnaparkhi, Adwait. 1999. Learning to
parse natural language with maximum
entropy models.Machine Learning,
34(1-3):151?175.
Roark, Brian and Kristy Hollingshead.
2008. Classifying chart cells for quadratic
complexity context-free inference. In
Proceedings of the 22nd International
Conference on Computational Linguistics
(COLING), pages 745?752, Manchester.
Roark, Brian and Kristy Hollingshead. 2009.
Linear complexity context-free parsing
pipelines via chart constraints. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL),
pages 647?655, Boulder, CO.
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields. In
Proceedings of HLT-NAACL, pages 134?141,
Edmonton.
Sogaard, Anders and Jonas Kuhn. 2009.
Using a maximum entropy-based tagger
to improve a very fast vine parser.
In Proceedings of the 11th International
Conference on Parsing Technologies,
IWPT ?09, pages 206?209, Paris.
Song, Xinying, Shilin Ding, and Chin-Yew
Lin. 2008. Better binarization for the CKY
parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 167?176,
Honolulu, HI.
Weiss, David, Benjamin Sapp, and Ben
Taskar. 2010. Sidestepping intractable
inference with structured ensemble
cascades. In Proceedings of NIPS,
pages 2415?2423, Vancouver.
Weiss, David and Benjamin Taskar. 2010.
Structured prediction cascades. Journal of
Machine Learning Research - Proceedings
Track, 9:916?923.
Xue, Naiwen, Fei Xia, Fu-dong Chiou, and
Marta Palmer. 2005. The Penn Chinese
treebank: Phrase structure annotation
of a large corpus. Natural Language
Engingeering, 11:207?238.
Younger, Daniel H. 1967. Recognition and
parsing of context-free languages in time
n3. Information and Control, 10(2):189?208.
Zhang, Hui, Min Zhang, Chew Lim Tan, and
Haizhou Li. 2009. K-best combination of
syntactic parsers. In Proceedings of the 2009
Conference on Empirical Methods in Natural
Language Processing: Volume 3, EMNLP ?09,
pages 1552?1560, Singapore.
Zhang, Yue, Byung Gyu Ahn, Stephen Clark,
Curt Van Wyk, James R. Curran, and
Laura Rimell. 2010. Chart pruning for
fast lexicalised-grammar parsing. In
Proceedings of the 23rd International
Conference on Computational Linguistics,
pages 1472?1479, Beijing.
753

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 600?608,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Prenominal Modifier Ordering via Multiple Sequence Alignment
Aaron Dunlop
Oregon Health & Science University
Portland, OR
dunlopa@cslu.ogi.edu
Margaret Mitchell
University of Aberdeen
Aberdeen, Scotland, U.K.
m.mitchell@abdn.ac.uk
Brian Roark
Oregon Health & Science University
Portland, OR
roark@cslu.ogi.edu
Abstract
Producing a fluent ordering for a set of
prenominal modifiers in a noun phrase
(NP) is a problematic task for natural lan-
guage generation and machine translation
systems. We present a novel approach
to this issue, adapting multiple sequence
alignment techniques used in computa-
tional biology to the alignment of modi-
fiers. We describe two training techniques
to create such alignments based on raw
text, and demonstrate ordering accuracies
superior to earlier reported approaches.
1 Introduction
Natural language generation and machine trans-
lation systems must produce text which not only
conforms to a reasonable grammatical model,
but which also sounds smooth and natural to
a human consumer. Ordering prenominal mod-
ifiers in noun phrases is particularly difficult
in these applications, as the rules underlying
these orderings are subtle and not well under-
stood. For example, the phrase ?big red ball?
seems natural, while ?red big ball? seems more
marked, suitable only in specific contexts. There
is some consensus that the order of prenom-
inal modifiers in noun phrases is governed in
part by semantic constraints, but there is no
agreement on the exact constraints necessary to
specify consistent orderings for any given set of
modifiers. General principles of modifier order-
ing based on semantic constraints also fall short
on larger domains, where it is not always clear
how to map prenominal modifiers to proposed
semantic groups.
With the recent advantages of large corpora
and powerful computational resources, work
on automatically ordering prenominal modifiers
has moved away from approaches based on gen-
eral principles, and towards learning ordering
preferences empirically from existing corpora.
Such approaches have several advantages: (1)
The predicted orderings are based on prior evi-
dence from ?real-world? texts, ensuring that they
are therefore reasonably natural. (2) Many (if
not all) prenominal modifiers can be ordered.
(3) Expanding the training data with more and
larger corpora often improves the system with-
out requiring significant manual labor.
In this paper, we introduce a novel approach
to prenominal modifier ordering adapted from
multiple sequence alignment (MSA) techniques
used in computational biology. MSA is generally
applied to DNA, RNA, and protein sequences,
aligning three or more biological sequences in or-
der to determine, for example, common ancestry
(Durbin et al, 1999; Gusfield, 1997; Carrillo and
Lipman, 1988). MSA techniques have not been
widely applied in NLP, but have produced some
promising results for building a generation map-
ping dictionary (Barzilay and Lee, 2002), para-
phrasing (Barzilay and Lee, 2003), and phone
recognition (White et al, 2006).
We believe that multiple sequence alignment
is well-suited for aligning linguistic sequences,
and that these alignments can be used to predict
prenominal modifier ordering for any given set
of modifiers. Our technique utilizes simple fea-
tures within the raw text, and does not require
any semantic information. We achieve good per-
formance using this approach, with results com-
petitive with earlier work (Shaw and Hatzivas-
siloglou, 1999; Malouf, 2000; Mitchell, 2009) and
higher recall and F-measure than that reported
in Mitchell (2009) when tested on the same cor-
pus.
600
2 Related work
In one of the first attempts at automatically or-
dering prenominal modifiers, Shaw and Hatzi-
vassiloglou (1999) present three empirical meth-
ods to order a variety of prenominal modifier
types. Their approach provides ordering deci-
sions for adjectives, gerunds (such as ?running?
in ?running man?), and past participles (such
as ?heated? in ?heated debate?), as well as for
modifying nouns (such as ?baseball? in ?base-
ball field?). A morphology module transforms
plural nouns and comparative/superlative forms
into their base forms, increasing the frequency
counts for each modifier. We will briefly re-
cap their three methods, which are categorized
as the direct evidence method, the transitivity
method, and the clustering method.
Given prenominal modifiers a and b in a train-
ing corpus, the direct evidence method com-
pares frequency counts of the ordered sequences
<a,b> and <b,a>. This approach works well,
but is limited by data sparsity; groups of two or
more modifiers before a noun are relatively in-
frequent in traditional corpora, and finding the
same pair of modifiers together more than once
is particularly rare.
To overcome this issue, Shaw and Hatzi-
vassiloglou?s transitivity and clustering meth-
ods make inferences about unseen orderings
among prenominal modifiers. In the transitiv-
ity method, given three modifiers a,b,c, where a
precedes b and b precedes c, the model concludes
that a precedes c. The clustering method calcu-
lates a similarity score between modifiers based
on where the modifiers occur in relation to the
other modifiers in the corpus. Those modifiers
that are most similar are clustered together, and
ordering decisions can be made between modi-
fiers in separate clusters. All three approaches
are designed to order pairs of modifiers; it is un-
clear how to extend these approaches to order
groups larger than a pair.
Shaw and Hatzivassiloglou find that NPs with
only adjectives as modifiers (including gerunds
and past participles) are considerably easier to
order than those which contain both adjectives
and nouns. They also find large differences in
accuracy across domains; their systems achieve
much lower overall accuracy on financial text
(the Wall Street Journal (WSJ) corpus (Marcus
et al, 1999)) than on medical discharge sum-
maries.
Looking at all modifier pairs, the authors
achieve their highest prediction accuracy of
90.7% using the transitivity technique on a med-
ical corpus. We do not have access to this cor-
pus, but we do have access to the WSJ corpus,
which provides a way to compare our methods.
On this corpus, their model produces predic-
tions for 62.5% of all modifier pairs and achieves
83.6% accuracy when it is able to make a predic-
tion. Random guessing on the remainder yields
an overall accuracy of 71.0%.
Malouf (2000) also examines the problem of
prenominal modifier ordering. He too proposes
several statistical techniques, achieving results
ranging from 78.3% to 91.9% accuracy. He
achieves his best results by combining memory-
based learning and positional probability to
modifiers from the first 100 million tokens of
the BNC. However, this evaluation is limited to
the ordering of prenominal adjectives, which is a
considerably simpler task than ordering all types
of prenominal modifiers. Malouf?s approaches
are also limited to ordering pairs of modifiers.
Mitchell (2009) proposes another approach,
grouping modifiers into classes and ordering
based on those classes. A modifier?s class is as-
signed based on its placement before a noun,
relative to the other modifiers it appears with.
Classes are composed of those modifiers that
tend to be placed closer to the head noun, those
modifiers that tend to be placed farther from the
head noun, etc., with each class corresponding
to a general positional preference. Unlike earlier
work, these classes allow more than one ordering
to be proposed for some pairs of modifiers.
Combining corpora of various genres,
Mitchell?s system achieves a token precision
of 89.6% (see Section 4 for discussion and
comparison of various evaluation metrics).
However, the model only makes predictions for
74.1% of all modifier pairs in the test data, so
recall is quite low (see Tables 4 and 6).
Overall, previous work in noun-phrase order-
601
ing has produced impressive accuracies in some
domains, but currently available systems tend
to adapt poorly to unseen modifiers and do not
generalize well to unseen domains.
3 Methods
3.1 Multiple Sequence Alignment
Multiple sequence alignment algorithms align
sequences of discrete tokens into a series of
columns. They attempt to align identical or
easily-substitutable tokens within a column, in-
serting gaps when such gaps will result in a bet-
ter alignment (more homogeneous token assign-
ments within each column). For example, con-
sider the simple alignment shown in Table 1.
The two sequences ?GAACTGAT? and ?AAGT-
GTAT? are aligned to maximize the number of
identical items that appear in the same column,
substituting tokens (column 3), and inserting
gaps (columns 1 and 6)1.
A full MSA is generally constructed by itera-
tively aligning each new sequence with an identi-
cal or similar sequence already in the MSA (so-
called ?progressive alignment?). The costs of
token substitution are often taken from a hand-
tuned substitution matrix. A cost may also be
associated with inserting a gap into the exist-
ing MSA (a ?gap penalty?). Once the full MSA
has been constructed, a Position Specific Score
Matrix (PSSM) can be induced, in which each
token (including a special gap token) is assigned
a separate alignment cost for each column. An
unseen sequence can then be aligned with the
full MSA by Viterbi search.
Predicting sequence ordering within a noun
phrase is a natural application for MSA tech-
niques, and it seems reasonable to propose that
aligning an unseen set of modifiers with such an
MSA model will yield acceptable orderings. Ta-
ble 2 illustrates how MSA may be applied to
modifiers before a noun. Given an NP preceded
by modifiers hungry, big, and Grizzly, alignment
of the modifiers with NPs seen in the training
corpus determines the prenominal ordering big
hungry Grizzly. We then align every permuta-
1See Durbin et al (1999) for details on standard align-
ment techniques.
G A C T G - A T
- A G T G T A T
1 2 3 4 5 6 7 8
Table 1: Alignment of the two DNA sequences
?GAACTGAT? and ?AAGTGTAT?.
small clumsy black bear
big - black cow
two-story - brown house
big clumsy - bull
small fuzzy brown duck
large - green house
big hungry Grizzly bear
Table 2: Example noun-phrase alignment.
tion of the NP and choose the best-scoring align-
ment.
The vocabulary for a linguistic alignment is
large enough to render a hand-tuned substitu-
tion matrix impractical, so we instead construct
a cost function based on features of the token
under consideration and those of the other to-
kens already aligned in a column.
We know of no prior work on methods for
training such an alignment. We present and
compare two training methods, each of which
produces competitive ordering accuracies. Both
training methods share the feature-set described
in Table 3. In each case, we train an MSA by
aligning each instance in the training data.
3.2 Maximum Likelihood Training
In our alignment approach, the features listed in
Table 3 are grouped into several classes. All ob-
served words are a class, all observed stems are
a class (Porter, 1980), and so on. We treat each
indicator feature as a separate class, and make
the assumption that classes are independent of
one another. This assumption is clearly false,
but serves as a reasonable first approximation,
similar to the independence assumption in Na??ve
Bayesian analysis. After aligning each instance,
we estimate the probability of a feature appear-
ing in a column as the simple maximum like-
lihood estimate given the observed occurrences
602
Identity Features
Word Token
Stem Word stem, derived by the Porter Stemmer
Length ?Binned? length indicators: 1, 2, 3, 4, 5-6, 7-8, 9-12, 13-18, >18 characters
Indicator Features
Capitalized Token begins with a capital
All-caps Entire token is capitalized
Hyphenated Token contains a hyphen
Numeric Entire token is numeric (e.g. 234)
Initial Numeric Token begins with a numeral (e.g. 123, 2-sided)
Endings Token ends with -al, -ble, -ed, -er, -est, -ic, -ing, -ive, -ly
Table 3: Description of the feature-set.
within its class.2 This produces a new PSSM
with which to align the next instance.
Our problem differs from alignment of biolog-
ical sequences in that we have little prior knowl-
edge of the similarity between sequences. ?Sim-
ilarity? can be defined in many ways; for bio-
logical sequences, a simple Levenshtein distance
is effective, using a matrix of substitution costs
or simple token identity (equivalent to a ma-
trix with cost 0 on the diagonal and 1 every-
where else). These matrices are constructed and
tuned by domain experts, and are used both in
choosing alignment order (i.e., which sequence
to align next) and during the actual alignment.
When aligning biological sequences, it is cus-
tomary to first calculate the pairwise distance
between each two sequences and then introduce
new sequences into the MSA in order of simi-
larity. In this way, identical sequences may be
aligned first, followed by less similar sequences
(Durbin et al, 1999).
However, we have no principled method of de-
termining the ?similarity? of two words in an NP.
We have no a priori notion of what the cost
of substituting ?two-story? for ?red? should be.
Lacking this prior knowledge, we have no opti-
mal alignment order and we must in effect learn
the substitution costs as we construct the MSA.
Therefore, we choose to add instances in the or-
der they occur in the corpus, and to iterate over
the entire MSA, re-introducing each sequence.
2We treat two special symbols for gaps and unknown
words as members of the word class.
This allows a word to ?move? from its original
column to a column which became more likely
as more sequences were aligned. Each iteration
is similar to a step in the EM algorithm: create a
model (build up an MSA and PSSM), apply the
model to the data (re-align all sequences), and
repeat. Randomly permuting the training cor-
pus did not change our results significantly, so
we believe our results are not greatly dependent
on the initial sequence order.
Instead of assigning substitution costs, we
compute the cost of aligning a word into a par-
ticular column, as follows:
C = The set of i feature classes, Ci ? C
j = Features 1 . . . |Ci| from class Ci
cnt(i, j, k) = The count of instances of
feature j from class
i in column k
?i = Laplace smoothing count
for feature class Ci
A = The number of aligned instances
f(w, i, j) =
?
??
??
1 if word w has feature j from
Ci,
0 otherwise
These help define feature positional probabilities
for column k:
p(i, j, k) =
cnt(i, j, k) + ?i
A+ ?i ? |Ci|
(1)
603
That is, the probability of feature j from class
i occurring in column k is a simple maximum-
likelihood estimate ? count the number of times
we have already aligned that feature in the col-
umn and divide by the number of sequences
aligned. We smooth that probability with sim-
ple Laplace smoothing.
We can now calculate the probability of align-
ing a word w into column k by multiplying the
product of the probabilities of aligning each of
the word?s features. Taking the negative log to
convert that probability into a cost function:
c(w, k) = ?
|C|?
i=1
|Ci|?
j=1
log (p(i, j, k) ? f(w, i, j)) (2)
Finally, we define the cost of inserting a new
column into the alignment to be equal to the
number of columns in the existing alignment,
thereby increasingly penalizing each inserted
column until additional columns become pro-
hibitively expensive.
i(j) = I ? Length of existing alignment (3)
The longest NPs aligned were 7 words, and
most ML MSAs ended with 12-14 columns.
We experimented with various column insertion
costs and values for the smoothing ? and found
no significant differences in overall performance.
3.3 Discriminative Training
We also trained a discriminative model, us-
ing the same feature-set. Discriminative train-
ing does not require division of the features
into classes or the independence assumption dis-
cussed in Section 3.2. We again produced a cost
vector for each column. We fixed the alignment
length at 8 columns, allowing alignment of the
longest instances in our test corpus.
Our training data consists of ordered se-
quences, but the model we are attempting to
learn is a set of column probabilities. Since we
have no gold-standard MSAs, we instead align
the ordered NPs with the current model and
treat the least cost alignment of the correct or-
dering as the reference for training.
We trained this model using the averaged per-
ceptron algorithm (Collins, 2002). A percep-
tron learns from classifier errors, i.e., when it
misorders an NP. At each training instance, we
align all possible permutations of the modifiers
with the MSA. If the least cost alignment does
not correspond to the correct ordering of the
modifiers, we update the perceptron to penal-
ize features occurring in that alignment and to
reward features occurring in the least cost align-
ment corresponding to the correct ordering, us-
ing standard perceptron updates.
Examining every permutation of the NP in-
volves a non-polynomial cost, but the sequences
under consideration are quite short (less than
1% of the NPs in our corpus have more than 3
modifiers, and the longest has 6; see Table 7). So
exhaustive search is practical for our problem; if
we were to apply MSA to longer sequences, we
would need to prune heavily.3
4 Evaluation
We trained and tested on the same corpus used
by Mitchell (2009), including identical 10-fold
cross-validation splits. The corpus consists of
all NPs extracted from the Penn Treebank,
the Brown corpus, and the Switchboard corpus
(Marcus et al, 1999; Kucera and Francis, 1967;
Godfrey et al, 1992). The corpus is heavily
biased toward WSJ text (74%), with approxi-
mately 13% of the NPs from each of the other
corpora.
We evaluated our system using several related
but distinct metrics, and on both modifier pairs
and full NPs.
We define:
T = The set of unique orderings found in the
test corpus
P = The set of unique orderings predicted by
the system
Type Precision (|P ? T|/|P|) measures the
probability that a predicted ordering is ?reason-
able? (where ?reasonable? is defined as orderings
which are found in the test corpus).
3The same issue arises when evaluating candidate or-
derings; see Section 4.
604
Token Accuracy Type Precision Type Recall Type F-measure
Mitchell N/A 90.3% (2.2) 67.2% (3.4) 77.1%
ML MSA 85.5% (1.0) 84.6% (1.1) 84.7% (1.1) 84.7%
Perceptron MSA 88.9% (0.7) 88.2% (0.8) 88.1% (0.8) 88.2%
Table 4: Results on the combined WSJ, Switchboard, and Brown corpus; averages and standard deviations
over a 10-fold cross validation. Winning scores are in bold.
Type Recall (|P?T|/|T|) measures the per-
centage of ?reasonable? orderings which the sys-
tem recreates.
Note that these two metrics differ only in no-
tation from those used by Mitchell (2009).
We also define a third metric, Token Accu-
racy, which measures accuracy on each individ-
ual ordering in the test corpus, rather than on
unique orderings. This penalizes producing or-
derings which are legal, but uncommon. For ex-
ample, if {a,b} occurs eight times in the test cor-
pus as <a,b> and two times as <b,a>, we will
be limited to a maximum accuracy of 80% (pre-
suming our system correctly predicts the more
common ordering). However, even though sug-
gesting <b,a> is not strictly incorrect, we gen-
erally prefer to reward a system that produces
more common orderings, an attribute not em-
phasized by type-based metrics. Our test cor-
pus does not contain many ambiguous pairings,
so our theoretical maximum token accuracy is
99.8%.
We define:
o1..N = All modifier orderings in the
test data
pred(oi) = The predicted ordering for
modifiers in oi
ai =
{
1 if pred(oi) = oi,
0 otherwise
Token Accuracy =
N?
i=0
ai
N
4.1 Pairwise Ordering
Most earlier work has focused on ordering pairs
of modifiers. The results in Table 4 are di-
rectly comparable to those found in Mitchell
(2009). Mitchell?s earlier approach does not gen-
erate a prediction when the system has insuffi-
cient evidence, and allows generation of multiple
predictions given conflicting evidence. In the-
ory, generating multiple predictions could im-
prove recall, but in practice her system appears
biased toward under-predicting, favoring preci-
sion. Our approach, in contrast, forces predic-
tion of a single ordering for each test instance,
occasionally costing some precision (in particu-
lar in cross-domain trials; see Table 5), but con-
sistently balancing recall and precision.
Our measurement of Token Accuracy is com-
parable to the accuracy measure reported in
Shaw and Hatzivassiloglou (1999) and Malouf
(2000) (although we evaluate on a different cor-
pus). Their approaches produce a single order-
ing for each test instance evaluated, so for each
incorrectly ordered modifier pair, there is a cor-
responding modifier pair in the test data that
was not predicted.
Shaw and Hatzivassiloglou found financial
text particularly difficult to order, and reported
that their performance dropped by 19% when
they included nouns as well as adjectives. Mal-
ouf?s system surpasses theirs, achieving an accu-
racy of 91.9%. However, his corpus was derived
from the BNC ? he did not attempt to order fi-
nancial text ? and he ordered only adjectives as
modifiers. In contrast, our test corpus consists
mainly of WSJ text, and we test on all forms
of prenominal modifiers. We believe this to be
a considerably more difficult task, so our peak
performance of 88.9% would appear to be ? at
worst ? quite competitive.
Table 5 presents an evaluation of cross-
domain generalization, splitting the same cor-
pus by genre ? Brown, Switchboard, and WSJ.
In each trial, we train on two genres and test on
605
Training Testing Token Type Type Type
Corpora Corpus Accuracy Precision Recall F-measure
Mitchell
Brown+WSJ Swbd N/A 94.2% 58.2% 72.0%
Swbd+WSJ Brown N/A 87.0% 51.2% 64.5%
Swbd+Brown WSJ N/A 82.4% 27.2% 40.9%
ML MSA
Brown+WSJ Swbd 74.6% 74.7% 75.3% 75.0%
Swbd+WSJ Brown 75.3% 74.7% 74.9% 74.8%
Swbd+Brown WSJ 70.2% 71.6% 71.8% 71.7%
Perceptron MSA
Brown+WSJ Swbd 77.2% 78.2% 77.6% 77.9%
Swbd+WSJ Brown 76.4% 76.7% 76.4% 76.5%
Swbd+Brown WSJ 77.9% 77.5% 77.3% 77.4%
Table 5: Cross-domain generalization.
Token Accuracy Token Precision Token Recall Token F-measure
Mitchell N/A 94.4% 78.6% (1.2) 85.7%
ML MSA 76.9% (1.6) 76.5% (1.4) 76.5% (1.4) 76.50%
Perceptron MSA 86.7% (0.9) 86.7% (0.9) 86.7% (0.9) 86.7%
Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To
compare directly with Mitchell (2009), we report token precision and recall instead of type. Our system
always proposes one and only one ordering, so token accuracy, precision, and recall are identical.
the third.4 Our results mirror those in the previ-
ous trials ? forcing a prediction costs some pre-
cision (vis-a-vis Mitchell?s 2009 system), but our
recall is dramatically higher, resulting in more
balanced performance overall.
4.2 Full NP Ordering
We now extend our analysis to ordering en-
tire NPs, a task we feel the MSA approach
should be particularly suited to, since (unlike
pairwise models) it can model positional prob-
abilities over an entire NP. To our knowledge,
the only previously reported work on this task
is Mitchell?s (2009). We train this model on
the full NP instead of on modifier pairs; this
makes little difference in pairwise accuracy, but
improves full-NP ordering considerably.
As seen in Table 6, both MSA models perform
quite well, the perceptron-trained MSA again
outperforming the maximum likelihood model.
However, we were somewhat disappointed in the
performance on longer sequences. We expected
the MSA to encode enough global information
4Note that the WSJ corpus is much larger than the
other two, comprising approximately 84% of the total.
Modifiers Frequency Token Pairwise
Accuracy Accuracy
2 89.1% 89.7% 89.7%
3 10.0% 64.5% 84.4%
4 0.9% 37.2% 80.7%
Table 7: Descriminative model performance on NPs
of various lengths, including pairwise measures.
to perform accurate full sequence ordering, but
found the accuracy drops off dramatically on
NPs with more modifiers. In fact, the accu-
racy on longer sequences is worse than we would
expect by simply extending a pairwise model.
For instance, ordering three modifiers requires
three pairwise decisions. We predict pairwise
orderings with 88% accuracy, so we would ex-
pect no worse than (.88)3, or 68% accuracy on
such sequences. However, the pairwise accu-
racy declines on longer NPs, so it underperforms
even that theoretical minimum. Sparse training
data for longer NPs biases the model strongly
toward short sequences and transitivity (which
our model does not encode) may become impor-
tant when ordering several modifiers.
606
5 Ablation Tests
We performed limited ablation testing on the
discriminative model, removing features individ-
ually and comparing token accuracy (see Table
8). We found that few of the features provided
great benefit individually; the overall system
performance remains dominated by the word.
The word and stem features appear to cap-
ture essentially the same information; note that
performance does not decline when the word
or stem features are ablated, but drops dras-
tically when both are omitted. Performance de-
clines slightly more when ending features are ab-
lated as well as words and stems, so it appears
that ? as expected ? the information captured
by ending features overlaps somewhat with lex-
ical identity. The effects of individual features
are all small and none are statistically signifi-
cant.
Feature(s) Gain/Loss
Word 0.0
Stem 0.0
Capitalization -0.1
All-Caps 0.0
Numeric -0.2
Initial-numeral 0.0
Length -0.1
Hyphen 0.0
-al 0.0
-ble -0.4
-ed -0.4
-er 0.0
-est -0.1
-ic +0.1
-ing 0.0
-ive -0.1
-ly 0.0
Word and stem -22.9
Word, stem, and endings -24.2
Table 8: Ablation test results on the discriminative
model.
6 Summary and Future Directions
We adapted MSA approaches commonly used
in computational biology to linguistic problems
and presented two novel methods for training
such alignments. We applied these techniques
to the problem of ordering prenominal modi-
fiers in noun phrases, and achieved performance
competitive with ? and in many cases, superior
to ? the best results previously reported.
In our current work, we have focused on rel-
atively simple features, which should be adapt-
able to other languages without expensive re-
sources or much linguistic insight. We are inter-
ested in exploring richer sources of features for
ordering information. We found simple morpho-
logical features provided discriminative clues for
otherwise ambiguous instances, and believe that
richer morphological features might be helpful
even in a language as morphologically impover-
ished as English. Boleda et al (2005) achieved
promising preliminary results using morphology
for classifying adjectives in Catalan.
Further, we might be able to capture some
of the semantic relationships noted by psycho-
logical analyses (Ziff, 1960; Martin, 1969) by
labeling words which belong to known seman-
tic classes (e.g., colors, size denominators, etc.).
We intend to explore deriving such labels from
resources such as WordNet or OntoNotes.
We also plan to continue exploration of MSA
training methods. We see considerable room
for refinement in generative MSA models; our
maximum likelihood training provides a strong
starting point for EM optimization, conditional
likelihood, or gradient descent methods. We are
also considering applying maximum entropy ap-
proaches to improving the discriminative model.
Finally (and perhaps most importantly), we
expect that our model would benefit from ad-
ditional training data, and plan to train on a
larger, automatically-parsed corpus.
Even in its current form, our approach im-
proves the state-of-the-art, and we believe MSA
techniques can be a useful tool for ordering
prenominal modifiers in NLP tasks.
7 Acknowledgements
This research was supported in part by NSF
Grant #IIS-0811745. Any opinions, findings,
conclusions or recommendations expressed in
this publication are those of the authors and do
not necessarily reflect the views of the NSF.
607
References
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence align-
ment. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing
- Volume 10, pages 164?171, Philadelphia. Asso-
ciation for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL), vol-
ume 15, pages 201?31, Edmonton, Canada. As-
sociation for Computational Linguistics.
Gemma Boleda, Toni Badia, and Sabine Schulte
im Walde. 2005. Morphology vs. syntax in adjec-
tive class acquisition. In Proceedings of the ACL-
SIGLEX Workshop on Deep Lexical Acquisition,
pages 77?86, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
Humberto Carrillo and David Lipman. 1988. The
multiple sequence alignment problem in biol-
ogy. SIAM Journal on Applied Mathematics,
48(5):1073?1082, October.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, volume 10, pages 1?8,
Philadelphia, July. Association for Computational
Linguistics.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1999. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, West
Nyack, NY, July.
John J. Godfrey, Edward C. Holliman, and Jane
McDaniel. 1992. SWITCHBOARD: telephone
speech corpus for research and development. In
Acoustics, Speech, and Signal Processing, IEEE
International Conference on, volume 1, pages 517?
520, Los Alamitos, CA, USA. IEEE Computer So-
ciety.
Dan Gusfield. 1997. Algorithms on Strings, Trees
and Sequences: Computer Science and Computa-
tional Biology. Cambridge University Press, West
Nyack, NY, May.
H. Kucera and W. N Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.
Robert Malouf. 2000. The order of prenominal ad-
jectives in natural language generation. In Pro-
ceedings of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 85?92,
Hong Kong, October. Association for Computa-
tional Linguistics.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium, Philadelphia.
J. E. Martin. 1969. Semantic determinants of pre-
ferred adjective order. Journal of Verbal Learning
& Verbal Behavior. Vol, 8(6):697?704.
Margaret Mitchell. 2009. Class-Based ordering of
prenominal modifiers. In Proceedings of the 12th
European Workshop on Natural Language Gener-
ation (ENLG 2009), pages 50?57, Athens, Greece,
March. Association for Computational Linguis-
tics.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
James Shaw and Vasileios Hatzivassiloglou. 1999.
Ordering among premodifiers. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 135?143, Col-
lege Park, Maryland, USA, June. Association for
Computational Linguistics.
Christopher White, Izhak Shafran, and Jean luc
Gauvain. 2006. Discriminative classifiers for
language recognition. In Proceedings of the
2006 IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP),
pages 213?216, Toulouse, France. IEEE.
Paul Ziff. 1960. Semantic Analysis. Cornell Univer-
sity Press, Ithaca, New York.
608
Proceedings of NAACL-HLT 2013, pages 211?220,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Discriminative Joint Modeling of Lexical Variation and Acoustic Confusion
for Automated Narrative Retelling Assessment
Maider Lehr?, Izhak Shafran?, Emily Prud?hommeaux? and Brian Roark?
?Center for Spoken Language Understanding, Oregon Health & Science University
?Center for Language Sciences, University of Rochester
{maiderlehr,zakshafran,emilpx,roarkbr}@gmail.com
Abstract
Automatically assessing the fidelity of a
retelling to the original narrative ? a task of
growing clinical importance ? is challenging,
given extensive paraphrasing during retelling
along with cascading automatic speech recog-
nition (ASR) errors. We present a word tag-
ging approach using conditional random fields
(CRFs) that allows a diversity of features
to be considered during inference, including
some capturing acoustic confusions encoded
in word confusion networks. We evaluate the
approach under several scenarios, including
both supervised and unsupervised training, the
latter achieved by training on the output of
a baseline automatic word-alignment model.
We also adapt the ASR models to the domain,
and evaluate the impact of error rate on per-
formance. We find strong robustness to ASR
errors, even using just the 1-best system out-
put. A hybrid approach making use of both au-
tomatic alignment and CRFs trained tagging
models achieves the best performance, yield-
ing strong improvements over using either ap-
proach alone.
1 Introduction
Narrative production tasks are an essential compo-
nent of many standard neuropsychological test bat-
teries. For example, narration of a wordless pic-
ture book is part of the Autism Diagnostic Obser-
vation Schedule (ADOS) (Lord et al, 2002) and
retelling of previously narrated stories is part of both
the Developmental Neuropsychological Assessment
(NEPSY) (Korkman et al, 1998) and the Wech-
sler Logical Memory (WLM) test (Wechsler, 1997).
Such tests also arise in reading comprehension, sec-
ond language learning and other computer-based tu-
toring systems (Xie et al, 2012; Zhang et al, 2008).
The accuracy of automated scoring of a narrative
retelling depends on correctly identifying which of
the source narrative?s propositions or events (what
we will call ?story elements?) have been included
in the retelling. Speakers may choose to relate
these elements using diverse words or phrases, and
an automated method of identifying these elements
needs to model the permissible variants and para-
phrasings. In previous work (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011), we developed models based on
automatic word-alignment methods, as described
briefly in Section 3. Such alignments are learned
in an unsupervised manner from a parallel corpus of
manual or ASR transcripts of retellings and the orig-
inal source narrative, much as in machine translation
training.
Relying on manual transcripts to train the align-
ment models limits the ability of these methods to
handle ASR errors. By instead training on ASR
transcripts, these methods can automatically capture
some regularities of lexical variants and their com-
mon realizations by the recognizer. Additionally, ev-
idence of acoustic confusability is available in word
lattice output from the recognizer, which can be ex-
ploited to yield more robust automatic scoring, par-
ticularly in high error-rate scenarios.
In this paper, we present and evaluate the use of
word tagging models for this task, in contrast to
just using automatic (unsupervised) word-alignment
methods. The approach is general enough to al-
211
low tagging of word confusion networks derived
from lattices, thus allowing us to explore the utility
of such representations to achieve robustness. We
present results under a range of experimental condi-
tions, including: variously adapting the ASR mod-
els to the domain; using maximum entropy models
rather than CRFs; differing tagsets (BIO versus IO);
and with varying degrees of supervision. Finally,
we demonstrate improved utility in terms of using
the automatic scores to classify elderly individuals
as having Mild Cognitive Impairment. Ultimately
we find that hybrid approaches, making use of both
word-alignment and tagging models, yield strong
improvements over either used independently.
2 Wechsler Logical Memory (WLM) task
The Wechsler Logical Memory (WLM) task (Wech-
sler, 1997), a widely used subtest of a battery of neu-
ropsychological tests used to assess memory func-
tion in adults, has been shown to be a good indicator
of Mild Cognitive Impairment (MCI) (Storandt and
Hill, 1989; Petersen et al, 1999; Wang and Zhou,
2002; Nordlund et al, 2005), the stage of cogni-
tive decline that is often a precursor to dementia of
the Alzheimer?s type. In the WLM, the subject lis-
tens to the examiner read a brief narrative and then
retells the narrative twice: immediately upon hear-
ing it and after about 20 minutes. The examiner
grades the subject?s response by counting how many
of the story elements the subject recalled.
An excerpt of the text read by the clinician while
administering the WLM task is shown in Figure 1.
The story elements in the text are delineated using
slashes, 25 elements in all. An example retelling
is shown in Figure 2 to illustrate how the retellings
are scored. The clinical evaluation guidelines spec-
ify what lexical substitutions, if any, are allowed
for each element. Some elements, such as cafeteria
and Thompson, must be recalled verbatim. In other
cases, subjects are given credit for variants, such as
Annie for Anna, or paraphrasing of concepts such as
sympathetic for touched by the woman?s story. The
example retelling received a score of 12, with one
point for each of the recalled story elements: Anna,
Boston, employed, as a cook, and robbed of, she had
four, small children, reported, station, touched by
the woman?s story, took up a collection and for her.
Anna / Thompson / of South / Boston / em-
ployed / as a cook / in a school / cafeteria /
reported / at the police / station / that she had
been held up / on State Street / the night be-
fore / and robbed / . . . / police / touched by the
woman?s story / took up a collection / for her.
Figure 1: Reference text and the set of story elements.
Ann Taylor worked in Boston as a cook. And
she was robbed of sixty-seven dollars. Is
that right? And she had four children and
reported at the some kind of station. The fel-
low sympathetic and made a collection for her
so that she can feed the children.
Figure 2: An example retelling with 12 recalled story elements.
3 Unsupervised generative automated
scoring with word alignment
In previous work (Lehr et al, 2012; Prud?hommeaux
and Roark, 2012; Prud?hommeaux and Roark,
2011), we developed a pipeline for automatically
scoring narrative retellings for the WLM task. The
utterances corresponding to a retelling were rec-
ognized using an ASR system. The story ele-
ments were identified from the 1-best ASR transcript
using word alignments produced by the Berkeley
aligner (Liang et al, 2006), an EM-based word
alignment package developed to align parallel texts
for machine translation. The word alignment model
was estimated in an unsupervised manner from a
parallel corpus consisting of source narrative and
manual transcripts of retellings from a small set of
training subjects, and from a pairwise parallel cor-
pus of manual retelling transcripts.
During inference or test, the ASR transcripts of
the retellings were aligned using the estimated align-
ment model to the source narrative text. If a word
in the retelling was mapped by the alignment model
to a content word in the source narrative, the ele-
ment associated with that content word was counted
as correctly recalled in that retelling. Recall that
the models were trained on unsupervised data so the
aligned words may not always be permissible vari-
ants of the target elements. To alleviate such extra-
neous as well as unaligned words, the alignments
below a threshold of posterior probability are dis-
carded while decoding.
212
4 Supervised discriminative automated
scoring with log-linear models
In this work, we frame the task of detecting story
elements as a tagging task. Thus, our problem re-
duces to assigning a tag to each word position in the
retelling, the tag indicating the story element that the
word is associated with. In its simplest form, we
have 26 tags: one for each of the 25 story elements
indicating the word is ?in? that element (e.g., I15);
and one for ?outside? of any story element (?O?). By
tagging word positions, we are framing the problem
in a general enough way to allow tagging of word
confusion networks (Mangu et al, 2000), which en-
code word confusions that may provide additional
robustness, particularly in high word-error rate sce-
narios. We make use of log-linear models, which
have been used for tagging confusion networks (Ku-
rata et al, 2012), and which allow very flexible fea-
ture vector definition and discriminative optimiza-
tion.
The model allows us to experiment with three
types of inputs as illustrated in the Figure 3 ? the
manual transcript, the 1-best ASR transcript, and the
word confusion network. To create supervised train-
ing data, we force-align ASR transcripts to manual
transcripts and transfer manually annotated story el-
ement tags from the reference transcripts to word po-
sitions in the confusion network or 1-best ASR out-
put using the word-level time marks. Our unsuper-
vised training scenario instead derives story element
tags from a baseline word-alignment based model.
Figure 3: Feature vectors at each word position includes lexi-
cal variants and acoustic confusions.
Markov order 0 Markov order 1
(MaxEnt) (CRF)
Context yi yi?1yi
independent (CI) yixi yi?1yixi
Context yixi?1 yi?1yixi?1
dependent (CD) yixi+1 yi?1yixi+1
Table 1: Feature templates either using or not using neighbor-
ing tag yi?1 (MaxEnt vs. CRF); and for using or not using
neighboring words xi?1, xi+1 (CI vs. CD).
4.1 Features
Given a sequence of word positions x = x1 . . . xn,
the tagger assigns a sequence of labels y = y1 . . . yn
from a tag lexicon. For each word xi in the se-
quence, we can define features in the log-linear
model based on word and tag identities. Table 1
presents several sets of features, defined over words
and tags at various positions relative to the current
word xi and tag yi and compound features are de-
noted as concatenated symbols.
Features that rely only on the current tag yi are
used in a Markov order 0 model, i.e., one for which
each tag is labeled independently. A maximum en-
tropy classifier (see Section 4.2) is used with these
feature sets. Features that include prior tags en-
code dependencies between adjacent tags, and are
used within conditional random fields models (see
Section 4.3). To examine the utility of surrounding
words xi?1 and xi+1, we distinguish between mod-
els trained with context independent features (just
xi) and context dependent features. Note that mod-
els including context dependent feature sets also in-
clude the context independent features, and Markov
order 1 models also include Markov order 0 features.
Two other details about our use of the feature tem-
plates are worth noting. First, when tagging confu-
sion networks, each word in the network at position
i results in a feature instance. Thus, if there are five
confusable words at position i, then there will be
five different xi values being used to instantiate the
features in Table 1. Second, following Kurata et al
(2012), we multiply the feature counts for the con-
text dependent features by a weight to control their
influence on the model. In this paper, the scaling
weight of the context-dependent features was 0.3.
We investigate two different tagsets for this task,
as presented in Table 2. The simpler tagset (IO) sim-
ply identifies words that are in a story element; the
213
Tagging anna rent was due
IO-tags I1 I19 I19 I19
BIO-tags B1 B19 I19 I19
Table 2: Two possible tagsets for labeling.
larger tagset (BIO) differentiates among positions in
a story element chunk. The latter tagset is only of
utility for models with Markov order greater than
zero, and hence are only used with CRF models.
4.2 MaxEnt-based multiclass classifier
Our baseline model is a Maximum Entropy (Max-
Ent) classifier where each position i from the
retelling x gets assigned one of the IO output tags
yi corresponding to the set of 25 story elements and
a null (?O?) symbol. The output tag is modeled as
the conditional probability p(yi | xi) given the word
xi at position i in the retelling.
p(yi | xi) =
exp
(
d?
k=1
?k?k(xi, yi)
)
Z(xi)
where Z(xi) is a normalization factor. The feature
functions ?(xi, yi) are the Markov order 0 features
as defined as in the previous section. The parame-
ters ? ? <d are estimated by optimizing the above
conditional probability, with L2 regularization. We
use the MALLET Toolkit (McCallum, 2002) with
default regularization parameters.
4.3 CRF-based sequence labeling model
The MaxEnt models assign a tag to each position
from the input retelling independently. However,
there are a few reasons why reframing the task as
a sequence modeling problem may improve tagging
performance. First, some of the story elements are
multiword sequences, such as she had been held up
or on State Street. Second, even if a retelling orders
recalled elements differently than the original narra-
tive, there is a tendency for story elements to occur
in certain orders.
The parameters of the CRF model, ? ? <d are
estimated by optimizing the following conditional
probability:
P (y | x) =
exp
(
d?
k=1
?k?k(x, y)
)
Z(x)
where ?(x, y) aggregates features across the entire
sequence, and Z(x) is a global normalization con-
stant over the sequence, rather than local for a partic-
ular position as with MaxEnt. Features for the CRF
model are Markov order 1 features, and as with the
MaxEnt training, we use default (L2) regularization
parameters within the MALLET toolkit.
5 Combining tagging and alignment
This paper contrasts a discriminatively trained tag-
ging approach with an unsupervised alignment-
based approach, but there are several ways in which
the two approaches can be combined. First, the
alignment model is unsupervised and can provide
its output as training data to the tagging approach,
resulting in an unsupervised discriminative model.
Second, the alignment model can provide features to
the log-linear tagging model in the supervised condi-
tion. We explore both methods of combination here.
5.1 Unsupervised discriminative tagger
The tagging task based on log-linear models pro-
vides an appropriate framework to easily incorpo-
rate diverse features and discriminatively estimate
the parameters of the model. However, this ap-
proach requires supervised tagged training data, in
this case manual labels indicating the correspon-
dence of phrases in the retellings with story elements
in the original narrative. These manual annotations
are used to derive sequences of story element tags
labeling the words of the retelling. Manually la-
beling the retellings is costly, and the scoring (thus
labeling) scheme is very specific to the test being
analyzed. To avoid manual labeling and provide a
general framework that can easily be adopted in any
retelling based assessment task, we experiment here
with an unsupervised discriminative approach.
In this unsupervised approach, the labeled train-
ing data required by the log-linear model is provided
by the automatic word alignments trained without
supervision. The resulting tag sequences replace the
manual tag sequences used in the standard super-
vised approach.
5.2 Word-alignment derived features
When training discriminative models it is a common
practice to incorporate into the feature space the out-
put from a generative model, since it is a good esti-
214
mator. Here we augment the feature space of the
log-linear models with the tags generated by the au-
tomatic word alignments. In addition to the features
defined in Section 4.1, we include new features that
match predicted labels zi from the word-alignment
model with possible labels in the tagger yi. Our fea-
tures include the current tagger label with (1) the
current predicted word-alignment label; (2) the pre-
vious predicted label; and (3) the next predicted la-
bel. Thus, the new features were yizi, yizi?1 and
yizi+1.
6 Experimental evaluations
Corpus: Our models were trained on immediate and
delayed retellings from 144 subjects with a mean
age of 85.4, of whom 36 were clinically diagnosed
with MCI (training set). We evaluated our models
on a set of retellings from 70 non-overlapping sub-
jects with a mean age of 88.5, half of whom had
received a diagnosis of MCI (test set). In contrast
to the unsupervised word-alignment based method,
the method outlined here required manual story el-
ement labels of the retellings. The training and
test sets from this paper are therefore different from
the sets used in previous work (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011), and the results are not directly
comparable.
The recordings were sometimes made in an infor-
mal setting, such as the subject?s home or a senior
center. For this reason, there are often extraneous
noises in the recordings such as music, footsteps,
and clocks striking the hour. Although this presents
a challenge for ASR, part of the goal of our work
is to demonstrate the robustness of our methods to
noisy audio.
6.1 Automatic transcription
The baseline ASR system used in the current work
is a Broadcast News system which is modeled af-
ter Kingsbury et al (2011). Briefly, the acoustics
of speech are modeled by 4000 clustered allophone
states defined over a pentaphone context, where
states are represented by Gaussian mixture models
with a total of 150K mixture components. The ob-
servation vectors consist of PLP features, stacked
from 10 neighboring frames and projected to a 50-
1-best oracle oracle
System (%) WCN(%) lat(%)
Baseline 47.2 39.7 27.7
AM adaptation 38.2 35.5 21.2
LM adaptation 28.3 30.7 19.9
AM+LM adaptation 25.6 26.5 16.5
Table 3: Improvement in ASR word error-rate by adapting the
Broadcast News models to the domain of narrative retelling.
dimension space using linear discriminant analysis
(LDA). The acoustic models were trained on 430
hours of transcribed speech from Broadcast News
corpus (LDC97S44, LDC98S71). The language
model is defined over an 84K vocabulary and con-
sists of about 1.8M, 1M and and 331K bigrams, tri-
grams and 4-grams, estimated from standard Broad-
cast news corpus. The decoding is performed in sev-
eral stages using successively refined acoustic mod-
els ? a context-dependent model, a vocal-tract nor-
malized model, a speaker-adapted maximum likeli-
hood linear regression (MLLR) model, and finally
a discriminatively trained model with the boosted
MMI criteria (Povey et al, 2008). The system gives
a word error rate of 13.1% on the 2004 Rich Tran-
scription benchmark by NIST (Fiscus et al, 2007),
which is comparable to state-of-the-art for equiva-
lent amounts of acoustic training data. On the WLM
corpus, the recognition word error rate was signifi-
cantly higher at 47.2% due to a mismatch in domain
and the skewed demographics (age) of the speakers.
We improved the performance of the above
Broadcast News models by adapting to the domain
of the WLM retellings. The acoustic models were
adapted using standard MLLR, where linear trans-
forms were estimated in an unsupervised manner
to maximize the likelihood over the transcripts of
the retellings. The transcripts were generated from
the baseline system after the final stage of decod-
ing with the discriminative model. The language
models were adapted by interpolating the in-domain
model (weight=0.7) with the out-of-domain model.
The gains from these adaptations are reported in
the Table 3. As expected, we find substantial gains
from both acoustic model (AM) and language model
(LM) adaptation. Furthermore, we find benefit in
employing them simultaneously. We also include
the oracle word error rate (WER) of the WCNs and
lattices for each ASR configuration.
215
One thing to note is that the oracle WER of the
WCNs is worse than the 1-best WER when adapting
the language models. We speculate that this is due
to bias introduced by the language model adapted
to the story retellings, resulting in word candidates
in the bins that are not truly acoustically confusable
candidates. This is one potential reason for the lack
of utility of WCNs in low WER conditions.
6.2 Evaluating retelling scoring
We analyzed the performance of the retelling scor-
ing methods under five different input conditions for
producing transcripts: (1) the out-of-domain Broad-
cast News recognizer with no adaptation; (2) do-
main adapted acoustic model; (3) domain adapted
language model; (4) domain adapted acoustic and
language models; and (5) manual (reference) tran-
scripts. Each story element is automatically labeled
by the systems as either having been recalled or not,
and this is compared with manual scores to derive an
F-score accuracy, by calculating precision and recall
of recalled story elements. Derived word alignments
or tag sequences are converted to binary story ele-
ment indicators by simply setting the element to 1
if any open-class word is tagged for (or aligned to)
that story element.
6.2.1 Word alignment based scoring
We evaluate the word alignment approach only on
1-best ASR transcripts and manual transcripts, not
WCNs. The first row of Table 4 reports the story ele-
ment F-scores for a range of ASR adaptation scenar-
ios. The performance of the model improves signifi-
cantly as the WER reduces with adaptation. With the
fully adapted ASR the F-score improves more than
13%, and it is only 3.4% worse than with the man-
ual transcripts. The alignments produced in each of
these scenarios are used as training data in the unsu-
pervised condition evaluated below.
6.2.2 Log-linear based automated scoring
Context-independent features Table 4 summa-
rizes the performance of the log-linear models us-
ing context independent features (CI) in supervised
(section 4), unsupervised (section 5.1) and hybrid
(section 5.2) training scenarios for different inputs
(reference transcript, ASR 1-best, and word confu-
sion network ASR output) and four different ASR
configurations.
The results show a few clear trends. Both in
the supervised and unsupervised training scenarios
the CRF model provides substantial improvements
over the MaxEnt classifier. The F-scores obtained
in the unsupervised training scenario are slightly
worse than with supervision, though they are compa-
rable to supervised results and an improvement over
just using the word alignment approach, particularly
in high WER scenarios. The hybrid training sce-
nario ? supervised learning with word alignment de-
rived features ? leads to reduced differences between
MaxEnt and CRF training compared to the other two
training scenarios. In fact, in high WER scenarios,
the MaxEnt slightly outperforms the CRF.
As expected the best performance is obtained with
manual transcripts and the worst with 1-best tran-
scripts generated by the out-of-domain ASR with
relatively high word error rate. For this ASR con-
figuration, using WCNs provide some gain, though
the gain is insignificant for the hybrid approach. In
the hybrid approach, the output labels of the word
alignment are already good indicators of the output
tag and incorporating the confusable words from the
Table 4: Story element F-score achieved by baseline word-alignment model and log-linear models (MaxEnt and CRF) using
context independent features (CI) under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 71.9 77.3 84.3 85.4 N/A 88.8
Supervised MaxEnt-CI 76.0 81.7 84.6 85.6 78.9 83.4 84.0 84.7 86.4
CRF-CI 80.3 87.3 89.7 91.4 83.7 88.8 88.2 90.8 94.4
Unsupervised MaxEnt-CI 72.1 79.3 82.7 84.2 77.5 81.2 83.4 83.2 84.8
CRF-CI 79.4 85.4 86.8 88.0 81.2 85.8 86.2 87.2 90.5
Hybrid MaxEnt-CI 88.1 89.4 89.2 89.6 87.6 89.2 88.8 89.5 91.8
CRF-CI 87.0 90.9 91.5 92.1 87.4 91.5 90.1 92.4 94.6
216
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Supervised MaxEnt-CD 80.1 87.3 90.0 91.1 83.5 88.6 88.2 90.3 93.3
CRF-CD-IO 80.6 88.0 89.9 91.2 84.2 89.6 88.8 90.5 94.7
CRF-CD-BIO 81.1 87.9 90.6 91.7 84.5 89.5 88.8 90.8 94.7
Un- MaxEnt-CD 77.1 83.1 86.5 89.0 80.2 85.0 86.2 87.6 90.7
supervised CRF-CD-IO 79.1 85.3 87.1 88.3 81.0 85.9 86.4 87.5 90.3
CRF-CD-BIO 79.1 85.6 87.2 88.4 81.3 85.9 86.2 87.3 90.6
Hybrid MaxEnt-CD 88.4 90.2 90.7 91.6 88.6 90.5 90.4 91.4 93.5
CRF-CD-IO 87.9 91.3 91.6 92.5 88.3 91.7 90.7 92.1 94.8
CRF-BIO 87.8 91.9 91.8 93.0 88.7 92.0 90.7 92.3 94.7
Table 5: Story element F-score achieved by log-linear models (MaxEnt and CRF) when adding context dependent features (CD)
and BIO tags for the CRF models, under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
WCN into the feature vector apparently mainly adds
noise.
When the transcripts are generated with the
adapted models, the word confidence score of the 1-
best is higher and the WCN bins have fewer acous-
tically confusable words. Still, the WCN input is
helpful in the AM-adapted ASR system. When
the transcripts are generated with LM adapted mod-
els, the performance is better with 1-best than with
WCNs. As mentioned earlier, adapting the lan-
guage models may introduce a bias due to the rel-
atively low LM perplexity for this domain. In the
lowest WER scenarios, the best performing systems
achieve over 90% F-score, within two percent of the
performance achieved with manual transcripts.
Context-dependent features Exercising the flex-
ibility of log-linear models, we investigated the im-
pact of using context-dependent (CD) features in-
stead of the CI features used in the previous exper-
iments. Our CD features take into account the two
immediately neighboring word positions. As men-
tioned earlier, following Kurata et al (2012), the
counts from the neighboring word positions were
weighted (? = 0.3) to avoid data sparsity. This re-
duces the sensitivity of the model to time alignment
errors between the tag and feature vector sequences
without increasing the dimensions. In Table 5, we
report the F-scores for the different ASR configu-
rations, inputs, and log-linear models with context
dependent features, using the standard IO tagset as
in Table 4.
Although there are some exceptions, adding con-
text information from the input features improves
the performance of the models. In particular, the
MaxEnt models benefit from incorporating this ex-
tra information. The MaxEnt models improve their
performance substantially for all three training sce-
narios, while the gains for the CRF models are more
modest, especially for the unsupervised approach
where the performance degrades or does not change
much, since some context information is already
captured by the Markov order 1 features.
BIO tagset As detailed in Section 4.1, story el-
ements sometimes span multiple words, so for the
CRF models we investigated two different schemes
for tagging, following typical practice in named en-
tity extraction (Ratinov and Roth, 2009) and syn-
tactic chunking (Sha and Pereira, 2003). The BIO
tagging scheme makes the distinction between the
tokens from the story elements that are in the be-
ginning from the ones that are not. The O tag is
assigned to the tokens that do not belong to any of
the story elements. The IO tagging uses a single tag
for the tokens that fall in the same story element,
which is the approach we have followed so far. In
addition to presenting results using context depen-
dent features, Table 5 presents results with the BIO
tagset.
For the supervised and hybrid approaches, the
BIO tagging provides insignificant but consistent
gains for most of the scenarios. The unsupervised
approach provides mixed results. This may be due to
the way in which the word alignment model scores
the retellings. It tags only those words from the
retelling that are aligned with a content word in the
source narrative, which may result in the loss of the
217
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 0.65 0.67 0.74 0.76 N/A 0.79
Supervised MaxEnt-CD 0.65 0.73 0.76 0.77 0.70 0.73 0.77 0.77 0.81
CRF-CD-BIO 0.69 0.76 0.77 0.76 0.73 0.76 0.77 0.78 0.82
Un- MaxEnt-CD 0.65 0.72 0.75 0.76 0.70 0.75 0.75 0.76 0.80
supervised CRF-CD-BIO 0.74 0.75 0.78 0.78 0.71 0.74 0.77 0.76 0.81
Hybrid MaxEnt-CD 0.72 0.76 0.77 0.78 0.74 0.76 0.77 0.77 0.82
CRF-CD-BIO 0.72 0.76 0.78 0.78 0.76 0.77 0.78 0.79 0.81
Table 6: Classification performance (AUC) for the baseline word-alignment model and the best performing log-linear models of
both types (MaxEnt and CRF) under 3 different scenarios with 3 types of input and 4 types of ASR models.
structure of some multiwords story elements that we
are trying to capture with the BIO scheme.
6.3 Evaluating MCI classification
Each of the individuals producing retellings in our
corpus underwent a battery of neuropsychological
tests, and were assigned a Clinical Dementia Rating
(CDR) (Morris, 1993), which is a composite score
derived from measures of cognitive function in six
domains, including memory. Importantly, it is as-
signed independently of the Wechsler Logical Mem-
ory test we are analyzing in this paper, which allows
us to evaluate the utility of our WLM analyses in
an unbiased manner. MCI is defined as a CDR of
0.5 (Ritchie and Touchon, 2000), and subjects in this
study have either a CDR of 0 (no impairment) or 0.5
(MCI).
In previous work, we found that the features
extracted from the retellings are useful in dis-
tinguishing subjects with MCI from neurotyp-
ical age-matched controls (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011). From each retellings, we extract
Boolean features for each story element, for a total
of 50 features for classification. Each feature indi-
cates whether the retelling contained that story ele-
ment.
In this paper, we carry out similar classification
experiments to investigate the impact of using log-
linear models on the extraction of features for classi-
fication. We build a support vector machine (SVM)
using the LibSVM (Chang and Lin, 2011) exten-
sion to the WEKA data mining Java API (Hall et al,
2009). This allows recollection of different elements
to be weighted differently. This is unlike the manual
scoring of WLM based on clinical guidelines where
all elements are weighted equally irrespective of the
difficulty. The SVM was trained on manually ex-
tracted story element feature vectors. We compared
the performance of the MCI classification for three
types of input and four ASR configurations under
the supervised, unsupervised, and hybrid scenarios.
For each scenario we chose the best scoring system
from among the automated systems reported in Ta-
bles 4 and 5. Classification results, evaluated as area
under the curve (AUC), are reported in Table 6, both
for the log-linear trained tagging models and for the
baseline word-alignment based method. For refer-
ence (not shown in the table), the SVM classifier
performed at 0.83 when features values are manu-
ally populated.
The results show that the AUC improves steadily
as the quality of the transcription is improved, go-
ing from the baseline system to the adapted mod-
els. This is consistent with the improvements seen in
the F-score for detecting story elements. The differ-
ent approaches for detecting the story elements from
the transcriptions did not ultimately show significant
differences in MCI classification results. Overall,
the best classification values are given by the hy-
brid approach, which performs slightly better than
the other two approaches. The best AUC in the
hybrid scenario (0.79, very close to the AUC=0.81
achieved with manual transcripts) is obtained with
a CRF trained with WCNs from the fully adapted
ASR model and with context dependent features and
BIO tags.
Comparing WCN versus 1-best as inputs, using
WCN as input improves classification performance
when the 1-best transcripts are poor, as in the case
of out-of-domain ASR. The adapted recognizer im-
proves the performance of the 1-best significantly
making it unnecessary to resort to WCN as inputs.
Comparing the MaxEnt model with CRF model
218
for extracting story elements, we see that the average
F-scores for the MaxEnt models trained on CD fea-
tures are nearly as good as and sometimes slightly
better than those produced using the CRF models.
The CRF extracted story elements, however, tend to
yield classifiers that perform slightly better, espe-
cially in the unsupervised approach with 1-best in-
puts.
7 Summary and discussion
This paper examines the task of automatically scor-
ing narrative retellings in terms of their fidelity to
the original narrative content, using discriminatively
trained log-linear tagging models. Fully automatic
scoring must account for both lexical variation and
acoustic confusion from ASR errors. Lexical vari-
ation ? due to extensive paraphrasing on the part
of the individuals retelling the narrative ? can be
modeled effectively using word-alignment models
such as those employed in machine translation sys-
tems (Lehr et al, 2012; Prud?hommeaux and Roark,
2011). This paper focuses on an alternative ap-
proach, where both lexical variation and ASR con-
fusions are modeled using log-linear models. In ad-
dition to very flexible feature definitions, the log-
linear models bring the advantage of a discrimina-
tive model to the task. We see improvements in
story element F-score using these models over unsu-
pervised word-alignment models. Further, the fea-
ture definition flexibility allows us to incorporate the
unsupervised word-alignment labels into these mod-
els, resulting either in fully unsupervised approaches
that perform competitively with the supervised mod-
els or in hybrid (supervised) approaches that provide
the best performing systems in this study.
Our tagging models are able to process word con-
fusion networks as inputs and thus improve perfor-
mance over using 1-best ASR transcripts in scenar-
ios where the speech recognition error rate is high.
These improvements carry through to the MCI clas-
sification task, making use of features computed
from the automatic scoring of narrative retelling.
One advantage of the word-alignment model is
that such approaches do not require manual anno-
tation of the story elements, which is more labor in-
tensive than typical manual transcription of speech.
Thus, the word-alignment model can exploit large
numbers of retellings in an unsupervised manner
when trained on ASR transcripts of the retellings.
Controlled experiments here with relatively limited
training sets demonstrate that semi-supervised ap-
proaches on larger untranscribed sets are likely to
be successful.
Finally, experiments with different amounts of
ASR adaptation show that both acoustic and lan-
guage model adaptations in this domain are effec-
tive, yielding scenarios that are competitive with
manual transcription both for detecting story ele-
ments as well as for subsequent classification. With
full model adaptation to the domain, the 1-best
transcripts improved significantly, and their perfor-
mance was found to be at par with WCNs.
In future work, we would like to investigate two
questions left open by these results. First, word-
alignment models can be extended to process ASR
lattices or word confusion networks as part of the
unsupervised alignment learning algorithm, and in-
corporated into our approach. Second, the con-
textual features can be refined (e.g., concatenated
features instead of smoothed features) when large
amounts of training data is available.
It is noteworthy to mention that the lexical vari-
ants and paraphrasing learned from the data using
automated method may be useful in refining the clin-
ical guidelines for scoring (e.g., allowing additional
lexical variants and paraphrasings, or assigning un-
equal credits for different story elements to reflect
the difficulty of recollecting them) or to create the
guidelines for new languages or stories.
Acknowledgments
This research was supported in part by NIH awards
5K25AG033723-02 and P30 AG024978-05 and
NSF awards 1027834, 0958585, 0905095, 0964102
and 0826654. Any opinions, findings, conclusions
or recommendations expressed in this publication
are those of the authors and do not reflect the views
of the NIH or NSF. We thank Brian Kingsbury and
IBM for the use of their ASR software tools; Jeffrey
Kaye and Diane Howeison for their valuable input;
and the clinicians at the Oregon Center for Aging
and Technology for their care in collecting the data.
219
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(27):1?
27.
Jonathan Fiscus, John Garofolo, Audrey Le, Alvin Mar-
tin, greg Sanders, Mark Przybocki, and David Pallett.
2007. 2004 spring nist rich transcription (rt-04s)
evaluation data. http://www.ldc.upenn.edu/
Catalog/catalogEntry.jsp?catalogId=
LDC2007S12.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Brian Kingsbury, Hagen Soltau, George Saon,
Stephen M. Chu, Hong-Kwang Kuo, Lidia Mangu,
Suman V. Ravuri, Nelson Morgan, and Adam Janin.
2011. The IBM 2009 GALE Arabic speech tran-
scription system. In Proceedings of ICASSP, pages
4672?4675.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Gakuto Kurata, Nobuyasu Itoh, Masafumi Nishimura,
Abhinav Sethy, and Bhuvana Ramabhadran. 2012.
Leveraging word confusion networks for named entity
modeling and detection from conversational telephone
speech. Speech Communication, 54(3):491?502.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran, and
Brian Roark. 2012. Fully automated neuropsycho-
logical assessment for detecting mild cognitive impair-
ment. In Interspeech.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
John Morris. 1993. The clinical dementia rating
(CDR): Current version and scoring rules. Neurology,
43:2412?2414.
A. Nordlund, S. Rolstad, P. Hellstrom, M. Sjogren,
S. Hansen, and A. Wallin. 2005. The Goteborg MCI
study: Mild cognitive impairment is a heterogeneous
condition. Journal of Neurology, Neurosurgery and
Psychiatry, 76(11):1485?1490.
Ronald Petersen, Glenn Smith, Stephen Waring, Robert
Ivnik, Eric Tangalos, and Emre Kokmen. 1999. Mild
cognitive impairment: Clinical characterizations and
outcomes. Archives of Neurology, 56:303?308.
Daniel Povey, Dimitri Kanevsky, Brian Kingsbury,
Bhuvana Ramabhadran, George Saon, and Karthik
Visweswariah. 2008. Boosted mmi for model and fea-
ture space discriminative training. In Proceedings of
ICASSP.
Emily Prud?hommeaux and Brian Roark. 2011. Align-
ment of spoken narratives for automated neuropsycho-
logical assessment. In Proceedings of ASRU.
Emily Prud?hommeaux and Brian Roark. 2012. Graph-
based alignment of narratives for automated neuropsy-
chological assessment. In Proceedings of the NAACL
2012 Workshop on Biomedical Natural Language Pro-
cessing (BioNLP).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
EMNLP.
Karen Ritchie and Jacques Touchon. 2000. Mild cogni-
tive impairment: Conceptual basis and current noso-
logical status. Lancet, 355:225?228.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL.
Martha Storandt and Robert Hill. 1989. Very mild senile
dementia of the Alzheimer?s type: II Psychometric test
performance. Archives of Neurology, 46:383?386.
Qing-Song Wang and Jiang-Ning Zhou. 2002. Retrieval
and encoding of episodic memory in normal aging and
patients with mild cognitive impairment. Brain Re-
search, 924:113?115.
David Wechsler. 1997. Wechsler Memory Scale - Third
Edition. The Psychological Corporation, San Antonio.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of HLT-NAACL.
Xiaonan Zhang, Xiaonan Zhang, Jack Mostow, Jack
Mostow, Nell Duke, Christina Trotochaud, Joseph Va-
leri, and Al Corbett. 2008. Mining free-form spoken
responses to tutor prompts. In Proceedings of the First
International Conference on Educational Data Min-
ing, pages 234?241.
220
Proceedings of NAACL-HLT 2013, pages 709?714,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distributional semantic models for the evaluation of disordered language
Masoud Rouhizadeh?, Emily Prud?hommeaux?, Brian Roark?, Jan van Santen?
?Center for Spoken Language Understanding, Oregon Health & Science University
?Center for Language Sciences, University of Rochester
{rouhizad,vansantj}@ohsu.edu, {emilypx,roarkbr}@gmail.com
Abstract
Atypical semantic and pragmatic expression is
frequently reported in the language of children
with autism. Although this atypicality often
manifests itself in the use of unusual or un-
expected words and phrases, the rate of use
of such unexpected words is rarely directly
measured or quantified. In this paper, we
use distributional semantic models to automat-
ically identify unexpected words in narrative
retellings by children with autism. The classi-
fication of unexpected words is sufficiently ac-
curate to distinguish the retellings of children
with autism from those with typical develop-
ment. These techniques demonstrate the po-
tential of applying automated language anal-
ysis techniques to clinically elicited language
data for diagnostic purposes.
1 Introduction
Autism spectrum disorder (ASD) is a neurodevelop-
mental disorder characterized by impaired commu-
nication and social behavior. Although the symp-
toms of ASD are numerous and varied, atypical
and idiosyncratic language has been one of the
core symptoms observed in verbal individuals with
autism since Kanner first assigned a name to the
disorder (Kanner, 1943). Atypical language cur-
rently serves as a diagnostic criterion in many of the
most widely used diagnostic instruments for ASD
(Lord et al, 2002; Rutter et al, 2003), and the phe-
nomenon is especially marked in the areas of seman-
tics and pragmatics (Tager-Flusberg, 2001; Volden
and Lord, 1991).
Because structured language assessment tools are
not always sensitive to the particular atypical seman-
tic and pragmatic expression associated with ASD,
measures of atypical language are often drawn from
spontaneous language samples. Expert manual an-
notation and analysis of spontaneous language in
young people with ASD has revealed that children
and young adults with autism include significantly
more bizarre and irrelevant content (Loveland et al,
1990; Losh and Capps, 2003) in their narratives and
more abrupt topic changes (Lam et al, 2012) in
their conversations than their language-matched typ-
ically developing peers. Most normed clinical in-
struments for analyzing children?s spontaneous lan-
guage, however, focus on syntactic measures and
developmental milestones related to the acquisition
of vocabulary and syntactic structures. Measures of
semantic and pragmatic atypicality in spontaneous
language are rarely directly measured. Instead, the
degree of language atypicality is often determined
via subjective parental reports (e.g., asking a par-
ent whether their child has ever used odd phrases
(Rutter et al, 2003)) or general impressions dur-
ing clinical examination (e.g., rating the child?s de-
gree of ?stereotyped or idiosyncratic use of words or
phrases? on a four-point scale (Lord et al, 2002)).
This has led to a lack of reliable and objective infor-
mation about the frequency of atypical language use
and its precise nature in ASD.
In this study, we attempt to automatically detect
instances of contextually atypical language in spon-
taneous speech at the lexical level in order to quan-
tify its prevalence in the ASD population. We first
determine manually the off-topic, surprising, or in-
709
appropriate words in a set of narrative retellings
elicited in a clinical setting from children with ASD
and typical development. We then apply word rank-
ing methods and distributional semantic modeling to
these narrative retellings in order to automatically
identify these unexpected words. The results indi-
cate not only that children with ASD do in fact pro-
duce more semantically unexpected and inappropri-
ate words in their narratives than typically develop-
ing children but also that our automated methods
for identifying these words are accurate enough to
serve as an adequate substitute for manual annota-
tion. Although unexpected off-topic word use is just
one example of the atypical language observed in
ASD, the work presented here highlights the poten-
tial of computational language evaluation and analy-
sis methods for improving our understanding of the
linguistic deficits associated with ASD.
2 Data
Participants in this study included 37 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was diag-
nosed via clinical consensus according to the DSM-
IV-TR criteria (American Psychiatric Association,
2000) and the established threshold scores on two
diagnostic instruments: the Autism Diagnostic Ob-
servation Schedule (ADOS) (Lord et al, 2002), a
semi-structured series of activities designed to allow
an examiner to observe behaviors associated with
autism; and the Social Communication Question-
naire (SCQ) (Rutter et al, 2003), a parental ques-
tionnaire. None of the children in this study met
the criteria for a language impairment, and there
were no significant between-group differences in
age (mean=6.4) or full-scale IQ (mean=114).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Korkman
et al, 1998), a large and comprehensive battery of
tasks that test neurocognitive functioning in chil-
dren. The NEPSY Narrative Memory (NNM) sub-
test is a narrative retelling test in which the subject
listens to a brief narrative, excerpts of which are
shown in Figure 1, and then must retell the narra-
tive to the examiner. The NNM was administered
to each participant in the study, and each partici-
pant?s retelling was recorded and transcribed. Us-
ing Amazon?s Mechanical Turk, we also collected
a large corpus of retellings from neurotypical adults,
who listened to a recording of the story and provided
written retellings. We describe how this corpus was
used in Section 3, below.
Two annotators, blind to the diagnosis of the ex-
perimental subjects, identified every word in each
retelling transcript that was unexpected or inappro-
priate given the larger context of the story. For in-
stance, in the sentence T-rex could smell things, both
T-rex and smell were marked as unexpected, since
there is no mention of either concept in the story. In
a seemingly more appropriate sentence, the boy sat
up off the bridge, the word bridge is considered un-
expected since the boy is trapped up in a tree rather
than on a bridge.
3 Methods
We start with the expectation that different retellings
of the same source narrative will share a common
vocabulary and semantic space. The presence of
words outside of this vocabulary or semantic space
in a retelling may indicate that the speaker has
strayed from the topic of the story. Our approach for
automatically identifying these unexpected words
relies on the ranking of words according to the
strength of their association with the target topic of
the corpus. The word association scores used in the
Figure 1: Excerpts from the NNM narrative.
Jim was a boy whose best friend was Pepper. Pepper was a big black dog. [...] Near Jim?s house was a
very tall oak tree with branches so high that he couldn?t reach them. Jim always wanted to climb that tree,
so one day he took a ladder from home and carried it to the oak tree. He climbed up [...] When he started
to get down, his foot slipped, his shoe fell off, and the ladder fell to the ground. [...] Pepper sat below the
tree and barked. Suddenly Pepper took Jim?s shoe in his mouth and ran away. [...] Pepper took the shoe to
Anna, Jim?s sister. He barked and barked. Finally, Anna understood that Jim was in trouble. She followed
Pepper to the tree where Jim was stuck. Anna put the ladder up and rescued Jim.
710
ranking are informed by the frequency of a word
in the child?s retelling relative to the frequency of
that word in other retellings in the larger corpus of
retellings. These association measures are similar
to those developed for the information retrieval task
of topic modeling, in which the goal is to identify
topic-specific words ? i.e., words that appear fre-
quently in only a subset of documents ? in order
to cluster together documents about a similar topic.
Details about how these scores are calculated and in-
terpreted are provided in the following sections.
The pipeline for determining the set of unusual
words in each retelling begins by calculating word
association scores, described below, for each word
in each retelling and ranking the words according to
these scores. A threshold over these scores is de-
termined for each child using leave-one-out cross
validation in order to select a set of potentially un-
expected words. This set of potential unexpected
words is then filtered using two external resources
that allow us to eliminate words that were not used
in other retellings but are likely to be semantically
related to topic of the narrative. This final set of
words is evaluated against the set of manually iden-
tified words in order determine the accuracy of our
unexpected word classification.
3.1 Word association measures
Before calculating the word association measures,
we tokenize, downcase, and stem (Porter, 1980) the
transcripts and remove all punctuation. We then use
two association measures to score each word in each
child?s retelling: tf-idf, the term frequency-inverse
document frequency measure (Salton and Buckley,
1988), and the log odds ratio (van Rijsbergen et al,
1981). We use the following formulation to calcu-
late tf-idf for each child?s retelling i and each word
in that retelling j, where cij is the count of word j
in retelling i; fj is the number of retellings from the
full corpus of child and adult retellings containing
that word j; and D is the total number of retellings
in the full corpus (Manning et al, 2008):
tf-idfij =
{
(1 + log cij) log Dfj if cij ? 1
0 otherwise
The log odds ratio, another association measure
used in information retrieval and extraction tasks, is
the ratio between the odds of a particular word, j,
appearing in a child?s retelling, i, as estimated us-
ing its relative frequency in that retelling, and the
odds of that word appearing in all other retellings,
again estimated using its relative frequency in all
other retellings. Letting the probability of a word
appearing in a retelling be p1 and the probability of
that word appearing in all other retellings be p2, we
can express the odds ratio as follows:
odds ratio =
odds(p1)
odds(p2)
=
p1/(1? p1)
p2/(1? p2)
A large tf-idf or log odds score indicates that the
word j is very specific to the retelling i, which in
turn suggests that the word might be unexpected or
inappropriate in the larger context of the NNM nar-
rative. Thus we expect that the words with higher as-
sociation measure scores are likely to be the words
that were manually identified as unexpected in the
context of the NNM narrative.
3.2 Application of word association measures
As previously mentioned, both of these word associ-
ation measures are used in information retrieval (IR)
to cluster together documents about a similar target
topic. In IR, words that appear only in a subset of
documents from a large and varied corpus of docu-
ments will have high word association scores, and
the documents containing those words will likely be
focused on the same topic. In our task, however,
we have a single cluster of documents focused on
a single topic: the NNM narrative. Topic-specific
words ought to occur much more frequently than
other words. As a result, words with high tf-idf and
log odds scores are likely to be those unrelated to
the topic of the NNM story. If a child veers away
from the topic of the NNM story and uses words that
do not occur frequently in the retellings produced
by neurotypical speakers, his retellings will contain
more words with high word association scores. We
predict that this set of high-scoring words is likely to
overlap significantly with the set of words identified
by the manual annotators as unexpected or off-topic.
Applying these word association scoring ap-
proaches to each word in each child?s retelling yields
a list of words from each retelling ranked in order of
decreasing tf-idf or log odds score. We use cross-
validation to determine, for each measure, the op-
711
erating point that maximizes the unexpected word
identification accuracy in terms of F-measure. For
each child, the threshold is found using the data from
all of the other children. This threshold is then ap-
plied to the ranked word list of the held-out child.
All words above this threshold are potential unex-
pected words, while all words below this threshold
are considered to be expected and appropriate in the
context of the NNM narrative. Table 1 shows the
recall, precision, and F-measure using the two word
association measures discussed here. We see that
these two techniques result in high recall at the ex-
pense of precision. The next stage in the pipeline is
therefore to use external resources to eliminate any
semantically appropriate words from the set of po-
tentially unexpected or inappropriate words gener-
ated via thresholding on the tf-idf or log odds score.
3.3 Filtering with external resources
Recall that the corpus of retellings used to gener-
ate the word association measures described above,
is very small. It is therefore quite possible that a
child may have used an entirely appropriate word
that by chance was never used by another child or
one of the neurotypical adults. One way of increas-
ing the lexical coverage of the corpus of retellings
is through semantic expansion using an external re-
source. For each word in the set of potential un-
expected words, we located the WordNet synset for
that word (Fellbaum, 1998). If any of the WordNet
synonyms of the potentially unexpected word was
present in the source narrative or in one of the adult
retellings, that word was removed from the set of
unexpected words.
In the final step, we used the CHILDES corpus
of transcripts of children?s conversational speech
(MacWhinney, 2000) to generate topic estimates for
each remaining potentially unexpected word. For
each of these words, we located every utterance in
the CHILDES corpus containing that potentially un-
expected word. We then measured the association
of that word with every other open-class word that
appeared in an utterance with that word using the
log likelihood ratio (Dunning, 1993). The 20 words
from the CHILDES corpus with the highest log like-
lihood ratio (i.e., the words most strongly associ-
ated with the potentially unexpected word), were as-
sumed to collectively represent a particular topic. If
more than two of the words in the vector of words
representing this topic were also present in the NNM
source narrative or the adult retellings, the word that
generated that topic was eliminated from the set of
unexpected words.
We note that the optimized threshold described
in Section 3.2, above, is determined after filtering.
There is therefore potentially a different threshold
for each condition tested, and hence we do not nec-
essarily expect precision to increase and recall to
decrease after filtering. Rather, since the threshold
is selected in order to optimize F-measure, we ex-
pect that if the filtering is effective, F-measure will
increase with each additional filtering condition ap-
plied.
4 Results
We evaluated the performance of our two word rank-
ing techniques, both individually and combined by
taking either the maximum of the two measures or
the sum, against the set of manually annotations de-
scribed in Section 2. In addition, we report the re-
sults of applying these word ranking techniques in
combination with the two filtering techniques. We
compare these results with a simple baseline method
in which every word used in a retelling that is never
used in another retelling is considered to be unex-
pected. Table 1 shows the precision, accuracy, and
F-measure of these approaches. We see that all of
the more sophisticated unexpected word identifica-
tion approaches outperform the baseline by a wide
margin, and that tf-idf and log odds perform compa-
rably under the condition without filtering and both
filtering conditions. Filtering improves F-measure
under both word ranking schemes, and combining
the two measures results in further improvements
under both filtering conditions. Although apply-
ing topic-estimate filtering yields the highest preci-
sion, the simple WordNet-based approach results in
the highest F-measure and a reasonable balance be-
tween precision and recall.
Recall that the purpose of identifying these un-
expected words was to determine whether children
with ASD produce unexpected and inappropriate
words at a higher rate than children with typical de-
velopment. This appears to be true in our manu-
ally annotated data. On average, 7.5% of the words
712
Unexpected word identification method P R F1
Baseline 46.3 74.0 57.0
TF-IDF 72.1 79.5 75.6
Log-odds 70.5 79.5 74.7
Sum(TF-IDF, Log-odds) 72.2 83.3 77.4
Max(TF-IDF, Log-odds) 69.9 83.3 76.0
TF-IDF+WordNet 83.8 80.5 82.1
Log-odds+WordNet 82.1 83.1 82.6
Sum(TF-IDF, Log-odds)+WordNet 84.2 83.1 83.7
Max(TF-IDF, Log-odds)+WordNet 83.3 84.4 83.9
TF-IDF+WordNet+topic 85.7 77.9 81.7
Log-odds+WordNet+topic 83.8 80.5 82.1
Sum(TF-IDF, Log-odds)+WordNet+topic 86.1 80.5 83.2
Max(TF-IDF, Log-odds)+WordNet+topic 85.1 81.8 83.4
Table 1: Accuracy of unexpected word identification.
types produced by children with ASD were marked
as unexpected, while only 2.5% of words produced
by children with TD were marked as unexpected, a
significant difference (p < 0.01, using a one-tailed
t-test). This significant between-group difference
in rate of unexpected word use holds even when
using the automated methods of unexpected word
identification, with the best performing unexpected
word identification method estimating a mean of
6.6% in the ASD group and 2.5% in the TD group
(p < 0.01).
5 Conclusions and future work
The automated methods presented here for rank-
ing and filtering words according to their distribu-
tions in different corpora, which are adapted from
techniques originally developed for topic modeling
in the context of information retrieval and extrac-
tion tasks, demonstrate the utility of automated ap-
proaches for the analysis of semantics and pragmat-
ics. We were able to use these methods to iden-
tify unexpected or inappropriate words with high
enough accuracy to replicate the patterns of unex-
pected word use manually observed in our two di-
agnostic groups. This work underscores the poten-
tial of automated techniques for improving our un-
derstanding of the prevalence and diagnostic utility
of linguistic features associated with ASD and other
communication and language disorders.
In future work, we plan to use a development set
to determine the optimal number of topical words
to select during the topic estimate filtering stage of
the pipeline in order to maintain improvements in
precision without a loss in recall. We would also
like to investigate using part-of-speech, word sense,
and parse information to improve our approaches
for both semantic expansion and topic estimation.
Although the rate of unexpected word use alone is
unlikely to provide sufficient power to classify the
two diagnostic groups investigated here, we expect
that it can serve as one feature in an array of fea-
tures that capture the broad range of semantic and
pragmatic atypicalities observed in the spoken lan-
guage of children with autism. Finally, we plan to
apply these same methods to identify the confabula-
tions and topic shifts often observed in the narrative
retellings of the elderly with neurodegenerative con-
ditions.
Acknowledgments
This work was supported in part by NSF
Grant #BCS-0826654, and NIH NIDCD grant
#1R01DC012033-01. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
713
Christian Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic language
deficits in children with high-functioning autism: De-
picting the phenotype using the pragmatic rating scale.
Research in Autism Spectrum Disorders, 6:792797.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability
in high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental Dis-
orders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tunali.
1990. Narrative story telling in autism and down?s
syndrome. British Journal of Developmental Psychol-
ogy, 8(1):9?23.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval. Cambridge University Press.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Helen Tager-Flusberg. 2001. Understanding the lan-
guage and communicative impairments in autism. In-
ternational Review of Research in Mental Retardation,
23:185?205.
C.J. van Rijsbergen, D.J. Harper, and M.F. Porter. 1981.
The selection of good search terms. Information Pro-
cessing and Management, 17(2):77?91.
Joanne Volden and Catherine Lord. 1991. Neologisms
and idiosyncratic language in autistic speakers. Jour-
nal of Autism and Developmental Disorders, 21:109?
130.
714
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 440?449,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Beam-Width Prediction for Efficient Context-Free Parsing
Nathan Bodenstab? Aaron Dunlop? Keith Hall? and Brian Roark?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?Google, Inc., Zurich, Switzerland
{bodensta,dunlopa,roark}@cslu.ogi.edu kbhall@google.com
Abstract
Efficient decoding for syntactic parsing has
become a necessary research area as statisti-
cal grammars grow in accuracy and size and
as more NLP applications leverage syntac-
tic analyses. We review prior methods for
pruning and then present a new framework
that unifies their strengths into a single ap-
proach. Using a log linear model, we learn
the optimal beam-search pruning parameters
for each CYK chart cell, effectively predicting
the most promising areas of the model space
to explore. We demonstrate that our method
is faster than coarse-to-fine pruning, exempli-
fied in both the Charniak and Berkeley parsers,
by empirically comparing our parser to the
Berkeley parser using the same grammar and
under identical operating conditions.
1 Introduction
Statistical constituent parsers have gradually in-
creased in accuracy over the past ten years. This
accuracy increase has opened the door to automati-
cally derived syntactic information within a number
of NLP tasks. Prior work incorporating parse struc-
ture into machine translation (Chiang, 2010) and Se-
mantic Role Labeling (Tsai et al, 2005; Punyakanok
et al, 2008) indicate that such hierarchical structure
can have great benefit over shallow labeling tech-
niques like chunking and part-of-speech tagging.
Although syntax is becoming increasingly impor-
tant for large-scale NLP applications, constituent
parsing is slow ? too slow to scale to the size of
many potential consumer applications. The exhaus-
tive CYK algorithm has computational complexity
O(n3|G|) where n is the length of the sentence and
|G| is the number of grammar productions, a non-
negligible constant. Increases in accuracy have pri-
marily been accomplished through an increase in
the size of the grammar, allowing individual gram-
mar rules to be more sensitive to their surround-
ing context, at a considerable cost in efficiency.
Grammar transformation techniques such as linguis-
tically inspired non-terminal annotations (Johnson,
1998; Klein and Manning, 2003b) and latent vari-
able grammars (Matsuzaki et al, 2005; Petrov et al,
2006) have increased the grammar size |G| from a
few thousand rules to several million in an explic-
itly enumerable grammar, or even more in an im-
plicit grammar. Exhaustive search for the maximum
likelihood parse tree with a state-of-the-art grammar
can require over a minute of processing for a sin-
gle sentence of 25 words, an unacceptable amount
of time for real-time applications or when process-
ing millions of sentences. Deterministic algorithms
for dependency parsing exist that can extract syntac-
tic dependency structure very quickly (Nivre, 2008),
but this approach is often undesirable as constituent
parsers are more accurate and more adaptable to new
domains (Petrov et al, 2010).
The most accurate constituent parsers, e.g., Char-
niak (2000), Petrov and Klein (2007a), make use
of approximate inference, limiting their search to
a fraction of the total search space and achieving
speeds of between one and four newspaper sen-
tences per second. The paradigm for building state-
of-the-art parsing models is to first design a model
structure that can achieve high accuracy and then,
after the model has been built, design effective ap-
proximate inference methods around that particu-
lar model; e.g., coarse-to-fine non-terminal hierar-
chies for a given model, or agenda-based methods
440
that are empirically tuned to achieve acceptable ef-
ficiency/accuracy operating points. While both of
the above mentioned papers use the CYK dynamic
programming algorithm to search through possible
solutions, their particular methods of approximate
inference are quite distinct.
In this paper, we examine a general approach to
approximate inference in constituent parsing that
learns cell-specific thresholds for arbitrary gram-
mars. For each cell in the CYK chart, we sort all
potential constituents in a local agenda, ordered by
an estimate of their posterior probability. Given fea-
tures extracted from the chart cell context ? e.g.,
span width; POS-tags and words surrounding the
boundary of the cell ? we train a log linear model
to predict how many constituents should be popped
from the local agenda and added to the chart. As
a special case of this approach, we simply pre-
dict whether the number to add should be zero or
greater than zero, in which case the method can be
seen as a cell-by-cell generalization of Roark and
Hollingshead?s (2008; 2009) tagger-derived Chart
Constraints. More generally, instead of a binary
classification decision, we can also use this method
to predict the desired cell population directly and
get cell closure for free when the classifier predicts
a beam-width of zero. In addition, we use a non-
symmetric loss function during optimization to ac-
count for the imbalance between over-predicting or
under-predicting the beam-width.
A key feature of our approach is that it does
not rely upon reference syntactic annotations when
learning to search. Rather, the beam-width predic-
tion model is trained to learn the rank of constituents
in the maximum likelihood trees.1 We will illus-
trate this by presenting results using a latent-variable
grammar, for which there is no ?true? reference la-
tent variable parse. We simply parse sections 2-21
of the WSJ treebank and train our search models
from the output of these trees, with no prior knowl-
edge of the non-terminal set or other grammar char-
acteristics to guide the process. Hence, this ap-
1Note that we do not call this method ?unsupervised? be-
cause all grammars used in this paper are induced from super-
vised data, although our framework can also accommodate un-
supervised grammars. We emphasize that we are learning to
search using only maximum likelihood trees, not that we are
doing unsupervised parsing.
Figure 1: Inside (grey) and outside (white) representations of
an example chart edge Ni,j .
proach is broadly applicable to a wide range of sce-
narios, including tuning the search to new domains
where domain mismatch may yield very different ef-
ficiency/accuracy operating points.
In the next section, we present prior work on
approximate inference in parsing, and discuss how
our method to learn optimal beam-search param-
eters unite many of their strengths into a single
framework. We then explore using our approach to
open or close cells in the chart as an alternative to
Roark and Hollingshead (2008; 2009). Finally, we
present results which combine cell closure and adap-
tive beam-width prediction to achieve the most effi-
cient parser.
2 Background
2.1 Preliminaries and notation
Let S = w1 . . . w|S| represent an input string of
|S| words. Let wi,j denote the substring from word
wi+1 to wj ; i.e., S = w0,|S|. We use the term chart
edge to refer to a non-terminal spanning a specific
substring of the input sentence. Let Ni,j denote the
edge labeled with non-terminalN spanning wi,j , for
example NP3,7. We define an edge?s figure-of-merit
(FOM) as an estimate of the product of its inside
(?) and outside (?) scores, conceptually the relative
merit the edge has to participate in the final parse
tree (see Figure 1). More formally:
?(Ni,j) = P (w0,i, Ni,j , wj,n)
?(Ni,j) = P (wi,j |N)
FOM(Ni,j) = ??(Ni,j)??(Ni,j)
441
With bottom-up parsing, the true inside probability
is accumulated and ?(Ni,j) does not need to be esti-
mated, improving the FOMs ability to represent the
true inside/outside distribution.
In this paper, we use a modified version of the
Caraballo and Charniak Boundary FOM (1998)
for local edge comparison, which computes ??(Ni,j)
using POS forward-backward scores and POS-to-
nonterminal constituent boundary transition proba-
bilities. Details can be found in (?).
We also note that in this paper we only use
the FOM scoring function to rank constituents in
a local agenda. Alternative approaches to rank-
ing competitors are also possible, such as Learning
as Search Optimization (Daume? and Marcu, 2005).
The method we present in this paper to learn the op-
timal beam-search parameters is applicable to any
ranking function, and we demonstrate this by com-
puting results with both the Boundary FOM and
only the inside probability in Section 6.
2.2 Agenda-based parsing
Agenda-based parsers maintain a global agenda of
edges, ranked by FOM score. At each iteration, the
highest-scoring edge is popped off of the agenda,
added to the chart, and combined with other edges
already in the chart. The agenda-based approach
includes best-first parsing (Bobrow, 1990) and A*
parsing (Klein and Manning, 2003a), which differ
in whether an admissible FOM estimate ??(Ni,j) is
required. A* uses an admissible FOM, and thus
guarantees finding the maximum likelihood parse,
whereas an inadmissible heuristic (best-first) may
require less exploration of the search space. Much
work has been pursued in both admissible and in-
admissible heuristics for agenda parsing (Caraballo
and Charniak, 1998; Klein and Manning, 2003a;
Pauls et al, 2010).
In this paper, we also make use of agendas, but
at a local rather than a global level. We maintain an
agenda for each cell, which has two significant ben-
efits: 1) Competing edges can be compared directly,
avoiding the difficulty inherent in agenda-based ap-
proaches of comparing edges of radically differ-
ent span lengths and characteristics; and 2) Since
the agendas are very small, the overhead of agenda
maintenance ? a large component of agenda-based
parse time ? is minimal.
2.3 Beam-search parsing
CYK parsing with a beam-search is a local pruning
strategy, comparing edges within the same chart cell.
The beam-width can be defined in terms of a thresh-
old in the number of edges allowed, or in terms of
a threshold on the difference in probability relative
to the highest scoring edge (Collins, 1999; Zhang et
al., 2010). For the current paper, we use both kinds
of thresholds, avoiding pathological cases that each
individual criteria is prone to encounter. Further, un-
like most beam-search approaches we will make use
of a FOM estimate of the posterior probability of an
edge, defined above, as our ranking function. Fi-
nally, we will learn log linear models to assign cell-
specific thresholds, rather than relying on a single
search parameter.
2.4 Coarse-to-Fine Parsing
Coarse-to-fine parsing, also known as multiple pass
parsing (Goodman, 1997; Charniak, 2000; Char-
niak and Johnson, 2005), first parses the input sen-
tence with a simplified (coarse) version of the tar-
get (fine) grammar in which multiple non-terminals
are merged into a single state. Since the coarse
grammar is quite small, parsing is much faster than
with the fine grammar, and can quickly yield an es-
timate of the outside probability ?(?) for use in sub-
sequent agenda or beam-search parsing with the fine
grammar. This approach can also be used iteratively
with grammars of increasing complexity (Petrov and
Klein, 2007a).
Building a coarse grammar from a fine gram-
mar is a non-trivial problem, and most often ap-
proached with detailed knowledge of the fine gram-
mar being used. For example, Goodman (1997)
suggests using a coarse grammar consisting of reg-
ular non-terminals, such as NP and VP, and then
non-terminals augmented with head-word informa-
tion for the more accurate second-pass grammar.
Such an approach is followed by Charniak (2000) as
well. Petrov and Klein (2007a) derive coarse gram-
mars in a more statistically principled way, although
the technique is closely tied to their latent variable
grammar representation.
To the extent that our cell-specific threshold clas-
sifier predicts that a chart cell should contain zero
edges or more than zero edges, it is making coarse
442
predictions about the unlabeled constituent structure
of the target parse tree. This aspect of our work is
can be viewed as a coarse-to-fine process, though
without considering specific grammatical categories
or rule productions.
2.5 Chart Constraints
Roark and Hollingshead (2008; 2009) introduced
a pruning technique that ignores entire chart cells
based on lexical and POS features of the input sen-
tence. They train two finite-state binary taggers:
one that allows multi-word constituents to start at
a word, and one that allows constituents to end at a
word. Given these tags, it is straightforward to com-
pletely skip many chart cells during processing.
In this paper, instead of tagging word positions to
infer valid constituent spans, we classify chart cells
directly. We further generalize this cell classification
to predict the beam-width of the chart cell, where a
beam-width of zero indicates that the cell is com-
pletely closed. We discuss this in detail in the next
section.
3 Open/Closed Cell Classification
3.1 Constituent Closure
We first look at the binary classification of chart cells
as either open or closed to full constituents, and pre-
dict this value from the input sentence alone. This
is the same problem that Roark and Hollingshead
(2008; 2009) solve with Chart Constraints; however,
where they classify lexical items as either beginning
or ending a constituent, we classify individual chart
cells as open or closed, an approach we call Con-
stituent Closure. Although the number of classifi-
cations scales quadratically with our approach, the
total parse time is still dominated by the O(n3|G|)
parsing complexity and we find that the added level
of specificity reduces the search space significantly.
To learn to classify a chart cell spanning words
wi+1 . . . wj of a sentence S as open or closed to full
constituents, we first map cells in the training corpus
to tuples:
?(S, i, j) = (x, y) (1)
where x is a feature-vector representation of the
chart cell and y is the target class 1 if the cell con-
tains an edge from the maximum likelihood parse
tree, 0 otherwise. The feature vector x is encoded
with the chart cell?s absolute and relative span width,
as well as unigram and bigram lexical and part-of-
speech tag items from wi?1 . . . wj+2.
Given feature/target tuples (x, y) for every chart
cell in every sentence of a training corpus ? , we train
a weight vector ? using the averaged perceptron al-
gorithm (Collins, 2002) to learn an open/closed bi-
nary decision boundary:
?? = argmin
?
?
(x,y)??(?)
L?(H(? ? x), y) (2)
where H(?) is the unit step function: 1 if the inner
product ? ?x > 0, and 0 otherwise; and L?(?, ?) is an
asymmetric loss function, defined below.
When predicting cell closure, all misclassifica-
tions are not equal. If we leave open a cell which
contains no edges in the maximum likelihood (ML)
parse, we incur the cost of additional processing, but
are still able to recover the ML tree. However, if we
close a chart cell which contains an ML edge, search
errors occur. To deal with this imbalance, we intro-
duce an asymmetric loss functionL?(?, ?) to penalize
false-negatives more severely during training.
L?(h, y) =
?
??
??
0 if h = y
1 if h > y
? if h < y
(3)
We found the value ? = 102 to give the best per-
formance on our development set, and we use this
value in all of our experiments.
Figures 2a and 2b compare the pruned charts of
Chart Constraints and Constituent Closure for a sin-
gle sentence in the development set. Note that both
of these methods are predicting where a complete
constituent may be located in the chart, not partial
constituents headed by factored nonterminals within
a binarized grammar. Depending on the grammar
factorization (right or left) we can infer chart cells
that are restricted to only edges with a factored left-
hand-side non-terminal. In Figure 2 these chart cells
are colored gray. Note that Constituent Closure re-
duces the number of completely open cells consider-
ably vs. Chart Constraints, and the number of cells
open to factored categories somewhat.
443
3.2 Complete Closure
Alternatively, we can predict whether a chart cell
contains any edge, either a partial or a full con-
stituent, an approach we call Complete Closure.
This is a more difficult classification problem as par-
tial constituents occur in a variety of contexts. Nev-
ertheless, learning this directly allows us to remove a
large number of internal chart cells from considera-
tion, since no additional cells need to be left open to
partial constituents. The learning algorithm is iden-
tical to Equation 2, but training examples are now
assigned a positive label if the chart cell contains any
edge from the binarized maximum likelihood tree.
Figure 2c gives a visual representation of Complete
Closure for the same sentence; the number of com-
pletely open cells increases somewhat, but the total
number of open cells (including those open to fac-
tored categories) is greatly reduced.
We compare the effectiveness of Constituent Clo-
sure, Complete Closure, and Chart Constraints, by
decreasing the percentage of chart cells closed un-
til accuracy over all sentences in our development
set start to decline. For Constituent and Complete
Closure, we also vary the loss function, adjusting
the relative penalty between a false-negative (clos-
ing off a chart cell that contains a maximum like-
lihood edge) and a false-positive. Results show that
using Chart Constrains as a baseline, we prune (skip)
33% of the total chart cells. Constituent Closure im-
proves on this baseline only slightly (36%), but we
see our biggest gains with Complete Closure, which
prunes 56% of all chart cells in the development set.
All of these open/closed cell classification meth-
ods can improve the efficiency of the exhaustive
CYK algorithm, or any of the approximate infer-
ence methods mentioned in Section 2. We empir-
ically evaluate them when applied to CYK parsing
and beam-search parsing in Section 6.
4 Beam-Width Prediction
The cell-closing approaches discussed in Section 3
make binary decisions to either allow or completely
block all edges in each cell. This all-on/all-off tactic
ignores the characteristics of the local cell popula-
tion, which, given a large statistical grammar, may
contain hundred of edges, even if very improbable.
Retaining all of these partial derivations forces the
(a) Chart Constraints (Roark and Hollingshead, 2009)
(b) Constituent Closure (this paper)
(c) Complete Closure (this paper)
Figure 2: Comparison of Chart Constraints (Roark and
Hollingshead, 2009) to Constituent and Complete Closure for a
single example sentence. Black cells are open to all edges while
grey cells only allow factored edges (incomplete constituents).
search in larger spans to continue down improbable
paths, adversely affecting efficiency. We can further
improve parsing speed in these open cells by lever-
aging local pruning methods, such as beam-search.
When parsing with a beam-search, finding the op-
timal beam-width threshold(s) to balance speed and
accuracy is a necessary step. As mentioned in Sec-
444
tion 2.3, two variations of the beam-width are of-
ten considered: a fixed number of allowed edges,
or a relative probability difference from the highest
scoring local edge. For the remainder of this pa-
per we fix the relative probability threshold for all
experiments and focus on adapting the number of
allowed edges per cell. We will refer to this number-
of-allowed-edges value as the beam-width, notated
by b, and leave adaptation of the relative probability
difference to future work.
The standard way to tune the beam-width is a sim-
ple sweep over possible values until accuracy on
a heldout data set starts to decline. The optimal
point will necessarily be very conservative, allowing
outliers (sentences or sub-phrases with above aver-
age ambiguity) to stay within the beam and produce
valid parse trees. The majority of chart cells will
require much fewer than b entries to find the max-
imum likelihood (ML) edge, yet, constrained by a
constant beam-width, the cell will continue to be
filled with unfruitful edges, exponentially increasing
downstream computation.
For example, when parsing with the Berkeley
latent-variable grammar and Boundary FOM, we
find we can reduce the global beam-width b to 15
edges in each cell before accuracy starts to decline.
However we find that 73% of the ML edges are
ranked first in their cell and 96% are ranked in the
top three. Thus, in 24 of every 25 cells, 80% of the
edges are unnecessary (12 of the top 15). Clearly,
it would be advantageous to adapt the beam-width
such that it is restrictive when we are confident in
the FOM ranking and more forgiving in ambiguous
contexts.
To address this problem, we learn the optimal
beam-width for each chart cell directly. We define
Ri,j as the rank of the ML edge in the chart cell
spanning wi+1 . . . wj . If no ML edge exists in the
cell, then Ri,j = 0. Given a global maximum beam-
width b, we train b different binary classifiers, each
using separate mapping functions ?k, where the tar-
get value y produced by ?k is 1 if Ri,j > k and 0
otherwise.
The same asymmetry noted in Section 3 applies
in this task as well. When in doubt, we prefer to
over-predict the beam-width and risk an increase in
processing time opposed to under-predicting at the
expense of accuracy. Thus we use the same loss
function L?, this time training several classifiers:
??k = argmin
?
?
(x,y)??k(?)
L?(H(? ? x), y) (4)
Note that in Equation 4 when k = 0, we re-
cover the open/closed cell classification of Equa-
tion 2, since a beam width of 0 indicates that the
chart cell is completely closed.
During decoding, we assign the beam-width
for chart cell spanning wi+1 . . . wj given models
?0, ?1, ...?b?1 by finding the lowest value k such that
the binary classifier ?k classifiesRi,j ? k. If no such
k exists, R?i,j is set to the maximum beam-width
value b:
R?i,j = argmin
k
?k ? xi ? 0 (5)
In Equation 5 we assume there are b unique clas-
sifiers, one for each possible beam-width value be-
tween 0 and b? 1, but this level of granularity is not
required. Choosing the number of classification bins
to minimize total parsing time is dependent on the
FOM function and how it ranks ML edges. With the
Boundary FOM we use in this paper, 97.8% of ML
edges have a local rank less than five and we find that
the added cost of computing b decision boundaries
for each cell is not worth the added specificity. We
searched over possible classification bins and found
that training four classifiers with beam-width deci-
sion boundaries at 0, 1, 2, and 4 is faster than 15 in-
dividual classifiers and more memory efficient, since
each model ?k has over 800,000 parameters. All
beam-width prediction results reported in this paper
use these settings.
Figure 3 is a visual representation of beam-width
prediction on a single sentence of the development
set using the Berkeley latent-variable grammar and
Boundary FOM. In this figure, the gray scale repre-
sents the relative size of the beam-width, black being
the maximum beam-width value, b, and the lightest
gray being a beam-width of size one. We can see
from this figure that very few chart cells are classi-
fied as needing the full 15 edges, apart from span-1
cells which we do not classify.
445
Figure 3: Visualization of Beam-Width Prediction for a single example sentence. The grey scale represents the size of the predicted
beam-width: white is 0 (cell is skipped) and black is the maximum value b (b=15 in this example).
5 Experimental Setup
We run all experiments on the WSJ treebank (Mar-
cus et al, 1999) using the standard splits: section
2-21 for training, section 22 for development, and
section 23 for testing. We preprocess the treebank
by removing empty nodes, temporal labels, and spu-
rious unary productions (X?X), as is standard in
published works on syntactic parsing.
The pruning methods we present in this paper can
be used to parse with any grammar. To achieve state-
of-the-art accuracy levels, we parse with the Berke-
ley SM6 latent-variable grammar (Petrov and Klein,
2007b) where the original treebank non-terminals
are automatically split into subclasses to optimize
parsing accuracy. This is an explicit grammar con-
sisting of 4.3 million productions, 2.4 million of
which are lexical productions. Exhaustive CYK
parsing with the grammar takes more than a minute
per sentence.
Accuracy is computed from the 1-best Viterbi
(max) tree extracted from the chart. Alternative de-
coding methods, such as marginalizing over the la-
tent variables in the grammar or MaxRule decod-
ing (Petrov and Klein, 2007a) are certainly possible
in our framework, but it is unknown how effective
these methods will be given the heavily pruned na-
ture of the chart. We leave investigation of this to
future work. We compute the precision and recall
of constituents from the 1-best Viterbi trees using
the standard EVALB script (?), which ignores punc-
tuation and the root symbol. Accuracy results are
reported as F-measure (F1), the harmonic mean be-
tween precision and recall.
We ran all timing tests on an Intel 3.00GHz pro-
cessor with 6MB of cache and 16GB of memory.
Our parser is written in Java and publicly available
at http://nlp.csee.ogi.edu.
6 Results
We empirically demonstrate the advantages of our
pruning methods by comparing the total parse time
of each system, including FOM initialization, chart
cell classification, and beam-width prediction. The
parse times reported for Chart Constraints do not in-
clude tagging times as we were provided with this
pre-tagged data, but tagging all of Section 22 takes
less than three seconds and we choose to ignore this
contribution for simplicity.
Figure 4 contains a timing comparison of the three
components of our final parser: Boundary FOM ini-
tialization (which includes the forward-backward al-
gorithm over ambiguous part-of-speech tags), beam-
446
Figure 4: Timing breakdown by sentence length for major
components of our parser.
width prediction, and the final beam-search, includ-
ing 1-best extraction. We bin these relative times
with respect to sentence length to see how each com-
ponent scales with the number of input words. As
expected, theO(n3|G|) beam-search begins to dom-
inate as the sentence length grows, but Boundary
FOM initialization is not cheap, and absorbs, on
average, 20% of the total parse time. Beam-width
prediction, on the other hand, is almost negligible
in terms of processing time even though it scales
quadratically with the length of the sentence.
We compare the accuracy degradation of beam-
width prediction and Chart Constraints in Figure 5
as we incrementally tighten their respective prun-
ing parameters. We also include the baseline beam-
search parser with Boundary FOM in this figure
to demonstrate the accuracy/speed trade-off of ad-
justing a global beam-width alone. In this figure
we see that the knee of the beam-width prediction
curve (Beam-Predict) extends substantially further
to the left before accuracy declines, indicating that
our pruning method is intelligently removing a sig-
nificant portion of the search space that remains un-
pruned with Chart Constraints.
In Table 1 we present the accuracy and parse time
for three baseline parsers on the development set:
exhaustive CYK parsing, beam-search parsing using
only the inside score ?(?), and beam-search parsing
using the Boundary FOM. We then apply our two
cell-closing methods, Constituent Closure and Com-
plete Closure, to all three baselines. As expected,
the relative speedup of these methods across the var-
ious baselines is similar since the open/closed cell
classification does not change across parsers. We
Figure 5: Time vs. accuracy curves comparing beam-width
prediction (Beam-Predict) and Chart Constraints.
also see that Complete Closure is between 22% and
31% faster than Constituent Closure, indicating that
the greater number of cells closed translates directly
into a reduction in parse time. We can further apply
beam-width prediction to the two beam-search base-
line parsers in Table 1. Dynamically adjusting the
beam-width for the remaining open cells decreases
parse time by an additional 25% when using the In-
side FOM, and 28% with the boundary FOM.
We apply our best model to the test set and report
results in Table 2. Beam-width prediction, again,
outperforms the baseline of a constant beam-width
by 65% and the open/closed classification of Chart
Constraints by 49%. We also compare beam-width
prediction to the Berkeley Coarse-to-Fine parser.
Both our parser and the Berkeley parser are written
in Java, both are run with Viterbi decoding, and both
parse with the same grammar, so a direct compari-
son of speed and accuracy is fair.2
7 Conclusion and Future Work
We have introduced three new pruning methods, the
best of which unites figure-of-merit estimation from
agenda-based parsing, local pruning from beam-
search parsing, and unlabeled constituent structure
2We run the Berkeley parser with the default search param-
eterization to achieve the fastest possible parsing time. We note
that 3 of 2416 sentences fail to parse under these settings. Using
the ?-accurate? option provides a valid parse for all sentences,
but increases parsing time of section 23 to 0.293 seconds per
sentence with no increase in F-score. We assume a back-off
strategy for failed parses could be implemented to parse all sen-
tences with a parsing time close to the default parameterization.
447
Parser Sec/Sent F1
CYK 70.383 89.4
CYK + Constituent Closure 47.870 89.3
CYK + Complete Closure 32.619 89.3
Beam + Inside FOM (BI) 3.977 89.2
BI + Constituent Closure 2.033 89.2
BI + Complete Closure 1.575 89.3
BI + Beam-Predict 1.180 89.3
Beam + Boundary FOM (BB) 0.326 89.2
BB + Constituent Closure 0.279 89.2
BB + Complete Closure 0.199 89.3
BB + Beam-Predict 0.143 89.3
Table 1: Section 22 development set results for CYK and
Beam-Search (Beam) parsing using the Berkeley latent-variable
grammar.
prediction from coarse-to-fine parsing and Chart
Constraints. Furthermore, our pruning method is
trained using only maximum likelihood trees, allow-
ing it to be tuned to specific domains without labeled
data. Using this framework, we have shown that we
can decrease parsing time by 65% over a standard
beam-search without any loss in accuracy, and parse
significantly faster than both the Berkeley parser and
Chart Constraints.
We plan to explore a number of remaining ques-
tions in future work. First, we will try combin-
ing our approach with constituent-level Coarse-to-
Fine pruning. The two methods prune the search
space in very different ways and may prove to be
complementary. On the other hand, our parser cur-
rently spends 20% of the total parse time initializing
the FOM, and adding additional preprocessing costs,
such as parsing with a coarse grammar, may not out-
weigh the benefits gained in the final search.
Second, as with Chart Constraints we do not
prune lexical or unary edges in the span-1 chart cells
(i.e., chart cells that span a single word). We ex-
pect pruning entries in these cells would notably re-
duce parse time since they cause exponentially many
chart edges to be built in larger spans. Initial work
constraining span-1 chart cells has promising results
(Bodenstab et al, 2011) and we hope to investigate
its interaction with beam-width prediction even fur-
ther.
Parser Sec/Sent F1
CYK 64.610 88.7
Berkeley CTF MaxRule 0.213 90.2
Berkeley CTF Viterbi 0.208 88.8
Beam + Boundary FOM (BB) 0.334 88.6
BB + Chart Constraints 0.244 88.7
BB + Beam-Predict (this paper) 0.125 88.7
Table 2: Section 23 test set results for multiple parsers using
the Berkeley latent-variable grammar.
Finally, the size and structure of the grammar is
the single largest contributor to parse efficiency. In
contrast to the current paradigm, we plan to inves-
tigate new algorithms that jointly optimize accuracy
and efficiency during grammar induction, leading to
more efficient decoding.
Acknowledgments
We would like to thank Kristy Hollingshead for
her valuable discussions, as well as the anony-
mous reviewers who gave very helpful feedback.
This research was supported in part by NSF Grants
#IIS-0447214, #IIS-0811745 and DARPA grant
#HR0011-09-1-0041. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or DARPA.
References
Robert J. Bobrow. 1990. Statistical agenda parsing. In
DARPA Speech and Language Workshop, pages 222?
224.
Nathan Bodenstab, Kristy Hollingshead, and Brian
Roark. 2011. Unary constraints for efficient context-
free parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics,
Portland, Oregon.
Sharon A Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Computational Linguistics, 24:275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
448
chapter of the Association for Computational Linguis-
tics conference, pages 132?139, Seattle, Washington.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 1443?1452.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD dissertation, Uni-
versity of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical Methods in
Natural Language Processing, volume 10, pages 1?8,
Philadelphia.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In Proceedings of the
22nd international conference on Machine learning,
ICML ?05, pages 169?176, New York, NY, USA.
Joshua Goodman. 1997. Global thresholding and
Multiple-Pass parsing. Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 11?25.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D. Manning. 2003a. A* pars-
ing. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL ?03), pages 40?47, Edmonton, Canada.
Dan Klein and Christopher D. Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423?430, Sap-
poro, Japan.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3,
Philadelphia.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics - ACL ?05, pages
75?82, Ann Arbor, Michigan.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34:513?553.
Adam Pauls, Dan Klein, and Chris Quirk. 2010. Top-
down k-best a* parsing. In In proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics Short Papers, ACLShort ?10, pages 200?204,
Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007a. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York.
Slav Petrov and Dan Klein. 2007b. Learning and in-
ference for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 433?440, Syd-
ney, Australia.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Donia Scott and Hans Uszkoreit, editors,
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 745?
752, Manchester, UK.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity Context-Free parsing pipelines via chart
constraints. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 647?655, Boulder, Colorado.
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-
Lian Hsu. 2005. Exploiting full parsing information
to label semantic roles using an ensemble of ME and
SVM via integer linear programming. In Proceed-
ings of the Ninth Conference on Computational Natu-
ral Language Learning, CONLL ?05, pages 233?236,
Morristown, NJ, USA.
Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast Lexicalised-Grammar parsing.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 1472?1479, Bei-
jing, China.
449
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 1?5,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Lexicographic Semirings for Exact Automata Encoding of Sequence Models
Brian Roark, Richard Sproat, and Izhak Shafran
{roark,rws,zak}@cslu.ogi.edu
Abstract
In this paper we introduce a novel use of the
lexicographic semiring and motivate its use
for speech and language processing tasks. We
prove that the semiring allows for exact en-
coding of backoff models with epsilon tran-
sitions. This allows for off-line optimization
of exact models represented as large weighted
finite-state transducers in contrast to implicit
(on-line) failure transition representations. We
present preliminary empirical results demon-
strating that, even in simple intersection sce-
narios amenable to the use of failure transi-
tions, the use of the more powerful lexico-
graphic semiring is competitive in terms of
time of intersection.
1 Introduction and Motivation
Representing smoothed n-gram language models as
weighted finite-state transducers (WFST) is most
naturally done with a failure transition, which re-
flects the semantics of the ?otherwise? formulation
of smoothing (Allauzen et al, 2003). For example,
the typical backoff formulation of the probability of
a word w given a history h is as follows
P(w | h) =
{
P(w | h) if c(hw) > 0
?hP(w | h?) otherwise
(1)
where P is an empirical estimate of the probabil-
ity that reserves small finite probability for unseen
n-grams; ?h is a backoff weight that ensures nor-
malization; and h? is a backoff history typically
achieved by excising the earliest word in the his-
tory h. The principle benefit of encoding the WFST
in this way is that it only requires explicitly storing
n-gram transitions for observed n-grams, i.e., count
greater than zero, as opposed to all possible n-grams
of the given order which would be infeasible in for
example large vocabulary speech recognition. This
is a massive space savings, and such an approach is
also used for non-probabilistic stochastic language
models, such as those trained with the perceptron
algorithm (Roark et al, 2007), as the means to ac-
cess all and exactly those features that should fire
for a particular sequence in a deterministic automa-
ton. Similar issues hold for other finite-state se-
quence processing problems, e.g., tagging, bracket-
ing or segmenting.
Failure transitions, however, are an implicit
method for representing a much larger explicit au-
tomaton ? in the case of n-gram models, all pos-
sible n-grams for that order. During composition
with the model, the failure transition must be inter-
preted on the fly, keeping track of those symbols
that have already been found leaving the original
state, and only allowing failure transition traversal
for symbols that have not been found (the semantics
of ?otherwise?). This compact implicit representa-
tion cannot generally be preserved when composing
with other models, e.g., when combining a language
model with a pronunciation lexicon as in widely-
used FST approaches to speech recognition (Mohri
et al, 2002). Moving from implicit to explicit repre-
sentation when performing such a composition leads
to an explosion in the size of the resulting trans-
ducer, frequently making the approach intractable.
In practice, an off-line approximation to the model
is made, typically by treating the failure transitions
as epsilon transitions (Mohri et al, 2002; Allauzen
et al, 2003), allowing large transducers to be com-
posed and optimized off-line. These complex ap-
proximate transducers are then used during first-pass
decoding, and the resulting pruned search graphs
(e.g., word lattices) can be rescored with exact lan-
guage models encoded with failure transitions.
Similar problems arise when building, say, POS-
taggers as WFST: not every pos-tag sequence will
have been observed during training, hence failure
transitions will achieve great savings in the size of
models. Yet discriminative models may include
complex features that combine both input stream
(word) and output stream (tag) sequences in a single
feature, yielding complicated transducer topologies
for which effective use of failure transitions may not
1
be possible. An exact encoding using other mecha-
nisms is required in such cases to allow for off-line
representation and optimization.
In this paper, we introduce a novel use of a semir-
ing ? the lexicographic semiring (Golan, 1999) ?
which permits an exact encoding of these sorts of
models with the same compact topology as with fail-
ure transitions, but using epsilon transitions. Unlike
the standard epsilon approximation, this semiring al-
lows for an exact representation, while also allow-
ing (unlike failure transition approaches) for off-line
composition with other transducers, with all the op-
timizations that such representations provide.
In the next section, we introduce the semiring, fol-
lowed by a proof that its use yields exact represen-
tations. We then conclude with a brief evaluation of
the cost of intersection relative to failure transitions
in comparable situations.
2 The Lexicographic Semiring
Weighted automata are automata in which the tran-
sitions carry weight elements of a semiring (Kuich
and Salomaa, 1986). A semiring is a ring that may
lack negation, with two associative operations? and
? and their respective identity elements 0 and 1. A
common semiring in speech and language process-
ing, and one that we will be using in this paper, is
the tropical semiring (R? {?},min,+,?, 0), i.e.,
min is the ? of the semiring (with identity?) and
+ is the ? of the semiring (with identity 0). This is
appropriate for performing Viterbi search using neg-
ative log probabilities ? we add negative logs along
a path and take the min between paths.
A ?W1,W2 . . .Wn?-lexicographic weight is a tu-
ple of weights where each of the weight classes
W1,W2 . . .Wn, must observe the path property
(Mohri, 2002). The path property of a semiring K
is defined in terms of the natural order on K such
that: a <K b iff a ? b = a. The tropical semiring
mentioned above is a common example of a semir-
ing that observes the path property, since:
w1 ? w2 = min{w1, w2}
w1 ? w2 = w1 + w2
The discussion in this paper will be restricted to
lexicographic weights consisting of a pair of tropi-
cal weights ? henceforth the ?T, T ?-lexicographic
semiring. For this semiring the operations ? and ?
are defined as follows (Golan, 1999, pp. 223?224):
?w1, w2? ? ?w3, w4? =
?
????
????
if w1 < w3 or
?w1, w2? (w1 = w3 &
w2 < w4)
?w3, w4? otherwise
?w1, w2? ? ?w3, w4? = ?w1 + w3, w2 + w4?
The term ?lexicographic? is an apt term for this
semiring since the comparison for ? is like the lexi-
cographic comparison of strings, comparing the first
elements, then the second, and so forth.
3 Language model encoding
3.1 Standard encoding
For language model encoding, we will differentiate
between two classes of transitions: backoff arcs (la-
beled with a ? for failure, or with  using our new
semiring); and n-gram arcs (everything else, labeled
with the word whose probability is assigned). Each
state in the automaton represents an n-gram history
string h and each n-gram arc is weighted with the
(negative log) conditional probability of the word w
labeling the arc given the history h. For a given his-
tory h and n-gram arc labeled with a word w, the
destination of the arc is the state associated with the
longest suffix of the string hw that is a history in the
model. This will depend on the Markov order of the
n-gram model. For example, consider the trigram
model schematic shown in Figure 1, in which only
history sequences of length 2 are kept in the model.
Thus, from history hi = wi?2wi?1, the word wi
transitions to hi+1 = wi?1wi, which is the longest
suffix of hiwi in the model.
As detailed in the ?otherwise? semantics of equa-
tion 1, backoff arcs transition from state h to a state
h?, typically the suffix of h of length |h| ? 1, with
weight (? log?h). We call the destination state a
backoff state. This recursive backoff topology ter-
minates at the unigram state, i.e., h = , no history.
Backoff states of order k may be traversed either
via ?-arcs from the higher order n-gram of order k+
1 or via an n-gram arc from a lower order n-gram of
order k?1. This means that no n-gram arc can enter
the zeroeth order state (final backoff), and full-order
states ? history strings of length n? 1 for a model
of order n ? may have n-gram arcs entering from
other full-order states as well as from backoff states
of history size n? 2.
3.2 Encoding with lexicographic semiring
For an LM machineM on the tropical semiring with
failure transitions, which is deterministic and has the
2
h i =wi-2wi-1 hi+1 =wi-1wiwi /-logP(wi |h i)
wi-1
?/-log ?hi
wi
?/-log ?h i+1
wi /-logP(wi|wi-1)
?/-log ?w i-1 wi /-logP(wi)
Figure 1: Deterministic finite-state representation of n-gram
models with negative log probabilities (tropical semiring). The
symbol ? labels backoff transitions. Modified from Roark and
Sproat (2007), Figure 6.1.
path property, we can simulate ?-arcs in a standard
LM topology by a topologically equivalent machine
M ? on the lexicographic ?T, T ? semiring, where ?
has been replaced with epsilon, as follows. For every
n-gram arc with label w and weight c, source state
si and destination state sj , construct an n-gram arc
with label w, weight ?0, c?, source state s?i, and des-
tination state s?j . The exit cost of each state is con-
structed as follows. If the state is non-final, ??,??.
Otherwise if it final with exit cost c it will be ?0, c?.
Let n be the length of the longest history string in
the model. For every ?-arc with (backoff) weight
c, source state si, and destination state sj repre-
senting a history of length k, construct an -arc
with source state s?i, destination state s
?
j , and weight
???(n?k), c?, where ? > 0 and ??(n?k) takes ? to
the (n ? k)th power with the ? operation. In the
tropical semiring, ? is +, so ??(n?k) = (n ? k)?.
For example, in a trigram model, if we are backing
off from a bigram state h (history length = 1) to a
unigram state, n ? k = 2 ? 0 = 2, so we set the
backoff weight to ?2?,? log?h) for some ? > 0.
In order to combine the model with another au-
tomaton or transducer, we would need to also con-
vert those models to the ?T, T ? semiring. For these
automata, we simply use a default transformation
such that every transition with weight c is assigned
weight ?0, c?. For example, given a word lattice
L, we convert the lattice to L? in the lexicographic
semiring using this default transformation, and then
perform the intersection L? ?M ?. By removing ep-
silon transitions and determinizing the result, the
low cost path for any given string will be retained
in the result, which will correspond to the path
achieved with ?-arcs. Finally we project the second
dimension of the ?T, T ? weights to produce a lattice
in the tropical semiring, which is equivalent to the
result of L ?M , i.e.,
C2(det(eps-rem(L? ?M ?))) = L ?M
where C2 denotes projecting the second-dimension
of the ?T, T ? weights, det(?) denotes determiniza-
tion, and eps-rem(?) denotes -removal.
4 Proof
We wish to prove that for any machine N ,
ShortestPath(M ? ? N ?) passes through the equiv-
alent states in M ? to those passed through in M for
ShortestPath(M ? N). Therefore determinization
of the resulting intersection after -removal yields
the same topology as intersection with the equiva-
lent ? machine. Intuitively, since the first dimension
of the ?T, T ? weights is 0 for n-gram arcs and > 0
for backoff arcs, the shortest path will traverse the
fewest possible backoff arcs; further, since higher-
order backoff arcs cost less in the first dimension of
the ?T, T ? weights in M ?, the shortest path will in-
clude n-gram arcs at their earliest possible point.
We prove this by induction on the state-sequence
of the path p/p? up to a given state si/s?i in the respec-
tive machines M/M ?.
Base case: If p/p? is of length 0, and therefore the
states si/s?i are the initial states of the respective ma-
chines, the proposition clearly holds.
Inductive step: Now suppose that p/p? visits
s0...si/s?0...s
?
i and we have therefore reached si/s
?
i
in the respective machines. Suppose the cumulated
weights of p/p? are W and ??,W ?, respectively. We
wish to show that whichever sj is next visited on p
(i.e., the path becomes s0...sisj) the equivalent state
s? is visited on p? (i.e., the path becomes s?0...s
?
is
?
j).
Let w be the next symbol to be matched leaving
states si and s?i. There are four cases to consider:
(1) there is an n-gram arc leaving states si and s?i la-
beled with w, but no backoff arc leaving the state;
(2) there is no n-gram arc labeled with w leaving the
states, but there is a backoff arc; (3) there is no n-
gram arc labeled with w and no backoff arc leaving
the states; and (4) there is both an n-gram arc labeled
with w and a backoff arc leaving the states. In cases
(1) and (2), there is only one possible transition to
take in either M or M ?, and based on the algorithm
for construction of M ? given in Section 3.2, these
transitions will point to sj and s?j respectively. Case
(3) leads to failure of intersection with either ma-
chine. This leaves case (4) to consider. In M , since
there is a transition leaving state si labeled with w,
3
the backoff arc, which is a failure transition, can-
not be traversed, hence the destination of the n-gram
arc sj will be the next state in p. However, in M ?,
both the n-gram transition labeled with w and the
backoff transition, now labeled with , can be tra-
versed. What we will now prove is that the shortest
path through M ? cannot include taking the backoff
arc in this case.
In order to emit w by taking the backoff arc out
of state s?i, one or more backoff () transitions must
be taken, followed by an n-gram arc labeled with
w. Let k be the order of the history represented
by state s?i, hence the cost of the first backoff arc
is ?(n? k)?,? log(?s?i)? in our semiring. If we
traverse m backoff arcs prior to emitting the w,
the first dimension of our accumulated cost will be
m(n? k+ m?12 )?, based on our algorithm for con-
struction of M ? given in Section 3.2. Let s?l be the
destination state after traversing m backoff arcs fol-
lowed by an n-gram arc labeled with w. Note that,
by definition, m ? k, and k ? m + 1 is the or-
der of state s?l. Based on the construction algo-
rithm, the state s?l is also reachable by first emit-
ting w from state s?i to reach state s
?
j followed by
some number of backoff transitions. The order of
state s?j is either k (if k is the highest order in the
model) or k + 1 (by extending the history of state
s?i by one word). If it is of order k, then it will re-
quire m? 1 backoff arcs to reach state s?l, one fewer
than the path to state s?l that begins with a back-
off arc, for a total cost of (m? 1)(n? k + m?12 )?
which is less than m(n? k + m?12 )?. If state
s?j is of order k + 1, there will be m backoff
arcs to reach state s?l, but with a total cost of
m(n? (k + 1) + m?12 )? = m(n? k +
m?3
2 )?
which is also less than m(n? k + m?12 )?. Hence
the state s?l can always be reached from s
?
i with a
lower cost through state s?j than by first taking the
backoff arc from s?i. Therefore the shortest path on
M ? must follow s?0...s
?
is
?
j . 2
This completes the proof.
5 Experimental Comparison of , ? and
?T, T ? encoded language models
For our experiments we used lattices derived from a
very large vocabulary continuous speech recognition
system, which was built for the 2007 GALE Ara-
bic speech recognition task, and used in the work
reported in Lehr and Shafran (2011). The lexico-
graphic semiring was evaluated on the development
set (2.6 hours of broadcast news and conversations;
18K words). The 888 word lattices for the develop-
ment set were generated using a competitive base-
line system with acoustic models trained on about
1000 hrs of Arabic broadcast data and a 4-gram lan-
guage model. The language model consisting of
122M n-grams was estimated by interpolation of 14
components. The vocabulary is relatively large at
737K and the associated dictionary has only single
pronunciations.
The language model was converted to the automa-
ton topology described earlier, and represented in
three ways: first as an approximation of a failure
machine using epsilons instead of failure arcs; sec-
ond as a correct failure machine; and third using the
lexicographic construction derived in this paper.
The three versions of the LM were evaluated by
intersecting them with the 888 lattices of the de-
velopment set. The overall error rate for the sys-
tems was 24.8%?comparable to the state-of-the-
art on this task1. For the shortest paths, the failure
and lexicographic machines always produced iden-
tical lattices (as determined by FST equivalence);
in contrast, 81% of the shortest paths from the ep-
silon approximation are different, at least in terms
of weights, from the shortest paths using the failure
LM. For full lattices, 42 (4.7%) of the lexicographic
outputs differ from the failure LM outputs, due to
small floating point rounding issues; 863 (97%) of
the epsilon approximation outputs differ.
In terms of size, the failure LM, with 5.7 mil-
lion arcs requires 97 Mb. The equivalent ?T, T ?-
lexicographic LM requires 120 Mb, due to the dou-
bling of the size of the weights.2 To measure speed,
we performed the intersections 1000 times for each
of our 888 lattices on a 2993 MHz Intel R? Xeon R?
CPU, and took the mean times for each of our meth-
ods. The 888 lattices were processed with a mean
of 1.62 seconds in total (1.8 msec per lattice) us-
ing the failure LM; using the ?T, T ?-lexicographic
LM required 1.8 seconds (2.0 msec per lattice), and
is thus about 11% slower. Epsilon approximation,
where the failure arcs are approximated with epsilon
arcs took 1.17 seconds (1.3 msec per lattice). The
1The error rate is a couple of points higher than in Lehr and
Shafran (2011) since we discarded non-lexical words, which are
absent in maximum likelihood estimated language model and
are typically augmented to the unigram backoff state with an
arbitrary cost, fine-tuned to optimize performance for a given
task.
2If size became an issue, the first dimension of the ?T, T ?-
weight can be represented by a single byte.
4
slightly slower speeds for the exact method using the
failure LM, and ?T, T ? can be related to the over-
head of computing the failure function at runtime,
and determinization, respectively.
6 Conclusion
In this paper we have introduced a novel applica-
tion of the lexicographic semiring, proved that it
can be used to provide an exact encoding of lan-
guage model topologies with failure arcs, and pro-
vided experimental results that demonstrate its ef-
ficiency. Since the ?T, T ?-lexicographic semiring
is both left- and right-distributive, other optimiza-
tions such as minimization are possible. The par-
ticular ?T, T ?-lexicographic semiring we have used
here is but one of many possible lexicographic en-
codings. We are currently exploring the use of a
lexicographic semiring that involves different semir-
ings in the various dimensions, for the integration of
part-of-speech taggers into language models.
An implementation of the lexicographic semir-
ing by the second author is already available as
part of the OpenFst package (Allauzen et al, 2007).
The methods described here are part of the NGram
language-model-training toolkit, soon to be released
at opengrm.org.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA. We thank Maider Lehr for
help in preparing the test data. We also thank the
ACL reviewers for valuable comments.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of Au-
tomata (CIAA 2007), Lecture Notes in Computer Sci-
ence, volume 4793, pages 11?23, Prague, Czech Re-
public. Springer.
Jonathan Golan. 1999. Semirings and their Applications.
Kluwer Academic Publishers, Dordrecht.
Werner Kuich and Arto Salomaa. 1986. Semirings,
Automata, Languages. Number 5 in EATCS Mono-
graphs on Theoretical Computer Science. Springer-
Verlag, Berlin, Germany.
Maider Lehr and Izhak Shafran. 2011. Learning a dis-
criminative weighted finite-state transducer for speech
recognition. IEEE Transactions on Audio, Speech, and
Language Processing, July.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69?88.
Mehryar Mohri. 2002. Semiring framework and algo-
rithms for shortest-distance problems. Journal of Au-
tomata, Languages and Combinatorics, 7(3):321?350.
Brian Roark and Richard Sproat. 2007. Computational
Approaches to Morphology and Syntax. Oxford Uni-
versity Press, Oxford.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373?392.
5
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 236?241,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Modeling for Prenominal Modifier Ordering
Margaret Mitchell
University of Aberdeen
Aberdeen, Scotland, U.K.
m.mitchell@abdn.ac.uk
Aaron Dunlop
Oregon Health & Science University
Portland, OR
dunlopa@cslu.ogi.edu
Brian Roark
Oregon Health & Science University
Portland, OR
roark@cslu.ogi.edu
Abstract
In this paper, we argue that ordering prenom-
inal modifiers ? typically pursued as a su-
pervised modeling task ? is particularly well-
suited to semi-supervised approaches. By
relying on automatic parses to extract noun
phrases, we can scale up the training data
by orders of magnitude. This minimizes
the predominant issue of data sparsity that
has informed most previous approaches. We
compare several recent approaches, and find
improvements from additional training data
across the board; however, none outperform
a simple n-gram model.
1 Introduction
In any given noun phrase (NP), an arbitrary num-
ber of nominal modifiers may be used. The order of
these modifiers affects how natural or fluent a phrase
sounds. Determining a natural ordering is a key task
in the surface realization stage of a natural language
generation (NLG) system, where the adjectives and
other modifiers chosen to identify a referent must be
ordered before a final string is produced. For ex-
ample, consider the alternation between the phrases
?big red ball? and ?red big ball?. The phrase ?big
red ball? provides a basic ordering of the words big
and red. The reverse ordering, in ?red big ball?,
sounds strange, a phrase that would only occur in
marked situations. There is no consensus on the ex-
act qualities that affect a modifier?s position, but it is
clear that some modifier orderings sound more natu-
ral than others, even if all are strictly speaking gram-
matical.
Determining methods for ordering modifiers
prenominally and investigating the factors underly-
ing modifier ordering have been areas of consider-
able research, including work in natural language
processing (Shaw and Hatzivassiloglou, 1999; Mal-
ouf, 2000; Mitchell, 2009; Dunlop et al, 2010), lin-
guistics (Whorf, 1945; Vendler, 1968), and psychol-
ogy (Martin, 1969; Danks and Glucksberg, 1971).
A central issue in work on modifier ordering is how
to order modifiers that are unobserved during sys-
tem development. English has upwards of 200,000
words, with over 50,000 words in the vocabulary of
an educated adult (Aitchison, 2003). Up to a quar-
ter of these words may be adjectives, which poses a
significant problem for any system that attempts to
categorize English adjectives in ways that are useful
for an ordering task. Extensive in-context observa-
tion of adjectives and other modifiers is required to
adequately characterize their behavior.
Developers of automatic modifier ordering sys-
tems have thus spent considerable effort attempting
to make reliable predictions despite sparse data, and
have largely limited their systems to order modifier
pairs instead of full modifier strings. Conventional
wisdom has been that direct evidence methods such
as simple n-gram modeling are insufficient for cap-
turing such a complex and productive process.
Recent approaches have therefore utilized in-
creasingly sophisticated data-driven approaches.
Most recently, Dunlop et al (2010) used both dis-
criminative and generative methods for estimat-
ing class-based language models with multiple-
sequence alignments (MSA). Training on manually
curated syntactic corpora, they showed excellent in-
domain performance relative to prior systems, and
decent cross-domain generalization.
However, following a purely supervised training
approach for this task is unduly limiting and leads
to conventional assumptions that are not borne out
in practice, such as the inapplicability of simple n-236
gram models. NP segmentation is one of the most
reliable annotations that automatic parsers can now
produce, and may be applied to essentially arbitrary
amounts of unlabeled data. This yields orders-of-
magnitude larger training sets, so that methods that
are sensitive to sparse data and/or are domain spe-
cific can be trained on sufficient data.
In this paper, we compare an n-gram language
model and a hidden Markov model (HMM) con-
structed using expectation maximization (EM) with
several recent ordering approaches, and demonstrate
superior performance of the n-gram model across
different domains, particularly as the training data
size is scaled up. This paper presents two important
results: 1) N-gram modeling performs better than
previously believed for this task, and in fact sur-
passes current class-based systems.1 2) Automatic
parsers can effectively provide essentially unlimited
training data for learning modifier ordering prefer-
ences. Our results point the way to larger scale data-
driven approaches to this and related tasks.
2 Related Work
In one of the earliest automatic prenominal mod-
ifier ordering systems, Shaw and Hatzivassiloglou
(1999) ordered pairs of modifiers, including adjec-
tives, nouns (?baseball field?); gerunds, (?running
man?); and participles (?heated debate?). They
described a direct evidence method, a transitivity
method, and a clustering method for ordering these
different kinds of modifiers, with the transitivity
technique returning the highest accuracy of 90.67%
on a medical text. However, when testing across
domains, their accuracy dropped to 56%, not much
higher than random guessing.
Malouf (2000) continued this work, ordering
prenominal adjective pairs in the BNC. He aban-
doned a bigram model, finding it achieved only
75.57% prediction accuracy, and instead pursued
statistical and machine learning techniques that are
more robust to data sparsity. Malouf achieved an
accuracy of 91.85% by combining three systems.
However, it is not clear whether the proposed or-
dering approaches extend to other kinds of modi-
fiers, such as gerund verbs and nouns, and he did
not present analysis of cross-domain generalization.
1But note that these approaches may still be useful, e.g.,
when the goal is to construct general modifier classes.
Dataset 2 mods 3 mods 4 mods
WSJ 02-21 auto 10,070 1,333 129
WSJ 02-21 manu 9,976 1,311 129
NYT 1,616,497 191,787 18,183
Table 1: Multi-modifier noun phrases in training data
Dataset 2 mods 3 mods 4 mods
WSJ 22-24 1,366 152 20
SWBD 1,376 143 19
Brown 1,428 101 9
Table 2: Multi-modifier noun phrases in testing data
Later, Mitchell (2009) focused on creating a class-
based model for modifier ordering. Her system
mapped each modifier to a class based on the fre-
quency with which it occurs in different prenominal
positions, and ordered unseen sequences based on
these classes. Dunlop et al (2010) used a Multiple
Sequence Alignment (MSA) approach to order mod-
ifiers, achieving the highest accuracy to date across
different domains. In contrast to earlier work, both
systems order full modifier strings.
Below, we evaluate these most recent systems,
scaling up the training data by several orders of mag-
nitude. Our results indicate that an n-gram model
outperforms previous systems, and generalizes quite
well across different domains.
3 Corpora
Following Dunlop et al (2010), we use the Wall St.
Journal (WSJ), Switchboard (SWBD) and Brown
corpus sections of the Penn Treebank (Marcus et al,
1993) as our supervised training and testing base-
lines. For semi-supervised training, we automati-
cally parse sections 02-21 of the WSJ treebank using
cross-validation methods, and scale up the amount
of data used by parsing the New York Times (NYT)
section of the Gigaword (Graff and Cieri, 2003) cor-
pus using the Berkeley Parser (Petrov and Klein,
2007; Petrov, 2010).
Table 1 lists the NP length distributions for each
training corpus. The WSJ training corpus yields just
under 5,100 distinct modifier types (without normal-
izing for capitalization), while the NYT data yields
105,364. Note that the number of NPs extracted
from the manual and automatic parses of the WSJ
are quite close. We find that the overlap between the
two groups is well over 90%, suggesting that extract-237
ing NPs from a large, automatically parsed corpus
will provide phrases comparable to manually anno-
tated NPs.
We evaluate across a variety of domains, includ-
ing (1) the WSJ sections 22-24, and sections com-
mensurate in size of (2) the SWBD corpus and (3)
the Brown corpus. Table 2 lists the NP length distri-
butions for each test corpus.
4 Methods
In this section, we present two novel prenominal
modifier ordering approaches: a 5-gram model and
an EM-trained HMM. In both systems, modifiers
that occur only once in the training data are given the
Berkeley parser OOV class labels (Petrov, 2010).
In Section 5, we compare these approaches to the
one-class system described in Mitchell (2010) and
the discriminative MSA described in Dunlop et al
(2010). We refer the interested reader to those pa-
pers for the details of their learning algorithms.
4.1 N-Gram Modeling
We used the SRILM toolkit (Stolcke, 2002) to build
unpruned 5-gram models using interpolated mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998). In the testing phase, each possible permuta-
tion is assigned a probability by the model, and the
highest probability sequence is chosen.
We explored building n-gram models based on
entire observed sequences (sentences) and on ex-
tracted multiple modifier NPs. As shown in Table
3, we found a very large (12% absolute) accuracy
improvement in a model trained with just NP se-
quences. This is likely due to several factors, in-
cluding the role of the begin string symbol <s>,
which helps to capture word preferences for occur-
ring first in a modifier sequence; also the behav-
ior of modifiers when they occur in NPs may dif-
fer from how they behave in other contexts. Note
that the full-sentence n-gram model performs sim-
ilarly to Malouf?s bigram model; although the re-
sults are not directly comparable, this may explain
the common impression that n-gram modeling is not
effective for modifier ordering. We find that syntac-
tic annotations are critical for this task; all n-gram
results presented in the rest of the paper are trained
on extracted NPs.
Training data for n-gram model Accuracy
Full sentences 75.9
Extracted multi-modifier NPs 88.1
Table 3: Modifier ordering accuracy on WSJ sections 22-
24, trained on sections 2-21
4.2 Hidden Markov Model
Mitchell?s single-class system and Dunlop et. al?s
MSA approach both group tokens into position clus-
ters. The success of these systems suggests that a
position-specific class-based HMM might perform
well on this task. We use EM (Dempster et al, 1977)
to learn the parameterizations of such an HMM.
The model is defined in terms of state transition
probabilities P(c? | c), i.e., the probability of transi-
tioning from a state labeled c to a state labeled c?;
and state observation probabilities P(w | c), i.e.,
the probability of emitting word w from a particu-
lar class c. Since the classes are predicting an or-
dering, we include hard constraints on class tran-
sitions. Specifically, we forbid a transition from a
class closer to the head noun to one farther away.
More formally, if the subscript of a class indicates
its distance from the head, then for any i, j, P(ci |
cj) = 0 if i ? j; i.e., ci is stipulated to never occur
closer to the head than cj .
We established 8 classes and an HMM Markov
order of 1 (along with start and end states) based
on performance on a held-out set (section 00 of the
WSJ treebank). We initialize the model with a uni-
form distribution over allowed transition and emis-
sion probabilities, and use add-? regularization in
the M-step of EM at each iteration. We empirically
determined ? smoothing values of 0.1 for emissions
and 500 for transitions. Rather than training to full
convergence of the corpus likelihood, we stop train-
ing when there is no improvement in ordering accu-
racy on the held-out dataset for five iterations, and
output the best scoring model.
Because of the constraints on transition probabil-
ities, straightforward application of EM leads to the
transition probabilities strongly skewing the learn-
ing of emission probabilities. We thus followed a
generalized EM procedure (Neal and Hinton, 1998),
updating only emission probabilities until no more
improvement is achieved, and then training both
emission and transition probabilities. Often, we238
WSJ Accuracy SWBD Accuracy Brown Accuracy
Training data Ngr 1-cl HMM MSA Ngr 1-cl HMM MSA Ngr 1-cl HMM MSA
WSJ manual 88.1 65.7 87.1 87.1 72.9 44.7 71.3 71.8 67.1 31.9 69.2 71.5
auto 87.8 64.6 86.7 87.2 72.5 41.6 71.5 71.9 67.4 31.3 69.4 70.6
NYT 10% 90.3 75.3 87.4 88.2 84.2 71.1 81.8 83.2 81.7 62.1 79.5 80.4
20% 91.8 77.2 87.9 89.3 85.2 72.2 80.9 83.1 82.2 65.9 78.9 82.1
50% 92.3 78.9 89.7 90.7 86.3 73.5 82.2 83.9 83.1 67.8 80.2 81.6
all 92.4 80.2 89.3 92.1 86.4 74.5 81.4 83.4 82.3 69.3 79.3 82.0
NYT+WSJ auto 93.7 81.1 89.7 92.2 86.3 74.5 81.3 83.4 82.3 69.3 79.3 81.8
Table 4: Results on WSJ sections 22-24, Switchboard test set, and Brown test set for n-gram model (Ngr), Mitchell?s
single-class system (1-cl), HMM and MSA systems, under various training conditions.
find no improvement with the inclusion of transition
probabilities, and they are left uniform. In this case,
test ordering is determined by the class label alone.
5 Empirical results
Several measures have been used to evaluate the
accuracy of a system?s modifier ordering, includ-
ing both type/token accuracy, pairwise accuracy, and
full string accuracy. We evaluate full string ordering
accuracy over all tokens in the evaluation set. For
every NP, if the model?s highest-scoring ordering is
identical to the actual observed order, it is correct;
otherwise, it is incorrect. We report the percentage
of orders correctly predicted.
We evaluate under a variety of training conditions,
on WSJ sections 22-24, as well as the testing sec-
tions from the Switchboard and Brown corpus por-
tions of the Penn Treebank. We perform no domain-
specific tuning, so the results on the Switchboard
and Brown corpora demonstrate cross-domain appli-
cability of the approaches.
5.1 Manual parses versus automatic parses
We begin by comparing the NPs extracted from
manual parses to those extracted from automatic
parses. We parsed Wall Street Journal sections 02
through 21 using cross-validation to ensure that the
parses are as errorful as when sentences have never
been observed by training.
Table 4 compares models trained on these two
training corpora, as evaluated on the manually-
annotated test set. No system?s accuracy degrades
greatly when using automatic parses, indicating that
we can likely derive useful training data by automat-
ically parsing a large, unlabeled training corpus.
5.2 Semi-supervised models
We now evaluate performance of the models on the
scaled up training data. Using the Berkeley parser,
we parsed 169 million words of NYT text from the
English Gigaword corpus (Graff and Cieri, 2003),
extracted the multiple modifier NPs, and trained our
various models on this data. Rows 3-6 of Table
4 show the accuracy on WSJ sections 22-24 after
training on 10%, 20%, 50% and 100% of this data.
Note that this represents approximately 150 times
the amount of training data as the original treebank
training data. Even with just 10% of this data (a
15-fold increase in the training data), we see across
the board improvements. Using all of the NYT data
results in approximately 5% absolute performance
increase for the n-gram and MSA models, yielding
roughly commensurate performance, over 92% ac-
curacy. Although we do not have space to present
the results in this paper, we found further improve-
ments (over 1% absolute, statistically significant) by
combining the four models, indicating a continued
benefit of the other models, even if none of them
best the n-gram individually.
Based on these results, this task is clearly
amenable to semi-supervised learning approaches.
All systems show large accuracy improvements.
Further, contrary to conventional wisdom, n-gram
models are very competitive with recent high-
accuracy frameworks. Additionally, n-gram models
appear to be domain sensitive, as evidenced by the
last row of Table 4, which presents results when the
1.8 million NPs in the NYT corpus are augmented
with just 11 thousand NPs from the WSJ (auto) col-
lection. The n-gram model still outperforms the
other systems, but improves by well over a percent,
while the class-based HMM and MSA approaches239
are relatively static. (The single-class system shows
some domain sensitivity, improving nearly a point.)
5.3 Cross-domain evaluation
With respect to cross-domain applicability, we see
that, as with the WSJ evaluation, the MSA and n-
gram approaches are roughly commensurate on the
Brown corpus; but the n-gram model shows a greater
advantage on the Switchboard test set when trained
on the NYT data. Perhaps this is due to higher re-
liance on conventionalized collocations in the spo-
ken language of Switchboard. Finally, it is clear
that the addition of the WSJ data to the NYT data
yields improvements only for the specific newswire
domain ? none of the results change much for these
two new domains when the WSJ data is included
(last row of the table).
We note that the improvements observed when
scaling the training corpus with in-domain data per-
sist when applied to very diverse domains. Interest-
ingly, n-gram models, which may have been consid-
ered unlikely to generalize well to other domains,
maintain their superior performance in each trial.
6 Discussion
In this paper, we demonstrated the efficacy of scal-
ing up training data for prenominal modifier or-
dering using automatic parses. We presented two
novel systems for ordering prenominal modifiers,
and demonstrated that with sufficient data, a simple
n-gram model outperforms position-specific models,
such as an EM-trained HMM and the MSA approach
of Dunlop et al (2010). The accuracy achieved by
the n-gram model is particularly interesting, since
such models have previously been considered inef-
fective for this task. This does not obviate the need
for a class based model ? modifier classes may in-
form linguistic research, and system combination
still yields large improvements ? but points to new
data-rich methods for learning such models.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
References
Jean Aitchison. 2003. Words in the mind: an intro-
duction to the mental lexicon. Blackwell Publishing,
Cornwall, United Kindgom, third edition. p. 7.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report, TR-10-98, Harvard University.
Joseph H. Danks and Sam Glucksberg. 1971. Psycho-
logical scaling of adjective order. Journal of Verbal
Learning and Verbal Behavior, 10(1):63?67.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society: Se-
ries B, 39(1):1?38.
Aaron Dunlop, Margaret Mitchell, and Brian Roark.
2010. Prenominal modier ordering via multiple se-
quence alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the ACL (HLT-NAACL 2010), pages 600?
608, Los Angeles, CA, USA. Association for Compu-
tational Linguistics.
David Graff and Christopher Cieri. 2003. English Giga-
word. Linguistic Data Consortium, Philadelphia, PA,
USA.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings of
the 38th ACL (ACL 2000), pages 85?92, Hong Kong.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
J. E. Martin. 1969. Semantic determinants of preferred
adjective order. Journal of Verbal Learning and Verbal
Behavior, 8(6):697?704.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 50?57, Athens, Greece. Associa-
tion for Computational Linguistics.
Margaret Mitchell. 2010. A flexible approach to class-
based ordering of prenominal modifiers. In E. Krah-
mer and M. Theune, editors, Empirical Methods in
Natural Language Generation, volume 5980 of Lec-
ture Notes in Computer Science. Springer, Berlin /
Heidelberg.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view
of the EM algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. Kluwer Academic Publish-
ers, Dordrecht.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American240
Chapter of the ACL (HLT-NAACL 2007), pages 404?
411, Rochester, NY, USA. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Berkeley parser. GNU General Pub-
lic License v.2.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the 37th
ACL (ACL 1999), pages 135?143, College Park, Mary-
land. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP
2002), volume 2, pages 901?904.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton, The Netherlands.
Benjamin Lee Whorf. 1945. Grammatical categories.
Language, 21(1):1?11.
241
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 676?681,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unary Constraints for Efficient Context-Free Parsing
Nathan Bodenstab? Kristy Hollingshead? and Brian Roark?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?University of Maryland Institute for Advanced Computer Studies, College Park, MD
{bodensta,roark}@cslu.ogi.edu hollingk@umiacs.umd.edu
Abstract
We present a novel pruning method for
context-free parsing that increases efficiency
by disallowing phrase-level unary productions
in CKY chart cells spanning a single word.
Our work is orthogonal to recent work on
?closing? chart cells, which has focused on
multi-word constituents, leaving span-1 chart
cells unpruned. We show that a simple dis-
criminative classifier can learn with high ac-
curacy which span-1 chart cells to close to
phrase-level unary productions. Eliminating
these unary productions from the search can
have a large impact on downstream process-
ing, depending on implementation details of
the search. We apply our method to four pars-
ing architectures and demonstrate how it is
complementary to the cell-closing paradigm,
as well as other pruning methods such as
coarse-to-fine, agenda, and beam-search prun-
ing.
1 Introduction
While there have been great advances in the statis-
tical modeling of hierarchical syntactic structure in
the past 15 years, exact inference with such models
remains very costly and most rich syntactic mod-
eling approaches resort to heavy pruning, pipelin-
ing, or both. Graph-based pruning methods such
as best-first and beam-search have both be used
within context-free parsers to increase their effi-
ciency. Pipeline systems make use of simpler mod-
els to reduce the search space of the full model. For
example, the well-known Charniak parser (Char-
niak, 2000) uses a simple grammar to prune the
search space for a richer model in a second pass.
Roark and Hollingshead (2008; 2009) have re-
cently shown that using a finite-state tagger to close
cells within the CKY chart can reduce the worst-case
and average-case complexity of context-free pars-
ing, without reducing accuracy. In their work, word
positions are classified as beginning and/or ending
multi-word constituents, and all chart cells not con-
forming to these constraints can be pruned. Zhang
et al (2010) and Bodenstab et al (2011) both ex-
tend this approach by classifying chart cells with a
finer granularity. Pruning based on constituent span
is straightforwardly applicable to all parsing archi-
tectures, yet the methods mentioned above only con-
sider spans of length two or greater. Lexical and
unary productions spanning a single word are never
pruned, and these can, in many cases, contribute sig-
nificantly to the parsing effort.
In this paper, we investigate complementary
methods to prune chart cells with finite-state pre-
processing. Informally, we use a tagger to re-
strict the number of unary productions with non-
terminals on the right-hand side that can be included
in cells spanning a single word. We term these sin-
gle word constituents (SWCs) (see Section 2 for a
formal definition). Disallowing SWCs alters span-1
cell population from potentially containing all non-
terminals to just pre-terminal part-of-speech (POS)
non-terminals. In practice, this decreases the num-
ber of active states in span-1 chart cells by 70%,
significantly reducing the number of allowable con-
stituents in larger spans. Span-1 chart cells are also
the most frequently queried cells in the CKY algo-
rithm. The search over possible midpoints will al-
ways include two cells spanning a single word ? one
as the first left child and one as the last right child. It
is therefore critical that the number of active states
676
(a) Original tree (b) Transformed tree (c) Dynamic programming chart
Figure 1: Example parse structure in (a) the original Penn treebank format and (b) after standard transformations have been
applied. The black cells in (c) indicate CKY chart cells containing a single-word constituent from the transformed tree.
in these cells be minimized so that the number of
grammar access requests is also minimized. Note,
however, that some methods of grammar access ?
such as scanning through the rules of a grammar and
looking for matches in the chart ? achieve less of a
speedup from diminished cell population than oth-
ers, something we investigate in this paper.
Importantly, our method is orthogonal to prior
work on tagging chart constraints and we expect ef-
ficiency gains to be additive. In what follows, we
will demonstrate that a finite-state tagger can learn,
with high accuracy, which span-1 chart cells can be
closed to SWCs, and how such pruning can increase
the efficiency of context-free parsing.
2 Grammar and Parsing Preliminaries
Given a probabilistic context-free grammar (PCFG)
defined as the tuple (V, T, S?, P, ?) where V is the
set of non-terminals, T is the set of terminals, S? is a
special start symbol, P is the set of grammar produc-
tions, and ? is a mapping of grammar productions to
probabilities, we divide the set of non-terminals V
into two disjoint subsets VPOS and VPHR such that
VPOS contains all pre-terminal part-of-speech tags
and VPHR contains all phrase-level non-terminals.
We define a single word constituent (SWC) unary
production as any production A?B ? P such that
A ? VPHR and A spans (derives) a single word. An
example SWC unary production, VP? VBP, can be
seen in Figure 1b. Note that ROOT ? SBAR and
RB ? ?quickly? in Figure 1b are also unary pro-
ductions, but by definition they are not SWC unary
productions.
One implementation detail necessary to leverage
the benefits of sparsely populated chart cells is the
grammar access method used by the inner loop of
the CKY algorithm.1 In bottom-up CKY parsing,
to extend derivations of adjacent substrings into new
constituents spanning the combined string, one can
either iterate over all binary productions in the gram-
mar and test if the new derivation is valid (gram-
mar loop), or one can take the cross-product of ac-
tive states in the cells spanning the substrings and
poll the grammar for possible derivations (cross-
product). With the cross-product approach, fewer
active states in either child cell leads to fewer gram-
mar access operations. Thus, pruning constituents
in lower cells directly affects the overall efficiency
of parsing. On the other hand, with the grammar
loop method there is a constant number of gram-
mar access operations (i.e., the number of grammar
rules) and the number of active states in each child
cell has no impact on efficiency. Therefore, with
the grammar loop implementation of the CYK algo-
rithm, pruning techniques such as unary constraints
will have very little impact on the final run-time effi-
ciency of the parser. We will report results in Section
5 with parsers using both approaches.
3 Treebank Unary Productions
In this section, we discuss the use of unary produc-
tions both in the Penn WSJ treebank (Marcus et al,
1999) and during parsing by analyzing their func-
tion and frequency. All statistics reported here are
computed from sections 2-21 of the treebank.
A common pre-processing step in treebank pars-
ing is to transform the original WSJ treebank be-
fore training and evaluation. There is some flex-
1Some familiarity with the CKY algorithm is assumed. For
details on the algorithm, see Roark and Sproat (2007).
677
Orig. Trans.
Empty nodes 48,895 0
Multi-Word Const. unaries 1,225 36,608
SWC unaries 98,467 105,973
Lexical unaries 950,028 950,028
Pct words with SWC unary 10.4% 11.2%
Table 1: Unary production counts from sections 2-21 of the
original and transformed WSJ treebank. All multisets are dis-
joint. Lexical unary count is identical to word count.
ibility in this process, but most pre-processing ef-
forts include (1) affixing a ROOT unary production
to the root symbol of the original tree, (2) removal
of empty nodes, and (3) striping functional tags and
cross-referencing annotations. See Figure 1 for an
example. Additional transforms include (4) remov-
ing X? X unary productions for all non-terminals
X, (5) collapsing unary chains to a single (possibly
composite) unary production (Klein and Manning,
2001), (6) introducing new categories such as AUX
(Charniak, 1997), and (7) collapsing of categories
such as PRT and ADVP (Collins, 1997). For this
paper we only apply transforms 1-3 and otherwise
leave the treebank in its original form. We also note
that ROOT unaries are a special case that do not af-
fect search, and we choose to ignore them for the
remainder of this paper.
These tree transformations have a large impact
on the number and type of unary productions in
the treebank. Table 1 displays the absolute counts
of unaries in the treebank before and after process-
ing. Multi-word constituent unary productions in the
original treebank are rare and used primarily to mark
quantifier phrases as noun phrases. But due to the
removal of empty nodes, the transformed treebank
contains many more unary productions that span
multiple words, such as S ? VP, where the noun
phrase was left unspecified in the original clause.
The number of SWC unaries is relatively un-
changed after processing the original treebank, but
note that only 11.2% of words in the transformed
treebank are covered by SWCs. This implies that
we are unnecessarily adding SWC productions to al-
most 90% of span-1 chart cells during search. One
may argue that an unsmoothed grammar will nat-
urally disallow most SWC productions since they
are never observed in the training data, for example
Mk2 Mk2+S Latent
|VPOS| 45 45 582
|VPHR| 26 26 275
SWC grammar rules 159 1,170 91,858
Active VPOS states 2.5 45 75
Active VPHR states 5.9 26 152
Table 2: Grammar statistics and averaged span-1 active state
counts for exhaustive parsing of section 24 using a Markov
order-2 (Mk2), a smoothed Markov order-2 (Mk2+S), and the
Berkeley latent variable (Latent) grammars.
VP ? DT. This is true to some extent, but gram-
mars induced from the WSJ treebank are notorious
for over-generation. In addition, state-of-the-art ac-
curacy in context-free parsing is often achieved by
smoothing the grammar, so that rewrites from any
one non-terminal to another are permissible, albeit
with low probability.
To empirically evaluate the impact of SWCs on
span-1 chart cells, we parse the development set
(section 24) with three different grammars induced
from sections 2-21. Table 2 lists averaged counts
of active Viterbi states (derivations with probabil-
ity greater than zero) from span-1 cells within the
dynamic programming chart, as well as relevant
grammar statistics. Note that these counts are ex-
tracted from exhaustive parsing ? no pruning has
been applied. We notice two points of interest.
First, although |VPOS| > |VPHR|, for the unsmoothed
grammars more phrase-level states are active within
the span-1 cells than states derived from POS tags.
When parsing with the Markov order-2 grammar,
70% of active states are non-terminals from VPHR,
and with the latent-variable grammar, 67% (152 of
227). This is due to the highly generative nature
of SWC productions. Second, although using a
smoothed grammar maximizes the number of active
states, the unsmoothed grammars still provide many
possible derivations per word.
Given the infrequent use of SWCs in the treebank,
and the search-space explosion incurred by includ-
ing them in exhaustive search, it is clear that restrict-
ing SWCs in contexts where they are unlikely to oc-
cur has the potential for large efficiency gains. In the
next section, we discuss how to learn such contexts
via a finite-state tagger.
678
4 Tagging Unary Constraints
To automatically predict if word wi from sentence
w can be spanned by an SWC production, we train a
binary classifier from supervised data using sections
2-21 of the Penn WSJ Treebank for training, section
00 as heldout, and section 24 as development. The
class labels of all words in the training data are ex-
tracted from the treebank, where wi ? U if wi is
observed with a SWC production and wi ? U other-
wise. We train a log linear model with the averaged
perceptron algorithm (Collins, 2002) using unigram
word and POS-tag2 features from a five word win-
dow. We also trained models with bi-gram and tri-
gram features, but tagging accuracy did not improve.
Because the classifier output is imposing hard
constraints on the search space of the parser, we
may want to choose a tagger operating point that fa-
vors precision over recall to avoid over-constraining
the downstream parser. To compare the tradeoff be-
tween possible precision/recall values, we apply the
softmax activation function to the perceptron output
to obtain the posterior probability of wi ? U :
P (U |wi, ?) = (1 + exp(?f(wi) ? ?))
?1 (1)
where ? is a vector of model parameters and f(?) is a
feature function. The threshold 0.5 simply chooses
the most likely class, but to increase precision we
can move this threshold to favor U over U . To tune
this value on a per-sentence basis, we follow meth-
ods similar to Roark & Hollingshead (2009) and
rank each word position with respect to its poste-
rior probability. If the total number of words wi
with P (U |wi, ?) < 0.5 is k, we decrease the thresh-
old value from 0.5 until ?k words have been moved
from class U to U , where ? is a tuning parameter be-
tween 0 and 1. Although the threshold 0.5 produces
tagging precision and recall of 98.7% and 99.4%
respectively, we can adjust ? to increase precision
as high as 99.7%, while recall drops to a tolerable
82.1%. Similar methods are used to replicate cell-
closing constraints, which are combined with unary
constraints in the next section.
2POS-tags were provided by a separately trained tagger.
5 Experiments and Results
To evaluate the effectiveness of unary constraints,
we apply our technique to four parsers: an exhaus-
tive CKY chart parser (Cocke and Schwartz, 1970);
the Charniak parser (Charniak, 2000), which uses
agenda-based two-level coarse-to-fine pruning; the
Berkeley parser (Petrov and Klein, 2007a), a multi-
level coarse-to-fine parser; and the BUBS parser
(Bodenstab et al, 2011), a single-pass beam-search
parser with a figure-of-merit constituent ranking
function. The Berkeley and BUBS parsers both
parse with the Berkeley latent-variable grammar
(Petrov and Klein, 2007b), while the Charniak
parser uses a lexicalized grammar, and the exhaus-
tive CKY algorithm is run with a simple Markov
order-2 grammar. All grammars are induced from
the same data: sections 2-21 of the WSJ treebank.
Figure 2 contrasts the merit of unary constraints
on the three high-accuracy parsers, and several inter-
esting comparisons emerge. First, as recall is traded
for precision within the tagger, each parser reacts
quite differently to the imposed constraints. We ap-
ply constraints to the Berkeley parser during the ini-
tial coarse-pass search, which is simply an exhaus-
tive CKY search with a coarse grammar. Applying
unary and cell-closing constraints at this point in the
coarse-to-fine pipeline speeds up the initial coarse-
pass significantly, which accounted for almost half
of the total parse time in the Berkeley parser. In ad-
dition, all subsequent fine-pass searches also bene-
fit from additional pruning as their search is guided
by the remaining constituents of the previous pass,
which is the intersection of standard coarse-to-fine
pruning and our imposed constraints.
We apply constraints to the Charniak parser dur-
ing the first-pass agenda-based search. Because an
agenda-based search operates at a constituent level
instead of a cell/span level, applying unary con-
straints alters the search frontier instead of reduc-
ing the absolute number of constituents placed in the
chart. We jointly tune lambda and the internal search
parameters of the Charniak parser until accuracy de-
grades.
Application of constraints to the CKY and BUBS
parsers is straightforward as they are both single
pass parsers ? any constituent violating the con-
straints is pruned. We also note that the CKY and
679
Figure 2: Development set results applying unary constraints
at multiple values of ? to three parsers.
BUBS parsers both employ the cross-product gram-
mar access method discussed in Section 2, while
the Berkeley parser uses the grammar loop method.
This grammar access difference dampens the benefit
of unary constraints for the Berkeley parser.3
Referring back to Figure 2, we see that both speed
and accuracy increase in all but the Berkeley parser.
Although it is unusual that pruning leads to higher
accuracy during search, it is not unexpected here as
our finite-state tagger makes use of lexical relation-
ships that the PCFG does not. By leveraging this
new information to constrain the search space, we
are indirectly improving the quality of the model.
Finally, there is an obvious operating point for
each parser at which the unary constraints are too
severe and accuracy deteriorates rapidly. For test
conditions, we set the tuning parameter ? based on
the development set results to prune as much of the
search space as possible before reaching this degra-
dation point.
Using lambda-values optimized for each parser,
we parse the unseen section 23 test data and present
results in Table 3. We see that in all cases, unary
constraints improve the efficiency of parsing without
significant accuracy loss. As one might expect, ex-
haustive CKY parsing benefits the most from unary
constraints since no other pruning is applied. But
even heavily pruned parsers using graph-based and
pipelining techniques still see substantial speedups
3The Berkeley parser does maintain meta-information about
where non-terminals have been placed in the chart, giving it
some of the advantages of cross-product grammar access.
Parser F-score Seconds Speedup
CKY 72.2 1,358
+ UC (?=0.2) 72.6 1,125 1.2x
+ CC 74.3 380 3.6x
+ CC + UC 74.6 249 5.5x
BUBS 88.4 586
+ UC (?=0.2) 88.5 486 1.2x
+ CC 88.7 349 1.7x
+ CC + UC 88.7 283 2.1x
Charniak 89.7 1,116
+ UC (?=0.2) 89.7 900 1.2x
+ CC 89.7 716 1.6x
+ CC + UC 89.6 679 1.6x
Berkeley 90.2 564
+ UC (?=0.4) 90.1 495 1.1x
+ CC 90.2 320 1.8x
+ CC + UC 90.2 289 2.0x
Table 3: Test set results applying unary constraints (UC) and
cell-closing (CC) constraints (Roark and Hollingshead, 2008)
to various parsers.
with the additional application of unary constraints.
Furthermore, unary constraints consistently provide
an additive efficiency gain when combined with cell-
closing constraints.
6 Conclusion
We have presented a new method to constrain
context-free chart parsing and have shown it to be or-
thogonal to many forms of graph-based and pipeline
pruning methods. In addition, our method parallels
the cell closing paradigm and is an elegant com-
plement to recent work, providing a finite-state tag-
ging framework to potentially constrain all areas of
the search space ? both multi-word and single-word
constituents.
Acknowledgments
We would like to thank Aaron Dunlop for his valu-
able discussions, as well as the anonymous review-
ers who gave very helpful feedback. This research
was supported in part by NSF Grants #IIS-0447214,
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
680
References
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Beam-width prediction for efficient
context-free parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, Portland, Oregon. Association for Com-
putational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Arti-
ficial Intelligence, pages 598?603, Menlo Park, CA.
AAAI Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139, Seattle, Washington.
Morgan Kaufmann Publishers Inc.
John Cocke and Jacob T. Schwartz. 1970. Programming
languages and their compilers. Technical report Pre-
liminary notes, Courant Institute of Mathematical Sci-
ences, NYU.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
eighth conference on European chapter of the Associ-
ation for Computational Linguistics, page 1623, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical Methods in
Natural Language Processing, volume 10, pages 1?
8, Philadelphia, July. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2001. Parsing
with treebank grammars: Empirical bounds, theoret-
ical models, and the structure of the Penn treebank.
In Proceedings of 39th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 338?345,
Toulouse, France, July. Association for Computational
Linguistics.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Slav Petrov and Dan Klein. 2007a. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2007b. Learning and in-
ference for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Donia Scott and Hans Uszkoreit, ed-
itors, Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING 2008),
pages 745?752, Manchester, UK, August. Association
for Computational Linguistics.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 647?655, Boulder, Colorado,
June. Association for Computational Linguistics.
Brian Roark and Richard W Sproat. 2007. Computa-
tional Approaches to Morphology and Syntax. Oxford
University Press, New York.
Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast lexicalised-grammar parsing. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 1472?1479, Beijing,
China, June.
681
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 38?43,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
An ERP-based Brain-Computer Interface for text entry
using Rapid Serial Visual Presentation and Language Modeling
K.E. Hild?, U. Orhan?, D. Erdogmus?, B. Roark?, B. Oken?, S. Purwar?, H. Nezamfar?, M. Fried-Oken?
?Oregon Health and Science University ?Cognitive Systems Lab, Northeastern University
{hildk,roarkb,oken,friedm}@ohsu.edu {orhan,erdogmus,purwar,nezamfar}@ece.neu.edu
Abstract
Event related potentials (ERP) corresponding
to stimuli in electroencephalography (EEG)
can be used to detect the intent of a per-
son for brain computer interfaces (BCI). This
paradigm is widely used to build letter-by-
letter text input systems using BCI. Neverthe-
less using a BCI-typewriter depending only on
EEG responses will not be sufficiently accu-
rate for single-trial operation in general, and
existing systems utilize many-trial schemes to
achieve accuracy at the cost of speed. Hence
incorporation of a language model based prior
or additional evidence is vital to improve accu-
racy and speed. In this demonstration we will
present a BCI system for typing that integrates
a stochastic language model with ERP classifi-
cation to achieve speedups, via the rapid serial
visual presentation (RSVP) paradigm.
1 Introduction
There exist a considerable number of people with se-
vere motor and speech disabilities. Brain computer
interfaces (BCI) are a potential technology to create
a novel communication environment for this popula-
tion, especially persons with completely paralyzed
voluntary muscles (Wolpaw, 2007; Pfurtscheller et
al., 2000). One possible application of BCI is typ-
ing systems; specifically, those BCI systems that
use electroencephalography (EEG) have been in-
creasingly studied in the recent decades to enable
the selection of letters for expressive language gen-
eration (Wolpaw, 2007; Pfurtscheller et al, 2000;
Treder and Blankertz, 2010). However, the use of
noninvasive techniques for letter-by-letter systems
lacks efficiency due to low signal to noise ratio and
variability of background brain activity. Therefore
current BCI-spellers suffer from low symbol rates
and researchers have turned to various hierarchi-
cal symbol trees to achieve system speedups (Serby
et al, 2005; Wolpaw et al, 2002; Treder and
Blankertz, 2010). Slow throughput greatly dimin-
ishes the practical usability of such systems. In-
corporation of a language model, which predicts
the next letter using the previous letters, into the
decision-making process can greatly affect the per-
formance of these systems by improving the accu-
racy and speed.
As opposed to the matrix layout of the popu-
lar P300-Speller (Wolpaw, 2007), shown in Fig-
ure 1, or the hexagonal two-level hierarchy of the
Berlin BCI (Treder and Blankertz, 2010), we uti-
lize another well-established paradigm: rapid se-
rial visual presentation (RSVP), shown in Figure
2. This paradigm relies on presenting one stimu-
lus at a time at the focal point of the screen. The
sequence of stimuli are presented at relatively high
speeds, each subsequent stimulus replacing the pre-
vious one, while the subject tries to perform men-
tal target matching between the intended symbol and
the presented stimuli. EEG responses corresponding
to the visual stimuli are classified using regularized
discriminant analysis (RDA) applied to stimulus-
locked temporal features from multiple channels.
The RSVP interface is of particular utility for the
most impaired users, including those suffering from
locked-in syndrome (LIS). Locked-in syndrome can
result from traumatic brain injury, such as a brain-
stem stroke1, or from neurodegenerative diseases
such as amyotrophic lateral sclerosis (ALS or Lou
Gehrig?s disease). The condition is characterized by
near total paralysis, though the individuals are cog-
nitively intact. While vision is retained, the motor
control impairments extend to eye movements. Of-
ten the only reliable movement that can be made by
1Brain stem stroke was the cause of LIS for Jean-Dominique
Bauby, who dictated his memoir The Diving Bell and the But-
terfly via eyeblinks (Bauby, 1997).
M
G
 
 A FEC
_9765
3 4Y 1Z
XWUTS
RQON
H
B
LKI
V
8
P
J
2
D
Figure 1: Spelling grid such as that used for the P300
speller (Farwell and Donchin, 1988). ? ? denotes space.
38
Figure 2: RSVP scanning interface.
an individual is a particular muscle twitch or single
eye blink, if that. Such users have lost the voluntary
motor control sufficient for such an interface. Rely-
ing on extensive visual scanning or complex gestu-
ral feedback from the user renders a typing interface
difficult or impossible to use for the most impaired
users. Simpler interactions via brain-computer in-
terfaces (BCI) hold much promise for effective text
communication for these most impaired users. Yet
these simple interfaces have yet to take full advan-
tage of language models to ease or speed typing.
In this demonstration, we will present a language-
model enabled interface that is appropriate for the
most impaired users.
In addition, the RSVP paradigm provides some
useful interface flexibility relative to the grid-based
paradigm. First, it allows for auditory rather than
visual scanning, for use by the visually impaired
or when visual access is inconvenient, such as in
face-to-face communication. Auditory scanning is
less straightforward when using a grid. Second,
multi-character substrings can be scanned in RSVP,
whereas the kind of dynamic re-organization of a
grid that would be required to support this can be
very confusing. Finally, language model integration
with RSVP is relatively straightforward, as we shall
demonstrate. See Roark et al (2010) for methods
integrating language modeling into grid scanning.
2 RSVP based BCI and ERP Classification
RSVP is an experimental psychophysics technique
in which visual stimulus sequences are displayed
on a screen over time on a fixed focal area and
in rapid succession. The Matrix-P300-Speller used
by Wadsworth and Graz groups (especially g.tec,
Austria) opts for a spatially distributed presentation
of possible symbols, highlighting them in different
orders and combinations to elicit P300 responses.
Berlin BCI?s recent variation utilizes a 2-layer tree
structure where the subject chooses among six units
(symbols or sets of these) where the options are laid
out on the screen while the subject focuses on a cen-
tral focal area that uses an RSVP-like paradigm to
elicit P300 responses. Full screen awareness is re-
quired. In contrast, our approach is to distribute
the stimuli temporally and present one symbol at a
time using RSVP and seek a binary response to find
the desired letter, as shown in Figure 2. The latter
method has the advantage of not requiring the user
to look at different areas of the screen, which can be
an important factor for those with LIS.
Our RSVP paradigm utilizes stimulus sequences
consisting of the 26 letters in the English alphabet
plus symbols for space and backspace, presented in
a randomly ordered sequence. When the user sees
the target symbol, the brain generates an evoked re-
sponse potential (ERP) in the EEG; the most promi-
nent component of this ERP is the P300 wave, which
is a positive deflection in the scalp voltage primar-
ily in frontal areas and that generally occurs with a
latency of approximately 300 ms. This natural nov-
elty response of the brain, occurring when the user
detects a rare, sought-after target, allows us to make
binary decisions about the user?s intent.
The intent detection problem becomes a signal
classification problem when the EEG signals are
windowed in a stimulus-time-locked manner start-
ing at stimulus onset and extending for a sufficient
duration ? in this case 500ms. Consider Figure
3, which shows the trial-averaged temporal signals
from various EEG channels corresponding to tar-
get and non-target (distractor) symbols. This graph
shows a clear effect between 300 and 500 ms for the
target symbols that is not present for the distractor
symbols (the latter of which clearly shows a com-
ponent having a periodicity of 400 ms, which is ex-
pected in this case since a new image was presented
every 400 ms). Figure 4, on the other hand, shows
the magnitude of the trial and distractor responses at
channel Cz on a single-trial basis, rather than aver-
aged over all trials. The signals acquired from each
EEG channel are incorporated and classified to de-
termine the class label: ERP or non-ERP.
Our system functions as follows. First, each chan-
nel is band-pass filtered. Second, each channel is
temporally-windowed. Third, a linear dimension
reduction (using principal components analysis) is
learned using training data and is subsequently ap-
plied to the EEG data when the system is being
used. Fourth, the data vectors obtained for each
channel and a given stimulus are concatenated to
create the data matrix corresponding to the speci-
fied stimulus. Fifth, Regularized Discriminant Anal-
ysis (RDA) (Friedman, 1989), which estimates con-
ditional probability densities for each class using
39
Figure 3: Trial-averaged EEG data corresponding to the target
response (top) and distractor response (bottom) for a 1 second
window.
Kernel Density Estimation (KDE), is used to deter-
mine a purely EEG-based classification discriminant
score for each stimulus. Sixth, the conditional prob-
ability of each letter given the typed history is ob-
tained from the language model. Seventh, Bayesian
fusion (which assumes the EEG-based information
and the language model information are statistically
independent given the class label) is used to combine
the RDA discriminant score and the language model
score to generate an overall score, from which we
infer whether or not a given stimulus represents an
intended (target) letter.
RDA is a modified quadratic discriminant anal-
ysis (QDA) model. Assuming each class has a
multivariate normal distribution and assuming clas-
sification is made according to the comparison of
posterior distributions of the classes, the optimal
Bayes classifier resides within the QDA model fam-
ily. QDA depends on the inverse of the class co-
variance matrices, which are to be estimated from
training data. Hence, for small sample sizes and
high-dimensional data, singularities of these matri-
ces are problematic. RDA applies regularization and
shrinkage procedures to the class covariance matrix
Figure 4: Single-trial EEG data at channel Cz corresponding
to the target response (top) and distractor response (bottom) for
a 1 second window.
estimates in an attempt to minimize problems asso-
ciated with singularities. The shrinkage procedure
makes the class covariances closer to the overall data
covariance, and therefore to each other, thus mak-
ing the quadratic boundary more similar to a linear
boundary. Shrinkage is applied as
??c(?) = (1? ?)??c + ???, (1)
where ? is the shrinkage parameter, ??c is the class
covariance matrix estimated for class c ? {0, 1},
c = 0 corresponds to the non-target class, c = 1 cor-
responds to the target class, and ?? is the weighted
average of class covariance matrices. Regularization
is administered as
??c(?, ?) = (1? ?)??c(?) +
?
d
tr[??c(?)]I, (2)
where ? is the regularization parameter, tr[?] is the
trace function, and d is the dimension of the data
vector.
After carrying out the regularization and shrink-
age on the estimated covariance matrices, the
Bayesian classification rule (Duda et al, 2001) is
applied by comparing the log-likelihood ratio (using
40
Figure 5: Timing of stimulus sequence presentation
the posterior probability distributions) with a confi-
dence threshold. The confidence threshold can be
chosen so that the system incorporates the relative
risks or costs of making an error for each class. The
corresponding log-likelihood ratio is given by
?RDA(x) = log
fN (x; ??1, ??1(?, ?))p?i1
fN (x; ??0, ??0(?, ?))p?i0
, (3)
where ?c and p?ic are the estimates of the class means
and priors, respectively, x is the data vector to be
classified, and fN (x;?,?) is the pdf of a multivari-
ate normal distribution.
The set of visual stimuli (letters plus two ex-
tra symbols, in our case) can be shown multiple
times to achieve a higher classification accuracy for
the EEG-based classifier. The information obtained
from showing the visual stimuli multiple times can
easily be combined by assuming the trials are sta-
tistically independent, as is commonly assumed in
EEG-based spellers2. Figure 5 presents a diagram of
the timing of the presentation of stimuli. We define
a sequence to be a randomly-ordered set of all the
letters (and the space and backspace symbols). The
letters are randomly ordered for each sequence be-
cause the magnitude of the ERP, hence the quality of
the EEG-based classification, is commonly thought
to depend on how surprised the user is to find the
intended letter. Our system also has a user-defined
parameter by which we are able to limit the max-
imum number of sequences shown to the user be-
fore our system makes a decision on the (single) in-
tended letter. Thus we are able to operate in single-
trial or multi-trial mode. We use the term epoch to
denote all the sequences that are used by our sys-
tem to make a decision on a single, intended let-
2The typical number of repetitions of visual stimuli is on the
order of 8 or 16, although g.tec claims one subject is able to
achieve reliable operation with 2 trials (verbal communication).
ter. As can be seen in the timing diagram shown
in Figure 5, epoch k contains between 1 and Mk
sequences. This figure shows the onset of each se-
quence, each fixation image (which is shown at the
beginning of each sequence), and each letter using
narrow pulses. After each sequence is shown, the
cumulative (overall) score for all letters is computed.
The cumulative scores are non-negative and sum to
one (summing over the 28 symbols). If the num-
ber of sequences shown is less than the user-defined
limit and if the maximum cumulative score is less
than 0.9, then another randomly-ordered sequence is
shown to the user. Likewise, if either the maximum
number of sequences has already been shown or if
the maximum cumulative score equals or exceeds
0.9, then the associated symbol (for all symbols ex-
cept the backspace) is added to the end of the list
of previously-detected symbols, the user is able to
take a break of indefinite length, and then the system
continues with the next epoch. If the symbol hav-
ing the maximum cumulative score is the backspace
symbol, then the last item in the list of previously-
detected symbols is removed and, like before, the
user can take a break and then the system continues
with the next epoch.
3 Language Modeling
Language modeling is important for many text pro-
cessing applications, e.g., speech recognition or ma-
chine translation, as well as for the kind of typ-
ing application being investigated here (Roark et al,
2010). Typically, the prefix string (what has al-
ready been typed) is used to predict the next sym-
bol(s) to be typed. The next letters to be typed be-
come highly predictable in certain contexts, partic-
ularly word-internally. In applications where text
generation/typing speed is very slow, the impact
of language modeling can become much more sig-
nificant. BCI-spellers, including the RSVP Key-
board paradigm presented here, can be extremely
low-speed, letter-by-letter writing systems, and thus
can greatly benefit from the incorporation of proba-
bilistic letter predictions from an accurate language
model.
For the current study, all language models were
estimated from a one million sentence (210M char-
acter) sample of the NY Times portion of the English
Gigaword corpus. Models were character n-grams,
estimated via relative frequency estimation. Corpus
normalization and smoothing methods were as de-
scribed in Roark et al (2010). Most importantly for
41
Figure 6: Block diagram of system architecture.
this work, the corpus was case normalized, and we
used Witten-Bell smoothing for regularization.
4 System Architecture
Figure 6 shows a block diagram of our system. We
use a Quad-core, 2.53 GHz laptop, with system code
written in Labview, Matlab, and C. We also use
the Psychophysics Toolbox3 to preload the images
into the video card and to display the images at
precisely-defined temporal intervals. The type UB
g.USBamp EEG-signal amplifier, which is manufac-
tured by g.tec (Austria), has 24 bits of precision and
has 16 channels. We use a Butterworth bandpass fil-
ter of 0.5 to 60 Hz, a 60 Hz notch filter, a sampling
rate of 256 Hz, and we buffer the EEG data until we
have 8 samples of 16-channel EEG data, at which
point the data are transmitted to the laptop. We
use either g.BUTTERfly or g.LADYbird active elec-
trodes, a g.GAMMA cap, and the g.GAMMAsys ac-
tive electrode system.
The output of the amplifier is fed to the laptop via
a USB connection with a delay that is both highly
variable and unknown a priori. Consequently, we
are unable to rely on the laptop system clock in or-
der to synchronize the EEG data and the onset of
the visual stimuli. Instead, synchronization between
the EEG data and the visual stimuli is provided by
sending a parallel port trigger, via an express card-
to-parallel port adaptor, to one of the digital inputs
of the amplifier, which is then digitized along with
the EEG data. The parallel port to g.tec cable was
custom-built by Cortech Solutions, Inc. (Wilming-
ton, North Carolina, USA). The parallel port trigger
is sent immediately after the laptop monitor sends
the vertical retrace signal. The mean and the stan-
3http://psychtoolbox.org/wikka.php?wakka=HomePage
dard deviation of the delay needed to trigger the par-
allel port has been measured to be on the order of
tens of microseconds, which should be sufficiently
small for our purposes.
5 Results
Here we report data collected from 2 subjects, one
of whom is a LIS subject with very limited experi-
ence using our BCI system, and the other a healthy
subject with extensive experience using our BCI sys-
tem. The symbol duration was set to 400 ms, the
duty cycle was set to 50%, and the maximum num-
ber of sequences per trial was set to 6. Before test-
ing, the classifier of our system was trained on data
obtained as each subject viewed 50 symbols with 3
sequences per epoch (the classifier was trained once
for the LIS subject and once for the healthy sub-
ject). The healthy subject was specifically instructed
to neither move nor blink their eyes, to the extent
possible, while the symbols are being flashed on the
screen in front of them. Instead, they were to wait
until the rest period, which occurs after each epoch,
to move or to blink. The subjects were free to pro-
duce whatever text they wished. The only require-
ment given to them concerning the chosen text was
that they must not, at any point in the experiment,
change what they are planning to type and they must
correct all mistakes using the backspace symbol.
Figure 7 shows the results for the non-expert,
LIS subject. A total of 10 symbols were correctly
typed by this subject, who had chosen to spell,
?THE STEELERS ARE GOING TO ...?. Notice
that the number of sequences shown exceeds the
maximum value of 6 for 3 of the symbols. This
occurs when the specified letter is mistyped one or
more times. For example, for each mistyped non-
backspace symbol, a backspace is required to delete
42
T H E _ S T E E L E0 
5 
10
15
20
25
30
35
40
45
No. 
of se
quen
ces 
to re
ach 
conf
iden
ce th
resh
old
Mean = 144/10 = 14.4 (seq/desired symbol)Mean = 5.1 (seq/symbol)                  
Figure 7: Number of sequences to reach the confidence thresh-
old for the non-expert, LIS subject.
T H E _ L A K E R S _ A R E _ I N _ F I0 
5 
10
15
20
25
30
35
40
45
No. 
of se
quen
ces 
to re
ach 
conf
iden
ce th
resh
old
Mean = 28/20 = 1.4 (seq/desired symbol)Mean = 1.4 (seq/symbol)                
Figure 8: Number of sequences to reach the confidence thresh-
old for the expert, healthy subject.
the incorrect symbol. Likewise, if a backspace sym-
bol is detected although it was not the symbol that
the subject wished to type, then the correct symbol
must be retyped. As shown in the figure, the mean
number of sequences for each correctly-typed sym-
bol is 14.4 and the mean number of sequences per
symbol is 5.1 (the latter of which has a maximum
value of 6 in this case).
Figure 8 shows the result for the expert, healthy
subject. A total of 20 symbols were cor-
rectly typed by this subject, who had chosen to
spell, ?THE LAKERS ARE IN FIRST PLACE?.
The mean number of sequences for each correctly-
typed symbol for this subject is 1.4 and the mean
number of sequences per symbol is also 1.4. Notice
that in 15 out of 20 epochs the classifier was able to
detect the intended symbol on the first epoch, which
corresponds to a single-trial presentation of the sym-
bols, and no mistakes were made for any of the 20
symbols.
There are two obvious explanations as to why the
healthy subject performed better than the LIS sub-
ject. First, it is possible that the healthy subject was
using a non-neural signal, perhaps an electromyo-
graphic (EMG) signal stemming from an unintended
muscle movement occurring synchronously with the
target onset. Second, it is also possible that the LIS
subject needs more training in order to learn how
to control the system. We believe the second ex-
planation is correct and are currently taking steps
to make sure the LIS subject has additional time to
train on our system in hopes of resolving this ques-
tion quickly.
Acknowledgments
This work is supported by NSF under grants
ECCS0929576, ECCS0934506, IIS0934509,
IIS0914808, BCS1027724 and by NIH under grant
1R01DC009834-01. The opinions presented here
are those of the authors and do not necessarily
reflect the opinions of the funding agencies.
References
J.-D. Bauby. 1997. The Diving Bell and the Butterfly.
Knopf, New York.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
classification. Citeseer.
L.A. Farwell and E. Donchin. 1988. Talking off the
top of your head: toward a mental prosthesis utiliz-
ing event-related brain potentials. Electroenceph Clin.
Neurophysiol., 70:510?523.
J.H. Friedman. 1989. Regularized discriminant analy-
sis. Journal of the American statistical association,
84(405):165?175.
G. Pfurtscheller, C. Neuper, C. Guger, W. Harkam,
H. Ramoser, A. Schlogl, B. Obermaier, and M. Pre-
genzer. 2000. Current trends in Graz brain-computer
interface (BCI) research. IEEE Transactions on Reha-
bilitation Engineering, 8(2):216?219.
B. Roark, J. de Villiers, C. Gibbons, and M. Fried-Oken.
2010. Scanning methods and language modeling for
binary switch typing. In Proceedings of the NAACL
HLT 2010 Workshop on Speech and Language Pro-
cessing for Assistive Technologies, pages 28?36.
H. Serby, E. Yom-Tov, and G.F. Inbar. 2005. An im-
proved P300-based brain-computer interface. Neural
Systems and Rehabilitation Engineering, IEEE Trans-
actions on, 13(1):89?98.
M.S. Treder and B. Blankertz. 2010. (C) overt atten-
tion and visual speller design in an ERP-based brain-
computer interface. Behavioral and Brain Functions,
6(1):28.
J.R. Wolpaw, N. Birbaumer, D.J. McFarland,
G. Pfurtscheller, and T.M. Vaughan. 2002. Brain-
computer interfaces for communication and control.
Clinical neurophysiology, 113(6):767?791.
J.R. Wolpaw. 2007. Brain?computer interfaces as new
brain output pathways. The Journal of Physiology,
579(3):613.
43
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 61?66,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
The OpenGrm open-source finite-state grammar software libraries
Brian Roark? Richard Sproat?? Cyril Allauzen? Michael Riley? Jeffrey Sorensen? & Terry Tai?
?Oregon Health & Science University, Portland, Oregon ?Google, Inc., New York
Abstract
In this paper, we present a new collection
of open-source software libraries that pro-
vides command line binary utilities and library
classes and functions for compiling regular
expression and context-sensitive rewrite rules
into finite-state transducers, and for n-gram
language modeling. The OpenGrm libraries
use the OpenFst library to provide an efficient
encoding of grammars and general algorithms
for building, modifying and applying models.
1 Introduction
The OpenGrm libraries1 are a (growing) collec-
tion of open-source software libraries for build-
ing and applying various kinds of formal gram-
mars. The C++ libraries use the OpenFst library2
for the underlying finite-state representation, which
allows for easy inspection of the resulting grammars
and models, as well as straightforward combination
with other finite-state transducers. Like OpenFst,
there are easy-to-use command line binaries for fre-
quently used operations, as well as a C++ library
interface, allowing library users to create their own
algorithms from the basic classes and functions pro-
vided.
The libraries can be used for a range of com-
mon string processing tasks, such as text normal-
ization, as well as for building and using large sta-
tistical models for applications like speech recogni-
tion. In the rest of the paper, we will present each of
the two libraries, starting with the Thrax grammar
compiler and then the NGram library. First, though,
we will briefly present some preliminary (infor-
mal) background on weighted finite-state transduc-
ers (WFST), just as needed for this paper.
1http://www.opengrm.org/
2http://www.openfst.org/
2 Informal WFST preliminaries
A weighted finite-state transducer consists of a set
of states and transitions between states. There is an
initial state and a subset of states are final. Each tran-
sition is labeled with an input symbol from an input
alphabet; an output symbol from an output alpha-
bet; an origin state; a destination state; and a weight.
Each final state has an associated final weight. A
path in the WFST is a sequence of transitions where
each transition?s destination state is the next transi-
tion?s origin state. A valid path through the WFST is
a path where the origin state of the first transition is
an initial state, and the the last transition is to a final
state. Weights combine along the path according to
the semiring of the WFST.
If every transition in the transducer has the same
input and output symbol, then the WFST represents
a weighted finite-state automaton. In the OpenFst
library, there are a small number of special sym-
bols that can be used. The  symbol represents the
empty string, which allows the transition to be tra-
versed without consuming any symbol. The ? (or
failure) symbol on a transition also allows it to be
traversed without consuming any symbol, but it dif-
fers from  in only allowing traversal if the symbol
being matched does not label any other transition
leaving the same state, i.e., it encodes the semantics
of otherwise, which is useful for language models.
For a more detailed presentation of WFSTs, see Al-
lauzen et al (2007).
3 The Thrax Grammar Compiler
The Thrax grammar compiler3 compiles grammars
that consist of regular expressions, and context-
dependent rewrite rules, into FST archives (fars) of
weighted finite state transducers. Grammars may
3The compiler is named after Dionysius Thrax (170?
90BCE), the reputed first Greek grammarian.
61
be split over multiple files and imported into other
grammars. Strings in the rules may be parsed
in one of three different ways: as a sequence of
bytes (the default), as utf8 encodings, or accord-
ing to a user-provided symbol table. With the
--save symbols flag, the transducers can be
saved out into fars with appropriate symbol tables.
The Thrax libraries provide full support for dif-
ferent weight (semiring) classes. The command-line
flag --semiring allows one to set the semiring,
currently to one of: tropical (default), log or log64
semirings.
3.1 General Description
Thrax revolves around rules which, typically, con-
struct an FST based on a given input. In the simplest
case, we can just provide a string that represents a
(trivial) transducer and name it using the assignment
operator:
pear = "pear";
In this example, we have an FST consisting of the
characters ?p?, ?e?, ?a?, and ?r? in a chain, assigned
to the identifier pear:
This identifier can be used later in order to build
further FSTs, using built-in operators or using cus-
tom functions:
kiwi = "kiwi";
fruits = pear | kiwi; # union
In Thrax, string FSTs are enclosed by double-quotes
(") whereas simple strings (often used as pathnames
for functions) are enclosed in single-quotes (?).
Thrax provides a set of built-in functions that
aid in the construction of more complex expres-
sions. We have already seen the disjunction ?|? in
the previous example. Other standard regular op-
erations are expr*, expr+, expr? and expr{m,n},
the latter repeating expr between m and n times,
inclusive. Composition is notated with ?@? so
that expr1 @ expr2 denotes the composition of
expr1 and expr2. Rewriting is denoted with ?:?
where expr1 : expr2 rewrites strings that match
expr1 into expr2. Weights can be added to expres-
sions using the notation ?<>?: thus, expr<2.4>
adds weight 2.4 to expr. Various operations on
FSTs are also provided by built-in functions, includ-
ing Determinize, Minimize, Optimize and
Invert, among many others.
3.2 Detailed Description
A Thrax grammar consists of a set of one or more
source files, each of which must have the extension
.grm. The compiler compiles each source file to a
single FST archive with the extension .far. Each
grammar file has sections: Imports and Body, each
of which is optional. The body section can include
statements interleaved with functions, as specified
below. Comments begin with a single pound sign
(#) and last until the next newline.
3.2.1 Imports
The Thrax compiler compiles source files (with
the extension .grm) into FST archive files (with
the extension .far). FST archives are an Open-
Fst storage format for a series of one or more FSTs.
The FST archive and the original source file then
form a pair which can be imported into other source
files, allowing a Python-esque include system that is
hopefully familiar to many. Instead of working with
a monolithic file, Thrax allows for a modular con-
struction of the final rule set as well as sharing of
common elements across projects.
3.2.2 Functions
Thrax has extensive support for functions that can
greatly augment the capabilities of the language.
Functions in Thrax can be specified in two ways.
The first is inline via the func keyword within grm
files. These functions can take any number of input
arguments and must return a single result (usually an
FST) to the caller via the return keyword:
func DumbPluralize[fst] {
# Concatenate with "s"...
result = fst "s";
# ...and then return to caller.
return result;
}
Alternatively, functions can be written C++ and
added to the language. Regardless of the func-
tion implementation method (inline in Thrax or
subclassed in C++), functions are integrated into
the Thrax environment and can be called directly
by using the function name and providing the
necessary arguments. Thus, assuming someone has
written a function called NetworkPluralize
that retrieves the plural of a word from some web-
site, one could write a grammar fragment as follows:
62
apple = "apple";
plural_apple = DumbPluralize[apple];
plural_tomato = NetworkPluralize[
"tomato",
?http://server:port/...?];
3.2.3 Statements
Functions can be interleaved with grammar state-
ments that generate the FSTs that are exported to the
FST archive as output. Each statement consists of an
assignment terminating with a semicolon:
foo = "abc";
export bar = foo | "xyz";
Statements preceded with the export keyword will
be written to the final output archive. Statements
lacking this keyword define temporaries that be used
later, but are themselves not output.
The basic elements of any grammar are string
FSTs, which, as mentioned earlier, are defined by
text enclosed by double quotes ("), in contrast to
raw strings, which are enclosed by single quotes (?).
String FSTs can be parsed in one of three ways,
which are denoted using a dot (.) followed by ei-
ther byte, utf8, or an identifier holding a symbol ta-
ble. Note that within strings, the backslash character
(\) is used to escape the next character. Of partic-
ular note, ?\n? translates into a newline, ?\r? into
a line feed, and ?\t? into the tab character. Literal
left and right square brackets also need escaping, as
they are used to generate symbols (see below). All
other characters following the backslash are unin-
terpreted, so that we can use \? and \? to insert an
actual quote (double) quote symbol instead of termi-
nating the string.
Strings, by default, are interpreted as sequences
of bytes, each transition of the resulting FST
corresponding to a single 1-byte character of the
input. This can be specified either by leaving off the
parse mode ("abc") or by explicitly using the byte
mode ("abc".byte). The second way is to use
UTF8 parsing by using the special keyword, e.g.:
Finally, we can load a symbol table and split
the string using the fst field separator flag
(found in fst/src/lib/symbol-table.cc)
and then perform symbol table lookups. Symbol ta-
bles can be loaded using the SymbolTable built-in
function:
arctic_symbol_table =
SymbolTable[?/path/to/bears.symtab?];
pb = "polar bear".arctic_symbol_table;
One can also create temporary symbols on the
fly by enclosing a symbol name inside brackets
within an FST string. All of the text inside the
brackets will be taken to be part of the symbol
name, and future encounters of the same symbol
name will map to the same label. By default, la-
bels use ?Private Use Area B? of the unicode ta-
ble (0x100000 - 0x10FFFD), except that the last two
code points 0x10FFFC and 0x10FFFD are reserved
for the ?[BOS]? and ?[EOS]? tags discussed below.
cross_pos = "cross" ("" : "_[s_noun]");
pluralize_nouns = "_[s_noun]" : "es";
3.3 Standard Library Functions and
Operations
Built-in functions are provided that operate on FSTs
and perform most of the operations that are available
in the OpenFst library. These include: closure, con-
catenation, difference, composition and union. In
most cases the notation of these functions follows
standard conventions. Thus, for example, for clo-
sure, the following syntax applies: fst* (accepts fst
0 or more times); fst+ (accepts fst 1 or more times);
fst? (accepts fst 0 or 1 times) fst{x,y} (accepts fst at
least x but no more than y times).
The operator ?@? is used for composition: a @
b denotes a composed with b. A ?:? is used to de-
note rewrite, where a : b denotes a transducer
that deletes a and inserts b. Most functions can also
be expressed using functional notation:
b = Rewrite["abc", "def"];
The delimiters< and> add a weight to an expres-
sion in the chosen semiring: a<3> adds the weight
3 (in the tropical semiring by default) to a.
Functions lacking operators (hence only called
by function name) include: ArcSort, Connect,
Determinize, RmEpsilon, Minimize,
Optimize, Invert, Project and Reverse.
Most of these call the obvious underlying OpenFst
function.
One function in particular, CDRewrite is worth
further discussion. This function takes a transducer
and two context acceptors (and the alphabet ma-
chine), and generates a new FST that performs a
context dependent rewrite everywhere in the pro-
vided contexts. The context-dependent rewrite algo-
rithm used is that of Mohri and Sproat (1996), and
63
see also Kaplan and Kay (1994). The fourth argu-
ment (sigma star) needs to be a minimized ma-
chine. The fifth argument selects the direction of
rewrite; we can either rewrite left-to-right or right-
to-left or simultaneously. The sixth argument selects
whether the rewrite is optional.
CDRewrite[tau, lambda, rho,
sigma_star,
?ltr?|?rtl?|?sim?,
?obl?|?opt?]
For context-dependent rewrite rules, two built-in
symbols ?[BOS]? and ?[EOS]? have a special mean-
ing in the context specifications: they refer to the
beginning and end of string, respectively.
There are also built-in functions that perform
other tasks. In the interest of space we concentrate
here on the StringFile function, which loads a
file consisting of a list of strings, or tab-separated
pairs of strings, and compiles them to an acceptor
that represents the union of the strings.
StringFile[?strings_file?]
While it is equivalent to the union of the individual
string (pairs), StringFile uses an efficient algo-
rithm for constructing a prefix tree (trie) from the
list and can be significantly more efficient than com-
puting a union for large lists. If a line consists of a
tab-separated pair of strings a, b, a transducer equiv-
alent to Rewrite[a, b] is compiled.
The optional keywords byte (default), utf8 or
the name of a symbol table can be used to specify
the parsing mode for the strings. Thus
StringFile[?strings_file?, utf8, my_symtab]
would parse a sequence of tab-separated pairs, using
utf8 parsing for the left-hand string, and the symbol
table my symtab for the right-hand string.
4 NGram Library
The OpenGrm NGram library contains tools for
building, manipulating and using n-gram language
models represented as weighted finite-state trans-
ducers. The same finite-state topology is used to en-
code raw counts as well as smoothed models. Here
we briefly present this structure, followed by details
on the operations manipulating it.
An n-gram is a sequence of n symbols: w1 . . . wn.
Each state in the model represents a prefix history
of the n-gram (w1 . . . wn?1), and transitions in the
model represent either n-grams or backoff transi-
tions following that history. Figure 1 lists conven-
tions for states and transitions used to encode the
n-grams as a WFST.
This representation is similar to that used in other
WFST-based n-gram software libraries, such as the
AT&T GRM library (Allauzen et al, 2005). One
key difference is the implicit representation of <s>
and </s>, as opposed to encoding them as symbols
in the grammar. This has the benefit of including all
start and stop symbol functionality while avoiding
common pitfalls that arise with explicit symbols.
Another difference from the GRM library repre-
sentation is explicit inclusion of failure links from
states to their backoff states even in the raw count
files. The OpenGrm n-gram FST format is consis-
tent through all stages of building the models, mean-
ing that model manipulation (e.g., merging of two
Figure 1: List of state and transition conventions used to encode collection of n-grams in WFST.
An n-gram is a sequence of n symbols: w1 . . . wn. Its proper prefixes include all sequences w1 . . . wk for k < n.
? There is a unigram state in every model, representing the empty string.
? Every proper prefix of every n-gram in the model has an associated state in the model.
? The state associated with an n-gram w1...wn has a backoff transition (labeled with ) to the state associated
with its suffix w2...wn.
? An n-gram w1...wn is represented as a transition, labeled with wn, from the state associated with its prefix
w1...wn?1 to a destination state defined as follows:
? If w1...wn is a proper prefix of an n-gram in the model, then the destination of the transition is the state
associated with w1...wn
? Otherwise, the destination of the transition is the state associated with the suffix w2...wn.
? Start and end of the sequence are not represented via transitions in the automaton or symbols in the symbol
table. Rather
? The start state of the automaton encodes the ?start of sequence? n-gram prefix (commonly denoted<s>).
? The end of the sequence (often denoted </s>) is included in the model through state final weights, i.e.,
for a state associated with an n-gram prefix w1...wn, the final weight of that state represents the weight
of the n-gram w1...wn</s>.
64
(a)
?
?
?
a/0
a/-1.1
b/-1.1
b/0
b/-0.69
a/-0.69
0
0
(b)
?/0.69
?/0.916
a/0.6
a/0.Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?52,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Smoothed marginal distribution constraints for language modeling
Brian Roark??, Cyril Allauzen? and Michael Riley?
?Oregon Health & Science University, Portland, Oregon ?Google, Inc., New York
roarkbr@gmail.com, {allauzen,riley}@google.com
Abstract
We present an algorithm for re-estimating
parameters of backoff n-gram language
models so as to preserve given marginal
distributions, along the lines of well-
known Kneser-Ney (1995) smoothing.
Unlike Kneser-Ney, our approach is de-
signed to be applied to any given smoothed
backoff model, including models that have
already been heavily pruned. As a result,
the algorithm avoids issues observed when
pruning Kneser-Ney models (Siivola et al,
2007; Chelba et al, 2010), while retain-
ing the benefits of such marginal distribu-
tion constraints. We present experimen-
tal results for heavily pruned backoff n-
gram models, and demonstrate perplexity
and word error rate reductions when used
with various baseline smoothing methods.
An open-source version of the algorithm
has been released as part of the OpenGrm
ngram library.1
1 Introduction
Smoothed n-gram language models are the de-
facto standard statistical models of language for
a wide range of natural language applications, in-
cluding speech recognition and machine transla-
tion. Such models are trained on large text cor-
pora, by counting the frequency of n-gram col-
locations, then normalizing and smoothing (reg-
ularizing) the resulting multinomial distributions.
Standard techniques store the observed n-grams
and derive probabilities of unobserved n-grams via
their longest observed suffix and ?backoff? costs
associated with the prefix histories of the unob-
served suffixes. Hence the size of the model grows
with the number of observed n-grams, which is
very large for typical training corpora.
1www.opengrm.org
Natural language applications, however, are
commonly used in scenarios requiring relatively
small footprint models. For example, applica-
tions running on mobile devices or in low latency
streaming scenarios may be required to limit the
complexity of models and algorithms to achieve
the desired operating profile. As a result, statisti-
cal language models ? an important component of
many such applications ? are often trained on very
large corpora, then modified to fit within some
pre-specified size bound. One method to achieve
significant space reduction is through random-
ized data structures, such as Bloom (Talbot and
Osborne, 2007) or Bloomier (Talbot and Brants,
2008) filters. These data structures permit effi-
cient querying for specific n-grams in a model
that has been stored in a fraction of the space
required to store the full, exact model, though
with some probability of false positives. Another
common approach ? which we pursue in this pa-
per ? is model pruning, whereby some number of
the n-grams are removed from explicit storage in
the model, so that their probability must be as-
signed via backoff smoothing. One simple prun-
ing method is count thresholding, i.e., discarding
n-grams that occur less than k times in the corpus.
Beyond count thresholding, the most widely used
pruning methods (Seymore and Rosenfeld, 1996;
Stolcke, 1998) employ greedy algorithms to re-
duce the number of stored n-grams by comparing
the stored probabilities to those that would be as-
signed via the backoff smoothing mechanism, and
removing those with the least impact according to
some criterion.
While these greedy pruning methods are highly
effective for models estimated with most com-
mon smoothing approaches, they have been shown
to be far less effective with Kneser-Ney trained
language models (Siivola et al, 2007; Chelba et
al., 2010), leading to severe degradation in model
quality relative to other standard smoothing meth-
43
4-gram models Backoff Interpolated
Perplexity n-grams Perplexity n-grams
Smoothing method full pruned (?1000) full pruned (?1000)
Absolute Discounting (Ney et al, 1994) 120.5 197.3 383.4 119.8 198.1 386.2
Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4
Ristad (1995) 126.4 203.6 395.6 ??- N/A ??-
Katz (1987) 119.8 198.1 386.2 ??- N/A ??-
Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7
Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1
Table 1: Reformatted version of Table 3 in Chelba et al (2010), demonstrating perplexity degradation of Kneser-Ney
smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training;
692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to
approximately 1.3% of the original size of 31,095,260 n-grams.
ods. Thus, while Kneser-Ney may be the preferred
smoothing method for large, unpruned models
? where it can achieve real improvements over
other smoothing methods ? when relatively sparse,
pruned models are required, it has severely dimin-
ished utility.
Table 1 presents a slightly reformatted version
of Table 3 from Chelba et al (2010). In their
experiments (see Table 1 caption for specifics on
training/test setup), they trained 4-gram Broad-
cast News language models using a variety of
both backoff and interpolated smoothing methods
and measured perplexity before and after Stol-
cke (1998) relative entropy based pruning. With
this size training data, the perplexity of all of
the smoothing methods other than Kneser-Ney
degrades from around 120 with the full model
to around 200 with the heavily pruned model.
Kneser-Ney smoothed models have lower perplex-
ity with the full model than the other methods by
about 5 points, but degrade with pruning to far
higher perplexity, between 270-285.
The cause of this degradation is Kneser-Ney?s
unique method for estimating smoothed language
models, which will be presented in more detail in
Section 3. Briefly, the smoothing method reesti-
mates lower-order n-gram parameters in order to
avoid over-estimating the likelihood of n-grams
that already have ample probability mass allocated
as part of higher-order n-grams. This is done via
a marginal distribution constraint which requires
the expected frequency of the lower-order n-grams
to match their observed frequency in the training
data, much as is commonly done for maximum
entropy model training. Goodman (2001) proved
that, under certain assumptions, such constraints
can only improve language models. Lower-order
n-gram parameters resulting from Kneser-Ney are
not relative frequency estimates, as with other
smoothing methods; rather they are parameters
estimated specifically for use within the larger
smoothed model.
There are (at least) a couple of reasons why such
parameters do not play well with model pruning.
First, the pruning methods commonly use lower
order n-gram probabilities to derive an estimate
of state marginals, and, since these parameters are
no longer smoothed relative frequency estimates,
they do not serve that purpose well. For this rea-
son, the widely-used SRILM toolkit recently pro-
vided switches to modify their pruning algorithm
to use another model for state marginal estimates
(Stolcke et al, 2011). Second, and perhaps more
importantly, the marginal constraints that were ap-
plied prior to smoothing will not in general be con-
sistent with the much smaller pruned model. For
example, if a bigram parameter is modified due
to the presence of some set of trigrams, and then
some or all of those trigrams are pruned from the
model, the bigram associated with the modified
parameter will be unlikely to have an overall ex-
pected frequency equal to its observed frequency
anymore. As a result, the resulting model degrades
dramatically with pruning.
In this paper, we present an algorithm that
imposes marginal distribution constraints of the
sort used in Kneser-Ney modeling on arbitrary
smoothed backoff n-gram language models. Our
approach makes use of the same sort of deriva-
tion as the original Kneser-Ney modeling, but,
among other differences, relies on smoothed es-
timates of the empirical relative frequency rather
than the unsmoothed observed frequency. The al-
gorithm can be applied after the smoothed model
has been pruned, hence avoiding the pitfalls asso-
ciated with Kneser-Ney modeling. Furthermore,
while Kneser-Ney is conventionally defined as a
variant of absolute discounting, our method can
be applied to models smoothed with any backoff
smoothing, including mixtures of models, widely
44
used for domain adaptation.
We next establish formal preliminaries and
our smoothed marginal distribution constraints
method.
2 Preliminaries
N-gram language models are typically presented
mathematically in terms of words w, the strings
(histories) h that precede them, and the suffixes
of the histories (backoffs) h? that are used in the
smoothing recursion. Let V be a vocabulary (al-
phabet), and V ? a string of zero or more symbols
drawn from V . Let V k denote the set of strings
w ? V ? of length k, i.e., |w| = k. We will use
variables u, v, w, x, y, z ? V to denote single sym-
bols from the vocabulary; h, g ? V ? to denote his-
tory sequences preceding the specific word; and
h?, g? ? V ? the respective backoff histories of h
and g as typically defined (see below). For a string
w = w1 . . . w|w| we can calculate the smoothed
conditional probability of each word wi in the se-
quence given the k words that preceded it, de-
pending on the order of the Markov model. Let
hki = wi?k . . . wi?1 be the previous k words in
the sequence. Then the smoothed model is defined
recursively as follows:
P(wi | hki ) =
{
P(wi | hki ) if c(hkiwi) > 0
?(hki ) P(wi | hk?1i ) otherwise
where c(hkiwi) is the count of the n-gram sequence
wi?k . . . wi in the training corpus; P is a regular-
ized probability estimate that provides some prob-
ability mass for unobserved n-grams; and ?(hki )
is a factor that ensures normalization. Note that
for h = hki , the typically defined backoff history
h? = hk?1i , i.e., the longest suffix of h that is not h
itself. When we use h? and g? (for notational con-
venience) in future equations, it is this definition
that we are using.
There are many ways to estimate P, includ-
ing absolute discounting (Ney et al, 1994), Katz
(1987) and Witten and Bell (1991). Interpolated
models are special cases of this form, where the P
is determined using model mixing, and the ? pa-
rameter is exactly the mixing factor value for the
lower order model.
N-gram language models allow for a sparse rep-
resentation, so that only a subset of the possible n-
grams must be explicitly stored. Probabilities for
the rest of the n-grams are calculated through the
?otherwise? semantics in the equation above. For
an n-gram language model G, we will say that an
n-gram hw ? G if it is explicitly represented in
the model; otherwise hw 6? G. In the standard n-
gram formulation above, the assumption is that if
c(hkiwi) > 0 then the n-gram has a parameter; yet
with pruning, we remove many observed n-grams
from the model, hence this is no longer the ap-
propriate criterion. We reformulate the standard
equation as follows:
P(wi|hki ) =
{
?(hkiwi) if hkiwi ? G
?(hki , hk?1i ) P(wi|hk?1i ) otherwise
(1)
where ?(hkiwi) is the parameter associated with
the n-gram hkiwi and ?(hki , hk?1i ) is the backoff
cost associated with going from state hki to state
hk?1i . We assume that, if hw ? G then all prefixes
and suffixes of hw are also in G.
Figure 1 presents a schema of an automaton rep-
resentation of an n-gram model, of the sort used in
the OpenGrm library (Roark et al, 2012). States
represent histories h, and the words w, whose
probabilities are conditioned on h, label the arcs,
leading to the history state for the subsequent
word. State labels are provided in Figure 1 as
a convenience, to show the (implicit) history en-
coded by the state, e.g., ?xyz? indicates that the
state represents a history with the previous three
symbols being x, y and z. Failure arcs, labeled
with a ? in Figure 1, encode an ?otherwise? se-
mantics and have as destination the origin state?s
backoff history. Many higher order states will
back off to the same lower order state, specifically
those that share the same suffix.
Note that, in general, the recursive definition of
backoff may require the traversal of several back-
yz
z
xyz
u/?(xyzu)
w/?(yzw)
w/?(zw)
?/?(xyz,yz)
?/?(yz,z)
zw
yyz
?/?(yyz,yz)
yzw
?
yzu yzv
v/?(yyzv)
w/?(yyzw)
?/?(z,?)
?/?(yzw,zw)
z/?(z)
Figure 1: N-gram weighted automaton schema. State labels
are presented for convenience, to specify the history implic-
itly encoded by the state.
45
off arcs before emitting a word, e.g., the highest
order states in Figure 1 needing to traverse a cou-
ple of ? arcs to reach state ?z?. We can define
the backoff cost between a state hki and any of its
suffix states as follows. Let ?(h, h) = 1 and for
m > 1,
?(hki , hk?mi ) =
m?
j=1
?(hk?j+1i , h
k?j
i ).
If hkiw 6? G then the probability of that n-gram
will be defined in terms of backoff to its longest
suffix hk?mi w ? G. Let hwG denote the longest
suffix of h such that hwGw ? G. Note that this
is not necessarily a proper suffix, since hwG could
be h itself or it could be . Then
P(w | h) = ?(h, hwG) ?(hwGw) (2)
which is equivalent to equation 1.
3 Marginal distribution constraints
Marginal distribution constraints attempt to match
the expected frequency of an n-gram with its ob-
served frequency. In other words, if we use the
model to randomly generate a very large corpus,
the n-grams should occur with the same rela-
tive frequency in both the generated and original
(training) corpus. Standard smoothing methods
overgenerate lower-order n-grams. Using standard
n-gram notation (where g? is the backoff history
for g), this constraint is stated in Kneser and Ney
(1995) as
P?(w | h?) =
?
g:g?=h?
P(g, w | h?) (3)
where P? is the empirical relative frequency esti-
mate. Taking this approach, certain base smooth-
ing methods end up with very nice, easy to cal-
culate solutions based on counts. Absolute dis-
counting (Ney et al, 1994) in particular, using the
above approach, leads to the well-known Kneser-
Ney smoothing approach (Kneser and Ney, 1995;
Chen and Goodman, 1998). We will follow this
same approach, with a couple of changes. First,
we will make use of regularized estimates of rela-
tive frequency P rather than raw relative frequency
P?. Second, rather than just looking at observed
histories h that back off to h?, we will look at
all histories (observed or not) of the length of
the longest history in the model. For notational
simplicity, suppose we have an n+1-gram model,
hence the longest history in the model is of length
n. Assume the length of the particular backoff his-
tory |h?| = k. Let V n?kh? be the set of strings
h ? V n with h? as a suffix. Then we can restate
the marginal distribution constraint in equation 3
as
P(w | h?) =
?
h?V n?kh?
P(h,w | h?) (4)
Next we solve for ?(h?w) parameters used in
equation 1. Note that h? is a suffix of any h ?
V n?kh?, so conditioning probabilities on h and h?
is the same as conditioning on just h. Each of
the following derivation steps simply relies on the
chain rule or definition of conditional probability,
as well as pulling terms out of the summation.
P(w | h?) =
?
h?V n?kh?
P(h,w | h?)
=
?
h?V n?kh?
P(w | h, h?) P(h | h?)
=
?
h?V n?kh?
P(w | h) P(h)?
g?V n?kh?
P(g)
= 1?
g?V n?kh?
P(g)
?
h?V n?kh?
P(w | h) P(h) (5)
Then, multiplying both sides by the normaliz-
ing denominator on the right-hand side and using
equation 2 to substitute ?(h, hwG) ?(hwGw) for
P(w | h):
P(w | h?)
?
g?V n?kh?
P(g) =
?
h?V n?kh?
P(w | h) P(h)
=
?
h?V n?kh?
?(h, hwG) ?(hwGw) P(h) (6)
Note that we are only interested in h?w ? G,
hence there are two disjoint subsets of histories
h ? V n?kh? that are being summed over: those
such that hwG = h? and those such that |hwG| >
|h?|. We next separate these sums in the next step
of the derivation:
P(w | h?)
?
g?V n?kh?
P(g) =
?
h?V n?kh?:|hwG|>|h?|
?(h, hwG) ?(hwGw) P(h) +
?
h?V n?kh?:hwG=h?
?(h, h?) ?(h?w) P(h) (7)
Finally, we solve for ?(h?w) in the second sum
on the right-hand side of equation 7, yielding the
formula in equation 8. Note that this equation is
the correlate of equation (6) in Kneser and Ney
46
?(h?w) =
P(w | h?)
?
g?V n?kh?
P(g) ?
?
h?V n?kh?:|hwG|>|h?|
?(h, hwG) ?(hwGw) P(h)
?
h?V n?kh?:hwG=h?
?(h, h?) P(h)
(8)
(1995), modulo the two differences noted earlier:
use of smoothed probability P rather than raw rel-
ative frequency; and summing over all history sub-
strings in V n?kh? rather than just those with count
greater than zero, which is also a change due to
smoothing. Keep in mind, P is the target expected
frequency from a given smoothed model. Kneser-
Ney models are not useful input models, since
their P n-gram parameters are not relative fre-
quency estimates. This means that we cannot sim-
ply ?repair? pruned Kneser-Ney models, but must
use other smoothing methods where the smoothed
values are based on relative frequency estimation.
There are, in addition, two other important dif-
ferences in our approach from that in Kneser and
Ney (1995), which would remain as differences
even if our target expected frequency were the
unsmoothed relative frequency P? instead of the
smoothed estimate P. First, the sum in the nu-
merator is over histories of length n, the highest
order in the n-gram model, whereas in the Kneser-
Ney approach the sum is over histories that imme-
diately back off to h?, i.e., from the next highest
order in the n-gram model. Thus the unigram dis-
tribution is with respect to the bigram model, the
bigram model is with respect to the trigram model,
and so forth. In our optimization, we sum in-
stead over all possible history sequences of length
n. Second, an early assumption made in Kneser
and Ney (1995) is that the denominator term in
their equation (6) (our Eq. 8) is constant across all
words for a given history, which is clearly false.
We do not make this assumption. Of course, the
probabilities must be normalized, hence the final
values of ?(h?w) will be proportional to the val-
ues in Eq. 8.
We briefly note that, like Kneser-Ney, if the
baseline smoothing method is consistent, then the
amount of smoothing in the limit will go to zero
and our resulting model will also be consistent.
The smoothed relative frequency estimate P and
higher order ? values on the right-hand side of Eq.
8 are given values (from the input smoothed model
and previous stages in the algorithm, respectively),
implying an algorithm that estimates highest or-
ders of the model first. In addition, steady state
history probabilities P(h) must be calculated. We
turn to the estimation algorithm next.
4 Model constraint algorithm
Our algorithm takes a smoothed backoff n-gram
language model in an automaton format (see Fig-
ure 1) and returns a smoothed backoff n-gram lan-
guage model with the same topology. For all n-
grams in the model that are suffixes of other n-
grams in the model ? i.e., that are backed-off to
? we calculate the weight provided by equation 8
and assign it (after normalization) to the appropri-
ate n-gram arc in the automaton. There are several
important considerations for this algorithm, which
we address in this section. First, we must provide
a probability for every state in the model. Second,
we must memoize summed values that are used
repeatedly. Finally, we must iterate the calcula-
tion of certain values that depend on the n-gram
weights being re-estimated.
4.1 Steady state probability calculation
The steady state probability P(h) is taken to be the
probability of observing h after a long word se-
quence, i.e., the state?s relative frequency in a long
sequence of randomly-generated sentences from
the model:
P(h) = lim
m??
?
w1...wm
P?(w1 . . . wmh) (9)
where P? is the corpus probability derived as fol-
lows: The smoothed n-gram probability model
P(w | h) is naturally extended to a sentence
s = w0 . . . wl, where w0 = <s> and wl = </s>
are the sentence initial and final words, by P(s) =?l
i=1 P(wi | hni ). The corpus probability s1 . . . sr
is taken as:
P?(s1 . . . sr) = (1? ?)?r?1
r?
i=1
P(si) (10)
where ? parameterizes the corpus length distri-
bution.2 Assuming the n-gram language model
automaton G has a single final state </s> into
2P? models words in a corpus rather than a single sen-
tence since Equation 9 tends to zero as m ? ? otherwise.
In Markov chain terms, the corpus distribution is made irre-
ducible to allow a non-trivial stationary distribution.
47
which all </s> arcs enter, adding a ? weighted
 arc from the </s> state to the initial state and
having a final weight 1 ? ? in order to leave the
automaton at the </s> state will model this cor-
pus distribution. According to Eq. 9, P (h) is then
the stationary distribution of the finite irreducible
Markov Chain defined by this altered automaton.
There are many methods for computing such a sta-
tionary distribution; we use the well-known power
method (Stewart, 1999).
One difficulty remains to be resolved. The
backoff arcs have a special interpretation in the
automaton: they are traversed only if a word fails
to match at the higher order. These failure arcs
must be properly handled before applying stan-
dard stationary distribution calculations. A simple
approach would be for each word w? and state h
such that hw? /? G, but h?w? ? G, add a w? arc
from state h to h?w? with weight ?(h, h?)?(h?w?)
and then remove all failure arcs (see Figure 2a).
This however results in an automaton with |V | arcs
leaving every state, which is unwieldy with larger
vocabularies and n-gram orders. Instead, for each
word w and state h such that hw ? G, add a w arc
from state h to h?w with weight ??(h, h?)?(h?w)
and then replace all failure labels with  labels (see
Figure 2b). In this case, the added negatively-
weighted arcs compensate for the excess probabil-
ity mass allowed by the epsilon arcs3. The number
of added arcs is no more than found in the original
model.
4.2 Accumulation of higher order values
We are summing over all possible histories of
length n in equation 8, and the steady state prob-
ability calculation outlined in the previous section
includes the probability mass for histories h 6? G.
The probability mass of states not inG ends up be-
ing allocated to the state representing their longest
suffix that is explicitly in G. That is the state that
would be active when these histories are encoun-
tered. Hence, once we have calculated the steady
state probabilities for each state in the smoothed
model, we only need to sum over states explicitly
in the model.
As stated earlier, the use of ?(hwGw) in the
numerator of equation 8 for hwG that are larger
than h? implies that the longer n-grams must be
3Since each negatively-weighted arc leaving a state
exactly cancels an epsilon arc followed by a matching
positively-weighted arc in each iteration of the power
method, convergence is assured.
(a) (b)
K
K

w/?(Kw)
w
/?(K
w
)
?/?(K,K
)
Kw
K
w

w
/?(K,K
)?(K
w
)
K
K

w/?(Kw)
w/?(K
w)
?/?(K,K
)
Kw
K
w
w/?(K,K
)?(K
w)
Figure 2: Schemata showing failure arc handling: (a) ?
removal: add w? arc (red), delete ? arc; (b) ? replacement:
add w arc (red), replace ? by  (red)
re-estimated first. Thus we process each history
length in descending order, finishing with the un-
igram state. Since we assume that, for every n-
gram hw ? G, every prefix and suffix is also
in G, we know that if h?w 6? G then there is
no history h such that h? is a suffix of h and
hw ? G. This allows us to recursively accumu-
late the ?(h, h?) P(h) in the denominator of Eq. 8.
For every n-gram, we can accumulate values re-
quired to calculate the three terms in equation 8,
and pass them along to calculate lower order n-
gram values. Note, however, that a naive imple-
mentation of an algorithm to assign these values is
O(|V |n). This is due to the fact that the denom-
inator factor must be accumulated for all higher
order states that do not have the given n-gram.
Hence, for every state h directly backing off to
h? (order |V |), and for every n-gram arc leaving
state h? (order |V |), some value must be accumu-
lated. This can be particularly clearly seen at the
unigram state, which has an arc for every unigram
(the size of the vocabulary): for every bigram state
(also order of the vocabulary), in the naive algo-
rithm we must look for every possible arc. Since
there are O(|V |n?2) lower order histories in the
model in the worst case, we have overall complex-
ity O(|V |n). However, we know that the number
of stored n-grams is very sparse relative to the pos-
sible number of n-grams, so the typical case com-
plexity is far lower. Importantly, the denominator
is calculated by first assuming that all higher order
states back off to the current n-gram, then subtract
out the mass associated with those that are already
observed at the higher order. In such a way, we
need only perform work for higher order n-grams
hw that are explicitly in the model. This opti-
mization achieves orders-of-magnitude speedups,
so that models take seconds to process.
Because smoothing is not necessarily con-
48
strained across n-gram orders, it is possible that
higher-order n-grams could be smoothed less than
lower order n-grams, so that the numerator of
equation 8 can be less than zero, which is not valid.
A value less than zero means that the higher or-
der n-grams will already produce the n-gram more
frequently than its smoothed expected frequency.
We set a minimum value  for the numerator, and
any n-gram numerator value less than  is replaced
with  (for the current study,  = 0.001). We
find this to be relatively infrequent, about 1% of
n-grams for most models.
4.3 Iteration
Recall that P and ? terms on the right-hand side of
equation 8 are given and do not change. But there
are two other terms in the equation that change as
we update the n-gram parameters. The ?(h, h?)
backoff weights in the denominator ensure nor-
malization at the higher order states, and change
as the n-gram parameters at the current state are
modified. Further, the steady state probabilities
will change as the model changes. Hence, at each
state, we must iterate the calculation of the denom-
inator term: first adjust n-gram weights and nor-
malize; then recalculate backoff weights at higher
order states and iterate. Since this only involves
the denominator term, each n-gram weight can be
updated by multiplying by the ratio of the old term
and the new term.
After the entire model has been re-estimated,
the steady state probability calculation presented
in Section 4.1 is run again and model estimation
happens again. As we shall see in the experimen-
tal results, this typically converges after just a few
iterations. At this time, we have no convergence
proofs for either of these iterative components to
the algorithm, but expect that something can be
said about this, which will be a priority in future
work.
5 Experimental results
All results presented here are for English Broad-
cast News. We received scripts for replicating the
Chelba et al (2010) results from the authors, and
we report statistics on our replication of their pa-
per?s results in Table 2. The scripts are distributed
in such a way that the user supplies the data from
LDC98T31 (1996 CSR HUB4 Language Model
corpus) and the script breaks the collection into
training and testing sets, normalizes the text, and
Smoothing Perplexity n-grams (?1000)
method full pruned model diff
Abs.Disc. 120.4 197.1 382.3 -1.1
Witten-Bell 118.7 196.1 379.3 -1.1
Ristad 126.2 203.4 394.6 -1.1
Katz 119.7 197.9 385.1 -1.1
Kneser-Ney? 114.4 234.1 375.4 -12.7
Table 2: Replication of Chelba et al (2010) using provided
script. Using the script, the size of the unpruned model is
31,091,219 ngrams, 4,041 fewer than Chelba et al (2010).
? Kneser-Ney model pruned using -prune-history-lm
switch in SRILM.
trains and prunes the language models using the
SRILM toolkit (Stolcke et al, 2011). Presumably
due to minor differences in text normalization, re-
sulting in very slightly fewer n-grams in all con-
ditions, we achieve negligibly lower perplexities
(one or two tenths of a point) in all conditions, as
can be seen when comparing with Table 1. All
of the same trends result, thus that paper?s result
is successfully replicated here. Note that we ran
our Kneser-Ney pruning (noted with a ? in the ta-
ble), using the new -prune-history-lm switch in
SRILM ? created in response to the Chelba et al
(2010) paper ? which allows the use of another
model to calculate the state marginals for pruning.
This fixes part of the problem ? perplexity does not
degrade as much as the Kneser-Ney pruned model
in Table 1 ? but, as argued earlier in this paper, this
is not the sole reason for the degradation and the
perplexity remains extremely inflated.
We follow Chelba et al (2010) in training and
test set definition, vocabulary size, and parame-
ters for reporting perplexity. Note that unigrams
in the models are never pruned, hence all models
assign probabilities over an identical vocabulary
and perplexity is comparable across models. For
all results reported here, we use the SRILM toolkit
for baseline model training and pruning, then con-
vert from the resulting ARPA format model to
an OpenFst format (Allauzen et al, 2007), as
used in the OpenGrm n-gram library (Roark et al,
2012). We then apply the marginal distribution
constraints, and convert the result back to ARPA
format for perplexity evaluation with the SRILM
toolkit. All models are subjected to full normaliza-
tion sanity checks, as with typical model functions
in the OpenGrm library.
Recall that our algorithm assumes that, for ev-
ery n-gram in the model, all prefix and suffix n-
grams are also in the model. For pruned mod-
els, the SRILM toolkit does not impose such a
requirement, hence explicit arcs are added to the
49
Perplexity n-grams
Smoothing Pruned Pruned (?1000)
Method Model +MDC ? in WFST
Abs.Disc. 197.1 187.4 9.7 389.2
Witten-Bell 196.1 185.7 10.4 385.0
Ristad 203.4 190.3 13.1 395.9
Katz 197.9 187.5 10.4 390.8
AD,WB,Katz
Mixture 196.6 186.3 10.3 388.7
Table 3: Perplexity reductions achieved with marginal dis-
tribution constraints (MDC) on the heavily pruned models
from Chelba et al (2010), and a mixture model. WFST n-
gram counts are slightly higher than ARPA format in Table 2
due to adding prefix and suffix n-grams.
model during conversion, with probability equal to
what they would receive in the the original model.
The resulting model is equivalent, but with a small
number of additional arcs in the explicit repre-
sentation (around 1% for the most heavily pruned
models).
Table 3 presents perplexity results for models
that result from applying our marginal distribution
constraints to the four heavily pruned models from
Table 2. In all four cases, we get perplexity reduc-
tions of around 10 points. We present the num-
ber of n-grams represented explicitly in the WFST,
which is a slight increase from those presented in
Table 2 due to the reintroduction of prefix and suf-
fix n-grams.
In addition to the four models reported in
Chelba et al (2010), we produced a mixture model
by interpolating (with equal weight) smoothed n-
gram probabilities from the full (unpruned) ab-
solute discounting, Witten-Bell and Katz models,
which share the same set of n-grams. After renor-
malizing and pruning to approximately the same
size as the other models, we get commensurate
gains using this model as with the other models.
Figure 3 demonstrates the importance of iterat-
ing the steady state history calculation. All of the
methods achieve perplexity reductions with sub-
sequent iterations. Katz and absolute discounting
achieve very little reduction in the first iteration,
but catch back up in the second iteration.
The other iterative part of the algorithm, dis-
cussed in Section 4.3, is the denominator of equa-
tion 8, which changes due to adjustments in the
backoff weights required by the revised n-gram
probabilities. If we do not iteratively update the
backoff weights when reestimating the weights,
the ?Pruned+MDC? perplexities in Table 3 in-
crease by between 0.2?0.4 points. Hence, iterat-
ing the steady state probability calculation is quite
important, as illustrated by Figure 3; iterating the
0 1 2 3 4 5 6180
185
190
195
200
205
Iterations of estimation (recalculating steady state probs)
Perp
lexit
y
 
 Witten?BellRistadKatzAbsolute DiscountingWB,AD,Katz mixture
Figure 3: Models resulting from different numbers of pa-
rameter re-estimation iterations. Iteration 0 is the baseline
pruned model.
denominator calculation much less so, at least for
these models. We noted in Section 3 that a key dif-
ference between our approach and Kneser and Ney
(1995) is that their approach treated the denomina-
tor as a constant. If we do this, the ?Pruned+MDC?
perplexities increase by between 4.5?5.6 points,
i.e., about half of the perplexity reduction is lost
for each method. Thus, while iteration of denomi-
nator calculation may not be critical, it should not
be treated as a constant.
We now look at the impacts on system perfor-
mance we can achieve with these new models4,
and whether the perplexity differences that we ob-
serve translate to real error rate reductions.
For automatic speech recognition experiments,
we used as test set the 1997 Hub4 evaluation set
consisting of 32,689 words. The acoustic model
is a tied-state triphone GMM-based HMM whose
input features are 9-frame stacked 13-dimensional
PLP-cepstral coefficients projected down to 39 di-
mensions using LDA. The model was trained on
the 1996 and 1997 Hub4 acoustic model train-
ing sets (about 150 hours of data) using semi-tied
covariance modeling and CMLLR-based speaker
adaptive training and 4 iterations of boosted MMI.
We used a multi-pass decoding strategy: two
quick passes for adaptation supervision, CMLLR
and MLLR estimation; then a slower full decoding
pass running about 3 times slower than real time.
Table 4 presents recognition results for the
heavily pruned models that we have been con-
sidering, both for first pass decoding and rescor-
ing of the resulting lattices using failure transi-
tions rather than epsilon backoff approximations.
4For space purposes, we exclude the Ristad method from
this point forward since it is not competitive with the others.
50
Word error rate (WER)
First pass Rescoring
Smoothing Pruned Pruned Pruned Pruned
Method Model +MDC Model +MDC
Abs.Disc. 20.5 19.7 20.2 19.6
Witten-Bell 20.5 19.9 20.1 19.6
Katz 20.5 19.7 20.2 19.7
Mixture 20.5 19.6 20.2 19.6
Kneser-Neya 22.1 22.2
Kneser-Neyb 20.5 20.6
Table 4: WER reductions achieved with marginal dis-
tribution constraints (MDC) on the heavily pruned models
from Chelba et al (2010), and a mixture model. Kneser-
Ney results are shown for: a) original pruning; and b) with
-prune-history-lm switch.
The perplexity reductions that were achieved for
these models do translate to real word error rate
reductions at both stages of between 0.5 and 0.9
percent absolute. All of these gains are sta-
tistically significant at p < 0.0001 using the
stratified shuffling test (Yeh, 2000). For pruned
Kneser-Ney models, fixing the state marginals
with the -prune-history-lm switch reduces the
WER versus the original pruned model, but no re-
ductions were achieved vs. baseline methods.
Table 5 presents perplexity and WER results
for less heavily pruned models, where the prun-
ing thresholds were set to yield approximately
1.5 million n-grams (4 times more than the pre-
vious models); and another set at around 5 mil-
lion n-grams, as well as the full, unpruned mod-
els. While the robust gains we?ve observed up to
now persist with the 1.5M n-gram models (WER
reductions significant, Witten-Bell at p < 0.02,
others at p < 0.0001), the larger models yield
diminishing gains, with no real WER improve-
ments. Performance of Witten-Bell models with
the marginal distribution constraints degrade badly
for the larger models, indicating that this method
of regularization, unmodified by aggressive prun-
ing, does not provide a well suited distribution for
this sort of optimization. We speculate that this
is due to underregularization, having noted some
floating point precision issues when allowing the
backoff recalculation to run indefinitely.
6 Summary and Future Directions
The presented method reestimates lower order
n-gram model parameters for a given smoothed
backoff model, achieving perplexity and WER re-
ductions for many smoothed models. There re-
main a number of open questions to investigate
in the future. Recall that the numerator in Eq.
8 can be less than zero, meaning that no param-
eterization would lead to a model with the target
frequency of the lower order n-gram, presumably
due to over- or under-regularization. We antic-
ipate a pre-constraint on the baseline smoothing
method, that would recognize this problem and ad-
just the smoothing to ensure that a solution does
exist. Additionally, it is clear that different reg-
ularization methods yield different behaviors, no-
tably that large, relatively lightly pruned Witten-
Bell models yield poor results. We will look to
identify the issues with such models and provide
general guidelines for prepping models prior to
processing. Finally, we would like to perform ex-
tensive controlled experimentation to examine the
relative contribution of the various aspects of our
approach.
Acknowledgments
Thanks to Ciprian Chelba and colleagues for the
scripts to replicate their results. This work was
supported in part by a Google Faculty Research
Award and NSF grant #IIS-0964102. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of
the NSF.
M Less heavily pruned model Moderately pruned model Full model
Smoothing D ngrams WER ngrams WER ngrams WER
Method C (?106) PPL FP RS (?106) PPL FP RS (?106) PPL FP RS
Abs. N 1.53 146.6 18.1 17.9 5.19 129.1 17.0 16.6 31.1 120.4 16.2 16.1
Disc. Y 141.2 17.2 17.2 126.3 16.6 16.6 31.1 117.0 16.0 16.0
Witten- N 1.54 145.8 18.1 17.6 5.08 129.4 17.3 16.8 31.1 118.7 16.3 16.1
Bell Y 139.7 17.9 17.4 126.4 18.4 17.3 31.1 118.4 18.1 17.6
Katz N 1.57 146.6 17.8 17.7 5.10 128.9 16.8 16.6 31.1 119.7 16.2 16.1
Y 141.1 17.3 17.3 125.7 16.6 16.6 31.1 114.7 16.2 16.1
Mixture N 1.55 145.5 18.1 17.7 5.11 128.2 16.9 16.6 31.1 118.5 16.3 16.1
Y 139.2 17.3 17.2 123.6 16.6 16.4 31.1 114.6 17.3 16.4
Kneser-Ney backoff model, unpruned: 31.1 114.4 15.8 15.9
Table 5: Perplexity (PPL) and both first pass (FP) and rescoring (RS) WER reductions for less heavily pruned models using
marginal distribution constraints (MDC).
51
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of
Automata (CIAA 2007), Lecture Notes in Computer
Science, volume 4793, pages 11?23.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and
Peng Xu. 2010. Study on interaction between en-
tropy pruning and Kneser-Ney smoothing. In Pro-
ceedings of Interspeech, page 24222425.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report, TR-10-98, Harvard
University.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15(4):403?434.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustic,
Speech, and Signal Processing, 35(3):400?401.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages
181?184.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochas-
tic language modeling. Computer Speech and Lan-
guage, 8:1?38.
Eric S. Ristad. 1995. A natural law of succession.
Technical Report, CS-TR-495-95, Princeton Univer-
sity.
Brian Roark, Richard Sproat, Cyril Allauzen, Michael
Riley, Jeffrey Sorensen, and Terry Tai. 2012. The
OpenGrm open-source finite-state grammar soft-
ware libraries. In Proceedings of the ACL 2012 Sys-
tem Demonstrations, pages 61?66.
Kristie Seymore and Ronald Rosenfeld. 1996. Scal-
able backoff language models. In Proceedings of
the International Conference on Spoken Language
Processing (ICSLP).
Vesa Siivola, Teemu Hirsimaki, and Sami Virpioja.
2007. On growing and pruning kneserney smoothed
n-gram models. IEEE Transactions on Audio,
Speech, and Language Processing, 15(5):1617?
1624.
William J Stewart. 1999. Numerical methods for com-
puting stationary distributions of finite irreducible
markov chains. Computational Probability, pages
81?111.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and out-
look. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU).
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505?513.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 468?476.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
A. Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceedings of
the 18th International COLING, pages 947?953.
52
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 364?369,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hippocratic Abbreviation Expansion
Brian Roark and Richard Sproat
Google, Inc, 79 Ninth Avenue, New York, NY 10011
{roark,rws}@google.com
Abstract
Incorrect normalization of text can be par-
ticularly damaging for applications like
text-to-speech synthesis (TTS) or typing
auto-correction, where the resulting nor-
malization is directly presented to the user,
versus feeding downstream applications.
In this paper, we focus on abbreviation
expansion for TTS, which requires a ?do
no harm?, high precision approach yield-
ing few expansion errors at the cost of
leaving relatively many abbreviations un-
expanded. In the context of a large-
scale, real-world TTS scenario, we present
methods for training classifiers to establish
whether a particular expansion is apt. We
achieve a large increase in correct abbrevi-
ation expansion when combined with the
baseline text normalization component of
the TTS system, together with a substan-
tial reduction in incorrect expansions.
1 Introduction
Text normalization (Sproat et al, 2001) is an im-
portant initial phase for many natural language and
speech applications. The basic task of text normal-
ization is to convert non-standard words (NSWs)
? numbers, abbreviations, dates, etc. ? into stan-
dard words, though depending on the task and the
domain a greater or lesser number of these NSWs
may need to be normalized. Perhaps the most de-
manding such application is text-to-speech synthe-
sis (TTS) since, while for parsing, machine trans-
lation and information retrieval it may be accept-
able to leave such things as numbers and abbre-
viations unexpanded, for TTS all tokens need to
be read, and for that it is necessary to know how
to pronounce them. Which normalizations are re-
quired depends very much on the application.
What is also very application-dependent is the
cost of errors in normalization. For some applica-
tions, where the normalized string is an interme-
diate stage in a larger application such as trans-
lation or information retrieval, overgeneration of
normalized alternatives is often a beneficial strat-
egy, to the extent that it may improve the accu-
racy of what is eventually being presented to the
user. In other applications, such as TTS or typing
auto-correction, the resulting normalized string it-
self is directly presented to the user; hence errors
in normalization can have a very high cost relative
to leaving tokens unnormalized.
In this paper we concentrate on abbreviations,
which we define as alphabetic NSWs that it would
be normal to pronounce as their expansion. This
class of NSWs is particularly common in personal
ads, product reviews, and so forth. For example:
home health care svcs stat home health llc
osceola aquatic ctr stars rating write
audi vw repair ser quality and customer
Each of the examples above contains an abbrevi-
ation that, unlike, e.g., conventionalized state ab-
breviations such as ca for California, is either only
slightly standard (ctr for center) or not standard at
all (ser for service).
An important principle in text normalization for
TTS is do no harm. If a system is unable to re-
liably predict the correct reading for a string, it is
better to leave the string alone and have it default
to, say, a character-by-character reading, than to
expand it to something wrong. This is particularly
true in accessibility applications for users who rely
on TTS for most or all of their information needs.
Ideally a navigation system should read turn on
30N correctly as turn on thirty north; but if it can-
not resolve the ambiguity in 30N, it is far better to
read it as thirty N than as thirty Newtons, since lis-
teners can more easily recover from the first kind
of error than the second.
We present methods for learning abbreviation
expansion models that favor high precision (incor-
rect expansions < 2%). Unannotated data is used
to collect evidence for contextual disambiguation
and to train an abbreviation model. Then a small
amount of annotated data is used to build models
to determine whether to accept a candidate expan-
364
sion of an abbreviation based on these features.
The data we report on are taken from Google
Maps
TM
and web pages associated with its map en-
tries, but the methods can be applied to any data
source that is relatively abbreviation rich.
We note in passing that similar issues arise
in automatic spelling correction work (Wilcox-
O?Hearn et al, 2008), where it is better to leave
a word alone than to ?correct? it wrongly.
2 Related work
There has been a lot of interest in recent years on
?normalization? of social media such as Twitter,
but that work defines normalization much more
broadly than we do here (Xia et al, 2006; Choud-
hury et al, 2007; Kobus et al, 2008; Beaufort et
al., 2010; Kaufmann, 2010; Liu et al, 2011; Pen-
nell and Liu, 2011; Aw and Lee, 2012; Liu et al,
2012a; Liu et al, 2012b; Hassan and Menezes,
2013; Yang and Eisenstein, 2013). There is a good
reason for us to focus more narrowly. For Twit-
ter, much of the normalization task involves non-
standard language such as ur website suxx brah
(from Yang and Eisenstein (2013)). Expanding the
latter to your website sucks, brother certainly nor-
malizes it to standard English, but one could argue
that in so doing one is losing information that the
writer is trying to convey using an informal style.
On the other hand, someone who writes svc ctr
for service center in a product review is probably
merely trying to save time and so expanding the
abbreviations in that case is neutral with respect to
preserving the intent of the original text.
One other difference between the work we re-
port from much of the recent work cited above is
that that work focuses on getting high F scores,
whereas we are most concerned with getting high
precision. While this may seem like a trivial
trade off between precision and recall, our goal
motivates developing measures that minimize the
?risk? of expanding a term, something that is im-
portant in an application such as TTS, where one
cannot correct a misexpansion after it is spoken.
3 Methods
Since our target application is text-to-speech, we
define the task in terms of an existing TTS lexi-
con. If a word is already in the lexicon, it is left
unprocessed, since there is an existing pronuncia-
tion for it; if a word is out-of-vocabulary (OOV),
we consider expanding it to a word in the lexicon.
We consider a possible expansion for an abbrevi-
ation to be any word in the lexicon from which
the abbreviation can be derived by only deletion of
letters.
1
For present purposes we use the Google
English text-to-speech lexicon, consisting of over
430 thousand words. Given an OOV item (possi-
ble abbreviation) in context, we make use of fea-
tures of the context and of the OOV item itself to
enumerate and score candidate expansions.
Our data consists of 15.1 billion words of text
data from Google Maps
TM
, lower-cased and tok-
enized to remove punctuation symbols. We used
this data in several ways. First, we used it to boot-
strap a model for assigning a probability of an ab-
breviation/expansion pair. Second, we used it to
extract contextual n-gram features for predicting
possible expansions. Finally, we sampled just over
14 thousand OOV items in context and had them
manually labeled with a number of categories, in-
cluding ?abbreviation?. OOVs labeled as abbrevia-
tions were also labeled with the correct expansion.
We present each of these uses in turn.
3.1 Abbreviation modeling
We collect potential abbreviation/full-word pairs
by looking for terms that could be abbreviations
of full words that occur in the same context. Thus:
the svc/service center
heating clng/cooling system
dry clng/cleaning system
contributes evidence that svc is an abbreviation
of service. Similarly instances of clng in con-
texts that can contain cooling or cleaning are evi-
dence that clng could be an abbreviation of either
of these words. (The same contextual information
of course is used later on to disambiguate which
of the expansions is appropriate for the context.)
To compute the initial guess as to what can be a
possible abbreviation, a Thrax grammar (Roark et
al., 2012) is used that, among other things, speci-
fies that: the abbreviation must start with the same
letter as the full word; if a vowel is deleted, all ad-
jacent vowels should also be deleted; consonants
may be deleted in a cluster, but not the last one;
and a (string) suffix may be deleted.
2
We count
a pair of words as ?co-occurring? if they are ob-
served in the same context. For a given context C,
e.g., the center, letW
C
be the set of words found
in that context. Then, for any pair of words u, v,
we can assign a pair count based on the count of
contexts where both occur:
c(u, v) = |{C : u ?W
C
and v ?W
C
}|
1
We do not deal here with phonetic spellings in abbrevia-
tions such as 4get, or cases where letters have been transposed
due to typographical errors (scv).
2
This Thrax grammar can be found at
http://openfst.cs.nyu.edu/twiki/bin/
view/Contrib/ThraxContrib
365
blvd boulevard rd road yrs years
ca california fl florida ctr center
mins minutes def definitely ste suite
Table 1: Examples of automatically mined abbrevia-
tion/expansion pairs.
Let c(u) be defined as
?
v
c(u, v). From these
counts, we can define a 2?2 table and calculate
statistics such as the log likelihood statistic (Dun-
ning, 1993), which we use to rank possible abbre-
viation/expansion pairs. Scores derived from these
type (rather than token) counts highly rank pairs of
in-vocabulary words and OOV possible abbrevia-
tions that are substitutable in many contexts.
We further filter the potential abbreviations by
removing ones that have a lot of potential expan-
sions, where we set the cutoff at 10. This removes
mostly short abbreviations that are highly ambigu-
ous. The resulting ranked list of abbreviation ex-
pansion pairs is then thresholded before building
the abbreviation model (see below) to provide a
smaller but more confident training set. For this
paper, we used 5-gram contexts (two words on ei-
ther side) to extract abbreviations and their expan-
sions. See Table 1 for some examples.
Our abbreviation model is a pair character lan-
guage model (LM), also known as a joint multi-
gram model (Bisani and Ney, 2008), whereby
aligned symbols are treated as a single token and
a smoothed n-gram model is estimated. This de-
fines a joint distribution over input and output
sequences, and can be efficiently encoded as a
weighted finite-state transducer. The extracted
abbreviation/expansion pairs are character-aligned
and a 7-gram pair character LM is built over
the alignments using the OpenGrm n-gram library
(Roark et al, 2012). For example:
c:c :e :n t:t :e r:r
Note that, as we?ve defined it, the alignments from
abbreviation to expansion allow only identity and
insertion, no deletions or substitutions. The cost
from this LM, normalized by the length of the ex-
pansion, serves as a score for the quality of a pu-
tative expansion for an abbreviation.
For a small set of frequent, conventionalized
abbreviations (e.g., ca for California ? 63 pairs
in total ? mainly state abbreviations and similar
items), we assign an fixed pair LM score, since
these examples are in effect irregular cases, where
the regularities of the productive abbreviation pro-
cess do not capture their true cost.
3.2 Contextual features
To predict the expansion given the context, we ex-
tract n-gram observations for full words in the TTS
lexicon. We do this in two ways. First, we sim-
ply train a smoothed n-gram LM from the data.
Because of the size of the data set, this is heav-
ily pruned using relative entropy pruning (Stolcke,
1998). Second, we use log likelihood and log odds
ratios (this time using standardly defined n-gram
counts) to extract reliable bigram and trigram con-
texts for words. Space precludes a detailed treat-
ment of these two statistics, but, briefly, both can
be derived from contingency table values calcu-
lated from the frequencies of (1) the word in the
particular context; (2) the word in any context; (3)
the context with any word; and (4) all words in
the corpus. See Agresti (2002), Dunning (1993)
and Monroe et al (2008) for useful overviews of
how to calculate these and other statistics to de-
rive reliable associations. In our case, we use them
to derive associations between contexts and words
occuring in those contexts. The contexts include
trigrams with the target word in any of the three
positions, and bigrams with the target word in ei-
ther position. We filter the set of n-grams based on
both their log likelihood and log odds ratios, and
provide those scores as features.
3.3 Manual annotations
We randomly selected 14,434 OOVs in their full
context, and had them manually annotated as
falling within one of 8 categories, along with the
expansion if the category was ?abbreviation?. Note
that these are relatively lightweight annotations
that do not require extensive linguistics expertise.
The abbreviation class is defined as cases where
pronouncing as the expansion would be normal.
Other categories included letter sequence (expan-
sion would not be normal, e.g., TV); partial let-
ter sequence (e.g., PurePictureTV); misspelling;
leave as is (part of a URL or pronounced as a
word, e.g., NATO); foreign; don?t know; and junk.
Abbreviations accounted for nearly 23% of the
cases, and about 3/5 of these abbreviations were
instances from the set of 63 conventional abbrevi-
ation/expansion pairs mentioned in Section 3.1.
3.4 Abbreviation expansion systems
We have three base systems that we compare here.
The first is the hand-built TTS normalization sys-
tem. This system includes some manually built
patterns and an address parser to find common ab-
breviations that occur in a recognizable context.
For example, the grammar covers several hundred
city-state combinations, such as Fairbanks AK,
yielding good performance on such cases.
The other two systems were built using data ex-
tracted as described above. Both systems make
use of the pair LM outlined in Section 3.1, but
differ in how they model context. The first sys-
366
tem, which we call ?N-gram?, uses a pruned Katz
(1987) smoothed trigram model. The second sys-
tem, which we call ?SVM?, uses a Support Vec-
tor Machine (Cortes and Vapnik, 1995) to classify
candidate expansions as being correct or not. For
both systems, for any given input OOV, the pos-
sible expansion with the highest score is output,
along with the decision of whether to expand.
For the ?N-gram? system, n-gram negative log
probabilities are extracted as follows. Let w
i
be
the position of the target expansion. We extract the
part of the n-gram probability of the string that is
not constant across all competing expansions, and
normalize by the number of words in that window.
Thus the score of the word is:
S(w
i
) = ?
1
k + 1
i+k
?
j=i
log P(w
j
| w
j?1
w
j?2
)
In our experiments, k = 2 since we have a trigram
model, though in cases where the target word is the
last word in the string, k = 1, because there only
the end-of-string symbol must be predicted in ad-
dition to the expansion. We then take the Bayesian
fusion of this model with the pair LM, by adding
them in the log space, to get prediction from both
the context and abbreviation model.
For the ?SVM? model, we extract features from
the log likelihood and log odds scores associated
with contextual n-grams, as well as from the pair
LM probability and characteristics of the abbrevi-
ation itself. We train a linear model on a subset of
the annotated data (see section 4). Multiple con-
textual n-grams may be observed, and we take the
maximum log likelihood and log odds scores for
each candidate expansion in the observed context.
We then quantize these scores down into 16 bins,
using the histogram in the training data to define
bin thresholds so as to partition the training in-
stances evenly. We also create 16 bins for the pair
LM score. A binary feature is defined for each
bin that is set to 1 if the current candidate?s score
is less than the threshold of that bin, otherwise 0.
Thus multiple bin features can be active for a given
candidate expansion of the abbreviation.
We also have features that fire for each type of
contextual feature (e.g., trigram with expansion as
middle word, etc.), including ?no context?, where
none of the trigrams or bigrams from the current
example that include the candidate expansion are
present in our list. Further, we have features for
the length of the abbreviation (shorter abbrevia-
tions have more ambiguity, hence are more risky
to expand); membership in the list of frequent,
conventionalized abbreviations mentioned earlier;
and some combinations of these, along with bias
features. We train the model using standard op-
tions with Google internal SVM training tools.
Note that the number of n-grams in the two
models differs. The N-gram system has around
200M n-grams after pruning; while the SVM
model uses around a quarter of that. We also tried
a more heavily pruned n-gram model, and the re-
sults are only very slightly worse, certainly accept-
able for a low-resource scenario.
4 Experimental Results
We split the 3,209 labeled abbreviations into a
training set of 2,209 examples and a held aside de-
velopment set of 1,000 examples. We first evaluate
on the development set, then perform a final 10-
fold cross validation over the entire set of labeled
examples. We evaluate in terms of the percent-
age of abbreviations that were correctly expanded
(true positives, TP) and that were incorrectly ex-
panded (false positives, FP).
Results are shown in Table 2. The first two rows
show the baseline TTS system and SVM model.
On the development set, both systems have a false
positive rate near 3%, i.e., three abbreviations are
expanded incorrectly for every 100 examples; and
over 50% true positive rate, i.e., more than half of
the abbreviations are expanded correctly. To re-
port true and false positive rates for the N-gram
system we would need to select an arbitrary de-
cision threshold operating point, unlike the deter-
ministic TTS baseline and the SVM model with
its decision threshold of 0. Rather than tune such a
meta-parameter to the development set, we instead
present an ROC curve comparison of the N-gram
and SVM models, and then propose a method
for ?intersecting? their output without requiring a
tuned decision threshold.
Figure 1 presents an ROC curve for the N-gram
and SVM systems, and for the simple Bayesian
fusion (sum in log space) of their scores. We can
see that the SVM model has very high precision
for its highest ranked examples, yielding nearly
20% of the correct expansions without any in-
correct expansions. However the N-gram system
achieves higher true positive rates when the false
Percent of abbreviations
dev set full set
System TP FP TP FP
TTS baseline 55.0 3.1 40.0 3.0
SVM model 52.6 3.3 53.3 2.6
SVM ? N-gram 50.6 1.1 50.3 0.9
SVM ? N-gram, then TTS 73.5 1.9 74.5 1.5
Table 2: Results on held-out labeled data, and with final
10-fold cross-validation over the entire labeled set. Percent-
age of abbreviations expanded correctly (TP) and percentage
expanded incorrectly (FP) are reported for each system.
367
C
o
r
r
e
c
t
e
x
p
a
n
s
i
o
n
p
e
r
c
e
n
t
a
g
e
(
T
P
)
0 1 2 3 4
0
10
20
30
40
50
60
N-?gram SVM SVM ?+ ?N-?gram SVM ?intersect ?N-?gram
Incorrect ?expansion ?percentage ?(FP)
C
o
r
r
e
c
t
 ?
e
x
p
a
n
s
i
o
n
 ?
p
e
r
c
e
n
t
a
g
e
 ?
(
T
P
)
F
i
g
u
r
e
1
:
R
O
C
c
u
r
v
e
p
l
o
t
t
i
n
g
t
r
u
e
p
o
s
i
t
i
v
e
(
c
o
r
r
e
c
t
e
x
p
a
n
-
s
i
o
n
)
p
e
r
c
e
n
t
a
g
e
s
v
e
r
s
u
s
f
a
l
s
e
p
o
s
i
t
i
v
e
(
i
n
c
o
r
r
e
c
t
e
x
p
a
n
s
i
o
n
)
p
e
r
c
e
n
t
a
g
e
s
f
o
r
s
e
v
e
r
a
l
s
y
s
t
e
m
s
o
n
t
h
e
d
e
v
e
l
o
p
m
e
n
t
s
e
t
.
a
t
t
h
e
S
V
M
?
s
d
e
c
i
s
i
o
n
t
h
r
e
s
h
o
l
d
c
o
r
r
e
s
p
o
n
d
i
n
g
t
o
a
r
o
u
n
d
3
.
3
%
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
.
T
h
e
s
i
m
p
l
e
c
o
m
-
b
i
n
a
t
i
o
n
o
f
t
h
e
i
r
s
c
o
r
e
s
a
c
h
i
e
v
e
s
s
t
r
o
n
g
i
m
p
r
o
v
e
-
m
e
n
t
s
o
v
e
r
e
i
t
h
e
r
m
o
d
e
l
,
w
i
t
h
a
n
o
p
e
r
a
t
i
n
g
p
o
i
n
t
a
s
s
o
c
i
a
t
e
d
w
i
t
h
t
h
e
S
V
M
d
e
c
i
s
i
o
n
b
o
u
n
d
a
r
y
t
h
a
t
y
i
e
l
d
s
a
c
o
u
p
l
e
o
f
p
o
i
n
t
s
i
m
p
r
o
v
e
m
e
n
t
i
n
t
r
u
e
p
o
s
-
i
t
i
v
e
s
a
n
d
a
f
u
l
l
1
%
r
e
d
u
c
t
i
o
n
i
n
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
.
O
n
e
s
i
m
p
l
e
w
a
y
t
o
c
o
m
b
i
n
e
t
h
e
s
e
t
w
o
s
y
s
t
e
m
o
u
t
p
u
t
s
i
n
a
w
a
y
t
h
a
t
d
o
e
s
n
o
t
r
e
q
u
i
r
e
t
u
n
i
n
g
a
d
e
-
c
i
s
i
o
n
t
h
r
e
s
h
o
l
d
i
s
t
o
e
x
p
a
n
d
t
h
e
a
b
b
r
e
v
i
a
t
i
o
n
i
f
a
n
d
o
n
l
y
i
f
(
1
)
b
o
t
h
t
h
e
S
V
M
m
o
d
e
l
a
n
d
t
h
e
N
-
g
r
a
m
m
o
d
e
l
a
g
r
e
e
o
n
t
h
e
b
e
s
t
e
x
p
a
n
s
i
o
n
;
a
n
d
(
2
)
t
h
e
S
V
M
m
o
d
e
l
s
c
o
r
e
i
s
g
r
e
a
t
e
r
t
h
a
n
z
e
r
o
.
I
n
a
s
l
i
g
h
t
a
b
u
s
e
o
f
t
h
e
t
e
r
m
?
i
n
t
e
r
s
e
c
t
i
o
n
?
,
w
e
c
a
l
l
t
h
i
s
c
o
m
b
i
n
a
t
i
o
n
?
S
V
M
i
n
t
e
r
s
e
c
t
N
-
g
r
a
m
?
(
o
r
?
S
V
M
\
N
-
g
r
a
m
?
i
n
T
a
b
l
e
2
)
.
U
s
i
n
g
t
h
i
s
a
p
p
r
o
a
c
h
,
o
u
r
t
r
u
e
p
o
s
i
t
i
v
e
r
a
t
e
o
n
t
h
e
d
e
v
s
e
t
d
e
c
l
i
n
e
s
a
b
i
t
t
o
j
u
s
t
o
v
e
r
5
0
%
,
b
u
t
o
u
r
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
d
e
c
l
i
n
e
s
o
v
e
r
t
w
o
f
u
l
l
p
e
r
c
e
n
t
a
g
e
p
o
i
n
t
s
t
o
1
.
1
%
,
y
i
e
l
d
i
n
g
a
v
e
r
y
h
i
g
h
p
r
e
c
i
s
i
o
n
s
y
s
t
e
m
.
T
a
k
i
n
g
t
h
i
s
v
e
r
y
h
i
g
h
p
r
e
c
i
s
i
o
n
s
y
s
t
e
m
c
o
m
b
i
-
n
a
t
i
o
n
o
f
t
h
e
N
-
g
r
a
m
a
n
d
S
V
M
m
o
d
e
l
s
,
w
e
t
h
e
n
c
o
m
b
i
n
e
w
i
t
h
t
h
e
b
a
s
e
l
i
n
e
T
T
S
s
y
s
t
e
m
a
s
f
o
l
l
o
w
s
.
F
i
r
s
t
w
e
a
p
p
l
y
o
u
r
s
y
s
t
e
m
,
a
n
d
e
x
p
a
n
d
t
h
e
i
t
e
m
i
f
i
t
s
c
o
r
e
s
a
b
o
v
e
t
h
r
e
s
h
o
l
d
;
f
o
r
t
h
o
s
e
i
t
e
m
s
l
e
f
t
u
n
-
e
x
p
a
n
d
e
d
,
w
e
l
e
t
t
h
e
T
T
S
s
y
s
t
e
m
p
r
o
c
e
s
s
i
t
i
n
i
t
s
o
w
n
w
a
y
.
I
n
t
h
i
s
w
a
y
,
w
e
a
c
t
u
a
l
l
y
r
e
d
u
c
e
t
h
e
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
o
n
t
h
e
d
e
v
s
e
t
o
v
e
r
t
h
e
b
a
s
e
l
i
n
e
T
T
S
s
y
s
t
e
m
b
y
o
v
e
r
1
%
a
b
s
o
l
u
t
e
t
o
l
e
s
s
t
h
a
n
2
%
,
w
h
i
l
e
a
l
s
o
i
n
c
r
e
a
s
i
n
g
t
h
e
t
r
u
e
p
o
s
i
t
i
v
e
r
a
t
e
t
o
7
3
.
5
%
,
a
n
i
n
c
r
e
a
s
e
o
f
1
8
.
5
%
a
b
s
o
l
u
t
e
.
O
f
c
o
u
r
s
e
,
a
t
t
e
s
t
t
i
m
e
,
w
e
w
i
l
l
n
o
t
k
n
o
w
w
h
e
t
h
e
r
a
n
O
O
V
i
s
a
n
a
b
b
r
e
v
i
a
t
i
o
n
o
r
n
o
t
,
s
o
w
e
a
l
s
o
l
o
o
k
e
d
a
t
t
h
e
p
e
r
f
o
r
m
a
n
c
e
o
n
t
h
e
r
e
s
t
o
f
t
h
e
c
o
l
l
e
c
t
e
d
d
a
t
a
,
t
o
s
e
e
h
o
w
o
f
t
e
n
i
t
e
r
r
o
-
n
e
o
u
s
l
y
s
u
g
g
e
s
t
s
a
n
e
x
p
a
n
s
i
o
n
f
r
o
m
t
h
a
t
s
e
t
.
O
f
t
h
e
1
1
,
1
5
7
e
x
a
m
p
l
e
s
t
h
a
t
w
e
r
e
h
a
n
d
-
l
a
b
e
l
e
d
a
s
n
o
n
-
a
b
b
r
e
v
i
a
t
i
o
n
s
,
o
u
r
S
V
M
\
N
-
g
r
a
m
s
y
s
t
e
m
e
x
-
p
a
n
d
e
d
4
5
i
t
e
m
s
,
w
h
i
c
h
i
s
a
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
o
f
0
.
4
%
u
n
d
e
r
t
h
e
a
s
s
u
m
p
t
i
o
n
t
h
a
t
n
o
n
e
o
f
t
h
e
m
s
h
o
u
l
d
b
e
e
x
p
a
n
d
e
d
.
I
n
f
a
c
t
,
m
a
n
u
a
l
i
n
s
p
e
c
t
i
o
n
f
o
u
n
d
t
h
a
t
2
0
%
o
f
t
h
e
s
e
w
e
r
e
c
o
r
r
e
c
t
e
x
p
a
n
s
i
o
n
s
o
f
a
b
b
r
e
v
i
a
t
i
o
n
s
t
h
a
t
h
a
d
b
e
e
n
m
i
s
-
l
a
b
e
l
e
d
.
D
u
r
i
n
g
s
y
s
t
e
m
d
e
v
e
l
o
p
m
e
n
t
,
w
e
a
l
s
o
e
x
p
e
r
i
-
m
e
n
t
e
d
w
i
t
h
a
n
u
m
b
e
r
o
f
a
l
t
e
r
n
a
t
i
v
e
h
i
g
h
p
r
e
c
i
-
s
i
o
n
a
p
p
r
o
a
c
h
e
s
t
h
a
t
s
p
a
c
e
p
r
e
c
l
u
d
e
s
o
u
r
p
r
e
s
e
n
t
-
i
n
g
i
n
d
e
t
a
i
l
h
e
r
e
,
i
n
c
l
u
d
i
n
g
:
p
r
u
n
i
n
g
t
h
e
n
u
m
-
b
e
r
o
f
e
x
p
a
n
s
i
o
n
c
a
n
d
i
d
a
t
e
s
b
a
s
e
d
o
n
t
h
e
p
a
i
r
l
a
n
-
g
u
a
g
e
m
o
d
e
l
s
c
o
r
e
;
o
n
l
y
a
l
l
o
w
i
n
g
a
b
b
r
e
v
i
a
t
i
o
n
e
x
-
p
a
n
s
i
o
n
w
h
e
n
a
t
l
e
a
s
t
o
n
e
e
x
t
r
a
c
t
e
d
n
-
g
r
a
m
c
o
n
-
t
e
x
t
i
s
p
r
e
s
e
n
t
f
o
r
t
h
a
t
e
x
p
a
n
s
i
o
n
i
n
t
h
a
t
c
o
n
t
e
x
t
;
a
n
d
C
A
R
T
t
r
e
e
(
B
r
e
i
m
a
n
e
t
a
l
.
,
1
9
8
4
)
t
r
a
i
n
i
n
g
w
i
t
h
r
e
a
l
v
a
l
u
e
d
s
c
o
r
e
s
.
S
o
m
e
o
f
t
h
e
s
e
y
i
e
l
d
e
d
v
e
r
y
h
i
g
h
p
r
e
c
i
s
i
o
n
s
y
s
t
e
m
s
,
t
h
o
u
g
h
a
t
t
h
e
c
o
s
t
o
f
l
e
a
v
i
n
g
m
a
n
y
m
o
r
e
a
b
b
r
e
v
i
a
t
i
o
n
s
u
n
e
x
p
a
n
d
e
d
.
W
e
f
o
u
n
d
t
h
a
t
,
f
o
r
u
s
e
i
n
c
o
m
b
i
n
a
t
i
o
n
w
i
t
h
t
h
e
b
a
s
e
-
l
i
n
e
T
T
S
s
y
s
t
e
m
,
l
a
r
g
e
o
v
e
r
a
l
l
r
e
d
u
c
t
i
o
n
s
i
n
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
w
e
r
e
a
c
h
i
e
v
e
d
b
y
u
s
i
n
g
a
n
i
n
i
t
i
a
l
s
y
s
-
t
e
m
w
i
t
h
s
u
b
s
t
a
n
t
i
a
l
l
y
h
i
g
h
e
r
T
P
a
n
d
s
o
m
e
w
h
a
t
h
i
g
h
e
r
F
P
r
a
t
e
s
,
s
i
n
c
e
f
a
r
f
e
w
e
r
a
b
b
r
e
v
i
a
t
i
o
n
s
w
e
r
e
t
h
e
n
p
a
s
s
e
d
a
l
o
n
g
u
n
e
x
p
a
n
d
e
d
t
o
t
h
e
b
a
s
e
l
i
n
e
s
y
s
-
t
e
m
,
w
i
t
h
i
t
s
r
e
l
a
t
i
v
e
l
y
h
i
g
h
3
%
F
P
r
a
t
e
.
T
o
e
n
s
u
r
e
t
h
a
t
w
e
h
a
d
n
o
t
o
v
e
r
-
t
u
n
e
d
o
u
r
s
y
s
-
t
e
m
s
t
o
t
h
e
d
e
v
s
e
t
t
h
r
o
u
g
h
e
x
p
e
r
i
m
e
n
t
a
t
i
o
n
,
w
e
p
e
r
f
o
r
m
e
d
1
0
-
f
o
l
d
c
r
o
s
s
v
a
l
i
d
a
t
i
o
n
o
v
e
r
t
h
e
f
u
l
l
s
e
t
o
f
a
b
b
r
e
v
i
a
t
i
o
n
s
,
a
n
d
t
h
e
r
e
s
u
l
t
s
a
r
e
p
r
e
s
e
n
t
e
d
i
n
T
a
b
l
e
2
.
M
o
s
t
n
o
t
a
b
l
y
,
t
h
e
T
T
S
b
a
s
e
l
i
n
e
s
y
s
t
e
m
h
a
s
a
m
u
c
h
l
o
w
e
r
t
r
u
e
p
o
s
i
t
i
v
e
r
a
t
e
;
y
e
t
w
e
fi
n
d
o
u
r
s
y
s
t
e
m
s
a
c
h
i
e
v
e
p
e
r
f
o
r
m
a
n
c
e
v
e
r
y
c
l
o
s
e
t
o
t
h
a
t
f
o
r
t
h
e
d
e
v
e
l
o
p
m
e
n
t
s
e
t
,
s
o
t
h
a
t
o
u
r
fi
n
a
l
c
o
m
b
i
n
a
t
i
o
n
w
i
t
h
t
h
e
T
T
S
b
a
s
e
l
i
n
e
w
a
s
a
c
t
u
a
l
l
y
s
l
i
g
h
l
y
b
e
t
t
e
r
t
h
a
n
t
h
e
n
u
m
b
e
r
s
o
n
t
h
e
d
e
v
e
l
o
p
m
e
n
t
s
e
t
.
5
C
o
n
c
l
u
s
i
o
n
s
N
o
t
e
s
f
o
r
i
n
t
e
r
n
a
l
r
e
v
i
e
w
e
r
s
?
M
a
y
b
e
m
o
r
e
t
e
c
h
n
i
c
a
l
e
x
p
l
a
n
a
t
i
o
n
o
f
l
o
g
l
i
k
e
l
i
h
o
o
d
a
n
d
l
o
g
o
d
d
s
s
c
o
r
e
s
?
R
e
v
a
m
p
e
d
i
n
t
r
o
,
b
a
c
k
g
r
o
u
n
d
,
c
o
n
c
l
u
s
i
o
n
a
n
d
e
x
p
a
n
d
e
d
r
e
f
s
.
?
A
n
o
n
y
m
i
z
a
t
i
o
n
f
o
r
s
u
b
m
i
s
s
i
o
n
.
Incorrect expansion percentage (FP)
Figure 1: ROC curve plotting true positive (correct expan-
sion) percentages versus false positive (incorrect expansion)
percentages for several systems on the development set.
positive rate falls between 1 and 3 percent, though
both systems reach roughly the same performance
at the SVM?s decision threshold corresponding to
around 3.3% false positive rate. The simple com-
bination of their scores achieves strong improve-
ments over either model, with an operating point
associated with the SVM decision boundary that
yields a couple of points improvement in true pos-
itives and a full 1% reduction in false positive rate.
One simple way to combine these two system
outputs in a way that does not require tuning a de-
cision threshold is to expand the abbreviation if
and only if (1) both the SVM model and the N-
gram model agree on the best expansion; and (2)
the SVM model score is greater than zero. In a
slight abuse of the term ?intersection?, we call this
combination ?SVM intersect N-gram? (or ?SVM
? N-gram? in Table 2). Using this approach, our
true positive rate on the development set declines
a bit to just over 50%, but our false positive rate
declines over two full percentage points to 1.1%,
yielding a very high precision system.
Taking this very high precision system combi-
nation of the N-gram and SVM models, we then
combine with the baseline TTS system as follows.
First we apply our system, and expand the item if
it scores above threshold; for those items left un-
expanded, we let the TTS system process it in its
own way. In this way, we actually reduce the false
positive rate on the development set over the base-
line TTS system by over 1% absolute to less than
2%, while also increasing the true positive rate to
73.5%, an increase of 18.5% absolute.
Of course, at test time, we will not know
whether an OOV is an abbreviation or not, so
we also looked at the performance on the rest
of the collected data, to see how often it erro-
neously suggests an expansion from that set. Of
the 11,157 examples that were hand-labeled as
non-abbreviations, our SVM ?N-gram system ex-
panded 45 i ems, which is a false positive rate
of 0.4% under the assumption that none of them
should be expanded. In fact, m nual inspection
found that 20% of these were correct expansions
of abbreviations that had been mis-labeled.
We also experimented with a umber of alter-
native hig precision approaches tha space p e-
cludes our presenting in detail here, including:
pruning the number of expansion candidates based
on the pair LM score; only allowing abbreviation
expansion when at least one extracted n-gram con-
text is present for that expansion in that context;
and CART tree (Breiman et al, 1984) training
with real valued scores. Some of these yielded
very high precision systems, though at the cost
of leaving many more abbreviations unexpanded.
We found that, for use in combination with the
baseline TTS system, large overall reductions in
FP rate were achieved by using an initial system
with substantially higher TP and somewhat higher
FP rates, since far fewer abbreviations were then
passed along unexpanded to the baseline system,
with its relatively high 3% FP rate.
To ensure that we did not overtune our systems
to the development set through experimentation,
we performed 10-fold cross validation over the full
set of abbreviations. These results are presented
in Table 2. Most notably, the TTS baseline system
has a much lower true positive rate; yet we find our
systems achieve performance very close to that for
the development set, so that our final combination
with the TTS baseline was actually slighly better
than the numbers on the development set.
5 Conclusions
In this paper we have presented methods for high
precision abbreviation expansion for a TTS appli-
cation. The methods are largely self-organizing,
using in-domain unannotated data, and depend on
only a small amount of annotated data. Since the
SVM features relate to general properties of ab-
breviations, expansions and contexts, the classi-
fier parameters will likely carry over to new (En-
glish) domains. We demonstrate that in combi-
nation with a hand-built TTS baseline, the meth-
ods afford dramatic improvement in the TP rate
(to about 74% from a starting point of about 40%)
and a reduction of FP to below our goal of 2%.
Acknowledgments
We would like to thank Daan van Esch and the
Google Speech Data Operations team for their
work on preparing the annotated data. We also
thank the reviewers for their comments.
368
References
Alan Agresti. 2002. Categorical data analysis. John
Wiley & Sons, 2nd edition.
Ai Ti Aw and Lian Hau Lee. 2012. Personalized nor-
malization for a multilingual chat system. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 31?36, Jeju Island, Korea, July. Association
for Computational Linguistics.
Richard Beaufort, Sophie Roekhaut, Louise-Am?elie
Cougnon, and C?edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing SMS messages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 770?779, Uppsala, Sweden, July.
Association for Computational Linguistics.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and Re-
gression Trees. Wadsworth & Brooks, Pacific Grove
CA.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Sudesha
Sarkar, and Anupam Basu. 2007. Investigation and
modeling of the structure of texting language. Int. J.
Doc. Anal. Recognit., 10:157?174.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1577?
1586.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
Max Kaufmann. 2010. Syntactic normalization of
Twitter messages. In International Conference on
NLP.
Catherine Kobus, Franc?ois Yvon, and G?eraldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 441?448, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? Nor-
malizing text messages without pre-categorization
nor supervision. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 71?
76, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A
broad-coverage normalization system for social me-
dia language. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1035?
1044, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012b. Joint
inference of named entity recognition and nor-
malization for tweets. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
526?535, Jeju Island, Korea, July. Association for
Computational Linguistics.
Burt L Monroe, Michael P Colaresi, and Kevin M
Quinn. 2008. Fightin?words: Lexical feature se-
lection and evaluation for identifying the content of
political conflict. Political Analysis, 16(4):372?403.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP. Papers/pennell-
liu3.pdf.
Brian Roark, Michael Riley, Cyril Allauzen, Terry Tai,
and Richard Sproat. 2012. The OpenGrm open-
source finite-state grammar software libraries. In
ACL, Jeju Island, Korea.
Richard Sproat, Alan Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
Amber Wilcox-O?Hearn, Graeme Hirst, and Alexander
Budanitsky. 2008. Real-word spelling correction
with trigrams: A reconsideration of the Mays, Dam-
erau, and Mercer model. In CICLing 2008, volume
4919 of LNCS, pages 605?616, Berlin. Springer.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006.
A phonetic-based approach to Chinese chat text nor-
malization. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 993?1000, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 61?72.
369
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 797?802,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Transforming trees into hedges and parsing with ?hedgebank? grammars
Mahsa Yarmohammadi
?
, Aaron Dunlop
?
and Brian Roark
?
?
Oregon Health & Science University, Portland, Oregon
?
Google, Inc., New York
yarmoham@ohsu.edu, {aaron.dunlop,roarkbr}@gmail.com
Abstract
Finite-state chunking and tagging meth-
ods are very fast for annotating non-
hierarchical syntactic information, and are
often applied in applications that do not
require full syntactic analyses. Scenar-
ios such as incremental machine transla-
tion may benefit from some degree of hier-
archical syntactic analysis without requir-
ing fully connected parses. We introduce
hedge parsing as an approach to recover-
ing constituents of length up to some max-
imum span L. This approach improves ef-
ficiency by bounding constituent size, and
allows for efficient segmentation strategies
prior to parsing. Unlike shallow parsing
methods, hedge parsing yields internal hi-
erarchical structure of phrases within its
span bound. We present the approach and
some initial experiments on different infer-
ence strategies.
1 Introduction
Parsing full hierarchical syntactic structures is
costly, and some NLP applications that could ben-
efit from parses instead substitute shallow prox-
ies such as NP chunks. Models to derive such
non-hierarchical annotations are finite-state, so in-
ference is very fast. Still, these partial annota-
tions omit all but the most basic syntactic segmen-
tation, ignoring the abundant local structure that
could be of utility even in the absence of fully con-
nected structures. For example, in incremental (si-
multaneous) machine translation (Yarmohammadi
et al, 2013), sub-sentential segments are trans-
lated independently and sequentially, hence the
fully-connected syntactic structure is not generally
available. Even so, locally-connected source lan-
guage parse structures can inform both segmen-
tation and translation of each segment in such a
translation scenario.
One way to provide local hierarchical syntactic
structures without fully connected trees is to fo-
cus on providing full hierarchical annotations for
structures within a local window, ignoring global
constituents outside that window. We follow the
XML community in naming structures of this type
hedges (not to be confused with the rhetorical de-
vice of the same name), due to the fact that they are
like smaller versions of trees which occur in se-
quences. Such structures may be of utility to var-
ious structured inference tasks, as well as within
a full parsing pipeline, to quickly constrain subse-
quent inference, much as finite-state models such
as supertagging (Bangalore and Joshi, 1999) or
chart cell constraints (Roark and Hollingshead,
2008; Roark et al, 2012) are used.
In this paper, we consider the problem of hedge
parsing, i.e., discovering every constituent of
length up to some span L. Similar constraints
have been used in dependency parsing (Eisner
and Smith, 2005; Dreyer et al, 2006), where the
use of hard constraints on the distance between
heads and dependents is known as vine parsing.
It is also reminiscent of so-called Semi-Markov
models (Sarawagi and Cohen, 2004), which allow
finite-state models to reason about segments rather
than just tags by imposing segment length limits.
In the XML community, trees and hedges are used
for models of XML document instances and for
the contents of elements (Br?uggemann-Klein and
Wood, 2004). As far as we know, this paper is
the first to consider this sort of partial parsing ap-
proach for natural language.
We pursue this topic via tree transformation,
whereby non-root non-terminals labeling con-
stituents of span > L in the tree are recursively
elided and their children promoted to attach to
their parent. In such a way, hedges are sequen-
tially connected to the top-most non-terminal in
the tree, as demonstrated in Figure 1. After apply-
ing such a transform to a treebank, we can induce
grammars and modify parsing to search as needed
to recover just these constituents.
In this paper, we propose several methods to
797
a)
33
,1 13
116
LQYHVWRUV
IRU
$'-3
--
WUHDFKHURXV
9%
UHPDLQ
93
0'
ZLOO
93
'7 -- 11
WKH KLJK\LHOG PDUNHW
13
,1
RI
33
--
PXFK
13
13
116
$QDO\VWV
13
9%3
DUH
--
FRQFHUQHG
,1
WKDW


6
93
6
$'-3
6%$5
b)
33
,1 13
116
LQYHVWRUV
IRU
$'-3
--
WUHDFKHURXV
9%
UHPDLQ
93
0'
ZLOO
93
'7 -- 11
WKH KLJK\LHOG PDUNHW
13
,1
RI
33
--
PXFK
13
13
116
$QDO\VWV
13 9%3
DUH
--
FRQFHUQHG
,1
WKDW


6
Figure 1: a) Full parse tree, b) Hedge parse tree with maximum constituent span of 7 (L = 7).
parse hedge constituents and examine their accu-
racy/efficiency tradeoffs. This is compared with
a baseline of parsing with a typically induced
context-free grammar and transforming the result
via the hedge transform, which provides a ceiling
on accuracy and a floor on efficiency. We inves-
tigate pre-segmenting the sentences with a finite-
state model prior to hedge parsing, and achieve
large speedups relative to hedge parsing the whole
string, though at a loss in accuracy due to cas-
cading segmentation errors. In all cases, we find
it crucial that our ?hedgebank? grammars be re-
trained to match the conditions during inference.
2 Methods
In this section, we present the details of our ap-
proach. First, we present the simple tree transform
from a full treebank parse tree to a (root attached)
sequence of hedges. Next, we discuss modifica-
tions to inference and the resulting computational
complexity gains. Finally, we discuss segmenting
to further reduce computational complexity.
2.1 Hedge Tree Transform
The hedge tree transform converts the original
parse tree into a hedge parse tree. In the resulting
hedge parse tree, every child of the top-most node
spans at most L words. To transform an original
tree to a hedge tree, we remove every non-terminal
with span larger than L and attach its children to its
parent. We label span length on each node by re-
cursively summing the span lengths of each node?s
children, with terminal items by definition having
span 1. A second top-down pass evaluates each
node before evaluating its children, and removes
nodes spanning > L words. For example, the span
of the non-root S, SBAR, ADJP, and VP nodes in
Figure 1(a) have spans between 10 and 13, hence
are removed in the tree in Figure 1(b).
If we apply this transform to an entire tree-
bank, we can use the transformed trees to induce
a PCFG for parsing. Figure 2 plots the percentage
of constituents from the original WSJ Penn tree-
bank (sections 2-21) retained in the transformed
version, as we vary the maximum span length pa-
rameter L. Over half of constituents have span 3 or
less (which includes frequent base noun phrases);
L = 7 covers approximately three quarters of the
original constituents, and L = 15 over 90%. Most
experiments in this paper will focus on L = 7,
which is short enough to provide a large speedup
yet still cover a large fraction of constituents.
2.2 Hedge Parsing
As stated earlier, our brute-force baseline ap-
proach is to parse the sentence using a full context-
free grammar (CFG) and then hedge-transform the
result. This method should yield a ceiling on
798
P
c
t
.
o
f
c
o
n
s
t
i
t
u
e
n
t
s
r
e
t
a
i
n
e
d
0 5 10 15 20
50
60
70
80
90
100
Maximum ?span ?size ?(L)
P
e
r
c
e
n
t
a
g
e
 ?
o
f
 ?
c
o
n
s
t
i
t
u
e
n
t
s
 ?
r
e
t
a
i
n
e
d
c
e
s
s
t
o
c
o
m
p
a
r
e
d
w
i
t
h
g
r
a
m
m
a
r
s
t
r
a
i
n
e
d
o
n
t
r
a
n
s
-
f
o
r
m
e
d
t
r
e
e
s
;
b
u
t
i
t
w
i
l
l
b
e
s
l
o
w
e
r
t
o
p
a
r
s
e
.
W
e
a
i
m
t
o
d
r
a
m
a
t
i
c
a
l
l
y
i
m
p
r
o
v
e
e
f
fi
c
i
e
n
c
y
u
p
o
n
t
h
i
s
b
a
s
e
l
i
n
e
w
h
i
l
e
l
o
s
i
n
g
a
s
l
i
t
t
l
e
a
c
c
u
r
a
c
y
a
s
p
o
s
s
i
b
l
e
.
S
i
n
c
e
w
e
l
i
m
i
t
t
h
e
s
p
a
n
o
f
n
o
n
-
t
e
r
m
i
n
a
l
l
a
-
b
e
l
s
,
w
e
c
a
n
c
o
n
s
t
r
a
i
n
t
h
e
s
e
a
r
c
h
p
e
r
f
o
r
m
e
d
b
y
t
h
e
p
a
r
s
e
r
,
g
r
e
a
t
l
y
r
e
d
u
c
e
t
h
e
C
Y
K
p
r
o
c
e
s
s
i
n
g
t
i
m
e
.
I
n
e
s
s
e
n
c
e
,
w
e
p
e
r
f
o
r
m
n
o
w
o
r
k
i
n
c
h
a
r
t
c
e
l
l
s
s
p
a
n
-
n
i
n
g
m
o
r
e
t
h
a
n
L
w
o
r
d
s
,
e
x
c
e
p
t
f
o
r
t
h
e
c
e
l
l
s
a
l
o
n
g
t
h
e
p
e
r
i
p
h
e
r
y
o
f
t
h
e
c
h
a
r
t
,
w
h
i
c
h
a
r
e
j
u
s
t
u
s
e
d
t
o
c
o
n
n
e
c
t
t
h
e
h
e
d
g
e
s
t
o
t
h
e
r
o
o
t
.
C
o
n
s
i
d
e
r
t
h
e
fl
a
t
t
r
e
e
i
n
F
i
g
u
r
e
1
.
F
o
r
u
s
e
b
y
a
C
Y
K
p
a
r
s
i
n
g
a
l
-
g
o
r
i
t
h
m
,
t
r
e
e
s
a
r
e
b
i
n
a
r
i
z
e
d
p
r
i
o
r
t
o
g
r
a
m
m
a
r
i
n
-
d
u
c
t
i
o
n
,
r
e
s
u
l
t
i
n
g
i
n
s
p
e
c
i
a
l
n
o
n
-
t
e
r
m
i
n
a
l
s
c
r
e
a
t
e
d
b
y
b
i
n
a
r
i
z
a
t
i
o
n
.
O
t
h
e
r
t
h
a
n
t
h
e
s
y
m
b
o
l
a
t
t
h
e
r
o
o
t
o
f
t
h
e
t
r
e
e
,
t
h
e
o
n
l
y
c
o
n
s
t
i
t
u
e
n
t
s
w
i
t
h
s
p
a
n
l
e
n
g
t
h
g
r
e
a
t
e
r
t
h
a
n
L
i
n
t
h
e
b
i
n
a
r
i
z
e
d
t
r
e
e
w
i
l
l
b
e
l
a
b
e
l
e
d
w
i
t
h
t
h
e
s
e
s
p
e
c
i
a
l
b
i
n
a
r
i
z
a
t
i
o
n
n
o
n
-
t
e
r
m
i
n
a
l
s
.
F
u
r
-
t
h
e
r
,
i
f
t
h
e
b
i
n
a
r
i
z
a
t
i
o
n
s
y
s
t
e
m
a
t
i
c
a
l
l
y
g
r
o
u
p
s
t
h
e
l
e
f
t
m
o
s
t
o
r
t
h
e
r
i
g
h
t
m
o
s
t
c
h
i
l
d
r
e
n
u
n
d
e
r
t
h
e
s
e
n
e
w
n
o
n
-
t
e
r
m
i
n
a
l
s
(
t
h
e
m
o
s
t
c
o
m
m
o
n
s
t
r
a
t
e
g
y
)
,
t
h
e
n
c
o
n
s
t
i
t
u
e
n
t
s
w
i
t
h
s
p
a
n
g
r
e
a
t
e
r
t
h
a
n
L
w
i
l
l
e
i
t
h
e
r
b
e
g
i
n
a
t
t
h
e
fi
r
s
t
w
o
r
d
(
l
e
f
t
m
o
s
t
g
r
o
u
p
i
n
g
)
o
r
e
n
d
a
t
t
h
e
l
a
s
t
w
o
r
d
(
r
i
g
h
t
m
o
s
t
g
r
o
u
p
i
n
g
)
,
f
u
r
t
h
e
r
c
o
n
-
s
t
r
a
i
n
i
n
g
t
h
e
n
u
m
b
e
r
o
f
c
e
l
l
s
i
n
t
h
e
c
h
a
r
t
r
e
q
u
i
r
i
n
g
w
o
r
k
.
C
o
m
p
l
e
x
i
t
y
o
f
p
a
r
s
i
n
g
w
i
t
h
a
f
u
l
l
C
Y
K
p
a
r
s
e
r
i
s
O
(
n
3
|
G
|
)
w
h
e
r
e
n
i
s
t
h
e
l
e
n
g
t
h
o
f
i
n
p
u
t
a
n
d
|
G
|
i
s
t
h
e
g
r
a
m
m
a
r
s
i
z
e
c
o
n
s
t
a
n
t
.
I
n
c
o
n
t
r
a
s
t
,
c
o
m
p
l
e
x
-
i
t
y
o
f
p
a
r
s
i
n
g
w
i
t
h
a
h
e
d
g
e
c
o
n
s
t
r
a
i
n
e
d
C
Y
K
i
s
r
e
-
d
u
c
e
d
t
o
O
(
(
n
L
2
+
n
2
)
|
G
|
)
.
T
o
s
e
e
t
h
a
t
t
h
i
s
i
s
t
h
e
c
a
s
e
,
c
o
n
s
i
d
e
r
t
h
a
t
t
h
e
r
e
a
r
e
O
(
n
L
)
c
e
l
l
s
o
f
s
p
a
n
L
o
r
l
e
s
s
,
a
n
d
e
a
c
h
h
a
s
a
m
a
x
i
m
u
m
o
f
L
m
i
d
p
o
i
n
t
s
,
w
h
i
c
h
a
c
c
o
u
n
t
s
f
o
r
t
h
e
fi
r
s
t
t
e
r
m
.
B
e
y
o
n
d
t
h
e
s
e
,
t
h
e
r
e
a
r
e
O
(
n
)
r
e
m
a
i
n
i
n
g
a
c
t
i
v
e
c
e
l
l
s
w
i
t
h
O
(
n
)
p
o
s
s
i
b
l
e
m
i
d
p
o
i
n
t
s
,
w
h
i
c
h
a
c
c
o
u
n
t
s
f
o
r
t
h
e
s
e
c
o
n
d
t
e
r
m
.
N
o
t
e
,
h
o
w
e
v
e
r
,
t
h
a
t
t
h
e
w
o
r
k
i
n
t
h
e
s
e
l
a
t
t
e
r
c
e
l
l
s
m
a
y
b
e
l
e
s
s
,
s
i
n
c
e
t
h
e
s
e
t
o
f
p
o
s
s
i
b
l
e
n
o
n
-
t
e
r
m
i
n
a
l
s
i
s
r
e
d
u
c
e
d
.
I
t
i
s
p
o
s
s
i
b
l
e
t
o
p
a
r
s
e
w
i
t
h
a
s
t
a
n
d
a
r
d
l
y
i
n
d
u
c
e
d
P
C
F
G
u
s
i
n
g
t
h
i
s
s
o
r
t
o
f
h
e
d
g
e
c
o
n
s
t
r
a
i
n
e
d
p
a
r
s
-
i
n
g
t
h
a
t
o
n
l
y
c
o
n
s
i
d
e
r
s
a
s
u
b
s
e
t
o
f
t
h
e
c
h
a
r
t
c
e
l
l
s
,
a
n
d
s
p
e
e
d
u
p
s
a
r
e
a
c
h
i
e
v
e
d
,
h
o
w
e
v
e
r
t
h
i
s
i
s
c
l
e
a
r
l
y
n
o
n
-
o
p
t
i
m
a
l
,
s
i
n
c
e
t
h
e
m
o
d
e
l
i
s
i
l
l
-
s
u
i
t
e
d
t
o
c
o
m
-
b
i
n
i
n
g
h
e
d
g
e
s
i
n
t
o
fl
a
t
s
t
r
u
c
t
u
r
e
s
a
t
t
h
e
r
o
o
t
o
f
t
h
e
t
r
e
e
.
T
h
i
s
r
e
s
u
l
t
s
i
n
d
e
g
r
a
d
a
t
i
o
n
o
f
p
a
r
s
i
n
g
p
e
r
-
f
o
r
m
a
n
c
e
b
y
t
e
n
s
o
f
p
o
i
n
t
s
o
f
F
-
m
e
a
s
u
r
e
v
e
r
s
u
s
s
t
a
n
d
a
r
d
p
a
r
s
i
n
g
.
I
n
s
t
e
a
d
,
i
n
a
l
l
s
c
e
n
a
r
i
o
s
w
h
e
r
e
a
t
h
e
c
h
a
r
t
i
s
c
o
n
s
t
r
a
i
n
e
d
t
o
s
e
a
r
c
h
f
o
r
h
e
d
g
e
s
,
w
e
l
e
a
r
n
a
g
r
a
m
m
a
r
f
r
o
m
a
h
e
d
g
e
t
r
a
n
s
f
o
r
m
e
d
t
r
e
e
-
b
a
n
k
,
m
a
t
c
h
e
d
t
o
t
h
e
m
a
x
i
m
u
m
l
e
n
g
t
h
a
l
l
o
w
e
d
b
y
t
h
e
p
a
r
s
e
r
,
w
h
i
c
h
w
e
c
a
l
l
a
h
e
d
g
e
b
a
n
k
g
r
a
m
m
a
r
.
A
h
e
d
g
e
b
a
n
k
g
r
a
m
m
a
r
i
s
a
f
u
l
l
y
f
u
n
c
t
i
o
n
a
l
P
C
F
G
a
n
d
i
t
c
a
n
b
e
u
s
e
d
w
i
t
h
a
n
y
s
t
a
n
d
a
r
d
p
a
r
s
i
n
g
a
l
-
g
o
r
i
t
h
m
,
i
.
e
.
,
t
h
e
s
e
a
r
e
n
o
t
g
e
n
e
r
a
l
l
y
fi
n
i
t
e
-
s
t
a
t
e
e
q
u
i
v
a
l
e
n
t
m
o
d
e
l
s
.
H
o
w
e
v
e
r
,
u
s
i
n
g
t
h
e
B
e
r
k
e
-
l
e
y
g
r
a
m
m
a
r
l
e
a
r
n
e
r
(
s
e
e
S
e
c
t
i
o
n
3
)
,
w
e
fi
n
d
t
h
a
t
h
e
d
g
e
b
a
n
k
g
r
a
m
m
a
r
s
a
r
e
t
y
p
i
c
a
l
l
y
s
m
a
l
l
e
r
t
h
a
n
t
r
e
e
b
a
n
k
g
r
a
m
m
a
r
s
,
a
l
s
o
c
o
n
t
r
i
b
u
t
i
n
g
t
o
p
a
r
s
i
n
g
s
p
e
e
d
u
p
v
i
a
t
h
e
g
r
a
m
m
a
r
c
o
n
s
t
a
n
t
.
A
u
n
i
q
u
e
p
r
o
p
e
r
t
y
o
f
h
e
d
g
e
c
o
n
s
t
i
t
u
e
n
t
s
c
o
m
-
p
a
r
e
d
t
o
c
o
n
s
t
i
t
u
e
n
t
s
i
n
t
h
e
o
r
i
g
i
n
a
l
p
a
r
s
e
t
r
e
e
s
i
s
t
h
a
t
t
h
e
y
a
r
e
s
e
q
u
e
n
t
i
a
l
l
y
c
o
n
n
e
c
t
e
d
t
o
t
h
e
T
O
P
n
o
d
e
.
T
h
i
s
p
r
o
p
e
r
t
y
e
n
a
b
l
e
s
u
s
t
o
c
h
u
n
k
t
h
e
s
e
n
t
e
n
c
e
i
n
t
o
s
e
g
m
e
n
t
s
t
h
a
t
c
o
r
r
e
s
p
o
n
d
t
o
c
o
m
-
p
l
e
t
e
h
e
d
g
e
s
,
a
n
d
p
a
r
s
e
t
h
e
s
e
g
m
e
n
t
s
i
n
d
e
p
e
n
-
d
e
n
t
l
y
(
a
n
d
s
i
m
u
l
t
a
n
e
o
u
s
l
y
)
i
n
s
t
e
a
d
o
f
p
a
r
s
i
n
g
t
h
e
e
n
t
i
r
e
s
e
n
t
e
n
c
e
.
I
n
s
e
c
t
i
o
n
2
.
3
,
w
e
p
r
e
s
e
n
t
o
u
r
a
p
-
p
r
o
a
c
h
t
o
h
e
d
g
e
s
e
g
m
e
n
t
a
t
i
o
n
.
N
o
t
e
t
h
a
t
p
a
r
s
i
n
g
s
e
g
m
e
n
t
s
w
i
t
h
a
g
r
a
m
m
a
r
t
r
a
i
n
e
d
o
n
w
h
o
l
e
s
t
r
i
n
g
s
Maximum span size (L)
Figure 2: Percentage of constituents retained at various
span length parameters
hedge-parsing accuracy, as it has access to rich
contextual information (as compared to grammars
trained on transformed trees). Naturally, inference
will be slow; we aim to improve efficiency upon
this baseline while minimizing accuracy loss.
Since we limit the span of non-terminal la-
bels, we can constrain the search performed by the
parser, greatly reduce the CYK processing time. In
essence, we perform no work in chart cells span-
ning more than L words, except for the cells along
the periphery of the chart, which are just used to
connect the hedges to the root. Consider the flat
tree in Figure 1(b). For use by a CYK parsing al-
gorithm, trees are binarized prior to grammar in-
duction, resulting in special non-terminals created
by binarization. Other than the symbol at the root
of the tree, the only constituents with span length
greater than L in the binarized tree will be labeled
with these special binarization non-terminals. Fur-
ther, if the binarization systematically groups the
leftmost or the rightmost children under these new
non-terminals (the most common strategy), then
constituents with span greater than L will either
begin at the first word (leftmost grouping) or end
at the last word (rightmost), further constraining
the number of cells in the chart requiring work.
Complexity of parsing with a full CYK parser is
O(n
3
|G|) where n is the length of input and |G| is
the grammar size constant. In contrast, complex-
ity of parsing with a hedge constrained CYK is re-
duced to O((nL
2
+n
2
)|G|). To see that this is the
case, consider that there are O(nL) cells of span L
or less, and each has a maximum of L midpoints,
which accounts for the first term. Beyond these,
there are O(n) remaining active cells with O(n)
possible midpoints, which accounts for the second
term. Note also that these latter cells (spanning
> L words) may be less expensive, as the set of
possible non-terminals is reduced to only those in-
troduced by binarization.
It is possible to parse with a standardly induced
PCFG using this sort of hedge constrained pars-
ing that only considers a subset of the chart cells,
and speedu s are achiev d, however this is clearly
non-optimal, since the mod l is ill-suited to com-
bining hedges into flat structures at the root of the
tree. Space constraints preclude inclusion of tri-
als with this method, b t the net result is a se-
vere degradation in accuracy (tens of points of F-
measure) versus standar parsing. Thus, we train
a grammar in a matched condition, which we call
it a hedgebank grammar. A hedgebank gram-
mar is a fully functional PCFG which is learned
from a hedge transformed treebank. A hedgebank
grammar can be used with any standard parsing
algorithm, i.e., these are not generally finite-state
equivalent models. However, using the Berke-
ley grammar learner (see ?3), we find that hedge-
bank grammars are typically smaller than tree-
bank grammars, reducing the grammar constant
and contributing to faster inference.
A unique property of hedge constituents com-
pared to constituents in the original parse trees
is that they are sequentially connected to the top-
most node. This property enables us to chunk the
sentence into segments that correspond to com-
plete hedges, and parse the segments indepen-
dently (and simultaneously) instead of parsing the
entire sentence. In section 2.3, we present our ap-
proach to hedge segmentation.
In all scenarios where the chart is constrained
to search for hedges, we learn a hedgebank gram-
mar, which is matched to the maximum length al-
lowed by the parser. In the pre-segmentation sce-
nario, we first decompose the hedge transformed
treebank into its hedge segments and then learn a
hedgebank grammar from the new corpus.
2.3 Hedge Segmentation
In this section we present our segmentation model
which takes the input sentence and chunks it into
appropriate segments for hedge parsing. We treat
this as a binary classification task which decides
if a word can begin a new hedge. We use hedge
segmentation as a finite-state pre-processing step
for hedge context-free parsing.
Our task is to learn which words can begin
(B) a hedge constituent. Given a set of labeled
pairs (S,H) where S is a sentence of n words
w
1
. . . w
n
and H is its hedge parse tree, word w
b
belongs to B if there is a hedge constituent span-
ning w
b
. . . w
e
for some e ? b and w
b
belongs to
?
B
otherwise. To predict the hedge boundaries more
accurately, we grouped consecutive unary or POS-
799
tag hedges together under a new non-terminal la-
beled G. Unlabeled segmentation tags for the
words in the example sentence in Figure 1(b) are:
?Analysts/B are/
?
B concerned/
?
B that/
?
B much/B
of/
?
B the/
?
B high-yield/
?
B market/
?
B will/B
remain/
?
B treacherous/
?
B for/
?
B investors/
?
B ./B?
In addition to the simple unlabeled segmentation
with B and
?
B tags, we try a labeled segmenta-
tion with B
C
and
?
B
C
tags where C is hedge con-
stituent type. We restrict the types to the most im-
portant types ? following the 11 chunk types an-
notated in the CoNLL-2000 chunking task (Sang
and Buchholz, 2000) ? by replacing all other types
with a new type OUT. Thus, ?Analysts? is labeled
B
G
; ?much?, B
NP
; ?will?, B
VP
and so on.
To automatically predict the class of each word
position, we train a multi-class classifier from la-
beled training data using a discriminative linear
model, learning the model parameters with the av-
eraged perceptron algorithm (Collins, 2002). We
follow Roark et al (2012) in the features they used
to label words as beginning or ending constituents.
The segmenter extracts features from word and
POS-tag input sequences and hedge-boundary tag
output sequences. The feature set includes tri-
grams of surrounding words, trigrams of surround-
ing POS tags, and hedge-boundary tags of the pre-
vious words. An additional orthographical fea-
ture set is used to tag rare
1
and unknown words.
This feature set includes prefixes and suffixes of
the words (up to 4 characters), and presence of
a hyphen, digit, or an upper-case character. Re-
ported results are for a Markov order-2 segmenter,
which includes features with the output classes of
the previous two words.
3 Experimental Results
We ran all experiments on the WSJ Penn Tree-
bank corpus (Marcus et al, 1999) using section
2-21 for training, section 24 for development, and
section 23 for testing. We performed exhaustive
CYK parsing using the BUBS parser
2
(Bodenstab
et al, 2011) with Berkeley SM6 latent-variable
grammars (Petrov and Klein, 2007) learned by the
Berkeley grammar trainer with default settings.
We compute accuracy from the 1-best Viterbi
tree extracted from the chart using the standard
EVALB script. Accuracy results are reported as
precision, recall and F1-score, the harmonic mean
between the two. In all trials, we evaluate accuracy
with respect to the hedge transformed reference
1
Rare words occur less than 5 times in the training data.
2
https://code.google.com/p/bubs-parser
Hedge Parsing Acc/Eff
Parser P R F1 w/s
Full w/full CYK 88.8 89.2 89.0 2.4
Hedgebank 87.6 84.4 86.0 25.7
Table 1: Hedge parsing results on section 24 for L = 7.
treebank, i.e., we are not penalizing the parser for
not discovering constituents longer than the max-
imum length. Segmentation accuracy is reported
as an F1-score of unlabeled segment bracketing.
We ran timing tests on an Intel 2.66GHz proces-
sor with 3MB of cache and 2GB of memory. Note
that segmentation time is negligible compared to
the parsing time, hence is omitted in reported time.
Efficiency results are reported as number of words
parsed per second (w/s).
Table 1 presents hedge parsing accuracy on
the development set for the full parsing baseline,
where the output of regular PCFG parsing is trans-
formed to hedges and evaluated, versus parsing
with a hedgebank grammar, with no segmenta-
tion of the strings. We find an order of magnitude
speedup of parsing, but at the cost of 3 percent F-
measure absolute. Note that most of that loss is
in recall, indicating that hedges predicted in that
condition are nearly as reliable as in full parsing.
Table 2 shows the results on the development
set when segmenting prior to hedge parsing. The
first row shows the result with no segmentation,
the same as the last row in Table 1 for ease of ref-
erence. The next row shows behavior with per-
fect segmentation. The final two rows show per-
formance with automatic segmentation, using a
model that includes either unlabeled or labeled
segmentation tags, as described in the last section.
Segmentation accuracy is better for the model with
labels, although overall that accuracy is rather low.
We achieve nearly another order of magnitude
speedup over hedge parsing without segmentation,
but again at the cost of nearly 5 percent F1.
Table 3 presents results of our best configura-
tions on the eval set, section 23. The results show
the same patterns as on the development set. Fi-
nally, Figure 3 shows the speed of inference, la-
Table 2: Hedge segmentation and parsing results on section
24 for L = 7.
Segmen- Seg Hedge Parsing Acc/Eff
tation F1 P R F1 w/s
None n/a 87.6 84.4 86.0 25.7
Oracle 100 91.3 88.9 90.1 188.6
Unlabeled 80.6 77.2 75.3 76.2 159.1
Labeled 83.8 83.1 79.5 81.3 195.8
800
Segmentation Grammar
Segmentation Acc Hedge Parsing Acc/Eff
P R F1 P R F1 w/s
None Full w/full CYK n/a 90.3 90.3 90.3 2.7
None Hedgebank n/a 88.3 85.3 86.8 26.2
Labeled Hedgebank 84.0 86.6 85.3 85.1 81.1 83.0 203.0
Table 3: Hedge segmentation and parsing results on test data, section 23, for L = 7.
W
o
r
d
s
p
e
r
s
e
c
o
n
d
Full ?Parsing
Hedge ?No ?Seg
Hedge ?With ?Seg
0 5 10 15 20
0
200
400
600
800
Maximum ?span ?size ?(L)
W
o
r
d
s
 ?
p
a
r
s
e
d
 ?
p
e
r
 ?
s
e
c
o
n
d
W
o
r
d
s
p
e
r
s
e
c
o
n
d
Full ?Parsing
Hedge ?No ?Seg
Hedge ?With ?Seg
0 5 10 15 20
0
200
400
600
800
Maximum ?span ?size ?(L)
W
o
r
d
s
 ?
p
a
r
s
e
d
 ?
p
e
r
 ?
s
e
c
o
n
d
H
e
d
g
e
P
r
e
c
i
s
i
o
n
0 5 10 15 20
75
80
85
90
95
Maximum ?span ?size ?(L)
H
e
d
g
e
 ?
P
r
e
c
i
s
i
o
n
H
e
d
g
e
R
e
c
a
l
l
0 5 10 15 20
75
80
85
90
95
Maximum ?span ?size ?(L)
H
e
d
g
e
 ?
R
e
c
a
l
l
Figure 4: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3  20.
Maximum span size (L) axi um span size (L)
a) b)
Figure 3: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3?20.
beled precision and labeled recall of annotating
hedge constituents on the test set as a function
of the maximum span parameter L, versus the
baseline parser. Keep in mind that the number
of reference constituents increases as L increases,
hence both precision and recall can decrease as
the parameter grows. Segmentation achieves large
speedups for smaller L values, but the accuracy
degradation is consistent, pointing to the need for
improved segmentation.
4 Conclusion and Future Work
We proposed a novel partial parsing approach for
applications that require a fast syntactic analysis
of the input beyond shallow bracketing. The span-
limit parameter allows tuning the annotation of in-
ternal structure as appropriate for the application
domain, trading off annotation complexity against
inference time. These properties make hedge pars-
ing potentially very useful for incremental text or
speech processing, such as streaming text analysis
or simultaneous translation.
One interesting characteristic of these anno-
tations is that they allow for string segmenta-
tion prior to inference, provided that the segment
boundaries do not cross any hedge boundaries. We
found that baseline segmentation models did pro-
vide a significant speedup in parsing, but that cas-
cading errors remain a problem.
There are many directions of future work to
pursue here. First, the current results are all for
exhaustive CYK parsing, and we plan to per-
form a detailed investigation of the performance
of hedgebank parsing with prioritization and prun-
ing methods of the sort available in BUBS (Bo-
denstab et al, 2011). Further, this sort of annota-
tion seems well suited to incremental parsing with
beam search, which has been shown to achieve
high accuracies even for fully connected parsing
(Zhang and Clark, 2011). Improvements to the
transform (e.g., grouping items not in hedges un-
der non-terminals) and to the segmentation model
(e.g., increasing precision at the expense of recall)
could improve accuracy without greatly reducing
efficiency. Finally, we intend to perform an ex-
trinsic evaluation of this parsing in an on-line task
such as simultaneous translation.
Acknowledgments
This work was supported in part by NSF grant
#IIS-0964102. Any opinions, findings, conclu-
sions or recommendations expressed in this pub-
lication are those of the authors and do not neces-
sarily reflect the views of the NSF.
801
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and
Brian Roark. 2011. Beam-width prediction for ef-
ficient context-free parsing. In Proceedings of the
49th Annual Meeting ACL: HLT, pages 440?449.
Anne Br?uggemann-Klein and Derick Wood. 2004.
Balanced context-free grammars, hedge grammars
and pushdown caterpillar automata. In Extreme
Markup Languages.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1?8.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking
for speed and precision. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 201?205.
Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
Proceedings of the Ninth International Workshop on
Parsing Technology (IWPT), pages 30?41.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium, Philadelphia.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceedings
of the 22nd national conference on Artificial intelli-
gence, pages 1663?1666.
Brian Roark and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 745?751.
Brian Roark, Kristy Hollingshead, and Nathan Boden-
stab. 2012. Finite-state chart constraints for reduced
complexity context-free parsing pipelines. Compu-
tational Linguistics, 38(4):719?753.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of Conference on Com-
putational Natural Language Learning (CoNLL),
pages 127?132.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 1185?1192.
Mahsa Yarmohammadi, Vivek K. Rangarajan Sridhar,
Srinivas Bangalore, and Baskaran Sankaran. 2013.
Incremental segmentation and decoding strategies
for simultaneous translation. In Proceedings of the
6th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 1032?1036.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
802
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 28?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Scanning methods and language modeling for binary switch typing
Brian Roark?, Jacques de Villiers?, Christopher Gibbons? and Melanie Fried-Oken?
?Center for Spoken Language Understanding ?Child Development & Rehabilitation Center
Oregon Health & Science University
{roark,jacques}@cslu.ogi.edu {gibbons,mfo}@ohsu.edu
Abstract
We present preliminary experiments of a
binary-switch, static-grid typing interface
making use of varying language model contri-
butions. Our motivation is to quantify the de-
gree to which language models can make the
simplest scanning interfaces ? such as show-
ing one symbol at a time rather than a scan-
ning a grid ? competitive in terms of typing
speed. We present a grid scanning method
making use of optimal Huffman binary codes,
and demonstrate the impact of higher order
language models on its performance. We also
investigate the scanning methods of highlight-
ing just one cell in a grid at any given time
or showing one symbol at a time without a
grid, and show that they yield commensurate
performance when using higher order n-gram
models, mainly due to lower error rate and a
lower rate of missed targets.
1 Introduction
Augmentative and Alternative Communication
(AAC) is a well-defined subfield of assistive tech-
nology, focused on methods that assist individuals
for whom conventional spoken or written communi-
cation approaches are difficult or impossible. Those
who cannot make use of standard keyboards for text
entry have a number of alternative text entry meth-
ods that permit typing. One of the most common of
these alternative text entry methods is the use of a
binary switch ? triggered by button-press, eye-blink
or even through event related potentials (ERP) such
as the P300 detected in EEG signals ? that allows
the individual to make a selection based on some
method for scanning through alternatives (Lesher et
al., 1998). Typing speed is a challenge, yet critically
important for usability. One common approach is
row/column scanning on a matrix of characters,
symbols or images (a ?spelling grid?), which allows
the user of a binary yes/no switch to select the row
and column of a target symbol, by simply indicating
?yes? (pressing a button or blinking an eye) when the
row or column of the target symbol is highlighted.
Figure 1 shows the 6?6 spelling grid used for the
P300 Speller (Farwell and Donchin, 1988).
For any given scanning method, the use of a bi-
nary switch to select from among a set of options
(letter, symbols, or images) amounts to the assign-
ment of binary codes to each symbol. For example,
the standard row/column scanning algorithm works
by scanning each row until a selection is made, then
scanning each column until a selection is made, and
returning the symbol at the selected row and column.
This can be formalized as follows:
1 for i = 1 to (# of rows) do
2 HIGHLIGHTROW(i)
3 if YESSWITCH
4 for j = 1 to (# of columns) do
5 HIGHLIGHTCOLUMN(j)
6 if YESSWITCH
7 return (i, j)
8 return (i, 0)
9 return (0, 0)
where the function YESSWITCH returns true if the
button is pressed (or whatever switch event counts as
a ?yes? response) within the parameterized latency.
If the function returns (0, 0) then nothing has been
selected, requiring rescanning. If the function re-
turns (i, 0) for i > 0, then row i has been selected,
but columns must be rescanned. Under this scanning
method, the binary code for the letter ?J? in the ma-
trix in Figure 1 is 010001; the letter ?T? is 000101.
The length of the binary code for a symbol is re-
M
G
 
 
A FEC
_9765
3 4Y 1Z
XWUTS
RQON
H
B
LKI
V
8
P
J
2
D
Figure 1: Spelling grid such as that used for the P300
speller (Farwell and Donchin, 1988). ? ? denotes space.
28
lated to the time required to type it. In the ma-
trix in Figure 1, the space character is in the bot-
tom right-hand corner, yielding the maximum binary
code length for that grid size (12), despite that, in
typical written English we would expect the space
character to be used about 20% of the time. A more
efficient strategy would be to place the space charac-
ter in the upper left-hand corner of the grid, leading
to the much shorter binary code ?11?.
Ordering symbols in a fixed grid so that frequent
symbols are located in the upper left-hand corner is
one method for making use of a statistical model of
the language so that likely symbols receive the short-
est codes. Such a language model, however, does
not take into account what has already been typed,
but rather assigns its code identically in all contexts.
In this paper we examine alternative fixed-grid scan-
ning methods that do take into account context in the
language models used to establish codes, i.e., the
codes in these methods vary in different contexts,
so that high probability symbols receive the short-
est codes and hence require the fewest keystrokes.
We show that n-gram language models can provide
a large improvement in typing speed.
Before presenting our methods and experimental
results, we next provide further background on alter-
native text entry methods, language modeling, and
binary coding based on language models.
2 Preliminaries and background
2.1 Alternative text entry
Of the ways in which AAC typing interfaces differ,
perhaps most relevant to the current paper is whether
the symbol positions are fixed or can move dynam-
ically, because such dynamic layouts facilitate in-
tegration of richer language models. For example,
if we re-calculate character probabilities after each
typed character, then we could re-arrange the char-
acters in the grid so that the most likely are placed
in the upper left-hand corner for row/column scan-
ning. Conventional wisdom, however, is that the
cognitive overhead of processing a different grid ar-
rangement after every character would slow down
typing more than the speedup due to the improved
binary coding (Baletsa et al, 1976; Lesher et al,
1998). The GazeTalk system (Hansen et al, 2003),
which presents the user with a 3?4 grid and captures
which cell the user?s gaze fixates upon, is an instance
of a dynamically changing grid. The cell layouts
are configurable, but typically one cell contains a set
of likely word completions; others are allocated to
space and backspace; and around half of the cells are
allocated to the most likely single character contin-
uation of the input string, based on language model
predictions. Hansen et al (2003) report that users
produced more words per minute with a static key-
board than with the predictive grid interface, illus-
trating the impact of the cognitive overhead that goes
along with this sort of scanning.
The likely word completions in the GazeTalk sys-
tem illustrates another common way in which lan-
guage modeling is integrated into AAC typing sys-
tems. Much of the language modeling research
within the context of AAC has been for word com-
pletion/prediction for keystroke reduction (Darragh
et al, 1990; Li and Hirst, 2005; Trost et al, 2005;
Trnka et al, 2006; Trnka et al, 2007; Wandmacher
and Antoine, 2007). The typical scenario for this is
allocating a region of the interface to contain a set of
suggested words that complete what the user has be-
gun typing. The expectation is to derive a keystroke
savings when the user selects one of the alternatives
rather than typing the rest of the letters. The cogni-
tive load of monitoring a list of possible completions
has made the claim that this speeds typing contro-
versial (Anson et al, 2004); yet some results have
shown this to speed typing under certain conditions
(Trnka et al, 2007).
One innovative language-model-driven AAC typ-
ing interface is Dasher (Ward et al, 2002), which
uses language models and arithmetic coding to
present alternative letter targets on the screen with
size relative to their likelihood given the history.
Users can type by continuous motion, such as eye
gaze or mouse cursor movement, targeting their cur-
sor at the intended letter and moving the cursor
from left-to-right through the interface, while its
movements are tracked. This is an extremely effec-
tive typing interface alternative to keyboards, pro-
vided the user has sufficient motor control to per-
form the required systematic visual scanning. The
most severely impaired users, such as those with
locked-in syndrome (LIS), have lost the voluntary
motor control sufficient for such an interface.
Relying on extensive visual scanning, such as that
required in dynamically reconfiguring spelling grids
or Dasher, or requiring complex gestural feedback
from the user renders a typing interface difficult or
impossible to use for those with the most severe im-
pairments. Indeed, even spelling grids like the P300
speller can be taxing as an interface for users. Re-
cent attempts to use the P300 speller as a typing
interface for locked-in individuals with ALS found
29
1 A? V  initialize A as symbol set V
2 k ? 1  initialize bit position k to 1
3 while |A| > 1 do
4 P ? {a ? A : a[k] = 1}
5 Q? {a ? A : a[k] = 0}
6 Highlight symbols in P
7 if selected then A? P
8 else A? Q
9 k ? k + 1
10 return a ? A  Only 1 element in A
Figure 2: Algorithm for binary code symbol selection
that the number of items in the grid caused prob-
lems for these patients, because of difficulty orient-
ing attention to specific locations in the spelling grid
(Sellers et al, 2003). This is another illustration of
the need to reduce the cognitive overhead of such in-
terfaces. Yet the success of classification of ERP in
a simpler task for this population indicates that the
P300 is a binary response mechanism of utility for
this task (Sellers and Donchin, 2006).
Simpler interactions via brain-computer inter-
faces (BCI) hold much promise for effective text
communication. Yet these simple interfaces have yet
to take full advantage of language models to ease or
speed typing. In this paper we will make use of a
static grid, or a single letter linear scanning inter-
face, yet scan in a way that allows for the use of
contextual language model probabilities when con-
structing the binary code for each symbol.
2.2 Binary codes for typing interfaces
Row/column scanning, as outlined in the previous
section, is not the only means by which the spelling
grid in Figure 1 can be used as a binary response
typing interface. Rather than highlighting full rows
or full columns, arbitrary subsets of letters could be
highlighted, and letter selection again driven by a
binary response mechanism. An algorithm to do this
is as follows. Assign a unique binary code to each
symbol in the symbol set V (letters in this case). For
each symbol a ? V , there are |a| bits in the code
representing the letter. Let a[k] be the kth bit of the
code for symbol a. We will assume that no symbol?s
binary code is a prefix of another symbol?s binary
code. Given such an assignment of binary codes to
the symbol set V , the algorithm in Figure 2 can be
used to select the target symbol in a spelling grid.
One key question in this paper is how to produce
such a binary code, which is how language models
can be included in scanning. Figure 3 shows two
different binary trees, which yield different binary
codes for six letters in a simple, artificial example.
Huffman:
 
1
 
 
011 0
1 0 1
0
0
000feac
b d
001110111
0110
Linear:
1
01
   
c
01d
b1
  
 
fe
a
0000000001
0001
001
1 0
01
01
0
Letter: a b c d e f
Probability: 0.15 0.25 0.18 0.2 0.12 0.1
Huffman bits: 3 2 3 2 3 3
Linear bits: 4 1 3 2 5 5
Figure 3: Two binary trees for encoding letters based on
letter probabilities: (1) Huffman coding; and (2) Linear
coding via a right-branching tree (right-linear). Expected
bits are 2.55 for Huffman and 2.89 for linear coding.
Huffman coding (Huffman, 1952) builds a binary
tree that minimizes the expected number of bits ac-
cording to the provided distribution. There is a lin-
ear complexity algorithm for building this tree given
a list of items sorted by descending probability.
Another type of binary code, which we will call a
linear code, provides a lot of flexibility in the kind of
interface that it allows, relative to the other methods
mentioned above. In this binary code, each itera-
tion of the WHILE loop in the Figure 2 algorithm
would have a set P on line 4 with exactly one mem-
ber. With such a code, the spelling grid in Figure
1 would highlight exactly one letter at a time for
selection. Alternately, symbols could be presented
one at a time with no grid, which we call rapid serial
visual presentation (RSVP, see Fig.7). Linear cod-
ing builds a simple right-linear tree (seen in Figure
3) that preserves the sorted order of the set, putting
higher probability symbols closer to the root of the
tree, thus obtaining shorter binary codes. Linear
coding can never produce codes with fewer expected
bits than Huffman coding, though the linear code
may reach the minimum under certain conditions.
The simplicity of an interface that presents a sin-
gle letter at a time may reduce user fatigue, and even
make typing feasible for users that cannot maintain
focus on a spelling grid. Additionally, single symbol
auditory presentation would be possible, for visually
impaired individuals, something that is not straight-
forwardly feasible with the sets of symbols that must
be presented when using Huffman codes.
2.3 Language modeling for typing interfaces
The current task is very similar to word prediction
work discussed in Section 2.1, except that the pre-
30
diction interface is the only means by which text
is input, rather than a separate window with com-
pletions being provided. In principle, the symbols
that are being predicted (hence typed) can be from
a vocabulary that includes multiple symbol strings
such as words. However, a key requirement in a
composition-based typing interface is an open vo-
cabulary ? the user should be able to type any word,
whether or not it is in some fixed vocabulary. In-
cluded in such a mechanism is the ability to repair:
delete symbols and re-type new ones. In contrast,
a word prediction component must be accompanied
by some additional mechanism in place for typing
words not in the vocabulary. The current problem is
to use symbol prediction for that core typing inter-
face, and this paper will focus on predicting single
ASCII and control characters, rather than multiple
character strings. The task is actually very similar
to the well known Shannon game (Shannon, 1950),
where text is guessed one character at a time.
Character prediction is done in the Dasher and
GazeTalk interfaces, as discussed in an earlier sec-
tion. There is also a letter prediction component to
the Sibyl/Sibylle interfaces (Schadle, 2004; Wand-
macher et al, 2008), alongside a separate word pre-
diction component. Interestingly, the letter predic-
tion component of Sibylle (Sibyletter) involves a lin-
ear scan of the letters, one at a time in order of proba-
bility (as determined by a 5-gram character language
model), rather than a row/column scanning of the
P300 speller. This approach was based on user feed-
back that the row/column scanning was a much more
tiring interface than the linear scan interface (Wand-
macher et al, 2008), which is consistent with the
results previously discussed on the difficulty of ALS
individuals with the P300 speller interface.
Language modeling for a typing interface task of
this sort is very different from other common lan-
guage modeling tasks. This is because, at each sym-
bol in the string, the already typed prefix string is
given ? there is no ambiguity in the prefix string,
modulo subsequent repairs. In contrast, in speech
recognition, machine translation, optical character
recognition or T9 style text input, the actual pre-
fix string is not known; rather, there is a distribu-
tion over possible prefix strings, and a global in-
ference procedure is required to find the best string
as a whole. For typing, once the symbol has been
produced and not repaired, the model predicting the
next symbol is given the true context. This has sev-
eral important ramifications for language modeling,
including the availability of supervised adaptation
data and the fact that the models trained with rel-
ative frequency estimation are both generative and
discriminative. See Roark (2009) for extensive dis-
cussion of these issues. Here we will consider n-
gram language models of various orders, estimated
via smoothed relative frequency estimation (see ?
3.1). The principal novelty in the current approach
is the principled incorporation of error probabilities
into the binary coding approaches, and the experi-
mental demonstration of how linear coding for grids
or RSVP interfaces compare to Huffman coding and
row/column scanning for grids.
3 Methods
3.1 Character-based language models
For this paper, we use character n-gram models.
Carpenter (2005) has an extensive comparison of
large scale character-based language models, and
we adopt smoothing methods from that paper. It
presents a version of Witten-Bell smoothing (Wit-
ten and Bell, 1991) with an optimized hyperparam-
eter K, which is shown to be as effective as Kneser-
Ney smoothing (Kneser and Ney, 1995) for higher
order n-grams. We refer readers to that paper for de-
tails on this standard n-gram language modeling ap-
proach. For the experimental results presented here,
we trained unigram and 8-gram models from the NY
Times portion of the English Gigaword corpus.
We performed extensive normalization of this
corpus, detailed in Roark (2009). We de-cased
the resulting corpus and selected sentences that
only included characters that would appear in
our 6?6 spelling grid. Those characters are:
the 26 letters of the English alphabet, the space
character, a delete symbol, comma, period, double
and single quote, dash, dollar sign, colon and
semi-colon. We used a 42 million character subset
of this corpus for training the model. Finally, we
appended to this corpus approximately 112 thou-
sand words from the CMU Pronouncing Dictionary
(www.speech.cs.cmu.edu/cgi-bin/cmudict),
which also contained only the symbols from the
grid. For hyper-parameter settings, we used a 100k
character development set. Our best performing
hyper-parameter for the Witten-Bell smoothing was
K = 15, which is comparable to optimal settings
found by Carpenter (2005) for 12-grams.
3.2 Binary codes
Given what has been typed so far, we can use a char-
acter n-gram language model to assign probabilities
31
Figure 4: Row/column scanning interface.
to all next symbols in the symbol set V . After sort-
ing the set in order of decreasing probability, we can
use these probabilities to build binary coding trees
for the set. Hence the binary code assigned to each
symbol in the symbol set differs depending on what
has been typed before. For Huffman coding, we
used the algorithm from Perelmouter and Birbaumer
(2000) that accounts for any probability of error in
following a branch of the tree, and builds the optimal
coding tree even when there is non-zero probability
of taking a branch in error. Either linear or Huffman
codes can be built from the language model proba-
bilities, and can then be used for a typing interface,
using the algorithm presented in Figure 2.
3.3 Scanning systems
For these experiments, we developed an interface
for controlled testing of typing performance under
a range of scanning methods. These include: (i)
row/column scanning, both auto scan (button press
selects) and step scan (lack of button press selects);
(ii) Scanning with a Huffman code, either derived
from a unigram language model, or from an 8-gram
language model; and (iii) Scanning with a linear
code, either on the 6?6 grid, or using RSVP, which
shows one symbol at a time. Each trial involved giv-
ing subjects a target phrase with instructions to type
the phrase exactly as displayed. All errors in typing
were required to be corrected by deleting (via?) the
incorrect symbol and re-typing the correct symbol.
Figure 4 shows our typing interface when config-
ured for row/column scanning. At the top of the
application window is the target string to be typed
by the subject (?we run the risk of failure?). Below
that is the buffer displaying what has already been
typed (?we run t?). Spaces between words must also
be typed ? they are represented by the underscore
character in the upper left-hand corner of the grid.
Spaces are treated like any other symbol in our lan-
guage model ? they must be typed, thus they are pre-
Figure 5: Error in row/column scanning interface.
dicted along with the other symbols. Figure 5 shows
how the display updates when an incorrect character
is typed. The errors are highlighted in red, followed
by the backarrow symbol to remind users to delete.
If a row has not been selected after a pass over all
rows, scanning begins again at the top. After row
selection, column scanning commences; if a column
is not selected after three passes from left-to-right
over the columns, then row scanning re-commences
at the following row. Hence, even if a wrong row is
selected, the correct symbol can still be typed.
Note that the spelling grid has been sorted in uni-
gram frequency order, so that the most frequent sym-
bols are in the upper left-hand corner. This same grid
is used in all grid scanning conditions, and provides
language modeling benefit to row/column scanning.
Figure 6 shows our typing interface when config-
ured for what we term Huffman scanning. In this
scanning mode, the highlighted subset is dictated by
the Huffman code, and is not necessarily contiguous.
Not requiring contiguity of highlighted symbols al-
lows the coding to vary with the context, thus allow-
ing use of an n-gram language model. As far as we
know, this is the first time that contiguity of high-
lighting is relaxed in a scanning interface to accom-
modate Huffman coding. Baljko and Tam (2006)
used Huffman coding for a grid scanning interface,
but using a unigram model and the grid layout was
selected to ensure that highlighted regions would al-
ways be contiguous, thus precluding n-grammodels.
In our Huffman scanning approach, when the se-
lected set includes just one character, it is typed. As
with row/column scanning, when the wrong charac-
ter is typed, the backarrow symbol must be chosen
to delete it. If an error is made in selection that does
not result in a typed character ? i.e., if the incorrectly
selected set has more than one member ? then we
need some mechanism for allowing the target sym-
bol to still be selected, much as we have a mecha-
32
Figure 6: Huffman scanning interface.
nism in row/column scanning for recovering if the
wrong row is selected. Section 3.4 details our novel
method for recalculating the binary codes based on
an error rate parameter. At no point in typing is any
character ruled out from being selected.
The grids shown in Figures 4-6 can be straightfor-
wardly used with linear coding as well, by simply
highlighting one cell at a time in descending proba-
bility order. Additionally, linear coding can be used
with an RSVP interface, shown in Figure 7, which
displays one character at a time.
Each interface needs a scan rate, specifying how
long to wait for a button press before advancing. The
scan rate for each condition was set for each individ-
ual during a training/calibration session (see ?4.1).
3.4 Errors in Huffman and Linear scanning
In this section we briefly detail how we account for
the probability of error in scanning with Huffman
and linear codes. The scanning interface takes a pa-
rameter p, which is the probability that, when a se-
lection is made, it is correct. Thus 1?p is the proba-
bility of an error. Recall that if a selection leads to a
single symbol, then that symbol is typed. Otherwise,
if a selection leads to a set with more than one sym-
bol, then all symbol probabilities (even those not in
the selected set) are updated based on the error prob-
ability and scanning continues. If a non-target (in-
correct) symbol is selected, the delete (backarrow)
symbol must be chosen to correct the error, after
which the typing interface returns to the previous
position. Three key questions must be answered in
such an approach: (1) how are symbol probabilities
updated after a keystroke, to reflect the probability
of error? (2) how is the probability of backarrow es-
timated? and (3) when the typing interface returns
to the previous position, where does it pick up the
scanning? Here we answer all three questions.
Consider the Huffman coding tree in Figure 3. If
the left-branch (?1?) is selected by the user, the prob-
ability that it was intended is p versus an error with
Figure 7: RSVP scanning interface.
probability 1?p. If the original probability of a sym-
bol is q, then the updated probability of the symbol
is pq if it starts with a ?1? and (1?p)q if it starts with
a ?0?. After updating the scores and re-normalizing
over the whole set, we can build a new binary cod-
ing tree. The user then selects a branch at the root
of the new tree. A symbol is finally selected when
the user selects a branch leading to a single symbol.
The same approach is used with a linear coding tree.
The probability of requiring the delete (backar-
row) character can be calculated directly from the
probability of keystroke error ? in fact, the probabil-
ity of backarrow is exactly the probability of error
1?p. To understand why this is the case, consider
that a non-target (incorrect) symbol can be chosen
according to the approach in the previous paragraph
only with a final keystroke error. Any keystroke
error that does not select a single symbol does not
eliminate the target symbol, it merely re-adjusts the
target symbol?s probability along with all other sym-
bols. Hence, no matter how many keystrokes have
been made, the probability that a selected symbol
was not the target symbol is simply the probability
that the last keystroke was in error, i.e., 1?p.
Finally, if backarrow is selected, the previous po-
sition is revisited, and the probabilities are reset as
though no prior selection had been made.
4 Empirical results
4.1 Subjects and scan rate calibration
We recruited 10 native English speakers between the
ages of 24 and 48 years, who had not used our typ-
ing interface, are not users of scanning interfaces
for typing, and have typical motor function. Each
subject participated in two sessions, one for training
and calibration of scan rates; and another for testing.
We use the phrase set from MacKenzie and Souko-
reff (2003) to evaluate typing performance. Of the
500 phrases in that set, 20 were randomly set aside
for testing, the other 480 available during training
and calibration phases. Five of the 20 evaluation
33
strings were used in this study. We used an Ablenet
Jellybean R? button as the binary switch. For these
trials, to estimate error rates in modeling, we fixed
p = 0.95, i.e., 5% error rate.
The scan rate for row/column scanning is typi-
cally different than for Huffman or linear scanning,
since row/column scanning methods allow for an-
ticipation: one can tell from the current highlight-
ing whether the desired row or column will be high-
lighted next. For the Huffman and linear scanning
approaches that we are investigating, that is not the
case: any cell can be highlighted (or symbol dis-
played) at any time, even multiple times in a row.
Hence the scan rate for these methods depends more
on reaction time than row/column scanning, where
anticipation allows for faster rates.
The scan rate also differs between the two
row/column scanning approaches (auto scan and
step scan), due to the differences in control needed
to advance scanning with a button press versus se-
lecting with a button press. We thus ran scan rate
calibration under three conditions: row/column step
scan; row/column auto scan; and Huffman scan-
ning, using a unigram language model. The Huff-
man scanning scan rate was then used for all of the
Huffman and linear scanning approaches.
Calibration involved two stages for each of the
three approaches, and the first stage of all three is
completed before running the second stage, thus fa-
miliarizing subjects with all interfaces prior to final
calibration. The first stage of calibration starts with
slow scan rate (1200 ms dwell time), then speeds up
the scan rate by reducing dwell time by 200 ms when
a target string is successfully typed. Success here
means that the string is correctly typed with less than
10% error rate. The subject gets three tries to type a
string successfully at a given scan rate, after which
they are judged to not be able to complete the task
at that rate. In the first stage, this stops the stage for
that method and the dwell time is recorded. In the
second stage, calibration starts at a dwell time 500
ms higher than where the subject failed in the first
stage, and the dwell time decreases by 100 ms in-
crements when target strings are successfully typed.
When subjects cannot complete the task at a dwell
time, the dwell time then increases at 50 ms incre-
ments until they can successfully type a target string.
Table 1 shows the mean (and std) scan rates (dwell
time) for each condition. Step scanning generally
had a slower scan rate than auto scanning, and Huff-
man scanning (unsurprisingly) was slowest.
4.2 Testing stage and results
In the testing stage of the protocol, there were
six conditions: (1) row/column step scan; (2)
row/column auto scan; (3) Huffman scanning with
codes derived from the unigram language model; (4)
Huffman scanning with codes derived from the 8-
gram language model; (5) Linear scanning on the
6?6 spelling grid with codes derived from the 8-
gram language model; and (6) RSVP single letter
presentation with codes derived from the 8-gram
language model. The ordering of the conditions for
each subject was randomized. In each condition, in-
structions were given (identical to instructions dur-
ing calibration phase), and the subjects typed prac-
tice phrases until they successfully reached error rate
criterion performance (10% error rate or lower), at
which point they were given the test phrases to type.
Recall that the task is to type the stimulus phrase
exactly as presented, hence the task is not com-
plete until the phrase has been correctly typed. To
avoid non-termination scenarios ? e.g., the subject
does not recognize that an error has occurred, what
the error is, or simply cannot recover from cascad-
ing errors ? the trial is stopped if the total errors
in typing the target phrase reach 20, and the sub-
ject is presented with the same target phrase to type
again from the beginning, i.e., the example is re-
set. Only 2 subjects in the experiment had a phrase
reset in this way (just one phrase each), both in
row/column scanning conditions. Of course, the
time and keystrokes spent typing prior to reset are
included in the statistics of the condition.
Table 1 shows the mean (and std) of several mea-
sures for the 10 subjects. Speed is reported in char-
acters per minute. Bits per character represents
the number of keypress and non-keypress (timeout)
events that were used to type the symbol. Note that
bits per character does not correlate perfectly with
speed, since a non-keypress bit due to a timeout
takes the full dwell time, while the time for a key-
press event may be less than that full time. For any
given symbol the bits may involve making an error,
followed by deleting the erroneous symbol and re-
typing the correct symbol. Alternately, the subject
may scan pass the target symbol, but still return to
type it correctly, resulting in extra keystrokes, i.e., a
longer binary code than optimal. In addition to the
mean and standard deviation of bits per character,
we present the optimal could be achieved with each
method. Finally we characterize the errors that are
made by subjects by the error rate, which is the num-
34
Scan rate (ms) Speed (cpm) Bits per character Error rate Long code rate
Scanning condition mean (std) mean (std) mean (std) opt. mean (std) mean (std)
row/column step scan 425 (116) 20.7 (3.6) 8.5 (2.6) 4.5 6.3 (5.1) 29.9 (19.0)
auto scan 310 (70) 19.1 (2.2) 8.4 (1.2) 4.5 5.4 (2.8) 33.8 (11.5)
Huffman unigram 475 (68) 12.5 (2.3) 8.4 (1.9) 4.4 4.4 (2.2) 39.2 (13.5)
8-gram 475 (68) 23.4 (3.7) 4.3 (1.1) 2.6 4.1 (2.2) 19.3 (14.2)
Linear grid 8-gram 475 (68) 23.2 (2.1) 4.2 (0.7) 3.4 2.4 (1.5) 5.0 (4.1)
RSVP 8-gram 475 (68) 20.3 (5.1) 6.1 (2.6) 3.4 7.7 (5.4) 5.2 (4.0)
Table 1: Typing results for 10 users on 5 test strings (total 31 words, 145 characters) under six conditions.
ber of incorrect symbols typed divided by the total
symbols typed. The long code rate is the percent-
age of correctly typed symbols for which a longer
than optimal code was used to type the symbol, by
making an erroneous selection that does not result in
typing the wrong symbol.
We also included a short survey, using a Likert
scale for responses, and mean scores are shown in
Table 2 for four questions: 1) I was fatigued by the
end of the trial; 2) I was stressed by the end of the
trial; 3) I liked this trial; and 4) I was frustrated by
this trial. The responses showed a consistent prefer-
ence for Huffman and linear grid conditions with an
8-gram language model over the other conditions.
Survey Row/Column Huffman Linear
Question step auto 1-grm 8-grm grid RSVP
Fatigued 3.2 2.4 3.6 2.0 2.4 2.8
Stressed 2.7 2.4 2.7 1.5 1.8 2.6
Liked it 2.2 3.3 2.3 4.2 3.8 3.2
Frustrated 3.2 1.7 3.1 1.7 1.7 2.3
Table 2: Mean Likert scores to survey questions
(5 = strongly agree; 1 = strongly disagree)
4.3 Discussion of results
While this is a preliminary study of just 10 sub-
jects, several things stand out from the results. First,
comparing the three methods using just unigram fre-
quencies to inform scanning (row/column and Huff-
man unigram), we can see that Huffman unigram
scanning is significantly slower than the other two,
mainly due to a slower scan rate with no real im-
provement in bits per character (real or optimal). All
three methods have a high rate of longer than opti-
mal codes, leading to nearly double the bits per char-
acter that would optimally be required.
Next, with the use of the 8-gram language model
in Huffman scanning, both the optimal bits per char-
acter and the difference between real and optimal are
reduced, leading to nearly double the speed. Inter-
estingly, use of the linear code on the grid leads to
fewer bits per character than Huffman scanning, de-
spite nearly 1 bit increase in optimal bits per charac-
ter, due to a decrease in error rate and a very large
decrease in long code rate. We speculate that this is
because highlighting a single cell at a time draws the
eye to that cell, making visual scanning easier.
Finally, despite using the same model, RSVP is
found to be slightly slower than the Huffman 8-
gram or Linear grid conditions, though commensu-
rate with the row/column scanning, mainly due to an
increase in error rate. Monitoring a single cell, rec-
ognizing symbol identity and pressing the switch is
apparently somewhat harder than finding the symbol
on a grid and waiting for the cell to light up.
5 Summary and future directions
We have presented methods for including language
modeling in simple scanning interfaces for typing,
and evaluated performance of novice subjects with
typical motor control. We found that language mod-
eling can make a very large difference in the us-
ability of the Huffman scanning condition. We also
found that, despite losing bits to optimal Huffman
coding, linear coding leads to commensurate typ-
ing speed versus Huffman coding presumably due
to lower cognitive overhead of scanning and thus
fewer mistakes. Finally, we found that RSVP was
somewhat slower than grid scanning with the same
language model and code.
This research is part of a program to make the
simplest scanning approaches as efficient as possi-
ble, so as to facilitate the use of binary switches for
individuals with the most severe impairments, in-
cluding ERP for locked-in subjects. While our sub-
jects in this study have shown slightly better perfor-
mance using a grid versus RSVP, these individuals
have no problem with visual scanning or fixation
on relatively small cells in the grid. It is encourag-
ing that subjects can achieve nearly the same perfor-
mance with an interface that simply displays an op-
tion and requests a yes or a no. We intend to run this
study with subjects with impairment, and are incor-
porating the interfaces with an ERP detection system
for use as a brain-computer interface.
35
Acknowledgments
This research was supported in part by NIH Grant
#1R01DC009834-01 and NSF Grant #IIS-0447214.
Any opinions, findings, conclusions or recommen-
dations expressed in this publication are those of the
authors and do not necessarily reflect the views of
the NSF or NIH.
References
D. Anson, P. Moist, M. Przywars, H. Wells, H. Saylor,
and H. Maxime. 2004. The effects of word com-
pletion and word prediction on typing rates using on-
screen keyboards. Assistive Technology, 18(2):146?
154.
G. Baletsa, R. Foulds, and W. Crochetiere. 1976. Design
parameters of an intelligent communication device. In
Proceedings of the 29th Annual Conference on Engi-
neering in Medicine and Biology, page 371.
M. Baljko and A. Tam. 2006. Indirect text entry using
one or two keys. In Proceedings of the Eigth Inter-
national ACM Conference on Assistive Technologies
(ASSETS), pages 18?25.
B. Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software, pages 86?99.
J.J. Darragh, I.H. Witten, and M.L. James. 1990. The
reactive keyboard: A predictive typing aid. Computer,
23(11):41?49.
L.A. Farwell and E. Donchin. 1988. Talking off the
top of your head: toward a mental prosthesis utiliz-
ing event-related brain potentials. Electroenceph Clin.
Neurophysiol., 70:510?523.
J.P. Hansen, A.S. Johansen, D.W. Hansen, K. Itoh, and
S. Mashino. 2003. Language technology in a pre-
dictive, restricted on-screen keyboard with ambiguous
layout for severely disabled people. In Proceedings of
EACL Workshop on Language Modeling for Text Entry
Methods.
D.A. Huffman. 1952. A method for the construction of
minimum redundancy codes. In Proceedings of the
IRE, volume 40(9), pages 1098?1101.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 181?184.
G.W. Lesher, B.J. Moulton, and D.J. Higginbotham.
1998. Techniques for augmenting scanning commu-
nication. Augmentative and Alternative Communica-
tion, 14:81?101.
J. Li and G. Hirst. 2005. Semantic knowledge in word
completion. In Proceedings of the 7th International
ACM Conference on Computers and Accessibility.
I.S. MacKenzie and R.W. Soukoreff. 2003. Phrase sets
for evaluating text entry techniques. In Proceedings of
the ACM Conference on Human Factors in Computing
Systems (CHI), pages 754?755.
J. Perelmouter and N. Birbaumer. 2000. A binary
spelling interface with random errors. IEEE Transac-
tions on Rehabilitation Engineering, 8(2):227?232.
B. Roark. 2009. Open vocabulary language modeling
for binary response typing interfaces. Technical
Report #CSLU-09-001, Center for Spoken Language
Processing, Oregon Health & Science University.
cslu.ogi.edu/publications/ps/roark09.pdf.
I. Schadle. 2004. Sibyl: AAC system using NLP tech-
niques. In Proceedings of the 9th International Con-
ference on Computers Helping People with Special
needs (ICCHP), pages 1109?1015.
E.W. Sellers and E. Donchin. 2006. A p300-based brain-
computer interface: initial tests by als patients. Clini-
cal Neuropsysiology, 117:538?548.
E.W. Sellers, G. Schalk, and E. Donchin. 2003. The
p300 as a typing tool: tests of brain-computer interface
with an als patient. Psychophysiology, 40:77.
C.E. Shannon. 1950. Prediction and entropy of printed
English. Bell System Technical Journal, 30:50?64.
K. Trnka, D. Yarrington, K.F. McCoy, and C. Pennington.
2006. Topic modeling in fringe word prediction for
AAC. In Proceedings of the International Conference
on Intelligent User Interfaces, pages 276?278.
K. Trnka, D. Yarrington, J. McCaw, K.F. McCoy, and
C. Pennington. 2007. The effects of word predic-
tion on communication rate for AAC. In Proceed-
ings of HLT-NAACL; Companion Volume, Short Pa-
pers, pages 173?176.
H. Trost, J. Matiasek, and M. Baroni. 2005. The lan-
guage component of the FASTY text prediction sys-
tem. Applied Artificial Intelligence, 19(8):743?781.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 506?513.
T. Wandmacher, J.Y. Antoine, F. Poirier, and J.P. De-
parte. 2008. Sibylle, an assistive communication sys-
tem adapting to the context and its user. ACM Transac-
tions on Accessible Computing (TACCESS), 1(1):6:1?
30.
D.J. Ward, A.F. Blackwell, and D.J.C. MacKay. 2002.
DASHER ? a data entry interface using continuous
gestures and language models. Human-Computer In-
teraction, 17(2-3):199?228.
I.H. Witten and T.C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4):1085?1094.
36
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, page 71,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Workshop on Speech and Language Processing for Assistive Technologies
Demo Session
1 ?How was School today...?? A Prototype
System that Uses Environmental Sensors
and NLG to Support Personal Narrative
for Children with Complex
Communication Needs
Rolf Black, Joseph Reddington, Ehud Reiter, Nava
Tintarev and Annalu Waller
We will show an in-situ sensor based proto-
type that supports personal narrative for children
with complex communication needs. We will
demonstrate the process from data collection, story
generation and editing, to the interactive narration
of stories about a child?s school day. The challeng-
ing environment of a special school for prototype
testing will be discussed and improvements of the
next generation prototype presented.
2 Interactive SIGHT Demo: Textual
Summaries of Simple Bar Charts
Seniz Demir, David Oliver, Edward Schwartz,
Stephanie Elzer, Sandra Carberry and Kathleen F.
McCoy
Interactive SIGHT is intended to provide peo-
ple with visual impairments access to the kind
of information graphics found in popular media.
It works as a browser extension, and is able to
generate a summary of a simple bar chart containing
its high-level intention as natural language text.
The user may request further information about the
graphic through a follow-up question facility.
3 Project Jumbo: Transcription as an
Assistive Technology for Instant
Messaging
Ira R. Forman and Allen K. Wilson
The integration of VoIP into Instant Messaging
may be a boon for most of us, but not for those
who are deaf and hard of hearing. The IBM Human
Ability & Accessibility Center initiated Project
Jumbo to address this problem. Our remedy is to
add a speech-to-text capability to augment voice
services with transcripts. In particular, Project
Jumbo augments IBM Lotus Sametime. Project
Jumbo, which is transitioning to product status
under name IBM AbilityLab Sametime Conference
Transcriber, will be demonstrated. The demo
consists of a chat between the demonstrator and a
remote colleague in which the demonstrator speaks
rather than types. A major point of the demo is
that interactive communication is a new domain
for ASR. This domain differs from dictation in a
number of ways; prominent among them is that
most speech recognition errors do not need to be
corrected.
4 COMUNICA - A Voice Question
Answering System for Portuguese
Rodrigo Wilkens, Aline Villavicencio, Leandro
Wives, Daniel Muller, Fabio da Silva and Stanley
Loh
This is a voice QA system for Brazilian Por-
tuguese that performs speech recognition, text
processing, database access and speech synthesis
for consulting both structured and unstructured
datasets. This system provides multi-modal com-
munication and has the potential to help users with
disabilities to access relevant information, and may
help to significantly increase digital inclusion.
71
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 88?96,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Classification of atypical language in autism
Emily T. Prud?hommeaux, Brian Roark, Lois M. Black, and Jan van Santen
Center for Spoken Language Understanding
Oregon Health & Science University
20000 NW Walker Rd., Beaverton, Oregon 97006
{emily,roark,lmblack,vansanten}@cslu.ogi.edu
Abstract
Atypical or idiosyncratic language is a char-
acteristic of autism spectrum disorder (ASD).
In this paper, we discuss previous work iden-
tifying language errors associated with atyp-
ical language in ASD and describe a proce-
dure for reproducing those results. We de-
scribe our data set, which consists of tran-
scribed data from a widely used clinical di-
agnostic instrument (the ADOS) for children
with autism, children with developmental lan-
guage disorder, and typically developing chil-
dren. We then present methods for automati-
cally extracting lexical and syntactic features
from transcripts of children?s speech to 1)
identify certain syntactic and semantic errors
that have previously been found to distinguish
ASD language from that of children with typ-
ical development; and 2) perform diagnostic
classification. Our classifiers achieve results
well above chance, demonstrating the poten-
tial for using NLP techniques to enhance neu-
rodevelopmental diagnosis and atypical lan-
guage analysis. We expect further improve-
ment with additional data, features, and clas-
sification techniques.
1 Introduction
Atypical language and communication have been as-
sociated with autism spectrum disorder (ASD) since
Kanner (1943) first gave the name autism to the dis-
order. The Autism Diagnostic Observation Sched-
ule (ADOS) (Lord et al, 2002) and other widely
used diagnostic instruments include unusual word
use as a diagnostic criterion. The broad and con-
flicting definitions used in diagnostic instruments for
ASD, however, can lead to difficulty distinguishing
the language peculiarities associated with autism.
The most recent and the most systematic study of
unusual word use in ASD (Volden and Lord, 1991)
found that certain types of atypical word use were
significantly more prevalent in ASD speech than
in the speech of children with typical development
(TD). Although the results provided interesting in-
formation about unusual language in ASD, the pro-
cess of coding these types of errors was laborious
and required substantial linguistic and clinical ex-
pertise.
In this paper, we first use our own data to repro-
duce a subset of the results reported in Volden and
Lord (1991). We then present a method of automat-
ically identifying the types of errors associated with
ASD using spoken language features and machine
learning techniques. These same features are then
used to differentiate subjects with ASD or a devel-
opmental language disorder (DLD) from those with
TD. Although these linguistic features yield strong
classification results, they also reveal a number of
obstacles to distinguishing language characteristics
associated with autism from those associated with
language impairment.
2 Previous Work
Since it was first recognized as a neurodevelop-
mental disorder, autism has been associated with
language described variously as: ?seemingly non-
sensical and irrelevant?, ?peculiar and out of place
in ordinary conversation? (Kanner, 1946); ?stereo-
typed?, ?metaphorical?, ?inappropriate? (Bartak et
al., 1975); and characterized by ?a lack of ease in
88
the use of words? (Rutter, 1965) and ?the use of
standard, familiar words or phrases in idiosyncratic
but meaningful way? (Volden and Lord, 1991). The
three most common instruments used in ASD diag-
nosis ? the Autism Diagnostic Observation Sched-
ule (ADOS) (Lord et al, 2002), the Autism Di-
agnostic Interview-Revised (ADI-R) (Lord et al,
1994), and the Social Communication Questionnaire
(SCQ) (Rutter et al, 2003) ? make reference to
these language particularities in their scoring algo-
rithms. Unfortunately, the guidelines for identify-
ing this unusual language are often vague (SCQ:
?odd?, ADI-R: ?idiosyncratic?, ADOS: ?unusual?)
and sometimes contradictory (ADOS: ?appropriate?
vs. ADI-R: ?inappropriate?; ADOS: ?phrases...they
could not have heard? vs. SCQ: ?phrases that he/she
has heard other people use?).
In what is one of the only studies focused specif-
ically on unusual word use in ASD, Volden and
Lord (1991) transcribed two 10-minute speech sam-
ples from the ADOS for 20 school-aged, high-
functioning children with autism and 20 with typi-
cal development. Utterances containing non-English
words or the unusual use of a word or phrase were
flagged by student workers and then categorized by
the authors into one of three classes according to the
type of error:
? Developmental syntax error: a violation of a
syntactic rule normally acquired in early child-
hood, such as the use of object pronoun in sub-
ject position or an overextension of a regular
morphological rule, e.g., What does cows do?
? Non-developmental syntax error: a syntactic
error not commonly observed in the speech of
children acquiring language, e.g., But in the car
it?s some.
? Semantic error: a syntactically intact sentence
with an odd or unexpected word given the con-
text and intended meaning, e.g., They?re siding
the table.
The authors found that high-functioning chil-
dren with ASD produced significantly more non-
developmental and semantic errors than children
with typical development. The number of develop-
mental syntax errors was not significantly different
between these two groups.
Although there has been virtually no previous
work on automated analysis of unannotated tran-
scripts of the speech of children with ASD, auto-
matically extracted language features have shown
promise in the identification of other neurological
disorders such as language impairment and cogni-
tive impairment. Gabani et al (2009) used part-of-
speech language models to derive perplexity scores
for transcripts of the speech of children with and
without language impairment. These scores offered
significant diagnostic power, achieving an F1 mea-
sure of roughly 70% when used within an support
vector machine (SVM) for classification. Roark et
al. (in press) extracted a much larger set of lan-
guage complexity features derived from syntactic
parse trees from transcripts of narratives produced
by elderly subjects for the diagnosis of mild cogni-
tive impairment. Selecting a subset of these features
for classification with an SVM yielded accuracy, as
measured by the area under the receiver operating
characteristic curve, of 0.73.
Language models have also been applied to the
task of error identification, but primarily in writ-
ing samples of ESL learners. Gamon et al (2008)
used word-based language models to detect and
correct common ESL errors, while Leacock and
Chodorow (2003) used part-of-speech bigram lan-
guage models to identify potentially ungrammatical
two-word sequences in ESL essays. Although these
tasks differ in a number of ways from our tasks, they
demonstrate the utility of using both word and part-
of-speech language models for error detection.
3 Data Collection
3.1 Subjects
Our first objective was to gather data in order repro-
duce the results reported in Volden and Lord (1991).
As shown in Table 1, the participants in our study
were 50 children ages 4 to 8 with a performance
IQ greater than 80 and a diagnosis of either typical
Diagnosis Count Age (s.d.) IQ (s.d.)
TD 17 6.24 (1.38) 125.7 (11.63)
ASD 20 6.38 (1.25) 108.9 (16.41)
DLD 13 7.01 (1.10) 100.6 (10.95)
Table 1: Count, mean age and IQ by subject group.
89
development (TD, n=17), autism spectrum disorder
(ASD, n=20), or developmental language disorder
(DLD, n=13).
Developmental language disorder (DLD), also
sometimes known as specific language impairment
(SLI), is generally defined as the delayed or im-
paired acquisition of language without accompany-
ing comparable delays or deficits in hearing, cogni-
tion, and socio-emotional development (McCauley,
2001). The language impairments that characterize
DLD are not related to articulation or ?speech im-
pediments? but rather are associated with more pro-
found problems producing and often comprehend-
ing language in terms of its pragmatics, syntax, se-
mantics, and phonology. The DSM-IV-TR (Ameri-
can Psychiatric Association, 2000) includes neither
DLD nor SLI as a disorder, but for the purposes
of this work, DLD corresponds to the DSM?s des-
ignations Expressive Language Disorder and Mixed
Expressive-Receptive Language Disorder.
For this study, a subject received a diagnosis of
DLD if he or she met one of two commonly used
criteria: 1) The Tomblin Epi-SLI criteria (Tomblin,
et al, 1996), in which diagnosis of language im-
pairment is indicated when scores in two out of five
domains (vocabulary, grammar, narrative, receptive,
and expressive) are greater than 1.25 standard devia-
tions below the mean; and 2) The CELF-Preschool-
2/CELF-4 criteria, in which diagnosis of language
impairment is indicated when one out of three index
scores and one out of three spontaneous language
scores are more than one standard deviation below
the mean.
A diagnosis of ASD required a previous medi-
cal, educational, or clinical diagnosis of ASD, which
was then confirmed by our team of clinicians ac-
cording to the criteria of the DSM-IV-TR (Ameri-
can Psychiatric Association, 2000), the revised al-
gorithm of the ADOS (Lord et al, 2002), and the
SCQ parental interview (Rutter et al, 2003). Fifteen
of the 20 ASD subjects participating in this study
also met at least one of the above described criteria
for DLD.
3.2 Data Preparation
The ADOS (Lord et al, 2002), a semi-structured se-
ries of activities designed to reveal behaviors asso-
ciated with autism, was administered to all 50 sub-
jects. Five of the ADOS activities that require sig-
nificant amounts spontaneous speech (Make-Believe
Play, Joint Interactive Play, Description of a Pic-
ture, Telling a Story From a Book, and Conversa-
tion and Reporting) were then transcribed at the ut-
terance level for all 50 speakers. All utterances from
the transcripts longer than four words (11,244) were
presented to individuals blind to the purposes of the
study, who were asked to flag any sentence with
atypical or unusual word use. Those sentences were
then classified by the authors as having no errors or
one of the three error types described in Volden and
Lord. Examples from our data are given in Table 2.
3.3 Reproducing Previous Results
In order to compare our results to those reported in
Volden and Lord, we calculated the rates of the three
types of errors for each subject, as shown in Ta-
ble 2. With a two-sample (TD v. ASD) t-test, the
rates of nondevelopmental and semantic errors were
significantly higher in the ASD group than in the
TD group, while there was no significant difference
in developmental errors between the two groups.
These results reflect the same trends observed in
Volden and Lord, in which the raw counts of both
developmental and semantic errors were higher in
the ASD group.
Using ANOVA for significance testing over all
three diagnostic groups, we found that the rate of
developmental errors was significantly higher in the
DLD group than in the other groups. The difference
in semantic error rate between TD and ASD using
the t-test was preserved, but the difference in nonde-
velopmental error rate was lost when comparing all
three diagnostic groups with ANOVA, as shown in
Figure 1.
Error Example
Dev.
I have a games.
The baby drinked it.
The frogs was watching TV.
Nondev.
He locked him all of out.
Would you like to be fall down?
He got so the ball went each way.
Sem.
Something makes my eyes poke.
It smells like it?s falling on your head.
All the fish are leaving in the air.
Table 2: Examples of error types.
90
00.02
0.04
0.06
0.08
Dev. Nondev. Sem.
TD
ASD
DLD
*
*
*
Figure 1: Error rates by diagnostic group (*p <0.05).
The process of manually identifying sentences
with atypical or unusual language was relatively
painless, but determining the specific error types is
subjective and time-consuming, and requires a great
deal of expertise. In addition, although we do ob-
serve significant differences between groups, it is
not clear whether the differences are sufficient for
diagnostic classification or discrimination.
We now propose automatically extracting from
the transcripts various measures of linguistic likeli-
hood, complexity, and surprisal that have the poten-
tial to objectively capture qualities that differentiate
1) the three types of errors described above, and 2)
the three diagnostic groups discussed above. In the
next three sections, we will discuss the various lin-
guistic features we extract; methods for using these
features to classify each sentence according to its er-
ror type for the purpose of automatic error-detection;
and methods for using these features, calculated for
each subject, for diagnostic classification.
4 Features
N-gram cross entropy. Following previous work
in both error detection (Gamon et al, 2008; Leacock
and Chodorow, 2003) and neurodevelopmental di-
agnostic classification (Gabani et al, 2009), we be-
gin with simple bigram language model features. A
bigram language model provides information about
the likelihood of a given item (e.g., a word or part
of speech) in a sentence given the previous item in
that sentence. We suspect that some of the types
of unusual language investigated here, in particular
those seen in the syntactic errors shown in Table 2,
are characterized by unlikely words (drinked) and
word or part-of-speech sequences (a games, all of
out) and hence might be distinguished by language
model-based scores.
We build a word-level bigram language model and
a part-of-speech level bigram language model from
the Switchboard (Godfrey et al, 1992) corpus. We
then automatically generate part-of-speech tags for
each sentence (where the tags were derived from
the best scoring output of the full syntactic parser
mentioned below), and then apply the two models
to each sentence. For each sentence, we calculate
its cross entropy and perplexity. For a word string
w1 . . . wn of length n, the cross entropy H is
H(w1 . . . wn) = ?
1
n
log P(w1 . . . wn) (1)
where P(w1 . . . wn) is calculated as the product of
the n-gram probabilities of each word in the string.
The corresponding measure can be calculated for the
POS-tag sequence, based on an n-gram model of
tags. Perplexity is simply 2H .
While we would prefer to use a corpus that is
closer to the child language that we are attempting
to model, we found the conversational style of the
Switchboard corpus to be the most effective large
corpus that we had at our disposal for this study.
As the size of our small corpus grows, we intend to
make use of the text to assist with model building,
but for this study, we used all out-of-domain data
for n-gram language models and parsing models.
Using Switchboard also allowed us to use the same
corpus to train both n-gram and parsing models.
Surprisal-based features. Surprisal, or the unex-
pectedness of a word or syntactic category in a given
context, is often used as a psycholinguistic mea-
sure of sentence-processing difficulty (Hale, 2001;
Boston et al, 2008). Although surprisal is usually
discussed in the context of cognitive load for lan-
guage processing, we hoped that it might also cap-
ture some of the language characteristics of the se-
mantic errors like those in Table 2, which often con-
tain common words used in surprising ways, and
the nondevelopmental syntax errors, which often in-
clude strings of function words presented in an order
that would be difficult to anticipate.
To derive surprisal-based features, each sentence
is parsed using the Roark (2001) incremental
top-down parser relying on a model built again on
91
the Switchboard corpus. The incremental output of
the parser shows the surprisal for each word, as well
as other scores, as presented in Roark et al (2009).
For each sentence, we collected the mean surprisal
(equivalent to the cross entropy given the model);
the mean syntactic surprisal; and the mean lexical
surprisal. The lexical and syntactic surprisal are a
decomposition of the total surprisal into that portion
due to probability mass associated with building
non-terminal structure (syntactic surprisal) and that
portion due to probability mass associated with
building terminal lexical items in the tree (lexical
surprisal). We refer the reader to that paper for
further details.
Other linguistic complexity measures The non-
developmental syntax errors in Table 2 are charac-
terized by their ill-formed syntactic structure. Fol-
lowing Roark et al (in press), in which the authors
explored the relationship between linguistic struc-
tural complexity and cognitive decline, and Sagae
(2005), in which the authors used automatic syntac-
tic annotation to assess syntactic development, we
also investigated the following measures of linguis-
tic complexity: words per clause, tree nodes per
word, dependency length per word, and Ygnve and
Frazier scores per word. Each of these scores can
be calculated from a provided syntactic parse tree,
and to generate these we made use of the Charniak
parser (Charniak, 2000), also trained on the Switch-
board treebank.
Briefly, words per clause is the total number of
words divided by the total number of clauses; and
tree nodes per word is the total number of nodes
in the parse tree divided by the number of words.
The dependency length for a word is the distance (in
word tokens) between that word and its governor,
as determined through standard head-percolation
methods from the output of the Charniak parser. We
calculate the mean of this length over all words in
the utterance. The Yngve score of a word is the
size of the stack of a shift-reduce parser after that
word; and the Frazier score essentially counts how
many intermediate nodes exist in the tree between
the word and its lowest ancestor that is either the
root or has a left sibling in the tree. We calculate
the mean of both of these scores over the utterance.
We refer the reader to the above cited paper for more
details on these measures.
As noted in Roark et al (in press), some of these
measures are influenced by particular characteristics
of the Penn Treebank style trees ? e.g., flat noun
phrases, etc. ? and measures vary in the degree to
which they capture divergence from typical struc-
tures. Some (including Yngve) are sensitive to the
breadth of trees (e.g., flat productions with many
children); others (including Frazier) are sensitive to
depth of trees. This variability is a key reason for
including multiple, complementary features, such as
both Frazier and Yngve scores, to capture more sub-
tle syntactic characteristics than would be available
from any of these measures alone.
Although we were not able to measure parsing ac-
curacy on our data set and how it might affect the re-
liability of these features, Roark et al (in press) did
investigate this very issue. They found that all of the
above described syntactic measures, when they were
derived from automatically generated parse trees,
correlated very highly (greater than 0.9) with those
measures when they were derived from manually
generated parse trees. For the moment, we assume
that the same principle holds true for our data set,
though we do intend both to verify this assump-
tion and to supplement our parsing models with data
from child speech. Based on manual inspection of
parser output, the current parsing model does seem
to be recovering largely valid structures.
5 Error Classification
The values for 8 of the 12 features were significantly
different over the three error classes, as measured
by one-way ANOVA: words per clause, Yngve, de-
pendency, word cross-entropy all significant at p <
0.001; Frazier, nodes per word at p < 0.01; overall
surprisal and lexical surprisal at p < 0.05. We built
classification and regression trees (CART) using the
Weka data mining software (Hall et al, 2009) us-
ing all of the 12 features described above to predict
which error each sentence contained, and we report
the accuracy, weighted F measure, and area under
the receiver operating characteristic curve (AUC).
Including all 12 features in the CART using 10-
fold cross validation resulted in an AUC of 0.68,
while using only those features with significant
between-group differences yielded an AUC of 0.65.
92
Classifier Acc. F1 AUC
Baseline 1 41% 0.24 0.5
Baseline 2 33% 0.32 0.5
All features 53% 0.53 0.68
Feature subset 49% 0.49 0.65
Table 3: Error-type classification results.
These are both substantial improvements over a
baseline with an unbalanced corpus in which the
most frequent class is chosen for all input items
(Baseline 1) or a baseline with a balanced corpus in
which class is chosen at random (Baseline 2), which
both have an AUC of 0.5. The results for each of
these classifiers, provided in Table 3, show potential
for automating the identification of error type.
6 Diagnostic Classification
In Section 3, we found a number of significant dif-
ferences in error type production rates across our
three diagnostic groups. Individual rates of error
production, however, provide almost no classifica-
tion power within a CART (AUC = 0.51). Perhaps
the phenomena being observed in ASD and DLD
language are related to subtle language features that
are less easily identified than simply the membership
of a sentence in one of these three error categories.
Given the ability of our language features to dis-
criminate error types moderately well, as shown in
Section 5, we decided to extract these same 12 fea-
tures from every sentence longer than 4 words from
the entire transcript for each of the subjects. We
then took the mean of each feature over all of the
sentences for each speaker. These per-speaker fea-
ture vectors were used for diagnostic classification
within a CART.
We first performed classification over the three di-
agnostic groups using the full set of 12 features de-
scribed in Section 4. This results in only modest
gains in performance over the baseline that uses er-
ror rates as the only features. We then used ANOVA
to determine which of the 12 features differed sig-
nificantly across the three groups. Only four fea-
tures were found to be significantly different across
the three groups (words per clause, Yngve, depen-
dency, word cross entropy), and none of them dif-
ferent significantly between the ASD group and the
DLD group. As expected, classification did not im-
Features Acc. F1 AUC
Error rates 33% 0.32 0.51
All features 42% 0.38 0.59
Feature subset 40% 0.37 0.6
Table 4: All subjects: Diagnostic classification results.
prove with this feature subset, as reported in Table 4.
Recall that 15 of the 20 ASD subjects also met at
least one criterion for a developmental language dis-
order. Perhaps the language peculiarities we observe
in our subjects with ASD are related in part to lan-
guage characteristics of DLD rather than ASD. We
now attempt to tease apart these two sources of un-
usual language by investigating three separate clas-
sification tasks: TD vs. ASD, TD vs. DLD, and
ASD vs. DLD.
6.1 TD vs. ASD
We perform classification of the TD and ASD sub-
jects with three feature sets: 1) per-subject error
rates; 2) all 12 features described in Section 4; and
3) the subset of significantly different features. We
found that 7 of the 12 features explored in Section 4
differed significantly between the TD group and the
ASD group: words per clause, Yngve, dependency,
word cross-entropy, overall surprisal, syntactic sur-
prisal, and lexical surprisal. Classification results are
shown in Table 5. We see that using the automati-
cally derived linguistic features improves classifica-
tion substantially over the baseline using per-subject
error rates, particularly when we use the feature sub-
set. Note that the best classification accuracy results
are comparable to those reported in related work on
language impairment and mild cognitive impairment
described in Section 2.
6.2 TD vs. DLD
We perform classification of TD and DLD subjects
with the same three feature sets used for the TD
vs. ASD classification. We found that 6 of the 12
Features Acc. F1 AUC
Error rates 62% 0.62 0.56
All features 62% 0.62 0.65
Feature subset 68% 0.67 0.72
Table 5: TD vs. ASD: Diagnostic classification results.
93
Features Acc. F1 AUC
Error rates 67% 0.67 0.72
All features 80% 0.79 0.75
Feature subset 77% 0.75 0.66
Table 6: TD vs. DLD: Diagnostic classification results.
features explored in Section 4 different significantly
between the TD group and the ASD group: words
per clause, Yngve, dependency, word cross-entropy,
overall surprisal, and lexical surprisal. Note that this
is a subset of the features that differed between the
TD group and ASD group. Classification results are
shown in Table 6. Interestingly, using per-subject er-
ror rates for classification of TD and DLD subjects
was quite robust. Using all of the features improved
classification somewhat, while using only a subset
resulted in degraded performance. We see that the
discriminative power of these features is superior to
that reported in earlier work using LM-based fea-
tures for classification of specific language impair-
ment (Gabani et al, 2009).
6.3 ASD vs. DLD
Finally, we perform classification of the ASD and
DLD subjects using only the first two features
sets, since there were no features found to be even
marginally significantly different between these two
groups. Classification results, which are dismal for
both feature sets, are shown in Table 7.
6.4 Discussion
It seems quite clear that the error rates, feature val-
ues, and classification performance are all being in-
fluenced by the fact that a majority of the ASD sub-
jects also meet at least one criterion for a develop-
mental language disorder. Neither error rates nor
feature values could discriminate between the ASD
and DLD group. Nevertheless we see that our ASD
group and DLD group do not follow the same pat-
terns in their error production or language feature
scores. Clearly there are differences in the language
Features Acc. F1 AUC
Error rates 55% 0.52 0.48
All features 58% 0.44 0.40
Table 7: ASD vs. DLD: Diagnostic classification results.
patterns of the two groups that are not being cap-
tured with any of the methods discussed here.
We also observe that the error rates them-
selves, while sometimes significantly different
across groups as originally observed in Volden and
Lord, do not perform well as diagnostic features
for ASD in our framework. Volden and Lord did
not attempt classification in their study, so it is not
known whether the authors would have encountered
the same problem. There are, however, a number
of possible explanations for a discrepancy between
our results and theirs. First, our data was gath-
ered from pre-school and young school-aged chil-
dren, while the Volden and Lord subjects were gen-
erally teenagers and young adults. The way in which
their spoken language samples were elicited allowed
Volden and Lord to use raw error counts rather than
error rates. There may also have been important dif-
ferences in the way we carried out the manual er-
ror identification process, despite our best efforts to
replicate their procedure. Further development of
our classification methods and additional data col-
lection are needed to determine the utility of error
type identification for diagnostic purposes.
7 Future Work
Although our classifiers using automatically ex-
tracted features were generally robust, we expect
that including additional classification techniques,
subjects (especially ASD subjects without DLD),
and features will further improve our results. In
particular, we would like to explore semantic and
lexical features that are less dependent on linear or-
der and syntactic structure, such as Resnik similarity
and features derived using latent semantic analysis.
We also plan to expand the training input for
the language model and parser to include children?s
speech. The Switchboard corpus is conversational
speech, but it may fail to adequately model many lin-
guistic features characteristic of small children. The
CHILDES database of children?s speech, although
it is not large enough to be used on its own for our
analysis and would require significant manual syn-
tactic annotation, might provide enough data for us
to adapt our models to the child language domain.
Finally, we would like to investigate how infor-
mative the error types are and whether they can be
94
reliably coded by multiple judges. When we exam-
ined the output of our error-type classifier, we no-
ticed that many of the misclassified examples could
be construed, upon closer inspection, as belonging
to multiple error classes. The sentence He?s flying
in a lily-pond, for instance, could contain a devel-
opmental error (i.e., the child has not yet acquired
the correct meaning of in) or a semantic error (i.e.,
the child is using the word flying instead of swim-
ming). Without knowing the context in which the
sentence was uttered, it is not possible to determine
the type of error through any manual or automatic
means. The seemingly large number of misclassifi-
cations of sentences like this indicates the need for
further investigation of the existing coding proce-
dure and in-depth classification error analysis.
8 Conclusions
Our method of automatically identifying error type
shows promise as a supplement to, or substitute for,
the time-consuming and subjective manual coding
process described in Volden and Lord (Volden and
Lord, 1991). However, the superior performance of
our automatically extracted language features sug-
gests that perhaps it may not be the errors them-
selves that characterize the speech of children with
ASD and DLD but rather a preference for certain
structures and word sequences that sometimes mani-
fest themselves as clear language errors. Such varia-
tions in complexity and likelihood might be too sub-
tle for humans to reliably observe.
In summary, the methods explored in this paper
show potential for improving diagnostic discrimina-
tion between typically developing children and those
with these neurodevelopmental disorders. Further
research is required, however, in finding the most re-
liable markers that can be derived from such spoken
language samples.
Acknowledgments
This work was supported in part by NSF Grant
#BCS-0826654; an Innovative Technology for
Autism grant from Autism Speaks; and NIH NIDCD
grant #1R01DC007129-01. Any opinions, findings,
conclusions or recommendations expressed in this
publication are those of the authors and do not nec-
essarily reflect the views of the NSF, Autism Speaks,
or the NIH. Thanks to Meg Mitchell and Cheryl
Greene for their assistance with this project.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC, 4th edition.
Laurence Bartak, Michael Rutter, and Anthony Cox.
1975. A comparative study of infantile autism
and specific developmental receptive language disor-
der. I. The children. British Journal of Psychiatry,
126:27145.
Mariss Ferrara Boston, John Hale, Reinhold Kliegl, and
Shravan Vasishth. 2008. Surprising parser actions
and reading difficulty. In Proceedings of ACL-08:HLT,
Short Papers.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Conference of the
North American Chapter of the ACL, pages 132?139.
Keyur Gabani, Melissa Sherman, Thamar Solorio, and
Yang Liu. 2009. A corpus-based approach for the
prediction of language impairment in monolingual en-
glish and spanish-english bilingual children. In Pro-
ceedings of NAACL-HLT, pages 46?55.
Michael Gamon, Jianfeng Gao, Chris Brockett, and
Re Klementiev. 2008. Using contextual speller tech-
niques and language modeling for ESL error correc-
tion. In Proceedings of IJCNLP.
John J. Godfrey, Edward Holliman, and Jane McDaniel.
1992. SWITCHBOARD: telephone speech corpus for
research and development. In Proceedings of ICASSP,
volume 1, pages 517?520.
John T. Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the 2nd
meeting of NAACL.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
Leo Kanner. 1946. Irrelevant and metaphorical lan-
guage. American Journal of Psychiatry, 103:242?246.
Claudia Leacock and Martin Chodorow. 2003. Auto-
mated grammatical error detection. In M.D. Shermis
and J. Burstein, editors, Automated essay scoring: A
cross-disciplinary perspective. Lawrence Erlbaum As-
sociates, Inc., Hillsdale, NJ.
Catherine Lord, Michael Rutter, and Anne LeCouteur.
1994. Autism diagnostic interview-revised: A revised
95
version of a diagnostic interview for caregivers of in-
dividuals with possible pervasive developmental disor-
ders. Journal of Autism and Developmental Disorders,
24:659?685.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Rebecca McCauley. 2001. Assessment of language dis-
orders in children. Lawrence Erlbaum Associates,
Mahwah, NJ.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In Pro-
ceedings of EMNLP, pages 324?333.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristina Hollingshead, and Jeffrey Kaye. in press.
Spoken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Audio,
Speech and Language Processing.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
Michael Rutter. 1965. Speech disorders in a series of
autistic children. In A. Franklin, editor, Children with
communication problems, pages 39?47. Pitman.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the ACL.
Joanne Volden and Catherine Lord. 1991. Neologisms
and idiosyncratic language in autistic speakers. Jour-
nal of Autism and Developmental Disorders, 21:109?
130.
96
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 22?31,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards technology-assisted co-construction with communication partners
Brian Roark
?
, Andrew Fowler
?
, Richard Sproat
?
, Christopher Gibbons
?
, Melanie Fried-Oken?
?Center for Spoken Language Understanding ?Child Development & Rehabilitation Center
Oregon Health & Science University
{roark,fowlera,sproatr}@cslu.ogi.edu {gibbons,mfo}@ohsu.edu
Abstract
In this paper, we examine the idea of
technology-assisted co-construction, where
the communication partner of an AAC user
can make guesses about the intended mes-
sages, which are included in the user?s word
completion/prediction interface. We run some
human trials to simulate this new interface
concept, with subjects predicting words as the
user?s intended message is being generated in
real time with specified typing speeds. Re-
sults indicate that people can provide substan-
tial keystroke savings by providing word com-
pletion or prediction, but that the savings are
not as high as n-gram language models. In-
terestingly, the language model and human
predictions are complementary in certain key
ways ? humans doing a better job in some
circumstances on contextually salient nouns.
We discuss implications of the enhanced co-
construction interface for real-time message
generation in AAC direct selection devices.
1 Introduction
Individuals who cannot use standard keyboards for
text entry because of physical disabilities have a
number of alternative text entry methods that per-
mit typing. Referred to as keyboard emulation
within augmentative and alternative communication
(AAC), there are many different access options for
the user, ranging from direct selection of letters with
any anatomical pointer (e.g., head, eyes) to use of a
binary switch ? triggered by button-press, eye-blink
or even through event related potentials (ERP) such
as the P300 detected in EEG signals. These options
allow the individual to indirectly select a symbol
based on some process for scanning through alter-
natives (Lesher et al, 1998). Typing speed is a chal-
lenge, yet is critically important for usability, and
as a result there is a significant line of research into
the utility of statistical language models for improv-
ing typing speed (McCoy et al, 2007; Koester and
Levine, 1996; Koester and Levine, 1997; Koester
and Levine, 1998). Methods of word, symbol,
phrase and message prediction via statistical lan-
guage models are widespread in both direct selec-
tion and scanning devices (Darragh et al, 1990; Li
and Hirst, 2005; Trost et al, 2005; Trnka et al,
2006; Trnka et al, 2007; Wandmacher and Antoine,
2007; Todman et al, 2008). To the extent that the
predictions are accurate, the number of keystrokes
required to type a message can be dramatically re-
duced, greatly speeding typing.
AAC devices for spontaneous and novel text gen-
eration are intended to empower the user of the sys-
tem, to place them in control of their own com-
munication, and reduce their reliance on others for
message formulation. As a result, all such devices
(much like standard personal computers) are built
for a single user, with a single keyboard and/or alter-
native input interface, which is driven by the user of
the system. The unilateral nature of these high tech-
nology solutions to AAC stands in contrast to com-
mon low technology solutions, which rely on collab-
oration between the individual formulating the mes-
sage and their communication partner. Many adults
with acquired neurological conditions rely on com-
munication partners for co-construction of messages
(Beukelman et al, 2007).
One key reason why low-tech co-construction
may be preferred to high-tech stand-alone AAC sys-
tem solutions is the resulting speed of communica-
tion. Whereas spoken language reaches more than
one hundred words per minute and an average speed
typist using standard touch typing will achieve ap-
proximately 35 words per minute, a user of an AAC
device will typically input text in the 3-10 words per
minute range. With a communication partner guess-
22
ing the intended message and requesting confirma-
tion, the communication rate can speed up dramati-
cally. For face-to-face communication ? a modality
that is currently very poorly served by AAC devices
? such a speedup is greatly preferred, despite any
potential authorship questions.
Consider the following low-tech scenario. Sandy
is locked-in, with just a single eye-blink serving to
provide binary yes/no feedback. Sandy?s commu-
nication partner, Kim, initiates communication by
verbally stepping through an imagined row/column
grid, first by number (to identify the row); then by
letter. In such a way, Sandy can indicate the first
desired symbol. Communication can continue in
this way until Kim has a good idea of the word that
Sandy intends and proposes the word. If Sandy says
yes, the word has been completed, much as auto-
matic word completion may occur within an AAC
device. But Kim doesn?t necessarily stop with word
completion; subsequent word prediction, phrase pre-
diction, in fact whole utterance prediction can fol-
low, driven by Kim?s intuitions derived from knowl-
edge of Sandy, true sensitivity to context, topic, so-
cial protocol, etc. It is no wonder that such methods
are often chosen over high-tech alternatives.
In this paper, we present some preliminary ideas
and experiments on an approach to providing tech-
nology support to this sort of co-construction during
typing. The core idea is to provide an enhanced in-
terface to the communication partner (Kim in the ex-
ample above), which does not allow them to directly
contribute to the message construction, but rather
to indirectly contribute, by predicting what they be-
lieve the individual will type next. Because most text
generation AAC devices typically already rely upon
symbol, word and phrase prediction from statistical
language models to speed text input, the predictions
of the conversation partner could be used to influ-
ence (or adapt) the language model. Such adaptation
could be as simple as assigning high probability to
words or symbols explicitly predicted by the com-
munication partner, or as complex as deriving the
topic or context from the partner?s predictions and
using that context to improve the model.
Statistical language models in AAC devices can
capture regularities in language, e.g., frequent word
collocations or phrases and names commonly used
by an individual. People, however, have access to
much more information than computational mod-
els, including rich knowledge of language, any rel-
evant contextual factors that may skew prediction,
familiarity with the AAC user, and extensive world
knowledge ? none of which can be easily included in
the kinds of simple statistical models that constitute
the current state of the art. People are typically quite
good at predicting what might come next in a sen-
tence, particularly if it is part of a larger discourse or
dialogue. Indeed, some of the earliest work looking
at statistical models of language established the en-
tropy of English by asking subjects to play a simple
language guessing game (Shannon, 1950). The so-
called ?Shannon game? starts with the subject guess-
ing the first letter of the text. Once they have guessed
correctly, it is uncovered, and the subject guesses
the next letter, and so on. A similar game could be
played with words instead of letters. The number of
guesses required is a measure of entropy in the lan-
guage. People are understandably very good at this
game, often correctly predicting symbols on the first
try for very long stretches of text. No purely com-
putational model can hope to match the contextual
sensitivity, partner familiarity, or world knowledge
that a human being brings to such a task.
A co-construction scenario differs from a Shan-
non game in terms of the time constraints under
which it operates. The communication partner in
such a scenario must offer completions and predic-
tions to the user in a way that actually speeds com-
munication relative to independent text generation.
Given an arbitrary amount of time, it is clear that
people have greater information at their disposal for
predicting subsequent content; what happens under
time constraints is less clear. Indeed, in this paper
we demonstrate that the time constraints put human
subjects at a strong disadvantage relative to language
models in the scenarios we simulated. While it is
far from clear that this disadvantage will also apply
in scenarios closer to the motivating example given
above, it is certainly the case that providing useful
input is a challenging task.
The principal benefit of technology-assisted co-
construction with communication partners is making
use of the partner?s knowledge of language and con-
text, as well as their familiarity with the AAC user
and the world, to yield better predictions of likely
continuations than are currently made by the kinds
23
of relatively uninformed (albeit state of the art) com-
putational language models. A secondary benefit is
that such an approach engages the conversation part-
ner in a high utility collaboration during the AAC
user?s turn, rather than simply sitting and waiting for
the reply to be produced. Lack of engagement is a
serious obstacle to successful conversation in AAC
(Hoag et al, 2004). The slow speed of AAC input is
itself a contributing factor to AAC user dissatisfac-
tion with face-to-face conversation, one of the most
critical modes of human social interaction, and the
one least served by current technology. Because of
the slow turnaround, the conversation partner tends
to lose focus and interest in the conversation, leading
to shorter and less satisfying exchanges than those
enjoyed by those using spoken language. A system
which leverages communication partner predictions
will more fully engage the conversation partner in
the process, rather than forcing them to wait for a
response with nothing to do.
Importantly, an enhanced interface such as that
proposed here provides predictive input from the
communication partner, but not direct compositional
input. The responsibility of selecting symbols and
words during text entry remains with the AAC user,
as the sole author of the text. In the preliminary
experiments presented later in the paper, we simu-
late a direct selection typing system with word pre-
diction, and measure the utility of human generated
word completions and predictions relative to n-gram
models. In such a scenario, n-gram predictions can
be replaced or augmented by human predictions.
This illustrates how easily technology assisted co-
construction with communication partners could po-
tentially be integrated into a user?s interface.
Despite the lack of speedup achieved versus n-
gram models in the results reported below, the po-
tential for capturing communication partner intu-
itions about AAC user intended utterances seems a
compelling topic for future research.
2 Background and Related Work
Over the past forty years, there has been a vast
array of technological solutions to aid AAC users
who present with severe speech and physical im-
pairments, from methods for generating possible
responses, to techniques for selecting among re-
sponses. The simplest methods to generate lan-
guage involve the use of pre-stored phrases, such as
?hello?, ?thank you?, ?I love you?, etc., which are
available on many AAC devices. Some studies have
indicated that use of such phrases improves the per-
ception of fluid communication (McCoy et al, 2007;
Hoag et al, 2008).
Prediction options vary in AAC devices, rang-
ing from letter-by-letter prediction ? see Higgin-
botham (1992) and Lesher et al (1998) for some
reviews ? to word-based prediction. Some systems
can be quite sophisticated, for example incorporat-
ing latent semantic analysis to aid in the better mod-
eling of discourse-level information (Wandmacher
and Antoine, 2007). The WebCrawler project in Jef-
frey Higginbotham?s lab uses topic-related wordlists
mined from the Web to populate a user?s AAC de-
vice with terminology that is likely to be of utility to
the current topic of conversation.
Going beyond word prediction, there has been
an increased interest in utterance-based approaches
(Todman et al, 2008), which extend prediction
from the character or word level to the level
of whole sentences. For example, systems like
FrameTalker/Contact (Higginbotham and Wilkins,
1999; Wilkins and Higginbotham, 2006) populate
the AAC device with pre-stored phrases that can be
organized in various ways. In a similar vein, re-
cent work reported in Wisenburn and Higginbotham
(2008; 2009) proposed a novel method that uses au-
tomatic speech recognition (ASR) on the speech of
the communication partner, extracts noun phrases
from the speech, and presents those noun phrases on
the AAC device, with frame sentences that the AAC
user can select. Thus if the communication partner
says ?Paris?, the AAC user will be able to select
from phrases like ?Tell me more about Paris? or ?I
want to talk about Paris?. This can speed up the con-
versation by providing topically-relevant responses.
Perhaps the most elaborate system of this kind is the
How Was School Today system (Reiter et al, 2009).
This system, which is geared towards children with
severe communication disabilities, uses data from
sensors, the Web, and other sources as input for a
natural language generation system. The system ac-
quires information about the child?s day in school:
which classes he or she attended, what activities
there were, information about visitors, food choices
at the cafeteria, and so forth. The data are then used
24
to generate natural language sentences, which are
converted to speech via a speech synthesizer. At the
end of the day, the child uses a menu to select sen-
tences that he or she wants the system to utter, and
thereby puts together a narrative that describes what
he/she did. The system allows for vastly more rapid
output than a system where the child constructs each
sentence from scratch.
Perhaps the closest work to what we are proposing
is the study of non-disabled adults in Cornish and
Higginbotham (No Date), where one of the adults
played the role of an AAC user, and the other a non-
disabled communication partner. The participants
completed a narrative, a map and a puzzle task. Of
interest was the relative amount of co-construction
of the other?s utterances by each partner, and in
particular its relation to which of the partners was
the one initiating the attempt to achieve a common
ground with the other speaker ? the ?grounded
contribution owner?. In all tasks both the commu-
nication partner and the AAC user co-constructed
each other?s contributions, but there was the great-
est asymmetry between the two users in the puzzle
task.
In what follows, we will first describe a prelim-
inary experiment of word completion for a simu-
lated AAC user, using sentences from the Enron
email corpus and the New York Times. We then
will present results for word completion and pre-
diction within the context of dialogs in the Switch-
board corpus. While we ultimately believe that
the potential for co-construction goes far beyond
simple word completion/prediction, these experi-
ments serve as a first indication of the challenges
to an enhanced technology-assisted interface for co-
construction with communication partners during
novel text generation.
3 Preliminary experiment
In this section, we present a preliminary experiment
to evaluate the potential utility of our technology-
assisted co-construction scenario. The experiment is
akin to a Shannon Game (Shannon, 1950), but with
a time limit for guesses imposed by the speed of typ-
ing. For the current experiment we chose 5 seconds
per keystroke as the simulated typing speed: target
sentences appeared one character at a time, every
five seconds. The subjects? task was to provide a
Figure 1: Preliminary experimental interface in terminal
window, with 4 predicted completions and cursor below
completion for the current word. If the correct word
is provided by the subject, it is selected by the sim-
ulated AAC user as the next keystroke.
For this preliminary experiment, we used a sim-
ple program running in the terminal window of a
Mac laptop. Figure 1 shows a screenshot from this
program in operation. The target string is displayed
at the top of the terminal window, one character at
a time, with the carat symbol showing white space
word boundaries. Predicted word completions are
made by typing with a standard qwerty keyboard;
and when the enter key is pressed, the word that has
been typed is aligned with the current incomplete
word. If it is consistent with the prefix of the word
that has been typed, it remains as a candidate for
completion. When the current five second interval
has passed, the set of accumulated predictions are
filtered to just those which are consistent with the
new letter that the user would have typed (e.g., ?i?
in Figure 1). If the correct word completion for the
target string is present, it is selected with the follow-
ing keystroke. Otherwise the following letter will
be typed (with the typical 5-second delay) and the
interface proceeds as before.
Three able-bodied, adult, literate subjects were
recruited for this initial experiment, and all three
completed trials with both Enron email and New
York Times target strings. The Enron data
comes from the Enron email dataset (http://www-
2.cs.cmu.edu/?enron/) and the NY Times data from
the English Gigaword corpus (LDC2007T07). Both
corpora were pre-processed to remove duplicate data
(e.g., spam or multiple recipient emails), tabular
data and other material that does not represent writ-
ten sentences. Details on this normalization can be
found in Roark (2009). Both corpora consist of writ-
ten sentences, one heavily edited (newspaper), the
other less formal (email); and both are large enough
to allow for robust statistical language modeling.
25
Ngram training Testing
Task sents words sents words chars
NYT 1.9M 35.6M 10 201 1199
Enron 0.6M 6.1M 10 102 528
Table 1: Statistics for each task of n-gram training corpus
size and test set size in terms of sentences, words and
characters (baseline keystrokes)
The two corpora were split into training and test-
ing sets, to allow for training of n-gram language
models to compare word completion performance.
To ensure fair comparison between n-gram and hu-
man word completion performance, no sentences in
the test sets were seen in the training data. From
each test corpus, we extracted sets of 10 contiguous
sentences at periodic intervals, to use as test or prac-
tice sets. Each subject used a 10 sentence practice
set from the NY Times to become familiar with the
task and interface; then performed the word com-
pletion task on one 10 sentence set from the NY
Times and one 10 sentence set from the Enron cor-
pus. Statistics of the training and test sets are given
in Table 1.
Language models were n-gram word-based mod-
els trained from the given corpora using Kneser-Ney
smoothing (Kneser and Ney, 1995). We performed
no pruning on the models.
We evaluate in terms of keystroke savings per-
centage. Let k be the baseline number of keystrokes
without word completion, which is the number of
characters in the sample, i.e., 1 keystroke per char-
acter. With a given word completion method, let c be
the number of keystrokes required to enter the text,
i.e., if the word completion method provides correct
words for selection, those will reduce the number of
keystrokes required1. Then keystroke savings per-
centage is 100?(k?c)/k, the percentage of original
keystrokes that were saved with word completion.
Table 2 shows the keystroke savings percentage on
our two tasks for three n-gram language models (un-
igram, bigram and trigram) and our three subjects.
It is clear from this table that the n-gram language
models are achieving much higher keystroke savings
than our three human subjects. Further, our three
subjects performed quite similarly, not only in com-
1Each word completion requires a selection keystroke, but
saves the keystrokes associated with the remaining characters
in the selected word.
N-gram Subject
Task 1g 2g 3g 1 2 3
NYT 47.4 54.5 56.0 36.5 32.0 32.9
Enron 54.4 61.4 64.4 34.5 32.0 34.1
Table 2: Keystroke savings percentage for test set across
models and subjects
parison with each other, but across the two tasks.
On the face of it, the relatively poor performance
of the human predictors might be surprising, given
that the original Shannon game was intended to es-
tablish a lower bound on the entropy of English. The
assumption has always been that people have better
language models than we can hope to learn automat-
ically. However, in contrast to the original Shannon
game, our predictions are carried out with a fairly
tight time limit, i.e., predictions need to be made
within a fairly short period in order to be made avail-
able to individuals for word completion. The time
limit within the current scenario is one factor that
seems to be putting the subjects at a disadvantage
compared to automated n-gram models on this task.
There are a couple of additional reasons why n-
gram models are performing better on these tasks.
First, they are specific domains with quite ample
training data for the language models. As the
amount of training data decreases ? which would
certainly be the case for individual AAC users ? the
efficacy of the n-gram models decrease. Second,
there is a 1-character advantage of n-gram models
relative to human predictions in this approach. To
see this point clearly, consider the position at the
start of the string. N-gram models can (for prac-
tical purposes) instantaneously provide predictions
for that word. But our subjects must begin typing
the words that they are predicting for this position
at the same time the individual is making their first
keystroke. Those predictions do not become opera-
tive until after that keystroke. Hence the time over-
head of prediction places a lag relative to what is
possible for the n-gram model. We will return to
this point in the discussion section at the end of the
paper.
There are some scenarios, however, where the
subjects did provide word completions prior to the
trigram language model in both domains. Interest-
ingly, a fairly large fraction of these words were
faster than n-gram for more than one of the three
26
NY Times Enron
company cranbury creditor hearing
creditors denied facility suggestions
foothill jamesway jamesways stairs
plan proposal sandler savings
stock stockholders warrants
Table 3: Words completed using subject suggestions with
fewer keystrokes than trigram model. Bold indicates
more than one subject was faster for that word.
subjects. Table 3 shows the list of these words for
our trials. These tended to be longer, open-class
words with high topical importance. In addition,
they tended to be words with common word pre-
fixes, which lead to higher confusability in the n-
gram model. Of course, common prefixes also lead
to higher confusability in our subjects, yet they ap-
pear to be able to leverage their superior context sen-
sitivity to yield effective disambiguation earlier than
the n-gram model in these cases.
Based on these results, we designed a second ex-
periment, with a few key changes from this prelim-
inary experiment, including an improved interface,
the ability to predict as well as complete, and a do-
main that is closer to a proposed model for this co-
construction task.
4 Switchboard experiment
Based on the preliminary experiment, we created a
new protocol and ran seven able-bodied, adult, lit-
erate subjects. We changed the interface and do-
main in ways that we believed would make a dif-
ference in the ability of subjects to compete with n-
gram models in keystroke savings. What remained
the same was the timing of the interface: characters
for target strings were displayed every five seconds.
Word completions were then evaluated for consis-
tency with what had been typed, and if the correct
word was present, the word was completed and re-
vealed, and typing continued.
Data Our primary motivating case for technology-
assisted co-construction comes from face-to-face di-
alog, yet the corpora from which target strings were
extracted in the preliminary experiments were from
large corpora of text produced under very different
conditions. One corpus that does represent a varied-
topic, conversational dialog scenario is the Switch-
board corpus (Godfrey et al, 1992), which contains
transcripts of both sides of telephone conversations.
The idea in using this data was to provide some num-
ber of utterances of dialog context (from the 10 pre-
vious dialog turns), and then ask subjects to provide
word completions for some number of subsequent
utterances.
While the Switchboard corpus does represent the
kind of conversational dialog we are interested in, it
is a spoken language corpus, yet we are modeling
written (typed) language. The difference between
written and spoken language does present something
of an issue for our task. To mitigate this mismatch
somewhat, we made use of the Switchboard section
of the Penn Treebank (Marcus et al, 1993), which
contains syntactic annotations of the Switchboard
transcripts, including explicit marking of disfluen-
cies (?EDITED? non-terminals in the treebank), in-
terjections or parentheticals such as ?I mean? or
?you know?. Using these syntactic annotations, we
produced edited transcripts that omit much of the
spoken language specific phenomena, thus provid-
ing a closer approximation to the kind of written di-
alogs we would like to simulate. In addition, we de-
cased the corpus and removed all characters except
the following: the 26 letters of the English alphabet,
the apostrophe, the space, and the dash.
Interface Figure 2 shows the graphical user inter-
face that was created for these trials. In the upper
box, ten utterances from the context of the dialog are
presented, with an indication of which speaker (A or
B) took the turn. Participants are asked to first read
this context and then press enter to begin the session.
Below this box, the current utterance is displayed,
along with which of the two participants is currently
producing the utterance. As in the previous experi-
ment, the string is displayed one character at a time
in this region. Below this is a text box where word
completions and predictions are entered. Finally, at
the bottom of the interface, Figure 2 shows two of
the five rows of current word completions (left col-
umn) and next word predictions (right column).
Perhaps the largest departure from the preliminary
experiment is the ability to not only complete the
current word but also to provide predictions about
the subsequent word. The subject uses a space de-
limiter to indicate whether predictions are for the
current word or for the subsequent word. Words
preceding a space are taken as current word com-
pletions; the first word after a space is taken as a
27
Figure 2: Experimental graphical user interface
subsequent word prediction. To just predict the sub-
sequent word, one can lead with a space, which re-
sults in no current word completion and whatever
comes after the space as next word prediction. Once
the current word is complete, any words on the sub-
sequent word prediction list are immediately shifted
to the word completion list. We limited current and
next word predictions to five.
We selected ten test dialogs, and subjects pro-
duced word completions and predictions for three
utterances per dialog, for a total of thirty utterances.
We selected the test dialogs to conform to the fol-
lowing characteristics:
1. Each group of three utterances was consecutive
and spoken by the same person.
2. Each utterance contained more than 15 charac-
ters of text.
3. Each group of three utterances began turn-
initially; the first of the three utterances was
always immediately after the other speaker in
the corpus had spoken at least two consecutive
utterances of 15 characters or more.
4. Each group of three utterances was far enough
into its respective conversation that there was
enough text to provide the ten lines of context
required above.
Language models used to contrast with human
performance on this task were trained separately for
every conversation in the test set. For each conver-
sation, Kneser-Ney smoothed n-gram models were
built using all other conversations in the normalized
Switchboard corpus. Thus no conversation is in its
own training data. Table 4 shows statistics of train-
ing and test sets.
Table 5 shows the results for n-gram models and
our seven subjects on this test. Despite the differ-
ences in the testing scenario from the preliminary
experiment, we can see that the results are very sim-
ilar to what was found in that experiment. Also sim-
ilar to the previous trial was the fact that a large per-
centage of tokens for which subjects provided faster
word completion than the trigram model were faster
for multiple subjects. Table 6 shows the nine words
that were completed faster by more than half of the
subjects than the trigram model. Thus, while there is
some individual variation in task performance, sub-
jects were fairly consistent in their ability to predict.
5 Discussion
In this paper we presented two experiments that
evaluated a new kind of technology-assisted co-
construction interface for communication partners
during time-constrained text generation. Results
Ngram training Testing
Task sents words sents words chars
SWBD 0.66M 3.7M 30 299 1501
Table 4: Statistics for the Switchboard task of n-gram
training corpus size and test set size in terms of utter-
ances, words and characters (baseline keystrokes)
28
N-gram Subject
Task 1g 2g 3g 1 2 3 4 5 6 7
Switchboard 51.0 59.0 60.0 28.7 33.1 28.4 28.6 34.1 31.8 32.5
Table 5: Keystroke savings percentage for Switchboard test set across models and subjects
applied can?t comes
every failure named
physics should supervisor
Table 6: Words completed in more than half of the
Switchboard trials using subject suggestions with fewer
keystrokes than trigram model.
from both experiments are negative, in terms of the
ability of our human subjects to speed up communi-
cation via word prediction under time constraints be-
yond what is achievable with n-gram language mod-
els. These results are somewhat surprising given
conventional wisdom about the superiority of hu-
man language models versus their simplified compu-
tational counterparts. One key reason driving the di-
vergence from conventional wisdom is the time con-
straint on production of predictions. Another is the
artificiality of the task and relative unfamiliarity of
the subjects with the individuals communicating.
While these results are negative, there are reasons
why they should not be taken as an indictment of
the approach as a whole, rather an indication of the
challenges faced by this task. First, we would stress
the fact that we have not yet tested the approach in a
situation where the user knows the speaker well, and
therefore can be presumed to have knowledge well
beyond general knowledge of English and general
topical knowledge. In future work we are planning
experiments based on interactions between people
who have a close relationship with each other. In
such a scenario, we can expect that humans would
have an advantage over statistical language models,
for which appropriate training data would not, in any
case, be available.
None of the domains that we evaluated were a per-
fect match to the application: the text data was not
dialog, and the dialogs were spoken rather than writ-
ten language. Further, the tasks that we evaluated in
this paper are quite rigid compared to what might
be considered acceptable in real use. For example,
our task required the prediction of a particular word
type, whereas in actual use synonyms or other ways
of phrasing the same information will likely be quite
acceptable to most AAC users. In such an applica-
tion, the task is not to facilitate production of a spe-
cific word string, rather production of an idea which
might be realized variously. We were interested in
the tasks reported here as a first step towards under-
standing the problem, and among the lessons learned
are the shortcomings of these very tasks.
Another take-away message relates to the util-
ity of the new interface itself. The subjects in
these trials had the difficult task of quickly pre-
dicting intended words; this is also a communica-
tion task that may be assisted. Providing access to
what n-gram models are predicting may allow the
communication partner to quickly select or winnow
down the options. Further, it is apparent that single
word completions or predictions is not where com-
munication partners are going to achieve order-of-
magnitude speedups in communication; rather such
speedups may be realized in facilitation of larger
phrase or whole utterance production, particularly
when the communication is between familiar part-
ners on known topics.
In summary, this paper presented preliminary re-
sults on the ability of human subjects to provide
word completion and prediction information to users
of AAC systems, through simulation of such a new
interface concept. While the subjects were not
able to match n-gram language models in terms
of keystroke reduction, we did see consistent per-
formance across many subjects and across several
domains, yielding real keystroke reductions on the
stimulus strings. Ultimately, the tasks were not as
representative of real co-construction scenarios a we
would have liked, but they serve to illustrate the
challenges of such an application.
Acknowledgments
This research was supported in part by NIH Grant
#1R01DC009834-01. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NIH.
29
References
D.R. Beukelman, S. Fager, L. Ball, and A. Dietz. 2007.
AAC for adults with acquired neurological conditions:
A review. Augmentative and Alternative Communica-
tion, 23(3):230?242.
Jennifer Cornish and Jeffrey Higginbotham. No Date.
Assessing AAC interaction III: Effect of task type
on co-construction & message repair. AAC-
RERC, available from http:aac-rerc.psu.
edu/_userfiles/asha3.pdf.
J.J. Darragh, I.H. Witten, and M.L. James. 1990. The
reactive keyboard: A predictive typing aid. Computer,
23(11):41?49.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
Switchboard: A telephone speech corpus for research
and develpment. In Proceedings of ICASSP, volume I,
pages 517?520.
D. Jeffery Higginbotham and David Wilkins. 1999.
Frametalker: A system and method for utilizing com-
munication frames in augmented communication tech-
nologies. US Patent No. 5,956,667.
D. Jeffery Higginbotham. 1992. Evaluation of keystroke
savings across five assistive communication technolo-
gies. Augmentative and Alternative Communication,
8:258?272.
Linda A. Hoag, Jan L. Bedrosian, Kathleen F. McCoy,
and Dallas Johnson. 2004. Informativeness and speed
of message delivery trade-offs in augmentative and
alternative communication. Journal of Speech, Lan-
guage, and Hearing Research, 47:1270?1285.
Linda A. Hoag, Jan L. Bedrosian, Kathleen F. Mc-
Coy, and Dallas Johnson. 2008. Hierarchy of
conversational rule violations involving utterance-
based augmentative and alternative communication
systems. Augmentative and Alternative Communica-
tion, 24(2):149?161.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 181?184.
Heidi H. Koester and Simon Levine. 1996. Ef-
fect of a word prediction feature on user perfor-
mance. Augmentative and Alternative Communica-
tion, 12(3):155?168.
Heidi H. Koester and Simon Levine. 1997. Keystroke-
level models for user performance with word predic-
tion. Augmentative and Alternative Communication,
13(4):239257.
Heidi H. Koester and Simon Levine. 1998. Model
simulations of user performance with word predic-
tion. Augmentative and Alternative Communication,
14(1):25?36.
G.W. Lesher, B.J. Moulton, and D.J. Higginbotham.
1998. Techniques for augmenting scanning commu-
nication. Augmentative and Alternative Communica-
tion, 14:81?101.
J. Li and G. Hirst. 2005. Semantic knowledge in word
completion. In Proceedings of the 7th International
ACM Conference on Computers and Accessibility.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Kathleen F. McCoy, Jan L. Bedrosian, Linda A. Hoag,
and Dallas E. Johnson. 2007. Brevity and speed of
message delivery trade-offs in augmentative and alter-
native communication. Augmentative and Alternative
Communication, 23(1):76?88.
Ehud Reiter, Ross Turner, Norman Alm, Rolf Black,
Martin Dempster, and Annalu Waller. 2009. Us-
ing NLG to help language-impaired users tell stories
and participate in social dialogues. In 12th European
Workshop on Natural Language Generation, pages 1?
8. Association for Computational Linguistics.
B. Roark. 2009. Open vocabulary language modeling
for binary response typing interfaces. Technical
Report #CSLU-09-001, Center for Spoken Language
Processing, Oregon Health & Science University.
cslu.ogi.edu/publications/ps/roark09.pdf.
C.E. Shannon. 1950. Prediction and entropy of printed
English. Bell System Technical Journal, 30:50?64.
John Todman, Norman Alm, D. Jeffery Higginbotham,
and Portia File. 2008. Whole utterance approaches in
AAC. Augmentative and Alternative Communication,
24(3):235?254.
K. Trnka, D. Yarrington, K.F. McCoy, and C. Pennington.
2006. Topic modeling in fringe word prediction for
AAC. In Proceedings of the International Conference
on Intelligent User Interfaces, pages 276?278.
K. Trnka, D. Yarrington, J. McCaw, K.F. McCoy, and
C. Pennington. 2007. The effects of word predic-
tion on communication rate for AAC. In Proceed-
ings of HLT-NAACL; Companion Volume, Short Pa-
pers, pages 173?176.
H. Trost, J. Matiasek, and M. Baroni. 2005. The lan-
guage component of the FASTY text prediction sys-
tem. Applied Artificial Intelligence, 19(8):743?781.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 506?513.
David Wilkins and D. Jeffery Higginbotham. 2006. The
short story of Frametalker: An interactive AAC de-
vice. Perspectives on Augmentative and Alternative
Communication, 15(1):18?21.
30
Bruce Wisenburn and D. Jeffery Higginbotham. 2008.
An AAC application using speaking partner speech
recognition to automatically produce contextually rel-
evant utterances: Objective results. Augmentative and
Alternative Communication, 24(2):100?109.
Bruce Wisenburn and D. Jeffery Higginbotham. 2009.
Participant evaluations of rate and communication ef-
ficacy of an AAC application using natural language
processing. Augmentative and Alternative Communi-
cation, 25(2):78?89.
31
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 43?51,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Asynchronous fixed-grid scanning with dynamic codes
Russ Beckley and Brian Roark
Center for Spoken Language Understanding, Oregon Health & Science University
{beckleyr,roark}@cslu.ogi.edu
Abstract
In this paper, we examine several methods
for including dynamic, contextually-sensitive
binary codes within indirect selection typing
methods using a grid with fixed symbol posi-
tions. Using Huffman codes derived from a
character n-gram model, we investigate both
synchronous (fixed latency highlighting) and
asynchronous (self-paced using long versus
short press) scanning. Additionally, we look
at methods that allow for scanning past a tar-
get and returning to it versus methods that re-
move unselected items from consideration. Fi-
nally, we investigate a novel method for dis-
playing the binary codes for each symbol to
the user, rather than using cell highlighting, as
the means for identifying the required input
sequence for the target symbol. We demon-
strate that dynamic coding methods for fixed
position grids can be tailored for very diverse
user requirements.
1 Introduction
For many years, a key focus in Augmentative and
Alternative Communication (AAC) has been provid-
ing text processing capabilities to those for whom
direct selection of symbols on a keyboard (virtual or
otherwise) is not a viable option. In lieu of direct
selection, a binary (yes/no) response can be given
through any number of switches, including buttons
or pads that are pressed with hand, head, or foot,
eyeblink detectors, or other switches that can lever-
age whatever reliable movement is available. These
indirect selection methods typically involve system-
atically scanning through options and eliciting the
binary yes/no response at each step of scanning. For
example, row/column scanning is a very common
approach for indirect selection. Auto row/column
scanning on a square grid, such as that shown in Fig-
ure 1, will highlight each row in turn for some fixed
duration (dwell time); if the binary switch is trig-
gered before the dwell time expires, the row is se-
lected; otherwise the next row is highlighted. Once
a row is selected, cells in this row are then individu-
ally highlighted in turn, until one is selected, which
identifies the intended character.
This sort of indirect selection method amounts to
assigning a binary code to every symbol in the grid.
If triggering the switch (e.g., pressing a button or
blinking) is taken as a ?yes? or 1, then its absence is
taken as a ?no? or 0. In such a way, every letter in the
grid has a binary code based on the scanning strat-
egy. For example, in Figure 1, the letter ?n? is in the
third row and fourth column; if row scanning starts
at the top, it takes two ?no?s and a ?yes? to select the
correct row; and then three ?no?s and a ?yes? to select
the correct column. This translates to a binary code
of ?0010001?.
In the preceding example, the codes for all sym-
bols are determined by their position in the alpha-
ordered grid. However, faster input can be achieved
by assigning shorter codes to likely symbols. For ex-
ample, imagine a user has just typed ?perso? and is
ready to type the next letter. In this context, the let-
ter ?n? is quite likely in English, hence if a very short
code is assigned to that letter (e.g., ?01?), then the
user requires only two actions (a ?no? and a ?yes?)
to produce the letter, rather than the 7 actions re-
Figure 1: Spelling grid in rough alpha order.
43
quired by the row/column code given above. There
are methods for assigning codes that minimize the
expected code length for a given probability model
(Huffman, 1952). The quality of the probability
model used for deriving codes can make a large dif-
ference in the code length and hence in the efficiency
of the input method. When the model can accurately
assign probabilities to symbols, the shortest binary
codes can be assigned to the likeliest symbols, which
thus require the fewest inputs (either yes or no) from
the user. The best probabilistic models will take into
account what has already been typed to assign prob-
ability to each symbol. The probabilities are contex-
tually dependent, and therefore so are the optimal
binary code assignments. This was illustrated in the
?person? example provided earlier. To provide an-
other example, the probability of the letter ?u? is not
particularly high overall in English (less than 0.02),
but if the previously typed symbol is ?q?, its proba-
bility is very high. Thus, in many contexts, there are
other letters that should get the shortest code, but
in that particular context, following ?q?, ?u? is very
likely, hence it should receive the shortest code.
Common scanning methods, however, present a
problem when trying to leverage contextually sen-
sitive language models for efficient scanning. In
particular, methods of scanning that rely on high-
lighting contiguous regions ? such as widely used
row/column scanning ? define their codes in terms
of location in the grid, e.g., upper left-hand cor-
ner requires fewer keystrokes to select than lower
right-hand corner using row/column scanning. To
improve the coding in such an approach requires
moving characters to short-code regions of the grid.
In other words, with row/column scanning meth-
ods, the symbol needing the shortest code must
move into the upper left-hand corner of the grid.
Yet the cognitive overhead of dealing with frequent
grid reorganization is typically thought to outweigh
any speedup that is achieved through more efficient
coding (Baletsa et al, 1976; Lesher et al, 1998).
If one assumes a fixed grid, i.e., no dynamic re-
organization of the symbols, then row/column scan-
ning can gain efficiency by placing frequent char-
acters in the upper left-hand corner, but cannot use
contextually informed models. This is akin to Morse
code, which assigns fixed codes to symbols based on
overall frequency, without considering context.
Figure 2: Scanning of non-contiguous sets of cells
Roark et al (2010) presented a new approach
which dropped the requirement of contiguous high-
lighting, thus allowing the use of variable codes on a
fixed grid. For example, consider the grid in Figure
2, where two symbols in different rows and columns
are jointly highlighted. This approach, which we
will term ?Huffman scanning?, allowed the binary
codes to be optimized using Huffman coding meth-
ods (see Section 2.2) with respect to contextually
sensitive language models without dynamic reorga-
nization of the grid. The method resulted in typing
speedups over conventional row/column scanning.
One downside to the variable scanning that results
from Huffman scanning is that users cannot antici-
pate their target symbol?s binary code in any given
context. In row/column scanning, the binary code
of each symbol is immediately obvious from its lo-
cation in the grid, hence users can anticipate when
they will need to trigger the switch. In Huffman
scanning, users must continuously monitor and react
when their target cells light up. The time required to
allow for this motor reaction means that scan rates
are typically slower than in row/column scanning;
and stress levels ? due to the demands of immediate
response to highlighting ? higher.
Huffman scanning is not the only way to allow
variable coding on a fixed grid. In this paper, we in-
vestigate alternatives to Huffman scanning that also
allow for efficient coding on a fixed grid. The three
alternative methods that we investigate are asyn-
chronous methods, i.e., all of the scanning is self-
paced; there is no scan rate that must be matched by
the user. Rather than ?yes? being a button press and
?no? a timeout, these approaches, like Morse code,
differentiate between short and long presses1. There
are several benefits of this sort of asynchronous ap-
1Alternatively, two switches can be used.
44
proach: individuals who struggle with the timing re-
quirements of auto, step or directed scanning can
proceed without having to synchronize their move-
ments to the interface; individuals can interrupt their
communication ? e.g., for side talk ? for an arbitrary
amount of time and come back to it in exactly the
same state; and it reduces the stress of constantly
monitoring the scanning sequence and reacting to it
within the time limits of the interface.
The last of our alternative methods is a novel ap-
proach that displays the code for each symbol at
once as a series of dots and dashes underneath the
symbol ? as used in Morse code ? rather than us-
ing cell highlighting to prompt the user as in the
other conditions. Unlike Morse code, these codes
are derived using Huffman coding based on n-gram
language models, thus change with every context.
Since they are displayed for the user, no code mem-
orization is required. This novel interface differs
from Huffman scanning in several ways, so we also
present intermediate methods that differ in only one
or another dimension, so that we can assess the im-
pact of each characteristic.
Our results show that displaying entire codes at
once for asynchronous scanning was a popular and
effective method for indirect selection, despite the
fact that it shared certain dis-preferred characteris-
tics with the least popular of our methods. This
points the way to future work investigating methods
to combine the preferred characteristics from our set
of alternatives into a yet more effective interface.
2 Background and Related Work
2.1 Indirect selection
Some of the key issues influencing the work in this
paper have already been mentioned above, such as
the tradeoffs between fixed versus dynamic grids.
For a full presentation of the range of indirect selec-
tion methods commonly in use, we refer the read-
ers to Beukelman and Mirenda (1998). But in this
section we will highlight several key distinctions of
particular relevance to this work.
As mentioned in the previous section, indirect se-
lection strategies allow users to select target sym-
bols through a sequence of simpler operations, typi-
cally a yes/no indication. This is achieved by scan-
ning through options displayed in the user inter-
face. Beukelman and Mirenda (1998) mention cir-
cular scanning (around a circular interface), linear
scanning (one at a time), and group-item scanning
(e.g., row/column scanning to find the desired cell).
Another variable in scanning is the speed of scan-
ning ? e.g., how long does the highlighting linger
on the options before advancing. Finally, there are
differences in selection control strategy. Beukel-
man and Mirenda (1998) mention automatic scan-
ning, where highlighted options are selected by ac-
tivating a switch, and advance automatically if the
switch is not activated within the specified dwell
time; step scanning, where highlighted options are
selected when the switch is not activated within the
specified dwell time, and advance only if the switch
is activated; and directed scanning, where the high-
lighting moves while the switch is activated and se-
lection occurs when the switch is released. In all of
these methods, synchrony with the scan rate of the
interface is paramount.
Speech and language pathologists working with
AAC users must assess the specific capabilities of
the individual to determine their best interface op-
tion. For example, an individual who has difficulty
precisely timing short duration switch activation but
can hold a switch more easily might do better with
directed scanning.
Morse code, with its dots and dashes, is also an in-
direct selection method that has been used in AAC,
but it is far less common than the above mentioned
approaches due to the overhead of memorizing the
codes. Once learned, however, this approach can
be an effective communication strategy, as discussed
with specific examples in Beukelman and Mirenda
(1998). Often the codes are entered with switches
that allow for easy entry of both dots and dashes,
e.g., using two switches, one for dot and one for
dash. In this study, we have one condition that
is similar to Morse code in using dots and dashes,
but without requiring code memorization2. The in-
terface used for the experiments identifies dots and
dashes with short and long keypresses.
2Thanks to a reviewer for pointing out that DynaVox Series
5 displays dynamically-assigned codes for non-letter buttons in
their Morse code interface, much as we do for the entire symbol
set. In contrast to our approach, their codes are not assigned
using probabilistic models, rather to contrast with the standard
Morse codes, which are used for the letters. Further, the cursor
that we use to identify position within the code (see Section 3.5)
is not used in the Dynavox interface.
45
2.2 Binary codes
In indirect selection, the series of actions required to
select a given character is determined by the binary
code. As mentioned in Section 1, row/column scan-
ning assigns binary codes based on location within
the grid. Ordering the symbols so that frequent
characters are located in the upper left-hand cor-
ner of the grid will provide those frequent charac-
ters with short codes with a row/column scanning
approach, though not the minimal possible binary
codes. Given a probability distribution over sym-
bols, there are known algorithms for building a bi-
nary code that has the minimum expected bits ac-
cording to the distribution, i.e., codes will be op-
timally short (Huffman, 1952). The quality of the
codes, however, depends on the quality of the prob-
ability model, i.e., whether the model fits the actual
distribution in that context.
Roark et al (2010) presented a scanning approach
for a fixed grid that used Huffman codes derived
from n-gram language models (see Section 2.3).
The approach leveraged better probability models to
achieve shorter code lengths, and achieved an over-
all speedup over row/column scanning for the 10
subjects in the trial, despite the method being closely
tied to reaction time. The method requires monitor-
ing of the target cell in the grid and reaction when it
is highlighted, since the pattern of highlighting is not
predictable from symbol position in the grid, unlike
row/column scanning.
2.3 Language modeling
Language models assign probabilities to strings in
the language being modeled, which has broad utility
for many tasks in speech and language processing.
The most common language modeling approach is
the n-gram model, which estimates probabilities of
strings as the product of the conditional probability
of each symbol given previous symbols in the string,
under a Markov assumption. That is, for a string
S = s1 . . . sn of n symbols, a k+1-gram model is
defined as
P(S) = P(s1)
n?
i=2
P(si | s1 . . . si?1)
? P(s1)
n?
i=2
P(si | si?k . . . si?1)
where the approximation is made by imposing the
Markov assumption. Note that the probability of the
first symbol s1 is typically conditioned on the fact
that it is first in the string. Each of the conditional
probabilities in such a model is a multinomial dis-
tribution over the symbols in a vocabulary ?, and
the models are typically regularized (or smoothed)
to avoid assigning zero probability to strings in ??.
See Chen and Goodman (1998) for an excellent
overview of modeling and regularization methods.
For the current application, the conditional prob-
ability P(si | si?k . . . si?1) can be used to as-
sign probabilities to all possible next symbols, and
these probabilities can be used to assign Huff-
man codes. For example, if the user has typed
?the perso? and is preparing to type the next letter,
we estimate P( n | t h e p e r s o ) as well as
P( m | t h e p e r s o ) and every other possi-
ble next symbol, from a large corpus. Note that
smoothing methods mentioned above ensure that ev-
ery symbol receives non-zero probability mass. Also
note that the space character (represented above as
? ?) is a symbol in the model, hence the models take
into account context across word boundaries. Given
these estimated probabilities, known algorithms for
assigning Huffman codes are used to assign short
codes to the most likely next symbols, in a way that
minimizes expected code length.
3 Methods
Since this paper aims to compare new methods with
Huffman scanning presented in Roark et al (2010),
we follow that paper in many key respects, including
training data, test protocol, and evaluation measures.
For all trials we use a 6?6 grid, as shown in Fig-
ures 1 and 2, which includes the 26 characters in the
English alphabet, 8 punctuation characters (comma,
period, double quote, single quote, dash, dollar sign,
colon and semi-colon), a white space delimiter (de-
noted with underscore) and a delete symbol (de-
noted with ?). Unlike Roark et al (2010), our
grid is in rough alphabetic order rather than in fre-
quency order. In that paper, they compared Huffman
scanning with row/column scanning, which would
have been put at a disadvantage with alphabetic or-
der, since frequent characters would have received
longer codes than they do in a frequency ordered
grid. In this paper, however, all of the approaches
46
are using Huffman codes and scanning of possibly
non-contiguous subsets of characters, so the code
efficiency does not depend on location in the grid.
Thus for ease of visual scanning, we chose in this
study to use alphabetic ordering.
3.1 Language models and binary codes
We follow Roark et al (2010) and build character-
based smoothed 8-gram language models from a
normalized 42M character subset of the English gi-
gaword corpus and the CMU pronunciation dictio-
nary. This latter lexicon is used to increase coverage
of words that are unobserved in the corpus, and is in-
cluded in training as one observation per word in the
lexicon. Smoothing is performed with a generalized
version of Witten-Bell smoothing (Witten and Bell,
1991) as presented in Carpenter (2005). Text nor-
malization and smoothing parameterizations were as
presented in Roark et al (2010). Probability of the
delete symbol ? was taken to be 0.05 in all trials
(the same as the probability of an error, see Sec-
tion 3.2), and all other probabilities derived from the
trained n-gram language model.
3.2 Huffman scanning
Our first scanning condition replicates the Huffman
scanning from Roark et al (2010), with two differ-
ences. First, as stated above, we use an alphabetic
ordering of the grid as shown in Figure 2, in place
of their frequency ordered grid. Second, rather than
calibrating the scan rate of each individual, we fixed
the scan rate at 600 ms across all subjects.
One key aspect of their method is dealing with
errors of omission and commission, i.e., what hap-
pens when a subject misses their target symbol. In
standard row/column scanning, rows are highlighted
starting from the top of the grid, incrementing down-
wards one row at a time. If no row has been selected
after iterating through all rows, the scanning begins
again at the top. In such a way, if the subject mistak-
enly neglects to select their intended row, they can
just wait until it is highlighted again. Similarly, if the
wrong row is selected, there is usually a mechanism
whereby the columns are scanned for some number
of iterations, at which point row scanning resumes.
The upshot of this is that users can make an error and
still manage to select their intended symbol after the
scanning system returns to it.
Roark et al (2010) present a method for allow-
ing the same kind of robustness to error in Huff-
man scanning, by recomputing the Huffman code
after every bit. If the probability that the bit was
correct is p, then the probability that it was incor-
rect is 1?p. In Huffman scanning, a subset is high-
lighted and the user indicates yes or no ? yes, the
target symbol is in the set; or no, the target symbol
is not in the set. If the answer is ?yes? and the set
includes exactly one symbol, it is typed. Otherwise,
for all symbols in the selected set (highlighted sym-
bols if ?yes?; non-highlighted if ?no?), their proba-
bilities are multiplied by p (the probability of being
correct), while the probabilities of the other set of
symbols are multiplied by 1?p. The probabilities
are then re-normalized and a new Huffman code is
generated, the first bit of which drives which sym-
bols are highlighted at the next step. In such a way,
even if the target symbol is in the highlighted set
when it is not selected (or vice versa), it is not elim-
inated from consideration; rather its probability is
diminished (by multiplying by 1?p, which in this
paper is set to 0.05) and scanning continues. Even-
tually the symbol will be highlighted again, much
as is the case in row/column scanning. We also use
this method within the Huffman scanning condition
reported in this paper.
3.3 Asynchronous scanning
Our second condition replaces the scan rate of 600
ms from the Huffman scanning approach outlined
in Section 3.2 with an asynchronous approach that
does not rely upon a scan rate. The grid and scan-
ning method remain identical, but instead of switch
versus no switch, we use short switch (rapid release)
versus long switch (slower release). This is similar
to the dot/dash distinction in Morse code. For this
paper, we used a threshold of 200 ms to distinguish
a short versus a long switch, i.e., if the button press
is released within 200 ms it is short; otherwise long.
Since Huffman scanning already has switch activa-
tion as ?yes?, this could be thought of as having the
long press replace no-press in the interface.
With this change, the scanning does not automat-
ically advance to the next set, but waits indefinitely
for the user to enter the next bit of the code. The
same method for dealing with errors as with Huff-
man scanning is employed in this condition, i.e., re-
47
Figure 3: Scanning of non-contiguous sets of cells, with
symbols that have been eliminated from consideration
deemphasized (a, b, c, e, o, t)
computing the Huffman code after every bit and tak-
ing into account the probability of the bit being in
error. One might see this as a self-paced version of
Huffman scanning.
One benefit of this approach is that it does not re-
quire the user to synchronize their movements to a
particular scan rate of the interface. One potential
downside for some users is that it does require more
active keypresses than auto scanning. In auto scan-
ning, only the ?1? bits of the code require switch ac-
tivation; the ?0? bits are produced passively by wait-
ing for the dwell time to expire. In contrast, all bits
in the asynchronous approaches require one of two
kinds of switch activation.
3.4 Not returning to non-selected symbols
Our third condition is just like the second except
it does not recompute the Huffman codes after ev-
ery bit, changing the way in which user errors are
handled. At the start of the string or immediately
after a letter has been typed, the Huffman codes
are calculated in exactly the same way as the pre-
vious two conditions, based on the n-gram language
model given the history of what has been typed so
far. However, after each bit is entered for the cur-
rent symbol, rather than multiplying by p and 1?p as
detailed in Section 3.2, symbols that have not been
selected are eliminated from consideration and will
not be highlighted again, i.e., will not be returned to
for subsequent selection. For example, in Figure 3
we see that there is a set of highlighted characters,
but also a set of characters that have been eliminated
from consideration and are deemphasized in the in-
terface to indicate that they can no longer be selected
(specifically: a, b, c, e, o and t). Those are symbols
that were not selected in previous steps of the scan-
ning, and are no longer available to be typed in this
position. If the user makes a mistake in the input,
eliminating the actual target symbol, the only way
to fix it is to type another symbol, delete it, and re-
type the intended symbol.
This condition is included in the study because
recalculation of codes after every bit becomes prob-
lematic when the codes are explicitly displayed (the
next condition). By including these results, we can
tease apart the impact of not recalculating codes af-
ter every bit versus the impact of displaying codes in
the next condition. Later, in the discussion, we will
return to this characteristic of the interface and dis-
cuss some alternatives that may allow for different
error recovery strategies.
This change to the interface has a couple of impli-
cations. First, the optimal codes are slightly shorter
than with the previous Huffman scanning methods,
since no probability mass is reserved for errors. In
other words, the perfect user that never makes a mis-
take would be able to type somewhat faster with
this method, which is not surprising, since reserv-
ing probability for returning to something that was
rejected is of no utility if no mistakes are ever made.
The experimental results presented later in the pa-
per will show explicitly how much shorter the codes
are for our particular test set. Second, it is possi-
ble to type a symbol without ever actively selecting
it, if all other symbols in the grid have been elimi-
nated. For example, if there are two symbols left and
the system highlights one symbol, which is rejected,
then the other symbol is typed. This contrasts with
the previous methods that only type when a single
character set is actively selected.
3.5 Displaying codes
Our final condition also does not recompute codes
after every bit, but in addition does away with high-
lighting of cells as the mechanism for scanning, and
instead displays dots and dashes directly beneath
each letter in the fixed grid. For example, Figure
4 shows the dots and dashes required for each let-
ter directly below that letter in the grid, and Figure
5 shows a portion of that grid magnified for easier
detailed viewing. Each code includes the dots and
dashes required to input that symbol, plus a cursor
?|? that indicates how much of the code has already
48
Figure 4: Scanning of non-contiguous sets of cells, dis-
playing dots and dashes rather than highlighting
Figure 5: A magnification of part of the above grid
been entered. For example, to type the letter ?s? us-
ing the code in Figure 5 , one must input: long, short,
short, long, short.
Since these codes are displayed, there is no mem-
orization required to input the target symbol. Like
row/column scanning, once the target symbol has
been found in the grid, the input sequence is known
in entirety by the user, which can facilitate planning
of sequences of actions rather than simply reacting
to updates in the interface. The cursor helps the user
know where they are in the code, which can be help-
ful for long codes. Figure 6 shows a magnification
of the interface when there are only two options re-
maining ? a dot selects ?l? and a dash selects ?u?.
4 Experiments
We recruited 10 native English speaking subjects be-
tween the ages of 26 and 50 years, who are not users
Figure 6: Cursor shows how much of code has been en-
tered
of scanning interfaces for typing and have typical
motor function. Following Roark et al (2010), we
use the phrase set from MacKenzie and Soukoreff
(2003) to measure typing performance, and the same
five strings from that set were used as evaluation
strings in this study as in Roark et al (2010). Prac-
tice strings were randomly selected from the rest of
the phrase set. Subjects used an Ablenet Jellybean R?
button as the binary switch. The error rate parameter
was fixed at 5% error rate.
The task in all conditions was to type the pre-
sented phrase exactly as it is presented. Symbols
that are typed in error ? as shown in Figure 7 ? must
be repaired by selecting the delete symbol (?) to
delete the incorrect symbol, followed by the correct
symbol. The reported times and bits take into ac-
count the extra work required to repair errors.
We tested subjects under four conditions. All four
conditions made use of 8-gram character language
models and Huffman coding, as described in Sec-
tion 3.1, and an alpha-ordered grid. The first condi-
tion is a replication of the Huffman scanning condi-
tion from Roark et al (2010), with the difference in
scan rate (600ms versus mean 475ms in their paper)
and the grid layout. This is an auto scan approach,
where the highlighting advances at the end of the
dwell time, as described in Section 3.2. The second
condition is asynchronous scanning, i.e., replacing
the dwell time with a long button press as described
in Section 3.3, but otherwise identical to condition 1.
The third condition was also asynchronous, but did
not recompute the binary code after every bit, so that
there is no return to characters eliminated from con-
sideration, as described in Section 3.4, but otherwise
identical to condition 2. Finally, the fourth condition
Figure 7: After an incorrect symbol is typed, it must be
deleted and the correct symbol typed in its place
49
Speed (cpm) Bits per character Error rate Long code rate
Scanning condition mean (std) mean (std) opt. mean (std) mean (std)
1.Huffman Roark et al (2010) 23.4 (3.7) 4.3 (1.1) 2.6 4.1 (2.2) 19.3 (14.2)
synchronous This paper 25.5 (3.2) 3.3 (0.4) 2.6 1.8 (1.1) 7.3 (4.1)
2. Huffman asynchronous 20.0 (3.7) 3.1 (0.2) 2.6 3.1 (2.5) 3.8 (1.2)
3. Huffman asynch, no return 17.2 (3.2) 3.1 (0.3) 2.4 7.7 (2.7) 0 (0)
4. Huffman asynch, display codes 18.7 (3.9) 3.0 (0.3) 2.4 6.9 (2.5) 0 (0)
Table 1: Typing results for 10 users on 5 test strings (total 31 words, 145 characters) under 4 conditions.
displays the codes for each character as described in
Section 3.5, without highlighting, but is otherwise
identical to condition 3.
Subjects were given a brief demo of the four con-
ditions by an author, then proceeded to a practice
phase. Practice phrases were given in each of the
four conditions, until subjects reached sufficient pro-
ficiency in the method to type a phrase with fewer
than 10% errors. After the practice phases in all four
conditions were completed, the test phases com-
menced. The ordering of the conditions in the test
phase was random. Subjects again practiced in a
condition until they typed a phrase with fewer than
10% errors, and then were presented with the five
test strings in that condition. After completion of
the test phase for a condition, they were prompted to
fill out a short survey about the condition.
Table 1 presents means and standard deviations
across our subjects for characters per minute, bits
per character, error rate and what Roark et al (2010)
termed ?long code rate?, i.e., percentage of sym-
bols that were correctly selected after being scanned
past. For condition 1, we also present the result for
the same condition reported in Roark et al (2010).
Comparing the first two rows of that table, we can
see that our subjects typed slightly faster than those
reported in Roark et al (2010) in condition 1, with
fewer bits per character, mainly due to lower error
rates and less scanning past targets. This can be at-
tributed to either the slower scanning speed or the al-
phabetic ordering of the grid (or both). In any case,
even with the slower scan rate, the overall speed is
faster in this condition than what was reported in that
paper.
The other three conditions are novel to this paper.
Moving from synchronous to asynchronous (with
long press) but leaving everything else the same
Survey Huffman Huffman No Display
Question synch asynch return codes
Fatigued 2.1 3.2 3.4 2.5
Stressed 1.9 2.2 2.9 2.0
Liked it 3.8 3.0 2.3 3.5
Frustrated 1.9 2.8 4.0 2.4
Table 2: Mean Likert scores to survey questions (5 = a
lot; 1 = not at all)
(condition 2) leads to slower typing speed but fewer
bits per character. The error rate is higher than in
the synchronous condition 1, but there is less scan-
ning past the target symbol. In discussion with sub-
jects, the higher error rate might be attributed to los-
ing track of which button press (short or long) goes
with highlighting, or also to intended short presses
being registered by the system as long.
The final two conditions allow no return to char-
acters once they have been scanned past, hence the
?long code rates? go to zero, and the error rates in-
crease. Note that the optimal bits per character are
slightly better than in the other trials, as mentioned
in Section 3.4, yet the subject bits per character stay
mostly the same as with condition 2. Typing speed
is slower in these two conditions, though slightly
higher when the codes are displayed versus the use
of highlighting.
In Table 2 we present the mean Likert scores from
the survey. The four statements that subjects as-
sessed were:
1. I was fatigued by the end of the trial
2. I was stressed by the end of the trial
3. I liked this trial
4. I was frustrated by this trial
The scores were: 1 (not at all); 2 (a little); 3 (not
sure); 4 (somewhat) and 5 (a lot).
50
The results in Table 2 show high frustration and
stress with condition 3, and much lower fatigue,
stress and frustration (hence higher ?liking?) for con-
dition 4, where the codes are displayed. Overall,
there seemed to be a preference for Huffman syn-
chronous, followed by displaying the codes.
5 Discussion
There are several take-away lessons from this ex-
periment. First, the frustration and slowdown that
result from the increased error rates in condition 3
make this a dispreferred solution, even though dis-
allowing returning to symbols that have been ruled
out in scanning reduced the bits per character (opti-
mal and in practice). Yet in order to display a stable
code in condition 4 (which was popular), recalcula-
tion of codes after every bit (as is done in the first
two conditions) is not an option. To make condi-
tion 4 more effective, some effective means for al-
lowing scanning to return to symbols that have been
scanned past must be devised.
Second, asynchronous scanning does seem to be
a viable alternative to auto scanning, which may be
of utility for certain AAC users. Such an approach
may be well suited to individuals using two switches
for asynchronous row/column scanning. Other users
may find the increased level of switch activation re-
quired for scanning in these conditions too demand-
ing. One statistic not shown in Table 1 is number
of keypresses required. In condition 1, some of the
?bits? required to type the character are produced by
not pressing the button. In the other three conditions,
all ?bits? result from either a short or long press, so
the button is pressed for every bit. In condition 1,
the mean number of key presses per character was
1.5, which is approximately half of the total button
presses required per character in the other methods.
Future directions include investigations into
methods that combine some of the strengths of the
various approaches. In particular, we are interested
in methods that allow for the direct display of codes
for either synchronous or asynchronous scanning,
but which also allow for scanning past and return to
target characters that were mistakenly not selected.
The benefit of displaying codes ? allowing for an-
ticipation and planning in scanning ? are quite high,
and this paper has not exhausted the exploration of
such approaches. Among the alternatives being con-
sidered are: requiring all codes to have a short press
(confirmation) bit as the last bit of the code; having
a ?reset? symbol or gesture; and recalculating codes
after some number of bits, greater than one. Each
of these methods would somewhat increase the op-
timal bits per character, but may result in superior
user performance. Finally, we intend to include ac-
tive AAC users in subsequent studies of these meth-
ods.
References
G. Baletsa, R. Foulds, and W. Crochetiere. 1976. Design
parameters of an intelligent communication device. In
Proceedings of the 29th Annual Conference on Engi-
neering in Medicine and Biology, page 371.
D. Beukelman and P. Mirenda. 1998. Augmentative and
Alternative Communication: Management of Severe
Communication Disorders in Children and Adults.
Paul H. Brookes, Baltimore, MD, second edition.
B. Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software, pages 86?99.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report, TR-10-98, Harvard University.
D.A. Huffman. 1952. A method for the construction of
minimum redundancy codes. In Proceedings of the
IRE, volume 40(9), pages 1098?1101.
G.W. Lesher, B.J. Moulton, and D.J. Higginbotham.
1998. Techniques for augmenting scanning commu-
nication. Augmentative and Alternative Communica-
tion, 14:81?101.
I.S. MacKenzie and R.W. Soukoreff. 2003. Phrase sets
for evaluating text entry techniques. In Proceedings of
the ACM Conference on Human Factors in Computing
Systems (CHI), pages 754?755.
B. Roark, J. de Villiers, C. Gibbons, and M. Fried-Oken.
2010. Scanning methods and language modeling for
binary switch typing. In Proceedings of the NAACL-
HLT Workshop on Speech and Language Processing
for Assistive Technologies (SLPAT), pages 28?36.
I.H. Witten and T.C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4):1085?1094.
51
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 56?64,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Robust kaomoji detection in Twitter
Steven Bedrick, Russell Beckley, Brian Roark, Richard Sproat
Center for Spoken Language Understanding, Oregon Health & Science University
Portland, Oregon, USA
Abstract
In this paper, we look at the problem of robust
detection of a very productive class of Asian
style emoticons, known as facemarks or kao-
moji. We demonstrate the frequency and pro-
ductivity of these sequences in social media
such as Twitter. Previous approaches to detec-
tion and analysis of kaomoji have placed lim-
its on the range of phenomena that could be
detected with their method, and have looked
at largely monolingual evaluation sets (e.g.,
Japanese blogs). We find that these emoticons
occur broadly in many languages, hence our
approach is language agnostic. Rather than
relying on regular expressions over a prede-
fined set of likely tokens, we build weighted
context-free grammars that reward graphical
affinity and symmetry within whatever sym-
bols are used to construct the emoticon.
1 Introduction
Informal text genres, such as email, SMS or social
media messages, lack some of the modes used in
spoken language to communicate affect ? prosody
or laughter, for example. Affect can be provided
within such genres through the use of text format-
ting (e.g., capitalization for emphasis) or through the
use of extra-linguistic sequences such as the widely
used smiling, winking ;) emoticon. These sorts of
vertical face representations via ASCII punctuation
sequences are widely used in European languages,
but in Asian informal text genres another class of
emoticons is popular, involving a broader symbol set
and with a horizontal facial orientation. These go by
the name of facemarks or kaomoji. Figure 1 presents
|?_?|
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
57603166048497664
57596757185544192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
57745944376119296 (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
[o_-]
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
6031 6048497664
5967 7185 4192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
74594 376119  (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
\(?v?)/
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
57603166048497664
57596757185544192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
57745944376119296 (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
Figure 1: Some representative kaomoji emoticons
several examples of these sequences, including both
relatively common kaomoji as well as more exotic
and complex creations.
This class of emoticon is far more varied and pro-
ductive than the sideways European style emoticons,
and even lists of on the order of ten thousand emoti-
cons will fail to cover all instances in even a mod-
est sized sample of text. This relative productiv-
ity is due to several factors, including the horizon-
tal orientation, which allows for more flexibility in
configuring features both within the face and sur-
rounding the face (e.g., arms) than the vertical ori-
entation. Another important factor underlying kao-
moji productivity is historical in nature. kaomoji
were developed and popularized in Japan and other
Asian countries whose scripts have always required
multibyte character encodings, and whose users of
electronic communication systems have significant
experience working with characters beyond those
found in the standard ASCII set.
Linguistic symbols from various scripts can be
appropriated into the kaomoji for their resemblence
to facial features, such as a winking eye, and au-
thors of kaomoji sometimes use advanced Unicode
techniques to decorate glyphs with elaborate com-
binations of diacritic marks. For example, the kao-
56
moji in the top righthand corner of Figure 1, includes
an Arabic letter, and Thai vowel diacritics. Accu-
rate detection of these tokens ? and other common
sequences of extra-linguistic symbol sequences ? is
important for normalization of social media text for
downstream applications.
At the most basic level, the complex and unpre-
dictable combinations of characters found within
many kaomoji (often including punctuation and
whitespace, as well as irregularly-used Unicode
combining characters) can seriously confound sen-
tence and word segmentation algorithms that at-
tempt to operate on kaomoji-rich text; since segmen-
tation is typically the first step in any text process-
ing pipeline, issues here can cause a wide variety
of problems downstream. Accurately removing or
normalizing such sequences before attempting seg-
mentation can ensure that existing NLP tools are
able to effectively work with and analyze kaomoji-
including text.
At a higher level, the inclusion of a particular
kaomoji in a text represents a conscious decision
on the part of the text?s author, and fully interpret-
ing the text necessarily involves a degree of inter-
pretation of the kaomoji that they chose to include.
European-style emoticons form a relatively closed
set and are often fairly straightforward to interpret
(both in terms of computational, as well as human,
effort); kaomoji, on the other hand, are far more di-
verse, and interpretation is rarely simple.
In this paper, we present preliminary work on
defining robust models for detecting kaomoji in so-
cial media text. Prior work on detecting and classi-
fying these extra-linguistic sequences has relied on
the presence of fixed attested patterns (see discus-
sion in Section 2) for detection, and regular expres-
sions for segmentation. While such approaches can
capture the most common kaomoji and simple vari-
ants of them, the productive and creative nature of
the phenomenon results in a non-negligible out-of-
vocabulary problem. In this paper, we approach the
problem by examining a broader class of possible
sequences (see Section 4.2) for symmetry using a
robust probabilistic context-free grammar with rule
probabilities proportional to the symmetry or affin-
ity of matched terminal items in the rule. Our PCFG
is robust in the sense that every candidate sequence
is guaranteed to have a valid parse. We use the re-
sulting Viterbi best parse to provide a score to the
candidate sequence ? reranking our high recall list
to achieve, via thresholds, high precision. In addi-
tion, we investigate unsupervised model adaptation,
by incorporating Viterbi-best parses from a small set
of attested kaomoji scraped from websites; and in-
ducing grammars with a larger non-terminal set cor-
responding to regions of the face.
We present bootstrapping experiments for deriv-
ing highly functional, language independent models
for detecting kaomoji in text, on multilingual Twit-
ter data. Our approach can be used as part of a
stand-alone detection model, or as input into semi-
automatic kaomoji lexicon development. Before de-
scribing our approach, we will first present prior
work on this class of emoticon.
2 Prior Work
Nakamura et al (2003) presented a natural language
dialogue system that learned a model for generat-
ing kaomoji face marks within Japanese chat. They
trained a neural net to produce parts of the emoti-
con ? mouth, eyes, arms and ?optional things? as
observed in real world data. They relied on a hand-
constructed inventory of observed parts within each
of the above classes, and stitched together predicted
parts into a complete kaomoji using simple tem-
plates.
Tanaka et al (2005) presented a finite-state
chunking approach for detecting kaomoji in
Japanese on-line bulletin boards using SVMs with
simple features derived from a 7 character window.
Training was performed on kaomoji dictionaries
found online. They achieved precision and recall in
the mid-80s on their test set, which was a significant
recall improvement (17% absolute) and modest
precision improvement (1.5%) over exact match
within the dictionaries. They note certain kinds of
errors, e.g., ?(Thu)? which demonstrate that their
chunking models are (unsurprisingly) not capturing
the typical symmetry of kaomoji. In addition, they
perform classification of the kaomoji into 6 rough
categories (happy, sad, angry, etc.), achieving high
performance (90% accuracy) using a string kernel
within an SVM classifier.
Ptaszynski et al (2010) present work on a large
database of kaomoji, which makes use of an analy-
57
sis of the gestures conveyed by the emoticons and
their relation to a theory of non-verbal expressions.
They created an extensive (approximately 10,000
entry) lexicon with 10 emotion classes, and used
this database as the basis of both emoticon extrac-
tion from text and emotion classification. To detect
an emoticon in text, their system (named ?CAO?)
looked for three symbols in a row from a vocabulary
of the 455 most frequent symbols in their database.
Their approach led to a 2.4% false negative rate
when evaluated on 1,000 sentences extracted from
Japanese blogs. Once detected, the system extracts
the emoticon from the string using a gradual relax-
ation from exact match to approximate match, with
various regular expressions depending on specific
partial match criteria. A similar deterministic al-
gorithm based on sequenced relaxation from exact
match was used to assign affect to the emoticon.
Our work focuses on the emoticon detection
stage, and differs from the above systems in a num-
ber of ways. First, while kaomoji were popularized
in Asia, and are most prevalent in Asian languages,
they not only found in messages in those languages.
In Twitter, which is massively multilingual, we find
kaomoji with some frequency in many languages,
including European languages such as English and
Portuguese, Semitic languages and a range of Asian
languages. Our intent is to have a language inde-
pendent algorithm that looks for such sequences in
any message. Further, while we make use of online
dictionaries as development data, we appreciate the
productivity of the phenomenon and do not want to
restrict the emoticons that we detect to those con-
sisting of pre-observed characters. Hence we focus
instead on characteristics of kaomoji that have been
ignored in the above models: the frequent symmetry
of the strings. We make use of context-free mod-
els, built in such a way as to guarantee a parse for
any candidate sequence, which permits exploration
of a much broader space of potential candidates than
the prior approaches, using very general models and
limited assumptions about the key components of
the emoticons.
3 Data
Our starting resources consisted of a large, multi-
lingual corpus of Twitter data as well as a smaller
collection of kaomoji scraped from Internet sources.
Our Twitter corpus consists of approximately 80
million messages collected using Twitter?s ?Stream-
ing API? over a 50-day period from June through
August 2011. The corpus is extremely linguistically
diverse; human review of a small sample identified
messages written in >30 languages. The messages
themselves exhibit a wide variety of phenomena, in-
cluding substantial use of different types of Inter-
net slang and written dialect, as well as numerous
forms of non-linguistic content such as emoticons
and ?ASCII art.?
We took a two-pronged approach to developing
a set of ?gold-standard? kaomoji. Our first ap-
proach involved manually ?scraping? real-world ex-
amples from the Internet. Using a series of hand-
written scripts, we harvested 9,193 examples from
several human-curated Internet websites devoted to
collecting and exhibiting kaomoji. Many of these
consisted of several discrete sub-units, typically in-
cluding at least one ?face? element along with a
small amount of additional content. For example,
consider the following kaomoji, which appeared in
this exact form eight times in our Twitter corpus:
?(*???)??+.???????+.??
(???*)? 9
?(!"?" ? ?)??????#? 8
?(??????)??? 9
?(???`;)???? 52
??????? 5
?_(?_? ) 1
?/T?T)/??????? 4
???????????????????????? 1
?????? 1
????? 426
?????? 1
?(?????)/????? 1
?????????????? 4
??(?????)(??_ _)???? 10
????(???)??????? 1
????(T-T)?(^^ )???? 2
???Uo???oU??? 1
?????+.(???)(???)?+.?!! 1
. Note that, in this case, the
?face? is followed by a small amount of hiragana,
and that the message concludes with a dingbat in the
form of a ?heart? symbol.1
Of these 9,193 scraped examples, we observed
?3,700 to appear at least once in our corpus of
Twitter messages, and ?2,500 more than twice.
The most common kaomoji occurred with frequen-
cies in the low hundreds of thousands, although the
frequency with which individual kaomoji appeared
roughly followed a power-law distribution, meaning
that there were a small number that occurred with
great frequency and a much larger number that only
appeared rarely.
From this scraped corpus, we attempted to iden-
tify a subset that consisted solely of ?faces? to serve
as a high-precision training set. After observing
that nearly all of the faces involved a small number
of characters bracketed one of a small set of natu-
ral grouping characters (parentheses, ?curly braces,?
1Note as well that this kaomoji includes not only a wide va-
riety of symbols, but that some of those symbols are themselves
modified using combining diacritic marks. This is a common
practice in modern kaomoji, and one that complicates analysis.
58
etc.), we extracted approximately 6,000 substrings
matching a very simple regular expression pattern.
This approach missed many kaomoji, and of the
examples that it did detect, many were incom-
plete (in that they were missing any extra-bracketed
content? arms, ears, whiskers, etc.) However, the
contents of this ?just faces? sub-corpus offered de-
cent coverage of many of the core kaomoji phenom-
ena in a relatively noise-free manner. As such, we
found it to be useful as ?seed? data for the grammar
adaptation described in section 4.4.
In addition to our ?scraped? kaomoji corpus, we
constructed a smaller corpus of examples drawn di-
rectly from our Twitter corpus. The kaomoji phe-
nomenon is complex enough that capturing it in its
totality is difficult. However, it is possible to capture
a subset of kaomoji by looking for regions of per-
fect lexical symmetry. This approach will capture
many of the more regularly-formed and simple kao-
moji (for example, ?(-_-)?), although it will miss
many valid kaomoji. Using this approach, we iden-
tied 3,580 symmetrical candidate sequences; most
of these were indeed kaomoji, although there were
several false positives (for example, symmetrical se-
quences of repeated periods, question marks, etc.).
Using simple regular expressions, we were able to
remove 289 such false positives.
Interestingly, there was very little overlap be-
tween the corpus scraped from the Web and the sym-
metry corpus. A total of 39 kaomoji appeared in ex-
actly the same form in both sets. We noted, however,
that the kaomoji harvested from the Web tended to
be longer and more elaborate than those identified
from our Twitter corpus using the symmetry heuris-
tic (Mann-Whitney U, p < 0.001), and as previously
discussed, the Web kaomoji often contained one or
more face elements. Thus we expanded our defi-
nition of overlap, and counted sequences from the
symmetrical corpus that were substrings of scraped
kaomoji. Using this criterion, we identified 177 pos-
sibly intersecting kaomoji. The fact that so few indi-
vidual examples occurred in both corpora illustrates
the extremely productive nature of the phenomenon.
4 Methods
4.1 Graphical similarity
The use of particular characters in kaomoji is ul-
timately based on their graphical appearance. For
Figure 2: Ten example character pairs with imperfect
(but very high) symmetry identified by our algorithm.
Columns are: score, hex code point 1, hex code point
2, glyph 1, glyph 2.
example, good face delimiters frequently include
mated brackets or parentheses, since these elements
naturally look as if they delimit material. Further-
more, there are many characters which are not tech-
nically ?paired,? but look roughly more-or-less sym-
metrical. For example, the Arabic-Indic digits!???"and
!???" are commonly used as bracketing delimiters, for
example: !???". These characters can serve both as
?arms? as well as ?ears.?
Besides bracketing, symmetry plays an additional
role in kaomoji construction. Glyphs that make good
?eyes? are often round; ?noses? are often symmet-
ric about their central axis. Therefore a measure of
graphical similarity between characters is desirable.
To that end, we developed a very simple measure
of similarity. From online sources, we downloaded
a sample glyph for each code point in the Unicode
Basic Multilingual Plane, and extracted a bitmap for
each. In comparing two glyphs we first scale them
to have the same aspect ratio if necessary, and we
then compute the proportion of shared pixels be-
tween them, with a perfect match being 1 and the
worst match being 0. We can thus compute whether
two glyphs look similar; whether one glyph is a good
mirror image of the other (by comparing glyph A
with the mirror image of glyph B); and whether a
glyph is (vertically) symmetric (by computing the
similarity of the glyph and its vertical mirror image).
The method, while clearly simple-minded,
nonetheless produces plausible results, as seen in
Figure 2, which shows the best 10 candidates for
mirror image character pairs. We also calculate
the same score without flipping the image verti-
cally, which is also used to score possible symbol
matches, as detailed in Section 4.3.
59
4.2 Candidate extraction
We perform candidate kaomoji extraction via a very
simple hidden Markov model, which segments all
strings of Unicode graphemes into contiguous re-
gions that are either primarily linguistic (mainly
language encoding symbols2) or primarily non-
linguistic (mainly punctuation, or other symbols).
Our candidate emoticons, then, are this extensive
list of mainly non-linguistic symbol sequences. This
is a high recall approach, returning most sequences
that contain valid emoticons, but quite low precision,
since it includes many other sequences as well (ex-
tended runs of punctuation, etc.).
The simple HMM consists of 2 states: call them
A (mainly linguistic) and @ (mainly non-linguistic).
Since there are two emitted symbol classes (linguis-
tic L and non-linguistic N ), each HMM state must
have two emission probabilities, one for its domi-
nant symbol class (L in A and N in @) and one
for the other symbol class. Non-linguistic symbols
occur quite often in linguistic sequences, as punc-
tuation for example. However, sequences of, say,
3 or more in a row are not particularly frequent.
Similarly, linguistic symbols occur often in kaomoji,
though not often in sequences of, say, 3 or more.
Hence, to segment into contiguous sequences of a
certain number in a row, the probability of transition
from state A to state @ or vice versa must be signif-
icantly lower than the probability of emitting one or
two N from A states or L from @ states. We thus
have an 8 parameter HMM (four transition and four
emission probabilities) that was coarsely parameter-
ized to have the above properties, and used it to ex-
tract candidate non-linguistic sequences for evalua-
tion by our PCFG model.
Note that this approach does have the limitation
that it will trim off some linguistic symbols that oc-
cur on the periphery of an emoticon. Future versions
of this part of the system will address this issue by
extending the HMM. For this paper, we made use of
a slightly modified version of this simple HMM for
candidate extraction. The modifications involved the
addition of a special input state for whitespace and
full-stop punctuation, which helped prevent certain
very common classes of false-positive.
2Defined as a character having the Unicode ?letter? charac-
ter property.
rule score rule score
X? a X b S(a,b) X? a b S(a,b)
X? a X  X? X a 
X? a ? X? X X ?
Table 1: Rule schemata for producing PCFG
4.3 Baseline grammar induction
We perform a separate PCFG induction for ev-
ery candidate emoticon sequence, based on a small
set of rule templates methods for assigning rule
weights. By inducing small, example-specific
PCFGs, we ensure that every example has a valid
parse, without growing the grammar to the point that
the grammar constant would seriously impact parser
efficiency.
Table 1 shows the rule schemata that we used for
this paper. The resulting PCFG would have a single
non-terminal (X) and the variables a and b would be
instantiated with terminal items taken from the can-
didate sequence. Each instantiated rule receives a
probability proportional to the assigned score. For
the rules that ?pair? symbols a and b, a score is as-
signed in two ways, call them S1(a, b) and S2(a, b)
(they will be defined in a moment). Then S(a, b) =
max(S1(a, b) and S2(a, b)). If S(a, b) < ?, for some
threshold ?,3 then no rule is generated. S1 is the
graphical similarity of the first symbol with the verti-
cal mirror image of the second symbol, calculated as
presented in Section 4.1. This will give a high score
for things like balanced parentheses. S2 is the graph-
ical similarity of the first symbol with the second
symbol (not vertically flipped), which gives high
scores to the same or similar symbols. This permits
matches for, say, eyes that are not symmetric due to
an orientation of the face, e.g.,
!???"
(!#!). The other pa-
rameters (, ? and ?) are included to allow for, but
penalize, unmatched symbols in the sequence.
All possible rules for a given sequence are instan-
tiated using these templates, by placing each symbol
in the a slot with all subsequent symbols in the b slot
and scoring, as well as creating all rules with just a
alone for that symbol. For example, if we are given
the kaomoji (o o;) specific rules would be created
if the similarity scores were above threshold. For the
second symbol ?o?, the algorithm would evaluate the
3For this paper, ? was chosen to be 0.7.
60
similarity between ?o? and each of the four symbols
to its right , o, ; and ).
The resulting PCFG is normalized by summing
the score for each rule and normalizing by the score.
The grammar is then transformed to a weakly equiv-
alent CNF by binarizing the ternary rules and in-
troducing preterminal non-terminals. This grammar
is then provided to the parser4, which returns the
Viterbi best parse of the candidate emoticon along
with its probability. The score is then converted to
an approximate perplexity by dividing the negative
log probability by the number of unique symbols in
the sequence and taking the exponential.
4.4 Grammar enhancement and adaptation
The baseline grammar induction approach outlined
in the previous section can be improved in a cou-
ple of ways, without sacrificing the robustness of the
approach. One way is through grammar adaptation
based on automatic parses of attested kaomoji. The
other is by increasing the number of non-terminals
in the grammar, according to a prior understanding
of their typical (canonical) structure. We shall dis-
cuss each in turn.
Given a small corpus of attested emoticons (in our
case, the ?just faces? sub-corpus described in sec-
tion 3), we can apply the parser above to those ex-
amples, and extract the Viterbi best parses into an
automatically created treebank. From that treebank,
we extract counts of rule productions and use these
rule counts to inform our grammar estimation. The
benefit of this approach is that we will obtain addi-
tional probability mass for frequently observed con-
structions in that corpus, thus preferring commonly
associated pairs within the grammar. Of course, the
corpus only has a small fraction of the possible sym-
bols that we hope to cover in our robust approach, so
we want to incorporate this information in a way that
does not limit the kinds of sequences we can parse.
We can accomplish this by using simple Maxi-
mum a Posteriori (MAP) adaptation of the grammar
(Bacchiani et al, 2006). In this scenario, we will
first use our baseline method of grammar induction,
using the schemata shown in Table 1. The scores
derived in that process then serve as prior counts
4We used the BUBS parser (Bodenstab et al, 2011).
http://code.google.com/p/bubs-parser/
for the rules in the grammar, ensuring that all of
these rules continue to receive probability mass. We
then add in the counts for each of the rules from the
treebank. Many of the rules may have been unob-
served in the corpus, in which case they receive no
additional counts; observed rules, however, will re-
ceive extra weight proportional to their frequency in
that corpus. Note that these additional weights can
be scaled according to a given parameter. After in-
corporating these additional counts, the grammar is
normalized and parsing is performed as before. Of
course, this process can be iterated ? a new auto-
matic treebank can be produced based on an adapted
grammar, and so on.
In addition to grammar adaptation, we can en-
rich our grammars by increasing the non-terminal
sets. To do this, we created a nested hierarchy
of ?regions? of the emoticons, with constraints re-
lated to the canonical composition of the faces,
e.g., eyes are inside of faces, noses/mouths between
eyes, etc. These non-terminals replace our generic
non-terminal X in the rule schemata. For the cur-
rent paper, we included the following five ?region?
non-terminals: ITEM, OUT, FACE, EYES, NM. The
non-terminal ITEM is intended as a top-most non-
terminal to allow multiple emoticons in a single se-
quence, via an ITEM ? ITEM ITEM production.
None of the others non-terminals have repeating pro-
ductions of that sort ? so this replaces the X? X X
production from Table 1.
Every production (other than ITEM ? ITEM
ITEM) has zero or one non-terminals on the right-
hand side. In our new schemata, non-terminals on
the left-hand side can only have non-terminals on the
right-hand side at the same or lower levels. This en-
forces the nesting constraint, i.e., that eyes are inside
of the face. Levels can be omitted however ? e.g.,
eyes but no explicit face delimiter ? hence we can
?skip? a level using unary projections, e.g., FACE?
EYES. Those will come with a ?skip level? weight.
Categories can also rewrite to the same level (with a
?stay level? weight) or rewrite to the next level af-
ter emitting symbols (with a ?move to next level?
weight).
To encode a preference to move to the next level
rather than to stay at the same level, we assign a
weight of 1 to moving to the next level and a weight
of 0.5 to staying at the same level. The ?skip?
61
rule score
ITEM ? ITEM ITEM ?
ITEM ? OUT ?
OUT ? a OUT b S(a,b) + 0.5
OUT ? a OUT  + 0.5
OUT ? OUT a  + 0.5
OUT ? a FACE b S(a,b) + 1
OUT ? a FACE  +1
OUT ? FACE a  +1
OUT ? FACE 0.5
FACE ? a FACE b S(a,b) + 0.5
FACE ? a FACE  + 0.5
FACE ? FACE a  + 0.5
FACE ? a EYES b S(a,b) + 1
FACE ? a EYES  +1
FACE ? EYES a  +1
FACE ? EYES 0.1
EYES ? a EYES b S(a,b) + 0.5
EYES ? a EYES  + 0.5
EYES ? EYES a  + 0.5
EYES ? a NM b S(a,b) + 1
EYES ? a NM  +1
EYES ? NM a  +1
EYES ? NM 0.1
EYES ? a b S(a,b) + 1
NM ? a NM 
NM ? NM a 
NM ? a ?
Table 2: Rule schemata for expanded non-terminal set
weights depend on the level, e.g., skipping OUT
should be cheap (weight of 0.5), while skipping
the others more expensive (weight of 0.1). These
weights are like counts, and are added to the similar-
ity counts when deriving the probability of the rule.
Finally, there is a rule in the schemata in Table 1 with
a pair of symbols and no middle non-terminal. This
is most appropriate for eyes, hence will only be gen-
erated at that level. Similarly, the single symbol on
the right-hand side is for the NM (nose/mouth) re-
gion. Table 2 presents our expanded rule schemata.
Note that the grammar generated with this ex-
panded set of non-terminals is robust, just as the ear-
lier grammar is, in that every sequence is guaranteed
to have a parse. Further, it can be adapted using the
same methods presented earlier in this section.
5 Experimental Results
Using the candidate extraction methodology de-
scribed in section 4.2, we extracted 1.6 million dis-
tinct candidates from our corpus of 80 million Twit-
ter messages (candidates often appeared in multi-
ple messages). These candidates included genuine
emoticons, as well as extended strings of punc-
tuation and other ?noisy? chunks of text. Gen-
uine kaomoji were often picked up with some
amount of leading or trailing punctuation, for exam-
ple: ?..\(??`)/?; other times, kaomoji beginning
with linguistic characters were truncated: (^?*)?.
We provided these candidates to our parser un-
der four different conditions, each one producing
1.5 million parse trees: the single non-terminal ap-
proach described in section 4.3 or the enhanced mul-
tiple non-terminal approach described in section 4.4,
both with and without training via the Maximum A
Posteriori approach described in section 4.4.
Using the weighted-inside-score method de-
scribed in section 4.3, we produced a ranked list
of candidate emoticons from each condition?s out-
put. ?Well-scoring? candidates were ones for which
the parser was able to construct a low-cost parse.
We evaluated our approach in two ways. The first
way examined precision? how many of the best-
scoring candidate sequences actually contained kao-
moji? Manually reviewing all 1.6 million candidates
was not feasible, so we evaluated this aspect of our
system?s performance on a small subset of its out-
put. Computational considerations forced us to pro-
cess our large corpus in parallel, meaning that our set
of 1.6 million candidate kaomoji was already parti-
tioned into 160 sets of?10,000 candidates each. We
manually reviewed the top 1,000 sorted results from
one of these partitions, and flagged any entries that
did not contain or consist of a face-like kaomoji. The
results of each condition are presented in table 3.
The second evaluation approach we will exam-
ine looks at how our method compares with the
trigram-based approach described by (Yamada et al,
2007) (as described by (Ptaszynski et al, 2010)).
We trained both smoothed and unsmoothed lan-
guage models 5 on the ?just faces? sub-corpus used
for the A Posteriori grammar enhancement, and
computed perplexity measurements for the same
set ?10,000 candidates used previously. Table 3
presents these results; clearly, a smoothed trigram
model can achieve good results. The unsmoothed
model at first glance seems to have performed very
well; note, however, that only approximately 600
(out of nearly 10,000) candidates were ?matched?
by the unsmoothed model (i.e., they did not contain
any OOV symbols and therefore had finite perplex-
ity scores), yielding a very small but high-precision
set of emoticons.
Looking at precision, the model-based ap-
proaches outperformed our grammar approach. It
5Using the OpenGrm ngram language modeling toolkit.
62
Condition P@1000 MAP
Single Nonterm, Untrained 0.662 0.605
Single Nonterm, Trained 0.80 0.945
Multiple Nonterm, Untrained 0.795 0.932
Multiple Nonterm, Trained 0.885 0.875
Unsmoothed 3-gram 0.888 0.985
Smoothed 3-gram 0.905 0.956
Mixed, Single Nonterm, Untrained 0.662 0.902
Mixed, Single Nonterm, Trained 0.804 0.984
Mixed, Multiple Nonterm, Untrained 0.789 0.932
Mixed, Multiple Nonterm, Trained 0.878 0.977
Table 3: Experimental Results.
should be noted, however, that the trigram approach
was much less tolerant of certain non-standard for-
mulations involving novel characters or irregular
formulations ((?!?)?(?o?)?(??o??)
and
(?!?)?(?o?)?(??o??) are examples of kao-moji that our grammar-based approach ranked more
highly than did the trigram approach). The two
approaches also had different failure profiles. The
grammar approach?s false positives tended to be
symmetrical sequences of punctuation, whereas the
language models? were more variable. Were we to
review a larger selection of candidates, we believe
that the structure-capturing nature of the grammar
approach would enable it to outperform the more
simplistic approach.
We also attempted a hybrid ?mixed? approach in
which we used the language models to re-rank the
top 1,000 ?best? candidates from our parser?s output.
This generally resulted in improved performance,
and for some conditions the improvement was sub-
stantial. Future work will explore this approach in
greater detail and over larger amounts of data.
6 Discussion
We describe an almost entirely unsupervised ap-
proach to detecting kaomoji in irregular, real-world
text. In its baseline state, our system is able to ac-
curately identify a large number of examples using
a very simple set of templates, and can distinguish
kaomoji from other non-linguistic content (punctu-
ation, etc.). Using minimal supervision, we were
able to effect a dramatic increase in our system?s
performance. Visual comparison of the ?untrained?
results with the ?trained? results was instructive.
The untrained systems? results were very heavily in-
fluenced by their template rules? strong preference
for visual symmetry. Many instances of symmet-
rical punctuation sequences (e.g., ..?..) ended
up being ranked more highly than even fairly sim-
ple kaomoji, and in the absence of other informa-
tion, the length of the input strings also played a too-
important role in their rankings.
The MAP-ehanced systems? results, on the other
hand, retained their strong preference for symme-
try, but were also influenced by the patterns and
characters present in their training data. For ex-
ample, two of the top-ranked ?false positives? from
the enhanced system were the sequences >,< and
= =, both of which (while symmetrical) also con-
tain characters often seen in kaomoji. By using more
structurally diverse training data, we expect further
improvements in this area. Also, our system cur-
rently relies on a very small number of relatively
simplistic grammar templates; expanding these to
encode additional structure may also help.
Due to our current scoring mechanism, our parser
is biased against certain categories of kaomoji. Par-
ticularly poorly-scored are complex creations such
as (((| ???? ??|? ???=???| ???? ??|?))). In this example, the large number
of combining characters and lack of obvious nest-
ing therein confounded our templates and produced
expensive parse trees. Future work will involve im-
proved handling of such cases, either by modified
parsing schemes or additional templates.
One other area of future work is to match par-
ticular kaomoji, or fragments of kaomoji (e.g. par-
ticular eyes), to particular affective states, or other
features of the text. Some motifs are already well
known: for example, there is wide use of TT, or the
similar-looking Korean hangeul vowel yu, to repre-
sent crying eyes. We propose to do this initially by
computing the association between particular kao-
moji and words in the text. Such associations may
yield more than just information on the likely af-
fect associated with a kaomoji. So, for example,
using pointwise mutual information as a measure
of association, we found that in our Twitter corpus,
(*?_?*) seems to be highly associated with tweets
about Korean pop music, *-* with Brazilian post-
ings, and with Indonesian postings. Such
associations presumably reflect cultural preferences,
and could prove useful in identifying the provenance
of a message even if more conventional linguistic
techniques fail.
63
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Adaptive beam-width prediction for ef-
ficient cyk parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 440?449.
Junpei Nakamura, Takeshi Ikeda, Nobuo Inui, and
Yoshiyuki Kotani. 2003. Learning face mark for nat-
ural language dialogue system. In Proc. Conf. IEEE
Int?l Conf. Natural Language Processing and Knowl-
edge Eng, pages 180?185.
Michal Ptaszynski, Jacek Maciejewski, Pawel Dybala,
Rafal Rzepka, and Kenji Araki. 2010. Cao: A fully
automatic emoticon analysis system based on theory
of kinesics. IEEE Transactions on Affective Comput-
ing, 1:46?59.
Yuki Tanaka, Hiroya Takamura, and Manabu Okumura.
2005. Extraction and classification of facemarks with
kernel methods. In Proc. 10th Int?l Conf. Intelligent
User Interfaces.
T. Yamada, S. Tsuchiya, S. Kuroiwa, and F. Ren. 2007.
Classification of facemarks using n-gram. In Inter-
national Conference on Natural Language Processing
and Knowledge Engineering, pages 322?327.
64
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 1?10,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Graph-based alignment of narratives for automated neurological assessment
Emily T. Prud?hommeaux and Brian Roark
Center for Spoken Language Understanding
Oregon Health & Science University
{emilypx,roarkbr}@gmail.com
Abstract
Narrative recall tasks are widely used in neu-
ropsychological evaluation protocols in or-
der to detect symptoms of disorders such
as autism, language impairment, and demen-
tia. In this paper, we propose a graph-based
method commonly used in information re-
trieval to improve word-level alignments in
order to align a source narrative to narra-
tive retellings elicited in a clinical setting.
From these alignments, we automatically ex-
tract narrative recall scores which can then be
used for diagnostic screening. The signifi-
cant reduction in alignment error rate (AER)
afforded by the graph-based method results
in improved automatic scoring and diagnos-
tic classification. The approach described here
is general enough to be applied to almost any
narrative recall scenario, and the reductions in
AER achieved in this work attest to the po-
tential utility of this graph-based method for
enhancing multilingual word alignment and
alignment of comparable corpora for more
standard NLP tasks.
1 Introduction
Much of the work in biomedical natural language
processing has focused on mining information from
electronic health records, clinical notes, and medical
literature, but NLP is also very well suited for ana-
lyzing patient language data, in terms of both con-
tent and linguistic features, for neurological eval-
uation. NLP-driven analysis of clinical language
data has been used to assess language development
(Sagae et al, 2005), language impairment (Gabani
et al, 2009) and cognitive status (Roark et al, 2007;
Roark et al, 2011). These approaches rely on the ex-
traction of syntactic features from spoken language
transcripts in order to identify characteristics of lan-
guage use associated with a particular disorder. In
this paper, rather than focusing on linguistic fea-
tures, we instead propose an NLP-based method for
automating the standard manual method for scoring
the Wechsler Logical Memory (WLM) subtest of the
Wechsler Memory Scale (Wechsler, 1997) with the
eventual goal of developing a screening tool for Mild
Cognitive Impairment (MCI), the earliest observable
precursor to dementia. During standard administra-
tion of the WLM, the examiner reads a brief narra-
tive to the subject, who then retells the story to the
examiner, once immediately upon hearing the story
and a second time after a 30-minute delay. The ex-
aminer scores the retelling in real time by counting
the number of recalled story elements, each of which
corresponds to a word or short phrase in the source
narrative. Our method for automatically extracting
the score from a retelling relies on an alignment be-
tween substrings in the retelling and substrings in
the original narrative. The scores thus extracted can
then be used for diagnostic classification.
Previous approaches to alignment-based narra-
tive analysis (Prud?hommeaux and Roark, 2011a;
Prud?hommeaux and Roark, 2011b) have relied ex-
clusively on modified versions of standard word
alignment algorithms typically applied to large bilin-
gual parallel corpora for building machine transla-
tion models (Liang et al, 2006; Och et al, 2000).
Scores extracted from the alignments produced us-
ing these algorithms achieved fairly high classifi-
1
cation accuracy, but the somewhat weak alignment
quality limited performance. In this paper, we com-
pare these word alignment approaches to a new ap-
proach that uses traditionally-derived word align-
ments between retellings as the input for graph-
based exploration of the alignment space in order to
improve alignment accuracy. Using both earlier ap-
proaches and our novel method for word alignment,
we then evaluate the accuracy of automated scoring
and diagnostic classification for MCI.
Although the alignment error rates for our data
might be considered high in the context of building
phrase tables for machine translation, the alignments
produced using the graph-based method are remark-
ably accurate given the small size of our training
corpus. In addition, these more accurate alignments
lead to gains in scoring accuracy and to classification
performance approaching that of manually derived
scores. This method for word alignment and score
extraction is general enough to be easily adapted
to other tests used in neuropsychological evalua-
tion, including not only those related to narrative re-
call, such as the NEPSY Narrative Memory subtest
(Korkman et al, 1998) but also picture description
tasks, such as the Cookie Theft picture description
task of the Boston Diagnostic Aphasia Examination
(Goodglass et al, 2001) or the Renfrew Bus Story
(Glasgow and Cowley, 1994). In addition, this tech-
nique has the potential to improve word alignment
for more general NLP tasks that rely on small cor-
pora, such as multilingual word alignment or word
alignment of comparable corpora.
2 Background
The act of retelling or producing a narrative taps into
a wide array of cognitive functions, not only mem-
ory but also language comprehension, language pro-
duction, executive function, and theory of mind. The
inability to coherently produce or recall a narrative
is therefore associated with many different cogni-
tive and developmental disorders, including demen-
tia, autism (Tager-Flusberg, 1995), and language im-
pairment (Dodwell and Bavin, 2008; Botting, 2002).
Narrative tasks are widely used in neuropsycholog-
ical assessment, and many commonly used instru-
ments and diagnostic protocols include a task in-
volving narrative recall or production (Korkman et
al., 1998; Wechsler, 1997; Lord et al, 2002).
In this paper, we focus on evaluating narrative re-
call within the context of Mild Cognitive Impair-
ment (MCI), the earliest clinically significant pre-
cursor of dementia. The cognitive and memory
problems associated with MCI do not necessarily
interfere with daily living activities (Ritchie and
Touchon, 2000) and can therefore be difficult to
diagnose using standard dementia screening tools,
such as the Mini-Mental State Exam (Folstein et al,
1975). A definitive diagnosis of MCI requires an
extensive interview with the patient and a family
member or caregiver. Because of the effort required
for diagnosis and the insensitivity of the standard
screening tools, MCI frequently goes undiagnosed,
delaying the introduction of appropriate treatment
and remediation. Early and unobtrusive detection
will become increasingly important as the elderly
population grows and as research advances in delay-
ing and potentially stopping the progression of MCI
into moderate and severe dementia.
Narrative recall tasks, such as the test used in re-
search presented here, the Wechsler Logical Mem-
ory subtest (WLM), are often used in conjunction
with other cognitive measures in attempts to identify
MCI and dementia. Multiple studies have demon-
strated a significant difference in performance on the
WLM between subjects with MCI and typically ag-
ing controls, particularly in combination with tests
of verbal fluency and memory (Storandt and Hill,
1989; Peterson et al, 1999; Nordlund et al, 2005).
The WLM can also serve as a cognitive indicator of
physiological characteristics associated with symp-
tomatic Alzheimers disease, even in the absence of
previously reported dementia (Schmitt et al, 2000;
Bennett et al, 2006).
Some previous work on automated analysis of the
WLM has focused on using the retellings as a source
of linguistic data for extracting syntactic and pho-
netic features that can distinguish subjects with MCI
from typically aging controls (Roark et al, 2011).
There has been some work on automating scoring
of other narrative recall tasks using unigram overlap
(Hakkani-Tur et al, 2010), but Dunn et al (2002)
are among the only researchers to apply automated
methods to scoring the WLM for the purpose of
identifying dementia, using latent semantic analysis
to measure the semantic distance between a retelling
2
Dx n Age Education
MCI 72 88.7 14.9 yr
Non-MCI 163 87.3 15.1 yr
Table 1: Subject demographic data.
and the source narrative. Although scoring automa-
tion is not typically used in a clinical setting, the
objectivity offered by automated measures is par-
ticularly important for tests like the WLM, which
are often administered by practitioners working in a
community setting and serving a diverse population.
Researchers working on NLP tasks such as para-
phrase extraction (Barzilay and McKeown, 2001),
word-sense disambiguation (Diab and Resnik,
2002), and bilingual lexicon induction (Sahlgren and
Karlgren, 2005), often rely on aligned parallel or
comparable corpora. Recasting the automated scor-
ing of a neuropsychological test as another NLP task
involving the analysis of parallel texts, however, is a
relatively new idea. We hope that the methods pre-
sented here will both highlight the flexibility of tech-
niques originally developed for standard NLP tasks
and attract attention to the wide variety of biomed-
ical data sources and potential clinical applications
for these techniques.
3 Data
3.1 Subjects
The data examined in this study was collected from
participants in a longitudinal study on brain aging
at the Layton Aging and Alzheimers Disease Cen-
ter at the Oregon Health and Science University
(OHSU), including 72 subjects with MCI and 163
typically aging seniors roughly matched for age and
years of education. Table 1 shows the mean age
and mean years of education for the two diagnos-
tic groups. There were no significant between-group
differences in either measure.
Following (Shankle et al, 2005), we assign a di-
agnosis of MCI according to the Clinical Dementia
Rating (CDR) (Morris, 1993). A CDR of 0.5 corre-
sponds to MCI (Ritchie and Touchon, 2000), while
a CDR of zero indicates the absence of MCI or any
dementia. The CDR is measured via the Neurobe-
havioral Cognitive Status Examination (Kiernan et
al., 1987) and a semi-structured interview with the
patient and a family member or caregiver that allows
the examiner to assess the subject in several key ar-
eas of cognitive function, such as memory, orienta-
tion, problem solving, and personal care. The CDR
has high inter-annotator reliability (Morris, 1993)
when conducted by trained experts. It is crucial to
note that the calculation of CDR is completely inde-
pendent of the neuropsychological test investigated
in this paper, the Wechsler Logical Memory subtest
of the Wechsler Memory Scale. We refer readers to
the above cited papers for a further details.
3.2 Wechsler Logical Memory Test
The Wechsler Logical Memory subtest (WLM) is
part of the Wechsler Memory Scale (Wechsler,
1997), a diagnostic instrument used to assess mem-
ory and cognition in adults. In the WLM, the subject
listens to the examiner read a brief narrative, shown
in Figure 1. The subject then retells the narrative to
the examiner twice: once immediately upon hearing
it (Logical Memory I, LM-I) and again after a 30-
minute delay (Logical Memory II, LM-II). The nar-
rative is divided into 25 story elements. In Figure 1,
the boundaries between story elements are denoted
by slashes. The examiner notes in real time which
story elements the subject uses. The score that is re-
ported under standard administration of the task is
a summary score, which is simply the raw number
of story elements recalled. Story elements do not
need to be recalled verbatim or in the correct tempo-
ral order. The published scoring guidelines describe
the permissible substitutions for each story element.
The first story element, Anna, can be replaced in the
retelling with Annie or Ann, while the 16th story
element, fifty-six dollars, can be replaced with any
number of dollars between fifty and sixty.
An example LM-I retelling is shown in Figure 2.
According to the published scoring guidelines, this
retelling receives a score of 12, since it contains the
following 12 elements: Anna, employed, Boston, as
a cook, was robbed of, she had four, small children,
reported, station, touched by the woman?s story,
took up a collection, and for her.
3.3 Word alignment data
The Wechsler Logical Memory immediate and de-
layed retellings for all of the 235 experimental sub-
jects were transcribed at the word level. We sup-
3
Anna / Thompson / of South / Boston / em-
ployed / as a cook / in a school / cafeteria /
reported / at the police / station / that she had
been held up / on State Street / the night be-
fore / and robbed of / fifty-six dollars. / She
had four / small children / the rent was due /
and they hadn?t eaten / for two days. / The po-
lice / touched by the woman?s story / took up
a collection / for her.
Figure 1: Text of WLM narrative segmented into 25 story
elements.
Ann Taylor worked in Boston as a cook. And
she was robbed of sixty-seven dollars. Is that
right? And she had four children and reported
at the some kind of station. The fellow was
sympathetic and made a collection for her so
that she can feed the children.
Figure 2: Sample retelling of the Wechsler narrative.
plemented the data collected from our experimental
subjects with transcriptions of retellings from 26 ad-
ditional individuals whose diagnosis had not been
confirmed at the time of publication or who did
not meet the eligibility criteria for this study. Par-
tial words, punctuation, and pause-fillers were ex-
cluded from all transcriptions used for this study.
The retellings were manually scored according to
published guidelines. In addition, we manually pro-
duced word-level alignments between each retelling
and the source narrative presented in Figure 1.
Word alignment for phrase-based machine trans-
lation typically takes as input a sentence-aligned
parallel corpus or bi-text, in which a sentence on
one side of the corpus is a translation of the sen-
tence in that same position on the other side of the
corpus. Since we are interested in learning how to
align words in the source narrative to words in the
retellings, our primary parallel corpus must consist
of source narrative text on one side and retelling
text on the other. Because the retellings contain
omissions, reorderings, and embellishments, we are
obliged to consider the full text of the source narra-
tive and of each retelling to be a ?sentence? in the
parallel corpus.
We compiled three parallel corpora to be used for
the word alignment experiments:
? Corpus 1: A roughly 500-line source-to-
retelling corpus consisting of the source narra-
tive on one side and each retelling on the other.
? Corpus 2: A roughly 250,000-line pairwise
retelling-to-retelling corpus, consisting of ev-
ery possible pairwise combination of retellings.
? Corpus 3: A roughly 900-line word identity
corpus, consisting of every word that appears
in every retelling and the source narrative.
The explicit parallel alignments of word identities
that compose Corpus 3 are included in order to en-
courage the alignment of a word in a retelling to that
same word in the source, if it exists.
The word alignment techniques that we use are
entirely unsupervised. Therefore, as in the case
with most experiments involving word alignment,
we build a model for the data we wish to evalu-
ate using that same data. We do, however, use the
retellings from the 26 individuals who were not ex-
perimental subjects as a development set for tuning
the various parameters of our system, which is de-
scribed below.
4 Word Alignment
4.1 Baseline alignment
We begin by building two word alignment models
using the Berkeley aligner (Liang et al, 2006), a
state-of-the-art word alignment package that relies
on IBM mixture models 1 and 2 (Brown et al, 1993)
and an HMM. We chose to use the Berkeley aligner,
rather than the more widely used Giza++ alignment
package, for this task because its joint training and
posterior decoding algorithms yield lower alignment
error rates on most data sets and because it offers
functionality for testing an existing model on new
data and for outputting posterior probabilities. The
smaller of our two Berkeley-generated models is
trained on Corpus 1 (the source-to-retelling parallel
corpus described above) and ten copies of Corpus
3 (the word identity corpus). The larger model is
trained on Corpus 1, Corpus 2 (the pairwise retelling
corpus), and 100 copies of Corpus 3. Both models
are then tested on the 470 retellings from our 235 ex-
perimental subjects. In addition, we use both mod-
els to align every retelling to every other retelling so
that we will have all pairwise alignments available
for use in the graph-based model.
4
Figure 3: Depiction of word graph.
The first two rows of Table 2 show the preci-
sion, recall, F-measure, and alignment error rate
(AER) (Och and Ney, 2003) for these two Berkeley
aligner models. We note that although AER for the
larger model is lower, the time required to train the
model is significantly larger. The alignments gen-
erated by the Berkeley aligner serve not only as a
baseline for comparison but also as a springboard
for the novel graph-based method of alignment we
will now discuss.
4.2 Graph-based refinement
Graph-based methods, in which paths or random
walks are traced through an interconnected graph of
nodes in order to learn more about the nodes them-
selves, have been used for various NLP tasks in in-
formation extraction and retrieval, including web-
page ranking (PageRank (Page et al, 1999)) and ex-
tractive summarization (LexRank (Erkan and Radev,
2004; Otterbacher et al, 2009)). In the PageRank al-
gorithm, the nodes of the graph are web pages and
the edges connecting the nodes are the hyperlinks
leading from those pages to other pages. The nodes
in the LexRank algorithm are sentences in a docu-
ment and the edges are the similarity scores between
those sentences. The likelihood of a random walk
through the graph starting at a particular node and
ending at another node provides information about
the relationship between those two nodes and the im-
portance of the starting node.
In the case of our graph-based method for word
alignment, each node represents a word in one of the
retellings or in the source narrative. The edges are
Figure 4: Changes in AER as ? increases.
the normalized posterior-weighted alignments that
the Berkeley aligner proposes between each word
and (1) words in the source narrative, and (2) words
in the other retellings, as depicted in Figure 3. Start-
ing at a particular node (i.e., a word in one of the
retellings), our algorithm can either walk from that
node to another node in the graph or to a word in
the source narrative. At each step in the walk, there
is a set probability ? that determines the likelihood
of transitioning to another retelling word versus a
word in the source narrative. When transitioning to
a retelling word, the destination word is chosen ac-
cording to the posterior probability assigned by the
Berkeley aligner to that alignment. When the walk
arrives at a source narrative word, that word is the
new proposed alignment for the starting word.
For each word in each retelling, we perform 1000
of these random walks, thereby generating a distri-
bution for each retelling word over all of the words
in the source narrative. The new alignment for the
word is the source word with the highest frequency
in that distribution.
We build two graphs on which to carry out these
random walks: one graph is built using the align-
ments generated by the smaller Berkeley alignment
model, and the other is built from the alignments
generated by the larger Berkeley alignment model.
Alignments with posterior probabilities of 0.5 or
greater are included as edges within the graph, since
this is the default posterior threshold used by the
Berkeley aligner. The value of ?, the probability of
walking to a retelling word node rather than a source
word, is tuned to the development set of retellings,
5
Model P R F AER
Berkeley-Small 72.1 79.6 75.6 24.5
Berkeley-Large 78.6 80.5 79.5 20.5
Graph-Small 77.9 81.2 79.5 20.6
Graph-Large 85.4 76.9 81.0 18.9
Table 2: Aligner performance comparison.
discussed in Section 3.3. Figure 4 shows how AER
varies according to the value of ? for the two graph-
based approaches.
Each of these four alignment models produces,
for each retelling, a set of word pairs containing one
word from the original narrative and one word from
the retelling. The manual gold alignments for the
235 experimental subjects were evaluated against
the alignments produced by each of the four models.
Table 2 shows the accuracy of word alignment us-
ing these two graph-based models in terms of preci-
sion, accuracy, F-measure, and alignment error rate,
alongside the same measures for the two Berkeley
models. We see that each of the graph-based models
outperforms the Berkeley model of the same size.
The performance of the small graph-based model is
especially remarkable since it an AER comparable
to the large Berkeley model while requiring signif-
icantly fewer computing resources. The difference
in processing time between the two approaches was
especially remarkable: the graph-based model com-
pleted in only a few minutes, while the large Berke-
ley model required 14 hours of training.
Figures 5 and 6 show the results of aligning
the retelling presented in Figure 2 using the small
Berkeley model and the large graph-based model,
respectively. Comparing these two alignments, we
see that the latter model yields more precise align-
ments with very little loss of recall, as is borne out
by the overall statistics shown in Table 2.
5 Scoring
The published scoring guidelines for the WLM spec-
ify the source words that compose each story ele-
ment. Figure 7 displays the source narrative with
the element IDs (A? Y ) and word IDs (1? 65) ex-
plicitly labeled. Element Q, for instance, consists of
the words 39 and 40, small children. Using this in-
formation, we extract scores from the alignments as
follows: for each word in the original narrative, if
[A anna1] [B thompson2] [C of3 south4]
[D boston5] [E employed6] [F as7 a8
cook9] [G in10 a11 school12] [H cafeteria13]
[I reported14] [J at15 the16 police17] [K
station18] [L that19 she20 had21 been22 held23
up24] [M on25 state26 street27] [N the28
night29 before30] [O and31 robbed32 of33] [P
fifty-six34 dollars35] [Q she36 had37 four38]
[R small39 children40] [S the41 rent42 was43
due44] [T and45 they46 had47 n?t48 eaten49]
[U for50 two51 days52] [V the53 police54] [W
touched55 by56 the57 woman?s58 story59] [X
took60 up61 a62 collection63] [Y for64 her65]
Figure 7: Text of Wechsler Logical Memory narrative
with story-element labeled bracketing and word IDs.
anna(1) : A
thompson(2) : B
employed(6) : E
boston(5) : D
cook(9) : F
robbed(32) : O
fifty-six(34) : P
four(38) : Q
children(40) : R
reported(14) : I
station(18) : K
took(60) : X
collection(63) : X
for(64) : Y
her(65) : Y
Figure 8: Source content words from the alignment in
Figure 6 with corresponding story element IDs.
that word is aligned to a word in the retelling, the
story element that it is associated with is considered
to be recalled. Figure 8 shows the story elements
extracted from the word alignments in Figure 6.
When we convert alignments to scores in this way,
any alignment can be mapped to an element, even an
alignment between function words such as the and
of, which would be unlikely to indicate that the story
element had been recalled. To avoid such scoring er-
rors, we disregard any word-alignment pair contain-
ing a source function word. The two exceptions to
this rule are the final two words, for her, which are
not content words but together make a single story
element.
The element-level scores induced from the four
word alignments for all 235 experimental sub-
jects were evaluated against the manual per-element
scores. We report the precision, recall, and f-
measure for all four alignment models in Table 3. In
addition, report Cohen?s kappa as a measure of reli-
ability between our automated scores and the man-
ually assigned scores. We see that as AER im-
proves, scoring accuracy also improves, with the
large graph-based model outperforming all other
models in terms of precision, f-measure, and inter-
6
ann(1) : anna(1)
worked(3) : employed(6)
in(4) : in(10)
boston(5) : boston(5)
as(6) : as(7)
a(7) : a(8)
cook(8) : cook(9)
and(9) : and(31)
robbed(12) : robbed(32)
of(13) : of(33)
dollars(15) : dollars(35)
is(16) : was(43)
that(17) : that(19)
and(19) : and(45)
she(20) : she(36)
had(21) : had(37)
four(22) : four(38)
children(23) : children(40)
reported(25) : reported(14)
at(26) : at(15)
the(27) : the(16)
some(28) : police(17)
station(31) : station(18)
made(37) : up(61)
made(37) : took(60)
a(38) : a(62)
collection(39) : collection(63)
for(40) : for(64)
her(41) : her(65)
so(42) : woman?s(58)
she(44) : she(20)
Figure 5: Word alignment generated by the small Berkeley alignment model with retelling words italicized.
ann(1) : anna(1)
taylor(2) : thompson(2)
worked(3) : employed(6)
in(4) : in(10)
boston(5) : boston(5)
as(6) : as(7)
a(7) : a(8)
cook(8) : cook(9)
robbed(12) : robbed(32)
of(13) : of(33)
sixty-seven(14) : fifty-six(34)
dollars(15) : dollars(35)
she(20) : she(36)
had(21) : had(37)
four(22) : four(38)
children(23) : children(40)
reported(25) : reported(14)
at(26) : at(15)
the(27) : the(16)
station(31) : station(18)
made(37) : took(60)
a(38) : a(62)
collection(39) : collection(63)
for(40) : for(64)
her(41) : her(65)
Figure 6: Word alignment generated by the large graph-based model with retelling words italicized.
Model P R F ?
Berkeley-Small 87.2 88.9 88.0 76.1
Berkeley-Large 86.8 90.7 88.7 77.1
Graph-Small 84.7 93.6 88.9 76.9
Graph-Big 88.8 89.3 89.1 78.3
Table 3: Scoring accuracy results.
rater reliability. The scoring accuracy levels re-
ported here are comparable to the levels of inter-rater
agreement typically reported for the WLM, and re-
liability between our automated scores and the man-
ual scores, as measured by Cohen?s kappa, is well
within the ranges reported in the literature (Johnson
et al, 2003). As will be shown in the following sec-
tion, scoring accuracy is very important for achiev-
ing high diagnostic classification accuracy, which is
the ultimate goal of this work.
6 Diagnostic Classification
As discussed in Section 2, poor performance on the
Wechsler Logical Memory test is associated with
Mild Cognitive Impairment. We now use the scores
we have extracted from the word alignments as fea-
tures with a support vector machine (SVM) to per-
form diagnostic classification for distinguishing sub-
jects with MCI from those without. For each of the
235 experimental subjects, we generate 2 summary
scores: one for the immediate retelling and one for
the delayed retelling. The summary score ranges
from 0, indicating that no elements were recalled,
to 25, indicating that all elements were recalled. In
addition to the summary score, we also provide the
SVM with a vector of 50 per-element scores: for
each of the 25 element in each of the two retellings
per subject, there is a vector element with the value
of 0 if the element was not recalled, or 1 if the el-
ement was recalled. Since previous work has indi-
cated that certain elements may be more powerful in
their ability to predict the presence of MCI, we ex-
pect that giving the SVM these per-elements scores
may improve classification performance. To train
and test our classifiers, we use the WEKA API (Hall
et al, 2009) and LibSVM (Chang and Lin, 2011),
with a second-order polynomial kernel and default
parameter settings.
We evaluate the performance of the SVMs us-
ing a leave-pair-out validation scheme (Cortes et al,
2007; Pahikkala et al, 2008). In the leave-pair-out
technique, every pairing between a negative exam-
ple and a positive example is tested using a classi-
fier trained on all of the remaining examples. The
resulting pairs of scores can be used to calculate
the area under the receiver operating characteristic
(ROC) curve (Egan, 1975), which is a plot of the
false positive rate of a classifier against its true pos-
itive rate. The area under this curve (AUC) has a
7
Model Summ. (s.d.) Elem. (s.d.)
Manual Scores 73.3 (3.76) 81.3 (3.32)
Berkeley-Small 73.7 (3.74) 77.9 (3.52)
Berkeley-Big 75.1 (3.67) 79.2 (3.45)
Graph-Small 74.2 (3.71) 78.9 (3.47)
Graph-Big 74.8 (3.69) 78.6 (3.49)
Table 4: Classification accuracy results (AUC).
value of 0.5 when the classifier performs at chance
and a value 1.0 when perfect classification accuracy
is achieved.
Table 4 shows the classification results for the
scores derived from the four alignment models along
with the classification results using the examiner-
assigned manual scores. It appears that, in all cases,
the per-element scores are more effective than the
summary scores in classifying the two diagnostic
groups. In addition, we see that our automated
scores have classificatory power comparable to that
of the manual gold scores, and that as scoring ac-
curacy increases from the small Berkeley model to
the graph-based models and bigger models, classifi-
cation accuracy improves. This suggests both that
accurate scores are crucial for accurate classifica-
tion and that pursuing even further improvements in
word alignment is likely to result in improved di-
agnostic differentiation. We note that although the
large Berkeley model achieved the highest classi-
fication accuracy, this very slight margin of differ-
ence may not justify its significantly greater compu-
tational requirements.
7 Conclusions and Future Work
The work presented here demonstrates the utility
of adapting techniques drawn from a diverse set of
NLP research areas to tasks in biomedicine. In par-
ticular, the approach we describe for automatically
analyzing clinically elicited language data shows
promise as part of a pipeline for a screening tool for
Mild Cognitive Impairment. Our novel graph-based
approach to word alignment resulted in large reduc-
tions in alignment error rate. These reductions in er-
ror rate in turn led to human-level scoring accuracy
and improved diagnostic classification.
As we have mentioned, the methods outlined here
are general enough to be used for other episodic
recall and description scenarios. Although the re-
sults are quite robust, several enhancements and im-
provements should be made before we apply the sys-
tem to other tasks. First, although we were able to
achieve decent word alignment accuracy, especially
with our graph-based approach, many alignment er-
rors remain. As shown in Figure 4, the graph-based
alignment technique could potentially result in an
AER of as low as 11%. We expect that our deci-
sion to select as a new alignment the most frequent
source word over the distribution of source words at
the end of 1000 walks could be improved, since it
does not allow for one-to-many mappings. In addi-
tion, it would be worthwhile to experiment with sev-
eral posterior thresholds, both during the decoding
step of the Berkeley aligner and in the graph edges.
In order to produce a viable clinical screening
tool, it is crucial that we incorporate speech recogni-
tion in the pipeline. Our very preliminary investiga-
tion into using ASR to generate transcripts for align-
ment seems promising and surprisingly robust to the
problems that might be expected when working with
noisy audio. In our future work, we also plan to ex-
amine longitudinal data for individual subjects to see
whether our techniques can detect subtle differences
in recall and coherence between a recent retelling
and a series of earlier baseline retellings. Since the
metric commonly used to quantify the progression
of dementia, the Clinical Dementia Rating, relies on
observed changes in cognitive function over time,
longitudinal analysis of performance on the Wech-
sler Logical Memory task may be the most promis-
ing application for our research.
References
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceeding of ACL.
D.A. Bennett, J.A. Schneider, Z. Arvanitakis, J.F. Kelly,
N.T. Aggarwal, R.C. Shah, and R.S. Wilson. 2006.
Neuropathology of older persons without cognitive
impairment from two community-based studies. Neu-
rology, 66:1837?844.
Nicola Botting. 2002. Narrative as a tool for the assess-
ment of linguistic and pragmatic impairments. Child
Language Teaching and Therapy, 18(1).
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
8
tical machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(27):1?
27.
Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.
2007. An alternative ranking problem for search en-
gines. In Proceedings of WEA2007, LNCS 4525,
pages 1?21. Springer-Verlag.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
Kristy Dodwell and Edith L. Bavin. 2008. Children
with specific language impair ment: an investigation of
their narratives and memory. International Journal of
Language and Communication Disorders, 43(2):201?
218.
John C. Dunn, Osvaldo P. Almeida, Lee Barclay, Anna
Waterreus, and Leon Flicker. 2002. Latent seman-
tic analysis: A new method to measure prose recall.
Journal of Clinical and Experimental Neuropsychol-
ogy, 24(1):26?35.
James Egan. 1975. Signal Detection Theory and ROC
Analysis. Academic Press.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
M. Folstein, S. Folstein, and P. McHugh. 1975. Mini-
mental state - a practical method for grading the cog-
nitive state of patients for the clinician. Journal of Psy-
chiatric Research, 12:189?198.
Keyur Gabani, Melissa Sherman, Thamar Solorio, and
Yang Liu. 2009. A corpus-based approach for the
prediction of language impairment in monolingual En-
glish and Spanish-English bilingual children. In Pro-
ceedings of NAACL-HLT, pages 46?55.
Cheryl Glasgow and Judy Cowley. 1994. Renfrew
Bus Story test - North American Edition. Centreville
School.
H Goodglass, E Kaplan, and B Barresi. 2001. Boston
Diagnostic Aphasia Examination. 3rd ed. Pro-Ed.
Dilek Hakkani-Tur, Dimitra Vergyri, and Gokhan Tur.
2010. Speech-based automated cognitive status as-
sessment. In Proceedings of Interspeech, pages 258?
261.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
David K. Johnson, Martha Storandt, and David A. Balota.
2003. Discourse analysis of logical memory recall in
normal aging and in dementia of the alzheimer type.
Neuropsychology, 17(1):82?92.
R.J. Kiernan, J. Mueller, J.W. Langston, and C. Van
Dyke. 1987. The neurobehavioral cognitive sta-
tus examination, a brief but differentiated approach to
cognitive assessment. Annals of Internal Medicine,
107:481?485.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT NAACL.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services.
John Morris. 1993. The clinical dementia rating
(CDR): Current version and scoring rules. Neurology,
43:2412?2414.
A Nordlund, S Rolstad, P Hellstrom, M Sjogren,
S Hansen, and A Wallin. 2005. The goteborg mci
study: mild cognitive impairment is a heterogeneous
condition. Journal of Neurology, Neurosurgery and
Psychiatry, 76:1485?1490.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, , and Hermann
Ney. 2000. Improved alignment models for statisti-
cal machine translation. In Proceedings of ACL, pages
440?447.
Jahna Otterbacher, Gu?nes Erkan, and Dragomir R. Radev.
2009. Biased lexrank: Passage retrieval using random
walks with question-based priors. Inf. Process. Man-
age., 45(1):42?54.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number =
SIDL-WP-1999-0120.
Tapio Pahikkala, Antti Airola, Jorma Boberg, and Tapio
Salakoski. 2008. Exact and efficient leave-pair-out
cross-validation for ranking RLS. In Proceedings of
AKRR 2008, pages 1?8.
Ronald Peterson, Glenn Smith, Stephen Waring, Robert
Ivnik, Eric Tangalos, and Emre Kokmen. 1999. Mild
cognitive impairment: Clinical characterizations and
outcomes. Archives of Neurology, 56:303?308.
Emily T. Prud?hommeaux and Brian Roark. 2011a.
Alignment of spoken narratives for automated neu-
ropsychological assessment. In Proceedings of ASRU.
Emily T. Prud?hommeaux and Brian Roark. 2011b. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Proceedings of Interspeech.
Karen Ritchie and Jacques Touchon. 2000. Mild cogni-
tive impairment: Conceptual basis and current noso-
logical status. Lancet, 355:225?228.
9
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for de-
tecting mild cognitive impairment. In Proceedings of
the ACL 2007 Workshop on Biomedical Natural Lan-
guage Processing (BioNLP), pages 1?8.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristina Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Audio,
Speech and Language Processing, 19(7):2081?2090.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of ACL, pages 197?
204.
Magnus Sahlgren and Jussi Karlgren. 2005. Automatic
bilingual lexicon acquisition using random indexing
of parallel corpora. Natural Language Engineering,
11(3).
F.A. Schmitt, D.G. Davis, D.R. Wekstein, C.D. Smith,
J.W. Ashford, and W.R. Markesbery. 2000. Preclini-
cal ad revisited: Neuropathology of cognitively normal
older adults. Neurology, 55:370?376.
William R. Shankle, A. Kimball Romney, Junko Hara,
Dennis Fortier, Malcolm B. Dick, James M. Chen,
Timothy Chan, and Xijiang Sun. 2005. Methods
to improve the detection of mild cognitive impair-
ment. Proceedings of the National Academy of Sci-
ences, 102(13):4919?4924.
Martha Storandt and Robert Hill. 1989. Very mild senile
dementia of the alzheimers type: Ii psychometric test
performance. Archives of Neurology, 46:383?386.
Helen Tager-Flusberg. 1995. Once upon a ribbit: Stories
narrated by autistic children. British journal of devel-
opmental psychology, 13(1):45?59.
David Wechsler. 1997. Wechsler Memory Scale - Third
Edition Manual. The Psychological Corporation.
10
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?10,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
The Utility of Manual and Automatic Linguistic Error Codes
for Identifying Neurodevelopmental Disorders?
Eric Morley, Brian Roark and Jan van Santen
Center for Spoken Language Understanding, Oregon Health & Science University
morleye@gmail.com, roarkbr@gmail.com, vansantj@ohsu.edu
Abstract
We investigate the utility of linguistic features
for automatically differentiating between chil-
dren with varying combinations of two po-
tentially comorbid neurodevelopmental disor-
ders: autism spectrum disorder and specific
language impairment. We find that certain
manual codes for linguistic errors are useful
for distinguishing between diagnostic groups.
We investigate the relationship between cod-
ing detail and diagnostic classification perfor-
mance, and find that a simple coding scheme
is of high diagnostic utility. We propose a sim-
ple method to automate the pared down coding
scheme, and find that these automatic codes
are of diagnostic utility.
1 Introduction
In Autism Spectrum Disorders (ASD), language im-
pairments are common, but not universal (American
Psychiatric Association, 2000). Whether these lan-
guage impairments are distinct from those in Spe-
cific Language Impairment (SLI) is an unresolved
issue (Williams et al, 2008; Kjelgaard and Tager-
Flusberg, 2001). Accurate and detailed characteri-
zation of these impairments is important not only for
resolving this issue, but also for diagnostic practice
and remediation.
Language ability is typically assessed with struc-
tured instruments (?tests?) that elicit brief, easy to
?This research was supported in part by NIH NIDCD award
R01DC012033 and NSF award #0826654. Any opinions, find-
ings, conclusions or recommendations expressed in this publi-
cation are those of the authors and do not reflect the views of
the NIH or NSF. Thanks to Emily Prud?hommeaux for useful
discussion on this topic and help with the data.
score, responses to a sequence of items. For exam-
ple, the CELF-4 includes nineteen multi-item sub-
tests with tasks such as object naming, word defini-
tion, reciting the days of the week, or repeating sen-
tences (Semel et al, 2003). Researchers are begin-
ning to discuss the limits of structured instruments in
terms of which language impairments they tap into
and how well they do so, and are advocating the po-
tential benefits of language sample analysis ? an-
alyzing natural language samples ? to complement
structured assessment, specifically for language as-
sessment in ASD where pragmatic and social com-
munication issues are paramount yet are hard to
assess in a conventional test format (e.g. Tager-
Flusberg et al 2009). However, language sample
analysis faces two labor-intensive steps: transcrip-
tion and detailed coding of the transcripts.
To illustrate the latter, consider the Systematic
Analysis of Language Transcripts (SALT) (Miller
and Chapman, 1985; Miller et al, 2011), which is
the de-facto standard choice by clinicians looking
to code elicited language samples. SALT comprises
a scheme for coding transcripts of recorded speech,
together with software that tallies these codes, com-
putes scores describing utterance length and error
counts, and compares these scores with normative
samples. SALT codes indicate bound morphemes,
edits (which are referred to in the clinical literature
as ?mazes?), and several types of errors in transcripts
of natural language, e.g., omitted or inappropriate
words.
Although this has not been formally documented,
our experience with SALT coding has shown that the
codes vary in terms of: 1) difficulty of manual cod-
ing ? e.g., relatively subtle pragmatic errors versus
overgeneralization or marking bound morphemes;
1
2) utility for identifying particular disorders; and 3)
difficulty of automating the code. This raises an im-
portant question: Is there a combination of codes
that jointly discriminate well between relevant diag-
nostic groups, and at the same time are either easy
to code manually or can in principle be automated?
This paper explores, first, how well the various man-
ual SALT codes classify certain diagnostic groups;
and, second, whether we can automate manual codes
that are of diagnostic utility. Our goal is limited: it
is not the automation of all SALT codes, but the au-
tomation of those that in combination are of high di-
agnostic utility. Automating all SALT codes is sub-
stantially more challenging; yet, we note that even
when some of these codes do not aid in classify-
ing groups, they nevertheless may be of importance
for developing remediation strategies for individual
children. We are particularly interested in the im-
pact of Autism in addition to language impairments
for the utility of particular SALT codes.
The diagnostic groups are carefully chosen to
be pairwise matched either on language abilities or
on autism symptomatology, thus enabling a pre-
cise, ?surgical? determination of the degrees to
which SALT codes reflect language-specific vs.
autism-specific factors. Specifically, the groups in-
clude children with ASD with language impairment
(ALI); ASD with no language impairment (ALN);
SLI alone; and typically developing (TD), which is
strictly defined to exclude any neurodevelopmental
disorder. The TD and ALN groups, as well as the
ALI and SLI groups, are matched on language and
overall cognitive abilities, while the ALN and ALI
groups are matched on autism symptomatology but
not on language and overall cognitive abilities; all
groups are matched on chronological age.
Regarding our algorithmic approach, we note that
automatic detection of relatively subtle errors may
be exceedingly difficult, but perhaps such subtle er-
rors are less critical for diagnosis than more obvi-
ous ones. Most prior work in grammaticality de-
tection in spoken language has focused on special-
ized detectors (e.g., Caines and Buttery 2010; Has-
sanali and Liu 2011), such as mis-use of particular
verb constructions rather than coarser detectors for
the presence of diverse classes of errors. We demon-
strate that these specialized error detectors can break
down when confronted with real world dialogue, and
that in general, the features in these detectors re-
stricts their utility in detecting other sorts of errors.
We implement a detector to automatically extract
coarse SALT codes from an uncoded transcript. This
detector only depends upon part of speech tags, as
opposed to the parse features that are often used in
grammaticality detectors. In most cases, these au-
tomatically extracted codes enable us to distinguish
between diagnostic groups more effectively than do
features that can be extracted trivially from an un-
coded transcript.
As far as we know, researchers have not pre-
viously considered the utility of grammatical er-
ror codes to identify ASD or SLI. Prudhommeaux
and Rouhizadeh (2012), however, found that au-
tomatically extracted pragmatic features are useful
for identifying children with ASD, among children
both with and without SLI. Gabani et al (2009)
found that features derived from language models
are useful for distinguishing between children with
and without a language impairment, both in mono-
lingual English speakers, and in children who are
bilingual in English and Spanish.
Improving the characterization of a child?s lan-
guage impairments is a prerequisite to developing a
sound plan for language training and education for
that child. This paper presents a step in the direction
of effective automated analysis of linguistic samples
that can provide useful information even in the face
of comorbid disorders such as ASD and SLI.
2 Systematic Analysis of Language
Transcripts
Here we give an overview of what SALT requires of
transcriptions, and of SALT coding. The approach
has been in wide use for nearly 30 years (Miller and
Chapman, 1985), and now also exists as a software
package1 providing transcription and coding support
along with tools for aggregating statistics for man-
ual codes over the annotated corpora and comparing
with age norms. The SALT software is not the focus
of this investigation, so we do not discuss it further.
2.1 Basic Transcription
We apply the automated methods to what will be
called basic transcripts. Key for this concept is that,
first, these transcripts do not require linguistic ex-
pertise and thus can be performed by standard tran-
scription services; and, second, that ? as we shall
1http://www.saltsoftware.com/
2
see ? useful features can be automatically computed
from them.
Following the SALT guidelines, a basic transcript
should indicate: the speaker of each utterance, par-
tial words (or stuttering), overlapping speech, unin-
telligible words, and non-speech sounds. It should
be verbatim, regardless of whether a child?s utter-
ance contains neologisms (novel words) or gram-
matical errors (for example ?I goed? should be writ-
ten as such).
A somewhat subtle issue is that SALT prescribes
that the basic transcript be broken into communi-
cation units (which in this paper will be synony-
mous with utterance). Communication units are
defined as ?a main clause with all its dependent
clauses? (Miller et al, 2011). One reason for defin-
ing utterance boundaries with communication units,
rather than turns or sentences, is that in addition to
this being standard practice in language sample anal-
ysis, doing so does not reward children for making
long, but rather simple statements, nor does it penal-
ize children for being interrupted. To illustrate the
first point, the utterance ?I like apples, and bananas,
and pears, and oranges, and grapes.? is one sen-
tence long, but has five communication units (one at
each comma). If the sentence were used as the ba-
sic unit, the utterance would indicate the same level
complexity as the obviously more intricate ?for the
past three years we have lived in an apartment?. In
the basic transcript, each communication unit should
be terminated by one of the following punctuation
marks: ??? if it is a question, ??? if the speaker was
interrupted, ?>? if the speaker abandoned the utter-
ance, and ?.? in all other cases. Thus, the above
example would be transcribed as ?C: I like apples.
. . . C: and grapes.?
2.2 Markup
There are three broad categories of SALT codes: in-
dicators of 1) certain bound morphemes, 2) edits
(discussed below), and 3) errors.
Morphology The following inflectional suffixes
must be coded according to the SALT guidelines:
plural -s (/S), possessive -?s (/Z), possessive plural
-s? (/S/Z), past tense -ed (/ED), 3rd person singular
-s (/3S), progressive -ing (/ING). The following cl-
itics must also be delimited with a ?/?, provided the
resulting root is unmodified in the surface form: n?t,
?t, ?d, ?re, ?s, ?ve. Since these morphemes are only in-
dicated if the root is unmodified in the surface form,
?won?t? will remain unsegmented because ?wo? is
not the root; ?can?t? will be segmented ?can/?T? and
?don?t? will be segmented ?do/N?T?, so as to pre-
serve their respective roots. Nominal or verbal forms
with any of the preceding suffixes or clitics are writ-
ten as the base form with the code appended, for ex-
ample hitting? hit/ING, bases? base/S.
Edits Edits consist of filler words such as ?like?,
?um? and ?uh?, false starts, and revisions. There may
be multiple edits in a single utterance, as well as
multiple adjacent edits. Edits are indicated by paren-
theses, for example: ?(And they like) and she (like)
faint/3S.? Note that in the SALT manual, and the lan-
guage sample analysis literature, edits are referred to
as mazes. We use the term edit here because this is
the more widely used term for this phenomenon in
natural language processing.
Error codes The exact set of error codes used de-
pends upon the clinician?s needs and the errors of
interest. Here we consider several key errors out-
lined in the SALT manual. These error codes and
examples are shown in Table 1. Some of these codes
describe precise classes of errors, for example [EO]
or [OW], but others do not. For example, [EW]
can describe using the wrong verb, tense, preposi-
tion or pronoun (in terms of case, person or gender),
as well as other errors. Note that [EU] (and [EC]) er-
ror codes can occur in grammatical utterances. The
[EU] code marks utterances that are ungrammatical
for reasons not captured by the other error codes, for
example severe problems with word order, or utter-
Table 1: SALT error codes and examples
Code Meaning Example Count in Corpus
[EC] Inappropriate response Did you help yourself stop? Mom[EC]. 9
[EO] Overgeneralization Yeah, cuz I almost saw/ED[EO] one. 229
[EW] Error word I play/ED of[EW] the cat. 1,456
[EU] Utterance-level error You can see it very hard because it/?S under my hair[EU]. 532
[EX] Extraneous word Would you like to be[EX] fall down? 322
[OM] Omitted morpheme The cat eat[OM] fish. 881
[OW] Omitted word He [OW] going now. 770
3
ances which are simply nonsensical, as in Table 1.
3 Evaluation of Manual Codes
In this section we use features extracted from SALT-
coded transcripts for classification. We consider two
different types of features: baseline features, which
are easily derived from a basic transcript; and fea-
tures derived from SALT codes. We investigate
these features to determine which SALT codes are
most worth automating for classification.
3.1 Data
Our data is a collection of 144 transcripts of the
Autism Diagnostic Observation Schedule (ADOS),
which is a semi-structured task that includes an
examiner and a child (Lord et al, 2002). Semi-
structured means that the examiner carries out a
sequence of rigorously specified activities, but her
prompts and questions are not scripted verbatim for
all of them. Detailed guidelines exist for scoring
the ADOS, but these are not considered in the cur-
rent paper. All transcripts have been manually coded
with SALT codes, described in Table 1.
Subjects ranged in age between 4 and 8 years and
were required to be intelligible, to have a full-scale
IQ of greater than 70, and to have a mean length of
utterance (MLU) of at least 3. Diagnoses of ASD
and of SLI followed standard procedures, and were
based on clinical consensus in accordance to diag-
nostic criteria outlined in the DSM-IV (American
Psychiatric Association, 2000). Furthermore, ASD
diagnosis required ADOS and Social Communica-
tion Questionnaire scores (SCQ) (Berument et al,
1999) to meet conventional thresholds. Diagnosis
of SLI required a CELF Core Language Score of at
least 1 standard deviation below the mean, in addi-
tion to exclusion of ASD.
Children were partitioned into pairs of groups
matched on certain key measures. Table 2 shows
these pairs and what they were matched on. The
individuals were selected from the initial pool of
all participants using the algorithm proposed by van
Santen et al (2010), in which, for a given pair of
groups, children are iteratively removed from each
group until there is no significant difference (at p <
0.02) on any measure on which we want the pair to
be matched. We combined some groups into com-
posite groups: ASD (ALI and ALN), nASD (SLI
and TD), LN (?language normal?: ALN and TD),
and LI (?language impaired?: ALI and SLI).
Group 1 Group 2
Group N Group N Matched on
ALI 25 ALN 21 Age, ADOS, SCQ
ALI 24 SLI 19 Age, NVIQ, VIQ
ALN 25 TD 27 Age, NVIQ, VIQ
ASD 48 nASD 61 Age
LN 61 LI 39 Age
SLI 15 TD 38 Age
Table 2: Matched measures for paired groups (ADOS =
ADOS score, NVIQ = non-verbal IQ, VIQ = verbal IQ)
3.2 Features
The term ?feature? will be used to refer to instances
of various classes of SALT codes as well as to in-
stances of other events that can be trivially extracted
from the basic transcripts but do not involve SALT
codes (e.g, the ratio of ?uh? to ?um?). We distinguish
between five levels of features, enumerated in Table
3, that vary in the number and complexity of codes
required. This ranges from the baseline features that
require no manual codes to SALT-5 features that re-
quire full SALT coding. We consider two normal-
ized variants of each feature: one normalized by the
number of utterances spoken by the child, and the
other normalized by the number of words spoken
by the child (except for TKCT). The ratios OCRAT
and UMUHRAT are never normalized. Each feature
level includes all features on lower levels. Finally,
to make our investigation into feature combinations
more tractable, we do not consider combining two
different normalizations of the same feature.
3.3 Classification
We perform six classification tasks in our investi-
gation, according to the paired groups in Table 2:
ALI/ALN; ALI/SLI; ALN/TD; ASD/nASD; LN/LI;
and SLI/TD. We extract various features from the
ADOS transcripts, and then classify the children in
a leave-pair-out (LPO) schema (Cortes et al, 2007)
using the scikit logistic regression classifier with de-
fault parameters (Pedregosa et al, 2011). For LPO
analysis, we iterate over all possible pairs that con-
tain one positive and one negative instance (i.e. chil-
dren with different diagnoses), training on all other
instances, and testing on that pair. We count a trial
as a success if the classifier assigns a higher proba-
bility of being positive to the positive instance than
to the negative instance. We then divide the num-
ber of successes by the number of pairs to get an
unbiased estimate of the area under the receiver op-
erating curve (AUC) (Airola et al, 2011). AUC is
4
Group Feature Description
Baseline CEOLP # of times examiner speaks while child is talking
ECOLP # of times child speaks while examiner is talking
INCCT Incomplete word count
OCRAT Ratio of open- to closed-class words
TKCT Token count
TPCT Type count
UMUHRAT Ratio of ?uh? to ?um?
UINTCT Unintelligible word count
SALT-1 All baseline features +
MPCT Morpheme count
EDITCT Edit count
SALT-2 All SALT-1 features +
NERRUTT Number of utterances with any SALT error codes
SALT-3 All SALT-2 features +
ERRCT Count of SALT error codes
SALT-4 All SALT-3 features +
UTLERRCT Count of utterance level errors (EC / EU)
WDLERRCT Count of word level errors (all other error codes)
SALT-5 All SALT-4 features +
XCT Count of individual error codes (X=EC, EO, . . . ; see Table 1)
Table 3: Features by Level
the probability that the classifier will assign a higher
score to a randomly chosen positive example than to
a randomly chosen negative example.
3.4 Determining Relevant Features
We use a t-test based criterion as a simple way to de-
termine which features to investigate for each clas-
sification task. For a given classification task, we
perform a t-test for independent samples on each
feature under both normalization schemes (if ap-
propriate). We retain a feature for investigation if
that feature is significantly different between the two
groups at the ? = 0.10 level. If a particular feature
varies significantly between groups under both nor-
malization schemes, we retain the version that has
the larger T-statistic. For the sake of brevity, we
do not report all of the features that varied between
groups here, but this data is available upon request
from the authors.
3.5 Initial Feature Ablation
We perform feature ablation to see which features
are most useful for performing each classification
task. Figure 1 shows the maximum performance (in
terms of AUC) over all subsets of features at each
feature level (on the x-axis) on each of the six di-
agnostic classification tasks. Missing values for a
particular level of features for any comparison indi-
cate that no features in that level that passed the t-test
based criterion for the two groups being compared.
Figure 1 illustrates two important points. First,
classification difficulty depends heavily on the pair
that is being compared. For example, the AUC
for ALI/SLI is at most 0.723 (SALT-5), while the
AUC for SLI/TD reaches 0.982 (SALT-5). This is
not surprising, as some pairs, most notably SLI/TD,
differ widely in coarse measures of language abil-
ity (such as non-verbal IQ), while other pairs, in-
cluding ALI/SLI, do not. Second, in many of the
tasks, SALT-derived features are of high utility, but
the biggest gain in classification performance comes
with SALT-2, which is a count of the number of
sentences containing any SALT error code. In fact,
for all but one classification task (ASD/nASD), the
AUC achieved with SALT-2 is at least 96% of the
maximum AUC. Furthermore, the best feature set
using SALT-2 features for most of these tasks is ei-
ther the NERRUTT feature alone, or in the case of
ALI/SLI, NERRUTT and TPCT. These results lead
us to conclude that the most important SALT-derived
feature to code is NERRUTT.
Perhaps surprisingly, Figure 1 also shows that for
ALN/TD and SLI/TD, performance at SALT-1 is
lower than the baseline. There are two reasons for
this, which we explain in turn: 1) the SALT-1 fea-
ture set must include a feature that is less useful than
those in the optimal baseline feature set, and 2) the
classifier will not ignore this feature. MPCT must be
included in SALT-1 for both pairs, because the only
5
Figure 1: Maximum classification performance (AUC) at different feature levels (Bln=Baseline, S-N=SALT-N)
other SALT-1 feature, EDITCT, does not vary signif-
icantly between either ALN/TD or SLI/TD. Further-
more, MPCT is highly correlated with TKCT, yet
TKCT is not in the best baseline feature set for ei-
ther of these pairs. Therefore, the SALT-1 feature
set is required to include a feature that is less useful
than the most useful ones in the baseline set, which
results in lower performance. Once MPCT is in-
cluded in the SALT-1 feature set, the logistic regres-
sion classifier will not ignore it by assigning it a zero
coefficient. This is because MPCT distinguishes be-
tween groups, and because the classifier is trained
at each round of LPO classification to maximize the
likelihood of the training data, rather than the AUC
estimate provided by LPO classification.
3.6 Counting Specific Error Codes
The single feature in SALT-2, NERRUTT, counts
how many utterances spoken by the child contain at
least one SALT error code. Some of these heteroge-
nous errors, for example overgeneralization errors
([EO]), should be straightforward to identify auto-
matically. Automatically identifying others, for ex-
ample utterances that are inappropriate in context
([EC]), would be more difficult. Therefore, before
automating the extraction of NERRUTT, we should
see which errors most need to be identified, and
which can safely be ignored. To do this, we repeat
our LPO classification procedure on various tasks
using SALT-2 features.
We perform the following procedure to identify
the most diagnostically informative errors: for each
subset s of SALT error codes, 1) compute the fea-
ture NERRUTTSUBSET by counting the number of
utterances that contain any of the errors in s; then 2)
perform the LPO diagnostic classification task using
NERRUTTSUBSET as the only feature. The results
of this experiment are in Table 4. The ?% Max? col-
umn shows classification performance when a par-
ticular subset of error codes were counted, relative
to the maximum performance yielded by any subset
of error codes for that particular task. We exclude
the ALN/TD and ASD/nASD tasks from this exper-
iment because NERRUTT does not improve perfor-
mance on these tasks. This is perhaps unsurprising,
because SALT codes were designed to be diagnostic
of SLI, not ASD.
We find that in all tasks, ignoring certain error
codes raises performance. These results also show
that it is not necessary, and indeed not ideal, to iden-
tify utterances containing any SALT code. Identi-
fying utterances that contain any of the following
three codes is sufficient to achieve at least 97% of
the maximum AUC enabled by counting any sub-
set of SALT codes: [EW], [OM], [OW]. For clarity,
NERRUTTMOD is the count of utterances that con-
tain any of those three SALT codes.
Table 4: AUC from Counting Subsets of Errors
Classification Errors Counted AUC % Max
ALI/ALN EW, OM 0.762 100
EW, OM, OW 0.739 97
all 0.724 93
ALI/SLI EW, OM 0.715 100
EW, OM, OW 0.704 98
all 0.676 95
LN/LI EW, OM, OW 0.901 100
all 0.881 98
SLI/TD OM, OW 0.984 100
EW, OM, OW 0.970 99
all 0.951 97
6
3.7 Robustness of NERRUTTMOD feature to
noise: a simulation experiment
We will consider two general ways of automatically
extractingNERRUTTMOD. The first way is to build
a detector to identify utterances that contain at least
one relevant error. The second way is to make de-
tectors for the each relevant error, then combine the
output of these detectors. It is unlikely that any error
detector will perform perfectly. Prior to investiga-
tion of automation strategies, we would like to get an
idea of how much such errors will affect diagnostic
classification performance. To this end, we investi-
gate how well we can perform the diagnostic classi-
fication tasks when noise is deliberately introduced
into the NERRUTTMOD values via simulation.
We consider two scenarios. In the first, we as-
sume a single error detector will be used to extract
NERRUTTMOD. We take each manually coded ut-
terance, then randomly change whether or not that
sentence is counted as having an error to simulate
different precision and recall levels of the automated
NERRUTTMOD extractor. We repeat this procedure
100 times for each classification task, and then ex-
amine the mean AUC over all trials. In the sec-
ond scenario, we assume a detector for each error
code that counts a sentence as having an error any
time one of the detectors fires. We randomly cor-
rupt the detection of each error code considered in
NERRUTTMOD in turn to simulate different preci-
sion and recall levels of each individual error detec-
tor. We assume perfect detection of all errors not
being randomly corrupted. Again, we repeat this
procedure 100 times for each classification task, and
consider the mean AUC over all trials.
In both experiments, and in all classification tasks,
we find that the NERRUTTMOD feature is ex-
tremely robust to noise. For example, finding the
NERRUTTMOD feature with a single detector with
a precision/recall of 0.1/0.3 enables SLI/TD clas-
sification with an average AUC of 0.975, as com-
pared to the maximum AUC of 0.984, enabled by
a perfect detector. When we use a cascaded de-
tector to corrupt each of the two errors counted in
NERRUTTMOD for classifying SLI/TD, so long as
one error is detected perfectly, the other error only
needs to be detected with precision and recall of 0.1
to enable a classification AUC within 0.02 of the
maximum.
The extreme robustness of this feature may appear
surprising, but it is easily explained by the data. The
mean value of NERRUTTMOD for the SLI group
is 7.8 times the mean value of this feature for the
TD group. So long as there is a correlation between
the true value of NERRUTTMOD and the estimated
value, as we have assumed in this experiment, then
the estimated value is bound to be of utility in clas-
sification. This bodes well for the utility of automa-
tion, even for a difficult task of discovering some of
the relatively subtle errors coded in SALT.
4 Automatic Feature Extraction
4.1 Evaluating Hassanali and Liu?s System
Hassanali and Liu developed two grammaticality de-
tectors that they used to identify ungrammatical ut-
terances in transcriptions of speech from children
both with and without language impairments (Has-
sanali and Liu, 2011). They tested their grammati-
cality detectors on the Paradise corpus, which con-
sists of conversations with children elicited during
an investigation of otitis media, a hearing disor-
der. They present both a rule-based and a statis-
tical grammaticality detector. Both detectors con-
sist of sub-detectors for the errors shown in Table
5. The rule-based and statistical detectors perform
well, with the statistical detector outperforming the
rule-based one (F1=0.967 vs. 0.929). The statistical
detector, however, requires each error identified by
any of the sub-detectors to be manually identified in
the training data.
We reimplement both the rule based and statis-
tical detectors proposed by Hassanali and Liu, and
apply it to our data, with three modifications. The
first two are minor: 1) we substitute the Charniak-
Johnson reranking parser (2005) for Charniak?s
original parser (Charniak, 2000), and 2) we use the
scikit multinomial naive bayes classifier (Pedregosa
et al, 2011) instead of the one in WEKA (Hall et al,
2009). The third difference is that we use these de-
tectors to identify SALT error codes rather than the
errors these classifiers were originally built to detect.
The mapping of the original errors to SALT error
codes is given in Table 5. To clarify, if we are train-
ing the ?Missing Verb? detector, then any utterance
with an [OW] code is taken to be a positive exam-
ple. This issue does not present itself with the rule-
based detector because it is not trained. Note that the
two verb agreement features may correspond to ei-
ther [EW] or [OM] SALT codes. For example, ?you
does? would be [EW] because of the otiose 3rd per-
7
Error SALT code
Misuse of -ing participle [EW]
Missing copulae [OW]
Missing verb [OW]
Subject-auxilliary agreement [EW]
Subject-verb agreement [EW]/[OM]
Missing infinitive ?to? [OW]
Table 5: Error detectors proposed by Hassanali and Liu
son singular suffix, while ?he do? would be an [OM]
because it is missing that same suffix.
Hassanali and Liu?s error detectors perform
poorly on our data. Table 6 reports the performance
of their detectors detecting utterances with various
error codes. Five of the six statistical error detec-
tors that Hassanali and Liu proposed are unable to
identify any of the errors in our data. The?misuse
of -ing participle? detector, however, is an excep-
tion, and its performance detecting the analogous
error code [EW], using 10-fold cross validation is,
shown in Table 6. To detect the two pairs of er-
ror codes, [EW][OM] and [OM][OW], and all three
relevant error codes ([EW][OM][OW]), we use the
appropriate rule based detectors. For example, to
detect utterances with either [EW] or [OM] errors,
we pool the detectors for the analogous error codes:
?misuse of -ing participle?, ?subject-auxilliary agree-
ment?, and ?subject-verb agreement?.
There are three factors that may explain the poor
performance observed with most of Hassanali and
Liu?s error detectors when used with our data. The
first is that the three SALT codes we try to detect
([EW], [OM], and [OW]) capture a wider variety of
errors than the six in Hassanali and Liu?s system.
This could account for the low recall. Second, there
are many utterances in our data that Hassanali and
Liu?s system would label an error, but which are not
marked with any SALT error codes. For example, if
the examiner asks the child what she is doing, ?eat-
ing spaghetti? is a faultless response, even though it
is missing both the subject and auxiliary verb. Such
utterances may account for the low precision. Fi-
nally, most of Hassanali and Liu?s sub-detectors de-
pend upon features describing the presence or ab-
sence of specific structures in the parses of the input.
The exception to this is the statistical ?misuse of -ing
participle? detector, which uses part of speech (POS)
tag bigrams and skip bigrams as features. It should
come as no surprise then that the ?misuse of -ing par-
ticiple? is the most robust of these detectors. Indeed,
Codes
System Detected P R F1
Hassanali [EW]? 0.074 0.218 0.110
& Liu [EW][OM]* 0.049 0.277 0.083
[OM][OW]* 0.028 0.191 0.049
All three* 0.066 0.354 0.111
POS-tag [EW] 0.074 0.218 0.110
feature- [OM] 0.070 0.191 0.103
based [OW] 0.064 0.210 0.099
classifier [EW][OM] 0.102 0.269 0.148
[OM][OW] 0.102 0.269 0.148
All three 0.127 0.308 0.180
Table 6: Performance on automatic detection of utter-
ances with certain error codes using Hassanali and Liu?s
detectors, and general POS-tag-feature-based classifier.
? = ?misuse of -ing participle?, statistical; * = rule-based
in what follows, we make use of general POS-tag
features (tag n-gram and skip n-grams) as they do in
this detector, for a general purpose detector not tar-
geted specifically at this particular construction, but
rather to detect the presence of arbitrary given sets
of error tags.
4.2 Automatic SALT error code detection
We compare three types of automatic error code de-
tectors: 1) individual error code detectors; 2) pair
detectors, each of which detects a pair of error codes
included in NERRUTTMOD, following Table 4; and
3) a generic detector that identifies any utterance
containing any of the following SALT codes: [EW],
[OM], or [OW]. We investigate four different fea-
tures, all of which are easily derived from the basic
transcript: bigrams and skip bigrams of words, and
POS tags. We use POS tags extracted from the out-
put of the Charniak-Johnson reranking parser (2005)
(also used in our reimplementation of Hassanali and
Liu?s detectors) for simplicity. We use the Bernoulli
Naive Bayes classifier in scikit with the default set-
tings (Pedregosa et al, 2011).
We find that the word features do not aid clas-
sification in any condition, and that using both bi-
grams and skip bigrams of POS tags improves on
using either alone. We report the performance of
the three types of error detectors in Table 6. These
results are from 10-fold cross-validation using POS
tag bigrams and skip bigrams as features. Note that
the general POS-tag-feature-based classifier uses the
same features as Hassanali and Liu?s statistical ?mis-
use of -ing participle? detector, which is why the
performance for detecting [EW] error codes alone
8
Manual features Automatic extraction
Baseline SALT-2 SALT-2 features
Baseline ? Optimized ?
Diagnoses AUC AUC ? AUC ? AUC
ALI/ALN 0.619? 0.723 0.5 0.611 0.94 0.676
ALI/SLI 0.562 0.686 0.5 0.632 0.99 0.671
LN/LI 0.755 0.881 0.5 0.801 0.50 0.801
SLI/TD 0.840 0.951 0.5 0.805 0.99 0.840
? SALT-1; no significantly different baseline features
Table 7: Diagnostic classification AUC using automatically extracted NERRUTTMOD
is identical between the two systems.
The generic error detector yields higher perfor-
mance than either the individual or pair error detec-
tors. Coding training data for the generic detector is
simpler than doing so for the others because it only
involves a single round of binary coding.
4.3 Diagnostic Classification
We repeat the LPO diagnostic classification tasks
using the automatically extracted NERRUTTMOD
feature. We recompute NERRUTTMOD for each
speaker at each iteration, training on all data except
for the two speakers in the test pair, and the speaker
whose NERRUTTMOD feature we are predicting.
The results from this task are shown in Table 7.
As can be seen in Table 7, diagnostic classifica-
tion performance using the automatically extracted
the NERRUTTMOD feature is markedly lower than
when we extracted this feature from manual codes.
However, raising the probability threshold ? at
which utterances are counted as containing an er-
ror from its default value of 0.5, improves diagnos-
tic classification performance for all but one pair
(LN/LI). This is because increasing the probability
threshold at which we count an utterance as hav-
ing an error improves in NERRUTTMOD detection.
For example, in the ALI/SLI group, using the de-
fault ? = 0.5, and a leave-one-out scenario, we can
automatically extract NERRUTTMOD with a preci-
sion/recall score of 0.19/0.47. When we increase ?
to 0.99, the precision and recall become 0.23/0.24.
Even though there is a massive drop in recall, the
improvement in precision is able to boost diagnostic
classification performance.
In all but one pair (SLI/TD), the automati-
cally extracted NERRUTTMOD feature improves
classification over the baseline, even though the
NERRUTTMOD extractor performs poorly in terms
of intrinsic evaluation, with an F1 score of 0.180.
These results are in line with the experiments per-
forming diagnostic classification with an artificially
noisy NERRUTTMOD feature (see Section 3.7).
These results also demonstrate that the automati-
cally extracted values of NERRUTTMOD are suffi-
ciently correlated with the true values of this feature
to be of some diagnostic utility.
5 Conclusions
We have found that the SALT codes provide use-
ful information for distinguishing between certain
diagnostic groups, but not all of them. Specifi-
cally, and not surprisingly given SALT?s focus on
language disorders and not generally on atypical
language use characteristic of ASD, adding SALT-
derived features to baseline features added little
to ASD/nASD, ALI/SLI, or ALN/TD classifica-
tion accuracy, but added substantially to SLI/TD,
ALI/ALN, and LN/LI classification accuracy. Fur-
thermore, we found that a simplified coding schema
is almost as useful as the complete one for differ-
entiating between these groups. Finally, we have
proposed a simple method to automatically extract
a variant of the most useful SALT-derived feature,
NERRUTTMOD, which is a count of sentences that
contain any of three types of errors (omitted mor-
phemes or words, and generic word-level errors).
Although this feature?s utility degrades when ex-
tracted automatically, it still has considerable dis-
criminative value.
In future work, we will investigate the util-
ity of more sophisticated features for extracting
NERRUTTMOD and other SALT-derived features.
We will also investigate the utility of other linguistic
features, for example parse structure, for the diag-
nostic classification task. Finally, we will also con-
sider whether we can perform the diagnostic classi-
fication task more effectively using cascaded binary
classifiers (for example language impaired vs. lan-
guage normal), as opposed to having a classifier for
every diagnostic pair.
9
References
Antti Airola, Tapio Pahikkala, Willem Waegeman,
Bernard De Baets, and Tapio Salakoski. 2011. An ex-
perimental comparison of cross-validation techniques
for estimating the area under the roc curve. Computa-
tional Statistics & Data Analysis, 55(4):1828?1844.
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC, 4th edition.
Sibel Kazak Berument, Michael Rutter, Catherine Lord,
Andrew Pickles, and Anthony Bailey. 1999. Autism
screening questionnaire: diagnostic validity. The
British Journal of Psychiatry, 175(5):444?451.
Andrew Caines and Paula Buttery. 2010. You talking to
me?: A predictive model for zero auxiliary construc-
tions. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
43?51. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139. Morgan Kaufmann
Publishers Inc.
Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.
2007. An alternative ranking problem for search en-
gines. In Proceedings of WEA-2007, LNCS 4525,
pages 1?21. Springer-Verlag.
Keyur Gabani, Melissa Sherman, Thamar Solorio, Yang
Liu, Lisa M Bedore, and Elizabeth D Pena. 2009.
A corpus-based approach for the prediction of lan-
guage impairment in monolingual english and spanish-
english bilingual children. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 46?55. Association
for Computational Linguistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
K. Hassanali and Y. Liu. 2011. Measuring language de-
velopment in early childhood education: a case study
of grammar checking in child language transcripts. In
Proceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications, pages
87?95. Association for Computational Linguistics.
Margaret M Kjelgaard and Helen Tager-Flusberg. 2001.
An investigation of language impairment in autism:
Implications for genetic subgroups. Language and
cognitive processes, 16(2-3):287?308.
Catherine Lord, Michael Rutter, PC DiLavore, and Susan
Risi. 2002. Autism diagnostic observation schedule:
ADOS. Western Psychological Services.
J. Miller and R. Chapman. 1985. Systematic analysis of
language transcripts. Madison, WI: Language Analy-
sis Laboratory.
Jon F. Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT soft-
ware: A Clinician?s Guide to Language Sample Anal-
ysis. SALT Software, LLC.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
Emily Prudhommeaux and Masoud Rouhizadeh. 2012.
Automatic detection of pragmatic deficits in children
with autism. In Proceedings of the 3rd Workshop on
Child, Computer and Interaction (WOCCI 2012).
Eleanor Messing Semel, Elisabeth Hemmersam Wiig,
and Wayne Secord. 2003. Clinical evaluation of lan-
guage fundamentals. The Psychological Corporation,
A Harcourt Assessment Company, Toronto, Canada,
fourth edition.
Helen Tager-Flusberg, Sally Rogers, Judith Cooper, Re-
becca Landa, Catherine Lord, Rhea Paul, Mabel Rice,
Carol Stoel-Gammon, Amy Wetherby, and Paul Yoder.
2009. Defining spoken language benchmarks and se-
lecting measures of expressive language development
for young children with autism spectrum disorders.
Journal of Speech, Language and Hearing Research,
52(3):643.
Jan PH van Santen, Emily T Prud?hommeaux, Lois M
Black, and Margaret Mitchell. 2010. Computational
prosodic markers for autism. Autism, 14(3):215?236.
David Williams, Nicola Botting, and Jill Boucher. 2008.
Language in autism and specific language impair-
ment: Where are the links? Psychological Bulletin,
134(6):944.
10
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69?77,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Challenges in Automating Maze Detection
Eric Morley
CSLU
OHSU
Portland, OR 97239
morleye@gmail.com
Anna Eva Hallin
Department of Communicative
Sciences and Disorders
New York University
New York, NY
ae.hallin@nyu.edu
Brian Roark
Google Research
New York, NY 10011
roarkbr@gmail.com
Abstract
SALT is a widely used annotation ap-
proach for analyzing natural language
transcripts of children. Nine annotated
corpora are distributed along with scoring
software to provide norming data. We ex-
plore automatic identification of mazes ?
SALT?s version of disfluency annotations
? and find that cross-corpus generalization
is very poor. This surprising lack of cross-
corpus generalization suggests substantial
differences between the corpora. This is
the first paper to investigate the SALT cor-
pora from the lens of natural language pro-
cessing, and to compare the utility of dif-
ferent corpora collected in a clinical set-
ting to train an automatic annotation sys-
tem.
1 Introduction
Assessing a child?s linguistic abilities is a critical
component of diagnosing developmental disorders
such as Specific Language Impairment or Autism
Spectrum Disorder, and for evaluating progress
made with remediation. Structured instruments
(?tests?) that elicit brief, easy to score, responses
to a sequence of items are a popular way of per-
forming such assessment. An example of a struc-
tured instrument is the CELF-4, which includes
nineteen multi-item subtests with tasks such as
object naming, word definition, reciting the days
of the week, or repeating sentences (Semel et al.,
2003). Over the past two decades, researchers
have discussed the limitations of standardized tests
and how well they tap into different language im-
pairments. Many have advocated the potential
benefits of language sample analysis (LSA) (John-
ston, 2006; Dunn et al., 1996). The analysis of
natural language samples may be particularly ben-
eficial for language assessment in ASD, where
pragmatic and social communication issues are
paramount yet may be hard to assess in a conven-
tional test format (Tager-Flusberg et al., 2009).
At present, the expense of LSA prevents it from
being more widely used. Heilmann (2010), while
arguing that LSA is not too time-consuming, esti-
mates that each minute of spoken language takes
five to manually transcribe and annotate. At this
rate, it is clearly impractical for clinicians to per-
form LSA on hours of speech. Techniques from
natural language processing could be used to build
tools to automatically annotate transcripts, thus fa-
cilitating LSA.
Here, we evaluate the utility of a set of anno-
tated corpora for automating a key annotation in
the de facto standard annotation schema for LSA:
the Systematic Analysis of Language Transcripts
(SALT) (Miller et al., 2011). SALT comprises a
scheme for coding transcripts of recorded speech,
together with software that tallies these codes,
computes scores describing utterance length and
error counts, among a range of other standard mea-
sures, and compares these scores with normative
samples. SALT codes indicate bound morphemes,
several types of grammatical errors (for example
using a pronoun of the wrong gender or case), and
mazes, which are defined as ?filled pauses, false
starts, and repetitions and revisions of words, mor-
phemes and phrases? (Miller et al., 2011, p. 48).
Mazes have sparked interest in the child lan-
guage disorders literature for several reasons.
They are most often analyzed from a language
processing perspective where the disruptions are
viewed as a consequence of monitoring, detect-
ing and repairing language, potentially including
speech errors (Levelt, 1993; Postma and Kolk,
1993; Rispoli et al., 2008). Several studies have
found that as grammatical complexity and utter-
ance length increase, the number of mazes in-
creases in typically developing children and chil-
dren with language impairments (MacLachlan and
69
Chapman, 1988; Nippold et al., 2008; Reuter-
ski?old Wagner et al., 2000; Wetherell et al., 2007).
Mazes in narrative contexts have been shown
to differ between typical children and children
with specific language impairment (MacLachlan
and Chapman, 1988; Thordardottir and Weismer,
2001), though others have not found reliable group
differences (Guo et al., 2008; Scott and Windsor,
2000). Furthermore, outside the potential useful-
ness of looking at mazes in themselves, mazes al-
ways have to be detected and excluded in order
to calculate other standard LSA measures such
as mean length of utterance and type or token
counts. Mazes also must be excluded when ana-
lyzing speech errors, since some mazes are in fact
self-corrections of language or speech errors.
Thus, automatically delimiting mazes could be
clinically useful in several ways. First, if mazes
can be automatically detected, standard measures
such as token and type counts can be calculated
with ease, as noted above. Automatic maze detec-
tion could also be a first processing step for au-
tomatically identifying errors: error codes cannot
appear in mazes, and certain grammatical errors
may be easier to identify once mazes have been
excised. Finally, after mazes have been identified,
further analysis of the mazes themselves (e.g. the
number of word in mazes, and the placement of
mazes in the sentence) can provide supplementary
information about language formulation abilities
and word retrieval abilities (Miller et al., 2011, p.
87-89).
We use the corpora included with the SALT
software to train maze detectors. These are the
corpora that the software uses to compute refer-
ence counts. These corpora share several charac-
teristics we expect to be typical of clinical data:
they were collected under a diverse set of circum-
stances; they were annotated by different groups;
the annotations ostensibly follow the same guide-
lines; and the annotations were not designed with
automation in mind. We will investigate whether
we can extract usable generalizations from the
available data, and explore how well the auto-
mated system performs, which will be of interest
to clinicians looking to expedite LSA.
2 Background
Here we provide an overview of SALT and maze
annotations. We are not aware of any attempts
to automate maze detection, although maze de-
tection closely resembles the well-established task
of edited word detection. We also provide an
overview of the corpora included with the SALT
software, which are the ones we will use to train
maze detectors.
2.1 SALT and Maze Annotations
The approach used in SALT has been in wide use
for nearly 30 years (Miller and Chapman, 1985),
and now also exists as a software package
1
pro-
viding transcription and coding support along with
tools for aggregating statistics for manual codes
over the annotated corpora and comparing with
age norms. The SALT software is not the focus of
this investigation, so we do not discuss it further.
Following the SALT guidelines, speech should
be transcribed orthographically and verbatim. The
transcript must include and indicate: the speaker
of each utterance, partial words or stuttering, over-
lapping speech, unintelligible words, and any non-
speech sounds from the speaker. Even atypical
language, for example neologisms (novel words)
or grammatical errors (for example ?her went?)
should be written as such.
There are three broad categories of SALT anno-
tations: indicators of 1) certain bound morphemes,
2) errors, and 3) mazes. In general, verbal suffixes
that are visible in the surface form (for example
-ing in ?going?) and clitics that appear with an un-
modified root (so for example -n?t in ?don?t?, but
not the -n?t in ?won?t?) must be indicated. SALT
includes various codes to indicate grammatical er-
rors including, but not limited to: overgeneral-
ization errors (?goed?), extraneous words, omit-
ted words or morphemes, and inappropriate ut-
terances (e.g. answering a yes/no question with
?fight?). For more information on these standard
annotations, we refer the reader to the SALT man-
ual (Miller et al., 2011).
Here, we are interested in automatically delim-
iting mazes. In SALT, filled pauses, repetitions
and revisions are included in the umberella term
?mazes? but the manual does not include defini-
tions for any of these categories. In SALT, mazes
are simply delimited by parentheses; they have no
internal structure, and cannot be nested. Contigu-
ous spans of maze words are delimited by a single
set of parentheses, as in the following utterance:
(1) (You have you have um there/?s only)
there/?s ten people
1
http://www.saltsoftware.com/
70
To be clear, we define the task of automatically ap-
plying maze detections as taking unannotated tran-
scripts of speech as input, and then outputting a
binary tag for each word that indicates whether or
not it is in a maze.
2.2 Edited Word Detection
Although we are not aware of any previous work
on automating maze detection, there is a well-
established task in natural language processing
that is quite similar: edited word detection. The
goal of edited word detection is to identify words
that have been revised or deleted by the speaker,
for example ?to Dallas? in the utterance ?I want to
go to Dallas, um I mean to Denver.?. Many in-
vestigations have approached edited word detec-
tion from what Nakatani et al. (1993) have termed
?speech-first? perspective, meaning that edited de-
tection is performed with features from the speech
signal in addition to a transcript. These ap-
proaches, however, are not applicable to the SALT
corpora, because they only contain transcripts. As
a result, we must adopt a text-first approach to
maze detection, using only features extracted from
a transcript.
The text-first approach to edited word detec-
tion is well established. One of the first investi-
gations taking a text-first approach was conducted
by Charniak and Johnson (2001). There, they
used boosted linear classifiers to identify edited
words. Later, Johnson and Charniak (2004) im-
proved upon the linear classifiers? performance
with a tree adjoining grammar based noisy chan-
nel model. Zwarts and Johnson (2011) improve
the noisy channel model by adding in a reranker
that leverages features extracted with the help of a
large language model.
Qian and Liu (2013) have developed what is
currently the best-performing edited word detec-
tor, and it takes a text-first approach. Unlike the
detector proposed by Zwarts and Johnson, Qian
and Liu?s does not rely on any external data. Their
detector operates in three passes. In the first pass,
filler words (?um?, ?uh?, ?I mean?, ?well?, etc.) are
detected. In the second and third passes, edited
words are detected. The reason for the three passes
is that in addition to extracting features (mostly
words and part of speech tags) from the raw tran-
script, the second and third steps use features ex-
tracted from the output of previous steps. An ex-
ample of such features is adjacent words from the
utterance with filler words and some likely edited
words removed.
3 Overview of SALT Corpora
We explore nine corpora included with the SALT
software. Table 1 has a high level overview of
these corpora, showing where each was collected,
the age ranges of the speakers, and the size of each
corpus both in terms of transcripts and utterances.
Note that only utterances spoken by the child are
counted, as we throw out all others.
Table 1 shows several divisions among the cor-
pora. We see that one group of corpora comes
from New Zealand, while the majority come from
North America. All of the corpora, except for Ex-
pository, include children at very different stages
of language development.
Four research groups were responsible for the
transcriptions and annotations of the corpora in
Table 1. One group produced the CONVERSA-
TION, EXPOSITORY, NARRATIVESSS, and NAR-
RATIVESTORYRETELL corpora. Another was
responsible for all of the corpora from New
Zealand. Finally, the ENNI and GILLAMNT cor-
pora were transcribed and annotated by two dif-
ferent groups. For more details on these cor-
pora, how they were collected, and the anno-
tators, we refer the reader to the SALT web-
site at http://www.saltsoftware.com/
resources/databases.html.
Some basic inspection reveals that the corpora
can be put into three groups based on the me-
dian utterance lengths, and the distribution of ut-
Table 1: Description of SALT corpora
Corpus Transcripts Utterances Age Range Speaker Location
CONVERSATION 584 82,643 2;9 ? 13;3 WI & CA
ENNI 377 56,108 3;11 ? 10;0 Canada
EXPOSITORY 242 4,918 10;7 ? 15;9 WI
GILLAMNT 500 40,102 5;0 ? 11;11 USA
NARRATIVESSS 330 16,091 5;2 ? 13;3 WI & CA
NARRATIVESTORYRETELL 500 14,834 4;4 ? 12;8 WI & CA
NZCONVERSATION 248 25,503 4;5 ? 7;7 NZ
NZPERSONALNARRATIVE 248 20,253 4;5 ? 7;7 NZ
NZSTORYRETELL 264 2,574 4;0 ? 7;7 NZ
71
terance
2
lengths, following the groups Figure 1,
with the EXPOSITORY and CONVERSATION cor-
pora in their own groups. Note that the counts
in Figure 1 are of all of the words in each ut-
terance, including those in mazes. We see that
the corpora in Group A have a modal utterance
length ranging from seven to ten words. There are
many utterances in these corpora that are shorter
or longer than the median length. Compared to
the corpora in Group A, those in Group B have
a shorter modal utterance length, and fewer long
utterances. In Figure 1, we see that the CONVER-
SATION corpus consists mostly of very short utter-
ances. At the other extreme is the EXPOSITORY
corpus, which resembles the corpora in Group A
in terms of modal utterance length, but which gen-
erally contains longer utterances than any of the
other corpora.
4 Maze Detection Experiments
4.1 Maze Detector
We carry out our experiments in automatic maze
detection using a statistical maze detector that
learns to identify mazes from manually labeled
data using features extracted from words and auto-
matically predicted part of speech tags. The maze
detector uses the feature set shown in Table 2.
This set of features is identical to the ones used by
the ?filler word? detector in Qian and Liu?s disflu-
ency detector (2013). We also use the same clas-
2
All of these corpora are reported to have been segmented
into c-units, which is defined as ?an independent clause with
its modifiers? (Miller et al., 2011).
Table 2: Feature templates for maze word detection, follow-
ing Qian and Liu (2013). We extract all of the above features
from both words and POS tags, albeit separately. t
0
indicates
the current word or POS tag, while t
?1
is the previous one
and t
1
is the following. The function I(a, b) is 1 if a and b
are identical, and otherwise 0. y
?1
is the tag predicted for the
previous word.
Category Features
Unigrams t
?2
, t
?1
, t
0
, t
1
, t
2
Bigrams t
?1
t
0
, t
0
t
1
Trigrams t
?2
t
?1
t
0
, t
?1
t
0
t
1
, t
0
t
1
t
2
Logic Unigrams I(t
i
, t
0
), I(p
i
, p
0
);
?4 ? i ? 4; i 6= 0
Logic Bigrams I(t
i?2
t
i?1
, t
?1
t
0
)
I(t
i
t
i+1
, t
0
t
i+1
);
?4 ? i ? 4; i 6= 0
Predicted tag y
?1
(a) Group A
(b) Group B
(c) Others
Figure 1: Histograms of utterance length (including words
in mazes) in SALT corpora
sifier as the second and third steps of their system:
the Max Margin Markov Network ?M3N? classi-
fier in the pocketcrf toolkit (available at http://
code.google.com/p/pocketcrf/). The
M3N classifier is a kernel-based classifier that is
able to leverage the sequential nature the data in
this problem (Taskar et al., 2003). We use the fol-
lowing label set: S-O (not in maze); S-M (sin-
gle word maze); B-M (beginning of multi-word
72
maze); I-M (in multi-word maze); and E-M (end
of multi-word maze). The M3N classifier allows
us to set a unique penalty for each pair of con-
fused labels, for example penalizing an erroneous
prediction of S-O (failing to identify maze words)
more heavily than spurious predictions of maze
words (all -M labels). This ability is particularly
useful for maze detection because maze words are
so infrequent compared to words that are not in
mazes.
4.2 Evaluation
We split each SALT corpus into training, develop-
ment, and test partitions. Each training partition
contains 80% of the utterances the corpus, while
the development and test partitions each contain
10% of the utterances. We use the development
portion of each corpus to set the penalty matrix
system to roughly balance precision and recall.
We evaluate maze detection in terms of both
tagging performance and bracketing performance,
both of which are standard forms of evaluation
for various tasks in the Natural Language Pro-
cessing literature. Tagging performance captures
how effectively maze detection is done on a word-
by-word basis, while bracketing performance de-
scribes how well each maze is identified in its en-
tirety. For both tagging and bracketing perfor-
mance, we count the number of true and false
positives and negatives, as illustrated in Figure 2.
In tagging performance, each word gets counted
once, while in bracketing performance we com-
pare the predicted and observed maze spans. We
use these counts to compute the following metrics:
(P)recision =
tp
tp + fp
(R)ecall =
tp
tp + fn
F1 =
2PR
P + R
Note that partial words and punctuation are both
ignored in evaluation. We exclude punctuation be-
cause punctuation does not need to be included
in mazes: it is not counted in summary statistics
(e.g. MLU, word count, etc.), and punctuation er-
rors are not captured by the SALT error codes.
We exclude partial words because they are always
in mazes, and therefore can be detected trivially
with a simple rule. Furthermore, because par-
tial words are excluded from evaluation, the per-
formance metrics are comparable across corpora,
even if they vary widely in the frequency of partial
words.
For both space and clarity, we do not present
the complete results of every experiment in this
paper, although they are available online
3
. In-
stead, we present the complete baseline results,
and then report F1 scores that are significantly
better than the baseline. We establish statistical
significance by using a randomized paired-sample
test (see Yeh (2000) or Noreen (1989)) to com-
pare the baseline system (system A) and the pro-
posed system (system B). First, we compute the
difference d in F1 score between systems A and B.
Then, we repeatedly construct a random set of pre-
dictions for each input item by choosing between
the outputs of system A and B with equal proba-
bility. We compute the F1 score of these random
predictions, and if it exceeds the F1 score of the
baseline system by at least d, we count the itera-
tion as a success. The significance level is at most
the number of successes divided by one more than
the number of trials (Noreen, 1989).
4.3 Baseline Results
For each corpus, we train the maze detector on
the training partition and test it on the devel-
opment partition. The results of these runs are
in Table 3, which also includes the rank of the
size of each corpus (1 = biggest, 9 = smallest).
We see immediately that our maze detector per-
forms far better on some corpora than on oth-
ers, both in terms of tagging and bracketing per-
formance. We note that maze detection perfor-
mance is not solely determined by corpus size:
tagging performance is substantially worse on the
largest corpus (CONVERSATION) than the small-
3
http://bit.ly/1dtFTPl
Figure 2: Tagging and bracketing evaluation for maze detection. TP = True Positive, FP = False Positive, TN = True Negative,
FN = False Negative
Pred. ( and then it ) oh and then it ( um ) put his wings out .
Gold ( and then it oh ) and then it ( um ) put his wings out .
Tag TP ?3 FN TN ?3 TP TN ?4
Brack. FP, FN TP
73
Tagging Bracketing
Corpus Size Rank P R F1 P R F1
CONVERSATION 1 0.821 0.779 0.800 0.716 0.729 0.723
ENNI 2 0.923 0.882 0.902 0.845 0.837 0.841
EXPOSITORY 8 0.703 0.680 0.691 0.620 0.615 0.618
GILLAMNT 3 0.902 0.907 0.904 0.827 0.843 0.835
NARRATIVESSS 6 0.781 0.768 0.774 0.598 0.679 0.636
NARRATIVESTORYRETELL 7 0.799 0.774 0.786 0.627 0.671 0.649
NZCONVERSATION 4 0.832 0.835 0.838 0.707 0.757 0.731
NZPERSONALNARRATIVE 5 0.842 0.835 0.838 0.707 0.757 0.731
NZSTORYRETELL 9 0.905 0.862 0.883 0.773 0.780 0.776
Table 3: Baseline maze detection performance on development sections of SALT corpora: corpus-specific models
est (NZSTORYRETELL).
4.4 Generic Model
We train a generic model for maze detection on
all of the training portions of the nine SALT cor-
pora. We use the combined development sections
of all of the corpora to tune the loss matrix for bal-
anced precision and recall. We then test the re-
sulting model on the development section of each
SALT corpus, and evaluate in terms of tagging and
bracketing accuracy.
We find that the generic model performs worse
than the baseline in terms of both tagging and
bracketing performance on six of the nine corpora
corpora. The generic model significantly improves
tagging (F1=0.925, p ? 0.0022) on the NZSTO-
RYRETELL corpus, but the improvement in brack-
eting performance is not significant (p ? 0.1635).
There is improvement of both tagging (F1=0.805,
p ? 0.0001) and bracketing (F1=0.677, p ?
0.0025) performance on the NARRATIVESSS cor-
pus. The generic model does not perform better
than the baseline corpus-specific models on any
other corpora.
The poor performance of the generic model is
somewhat surprising, as it is trained with far more
data than any of the corpus-specific models. In
many tasks in natural language processing, in-
creasing the amount of training data improves the
resulting model, although this is not necessarily
the case if the additional data is noisy or out-of-
domain. This suggests two possibilities: 1) the
language in the corpora varies substantially, per-
haps due to the speakers? ages or the activity that
was transcribed; and 2) the maze annotations are
inconsistent between corpora.
4.5 Multi-Corpus Models
It is possible that poor performance of the generic
model relative to the baseline corpus-specific
models can be attributed to systematic differences
between the SALT corpora. We may be able to
train a model for a set of corpora that share particu-
lar characteristics that can outperform the baseline
models because such a model could leverage more
training data. We first evaluate a model for corpora
that contain transcripts collected from children of
similar ages. We also evaluate task-specific mod-
els, specifically a maze-detection model for story
retellings, and another for conversations. These
two types of models could perform well if chil-
dren of similar ages or performing similar tasks
produce mazes in a similar manner. Finally, we
train models for each group of annotators to see
whether systematic variation in annotation stan-
dards between research groups could be respon-
sible for the generic model?s poor performance.
We train all of these models similarly to the
generic model: we pool the training sections of
the selected corpora, train the model, then test on
the development section of each selected corpus.
We use the combined development sections of the
selected corpora to tune the penalty matrix to bal-
ance precision and recall.
Again, we only report F1 scores that are higher
than the baseline model?s, and we test whether
the improvement is statistically significant. We
do not report results where just the precision or
just the recall exceeds the baseline model perfor-
mance, but not F1, because these are typically the
result of model imbalance, favoring precision at
the expense of recall or vice versa. Bear in mind
that we roughly balance precision and recall on the
combined development sets, not each corpus?s de-
velopment set individually.
4.5.1 Age-Specific Model
We train a single model on the following cor-
pora: ENNI, GILLAMNT, NARRATIVESSS, and
NARRATIVESTORYRETELL. As shown in Ta-
ble 1, these corpora contain transcripts collected
from children roughly aged 4-12. In three of the
four corpora, the age-based model performs worse
than the baseline. The only exception is NAR-
74
RATIVESTORYRETELL, for which the age-based
model outperforms the baseline in terms of both
tagging (F1=0.794, p ? 0.0673) and bracketing
(F1=0.679, p ? 0.0062).
4.5.2 Task-Specific Models
We construct two task-specific models for maze
detection: one for conversations, and the other
for narrative tasks. A conversational model
trained on the CONVERSATION and NZCON-
VERSATION corpora does not improve perfor-
mance on either corpus relative to the base-
line. A model for narrative tasks trained on the
ENNI, GILLAMNT, NARRATIVESSS, NARRA-
TIVESTORYRETELL, NZPERSONALNARRATIVE
and NZSTORYRETELL corpora only improves
performance on one of these, relative to the base-
line. Specifically, the narrative task model im-
proves performance on the NARRATIVESSS cor-
pus both in terms of tagging (F1=0.797, p ?
0.0005) and bracketing (F1=0.693, p ? 0.0002).
4.5.3 Research Group-Specific Models
There are two groups of researchers that have
annotated multiple corpora: a group in New
Zealand, which annotated the NZCONVERSA-
TION, NZPERSONALNARRATIVE, and NZSTO-
RYRETELL corpora; and another group in Wis-
consin, which annotated the CONVERSATION,
EXPOSITORY, NARRATIVESSS, and NARRA-
TIVESTORYRETELL corpora. We trained re-
search group-specific models, one for each of
these groups.
Overall, these models do not improve perfor-
mance. The New Zealand research group model
does not significantly improve performance on any
of the corpora they annotated, relative to the base-
line. The Wisconsin research group model yields
significant improvement on the NARRATIVESSS
corpus, both in terms of tagging (F1=0.803, p ?
0.0001) and bracketing (F1=0.699, p ? 0.0001)
performance. Performance on the CONVERSA-
TION and EXPOSITORY corpora is lower with
the Wisconsin research group model than with
the corpus-specific baseline models, while perfor-
mance on NARRATIVESTORYRETELL is essen-
tially the same with the two models.
5 Discussion
We compared corpus-specific models for maze de-
tection to more generic models applicable to mul-
tiple corpora, and found that the generic models
performed worse than the corpus-specific ones.
This was surprising because the more generic
models were able to leverage more training data
than the corpus specific ones, and more training
data typically improves the performance of data-
driven models such as our maze detector. These
results strongly suggest that there are substantial
differences between the nine SALT corpora.
We suspect there are many areas in which the
SALT corpora diverge from one another. One
such area may be the nature of the language: per-
haps the language differs so much between each
of the corpora that it is difficult to learn a model
appropriate for one corpus from any of the oth-
ers. Another potential source of divegence is in
transcription, which does not always follow the
SALT guidelines (Miller et al., 2011). Two of the
idiosyncracies we have observed are: more than
three X?s (or a consonant followed by multiple
X?s) to indicate unintelligble language, instead of
the conventional X, XX, and XXX for unintelligi-
ble words, phrases, and utterances, respectively;
and non-canonical transcriptions of what appear
to be filled pauses, including ?uhm? and ?umhm?.
These idiosyncracies could be straightforward to
normalize using automated methods, but doing so
requires that they be identified to begin with. Fur-
thermore, although these idiosyncracies may ap-
pear to be minor, taken together they may actually
be substantial.
Another potential source of variation between
corpora is likely in the maze annotations them-
selves. SALT?s definition of mazes, ?filled pauses,
false starts, and repetitions and revisions of words,
morphemes and phrases? (Miller et al., 2011, p.
48), is very short, and none of the components
is defined in the SALT manual. In contrast, the
Disfluency Annotation Stylebook for Switchboard
Corpus (Meteer et al., 1995) describes a system
of disfluency annotations over approximately 25
pages, devoting two pages to filled pauses and five
to restarts. The Switchboard disfluency annota-
tions are much richer than SALT maze annota-
tions, and we are not suggesting that they are ap-
propriate for a clinical setting. However, between
the stark contrast in detail of the two annotation
systems? guidelines, and our finding that cross-
corpus models for maze detection perform poorly,
we recommend that SALT?s definition of mazes
and their components be elaborated and clarified.
This would be of benefit not just to those trying to
75
automate the application of SALT annotations, but
also to clinicians who use SALT and depend upon
consistently annotated transcripts.
There are two clear tasks for future research that
build upon these results. First, maze detection per-
formance can surely be improved. We note, how-
ever, that evaluating maze detectors in terms of F1
score may not always be appropriate if such a de-
tector is used in a pipeline. For example, there
may be a minimum acceptable level of precision
for a maze detector used in a preprocessing step
to applying SALT error codes so that maze exci-
sion does not create additional errors. In such a
scenario, the goal would be to maximize recall at
a given level of precision.
The second task suggested by this paper is to ex-
plore the hypothesized differences within and be-
tween corpora. Such exploration could ultimately
result in more rigorous, communicable guidelines
for maze annotations, as well as other annotations
and conventions in SALT. If there are systematic
differences in maze annotations across the SALT
corpora, such exploration could suggest ways of
making the annotations consistent without com-
pletely redoing them.
Acknowledgments
We would like to thank members of the ASD re-
search group at the Center for Spoken Language
Understanding at OHSU, for useful input into this
study: Jan van Santen, Alison Presmanes Hill,
Steven Bedrick, Emily Prud?hommeaux, Kyle
Gorman and Masoud Rouhizadeh. This research
was supported in part by NIH NIDCD award
R01DC012033 and NSF award #0826654. Any
opinions, findings, conclusions or recommenda-
tions expressed in this publication are those of the
authors and do not reflect the views of the NIH or
NSF.
References
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the second meeting of the North American
Chapter of the Association for Computational Lin-
guistics on Language technologies, pages 1?9. As-
sociation for Computational Linguistics.
Michelle Dunn, Judith Flax, Martin Sliwinski, and
Dorothy Aram. 1996. The use of spontaneous lan-
guage measures as criteria for identifying children
with specific language impairment: An attempt to
reconcile clinical and research incongruence. Jour-
nal of Speech and Hearing research, 39(3):643.
Ling-yu Guo, J Bruce Tomblin, and Vicki Samel-
son. 2008. Speech disruptions in the narratives
of english-speaking children with specific language
impairment. Journal of Speech, Language, and
Hearing Research, 51(3):722?738.
John J Heilmann. 2010. Myths and realities of lan-
guage sample analysis. SIG 1 Perspectives on Lan-
guage Learning and Education, 17(1):4?8.
Mark Johnson and Eugene Charniak. 2004. A tag-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 33?39, Barcelona, Spain, July.
Judith R Johnston. 2006. Thinking about child lan-
guage: Research to practice. Thinking Publications.
Willem JM Levelt. 1993. Speaking: From intention to
articulation, volume 1. MIT press, Cambridge, MA.
Barbara G MacLachlan and Robin S Chapman. 1988.
Communication breakdowns in normal and lan-
guage learning-disabled children?s conversation and
narration. Journal of Speech and Hearing Disor-
ders, 53(1):2.
Marie W Meteer, Ann A Taylor, Robert MacIntyre,
and Rukmini Iyer. 1995. Dysfluency annotation
stylebook for the switchboard corpus. University of
Pennsylvania.
Jon Miller and Robin Chapman. 1985. Systematic
analysis of language transcripts. Madison, WI: Lan-
guage Analysis Laboratory.
Jon F Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT
software: A clinician?s guide to language sample
analysis. SALT Software, LLC.
Christine Nakatani and Julia Hirschberg. 1993. A
speech-first model for repair detection and correc-
tion. In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
pages 46?53, Columbus, Ohio, USA, June. Associ-
ation for Computational Linguistics.
Marilyn A Nippold, Tracy C Mansfield, Jesse L Billow,
and J Bruce Tomblin. 2008. Expository discourse
in adolescents with language impairments: Exam-
ining syntactic development. American Journal of
Speech-Language Pathology, 17(4):356?366.
Eric W Noreen. 1989. Computer intensive methods
for testing hypotheses. an introduction. 1989. John
Wiley & Sons, 2(5):33.
Albert Postma and Herman Kolk. 1993. The covert
repair hypothesis: Prearticulatory repair processes in
normal and stuttered disfluencies. Journal of Speech
and Hearing Research, 36(3):472.
76
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 820?825, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Christina Reuterski?old Wagner, Ulrika Nettelbladt, Bir-
gitta Sahl?en, and Claes Nilholm. 2000. Conver-
sation versus narration in pre-school children with
language impairment. International Journal of Lan-
guage & Communication Disorders, 35(1):83?93.
Matthew Rispoli, Pamela Hadley, and Janet Holt.
2008. Stalls and revisions: A developmental per-
spective on sentence production. Journal of Speech,
Language, and Hearing Research, 51(4):953?966.
Cheryl M Scott and Jennifer Windsor. 2000. General
language performance measures in spoken and writ-
ten narrative and expository discourse of school-age
children with language learning disabilities. Journal
of Speech, Language & Hearing Research, 43(2).
Eleanor Messing Semel, Elisabeth Hemmersam Wiig,
and Wayne Secord. 2003. Clinical evaluation of
language fundamentals. The Psychological Corpo-
ration, A Harcourt Assessment Company, Toronto,
Canada, fourth edition.
Helen Tager-Flusberg, Sally Rogers, Judith Cooper,
Rebecca Landa, Catherine Lord, Rhea Paul, Ma-
bel Rice, Carol Stoel-Gammon, Amy Wetherby, and
Paul Yoder. 2009. Defining spoken language bench-
marks and selecting measures of expressive lan-
guage development for young children with autism
spectrum disorders. Journal of Speech, Language
and Hearing Research, 52(3):643.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Maximum-margin markov networks. In Neural In-
formation Processing Systems (NIPS).
Elin T Thordardottir and Susan Ellis Weismer. 2001.
Content mazes and filled pauses in narrative lan-
guage samples of children with specific language
impairment. Brain and cognition, 48(2-3):587?592.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007. Narrative in adolescent specific
language impairment (sli): A comparison with peers
across two different narrative genres. International
Journal of Language & Communication Disorders,
42(5):583?605.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics-Volume 2, pages 947?953. Association
for Computational Linguistics.
Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair dis-
fluency detection. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
703?711, Portland, Oregon, USA, June. Association
for Computational Linguistics.
77
