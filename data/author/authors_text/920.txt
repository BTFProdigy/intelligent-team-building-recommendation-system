Formalising Multi-layer Corpora in OWL DL ?
Lexicon Modelling, Querying and Consistency Control
Aljoscha Burchardt1, Sebastian Pad?2?, Dennis Spohr3?, Anette Frank4?and Ulrich Heid3
1Dept. of Comp. Ling. 2Dept. of Linguistics 3Inst. for NLP 4Dept. of Comp. Ling.
Saarland University Stanford University University of Stuttgart University of Heidelberg
Saarbr?cken, Germany Stanford, CA Stuttgart, Germany Heidelberg, Germany
albu@coli.uni-sb.de pado@stanford.edu spohrds,heid@ims.uni-stuttgart.de frank@cl.uni-heidelberg.de
Abstract
We present a general approach to formally
modelling corpora with multi-layered anno-
tation, thereby inducing a lexicon model in a
typed logical representation language, OWL
DL. This model can be interpreted as a graph
structure that offers flexible querying func-
tionality beyond current XML-based query
languages and powerful methods for consis-
tency control. We illustrate our approach by
applying it to the syntactically and semanti-
cally annotated SALSA/TIGER corpus.
1 Introduction
Over the years, much effort has gone into the creation
of large corpora with multiple layers of linguistic an-
notation, such as morphology, syntax, semantics, and
discourse structure. Such corpora offer the possibility
to empirically investigate the interactions between
different levels of linguistic analysis.
Currently, the most common use of such corpora
is the acquisition of statistical models that make use
of the ?more shallow? levels to predict the ?deeper?
levels of annotation (Gildea and Jurafsky, 2002; Milt-
sakaki et al, 2005). While these models fill an im-
portant need for practical applications, they fall short
of the general task of lexicon modelling, i.e., creat-
ing an abstracted and compact representation of the
corpus information that lends itself to ?linguistically
informed? usages such as human interpretation or
integration with other knowledge sources (e.g., deep
grammar resources or ontologies). In practice, this
task faces three major problems:
?At the time of writing, Sebastian Pad? and Dennis Spohr
were affiliated with Saarland University, and Anette Frank with
DFKI Saarbr?cken and Saarland University.
Ensuring consistency. Annotation reliability and
consistency are key prerequisites for the extraction of
generalised linguistic knowledge. However, with the
increasing complexity of annotations for ?deeper? (in
particular, semantic) linguistic analysis, it becomes
more difficult to ensure that all annotation instances
are consistent with the annotation scheme.
Querying multiple layers of linguistic annotation.
A recent survey (Lai and Bird, 2004) found that cur-
rently available XML-based corpus query tools sup-
port queries operating on multiple linguistic levels
only in very restricted ways. Particularly problematic
are intersecting hierarchies, i.e., tree-shaped analyses
on multiple linguistic levels.
Abstractions and application interfaces. A per-
vasive problem in annotation is granularity: The gran-
ularity offered by a given annotation layer may di-
verge considerably from the granularity that is needed
for the integration of corpus-derived data in large
symbolic processing architectures or general lexical
resources. This problem is multiplied when more
than one layer of annotation is considered, for exam-
ple in the characterisation of interface phenomena.
While it may be possible to obtain coarser-grained
representations procedurally by collapsing categories,
such procedures are not flexibly configurable.
Figure 1 illustrates these difficulties with a sentence
from the SALSA/TIGER corpus (Burchardt et al,
2006), a manually annotated German newspaper cor-
pus which contains role-semantic analyses in the
FrameNet paradigm (Fillmore et al, 2003) on top
of syntactic structure (Brants et al, 2002).1 The se-
1While FrameNet was originally developed for English, the
majority of frames has been found to generalise well to other
389
which the official Croatia but in significant international-law difficulties bring would
Figure 1: Multi-layer annotation of a German phrase with syntax and frame semantics (?which would bring
official Croatia into significant difficulties with international law?)
mantic structure consists of frames, semantic classes
assigned to predicating expressions, and the semantic
roles introduced by these classes. The verb bringen
(?to bring?) is used metaphorically and is thus analy-
sed as introducing one frame for the ?literal? reading
(PLACING) and one for the ?understood? reading
(CAUSATION), both with their own role sets.
The high complexity of the semantic structure even
on its own shows the necessity of a device for con-
sistency checking. In conjunction with syntax, it
presents exactly the case of intersecting hierarchies
which is difficult to query. With respect to the issue of
abstraction, note that semantic roles are realised vari-
ously as individual words (was (?which?) ) and con-
stituents (NPs, PPs), a well-known problem in deriv-
ing syntax-semantics mappings from corpora (Frank,
2004; Babko-Malaya et al, 2006).
Our proposal. We propose that the problems in-
troduced above can be addressed by formalising cor-
pora in an integrated, multi-layered corpus and lexi-
con model in a declarative logical framework, more
specifically, the description logics-based OWL DL
formalism. The major benefits of this approach are
that all relevant properties of the annotation and the
underlying model are captured in a uniform represen-
tation and, moreover, that the formal semantics of the
model makes it possible to use general and efficient
knowledge representation techniques for consistency
control. Finally, we can extract specific subsets from
a corpus by defining task-specific views on the graph.
After a short discussion of related approaches in
languages (Burchardt et al, 2006; Boas, 2005).
Section 2, Section 3 provides details on our method-
ology. Sections 4 and 5 demonstrate the benefits of
our strategy on a model of the SALSA/TIGER data.
Section 6 concludes.
2 Related Work
One recent approach to lexical resource modelling
is the Lexical Systems framework (Polgu?re, 2006),
which aims at providing a highly general represen-
tation for arbitrary kinds of lexica. While this is
desirable from a representational point of view, the
resulting models are arguably too generic to support
strong consistency checks on the encoded data.
A further proposal is the currently evolving Lex-
ical Markup Framework (LMF; Francopoulo et
al. (2006)), an ISO standard for lexical resource mod-
elling, and an LMF version of FrameNet exists. How-
ever, we believe that our usage of a typed formalism
takes advantage of a strong logical foundation and
the notions of inheritance and entailment (cf. Schef-
fczyk et al (2006)) and is a crucial step beyond the
representational means provided by LMF.
Finally, the closest neighbour to our proposal is
the ATLAS project (Laprun et al, 2002), which
combines annotations with a descriptive meta-model.
However, to our knowledge, ATLAS only models
basic consistency constraints, and does not capture
dependencies between different layers of annotation.
390
3 Modelling Multilevel Corpora in OWL DL
3.1 A formal graph-based Lexicon
This section demonstrates how OWL DL, a strongly
typed representation language, can serve to transpar-
ently formalise corpora with multi-level annotation.
OWL DL is a logical language that combines the
expressivity of OWL2 with the favourable computa-
tional properties of Description Logics (DL), notably
decidability and monotonicity (Baader et al, 2003).
The strongly typed, well-defined model-theoretic se-
mantics distinguishes OWL DL from recent alterna-
tive approaches to lexicon modelling.
Due to the fact that OWL DL has been defined
in the Resource Description Framework (RDF3), the
first central benefit of using OWL DL is the possibil-
ity to conceive of the lexicon as a graph ? a net-like
entity with a high degree of interaction between lay-
ers of linguistic description, with an associated class
hierarchy. Although OWL DL itself does not have a
graph model but a model-theoretic semantics based
on First Order Logic, we will illustrate our ideas with
reference to a graph-like representation, since this is
what we obtain by transforming our OWL DL files
into an RDFS database.
Each node in the graph instantiates one or more
classes that determine the properties of the node. In
a straightforward sense, properties correspond to la-
belled edges between nodes. They are, however, also
represented as nodes in the graph which instantiate
(meta-)classes themselves.
The model is kept compact by OWL?s support for
multiple instantiation, i.e., the ability of instances
to realise more than one class. For example, in a
syntactically and semantically annotated corpus, all
syntactic units (constituents, words, or even parts
of words) can instantiate ? in addition to a syntac-
tic class ? one or more semantic classes. Multiple
instantiation enables the representation of informa-
tion about several annotation layers within single
instances.
As we have argued in Section 2, we believe that
having one generic model that can represent all cor-
pora is problematic. Instead, we propose to construct
lexicon models for specific types of corpora. The
2http://www.w3.org/2004/OWL/
3http://www.w3.org/RDF/
design of such models faces two central design ques-
tions: (a) Which properties of the annotated instances
should be represented?; (b) How are different types
of these annotation properties modelled in the graph?
Implicit features in annotations. Linguistic anno-
tation guidelines often concentrate on specifying the
linguistic data categories to be annotated. However,
a lot of linguistically relevant information often re-
mains implicit in the annotation scheme. Examples
from the SALSA corpus include, e.g., the fact that
the annotation in Figure 1 is metaphorical. This in-
formation has to be inferred from the configuration
that one predicate evokes two frames. As such infor-
mation about different annotation types is useful in
final lexicon resources, e.g. to define clean generali-
sations over the data (singling out ?special cases?), to
extract information about special data categories, and
to define formally grounded consistency constraints,
we include it in the lexicon model.
Form of representation. All relevant information
has to be represented either as assertional statements
in the model graph (i.e., nodes connected by edges),
or as definitional axioms in the class hierarchy.4
This decision involves a fundamental trade-off be-
tween expressivity and flexibility. Modelling features
as axioms in the class hierarchy imposes definitional
constraints on all instances of these classes and is
arguably more attractive from a cognitive perspec-
tive. However, modelling features as entities in the
graph leads to a smaller class hierarchy, increased
querying flexibility, and more robustness in the face
of variation and noise in the data.
3.2 Modelling SALSA/TIGER Data
We now illustrate these decisions concretely by de-
signing a model for a corpus with syntactic and
frame-semantic annotation, more concretely the
SALSA/TIGER corpus. However, the general points
we make are valid beyond this particular setting.
As concerns implicit annotation features, we have
designed a hierarchy of annotation types which now
explicitly expresses different classes of annotation
phenomena and which allows for the definition of
annotation class-specific properties. For example,
frame targets are marked as a multi-word target if
4This choice corresponds to the DL distinction between TBox
(?intensional knowledge?) and ABox (?extensional knowledge?).
391
L
in
gu
is
ti
c
m
od
el
? Frames
w Intentionally_affect
w Placing
w Motion, . . .
? Roles
w Intentionally_affect.Act
w Placing.Means
? TIGER edge labels and POS
w SB, OA, PPER, ADJA, . . .
? Generalised functions and categories
w subj, obj, NounP, AdjP, . . .
A
nn
ot
at
io
n
ty
pe
s
? Frame Annotations
w Simple
w Metaphoric
w Underspecified
? Role Annotations
w Simple
w Underspecified
? Target Annotations
w Single-word targets
w Multi-word targets
? Sentences, syntactic units, . . .
Figure 2: Schema of the OWL DL model?s class hierarchy (?TBox?)
their span contains at least two terminal nodes. The
hierarchy is shown on the right of Figure 2, which
shows parts of the bipartite class hierarchy.
The left-hand side of Figure 2 illustrates the lin-
guistic model, in which frames and roles are organ-
ised according to FrameNet?s inheritance relation.
Although this design seems to be straightforward, it
is the result of careful considerations concerning the
second design decision. Since FrameNet is a hierar-
chically structured resource with built-in inheritance
relations, one important question is whether to model
individual frames, such as SELF_MOTION or LEAD-
ERSHIP, and their relations either as instances of a
general class Frame and as links between these in-
stances, or as hierarchically structured classes with
richer axiomatisation. In line with our focus on con-
sistency checking, we adopt the latter option, which
allows us to use built-in reasoning mechanisms of
OWL DL to ensure consistency.
Annotation instances from the corpus instantiate
multiple classes in both hierarchies (cf. Figure 2): On
the annotation side according to their types of phe-
nomena; on the linguistic side based on their frames,
roles, syntactic functions, and categories.
Flexible abstraction. Section 1 introduced granu-
larity as a pervasive problem in the use of multi-level
corpora. Figure 2 indicates that the class hierarchy
of the OWL DL model offers a very elegant way
of defining generalised data categories that provide
abstractions over model classes, both for linguistic
categories and annotation types. Moreover, proper-
ties can be added to each abstracting class and then
be used, e.g., for consistency checking. In our case,
Figure 2 shows (functional) edge labels and part-of-
speech tags provided by TIGER, as well as sets of
(largely theory-neutral) grammatical functions and
categories that subsume these fine-grained categories
and support the extraction of generalised valence in-
formation from the lexicon.
An annotated corpus sentence. To substantiate
the above discussion, Figure 3 shows a partial lexicon
representation of the example in Figure 1. The boxes
represent instance nodes, with classes listed above
the horizontal line, and datatype properties below
it.5 The links between these instances indicate OWL
object properties which have been defined for the
instantiated classes. For example, the metaphorical
PLACING frame is shown as a grey box in the middle.
Multiple inheritance is indicated by instances
carrying more than one class, such as the in-
stance in the left centre, which instantiates the
classes SyntacticUnit, NP, OA, NounP and
obj. Multi-class instances inherit the properties
of each of these classes, so that e.g., the meta-
phoric frame annotation of the PLACING frame
in the middle has both the properties defined for
frames (hasCoreRole) and for frame annotations
(hasTarget). The generalised syntactic categories
discussed above are given in italics (e.g., NounP).
The figure highlights the model?s graph-based
structure with a high degree of interrelation between
the lexicon entities. For example, the grey PLAC-
ING frame instance is directly related to its roles
(left, bottom), its lexical anchor (right), the surround-
ing sentence (top), and a flag (top left) indicating
metaphorical use.
5For the sake of simplicity, we excluded explicit ?is-a? links.
392
MetaphoricFrameAnnotationUspFrameAnnotationCausation
SyntacticUnitPRELSSB
hasTigerIDhasContent
NounPsubj
SyntacticUnitNENKhasTigerIDhasContent s2910_17"Kroatien"
Source
SyntacticUnitNPOA
hasTigerIDhasContent
SimpleRoleAnnotationPlacing.ThemehasContent
UspFrameAnnotationSupport LemmahasLemma
LexicalUnitrdf:ID bringen.Placing
SingleWordTargethasContent
SyntacticUnitVVINFHDhasTigerIDhasContent
SyntacticUnitNNNKhasTigerIDhasContent
NounPobj
s2910_15
"das"
s2910_502
"das offizielle Kroatien"
s2910_14
"was"SyntacticUnitARTNKhasTigerIDhasContent
"bringen"
"bringen"
SimpleRoleAnnotationPlacing.CausehasContent "was"
SentenceAnnotationhasSentenceIDhasContent s2910"Die Ausrufung des ..."
MetaphoricFrameAnnotationPlacing
"das offizielle Kroatien"
SimpleRoleAnnotationPlacing.GoalhasContent "in betr?chtliche v?lker..."
"bringen"s2910_23
"Schwierigkeiten"s2910_22
consistsOf
isAssignedTo hasFlag
hasFrameAnnotation hasFrameAnnotation
isUspWith
hasFrameAnnotation
hasCoreRole
hasCoreRoleisAssignedTo
hasCoreRole
hasTargetisTargetOf isAssignedTo hasHead
hasAnnotation?Instance
hasReadingisReadingOf
isAnnotationInstanceOf
isAssignedToSyntacticUnitADJANKhasTigerIDhasContent s2910_16"offizielle"
consistsOf consistsOf
...
Figure 3: Partial lexicon representation of an annotated corpus sentence
4 Querying the Model
We now address the second desideratum introduced
in Section 1, namely a flexible and powerful query
mechanism. For OWL DL models, such a mecha-
nism is available in the form of the Sesame (Broekstra
et al, 2002) SeRQL query language. Since SeRQL
makes it possible to extract and view arbitrary sub-
graphs of the model, querying of intersective hierar-
chies is possible in an intuitive manner.
An interesting application for this querying mecha-
nism is to extract genuine lexicon views on the corpus
annotations, e.g., to extract syntax-semantics map-
ping information for particular senses of lemmas, by
correlating role assignments with deep syntactic in-
formation. These can serve both for inspection and
for interfacing the annotation data with deep gram-
matical resources or general lexica. Applied to our
complete corpus, this ?lexicon? contains on average
8.5 role sets per lemma, and 5.6 role sets per frame.
The result of such a query is illustrated in Table 1 for
the lemma senken (?to lower?).
From such view, frame- or lemma-specific role
sets, i.e., patterns of role-category-function assign-
ments can easily be retrieved. A typical example is
given in Table 2, with additional frequency counts.
The first row indicates that the AGENT role has been
realised as a (deep) subject noun phrase and the ITEM
as (deep) object noun phrase.
We found that generalisations over corpus cate-
gories encoded in the class hierarchies are central
Role Cat Func Freq
Item NounP obj 26
Agent NounP subj 15
Difference PrepP mod-um 6
Cause NounP subj 4
Value_2 PrepP mod-auf 3
Value_2 PrepP pobj-auf 2
Value_1 PrepP mod-von 1
Table 1: Role-category-function assignments for
senken / CAUSE_CHANGE_OF_SCALAR_POSITION (CCSP)
Role set for senken / CCSP Freq
Agent Item 11
subj obj
NounP NounP
Cause Item 4
subj obj
NounP NounP
Item 4
obj
NounP
Agent Item Difference 2
subj obj mod-um
NounP NounP PrepP
Table 2: Sample of role sets for senken / CCSP
to the usefulness of the resulting patterns. For ex-
ample, the number of unique mappings between se-
mantic roles and syntactic categories in our corpus
is 5,065 for specific corpus categories, and 2,289 for
abstracted categories. Thus, the definition of an ab-
straction layer, in conjunction with a flexible query
mechanism, allows us to induce lexical characterisa-
tions of the syntax-semantics mapping ? aggregated
393
and generalised from disparate corpus annotations.
Incremental refinements. Querying, and the re-
sulting lexical views, can serve yet another purpose:
Such aggregates make it possible to conduct a data-
driven search for linguistic generalisations which
might not be obvious from a theoretical perspective,
and allow quick inspection of the data for counterex-
amples to plausible regularities.
In the case of semantic roles, for example, such
a regularity would be that semantic roles are not
assigned to conflicting grammatical functions (e.g.,
deep subject and object) within a given lemma. How-
ever, some of the role sets we extracted contained
exactly such configurations. Further inspection re-
vealed that these irregularities resulted from either
noise introduced by errors in the automatic assign-
ment of grammatical functions, or instances with
syntactically non-local role assignments.
Starting from such observations, our approach sup-
ported a semi-automatic, incremental refinement of
the linguistic and annotation models, in this case in-
troducing a distinction between local and non-local
role realisations.
Size of the lexicon. Using a series of SeRQL
queries, we have computed the size of the cor-
pus/lexicon model for the SALSA/TIGER data (see
Table 3). The lexicon model architecture as described
in Section 3 results in a total of more than 304,000
instances in the lexicon, instantiating 581 different
frame classes and 1,494 role classes.
5 Consistency Control
The first problem pointed out in Section 1 was the
need for efficient consistency control mechanisms.
Our OWL DL-based model in fact offers two mech-
anisms for consistency checking: axiom-based and
query-based checking.
Axiom-based checking. Once some constraint has
been determined to be universally applicable, it can
be formulated in Description Logics in the form of
axiomatic expressions on the respective class in the
model. Although the general interpretation of these
axioms in DL is that they allow for inference of new
statements, they can still be used as a kind of well-
formedness ?constraint?. For example, if an individ-
ual is asserted as an instance of a particular class, the
Type No. of instances
Lemmas 523
Lemma-frame pairs (LUs) 1,176
Sentences 13,353
Syntactic units 223,302
Single-word targets 16,268
Multi-word targets 258
Frame annotations 16,526
Simple 14,700
Underspecified 995
Metaphoric 785
Elliptic 107
Role annotations 31,704
Simple 31,112
Underspecified 592
Table 3: Instance count based on the first SALSA
release
reasoner will detect an inconsistency if this instance
does not adhere to the axiomatic class definition. For
semantic role annotations, axioms can e.g. define the
admissible relations between a particular frame and
its roles. This is illustrated in the DL statements be-
low, which express that an instance of PLACING may
at most have the roles GOAL, PATH, etc.
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Relations between roles can be formalised in a
similar way. An example is the excludes relation in
FrameNet, which prohibits the co-occurrence of roles
like CAUSE and AGENT of the PLACING frame. This
can be expressed by the following statement.
Placing v ?((?.hasRole Placing.Cause)u
(?.hasRole Placing.Agent))
The restrictions are used in checking the consistency
of the semantic annotation; violations of these con-
straints lead to inconsistencies that can be identified
by theorem provers. Although current state-of-the-art
reasoners do not yet scale to the size of entire cor-
pora, axiom-based checking still works well for our
data due to SALSA?s policy of dividing the original
TIGER corpus into separate subcorpora, each deal-
ing with one particular lemma (cf. Scheffczyk et al
(2006)).
394
Query-based checking. Due to the nature of our
graph representation, constraints can combine dif-
ferent types of information to control adherence to
annotation guidelines. Examples are the assignment
of the SUPPORTED role of support verb constructions,
which ought to be assigned to the maximal syntactic
constituent projected by the supported noun, or the
exclusion of reflexive pronouns from the span of the
target verb. However, the consistency of multi-level
annotation is often difficult to check: Not only are
some types of classification (e.g. assignment of se-
mantic classes) inherently difficult; the annotations
also need to be considered in context. For such cases,
axiom-based checking is too strict. In practice, it is
important that manual effort can be reduced by auto-
matically extracting subsets of ?suspicious? data for
inspection. This can be done using SeRQL queries
which ? in contrast to the general remarks on the
scalability of reasoners ? are processed and evaluated
very quickly on the entire annotated corpus data.
Example queries that we formulated examine sus-
picious configurations of annotation types, such as
target words evoking two or more frame annota-
tions which are neither marked as underspecified nor
tagged as a pair of (non-)literal metaphorical frame
annotations. Here, we identified 8 cases of omitted
annotation markup, namely 4 missing metaphor flags
and 4 omitted underspecification links.
On the semantic level, we extracted annotation
instances (in context) for metaphorical vs. non-
metaphorical readings, or frames that are involved
in underspecification in certain sentences, but not in
others. While the result sets thus obtained still re-
quire manual inspection, they clearly illustrate how
the detection of inconsistencies can be enhanced by
a declarative formalisation of the annotation scheme.
Another strategy could be to concentrate on frames
or lemmas exhibiting proportionally high variation
in annotation (Dickinson and Meurers, 2003).
6 Conclusion
In this paper, we have constructed a Description
Logics-based lexicon model directly from multi-layer
linguistic corpus annotations. We have shown how
such a model allows for explicit data modelling, and
for flexible and fine-grained definition of various de-
grees of abstractions over corpus annotations.
Furthermore, we have demonstrated that a pow-
erful logical formalisation which integrates an un-
derlying annotation scheme can be used to directly
control consistency of the annotations using general
KR techniques. It can also overcome limitations
of current XML-based search tools by supporting
queries which are able to connect multiple levels of
linguistic analysis. These queries can be used vari-
ously as an additional means of consistency control,
to derive quantitative tendencies from the data, to
extract lexicon views tailored to specific purposes,
and finally as a general tool for linguistic research.
Acknowledgements
This work has been partly funded by the German
Research Foundation DFG (grant PI 154/9-2). We
also thank the two anonymous reviewers for their
valuable comments and suggestions.
References
Franz Baader, Diego Calvanese, Deborah L. McGuinness,
Daniele Nardi, and Peter F. Patel-Schneider. 2003.
The Description Logic Handbook: Theory, Implemen-
tation and Applications. CUP.
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in Synchronizing the English
Treebank and PropBank. In Proceedings of the COL-
ING/ACL Workshop on Frontiers in Linguistically An-
notated Corpora, Sydney.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases. In-
ternational Journal of Lexicography, 18(4):445?478.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Jeen Broekstra, Arjohn Kampman, and Frank van Herme-
len. 2002. Sesame: A generic architecture for storing
and querying RDF and RDF Schema. In Proceedings
of the 1st ISWC, Sardinia.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th LREC,
Genoa.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th EACL, Budapest.
395
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Gil Francopoulo, Monte George, Nicoletta Calzolari,
Monica Monachini, Nuria Bel, Mandy Pet, and Clau-
dia Soria. 2006. LMF for multilingual, specialized
lexicons. In Proceedings of the 5th LREC, Genoa.
Anette Frank. 2004. Generalisations over corpus-
induced frame assignment rules. In Proceedings of the
LREC Workshop on Building Lexical Resources From
Semantically Annotated Corpora, Lisbon.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Catherine Lai and Steven Bird. 2004. Querying and up-
dating treebanks: A critical survey and requirements
analysis. In Proceedings of the Australasian Language
Technology Workshop, Sydney.
Christophe Laprun, Jonathan Fiscus, John Garofolo, and
Sylvain Pajot. 2002. Recent Improvements to the AT-
LAS Architecture. In Proceedings of HLT 2002, San
Diego.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Exper-
iments on sense annotations and sense disambigua-
tion of discourse connectives. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, Barcelona, Spain.
Alain Polgu?re. 2006. Structural properties of lexi-
cal systems: Monolingual and multilingual perspec-
tives. In Proceedings of the COLING/ACL Workshop
on Multilingual Language Resources and Interoper-
ability, Sydney.
Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.
2006. Ontology-based reasoning about lexical re-
sources. In Proceedings of the 5th OntoLex, Genoa.
396
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 10?15,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Semantic Approach To Textual Entailment:
System Evaluation and Task Analysis
Aljoscha Burchardt, Nils Reiter, Stefan Thater
Dept. of Computational Linguistics
Saarland University
Saarbr?cken, Germany
 
		Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65?70,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluate with Confidence Estimation: Machine ranking of translation
outputs using grammatical features
Eleftherios Avramidis, Maja Popovic, David Vilar, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
We present a pilot study on an evaluation
method which is able to rank translation out-
puts with no reference translation, given only
their source sentence. The system employs a
statistical classifier trained upon existing hu-
man rankings, using several features derived
from analysis of both the source and the tar-
get sentences. Development experiments on
one language pair showed that the method
has considerably good correlation with human
ranking when using features obtained from a
PCFG parser.
1 Introduction
Automatic evaluation metrics for Machine Transla-
tion (MT) have mainly relied on analyzing both the
MT output against (one or more) reference transla-
tions. Though, several paradigms in Machine Trans-
lation Research pose the need to estimate the quality
through many translation outputs, when no reference
translation is given (n-best rescoring of SMT sys-
tems, system combination etc.). Such metrics have
been known as Confidence Estimation metrics and
quite a few projects have suggested solutions on this
direction. With our submission to the Shared Task,
we allow such a metric to be systematically com-
pared with the state-of-the-art reference-aware MT
metrics.
Our approach suggests building a Confidence Es-
timation metric using already existing human judg-
ments. This has been motivated by the existence
of human-annotated data containing comparisons of
the outputs of several systems, as a result of the
evaluation tasks run by the Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al,
2008; Callison-Burch et al, 2009; Callison-Burch
et al, 2010). This amount of data, which has been
freely available for further research, gives an op-
portunity for applying machine learning techniques
to model the human annotators? choices. Machine
Learning methods over previously released evalua-
tion data have been already used for tuning com-
plex statistical evaluation metrics (e.g. SVM-Rank
in Callison-Burch et al (2010)). Our proposition
is similar, but works without reference translations.
We develop a solution of applying machine learning
in order to build a statistical classifier that performs
similar to the human ranking: it is trained to rank
several MT outputs, given analysis of possible qual-
itative criteria on both the source and the target side
of every given sentence. As qualitative criteria, we
use statistical features indicating the quality and the
grammaticality of the output.
2 Automatic ranking method
2.1 From Confidence Estimation to ranking
Confidence estimation has been seen from the Nat-
ural Language Processing (NLP) perspective as a
problem of binary classification in order to assess
the correctness of a NLP system output. Previ-
ous work focusing on Machine Translation includes
statistical methods for estimating correctness scores
or correctness probabilities, following a rich search
over the spectrum of possible features (Blatz et al,
2004a; Ueffing and Ney, 2005; Specia et al, 2009;
Raybaud and Caroline Lavecchia, 2009; Rosti et al,
65
2007).
In this work we slightly transform the binary clas-
sification practice to fit the standard WMT human
evaluation process. As human annotators have pro-
vided their evaluation in the form of ranking of five
system outputs at a sentence level, we build our eval-
uation mechanism with similar functionality, aim-
ing to training from and evaluating against this data.
Evaluation scores and results can be then calculated
based on comparative analysis of the performance of
each system.
Whereas latest work, such as Specia et al (2010),
has focused on learning to assess segment perfor-
mance independently for each system output, our
contribution measures the performance by compar-
ing the system outputs with each other and con-
sequently ranking them. The exact method is de-
scribed below.
2.2 Internal pairwise decomposition
We build one classifier over all input sentences.
While the evaluation mechanism is trained and eval-
uated on a multi-class (ranking) basis as explained
above, the classifier is expected to work on a binary
level: we provide the features from the analysis of
the two system outputs and the source, and the clas-
sifier should decide if the first system output is better
than the second one or not.
In order to accomplish such training, the n sys-
tems? outputs for each sentence are broken down to
n ? (n ? 1) pairs, of all possible comparisons be-
tween two system outputs, in both directions (sim-
ilar to the calculation of the Spearman correlation).
For each pair, the classifier is trained with a class
value c, for the pairwise comparison of system out-
puts ti and tj with respective ranks ri and rj , deter-
mined as:
c(ri, rj) =
{
1 ri < rj
?1 ri > rj
At testing time, after the classifier has made all
the pairwise decisions, those need to be converted
back to ranks. System entries are ordered, according
to how many times each of them won in the pair-
wise comparison, leading to rank lists similar to the
ones provided by human annotators. Note that this
kind of decomposition allows for ties when there are
equal times of winnings.
2.3 Acquiring features
In order to obtain features indicating the quality of
the MT output, automatic NLP analysis tools are ap-
plied on both the source and the two target (MT-
generated) sentences of every pairwise comparison.
Features considered can be seen in the following cat-
egories, according to their origin:
? Sentence length: Number of words of source
and target sentences, source-length to target-
length ratio.
? Target language model: Language models
provide statistics concerning the correctness of
the words? sequence on the target language.
Such language model features include:
? the smoothed n-gram probability of the
entire target sentence for a language
model of order 5, along with
? uni-gram, bi-gram, tri-gram probabilities
and a
? count of unknown words
? Parsing: Processing features acquired from
PCFG parsing (Petrov et al, 2006) for both
source and target side include:
? parse log likelihood,
? number of n-best trees,
? confidence for the best parse,
? average confidence of all trees.
Ratios of the above target features to their re-
spective source features were included.
? Shallow grammatical match: The number of
occurences of particular node tags on both the
source and the target was counted on the PCFG
parses. In particular, NPs, VPs, PPs, NNs and
punctuation occurences were counted. Then
the ratio of the occurences of each tag in the
target sentence by its occurences on the source
sentence was also calculated.
2.4 Classifiers
The machine learning core of the system was built
supporting two classification approaches.
66
? Na?ve Bayes allows prediction of a binary
class, given the assumption that the features are
statistically independent.
p(C,F1, . . . , Fn) = p(C)
i=1?
n
p(Fi|C)
p(C) is estimated by relative frequencies of
the training pairwise examples, while p(Fi|C)
for our continuous features are estimated with
LOESS (locally weighted linear regression
similar to Cleveland (1979))
? k-nearest neighbour (knn) algorithm allows
classifying based on the closest training exam-
ples in the feature space.
3 Experiment
3.1 Experiment setup
A basic experiment was designed in order to deter-
mine the exact setup and the feature set of the metric
prior to the shared task submission. The classifiers
for the task were learnt using the German-English
testset of the WMT 2008 and 2010 (about 700 sen-
tences)1. For testing, the classifiers were used to per-
form ranking on a test set of 184 sentences which
had been kept apart from the 2010 data, with the cri-
terion that they do not contain contradictions among
human judgments.
In order to allow further comparison with other
evaluation metrics, we performed an extended ex-
periment: we trained the classifiers over the WMT
2008 and 2009 data and let them perform automatic
ranking on the full WMT 2010 test set, this time
without any restriction on human evaluation agree-
ment.
In both experiments, tokenization was performed
with the PUNKT tokenizer (Kiss et al, 2006; Gar-
rette and Klein, 2009), while n-gram features were
generated with the SRILM toolkit (Stolcke, 2002).
The language model was relatively big and had been
built upon all lowercased monolingual training sets
for the WMT 2011 Shared Task, interpolated on
the 2007 test set. As a PCFG parser, the Berkeley
Parser (Petrov and Klein, 2007) was preferred, due
1data acquired from http://www.statmt.org/wmt11
to the possibility of easily obtaining complex inter-
nal statistics, including n-best trees. Unfortunately,
the time required for parsing leads to significant de-
lays at the overall processing. The machine learn-
ing algorithms were implemented with the Orange
toolkit (Dem?ar et al, 2004).
3.2 Feature selection
Although the automatic NLP tools provided a lot of
features (section 2.3), the classification methods we
used (and particularly na?ve Bayes were the develop-
ment was focused on) would be expected to perform
better given a smaller group of statistically inde-
pendent features. Since exhaustive training/testing
of all possible feature subsets was not possible,
we performed feature selection based on the Reli-
eff method (Kononenko, 1994; Kira and Rendell,
1992). Automatic ranking was performed based on
the most promising feature subsets. The results are
examined below.
3.3 Results
The performance of the classifier is measured after
the classifier output has been converted back to rank
lists, similar to the WMT 2010 evaluation. We there-
fore calculated two types of rank coefficients: aver-
aged Kendall?s tau on a segment level, and Spear-
man?s rho on a system level, based on the percentage
that the each system?s translations performed better
than or equal to the translations of any other system.
The results for the various combinations of fea-
tures and classifiers are depicted on Table 1. Na?ve
Bayes provides the best score on the test set, with
? = 0.81 on a system level and ? = 0.26 on a
segment level, trained with features including the
number of the unknown words, the source-length
by target-length ratio, the VP count ratio and the
source-target ratio of the parsing log-likelihood. The
number of unknown words particularly appears to be
a strong indicator for the quality of the sentence. On
the first part of the table we can also observe that
language model features do not perform as well as
the features deriving from the processing informa-
tion delivered by the parser. On the second part of
the table we compare the use of various grammatical
combinations. The third part contains the correlation
obtained by various similar internal parsing-related
features.
67
features na?ve Bayes knn
rho tau rho tau
basic experiment
ngram 0.19 0.05 0.13 0.01
unk, len 0.67 0.20 0.73 0.24
unk, len, bigram 0.61 0.21 0.74 0.21
unk, len, ngram 0.63 0.19 0.59 0.21
unk, len, trigram 0.67 0.20 0.76 0.21
unk, len, logparse 0.75 0.21 0.74 0.25
unk, len, nparse, VP 0.67 0.24 0.61 0.20
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, NP, confbestparse 0.78 0.23 0.74 0.23
unk, len, nparse, VP, confavg 0.75 0.21 0.78 0.23
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, VP, logparse 0.81 0.26 0.75 0.23
extended experiment
unk, len, nparse, VP, logparse 0.60 0.23 0.28 0.02
Table 1: System-level Spearman?s rho and segment-level Kendall?s tau correlation coefficients achieved on automatic
ranking (average absolute value)
The correlation coefficients of the extended exper-
iment, allowing comparison with last year?s shared
task, are shown on the last line of the table. With
coefficients ? = 0.60 and ? = 0.23, our metric
performs relatively low compared to the other met-
rics of WMT10 (indicatively iBLEU: ? = 0.95,
? = 0.39 according to Callison-Burch et al (2010).
Though, it still has a position in the list, scoring bet-
ter than several other reference-aware metrics (e.g.
of ? = 0.47 and ? = 0.12 respectively) for the par-
ticular language pair.
4 Discussion
A concern on the use of Confidence Estimation for
MT evaluation has to do with the possibility of a
system ?tricking? such metrics. This would for ex-
ample be the case when a system offers a well-
formed candidate translation and gets a good score,
despite having no relation to the source sentence
in terms of meaning. We should note that we are
not capable of fully investigating this case based
on the current set of experiments, because all of
the systems in our data sets have shown acceptable
scores (11-25 BLEU and 0.58-0.78 TERp accord-
ing to Callison-Burch et al (2010)), when evaluated
against reference translations. Though, we would
assume that we partially address this problem by us-
ing ratios of source to target features (length, syn-
tactic constituents), which means that in order for a
sentence to trick the metric, it would need a com-
parable sentence length and a grammatical structure
that would allow it to achieve feature ratios similar
to the other systems? outputs. Previous work (Blatz
et al, 2004b; Ueffing and Ney, 2005) has used fea-
tures based on word alignment, such as IBM Mod-
els, which would be a meaningful addition from this
aspect.
Although k-nearest-neighbour is considered to be
a superior classifier, best results are obtained by
na?ve Bayes. This may have been due of the fact
that feature selection has led to small sets of uncor-
related features, where na?ve Bayes is known to per-
form well. K-nearest-neighbour and other complex
classification methods are expected to prove useful
when more complex feature sets are employed.
5 Conclusion and Further work
The experiments presented in this article indicate
that confidence metrics trained over human rankings
can be possibly used for several tasks of evaluation,
given particular conditions, where e.g. there is no
reference translation given. Features obtained from
68
a PCFG parser seem to be leading to better correla-
tions, given our basic test set. Although correlation
is not particularly high, compared to other reference-
aware metrics in WMT 10, there is clearly a poten-
tial for further improvement.
Nevertheless this is still a small-scale experiment,
given the restricted data size and the single transla-
tion direction. The performance of the system on
broader training and test sets will be evaluated in the
future. Feature selection is also subject to change
if other language pairs are introduced, while more
sophisticated machine learning algorithms, allowing
richer feature sets, may also lead to better results.
Acknowledgments
This work was done with the support of the
TaraXU? Project2, financed by TSB Technologie-
stiftung Berlin?Zukunftsfonds Berlin, co-financed
by the European Union?European fund for regional
development.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004a. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004b. Confidence estimation for
machine translation. In M. Rollins (Ed.), Mental Im-
agery. Yale University Press.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2http://taraxu.dfki.de
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American statistical association, 74(368):829?836.
Janez Dem?ar, Blaz Zupan, Gregor Leban, and Tomaz
Curk. 2004. Orange: From experimental machine
learning to interactive data mining. In Principles of
Data Mining and Knowledge Discovery, pages 537?
539.
Dan Garrette and Ewan Klein. 2009. An extensi-
ble toolkit for computational semantics. In Proceed-
ings of the Eighth International Conference on Com-
putational Semantics, IWCS-8 ?09, pages 116?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kenji Kira and Larry A. Rendell. 1992. The feature se-
lection problem: traditional methods and a new algo-
rithm. In Proceedings of the tenth national conference
on Artificial intelligence, AAAI?92, pages 129?134.
AAAI Press.
Tibor Kiss, Jan Strunk, Ruhr universit?t Bochum, and
Ruhr universit?t Bochum. 2006. Unsupervised mul-
tilingual sentence boundary detection. In Proceedings
of IICS-04, Guadalajara, Mexico and Springer LNCS
3473.
Igor Kononenko. 1994. Estimating attributes: analy-
sis and extensions of relief. In Proceedings of the
European conference on machine learning on Ma-
chine Learning, pages 171?182, Secaucus, NJ, USA.
Springer-Verlag New York, Inc.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In In HLT-NAACL ?07.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In In ACL ?06, pages 433?
440.
Sylvain Raybaud and Kamel Smaili Caroline Lavecchia,
David Langlois. 2009. Word-and sentence-level con-
fidence measures for machine translation. In Euro-
pean Association of Machine Translation 2009.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of the North
American Chapter of the Association for Compu-
tational Linguistics Human Language Technologies,
pages 228?235.
69
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009. Improv-
ing the confidence of machine translation quality es-
timates. In Machine Translation Summit XII, Ottawa,
Canada.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24:39?50, March.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP 2002, pages 901?904.
Nicola Ueffing and Hermann Ney. 2005. Word-level
confidence estimation for machine translation using
phrase-based translation models. Computational Lin-
guistics, pages 763?770.
70
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99?103,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluation without references:
IBM1 scores as evaluation metrics
Maja Popovic?, David Vilar, Eleftherios Avramidis, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
Current metrics for evaluating machine trans-
lation quality have the huge drawback that
they require human-quality reference transla-
tions. We propose a truly automatic evalua-
tion metric based on IBM1 lexicon probabili-
ties which does not need any reference transla-
tions. Several variants of IBM1 scores are sys-
tematically explored in order to find the most
promising directions. Correlations between
the new metrics and human judgments are cal-
culated on the data of the third, fourth and fifth
shared tasks of the Statistical Machine Trans-
lation Workshop. Five different European lan-
guages are taken into account: English, Span-
ish, French, German and Czech. The results
show that the IBM1 scores are competitive
with the classic evaluation metrics, the most
promising being IBM1 scores calculated on
morphemes and POS-4grams.
1 Introduction
Currently used evaluation metrics such as BLEU (Pa-
pineni et al, 2002), METEOR (Banerjee and Lavie,
2005), etc. are based on the comparison between
human reference translations and the automatically
generated hypotheses in the target language to be
evaluated. While this scenario helps in the design
of machine translation systems, it has two major
drawbacks. The first one is the practical criticism
that using reference translations is inefficient and ex-
pensive: in real-life situations, the quality of ma-
chine translation must be evaluated without having
to pay humans for producing reference translations
first. The second criticism is methodological: in
using reference translation, the problem of evalu-
ating translation quality (e.g., completeness, order-
ing, domain fit, etc.) is transformed into a kind of
paraphrase evaluation in the target language, which
is a very difficult problem itself. In addition, the
set of selected references always represents only a
small subset of all good translations. To remedy
these drawbacks, we propose a truly automatic eval-
uation metric which is based on the IBM1 lexicon
scores (Brown et al, 1993).
The inclusion of IBM1 scores in translation sys-
tems has shown experimentally to improve transla-
tion quality (Och et al, 2003). They also have been
used for confidence estimation for machine transla-
tion (Blatz et al, 2003). To the best of our knowl-
edge, these scores have not yet been used as an eval-
uation metric.
We carry out a systematic comparison between
several variants of IBM1 scores. The Spearman?s
rank correlation coefficients on the document (sys-
tem) level between the IBM1 metrics and the hu-
man ranking are computed on the English, French,
Spanish, German and Czech texts generated by var-
ious translation systems in the framework of the
third (Callison-Burch et al, 2008), fourth (Callison-
Burch et al, 2009) and fifth (Callison-Burch et al,
2010) shared translation tasks.
2 IBM1 scores
The IBM1 model is a bag-of-word translation model
which gives the sum of all possible alignment proba-
bilities between the words in the source sentence and
the words in the target sentence. Brown et al (1993)
defined the IBM1 probability score for a translation
99
pair fJ1 and eI1 in the following way:
P (fJ1 |eI1) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(fj |ei) (1)
where fJ1 is the source language sentence of length
J and eI1 is the target language sentence of length I .
As it is a conditional probability distribution, we
investigated both directions as evaluation metrics. In
order to avoid frequent confusions about what is the
source and what the target language, we defined our
scores in the following way:
? source-to-hypothesis (sh) IBM1 score:
IBM1sh =
1
(H + 1)S
S
?
j=1
H
?
i=0
p(sj|hi) (2)
? hypothesis-to-source (hs) IBM1 score:
IBM1hs =
1
(S + 1)H
H
?
i=1
S
?
j=0
p(hi|sj) (3)
where sj are the words of the original source lan-
guage sentence, S is the length of this sentence, hi
are the words of the target language hypothesis, and
H is the length of this hypothesis.
In addition to the standard IBM1 scores calculated
on words, we also investigated:
? MIBM1 scores ? IBM1 scores of word mor-
phemes in each direction;
? PnIBM1 scores ? IBM1 scores of POS n-grams
in each direction.
A parallel bilingual corpus for the desired lan-
guage pair and a tool for training the IBM1 model
are required in order to obtain IBM1 probabilities
p(fj|ei). For the POS n-gram scores, appropriate
POS taggers for each of the languages are necessary.
The POS tags cannot be only basic but must have
all details (e.g. verb tenses, cases, number, gender,
etc.). For the morpheme scores, a tool for splitting
words into morphemes is necessary.
3 Experiments on WMT 2008, WMT 2009
and WMT 2010 test data
3.1 Experimental set-up
The IBM1 probabilities necessary for the IBM1
scores are learnt using the WMT 2010 News
Commentary bilingual corpora consisting of the
Spanish-English, French-English, German-English
and Czech-English parallel texts. Spanish, French,
German and English POS tags were produced using
the TreeTagger1, and the Czech texts are tagged us-
ing the COMPOST tagger (Spoustova? et al, 2009).
The morphemes for all languages are obtained us-
ing the Morfessor tool (Creutz and Lagus, 2005).
The tool is corpus-based and language-independent:
it takes a text as input and produces a segmenta-
tion of the word forms observed in the text. The
obtained results are not strictly linguistic, however
they often resemble a linguistic morpheme segmen-
tation. Once a morpheme segmentation has been
learnt from some text, it can be used for segment-
ing new texts. In our experiments, the splitting are
learnt from the training corpus used for the IBM1
lexicon probabilities. The obtained segmentation is
then used for splitting the corresponding source texts
and hypotheses. Detailed corpus statistics are shown
in Table 1.
Using the obtained IBM1 probabilities of words,
morphemes and POS n-grams, the scores de-
scribed in Section 2 are calculated for the
Spanish-English, French-English, German-English
and Czech-English translation outputs from each
translation direction. For each of the IBM1 scores,
the system level Spearman correlation coefficients ?
with the human ranking are calculated for each doc-
ument. In total, 32 correlation coefficients are ob-
tained for each score ? four English outputs from
the WMT 2010 task, four from the WMT 2009 and
eight from the WMT 2008 task, together with six-
teen outputs in other four target languages. The ob-
tained correlation results were then summarised into
the following three values:
? mean
a correlation coefficient averaged over all trans-
lation outputs;
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
100
Spanish English French English German English Czech English
sentences 97122 83967 100222 94693
running words 2661344 2338495 2395141 2042085 2475359 2398780 2061422 2249365
vocabulary:
words 69620 53527 56295 50082 107278 54270 125614 52081
morphemes 14178 13449 12004 12485 22211 13499 18789 12961
POS tags 69 44 33 44 54 44 611 44
POS-2grams 2459 1443 826 1443 1611 1454 27835 1457
POS-3grams 27350 20474 10409 19838 19928 20769 209481 20522
POS-4grams 135166 121182 62177 114555 114314 123550 637337 120646
Table 1: Statistics of the corpora for training IBM1 lexicon models.
? rank>
percentage of documents where the particular
score has better correlation than the other IBM1
scores;
? rank?
percentage of documents where the particular
score has better or equal correlation than the
other IBM1 scores.
3.2 Comparison of IBM1 scores
The first step towards deciding which IBM1 score
to submit to the WMT 2011 evaluation task was a
comparison of the average correlations i.e. mean
values. These values for each of the IBM1 scores
are presented in Table 2. The left column shows
average correlations of the source-hypothesis (sh)
scores, and the right one of the hypothesis-source
(hs) scores.
mean IBM1sh IBM1hs
words 0.066 0.308
morphemes 0.227 0.445
POS tags 0.006 0.337
POS-2grams 0.058 0.337
POS-3grams 0.172 0.376
POS-4grams 0.196 0.442
Table 2: Average correlations of source-hypothesis (left
column) and hypothesis-source (right column) IBM1
scores.
It can be seen that the morpheme, POS-3gram and
POS-4gram scores have the best correlations in both
directions. Apart from that, it can be observed that
all the hs scores have better correlations than sh
scores. Therefore, all the further experiments will
deal only with the hs scores, and the subscript hs is
omitted.
In the next step, all the hs scores are sorted ac-
cording to each of the three values described in
Section 3.1, i.e. average correlation mean, rank>
and rank?, and the results are shown in Table 3.
The most promising scores according to each of
the three values are morpheme score MIBM1, POS-
3gram score P3IBM1 and POS-4gram score P4IBM1.
3.2.1 Combined IBM1 scores
The last experiment was to combine the most
promising IBM1 scores in order to see if the correla-
tion with human rankings can be further improved.
In general, a combined IBM1 score is defined as
arithmetic mean of various individual IBM1hs scores
described in Section 2:
COMBIBM1 =
K
?
k=1
wk ? IBM1k (4)
The following combinations were investigated:
? P1234IBM1
combination of all POS n-gram scores;
? MP1234IBM1
combination of all POS n-gram scores and the
morpheme score;
? MP34IBM1
combination of the most promising individual
scores, i.e. POS-3gram, POS-4gram and mor-
pheme scores;
101
mean rank> rank?
0.445 morphemes 60.6 POS-4grams 71.3 POS-4grams
0.442 POS-4grams 54.4 morphemes 61.3 POS-3grams
0.376 POS-3grams 50.6 POS-3grams 56.3 morphemes
0.337 POS-2grams 39.4 POS tags 48.1 POS tags
0.337 POS tags 36.3 words 43.7 POS-2grams
0.308 words 35.6 POS-2grams 42.5 words
Table 3: IBM1hs scores sorted by average correlation (column 1), rank> value (column 2) and rank? value (column
3). The most promising scores are those calculated on morphemes (MIBM1), POS-3grams (P3IBM1) and POS-4grams
(P4IBM1).
? MP4IBM1
combination of the two most promising indi-
vidual scores, i.e. POS-4gram score and mor-
pheme score.
For each of the scores, two variants were investi-
gated, with and without (i.e. with uniform) weights
wk. The weigths were choosen proportionally to
the average correlation of each individual score. Ta-
ble 4 contains average correlations for all combined
scores, together with the weight values.
combined score mean
P1234IBM1 0.403
+weights (0.15, 0.15, 0.3, 0.4) 0.414
MP1234IBM1 0.466
+weights (0.2, 0.05, 0.05, 0.2, 0.5) 0.486
MP34IBM1 0.480
+weights (0.25, 0.25, 0.5) 0.498
MP4IBM1 0.494
+weights (0.4, 0.6) 0.496
Table 4: Average correlations of the investigated IBM1hs
combinations. The weight values are choosen accord-
ing to the average correlation of the particular individual
IBM1 score.
The POS n-gram combination alone does not yield
any improvement over the best individual scores.
Introduction of the morpheme score increases the
average correlation, especially when only the best
n-gram scores are chosen. Apart from that, intro-
ducing weights improves the average correlation for
each of the combined scores.
The final step in our experiments consists of rank-
ing the weighted combined scores. The rank> and
rank? values for these scores are presented in Ta-
ble 5. According to the rank> values, the MP4IBM1
score clearly outperforms all other scores. This
score also has the highest mean value together with
the MP34IBM1 score. As for rank? values, all
morpheme-POS scores have similar values signifi-
cantly outperforming the P1234IBM1 score.
combined score rank> rank?
P1234IBM1 25.0 36.4
MP1234IBM1 44.8 68.7
MP34IBM1 39.6 64.6
MP4IBM1 55.2 65.7
Table 5: rank> (column 1) and rank? (column 2) values
of the weighted IBM1hs combinations.
Following all these observations, we decided to
submit the MP4IBM1 score to the WMT 2011 evalu-
ation task.
4 Conclusions and outlook
The results presented in this article show that the
IBM1 scores have the potential to be used as replace-
ment of current evaluation metrics based on refer-
ence translations. Especially the scores abstracting
away from word surface particularities (i.e. vocabu-
lary, domain) based on morphemes, POS-3grams and
4grams show a high average correlation of about 0.5
(the average correlation of the BLEU score on the
same data is 0.566).
An important point for future optimisation is to
investigate effects of the selection of training data
for the IBM1 models (and its similarity to the train-
ing data of the involved statistical translation sys-
tems). Furthermore, investigation of how to assign
the weights for combining the corresponding indi-
102
vidual scores, as well as of the possible impact of
different morpheme splittings should be carried out.
Other direction for future work is combination with
other features (i.e. POS language models).
This method is currently being tested and fur-
ther developed in the framework of the TARAX ?U
project2. In this project, three industry and one re-
search partners develop a hybrid machine transla-
tion architecture that satisfies current industry needs,
which includes a number of large-scale evalua-
tion rounds involving various languages: English,
French, German, Czech, Spanish, Russian, Chinese
and Japanese. By the time of writing this article, the
first human evaluation round in TARAX ?U on a pilot
set of about 7000 sentences is running. The metrics
proposed in this paper will be tested on the TARAX ?U
data as soon as they are available. First results will
be reported in the presentation of this paper.
Acknowledgments
This work has been partly developed within the
TARAX ?U project financed by TSB Technologies-
tiftung Berlin ? Zukunftsfonds Berlin, co-financed
by the European Union ? European fund for regional
development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In Pro-
ceedings of the ACL 05 Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for MT and/or Summa-
rization, pages 65?72, Ann Arbor, MI, June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Final report, JHU/CLSP Summer
Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
Meta-Evaluation of Machine Translation. In Proceed-
ings of the 3rd ACL 08 Workshop on Statistical Ma-
2http://taraxu.dfki.de/
chine Translation (WMT 08), pages 70?106, Colum-
bus, Ohio, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR
(WMT 10), pages 17?53, Uppsala, Sweden, July.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical Re-
port Report A81, Computer and Information Science,
Helsinki University of Technology, Helsinki, Finland,
March.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation. Technical re-
port, Johns Hopkins University 2003 Summer Work-
shop on Language Engineering, Center for Language
and Speech Processing, Baltimore, MD, USA, August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 02), pages 311?318, Philadel-
phia, PA, July.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 763?771,
Athens, Greece, March.
103
