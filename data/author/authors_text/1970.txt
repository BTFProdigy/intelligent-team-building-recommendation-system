BioNLP 2007: Biological, translational, and clinical language processing, pages 81?88,
Prague, June 2007. c?2007 Association for Computational Linguistics
ConText: An Algorithm for Identifying Contextual Features 
from Clinical Text 
Wendy W. Chapman, David Chu, John N. Dowling 
Department of Biomedical Informatics 
University of Pittsburgh 
Pittsburgh, PA 
chapman@cbmi.pitt.edu 
 
Abstract 
Applications using automatically indexed 
clinical conditions must account for con-
textual features such as whether a condition 
is negated, historical or hypothetical, or 
experienced by someone other than the pa-
tient. We developed and evaluated an algo-
rithm called ConText, an extension of the 
NegEx negation algorithm, which relies on 
trigger terms, pseudo-trigger terms, and 
termination terms for identifying the values 
of three contextual features. In spite of its 
simplicity, ConText performed well at 
identifying negation and hypothetical status. 
ConText performed moderately at identify-
ing whether a condition was experienced 
by someone other than the patient and 
whether the condition occurred historically.  
1 Introduction 
Natural language processing (NLP) techniques can 
extract variables from free-text clinical records 
important for medical informatics applications per-
forming decision support, quality assurance, and 
biosurveillance [1-6]. Many applications have fo-
cused on identifying individual clinical conditions 
in textual records, which is the first step in making 
the conditions available to computerized applica-
tions. However, identifying individual instances of 
clinical conditions is not sufficient for many medi-
cal informatics tasks?the context surrounding the 
condition is crucial for integrating the information 
within the text to determine the clinical state of a 
patient.  
For instance, it is important to understand 
whether a condition is affirmed or negated, acute 
or chronic, or mentioned hypothetically. We refer 
to these as contextual features, because the infor-
mation is not usually contained in the lexical repre-
sentation of the clinical condition itself but in the 
context surrounding the clinical condition. We de-
veloped an algorithm called ConText for identify-
ing three contextual features relevant for biosur-
veillance from emergency department (ED) reports 
and evaluated its performance compared to physi-
cian annotation of the features. 
2 Background 
2.1 Encoding Contextual Information from-
Clinical Texts 
NLP systems designed to encode detailed informa-
tion from clinical reports, such as MedLEE [1], 
MPLUS [7], and MedSyndikate [4], encode con-
textual features such as negation, uncertainty, 
change over time, and severity. Over the last ten 
years, several negation algorithms have been de-
scribed in the literature [8-12]. Recently, research-
ers at Columbia University have categorized tem-
poral expressions in clinical narrative text and 
evaluated a temporal constraint structure designed 
to model the temporal information for discharge 
summaries [13, 14]. 
ConText differs from most other work in this 
area by providing a stand-alone algorithm that can 
be integrated with any application that indexes 
clinical conditions from text.  
2.2 Biosurveillance from ED Data 
Biosurveillance and situational awareness are im-
perative research issues in today?s world. State-of-
the-art surveillance systems rely on chief com-
plaints and ICD-9 codes, which provide limited 
clinical information and have been shown to per-
form with only fair to moderate sensitivity [15-18]. 
ED reports are a timely source of clinical informa-
81
tion that may be useful for syndromic surveillance. 
We are developing NLP-based methods for identi-
fying clinical conditions from ED reports. 
2.3 SySTR 
We are developing an NLP application called 
SySTR (Syndromic Surveillance from Textual Re-
cords). It currently uses free-text descriptions of 
clinical conditions in ED reports to determine 
whether the patient has an acute lower respiratory 
syndrome.  We previously identified 55 clinical 
conditions (e.g. cough, pneumonia, oxygen desatu-
ration, wheezing) relevant for determining whether 
a patient has an acute lower respiratory condition 
[19]. SySTR identifies instances of these 55 clini-
cal conditions in ED reports to determine if a pa-
tient has an acute lower respiratory syndrome. 
SySTR has four modules: 
(1) Index each instance of the 55 clinical condi-
tions in an ED report; 
(2) For each indexed instance of a clinical condi-
tion, assign values to three contextual features; 
(3) Integrate the information from indexed in-
stances to determine whether each of the 55 
conditions are acute, chronic, or absent; 
(4) Use the values of the 55 conditions to deter-
mine whether a patient has an acute lower res-
piratory syndrome. 
We built SySTR on top of an application 
called caTIES [20], which comprises a GATE 
pipeline of processing resources (http://gate.ac.uk/). 
Module 1 uses MetaMap [5] to index UMLS con-
cepts in the text and then maps the UMLS concepts 
to the 55 clinical conditions. For instance, Module 
1 would identify the clinical condition Dyspnea in 
the sentence ?Patient presents with a 3 day history 
of shortness of breath.? For each instance of the 55 
conditions identified by Module 1, Module 2 as-
signs values to three contextual features: Negation 
(negated, affirmed); Temporality (historical, re-
cent, hypothetical); and Experiencer (patient, 
other). For the sentence above, Module 2 would 
assign Dyspnea the following contextual features 
and their values: Negation?affirmed; Temporal-
ity?recent; Experiencer?patient. Module 3, as 
described in Chu and colleagues [21], resolves 
contradictions among multiple instances of clinical 
conditions, removes conditions not experienced by 
the patient, and assigns a final value of acute, 
chronic, or absent to each of the 55 conditions. 
Module 4 uses machine learning models to deter-
mine whether a patient has acute lower respiratory 
syndrome based on values of the conditions.  
The objective of this study was to evaluate an 
algorithm for identifying the contextual informa-
tion generated by Module 2. 
3 Methods 
We developed an algorithm called ConText for 
determining the values for three contextual features 
of a clinical condition: Negation, Temporality, and 
Experiencer. The same algorithm is applied to all 
three contextual features and is largely based on a 
regular expression algorithm for determining 
whether a condition is negated or not (NegEx [9]). 
ConText relies on trigger terms, pseudo-trigger 
terms, and scope termination terms that are specific 
to the type of contextual feature being identified. 
Below we describe the three contextual features 
addressed by the algorithm, details of how Con-
Text works, and our evaluation of ConText. 
3.1 Three Contextual Features 
Determining whether a patient had an acute epi-
sode of a clinical condition, such as cough, poten-
tially involves information described in the context 
of the clinical condition in the text. We performed 
a pilot study to learn which contextual features af-
fected classification of 55 clinical conditions as 
acute, chronic, or absent [21]. The pilot study 
identified which contextual features were critical 
for our task and reduced the number of values we 
initially used.  
The contextual features for each indexed clinical 
condition are assigned default values. ConText 
changes the values if the condition falls within the 
scope of a relevant trigger term. Below, we de-
scribe the contextual features (default values are in 
parentheses). 
(1) Negation (affirmed): ConText determines 
whether a condition is negated, as in ?No fe-
ver.? 
(2) Temporality (recent): ConText can change 
Temporality to historical or hypothetical. In its 
current implementation, historical is defined as 
beginning at least 14 days before the visit to 
the ED, but the algorithm can easily be modi-
fied to change the length of time. ConText 
would mark Fever in ?Patient should return if 
she develops fever? as hypothetical.   
82
(3) Experiencer (patient): ConText assigns condi-
tions ascribed to someone other than the pa-
tient an Experiencer of other, as in ?The pa-
tient?s father has a history of CHF.?  
3.2 Contextual Feature Algorithm 
As we examined how the contextual features were 
manifested in ED reports, we discovered similar 
patterns for all features and hypothesized that an 
existing negation algorithm, NegEx [9], may be 
applicable for all three features.  
NegEx uses two regular expressions (RE) to de-
termine whether an indexed condition is negated: 
RE1: <trigger term> <5w> <indexed term> 
RE2: <indexed term> <5w> <trigger term> 
<5w> represents five words (a word can be a sin-
gle word or a UMLS concept), and the text 
matched by this pattern is called the scope. NegEx 
relies on three types of terms to determine whether 
a condition is negated: trigger terms, pseudo-
trigger terms, and termination terms. Trigger terms 
such as ?no? and ?denies? indicate that the clinical 
conditions that fall within the scope of the trigger 
term should be negated. Pseudo-trigger terms, such 
as ?no increase,? contain a negation trigger term 
but do not indicate negation of a clinical concept.  
A termination term such as ?but? can terminate the 
scope of the negation before the end of the win-
dow, as in ?She denies headache but complains of 
dizziness.?  
ConText is an expansion of NegEx. It relies on 
the same basic algorithm but applies different term 
lists and different windows of scope depending on 
the contextual feature being annotated.  
3.3 ConText Term Lists  
Each contextual feature has a unique set of trigger 
terms and pseudo-trigger terms, as shown in Table 
1. The complete list of terms can be found at 
http://web.cbmi.pitt.edu/chapman/ConText.html. 
Most of the triggers apply to RE1, but a few 
(marked in table) apply to RE2. ConText assigns a 
default value to each feature, then changes that 
value if a clinical condition falls within the scope 
of a relevant trigger term.  
Although trigger terms are unique to the contex-
tual feature being identified, termination terms 
Table 1. Examples of trigger and pseudo-trigger terms for the three contextual features. If all terms are not 
represented in the table, we indicate the number of terms used by ConText in parentheses. 
Temporality (default = recent) 
Trigger terms for 
hypothetical 
Pseudo-trigger 
terms Trigger terms for historical Pseudo-trigger terms (10) 
if 
return 
should [he|she] 
should there 
should the patient 
as needed 
come back [for|to] 
if negative 
 
General triggers 
history 
previous^ 
History Section title^^ 
Temporal Measurement triggers^^^ 
  <time> of 
  [for|over] the [last|past] <time> 
  since (last) [day-of-week|week|month| 
    season|year] 
history, physical 
history taking 
poor history 
history and examination 
history of present illness 
social history 
family history 
sudden onset of 
Experiencer (default = patient)  Negation (default = affirmed) 
Trigger terms 
for other (12) 
Pseudo-trigger 
terms 
 Trigger terms for negated (125) Pseudo-trigger terms (16) 
father(?s) 
mother(?s) 
aunt(?s) 
  no 
not 
denies 
without 
no increase 
not extend 
gram negative 
^  the scope for ?previous? only extends one term forward (e.g., ?for previous headache?) 
^^Currently the only history section title we use is PAST MEDICAL HISTORY. 
^^^ <time> includes the following regular expression indicating a temporal quantification: x[-|space]   
[day(s)|hour(s)|week(s)|month(s)|year(s)]. x = any digit; words in brackets are disjunctions; items in parentheses are 
optional. The first two temporal measurement triggers are used with RE1; the third is used with RE2. For our 
current application, a condition lasting 14 days or more is considered historical.  
 
83
may be common to multiple contextual features. 
For instance, a termination term indicating that the 
physician is speaking about the patient can indicate 
termination of scope for the features Temporality 
and Experiencer. In the sentence ?History of 
COPD, presenting with shortness of breath,? the 
trigger term ?history? indicates that COPD is his-
torical, but the term ?presenting? terminates the 
scope of the temporality trigger term, because the 
physician is now describing the current patient 
visit. Therefore, the condition Dyspnea (?shortness 
of breath?) should be classified as recent. Simi-
larly, in the sentence ?Mother has CHF and patient 
presents with chest pain,? Experiencer for CHF 
should be other, but Experiencer for Chest Pain 
should be patient.  
We compiled termination terms into conceptual 
groups, as shown in Table 2.  
Table 2. ConText?s termination terms. Column 1 lists 
the type of termination term, the number of terms used 
by Context, and the contextual feature values using that 
type of termination term. Column 2 gives examples of 
the terms. 
Type of Term Examples 
Patient (5) 
Temporal (hypothetical) 
Experiencer (other) 
Patient, who, his, her, pa-
tient?s 
Presentation (12) 
Temporal (historical) 
Experiencer (other) 
Presents, presenting, com-
plains, was found, states, 
reports, currently, today 
Because (2) 
Temporal (hypothetical) Since, because 
Which (1) 
Experiencer (other) Which 
ED (2) 
Temporal (historical) Emergency department, ED 
But (8) 
Negation (negated) 
But, however, yet, though, 
although, aside from 
3.4 ConText Algorithm 
The input to ConText is an ED report with in-
stances of the 55 clinical concepts already indexed. 
For each clinical condition, ConText assigns val-
ues to the three contextual features. ConText?s al-
gorithm is as follows1: 
                                                 
1 This algorithm applies to RE1. The algorithm for RE2 
is the same, except that it works backwards from the 
trigger term and does not look for pseudo-trigger terms. 
Go to first trigger term in sentence 
If term is a pseudo-trigger term, 
   Skip to next trigger term  
Determine scope of trigger term 
If termination term within scope, 
   Terminate scope before termination term 
Assign appropriate contextual feature value to 
all indexed clinical concepts within scope.  
The scope of a trigger term depends on the con-
textual feature being classified. The default scope 
includes all text following the indexed condition 
until the end of the sentence. Thus, in the sentence 
?He should return for fever? the scope of the Tem-
porality (hypothetical) trigger term ?return? in-
cludes the segment ?for fever,? which includes an 
indexed condition Fever. The default scope is over-
ridden in a few circumstances. First, as described 
above, the scope can be terminated by a relevant 
termination term. Second, if the trigger term is a 
<section title>, the scope extends throughout the 
entire section, which is defined previous to Con-
Text?s processing. Third, a trigger term itself can 
require a different scope. The Temporality (histori-
cal) term ?previous? only extends one term for-
ward in the sentence. 
3.5 Evaluation 
We evaluated ConText?s ability to assign correct 
values to the three contextual features by compar-
ing ConText?s annotations with annotations made 
by a physician. 
Setting and Subjects. The study was conducted on 
reports for patients presenting to the University of 
Pittsburgh Medical Center Presbyterian Hospital 
ED during 2002. The study was approved by the 
University of Pittsburgh?s Institutional Review 
Board. We randomly selected 120 reports for pa-
tients with respiratory-related ICD-9 discharge di-
agnoses for manual annotation. For this study, we 
used 30 reports as a development set and 90 re-
ports as a test set. In addition to the annotated de-
velopment set, we used a separate set of 100 unan-
notated ED reports to informally validate our term 
lists. 
Reference Standard. A physician board-certified 
in internal medicine and infectious diseases with 
30 years of experience generated manual annota-
tions for the development and test reports. He used 
GATE (http://gate.ac.uk/) to highlight every indi-
84
vidual annotation in the text referring to any of the 
55 clinical conditions. For every annotation, he 
assigned values to the three contextual features, as 
shown in Figure 1.  
Previous experience in annotating the 55 condi-
tions showed that a single physician was inade-
quate for generating a reliable reference standard 
[19]. The main mistake made by a single physician 
was not marking a concept that existed in the text. 
We used NLP-assisted review to improve physi-
cian annotations by comparing the single physi-
cian?s annotations to those made by SySTR. The 
physician reviewed disagreements and made 
changes to his original annotations if he felt his 
original annotation was incorrect. A study by Mey-
stre and Haug [22] used a similar NLP-assisted 
review methodology and showed that compared to 
a reference standard not using NLP-assisted re-
view, their system had higher  recall and the same 
precision. 
Outcome Measures. For each contextual feature 
assigned to an annotation, we compared ConText?s 
value to the value assigned by the reference stan-
dard. We classified the feature as a true positive 
(TP) if ConText correctly changed the condition?s 
default value and a true negative (TN) if ConText 
correctly left the default value. We then calculated 
recall and precision using the following formulas: 
)(
:Recall
FNofnumberTPofnumber
TPofnumber
+
 
)(
:Precision
FPofnumberTPofnumber
TPofnumber
+
 
For the Temporality feature, we calculated recall 
and precision separately for the values historical 
and hypothetical. We calculated the 95% confi-
dence intervals (CI) for all outcome measures. 
4 Results 
Using NLP-assisted review, the reference standard 
physician made several changes to his initial anno-
tations. He indexed an additional 82 clinical condi-
tions and changed the title of the clinical condition 
for 48 conditions, resulting in a total of 1,620 in-
dexed clinical conditions in the 90 test reports. The 
reference standard physician also made 35 changes 
to Temporality values and 4 changes to Negation. 
The majority of Temporality changes were from 
historical to recent (17) and from hypothetical to 
recent (12).   
Table 3 shows ConText?s recall and precision 
values compared to the reference standard annota-
tions. About half of the conditions were negated 
(773/1620). Fewer conditions were historical 
(95/1620), hypothetical (40/1620), or experienced 
by someone other than the patient (8/1620). In 
spite of low frequency for these contextual feature 
values, identifying them is critical to understanding 
a patient?s current state. ConText performed best 
on Negation, with recall and precision above 97%. 
ConText performed well at assigning the Tempo-
rality value hypothetical, but less well on the Tem-
porality value historical. Experiencer had a small 
sample size, making results difficult to interpret. 
 
Table 3. Outcome measures for ConText on test set of 90 ED reports. 
Feature TP TN FP FN Recall 95% CI 
Precision 
95% CI 
Negation 750 824 23 23 97.0 96-98 
97.0 
96-98 
Temporality 
(historical) 66 1499 23 32 
67.4 
58-76 
74.2 
64-82 
Temporality 
(hypothetical) 33 1578 2 7 
82.5 
68-91 
94.3 
81-98 
Experiencer 4 1612 0 4 50.00 22-78 
100 
51-100 
5 Discussion 
We evaluated an extension of the NegEx algorithm 
for determining the values of two additional con-
textual features?Temporality and Experiencer. 
ConText performed with very high recall and pre-
cision when determining whether a condition was 
negated, and demonstrated moderate to high per-
formance on the other features. 
Figure 1. When the physician highlights text, 
GATE provides a drop-down menu to select the 
Clinical Condition and the values of the Contex-
tual Features. 
85
We performed an informal error analysis, which 
not only isolates ConText?s errors but also points 
out future research directions in contextual feature 
identification.  
5.1 Negation 
ConText?s negation identification performed sub-
stantially better than NegEx?s published results [9], 
even though ConText is very similar to NegEx and 
uses the same trigger terms.  Several possible ex-
planations exist for this boost in performance. First, 
our study evaluated negation identification in ED 
reports, whereas the referenced study on NegEx 
applied to discharge summaries. Second, ConText 
only applied to 55 clinical conditions, rather than 
the large set of UMLS concepts in the NegEx 
study. Third, the conditions indexed by SySTR that 
act as input to ConText are sometimes negated or 
affirmed before ConText sees them. For some con-
ditions, SySTR addresses internal negation in a 
word (e.g., ?afebrile? is classified as Fever with the 
Negation value negated). Also, SySTR assigns 
Negation values to some conditions with numeric 
values, such as negating Tachycardia from ?pulse 
rate 75.? Fourth, ConText does not use NegEx?s 
original scope of five words, but extends the scope 
to the end of the sentence. It would be useful to 
compare ConText?s scope difference directly 
against NegEx to determine which scope assign-
ment works better, but our results suggest the in-
creased scope may work well for ED reports. 
ConText?s errors in assigning the Negation 
value were equally distributed between FN?s and 
FP?s (23 errors each). Some false negatives re-
sulted from missing trigger terms (e.g., ?denying?). 
Several false negatives resulted from the interac-
tion between ConText and SySTR?s mapping rules. 
For example, in the sentence ?chest wall is without 
tenderness,? SySTR maps the UMLS concepts for 
?chest wall? and ?tenderness? to the condition 
Chest Wall Tenderness. In such a case, the nega-
tion trigger term ?without? is caught between the 
two UMLS concepts. Therefore, RE1 does not 
match, and ConText does not change the default 
from affirmed. False positive negations resulted 
from our not integrating the rule described in 
NegEx that a concept preceded by a definite article 
should not be negated [23] (e.g., ?has not been on 
steroids for his asthma?) and from descriptions in 
the text whose Negation status is even difficult for 
humans to determine, such as ?no vomiting with-
out having the cough? and ?patient does not know 
if she has a fever.? 
5.2 Temporality 
Historical. ConText identified historical condi-
tions with 67% sensitivity and 74% precision. 
Identifying historical conditions appears simple on 
the surface, but is a complex problem. The single 
trigger term ?history? is used for many of the his-
torical conditions, but the word ?history? is a rela-
tive term that can indicate a history of years (as in 
?history of COPD?) or of only a few days (as in 
?ENT: No history of nasal congestion?). The error 
analysis showed that ConText is missing trigger 
terms that act equivalently to the word ?history? 
such as ?in the past? (?has not been on steroids in 
the past for his asthma?) and ?pre-existing? (?pre-
existing shortness of breath?).  
Some conditions that the reference standard 
classified as historical had no explicit trigger in the 
text, as in the sentence ?When he sits up in bed, he 
develops pain in the chest.? It may be useful to 
implement rules involving verb tense for these 
cases. 
The most difficult cases for ConText were those 
with temporal measurement triggers. The few tem-
poral quantifier patterns we used were fairly suc-
cessful, but the test set contained multiple varia-
tions on those quantifiers, and a new dataset would 
probably introduce even more variations. For in-
stance, ConText falsely classified Non-pleuritic 
Chest Pain as historical in ?awoken at approxi-
mately 2:45 with chest pressure,? because Con-
Text?s temporal quantifiers do not account for time 
of the day. Also, even though ConText?s temporal 
quantifiers include the pattern ?last x weeks,? x 
represents a digit and thus didn?t match the phrase 
?intermittent cough the last couple of weeks.?  
We were hoping that identifying historical con-
ditions would not require detailed modeling of 
temporal information, but our results suggest oth-
erwise. We will explore the temporal categories 
derived by Hripcsak and Zhou [13] for discharge 
summaries to expand ConText?s ability to identify 
temporal measurement triggers.  
Hypothetical. ConText demonstrated 83% recall 
and 94% precision when classifying a condition as 
hypothetical rather than recent. Again, missing 
trigger terms (e.g., ?returning? and ?look out for?) 
and termination terms (e.g., ?diagnosis?) caused 
errors. The chief cause of false negatives was ter-
86
minating the scope of a trigger term too early. For 
instance, in the sentence ?She knows to return to 
the ED if she has anginal type chest discomfort 
which was discussed with her, shortness of breath, 
and peripheral edema? the scope of the trigger ?re-
turn? was terminated by ?her.? The major limita-
tion of regular expressions is evident in this exam-
ple in which ?her? is part of a relative clause modi-
fying ?chest discomfort,? not ?shortness of breath.?  
5.3 Experiencer 
ConText?s ability to identify an experiencer other 
than the patient suffered from low prevalence. In 
the test set of 90 reports, only 8 of the 1620 condi-
tions were experienced by someone other than the 
patient, and ConText missed half of them. Two of 
the false negatives came from not including the 
trigger term ?family history.? A more difficult er-
ror to address is recognizing that bronchitis is ex-
perienced by someone other than the patient in 
??due to the type of bronchitis that is currently 
being seen in the community.? ConText made no 
false positive classifications for Experiencer. 
5.4 Limitations and Future Work 
Some of ConText?s errors can be resolved by refin-
ing the trigger and termination terms. However, 
many of the erroneous classifications are due to 
complex syntax and semantics that cannot be han-
dled by simple regular expressions. Determining 
the scope of trigger terms in sentences with relative 
clauses and coordinated conjunctions is especially 
difficult. We believe ConText?s approach involv-
ing trigger terms, scope, and termination terms is 
still a reasonable model for this problem and hope 
to improve ConText?s ability to identify scope with 
syntactic information.  
A main limitation of our evaluation was the ref-
erence standard, which was comprised of a single 
physician. We used NLP-assisted review to in-
crease the identification of clinical conditions and 
decrease noise in his classifications. It is possible 
that the NLP-assisted review biased the reference 
standard toward ConText?s classifications, but the 
majority of changes made after NLP-assisted re-
view involved indexing the clinical conditions, 
rather than changing the values of the contextual 
features. Moreover, most of the changes to contex-
tual feature values involved a change in our anno-
tation schema after the physician had completed 
his first round of annotations. Specifically, we al-
lowed the physician to use the entire report to de-
termine whether a condition was historical, which 
caused him to mark recent exacerbations of his-
torical conditions as historical. A second physician 
is in the process of annotating the test set. The two 
physicians will come to consensus on their classi-
fications in generating a new reference standard.  
How good contextual feature identification has 
to be depends largely on the intended application. 
We tested SySTR?s ability to determine whether 
the 55 clinical conditions were acute, chronic, or 
absent on a subset of 30 test reports [24]. SySTR 
made 51 classification errors, 22 of which were 
due to ConText?s mistakes. In spite of the errors, 
SySTR demonstrated a kappa of 0.85 when com-
pared to physician classifications, suggesting that 
because of redundancy in clinical reports, Con-
Text?s mistakes may not have a substantial adverse 
effect on SySTR?s final output.  
5.5 Conclusion 
We evaluated a regular-expression-based algorithm 
for determining the status of three contextual fea-
tures in ED reports and found that ConText per-
formed very well at identifying negated conditions, 
fairly well at determining whether conditions were 
hypothetical or historical, and moderately well at 
determining whether a condition was experienced 
by someone other than the patient. ConText?s algo-
rithm is based on the negation algorithm NegEx, 
which is a frequently applied negation algorithm in 
biomedical informatics applications due to its sim-
plicity, availability, and generalizability to various 
NLP applications. Simple algorithms for identify-
ing contextual features of indexed conditions is 
important in medical language processing for im-
proving the accuracy of information retrieval and 
extraction applications and for providing a baseline 
comparison for more sophisticated algorithms. 
ConText accepts any indexed clinical conditions as 
input and thus may be applicable to other NLP ap-
plications. We do not know how well ConText will 
perform on other report types, but see similar con-
textual features in discharge summaries, progress 
notes, and history and physical exams. Currently, 
ConText only identifies three contextual features, 
but we hope to extend the algorithm to other fea-
tures in the future, such as whether a condition is 
mentioned as a radiology finding or as a diagnosis 
(e.g., Pneumonia).  
87
Over and above negation identification, which 
can be addressed by NegEx or other algorithms, 
ConText could be useful for a variety of NLP tasks, 
including flagging historical findings and eliminat-
ing indexed conditions that are hypothetical or 
were not experienced by the patient. Ability to 
modify indexed conditions based on their contex-
tual features can potentially improve precision in 
biosurveillance, real-time decision support, and 
information retrieval. 
Acknowledgments. This work was supported by 
NLM grant K22 LM008301, ?Natural language 
processing for respiratory surveillance.? 
References 
1. Friedman C. A broad-coverage natural language 
processing system. Proc AMIA Symp 2000:270-4. 
2. Fiszman M, Chapman WW, Aronsky D, Evans RS, 
Haug PJ. Automatic detection of acute bacterial pneu-
monia from chest X-ray reports. J Am Med Inform 
Assoc 2000;7(6):593-604. 
3. Taira R, Bashyam V, Kangarloo H. A field theory 
approach to medical natural language processing. IEEE 
Transactions in Inform Techn in Biomedicine 
2007;11(2). 
4. Hahn U, Romacker M, Schulz S. MEDSYNDI-
KATE-a natural language system for the extraction of 
medical information from findings reports. Int J Med Inf 
2002;67(1-3):63-74. 
5. Aronson AR. Effective mapping of biomedical text to 
the UMLS Metathesaurus: the MetaMap program. Proc 
AMIA Symp 2001:17-21. 
6. Hazlehurst B, Frost HR, Sittig DF, Stevens VJ. 
MediClass: A system for detecting and classifying en-
counter-based clinical events in any electronic medical 
record. J Am Med Inform Assoc 2005;12(5):517-29. 
7. Christensen L, Haug PJ, Fiszman M. MPLUS: a 
probabilistic medical language understanding system. 
Proc Workshop on Natural Language Processing in the 
Biomedical Domain 2002:29-36. 
8. Mutalik PG, Deshpande A, Nadkarni PM. Use of 
general-purpose negation detection to augment concept 
indexing of medical documents: a quantitative study 
using the UMLS. J Am Med Inform Assoc 
2001;8(6):598-609. 
9. Chapman WW, Bridewell W, Hanbury P, Cooper GF, 
Buchanan BG. A simple algorithm for identifying ne-
gated findings and diseases in discharge summaries. J 
Biomed Inform 2001;34(5):301-10. 
10. Elkin PL, Brown SH, Bauer BA, Husser CS, Carruth 
W, Bergstrom LR, et al A controlled trial of automated 
classification of negation from clinical notes. BMC Med 
Inform Decis Mak 2005;5(1):13. 
11. Herman T, Matters M, Walop W, Law B, Tong W, 
Liu F, et al Concept negation in free text components of 
vaccine safety reports. AMIA Annu Symp Proc 
2006:1122. 
12. Huang Y, Lowe HJ. A Novel Hybrid Approach to 
Automated Negation Detection in Clinical Radiology 
Reports. J Am Med Inform Assoc 2007. 
13. Hripcsak G, Zhou L, Parsons S, Das AK, Johnson 
SB. Modeling electronic discharge summaries as a sim-
ple temporal constraint satisfaction problem. J Am Med 
Inform Assoc 2005;12(1):55-63. 
14. Zhou L, Melton GB, Parsons S, Hripcsak G. A tem-
poral constraint structure for extracting temporal infor-
mation from clinical narrative. J Biomed Inform 2005. 
15. Chapman WW, Dowling JN, Wagner MM. Classifi-
cation of emergency department chief complaints into 
seven syndromes:  a retrospective analysis of 527,228 
patients. Ann Emerg Med 2005;46(5):445-455. 
16. Ivanov O, Wagner MM, Chapman WW, Olszewski 
RT. Accuracy of three classifiers of acute gastrointesti-
nal syndrome for syndromic surveillance. Proc AMIA 
Symp 2002:345-9. 
17. Chang HG, Cochrane DG, Tserenpuntsag B, Allegra 
JR, Smith PF. ICD9 as a surrogate for chart review in 
the validation of a chief complaint syndromic surveil-
lance system. In: Syndromic Surveillance Conference 
Seattle, Washington; 2005. 
18. Beitel AJ, Olson KL, Reis BY, Mandl KD. Use of 
emergency department chief complaint and diagnostic 
codes for identifying respiratory illness in a pediatric 
population. Pediatr Emerg Care 2004;20(6):355-60. 
19. Chapman WW, Fiszman M, Dowling JN, Chapman 
BE, Rindflesch TC. Identifying respiratory findings in 
emergency department reports for biosurveillance using 
MetaMap. Medinfo 2004;2004:487-91. 
20. Mitchell KJ, Becich MJ, Berman JJ, Chapman WW, 
Gilbertson J, Gupta D, et al Implementation and 
evaluation of a negation tagger in a pipeline-based sys-
tem for information extraction from pathology reports. 
Medinfo 2004;2004:663-7. 
21. Chu D, Dowling JN, Chapman WW. Evaluating the 
effectiveness of four contextual features in classifying 
annotated clinical conditions in emergency department 
reports. AMIA Annu Symp Proc 2006:141-5. 
22. Meystre S, Haug PJ. Natural language processing to 
extract medical problems from electronic clinical docu-
ments: performance evaluation. J Biomed Inform 
2006;39(6):589-99. 
23. Goldin I, Chapman WW. Learning to detect nega-
tion with 'not' in medical texts. In: Proc Workshop on 
Text Analysis and Search for Bioinformatics at the 26th 
Annual International ACM SIGIR Conference (SIGIR-
2003); 2003. 
24. Chu D. Clinical feature extraction from emergency 
department reports for biosurveillance [Master's Thesis]. 
Pittsburgh: University of Pittsburgh; 2007. 
88
Proceedings of the Workshop on BioNLP, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Distinguishing Historical from Current Problems in Clinical      
Reports?Which Textual Features Help?  
 
Danielle L. Mowery MS, Henk Harkema PhD, John N. Dowling MS MD,  
Jonathan L. Lustgarten PhD, Wendy W. Chapman PhD 
Department of Biomedical Informatics 
University of Pittsburgh, Pittsburgh, Pa 15260, USA 
dlm31@pitt.edu, heh23@pitt.edu, dowling@pitt.edu, jll47@pitt.edu, wec6@pitt.edu 
 
 
Abstract 
Determining whether a condition is historical 
or recent is important for accurate results in 
biomedicine. In this paper, we investigate four 
types of information found in clinical text that 
might be used to make this distinction. We 
conducted a descriptive, exploratory study us-
ing annotation on clinical reports to determine 
whether this temporal information is useful 
for classifying conditions as historical or re-
cent. Our initial results suggest that few of 
these feature values can be used to predict 
temporal classification. 
1 Introduction 
Clinical applications for decision support, biosur-
veillance and quality of care assessment depend on 
patient data described in unstructured, free-text 
reports.  For instance, patient data in emergency 
department reports contain valuable indicators for 
biosurveillance applications that may provide early 
signs and symptoms suggestive of an outbreak. 
Quality assurance departments can use free-text 
medical record data to assess adherence to quality 
care guidelines, such as determining whether an 
MI patient was given an aspirin within twenty-four 
hours of arrival. In either application, one must 
consider how to address the question of time, but 
each of the applications requires a different level of 
temporal granularity: the biosurveillance system 
needs a coarse-grained temporal model that dis-
cerns whether the signs and symptoms are histori-
cal or recent. In contrast, the quality assurance 
system needs a fine-grained temporal model to 
identify the admission event, when (or if) aspirin 
was given, and the order and duration of time be-
tween these events. One important problem in nat-
ural language processing is extracting the appro-
priate temporal granularity for a given task. 
Many solutions exist for extracting temporal in-
formation, and each is designed to address ques-
tions of various degrees of temporal granularity, 
including determining whether a condition is his-
torical or recent, identifying explicit temporal ex-
pressions, and identifying temporal relations 
among events in text. (Chapman et al, 2007; Zhou 
et al, 2008; Irvine et al, 2008;  Verhagen and Pus-
tejovsky, 2008; Bramsen et al, 2006). We pre-
viously extended the NegEx algorithm in ConText, 
a simple algorithm that relies on lexical cues to 
determine whether a condition is historical or re-
cent (Chapman et al, 2007). However, ConText 
performs with moderate recall (76%) and precision 
(75%) across different report types implying that 
trigger terms and simple temporal expressions are 
not sufficient for the task of identifying historical 
conditions.  
In order to extend work in identifying historical 
conditions, we conducted a detailed annotation 
study of potentially useful temporal classification 
features for conditions found in six genres of clini-
cal text. Our three main objectives were: (1) cha-
racterize the temporal similarity and differences 
found in different genres of clinical text; (2) de-
termine which features successfully predict wheth-
er a condition is historical, and (3) compare 
ConText to machine learning classifiers that ac-
count for this broader set of temporal features. 
2 Temporality in Clinical Text 
For several decades, researchers have been study-
ing temporality in clinical records (Zhou and 
Hripcsak, 2007). Readers use a variety of clues to 
distinguish temporality from the clinical narrative, 
and we wanted to identify features from other tem-
10
poral models that may be useful for determining 
whether a condition is historical or recent.  
There are a number of automated systems for 
extracting, representing, and reasoning time in a 
variety of text. One system that emerged from the 
AQUAINT workshops for temporal modeling of 
newspaper articles is TARSQI. TARSQI processes 
events annotated in text by anchoring and ordering 
them with respect to nearby temporal expressions 
(Verhagen and Pustejovsky, 2008). A few recent 
applications, such as TimeText and TN-TIES 
(Zhou et al, 2008; Irvine et al, 2008), identify 
medically relevant events from clinical texts and 
use temporal expressions to order the events. One 
method attempts to order temporal segments of 
clinical narratives (Bramsen et al, 2006). One key 
difference between these previous efforts and our 
work is that these systems identify all temporal 
expressions from the text and attempt to order all 
events. In contrast, our goal is to determine wheth-
er a clinical condition is historical or recent, so we 
focus only on temporal information related to the 
signs, symptoms, and diseases described in the 
text. Therefore, we ignore explicit temporal ex-
pressions that do not modify clinical conditions. If 
a condition does not have explicit temporal mod-
ifiers, we still attempt to determine the historical 
status for that condition (e.g., ?Denies cough?). In 
order to improve the ability to determine whether a 
condition is historical, we carried out this annota-
tion study to identify any useful temporal informa-
tion related to the clinical conditions in six clinical 
genres. Building on work in this area, we explored 
temporal features used in other temporal annota-
tion studies. 
TimeML is a well-known standard for complex, 
temporal annotation. TimeML supports the annota-
tion of events defined as ?situations that happen or 
occur? and temporal expressions such as dates and 
durations in order to answer temporal questions 
about these events and other entities in news text 
(Saur??, et al, 2006). One notable feature of the 
TimeML schema is its ability to capture verb tense 
such as past or present and verb aspect such as 
perfective or progressing. We annotated verb tense 
and aspect in medical text according to the Time-
ML standard. 
Within the medical domain, Zhou et al (2006) 
developed an annotation schema used to identify 
temporal expressions and clinical events. They 
measured the prevalence of explicit temporal ex-
pressions and key medical events like admission or 
transfer found in discharge summaries. We used 
the Zhou categorization scheme to explore tempor-
al expressions and clinical events across genres of 
reports. 
A few NLP systems rely on lexical cues to ad-
dress time. MediClass is a knowledge-based sys-
tem that classifies the content of an encounter 
using both free-text and encoded information from 
electronic medical records (Hazelhurst et al, 
2005). For example, MediClass classifies smoking 
cessation care delivery events by identifying the 
status of a smoker as continued, former or history 
using words like continues. ConText, an extension 
of the NegEx algorithm, temporally classifies con-
ditions as historical, recent, or hypothetical using 
lexical cues such as history, new, and if, respec-
tively (Chapman et al, 2007). Drawing from these 
applications, we used state and temporal trigger 
terms like active, unchanged, and history to cap-
ture coarse, temporal information about a condi-
tion.  
Temporal information may also be implied in 
the document structure, particularly with regards to 
the section in which the condition appears. SecTag 
marks explicit and implicit sections found 
throughout patient H&P notes (Denny et al, 2008). 
We adopted some section headers from the SecTag 
terminology to annotate sections found in reports.  
Our long-term goal is to build a robust temporal 
classifier for information found in clinical text 
where the output is classification of whether a con-
dition is historical or recent (historical categoriza-
tion). An important first step in classifying 
temporality in clinical text is to identify and cha-
racterize temporal features found in clinical re-
ports. Specifically, we aim to determine which 
expressions or features are predictive of historical 
categorization of clinical conditions in dictated 
reports. 
3 Historical Assignment and Temporal 
Features 
We conducted a descriptive, exploratory study of 
temporal features found across six genres of clini-
cal reports. We had three goals related to our task 
of determining whether a clinical condition was 
historical or recent. First, to develop a temporal 
classifier that is generalizable across report types, 
we compared temporality among different genres 
11
of clinical text. Second, to determine which fea-
tures predict whether a condition is historical or 
recent, we observed common rules generated by 
three different rule learners based on manually an-
notated temporal features we describe in the fol-
lowing section. Finally, we compared the 
performance of ConText and automated rule learn-
ers and assessed which features may improve the 
ConText algorithm.  
Next, we describe the temporal features we as-
sessed for identification of historical signs, symp-
toms, or diseases, including temporal expressions, 
lexical cues, verb tense and aspect, and sections.  
(1) Temporal Expressions: Temporal expres-
sions are time operators like dates (May 5th 2005) 
and durations (for past two days), as well as clini-
cal processes related to the encounter (discharge, 
transfer). For each clinical condition, we annotated 
whether a temporal expression modified it and, if 
so, the category of temporal expression. We used 
six major categories from Zhou et al (2006) in-
cluding: Date and Time, Relative Date and Time, 
Durations, Key Events, Fuzzy Time, and No Tem-
poral Expression. These categories also have 
types. For instance, Relative Date and Time has a 
type Yesterday, Today or Tomorrow.  For the con-
dition in the sentence ?The patient had a stroke in 
May 2006?, the temporal expression category is 
Date and Time with type Date. Statements without 
a temporal expression were annotated No Tempor-
al Expression with type N/A. 
(2) Tense and Aspect: Tense and aspect define 
how a verb is situated and related to a particular 
time. We used TimeML Specification 1.2.1 for 
standardization of tense and aspect where exam-
ples of tense include Past or Present and aspect 
may be Perfective, Progressive, Both or None as 
found in Saur??, et al (2006). We annotated the 
verb that scoped a condition and annotated its tense 
and aspect. The primary verb may be a predicate 
adjective integral to interpretation of the condition 
(Left ventricle is enlarged), a verb preceding the 
condition (has hypertension), or a verb following a 
condition (Chest pain has resolved). In ?her chest 
pain has resolved,? we would mark ?has resolved? 
with tense Present and aspect Perfective. State-
ments without verbs (e.g., No murmurs) would be 
annotated Null for both.  
(3) Trigger Terms: We annotated lexical cues 
that provide temporal information about a condi-
tion. For example, in the statement, ?Patient has 
past history of diabetes,? we would annotate ?his-
tory? as Trigger Term: Yes and would note the ex-
act trigger term. 
     (4) Sections: Sections are ?clinically meaning-
ful segments which act independently of the 
unique narrative? for a patient (Denny et al 2008). 
Examples of report sections include Review of Sys-
tems (Emergency Department), Findings (Opera-
tive Gastrointestinal and Radiology) and 
Discharge Diagnosis (Emergency Department and 
Discharge Summary).  
We extended Denny?s section schema with ex-
plicit, report-specific section headers not included 
in the original terminology. Similar to Denny, we 
assigned implied sections in which there was an 
obvious change of topic and paragraph marker. For 
instance, if the sentence ?the patient is allergic to 
penicillin? followed the Social History section, we 
annotated the section as Allergies, even if there 
was not a section heading for allergies. 
4 Methods 
4.1 Dataset Generation 
We randomly selected seven reports from each of 
six genres of clinical reports dictated at the Univer-
sity of Pittsburgh Medical Center during 2007 
These included Discharge Summaries, Surgical 
Pathology, Radiology, Echocardiograms, Opera-
tive Gastrointestinal, and Emergency Department 
reports. The dataset ultimately contained 42 clini-
cal reports and 854 conditions. Figure 1 show our 
annotation process, which was completed in 
GATE, an open-source framework for building 
NLP systems (http://gate.ac.uk/). A physician 
board-certified in internal medicine and infectious 
diseases annotated all clinical conditions in the set 
and annotated each condition as either historical or 
recent. He used a general guideline for annotating 
a condition as historical if the condition began 
more than 14 days before the current encounter and 
as recent if it began or occurred within 14 days or 
during the current visit. However, the physician 
was not bound to this definition and ultimately 
used his own judgment to determine whether a 
condition was historical. 
Provided with pre-annotated clinical conditions 
and blinded to the historical category, three of the 
authors annotated the features iteratively in groups 
of six (one of each report type) using guidelines we 
12
developed for the first two types of temporal fea-
tures (temporal expressions and trigger terms.) 
Between iterations, we resolved disagreements 
through discussion and updated our guidelines. 
Cohen?s kappa for temporal expressions and trig-
ger terms by the final iteration was at 0.66 and 0.69 
respectively. Finally, one author annotated sec-
tions, verb tense, and aspect.  Cases in which as-
signing the appropriate feature value was unclear 
were resolved after consultation with one other 
author-annotator.  
4.2 Data Analysis 
 
We represented each condition as a vector with  
temporal features and their manually-assigned val-
ues as input features for predicting the binary out-
come value of historical or recent. We trained three 
rule learning algorithms to classify each condition 
as historical or recent: J48 Decision Tree, Ripper, 
and Rule Learner (RL) (Witten and Frank, 2005; 
Clearwater and Provost, 1990). Rule learners per-
form well at classification tasks and provide expli-
cit rules that can be viewed, understood, and 
potentially implemented in existing rule-based ap-
plications. We used Weka 3.5.8, an openly-
available machine learning application for predic-
tion modeling, to implement the Decision Tree 
(J48) and Ripper (JRip) algorithms, and we applied 
an in house version of RL retrieved from 
www.dbmi.pitt.edu\probe. For all rule learners, we 
used the default settings and ran ten-fold cross-
validation. The J48 algorithm produces mutually 
exclusive rules for predicting the outcome value. 
Thus, two rules cannot cover or apply to any one 
case. In contrast, both JRip and RL generate non-
mutually-exclusive rules for predicting the out-
come value. Although J48 and JRip are sensitive to 
bias in outcome values, RL accounts for skewed 
distribution of the data.  
We also applied ConText to the test cases to 
classify them as historical or recent. ConText looks 
for trigger terms and a limited set of temporal ex-
pressions within a sentence. Clinical conditions 
within the scope of the trigger terms are assigned 
the value indicated by the trigger terms (e.g., his-
torical for the term history). Scope extends from 
the trigger term to the end of the sentence or until 
the presence of a termination term, such as pre-
senting. For instance, in the sentence ?History of 
CHF, presenting with chest pain,? CHF would be 
annotated as historical.  
5 Evaluation 
To characterize the different reports types, we es-
tablished the overall prevalence and proportion of 
conditions annotated as historical for each clinical 
report genre.  We assessed the prevalence of each 
feature (temporal expressions, trigger terms, tense 
and aspect, and sections) by report genre to deter-
mine the level of similarity or difference between 
genres. To determine which features values are 
predictive of whether a condition is historical or 
recent, we observed common rules found by more 
than one rule learning algorithm. Amongst com-
mon rules, we identified new rules that could im-
prove the ConText algorithm.  
We also measured predictive performance with 
95% confidence intervals of the rule learners and 
ConText by calculating overall accuracy, as well as 
recall and precision for historical classifications 
and recall and precision for recent classifications.  
Table 1 describes equations for the evaluation me-
trics. 
 
Table 1. Description of evaluation metrics. RLP = rule 
learner prediction. RS = Reference Standard 
 
 
Figure 1. Annotation process for dataset and objectives 
for evaluation. 
13
Recall:                 number of TP              
(number of TP + number of FN) 
 
Precision:           number of TP              
(number of TP + number of FP) 
 
Accuracy:   number of instances correctly classified 
                      total number of possible instances  
6 Results 
Overall, we found 854 conditions of interest across 
all six report genre. Table 2 illustrates the preva-
lence of conditions across report genres. Emergen-
cy Department reports contained the highest 
concentration of conditions. Across report genres, 
87% of conditions were recent (741 conditions). 
All conditions were recent in Echocardiograms, in 
contrast to Surgical Pathology reports in which 
68% were recent.  
 
Table 2. Prevalence and count of conditions by temporal 
category and report genre. DS = Discharge Summary, 
Echo = Echocardiogram, ED = Emergency Department, 
GI = Operative Gastrointestinal, RAD = Radiology and 
SP = Surgical Pathology. (%) = percent; Ct = count.  
 
6.1 Prevalence of Temporal Features 
Table 3 shows that most conditions were not mod-
ified by a temporal expression or a trigger term. 
Conditions were modified by a temporal expres-
sion in Discharge Summaries more often than in 
other report genres. Similarly, Surgical Pathology 
had the highest prevalence of conditions modified 
by a trigger term. Operative Gastrointestinal and 
Radiology reports showed the lowest prevalence of 
both temporal expressions and trigger terms. Nei-
ther temporal expressions nor trigger terms oc-
curred in Echocardiograms. Overall, the 
prevalence of conditions scoped by a verb varied 
across report types ranging from 46% (Surgical 
Pathology) to 81% (Echocardiogram). 
Table 3. Prevalence of conditions modified by temporal 
features. All conditions were assigned a section and are 
thereby excluded. TE = temporal expression; TT = trig-
ger term; V = scoped by verb.  
 
6.2 Common Rules 
Rule learners generated a variety of rules. The J48 
Decision Tree algorithm learned 27 rules, six for 
predicting conditions as historical and the remain-
ing for classifying the condition as recent. The 
rules predominantly incorporated the trigger term 
and verb tense and aspect feature values. JRip 
learned nine rules, eight for classifying the histori-
cal temporal category and one ?otherwise? rule for 
the majority class. The JRip rules most heavily 
incorporated the section feature. The RL algorithm 
found 79 rules, 18 of which predict the historical 
category. Figure 2 illustrates historical rules 
learned by each rule learner. JRip and RL pre-
dicted the following sections alone can be used to 
predict a condition as historical: Past Medical His-
tory, Allergies and Social History. Both J48 and 
RL learned that trigger terms like previous, known 
and history predict historical. There was only one 
common, simple rule for the historical category 
found amongst all three learners: the trigger term 
no change predicts the historical category. All al-
gorithms learned a number of rules that include 
two features values; however, none of the com-
pound rules were common amongst all three algo-
rithms.    
 
Figure 2. Historical rules learned by each rule learner 
algorithm. Black dots represent simple rules whereas 
triangles represent compound rules. Common rules 
shared by each algorithm occur in the overlapping areas 
of each circle. 
14
6.3 Predictive Performance 
Table 4 shows predictive performance for each 
rule learner and for ConText. The RL algorithm 
outperformed all other algorithms in almost all 
evaluation measures. The RL scores were com-
puted based on classifying the 42 cases (eight his-
torical) for which the algorithm did not make a 
prediction as recent. ConText and J48, which ex-
clusively relied on trigger terms, had lower recall 
for the historical category.  
All of the rule learners out-performed ConText. 
JRip and RL showed substantially higher recall for 
assigning the historical category, which is the most 
important measure in a comparison with ConText, 
because ConText assigns the default value of re-
cent unless there is textual evidence to indicate a 
historical classification. Although the majority 
class baseline shows high accuracy due to high 
prevalence of the recent category, all other classifi-
ers show even higher accuracy, achieving fairly 
high recall and precision for the historical cases 
while maintaining high performance on the recent 
category. 
 
Table 4. Performance results with 95% confidence in-
tervals for three rule learners trained on manually anno-
tated features and ConText, which uses automatically 
generated features. Bolded values do not have overlap-
ping confidence intervals with ConText. MCB = Ma-
jority Class Baseline (recent class)   
 
7 Discussion 
Our study provides a descriptive investigation of 
temporal features found in clinical text. Our first 
objective was to characterize the temporal similari-
ties and differences amongst report types. We 
found that the majority of conditions in all report 
genres were recent conditions, indicating that a 
majority class classifier would produce an accura-
cy of about 87% over our data set.  According to 
the distributions of temporal category by report 
genre (Table 2), Echocardiograms exclusively de-
scribe recent conditions. Operative Gastrointestinal 
and Radiology reports contain similar proportions 
of historical conditions (9% and 6%). Echocardio-
grams appear to be most similar to Radiology re-
ports and Operative Gastrointestinal reports, which 
may be supported by the fact that these reports are 
used to document findings from tests conducted 
during the current visit. Emergency Department 
reports and Discharge Summaries contain similar 
proportions of historical conditions (17% and 19% 
respectively), which might be explained by the fact 
that both reports describe a patient?s temporal pro-
gression throughout the stay in the Emergency De-
partment or the hospital.  
Surgical Pathology reports may be the most 
temporally distinct report in our study, showing the 
highest proportion of historical conditions. This 
may seem counter-intuitive given that Surgical 
Pathology reports also facilitate the reporting of 
findings described from a recent physical speci-
men. However, we had a small sample size (28 
conditions in seven reports), and most of the his-
torical conditions were described in a single ad-
dendum report. Removing this report decreased the 
prevalence of historical conditions to 23% (3/13).  
Discharge Summaries and Emergency Depart-
ment reports displayed more variety in the ob-
served types of temporal expressions (9 to 14 
subtypes) and trigger terms (10 to 12 terms) than 
other report genres. This is not surprising consider-
ing the range of events described in these reports. 
Other reports tend to have between zero and three 
subtypes of temporal expressions and zero and 
seven different trigger terms. In all report types, 
temporal expressions were mainly subtype past, 
and the most frequent trigger term was history. 
Our second objective was to identify which fea-
tures predict whether a condition is historical or 
recent. Due to high prevalence of the recent cate-
gory, we were especially interested in discovering 
temporal features that predict whether a condition 
is historical. With one exception (date greater than 
four weeks prior to the current visit), temporal ex-
pression features always occurred in compound 
rules in which the temporal expression value had to 
co-occur with another feature value. For instance, 
any temporal expression in the category key event 
had to also occur in the secondary diagnosis sec-
tion to classify the condition as historical. For ex-
15
ample, in ?SECONDARY DIAGNOSIS: Status 
post Coronary artery bypass graft with complica-
tion of mediastinitis? the key event is the coronary 
artery bypass graft, the section is secondary diag-
nosis, and the correct classification is historical.  
Similarly, verb tense and aspect were only use-
ful in conjunction with other feature values. One 
rule predicted a condition as historical if the condi-
tion was modified by the trigger term history and 
fell within the scope of a present tense verb with 
no aspect. An example of this is ?The patient is a 
50 year old male with history of hypertension.? 
Intuitively, one would think that a past tense verb 
would always predict historical; however, we 
found the presence of a past tense verb with no 
aspect was a feature only when the condition was 
in the Patient History section.  Sometimes the ab-
sence of a verb in conjunction with another feature 
value predicted a condition as historical. For ex-
ample, in the sentences ?PAST MEDICAL 
HISTORY: History of COPD. Also diabetes?? 
also functioned as a trigger term that extended the 
scope of a previous trigger term, history, in the 
antecedent sentence.  
A few historical trigger terms were discovered 
as simple rules by the rule learners: no change, 
previous, known, status post, and history. A few 
rules incorporated both a trigger term and a partic-
ular section header value. One rule predicted his-
torical if the trigger term was status post and the 
condition occurred in the History of Present Illness 
section. This rule would classify the condition 
CABG as historical in ?HISTORY OF PRESENT 
ILLNESS: The patient is...status post CABG.? 
One important detail to note is that a number of the 
temporal expressions categorized as Fuzzy Time 
also act as trigger terms, such as history and status 
post?both of which were learned by J48. A histor-
ical trigger term did not always predict the catego-
ry historical. In the sentence ?No focal sensory or 
motor deficits on history,? history may suggest that 
the condition was not previously documented, but 
was interpreted as not presently identified during 
the current physical exam.   
Finally, sections appeared in the majority of 
JRip and RL historical rules: 4/8 simple rules and 
13/18 compound rules. A few sections were con-
sistently classified as historical: Past Medical His-
tory, Allergies, and Social History.  One important 
point to address is that these sections were manual-
ly annotated.  
Our results revealed a few unexpected observa-
tions. We found at least two trigger terms indicated 
in the J48 rules, also and status post, which did not 
have the same predictive ability across report ge-
nres.  For instance, in the statement ?TRANSFER 
DIAGNOSIS: status post coiling for left posterior 
internal carotid artery aneurysm,? status post indi-
cates the reason for the transfer as an inpatient 
from the Emergency Department and the condition 
is recent. In contrast, status post in a Surgical Pa-
thology report was interpreted to mean historical 
(e.g., PATIENT HISTORY: Status post double 
lung transplant for COPD.) In these instances, 
document knowledge of the meaning of the section 
may be useful to resolve these cases.  
One other unexpected finding was that the trig-
ger term chronic was predictive of recent rather 
than historical. This may seem counterintuitive; 
however, in the statement ?We are treating this as 
chronic musculoskeletal pain with oxycodone?, the 
condition is being referenced in the context of the 
reason for the current visit. Contextual information 
surrounding the condition, in this case treating or 
administering medication for the condition, may 
help discriminate several of these cases.  
Our third objective was to assess ConText in re-
lation to the rules learned from manually annotated 
temporal features. J48 and ConText emphasized 
the use of trigger terms as predictors of whether a 
condition was historical or recent and performed 
with roughly the same overall accuracy. JRip and 
RL learned rules that incorporated other feature 
values including sections and temporal expres-
sions, resulting in a 12% increase in historical re-
call over ConText and a 31% increase in historical 
recall over J48. 
Many of the rules we learned can be easily ex-
tracted and incorporated into ConText (e.g., trigger 
terms previous and no change). The ConText algo-
rithm largely relies on the use of trigger terms like 
history and one section header, Past Medical His-
tory. By incorporating additional section headers 
that may strongly predict historical, ConText could 
potentially predict a condition as historical when a 
trigger term is absent and the header title is the 
only predictor as in the case of ?ALLERGIES: 
peanut allergy?. Although these sections header 
may only be applied to Emergency Department 
and Discharge Summaries, trigger terms and tem-
poral expressions may be generalizable across ge-
nre of reports.  Some rules do not lend themselves 
16
to ConText?s trigger-term-based approach, particu-
larly those that require sophisticated representation 
and reasoning. For example, ConText only reasons 
some simple durations like several day history. 
ConText cannot compute dates from the current 
visit to reason that a condition occurred in the past 
(e.g., stroke in March 2000).  The algorithm per-
formance would gain from such a function; how-
ever, such a task would greatly add to its 
complexity.   
8 Limitations 
The small sample size of reports and few condi-
tions found in three report genres (Operative Ga-
strointestinal, Radiology, and Surgical Pathology) 
is a limitation in this study. Also, annotation of 
conditions, temporal category, sections, verb tense 
and aspect were conducted by a single author, 
which may have introduced bias to the study. Most 
studies on temporality in text focus on the temporal 
features themselves. For instance, the prevalence 
of temporal expressions reported by Zhou et al 
(2006) include all temporal expressions found 
throughout a discharge summary, whereas we an-
notated only those expressions that modified the 
condition. This difference makes comparing our 
results to other published literature challenging.  
9 Future Work  
Although our results are preliminary, we be-
lieve our study has provided a few new insights 
that may help improve the state of the art for his-
torical categorization of a condition. The next step 
to building on this work includes automatically 
extracting the predictive features identified by the 
rule learners. Some features may be easier to ex-
tract than others. Since sections appear to be strong 
indicators for historical categorization we may start 
by implementing the SecTag tagger. Often a sec-
tion header does not exist between text describing 
the past medical history and a description of the 
current problem, so relying merely on the section 
heading is not sufficient. The SecTag tagger identi-
fies both implicit and explicit sections and may 
prove useful for this task. To our knowledge, Sec-
Tag was only tested on Emergency Department 
reports, so adapting it to other report genres will be 
necessary. Both JRip and RL produced high per-
formance, suggesting a broader set of features may 
improve historical classification; however, because 
these features do not result in perfect performance, 
there are surely other features necessary for im-
proving historical classification. For instance, hu-
mans use medical knowledge about conditions that 
are inherently chronic or usually experienced over 
the course of a patient?s life (i.e., HIV, social ha-
bits like smoking, allergies etc). Moreover, physi-
cians are able to integrate knowledge about chronic 
conditions with understanding of the patient?s rea-
son for visit to determine whether a chronic condi-
tion is also a recent problem. An application that 
imitated experts would need to integrate this type 
of information. We also need to explore adding 
features captured at the discourse level, such as 
nominal and temporal coreference. We have begun 
work in these areas and are optimistic that they 
will improve historical categorization.  
10 Conclusion 
Although most conditions in six clinical report ge-
nres are recent problems, identifying those that are 
historical is important in understanding a patient?s 
clinical state. A simple algorithm that relies on lex-
ical cues and simple temporal expressions can 
classify the majority of historical conditions, but 
our results indicate that the ability to reason with 
temporal expressions, to recognize tense and as-
pect, and to place conditions in the context of their 
report sections will improve historical classifica-
tion. We will continue to explore other features to 
predict historical categorization. 
 
Acknowledgments 
 
This work was funded by NLM grant 1 
R01LM009427-01, ?NLP Foundational Studies 
and Ontologies for Syndromic Surveillance from 
ED Reports?.  
References 
 
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee, 
and Regina Barzilay. 2006. Finding Temporal Order 
in Discharge Summaries. AMIA Annu Symp Proc. 
2006; 81?85 
Wendy W Chapman, David Chu, and John N. Dowling. 
2007. ConText: An Algorithm for Identifying Contex-
tual Features from Clinical Text. Association for 
Computational Linguistics, Prague, Czech Republic 
17
Scott H. Clearwater and Foster J. Provost. 1990. RL4: A 
Tool for Knowledge-Based Induction. Tools for Ar-
tificial Intelligence, 1990. Proc of the 2nd Intern 
IEEE Conf: 24-30. 
Joshua C. Denny, Randolph A. Miller, Kevin B. John-
son, and Anderson Spickard III. 2008. Development 
and Evaluation of a Clinical Note Section Header 
Terminology. SNOMED. AMIA 2008 Symp. Pro-
ceedings: 156-160. 
Brian Hazlehurst, H. Robert Frost, Dean F. Sittig, and 
Victor J. Stevens. 2005. MediClass: A system for de-
tecting and classifying encounter-based clinical 
events in any electronic medical record. J Am Med 
Inform Assoc 12(5): 517-29 
Ann K. Irvine, Stephanie W. Haas, and Tessa Sullivan. 
2008. TN-TIES: A System for Extracting Temporal 
Information from Emergency Department Triage 
Notes. AMIA 2008 Symp Proc: 328-332. 
Roser Saur??, Jessica Littman, Bob Knippen, Robert 
Gaizauskas, Andrea Setzer, and James Pustejovsky. 
2006. TimeML Annotation Guidelines Version 1.2.1. 
at: 
http://www.timeml.org/site/publications/timeMLdocs
/annguide_1.2.1.pdf 
Marc Verhagen and James Pustejovsky. 2008. Temporal 
Processing with TARSQI Toolkit. Coling 2008: Com-
panion volume ? Posters and Demonstrations, Man-
chester, 189?192 
Ian H. Witten and Eibe Frank. 2005. Data Mining: 
Practical machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco, 2005. 
Li Zhou, Genevieve B. Melton, Simon Parsons and 
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clini-
cal narrative. J Biomed Inform 39(4): 424-439. 
Li Zhou and George Hripcsak. 2007. Temporal reason-
ing with medical data--a review with emphasis on 
medical natural language processing. J Biomed In-
form Apr; 40(2):183-202. 
Li Zhou, Simon Parson, and George Hripcsak. 2008. 
The Evaluation of a Temporal Reasoning System in 
Processing Discharge Summaries. J Am Med Inform 
Assoc 15(1): 99?106.  
 
18
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 58?66,
Beijing, August 2010
Developing a Biosurveillance Application Ontology for
Influenza-Like-Illness
Mike Conway, John Dowling and Wendy Chapman
Department of Biomedical Informatics
University of Pittsburgh
{conwaym|dowling|wec6}@pitt.edu
Abstract
Increasing biosurveillance capacity is a
public health priority in both the devel-
oped and the developing world. Effec-
tive syndromic surveillance is especially
important if we are to successfully iden-
tify and monitor disease outbreaks in their
early stages. This paper describes the
construction and preliminary evaluation
of a syndromic surveillance orientated ap-
plication ontology designed to facilitate
the early identification of Influenza-Like-
Illness syndrome from Emergency Room
clinical reports using natural language
processing.
1 Introduction and Motivation
Increasing biosurveillance capacity is a public
health priority in both the developed and devel-
oping world, both for the early identification of
emerging diseases and for pinpointing epidemic
outbreaks (Chen et al, 2010). The 2009 Mexican
flu outbreak provides an example of how an out-
break of a new disease (in this case a new vari-
ant of H1N1 influenza) can spend some weeks
spreading in a community before it is recognized
as a threat by public health officials.
Syndromic surveillance is vital if we are to de-
tect outbreaks at an early stage (Henning, 2004;
Wagner et al, 2006). The United States Cen-
ter for Disease Control (CDC) defines syndromic
surveillance as ?surveillance using health-related
data that precede diagnosis and signal a sufficient
probability of a case or outbreak to warrant fur-
ther public health response.?1 That is, the focus of
1www.webcitation.org/5pxhlyaxX
syndromic surveillance is the identification of dis-
ease outbreaks before the traditional public health
apparatus of confirmatory laboratory testing and
official diagnosis can be used. Data sources for
syndromic surveillance have included, over the
counter pharmacy sales (Tsui et al, 2003), school
absenteeism records (Lombardo et al, 2003), calls
to NHS Direct (a nurse led information and advice
service in the United Kingdom) (Cooper, 2007),
and search engine queries (Eysenbach, 2006).
However, in this paper we concentrate on min-
ing text based clinical records for outbreak data.
Clinical interactions between health workers and
patients generate large amounts of textual data ?
in the form of clinical reports, chief complaints,
and so on ? which provide an obvious source of
pre-diagnosis information. In order to mine the
information in these clinical reports we are faced
with two distinct problems:
1. How should we define a syndrome of inter-
est? That is, how are signs and symptoms
mapped to syndromes?
2. Given that we have established such a set
of mappings, how then do we map from the
text in our clinical reports to the signs and
symptoms that constitute a syndrome, given
the high level of terminological variability in
clinical reports.
This paper presents an application ontology that
attempts to address both these issues for the do-
main of Influenza-Like-Illness Syndrome (ILI).
The case definition for ILI, as defined by the
United States Center for Disease Control is ?fever
greater than or equal to 100 degrees Fahrenheit
58
and either cough or sore throat.?2 In contrast
to the CDC?s straightforward definition, the syn-
drome is variously described as a cluster of symp-
toms and findings, including fever and cold symp-
toms, cough, nausea, vomiting, body aches and
sore throat (Scholer, 2004). In constructing an ap-
plication specific syndrome definition for this on-
tology, we used a data driven approach to defining
ILI, generating a list of terms through an analysis
of Emergency Room reports.
The remainder of the paper is divided into five
parts. First, we briefly describe related work, be-
fore going on to report on the ontology develop-
ment process. We then set forth an evaluation of
the ontology with respect to its coverage of terms
in the target domain. We go on to outline areas for
future work, before finally presenting some con-
cluding comments.
2 Related Work
In recent years there has been significant progress
in interfacing lexical resources (in particular
WordNet (Miller, 1995)) and upper level ontolo-
gies (like the Descriptive Ontology for Linguistic
and Cognitive Engineering (DOLCE) (Gangemi
et al, 2002) and the Suggested Upper Merged On-
tology (SUMO) (Niles and Pease, 2003)). How-
ever, as our domain of interest employs a highly
specialized terminology, the use of general lin-
guistic resources like WordNet was inappropriate.
Our work has focused on the representation of
ILI relevant concepts that occur in clinical re-
ports in order to facilitate syndromic surveillance.
While the widely used medical taxonomies and
nomenclatures (for example Unified Medical Lan-
guage System3 and the Systematized Nomencla-
ture of Medicine Clinical Terms4) contain many
of the ILI relevant concepts found in clinical texts,
these general resources do not have the specific re-
lations (and lexical information) relevant to syn-
dromic surveillance from clinical reports. Cur-
rently, there are at least four major terminological
resources available that focus on the public health
domain: PHSkb, SSO, and the BioCaster Ontol-
ogy.
2www.webcitation.org/5q22KTcHx
3www.nlm.nih.gov/research/umls/
4www.ihtsdo.org/snomed-ct/
2.1 PHSkb
The Public Health Surveillance knowledge base
PHSkb (Doyle et al, 2005) developed by the CDC
is a coding system for the communication of no-
tifiable disease5 findings for public health profes-
sionals at the state and federal level in the United
States. There are however several difficulties in
using the PHSkb directly in an NLP orientated
syndromic surveillance context:
1. Syndromic surveillance requires that syn-
dromes and signs are adequately represented.
The PHSkb emphasizes diagnosed diseases.
That is, the PHSKb is focused on post diag-
nosis reporting, when laboratory tests have
been conducted and the presence of a disease
is confirmed. This approach is not suitable
for syndromic surveillance where we seek to
identify clusters of symptoms and signs be-
fore a diagnosis.
2. PHSkb is no longer under active develop-
ment.
2.2 SSO
The Syndromic Surveillance Ontology (SSO)
(Okhmatovskaia et al, 2009) was developed to
address a pressing problem for system develop-
ers and public health officials. How can we inte-
grate outbreak information when every site uses
different syndrome definitions? For instance, if
State X defines sore throat as part of ILI, yet State
Y does not, syndromic surveillance results from
each state will not be directly comparable. When
we apply this example to the wider national scene,
with federal regional and provincial public health
agencies attempting to share data with each other,
and international agencies, we can see the scale of
the problem to be addressed.
In order to manage this data sharing problem,
a working group of eighteen researchers, repre-
senting ten functional syndromic surveillance sys-
tems in the United States (for example, Boston
Public Health Department and the US Depart-
ment of Defense) convened to develop standard
5A notifiable disease is a disease (or by extension, con-
dition) that must, by law, be reported to the authorities for
monitoring purposes. In the United States, examples of noti-
fiable diseases are: Shigellosis, Anthrax and HIV infection.
59
definitions for four syndromes of interest (respi-
ratory, gastro-intestinal, constitutional and ILI)6
and constructed an OWL ontology based on these
definitions. While the SSO is a useful starting
points, there are several reasons why ? on its own
? it is insufficient for clinical report processing:
1. SSO is centered on chief complaints. Chief
complaints (or ?presenting complaints?) are
phrases that briefly describe a patient?s pre-
senting condition on first contact with a med-
ical facility. They usually describe symp-
toms, refrain from diagnostic speculation
and employ frequent abbreviations and mis-
spellings (for example ?vom + naus? for
?vomiting and nausea?). Clinical texts ?
the focus of attention in this paper ? are
full length documents, normally using cor-
rect spellings (even if they are somewhat
?telegraphic? in style). Furthermore, clini-
cal reports frequently list physical findings
(that is, physical signs elicited by the physi-
cian, like, for instance reflex tests) which are
not present in symptom orientated chief com-
plaints.
2. The range of syndromes represented in SSO
is limited to four. Although we are starting
out with ILI, we have plans (and data) to ex-
tend our resource to four new syndromes (see
Section 5 for details of further work).
3. The most distinctive feature of the SSO is
that the knowledge engineering process was
conducted in a face-to-face committee con-
text. Currently, there is no process in place
to extend the SSO to new syndromes, symp-
toms or domains.
2.3 BioCaster Ontology
The BioCaster application ontology was built to
facilitate text mining of news articles for disease
outbreaks in several different Pacific Rim lan-
guages (including English, Japanese, Thai and
Vietnamese) (Collier et al, 2006). However, the
6A demonstration chief complaint classifier based on
SSO is available at:
http://onto-classifier.dbmi.pitt.edu
/onto classify.html
ontology, as it stands, is not suitable for support-
ing text mining clinical reports, for the following
reasons:
1. The BioCaster ontology concentrates on the
types of concepts found in published news
outlets for a general (that is, non medical)
readership. The level of conceptual granular-
ity and degree of terminological sophistica-
tion is not always directly applicable to that
found in documents produced by health pro-
fessionals.
2. The BioCaster ontology, while it does repre-
sent syndromes (for example, constitutional
and hemorrhagic syndromes) and symptoms,
does not represent physical findings, as these
are beyond its scope.
In addition to the application ontologies de-
scribed above, the Infectious Disease Ontology
provides an Influenza component (and indeed
wide coverage of many diseases relevant to syn-
dromic surveillance). In Section 5 we describe
plans to link to other ontologies.
3 Constructing the Ontology
Work began with the identification of ILI terms
from clinical reports by author JD (a board-
certified infectious disease physician with thirty
years experience of clinical practice) supported by
an informatician [author MC]. The term identifi-
cation process involved the project?s domain ex-
pert reading multiple reports,7 searching through
appropriate textbooks, and utilizing professional
knowledge. After a provisional list of ILI con-
cepts had been identified, we compared our list
to the list of ILI concepts generated by the SSO
ILI component (see Section 2.2) and attempted to
reuse SSO concepts where possible. The resulting
ILI concept list consisted of 40 clinical concepts
taken from SSO and 15 new concepts. Clinical
concepts were divided into three classes: Disease
(15 concepts), Finding (21 concepts) and Symp-
tom (19 concepts). Figure 1 shows the clinical
7De-identified (that is, anonymized) clinical reports were
obtained through partnership with the University of Pitts-
burgh Medical Center.
60
concepts covered. As part of our knowledge en-
gineering effort, we identified concepts and as-
sociated relations for several different syndromes
which we plan to add to our ontology at a later
date.8
Early on in the project development process, we
took the decision to design our ontology in such a
way as to maintain consistency with the BioCaster
ontology. We adopted the BioCaster ontology as
a model for three reasons:
1. A considerable knowledge engineering effort
has been invested in BioCaster since 2006,
and both the domain (biosurveillance) and
application area (text mining) are congruent
to our own.
2. The BioCaster ontology has proven utility in
its domain (biosurveillance from news texts)
for driving NLP systems.
3. We plan to import BioCaster terms and re-
lations, and thus settled on a structure that
facilitated this goal.
The BioCaster ontology (inspired by the struc-
ture of EuroWordNet9) uses root terms as interlin-
gual pivots for the multiple languages represented
in the ontology.10 One consequence of following
this structure is that all clinical concepts are in-
stances.11 Additionally, all specified relations are
relations between instances.
Relations relevant to the syndromic surveil-
lance domain generally were identified by our
physician in conjunction with an informatician
(MC). Although some of these relations (like
is bioterrorismDisease) are less relevant
to ILI syndrome, they were retained in order to
maintain consistency with planned future work.
Additionally, we have added links to other ter-
minological resources (for example, UMLS and
Snomed-CT)
8Note that finer granularity was used in the initial knowl-
edge acquisition efforts (for example, we distinguished sign
from physical finding).
9http://www.illc.uva.nl/EuroWordNet/
10Note that we are using root term instead of the equivalent
EuroWordNet term Inter Lingual Index.
11Note that from a formal ontology perspective, concepts
are instantiated in text. For example, ?Patient X presents with
nausea and high fever? instantiates the concepts nausea and
high fever.
Lexical resources and regular expressions are
a vital component of our project, as the ontology
has been built with the public health audience in
mind (in practice, state or city public health IT
personnel). These users have typically had lim-
ited exposure to NLP pipelines, named entity rec-
ognizers, and so on. They require an (almost) ?off
the shelf? product that can easily be plugged into
existing systems for text analysis.
The ontology currently includes 484 English
keywords and 453 English regular expression.
The core classes and relations were developed in
Protege-OWL, and the populated ontology is gen-
erated from data stored in a spreadsheet (using a
Perl script). Version control was managed using
Subversion, and the ontology is available from a
public access Google code site.12 Figure 2 pro-
vides a simplified example of relations for the
clinical concept instance fever.
4 Evaluation
In recent years, significant research effort has
centered on the evaluation of ontologies and
ontology-like lexical resources, with a smorgas-
bord of techniques available (Zhu et al, 2009;
Brank et al, 2005). Yet no single evaluation
method has achieved ?best practice? status for all
contexts. As our ontology is an application on-
tology designed to facilitate NLP in a highly con-
strained domain (that is, text analysis and infor-
mation extraction from clinical reports) the notion
of coverage is vital. There are two distinct ques-
tions here:
1. Can we map between the various textual in-
stantiations of ILI concepts clinical reports
and our ontology concepts? That is, are
the NLP resources available in the ontology
(keywords, regular expressions) adequate for
the mapping task?
2. Do we have the right ILI concepts in our on-
tology? That is, do we adequately represent
all the ILI concepts that occur in clinical re-
ports?
Inspired by Grigonyte et al (2010), we at-
tempted to address these two related issues using
12http://code.google.com/p/ss-ontology
61
ClinicalConcept
Disease
SymptomFinding
Instances:
 - athma
 - bronchiolitis
 - croup
 - ili
 - influenza
 - pertussis
 - pharyngitis
 - pneumonia
 - pneumonitis
 - reactiveAirways
 - respiratorySyncytialVirus
Instances:
 - chill
 - conjunctivitis
 - coryza
 - cyanosis
 - dyspnea
 - elevatedTemperature
 - failureToThrive
 - fever
 - hemoptysis
 - infiltrate
 - lethargy
 - nasalObstruction
 - persistentNonProductiveCough
 - photophobia
 - rales
 - rhinorrhea
 - rigor
 - somnolent
 - throatSwelling
 - wheezing
Instances:
 - anorexia
 - arthralgia
 - asthenia
 - bodyAche
 - coldSymptom
 - cough
 - diarrhea
 - fatigue
 - generalizedMuscleAche
 - headache
 - hoarseness
 - malaise
 - myalgia
 - nausea
 - painOnEyeMovement
 - productiveCough
 - soreThroat
 - substernalDiscomfortOrBurning
 - viralSymptom
 
is_a
is_a
is_a
Figure 1: Clinical concepts.
techniques derived from terminology extraction
and corpus linguistics. Our method consisted of
assembling a corpus of twenty Emergency Room
clinical reports which had been flagged by ex-
perts (not the current authors) as relevant to ILI.
Note that these articles were not used in the initial
knowledge engineering phase of the project. We
then identified the ?best? twenty five terms from
these clinical reports using two tools, Termine and
KWExT.
1. Termine (Frantzi et al, 2000) is a term ex-
traction tool hosted by Manchester Univer-
sity?s National Centre for Text Mining which
can be accessed via web services.13 It uses
a method based on linguistic preprocessing
and statistical methods. We extracted 231
terms from our twenty ILI documents (using
Termine?s default configuration). Then we
identified the twenty-five highest ranked dis-
ease, finding and symptom terms (that is, dis-
carding terms like ?hospital visit? and ?chief
complaint?).
13www.nactem.ac.uk/software/termine/
2. KWExT (Keyword Extraction Tool) (Con-
way, 2010) is a Linux based statistical key-
word extraction tool.14 We used KWExT
to extract 1536 unigrams, bigrams and tri-
grams using the log-likelihood method (Dun-
ning, 1993). The log-likelihood method is
designed to identify n-grams that occur with
the most frequency compared to some ref-
erence corpus. We used the FLOB cor-
pus,15 a one million multi-genre corpus con-
sisting of American English from the early
1990s as our reference corpus. We ranked
all n-grams according to their statistical sig-
nificance and then manually identified the
twenty-five highest ranked disease, finding
and symptom terms.
Term lists derived using the Termine and
KWExT tools are presented in Tables 1 and 2 re-
spectively. For both tables, column two (?Term?)
details each of the twenty-five ?best? terms (with
respect to each term recognition algorithm) ex-
14http://code.google.com/p/kwext/
15www.webcitation.org/5q1aKtnf3
62
Thing
ClinicalConcept
Syndrome
Keyword
Link
Regular
Expression
SymptomFinding
Disease
UmlsLink
English
Keyword
EnglishRegular
Expression
ILI
fever
elevated
Temperature
chill
"febrile"
"fever"
\bfiebre\b
\bfeel.*?\s+hot\b
is_a
is_a
is_a
is_a
is_a
is_a
is_a
is_a
i
is_a
is_a
is_a
instance
instance
instance
instance
instance
fever
instance
instance class
is_a
(class to class)
instance
(instance of a class)
relation
(instance to instance relation)
hasAssociatedSyndrome
hasKeyword
hasKeyword
isSynonymous
hasRegularExpression
hasRegularExpression
hasLink
isRelatedTo
instance
instance
instance
Figure 2: Example of clinical concept ?fever? and its important relations (note the diagram is simpli-
fied).
tracted from our twenty document ILI corpus.
Column three (?Concept?) specifies the concept in
our ontology to which the term maps (that is, the
lexical resources in the ontology ? keywords and
regular expressions ? can map the term in col-
umn two to the clinical concept in column three).
For instance the extracted term slight crackles can
be mapped to the clinical concept RALE using the
keyword ?crackles.? Note that ?-? in column three
indicates that no mapping was possible. Under-
lined terms are those that should be mapped to
concepts in the ontology, but currently are not (ad-
ditional concepts and keywords will be added in
the next iteration of the ontology).
There are two ways that mappings can fail here
(mirroring the two questions posed at the begin-
ning of this section). ?Shortness of breath? should
map to the concept DYSPNEA, but there is no key-
word or regular expression that can bridge be-
tween text and concept. For the terms ?edema?
and ?lymphadenopathy? however, no suitable can-
didate concept exists in the ontology.
5 Further Work
While the current ontology covers only ILI, we
have firm plans to extend the current work along
several different dimensions:
? Developing new relations, to include model-
ing DISEASE ? SYMPTOM, and DISEASE
? FINDING relations (for example TONSIL-
LITIS hasSymptom SORE THROAT).
? Extend the application ontology beyond ILI
to several other syndromes of interest to the
biosurveillance community. These include:
? Rash Syndrome
? Hemorrhagic Syndrome
? Botulic Syndrome
? Neurological Syndrome
? Currently, we have links to UMLS (and also
Snomed-CT and BioCaster). We intend to
extend our coverage to the MeSH vocabu-
lary (to facilitate mining PubMed) and also
the Infectious Disease Ontology.
63
Term Concept
1 abdominal pain -
2 chest pain -
3 urinary tract infection -
4 sore throat SORE THROAT
5 renal disease -
6 runny nose CORYZA
7 body ache MYALGIA
8 respiratory distress PNEUMONIA
9 neck stiffness -
10 yellow sputum -
11 mild dementia -
12 copd -
13 viral syndrome VIRAL SYN.
14 influenza INFLUENZA
15 febrile illness FEVER
16 lung problem -
17 atrial fibrillation -
18 severe copd -
19 mild cough COUGH
20 asthmatic bronchitis BRONCHIOLITIS
21 coronary disease -
22 dry cough COUGH
23 neck pain -
24 bronchial pneumonia PNEUMONIA
25 slight crackles RALE
Table 1: Terms generated using the Termine tool
? Currently evaluation strategies have concen-
trated on coverage. We plan to extend our
auditing to encompass both intrinsic evalu-
ation (for example, have our relations eval-
uated by external health professionals using
some variant of the ?laddering? technique
(Bright et al, 2009)) and extrinsic evaluation
(for example, plugging the application ontol-
ogy into an NLP pipeline for Named Entity
Recognition and evaluating its performance
in comparison to other techniques).
In addition to these ontology development and
evaluation goals, we intend to use the ontology as
a ?gold standard? against which to evaluate au-
tomatic term recognition and taxonomy construc-
tion techniques for the syndromic surveillance do-
main. Further, we seek to integrate the resulting
ontology with the BioCaster ontology allowing
the potential for limited interlingual processing in
priority languages (in the United States, Spanish).
Currently we are considering two ontology in-
tegration strategies. First, using the existing map-
pings we have created between the ILI ontology
and BioCaster to access multi-lingual information
(using OWL datatype properties). Second, fully
Term Concept
1 cough COUGH
2 fever FEVER
3 pain -
4 shortness of breath -
5 vomiting -
6 influenza INFLUENZA
7 pneumonia PNEUMONIA
8 diarrhea DIARRHEA
9 nausea NAUSEA
10 chills CHILL
11 abdominal pain -
12 chest pain -
13 edema -
14 cyanosis CYANOSIS
15 lymphadenopathy -
16 dysuria -
17 dementia -
18 urinary tract inf -
19 sore throat SORE THROAT
20 wheezing WHEEZING
21 rhonchi -
22 bronchitis BRONCHIOLITIS
23 hypertension -
24 tachycardia -
25 respiratory distress PNEUMONIA
Table 2: Terms generated using the KWExT tool
integrating ? that is, merging ? the two on-
tologies and creating object property relations be-
tween them.
For example (using strategy 1), we could move
from the string ?flu? in a clinical report (iden-
tified by the \bflu\b regular expression) to
the ILI ontology concept ili:influenza. In
turn, ili:influenza could be linked (using
a datatype property) to the BioCaster root term
biocaster:DISEASE 378 (which has the la-
bel ?Influenza (Human).?) From the BioCaster
root term, we can ? for example ? generate the
translation ?Gripe (Humano)? (Spanish).
6 Conclusion
The ILI application ontology developed from the
need for knowledge resources for the text mining
of clinical documents (specifically, Emergency
Room clinical reports). Our initial evaluation in-
dicates that we have good coverage of our domain,
although we plan to incrementally work on im-
proving any gaps in coverage through a process of
active and regular updating. We have described
our future plans to extend the ontology to new
syndromes in order to provide a general commu-
64
nity resource to facilitate data sharing and inte-
gration in the NLP based syndromic surveillance
domain. Finally, we actively solicit feedback on
the design, scope and accuracy of the ontology.
Acknowledgments
This project was partially funded by Grant Num-
ber 3-R01-LM009427-02 (NLM) from the United
States National Institute of Health.
References
Brank, J., Grobelnik, M., and Mladenic?, D.
(2005). A Survey of Ontology Evaluation Tech-
niques. In Proceedings of the Conference on
Data Mining and Data Warehouses (SiKDD
2005), pages 166?170.
Bright, T., Furuya, E., Kuperman, G., and Bakken,
S. (2009). Laddering as a Technique for On-
tology Evaluation. In American Medical Infor-
matics Symposium (AMIA 2009).
Chen, H., Zeng, D., and Dang, Y. (2010). Infec-
tious Disease Informatics: Syndromic Surveil-
lance for Public Health and Bio-Defense.
Springer, New York.
Collier, N., Shigematsu, M., Dien, D., Berrero,
R., Takeuchi, K., and Kawtrakul, A. (2006).
A Multilingual Ontology for Infectious Dis-
ease Surveillance: Rationale, Design and Chal-
lenges. Language Resources and Evaluation,
40(3):405?413.
Conway, M. (2010). Mining a Corpus of Bio-
graphical Texts Using Keywords. Literary and
Linguistic Computing, 25(1):23?35.
Cooper, D. (2007). Disease Surveillance: A Pub-
lic Health Informatics Approach, chapter Case
Study: Use of Tele-health Data for Syndromic
Surveillance in England and Wales, pages 335?
365. Wiley, New York.
Doyle, T., Ma, H., Groseclose, S., and Hopkins,
R. (2005). PHSkb: A Knowledgebase to Sup-
port Notifiable Disease Surveillance. BMC Med
Inform Decis Mak, 5:27.
Dunning, T. (1993). Accurate Methods for the
Statistics of Surprise and Coincidence. Com-
putational Linguistics, 19(1):61?74.
Eysenbach, G. (2006). Infodemiology: Track-
ing Flu-Related Searches on the Web for Syn-
dromic Surveillance. In American Medical In-
formatics Association Annual Symposium Pro-
ceedings (AMIA 2006), pages 244?248.
Frantzi, K., Ananiadou, S., and Mima, H.
(2000). Automatic Recognition for Multi-word
Terms. International Journal of Digital Li-
braries, 3(2):117?132.
Gangemi, A., Guarino, N., Masolo, C., Oltramari,
A., and Schneider, L. (2002). Sweetening On-
tologies with DOLCE. In Proceedings of the
13th International Conference on Knowledge
Engineering and Knowledge Management. On-
tologies and the Semantic Web, pages 166?181.
Grigonyte, G., Brochhausen, M., Martin, L., Tsik-
nakis, M., and Haller, J. (2010). Evaluating
Ontologies with NLP-Based Terminologies - A
Case Study on ACGT and its Master Ontol-
ogy. In Formal Ontology in Information Sys-
tems: Proceedings of the Sixth International
Conference (FOIS 2010), pages 331?344.
Henning, K. (2004). What is Syndromic Surveil-
lance? MMWR Morb Mortal Wkly Rep, 53
Suppl:5?11.
Lombardo, J., Burkom, H., Elbert, E., Ma-
gruder, S., Lewis, S. H., Loschen, W., Sari,
J., Sniegoski, C., Wojcik, R., and Pavlin, J.
(2003). A Systems Overview of the Electronic
Surveillance System for the Early Notification
of Community-Based Epidemics (ESSENCE
II). J Urban Health, 80(2 Suppl 1):32?42.
Miller, G. (1995). WordNet: A Lexical Database
for English. Communications of the Associa-
tion for Computing Machinary, 38(11):39?41.
Niles, I. and Pease, A. (2003). Linking Lexicons
and Ontologies: Mapping WordNet to the Sug-
gested Upper Merged Ontology. In Proceed-
ings of the 2003 International Conference on
Information and Knowledge Engineering (IKE
03), pages 23?26.
Okhmatovskaia, A., Chapman, W., Collier, N.,
Espino, J., and Buckeridge, D. (2009). SSO:
The Syndromic Surveillance Ontology. In Pro-
ceedings of the International Society for Dis-
ease Surveillance.
65
Scholer, M. (2004). Development of a Syndrome
Definition for Influenza-Like-Illness. In Pro-
ceedings of American Public Health Associa-
tion Meeting (APHA 2004).
Tsui, F., Espino, J., Dato, V., Gesteland, P., Hut-
man, J., and Wagner, M. (2003). Technical De-
scription of RODS: a Real-Time Public Health
Surveillance System. J Am Med Inform Assoc,
10(5):399?408.
Wagner, M., Gresham, L., and Dato, V. (2006).
Handbook of Biosurveillance, chapter Case
Detection, Outbreak Detection, and Outbreak
Characterization, pages 27?50. Elsevier Aca-
demic Press.
Zhu, X., Fan, J.-W., Baorto, D., Weng, C., and
Cimino, J. (2009). A Review of Auditing
Methods Applied to the Content of Controlled
Biomedical Terminologies. Journal of Biomed-
ical Informatics, 42(3):413 ? 425.
66
