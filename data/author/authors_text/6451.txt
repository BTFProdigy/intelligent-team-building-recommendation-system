Proceedings of the Linguistic Annotation Workshop, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Annotation Type System for a Data-Driven NLP Pipeline
Udo Hahn Ekaterina Buyko Katrin Tomanek
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{hahn|buyko|tomanek}@coling-uni-jena.de
Scott Piao John McNaught Yoshimasa Tsuruoka Sophia Ananiadou
NaCTeM and School of Computer Science
University of Manchester
{scott.piao|john.mcnaught|yoshimasa.tsuruoka|sophia.ananiadou}@manchester.ac.uk
Abstract
We introduce an annotation type system for
a data-driven NLP core system. The specifi-
cations cover formal document structure and
document meta information, as well as the
linguistic levels of morphology, syntax and
semantics. The type system is embedded in
the framework of the Unstructured Informa-
tion Management Architecture (UIMA).
1 Introduction
With the maturation of language technology, soft-
ware engineering issues such as re-usability, in-
teroperability, or portability are getting more and
more attention. As dozens of stand-alone compo-
nents such as tokenizers, stemmers, lemmatizers,
chunkers, parsers, etc. are made accessible in vari-
ous NLP software libraries and repositories the idea
sounds intriguing to (re-)use them on an ?as is? basis
and thus save expenditure and manpower when one
configures a composite NLP pipeline.
As a consequence, two questions arise. First, how
can we abstract away from the specific code level of
those single modules which serve, by and large, the
same functionality? Second, how can we build NLP
systems by composing them, at the abstract level
of functional specification, from these already ex-
isting component building blocks disregarding con-
crete implementation matters? Yet another burning
issue relates to the increasing availability of multiple
metadata annotations both in corpora and language
processors. If alternative annotation tag sets are cho-
sen for the same functional task a ?data conversion?
problem is created which should be solved at the ab-
stract specification level as well (Ide et al, 2003).
Software engineering methodology points out that
these requirements are best met by properly identi-
fying input/output capabilities of constituent compo-
nents and by specifying a general data model (e.g.,
based on UML (Rumbaugh et al, 1999)) in or-
der to get rid of the low-level implementation (i.e.,
coding) layer. A particularly promising proposal
along this line of thought is the Unstructured Infor-
mation Management Architecture (UIMA) (Ferrucci
and Lally, 2004) originating from IBM research ac-
tivities.1 UIMA is but the latest attempt in a series
of proposals concerned with more generic NLP en-
gines such as ATLAS (Laprun et al, 2002) or GATE
(Cunningham, 2002). These frameworks have in
common a data-driven architecture and a data model
based on annotation graphs as an adaptation of the
TIPSTER architecture (Grishman, 1997). They suf-
fer, however, from a lack of standards for data ex-
change and abstraction mechanisms at the level of
specification languages.
This can be achieved by the definition of a com-
mon annotation scheme. We propose an UIMA
schema which accounts for a significant part of the
complete NLP cycle ? from the collection of doc-
uments and their internal formal structure, via sen-
tence splitting, tokenization, POS tagging, and pars-
ing, up until the semantic layer (still excluding dis-
course) ? and which aims at the implementation-
independent specification of a core NLP system.
1Though designed for any sort of unstructured data (text,
audio and video data), we here focus on special requirements
for the analysis of written documents.
33
2 Related work
Efforts towards the design of annotation schemata
for language resources and their standardization
have a long-standing tradition in the NLP commu-
nity. In the very beginning, this work often fo-
cused exclusively on subdomains of text analysis
such as document structure meta-information, syn-
tactic or semantic analysis. The Text Encoding Ini-
tiative (TEI)2 provided schemata for the exchange
of documents of various genres. The Dublin Core
Metadata Initiative3 established a de facto standard
for the Semantic Web.4 For (computational) lin-
guistics proper, syntactic annotation schemes, such
as the one from the Penn Treebank (Marcus et al,
1993), or semantic annotations, such as the one un-
derlying ACE (Doddington et al, 2004), are increas-
ingly being used in a quasi standard way.
In recent years, however, the NLP community is
trying to combine and merge different kinds of an-
notations for single linguistic layers. XML formats
play a central role here. An XML-based encod-
ing standard for linguistic corpora XCES (Ide et al,
2000) is based on CES (Corpus Encoding Standard)
as part of the EAGLES Guidelines.5 Work on TIGER
(Brants and Hansen, 2002) is an example for the li-
aison of dependency- and constituent-based syntac-
tic annotations. New standardization efforts such as
the Syntactic Annotation Framework (SYNAF) (De-
clerck, 2006) aim to combine different proposals and
create standards for syntactic annotation.
We also encounter a tendency towards multiple
annotations for a single corpus. Major bio-medical
corpora, such as GENIA (Ohta et al, 2002) or
PennBioIE,6 combine several layers of linguistic
information in terms of morpho-syntactic, syntac-
tic and semantic annotations (named entities and
events). In the meantime, the Annotation Compat-
ibility Working Group (Meyers, 2006) began to con-
centrate its activities on the mutual compatibility of
annotation schemata for, e.g., POS tagging, tree-
banking, role labeling, time annotation, etc.
The goal of these initiatives, however, has never
been to design an annotation scheme for a complete
2http://www.tei-c.org
3http://dublincore.org
4http://www.w3.org/2001/sw
5http://www.ilc.cnr.it/EAGLES96/
6http://bioie.ldc.upenn.edu
NLP pipeline as needed, e.g., for information ex-
traction or text mining tasks (Hahn and Wermter,
2006). This lack is mainly due to missing standards
for specifying comprehensive NLP software archi-
tectures. The MEANING format (Pianta et al, 2006)
is designed to integrate different levels of morpho-
syntactic annotations. The HEART OF GOLD mid-
dleware (Scha?fer, 2006) combines multidimensional
mark-up produced by several NLP components. An
XML-based NLP tool suite for analyzing and anno-
tating medical language in an NLP pipeline was also
proposed by (Grover et al, 2002). All these propos-
als share their explicit linkage to a specific NLP tool
suite or NLP system and thus lack a generic annota-
tion framework that can be re-used in other develop-
mental environments.
Buitelaar et al developed in the context of an in-
formation extraction project an XML-based multi-
layered annotation scheme that covers morpho-
syntactic, shallow parsing and semantic annotation
(Buitelaar et al, 2003). Their scheme borrows con-
cepts from object-oriented programming (e.g., ab-
stract types, polymorphism). The object-oriented
perspective already allows the development of a
domain-independent schema and extensions of core
types without affecting the base schema. This
schema is comprehensive indeed and covers a sig-
nificant part of advanced NLP pipelines but it is also
not connected to a generic framework.
It is our intention to come full circle within a
general annotation framework. Accordingly, we
cover a significant part of the NLP pipeline from
document meta information and formal document
structure, morpho-syntactic and syntactic analysis
up to semantic processing. The scheme we propose
is intended to be compatible with on-going work
in standardization efforts from task-specific annota-
tions and to adhere to object-oriented principles.
3 Data-Driven NLP Architecture
As the framework for our specification efforts, we
adopted the Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004). It
provides a formal specification layer based on UML,
as well as a run-time environment for the interpreta-
tion and use of these specifications. This dualism is
going to attract more and more researchers as a basis
34
for proper NLP system engineering.
3.1 UIMA-based Tool Suite
UIMA provides a platfrom for the integration
of NLP components (ANALYSIS ENGINES in the
UIMA jargon) and the deployment of complex
NLP pipelines. It is more powerful than other
prominent software systems for language engineer-
ing (e.g., GATE, ATLAS) as far as its pre- and
post-processing facilities are concerned ? so-called
COLLECTION READERS can be developed to handle
any kind of input format (e.g., WWW documents,
conference proceedings), while CONSUMERS, on
other hand, deal with the subsequent manipulation
of the NLP core results (e.g., automatic indexing).
Therefore, UIMA is a particularly suitable architec-
ture for advanced text analysis applications such as
text mining or information extraction.
We currently provide ANALYSIS ENGINES for
sentence splitting, tokenization, POS tagging, shal-
low and full parsing, acronym detection, named
entity recognition, and mapping from named enti-
ties to database term identifiers (the latter is mo-
tivated by our biological application context). As
we mainly deal with documents taken from the bio-
medical domain, our collection readers process doc-
uments from PUBMED,7 the most important liter-
ature resource for researchers in the life sciences.
PUBMED currently provides more than 16 million
bibliographic references to bio-medical articles. The
outcomes of ANALYSIS ENGINES are input for var-
ious CONSUMERS such as semantic search engines
or text mining tools.
3.2 Common Analysis System
UIMA is based on a data-driven architecture. This
means that UIMA components do not exchange or
share code, they rather exchange data only. The
components operate on common data referred to
as COMMON ANALYSIS SYSTEM (CAS)(Go?tz and
Suhre, 2004). The CAS contains the subject of anal-
ysis (document) and provides meta data in the form
of annotations. Analysis engines receive annotations
through a CAS and add new annotations to the CAS.
An annotation in the CAS then associates meta data
with a region the subject of the analysis occupies
7http://www.pubmed.gov
(e.g., the start and end positions in a document).
UIMA defines CAS interfaces for indexing, ac-
cessing and updating the CAS. CASes are modelled
independently from particular programming lan-
guages. However, JCAS, an object-oriented inter-
face to the CAS, was developed for JAVA. CASes are
crucial for the development and deployment of com-
plex NLP pipelines. All components to be integrated
in UIMA are characterized by abstract input/output
specifications, so-called capabilities. These speci-
fications are declared in terms of descriptors. The
components can be integrated by wrappers conform-
ing with the descriptors. For the integration task, we
define in advance what kind of data each component
may manipulate. This is achieved via the UIMA
annotation type system. This type system follows
the object-oriented paradigm. There are only two
kinds of data, viz. types and features. Features spec-
ify slots within a type, which either have primitive
values such as integers or strings, or have references
to instances of types in the CAS. Types, often called
feature structures, are arranged in an inheritance hi-
erarchy.
In the following section, we propose an ANNO-
TATION TYPE SYSTEM designed and implemented
for an UIMA Tool Suite that will become the back-
bone for our text mining applications. We distin-
guish between the design and implementation lev-
els, talking about the ANNOTATION SCHEME and
the TYPE SYSTEM, respectively.
4 Annotation Type System
The ANNOTATION SCHEME we propose currently
consists of five layers: Document Meta, Document
Structure & Style, Morpho-Syntax, Syntax and Se-
mantics. Accordingly, annotation types fall into five
corresponding categories. Document Meta and Doc-
ument Structure & Style contain annotations about
each document?s bibliography, organisation and lay-
out. Morpho-Syntax and Syntax describe the results
of morpho-syntactic and syntactic analysis of texts.
The results of lemmatisation, stemming and decom-
position of words can be represented at this layer, as
well. The annotations from shallow and full parsing
are represented at the Syntax layer. The appropri-
ate types permit the representation of dependency-
and constituency-based parsing results. Semantics
35
uima.tcas.Annotation
+begin: uima.cas.Integer
+end: uima.cas.Integer
Annotation
+componentId: uima.cas.String
+confidence: uima.cas.Double
Descriptor
pubmed.ManualDescriptor
+MeSHList: uma.cas.FSArray = MeSHHeading
+...
AutoDescriptor
+...
Header
+docType: uima.cas.String
+source: uima.cas.String
+docID: uima.cas.String
+language: uima.cas.String
+copyright: uima.cas.String
+authors: uima.cas.FSArray = AuthorInfo
+title: uima.cas.String
+pubTypeList: uima.cas.FSArray = PubType
+...
pubmed.Header
+citationStatus: uima.cas.String {...}
ManualDescriptor
+keywordList: uima.cas.FSArray = Keyword
+...
PubType
+name: uima.cas.Sting
Journal
+ISSN: uima.cas.String
+volume: uima.cas.String
+journalTitle: uima.cas.String
+impactFactor: uima.cas.String
Keyword
+name: uima.cas.String
+source: uima.cas.String
Token
+posTag: uima.cas.FSArray  = POSTag
+lemma: Lemma
+feats: GrammaticalFeats
+stemmedForm: StemmedForm
+depRelList: uima.cas.FSArray = DependencyRelation
+orthogr: uima.cas.FSArray = String
POSTag
+tagsetId: uima.cas.String
+language: uima.cas.String
+value: uima.cas.String
Lemma
+value: String
Acronym
Abbreviation
+expan: String
StemmedForm
+value: String
GrammaticalFeats
+language: uima.cas.String
DiscontinuousAnnotation
+value: FSArray = Annotation
PennPOSTag
NounFeats
+...
...
+...
...
+...
DependencyRelation
+head: Token
+projective: uima.cas.Boolean
+label: uima.cas.String
Relation
DepRelationSet...
Chunk
PhraseChunk
PTBConstituent
+formFuncDisc: uima.cas.String
+gramRole: uima.cas.String
+adv: uima.cas.String
+misc: uima.cas.String
+map: Constituent
+tpc: uima.cas.Boolean
+nullElement: uima.cas.String
+ref: Constituent
Constituent
+parent: Constituent
+head: Token
+cat: uima.cas.String
GENIAConstituent
+syn: uima.cas.String 
...
+...
...
+...
NP ...PP
Entity
+dbEntry: uima.cas.FSArray = DBEntry
+ontologyEntry: uima.cas.FSArray = OntologyEntry
+specificType: uima.cas.String
BioEntity
Cytokine
Organism VariationGene
...
LexiconEntry OntologyEntryDBEntry
ResourceEntry
+source: uima.cas.String
+entryId: uima.cas.String
+version: uima.cas.String
Zone 
Title TextBody Paragraph Figure
+caption: Caption
Section
+title: Title
+depth: uima.cas.Integer
Misc ... PersonOrganization
MUCEntity
...
2
3
4
5 6
1
CAS Core
Figure 1: Multi-Layered UIMA Annotation Scheme in UML Representation. 1: Basic Feature Structure and
Resource Linking. 2: Document Meta Information. 3: Morpho-Syntax. 4: Syntax. 5: Document Structure
& Style. 6: Semantics.
36
currently covers information about named entities,
events and relations between named entities.
4.1 Basic Feature Structure
All types referring to different linguistic lay-
ers derive from the basic type Annotation,
the root type in the scheme (cf. Figure 1-
1). The Annotation type itself derives infor-
mation from the default UIMA annotation type
uima.tcas.Annotation and, thus, inherits the
basic annotation features, viz. begin and end (mark-
ing spans of annotations in the subject of analysis).
Annotation extends this default feature structure
with additional features. The componentId marks
which NLP component actually computed this an-
notation. This attribute allows to manage multiple
annotations of the same type The unique linkage be-
tween an analysis component and an annotation item
is particularly relevant in cases of parallel annota-
tions. The component from which the annotation
originated also assigns a specific confidence score
to its confidence feature. Each type in the scheme is
at least supplied with these four slots inherited from
their common root type.
4.2 Document Meta Information
The Document Meta layer (cf. Figure 1-2) describes
the bibliographical and content information of a doc-
ument. The bibliographical information, often re-
trieved from the header of the analyzed document,
is represented in the type Header. The source
and docID attributes yield a unique identifier for
each document. We then adopted some Dublin Core
elements, e.g., language, title, docType. We dis-
tinguish between domain-independent information
such as language, title, document type and domain-
dependent information as relevant for text mining
in the bio-medical domain. Accordingly, the type
pubmed.Header was especially created for the
representation of PUBMED document information.
A more detailed description of the document?s pub-
lication data is available from types which specialize
PubType such as Journal. The latter contains
standard journal-specific attributes, e.g., ISSN, vol-
ume, journalTitle.
The description of the document?s content of-
ten comes with a list of keywords, informa-
tion assigned to the Descriptor type. We
clearly distinguish between content descriptors man-
ually provided by an author, indexer or cura-
tor, and items automatically generated by text
analysis components after document processing.
While the first kind of information will be stored
in the ManualDescriptor, the second one
will be represented in the AutoDescriptor.
The generation of domain-dependent descriptors is
also possible; currently the scheme contains the
pubmed.ManualDescriptor which allows to
assign attributes such as chemicals and genes.
4.3 Document Structure & Style
The Document Structure & Style layer (cf. Figure 1-
5) contains information about the organization and
layout of the analyzed documents. This layer en-
ables the marking-up of document structures such
as paragraphs, rhetorical zones, figures and tables,
as well as typographical information, such as italics
and special fonts. The focus of modeling this layer is
on the annotation of scientific documents, especially
in the life sciences. We adopted here the SCIXML8
annotation schema, which was especially developed
for marking-up scientific publications. The Zone
type refers to a distinct division of text and is the par-
ent type for various subtypes such as TextBody,
Title etc. While it seems impossible to predict all
of the potential formal text segments, we first looked
at types of text zones frequently occurring in sci-
entific documents. The type Section, e.g., repre-
sents a straightforward and fairly standard division
of scientific texts into introduction, methods and re-
sults sections. The divisions not covered by current
types can be annotated with Misc. The annotation
of tables and figures with corresponding types en-
ables to link text and additional non-textual infor-
mation, an issue which is gaining more and more
attention in the text mining field.
4.4 Morpho-Syntax
The Morpho-Syntax layer (cf. Figure 1-3) represents
the results of morpho-syntactic analysis such as to-
kenization, stemming, POS tagging. The small-
est annotation unit is Token which consists of five
attributes, including its part-of-speech information
8http://www.cl.cam.ac.uk/?aac10/
escience/sciborg.html
37
(posTag), stemmedForm, lemma, grammatical fea-
tures (feats), and orthographical information (or-
thogr).
With respect to already available POS tagsets,
the scheme allows corresponding extensions of
the supertype POSTag to, e.g., PennPOSTag
(for the Penn Tag Set (Marcus et al, 1993)) or
GeniaPOSTag (for the GENIA Tag Set (Ohta et
al., 2002)). The attribute tagsetId serves as a unique
identifier of the corresponding tagset. The value of
the POS tag (e.g., NN, VVD, CC) can be stored in
the attribute value. The potential values for the in-
stantiation of this attribute are always restricted to
the tags of the associated tagset. These constraints
enforce formal control on annotation processes.
As for morphologically normalized lexical items,
the Lemma type stores the canonical form of a lexi-
cal token which can be retrieved from a lexicon once
it is computed by a lemmatizer. The lemma value,
e.g., for the verb ?activates? would be ?activate?. The
StemmedForm represents a base form of a text to-
ken as produced by stemmers (e.g., ?activat-? for the
noun ?activation?).
Due to their excessive use in life science docu-
ments, abbreviations, acronyms and their expanded
forms have to be considered in terms of appropriate
types, as well. Accordingly, Abbreviation and
Acronym are defined, the latter one being a child
type of the first one. The expanded form of a short
one can easily be accessed from the attribute expan.
Grammatical features of tokens are represented
in those types which specialize the supertype
GrammaticalFeats. Its child types, viz.
NounFeats, VerbFeats, AdjectiveFeats,
PronounFeats (omitted from Figure 1-3) cover
the most important word categories. Attributes
of these types obviously reflect the properties
of particular grammatical categories. While
NounFeats comes with gender, case and num-
ber only, PronounFeats must be enhanced with
person. A more complex feature structure is asso-
ciated with VerbFeats which requires attributes
such as tense, person, number, voice and aspect. We
adapted here specifications from the TEI to allow
compatibility with other annotation schemata.
The type LexiconEntry (cf. Figure 1-1) en-
ables a link to the lexicon of choice. By designing
this type we achieve much needed flexibility in link-
ing text snaps (e.g., tokens, simplex forms, multi-
word terms) to external resources. The attributes
entryId and source yield, in combination, a unique
identifier of the current lexicon entry. Resource ver-
sion control is enabled through an attribute version.
Text annotations often mark disrupted text spans,
so-called discontinuous annotations. In coordinated
structures such as ?T and B cell?, the annotator
should mark two named entities, viz. ?T cell? and ?B
cell?, where the first one results from the combina-
tion of the disjoint parts ?T? and ?cell?. In order to
represent such discontinous annotations, we intro-
duced the type DiscontinuousAnnotation
(cf. Figure 1-1) which links through its attribute
value spans of annotations to an annotation unit.
4.5 Syntax
This layer of the scheme provides the types and at-
tributes for the representation of syntactic structures
of sentences (cf. Figure 1-4). The results from shal-
low and full parsing can be stored here.
Shallow parsing (chunking) aims at dividing
the flow of text into phrases (chunks) in a non-
overlapping and non-recursive manner. The type
Chunk accounts for different chunk tag sets by sub-
typing. Currently, the scheme supports Phrase-
Chunks with subtypes such as NP, VP, PP, or ADJP
(Marcus et al, 1993).
The scheme also reflects the most popular full
parsing approaches in NLP, viz. constituent-based
and dependency-based approaches. The results
from constituent-based parsing are represented in
a parse tree and can be stored as single nodes in
the Constituent type. The tree structure can
be reconstructed through links in the attribute par-
ent which stores the id of the parent constituent.
Besides the attribute parent, Constituent holds
the attributes cat which stores the complex syntac-
tic category of the current constituent (e.g., NP, VP),
and head which links to the head word of the con-
stituent. In order to account for multiple annota-
tions in the constituent-based approach, we intro-
duced corresponding constituent types which spe-
cialize Constituent. This parallels our approach
which we advocate for alternatives in POS tagging
and the management of alternative chunking results.
Currently, the scheme supports three differ-
ent constituent types, viz. PTBConstituent,
38
GENIAConstituent (Miyao and Tsujii, 2005)
and PennBIoIEConstituent. The attributes
of the type PTBConstituent cover the com-
plete repertoire of annotation items contained in
the Penn Treebank, such as functional tags for
form/function dicrepancies (formFuncDisc), gram-
matical role (gramRole), adverbials (adv) and mis-
cellaneous tags (misc). The representation of null
elements, topicalized elements and gaps with corre-
sponding references to the lexicalized elements in a
tree is reflected in attributes nullElement, tpc, map
and ref, respectively. GENIAConstituent and
PennBIoIEConstituent inherit from PTB-
Constituent all listed attributes and provide, in
the case of GENIAConstituent , an additional
attribute syn to specify the syntactic idiosyncrasy
(coordination) of constituents.
Dependency parsing results are directly linked to
the token level and are thus referenced in the Token
type. The DependencyRelation type inherits
from the general Relation type and introduces
additional features which are necessary for describ-
ing a syntactic dependency. The attribute label char-
acterizes the type of the analyzed dependency rela-
tion. The attribute head indicates the head of the
dependency relation attributed to the analyzed to-
ken. The attribute projective relates to the property
of the dependency relation whether it is projective
or not. As different dependency relation sets can be
used for parsing, we propose subtyping similar to
the constituency-based parsing approaches. In order
to account for alternative dependency relation sets,
we aggregate all possible annotations in the Token
type as a list (depRelList).
4.6 Semantics
The Semantics layer comprises currently the repre-
sentation of named entities, particularly for the bio-
medical domain. The entity types are hierarchically
organized. The supertype Entity (cf. Figure 1-
6) links annotated (named) entities to the ontologies
and databases through appropriate attributes, viz. on-
tologyEntry and sdbEntry. The attribute specific-
Type specifies the analyzed entity in a more detailed
way (e.g., Organism can be specified through
the species values ?human?, ?mouse?, ?rat?, etc.)
The subtypes are currently being developed in the
bio-medical domain and cover, e.g., genes, pro-
teins, organisms, diseases, variations. This hierar-
chy can easily be extended or supplemented with
entities from other domains. For illustration pur-
poses, we extended it here by MUC (Grishman
and Sundheim, 1996) entity types such as Person,
Organization, etc.
This scheme is still under construction and will
soon also incorporate the representation of relation-
ships between entities and domain-specific events.
The general type Relation will then be extended
with specific conceptual relations such as location,
part-of, etc. The representation of events will be
covered by a type which aggregates pre-defined re-
lations between entities and the event mention. An
event type such as InhibitionEventwould link
the text spans in the sentence ?protein A inhibits
protein B? in attributes agent (?protein A?), patient
(?protein B?), mention (?inhibits?).
5 Conclusion and Future work
In this paper, we introduced an UIMA annotation
type system which covers the core functionality
of morphological, syntactic and semantic analysis
components of a generic NLP system. It also in-
cludes type specifications which relate to the formal
document format and document style. Hence, the
design of this scheme allows the annotation of the
entire cycle of (sentence-level) NLP analysis (dis-
course phenomena still have to be covered).
The annotation scheme consists mostly of core
types which are designed in a domain-independent
way. Nevertheless, it can easily be extended with
types which fit other needs. The current scheme sup-
plies an extension for the bio-medical domain at the
document meta and structure level, as well as on the
semantic level. The morpho-syntactic and syntactic
levels provide types needed for the analysis of the
English language. Changes of attributes or attribute
value sets will lead to adaptations to other natural
languages.
We implemented the scheme as an UIMA type
system. The formal specifications are implemented
using the UIMA run-time environment. This direct
link of formal and implementational issues is a ma-
jor asset using UIMA unmatched by any previous
specification approach. Furthermore, all annotation
results can be converted to the XMI format within
39
the UIMA framework. XMI, the XML Metadata In-
terchange format, is an OMG9 standard for the XML
representation of object graphs.
The scheme also eases the representation of an-
notation results for the same task with alternative
and often competitive components. The identifica-
tion of the component which provided specific an-
notations can be retrieved from the attribute com-
ponentId. Furthermore, the annotation with alterna-
tive and multiple tag sets is supported as well. We
have designed for each tag set a type representing
the corresponding annotation parameters. The inher-
itance trees at almost all annotation layers support
the parallelism in annotation process (e.g., tagging
may proceed with different POS tagsets).
The user of the scheme can restrict the potential
values of the types or attributes. The current scheme
makes use of the customization capability for POS
tagsets, for all attributes of constituents and chunks.
This yields additional flexibility in the design and,
once specified, an increased potential for automatic
control for annotations.
The scheme also enables a straightforward con-
nection to external resources such as ontologies,
lexicons, and databases as evidenced by the corre-
sponding subtypes of ResourceEntry (cf. Figure
1-1). These types support the specification of a re-
lation between a concrete text span and the unique
item addressed in any of these resources.
With these considerations in mind, we strive for
the elaboration of a common standard UIMA type
system for NLP engines. The advantages of such a
standard include an easy exchange and integration
of different NLP analysis engines, the facilitation
of sophisticated evaluation studies (where, e.g., al-
ternative components for NLP tasks can be plugged
in and out at the spec level), and the reusability of
single NLP components developed in various labs.
Acknowledgments. This research was funded by the EC?s 6th Framework Programme
(4th call) within the BOOTStrep project under grant FP6-028099.
References
S. Brants and S. Hansen. 2002. Developments in the TIGER
annotation scheme and their realization in the corpus. In
Proc. of the 3rd LREC Conference, pages 1643?1649.
P. Buitelaar, T. Declerck, B. Sacaleanu, ?S. Vintar, D. Raileanu,
and C. Crispi. 2003. A multi-layered, XML-based approach
9http://www.omg.org
to the integration of linguistic and semantic annotations. In
Proc. of EACL 2003 Workshop NLPXML-03.
H. Cunningham. 2002. GATE, a general architecture for text
engineering. Computers and the Humanities, 36:223?254.
T. Declerck. 2006. SYNAF: Towards a standard for syntactic
annotation. In Proc. of the 5th LREC Conference.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program. In Proc. of the 4th LREC
Conference, pages 837?840.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural ap-
proach to unstructured information processing in the corpo-
rate research environment. Natural Language Engineering,
10(3-4):327?348.
T. Go?tz and O. Suhre. 2004. Design and implementation of the
UIMA Common Analysis System. IBM Systems Journal,
43(3):476?489.
R. Grishman and B. Sundheim. 1996. Message Understand-
ing Conference ? 6: A brief history. In Proc. of the 16th
COLING, pages 466?471.
R. Grishman. 1997. Tipster architecture design document,
version 2.3. Technical report, Defense Advanced Research
Projects Agency (DARPA), U.S. Departement of Defense.
C. Grover, E. Klein, M. Lapata, and A. Lascarides. 2002.
XML-based NLP tools for analysing and annotating medi-
cal language. In Proc. of the 2nd Workshop NLPXML-2002,
pages 1?8.
U. Hahn and J. Wermter. 2006. Levels of natural language pro-
cessing for text mining. In S. Ananiadou and J. McNaught,
editors, Text Mining for Biology and Biomedicine, pages 13?
41. Artech House.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-
based standard for linguistic corpora. In Proc. of the 2nd
LREC Conference, pages 825?830.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International
standard for a linguistic annotation framework. In Proc. of
the HLT-NAACL 2003 SEALTS Workshop, pages 25?30.
C. Laprun, J. Fiscus, J. Garofolo, and S. Pajot. 2002. A prac-
tical introduction to ATLAS. In Proc. of the 3rd LREC Con-
ference, pages 1928?1932.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
A. Meyers. 2006. Annotation compatibility working group re-
port. In Proc. of the COLING-ACL 2006 Workshop FLAC
2006?, pages 38?53.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proc. of the
ACL 2005, pages 83 ? 90.
T. Ohta, Y. Tateisi, and J.-D. Kim. 2002. The GENIA corpus:
An annotated research abstract corpus in molecular biology
domain. In Proc. of the 2nd HLT, pages 82?86.
E. Pianta, L. Bentivogli, C. Girardi, and B. Magnini. 2006.
Representing and accessing multilevel linguistic annotation
using the MEANING format. In Proc. of the 5th EACL-2006
Workshop NLPXML-2006, pages 77?80.
J. Rumbaugh, I. Jacobson, and G. Booch. 1999. The Unified
Modeling Language Reference Manual. Addison-Wesley.
U. Scha?fer. 2006. Middleware for creating and combining
multi-dimensional NLP markup. In Proc. of the 5th EACL-
2006 Workshop NLPXML-2006, pages 81?84.
40
METER: MEasuring TExt Reuse
Paul Clough and Robert Gaizauskas and Scott S.L. Piao and Yorick Wilks
Department of Computer Science
University of She?eld
Regent Court, 211 Portobello Street,
She?eld, England, S1 4DP
finitial.surname@dcs.shef.ac.ukg
Abstract
In this paper we present results from
the METER (MEasuring TExt Reuse)
project whose aim is to explore issues
pertaining to text reuse and derivation,
especially in the context of newspapers
using newswire sources. Although the
reuse of text by journalists has been
studied in linguistics, we are not aware
of any investigation using existing com-
putational methods for this particular
task. We investigate the classication
of newspaper articles according to their
degree of dependence upon, or deriva-
tion from, a newswire source using a
simple 3-level scheme designed by jour-
nalists. Three approaches to measur-
ing text similarity are considered: n-
gram overlap, Greedy String Tiling,
and sentence alignment. Measured
against a manually annotated corpus of
source and derived news text, we show
that a combined classier with fea-
tures automatically selected performs
best overall for the ternary classica-
tion achieving an average F
1
-measure
score of 0.664 across all three cate-
gories.
1 Introduction
A topic of considerable theoretical and practical
interest is that of text reuse: the reuse of existing
written sources in the creation of a new text. Of
course, reusing language is as old as the retelling
of stories, but current technologies for creating,
copying and disseminating electronic text, make
it easier than ever before to take some or all of
any number of existing text sources and reuse
them verbatim or with varying degrees of mod-
ication.
One form of unacceptable text reuse, plagia-
rism, has received considerable attention and
software for automatic plagiarism detection is
now available (see, e.g. (Clough, 2000) for a re-
cent review). But in this paper we present a
benign and acceptable form of text reuse that
is encountered virtually every day: the reuse of
news agency text (called copy) in the produc-
tion of daily newspapers. The question is not
just whether agency copy has been reused, but
to what extent and subject to what transforma-
tions. Using existing approaches from computa-
tional text analysis, we investigate their ability
to classify newspapers articles into categories in-
dicating their dependency on agency copy.
2 Journalistic reuse of a newswire
The process of gathering, editing and publish-
ing newspaper stories is a complex and spe-
cialised task often operating within specic pub-
lishing constraints such as: 1) short deadlines;
2) prescriptive writing practice (see, e.g. Evans
(1972)); 3) limits of physical size; 4) readability
and audience comprehension, e.g. a tabloid's
vocabulary limitations; 5) journalistic bias, e.g.
political and 6) a newspaper's house style. Of-
ten newsworkers, such as the reporter and edi-
tor, will rely upon news agency copy as the basis
of a news story or to verify facts and assess the
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 152-159.
                         Proceedings of the 40th Annual Meeting of the Association for
importance of a story in the context of all those
appearing on the newswire. Because of the na-
ture of journalistic text reuse, dierences will
arise between reused news agency copy and the
original text. For example consider the follow-
ing:
Original (news agency) A drink-driver who
ran into the Queen Mother's o?cial Daim-
ler was ned $700 and banned from driving
for two years.
Rewrite (tabloid) A DRUNK driver who
ploughed into the Queen Mother's limo was
ned $700 and banned for two years yes-
terday.
This simple example illustrates the types of
rewrite that can occur even in a very short
sentence. The rewrite makes use of slang and
exaggeration to capture its readers' attention
(e.g. DRUNK, limo, ploughed). Deletion (e.g.
from driving) has also been used and the addi-
tion of yesterday indicates when the event oc-
curred. Many of the transformations we ob-
served between moving from news agency copy
to the newspaper version have also been re-
ported by the summarisation community (see,
e.g., McKeown and Jing (1999)).
Given the value of the information news agen-
cies supply, the ease with which text can be
reused and commercial pressures, it would be
benecial to be able to identify those news sto-
ries appearing in the newspapers that have relied
upon agency copy in their production. Potential
uses include: 1) monitoring take-up of agency
copy; 2) identifying the most reused stories ; 3)
determining customer dependency upon agency
copy and 4) new methods for charging customers
based upon the amount of copy reused. Given
the large volume of news agency copy output
each day, it would be infeasible to identify and
quantify reuse manually; therefore an automatic
method is required.
3 A conceptual framework
To begin to get a handle on measuring text
reuse, we have developed a document-level clas-
sication scheme, indicating the level at which
a newspaper story as a whole is derived from
agency copy, and a lexical-level classication
scheme, indicating the level at which individ-
ual word sequences within a newspaper story
are derived from agency copy. This framework
rests upon the intuitions of trained journalists
to judge text reuse, and not on an explicit lex-
ical/syntactic denition of reuse (which would
presuppose what we are setting out to discover).
At the document level, newspaper stories
are assigned to one of three possible categories
coarsely reecting the amount of text reused
from the news agency and the dependency of
the newspaper story upon news agency copy
for the provision of \facts". The categories in-
dicate whether a trained journalist can iden-
tify text rewritten from the news agency in
a candidate derived newspaper article. They
are: 1) wholly-derived (WD): all text in
the newspaper article is rewritten only from
news agency copy; 2) partially-derived (PD):
some text is derived from the news agency, but
other sources have also been used; and 3) non-
derived (ND): news agency has not been used
as the source of the article; although words may
still co-occur between the newspaper article and
news agency copy on the same topic, the jour-
nalist is condent the news agency has not been
used.
At the lexical or word sequence level, individ-
ual words and phrases within a newspaper story
are classied as to whether they are used to ex-
press the same information as words in news
agency copy (i.e. paraphrases) and or used to
express information not found in agency copy.
Once again, three categories are used, based on
the judgement of a trained journalist: 1) verba-
tim: text appearing word-for-word to express
the same information; 2) rewrite: text para-
phrased to create a dierent surface appearance,
but express the same information and 3) new:
text used to express information not appearing
in agency copy (can include verbatim/rewritten
text, but being used in a dierent context).
3.1 The METER corpus
Based on this conceptual framework, we have
constructed a small annotated corpus of news
texts using the UK Press Association (PA) as
the news agency source and nine British daily
newspapers
1
who subscribe to the PA as candi-
date reusers. The METER corpus (Gaizauskas
et al, 2001) is a collection of 1716 texts (over
500,000 words) carefully selected from a 12
month period from the areas of law and court
reporting (769 stories) and showbusiness (175
stories). 772 of these texts are PA copy and 944
from the nine newspapers. These texts cover 265
dierent stories from July 1999 to June 2000 and
all newspaper stories have been manually classi-
ed at the document-level. They include 300
wholly-derived, 438 partially-derived and 206
non-derived (i.e. 77% are thought to have used
PA in some way). In addition, 355 have been
classied according to the lexical-level scheme.
4 Approaches to measuring text
similarity
Many problems in computational text analy-
sis involve the measurement of similarity. For
example, the retrieval of documents to full a
user information need, clustering documents ac-
cording to some criterion, multi-document sum-
marisation, aligning sentences from one lan-
guage with those in another, detecting exact and
near duplicates of documents, plagiarism detec-
tion, routing documents according to their style
and identifying authorship attribution. Meth-
ods typically vary depending upon the match-
ing method, e.g. exact or partial, the degree
to which natural language processing techniques
are used and the type of problem, e.g. search-
ing, clustering, aligning etc. We have not had
time to investigate all of these techniques, nor
is there space here to review them. We have
concentrated on just three: ngram overlap mea-
sures, Greedy String Tiling, and sentence align-
ment. The rst was investigated because it of-
fers perhaps the simplest approach to the prob-
lem. The second was investigated because it has
been successfully used in plagiarism detection, a
problem which at least supercially is quite close
1
The newspapers include ve popular papers (e.g. The
Sun, The Daily Mail, Daily Star, Daily Mirror) and four
quality papers (e.g. Daily Telegraph, The Guardian, The
Independent and The Times).
to the text reuse issues we are investigating. Fi-
nally, alignment (treating the derived text as a
\translation" of the rst) seemed an intriguing
idea, and contrasts, certainly with the ngram ap-
proach, by focusing more on local, as opposed to
global measures of similarity.
4.1 Ngram Overlap
An initial, straightforward approach to assessing
the reuse between two texts is to measure the
number of shared word ngrams. This method
underlies many of the approaches used in copy
detection including the approach taken by Lyon
et al (2001).
They measure similarity using the set-
theoretic measures of containment and resem-
blance of shared trigrams to separate texts writ-
ten independently and those with su?cient sim-
ilarity to indicate some form of copying.
We treat each document as a set of overlap-
ping n-word sequences (initially considering only
n-word types) and compute a similarity score
from this. Given two sets of ngrams, we use
the set-theoretic containment score to measure
similarity between the documents for ngrams of
length 1 to 10 words. For a source text A and
a possibly derived text B represented by sets of
ngrams S
n
(A) and S
n
(B) respectively, the pro-
portion of ngrams in B also in A, the ngram con-
tainment C
n
(A;B), is given by:
C
n
(A;B) =
j S
n
(A) \ S
n
(B) j
j S
n
(B) j
(1)
Informally containment measures the number
of matches between the elements of ngram sets
S
n
(A) and S
n
(B), scaled by the size of S
n
(B).
In other words we measure the proportion of
unique n-grams in B that are found in A. The
score ranges from 0 to 1, indicating none to all
newspaper copy shared with PA respectively.
We also compare texts by counting only those
ngrams with low frequency, in particular those
occurring once. For 1-grams, this is the same as
comparing the hapax legomena which has been
shown to discriminate plagiarised texts from
those written independently even when lexical
overlap between the texts is already high (e.g.
70%) (Finlay, 1999). Unlike Finlay's work, we
nd that repetition in PA copy
2
drastically re-
duces the number of shared hapax legomena
thereby inhibiting classication of derived and
non-derived texts. Therefore we compute the
containment of hapax legomena (hapax contain-
ment) by comparing words occurring once in the
newspaper, i.e. those 1-grams in S
1
(B) that oc-
cur once with all 1-grams in PA copy, S
1
(A).
This containment score represents the number
of newspaper hapax legomena also appearing at
least once in PA copy.
4.2 Greedy String-Tiling
Greedy String-Tiling (GST) is a substring
matching algorithm which computes the degree
of similarity between two strings, for exam-
ple software code, free text or biological subse-
quences (Wise, 1996). Compared with previous
algorithms for computing string similarity, such
as the Longest Common Subsequence or
Levenshtein distance, GST is able to deal with
transposition of tokens (in earlier approaches
transposition is seen as a number of single inser-
tions/deletions rather than a single block move).
The GST algorithm performs a 1:1 matching
of tokens between two strings so that as much of
one token stream is covered with maximal length
substrings from the other (called tiles). In our
problem, we consider how much newspaper text
can be maximally covered by words from PA
copy. A minimum match length (MML) can be
used to avoid spurious matches (e.g. of 1 or
2 tokens) and the resulting similarity between
the strings can be expressed as a quantitative
similarity match or a qualitative list of common
substrings. Figure 1 shows the result of GST for
the example in Section 2.
Figure 1: Example GST results (MML=3)
2
As stories unfold, PA release copy with new, as well
as previous versions of the story
Given PA copy A, a newspaper text B and a
set of maximal matches, tiles, of a given length
between A and B, the similarity, gstsim(A,B),
is expressed as:
gstsim(A;B) =
P
i2tiles
length
i
j B j
(2)
4.3 Sentence alignment
In the past decade, various alignment algorithms
have been suggested for aligning multilingual
parallel corpora (Wu, 2000). These algorithms
have been used to map translation equivalents
across dierent languages. In this specic case,
we investigate whether alignment can map de-
rived texts (or parts of them) to their source
texts. PA copy may be subject to various
changes during text reuse, e.g. a single sen-
tence may derive from parts of several source
sentences. Therefore, strong correlations of sen-
tence length between the derived and source
sentences cannot be guaranteed. As a result,
sentence-length based statistical alignment al-
gorithms (Brown et al, 1991; Gale and Church,
1993) are not appropriate for this case. On the
other hand, cognate-based algorithms (Simard
et al, 1992; Melamed, 1999) are more e?cient
for coping with change of text format. There-
fore, a cognate-based approach is adopted for
the METER task. Here cognates are dened as
pairs of terms that are identical, share the same
stems, or are substitutable in the given context.
The algorithm consists of two principal com-
ponents: a comparison strategy and a scoring
function. In brief, the comparison works as fol-
lows (more details may be found in Piao (2001)).
For each sentence in the candidate derived text
DT the sentences in the candidate source text
ST are compared in order to nd the best match.
A DT sentence is allowed to match up to three
possibly non-consecutive ST sentences. The
candidate pair with the highest score (see be-
low) above a threshold is accepted as a true
alignment. If no such candidate is found, the
DT sentence is assumed to be independent of
the ST. Based on individual DT sentence align-
ments, the overall possibility of derivation for
the DT is estimated with a score ranging be-
tween 0 and 1. This score reects the propor-
tion of aligned sentences in the newspaper text.
Note that not only may multiple sentences in
the ST be aligned with a single sentence in the
DT, but also multiple sentences in the DT may
be aligned with one sentence in the ST.
Given a candidate derived sentence DS and
a proposed (set of) source sentence(s) SS, the
scoring function works as follows. Three basic
measures are computed for each pair of candi-
date DS and SS: SNG is the sum of lengths
of the maximum length non-overlapping shared
n-grams with n  2; SWD is the number of
matched words sharing stems not in an n-gram
guring in SNG; and SUB is the number of
substitutable terms (mainly synonyms) not g-
uring in SNG or SWD. Let L
1
be the length of
the candidate DS and L
2
the length of candidate
SS. Then, three scores PD, PS (Dice score) and
PV S are calculated as follows:
PSD =
SWD + SNG + SUB
L
1
PS =
2(SWD + SNG + SUB)
L
1
+ L
2
PSNG =
SNG
SWD + SNG + SUB
These three scores reect dierent aspects of
relations between the candidate DS and SS:
1. PSD: The proportion of the DS which is
shared material.
2. PS: The proportion of shared terms in DS
and SS. This measure prefers SS's which not
only contain many terms in the DS, but also
do not contain many additional terms.
3. PSNG: The proportion of matching n-
grams amongst the shared terms. This
measure captures the intuition that sen-
tences sharing not only words, but word se-
quences are more likely to be related.
These three scores are weighted and combined
together to provide an alignment metric WS
(weighted score), which is calculated as follows:
WS = ?
1
PSD+ ?
2
PS + ?
3
PSNG
where ?
1
+?
2
+?
3
= 1. The three weighting vari-
ables ?
i
(i = 1; 2; 3) have been determined empir-
ically and are currently set to: ?
1
= 0:85; ?
2
=
0:05; ?
3
= 0:1.
5 Reuse Classiers
To evaluate the previous approaches for measur-
ing text reuse at the document-level, we cast the
problem into one of a supervised learning task.
5.1 Experimental Setup
We used similarity scores as attributes for a ma-
chine learning algorithm and used the Weka 3.2
software (Witten and Frank, 2000). Because of
the small number of examples, we used tenfold
cross-validation repeated 10 times (i.e. 10 runs)
and combined this with stratication to ensure
approximately the same proportion of samples
from each class were used in each fold of the
cross-validation. All 769 newspaper texts from
the courts domain were used for evaluation and
randomly permuted to generate 10 sets. For
each newspaper text, we compared PA source
texts from the same story to create results in
the form: newspaper; class; score. These results
were ordered according to each set to create the
same 10 datasets for each approach thereby en-
abling comparison.
Using this data we rst trained ve single-
feature Naive Bayes classiers to do the ternary
classication task. The feature in each case was
a variant of one of the three similarity measures
described in Section 4, computed between the
two texts in the training set. The target classi-
cation value was the reuse classication category
from the corpus. A Naive Bayes classier was
used because of its success in previous classi-
cation tasks, however we are aware of its naive
assumptions that attributes are assumed inde-
pendent and data to be normally distributed.
We evaluated results using the F
1
-measure
(harmonic mean of precision and recall given
equal weighting). For each run, we calculated
the average F
1
score across the classes. The
overall average F
1
-measure scores were com-
puted from the 10 runs for each class (a single
accuracy measure would su?ce but the Weka
package outputs F
1
-measures). For the 10 runs,
the standard deviation of F
1
scores was com-
puted for each class and F
1
scores between all
approaches were tested for statistical signi-
cance using 1-way analysis of variance at a 99%
condence-level. Statistical dierences between
results were identied using Bonferroni analy-
sis
3
.
After examining the results of these single fea-
ture classiers, we also trained a \combined"
classier using a correlation-based lter ap-
proach (Hall and Smith, 1999) to select the com-
bination of features giving the highest classica-
tion score ( correlation-based ltering evaluates
all possible combinations of features). Feature
selection was carried for each fold during cross-
validation and features used in all 10 folds were
chosen as candidates. Those which occurred in
at least 5 of the 10 runs formed the nal selec-
tion.
We also tried splitting the training data into
various binary partitions (e.g. WD/PD vs. ND)
and training binary classiers, using feature se-
lection, to see how well binary classication
could be performed. Eskin and Bogosian (1998)
have observed that using cascaded binary clas-
siers, each of which splits the data well, may
work better on n-ary classication problems
than a single n-way classier. We then com-
puted how well such a cascaded classier should
perform using the best binary classier results.
5.2 Results
Table 1 shows the results of the single ternary
classiers. The baseline F
1
measure is based
upon the prior probability of a document falling
into one of the classes. The gures in parenthe-
sis are the standard deviations for the F
1
scores
across the ten evaluation runs. The nal row
shows the results for combining features selected
using the correlation-based lter.
Table 2 shows the result of training binary
classiers using feature selection to select the
most discriminating features for various binary
splits of the training data.
For both ternary and binary classiers feature
selection produced better results than using all
3
Using SPSS v10.0 for Windows.
Approach Category Avg F-measure
Baseline WD 0.340 (0.000)
PD 0.444 (0.000)
ND 0.216 (0.000)
total 0.333 (0.000)
3-gram WD 0.631 (0.004)
containment PD 0.624 (0.004)
ND 0.549 (0.005)
total 0.601 (0.003)
GST Sim WD 0.669 (0.004)
MML = 3 PD 0.633 (0.003)
ND 0.556 (0.004)
total 0.620 (0.002)
GST Sim WD 0.681 (0.003)
MML = 1 PD 0.634 (0.003)
ND 0.559 (0.008)
total 0.625 (0.004)
1-gram WD 0.718 (0.003)
containment PD 0.643 (0.003)
ND 0.551 (0.006)
total 0.638 (0.003)
Alignment WD 0.774 (0.003)
PD 0.624 (0.005)
ND 0.537 (0.007)
total 0.645 (0.004)
hapax WD 0.736 (0.003)
containment PD 0.654 (0.003)
ND 0.549 (0.010)
total 0.646 (0.004)
hapax cont. WD 0.756 (0.002)
1-gram cont. PD 0.599 (0.006)
alignment ND 0.629 (0.008)
(\combined") total 0.664 (0.004)
Table 1: A summary of classication results
possible features, with the one exception of the
binary classication between PD and ND.
5.3 Discussion
From Table 1, we nd that all classier results
are signicantly higher than the baseline (at
p < 0:01) and all dierences are signicant ex-
cept between hapax containment and alignment.
The highest F-measure for the 3-class problem
is 0.664 for the \combined" classier, which is
signicantly greater than 0.651 obtained with-
out. We notice that highest WD classication
is with alignment at 0.774, highest PD classi-
cation is 0.654 with hapax containment and
highest ND classication is 0.629 with combined
features. Using hapax containment gives higher
results than 1-gram containment alone and in
fact provides results as good as or better than
the more complex sentence alignment and GST
approaches.
Previous research by (Lyon et al, 2001) and
(Wise, 1996) had shown derived texts could be
distinguished using trigram overlap and tiling
with a match length of 3 or more, respectively.
Attributes Category Avg F
1
Correlation- alignment WD 0.942 (0.008)
based ND 0.909 (0.011)
lter total 0.926 (0.010)
alignment PD/ND 0.870 (0.003)
WD 0.770 (0.003)
total 0.820 (0.002)
alignment WD 0.778 (0.003)
PD 0.812 (0.002)
total 0.789 (0.002)
hapax cont. WD/PD 0.882 (0.002)
alignment ND 0.649 (0.007)
1-gram cont. total 0.763 (0.002)
1-gram PD 0.802 (0.002)
GST mml 3 ND 0.638 (0.007)
GST mml 1 total 0.720 (0.004)
alignment
GST mml 1 WD/ND 0.672 (0.002)
alignment PD 0.662 (0.003)
total 0.668 (0.003)
Table 2: Binary Classiers with feature selection
However, our results run counter to this be-
cause the highest classication scores are ob-
tained with 1-grams and an MML of 1, i.e. as
n or MML length increases, the F
1
scores de-
crease. We believe this results from two factors
which are characteristic of reuse in journalism.
First, since even ND texts are thematically sim-
ilar (same events being described) there is high
likelihood of coincidental overlap of ngrams of
length 3 or more (e.g. quoted speech). Secondly,
when journalists rewrite it is rare for them not
to vary the source.
For the intended application { helping the PA
to monitor text reuse { the cost of dierent mis-
classications is not equal. If the classier makes
a mistake, it is better that WD and ND texts are
mis-classied as PD, and PD as WD. Given the
dierence in distribution of documents across
classes where PD contains the most documents,
the classier will be biased towards this class
anyway as required. Table 3 shows the confu-
sion matrix for the combined ternary classier.
WD PD ND
WD 203 55 4
PD 79 192 70
ND 3 53 109
Table 3: Confusion matrix for combined ternary
classier
Although the overall F
1
-measure score is low
(0.664), mis-classication of both WD as ND
and ND as WD is also very low, as most mis-
classications are as PD. Note the high mis-
classication of PD as both WD and ND, re-
ecting the di?culty of separating this class.
From Table 2, we nd alignment is a selected
feature for each binary partition of the data.
The highest binary classication is achieved be-
tween the WD and ND classes using alignment
only, and the highest three scores show WD is
the easiest class to separate from the others.
The PD class is the hardest to isolate, reect-
ing the mis-classications seen in Table 3.
To predict how well a cascaded binary classi-
er will perform we can reason as follows. From
the preceding discussion we see that WD can
be separated most accurately; hence we choose
WD versus PD/ND as the rst binary classier.
This forces the second classier to be PD versus
ND. From the results in Table 2 and the follow-
ing equation to compute the F
1
measure for a
two-stage binary classier
WD + (PD=ND)(
PD+ND
2
)
2
we obtain an overall F
1
measure for ternary clas-
sication of 0.703, which is signicantly higher
than the best single stage ternary classier.
6 Conclusions
In this paper we have investigated text reuse in
the context of the reuse of news agency copy, an
area of theoretical and practical interest. We
present a conceptual framework in which we
measure reuse and based on which the METER
corpus has been constructed. We have presented
the results of using similarity scores, computed
using n-gram containment, Greedy String Tiling
and an alignment algorithm, as attributes for
a supervised learning algorithm faced with the
task of learning how to classify newspaper sto-
ries as to whether they are wholly, partially or
non-derived from a news agency source. We
show that the best single feature ternary clas-
sier uses either alignment or simple hapax con-
tainment measures and that a cascaded binary
classier using a combination of features can
outperform this.
The results are lower than one might like,
and reect the problems of measuring journalis-
tic reuse, stemming from complex editing trans-
formations and the high amount of verbatim
text overlapping as a result of thematic simi-
larity and \expected" similarity due to, e.g., di-
rect/indirect quotes. Given the relative close-
ness of results obtained by all approaches we
have considered, we speculate that any compar-
ison method based upon lexical similarity will
probably not improve classication results by
much. Perhaps improved performance at this
task may possible by using more advanced nat-
ural language processing techniques, e.g. better
modeling of the lexical variation and syntactic
transformation that goes on in journalistic reuse.
Nevertheless the results we have obtained are
strong enough in some cases (e.g. wholly derived
texts can be identied with > 80% accuracy) to
begin to be exploited.
In summary measuring text reuse is an excit-
ing new area that will have a number of appli-
cations, in particular, but not limited to, mon-
itoring and controlling the copy produced by a
newswire.
7 Future work
We are adapting the GST algorithm to deal with
simple rewrites (e.g. synonym substitution) and
to observe the eects of rewriting upon nding
longest common substrings. We are also experi-
menting using the more detailed METER corpus
lexical-level annotations to investigate how well
the GST and ngrams approaches can identify
reuse at this level.
A prototype browser-based demo of both the
GST algorithm and alignment program, allow-
ing users to test arbitrary text pairs for simi-
larity, is now available
4
and will continue to be
enhanced.
Acknowledgements
The authors would like to acknowledge the
UK Engineering and Physical Sciences Re-
search Council for funding the METER project
(GR/M34041). Thanks also to Mark Hepple for
helpful comments on earlier drafts.
4
See http://www.dcs.shef.ac.uk/nlp/meter.
References
P.F. Brown, J.C. Lai, and R.L. Mercer. 1991. Aligning
sentences in parallel corpora. In Proceedings of the
29th Annual Meeting of the Assoc. for Computational
Linguistics, pages 169{176, Berkeley, CA, USA.
P Clough. 2000. Plagiarism in natural and programming
languages: An overview of current tools and technolo-
gies. Technical Report CS-00-05, Dept. of Computer
Science, University of She?eld, UK.
E. Eskin and M. Bogosian. 1998. Classifying text docu-
ments using modular categories and linguistically mo-
tivated indicators. In AAAI-98 Workshop on Learning
for Text Classication.
H. Evans. 1972. Essential English for Journalists, Edi-
tors and Writers. Pimlico, London.
S. Finlay. 1999. Copycatch. Master's thesis, Dept. of
English. University of Birmingham.
R. Gaizauskas, J. Foster, Y. Wilks, J. Arundel,
P. Clough, and S. Piao. 2001. The meter corpus:
A corpus for analysing journalistic text reuse. In Pro-
ceedings of the Corpus Linguistics 2001 Conference,
pages 214|223.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpus. Computational Lin-
guistics, 19:75{102.
M.A. Hall and L.A. Smith. 1999. Feature selection for
machine learning: Comparing a correlation-based l-
ter approach to the wrapper. In Proceedings of the
Florida Articial Intelligence Symposium (FLAIRS-
99), pages 235{239.
C. Lyon, J. Malcolm, and B. Dickerson. 2001. Detecting
short passages of similar text in large document collec-
tions. In Conference on Empirical Methods in Natural
Language Processing (EMNLP2001), pages 118{125.
K. McKeown and H. Jing. 1999. The decomposition of
human-written summary sentences. In SIGIR 1999,
pages 129{136.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics, pages
107{130.
Scott S.L. Piao. 2001. Detecting and measuring text
reuse via aligning texts. Research Memorandum
CS-01-15, Dept. of Computer Science, University of
She?eld.
M. Simard, G. Foster, and P. Isabelle. 1992. Using
cognates to align sentences in bilingual corpora. In
Proceedings of the 4th Int. Conf. on Theoretical and
Methodological Issues in Machine Translation, pages
67{81, Montreal, Canada.
M. Wise. 1996. Yap3: Improved detection of similarities
in computer programs and other texts. In Proceedings
of SIGCSE'96, pages 130{134, Philadelphia, USA.
I.H. Witten and E. Frank. 2000. Datamining - practi-
cal machine learning tools and techniques with Java
implementations. Morgan Kaufmann.
D. Wu. 2000. Alignment. In R. Dale and H. Moisl and
H. Somers (eds.), A Handbook of Natural Language
Processing, pages 415{458. New York: Marcel Dekker.
ASSIST: Automated semantic assistance for translators
Serge Sharoff, Bogdan Babych
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych}@leeds.ac.uk
Paul Rayson, Olga Mudraya, Scott Piao
UCREL, Computing Department
Lancaster University, LA1 4WA, UK
{p.rayson,o.moudraia,s.piao}@lancs.ac.uk
Abstract
The problem we address in this paper is
that of providing contextual examples of
translation equivalents for words from the
general lexicon using comparable corpora
and semantic annotation that is uniform
for the source and target languages. For
a sentence, phrase or a query expression in
the source language the tool detects the se-
mantic type of the situation in question and
gives examples of similar contexts from
the target language corpus.
1 Introduction
It is widely acknowledged that human transla-
tors can benefit from a wide range of applications
in computational linguistics, including Machine
Translation (Carl and Way, 2003), Translation
Memory (Planas and Furuse, 2000), etc. There
have been recent research on tools detecting trans-
lation equivalents for technical vocabulary in a re-
stricted domain, e.g. (Dagan and Church, 1997;
Bennison and Bowker, 2000). The methodology
in this case is based on extraction of terminology
(both single and multiword units) and alignment
of extracted terms using linguistic and/or statisti-
cal techniques (D?jean et al, 2002).
In this project we concentrate on words from the
general lexicon instead of terminology. The ratio-
nale for this focus is related to the fact that trans-
lation of terms is (should be) stable, while gen-
eral words can vary significantly in their transla-
tion. It is important to populate the terminologi-
cal database with terms that are missed in dictio-
naries or specific to a problem domain. However,
once the translation of a term in a domain has been
identified, stored in a dictionary and learned by
the translator, the process of translation can go on
without consulting a dictionary or a corpus.
In contrast, words from the general lexicon ex-
hibit polysemy, which is reflected differently in
the target language, thus causing the dependency
of their translation on corresponding context. It
also happens quite frequently that such variation
is not captured by dictionaries. Novice translators
tend to rely on dictionaries and use direct trans-
lation equivalents whenever they are available. In
the end they produce translations that look awk-
ward and do not deliver the meaning intended by
the original text.
Parallel corpora consisting of original texts
aligned with their translations offer the possibility
to search for examples of translations in their con-
text. In this respect they provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative: millions of pages of orig-
inal texts are produced daily by native speakers
in major languages, such as English, while trans-
lations are produced by a small community of
trained translators from a small subset of source
texts. The imbalance between original texts and
translations is also reflected in the size of parallel
corpora, which are simply too small for variations
in translation of moderately frequent words. For
instance, frustrate occurs 631 times in 100 million
words of the BNC, i.e. this gives in average about
6 uses in a typical parallel corpus of one million
words.
2 System design
2.1 The research hypothesis
Our research hypothesis is that translators can be
assisted by software which suggests contextual ex-
139
amples in the target language that are semantically
and syntactically related to a selected example in
the source language. To enable greater coverage
we will exploit comparable rather than parallel
corpora.
Our research hypothesis leads us to a number of
research questions:
? Which semantic and syntactic contextual fea-
tures of the selected example in the source
language are important?
? How do we find similar contextual examples
in the target language?
? How do we sort the suggested target lan-
guage contextual examples in order to max-
imise their usefulness?
In order to restrict the research to what is
achievable within the scope of this project, we are
focussing on translation from English to Russian
using a comparable corpus of British and Rus-
sian newspaper texts. Newspapers cover a large
set of clearly identifiable topics that are compara-
ble across languages and cultures. In this project,
we have collected a 200-million-word corpus of
four major British newspapers and a 70-million-
word corpus of three major Russian newspapers
for roughly the same time span (2003-2004).1
In our proposed method, contexts of uses of En-
glish expressions defined by keywords are com-
pared to similar Russian expressions, using se-
mantic classes such as persons, places and insti-
tutions. For instance, the word agreement in the
example the parties were frustratingly close to
an agreement = ??????? ???? ?? ???????? ??????
? ?????????? ?????????? belongs to a seman-
tic class that also includes arrangement, contract,
deal, treaty. In the result, the search for collo-
cates of ??????? (close) in the context of agree-
ment words in Russian gives a short list of mod-
ifiers, which also includes the target: ?? ????????
??????.
2.2 Semantic taggers
In this project, we are porting the Lancaster En-
glish Semantic Tagger (EST) to the Russian lan-
guage. We have reused the existing semantic field
taxonomy of the Lancaster UCREL semantic anal-
ysis system (USAS), and applied it to Russian. We
1Russian newspapers are significantly shorter than their
British counterparts.
have also reused the existing software framework
developed during the construction of a Finnish Se-
mantic Tagger (L?fberg et al, 2005); the main ad-
justments and modifications required for Finnish
were to cope with the Unicode character set (UTF-
8) and word compounding.
USAS-EST is a software system for automatic
semantic analysis of text that was designed at
Lancaster University (Rayson et al, 2004). The
semantic tagset used by USAS was originally
loosely based on Tom McArthur?s Longman Lexi-
con of Contemporary English (McArthur, 1981).
It has a multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories.2
In the ASSIST project, we have been working on
both improving the existing EST and developing a
parallel tool for Russian - Russian Semantic Tag-
ger (RST). We have found that the USAS semantic
categories were compatible with the semantic cat-
egorizations of objects and phenomena in Russian,
as in the following example:3
poor JJ I1.1- A5.1- N5- E4.1- X9.1-
?????? A I1.1- A6.3- N5- O4.2- E4.1-
However, we needed a tool for analysing the
complex morpho-syntactic structure of Russian
words. Unlike English, Russian is a highly in-
flected language: generally, what is expressed in
English through phrases or syntactic structures
is expressed in Russian via morphological in-
flections, especially case endings and affixation.
For this purpose, we adopted a Russian morpho-
syntactic analyser Mystem that identifies word
forms, lemmas and morphological characteristics
for each word. Mystem is used as the equivalent
of the CLAWS part-of-speech (POS) tagger in the
USAS framework. Furthermore, we adopted the
Unicode UTF-8 encoding scheme to cope with the
Cyrillic alphabet. Despite these modifications, the
architecture of the RST software mirrors that of
the EST components in general.
The main lexical resources of the RST include
a single-word lexicon and a lexicon of multi-word
expressions (MWEs). We are building the Russian
lexical resources by exploiting both dictionaries
and corpora. We use readily available resources,
e.g. lists of proper names, which are then se-
2For the full tagset, see http://www.comp.lancs.
ac.uk/ucrel/usas/
3I1.1- = Money: lack; A5.1- = Evaluation: bad; N5- =
Quantities: little; E4.1- = Unhappy; X9.1- = Ability, intel-
ligence: poor; A6.3- = Comparing: little variety; O4.2- =
Judgement of appearance: bad
140
mantically classified. To bootstrap the system, we
have hand-tagged the 3,000 most frequent Russian
words based on a large newspaper corpus. Subse-
quently, the lexicons will be further expanded by
feeding texts from various sources into the RST
and classifying words that remain unmatched. In
addition, we will experiment with semi-automatic
lexicon construction using an existing machine-
readable English-Russian bilingual dictionary to
populate the Russian lexicon by mapping words
from each of the semantic fields in the English lex-
icon in turn. We aim at coverage of around 30,000
single lexical items and up to 9,000 MWEs, com-
pared to the EST which currently contains 54,727
single lexical items and 18,814 MWEs.
2.3 The user interface
The interface is powered by IMS Corpus Work-
bench (Christ, 1994) and is designed to be used in
the day-to-day workflow of novice and practising
translators, so the syntax of the CWB query lan-
guage has been simplified to adapt it to the needs
of the target user community.
The interface implements a search model for
finding translation equivalents in monolingual
comparable corpora, which integrates a number of
statistical and rule-based techniques for extending
search space, translating words and multiword ex-
pressions into the target language and restricting
the number of returned candidates in order to max-
imise precision and recall of relevant translation
equivalents. In the proposed search model queries
can be expanded by generating lists of collocations
for a given word or phrase, by generating sim-
ilarity classes4 or by manual selection of words
in concordances. Transfer between the source
language and target language is done via lookup
in a bilingual dictionary or via UCREL seman-
tic codes, which are common for concepts in both
languages. The search space is further restricted
by applying knowledge-based and statistical fil-
ters (such as part-of-speech and semantic class fil-
ters, IDF filter, etc), by testing the co-occurrence
of members of different similarity classes or by
manually selecting the presented variants. These
procedures are elementary building blocks that are
used in designing different search strategies effi-
cient for different types of translation equivalents
4Simclasses consist of words sharing collocates and are
computed using Singular Value Decomposition, as used by
(Rapp, 2004), e.g. Paris and Strasbourg are produced for
Brussels, or bus, tram and driver for passenger.
and contexts.
The core functionality of the system is intended
to be self-explanatory and to have a shallow learn-
ing curve: in many cases default search parame-
ters work well, so it is sufficient to input a word
or an expression in the source language in or-
der to get back a useful list of translation equiv-
alents, which can be manually checked by a trans-
lator to identify the most suitable solution for a
given context. For example, the word combina-
tion frustrated passenger is not found in the ma-
jor English-Russian dictionaries, while none of the
candidate translations of frustrated are suitable in
this context. The default search strategy for this
phrase is to generate the similarity class for En-
glish words frustrate, passenger, produce all pos-
sible translations using a dictionary and to test co-
occurrence of the resulting Russian words in target
language corpora. This returns a list of 32 Rus-
sian phrases, which follow the pattern of ?annoyed
/ impatient / unhappy + commuter / passenger /
driver?. Among other examples the list includes
an appropriate translation ??????????? ????????
(?unsatisfied passenger?).
The following example demonstrates the sys-
tem?s ability to find equivalents when there is
a reliable context to identify terms in the two
languages. Recent political developments in
Russia produced a new expression ?????????????
?????????? (?representative of president?), which
is as yet too novel to be listed in dictionaries.
However, the system can help to identify the peo-
ple that perform this duty, translate their names
to English and extract the set of collocates that
frequently appear around their names in British
newspapers, including Putin?s personal envoy and
Putin?s regional representative, even if no specific
term has been established for this purpose in the
British media.
As words cannot be translated in isolation and
their potential translation equivalents also often
consist of several words, the system detects not
only single-word collocates, but also multiword
expressions. For instance, the set of Russian
collocates of ?????????? (bureaucracy) includes
???????? (Brussels), which offers a straightfor-
ward translation into English and has such mul-
tiword collocates as red tape, which is a suitable
contextual translation for ??????????.
More experienced users can modify default pa-
rameters and try alternative strategies, construct
141
their own search paths from available basic build-
ing blocks and store them for future use. Stored
strategies comprise several elementary stages but
are executed in one go, although intermediate re-
sults can also be accessed via the ?history? frame.
Several search paths can be tried in parallel and
displayed together, so an optimal strategy for a
given class of phrases can be more easily identi-
fied.
Unlike Machine Translation, the system does
not translate texts. The main thrust of the sys-
tem lies in its ability to find several target language
examples that are relevant to the source language
expression. In some cases this results in sugges-
tions that can be directly used for translating the
source example, while in other cases the system
provides hints for the translator about the range of
target language expressions beyond what is avail-
able in bilingual dictionaries. Even if the preci-
sion of the current version is not satisfactory for an
MT system (2-3 suitable translations out of 30-50
suggested examples), human translators are able
to skim through the suggested set to find what is
relevant for the given translation task.
3 Conclusions
The set of tools is now under further development.
This involves an extension of the English seman-
tic tagger, development of the Russian tagger with
the target lexical coverage of 90% of source texts,
designing the procedure for retrieval of semanti-
cally similar situations and completing the user in-
terface. Identification of semantically similar sit-
uations can be improved by the use of segment-
matching algorithms as employed in Example-
Based MT and translation memories (Planas and
Furuse, 2000; Carl and Way, 2003).
There are two main applications of the pro-
posed methodology. One concerns training trans-
lators and advanced foreign language (FL) learn-
ers to make them aware of the variety of transla-
tion equivalents beyond the set offered by the dic-
tionary. The other application pertains to the de-
velopment of tools for practising translators. Al-
though the Russian language is not typologically
close to English and uses another writing system
which does not allow easy identification of cog-
nates, Russian and English belong to the same
Indo-European family and the contents of Rus-
sian and English newspapers reflect the same set
of topics. Nevertheless, the application of this
research need not be restricted to the English-
Russian pair only. The methodology for multilin-
gual processing of monolingual comparable cor-
pora, first tested in this project, will provide a
blueprint for the development of similar tools for
other language combinations.
Acknowledgments
The project is supported by two EPSRC grants:
EP/C004574 for Lancaster, EP/C005902 for Leeds.
References
Peter Bennison and Lynne Bowker. 2000. Designing a
tool for exploiting bilingual comparable corpora. In
Proceedings of LREC 2000, Athens, Greece.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Oliver Christ. 1994. A modular and flexible archi-
tecture for an integrated corpus query system. In
COMPLEX?94, Budapest.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Herv? D?jean, ?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING 2002.
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka
Juntunen, Asko Nyk?nen, and Krista Varantola.
2005. A semantic tagger for the Finnish language.
In Proceedings of the Corpus Linguistics 2005 con-
ference.
Tom McArthur. 1981. Longman Lexicon of Contem-
porary English. Longman.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of LREC 2004, pages 395?398.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proceedings of the workshop on Be-
yond Named Entity Recognition Semantic labelling
for NLP tasks in association with LREC 2004, pages
7?12.
142
Extracting Multiword Expressions with A Semantic Tagger 
Scott S. L. Piao 
Dept. of Linguistics and MEL 
Lancaster University 
 s.piao@lancaster.ac.uk 
Paul Rayson 
Computing Department 
Lancaster University 
 paul@comp.lancs.ac.uk 
Dawn Archer 
Dept. of Linguistics and MEL 
Lancaster University 
d.archer@lancaster.ac.uk
Andrew Wilson 
Dept. of Linguistics and MEL 
Lancaster University 
 eiaaw@exchange.lancs.ac.uk 
Tony McEnery 
Dept. of Linguistics and MEL 
Lancaster University 
 amcenery@lancaster.ac.uk 
Abstract 
Automatic extraction of multiword 
expressions (MWE) presents a tough 
challenge for the NLP community 
and corpus linguistics. Although 
various statistically driven or knowl-
edge-based approaches have been 
proposed and tested, efficient MWE 
extraction still remains an unsolved 
issue. In this paper, we present our 
research work in which we tested 
approaching the MWE issue using a 
semantic field annotator. We use an 
English semantic tagger (USAS) de-
veloped at Lancaster University to 
identify multiword units which de-
pict single semantic concepts. The 
Meter Corpus (Gaizauskas et al, 
2001; Clough et al, 2002) built in 
Sheffield was used to evaluate our 
approach. In our evaluation, this ap-
proach extracted a total of 4,195 
MWE candidates, of which, after 
manual checking, 3,792 were ac-
cepted as valid MWEs, producing a 
precision of 90.39% and an esti-
mated recall of 39.38%. Of the ac-
cepted MWEs, 68.22% or 2,587 are 
low frequency terms, occurring only 
once or twice in the corpus. These 
results show that our approach pro-
vides a practical solution to MWE 
extraction. 
1 Introduction 
2 
Automatic extraction of Multiword ex-
pressions (MWE) is an important issue in the 
NLP community and corpus linguistics. An 
efficient tool for MWE extraction can be use-
ful to numerous areas, including terminology 
extraction, machine translation, bilin-
gual/multilingual MWE alignment, automatic 
interpretation and generation of language. A 
number of approaches have been suggested 
and tested to address this problem. However, 
efficient extraction of MWEs still remains an 
unsolved issue, to the extent that Sag et al 
(2001b) call it ?a pain in the neck of NLP?. 
In this paper, we present our work in 
which we approach the issue of MWE extrac-
tion by using a semantic field annotator. Spe-
cifically, we use the UCREL Semantic 
Analysis System (henceforth USAS), devel-
oped at Lancaster University to identify mul-
tiword units that depict single semantic 
concepts, i.e. multiword expressions. We have 
drawn from the Meter Corpus (Gaizauskas et 
al., 2001; Clough et al, 2002) a collection of 
British newspaper reports on court stories to 
evaluate our approach. Our experiment shows 
that it is efficient in identifying MWEs, in 
particular MWEs of low frequencies. In the 
following sections, we describe this approach 
to MWE extraction and its evaluation. 
Related Works 
Generally speaking, approaches to MWE 
extraction proposed so far can be divided into 
three categories: a) statistical approaches 
based on frequency and co-occurrence affin-
ity, b) knowledge?based or symbolic ap-
proaches using parsers, lexicons and language 
filters, and c) hybrid approaches combining 
different methods (Smadja 1993; Dagan and 
Church 1994; Daille 1995; McEnery et al 
1997; Wu 1997; Wermter et al 1997; Mi-
chiels and Dufour 1998; Merkel and Anders-
son 2000; Piao and McEnery 2001; Sag et al 
2001a, 2001b; Biber et al 2003). 
In practice, most statistical approaches use 
linguistic filters to collect candidate MWEs. 
Such approaches include Dagan and Church?s 
(1994) Termight Tool. In this tool, they first 
collect candidate nominal terms with a POS 
syntactic pattern filter, then use concordances 
to identify frequently co-occurring multiword 
units. In his Xtract system, Smadja (1993) 
first extracted significant pairs of words that 
consistently co-occur within a single syntactic 
structure using statistical scores called dis-
tance, strength and spread, and then exam-
ined concordances of the bi-grams to find 
longer frequent multiword units. Similarly, 
Merkel and Andersson (2000) compared fre-
quency-based and entropy based algorithms, 
each of which was combined with a language 
filter. They reported that the entropy-based 
algorithm produced better results. 
One of the main problems facing statistical 
approaches, however, is that they are unable 
to deal with low-frequency MWEs. In fact, 
the majority of the words in most corpora 
have low frequencies, occurring only once or 
twice. This means that a major part of true 
multiword expressions are left out by statisti-
cal approaches. Lexical resources and parsers 
are used to obtain better coverage of the lexi-
con in MWE extraction. For example, Wu 
(1997) used an English-Chinese bilingual 
parser based on stochastic transduction 
grammars to identify terms, including multi-
word expressions. In their DEFI Project, Mi-
chiels and Dufour (1998) used dictionaries to 
identify English and French multiword ex-
pressions and their translations in the other 
language. Wehrli (1998) employed a genera-
tive grammar framework to identify com-
pounds and idioms in their ITS-2 MT 
English-French system. Sag et al (2001b) 
introduced Head-driven Phrase Structure 
Grammar for analyzing MWEs. Like pure 
statistical approaches, purely knowledge-
based symbolic approaches also face prob-
lems. They are language dependent and not 
flexible enough to cope with complex struc-
tures of MWEs. As Sag et al (2001b) sug-
gest, it is important to find the right balance 
between symbolic and statistical approaches. 
In this paper, we propose a new approach 
to MWEs extraction using semantic field in-
formation. In this approach, multiword units 
depicting single semantic concepts are recog-
nized using the Lancaster USAS semantic 
tagger. We describe that system and the algo-
rithms used for identifying single and multi-
word units in the following section. 
3 
                                                          
Lancaster Semantic tagger 
The USAS system has been in develop-
ment at Lancaster University since 1990 1 . 
Based on POS annotation provided by the 
CLAWS tagger (Garside and Smith, 1997), 
USAS assigns a set of semantic tags to each 
item in running text and then attempts to dis-
ambiguate the tags in order to choose the 
most likely candidate in each context. Items 
can be single words or multiword expressions. 
The semantic tags indicate semantic fields 
which group together word senses that are 
related by virtue of their being connected at 
some level of generality with the same mental 
concept. The groups include not only syno-
nyms and antonyms but also hypernyms and 
hyponyms. 
The initial tagset was loosely based on 
Tom McArthur's Longman Lexicon of Con-
temporary English (McArthur, 1981) as this 
appeared to offer the most appropriate thesau-
rus type classification of word senses for this 
kind of analysis. The tagset has since been 
considerably revised in the light of practical 
tagging problems met in the course of the re-
search. The revised tagset is arranged in a 
hierarchy with 21 major discourse fields ex-
panding into 232 category labels. The follow-
ing list shows the 21 labels at the top level of 
the hierarchy (for the full tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas). 
 
1 This work is continuing to be supported by the Bene-
dict project, EU project IST-2001-34237. 
A general and abstract terms 
B the body and the individual 
C arts and crafts 
E emotion 
F food and farming 
G government and the public domain 
H architecture, buildings, houses and the 
home 
I money and commerce in industry 
K entertainment, sports and games 
L life and living things 
M movement, location, travel and trans-
port 
N numbers and measurement 
O substances, materials, objects and 
equipment 
P education 
Q linguistic actions, states and processes 
S social actions, states and processes 
T time 
W the world and our environment 
X psychological actions, states and 
processes 
Y science and technology 
Z names and grammatical words 
 
Currently, the lexicon contains just over 
37,000 words and the template list contains 
over 16,000 multiword units. These resources 
were created manually by extending and ex-
panding dictionaries from the CLAWS tagger 
with observations from large text corpora. 
Generally, only the base form of nouns and 
verbs are stored in the lexicon and a lemmati-
sation procedure is used for look-up. How-
ever, the base form is not sufficient in some 
cases. Stubbs (1996: 40) observes that ?mean-
ing is not constant across the inflected forms 
of a lemma?, and Tognini-Bonelli (2001: 92) 
notes that lemma variants have different 
senses. 
In the USAS lexicon, each entry consists 
of a word with one POS tag and one or more 
semantic tags assigned to it. At present, in 
cases where a word has more than one syntac-
tic tag, it is duplicated (i.e. each syntactic tag 
is given a separate entry).  
The semantic tags for each entry in the 
lexicon are arranged in approximate rank fre-
quency order to assist in manual post editing, 
and to allow for gross automatic selection of 
the common tag, subject to weighting by do-
main of discourse. 
In the multi-word-unit list, each template 
consists of a pattern of words and part-of-
speech tags. The semantic tags for each tem-
plate are arranged in rank frequency order in 
the same way as the lexicon. Various types of 
multiword expressions are included: phrasal 
verbs (e.g. stubbed out), noun phrases (e.g. ski 
boots), proper names (e.g. United States), true 
idioms (e.g. life of Riley).  
Figure 1 below shows samples of the actual 
templates used to identify these MWUs. Each 
of these example templates has only one se-
mantic tag associated with it, listed on the 
right-hand end of the template. However, the 
second example (ski boot) combines the 
clothing (B5) and sports (K5.1) fields into one 
tag. The pattern on the left of each template 
consists of a sequence of words joined to POS 
tags with the underscore character. The words 
and POS fields can include the asterisk wild-
card character to allow for inflectional vari-
ants and to write more powerful templates 
with wider coverage. USAS templates can 
match discontinuous MWUs, and this is illus-
trated by the first example, which includes 
optional intervening POS items marked 
within curly brackets. Thus this template can 
match stubbed out and stubbed the cigarette 
out. ?Np? is used to match simple noun 
phrases identified with a noun-phrase chun-
ker. 
 
stub*_* {Np/P*/R*} out_RP    O4.6- 
ski_NN1 boot*_NN*          B5/K5.1 
United_* States_N*              Z2 
life_NN1 of_IO Riley_NP1        K1 
 
Figure 1 Sample of USAS multiword templates 
 
As in the case of grammatical tagging, the 
task of semantic tagging subdivides broadly 
into two phases: Phase I (Tag assignment): 
attaching a set of potential semantic tags to 
each lexical unit and Phase II (Tag disam-
biguation): selecting the contextually appro-
priate semantic tag from the set provided by 
Phase I. USAS makes use of seven major 
techniques or sources of information in phase 
II. We will list these only briefly here, since 
they are described in more detail elsewhere 
(Garside and Rayson, 1997). 
  
1. POS tag. Some senses can be elimi-
nated by prior POS tagging. The CLAWS 
part-of-speech tagger is run prior to semantic 
tagging. 
2. General likelihood ranking for single-
word and MWU tags. In the lexicon and 
MWU list senses are ranked in terms of fre-
quency, even though at present such ranking 
is derived from limited or unverified sources 
such as frequency-based dictionaries, past 
tagging experience and intuition.  
3. Overlapping MWU resolution. Nor-
mally, semantic multi-word units take priority 
over single word tagging, but in some cases a 
set of templates will produce overlapping 
candidate taggings for the same set of words. 
A set of heuristics is applied to enable the 
most likely template to be treated as the pre-
ferred one for tag assignment.  
4. Domain of discourse. Knowledge of 
the current domain or topic of discourse is 
used to alter rank ordering of semantic tags in 
the lexicon and template list for a particular 
domain.  
5. Text-based disambiguation. It has 
been claimed (by Gale et al 1992) on the ba-
sis of corpus analysis that to a very large ex-
tent a word keeps the same meaning 
throughout a text. 
6. Contextual rules. The template 
mechanism is also used in identifying regular 
contexts in which a word is constrained to 
occur in a particular sense.  
7. Local probabilistic disambiguation. It 
is generally supposed that the correct seman-
tic tag for a given word is substantially de-
termined by the local surrounding context.  
 
After automatic tag assignment has been 
carried out, manual post-editing can take 
place, if desired, to ensure that each word and 
idiom carries the correct semantic classifica-
tion. 
From these seven disambiguation meth-
ods, our main interest in this paper is the third 
technique of overlapping MWU resolution. 
When more than one template match overlaps 
in a sentence, the following heuristics are ap-
plied in sequence: 
 
1. Prefer longer templates over shorter 
templates 
2. For templates of the same length, pre-
fer shorter span matches over longer 
span matches (a longer span indicates 
more intervening items for discon-
tinuous templates) 
3. If the templates do not apply to the 
same sequence of words, prefer the 
one that begins earlier in the sentence 
4. For templates matching the same se-
quence of words, prefer the one 
which contains the more fully defined 
template pattern (with fewer wild-
cards in the word fields) 
5. Prefer templates with a more fully de-
fined first word in the template 
6. Prefer templates with fewer wildcards 
in the POS tags 
 
These six rules were found to differentiate 
in all cases of overlapping MWU templates. 
Cases which failed to be differentiated indi-
cated that two (or more) templates in our 
MWU list were in fact identical, apart from 
the semantic tags and required merging to-
gether. 
4 Experiment of MWE extraction 
In order to test our approach of extracting 
MWEs using semantic information, we first 
tagged the newspaper part of the METER 
Corpus with the USAS tagger. We then col-
lected the multiword units assigned as a single 
semantic unit. Finally, we manually checked 
the results. 
The Meter Corpus chosen as the test data 
is a collection of court reports from the Brit-
ish Press Association (PA) and some leading 
British newspapers (Gaizauskas 2001; Clough 
et al, 2002). In our experiment, we used the 
newspaper part of the corpus containing 774 
articles with more than 250,000 words. It pro-
vides a homogeneous corpus (in the sense that 
the reports come from a restricted domain of 
court events) and is thus a good source from 
which to extract domain-specific MWEs. 
Another reason for choosing this corpus is 
that it has not been used in training the USAS 
system. As an open test, we assume the re-
sults of the experiment should reflect true ca-
pability of our approach for real-life 
applications. 
The current USAS tagger may assign mul-
tiple possible semantic tags for a term when it 
fails to disambiguate between them. As men-
tioned previously, the first one denotes the 
most likely semantic field of the term. There-
fore, in our experiment we chose the first tag 
when such situations arose. 
A major problem we faced in our experi-
ment is the definition of a MWE. Although it 
has been several years since people started to 
work on MWE extraction, we found that there 
is, as yet, no available ?clear-cut? definition 
for MWEs. We noticed various possible defi-
nitions have been suggested for MWE/MWU. 
For example, Smadja (1993) suggests a basic 
characteristic of collocations and multiword 
units is recurrent, domain-dependent and co-
hesive lexical clusters. Sag et el. (2001b) sug-
gest that MWEs can roughly be defined as 
?idiosyncratic interpretations that cross word 
boundaries (or spaces)?. Biber et al (2003) 
describe MWEs as lexical bundles, which 
they go on to define as combinations of words 
that can be repeated frequently and tend to be 
used frequently by many different speak-
ers/writers within a register. 
Although it is not difficult to interpret 
these deifications in theory, things became 
much more complicated when we undertook 
our practical checking of the MWE candi-
dates. Quite often, we experienced disagree-
ment between us about whether or not to 
accept a MWE candidate as a good one. In 
practice, we generally followed Biber et al?s 
definition, i.e. accept a candidate MWE as a 
good one if it can repeatedly co-occur in the 
corpus. 
Another difficulty we experienced relates 
to estimating recall. Because the MWEs in the 
METER Corpus are not marked-up, we could 
not automatically calculate the number of 
MWEs contained in the corpus. Conse-
quently, we had to manually estimate this fig-
ure. Obviously it is not practical to manually 
check though the whole corpus within the 
limited time allowed. Therefore, we had to 
estimate the recall on a sample of the corpus, 
as will be described in the following section. 
5 Evaluation 
In this section, we analyze the results of 
the MWE extraction in detail for a full 
evaluation of our approach to MWE extrac-
tion. 
Overall, after we processed the test corpus, 
the USAS tagger extracted 4,195 MWE can-
didates from the test corpus. After manually 
checking through the candidates, we selected 
3,792 as good MWEs, resulting in overall 
precision of 90.39%. 
As we explained earlier, due to the diffi-
culty of obtaining the total number of true 
MWEs in the entire test corpus, we had to 
estimate recall of the MWE extraction on a 
sample corpus. In detail, we first randomly 
selected fifty texts containing 14,711 words 
from the test corpus, then manually marked-
up good MWEs in the sample texts, finally 
counted the number of the marked-up MWUs. 
As a result, 1,511 good MWEs were found in 
the sample. Since the number of automatically 
extracted good MWEs in the sample is 595, 
the recall on the sample is calculated as fol-
lows: 
Recall=(595?1511)?100%=39.38%. 
Considering the homogenous feature of 
the test data, we assume this local recall is 
roughly approximate to the global recall of 
the test corpus. 
To analyze the performance of USAS in 
respect to the different semantic field catego-
ries, we divided candidates according to the 
assigned semantic tag, and calculated the pre-
cision for each of them. Table 1 lists these 
precisions, sorting the semantic fields by the 
number of MWE candidates (refer to section 
3 for definitions of the twenty-one main se-
mantic field categories). As shown in this ta-
ble, the USAS semantic tagger obtained 
precisions between 91.23% to 100.00% for 
each semantic field except for the field of 
?names and grammatical words? denoted by 
Z. As Z was the biggest field (containing 
45.39% of the total MWEs and 43.12% of the 
accepted MWEs), we examined these MWEs 
more closely. We discovered that numerous 
pairs of words are tagged as person names 
(Z1) and geographical names (Z2) by mistake, 
e.g. Blackfriars crown (tagged as Z1), stabbed 
Constance (tagged as Z2) etc. 
 
Semantic 
field 
Total 
MWEs 
Accepted 
MWEs 
Precision 
Z 1,904  1,635 85.87% 
T 497  459 92.35% 
A 351  328 93.44% 
M 254  241 94.88% 
N 227  211 92.95% 
S 180  177 98.33% 
B 131  128 97.71% 
G 118  110 93.22% 
X 114  104 91.23% 
I 74  72 97.30% 
Q 67  63 94.03% 
E 58  53 91.38% 
H 53  52 98.11% 
K 48  45 93.75% 
P 39  37 94.87% 
O 32  29 90.63% 
F 24  24 100.00% 
L 11  11 100.00% 
Y 6  6 100.00% 
C 5  5 100.00% 
W 2  2 100.00% 
Total 4,195 3,792 90.39% 
 
Table 1: Precisions for different semantic catego-
ries 
 
Another possible factor that affects the 
performance of the USAS tagger is the length 
of the MWEs. To observe the performance of 
our approach from this perspective, we 
grouped the MWEs by their lengths, and then 
checked precision for each of the categories. 
Table 2 shows the results (once again, they 
are sorted in descending order by MWE 
lengths). As we might expect, the number of 
MWEs decreases as the length increases. In 
fact, bi-grams alone constitute 80.52% and 
81.88% of the candidate and accepted MWEs 
respectively. The precision also showed a 
generally increasing trend as the MWE length 
increases, but with a major divergence of tri-
grams. One main type of error occurred on tri-
grams is that those with the structure of 
CIW(capital-initial word) + conjunction + 
CIW tend to be tagged as Z2 (geographical 
name). The table shows relatively high preci-
sion for longer MWEs, reaching 100% for 6-
grams. Because the longest MWEs extracted 
have six words, no longer MWEs could be 
examined.  
 
MWE 
length 
Total 
MWEs 
Accepted 
MWEs 
Precision 
2 3,378 3,105 91.92% 
3 700 575 82.14% 
4 95 91 95.44% 
5 18 17 94.44% 
6 4 4 100.00% 
Total 4,195 3,792 90.39% 
 
Table 2: Precisions for MWEs of different lengths 
 
As discussed earlier, purely statistical al-
gorithms of MWE extraction generally filter 
out candidates of low frequencies. However, 
such low-frequency terms in fact form major 
part of MWEs in most corpora. In our study, 
we attempted to investigate the possibility of 
extracting low frequency MWEs by using 
semantic field annotation. We divided MWEs 
into different frequency groups, then checked 
precision for each of the categories. Table 3 
shows the results, which are sorted by the 
candidate MWE frequencies. As we expected, 
69.46% of the candidate MWEs and 68.22% 
of the accepted MWEs occur in the corpus 
only once or twice. This means that, with a 
frequency filter of Min(f)=3, a purely statisti-
cal algorithm would exclude more than half of 
the candidates from the process. 
 
Freq. of 
MWE 
Total 
number 
Accepted 
MWEs 
Precision 
1 2,164  1,892 87.43% 
2 750  695 92.67% 
3 - 4 616 570 92.53% 
5 - 7 357 345 96.64% 
8 - 20 253 238 94.07% 
21 - 117 55 52 94.55% 
Total 4,195 3,792 90.39% 
 
Table 3: Precisions for MWEs with different fre-
quencies 
 
Table 3 also displays an interesting rela-
tionship between the precisions and the fre-
quencies. Generally, we would expect better 
precisions for MWEs of higher frequencies, 
as higher co-occurrence frequencies are ex-
pected to reflect stronger affinity between the 
words within the MWEs. By and large, 
slightly higher precisions were obtained for 
the latter groups of higher frequencies (5?7, 
8-20 and 21-117) than those for the preceding 
lower frequency groups, i.e. 94.07%-96.64% 
versus 87.43%-92.67%. Nevertheless, for the 
latter three groups of the higher frequencies 
(5-7, 8-20 and 21?117) the precision did not 
increase as the frequency increases, as we 
initially expected. 
When we made a closer examination of 
the error MWEs in this frequency range, we 
found that some frequent domain-specific 
terms are misclassified by the USAS tagger. 
For example, since the texts in the test corpus 
are newspaper reports of court stories, many 
law courts (e.g. Manchester crown court, 
Norwich crown court) are frequently men-
tioned throughout the corpus, causing high 
frequencies of such terms (f=20 and f=31 re-
spectively). Unfortunately, the templates used 
in the USAS tagger did not capture them as 
complete terms. Rather, fragments were as-
signed a Z1 person name tag (e.g. Manchester 
crown). A solution to this type of problem is 
to improve the multiword unit templates used 
in the USAS tagger. Other possible solutions 
may include incorporating a statistical algo-
rithm to help detect boundaries of complete 
MWEs. 
When we examined the error distribution 
within the semantic fields more closely, we 
found that most errors occurred within the Z 
and T categories (refer to Table 1). The errors 
occurring in these semantic field categories 
and their sub-divisions make up 76.18% of 
the total errors (403). Table 4 shows the error 
distribution across 14 sub-divisions (for defi-
nitions of these subdivisions, see: website: 
http://www.comp.lancs.ac.uk/ucrel/usas). No-
tice that the majority of errors are from four 
semantic sub-categories: Z1, Z2, Z3 and T1.3. 
Notice, also, that the first two of these ac-
count for 60.55% of the total errors. This 
shows that the main cause of the errors in the 
USAS tool is the algorithm and lexical entries 
used for identifying names - personal and 
geographical and, to a lesser extent, the algo-
rithm and lexical entries for identifying peri-
ods of time. If these components of the USAS 
can be improved, a much higher precision can 
be expected. 
In sum, our evaluation shows that our se-
mantic approach to MWE extraction is effi-
cient in identifying MWEs, in particular those 
of lower frequencies. In addition, a reasona-
bly wide lexical coverage is obtained, as indi-
cated by the recall of 39.38%, which is 
important for terminology building. Our ap-
proach provides a practical way for extracting 
MWEs on a large scale, which we envisage 
can be useful for both linguistic research and 
practical NLP applications. 
 
Stag  Err. Stag  Err. 
Z1:person names 119 T1.1.1:time-past 1 
Z2:geog. names 125 T1.1.2:time-present 1 
Z3:other names 16 T1.2:time-momentary 8 
Z4:discourse bin 3 T1.3:time-period 23 
Z5:gram. bin 2 T2:time-begin/end 2 
Z8:pronouns etc. 2 T3:time-age 1 
Z99:unmatched 2 T4:time-early/late 2 
 
Table 4: Errors for some semantic sub-divisions 
6 Conclusion 
In this paper, we have shown that it is a 
practical way to extract MWEs using seman-
tic field information. Since MWEs are lexical 
units carrying single semantic concepts, it is 
reasonable to consider the issue of MWE ex-
traction as an issue of identifying word bun-
dles depicting single semantic units. The main 
difficulty facing such an approach is that very 
few reliable automatic tools available for 
identifying lexical semantic units. However, a 
semantic field annotator, USAS, has been 
built in Lancaster University. Although it was 
not built aiming to the MWE extraction, we 
thought it might be very well suited for this 
purpose. Our experiment shows that the 
USAS tagger is indeed an efficient tool for 
MWE extraction. 
Nevertheless, the current semantic tagger 
does not provide a complete solution to the 
problem. During our experiment, we found 
that not all of the multiword units it collects 
are valid MWEs. An efficient algorithm is 
needed for distinguishing between free word 
combinations and relatively fixed, closely 
affiliated word bundles. 
References 
Douglas Biber, Susan Conrad and Viviana Cortes. 
2003. Lexical bundles in speech and writing: an 
initial taxonomy. In A. Wilson, P. Rayson and 
T. McEnery (eds.) Corpus Linguistics by the 
Lune: a festschrift for Geoffrey Leech, pp. 71-
92.  Peter Lang, Frankfurt. 
Paul Clough, Robert Gaizauskas and S. L. Piao. 
2002. Building and annotating a corpus for the 
study of journalistic text reuse. In Proceedings 
of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC-2002), 
pp. 1678-1691. Los Palmas de Gran Canaria, 
Spain. 
Ido Dagan, and Ken Church. 1994. Termight: 
identifying and translating technical terminol-
ogy. In Proceedings of the 4th Conference on 
Applied Natural Language Processing, pp. 34-
40. Stuttgard, German. 
B?atrice Daille. 1995. Combined approach for 
terminology extraction: lexical statistics and 
linguistic filtering. Technical paper. UCREL, 
Lancaster University. 
Robert Gaizauskas, Jonathan Foster, Yorick 
Wilks, John Arundel, Paul Clough and Scott 
Piao. 2001. The METER corpus: a corpus for 
analysing journalistic text reuse. In the Pro-
ceedings of the Corpus Linguistics 2001, pp: 
214-223. Lancaster, UK. 
William Gale, Kenneth Church, and David 
Yarowsky. 1992. One sense per discourse. In 
Proceedings of the 4th DARPA Speech and 
Natural Language Workshop, pp 233-7. 
Roger Garside and Nick Smith. 1997. A hybrid 
grammatical tagger: CLAWS4. In R. Garside, 
G. Leech and A. McEnery (eds.), Corpus 
Annotation: Linguistic Information from 
Computer Text Corpora, pp. 102-121. Long-
man, London. Roger Garside and Paul Rayson. 1997. Higher-
level annotation tools. In. Roger Garside, Geof-
frey Leech, and Tony McEnery (eds.) Corpus 
Annotation: Linguistic Information from Com-
puter Text Corpora, pp. 179 - 193. Longman, 
London.  
Tom McArthur. 1981. Longman Lexicon of 
Contemporary English. Longman, London. 
Tony McEnery, Lang? Jean-Marc, Oakes Michael 
and V?ronis Jean. 1997. The exploitation of 
multilingual annotated corpora for term extrac-
tion. In Garside Roger, Leech Geoffrey and 
McEnery Anthony (eds), Corpus annotation --- 
linguistic information from computer text cor-
pora, pp 220-230. London & New York, Long-
man. 
Magnus Merkel and Mikael Andersson. 2000. 
Knowledge-lite extraction of multi-word units 
with language filters and entropy thresholds. In 
Proceedings of 2000 Conference User-Oriented 
Content-Based Text and Image Handling 
(RIAO'00), pages 737--746, Paris, France. 
Archibald Michiels and Nicolas Dufour.1998. 
DEFI, a tool for automatic multi-word unit rec-
ognition, meaning assignment and translation 
selection. In Proceedings of the First Interna-
tional Conference on Language Resources & 
Evaluation, pp. 1179-1186. Granada, Spain. 
Scott Songlin Piao and Tony McEnery. 2001. 
Multi-word unit alignment in English-Chinese 
parallel corpora. In the Proceedings of the Cor-
pus Linguistics 2001, pp. 466-475. Lancaster, 
UK. 
Ivan A. Sag, Francis Bond, Ann Copestake and 
Dan Flickinger. 2001a. Multiword Expressions. 
LinGO Working Paper No. 2001-01. Stanford 
University, CA. 
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake and Dan Flickinger. 2001b. Multi-
word Expressions: A Pain in the Neck for NLP. 
LinGO Working Paper No. 2001-03. Stanford 
University, CA. 
Frank Smadja. 1993. Retrieving collocations from 
text: Xtract. Computational Linguistics 
19(1):143-177. 
Michael Stubbs. 1996. Text and corpus analysis: 
computer-assisted studies of language and cul-
ture. Blackwell, Oxford. 
Elena Tognini-Bonelli. 2001. Corpus linguistics at 
work. Benjamins, The Netherlands. 
Eric Wehrli. 1998. Translating idioms. In Pro-
ceedings of COLING-ACL ?98, Montreal, Can-
ada, Vol. 2, pp. 1388-1392. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics 23(3): 377-
401. 
 
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 2?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Measuring MWE Compositionality Using Semantic Annotation 
Scott S. L. Piao1, Paul Rayson1, Olga Mudraya2, Andrew Wilson2 and Roger Garside1 
 
1Computing Department 
Lancaster University 
Lancaster, UK 
{s.piao, p.rayson, r.garside}@lancaster.ac.uk 
2Dept. of Linguistics and EL 
Lancaster University 
Lancaster, UK 
{o.mudraya, a.wilson}@lancaster.ac.uk 
 
 
Abstract 
This paper reports on an experiment in 
which we explore a new approach to the 
automatic measurement of multi-word 
expression (MWE) compositionality. We 
propose an algorithm which ranks MWEs 
by their compositionality relative to a 
semantic field taxonomy based on the 
Lancaster English semantic lexicon (Piao 
et al, 2005a). The semantic information 
provided by the lexicon is used for meas-
uring the semantic distance between a 
MWE and its constituent words. The al-
gorithm is evaluated both on 89 manually 
ranked MWEs and on McCarthy et als 
(2003) manually ranked phrasal verbs. 
We compared the output of our tool with 
human judgments using Spearman?s 
rank-order correlation coefficient. Our 
evaluation shows that the automatic rank-
ing of the majority of our test data 
(86.52%) has strong to moderate correla-
tion with the manual ranking while wide 
discrepancy is found for a small number 
of MWEs. Our algorithm also obtained a 
correlation of 0.3544 with manual rank-
ing on McCarthy et als test data, which 
is comparable or better than most of the 
measures they tested. This experiment 
demonstrates that a semantic lexicon can 
assist in MWE compositionality meas-
urement in addition to statistical algo-
rithms. 
1 Introduction 
Over the past few years, compositionality and 
decomposability of MWEs have become impor-
tant issues in NLP research. Lin (1999) argues 
that ?non-compositional expressions need to be 
treated differently than other phrases in many 
statistical or corpus?based NLP methods?. Com-
positionality means that ?the meaning of the 
whole can be strictly predicted from the meaning 
of the parts? (Manning & Sch?tze, 2000). On the 
other hand, decomposability is a metric of the 
degree to which the meaning of a MWE can be 
assigned to its parts (Nunberg, 1994; Riehemann, 
2001; Sag et al, 2002). These two concepts are 
closely related. Venkatapathy and Joshi (2005) 
suggest that ?an expression is likely to be rela-
tively more compositional if it is decomposable?. 
While there exist various definitions for 
MWEs, they are generally defined as cohesive 
lexemes that cross word boundaries (Sag et al, 
2002; Copestake et al, 2002; Calzolari et al, 
2002; Baldwin et al, 2003), which include 
nominal compounds, phrasal verbs, idioms, col-
locations etc. Compositionality is a critical crite-
rion cutting across different definitions for ex-
tracting and classifying MWEs. While semantics 
of certain types of MWEs are non-compositional, 
like idioms ?kick the bucket? and ?hot dog?, 
some others can have highly compositional se-
mantics like the expressions ?traffic light? and 
?audio tape?. 
Automatic measurement of MWE composi-
tionality can have a number of applications. One 
of the often quoted applications is for machine 
translation (Melamed, 1997; Hwang & Sasaki, 
2005), in which non-compositional MWEs need 
special treatment. For instance, the translation of 
a highly compositional MWE can possibly be 
inferred from the translations of its constituent 
words, whereas it is impossible for non-
compositional MWEs, for which we need to 
identify the translation equivalent for the MWEs 
as a whole. 
In this paper, we explore a new method of 
automatically estimating the compositionality of 
MWEs using lexical semantic information, 
sourced from the Lancaster semantic lexicon 
(Piao et al, 2005a) that is employed in the 
USAS1 tagger (Rayson et al, 2004). This is a 
                                                 
1 UCREL Semantic Analysis System 
2
large lexical resource which contains nearly 
55,000 single-word entries and over 18,800 
MWE entries. In this lexicon, each MWE2 and 
the words it contains are mapped to their poten-
tial semantic categories using a semantic field 
taxonomy of 232 categories. An evaluation of 
lexical coverage on the BNC corpus showed that 
the lexical coverage of this lexicon reaches 
98.49% for modern English (Piao et al, 2004).  
Such a large-scale semantic lexical resource al-
lows us to examine the semantics of many 
MWEs and their constituent words conveniently 
without resorting to large corpus data. Our ex-
periment demonstrates that such a lexical re-
source provides an additional approach for auto-
matically estimating the compositionality of 
MWEs. 
One may question the necessity of measuring 
compositionality of manually selected MWEs. 
The truth is, even if the semantic lexicon under 
consideration was compiled manually, it does not 
exclusively consist of non-compositional MWEs 
like idioms. Built for practical discourse analysis, 
it contains many MWEs which are highly com-
positional but depict certain entities or semantic 
concepts. This research forms part of a larger 
effort to extend lexical resources for semantic 
tagging. Techniques are described elsewhere 
(e.g. Piao et al, 2005b) for finding new candi-
date MWE from corpora. The next stage of the 
work is to semi-automatically classify these can-
didates using an existing semantic field taxon-
omy and, to assist this task, we need to investi-
gate patterns of compositionality. 
2 Related Work  
In recent years, various approaches have been 
proposed to the analysis of MWE compositional-
ity. Many of the suggested approaches employ 
statistical algorithms. 
One of the earliest studies in this area was re-
ported by Lin (1999) who assumes that ?non-
compositional phrases have a significantly dif-
ferent mutual information value than the phrases 
that are similar to their literal meanings? and 
proposed to identify non-compositional MWEs 
in a corpus based on distributional characteristics 
of MWEs. Bannard et al (2003) tested tech-
niques using statistical models to infer the mean-
ing of verb-particle constructions (VPCs), focus-
                                                 
2 In this lexicon, many MWEs are encoded as templates, 
such as driv*_* {Np/P*/J*/R*} mad_JJ,  which represent 
variational forms of a single MWE, For further details, see 
Rayson et al, 2004.  
ing on prepositional particles. They tested four 
methods over four compositional classification 
tasks, reporting that, on all tasks, at least one of 
the four methods offers an improvement in preci-
sion over the baseline they used. 
McCarthy et al (2003) suggested that compo-
sitional phrasal verbs should have similar 
neighbours as for their simplex verbs. They 
tested various measures using the nearest 
neighbours of phrasal verbs and their simplex 
counterparts, and reported that some of the 
measures produced results which show signifi-
cant correlation with human judgments. Baldwin 
et al (2003) proposed a LSA-based model for 
measuring the decomposability of MWEs by ex-
amining the similarity between them and their 
constituent words, with higher similarity indicat-
ing the greater decomposability.  They evaluated 
their model on English noun-noun compounds 
and verb-particles by examining the correlation 
of the results with similarities and hyponymy 
values in WordNet. They reported that the LSA 
technique performs better on the low-frequency 
items than on more frequent items. Venkatapathy 
and Joshi (2005) measured relative composition-
ality of collocations having verb-noun pattern 
using a SVM (Support Vector Machine) based 
ranking function. They integrated seven various 
collocational and contextual features using their 
ranking function, and evaluated it against manu-
ally ranked test data. They reported that the SVM 
based method produces significantly better re-
sults compared to methods based on individual 
features. 
The approaches mentioned above invariably 
depend on a variety of statistical contextual in-
formation extracted from large corpus data. In-
evitably, such statistical information can be af-
fected by various uncontrollable ?noise?, and 
hence there is a limitation to purely statistical 
approaches. 
In this paper, we contend that a manually 
compiled semantic lexical resource can have an 
important part to play in measuring the composi-
tionality of MWEs. While any approach based on 
a specific lexical resource may lack generality, it 
can complement purely statistical approaches by 
importing human expert knowledge into the 
process. Particularly, if such a resource has a 
high lexical coverage, which is true in our case, 
it becomes much more useful for dealing with 
general English. It should be emphasized that we 
propose our semantic lexical-based approach not 
as a substitute for the statistical approaches. 
3
Rather we propose it as a potential complement 
to them.   
In the following sections, we describe our ex-
periment and explore this approach to the issue 
of automatic estimation of MWE compositional-
ity. 
3 Measuring MWE compositionality 
with semantic field information 
In this section, we propose an algorithm for 
automatically measuring MWE compositionality 
based on the Lancaster semantic lexicon. In this 
lexicon, the semantic field of each word and 
MWE is encoded in the form of semantic tags. 
We contend that the compositionality of a MWE 
can be estimated by measuring the distance be-
tween semantic fields of an MWE and its con-
stituent words based on the semantic field infor-
mation available from the lexicon. 
The lexicon employs a taxonomy containing 
21 major semantic fields which are further di-
vided into 232 sub-categories. 3  Tags are de-
signed to denote the semantic fields using letters 
and digits. For instance, tag N3.2 denotes the 
category of {SIZE} and Q4.1 denotes {media: 
Newspapers}. Each entry in the lexicon maps a 
word or MWE to its potential semantic field 
category/ies. More often than not, a lexical item 
is mapped to multiple semantic categories, re-
flecting its potential multiple senses. In such 
cases, the tags are arranged by the order of like-
lihood of meanings, with the most prominent one 
at the head of the list. For example, the word 
?mass? is mapped to tags N5, N3.5, S9, S5 and 
B2, which denote its potential semantic fields of 
{QUANTITIES},  {MEASUREMENT: 
WEIGHT}, {RELIGION AND SUPERNATU-
RAL}, {GROUPS AND AFFILIATION} and 
{HEALTH AND DISEASE}. 
 The lexicon provides direct access to the se-
mantic field information for large number of 
MWEs and their constituent words. Furthermore, 
the lexicon was analysed and classified manually 
by a team of linguists based on the analysis of 
corpus data and consultation of printed and elec-
tronic corpus-based dictionaries, ensuring a high 
level of consistency and accuracy of the semantic 
analysis.  
In our context, we interpret the task of measur-
ing the compositionality of MWEs as examining 
the distance between the semantic tag of a MWE 
and the semantic tags of its constituent words. 
                                                 
3 For the complete semantic tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas/ 
Given a MWE M and its constituent words wi (i 
= 1, .., n), the compositionality D can be meas-
ured by multiplying the semantic distance SD 
between M and each of its constituent words wi. 
In practice, the square root of the product is used 
as the score in order to reduce the range of actual 
D-scores, as shown below: 
 
(1)   ?
=
=
n
i
iwMSDMD
1
),()(  
 
where D-score ranges between [0, 1], with 1 in-
dicating the strongest compositionality and 0 the 
weakest compositionality. 
In the semantic lexicon, as the semantic in-
formation of function words is limited, they are 
classified into a single grammatical bin (denoted 
by tag Z5). In our algorithm, they are excluded 
from the measuring process by using a stop word 
list. Therefore, only the content constituent 
words are involved in measuring the composi-
tionality. Although function words may form an 
important part of many MWEs, such as phrasal 
verbs, because our algorithm solely relies on se-
mantic field information, we assume they can be 
ignored.  
 The semantic distance between a MWE and 
any of its constituent words is calculated by 
quantifying the similarity between their semantic 
field categories. In detail, if the MWE and a con-
stituent word do not share any of the major 21 
semantic domains, the SD is assigned a small 
value ?.4 If they do, three possible cases are con-
sidered: 
 
Case a. If they share the same tag, and the con-
stituent word has only one tag, then SD 
is one. 
Case b. If they share a tag or tags, but the con-
stituent words have multiple candidate 
tags, then SD is weighted using a vari-
able ? based on the position of the 
matched tag in the candidate list as well 
as the number of candidate tags. 
Case c. If they share a major category, but their 
tags fall into different sub-categories 
(denoted by the trailing digits following 
a letter), SD is further weighted using a 
                                                 
4 We avoid using zero here in order to avoid producing se-
mantic distance of zero indiscriminately when any one of 
the constituent words produces zero distance regardless of 
other constituent words. 
4
variable ? which reflects the difference 
of the sub-categories. 
With respect to weight ?, suppose L is the 
number of candidate tags of the constituent word 
under consideration, N is the position of the spe-
cific tag in the candidate list (the position starts 
from the top with N=1), then the weight ? is cal-
culated as 
 
(2)  
2
1
L
NL +?=? , 
 
where N=1, 2, ?, n and N<=L. Ranging between 
[1, 0), ? takes into account both the location of 
the matched tag in the candidate tag list and the 
number of candidate tags. This weight penalises 
the words having more candidate semantic tags 
by giving a lower value for their higher degree of 
ambiguity. As either L or N increases, the ?-
value decreases.       
Regarding the case c), where the tags share the 
same head letter but different digit codes, i.e. 
they are from the same major category but in 
different sub-categories, the weight ? is calcu-
lated based on the number of sub-categories they 
share. As we mentioned earlier, a semantic tag 
consists of an initial letter and some trailing dig-
its divided by points, e.g. S1.1.2 {RECIPROC-
ITY}, S1.1.3 {PARTICIPATION}, S1.1.4 {DE-
SERVE} etc. If we let T1, T2 be a pair of semantic 
tags with the same initial letters, which have ki 
and kj trailing digit codes (denoting the number 
of sub-division layers) respectively and share n 
digit codes from the left, or from the top layer, 
then ? is calculated as follows: 
 
(3)   
k
n=? ; 
(4)   . ),max( ji kkk =
 
where ? ranges between (0, 1). In fact, the cur-
rent USAS taxonomy allows only the maximum 
three layers of sub-division, therefore ? has one 
of three possible scores: 0.500 (1/2), 0.333 (1/3) 
and 0.666 (2/3). In order to avoid producing zero 
scores, if the pair of tags do not share any digit 
codes except the head letter, then n is given a 
small value of 0.5. 
Combining all of the weighting scores, the 
semantic distance SD in formula (1) is calculated 
as follows: 
 
(5)  ( )
??
?
?
??
?
?
?
=
?
?
=
=
.  then   c), if
;  then   b), if
1;  then   a), if
;   then   matches,   tagno if
,
1
1
n
i
ii
n
i
iiwMSD
??
?
?
 
where ? is given a small value of 0.001 for our 
experiment5. 
Some MWEs and single words in the lexicon 
are assigned with combined semantic categories 
which are considered to be inseparable, as shown 
below: 
petrol_NN1 station_NN1 M3/H1 
where the slash means that this MWE falls under 
the categories of M3 {VEHICLES AND TRANS-
PORTS ON LAND} and H1 {ARCHITECTURE 
AND KINDS OF HOUSES AND BUILDINGS} 
at the same time. For such cases, criss-cross 
comparisons between all possible tag pairs are 
carried out in order to find the optimal match 
between the tags of the MWE and its constituent 
words. 
By way of further explanation, the word 
?brush? as a verb has candidate semantic tags of 
B4 {CLEANING AND PERSONAL CARE} and 
A1.1.1 {GENERAL ACTION, MAKING} etc. On 
the other hand, the phrasal verb ?brush down? 
may fall under either B4 category with the sense 
of cleaning or G2.2 category {ETHICS} with the 
sense of reprimand. When we apply our algo-
rithm to it, we get the D-score of 1.0000 for the 
sense of cleaning, indicating a high degree of 
compositionality, whereas we get a low D-score 
of 0.0032 for the sense of reprimand, indicating 
a low degree of compositionality. Note that the 
word ?down? in this MWE is filtered out as it is 
a functional word. 
The above example has only a single constitu-
ent content word. In practice, many MWEs have 
more complex structures than this example. In 
order to test the performance of our algorithm, 
we compared its output against human judgments 
of compositionality, as reported in the following 
section. 
4 Manually Ranking MWEs for 
Evaluation 
In order to evaluate the performance of our 
tool against human judgment, we prepared a list 
                                                 
5 As long as ? is small enough, it does not affect the ranking 
of D-scores. 
5
of 89 MWEs6 and asked human raters to rank 
them via a website. The list includes six MWEs 
with multiple senses, and these were treated as 
separate MWE. The Lancaster MWE lexicon has 
been compiled manually by expert linguists, 
therefore we assume that every item in this lexi-
con is a true MWE, although we acknowledge 
that some errors may exist. 
Following McCarthy et al?s approach, we 
asked the human raters to assign each MWE a 
number ranging between 0 (opaque) and 10 
(fully compositional). Both native and non-native 
speakers are involved, but only the data from 
native speakers are used in this evaluation. As a 
result, three groups of raters were involved in the 
experiment.  Group 1 (6 people) rated MWEs 
with indexes of 1-30, Group 2 (4 people) rated 
MWEs with indexes of 31-59 and Group 3 (five 
people) rated MWEs with indexes of 6-89. 
In order to test the level of agreement between 
the raters, we used the procedures provided in 
the 'irr' package for R (Gamer, 2005). With this 
tool, the average intraclass correlation coefficient 
(ICC) was calculated for each group of raters 
using a two-way agreement model (Shrout & 
Fleiss, 1979). As a result, all ICCs exceeded 0.7 
and were significant at the 95% confidence level, 
indicating an acceptable level of agreement be-
tween raters. For Group 1, the ICC was 0.894 
(95% ci = 0.807 < ICC < 0.948), for Group 2 it 
was 0.9 (95% ci=0.783<ICC<0.956) and for 
Group 3 it was 0.886 (95% ci =  0.762 < ICC < 
0.948). 
Based on this test, we conclude that the man-
ual ranking of the MWEs is reliable and is suit-
able to be used in our evaluation. Source data for 
the human judgements is available from our 
website in spreadsheet form7. 
5 Evaluation 
In our evaluation, we focused on testing the 
performance of the D-score against human rat-
ers? judgment on ranking different MWEs by 
their degree of compositionality, as well as dis-
tinguishing the different degrees of composition-
ality for each sense in the case of multiple tags.  
The first step of the evaluation was to imple-
ment the algorithm in a program and run the tool 
on the 89 test MWEs we prepared. Fig. 1 illus-
trates the D-score distribution in a bar chart. As 
shown by the chart, the algorithm produces a 
widely dispersed distribution of D-scores across 
                                                 
6 Selected at random from the Lancaster semantic lexicon. 
7 http://ucrel.lancs.ac.uk/projects/assist/ 
the sample MWEs, ranging from 0.000032 to 
1.000000. For example, the tool assigned the 
score of 1.0 to the FOOD sense and 0.001 to the 
THIEF senses of ?tea leaf? successfully distin-
guishing the different degrees of compositional-
ity of these two senses. 
 
MWE Compositionality Distribution
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86
89 MWEs
D
-s
co
re
 
 
Fig 1: D-score distribution across 89 sample 
MWEs 
 
As shown in Fig. 1, some MWEs share the 
same scores, reflecting the limitation of the num-
ber of ranks that our algorithm can produce as 
well as the limited amount of semantic informa-
tion available from a lexicon. Nonetheless, the 
algorithm produced 45 different scores which 
ranked the MWEs into 45 groups (see the steps 
in the figure). Compared to the eleven scores 
used by the human raters, this provides a fine-
grained ranking of the compositionality.   
The primary issue in our evaluation is the ex-
tent to which the automatic ranking of the MWEs 
correlates with the manual ranking of them. As 
described in the previous section, we created a 
list of 89 manually ranked MWEs for this pur-
pose. Since we are mainly interested in the ranks 
rather than the actual scores, we examined the 
correlation between the automatic and manual 
rankings using Spearman?s correlation coeffi-
cient. (For the full ranking list, see Appendix). 
In the manually created list, each MWE was 
ranked by 3-6 human raters. In order to create a 
unified single test data of human ranking, we 
calculated the average of the human ranks for 
each MWE. For example, if two human raters 
give ranks 3 and 4 to a MWE, then its rank is 
(3+4)/2=3.5. Next, the MWEs are sorted by the 
averaged ranks in descending order to obtain the 
combined ranks of the MWEs. Finally, we sorted 
the MWEs by the D-score in the same way to 
obtain a parallel list of automatic ranks. For the 
calculation of Spearman?s correlation coefficient, 
if n MWEs are tied to a score (either D-score or 
the average manual ranks), their ranks were ad-
6
justed by dividing the sum of their ranks by the 
number of MWEs involved. Fig. 2 illustrates the 
correspondence between the adjusted automatic 
and manual rankings. 
 
Auto vs. Manual Ranks Comparison
(n=89, rho=0.2572)
0
20
40
60
80
100
0 20 40 60 80 100
auto ranks
m
an
ua
l r
an
ks
 
 
Fig. 2: Scatterplot of automatic vs. manual 
ranking. 
 
As shown in Fig. 2, the overall correlation seems 
quite weak. In the automatic ranking, quite a few 
MWEs are tied up to three ranks, illustrated by 
the vertically aligned points. The precise correla-
tion between the automatic and manual rankings 
was calculated using the function provided in R 
for Windows 2.2.1.  Spearman's rank correlation 
(rho) for these data was 0.2572 (p=0.01495), 
indicating a significant though rather weak posi-
tive relationship. 
In order to find the factors causing this weak 
correlation, we tested the correlation for those 
MWEs whose rank differences were less than 20, 
30, 40 and 50 respectively. We are interested to 
find out how many of them fall under each of the 
categories and which of their features affected 
the performance of the algorithm. As a result, we 
found 43, 54, 66 and 77 MWEs fall under these 
categories respectively, which yield different 
correlation scores, as shown in Table 1.  
 
numb of 
MWEs 
Percent 
(%) 
Rank 
diff 
rho-
score 
Sig. 
43 48.31 <20 0.9149 P<0.001 
54 60.67 <30 0.8321 P<0.001 
66 74.16 <40 0.7016 P<0.001 
77 86.52 <50 0.5084 P<0.001 
89 (total) 100.00 <=73 0.2572 P<0.02 
 
Table 1: Correlation coefficients corresponding 
different rank differences. 
 
As we expected, the rho decreases as the rank 
difference increases, but all of the four categories 
containing a total of 77 MWEs (86.52%) show 
reasonably high correlations, with the minimum 
score of 0.5084. 8 In particular, 66 of them 
(74.16%), whose ranking differences are less 
than 40, demonstrate a strong correlation with 
rho-score 0.7016, as illustrated by Fig. 3 
 
ScatterPlot of Auto vs. Man Ranks for 66 MWEs
(rank_diff < 40)
0
20
40
60
80
100
0 20 40 60 80 10auto ranks
m
an
 r
an
ks
0
 
 
Fig 3: ScatterPlot for 66 MWEs (rank_diff < 
40) which shows a strong correlation 
 
Our manual examination shows that the algo-
rithm generally pushes the highly compositional 
and non-compositional MWEs towards opposite 
ends of the spectrum of the D-score. For example, 
those assigned with score 1 include ?aid worker?, 
?audio tape? and ?unemployment figure?. On the 
other hand, MWEs such as ?tea leaf? (meaning 
thief), ?kick the bucket? and ?hot dog? are given 
a low score of 0.001. We assume these two 
groups of MWEs are generally treated as highly 
compositional and opaque MWEs respectively. 
However, the algorithm could be improved. A 
major problem found is that the algorithm pun-
ishes longer MWEs which contain function 
words. For example, ?make an appearance? is 
scored 0.000114 by the algorithm, but when the 
article ?an? is removed, it gets a higher score 
0.003608. Similarly, when the preposition ?up? 
is removed from ?keep up appearances?, it gets 
0.014907 compared to the original 0.000471, 
which would push up their rank much higher. To 
address this problem, the algorithm needs to be 
refined to minimise the impact of the function 
words to the scoring process. 
Our analysis also reveals that 12 MWEs with 
rank differences (between automatic and manual 
ranking) greater than 50 results in a degraded 
overall correlation. Table 2 lists these words, in 
which the higher ranks indicate higher composi-
tionality.  
 
                                                 
8 Salkind (2004: 88) suggests that r-score ranges 0.4~0.6, 
0.6~0.8 and 0.8~1.0 indicate moderate, strong and very 
strong correlations respectively. 
7
MWE Sem. Tag9 Auto 
rank 
Manual 
rank 
plough into A9- 53.5 3 
Bloody Mary F2 53.5 2 
pillow fight K6 26 80.5 
lollipop lady M3/S2 70 15 
cradle snatcher S3.2/T3/S2 73.5 17.5 
go bananas X5.2+++ 65 8.5 
make an appearance S1.1.3+ 2 58.5 
keep up appearances A8/S1.1.1 4 61 
sandwich course P1 69 11.5 
go bananas B2-/X1 68 10 
Eskimo roll M4 71.5 5 
in other words Z4 12.5 83 
 
Table 2: Twelve MWEs having rank differences 
greater than 50. 
 
Let us take ?pillow fight? as an example. The 
whole expression is given the semantic tag K6, 
whereas neither ?pillow? nor ?fight? as individ-
ual word is given this tag. In the lexicon, ?pil-
low? is classified as H5 {FURNITURE AND 
HOUSEHOLD FITTINGS} and ?fight? is as-
signed to four semantic categories including S8- 
{HINDERING}, X8+ {HELPING}, E3- {VIO-
LENT/ANGRY}, and K5.1 {SPORTS}. For this 
reason, the automatic score of this MWE is as 
low as 0.003953 on the scale of [0, 1]. On the 
contrary, human raters judged the meaning of 
this expression to be fairly transparent, giving it 
a high score of 8.5 on the scale of [0, 10]. Similar 
contrasts occurred with the majority of the 
MWEs with rank differences greater than 50, 
which are responsible for weakening the overall 
correlation. 
Another interesting case we noticed is the 
MWE ?pass away?. This MWE has two major 
senses in the semantic lexicon L1- {DIE} and 
T2- {END} which were ranked separately. Re-
markably, they were ranked in the opposite order 
by human raters and the algorithm. Human raters 
felt that the sense DIE is less idiomatic, or more 
compositional, than END, while the algorithm 
indicated otherwise. The explanation of this 
again lies in the semantic classification of the 
lexicon, where ?pass? as a single word contains 
the sense T2- but not L1-. Consequently, the 
automatic score for ?pass away? with the sense 
                                                 
                                                
9 Semantic tags occurring in Table 2: A8 (seem), A9 (giving 
possession), B2 (health and disease), F2 (drink), K6 (chil-
dren?s games and toys), M3 (land transport), M4 (swim-
ming), P1 (education), S1.1.1 (social actions), S1.1.3 (par-
ticipation), S2 (people), S3.2 (relationship), T3 (time: age), 
X1 (psychological actions), X5.2 (excited), Z4 (discourse 
bin) 
L1- is much lower (0.001) than that with the 
sense of T2- (0.007071). 
In order to evaluate our algorithm in compari-
son with previous work, we also tested it on the 
manual ranking list created by McCarthy et al
(2003).10 We found that 79 of the 116 phrasal 
verbs in that list are included in the Lancaster 
semantic lexicon. We applied our algorithm on 
those 79 items to compare the automatic ranks 
against the average manual ranks using the 
Spearman?s rank correlation coefficient (rho). As 
a result, we obtained rho=0.3544 with signifi-
cance level of p=0.001357. This result is compa-
rable with or better than most measures reported 
by McCarthy et al(2003). 
6 Discussion 
The algorithm we propose in this paper is dif-
ferent from previous proposed statistical methods 
in that it employs a semantic lexical resource in 
which the semantic field information is directly 
accessible for both MWEs and their constituent 
words. Often, typical statistical algorithms meas-
ure the semantic distance between MWEs and 
their constituent words by comparing their con-
texts comprising co-occurrence words in near 
context extracted from large corpora, such as 
Baldwin et als algorithm (2003). 
When we consider the definition of the com-
positionality as the extent to which the meaning 
of the MWE can be guessed based on that of its 
constituent words, a semantic lexical resource 
which maps MWEs and words to their semantic 
features provides a practical way of measuring 
the MWE compositionality. The Lancaster se-
mantic lexicon is one such lexical resource 
which allows us to have direct access to semantic 
field information of large number of MWE and 
single words. Our experiment demonstrates the 
potential value of such semantic lexical resources 
for the automatic measurement of MWE compo-
sitionality. Compared to statistical algorithms 
which can be affected by a variety of un-
controllable factors, such as size and domain of 
corpora, etc., an expert-compiled semantic lexi-
cal resource can provide much more reliable and 
?clean? lexical semantic information. 
However, we do not suggest that algorithms 
based on semantic lexical resources can substi-
tute corpus-based statistical algorithms. Rather, 
we suggest it as a complement to existing statis-
tical algorithms. As the errors of our algorithm 
 
10This list is available at website: 
http://mwe.stanford.edu/resources/  
8
reveal, the semantic information provided by the 
lexicon alone may not be rich enough for a very 
fine-grained distinction of MWE compositional-
ity. In order to obtain better results, this algo-
rithm needs to be combined with statistical tech-
niques. 
A limitation of our approach is language-
dependency. In order to port our algorithm to 
languages other than English, one needs to build 
similar semantic lexicon in those languages. 
However, similar semantic lexical resources are 
already under construction for some other lan-
guages, including Finnish and Russian (L?fberg 
et al, 2005; Sharoff et al, 2006), which will al-
low us to port our algorithm to those languages. 
7 Conclusion 
In this paper, we explored an algorithm based 
on a semantic lexicon for automatically measur-
ing the compositionality of MWEs. In our 
evaluation, the output of this algorithm showed 
moderate correlation with a manual ranking. We 
claim that semantic lexical resources provide 
another approach for automatically measuring 
MWE compositionality in addition to the exist-
ing statistical algorithms. Although our results 
are not yet conclusive due to the moderate scale 
of the test data, our evaluation demonstrates the 
potential of lexicon-based approaches for the 
task of compositional analysis. We foresee, by 
combining our approach with statistical algo-
rithms, that further improvement can be ex-
pected. 
8 Acknowledgement 
The work reported in this paper was carried 
out within the UK-EPSRC-funded ASSIST Pro-
ject (Ref. EP/C004574). 
References  
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, 
and Dominic Widdows. 2003. An Empirical Model 
of Multiword Expression Compositionality. In 
Proc. of the ACL-2003 Workshop on Multiword 
Expressions: Analysis, Acquisition and Treatment,  
pages 89-96, Sapporo, Japan. 
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proc. of the ACL2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pages 65?72, Sapporo. 
Nicoletta Calzolari, Charles Fillmore, Ralph Grish-
man, Nancy Ide, Alessandro Lenci, Catherine 
MacLeod, and Antonio Zampolli. 2002. Towards 
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the Third Interna-
tional Conference on Language Resources and 
Evaluation (LREC 2002), pages 1934?1940, Las 
Palmas, Canary Islands. 
Ann Copestake, Fabre Lambeau, Aline Villavicencio, 
Francis Bond, Timothy Baldwin, Ivan A. Sag, and 
Dan Flickinger. 2002. Multiword expressions: Lin-
guistic precision and reusability. In Proc. of the 
Third International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 1941?
1947, Las Palmas, Canary Islands. 
Matthias  Gamer. 2005. The irr Package: Various Co-
efficients of Interrater Reliability and Agreement. 
Version 0.61 of 11 October 2005.  Available from:   
cran.r-project.org/src/contrib/Descriptions/irr.html 
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual 
Meeting of the ACL, pages 317?324, College Park, 
USA. 
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka 
Juntunen, Asko Nyk?nen, and Krista Varantola. 
2005. A semantic tagger for the Finnish language. 
In Proc. of the Corpus Linguistics 2005 conference, 
Birmingham, UK. 
Christopher D. Manning and Hinrich Sch?tze. 2000. 
Foundations of Statistical Natural Language Proc-
essing. The MIT Press, Cambridge, Massachusetts. 
Diana McCarthy, Bill Keller, and John Carroll. 2003. 
Detecting a continuum of compositionality in 
phrasal verbs. In Proc. of the ACL-2003 Workshop 
on Multiword Expressions: Analysis, Acquisition 
and Treatment, pages 73?80, Sapporo, Japan. 
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing , Providence, USA. 
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 
1994. Idioms. Language, 70: 491?538. 
Scott S.L. Piao, Paul Rayson, Dawn Archer and Tony 
McEnery. 2004. Evaluating Lexical Resources for 
a Semantic Tagger. In Proc. of LREC-04, pages 
499?502, Lisbon, Portugal. 
Scott S.L. Piao, Dawn Archer, Olga Mudraya, Paul 
Rayson, Roger Garside, Tony McEnery and An-
drew Wilson. 2005a. A Large Semantic Lexicon 
for Corpus Annotation. In Proc. of the Corpus Lin-
guistics Conference 2005, Birmingham, UK. 
Scott S.L. Piao., Paul Rayson, Dawn Archer, Tony 
McEnery. 2005b. Comparing and combining a se-
mantic tagger and a statistical tool for MWE ex-
traction. Computer Speech and Language, 19, 4: 
378?397. 
9
Paul Rayson, Dawn Archer, Scott Piao, and Tony 
McEnery. 2004. The UCREL Semantic Analysis 
System. In Proc. of LREC-04 Workshop: Beyond 
Named Entity Recognition Semantic Labeling for 
NLP Tasks, pages 7?12, Lisbon, Portugal. 
Susanne Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. the-
sis, Stanford University, Stanford. 
 Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake, and Dan Flickinger. 2002. Multiword 
Expressions: A Pain in the Neck for NLP. In Proc. 
of the 3rd International Conference on Intelligent 
Text Processing and Computational Linguistics 
(CICLing-2002), pages 1?15, Mexico City, Mexico. 
Neil J. Salkind. 2004. Statistics for People Who Hate 
Statistics. Sage: Thousand Oakes, US. 
Serge Sharoff, Bogdan Babych, Paul Rayson, Olga 
Mudraya and Scott Piao. 2006. ASSIST: Auto-
mated semantic assistance for translators. Proceed-
ings of EACL 2006, pages 139?142, Trento, Italy. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intra-
class Correlations: Uses in Assessing Rater Reli-
ability. Psychological Bulletin (2), 420?428. 
Sriram Venkatapathy and Aravind K. Joshi. 2005. 
Measuring the relative compositionality of verb-
noun (V-N) collocations by integrating features. In 
Proc. of Human Language Technology Conference 
and Conference on Empirical Methods in Natural 
Language Processing (HLT/EMNLP 2005), pages 
899?906, Vancouver, Canada. 
 
Appendix: Manual vs. Automatic Ranks 
of Sample MWEs 
The table below shows the human and auto-
matic rankings of 89 sample MWEs. The MWEs 
are sorted in ascending order by manual average 
ranks. The top items are supposed to be the most 
compositional ones. For example, according to 
the manual ranking, facial expression is the most 
compositional MWE while tea leaf is the most 
opaque one. This table also shows that some 
MWEs are tied up with the same ranks. For the 
definitions of the full semantic tagset, see web-
site http://www.comp.lancs.ac.uk/ucrel/usas/. 
 
MWE Tag Sem tag Man 
rank 
Auto. 
rank 
facial expression B1 1 9 
aid worker S8/S2 2 4 
audio tape K3 3.5 4 
leisure activities K1 3.5 36.5 
advance warning T4/Q2.2 5 36.5 
living space H2 6 51 
in other words Z4 7 77.5 
unemployment fig-
ures 
I3.1/N5 8 4 
camera angle Q4.3 9.5 45 
pillow fight K6 9.5 64 
youth club S5/T3 11.5 4 
petrol station M3/H1 11.5 36.5 
palm tree L3 13 9 
rule book G2.1/Q4.1 14 4 
ball boy K5.1/S2.2 15 13 
goal keeper K5.1/S2 16.5 4 
kick in E3- 16.5 36.5 
ventilation shaft H2 18 47 
directory enquiries Q1.3 19 14 
phone box Q1.3/H1 21 18.5 
lose balance M1 21 53 
bend the rules A1.7 21 54.5 
big nose X7/X2.4 23 67 
quantity control N5/A1.7 24 11.5 
act of God S9 25 36.5 
air bag A15/M3 26 62.5 
mind stretching A12 27 59 
plain clothes B5 28 36.5 
keep up appearances A8/S1.1.1 29 86 
examining board P1 30 23 
open mind X6 31.5 49 
make an appearance S1.1.3+ 31.5 88 
cable television Q4.3 33 15 
king size N3.2 34 36.5 
action point X7 35 61 
keep tight rein on A1.7 36 28 
noughts and crosses K5.2 37 77.5 
tea leaf L3/F2 38 4 
single minded X5.1 39.5 77.5 
window dressing I2.2 39.5 77.5 
street girl G1.2/S5 42 36.5 
just over the horizon S3.2/S2.1 42 60 
pressure group T1.1.3 42 16.5 
air proof O4.1 44.5 57.5 
heart of gold S1.2.2 44.5 77.5 
lose heart X5.2 46 26 
food for thought X2.1/X5.1 47 89 
play part S8 48 68 
look down on S1.2.3 49 77.5 
arm twisting Q2.2 50 36.5 
take into account A1.8 51 69 
kidney bean F1 52 9 
come alive A3+ 53 52 
break new ground T3/T2 54 54 
make up to S1.1.2 55 65 
by virtue of C1 56.5 36.5 
snap shot A2.2 56.5 27 
pass away L1- 58 77.5 
long face E4.1 59 77.5 
bossy boots S1.2.3/S2 60 77.5 
plough into M1/A1.1.2 61 11.5 
kick in T2+ 62 50 
animal magnetism S1.2 63 55.5 
sixth former P1/S2 64 77.5 
pull the strings S7.1 65 62.5 
couch potato A1.1.1/S2 66 77.5 
think tank S5/X2.1 67 36.5 
come alive X5.2+ 68 24 
hot dog F1 69 77.5 
cheap shot G2.2-/Q2.2 70 66 
10
rock and roll K2 71 48 
bright as a button S3.2/T3/S2 72.5 87 
cradle snatcher X9.1+ 72.5 16.5 
alpha wave B1 74 77.5 
lollipop lady M3/S2 75 20 
pass away X5.2+ 76.5 57.5 
plough into T2- 76.5 36.5 
piece of cake P1 78.5 77.5 
sandwich course A12 78.5 21 
go bananas B2-/X1 80 22 
go bananas X5.2+++ 81.5 36.5 
go bananas E3- 81.5 25 
kick the bucket L1 83 77.5 
on the wagon F2 84 36.5 
Eskimo roll M4 85 18.5 
acid house K2 86 46 
plough into A9- 87 36.5 
Bloody Mary F2 88 36.5 
tea leaf G2.1-/S2mf 89 77.5 
 
11
Automatic Extraction of Chinese Multiword Expressions with a Statis-
tical Tool 
Scott S.L. Piao1 
s.piao@lancaster.ac.uk 
Guangfan Sun2 
morgan2001_sun@sohu.com 
Paul Rayson1 
paul@comp.lancs.ac.uk 
Qi Yuan2 
yq@trans.ccidnet.com 
 
1UCREL 
Computing Department 
Lancaster University 
Lancaster, UK 
2CIPOL 
China Centre for Information Industry De-
velopment (CCID) 
Beijing, China 
 
Abstract 
In this paper, we report on our experi-
ment to extract Chinese multiword ex-
pressions from corpus resources as part 
of a larger research effort to improve a 
machine translation (MT) system. For ex-
isting MT systems, the issue of multi-
word expression (MWE) identification 
and accurate interpretation from source to 
target language remains an unsolved 
problem. Our initial test on the Chinese-
to-English translation functions of 
Systran and CCID?s Huan-Yu-Tong MT 
systems reveal that, where MWEs are in-
volved, MT tools suffer in terms of both 
comprehensibility and adequacy of the 
translated texts. For MT systems to be-
come of further practical use, they need 
to be enhanced with MWE processing 
capability. As part of our study towards 
this goal, we test and evaluate a statistical 
tool, which was developed for English, 
for identifying and extracting Chinese 
MWEs. In our evaluation, the tool 
achieved precisions ranging from 61.16% 
to 93.96% for different types of MWEs.  
Such results demonstrate that it is feasi-
ble to automatically identify many Chi-
nese MWEs using our tool, although it 
needs further improvement. 
1 Introduction 
In real-life human communication, meaning is 
often conveyed by word groups, or meaning 
groups, rather than by single words. Very often, 
it is difficult to interpret human speech word by 
word. Consequently, for an MT system, it is im-
portant to identify and interpret accurate meaning 
of such word groups, or multiword expressions 
(MWE hereafter), in a source language and in-
terpret them accurately in a target language. 
However, accurate identification and interpreta-
tion of MWEs still remains an unsolved problem 
in MT research. 
In this paper, we present our experiment on 
identifying Chinese MWEs using a statistical 
tool for MT purposes. Here, by multiword ex-
pressions, we refer to word groups whose con-
stituent words have strong collocational relations 
and which can be translated in the target lan-
guage into stable translation equivalents, either 
single words or MWEs, e.g. noun phrases, 
prepositional phrases etc. They may include 
technical terminology in specific domains as well 
as more general fixed expressions and idioms. 
Our observations found that existing Chinese-
English MT systems cannot satisfactorily trans-
late MWEs, although some may employ a ma-
chine-readable bilingual dictionary of idioms. 
Whereas highly compositional MWEs may pose 
a trivial challenge to human speakers for inter-
pretation, they present a tough challenge for fully 
automatic MT systems to produce even remotely 
fluent translations. Therefore, in our context, we 
expand the concept of MWE to include those 
compositional ones which have relatively stable 
identifiable patterns of translations in the target 
language. 
By way of illustration of the challenge, we ex-
perimented with simple Chinese sentences con-
taining some commonly-used MWEs in 
SYSTRAN (http://www.systransoft.com/) and 
Huan-Yu-Tong (HYT henceforth) of CCID 
(China Centre for Information Industry Devel-
opment) (Sun, 2004). The former is one of the 
most efficient MT systems today, claiming to be 
?the leading provider of the world?s most scal-
able and modular translation architecture?, while 
the latter is one of the most successful MT sys-
tems in China. Table 1 shows the result, where 
SL and TL denote source and target languages 
respectively.. As shown by the samples, such 
17
highly sophisticated MT tools still struggle to 
produce adequate English sentences.. 
 
Chinese 
Sentences 
English 
(Systran) 
English 
(HYT) 
??????
???  ??
???? 
This afternoon 
can practice a 
ball game? I 
hope not to be 
able. 
Can practise a 
ball game this 
afternoon? I hope 
can not. 
??????
??????
???? 
You may not 
such do, let us 
pay respec-
tively each. 
You cannot do 
like that, and let 
us make it Dutch. 
??????
??????
??????
????? 
Perhaps does 
not have the 
means to let 
you sit shares a 
table, did you 
mind sits sepa-
rately? 
Perhaps no way 
out(ly) let you sit 
with table, are 
you situated be-
tween not mind 
to separate to sit? 
??????
??? 
Selects the 
milk coffee 
which ices. 
Ice breasts coffee 
take is selected. 
??????
??????
?? 
Good, I want 
the beer, again 
comes to select 
the coffee. 
Alright, I want 
beer, and take the 
coffee of order-
ing again. 
 
 
Table 1: Samples of Chinese-to-English transla-
tions of Systran and HYT. 
 
Ignoring the eccentric English syntactic struc-
tures these tools produced, we focus on the trans-
lations of Chinese MWEs (see the italic charac-
ters in the Table 1) which have straightforward 
expression equivalents in English. For example, 
in this context,  ???? can be translated into 
?hope not?, ??? into ?go Dutch?, ?? into 
?together? or ?at the same table?, ??? into 
?white coffee? or ?coffee with milk?, ??? 
into ?want some more (in addition to something 
already ordered)?. While these Chinese MWEs 
are highly compositional ones, when they are 
translated word by word, we see verbose and 
awkward translations (for correct translations, 
see the appendix). 
To solve such problems, we need algorithms 
and tools for identifying MWEs in the source 
language (Chinese in this case) and to accurately 
map them to their adequate translation equiva-
lents in the target language (English in our case) 
that are appropriate for given contexts. In the 
previous examples, an MT tool should be able to 
identify the Chinese MWE ???  and either 
provide the literal translation of ?pay for each? or 
map it to the more idomatic expressions of ?go 
Dutch?. 
Obviously, it would involve a wide range of 
issues and techniques for a satisfactory solution 
to this problem. In this paper, we focus on the 
sub-issue of automatically recognising and ex-
tracting Chinese MWEs. Specifically, we test 
and evaluate a statistical tool for automatic 
MWE extraction in Chinese corpus data. As the 
results of our experiment demonstrate, the tool is 
capable of identifying many MWEs with little 
language-specific knowledge. Coupled with an 
MT system, such a tool could be useful for ad-
dressing the MWE issue. 
2 Related Work  
The issue of MWE processing has attracted 
much attention from the Natural Language Proc-
essing (NLP) community, including Smadja, 
1993; Dagan and Church, 1994; Daille, 1995; 
1995; McEnery et al, 1997; Wu, 1997; Michiels 
and Dufour, 1998; Maynard and Ananiadou, 
2000; Merkel and Andersson, 2000; Piao and 
McEnery, 2001; Sag et al, 2001; Tanaka and 
Baldwin, 2003;  Dias, 2003; Baldwin et al, 
2003; Nivre and Nilsson, 2004 Pereira et al. 
2004; Piao et al, 2005. Study in this area covers 
a wide range of sub-issues, including MWE iden-
tification and extraction from monolingual and 
multilingual corpora, classification of MWEs 
according to a variety of viewpoints such as 
types, compositionality and alignment of MWEs 
across different languages. However studies in 
this area on Chinese language are limited. 
A number of approaches have been suggested, 
including rule-based and statistical approaches, 
and have achieved success to various extents. 
Despite this research, however, MWE processing 
still presents a tough challenge, and it has been 
receiving increasing attention, as exemplified by 
recent MWE-related ACL workshops. 
Directly related to our work is the develop-
ment of a statistical MWE tool at Lancaster for 
searching and identifying English MWEs in run-
ning text (Piao et al, 2003, 2005). Trained on 
corpus data in a given domain or genre, this tool 
can automatically identify MWEs in running text 
or extract MWEs from corpus data from the 
similar domain/genre (see further information 
about this tool in section 3.1). It has been tested 
and compared with an English semantic tagger 
(Rayson et al, 2004) and was found to be effi-
cient in identifying domain-specific MWEs in 
English corpora, and complementary to the se-
18
mantic tagger which relies on a large manually 
compiled lexicon. 
Other directly related work includes the de-
velopment of the HYT MT system at CCID in 
Beijing, China. It has been under development 
since 1991 (Sun, 2004) and it is one of the most 
successful MT systems in China. However, being 
a mainly rule-based system, its performance de-
grades when processing texts from domains pre-
viously unknown to its knowledge database. Re-
cently a corpus-based approach has been adopted 
for its improvement, and efforts are being made 
to improve its capability of processing MWEs.  
Our main interest in this study is in the appli-
cation of a MWE identification tool to the im-
provement of MT system. As far as we know, 
there has not been a satisfactory solution to the 
efficient handling of Chinese MWEs in MT sys-
tems, and our experiment contributes to a deeper 
understanding of this problem.  
3 Automatic Identification and extrac-
tion of Chinese MWEs  
In order to test the feasibility of automatic 
identification and extraction of Chinese MWEs 
on a large scale, we used an existing statistical 
tool built for English and a Chinese corpus built 
at CCID. A CCID tool is used for tokenizing and 
POS-tagging the Chinese corpus. The result was 
thoroughly manually checked by Chinese experts 
at CCID. In this paper, we aim to evaluate this 
existing tool from two perspectives a) its per-
formance on MWE extraction, and b) its per-
formance on a language other than English. In 
the following sections, we describe our experi-
ment in detail and discuss main issues that arose 
during the course of our experiment. 
3.1 MWE extraction tool 
The tool we used for the experiment exploits 
statistical collocational information between 
near-context words (Piao et al, 2005). It first 
collects collocates within a given scanning win-
dow, and then searches for MWEs using the col-
locational information as a statistical dictionary. 
As the collocational information can be extracted 
on the fly from the corpus to be processed for a 
reasonably large corpus, this process is fully 
automatic. To search for MWEs in a small cor-
pus, such as a few sentences, the tool needs to be 
trained on other corpus data in advance. 
With regards to the statistical measure of col-
location, the option of several formulae are 
available, including mutual information and log 
likelihood, etc. Our past experience shows that 
log-likelihood provides an efficient metric for 
corpus data of moderate sizes. Therefore it is 
used in our experiment. It is calculated as fol-
lows (Scott, 2001). 
For a given pair of words X and Y and a search 
window W, let a be the number of windows in 
which X and Y co-occur, let b be the number of 
windows in which only X occurs, let c be the 
number of windows in which only Y occurs, and 
let d be the number of windows in which none of 
them occurs, then 
 
G2 = 2 (alna + blnb + clnc + dlnd - (a+b)ln(a+b)  
        - (a+c)ln(a+c) - (b+d)ln(b+d)  
        - (c+d)ln(c+d)) + (a+b+c+d)ln(a+b+c+d)) 
 
In addition to the log-likelihood, the t-score is 
used to filter out insignificant co-occurrence 
word pairs (Fung and Church, 1994), which is 
calculated as follows: 
 
),(
1
)()(),(
ba
baba
WWprob
M
WprobWprobWWprob
t
?=  
 
In order to filter out weak collocates, a thresh-
old is often used, i.e. in the stage of collocation 
extraction, any pairs of items producing word 
affinity scores lower than a given threshold are 
excluded from the MWE searching process. Fur-
thermore, in order to avoid the noise caused by 
functional words and some extremely frequent 
words, a stop word list is used to filter such 
words out from the process. 
If the corpus data is POS-tagged, some simple 
POS patterns can be used to filter certain syntac-
tic patterns from the candidates. It can either be 
implemented as an internal part of the process, or 
as a post-process. In our case, such pattern filters 
are mostly applied to the output of the MWE 
searching tool in order to allow the tool to be 
language-independent as much as possible. 
Consequently, for our experiment, the major 
adjustment to the tool was to add a Chinese stop 
word list. Because the tool is based on Unicode, 
the stop words of different languages can be kept 
in a single file, avoiding any need for adjusting 
the program itself. Unless different languages 
involved happen to share words with the same 
form, this practice is safe and reliable. In our par-
ticular case, because we are dealing with English 
and Chinese, which use widely different charac-
ters, such a practice performs well. 
19
Another language-specific adjustment needed 
was to use a Chinese POS-pattern filter for se-
lecting various patterns of the candidate MWEs 
(see Table 6). As pointed out previously, it was 
implemented as a simple pattern-matching pro-
gram that is separate from the MWE tool itself, 
hence minimizing the modification needed for 
porting the tool from English to Chinese lan-
guage. 
A major advantage of this tool is its capability 
of identifying MWEs of various lengths which 
are generally representative of the given topic or 
domain. Furthermore, for English it was found 
effective in extracting domain-specific multi-
word terms and expressions which are not in-
cluded in manually compiled lexicons and dic-
tionaries. Indeed, due to the open-ended nature 
of such MWEs, any manually compiled lexicons, 
however large they may be, are unlikely to cover 
them exhaustively. It is also efficient in finding 
newly emerging MWEs, particularly technical 
terms, that reflect the changes in the real world. 
3.2 Experiment 
In this experiment, our main aim was to exam-
ine the feasibility of practical application of the 
MWE tool as a component of an MT system, 
therefore we used test data from some domains 
in which translation services are in strong de-
mand. We selected Chinese corpus data of ap-
proximately 696,000 tokenised words (including 
punctuation marks) which cover the topics of 
food, transportation, tourism, sports (including 
the Olympics) and business. 
In our experiment, we processed the texts 
from different topics together. These topics are 
related to each other under the themes of enter-
tainment and business. Therefore we assume, by 
mixing the data together, we could examine the 
performance of the MWE tool in processing data 
from a broad range of related domains. We ex-
pect that the different features of texts from dif-
ferent domains will have a certain impact on the 
result, but the examination of such impact is be-
yond the scope of this paper. 
As mentioned earlier, the Chinese word to-
keniser and POS tagger used in our experiment 
has been developed at CCID. It is an efficient 
tool running with accuracy of 98% for word to-
kenisation and 95% for POS annotation. It em-
ploys a part-of-speech tagset of 15 categories 
shown in Table 2. Although it is not a finely 
grained tagset, it meets the need for creating POS 
pattern filters for MWE extraction.  
 
N Name 
V Verb 
A Adjective 
F Adverb 
R Pronoun 
I Preposition 
J Conjunction 
U Number 
S classifier (measure word)  
G Auxiliary verb 
E Accessory word 
L directional noun 
P Punctuation 
H Onomatopoeia 
X Subject-predicate phrase 
  
Table 2: CCID Chinese tagset 
 
Since function words are found to cause noise 
in the process of MWE identification, a Chinese 
stop list was collected. First, a word frequency 
list was extracted. Next, the top items were con-
sidered and we selected 70 closed class words for 
the stop word list. When the program searches 
for MWEs, such words are ignored. 
The threshold of word affinity strength is an-
other issue to be addressed. In this experiment, 
we used log-likelihood to measure the strength of 
collocation between word pairs. Generally the 
log-likelihood score of 6.6 (p < 0.01 or 99% con-
fidence) is recommended as the threshold (Ray-
son et al, 2004), but it was found to produce too 
many false candidates in our case. Based on our 
initial trials, we used a higher threshold of 30, 
i.e. any word pairs producing log-likelihood 
score less than this value are ignored in the 
MWE searching process. Furthermore, for the 
sake of the reliability of the statistical score, 
when extracting collocates, a frequency threshold 
of five was used to filter out low-frequency 
words, i.e. word pairs with frequencies less than 
five were ignored. 
An interesting issue for us in this experiment 
is the impact of the length of collocation search-
ing window on the MWE identification. For this 
purpose, we tested two search window lengths 2 
and 3, and compared the results obtained by us-
ing them. Our initial hypothesis was that the 
shorter window length may produce higher pre-
cision while the longer window length may sacri-
fice precision but boost the MWE coverage. 
The output of the tool was manually checked 
by Chinese experts at CCID, including cross 
checking to guarantee the reliability of the re-
sults. There were some MWE candidates on 
which disagreements arose. In such cases, the 
20
candidate was counted as false. Furthermore, in 
order to estimate the recall, experts manually 
identified MWEs in the whole test corpus, so that 
the output of the automatic tool could be com-
pared against it.  In the following section, we 
present a detailed report on our evaluation of the 
MWE tool. 
3.3 Evaluation 
We first evaluated the overall precision of the 
tool. A total of 7,142 MWE candidates (types) 
were obtained for window lengths of 2, of which 
4,915 were accepted as true MWEs, resulting in 
a precision of 68.82%. On the other hand, a total 
of 8,123 MWE candidates (types) were obtained 
for window lengths of 3, of which 4,968 were 
accepted as true MWEs, resulting in a precision 
of 61.16%. This result is in agreement with our 
hypothesis that shorter search window length 
tends to produce higher precision. 
Next, we estimated the recall based on the 
manually analysed data. When we compared the 
accepted MWEs from the automatic result 
against the manually collected ones, we found 
that the experts tend to mark longer MWEs, 
which often contain the items identified by the 
automatic tool. For example, the manually 
marked MWE ?? ?? ?? ?? (develop-
ment plan for the tennis sport) contains shorter 
MWEs ?? ?? (tennis sport) and?? ?? 
(development plan) which were identified by the 
tool separately. So we decided to take the partial 
matches into account when we estimate the re-
call. We found that a total 14,045 MWEs were 
manually identified and, when the search win-
dow length was set to two and three, 1,988 and 
2,044 of them match the automatic output, pro-
ducing recalls of 14.15% and 14.55% respectively. 
It should be noted that many of the manually ac-
cepted MWEs from the automatic output were 
not found in the manual MWE collection. This 
discrepancy was likely caused by the manual 
analysis being carried out independently of the 
automatic tool, resulting in a lower recall than 
expected. Table 3 lists the precisions and recalls. 
 
Window length = 2 Window length = 3 
Precision Recall Precision Recall 
68.82% 14.15% 61.16% 14.55% 
 
Table 3: Overall precisions and recalls 
 
Furthermore, we evaluated the performance of 
the MWE tool from two aspects: frequency and 
MWE pattern. 
Generally speaking, statistical algorithms 
work better on items of higher frequency as it 
depends on the collocational information. How-
ever, our tool does not select MWEs directly 
from the collocates. Rather, it uses the colloca-
tional information as a statistical dictionary and 
searches for word sequences whose constituent 
words have significantly strong collocational 
bonds between them. As a result, it is capable of 
identifying many low-frequency MWEs. Table 4 
lists the breakdown of the precision for five fre-
quency bands (window length = 2). 
 
Freq Candidates True MWEs Precision 
>= 100 17 9 52.94% 
10 ~ 99 846 646 76.36% 
3 ~ 9 2,873 2,178 75.81% 
2 949 608 64.07% 
1 2,457 1,474 59.99% 
Total 7,142 4,915 68.82% 
 
Table 4: Breakdown of precision for frequencies 
(window length = 2). 
 
As shown in the table above, the highest preci-
sions were obtained for the frequency range be-
tween 3 and 99. However, 2,082 of the accepted 
MWEs have frequencies of one or two, account-
ing for 42.36% of the total accepted MWEs. 
Such a result demonstrates again that our tool is 
capable of identifying low-frequency items. An 
interesting result is for the top frequency band 
(greater than 100). Against our general assump-
tion that higher frequency brings higher preci-
sion, we saw the lowest precision in the table for 
this band. Our manual examination reveals this 
was caused by the high frequency numbers, such 
as ?one? or ?two? in the expressions ???? 
(a/one) and ???? ( a kind of). This type of ex-
pression were classified as uninteresting candi-
dates in the manual checking, resulting in higher 
error rates for the high frequency band. 
When we carry out a parallel evaluation for 
the case of searching window length of 3, we see 
a similar distribution of precision across the fre-
quency bands except that the lowest frequency 
band has the lowest precision, as shown by Table 
5. When we compare this table against Table 4, 
we can see, for all of the frequency bands except 
the top one, that the precision drops as the search 
window increases. This further supports our ear-
lier assumption that wider searching window 
tends to reduce the precision. 
 
 
21
Freq candidates true MWEs Precision 
>= 100 17 9 52.94% 
10 ~ 99 831 597 71.84% 
3 ~ 9 3,093 2,221 71.81% 
2 1,157 669 57.82% 
1 3,025 1,472 48.66% 
Total 8,123 4,968 61.16% 
 
Table 5: Breakdown of precision for frequencies 
(window length = 3). 
 
In fact, not only the top frequency band, much 
of the errors of the total output were found to be 
caused by the numbers that frequently occur in 
the test data, e.g. ?_U ?_S (one), ?_U ?_S 
(two) etc. When a POS filter was used to filter 
them out, for the window length 2, we obtained a 
total 5,660 candidates, of which 4,386 were ac-
cepted as true MWEs, producing a precision of 
77.49%. Similarly for the window length 3, a 
total of 6,526 candidates were extracted in this 
way and 4,685 of them were accepted as true 
MWEs, yielding a precision of 71.79%. 
Another factor affecting the performance of 
the tool is the type of MWEs. In order to exam-
ine the potential impact of MWE types to the 
performance of the tool, we used filters to select 
MWEs of the following three patterns: 
1) AN: Adjective + noun structure; 
2) NN: Noun + noun Structure; 
3) FV: Adverb + Verb. 
Table 6 lists the precision for each of the 
MWE types and for search window lengths of 2 
and 3. 
 
Search window length = 2 
Pattern Candidate True MWEs Precision 
A+N 236 221 93.64% 
N+N 644 589 91.46% 
F+V 345 321 93.04% 
total 1,225 1,131 92.33% 
Search window length = 3 
Pattern Candidate True MWEs Precision 
A+N 259 233 89.96% 
N+N 712 635 89.19% 
F+V 381 358 93.96% 
Total 1,352 1,226 90.68% 
 
Table 6: Precisions for three types of MWEs 
 
As shown in the table, the MWE tool achieved 
high precisions above 91% when we use a search 
window of two words. Even when the search 
window expands to three words, the tool still 
obtained precision around 90%. In particular, the 
tool is efficient for the verb phrase type. Such a 
result demonstrates that, when we constrain the 
search algorithm to some specific types of 
MWEs, we can obtain higher precisions. While 
one may argue that rule-based parser can do the 
same work, it must be noted that we are not in-
terested in all grammatical phrases, but those 
which reflect the features of the given domain. 
This is achieved by combining statistical word 
collocation measures, a searching strategy and 
simple POS pattern filters. 
Another interesting finding in our experiment 
is that our tool extracted clauses, such as???
?? (What would you like to drink?) and??
???? (Would you like a drink first?). The 
clauses occur only once or twice in the entire test 
data, but were recognized by the tool because of 
the strong collocational bond between their con-
stituent words. The significance of such per-
formance is that such clauses are typical expres-
sions which are frequently used in real-life con-
versation in the contexts of the canteen, tourism 
etc. Such a function of our tool may have practi-
cal usage in automatically collecting longer typi-
cal expressions for the given domains. 
4 Discussion 
As our experiment demonstrates, our tool pro-
vides a practical means of identifying and ex-
tracting domain specific MWEs with a minimum 
amount of linguistic knowledge. This becomes 
important in multilingual tasks in which it can be 
costly and time consuming to build comprehen-
sive rules for several languages. In particular, it 
is capable of detecting MWEs of various lengths, 
sometimes whole clauses, which are often typical 
of the given domains of the corpus data. For ex-
ample, in our experiment, the tool successfully 
identified several daily used long expressions in 
the domain of food and tourism. MT systems 
often suffer when translating conversation. An 
efficient MWE tool can potentially alleviate the 
problem by extracting typical clauses used in 
daily life and mapping them to adequate transla-
tions in the target language. 
 Despite the flexibility of the statistical tool, 
however, there is a limit to its performance in 
terms of precision. While it is quite efficient in 
providing MWE candidates, its output has to be 
either verified by human or refined by using lin-
guistic rules. In our particular case, we improved 
the precision of our tool by employing simple 
POS pattern filters. Another limitation of this 
tool is that currently it can only recognise con-
tinuous MWEs. A more flexible searching algo-
22
rithm is needed to identify discontinuous MWEs, 
which are important for NLP tasks. 
Besides the technical problem, a major unre-
solved issue we face is what constitutes MWEs. 
Despite agreement on the core MWE types, such 
as idioms and highly idiosyncratic expressions, 
like ?? (Cheng-Yu) in Chinese, it is difficult to 
reach agreement on less fixed expressions. 
We contend that MWEs may have different 
definitions for different research purposes. For 
example, for dictionary compilation, lexicogra-
phers tend to constrain MWEs to highly non-
compositional expressions (Moon, 1998: 18). 
This is because monolingual dictionary users can 
easily understand compositional MWEs and 
there is no need to include them in a dictionary 
for native speakers. For lexicon compilation 
aimed at practical NLP tasks, however, we may 
apply a looser definition of MWEs. For example, 
in the Lancaster semantic lexicon (Rayson et al, 
2004), compositional word groups such as 
?youth club? are considered as MWEs alongside 
non-compositional expressions such as ?food for 
thought? as they depict single semantic units or 
concepts. Furthermore, for the MT research 
community whose primary concern is cross-
language interpretation, any multiword units that 
have stable translation equivalent(s) in a target 
language can be of interest. 
As we discussed earlier, a highly idiomatic 
expression in a language can be translated into a 
highly compositional expression in another lan-
guage, and vice versa. In such situations, it can 
be more practically useful to identify and map 
translation equivalents between the source and 
target languages regardless of their level of com-
positionality.  
Finally, the long Chinese clauses identified by 
the tool can potentially be useful for the im-
provement of MT systems. In fact, most of them 
are colloquial expressions in daily conversation, 
and many such Chinese expressions are difficult 
to parse syntactically. It may be more feasible to 
identify such expressions and map them as a 
whole to English equivalent expressions. The 
same may apply to technical terms, jargon and 
slang. In our experiment, our tool demonstrated 
its capability of detecting such expressions, and 
will prove useful in this regard. 
5 Conclusion 
In this paper, we have reported on our experi-
ment of automatic extraction of Chinese MWEs 
using a statistical tool originally developed for 
English. Our statistical tool produced encourag-
ing results, although further improvement is 
needed to become practically applicable for MT 
system in terms of recall. Indeed, for some con-
strained types of MWEs, high precisions above 
90% have been achieved. This shows, enhanced 
with some linguistic filters, it can provide a prac-
tically useful tool for identifying and extracting 
MWEs. Furthermore, in our experiment, our tool 
demonstrated its capability of multilingual proc-
essing. With only minor adjustment, it can be 
ported to other languages. Meanwhile, further 
study is needed for a fuller understanding of the 
factors affecting the performance of statistical 
tools, including the text styles and topic/domains 
of the texts, etc.   
Acknowledgement 
This work was supported by the National Natural 
Science Foundation of China (grant no. 
60520130297) and the British Academy (grant 
no. SG-42140). 
References 
Biber, D., Conrad, S., Cortes, V., 2003. Lexical bun-
dles in speech and writing: an initial taxonomy. In: 
Wilson, A., Rayson P., McEnery, T. (Eds.), Corpus 
Linguistics by the Lune: A Festschrift for Geoffrey 
Leech. Peter Lang, Frankfurt. pp. 71-92.   
Baldwin, T., Bannard, C., Tanaka, T. and Widdows, 
D. 2003 An Empirical Model of Multiword Ex-
pression Decomposability, In Proceedings of the 
ACL-2003 Workshop on Multiword Expressions: 
Analysis, Acquisition and Treatment, Sapporo, Ja-
pan, pp. 89?96. 
Dagan, I., Church, K., 1994. Termight: identifying 
and translating technical terminology. In: Proceed-
ings of the 4th Conference on Applied Natural 
Language Processing, Stuttgart, German. pp. 34-
40. 
Daille, B., 1995. Combined approach for terminology 
extraction: lexical statistics and linguistic filtering. 
Technical paper 5, UCREL, Lancaster University. 
Dias, G., 2003. Multiword unit hybrid extraction. In: 
Proceedings of the Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, at 
ACL'03, Sapporo, Japan. pp. 41-48. 
Dunning, T., 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Lin-
guistics 19 (1), 61-74. 
Fung, P., Church, K., 1994. K-vec: a new approach 
for aligning parallel texts. In: Proceedings of COL-
ING '94, Kyoto, Japan. pp. 1996-2001. 
23
Maynard, D., Ananiadou, S., 2000. Trucks: a model 
for automatic multiword term recognition. Journal 
of Natural Language Processing 8 (1), 101-126. 
McEnery, T., Lange, J. M., Oakes, M., Vernonis, J.., 
1997. The exploitation of multilingual annotated 
corpora for term extraction. In: Garside, R., Leech, 
G., McEnery, A. (Eds.), Corpus Annotation --- 
Linguistic Information from Computer Text Cor-
pora. Longman,  London & New York. pp 220-
230. 
Merkel, M., Andersson, M., 2000. Knowledge-lite 
extraction of multi-word units with language filters 
and entropy thresholds. In: Proceedings of 2000 
Conference User-Oriented Content-Based Text and 
Image Handling (RIAO'00), Paris, France. pp. 737-
746. 
Michiels, A., Dufour, N., 1998. DEFI, a tool for 
automatic multi-word unit recognition, meaning 
assignment and translation selection. In: Proceed-
ings of the First International Conference on Lan-
guage Resources & Evaluation, Granada, Spain. 
pp. 1179-1186. 
Moon, R. 1998. Fixed expressions and idioms in Eng-
lish: a corpus-based approach. Clarendon Press: 
Oxford. 
Nivre, J., Nilsson, J., 2004. Multiword units in syntac-
tic parsing. In: Proceedings of LREC-04 Workshop 
on Methodologies & Evaluation of Multiword 
Units in Real-world Applications, Lisbon, Portugal. 
pp. 37-46. 
Pereira, R., Crocker, P., Dias, G., 2004. A parallel 
multikey quicksort algorithm for mining multiword 
units. In: Proceedings of LREC-04 Workshop on 
Methodologies & Evaluation of Multiword Units in 
Real-world Applications, Lisbon, Portugal. pp. 17-
23. 
Piao, S. L., Rayson, P., Archer, D. and McEnery, T. 
2005. Comparing and Combining A Semantic Tag-
ger and A Statistical Tool for MWE Extraction. 
Computer Speech & Language Volume 19, Issue 4,  
pp. 378-397. 
Piao, S.L , Rayson, P., Archer, D., Wilson, A. and 
McEnery, T. 2003. Extracting multiword expres-
sions with a semantic tagger. In Proceedings of the 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, at ACL'03, Sapporo, 
Japan, pp. 49-56. 
Piao, S., McEnery, T., 2001. Multi-word unit align-
ment in English-Chinese parallel corpora. In: Pro-
ceedings of the Corpus Linguistics 2001, Lancas-
ter, UK. pp. 466-475. 
Rayson, P., Archer, D., Piao, S. L., McEnery, T. 
2004. The UCREL semantic analysis system. In 
proceedings of the workshop on Beyond Named 
Entity Recognition Semantic labelling for NLP 
tasks in association with LREC 2004, Lisbon, Por-
tugal, pp. 7-12. 
Rayson, P., Berridge, D. and Francis, B. 2004. Ex-
tending the Cochran rule for the comparison of 
word frequencies between corpora. In Proceedings 
of the 7th International Conference on Statistical 
analysis of textual data (JADT 2004), Louvain-la-
Neuve, Belgium. pp. 926-936. 
Sag, I., Baldwin, T., Bond, F., Copestake, A., Dan, F., 
2001. Multiword expressions: a pain in the neck 
for NLP. LinGO Working Paper No. 2001-03, 
Stanford University, CA. 
Scott, M., 2001. Mapping key words to problem and 
solution. In: Scott, M., Thompson, G. (Eds.), Pat-
terns of Text: in Honour of Michael Hoey. Benja-
mins, Amsterdam. pp. 109 ? 127. 
Smadja, F., 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics 19 (1), 143-177. 
Sun, G. 2004. Design of an Interlingua-Based Chi-
nese-English Machine Translation System. In Pro-
ceedings of the 5th China-Korea Joint Symposium 
on Oriental Language Processing and Pattern Rec-
ognition, Qingdao, China. pp. 129-134. 
Tanaka, T., Baldwin, T., 2003. Noun-noun compound 
machine translation: a feasibility study on shallow 
processing. In: Proceedings of the ACL-03 Work-
shop on Multiword Expressions: Analysis, Acquisi-
tion and Treatment, Sapporo, Japan. pp. 17-24. 
Wu, D., 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics 23 (3), 377-401. 
Appendix: English translations of the 
sample Chinese sentences 
1. ????????? ?????? 
Tran: Do we have (football) training this af-
ternoon? I hope not. 
2. ???????????????? 
Tran: You can?t do that. Let?s go Dutch. 
3. ?????????????????
?????? 
Tran: I am afraid I can?t arrange for you to sit 
at the same table. Would you mind if 
you sit separately? 
4. ????????? 
Tran: I?d like iced white coffee (please). 
5. ?????????????? 
Tran: OK, I want beer and some coffee 
(please). 
24
