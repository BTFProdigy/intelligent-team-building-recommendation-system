 Semantic class induction and its application for a Chinese voice 
search system 
Yali Li 
ThinkIT laboratory, 
Institute of 
Acoustics, Chinese 
Academy of Sciences 
liyali@hccl.ioa.ac.cn 
Weiqun Xu 
ThinkIT laboratory, 
Institute of Acoustics, 
Chinese Academy of 
Sciences 
xuweiqun@hccl.ioa.ac.cn
Yonghong Yan 
ThinkIT laboratory, 
Institute of 
Acoustics, Chinese 
Academy of Sciences
yyan@hccl.ioa.ac.cn 
 
Abstract 
In this paper, we propose a novel 
similarity measure based on  
co-occurrence probabilities for inducing 
semantic classes. Clustering with the new 
similarity measure outperformed that 
with the widely used distance measure 
based on Kullback-Leibler divergence in 
precision, recall and F1 evaluation. We 
then use the induced semantic classes and 
structures by the new similarity measure 
to generate in-domain data. At last, we 
use the generated data to do language 
model adaptation and improve the result 
of character recognition from 85.2% to 
91%. 
1 Introduction 
Voice search (e.g. Wang et al, 2008) has 
recently become one of the major foci in spoken 
dialogue system research and development. In 
main stream large vocabulary ASR engines, 
statistical language models (n-grams in 
particular), usually trained with plenty of data, 
are widely used and proved very effective. But 
for a voice search system, we have to deal with 
the case where there is no or very little relevant 
data for language modeling. One of the 
conventional solutions to this problem is to 
collect and use some human-human or 
Wizard-of-Oz (WOZ) dialogue data. Once the 
initial system is up running, the performance can 
be further improved with human-computer data 
in a system-in-the-loop style. Another practical 
approach is to handcraft some grammar rules and 
generate some artificial data. But writing 
grammars manually is tedious and 
time-consuming and requires some linguistic 
expertise. 
In this paper, we introduced a new similarity 
measure to induce semantic classes and 
structures. We then generated a large number of 
data using the induced semantic classes and 
structures to make language model adaptation. 
At the end, we give the conclusion and implied 
the future work. 
2 Semantic Class Induction 
The studies on semantic class induction in spoken 
language (or spoken language acquisition in 
general) have received some attention since the 
middle 90's. One of the earlier works is carried 
out by Gorin (1995), who employed an 
information -theoretic connectionist network 
embedded in a feedback control system to acquire 
spoken language. Later on Arai et al (1999) 
further studied how to acquire grammar 
fragments in fluent speech through clustering 
similar phrases using Kullback-Leibler distance. 
Meng and Siu (2002) proposed to 
semi-automatically induce language structures 
from unannotated corpora for spoken language 
understanding, mainly using Kullback-Liebler 
divergence and mutual information. Pargellis et 
al. (2004) used similar measures (plus three 
others) to induce semantic classes for comparing 
domain concept independence and porting 
concepts across domains. Potamianos (2005, 
2006, 2007) and colleagues conducted a series of 
studies to further improve semantic class 
induction, including combining wide and narrow 
context similarity measures, and adopting a 
soft-clustering algorithm (via a probabilistic 
class-membership function). 
2.1 Clustering 
In general, words and phrases which appear in 
similar context usually share similar semantics. 
E.g., ????(Tsinghua University) and ??
??(Peking University) in the following two 
utterances (literal translations are given in 
brackets) are both names of place or 
organisation. 
 
? ? ???? ?? ? ??? 
Please/look for/Tsinghua University/near//bank 
(Please look for banks near Tsinghua 
University.) 
 
? ? ???? ?? ? ???? 
Please/look for/Peking University/nearby//gym 
(Please look for gyms near Peking University.) 
 
To automatically discover that the above two 
words have similar semantics from unannotated 
corpus, we try unsupervised clustering based on 
some similarity measures to induce semantic 
classes. Further details about similarity measures 
are given in section 2.2.  
Before clustering, the utterances are 
segmented into phrases using a simple maximum 
matching against a lexicon. Clustering are 
conducted on phrases, which may be of a single 
word. 
2.2 Similarity Measures 
For lexical distributional similarity, several 
measures have been proposed and adopted, e.g., 
Meng and Siu (2002), Lin(1998), Dagan et al 
(1999), Weeds et al (2004). 
We use two kinds of similarity measures in 
the experiments. One is similarity measure based 
on distance, and the other is a new similarity 
measure directly using the co-occurrence 
probabilities. 
2.3 Distance based similarity measures 
The relative entropy between two probability 
mass functions )(xp  and )(xq  is defined by 
(Cover and Thomas, 2006) as: 
)(
)(
log
)(
)(
log)()||(
xq
xp
E
xq
xp
xpqpD p
Xx
== ?
?
(1) 
The relative entropy, as an asymmetric 
distance between two distributions, measures the 
inefficiency of assuming that the distribution is 
q  when the true distribution is p . 
It is commonly used as a statistical distance 
and can be symmetry as follows: 
)||()||(),( pqDqpDqpdiv +=      (2) 
For two words in a similar context, e.g., in 
the sequence { ,...,,..., 11 www ? }, 
where w  can be word a  or b , the right 
bigram )||(1
RR baD and )||(1
RR abD  are 
defined as: 
 
??= W b)|p(w
a)|p(w
a)|p(wbaD
w
RR
1 1
1
11 log)||(
(3) 
and  
??= W a)|p(w
b)|p(w
b)|p(wabD
w
RR
1 1
1
11 log)||( (4) 
where W  is the set of words or phrases. 
And the symmetric divergence is 
)||()||(),( 111
RRRRRR abDbaDbadiv += (5) 
The left bigram symmetric divergence can be 
similarly defined. 
Using both left and right symmetric 
divergences, the distance between a  and b  
is 
),(),(),( 111
RRLL badivbadivbad +=     (6) 
So the KL distance becomes: 
?
?
?
?
?
?
?
?
? ?
?
?
? ?
?
?
W a)|p(w
b)|p(w
b)|p(w+
W b)|p(w
a)|p(w
a)|p(w+
W a)|p(w
b)|p(w
b)|p(w+
W b)|p(w
a)|p(w
a)|p(w=
)b,div(a+)b,div(a=b)KL(a,
w
w
w
w
RRLL
1 1
1
1
1 1
1
1
1 1
1
1
1
1
1
1
log
log
log
log
(7) 
This is the widely used distance measure for  
lexical semantic similarity, e.g., Dagan et al 
(1999); Meng and Siu (2002); Pargellis et al
(2004). We can also see the IR distance and L1 
distance below: 
?
?
?
?
? +
? +
? +
? +
? ??
?
?
? ??
?
?
W b)|p(wa)|p(w
b)|p(w
b)|p(w+
W b)|p(wa)|p(w
a)|p(w
a)|p(w+
W b)|p(wa)|p(w
b)|p(w
b)|p(w+
W b)|p(wa)|p(w
a)|p(w
a)|p(w=b)IR(a,
w
w
w
w
1 11
1
1
1 11
1
1
1 11
1
1
1 11
1
1
2
log
2
log
2
log
2
log
(8) 
We can see from the IR metric that it is 
similar to the KL distance. Manhattan-norm (L1) 
distance : 
?
?
?
?
?
?
?
??
W
b)|p(wa)|p(w+
W
b)|p(wa)|p(w=b)(a,L
w
w
1
11
1
11
||
||1
     (9) 
In Pargellis et al (2004), the lexical context 
is further extended from bigrams to trigrams as 
follows. For the sequence: 
,...,,,,..., 2112 wwwww ??  
where w  can be word a  or b , the trigram 
KL between  a  and b is: 
?
?
?
?
?
?
?
?
?? ??
??
??
?? ??
??
??
Ww, a)|wp(w
b)|wp(w
b)|wp(w+
Ww, b)|wp(w
a)|wp(w
a)|wp(w+
Ww, a)|wpw
b)|wp(w
b)|wp(w+
Ww, b)|wp(w
a)|wp(w
a)|wp(w=b)(a,KL
w
w
w 2
w
2
21 21
21
21
21
21
21
21
22 1
12
12
22
12
12
12
log
log
log
log
(10) 
Since more information is taken into account 
in b)(a,KL2 , more constraints are imposed on the 
similarity measure. This is expected to improve 
the precision of clustering but may lead to a lower 
recall. 
2.4 Co-occurrence Probability based 
similarity measures 
After a close investigation of the corpus, we 
came up with an intuitive similarity measure 
directly based on the co-occurrence probability. 
The key idea is that the more common 
neighbouring words or phrases any two words or 
phrases in question share, the more similar they 
are to each other. Therefore, for each left or right 
neighboring word or phrase, we take the lower 
conditional probability into account. 
Thus we have the following similarity 
measures: 
Similarity using the bigram context 
?
?
?
??
??
W
b))|p(wa),|(p(w+
W
b))|p(wa),|(p(w=b)(a,S
w
w
1
11
1
111
min
min
    (11) 
Similarity using the trigram context 
?
?
?
???
????
Ww,
b))|wp(wa),|w(p(w+
Ww
b))|wp(wa),|w(p(w=b)(a,S
w
w
21
2121
12,
12122
min
min
(12) 
Similarity extending b)(a,S1 , taking both left 
and right contexts into account simultaneously  
??? ??Ww, b))|wp(wa),|w(p(w+
S=b)(a,S
w 11
1111
13
min (13) 
After pairs of words or phrases are clustered 
above, those pairs with common members are 
further merged. 
2.5 Comparison of measures 
The KL distances emphasize on the difference of 
two probability but the new measure take the 
probability itself into account. Take the right 
bigram context the similarity measure for 
example: 
)log
log(
1
1
1
1 1
1
1
a)|p(w
b)|p(w
b)|p(w+
W b)|p(w
a)|p(w
a)|p(w=b)(a,KL
w
R ??
 (14) 
seeing  as )|( 1 awP x  and seeing  
as , the equation changed to: 
)|( 1 bwP
y
? )xyy+yx(x=y)(x,KL R loglog  (15) 
and y)(x,SR  becomes to: ?= ),min(),( yxyxS R              (16) 
We can also get the y)(x,IRR  and |1 y)(x,L R  
? ++= )yx yy+yx x(xy)(x,IR R 2log2log   (17) 
and ||1 yxy)(x,L R ?=                   (18) 
We can see the space distribution in Figure.1. 
 
 Figure 1. Space distribution of different metrics 
0=
=
z
yx
                              (19) 
zyx ==                            (20) 
We can see from the four figures (the space 
distribution of four bigram metrics) that four 
curve surface are all symmetric. The curve 
surface of the three distance (KL,IR, L1) all 
contain the curve of (19), and curve surface of 
the minimum similarity contains the curve of 
(20). We say that the KL distances, IR distances 
and L1 distances all emphasize only on the 
distances and don't take the probability itself into 
account.  
We take the right context of two pairs 
),( 11 ba  and ),( 22 ba  for example. If  
9.0)|(,1.0)|( 111 == awpawp  
9.0)|(,1.0)|( 121 == bwpbwp  
0.10.9, 232 =)a|p(w=)a|p(w
'   
1.0)|(,9.0)|( 242
' == bwpbwp  
The calculation is shown as follows: 
0
0.1
0.1
log*0.1
0.1
0.1
log*0.1
log
log
1
1
1
1
1
111
=
+=
)a|p(w
)b|p(w
)b|p(w+
)b|p(w
)a|p(w
)a|p(w=)b,(aKL R
 
0
0.9
0.9
log*0.9
0.9
0.9
log*0.9
log
log
2
2
2
2
2
222
=
+=
)a|p(w
)b|p(w
)b|p(w+
)b|p(w
)a|p(w=),b(aKL
'
'
'
'
'
R
)a|p(w '
 
1.0
)1.0,1.0min(
1111
=
=
R )|()|(min )bwp,aw(p=),b(aS
 
9.0
)9.0,9.0min(
2222
=
=
R )|()|(min )bwp,aw(p=),b(aS
''
 
The KL calculation result of two pairs is the 
same but the new similarity calculated that  
),( 22 ba   is more similar than ),( 11 ba  
because they have more similar context 
probability 0.9. 
3 Experiments and Results 
3.1 Data 
In our experiments, four types of corpora are 
exploited in different stages and different ways.  
z T: A large collection of text corpus is used 
to train a general n-gram language model.  
z H: Some WOZ dialogues were collected 
before the system is built, using a similar 
scenario where users talked in Chinese to a 
service provider (human) via telephone to 
search for local information, or information 
about some local points of interest (POI). 
These dialogues were manually transcribed 
and used for language model training. This 
is the best data we could get before the 
system is built though it is not the real but 
near in-domain data.  
z C: After the initial system was up running, 
some real human-computer dialogues were 
collected and transcribed. These dialogues 
were split into three sets. One (C1) is used 
for semantic class and structure induction. 
One (C2) is used as test data. The other (C3) 
is reserved.  
z A: Domain information (domain entities) is 
used in conjunction with the induced 
semantic classes and structures from C1 to 
generate a large amount of in-domain 
corpus for language model adaptation. In 
Table 1,  we give some statistics in terms 
of the number of utterances(no. u) and 
Chinese characters(no. c) for the above 
corpora. 
corpus no.u no.c 
T  38,636  8,706,340 
H  6,652  151,460 
C1  658  15,434 
C2  1,000  19,284 
C3  411  8,014 
A  14,205  365,576 
Table 1. statistics of different corpus 
3.2 Semantic Clustering 
We conducted clustering with the above 
similarity measures on the data set C1. 
During the clustering, it is required that all the 
probabilities involved in calculating similarity be 
larger than 0. We have no threshold except this 
constraint. 
The outcomes are pairs of phrases. 
It is noticed that most of the clustered words 
and phrases are domain entities. 
In our experiments, we merged the induced 
similar pairs into large clusters. For example, if 
a  is similar to b  and b  is similar to c , then 
( a , b , c ) are merged into one category. In the 
end we use the categories to replace those words 
and phrases in corpus C1 and obtained templates. 
Examples of  the results are given below. 
 
$ask $toponym $near $wh-word $sevice 
[??] $ask $toponym $near ? $sevice ? 
? ? $toponym $ask ??? $poi 
where: 
$ask = ?? | ???| ???? | ... 
$toponym = ???? | ??? | ... 
$sevice = ?? | ??? | ??? |... 
$near = ?? | ?? | ... 
$wh-word = ??? | ??? | ??? | ... 
$poi = ???? | ????? | ... 
 
To evaluate the induction performance, we 
compare the induced word pairs against manual 
annotation. We manually annotated each phrase 
with a tag like $toponym, $poi and so on. If a  
and b  are calculated as a pairs and the 
annotation is the same, we see that they are 
correctly induced which is referred to Pangos 
(2006).  
We compute the metrics of precision P , 
recall R  and f-score 1F  as follows: 
%100?=
M
m
P                        (21) 
where m  is the number of correctly induced 
pairs, and M  is the number of induced pairs. 
%100?=
N
n
R                        (22) 
where n  is the number of correctly induced 
words and phrases, and N  is the number of 
words and phrases in the annotation. 
%100
2
1 ?+
??=
RP
RP
F                 (23) 
which is a harmonic mean of P  and R . 
 
Figure 2. Induction process 
The iterate process we adopted is as in 
Pargellis et al (2004). In the first iteration, we 
calculated the similarity and use the largest 
similarity pairs to generate large classes which 
can be called semantic generalizer. Then we use 
these semantic classes to replace the corpus, and 
obtained new corpus just as the example 
presented above. Then we duplicate this process 
for the second iteration and so on. 
 
 
Figure 3. Precision according to iterations 
induced by KL and S1 similarity measure 
 
Figure 4. Recall according to iterations induced 
by KL and S1 similarity measure 
 
Figure 5. F1 according to iterations induced by 
KL and S1 similarity measure 
 
Figure 6. F1 according to iterations induced by 
all bigram similarity measure 
From figures (Figure 3-6), we can see that 
clustering with our new co-occurrence 
probability based similarity measures 
outperforms that with the widely used relative 
entropy based distance measure consistently for 
both bigram and trigram contexts. This confirms 
the effectiveness of our new and simple measure. 
Regarding the context size, the results from 
using the bigram context outperforms that from 
using the trigram context in precision. But recall 
and 1F  drops a lot.  This is due to that larger 
contexts bring more constraints. The context size 
effect holds for both types of similarity measures. 
And the best performance is achieved with the 
similarity measure 3S . It is based on 1S  and 
takes both left and right contexts into account at 
the same time. 
3.3 Corpus Generation 
Since the number of the domain entities 
(terminals) we can collect from the dialogues is 
very limited, we have to expand those variables 
(non-terminals) in the induced templates with 
domain information from the application 
database and relevant web sites. For example, we 
used all the words and phrases in the toponym  
cluster, e.g., ``????  | ???  | ...'', to 
replace $toponym in the templates above. Then 
we generated a large collection of artificial data 
which has a good coverage in both the utterance 
structures (the way people speak) and the domain 
entities. This resulted in the generated corpus A 
in Table 1. In generation we used the semantic 
classes and structures  induced with 3S  and 
manually corrected some obvious errors. In the 
generated data, there are 14,205 utterances and 
365,576 Chinese characters.: 
3.4 Language Model Adaptation 
There are some language model adaptation 
(LMA) work oriented to the dialogue systems e.g. 
Wang et al2006), Hakkani-T?r et al(2006),  
Bellegarda(2004). So far major effort has been 
spent on adaptation for large vocabulary speech 
recognition or transcription tasks. But recently 
there have been a few studies that are oriented 
toward dialogue systems, e.g. Wang et al2006), 
Hakkani-T?r et al(2006). In our experiments, 
three trigram language models were built, each 
trained separately on the large text collection (T), 
on the WOZ data (H) and on the artificially 
generated data (A). These trigram models were 
then combined through model interpolation as 
follows: We used the linear interpolation to adapt 
language model. The formula is shown as follows. 
T is the out-of-domain data, H is the 
humane-to-humane dialogues, and A is the 
corpus generated by grammars  
)ww|(wP?+
)ww|(wP?+
)ww|(wP?=)ww|P(w
iiiAA
iiiHH
iiiTTiii
21
21
2121
??
??
????
  (24) 
where 1,,0 << AHT ???  and 1=++ AHT ??? . 
The weights were determined empirically on 
the held-out data (C3 in Table 1}). 
All the language models were built with the 
Stolcke(2002)?s {SRILM} toolkit. 
Why we did not use the C corpus directly is that it 
does't have a good covering on the 
domain-entities and other users usually say 
utterances similar to C in structures but different 
domain entities. So we use the good covering 
generated data to make LMA.  
We evaluated the different language models 
with both intrinsic and extrinsic metrics. For 
intrinsic evaluation, we computed the perplexity. 
For extrinsic evaluation, we ran speech 
recognition experiments on the test data C2 and 
calculated the character error rate (CER).  
We can see that corpus A is useful to make 
model adaptation and it is closer to the in-domain 
data than the human-human data for 
human-computer dialogues. By using these 
generated sentences, our domain-specific 
Chinese speech recognition have a growth from 
85.2% to 91.4%. 
 
A
H
T
?
,?
,?
 
1,  
0,  
0  
0.2, 
0.8, 
0  
0.2, 
0, 
0.8  
0.2,
0.4,
0.4 
PP  984  95.4  33.6  23.3 
CER(%)  32.3  14.8  10.7  9.0 
Table 2. perplexity and character error rate 
according to model interpolation 
The optimized weights (0.2,0.4,0.4) is 
obtained from the develop sets C3. From Table 2, 
we can see that language models built using 
additional dialogue related data, either 
human-human/WOZ  dialogues or data 
generated from human-computer dialogues, 
shows significant improvement in both 
perplexity and speech recognition performance 
over the one built with the general text data only. 
For the two dialogue related data, the generated 
data is better than the WOZ data or closer to the 
test data, since perplexity further drops from 
103.5 to 38.1 and CER drops from 14.8 to 10.7. 
This confirms our conjecture that human-human 
WOZ dialogue data is near in-domain and not 
very proper for human-computer dialogues. 
Therefore, to effectively improve language 
modeling for human-computer dialogues, we 
need more in-domain data, even if it is generated 
or artificial. The best language model is obtained 
through interpolation of both language models 
from dialogue related data with the one from 
general text data. This may be because there is 
still some mismatch between data sets C1 (for 
induction and generation) and C2 (for test).  
And some of the missing bits in C1 appeared in 
the WOZ data (corpus A). 
4 Related Works 
The most relevant work to ours is done by Wang 
et al (2006), who generated in-domain data 
through out-of-domain data transformation. First 
some artificial sentences are generated through 
parsing and reconstructing out-of-domain data 
and the illegal ones are filtered out. Then the 
synthetic corpus is sampled to achieve a desired 
probability distribution, based on either 
simulated dialogues or semantic information 
extracted from development data. But we used a 
different approach in producing more in-domain 
data. First semantic classes and structures are 
induced from limited human-computer dialogues. 
Then large amount of artificial in-domain corpus 
is generated with the induced semantic classes 
and patterns augmented with domain entities. 
The main difference between the two works lies 
in how the data is generated and how the 
generated data helped. 
5 Conclusions and Future Work 
In this paper, we described our work on 
generating in-domain corpus using the 
auto-induced semantic classes and structures for 
language model adaptation in a Chinese voice 
search dialogue system. In inducing semantic 
classes we proposed a novel co-occurrence 
probability based similarity measure. Our 
experiments show that the simple co-occurrence 
probability based similarity measure is effective 
for semantic clustering which is used in our 
experiment. For interpolation based language 
model adaptation, the data generated using the 
induced semantic classes and structures 
enhanced with domain entities helped a lot for 
human-computer dialogues. Despite that we 
dealt with the language of Chinese, we believe 
that that approaches we employed are language 
independent and can be applied to other 
languages as well. 
In our experiment we noticed that the 
performance of semantic clustering was affected 
quite a lot by the noises in the data. For future 
work, we would like to investigate how to 
further improve the robustness of semantic 
clustering in noisy spoken language. The 
semantic structures induced above are very 
shallow. We would like to investigate how to 
find deep semantics and relations in the data. 
Acknowledgement 
 
This work is partially supported by The National 
Science & Technology Pillar Program 
(2008BAI50B03), National Natural Science 
Foundation of China (No. 10925419, 90920302, 
10874203, 60875014). 
 
References 
Arai, K. J. H., Wright, G. Riccardi, and Gorin, A. L. 
?Grammar fragment acquisition using syntactic 
and semantic clustering,? Speech Communication, 
vol. 27, iss. 1, pp. 43?62, 1999 
Bellegarda, J. R. Statistical language model 
adaptation: review and perspectives, Speech 
Communication, vol. 42, iss. 1, pp. 93?108, 2004 
Cover, T. M. and Thomas, J. A., Elements of 
Information Theory. Wiley-Interscience, 2006 
Dagan, I., Lee, L. and Pereira, F. C. N. 
?Similarity-Based Models of Word Cooccurrence 
Probabilities,? Machine Learning, 1999 
Gorin, A. L. ?On automated language acquisition,? 
Acoustical Society of America Journal, vol. 97, pp. 
3441?3461, 1995 
Hakkani-T?r, D. Z., Riccardi, G. and Tur, G. An 
active approach to spoken language processing, 
ACM Transactions on Speech and Language 
Processing (TSLP), vol. 3, iss. 3, pp. 1?31, 2006 
Lin, D. ?An information-theoretic definition of 
similarity,? in Proc. ICML ?98: Proceedings of the 
Fifteenth International Conference on Machine 
Learning, 1998 
Meng, H. M. and Siu, K.-C. ?Semiautomatic 
Acquisition of Semantic Structures for 
Understanding Domain-Specific Natural Language 
Queries,? IEEE Trans. Knowl. Data Eng. 2002 
Pargellis, A. N., Fosler-Lussier, E., Fosler-Lussier, 
Lee, C.-H., Potamianos, A. and Tsai, A. 
?Auto-induced semantic classes,? Speech 
Communication, vol. 43, iss. 3, pp. 183?203, 2004 
Pangos, A Combining statistical similarity measures 
for automatic induction of semantic classes, 2005 
Pangos, A., Iosif, E. and Tegos, A. Unsupervised 
combination of metrics for semantic class 
induction, SLT 2006, 2006 
Pangos, A. and Iosif, E., A Soft-Clustering Algorithm 
for Automatic Induction of Semantic Classes, 
interspeech07, 2007 
Stolcke, A. SRILM ? an extensible language 
modeling toolkit, in Proc. ICSLP, 2002 
Wang, C. Chung, G. and Seneff, S. Automatic 
induction of language model data for a spoken 
dialogue system, Language Resources and 
Evaluation, vol. 40, iss. 1, pp. 25?46, 2006 
Wang, Y.-Y. and Dong Yu, E. A., An introduction to 
voice search, Signal Processing Magazine, IEEE, 
vol. 25, iss. 3, pp. 28?38, 2008 
Weeds, J., Weir, D. and McCarthy, D. 
?Characterising measures of lexical distributional 
similarity,? in Proc. in Proc. COLING ?04, 2004, 
 
Proceedings of the SIGDIAL 2013 Conference, pages 457?461,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Dialog State Tracking using Conditional Random Fields
Hang Ren, Weiqun Xu, Yan Zhang,Yonghong Yan
The Key Laboratory of Speech Acoustics and Content Understanding
Institute of Acoustics, Chinese Academy of Sciences
21 North 4th Ring West Road, Beijing, China, 100190
{renhang, xuweiqun, zhangyan, yanyonghong}@hccl.ioa.ac.cn
Abstract
This paper presents our approach to dialog
state tracking for the Dialog State Track-
ing Challenge task. In our approach we
use discriminative general structured con-
ditional random fields, instead of tradi-
tional generative directed graphic models,
to incorporate arbitrary overlapping fea-
tures. Our approach outperforms the sim-
ple 1-best tracking approach.
1 Introduction
Spoken dialog systems have been widely devel-
oped in recent years. However, when dialogs are
conducted in noisy environments or the utterance
itself is noisy, it is difficult for machines to cor-
rectly recognize or understand user utterances. In
this paper we present a novel dialog state track-
ing method, which directly models the joint prob-
ability of hypotheses onN -best lists. Experiments
are then conducted on the DSTC shared corpus,
which provides a common dataset and an evalua-
tion framework
The remainder of this paper is organized as fol-
lows. Section 2 reviews relevant studies in dia-
log state tracking. Section 3 introduces our new
approach and presents the model and features we
used in detail. Section 4 describes experiment set-
tings and gives the result. Section 5 concludes this
paper with a discussion for possible future direc-
tions.
2 Previous Work
For the task of dialog state tracking, previous
research focused on dynamic Bayesian models
(DBN)(Young et al, 2013). User goal, dialog his-
tory and other variables are modeled in a graphi-
cal model. Usually, Markov assumptions are made
and in each turn the dialog state is dependent on
the ASR outputs and the dialog state of the pre-
vious turn. Dependency on other features, such
as system action, dialog history could be assumed
as long as their likelihood is modeled. For a
POMDP-based dialog model, the state update rule
is as follows:
bt+1(st+1) = ?P (ot+1|st+1, at)?
st
P (st+1|st, at)bt(st) (1)
where bt(st) is the belief state at time t, ot+1 is the
observation at time t+ 1, at is the machine action.
Thus the dialog states are estimated incrementally
turn by turn.
Since each node has hundreds, or even thou-
sands, of possible assignments, approximation is
necessary to make efficient computation possible.
In POMDP-based dialog systems, two common
approaches are adopted (Young et al, 2013), i.e.,
N -best approximation and domain factorization.
In theN -best approach, the probability distribu-
tion of user goals are approximated using N -best
list. The hidden information state (HIS) model
(Young et al, 2010) makes a further simplification
that similar user goals are grouped into a single
entity called partition, inside which all user goals
are assigned the same probabilities. The Bayesian
update of dialog state (BUDS) model (Thomson
and Young, 2010) is a representative of the second
approach and adopts a different approximation
strategy, where each node is further divided into
sub-nodes for different domain concepts and in-
dependence assumptions of sub-nodes across con-
cepts are made. Recent studies have suggested
that a discriminative model may yield better per-
formance than a generative one (Bohus and Rud-
nicky, 2006). In a discriminative model, the emis-
sion part of the state update rule is modeled dis-
criminatively. Possible flawed assumptions in a
completely generative models could be mitigated
457
in this way, such as the approximation of obser-
vation probability using SLU scores (Williams,
2012a; Williams, 2012b).
3 Proposed Method
3.1 Discriminative State Tracking Model
Most previous methods model the distribution of
user goals for each turn explicitly, which can lead
to high computation cost. In our work, the joint
probability of all items on the N -best lists from
SLU is modeled directly and the state tracking re-
sult is generated at a post-processing stage. Thus
the state tracking problem is converted into a la-
beling task as is shown in equation 2, which in-
volves modeling the joint probability of the N -
best hypotheses.
bt(st) = P (H1,1, H1,2, ...,Ht,m?1, Ht,m) (2)
where Ht,m is a binary variable indicating the
truthfulness of the m-th hypothesis at turn t.
For each turn, the model takes into account all
the slots on theN -best lists from the first turn up to
the current one, and those slots predicted to be true
are added to the dialog state. The graphical model
is illustrated in figure 1. To predict dialog state at
turn t, the N -best items from turn 1 to t are all
considered. Hypotheses assigned true labels are
included in the dialog state. Compared to the DBN
approach, the dialog states are built ?jointly?. This
approach is reasonable because what the tracker
generates is just some combinations of all N -best
lists in a session, and there is no point guessing be-
yond SLU outputs. We leverage general structured
Conditional Random Fields (CRFs) to model the
probabilities of the N -best items, where factors
are used to strengthen local dependency. Since
CRF is a discriminative model, arbitrary overlap-
ping features can be added, which is commonly
considered as an advantage over generative mod-
els.
3.2 Conditional Random Fields
CRF is first introduced to address the problem
of label bias in sequence prediction (Lafferty et
al., 2001). Linear-chain CRFs are widely used to
solve common sequence labeling problem in nat-
ural language processing. General structured CRF
has also been reported to be successful in various
tasks (Sutton and McCallum, 2012).
In general structured CRF, factor templates are
utilized to specify both model structure and pa-
...
Hyp1
Hyp2
HypN
Turn t
Slot1=...
Slot2=...
...
Turn t-1
Figure 1: Dialog state update using CRFs, where
the 8 rectangles above denote N -best hypothe-
ses for each turn, and the box below represents
the dialog state up to the current turn. Con-
nections between rectangles denote ?Label-Label?
factors. ?Label-Observation? factors are not shown
for simplicity.
rameter tying (Sutton and McCallum, 2012). Fac-
tors are partitioned into a series of templates, and
factors inside each template share the same param-
eters.
p(y|x) = 1Z(x)
?
Cp?C
?
?c?Cp
?c(xc,yc; ?p), (3)
where C is the set of factor templates and x,y are
inputs and labels respectively. Template factors
are written as
?c(xc,yc; ?p) = exp
K(p)?
k=1
?pkfpk (xc,yc) (4)
and Z(x) is the normalizing function
Z(x) =
?
y
?
Cp?C
?
?c?Cp
?c(xc,yc; ?p) (5)
In the experiment we use Factorie1 to define and
train the model.
3.3 Model Structure and Features
In the model, slots in every N -best item up
to the current turn are represented as binary
variables. For simplification of data structure,
each slot in a single N -best item is extracted
and represented using different label vari-
ables, with the same rank indicating their
1Available from https://github.com/
factorie/factorie.
458
original places in the N -best list. For exam-
ple, the item slots: [from: Pittsburgh,
data: Tuesday], score: 0.85, rank: 2,
is converted to two slots: slots: [from:
Pittsburgh], score: 0.85, rank: 2 and
slots: [date: Tuesday], score: 0.85,
rank: 2. Label-label connections are specified
using factor templates between slot pairs, and
Label-observation templates are used to add
slot-wise features. Without label-label connection
the model is reduced to a maximum entropy
model, and with more connections added, the
graph tends to have loopy structures.
Two classes of feature sets (templates) in the ex-
periment are defined as follows.
(1) Label-Label factor templates are used to
strengthen the bond between certain slots.
Pairwise-slots of the same rank This template is
built for pairs of slots in a turn with the same
rank to bind their boolean assignment. To
avoid creating too many loops and make in-
ference efficient, the factors are added in such
an order that the slots involved in a single turn
are linked in a linear way.
Pairwise-slots with identical value Slots with
identical value may appear in the N -best
list for multiple times. Besides, user can
mention the same slot in different turns,
making these slots more reliable. Similar
ordering mechanism is utilized to avoid
redundant loops.
(2) Label-observation templates are used to add
features for the identification of the truthfulness of
slots.
SLU score and rank of slot The score generated
by the ASR and SLU components is a direct
indicator of the correctness degree of slots.
However, a slot?s true reliability is not neces-
sarily linear with its score. The relationship is
quite different for various ASR and SLU al-
gorithms, and scores produced by some ASR
are not valid probabilities. As we adopt a
data-driven approach, we are able to learn
this relationship from data. In addition to the
SLU score, the slot rank is also added to the
feature set.
Dialog history (grounding information) In
most spoken dialog systems, explicit and
implicit groundings are adapted to indicate
the correctness of the system belief. This
information is useful to determine the
correctness of slots. The grounding infor-
mation includes grounding type (implicit
or explicit grounding), user reply (negation
or confirmation) and corresponding SLU
scores.
Count of slots with identical value As previ-
ously mentioned, slots with identical values
can appear several times and slots with more
frequent occurrences are more likely to be
correct.
Domain-specific features Slots for some domain
concepts often have values with specific
forms. For example, in the DSTC data sets,
the route slots are usually filled with values
like ?61d?, ?35b?, and SLU often generates
noisy outputs like ?6d?, ?3d?. Thus the lexi-
cal form is a very useful feature.
Baseline Tracker The simple and fast 1-best
tracking algorithm is used as the baseline
tracker and exhibits a satisfying performance.
Thus the tracking result is added as an addi-
tional feature. This indicates the possibility
of combining tracking outputs from differ-
ent algorithms in this discriminative model,
which may improve the overall tracking per-
formance.
4 Experiment
4.1 Task and Data
The Dialog State Tracking Challenge (DSTC)2
aims at evaluating dialog state tracking algorithms
on shared real-user dialog corpus. In each dia-
log session, ASR and SLU results are annotated,
making it possible to conduct direct comparison
among various algorithms. For further details,
please refer to the DSTC handbook (Williams et
al., 2013b).
4.2 Corpus Preprocessing
The ASR and SLU components can generate many
noisy hypotheses which are completely wrong,
rendering the dialog corpus seriously imbalanced
at the level of slots (there are more wrong slots
than true slots). We use resampling to prevent
2http://research.microsoft.com/en-us/
events/dstc/
459
the model from biasing towards making negative
judgements. Before training, the total number of
correct slots in a set is counted, and equal num-
ber of wrong slots are sampled from the subset of
all the wrong slots. These chosen negative slots
plus all the positive slots together constitute the
effective slot set for model training, with remain-
ing slots fixed to their true value and regarded as
observed variables. To make full use of the dia-
log corpus, this process is repeated for eight times,
producing a bigger and balanced corpus.
4.3 Model Training
In the training phase, the log-likelihood function
is optimized using the LBFGS method with L2-
regularization. Loopy belief propagation is used
as the inference routine. It can be shown that for
factor graphs without loops, the marginal proba-
bilities produced by loopy belief propagation are
exact. Model selection is done according to the
log-likelihood on the development set.
4.4 Predicting and Tracking
For each dialog session, the predicted slot labels
are mapped to tracking results. To produce a N -
best list of tracking results, the top N configura-
tions of slots and corresponding probability scores
are generated. Gibbs sampling is adopted. The
sampling is repeated for 1000 times in each cor-
pus, after each sampling the configuration of slots
is mapped to certain tracking state. More efficient
inference routines, such as M-best belief propaga-
tion (Yanover and Weiss, 2004), could be utilized,
which would be suitable for practical real-time ap-
plication.
4.5 Results
After tracker outputs are generated based on the
sampling results, they are scored using evaluation
tools provided by the DSTC organizers. Several
metrics are evaluated, including precisions, ROC
performance, etc. Individual and joint slots are
scored respectively. And different schedules are
used, which indicats the turns included for evalu-
ation. Partial results are shown in table 1. A sys-
tematic analysis by the organizers is in the DSTC
overview paper (Williams et al, 2013a). The com-
plete challenge results can be found on DSTC
website. On the test sets of test1, test2 and test3,
the CRF-based model achieves better performance
than the simple baseline in most metrics. How-
ever, on test4, the performance degrades seriously
when there is a mismatch between training data
and test data, since test4 is produced by a different
group and does not match the training set. It shows
that the CRF-based model is very flexible and is
able to learn the properties of ASR and SLU, thus
adapting to a specific system. But it has a tendency
of overfitting .
Test1 Test4
Metric CRF BASE CRF BASE
ACC 0.987 0.983 0.960 0.986
L2 0.020 0.021 0.046 0.017
MRR 0.990 0.988 0.980 0.990
CA05 0.987 0.983 0.960 0.986
EER 0.015 0.983 0.021 0.012
Table 1: Results of slot ?Date? on Test1 and Test4
(schedule1 is used). The tracker used on Test4 is
trained on Test3. Details of the metrics can be
found in DSTC handbook(Williams et al, 2013b)
5 Conclusions and Future Directions
We proposed a CRF-based discriminative ap-
proach for dialog state tracking. Preliminary re-
sults show that it achieves better performance than
the 1-best baseline tracker in most metrics when
the training set and testing set match. This indi-
cates the feasibility of our approach which directly
models joint probabilities of the N -best items.
In the future, we will focus on the following
possible directions to improve the performance.
Firstly, we will enrich the feature set and add more
domain-related features. Secondly, interactions of
slots between dialog turns are not well modeled
currently. This problem can be alleviated by tun-
ing graph structures, which deservers further stud-
ies. Moreover, it is challenging to use online train-
ing methods, so that the performance could be im-
proved incrementally when more training samples
are available.
6 Acknowledgments
This work is partially supported by the Na-
tional Natural Science Foundation of China (Nos.
10925419, 90920302, 61072124, 11074275,
11161140319, 91120001), the Strategic Prior-
ity Research Program of the Chinese Academy
of Sciences (Grant Nos. XDA06030100,
XDA06030500), the National 863 Program (No.
2012AA012503) and the CAS Priority Deploy-
ment Project (No. KGZD-EW-103-2).
460
References
Dan Bohus and Alex Rudnicky. 2006. A ?k hypotheses
+ other? belief updating model. In Proceedings of
the 2006 AAAI Workshop on Statistical and Empiri-
cal Approaches for Spoken Dialogue Systems, pages
13?18, Menlo Park, California. The AAAI Press.
John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
Charles A. Sutton and Andrew McCallum. 2012. An
introduction to conditional random fields. Founda-
tions and Trends in Machine Learning, 4(4):267?
373.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013a. The dialog state track-
ing challenge. In Proceedings of the 14th SIGdial
workshop on Discourse and Dialogue.
Jason D. Williams, Antoine Raux, Deepak Ra-
machandran, and Alan Black. 2013b. Dia-
log state tracking challenge handbook. Avail-
able from http://research.microsoft.
com/apps/pubs/?id=169024.
Jason D. Williams. 2012a. Challenges and opportu-
nities for state tracking in statistical spoken dialog
systems: Results from two public deployments. Se-
lected Topics in Signal Processing, IEEE Journal of,
6(8):959 ?970.
Jason D. Williams. 2012b. A critical analysis of two
statistical spoken dialog systems in public use. In
SLT, pages 55?60. IEEE.
Chen Yanover and Yair Weiss. 2004. Finding the
m most probable configurations using loopy belief
propagation. Advances in Neural Information Pro-
cessing Systems, 16:289?296.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Gas?ic?, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160?1179.
461
Proceedings of the SIGDIAL 2014 Conference, pages 327?331,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Markovian Discriminative Modeling for Dialog State Tracking
Hang Ren, Weiqun Xu, Yonghong Yan
The Key Laboratory of Speech Acoustics and Content Understanding
Institute of Acoustics, Chinese Academy of Sciences
21 North 4th Ring West Road, Beijing, China, 100190
{renhang, xuweiqun, yanyonghong}@hccl.ioa.ac.cn
Abstract
Discriminative dialog state tracking has
become a hot topic in dialog research com-
munity recently. Compared to genera-
tive approach, it has the advantage of be-
ing able to handle arbitrary dependent fea-
tures, which is very appealing. In this
paper, we present our approach to the
DSTC2 challenge. We propose to use dis-
criminative Markovian models as a natu-
ral enhancement to the stationary discrim-
inative models. The Markovian structure
allows the incorporation of ?transitional?
features, which can lead to more effi-
ciency and flexibility in tracking user goal
changes. Results on the DSTC2 dataset
show considerable improvements over the
baseline, and the effects of the Markovian
dependency is tested empirically.
1 Introduction
Spoken dialog systems (SDS) have become much
more popular these days, but still far from wide
adoption. One of the most outstanding problems
that affect user experience in an SDS is due to
automatic speech recognition (ASR) and spoken
language understanding (SLU) errors. While the
advancement of ASR technology has a positive ef-
fect on SDS, it is possible to improve the SDS user
experience by designing a module which explicitly
handles ASR and SLU errors. With accurately es-
timated dialog state, the dialog manager could se-
lect more effective and flexible dialog actions, re-
sulting in shorter dialogs and higher dialog success
rate. Dialog state tracking is the task of identifying
the correct dialog state (user action, user goal, etc.)
from ASR and SLU outputs in the presence of er-
rors. Commercial dialog systems these days usu-
ally use simple dialog state tracking strategies that
only consider the most probable SLU output. Pre-
vious research shows that several errors in dialog
state tracking can be rectified by considering the
full N-best results from the ASR and SLU compo-
nents (Williams, 2012). Thus it is very important
to develop robust and practical dialog state track-
ing models.
In statistical dialog state tracking, models
can be roughly divided into two major classes,
i.e. generative and discriminative. Generative
(Bayesian) dialog tracking models are prevalent
in early studies due to its close relationship with
the POMDP dialog management model (Young et
al., 2013). Generative models generally use Dy-
namic Bayesian Networks to model the observa-
tion probability P (O
t
|S
t
) and transition probabil-
ity P (S
t
|S
t?1
), where O
t
and S
t
are observations
and dialog state at turn t. In a discriminative
model, the conditional probability P (S
t
|O
t
1
) is
modeled directly, where O
t
1
is all the observations
from turn 1 to t. One problem with the generative
models is that the independent assumptions are al-
ways not realistic. For example, N-best hypothe-
ses are often assumed independent of each other,
which is flawed in realistic scenarios (Williams,
2012). Furthermore, it is intrinsically difficult for
generative models to handle overlapping features,
which prevents model designers from incorporat-
ing arbitrarily large feature set. Discriminative
model does not suffer from the above problems as
there is no need to make any assumptions about
the probabilistic dependencies of the features. As
a result. it is potentially able to handle much larger
feature sets and to make more accurate predic-
tions (Bohus and Rudnicky, 2006). Discrimina-
tive models also tend to be more data-driven, un-
like generative models in which many sub-models
parameters are heuristically tuned.
2 DSTC1 revisited
The first Dialog State Tracking Challenge
(DSTC1) for the first time provided a common
test bed for various state tracking methods, and
327
several participants employed various discrimi-
native models in the challenge. DSTC1 provided
real user dialog corpora in the domain of bus
route service to evaluate performance of various
state tracking methods. In DSTC1 there are 9
teams with 27 submissions, where discriminative,
generative and rule-based models are used in the
challenge. Maximum entropy models (Lee and
Eskenazi, 2013), conditional random fields (Lee,
2013) and neural networks (Henderson et al.,
2013) are the most frequently used discriminative
models, which gave competitive results on several
metrics. It has been empirically analyzed that dis-
criminative methods are especially advantageous
when the ASR/SLU confidence scores are poorly
estimated (Williams et al., 2013).
3 Discriminative modeling in dialog state
tracking
In the design of a slot-filling or task-oriented di-
alog systems, dialog state tracking can be consid-
ered as a classification problem, i.e. assigning pre-
defined values to a fixed number of slots. One
major problem in the formulation is that in com-
plex dialog scenarios the number of classes tends
to be very big, resulting in extremely sparse train-
ing instances for each class. This sparsity affects
the classification performance. A large predic-
tion domain also leads to computation inefficiency
which makes the model less practical. Usually we
could focus only on the on-list hypotheses, which
are the hypotheses appeared in the SLU results,
and all the other values in the slot value set are
grouped into a meta category Other. It is simi-
lar to the partition concept in HIS (Young et al.,
2010), and by doing this we could reduce the num-
ber of classes to a reasonable size. We use Y
t
to
denote the prediction domain at turn t. Although
the number of classes is reduced by focusing on
the dynamically generated Y
t
, some classes will
still suffer from the lack of training instances, and
what is even worse is that a large portion of the
classes will not have any training data, since in
practical SDS deployment it is hard to collect a
large dialog corpus. To handle the data sparseness
problem, parameters are often shared across dif-
ferent slots, or even data sets, and by doing this the
model complexity could be effectively controlled
and the overfitting problem would be alleviated.
Williams proposed to use various techniques from
multi-domain learning to improve model perfor-
Monday: 0.5
Thursday: 0.2
Other: 0.3
Monday: 0.7
Tuesday: 0.1
Thursday: 0.1
Other: 0.1
Observations 
from turn 1 to t
Turn t-1 Turn t
Figure 1: Markovian discriminative model depen-
dency diagram. In this figure the dialog state is
simplified to a single slot variable: date, the do-
main of the slot typically increases as dialog con-
tinues, which includes all the slot values appeared
as SLU results. As indicated by the arrows, S
t
depends on S
t?1
and O
t
1
. In stationary discrimi-
native model, there?s no dependency between ad-
jacent turns indicated by the upper arrow.
mance (Williams, 2013), which could be taken as
another way of parameter sharing.
3.1 Markovian discriminative model
A dialog can be naturally seen as a temporal se-
quence involving a user and an agent, where strong
dependencies exist between adjacent turns. In typ-
ical task-oriented dialogs, users often change their
goals when their original object cannot be satis-
fied. Even when the true user goal stays constant
in a dialog session, the agent?s perception of it will
tend to evolve and be more accurate as the con-
versation proceeds, and thus the dialog state will
often change. The states at adjacent turns are sta-
tistically correlated, and therefore it is important
to leverage this natural temporal relationship in
tracking dialog state. We enhance the stationary
discriminative model in a similar way as described
in (McCallum et al., 2000), by assuming Marko-
vian dependency between adjacent turns.
Thus, the original probability P (S
t
|O
t
1
) can be
factored into the following form:
P (S
t
|O
t
1
) = (1)
?
S
t?1
?S
P (S
t
|O
t
1
, S
t?1
)P (S
t?1
|O
t?1
1
)
The graphical model is shown is figure
1. Unlike stationary discriminative models,
328
we model the conditional transition probability
P (S
t
|O
t
1
, S
t?1
) instead of P (S
t
|O
t
1
) and the dia-
log state is updated according to equation 1 at each
turn. The feature functions in the structured model
can depend on the state of the previous turn, which
we call transitional features.
It is worth noting that stationary discriminative
model can include features built from dialog his-
tory (Metallinou et al., 2013). The major dif-
ference in utilizing this information from our ap-
proach is that by explicitly assuming the Marko-
vian dependency, the structured model is able to
exploit the whole probabilistic dialog state distri-
bution of the previous turn. The previous dialog
state S
t?1
is inferred from previous dialog history
O
t?1
1
, which contains higher level hypotheses than
the raw history features. Apart from that, the struc-
tured model can also use any stationary features
built from O
t
1
, which makes the stationary model
a special case of the structured one.
3.2 Neural network classifier
We use the family of multi-layer neural net-
works to model the transition probability
P (S
t
|O
t?1
1
, S
t?1
). To allow for the use of the
dynamic prediction domain, we utilize a forward
network structure similar to (Henderson et al.,
2013). Feature vectors for each class in Y
t
are
fed into the model and forwarded through several
hidden layers for non-linear transformation in the
hope that deeper layers may form higher abstrac-
tion of the raw inputs. The parameter vectors for
each class are shared. For each feature vector
the model generates a real score. The scores for
all the classes in Y
t
are then normalized using a
softmax function resulting in valid probabilities.
y
i
= W
l?1
? g
l?1
(. . . g
1
(W
1
?X
i
) . . .) (2)
P
Y
= Softmax(y
1
, . . . , y
|Y
t
|
) (3)
where g
1
to g
l?1
are sigmoid functions, W
i
is the
weight matrix for linear transformation at layer i
and X
i
= f(O
t
1
, y
i
) is the feature vector for class
i. We also test maximum entropy models, which
can be seen as a simple neural network without
hidden layers:
P (Y = y|O
t
1
) =
e
??f(O
t
1
,y)
?
y?Y
e
??f(O
t
1
,y)
(4)
4 DSTC2 challenge
DSTC2 is the second round of Dialog State Track-
ing Challenge, and it provides dialog corpora
collected from real human-machine dialogs in a
restaurant domain. The corpora are split into la-
beled training and development sets and unlabeled
test set. Test sets are collected from a SDS dif-
ferent from the training and development set to
reflect the mismatch in real deployment. Unlike
DSTC1, the user goal often changes in DSTC2
when the condition specified by the user cannot
be met. For evaluation DSTC2 defined a number
of metrics among which several featured metrics
are selected. Besides tracking user goals (the val-
ues of each slot), two additional states method and
requested slots are also defined, which track the
method to query and the slots requested by users
respectively. Further details about DSTC2 could
be found in (Henderson et al., 2014).
5 Feature set
We briefly describe the feature set used in our sys-
tem. We only use the live SLU information pro-
vided by the organizers, and no extra external data
is used. The features used can be divided into two
classes.
stationary features which only depend on the
observations and the class (slot value) pre-
dicted at current turn in the form of f(y
t
, O
t
).
transitional features that can also depends on
the predicted class at the previous turn in the
form of f(y
t
, y
t?1
, O
t
).
Stationary features include:
? SLU Scores: confidence scores of the current
prediction binned into boolean values, raw
scores are also added as real features.
? SLU Status: whether the prediction is denied,
informed and confirmed in the current turn.
? Dialog history: whether the prediction has
been denied, informed and confirmed in all
the dialog turns until the current one.
? User/system action: The most probable user
action and the machine action in the current
turn.
The transitional features are as follows:
? Transition1: whether the predictions in the
previous and the current turn are the same.
329
Name Model Class Hidden layers
Entry1 MEMM ?
Entry2 Structured NN [50]
Entry3 Structured NN [50, 30]
MLP Stationary NN [50, 30]
Table 1: Configurations of models. The model
MLP uses the same structure as Entry3, but with-
out the transitional features described in section 5.
Number in brackets denotes the number of units
used in each hidden layers.
? Transition2: joint feature of Transition1 in
conjunction with the machine action in cur-
rent turn, i.e. for each machine cation, Tran-
sision1 is replicated and only the one corre-
sponding to the machine action at current turn
is activated.
Transitional features are specific to Markovian
models while stationary features can be used in
any discriminative models.
6 Model training
Markovian models in various forms are tested to
find the most appropriate structure for the task.
Models for ?method? and ?state? are built sepa-
rately using similar structured models.
When using the maximum entropy model to
build the conditional probability, the Markovian
model is equivalent to the maximum-entropy
Markov model (MEMM) model introduced in
(McCallum et al., 2000). More sophisticated neu-
ral networks with different configurations are used
to fit the model to more complex patterns in the
input features. In tracking the state ?goal?, the
joint distribution of slots is built assuming differ-
ent slots are independent of each other. From the
perspective of practical implementation, one ad-
vantage of the simpler MEMM model is that the
training objective is convex. Thus the optimiza-
tion routine is guaranteed to find the global opti-
mum, while neural networks with hidden layers al-
ways have many local optima which require care-
ful initialization of the parameters. LBFGS (Liu
and Nocedal, 1989) is used in optimizing the batch
log-likelihood objective and L1 and L2 regulariz-
ers are used to penalize the model from overfitting.
We train the model on the training set, the devel-
opment set is used for model selection and models
produced at each training iteration are evaluated.
State Tracker ACC L2 CA05
Goal
Baseline 0.619 0.738 0.000
Entry1 0.707 0.447 0.223
Entry2 0.713 0.437 0.207
Entry3 0.718 0.461 0.100
MLP 0.713 0.448 0.128
Method
Baseline 0.875 0.217 0.000
Entry1 0.865 0.228 0.199
Entry2 0.871 0.211 0.290
Entry3 0.871 0.210 0.287
MLP 0.946 0.092 0.000
Requested
Baseline 0.884 0.196 0.000
Entry1 0.932 0.118 0.057
Entry2 0.947 0.093 0.218
Entry3 0.951 0.085 0.225
MLP 0.863 0.231 0.291
Table 2: Evaluation results on the DSTC2 test set.
ACC stands for accuracy, L2 measures the Eu-
clidean distance between the predicted distribution
and the ground truth vector with only the correct
hypothesis set to 1. CA05 is the correct accep-
tance rate when false acceptance rate is 5%. De-
tails of the metrics can be found in (Henderson et
al., 2014). Except L2, the larger the scores, the
better the performance.
In DSTC2 we submitted 3 trackers, an additional
tracker without the transitional features is trained
afterwards for comparison. Configurations of the
models are described in table 1.
7 Experiments and part of the results
Featured metrics on the test set are shown in ta-
ble 2. By most metrics our models are superior
to the simple baseline. Especially in tracking user
goals which is the most important state to track in
DSTC2, the discriminative trackers show consid-
erable performance gain. Judging from the per-
formance of Entry1 to Entry3, we can conclude
that the more complex 2-layer neural networks
have better performance. Markovian neural net-
works can fit to the training instances with much
more flexibility than the simple MEMM model.
We have also trained a standard multi-layer neural
network (MLP) model by disabling all the transi-
tional features. By comparing the model ?Entry 3?
and ?MLP?, which share the same network struc-
ture, we explicitly test the effect of the Marko-
vian structure. On the state ?goal? and ?requested?,
the Markovian model shows better tracking accu-
330
racies, which means that the Markovian structure
has a positive effect on fitting the target. But in
tracking the state ?method?, the MLP model has
the best performance among all the models com-
pared. Thus although the log-likelihood increases
considerably on the training set by adding the tran-
sitional features, the overfiting to the training set is
more serious in tracking ?method?.
8 Conclusion
We described the models used in the DSTC2 chal-
lenge. We proposed a novel approach to enhanc-
ing the model capability of stationary discrimina-
tive models in dialog state tracking by assuming
Markovian dependencies between adjacent turns.
The results showed better performance than the
simple baseline which uses the most probable hy-
pothesis, and we empirically compared the mod-
els with and without the Markovian dependency.
In future work, more discriminative models in dif-
ferent forms could be compared to evaluate their
capability, and the effects of the Markovian struc-
ture and transitional features needs to be further
studied.
Acknowledgments
We would like to thank the DSTC committee for
their great efforts in organizing the challenge. We
also thank the anonymous reviewers for their con-
structive comments.
This work is partially supported by the Na-
tional Natural Science Foundation of China (Nos.
10925419, 90920302, 61072124, 11074275,
11161140319, 91120001), the Strategic Prior-
ity Research Program of the Chinese Academy
of Sciences (Grant Nos. XDA06030100,
XDA06030500), the National 863 Program (No.
2012AA012503) and the CAS Priority Deploy-
ment Project (No. KGZD-EW-103-2).
References
Dan Bohus and Alex Rudnicky. 2006. A k-
hypotheses+ other belief updating model. In Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for
the dialog state tracking challenge. In Proceedings
of the SIGDIAL 2013 Conference, pages 467?471,
Metz, France, August. Association for Computa-
tional Linguistics.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGDIAL 2014
Conference, Baltimore, U.S.A., June.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, pages
414?422, Metz, France, August. Association for
Computational Linguistics.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the
SIGDIAL 2013 Conference, pages 442?451, Metz,
France, August. Association for Computational Lin-
guistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Andrew McCallum, Dayne Freitag, and Fernando C. N.
Pereira. 2000. Maximum entropy markov mod-
els for information extraction and segmentation. In
Pat Langley, editor, ICML, pages 591?598. Morgan
Kaufmann.
Angeliki Metallinou, Dan Bohus, and Jason Williams.
2013. Discriminative state tracking for spoken di-
alog systems. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 466?475, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference, page 404?413, Metz, France, August.
Association for Computational Linguistics.
Jason Williams. 2012. A critical analysis of two
statistical spoken dialog systems in public use. In
2012 IEEE Spoken Language Technology Workshop
(SLT), pages 55?60.
Jason Williams. 2013. Multi-domain learning and
generalization in dialog state tracking. In Proceed-
ings of the SIGDIAL 2013 Conference, pages 433?
441, Metz, France, August. Association for Compu-
tational Linguistics.
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech & Lan-
guage, 24(2):150?174.
Steve Young, Milica Ga?si?c, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE, 101(5):1160?1179.
331
