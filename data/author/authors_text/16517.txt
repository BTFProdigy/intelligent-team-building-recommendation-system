Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 608?616, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CoMeT: Integrating different levels of linguistic modeling for
meaning assessment
Niels Ott Ramon Ziai Michael Hahn Detmar Meurers
Sonderforschungsbereich 833
Eberhard Karls Universita?t Tu?bingen
{nott,rziai,mhahn,dm}@sfs.uni-tuebingen.de
Abstract
This paper describes the CoMeT system, our
contribution to the SemEval 2013 Task 7 chal-
lenge, focusing on the task of automatically
assessing student answers to factual questions.
CoMeT is based on a meta-classifier that uses
the outputs of the sub-systems we developed:
CoMiC, CoSeC, and three shallower bag ap-
proaches. We sketch the functionality of all
sub-systems and evaluate their performance
against the official test set of the challenge.
CoMeT obtained the best result (73.1% accu-
racy) for the 3-way unseen answers in Beetle
among all challenge participants. We also dis-
cuss possible improvements and directions for
future research.
1 Introduction
Our contribution to the SemEval 2013 Task 7 chal-
lenge (Dzikovska et al, 2013) presented here is based
on our research in the A4 project1 of the SFB 833,
which is dedicated to the question how meaning can
be computationally compared in realistic situations.
In realistic situations, utterances are not necessarily
well-formed or complete, there may be individual
differences in situative and world knowledge among
the speakers. This can complicate or even preclude
a complete linguistic analysis, leading us to the fol-
lowing research question: Which linguistic repre-
sentations can be used effectively and robustly for
comparing the meaning of sentences and text frag-
ments computationally?
1http://purl.org/dm/projects/sfb833-a4
In order to work on effective and robust processing,
we base our work on reading comprehension exer-
cises for foreign language learners, of which we are
also collecting a large corpus (Ott et al, 2012). Our
first system, CoMiC, is an alignment-based approach
which exists in English and German variants (Meur-
ers et al, 2011a; Meurers et al, 2011b). CoMiC
uses various levels of linguistic abstraction from sur-
face tokens to dependency parses. Further work that
we are starting to tackle includes the utilization of
Information Structure (Krifka, 2007) in the system.
The second approach emerging from the research
project is CoSeC (Hahn and Meurers, 2011; Hahn
and Meurers, 2012), a semantics-based system for
meaning comparison that was developed for German
from the start and was ported to operate on English
for this shared task. As a novel contribution in this
paper, we present CoMeT (Comparing Meaning in
Tu?bingen), a system that employs a meta-classifier
for combining the output of CoMiC and CoSeC and
three shallower bag approaches.
In terms of the general context of our work, short
answer assessment essentially comes in the two fla-
vors of meaning comparison and grading, the first
trying to determine whether or not two utterances
convey the same meaning, the latter aimed at grading
the abilities of students (cf. Ziai et al, 2012). Short
answer assessment is also closely related to the field
of Recognizing Textual Entailment (RTE, Dagan et
al., 2009), which this year is directly reflected by
the fact that SemEval 2013 Task 7 is the Joint Stu-
dent Response Analysis and 8th Recognizing Textual
Entailment Challenge.
608
Turning to the organization of this paper, section 2
introduces the three types of sub-systems and the
meta-classifier. In section 3, we report on the evalu-
ation results of each sub-system both for our devel-
opment set as well as for the official test set of the
shared task. We then discuss possible causes and
implications of the findings we made by participating
in the shared task.
2 Systems
The CoMeT system that we describe in this paper
is a combination of three types of sub-systems in
one meta-classifier. CoSeC and CoMiC are sys-
tems that align linguistic units in the student answer
to those in the reference answer. In contrast, the
bag-based approaches employ a vocabulary of words,
lemmas, and Soundex hashes constructed from all
of the student answers in the training data. In the
meta-classifier, we tried to combine the benefits of the
named sub-systems into one large system that eventu-
ally computed our submission to the SemEval 2013
Task 7 challenge.
2.1 CoMiC
CoMiC (Comparing Meaning in Context) is an
alignment-based system, i.e., it operates on a map-
ping of linguistic units found in a student answer to
those given in a reference answer. CoMiC started off
as a re-implementation of the Content Assessment
Module (CAM) of Bailey and Meurers (2008). It
exists in two flavors: CoMiC-DE for German, de-
scribed in Meurers et al (2011b), and CoMiC-EN for
English, described in Meurers et al (2011a). Both
systems are positioned in the landscape of the short
answer assessment field in Ziai et al (2012). In this
paper, we refer to CoMiC-EN simply as CoMiC.
Sketched briefly, CoMiC operates in three stages:
1. Annotation uses various NLP modules to equip
student answers and reference answers with lin-
guistic abstractions of several types.
2. Alignment creates links between these linguistic
abstractions from the reference answer to the
student answer.
3. Classification uses summary statistics of these
alignment links in machine learning in order to
assign labels to each student answer.
Automatic annotation and alignment are imple-
mented in the Unstructured Information Management
Architecture (UIMA, Ferrucci and Lally, 2004). Our
UIMA modules mainly wrap around standard NLP
tools of which we provide an overview in Table 1.
We used the standard statistical models which are
provided with the NLP tools.
Annotation Task NLP Component
Sentence Detection OpenNLP2
Tokenization OpenNLP
Lemmatization morpha (Minnen et al, 2001)
Spell Checking Edit distance (Levenshtein, 1966),
SCOWL word list3
Part-of-speech Tagging TreeTagger (Schmid, 1994)
Noun Phrase Chunking OpenNLP
Synonyms and WordNet (Fellbaum, 1998)
Semantic Types
Similarity Scores PMI-IR (Turney, 2001)
on UkWaC (Baroni et al, 2009)
Dependency Relations MaltParser (Nivre et al, 2007)
Keyword extraction Heads from dependency parse
Table 1: NLP tools used for CoMiC and Bag Approaches
Annotation ranges from very basic linguistic units
such as sentences and tokens with POS and lemmas,
over NP chunks, up to full dependency parses of
the input. For distributional semantic similarity via
PMI-IR (Turney, 2001), a local search engine based
on Lucene (Gospodnetic? and Hatcher, 2005) querying
the UkWaC corpus (Baroni et al, 2009) was used,
since all major search engines meanwhile have shut
down their APIs.
After the annotation of linguistic units has taken
place, candidate alignment links are created within
UIMA. In a simple example case, a candidate align-
ment link is a pair of tokens that is token identical
in the student answer and in the reference answer.
The same token in the student answer may also be
part of a candidate alignment link that maps to an-
other token in the reference answer that, e.g., has the
same lemma, or is a possible synonym, or again is
token identical. Other possible links are based on
spelling-corrected tokens, semantic types, or high
values of the PMI-IR similarity measure.
Words that are present in the reading comprehen-
sion question and that are also found in the student an-
swer are excluded from alignment, resulting in a very
2http://incubator.apache.org/opennlp
3http://wordlist.sourceforge.net
609
basic implementation of an approach to givenness
(cf. Halliday, 1967, p. 204 and many others since).
Subsequently, a globally optimal alignment of lin-
guistic units in the reference answer and student an-
swer is determined using the Traditional Marriage
Algorithm (Gale and Shapley, 1962).
At this point, processing within UIMA comes to
an end with an output module that generates the files
containing the features for machine learning. These
features basically are summary statistics of the types
of alignment links. An overview of these numeric
features used is given in Table 2.
Feature Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2./3. Token Overlap Percent of aligned
target/learner tokens
4./5. Chunk Overlap Percent of aligned
target/learner chunks
6./7. Triple Overlap Percent of aligned
target/learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of
(0-5) token-level alignments
Table 2: Features used in CoMiC?s classification phase
Current versions of CoMiC use the WEKA toolkit
(Hall et al, 2009), allowing us to experiment with
different machine learning strategies. In general, any
type of classification can be trained in this machine
learning phase, a binary correct vs. incorrect de-
cision as in the 2-way task being the simplest case.
The best results with CoMiC on our held-out develop-
ment set were achieved using WEKA?s J48 classifier,
which is an implementation of decision tree based on
Quinlan (1993).
In terms of linguistic abstractions, CoMiC leaves
the choice of representations used to its alignment
step. However, in the final machine learning step, no
concrete information about linguistic units is present
any more. The machine learning component only
sees alignment configurations which are indepen-
dent of concrete words, phrases, or any other lin-
guistic information. This high level of abstraction
suggests that CoMiC should perform better than other
approaches on unseen topics and unseen questions,
since it does not rely on concrete units as, e.g., a
bag-of-words approach does.
2.2 CoSeC
CoSeC (Comparing Semantics in Context) performs
meaning comparison on the basis of an underspec-
ified semantic representation robustly derived from
the learner and the reference answers. The sys-
tem was developed for German (Hahn and Meurers,
2012), on the basis of which we created the English
CoSeC-EN for the SemEval 2013 Task 7 challenge.
Using an explicit semantic formalism in principle
makes it possible to precisely represent meaning dif-
ferences. It also supports a direct representation of
Information Structure as a structuring of semantics
representations (Krifka, 2007).
CoSeC is based on Lexical Resource Semantics
(LRS, Richter and Sailer, 2004). Being an under-
specified semantic formalism, LRS avoids the costly
computation of all readings and provides access to
the building blocks of the semantic representation,
while additional constraints provide the information
about their composition.
As described in Hahn and Meurers (2011), LRS
representations can be derived automatically using
a two-step approach based on part-of-speech tags
assigned by TreeTagger (Schmid, 1994) and depen-
dency parses by MaltParser (Nivre et al, 2007). First,
the dependency structure is transformed into a com-
pletely lexicalized syntax-semantics interface rep-
resentation, which abstracts away from some form
variation at the surface. These representations are
then mapped to LRS representations. The approach
is robust in that it always results in an LRS structure,
even for ill-formed sentences.
CoSeC then aligns the LRS representations of the
reference answer and the student answer to each other
and also to the representation of the question. The
alignment approach takes into account local criteria,
namely the semantic similarity of pairs of elements
that are linked by the alignment, as well as global
criteria measuring the extent to which the alignment
610
preserves structure at the levels of variables and the
subterm structure of the semantic formulas.
Local similarity of semantic expressions is esti-
mated using WordNet (Fellbaum, 1998), FrameNet
(Baker et al, 1998), PMI-IR (Turney, 2001) on the
UkWaC (Baroni et al, 2009) as used in CoMiC, the
Minimum Edit Distance (Levenshtein, 1966), and
special parameters for comparing functional elements
such as quantifiers and grammatical function labels.
Based on the alignments, the system marks ele-
ments which are not linked to elements in the ques-
tion or which are linked to the semantic contribution
of an alternative in an alternative question as ?fo-
cused?. This is intended as a first approximation of
the concept of focus in the sense of Information Struc-
ture (von Heusinger, 1999; Kruijff-Korbayova? and
Steedman, 2003; Krifka, 2007), an active field of re-
search in linguistics addressing the question how the
information in sentences is packaged and integrated
into discourse. Focus elements are expected to be
particularly relevant for determining the correctness
of an answer (Meurers et al, 2011b).
Overall meaning comparison is then done based
on a set of numerical scores computed from the align-
ments and their quality. For each of these scores, a
threshold is empirically determined, over which the
student answer is considered to be correct. Among
the scores discussed by Hahn and Meurers (2011),
weighted-target focus, consistently scored best in the
development set. This score measures the percent-
age of terms in the semantic representation of the
reference answer which are linked to elements of
the student answer in relation to the number of all
elements in the representation of the reference an-
swer. Only terms that were marked as focused in
the preceding step are counted. Functional elements,
i.e., quantifiers, predicates representing grammatical
function labels, or the lambda operator, are weighted
differently from other elements.
This threshold method can only be used to perform
2-way classification. Unlike the machine learning
step in CoMiC, it does not generalize to 3-way or
5-way classification.
The alignment algorithm uses several numerical
parameters, such as weights for the different compo-
nents measuring semantic similarities, weights for
the different overall local and global criteria, and
the weight of the weighted-target focus score. These
parameters are optimized using Powells algorithm
combined with grid-based line optimization (Press et
al., 2002). To avoid overfitting, the parameters and
the threshold are determined on disjoint partitions of
the training set.
In terms of linguistic abstractions, meaning assess-
ment in CoSeC is based entirely on underspecified
semantic representations. Surface forms are indi-
rectly encoded by the structure of the representation
and the predicate names, which are usually derived
from the lemmas. As with CoMiC, parameter opti-
mization and the determination of the thresholds for
the numerical scores do not involve concrete infor-
mation about linguistic objects. Again, the high level
of abstraction suggests that CoSeC should perform
better than other approaches on unseen topics and
unseen questions.
2.3 The Bag Approaches
Inspired by the bag-of-words concept that emerged
from information retrieval (Salton and McGill, 1983),
we designed a system that uses bag representations
of student answers. For each student answer, there
are three bags, each containing one of the following
representations: words, lemmas and Soundex hashes
of that answer. The question ID corresponding to
the answer is added to each bag as a pseudo-word,
allowing the machine learner to adjust to question-
specific properties. Based on the bag representations,
the approach compares a given student answer to a
model trained on all other known student answers.
On the one hand, this method ignores the presence of
reference answers (although they could be added to
the training set as additional correct answers), on the
other hand it makes use of information not taken into
account by alignment-based systems such as CoMiC
or CoSeC.
Concerning pre-processing, the linguistic anal-
yses such as tokenization and lemmatization are
identical to those of CoMiC, since the bag gener-
ator technically is just another output module of the
UIMA-based pipeline used there. No stop-word list
is used. The bags are fed into a support vector-based
machine learner. We used WEKA?s Sequential Min-
imal Optimization (SMO, Platt, 1998) implementa-
tion with the radial basis function (RBF) kernel, since
it yielded good results on our development set and
since it supports output of the estimated probabilities
611
for each class. The optimal gamma parameter and
complexity constant were estimated via 10-fold grid
search.
In terms of abstractions, all bag-based approaches
simply disregard word order and in case of binary
bags even word frequency. Still, a bit of the relation
between words is essentially encoded in their mor-
phology. This piece of information is discarded in
the bags of lemmas, eventually, e.g., putting words
like ?bulb? and ?bulbs? in the same vector slot. Fur-
ther away from the surface are the Soundex hashes,
a phonetic representation of English words patented
by Russell (1918). The well-known algorithm trans-
forms similar-sounding English words into the same
representation of characters and numbers, thereby
ironing out many spelling mistakes and common
confusion cases of homophones such as ?there? vs.
?their?. The MorphAdorner4 implementation we used
returns empty Soundex hashes for input tokens that
do not start with a letter of the alphabet. However,
we found in our experiments, that the presence of
these empty hashes in the bags has a positive impact
on performance. This is most likely due to the fact
that it discriminates answers containing punctuation
(not a letter of the alphabet) from those which do not.
Since the bag approaches use Soundex as pho-
netic equivalence classes, but no semantic equiva-
lence classes, they should perform best on the unseen
answers data in which most lexical material from the
test set is likely to already be present in the training
set.
2.4 CoMeT: A Meta-Classifier
As described in the previous sections, our sub-
systems perform short answer evaluation on differ-
ent representations and at different levels of abstrac-
tion. The bag approaches are very surface-oriented,
whereas CoSeC uses a semantic formalism to com-
pare answers to each other. We expected each system
to show its strengths in different test scenarios, so a
way was needed to combine the predictions of differ-
ent systems into the final result.
CoMeT (Comparing Meaning in Tu?bingen) is a
meta-classifier which builds on the predictions of
our individual systems (feature stacking, see Wolpert,
1992). The rationale is that if systems are comple-
4http://morphadorner.northwestern.edu
mentary, their combination will perform better (or at
least as good) than any individual system on its own.
The design is as follows:
Each system produces predictions on the training
set, using 10-fold cross-validation, and on the test set.
In addition to the predicted class, each system was
also made to output probabilities for each possible
class (cf., e.g., Tetreault et al, 2012a). The class
probabilities were then used as features in the meta
classifier to train a model for the test data. In addition
to the probabilities, we also used the question ID and
module ID in the meta-classifier, in the hope that they
would allow differentiation between scenarios. For
example, an unseen question ID means that we are
not testing on unseen answers and thus predictions
from systems with more abstraction from the surface
may be preferred.
The class probabilities come from different
sources, depending on the system. In the case of
CoMiC, they are extracted directly from the decision
trees. For the bag approaches, we used WEKA?s op-
tion to fit logistic models to the SVM output after
classification in order to estimate probabilities. Fi-
nally, the CoSeC probabilities are derived directly
from its final score. As mentioned in section 2.2,
CoSeC only does binary classification, so those prob-
abilities are used in the meta-classifier for all tasks.
Based on the results on our internal development
set (see section 3.1), we chose different system com-
binations for different scenarios. For unseen topics
and unseen questions, we used only CoMiC in com-
bination with CoSeC, since the inclusion of the bag
approaches had a negative impact on results. For un-
seen answers, we additionally included the bag mod-
els. All meta-classification was done using WEKA?s
Logistic Regression implementation. The results are
discussed in section 3.
3 Evaluation
In this section, we present the results for each of the
sub-systems, both on the custom-made split of the
training data we used in our development, as well as
on the official test data of the SemEval 2013 Task 7
challenge. Subsequently, we discuss possible causes
for issues raised by our evaluation results.
612
3.1 Development Set
In order to be as close as possible to the final test
setting, we replicated the official test scenarios on
the training set, resulting in a train/dev/test split for
each of the corpora. For Beetle, we held out all an-
swers to two random questions for each module to
form the unseen questions scenario, and five random
answers from each remaining question to form the
unseen answers scenario. For SciEntsBank, we held
out module LF for dev and module VB for test to
form the unseen topics scenario, because they have
an average number of questions (11). The LF module
turned out to be far more skewed towards incorrect
answers (76.8%) than the training set on average
(57.5%). While this skewedness needs to be taken
into account for the interpretation of the development
results, it did not have a negative effect on our fi-
nal test results. Furthermore, analogous to Beetle,
we held out all answers to one random question for
each remaining module for unseen-questions, and
two random answers from each remaining question
for unseen answers.
The dev set was used for tuning and design deci-
sions concerning which individual systems to com-
bine in the stacked classifier, while we envisaged
the test set to be used as a final checkpoint before
submission.
The accuracy results for all sub-systems on the
development set are reported in detail in Table 3.
The majority baseline reflects the accuracy a system
would achieve by always labelling any student answer
as ?incorrect?, hence it is equivalent to the percentage
of incorrect answers in the data. The lexical baseline
is the performance of the system provided by the
challenge organizers.
Beetle SciEntsBank
System d-uA d-uQ d-uA d-uQ d-uT
Maj. Baseline 57.14% 59.28% 54.30% 60.70% 76.84%
Lex. Baseline 75.43% 71.10% 63.44% 66.05% 59.54%
CoMiC 76.57% 71.52% 67.20% 70.23% 64.63%
Bag of Words 85.14% 62.03% 80.65% 54.65% 73.79%
? of Lemmas 85.71% 58.02% 80.11% 52.33% 74.55%
? of Soundex 86.86% 60.76% 81.18% 53.95% 72.77%
CoSeC 76.00% 74.89% 64.52% 73.49% 68.96%
CoMeT 88.00% 75.95% 81.18% 66.74% 68.45%
Table 3: Development set: accuracy for 2-way task (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
The systems presented in section 2 performed as
expected: The Bag-of-Soundex system achieved its
best scores on the unseen answers where overlap of
vocabulary was most likely, outperforming CoMiC
and CoSeC with accuracy values as high as 86.86%.
For Beetle unseen answers, the meta-classifier op-
erated as expected and improved the overall results
to 88.86%. For SciEntsBank unseen answers, it re-
mained stable at 81.18%.
As expected, CoMiC and CoSeC with their align-
ment not depending on vocabulary outperformed the
bag approaches in the other scenarios, in which the
question or even the domain were not known during
training. However, both alignment-based systems
failed on SciEntsBank?s unseen topics in comparison
to the rather high majority baseline.
3.2 Official Test Set
For our submission to the SemEval 2013 Task 7 chal-
lenge, we trained our sub-systems on the entire of-
ficial training set. The overall performance of the
CoMeT system on all sub-tasks is shown in Table 4.
Beetle SciEntsBank
uA uQ uA uQ uT
Lexical 2-way 79.7% 74.0% 66.1% 67.4% 67.6%
Overlap 3-way 59.5% 51.2% 55.6% 54.0% 57.7%
Baseline 5-way 51.9% 48.0% 43.7% 41.3% 41.5%
Best 2-way 84.5% 74.1% 77.6% 74.5% 71.1%
System 3-way 73.1% 59.6% 72.0% 66.3% 63.7%
5-way 71.5% 62.1% 64.3% 53.2% 51.2%
CoMeT 2-way 83.8% 70.2% 77.4% 60.3% 67.6%
3-way 73.1% 51.8% 71.3% 54.6% 57.9%
5-way 68.8% 48.8% 60.0% 43.7% 42.1%
Table 4: Official test set: overall accuracy of CoMeT (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
While CoMeT won the Beetle 3-way task in unseen
answers, our main focus is on the 2-way task. The
results for the 2-way task of our sub-systems on the
official test set are shown in Table 5.
The first row of the table reports the results of the
winning system of the challenge; the two baselines
are computed as before. In general, the accuracy val-
ues of CoMeT exhibit a drop of around 5% from
our development set to the official test set. The
meta-classifier was unable to benefit from the dif-
ferent sub-systems except for the unseen answers in
SciEntsBank that slightly outperformed the best bag
approach.
613
Beetle SciEntsBank
System uA uQ uA uQ uT
Best 84.50% 74.10% 77.60% 74.50% 71.10%
Maj. Baseline 59.91% 58.00% 56.85% 58.94% 57.98%
Lex. Baseline 79.70% 74.00% 66.10% 67.40% 67.60%
CoMiC 76.08% 70.57% 67.96% 66.30% 67.97%
Bag of Words 83.14% 67.52% 75.93% 57.84% 59.84%
? of Lemmas 83.60% 67.16% 76.67% 58.25% 58.81%
? of Soundex 84.05% 68.38% 75.93% 57.57% 58.02%
CoSeC 62.19% 63.61% 67.22% 58.94% 62.36%
CoMeT 83.83% 70.21% 77.41% 60.30% 67.62%
CoSeC* 75.40% 70.82% 72.04% 64.94% 70.60%
CoMeT* 84.51% 71.43% 79.26% 65.35% 69.53%
Table 5: Official test set: accuracy for 2-way task (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
Even though it does not live up to the standards of
the bag approaches in their area of expertise (unseen
answers), the CoMiC systems outperforms the bags
on the unseen question and unseen topic sub-sets as
expected. Note that on unseen topics, CoMiC still
scores 10% above the majority baseline on the official
test set, in contrast to the drop of more than 10%
below the baseline for the corresponding (skewed)
development set.
However, the results for CoSeC are around 10%
lower on the unseen questions, and almost 7% lower
on the unseen topics of the test data than on the de-
velopment set, a drop that the overall meta-classifier
(CoMeT) was unable to catch. Investigating this drop
in comparison to our development set, we checked
the correctness of the training script and discovered a
bug in the CoSeC setup that led to the parameters and
the thresholds being computed on the same partition
of the training set, i.e., the system overfitted to this
partition, while the remainder of the training set was
not used for training. Correcting the bug resulted in
CoSeC accuracy values broadly comparable to those
of CoMiC, as was the case on the development set.
This confirms that the reason for the drop in the sub-
mission was not a flaw in the CoSeC system as such,
but a programming bug in a peripheral component.
With this bug fixed, CoSeC performs 5%?13%
better on the test set, and the meta-classifier would
have been able to benefit from the regularly perform-
ing CoSeC, improving in performance up to 5%.
These two amended systems are listed as CoSeC*
and CoMeT* in Table 5. For the two unseen an-
swers scenarios, CoMeT* would outperform the best
scoring systems of the challenge in the 2-way task.
3.3 Discussion
In this section, we try to identify some general ten-
dencies from studying the results. Firstly, we can
observe that due to the strong performance of the bag
models, unseen answers scores are generally higher
than their counterparts. It seems that if questions
have been seen before, surface-oriented methods out-
perform more abstract approaches. However, the
picture is different for unseen domains and unseen
questions. We are generally puzzled by the fact that
many systems in the shared task scored worse on
unseen questions, where in-domain training data is
available, than on unseen domains, where this is not
the case. The CoMeT classifier suffered especially in
unseen questions of SciEntsBank, scoring lower than
our best system would have on its own (see Table 5);
even after the CoSeC bug was fixed, CoMeT* still
scored worse there than CoMiC on its own.
In general, we likely would have benefited from
domain adaptation, as described in, e.g., Daume III
(2007). Consider that the input for the meta-classifier
always consists of the same set of features produced
via standard cross-validation, regardless of the test
scenario. Instead, the trained model should have dif-
ferent feature weights depending on what the model
will be tested on.
4 Conclusion and Outlook
We presented our approach to Task 7 of SemEval
2013, consisting of a combination of surface-oriented
bag models and the increasingly abstract alignment-
based systems CoMiC and CoSeC. Predictions of
all systems were combined using a meta classifier in
order to produce the final result for CoMeT.
The results presented show that our approach per-
forms competitively, especially in the unseen answers
test scenarios, where we obtained the best result of all
participants in the 3-way task with the Beetle corpus
(73.1% accuracy). As expected, the unseen topics
scenario proved to be more challenging, with results
at 67.6% accuracy in the 2-way task for CoMeT. Sur-
prisingly, CoMeT performed consistently worse in
the unseen questions scenarios, which we attribute
to rather low CoSeC results there and to the way the
meta classifier is trained, which currently does not
take into account the test scenario it is trained for
and instead uses the module and question IDs as fea-
614
tures, which turned out not to be an effective domain
adaptation approach.
In our future research, work on CoMiC will con-
centrate on integrating two aspects of the context:
First, we are planning to develop an automatic ap-
proach to focus identification in order to pinpoint the
essential parts of the student answers. Second, for
data sets where a reading text is available, we will
try to automatically determine the location of the rel-
evant source information given the question, which
can then be used as alternative or additional reference
material for answer evaluation.
The CoMiC system currently also relies on the
Traditional Marriage Algorithm to select the optimal
global alignment between student answer and refer-
ence answer. We plan to replace this algorithm by
a machine learning component that can handle this
selection in a data-driven way.
For CoSeC, we plan to develop an extension that
allows for n-to-m mappings, hence improving the
alignment performance for multi-word units such as,
e.g., phrasal verb constructions.
The bag approaches could be augmented by explor-
ing additional levels of abstractions, e.g., semantic
equivalence classes constructed via WordNet lookup.
In sum, while we will also plan to explore opti-
mizations to the training setup of the meta-classifier
(e.g., domain adaptation along the lines of Daume
III, 2007), the main focus of our further research lies
in improving the individual sub-systems, which then
again are expected to push the overall performance
of the CoMeT meta-classifier system.
Acknowledgements
We are thankful to Sowmya Vajjala and Serhiy Bykh
for their valuable advice on meta-classifiers and other
machine learning techniques. We also thank the re-
viewers for their comments; in consultation with the
SemEval organizers we kept the length at 8 pages
plus references, the page limit for papers describing
multiple systems.
References
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In Joel Tetreault, Jill Burstein,
and Rachele De Felice, editors, Proceedings of the
3rd Workshop on Innovative Use of NLP for Building
Educational Applications (BEA-3) at ACL?08, pages
107?115, Columbus, Ohio. http://aclweb.org/
anthology/W08-0913.pdf.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings of
the 36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Conference
on Computational Linguistics, volume 1, pages 86?90,
Montreal, Quebec, Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A collec-
tion of very large linguistically processed web-crawled
corpora. Journal of Language Resources and Evalua-
tion, 3(43):209?226.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch, 2007. TiMBL: Tilburg
Memory-Based Learner Reference Guide, ILK Techni-
cal Report ILK 07-03. Induction of Linguistic Knowl-
edge Research Group Department of Communication
and Information Sciences, Tilburg University, Tilburg,
The Netherlands, July 11. Version 6.0.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, evalu-
ation and approaches. Natural Language Engineering,
15(4):i?xvii, 10.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student response
analysis and 8th recognizing textual entailment chal-
lenge. In *SEM 2013: The First Joint Conference on
Lexical and Computational Semantics, Atlanta, Geor-
gia, USA, 13-14 June.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts.
David Ferrucci and Adam Lally. 2004. UIMA: An ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natural
Language Engineering, 10(3?4):327?348.
David Gale and Lloyd S. Shapley. 1962. College admis-
sions and the stability of marriage. American Mathe-
matical Monthly, 69:9?15.
Otis Gospodnetic? and Erik Hatcher. 2005. Lucene in
Action. Manning, Greenwich, CT.
Michael Hahn and Detmar Meurers. 2011. On deriv-
ing semantic representations from dependencies: A
615
practical approach for evaluating meaning in learner
corpora. In Proceedings of the Intern. Confer-
ence on Dependency Linguistics (DEPLING 2011),
pages 94?103, Barcelona. http://purl.org/
dm/papers/hahn-meurers-11.html.
Michael Hahn and Detmar Meurers. 2012. Evaluat-
ing the meaning of answers to reading comprehen-
sion questions: A semantics-based approach. In Pro-
ceedings of the 7th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA-7) at
NAACL-HLT 2012, pages 94?103, Montreal. http:
//aclweb.org/anthology/W12-2039.pdf.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The WEKA data mining software: An update. In The
SIGKDD Explorations, volume 11, pages 10?18.
Michael Halliday. 1967. Notes on Transitivity and Theme
in English. Part 1 and 2. Journal of Linguistics, 3:37?
81, 199?244.
Manfred Krifka. 2007. Basic notions of information struc-
ture. In Caroline Fery, Gisbert Fanselow, and Manfred
Krifka, editors, The notions of information structure,
volume 6 of Interdisciplinary Studies on Information
Structure (ISIS). Universita?tsverlag Potsdam, Potsdam.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003. Dis-
course and information structure. Journal of Logic,
Language and Information (Introduction to the Special
Issue), 12(3):249?259.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-
ley. 2011a. Integrating parallel analysis modules to
evaluate the meaning of answers to reading comprehen-
sion questions. IJCEELL. Special Issue on Automatic
Free-text Evaluation, 21(4):355?369. http://purl.
org/dm/papers/meurers-ea-11.html.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.
2011b. Evaluating answers to reading comprehen-
sion questions in context: Results for German and
the role of information structure. In Proceedings of
the TextInfer 2011 Workshop on Textual Entailment,
pages 1?9, Edinburgh, Scotland, UK, July. http:
//aclweb.org/anthology/W11-2401.pdf.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natural
Language Engineering, 7(3):207?233.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Cre-
ation and analysis of a reading comprehension exercise
corpus: Towards evaluating meaning in context. In
Thomas Schmidt and Kai Wo?rner, editors, Multilin-
gual Corpora and Multilingual Corpus Analysis, Ham-
burg Studies in Multilingualism (HSM), pages 47?69.
Benjamins, Amsterdam. http://purl.org/dm/
papers/ott-ziai-meurers-12.html.
John C. Platt. 1998. Sequential minimal optimization:
A fast algorithm for training support vector machines.
Technical Report MSR-TR-98-14, Microsoft Research.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge, UK.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers.
Frank Richter and Manfred Sailer. 2004. Basic concepts
of lexical resource semantics. In Arnold Beckmann and
Norbert Preining, editors, European Summer School in
Logic, Language and Information 2003. Course Mate-
rial I, volume 5 of Collegium Logicum, pages 87?143.
Publication Series of the Kurt Go?del Society, Wien.
Robert C. Russell. 1918. US patent number 1.261.167, 4.
Gerard Salton and Michael J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill, New
York.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native language
identification. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING),
pages 2585?2602, Mumbai, India.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502, Freiburg, Germany.
Klaus von Heusinger. 1999. Intonation and Information
Structure. The Representation of Focus in Phonology
and Semantics. Habilitationssschrift, Universita?t Kon-
stanz, Konstanz, Germany.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5(2):241?259.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012. Short
answer assessment: Establishing links between re-
search strands. In Joel Tetreault, Jill Burstein, and
Claudial Leacock, editors, Proceedings of the 7th Work-
shop on Innovative Use of NLP for Building Edu-
cational Applications (BEA-7) at NAACL-HLT 2012,
pages 190?200, Montreal, June. http://aclweb.
org/anthology/W12-2022.pdf.
616
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 326?336,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Evaluating the Meaning of Answers to Reading Comprehension Questions
A Semantics-Based Approach
Michael Hahn Detmar Meurers
SFB 833 / Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{mhahn,dm}@sfs.uni-tuebingen.de
Abstract
There is a rise in interest in the evaluation of
meaning in real-life applications, e.g., for as-
sessing the content of short answers. The ap-
proaches typically use a combination of shal-
low and deep representations, but little use is
made of the semantic formalisms created by
theoretical linguists to represent meaning.
In this paper, we explore the use of the un-
derspecified semantic formalism LRS, which
combines the capability of precisely repre-
senting semantic distinctions with the ro-
bustness and modularity needed to represent
meaning in real-life applications.
We show that a content-assessment approach
built on LRS outperforms a previous approach
on the CREG data set, a freely available cor-
pus of answers to reading comprehension ex-
ercises by learners of German. The use of such
a formalism also readily supports the integra-
tion of notions building on semantic distinc-
tions, such as the information structuring in
discourse, which we show to be useful for con-
tent assessment.
1 Introduction
There is range of systems for the evaluation of short
answers. While the task is essentially about eval-
uating sentences based on their meaning, the ap-
proaches typically use a combination of shallow and
deep representations, but little use is made of the se-
mantic formalisms created by theoretical linguists to
represent meaning. One of the reasons for this is that
semantic structures are difficult to derive because of
the complex compositionality of natural language.
Another difficulty is that form errors in the input cre-
ate problems for deep processing, which is required
for extracting semantic representations.
On the other hand, semantic representations have
the significant advantage that they on the one hand
abstract away from variation in the syntactic real-
ization of the same meaning and on the other hand
clearly expose those distinctions which do make a
difference in meaning. For example, the difference
between dog bites man and man bites dog is still
present in deeper syntactic or semantic representa-
tions, while semantic representations abstract way
from meaning-preserving form variation, such as the
active-passive alternation (dog bites man ? man was
bitten by dog). This suggests that sufficiently robust
approaches using appropriate semantic formalisms
can be useful for the evaluation of short answers.
In this paper, we explore the use of Lexical Re-
source Semantics (Richter and Sailer, 2003), one
of the underspecified semantic formalisms combin-
ing the capability of precisely representing seman-
tic distinctions with the robustness and modularity
needed to represent meaning in real-life applica-
tions. Specifically, we address the task of evaluating
the meaning of answers to reading comprehension
exercises.
We will base our experiments on the freely avail-
able data set used for the evaluation of the CoMiC-
DE system (Meurers et al, 2011), which does not
use semantic representations. The data consists of
answers to reading comprehension exercise written
by learners of German together with questions and
corresponding target answers.
326
2 Related Work
There are several systems which assess the content
of short answers. Mitchell et al (2002) use hand-
crafted patterns which indicate correct answers to a
question. Similarly, Nielsen et al (2009) use manu-
ally annotated word-word relations or ?facets?. Pul-
man and Sukkarieh (2005) use machine learning
to automatically find such patterns. Other systems
evaluate the correctness of answers by comparing
them to one or more manually annotated target an-
swers. C-Rater (Leacock and Chodorow, 2003) and
the system of Mohler et al (2011) compare the syn-
tactic parse to the parse of target answers. A com-
parison of a range of content assessment approaches
can be found in Ziai et al (2012).
The work in this paper is most similar to a line
of work started by Bailey and Meurers (2008), who
present a system for automatically assessing an-
swers to reading comprehension questions written
by learners of English. The basic idea is to align
the student answers to a target answer using a par-
allel approach with several levels on which words
or chunks can be matched to each other. Classifica-
tion is done by a machine learning component. The
CoMiC-DE system for German is also based on this
approach (Meurers et al, 2011).
In terms of broader context, the task is related
to the research on Recognizing Textual Entailment
(RTE) (Dagan et al, 2006). In particular, align-
ment (e.g., MacCartney et al, 2008, Sammons et al,
2009) and graph matching approaches (Haghighi et
al., 2005, Rus et al, 2007) are broadly similar to our
approach.
3 General Setup
3.1 Empirical challenge: CREG
Our experiments are based on the freely available
Corpus of Reading comprehension Exercises in Ger-
man (CREG, Ott et al, 2012) . It consists of texts,
questions, target answers, and corresponding student
answers written by learners of German. For each
student answer, two independent annotators evalu-
ated whether it correctly answers the question. An-
swers were only assessed with respect to meaning;
the assessment is in principle intended to be inde-
pendent of grammaticality and orthography. The
task of our system is to decide which answers cor-
rectly answer the given question and which do not.
3.2 Formal basis: Lexical Resource Semantics
Lexical Resource Semantics (LRS) (Richter and
Sailer, 2003) is an underspecified semantic formal-
ism which embeds model-theoretic semantic lan-
guages like IL or Ty2 into constraint-based typed
feature structure formalisms as used in HPSG. It
is formalized in the Relational Speciate Reentrancy
Language (RSRL) (Richter, 2000).
While classical formal semantics uses fully ex-
plicit logical formulae, the idea of underspecified
formalisms such as LRS is to derive semantic rep-
resentations which are not completely specified and
subsume a set of possible resolved expressions, thus
abstracting away from ambiguities, in particular, but
not exclusively, scope ambiguities.
As an example for the representations, consider
the ambiguous example (1) from the CREG corpus.
(1) Alle
all
Zimmer
rooms
haben
have
nicht
not
eine
a
Dusche.
shower
?Not every room has a shower.?
?No room has a shower.?
The LRS representation of (1) is shown in Figure
1, where INCONT (INTERNAL CONTENT) encodes
the core semantic contribution of the head, EXCONT
(EXTERNAL CONTENT) the semantic representation
of the sentence, and PARTS is a list containing the
subterms of the representation.
?
?
?
?
?
?
?
?
?
?
?
INCONT haben(e)
EXCONT A
PARTS
?A, haben(e), ?x1(B? C),
zimmer(x1), ?x2 (D ? E), ? F,
dusche(x2), subj(e,x1), obj(e,x2)
?e(haben(e) ? subj(e,x1) ? obj(e,x2)
?
?
?
?
?
?
?
?
?
?
?
?
Ex2(D & E)
(haben(e) & subj(e,x1) & obj(e,x2))
    F
Ax1(B    C)
zimmer(x1) dusche(x2) Ee 
    A
Figure 1: LRS and dominance graph for (1)
The representation also includes a set of subterm
constraints, visualized as a dominance graph at the
327
bottom of the figure. The example (1) has several
readings, which is reflected in the fact that the rel-
ative scope of the two quantifiers and the negation
is not specified. The different readings of the sen-
tence can be obtained by identifying each of the
meta-variables A, . . . , F with one of the subformu-
las. Meta-variables are labels that indicate where a
formula can be plugged in; they are only part of the
underspecified representation and do not occur in the
resolved representation.
This illustrates the main strengths of an under-
specified semantic formalism such as LRS for prac-
tical applications. All elements of the semantic rep-
resentation are explicitly available on the PARTS list,
with dominance constraints and variable bindings
providing separate control over the structure of the
representation. The underspecified nature of LRS
also supports partial analyses for severely ill-formed
input or fragments, which is problematic for clas-
sical approaches to semantic compositionality such
as Montague semantics (Montague, 1973). Another
advantage of LRS as an underspecified formalism
is that it abstracts away from the computationally
costly combinatorial explosion of possible readings
of ambiguous sentences, yet it also is able to rep-
resent fine-grained semantic distinctions which are
difficult for shallow semantic methods to capture.
3.3 Our general approach
In a first step, LRS representations for the student
answer, the target answer, and the question are auto-
matically derived on the basis of the part-of-speech
tags assigned by TreeTagger (Schmid, 1994) and the
dependency parses by MaltParser (Nivre and Hall,
2005) in the way discussed in Hahn and Meurers
(2011). In this approach, LRS structures are de-
rived in two steps. First, surface representations
are mapped to syntax-semantics-interface represen-
tations, which abstract away from some form vari-
ation at the surface. In the second step, rules map
these interface representations to LRS representa-
tions. The approach is robust in that it always results
in an LRS structure, even for ill-formed sentences.
Our system then aligns the LRS representations
of the target answer and the student answer to each
other and also to the representation of the ques-
tion. Alignment takes into account both local crite-
ria, in particular semantic similarity, and global cri-
teria, which measure the extent to which the align-
ment preserves structure on the level of variables and
dominance constraints.
The alignments between answers and the question
are used to determine which elements of the seman-
tic representations are focused in the sense of In-
formation Structure (von Heusinger, 1999; Kruijff-
Korbayova? and Steedman, 2003; Krifka, 2008), an
active field of research in linguistics addressing the
question how the information in sentences is pack-
aged and integrated into discourse.
Overall meaning comparison in our approach is
then done based on a set of numerical scores com-
puted from potential alignments and their quality.
Given its LRS basis, we will call the system CoSeC-
DE (Comparing Semantics in Context).
4 Aligning Meaning Representations
The alignment is done on the level of the PARTS lists,
on which all elements of the semantic representation
are available:
Definition 1. An alignment a between two LRS
representations S and T with PARTS lists pn1 and
qm1 is an injective partial function from {1,...,n} to
{1,...,m}.
Requiring a to be injective ensures that every ele-
ment of one representation can be aligned to at most
one element of the other representation. Note that
this definition is symmetrical in the sense that the
direction can be inverted simply by inverting the in-
jective alignment function.
To automatically derive alignments, we define a
maximization criterion which combines three fac-
tors measuring different aspects of alignment qual-
ity. In addition to i) the similarity of the align-
ment links, the quality Q of the alignment a takes
into account the structural correspondence between
aligned elements by evaluating the consistency of
alignments ii) with respect to the induced variable
bindings ? and, and iii) with respect to dominance
constraints:
Q(a, ?|S, T ) = linksScore(a|S, T )
? variableScore(?)
? dominanceScore(a|S, T )
(1)
The approach thus uses a deep representation ab-
stracting away from the surface, but the meaning
328
comparison approach on this deep level is flat, yet
at the same time is able to take into account struc-
tural criteria. In consequence, the approach is mod-
ular because it uses the minimal building blocks of
semantic representations, but is able to make use of
the full expressive power of the semantic formalism.
4.1 Evaluating the Quality of Alignment Links
The quality of an alignment link between two ex-
pressions is evaluated by recursively evaluating the
similarity of their components. In the base case,
variables can be matched with any variable of the
same semantic type:
sim(x? , y? ) = 1
Meta-variables can be matched with any meta-
variable of the same semantic type:
sim(A? ,B? ) = 1
For predicates with arguments, both the predicate
name and the arguments are compared:
sim(P1(a
k
1), P2(b
k
1)) =
sim(P1, P2) ?
k?
i=1
sim(ai, bi)
(2)
If the predicates have different numbers of argu-
ments, similarity is zero. Linguistically well-known
phenomena where the number of arguments of se-
mantically similar predicates differ do not cause a
problem for this definition, because semantic roles
are linked to the verbal predicate via grammatical
function terms such as subj and obj predicating over
a Davidsonian event variable, as in Figure 1.1
For formulas with generalized quantifiers, the
quantifiers, the variables, the scopes and the restric-
tors are compared:
sim(Q1x1(? ? ?), Q2x2(? ? ?)) =
sim(Q1, Q2) ? sim(x1, x2)
?sim(?, ?) ? sim(?, ?)
(3)
Lambda abstraction is dealt with analogously.
The similarity sim(P1, P2) of names of predicates
and generalized quantifiers takes into account sev-
eral sources of evidence and is estimated as the max-
imum of the following quantities:
1In this paper, we simply use grammatical function names
in place of semantic role labels in the formulas. A more sophis-
ticated, real mapping from syntactic functions to semantic roles
could usefully be incorporated.
As a basic similarity, the Levenshtein distance
normalized to the interval [0,1] (with 1 denoting
identity and 0 total dissimilarity) is used. This ac-
counts for the high frequency of spelling errors in
learner language.
Synonyms in GermaNet (Hamp and Feldweg,
1997) receive the score 1.
For numbers, the (normalized) difference
|n1?n2|
max(n1,n2)
is used.
For certain pairs of dissimilar elements which be-
long to the same category, constant costs are de-
fined. This encourages the system to align these el-
ements, unless the structural factors, i.e., the quality
of the unifier and the consistency with dominance
constraints, discourage this. Such constants are de-
fined for pairs of grammatical function terms. Other
constants are defined for pairs of numerical terms
and for pairs of terms encoding affirmative and neg-
ative natural language expressions and logical nega-
tion.
Having defined how to compute the quality for
single alignment links, we still need to define how to
compute the combined score of the alignment links,
which we define to be the sum of the qualities of the
links:
linksScore(a|pn1 , q
m
1 ) =
n?
k=1
{
sim(pk, qa(k)) if a(k) is defined,
?NULL else.
(4)
The quality of a given overall alignment thus is
determined by the quality of the alignment links of
the PARTS elements which are aligned. For those
PARTS elements not aligned, a constant cost ?NULL
must be paid, which, however, may be smaller than
a costly alignment link in another overall alignment.
4.2 Evaluating Unifiers
Alignments between structurally corresponding se-
mantic elements should be preferred. For situations
in which they structurally do not correspond, this
may have the effect of dispreferring the pairing of
elements which in terms of the words on the surface
are identical or very similar. Consider the sentence
pair in (2), where Frau in (2a) syntactically corre-
sponds to Mann in (2b).
329
(2) a. Eine
a
Frau
woman
sieht
sees
einen
a
Mann
man
?A woman sees a man.?
b. Ein
a
Mann
man
sieht
sees
eine
a
Frau
woman
?A man sees a woman.?
On the level of the semantic representation, this
is reflected in the correspondence between the vari-
ables x1 and y1, both of which occur as arguments
of subj, as shown in Figure 2.
Ex2x(Dx &&&&&&)&habensujD&&& be
,1oDxF&&& be
Ex2x(Dx 
Au((D&&& bB&&&&&&)&h
abB
,1oDxF&&& bB
&&&&&&C&zaiBnsujD&&& iB
Ej1oDxF&&& iB
&&&&&&C&zaieAu((D&&& ie
Ej1oDxF&&& ie
Figure 2: An excerpt of an alignment between the PARTS
lists of (2a) on the left and (2b) on the right. Dotted align-
ment links are the ones only plausible on the surface.
Our solution to capture this distinction is to use
the concept of a unifier, well-known from logic pro-
gramming. A unifier for terms ?, ? is a substitu-
tion ? such that ?? = ??. Every alignment in-
duces a unifier, which unifies all variables which are
matched by the alignment.
The alignment in Figure 2 (without the dotted
links) induces the unifier
?1 = [(x1, y1) 7? z1; (x2, y2) 7? z2].
If links between the matching predicates mann and
frau, respectively, are added, one also has to unify x1
with y2 and x2 with y1 and thus obtains the unifier
?2 = [(x1, x2, y1, y2) 7? z].
Intuitively, a good unifier unifies only variables
which correspond to the same places in the seman-
tic structures to be aligned. In the case of Figure 2,
choosing an alignment including the dotted links re-
sults in the unifier ?2 which unifies x1 and x2 ? yet
they are structurally different, with one belonging to
the subject and the other one to the object.
In general, it can be expected that an alignment
which preserves the structure will not unify two dis-
tinct variables from the same LRS representation,
since they are known to be structurally distinct. So
we want to capture the information loss resulting
from unification. This intuition is captured by (5),
which answers the following question: Given some
variable z in a unified expression, how many addi-
tional bits do we need on average2 to encode the
original pair of variables x, y in the PARTS lists p
and q, respectively?
H(?) =
1
Zp,q
?
z?Ran(?)
W?(z) log(W?(z)) (5)
where W?(z) = |{x ? V ar(p)|x? = z}|
? |{y ? V ar(q)|y? = z}|
(6)
Zp,q = |V ar(p)| ? |V ar(q)| (7)
The value of a unifier ? is then defined as follows:
variableScore(?) =
(
1?
H(?)
H?
)k
(8)
where k is a numerical parameter with 0 ? k ? 1
and H? is a (tight) upper bound on H(?) obtained
by evaluating the worst unifier, i.e., the unifier that
unifies all variables H? = log(Zp,q).
4.3 Evaluating consistency with dominance
constraints
While evaluating unifiers ensures that alignments
preserve the structure on the level of variables, it is
also important to evaluate their consistency with the
dominance structure of the underspecified semantic
representations, such as the one we saw in Figure 1.
Consider the following pair:
(3) a. Peter
Peter
kommt
comes
und
and
Hans
Hans
kommt
comes
nicht.
not
?Peter comes and Hans does not come.?
b. Peter
Peter
kommt
comes
nicht
not
und
and
Hans
Hans
kommt.
comes
?Peter does not come and Hans comes.?
While the words and also the PARTS lists of the
sentences are identical, they clearly differ in mean-
ing. Figure 3 on the next page shows the LRS domi-
nance graphs for the two sentences together with an
2For simplicity, it is assumed that every combination in
V ar(p)? V ar(q) occurs the same number of times.
330
alignment between them. The semantic difference
between the two sentences is reflected in the posi-
tion of the negation in the dominance graph: while
it dominates kommen(e2) ? subj(e2,hans) in (3a), it
dominates kommen(f1) ? subj(f1,peter) in (3b).
To account for this issue, we evaluate the consis-
tency of the alignment with respect to dominance
constraints. An alignment a is optimally consistent
with respect to dominance structure if it defines an
isomorphism between its range and its domain with
respect to the relation / ?is dominated by?.
Figure 3 shows an alignment which aligns all
matching elements in (3b) and (3a). The link be-
tween the negations violates the isomorphism re-
quirement: the negation dominates kommen(e2) ?
subj(e2,hans) in (3a), while it does not dominate the
corresponding elements in (3b). An optimally con-
sistent alignment will thus leave the negations un-
aligned. Unaligned negations can later be used in
the overall meaning comparison as strong evidence
that the sentences do not mean the same.
dominanceScore measures how ?close? a is to
defining an isomorphism. We use the following sim-
ple score, which is equal to 1 if and only if a defines
an isomorphism:
dominanceScore(a|S, T ) =
1
1 +
?
i,j?Dom(a) ?
?
?
?
?
pi / pj ,
pi . pj ,
qa(i) / qa(j),
qa(i) . qa(j)
?
?
?
?
(9)
where ? is a function taking four truth values as its
arguments. It measures the extent to which the iso-
morphism requirement is violated by an alignment.
?(t1, t2, t1, t2) is defined as 0 because there is no
violation if the dominance relation between pi and
pj is equal to that between the elements they are
aligned with, qa(i) and qa(j). For other combinations
of truth values, ? should be set to values greater than
zero, empirically determined on a development set.
4.4 Finding the best alignment
Because of the use of non-local criteria in the max-
imization criterion Q(a, ?|S, T ) defined in equation
(1), an efficient method is needed to find the align-
ment maximizing the criterion. We exploit the struc-
ture inherent in the set of possible alignments to ap-
ply the A* algorithm (Russel and Norvig, 2010). We
first generalize the notion of an alignment.
Definition 2. A partial alignment of order i is an
index i together with an alignment which does not
have alignment links for any pj with j > i.
A partial alignment can be interpreted as a class
of alignments which agree on the first i elements.
Definition 3. The refinements ?(a) of the partial
alignment a (of order i) are the partial alignments b
such that (1) b is of order i+1, and (2) a and b agree
on {1, ..., i}.
Intuitively, refinements of an alignment of order i
are obtained by deciding how to align element i+1.
? induces a tree over the set of partial alignments,
whose leaves are exactly the complete alignments.
A simple optimistic estimate for the value of all
complete descendants of an alignment a of order i is
given by the following expression:
optimistic(a, ?|S, T ) = variableScore(?)
?dominanceScore(a, S, T )
?(linksScorei(a, ?|p, q)+
n?
k=i+1
heuristic(k, a, pn1 , q
m
1 ))
(10)
where linksScorei is the sum in (4) restricted
to 1 ? k ? i, and heuristic(k, a, pn1 , q
m
1 ) is
0 if pk is aligned and a simple, optimistic esti-
mate for the quality of the best possible align-
ment link containing pk if pk is unaligned. It
is estimated as the maximum of ?NULL and
max{sim(pk, qj) | qj unaligned}.
The estimate in (10) is optimistic in the sense
that it provides an upper bound on the values of all
complete alignments below a. It defines a mono-
tone heuristic and thus allows complete and optimal
search using the A* algorithm. To obtain an efficient
implementation, additional issues such as the order
of elements in the PARTS lists were taken care of. As
they do not play a role for the conceptualization of
our approach, they are not discussed here.
The crucial part at this point of the discussion
is that the A* search can determine the best align-
ment between two PARTS lists. As mentioned in
the overview in section 3.3, we compute three such
331
Ex2(2D &)h
Ea2(2b &)e
nsuu)jE)e ,1oFE)eAB)C)z 
nsuu)jE)h ,1oFE)hAi mj, 
r2(2d
&
Ec2(2 &h
E 2(2! &e
nsuu)jEe ,1oFEeAB)C)z 
nsuu)jEh ,1oFEhAimj, 
"2(2#
$
Figure 3: Alignment between the dominance graphs of (3a) and (3b). The red dotted link violates isomorphism.
alignments: between the student and the target an-
swer, between the question and the student answer,
and between the question and the target answer.
5 From Alignment to Meaning Comparison
Based on the three alignments computed using the
just discussed algorithm, we now explore different
options for computing whether the student answer
is correct or not. We discuss several alternatives,
all involving the computation of a numerical score
based on the alignments. For each of these scores, a
threshold is empirically determined, over which the
student answer is considered to be correct.
Basic Scores The simplest score, ALIGN, is com-
puted by dividing the alignment quality Q between
the student answer and the target answer as defined
in equation (1) by the number of elements in the
smaller PARTS list. Two other scores are computed
based on the number of alignment links between
student and target answer, which for the EQUAL-
Student score is divided by the number of elements
of the PARTS list of the student answer, and for the
EQUAL-Target score by those of the target answer.
For dealing with functional elements, i.e., predi-
cates like subj, obj, quantifiers and the lambda op-
erator, we tried out three options. The straight case
is the one mentioned above, treating all elements on
the PARTS list equally (EQUAL). As a second op-
tion, to see how important the semantic relations be-
tween words are, and how much is just the effect of
the elements themselves, we defined a score which
ignores functional elements (IGNORE). A third pos-
sibility is to weight elements so that functional and
non-functional ones differ in impact (WEIGHTED).
Each of the three scores (EQUAL, IGNORE,
WEIGHTED) is either divided by the number of el-
ements of the PARTS list of the student answer or
the target, resulting in six scores. In addition, three
more scores result from computing the average of
the student and target answer scores.
Information Structure Scores Basing meaning
comparison on actual semantic representation also
allows us to directly take into account Information
Structure as a structuring of the meaning of a sen-
tence in relation to the discourse. Bailey and Meur-
ers (2008), Meurers et al (2011), and Mohler et al
(2011) showed that excluding those parts of the an-
swer which are mentioned (given) in the question
greatly improves classification accuracy. Meurers
et al (2011) argue that the relevant linguistic as-
pect is not whether the material was mentioned in
the question, but the distinction between focus and
background in Information Structure (Krifka, 2008).
The focus essentially is the information in the an-
swer which selects between the set of alternatives
that the question raises.
This issue becomes relevant, e.g., in the case of
?or? questions, where the focused information de-
termining whether the answer is correct is explicitly
given in the question. This is illustrated by the ques-
tion in (4) with target answer (5a) and student an-
swer (5b), from the CREG corpus. While all words
in the answers are mentioned in the question, the
part of the answers which actually answer the ques-
tion are the focused elements shown in boldface.
(4) Ist
is
die
the
Wohnung
flat
in
in
einem
a
Altbau
old building
oder
or
Neubau?
new building
(5) a. Die
the
Wohnung
flat
ist
is
in
in
einem
a
Altbau.
old.building
b. Die
the
Wohnung
flat
ist
is
in
in
einem
a
Neubau.
new.building
332
To realize a focus-based approach, one naturally
needs a component which automatically identifies
the focus of an answer in a question-answer pair. As
a first approximation, this currently is implemented
by a module which marks the elements of the PARTS
lists of the answers for information structure. El-
ements which are not aligned to the question are
marked as focused. Furthermore, in answers to ?or?
questions, it marks as focused all elements which
are aligned to the semantic contribution of a word
belonging to one of the alternatives. ?Or? questions
are recognized by the presence of oder (?or?) and the
absence of a wh-word.
While previous systems simply ignored all words
given in the question during classification, our sys-
tem aligns all elements and recognizes givenness
based on the alignments. Therefore, givenness is
still recognized if the surface realization is differ-
ent. Furthermore, material which incidentally is also
found in the question, but which is structurally dif-
ferent, is not assumed to be given.
Scores using information structure were obtained
in the way of the BASIC scores but counting only
those elements which are recognized as focused
(FOCUS). For comparison, we also used the same
scores with givenness detection instead of focus de-
tection, i.e., in these scores, all elements aligned to
the question were excluded (GIVEN).
Annotating semantic rather than surface represen-
tations for information structure has the advantage
that the approach can be extended to cover focus-
ing of relations in addition to focusing of entities.
The general comparison approach also is compat-
ible with more sophisticated focus detection tech-
niques capable of integrating a range of cues, in-
cluding syntactic cues and specialized constructions
such as clefts, or prosodic information for spoken
language answers ? an avenue we intend to pursue
in future research.
Dissimilar score We also explored one special-
ized score paying particular attention to dissimi-
lar aligned elements, as mentioned in section 4.1.
Where a focused number is aligned to a different
number, or a focused polarity expression is aligned
to the opposite polarity, or a logical negation is not
aligned, then 0 is returned as score, i.e., the student
answer is false. In all other cases, the DISSIMILAR
score is identical to the WEIGHTED-Average FOCUS
score, i.e., the score based on the average of the stu-
dent and target scores with weighting and focus de-
tection.
6 Experiments
6.1 Corpus
We base the experiments on the 1032 answers from
the CREG corpus which are used in the evaluation
of the CoMiC-DE system reported by Meurers et al
(2011). The corpus is balanced, i.e., the numbers of
correct and of incorrect answers are the same. It con-
tains only answers where the two human annotators
agreed on the binary label.
6.2 Setup
The alignment algorithm contains a set of numeri-
cal parameters which need to be determined empir-
ically, such as ?NULL and the function ?. In a first
step, we optimized these parameters and the weights
used in the WEIGHTED scores using grid search on
a development set of 379 answers. These answers
are from CREG, but do not belong to the 1032 an-
swers used for testing. We used the accuracy of the
DISSIMILAR score as performance metric.
In our experiment, we explored each score sep-
arately to predict which answers are correct and
which not. For each score, classification is based
on a threshold which is estimated as the arithmetic
mean of the average score of correct and the average
score of incorrect answers. Training and testing was
performed using the leave-one-out scheme (Weiss
and Kulikowski, 1991). When testing on a particular
answer, student answers answering the same ques-
tion were excluded from training.
6.3 Results
Figure 4 shows the accuracy results obtained in our
experiments together with the result of CoMiC-DE
on the same dataset. With an accuracy of up to
86.3%, the WEIGHTED-Average FOCUS score out-
perform the 84.6% reported for CoMiC-DE (Meur-
ers et al, 2011) on the same dataset. This is remark-
able given that CoMiC-DE uses several (but com-
parably shallow) levels of linguistic abstraction for
finding alignment links, whereas our approach is ex-
clusively based on the semantic representations.
333
Score BASIC GIVEN FOCUS
ALIGN 77.1
EQUAL
Student 69.8 75.3 75.2
Target 70.0 75.5 75.2
Average 76.6 80.8 80.7
IGNORE
Student 75.8 80.1 80.3
Target 77.2 82.2 82.3
Average 79.8 84.7 84.9
WEIGHTED
Student 75.0 80.6 80.7
Target 76.1 83.3 83.3
Average 80.9 86.1 86.3
DISSIMILAR 85.9
CoMiC-DE 84.6
Figure 4: Classification accuracy of CoSeC-DE
The fact that WEIGHTED-Average outperforms
the IGNORE-Average scores shows that the inclu-
sion of functional element (i.e., predicates like subj,
obj), which are not available to approaches based
on aligning surface strings, improves the accuracy.3
On the other hand, the lower performance of EQUAL
shows that functional elements should be treated dif-
ferently from content-bearing elements.
Of the 13.7% answers misclassified by
WEIGHTED-Average FOCUS, 53.5% are false
negatives and 46.5% are false positives.
We also investigated the impact of grammaticality
on the result by manually annotating a sample of 220
student answers for grammatical well-formedness,
66% of which were ungrammatical. On this sam-
ple, grammatical and ungrammatical student an-
swers were evaluated with essentially the same ac-
curacy (83% for ungrammatical answers, 81% for
grammatical answers).
The decrease in accuracy of the COMBINED score
over the best score can be traced to some yes-no-
questions which have an unaligned negation but are
correct. On the other hand, testing only on answers
with focused numbers results in an accuracy of 97%.
The performance of GIVEN and FOCUS scores
3We also evaluated IGNORE scores using parameter values
optimized for these scores, but their performance was still be-
low those of the corresponding WEIGHTED-Average scores.
compared to BASIC confirms that information struc-
turing helps in targeting the relevant parts of the an-
swers. Since CoMiC-DE also demotes given mate-
rial, the better GIVEN results of our approach must
result from other aspects than the information struc-
ture awareness. Unlike previous approaches, the FO-
CUS scores support reference to the material focused
in the answers. However, since currently the FOCUS
scores only differs from the GIVEN scores for alter-
native questions, and the test corpus only contains
seven answers to such ?or? questions, we see no se-
rious quantitative difference in accuracy between the
FOCUS and GIVENNESS results.
While the somewhat lower accuracy of the score
ALIGN shows that the alignment scores are not suf-
ficient for classification, the best-performing scores
do not require much additional computation and do
not need any information that is not in the align-
ments or the automatic focus annotation.
7 Future Work
The alert reader will have noticed that our ap-
proach currently does not support many-to-many
alignments. As is known, e.g., from phrase-based
machine translation, this is an interesting avenue for
dealing with non-compositional expressions, which
we intend to explore in future work. The align-
ment approach can be adapted to such alignments
by adding a factor measuring the quality of many-to-
many links to linkScore (4) and optimistic (10).
8 Conclusion
We presented the CoSeC-DE system for evaluating
the content of answers to reading comprehension
questions. Unlike previous content assessment sys-
tems, it is based on formal semantics, using a novel
approach for aligning underspecified semantic rep-
resentations. The approach readily supports the in-
tegration of important information structural differ-
ences in a way that is closely related to the informa-
tion structure research in formal semantics and prag-
matics. Our experiments showed the system to out-
perform our shallower multi-level system CoMiC-
DE on the same CREG-1032 data set, suggesting
that formal semantic representations can indeed be
useful for content assessment in real-world contexts.
334
Acknowledgements
We are grateful to the three anonymous BEA re-
viewers for their very encouraging and helpful com-
ments.
References
Stacey Bailey and Detmar Meurers. 2008. Diagnos-
ing meaning errors in short answers to reading com-
prehension questions. In Joel Tetreault, Jill Burstein,
and Rachele De Felice, editors, Proceedings of the 3rd
Workshop on Innovative Use of NLP for Building Edu-
cational Applications (BEA-3) at ACL?08, pages 107?
115, Columbus, Ohio.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In J. Quionero-Candela, I. Dagan,
B. Magnini, and F. d?Alch Buc, editors, Machine
Learning Challenges, volume 3944 of Lecture Notes
in Computer Science, pages 177?190. Springer.
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, pages 387?394. Asso-
ciation for Computational Linguistics.
Michael Hahn and Detmar Meurers. 2011. On deriv-
ing semantic representations from dependencies: A
practical approach for evaluating meaning in learner
corpora. In Kim Gerdes, Eva Hajicov, and Leo Wan-
ner, editors, Depling 2011 Proceedings, pages 94?103,
Barcelona.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a
Lexical-Semantic Net for German. In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9?15.
Manfred Krifka. 2008. Basic notions of information
structure. Acta Linguistica Hungarica, 55(3):243?
276.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003.
Discourse and information structure. Journal of Logic,
Language and Information (Introduction to the Special
Issue), pages 249?259.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 802?811. Association for Compu-
tational Linguistics.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina
Kopp. 2011. Evaluating answers to reading compre-
hension questions in context: Results for German and
the role of information structure. In Proceedings of the
TextInfer 2011 Workshop on Textual Entailment, pages
1?9, Edinburgh, Scotland, UK, July. Association for
Computational Linguistics.
Tom Mitchell, Terry Russell, Peter Broomhead, and
Nicola Aldridge. 2002. Towards robust computerised
marking of free-text responses. In Proceedings of
the 6th International Computer Assisted Assessment
(CAA) Conference.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Comnputational Linguistics,
pages 752?762.
Richard Montague. 1973. The Proper Treatment of Qun-
tification in Ordinary English. In Jaakko Hintikka,
Julius Moravcsik, and Patrick Suppes, editors, Ap-
proaches to Natural Language, pages 221?242. Rei-
del, Dordrecht.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. Natural Language Engineering, 15(4):479?
501.
Joakim Nivre and Johan Hall. 2005. Maltparser: A
language-independent system for data-driven depen-
dency parsing. In Proceedings of the Fourth Workshop
on Treebanks and Linguistic Theories, pages 13?95.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Cre-
ation and analysis of a reading comprehension exercise
corpus: Towards evaluating meaning in context. In
Thomas Schmidt and Kai Wrner, editors, Multilingual
Corpora and Multilingual Corpus Analysis, Hamburg
Studies in Multilingualism (HSM). Benjamins, Ams-
terdam.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
2nd Workshop on Building Educational Applications
Using NLP, pages 9?16.
Frank Richter and Manfred Sailer. 2003. Basic Concepts
of Lexical Resource Semantics. In Arnold Beckmann
and Norbert Preining, editors, ESSLLI 2003 ? Course
Material I, volume 5 of Collegium Logicum, pages 87?
143, Wien. Kurt Go?del Society.
Frank Richter. 2000. A Mathematical Formalism
for Linguistic Theories with an Application in Head-
Driven Phrase Structure Grammar. Phil. dissertation,
Eberhard-Karls-Universita?t Tu?ingen.
Vasile Rus, Arthur Graesser, and Kirtan Desai. 2007.
Lexico-syntactic subsumption for textual entailment.
335
Recent Advances in Natural Language Processing IV:
Selected Papers frp, RANLP 2005, pages 187?196.
Stuart Russel and Peter Norvig. 2010. Artificial Intelli-
gence. A Modern Approach. Pearson, 2nd edition.
Mark Sammons, V.G.Vinod Vydiswaran, Tim Vieira,
Nikhil Johri, Ming-Wei Chang, Dan Goldwasser,
Vivek Srikumar, Gourab Kundu, Yuancheng Tu, Kevin
Small, Joshua Rule, Quang Do, and Dan Roth. 2009.
Relation Alignment for Textual Entailment Recogni-
tion. In Text Analysis Conference (TAC).
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Klaus von Heusinger. 1999. Intonation and Information
Structure. The Representation of Focus in Phonology
and Semantics. Habilitationssschrift, Universita?t Kon-
stanz, Konstanz, Germany.
Sholom M. Weiss and Casimir A. Kulikowski. 1991.
Computer systems that learn. Morgan Kaufmann, San
Mateo, CA.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012.
Short answer assessment: Establishing links between
research strands. In Proceedings of the 7th Workshop
on Innovative Use of NLP for Building Educational
Applications (BEA-7) at NAACL-HLT 2012, Montreal.
336
