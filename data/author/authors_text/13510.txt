Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 33?40,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
BagPack: A general framework to represent semantic relations
Ama? Herdag?delen
CIMEC, University of Trento
Rovereto, Italy
amac@herdagdelen.com
Marco Baroni
CIMEC, University of Trento
Rovereto, Italy
marco.baroni@unitn.it
Abstract
We introduce a way to represent word pairs
instantiating arbitrary semantic relations that
keeps track of the contexts in which the words
in the pair occur both together and indepen-
dently. The resulting features are of sufficient
generality to allow us, with the help of a stan-
dard supervised machine learning algorithm,
to tackle a variety of unrelated semantic tasks
with good results and almost no task-specific
tailoring.
1 Introduction
Co-occurrence statistics extracted from corpora lead
to good performance on a wide range of tasks that
involve the identification of the semantic relation be-
tween two words or concepts (Sahlgren, 2006; Turney,
2006). However, the difficulty of such tasks and the
fact that they are apparently unrelated has led to the
development of largely ad-hoc solutions, tuned to spe-
cific challenges. For many practical applications, this is
a drawback: Given the large number of semantic rela-
tions that might be relevant to one or the other task, we
need a multi-purpose approach that, given an appropri-
ate representation and training examples instantiating
an arbitrary target relation, can automatically mine new
pairs characterized by the same relation. Building on a
recent proposal in this direction by Turney (2008), we
propose a generic method of this sort, and we test it
on a set of unrelated tasks, reporting good performance
across the board with very little task-specific tweaking.
There has been much previous work on corpus-based
models to extract broad classes of related words. The
literature on word space models (Sahlgren, 2006) has
focused on taxonomic similarity (synonyms, antonyms,
co-hyponyms. . . ) and general association (e.g., find-
ing topically related words), exploiting the idea that
taxonomically or associated words will tend to occur
in similar contexts, and thus share a vector of co-
occurring words. The literature on relational similar-
ity, on the other hand, has focused on pairs of words,
devising various methods to compare how similar the
contexts in which target pairs appear are to the contexts
of other pairs that instantiate a relation of interest (Tur-
ney, 2006; Pantel and Pennacchiotti, 2006). Beyond
these domains, purely corpus-based methods play an
increasingly important role in modeling constraints on
composition of words, in particular verbal selectional
preferences ? finding out that, say, children are more
likely to eat than apples, whereas the latter are more
likely to be eaten (Erk, 2007; Pad? et al, 2007). Tasks
of this sort differ from relation extraction in that we
need to capture productive patterns: we want to find
out that shabu shabu (a Japanese meat dish) is eaten
whereas ink is not, even if in our corpus neither noun is
attested in proximity to forms of the verb to eat.
Turney (2008) is the first, to the best of our knowl-
edge, to raise the issue of a unified approach. In par-
ticular, he treats synonymy and association as special
cases of relational similarity: in the same way in which
we might be able to tell that hands and arms are in
a part-of relation by comparing the contexts in which
they co-occur to the contexts of known part-of pairs,
we can guess that cars and automobiles are synonyms
by comparing the contexts in which they co-occur to
the contexts linking known synonym pairs.
Here, we build on Turney?s work, adding two main
methodological innovations that allow us further gen-
eralization. First, merging classic approaches to taxo-
nomic and relational similarity, we represent concept
pairs by a vector that concatenates information about
the contexts in which the two words occur indepen-
dently, and the contexts in which they co-occur (Mirkin
et al 2006 also integrate information from the lexi-
cal patterns in which two words co-occur and simi-
larity of the contexts in which each word occurs on
its own, to improve performance in lexical entailment
acquisition). Second, we represent contexts as bag of
words and bigrams, rather than strings of words (?pat-
terns?) of arbitrary length: we leave it to the machine
learning algorithm to zero in on the most interesting
words/bigrams.
Thanks to the concatenated vector, we can tackle
tasks in which the two words are not expected to
co-occur even in very large corpora (such as selec-
tional preference). Concatenation, together with un-
igram/bigram representation of context, allows us to
scale down the approach to smaller training corpora
(Turney used a corpus of more than 50 billion words),
since we do not need to see the words directly co-
occurring, and the unigram/bigram dimensions of the
33
vectors are less sparse than dimensions based on longer
strings of words. We show that our method produces
reasonable results also on a corpus of 2 billion words,
with many unseen pairs. Moreover, our bigram and
unigram representation is general enough that we do
not need to extract separate statistics nor perform ad-
hoc feature selection for each task: we build the co-
occurrence matrix once, and use the same matrix in all
experiments. The bag-of-words assumption also makes
for faster and more compact model building, since the
number of features we extract from a context is linear
in the number of words in the context, whereas it is ex-
ponential for Turney. On the other hand, our method
is currently lagging behind Turney?s in terms of perfor-
mance, suggesting that at least some task-specific tun-
ing will be necessary.
Following Turney, we focus on devising a suitably
general featural representation, and we see the spe-
cific machine learning algorithm employed to perform
the various tasks as a parameter. Here, we use Sup-
port Vector Machines since they are a particularly ef-
fective general-purpose method. In terms of empirical
evaluation of the model, besides experimenting with
the ?classic? SAT and TOEFL datasets, we show how
our algorithm can tackle the selectional preference task
proposed in Pad? (2007) ? a regression task ? and we
introduce to the corpus-based semantics community a
challenge from the ConceptNet repository of common-
sense knowledge (extending such repository by auto-
mated means is the original motivation of our project).
In the next section, we will present our proposed
method along with the corpora and model parameter
choices used in the implementation. In Section 3, we
describe the tasks that we use to evaluate the model.
Results are reported in Section 4 and we conclude in
Section 5, with a brief overview of the contributions of
this paper.
2 Methodology
2.1 Model
The central idea in BagPack (Bag-of-words represen-
tation of Paired concept knowledge) is to construct a
vector-based representation of a pair of words in such a
way that the vector represents both the contexts where
the two words co-occur and the contexts where the sin-
gle words occur on their own. A straightforward ap-
proach is to construct three different sub-vectors, one
for the first word, one for the second word, and one for
the co-occurring pair. The concatenation of these three
sub-vectors is the final vector that represents the pair.
This approach provides us a graceful fall back mech-
anism in case of data scarcity. Even if the two words are
not observed co-occurring in the corpus ? no syntag-
maic information about the pair ?, the corresponding
vector will still represent the individual contexts where
the words are observed on their own. Our hypothesis
(and hope) is that this information will be representa-
tive of the semantic relation between the pair, in the
sense that, given pairs characterized by same relation,
there should be paradigmatic similarity across the first,
resp. second elements of the pairs (e.g., if the relation
is between professionals and the typical tool of their
trade, it is reasonable to expect that that both profes-
sionals and tools will tend to share similar contexts).
Before going into further details, we need to describe
what a ?co-occurrence? precisely means, define the no-
tion of context, and determine how to structure our vec-
tor. For a single word W , the following pseudo regular
expression identifies an observation of occurrence:
?C W D? (1)
where C and D can be empty strings or concatena-
tions of up to 4 words separated by whitespace (i.e.
C1, . . . , Ci and D1, . . . , Dj where i, j ? 4). Each ob-
servation of this pattern constitutes a single context of
W . The pattern is matched with the longest possible
substring without crossing sentence boundaries.
Let (W1,W2) denote an ordered pair of words W1
and W2. We say the two words occur as a pair when-
ever one of the following pseudo regular expressions is
observed in the corpus:
?C W1 DW2 E? (2)
?C W2 D W1 E? (3)
where C and E can be empty strings or concatena-
tions of up to 2 words and similarly, D can be ei-
ther an empty string or concatenation of up to 5 words
(i.e. C1, . . . , Ci, D1, . . . , Dj , and E1, . . . , Ek where
i, j ? 2 and k ? 5). Together, patterns 2 and 3 con-
stitute the pair context for W1 and W2. The pattern is
matched with the longest possible substring while mak-
ing sure that D does not contain neither W1 nor W2.
The number of context words allowed before, after,
and between the targets are actually model parameters
but for the experiments reported in this study, we used
the aforementioned values with no attempt at tuning.
The vector representing (W1,W2) is a concatenation
v1v2v1,2, where, the sub-vectors v1 and v2 are con-
structed by using the single contexts of W1 and W2
correspondingly (i.e. by pattern 1) and the sub-vector
v1,2 is built by using the pair contexts identified by
the patterns 2 and 3. We refer to the components as
single-occurrence vectors and pair-occurrence vector
respectively.
The population of BagPack starts by identifying the
b most frequent unigrams and the b most frequent bi-
grams as basis terms. Let T denote a basis term. For
the construction of v1, we create two features for each
term T : tpre corresponds to the number of observations
of T in the single contexts of W1 occurring before W1
and tpost corresponds to the number of observations of
T in the single occurrence of W1 where T occurs after
W1 (i.e. number of observations of the pattern 1 where
T ? C and T ? D correspondingly). The construc-
tion of v2 is identical except that this time the features
34
correspond to the number of times the basis term is ob-
served before and after the target word W2 in single
contexts. The construction of the pair-occurrence sub-
vector v1,2 proceeds in a similar fashion but in addi-
tion, we incorporate also the order of W1 and W2 as
they co-occur in the pair context: The number of ob-
servations of the pair contexts where W1 occurs before
W2 and T precedes (follows) the pair, are represented
by feature t+pre (t+post). The number of cases where
the basis term is in between the target words is repre-
sented by t+betw. The number of cases where W2 oc-
curs before W1 and T precedes the pair is represented
by the feature t?pre. Similarly the number of cases
where T follows (is in between) the pair is represented
by the feature t?post (t?betw).
Assume that the words "only" and "that" are our ba-
sis terms and consider the following context for the
word pair ("cat", "lion"): "Lion is the only cat that
lives in large social groups." The observation of the ba-
sis terms should contribute to the pair-occurrence sub-
vector v1,2 and since the target words occur in reverse
order, this context results in the incrementation of the
features only?betw and that?post by one.
To sum up, we have 2b basis terms (b unigrams and
b bigrams). Each of the single-occurrence sub-vectors
v1 and v2 consists of 4b features: Each basis term
gives rise to 2 features incorporating the relative posi-
tion of basis term with respect to the single word. The
pair-occurrence sub-vector, v1,2, consists of 12b fea-
tures: Each basis term gives rise to 6 new features; ?3
for possible relative positions of the basis term with re-
spect to the pair and ?2 for the order of the words.
Importantly, the 2b basis terms are picked only once,
and the overall co-occurrence matrix is built once and
for all for all the tasks: unlike Turney, we do not need
to go back to the corpus to pick basis terms and collect
separate statistics for different tasks.
The specifics of the adaptation to each task will be
detailed in Section 3. For the moment, it should suffice
to note that the vectors v1 and v2 represent the con-
texts in which the two words occur on their own, thus
encode paradigmatic information. However, v1,2 rep-
resents the contexts in which the two words co-occur,
thus encode sytagmatic information.
The model training and evaluation is done in a 10-
fold cross-validation setting whenever applicable. The
reported performance measures are the averages over
all folds and the confidence intervals are calculated by
using the distribution of fold-specific results. The only
exception to this setting is the SAT analogy questions
task simply because we consider each question as a
separate mini dataset as described in Section 3.
2.2 Source Corpora
We carried out our tests on two different corpora:
ukWaC, a Web-derived, POS-tagged and lemmatized
collection of about 2 billion tokens,1 and the Yahoo!
1http://wacky.sslmit.unibo.it
database queried via the BOSS service.2 We will refer
to these corpora as ukWaC and Yahoo from now on.
In ukWaC, we limited the number of occurrence and
co-occurrence queries to the first 5000 observations
for computational efficiency. Since we collect cor-
pus statistics at the lemma level, we construct Yahoo!
queries using disjunctions of inflected forms that were
automatically generated with the NodeBox Linguistics
library.3 For example, the query to look for ?lion? and
?cat? with 4 words in the middle is: ?(lion OR lions) *
* * * (cat OR cats OR catting OR catted)?. Each pair
requires 14 Yahoo! queries (one for W1, one for W2,
6 for (W1,W2), in that order, with 0-to-5 intervening
words, 6 analogous queries for (W2,W1)). Yahoo! re-
turns maximally 1,000 snippets per query, and the latter
are lemmatized with the TreeTagger4 before feature ex-
traction.
2.3 Model implementation
We did not carry out a search for ?good? parameter val-
ues. Instead, the model parameters are generally picked
at convenience to ease memory requirements and com-
putational efficiency. For instance, in all experiments,
b is set to 1500 unless noted otherwise in order to fit
the vectors of all pairs at our hand into the computer
memory.
Once we construct the vectors for a set of word pairs,
we get a co-occurrence matrix with pairs on the rows
and the features on the columns. In all of our exper-
iments, the same normalization method and classifi-
cation algorithm is used with the default parameters:
First, a TF-IDF feature weighting is applied to the co-
occurrence matrix (Salton and Buckley, 1988). Then
following the suggestion of Hsu and Chang (2003),
each feature t?s [??t?2??t, ??t+2??t] interval is scaled to
[0, 1], trimming the exceeding values from upper and
lower bounds (the symbols ??t and ??t denote the av-
erage and standard deviation of the feature values re-
spectively). For the classification algorithm, we use the
C-SVM classifier and for regression the -SVM regres-
sor, both implemented in the Matlab toolbox of Canu
et al (2005). We employed a linear kernel. The cost
parameter C is set to 1 for all experiments; for the re-
gressor,  = 0.2. For other pattern recognition related
coding (e.g., cross validation, scaling, etc.) we made
use of the Matlab PRTools (Duin, 2001).
For each task that will be defined in the next section,
we evaluated our algorithm on the following represen-
tations: 1) Single-occurrence vectors (v1v2 condition)
2) Pair-occurrence vectors (v1,2 condition) 3) Entire
co-occurrence matrix (v1v2v1,2 condition).
2http://developer.yahoo.com/search/
boss/
3http://nodebox.net/code/index.php/
Linguistics
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
35
3 Tasks
3.1 SAT Analogy Questions
The first task we evaluated our algorithm on is the
SAT analogy questions task introduced by Turney et al
(2003). In this task, there are 374 multiple choice ques-
tions with a pair of related words like (lion,cat) as the
stem and 5 other pairs as the choices. The correct an-
swer is the choice pair which has the relationship most
similar to that in the stem pair.
We adopt a similar approach to the one used in Tur-
ney (2008) and consider each question as a separate bi-
nary classification problem with one positive training
instance and 5 unknown pairs. For a question, we pick
a pair at random from the stems of other questions as a
pseudo negative instance and train our classifier on this
two-instance training set. Then the trained classifier is
evaluated on the choice pairs and the pair with the high-
est posterior probability for the positive class is called
the winner. The procedure is repeated 10 times pick-
ing a different pseudo-negative instance each time and
the choice pair which is selected as the winner most of-
ten is taken as the answer to that question. The perfor-
mance measure on this task is defined as the percent-
age of correctly answered questions. The mean score
and confidence intervals are calculated over the perfor-
mance scores obtained for all folds.
3.2 TOEFL Synonym Questions
This task, introduced by Landauer and Dumais (1997),
consists of 80 multiple choice questions in which a
word is given as the stem and the correct choice is the
word which has the closest meaning to that of the stem,
among 4 candidates. To fit the task into our frame-
work, we pair each choice with the stem word and ob-
tain 4 word pairs for each question. The word pair
constructed with the stem and the correct choice is la-
beled as positive and the other pairs are labeled as neg-
ative. We consider all 320 pairs constructed for all 80
questions as our dataset. Thus, the problem is turned
into a binary classification problem where the task is
to discriminate the synonymous word pairs (i.e. pos-
itive class) from the other pairs (i.e. negative class).
We made sure that the pairs constructed for the same
question were never split between training and test set,
so that no question-specific learning is performed. The
reason for this precaution is that the evaluation is done
on a per-question basis. The estimated posterior class
probabilities of the pairs constructed for the same ques-
tion are compared to each other and the pair with the
highest probability for the positive class is selected as
the answer for the question. By keeping the pairs of
a question in the same set we make sure their posteri-
ors are calculated by the same trained classifier. The
performance measure is the percentage of correctly an-
swered questions and we report the mean performance
over all 10 folds.
3.3 Selectional Preference Judgments
Linguists have long been interested in the semantic
constraints that verbs impose on their arguments, a
broad area that has also attracted computational mod-
eling, with increasing interest in purely corpus-based
methods (Erk, 2007; Pad? et al, 2007). This task is
of particular interest to us as an example of a broader
class of linguistic problems that involve productive
constraints on composition. As has been stressed at
least since Chomsky?s early work (Chomsky, 1957), no
matter how large a corpus is, if a phenomenon is pro-
ductive there will always be new well-formed instances
that are not in the corpus. In the domain of selectional
restrictions this is particularly obvious: we would not
say that an algorithm learned the constraints on the pos-
sible objects/patients of eating simply by producing the
list of all the attested objects of this verb in a very large
corpus; the interesting issue is whether the algorithm
can detect if an unseen object is or is not a plausible
?eatee?, like humans do without problems. Specifi-
cally, we test selectional preferences on the dataset con-
structed by Pad? (2007), that collects average plausi-
bility judgments (from 20 speakers) for nouns as either
subjects or objects of verbs (211 noun-verb pairs).
We formulate this task as a regression problem. We
train the -SVM regressor with 18-fold cross valida-
tion: Since the pair instances are not independent but
grouped according to the verbs, one fold is constructed
for each of the 18 verbs used in the dataset. In each
fold, all instances sharing the corresponding verb are
left out as the test set. The performance measure for
this task is the Spearman correlation between the hu-
man judgments and our algorithm?s estimates. There
are two possible ways to calculate this measure. One is
to get the overall correlation between the human judg-
ments and our estimates obtained by concatenating the
output of each cross-validation fold. That measure al-
lows us to compare our method with the previously re-
ported results. However, it cannot control for a possi-
ble verb-effect on the human judgment values: If the
average judgment values of the pairs associated with a
specific verb is significantly higher (or lower) than the
average of the pairs associated with another verb, then
any regressor which simply learns to assign the aver-
age value to all pairs associated with that verb (regard-
less of whether there is a patient or agent relation be-
tween the pairs) will still get a reasonably high correla-
tion because of the variation of judgment scores across
the verbs. To control for this effect, we also calculated
the correlation between the human judgments and our
estimates for each verb?s plausibility values separately,
and we report averages across these separate correla-
tions (the ?mean? results reported below).
3.4 Common-sense Relations from ConceptNet
Open Mind Common Sense5 is an ongoing project of
acquisition of common-sense knowledge from ordinary
5http://commons.media.mit.edu/en/
36
Relation Pairs Relation Pairs
IsA 316 PartOf 139
UsedFor 198 LocationOf 1379
CapableOf 228 Total 1943
Table 1: ConceptNet relations after filtering.
people by letting them carry out simple semantic and
linguistics tasks. An end result of the project is Con-
ceptNet 3, a large scale semantic network consisting of
relations between concept pairs (Havasi et al, 2007). It
is possible to view this network as a collection of se-
mantic assertions, each of which can be represented by
a triple involving two concepts and a relation between
them, e.g. UsedFor(piccolo, make music). One moti-
vation for this project is the fact that common-sense
knowledge is assumed to be known by both parties in
a communication setting and usually is not expressed
explicitly. Thus, corpus-based approaches may have
serious difficulties in capturing these relations (Havasi
et al, 2007), but there are reasons to believe that they
could still be useful: Eslick (2006) uses the assertions
of ConceptNet as seeds to parse Web search results and
augment ConceptNet by new candidate relations.
We use the ConceptNet snapshot released in June
2008, containing more than 200.000 assertions with
around 20 semantic relations like UsedFor, Desirious-
EffectOf, or SubEventOf. Each assertion has a confi-
dence rating based on the number of people who ex-
pressed or confirmed that assertion. For simplicity we
limited ourselves to single word concepts and the re-
lations between them. Furthermore, we eliminated the
assertions with a confidence score lower than 3 in an
attempt to increase the "quality" of the assertions and
focused on the most populated 5 relations of the re-
maining set, as given in Table 3.4. There may be more
than one relation between a pair of concepts, so the to-
tal number is less than the sum of the size of the indi-
vidual relation sets.
4 Results
For the multiple choice question tasks (i.e. SAT and
TOEFL), we say a question is complete when all of the
related pairs (stem and choice) are represented by vec-
tors with at least one non-zero component. If a ques-
tion has at least one pair represented by a zero-vector
(missing pairs), then we say that the question is partial.
For these tasks, we report the worst-case performance
scores where we assume that a random guessing per-
formance is obtained on the partial questions. This is
a strict lower bound because it discards all information
we have about a partial question even if it has only one
missing pair. We define coverage as the percentage of
complete questions.
4.1 SAT
In Yahoo, the coverage is quite high. In the v1,2 only
condition, 4 questions had at least some choice/stem
pairs with all zero components. In all other cases, all of
the pairs were represented by vectors with at least one
non-zero component. The highest score is obtained for
the v1v2v1,2 condition with a 44.1% of correct ques-
tions, that is not significantly above the 42.5% perfor-
mance of v1,2 (paired t-test, ? = 0.05). The v1v2 only
condition results in a poorer performance of 33.9% cor-
rect questions, statistically lower than the former two
conditions.
For ukWaC, the v1,2 only condition provides a rel-
atively low coverage. Only 238 questions out of 374
were complete. For the other conditions, we get a com-
plete coverage. The performances are statistically in-
distinguishable from each other and are 38.0%, 38.2%,
and 39.6% for v1,2, v1v2, and v1v2v1,2 respectively.
Condition Yahoo ukWaC
v1,2 42.5% 38.0%
v1v2 33.9% 38.2%
v1v2v1,2 44.1% 39.6%
Table 2: Percentage of correctly answered questions in
SAT analogy task, worst-case scenario.
In Fig. 1, the best performances we get for Yahoo
and ukWaC are compared to previous studies with 95%
binomial confidence intervals plotted. The reported
values are taken from the ACL wiki page on the state of
the art for SAT analogy questions6. The algorithm pro-
posed by Turney (2008) is labeled as Turney-PairClass.
35
40
45
50
55
60
65
Perc
enta
ge of
 corr
ect a
nswe
rs
BagPack
?ukWaCMangalat
h et al
Veale?K
NOW?BE
ST
Bicici an
d Yuret
BagPack
?Yahoo
Turney a
nd Littma
n
Turney?P
airClassTurney?P
ERTTurney?L
RA
Figure 1: Comparison with previous algorithms on
SAT analogy questions.
Overall, the performance of BagPack is not at the
level of the state of the art but still provides a reasonable
level even in the v1v2 only condition for which we do
not utilize the contexts where the two words co-occur.
This aspect is most striking for ukWaC where the cov-
erage is low and by only utilizing the single-occurrence
sub-vectors we obtain a performance of 38.2% cor-
rect answers (the comparable ?attributional? models re-
6See http://aclweb.org/aclwiki/ for further
information and references
37
ported in Turney, 2006, have an average performance of
31%).
4.2 TOEFL
For the v1,2 sub-vector calculated for Yahoo, we have
two partial questions out of 80 and the system answers
80.0% of the questions correctly. The single occur-
rence case v1v2 instead provides a correct percentage
of 41.2% which is significantly above the random per-
formance of 25% but still very poor. The combined
case v1v2v1,2 provides a score of 75.0% with no sta-
tistically significant difference from the v1,2 case. The
reason of the low performance for v1v2 is an open
question.
For ukWaC, the coverage for the v1v2 case is pretty
low. Out of 320 pairs, 70 were represented by zero-
vectors, resulting in 34 partial questions out of 80.
The performance is at 33.8%. The v1v2 case on its
own does not lead to a performance better than random
guessing (27.5%) but the combined case v1v2v1,2
provides the highest ukWaC score of 42.5%.
Condition Yahoo ukWaC
v1,2 80.0% 33.8%
v1v2 41.2% 27.5%
v1v2v1,2 75.0% 42.5%
Table 3: Percentage of correctly answered questions in
TOEFL synonym task, worst-case scenario.
To our knowledge, the best performance with a
purely corpus-based approach is that of Rapp (2003)
who obtained a score of 92.5% with SVD. Fig. 2 re-
ports our results and a list of other corpus-based sys-
tems which achieve scores higher than 70%, along with
95% confidence interval values. The results are taken
from the ACL wiki page on the state of the art for
TOEFL synonym questions.
30
40
50
60
70
80
90
100
Perc
enta
ge of
 corr
ect a
nswe
rs
BagPack
?ukWaC
Pado and
 LapataTurney?P
MI?IR
Turney?P
airClass
BagPack
?Yahoo
Terra an
d Clarke
Bullinaria
 and Lev
y
Matveev
a et al Rapp
Figure 2: Comparison with previous algorithms on
TOEFL synonym questions with 95% confidence in-
tervals.
We note that our results obtained for Yahoo are com-
parable to the results of Turney but even the best re-
sults obtained for ukWaC and the Yahoo?s results for
v1v2 only condition are very poor. Whether this is
because of the inability of the sub-vectors to capture
synonymity or because the default parameter values of
SVM are not adequate is an open question. Notice that
our concatenated v1v2 vector does not exploit infor-
mation about the similarity of v1 to v2, that, presum-
ably, should be of great help in solving the synonym
task.
4.3 Selectional Preference
The coverage for this dataset is quite high. All pairs
were represented by non-zero vectors for Yahoo while
only two pairs had zero-vectors for ukWaC. The two
pairs are discarded in our experiments. For Yahoo, the
best results are obtained for the v1,2 case. The single-
occurrence case, v1v2, provides an overall correlation
of 0.36 and mean correlation of 0.26. However low, in
case of rarely co-occurring word pairs this data could
be the only data we have in our hands and it is impor-
tant that it provides reasonable judgment estimates.
For the ukWaC corpus, the best results we get are
an overall correlation of 0.60 and a mean correlation of
0.52 for the combined case v1v2v1,2. The results for
v1,2 and v1v2v1,2 are statistically indistinguishable.
Yahoo ukWaC
Condition Overall Mean Overall Mean
v1,2 0.60 0.45 0.58 0.48
v1v2 0.36 0.26 0.33 0.22
v1v2v1,2 0.55 0.42 0.60 0.52
Table 4: Spearman correlations between the targets and
estimations for selectional preference task.
In Fig. 3, we present a comparison of our results with
some previous studies reported in Pad? et al (2007).
The best result reported so far is a correlation of 0.52.
Our results for Yahoo and ukWaC are currently the
highest correlation values reported. Even the verb-
effect-controlled correlations achieve competitive per-
formance.
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
Spea
rman
 corr
elatio
n
Resnik
BagPack
?Yahoo (m
ean)
Pado, Pa
do, & Erk
 (parsed co
sine)
BagPack
?ukWaC 
(mean)
Pado, Ke
ller, & Cr
ocker
BagPack
?Yahoo (ov
erall)
BagPack
?ukWaC 
(overall)
Figure 3: Comparison of algorithms on selectional
preference task.
38
4.4 ConceptNet
Only for this task, (because of practical memory limita-
tions) we reduced the model parameter b to 500, which
means we used the 500 most frequent unigrams and
500 most frequent bigrams as our basis terms. For each
of the 5 relations at our hand, we trained a different
SVM classifier by labeling the pairs with the corre-
sponding relation as positive and the rest as negative.
To eliminate the issue of unbalanced number of nega-
tive and positive instances we randomly down-sampled
the positive or negative instances set (whichever is
larger). For the IsA, UsedFor, CapableOf, and PartOf
relations, the down-sampling procedure means keep-
ing some of the negative instances out of the training
and test sets while for the LocationOf relation it means
keeping a subset of the positive instances out. We per-
formed 5 iterations of the down-sampling procedure
and for each iteration we carried out a 10-fold cross-
validation to train and test our classifier. The results are
test set averages over all iterations and folds. The per-
formance measure we use is the area under the receiver
operating characteristic (AUC in short for area under
the curve). The AUC of a classifier is the area under the
curve defined by the corresponding true positive rate
and false positive rate values obtained for varying the
threshold of the classifier to accept an instance as posi-
tive. Intuitively, AUC is the probability that a randomly
picked positive instance?s estimated posterior probabil-
ity is higher than a randomly picked negative instance?s
estimated posterior probability (Fawcett, 2006).
The coverage is quite high for both corpora: Out of
1943 pairs,only 3 were represented by a zero-vector in
Yahoo while in ukWaC this number is 68. For sim-
plicity, we discarded missing pairs from our analysis.
We report only the results obtained for the entire co-
occurrence matrix. The results are virtually identi-
cal for the other conditions too: Both for Yahoo and
ukWaC, almost all of the AUC values obtained for all
relations and for all conditions are above 95%. Only
the PartOf relation has AUC values above 90% (which
is still a very good result).
Relation Yahoo ukWaC
IsA 99.0% 98.0%
UsedFor 98.2% 98.5%
CapableOf 98.9% 99.1%
PartOf 97.6% 95.0%
LocationOf 99.0% 98.8%
Table 5: AUC scores for 5 relations of ConceptNet,
classifier trained for v1v2v1,2 condition.
The very high performance we observe for the Con-
ceptNet task is surprising when compared to the mod-
erate performance we observe for other tasks. Our ex-
tensive filtering of the assertions could have resulted
in a biased dataset which might have made the job of
the classifier easy while reducing its generalization ca-
pacity. To investigate this, we decided to use the pairs
coming from the SAT task as a validation set.
Again, we trained an SVM classifier on the Concept-
Net data for each of the 5 relations like we did previ-
ously, but this time without cross-validation (i.e. after
the down-sampling, we used the entire set as the train-
ing dataset in each iteration). Then we evaluated the
classifiers on the 2224 pairs of the SAT analogy task
(removing pairs that were in the training data) and av-
eraged the posterior probability reported by each SVM
over each down-sampling iteration. The 5 pairs which
are assigned the highest posterior probability for each
relation are reported in Table 6. We have not yet quan-
tified the performance of BagPack in this task but the
preliminary results in this table are, qualitatively, ex-
ceptionally good.
5 Conclusions
We presented a general way to build a vector-based
space to represent the semantic relations between word
pairs and showed how that representation can be used
to solve various tasks involving semantic similarity.
For SAT and TOEFL, we obtained reasonable perfor-
mances comparable to the state of the art. For the es-
timation of selective preference judgments about verb-
noun pairs, we achieved state of the art performance.
Perhaps more importantly, our representation format
allows us to provide meaningful estimates even when
the verb and noun are not observed co-occurring in the
corpus ? which is an obvious advantage over the mod-
els which rely on sytagmatic contexts alone and cannot
provide estimates for word pairs that are not seen di-
rectly co-occurring. We also obtained very promising
results for the automated augmentation of ConceptNet.
The generality of the proposed method is also re-
flected in the fact that we built a single feature space
based on frequent basis terms and used the same fea-
tures for all pairs coming from different tasks. The
use of the same feature set for all pairs makes it pos-
sible to build a single database of word-pair vectors.
For example, we were able to re-use the vectors con-
structed for SAT pairs as a validation set in the Con-
ceptNet task. Furthermore, the results reported here are
obtained for the same machine learning model (SVM)
without any parameter tweaking, which renders them
very strict lower bounds.
Another contribution is that the proposed method
provides a way to represent the relations between
words even if they are not observed co-occurring in the
corpus. Employing a larger corpus can be an alternative
solution for some cases but this is not always possible
and some tasks, like estimating selectional preference
judgments, inherently call for a method that does not
exclusively depends on paired co-occurrence observa-
tions.
Finally, we introduced ConceptNet, a common-sense
semantic network, to the corpus-based semantics com-
munity, both as a new challenge and as a repository we
39
Rank IsA UsedFor PartOf CapableOf LocationOf
1 watch,timepiece pencil,draw vehicle,wheel motorist,drive spectator,arena
2 emerald,gem blueprint,build spider,leg volatile,vaporize water,riverbed
3 cherry,fruit detergent,clean keyboard,finger concrete,harden bovine,pasture
4 dinosaur,reptile guard,protect train,caboose parasite,contribute benediction,church
5 ostrich,bird buttress,support hub,wheel immature,develop byline,newspaper
Table 6: Top 5 SAT pairs classified as positive for ConceptNet relations, classifier trained for v1v2v1,2 condition.
can benefit from.
In future work, one of the most pressing issue we
want to explore is how to better exploit the informa-
tion in the single occurrence vectors: currently, we do
not make any use of the overlap between v1 and v2.
In this way, we are missing the classic intuition that
taxonomically similar words tend to occur in similar
contexts, and it is thus not surprising that v1v2 flunks
the TOEFL. We are currently looking at ways to aug-
ment our concatenated vector with ?meta-information?
about vector overlap.
References
S. Canu, Y. Grandvalet, V. Guigue and A. Rakotoma-
monjy. 2005. SVM and Kernel Methods Matlab
Toolbox, Perception Syst?mes et Information, INSA
de Rouen, Rouen, France
N. Chomsky. 1957. Syntactic structures. Mouton, The
Hague.
R. P. W. Duin. 2001. PRTOOLD (Version 3.1.7),
A Matlab toolbox for pattern recognition. Pattern
Recognition Group. Delft University of Technology.
K. Erk. 2007. A simple, similarity-based model for
selectional preferences. Proceedings of ACL 2007.
K. Erk and S. Pad?. 2008. A structured vector space
model for word meaning in context. Proceedings of
EMNLP 2008.
I. Eslick. 2006. Searching for commonsense. Master?s
thesis, Massachusetts Institute of Technology.
T. Fawcett. 2006. An introduction to roc analysis.
Pattern Recogn. Lett., 27(8):861?874.
C. Havasi, R. Speer and J. Alonso. 2007. Concept-
net 3: a flexible, multilingual semantic network for
common sense knowledge. In Recent Advances in
Natural Language Processing, Borovets, Bulgaria,
September.
C.-W. Hsu, C.-C Chang. 2003. A practical guide
to support vector classification. Technical report,
Department of Computer Science, National Taiwan
University.
T.K. Landauer and S.T. Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2): 211?
240.
H. Liu and P. Singh. 2004. ConceptNet ? A practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4) 211?226.
S. Mirkin, I. Dagan and M. Geffet. 2006. Integrat-
ing pattern-based and distributional similarity meth-
ods for lexical entailment acquisition. Proceedings
of COLING/ACL 2006, 579?586.
S. Pad?, S. Pad? and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
Proceedings EMNLP 2007, 400?409.
U. Pad?. 2007. The Integration of Syntax and Semantic
Plausibility in a Wide-Coverage Model of Sentence
Processing. Ph.D. thesis, Saarland University.
P. Pantel and M. Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. Proceedings of COL-
ING/ACL 2006, 113?120.
R. Rapp. 2003. Word sense discovery based on sense
descriptor dissimilarity. Proceedings of MT Summit
IX: 315?322.
M. Sahlgren. 2006. The Word-space model. Ph.D. dis-
sertation, Stockholm University, Stockholm.
G. Salton and C. Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information
Processing and Management, 24(5): 513?523.
R. Speer, C. Havasi and H. Lieberman. 2008. Anal-
ogyspace: Reducing the dimensionality of common
sense knowledge. In Dieter Fox and Carla P. Gomes,
editors, AAAI, pages 548?553. AAAI Press.
P. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3): 379?416.
P. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. Proceedings
of COLING 2008, 905?912.
40
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 50?53,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Measuring semantic relatedness with vector space models and random
walks
Amac? Herda
?
gdelen
Center for Mind/Brain Sciences
University of Trento
amac@herdagdelen.com
Katrin Erk
Linguistics Department
University of Texas at Austin
katrin.erk@mail.utexas.edu
Marco Baroni
Center for Mind/Brain Sciences
University of Trento
marco.baroni@unitn.it
Abstract
Both vector space models and graph ran-
domwalk models can be used to determine
similarity between concepts. Noting that
vectors can be regarded as local views of
a graph, we directly compare vector space
models and graph random walk models on
standard tasks of predicting human simi-
larity ratings, concept categorization, and
semantic priming, varying the size of the
dataset from which vector space and graph
are extracted.
1 Introduction
Vector space models, representing word mean-
ings as points in high-dimensional space, have
been used in a variety of semantic relatedness
tasks (Sahlgren, 2006; Pad?o and Lapata, 2007).
Graphs are another way of representing relations
between linguistic entities, and they have been
used to capture semantic relatedness by using both
corpus-based evidence and the graph structure of
WordNet and Wikipedia (Pedersen et al, 2004;
Widdows and Dorow, 2002; Minkov and Cohen,
2008). We study the relationship between vec-
tor space models and graph random walk mod-
els by embedding vector space models in graphs.
The flexibility offered by graph randomwalk mod-
els allows us to compare the vector space-based
similarity measures to extended notions of relat-
edness and similarity. In particular, a random
walk model can be viewed as smoothing direct
similarity between two vectors using second-order
and even higher-order vectors. This view leads
to the second focal point of this paper: We in-
vestigate whether random walk models can sim-
ulate the smoothing effects obtained by methods
like Singular Value Decomposition (SVD). To an-
swer this question, we compute models on reduced
(downsampled) versions of our dataset and evalu-
ate the robustness of random walk models, a clas-
sic vector-based model, and SVD-based models
against data sparseness.
2 Model definition and implementation
We use directed graphs with weighted edges, G =
(V,E,w) where V is a set of nodes, E = V ? V
is a set of edges and w : E ? R is the weight-
ing function on edges. For simplicity, we assume
that G is fully connected, edges with zero weights
can be considered as non-existing in the graph. On
these graphs, we perform random walks with an
initial probability distribution q over the nodes (a
1 ? |V | vector). We then follow edges with prob-
ability proportional to their weights, so that the
probability of walking from node v
1
to node v
2
is w(v
1
, v
2
)/
?
v
w(v
1
, v). A fixed length random
walk ends after a predetermined number of steps.
In flexible walks, there is a constant probability ?
of stopping at each step. Thus, walk length fol-
lows a geometric distribution with parameter ?,
the probability of a walk of length k is ?(1??)
k?1
and the expected walk length is 1/?. For example,
a flexible walk with ? = 1/2 will produce 1-step,
2-step, and higher-step walks while the expected
average length is 2.
Relating vectors and graphs. Corpus co-
occurrence (e
1
, e
2
, a
12
) of two entities e
1
and e
2
that co-occur with (potentially transformed) count
a
12
can be represented in either a vector or a graph.
In a vector, it corresponds to a dimension value of
a
12
for the dimension e
2
of entity e
1
. In a graph,
it corresponds to two nodes labeled e
1
and e
2
con-
nected by an edge with weight a
12
.
Similarity measures. Let R(q) = p denote a
specific random walk process which transforms an
50
initial probability distribution q to a final prob-
ability distribution p over the nodes. We write
q(m) for the probability assigned to the node m
under q. If the initial distribution q concentrates
all probability on a single node n, i.e., q(n) = 1
and q(x) = 0 for all nodes x 6= n, we write
pr(n ? m) for the probability p(m) of ending
up at node m.
The simplest way of measuring relatedness
through random walks is to consider the probabil-
ity p(m) of a single node m as an endpoint for a
walk starting with start probability distribution q,
that is, p = R(q). We call this a direct, one-
direction measure of relatedness between q and
m. Direct, one-direction measures are typically
asymmetric. In case all start probability is con-
centrated on a single node n, we can also consider
direct, two-direction measures, which will be a
combination of pr(m ? n) and pr(n ? m). The
point of using two-direction measures is that these
can be made symmetric, which is an advantage
when we are modeling undirected semantic sim-
ilarity. In the experiments below we focus on the
average of the two probabilities.
In addition to direct measures, we will use in-
direct measures, in which we compute the relat-
edness of endpoint probability distributions p
1
=
R(q
1
) and p
2
= R(q
2
). As endpoint distribu-
tions can be viewed both as probability distribu-
tions and as vectors, we used three indirect mea-
sures: 1) Jensen/Shannon divergence, a symmet-
ric variant of the Kullback/Leibler divergence be-
tween probability distributions. 2) cosine similar-
ity, and 3) dot product. Dot product is a natural
choice in a graph setting because we can view it as
the probability of a pair of walks, one starting at a
node determined by q
1
and the other starting at a
node governed by q
2
, ending at the same node.
Discussion. Direct and indirect relatedness mea-
sures together with variation in walk length give us
a simple, powerful and flexible way to capture dif-
ferent kinds of similarity (with traditional vector-
based approach as a special case). Longer walks
or flexible walks will capture higher order effects
that may help coping with data sparseness, similar
to the use of second-order vectors. Dimensionality
reduction techniques like Singular Value Decom-
position (SVD) also capture these higher-order ef-
fects, and it has been argued that that makes them
more resistant against sparseness (Sch?utze, 1997).
To our knowledge, no systematic comparison of
SVD and classical vector-based methods has been
done on different corpus sizes. In our experiments,
we will compare the performance of SVD and
flexible-walk smoothing at different corpus sizes
and for a variety of tasks.
Implementation: We extract tuples from the 2-
billion word ukWaC corpus,
1
dependency-parsed
with MINIPAR.
2
Following Pad?o and Lapata
(2007), we only consider co-occurrences where
two target words are connected by certain de-
pendency paths, namely: the top 30 most fre-
quent preposition-mediated noun-to-noun paths
(soldier+with+gun), the top 50 transitive-verb-
mediated noun-to-noun paths (soldier+use+gun),
the top 30 direct or preposition-mediated verb-
noun paths (kill+obj+victim, kill+in+school), and
the modifying and predicative adjective-to-noun
paths. Pairs (w
1
, w
2
) that account for 0.01%
or less of the marginal frequency of w
1
were
trimmed. The resulting tuple list, with raw counts
converted to mutual information scores, contains
about 25 million tuples.
To test how well graph-based and alternative
methods ?scale down? to smaller corpora, we sam-
pled random subsets of tuples corresponding to
0.1%, 1%, 10%, and 100% of the full list. To put
things into perspective, the full list was extracted
from a corpus of about 2 billion words; so, the
10% list is on the order of magnitude of the BNC,
and the 0.1% list is on the order of magnitude of
the Brown corpus. From each of the 4 resulting
datasets, we built one graph and two vector space
models: one space with full dimensionality, and
one space reduced to 300 dimensions using singu-
lar value decomposition.
3 Experiments
First, we report the results for all tasks obtained on
the full data-set and then proceed with the compar-
ison of different models on differing graph sizes
to see the robustness of the models against data
sparseness.
Human similarity ratings: We use the dataset
of Rubenstein and Goodenough (1965), consist-
ing of averages of subject similarity ratings for
65 noun pairs. We use the Pearson?s coefficient
between estimates and human judgments as our
performance measure. The results obtained for
1
http://wacky.sslmit.unibo.it
2
http://www.cs.ualberta.ca/
?
lindek/
minipar.htm
51
Direct (average) Vector (cosine) Indirect (dot product) Previous
0.5 1 2 svd vector 0.5 1 2
RG 0.409 0.326 0.571 0.798 0.689 0.634 0.673 0.400 BL: 0.70
CLW: 0.849
AAMP Purity 0.480 0.418 0.669 0.701 0.704 0.664 0.667 0.612 AP: 0.709
RS: 0.791
Hodgson
synonym 2, 563 1.289 5, 408
??
10.015
??
6, 623
??
5, 462
??
5, 954
??
5, 537
??
coord 4, 275
??
3, 969
??
6, 319
??
11.157
??
7, 593
??
8, 466
??
8, 477
??
4, 854
??
antonym 2, 853? 2, 237 5, 319
??
7, 724
??
5, 455
??
4, 589
??
4, 859
??
6, 810
??
conass 9, 209
??
10.016
??
5, 889
??
9, 299
??
6, 950
??
5, 993
??
5, 455
??
4, 994
??
supersub 4, 038
??
4, 113
??
6, 773
??
10.422
??
7, 901
??
6, 792
??
7, 165
??
4, 828
??
phrasacc 4, 577
??
4, 718
??
2, 911
?
3, 532
?
3, 023
?
3, 506
?
3, 612
?
1.038
Table 1: All datasets. * (**) indicates significance level p < 0.01 (p < 0.001). BL: (Baroni and Lenci,
2009), CLW: (Chen et al, 2006), AP: (Almuhareb, 2006), RS: (Rothenh?ausler and Sch?utze, 2009)
0.1% 1% 10%
cos svd cos vector dot 2 cos svd cos vector dot 2 cos svd cos vector dot 2
RG 0.219 0.244 0.669 0.676 0.700 1.159 0.911 0.829 1.068
AAMP 0.379 0.339 0.366 0.723 0.622 0.634 0.923 0.886 0.948
Synonym 0.369 0.464 0.610 0.493 0.590 0.833 0.857 0.770 1.081
Antonym 0.449 0.493 0.231 0.768 0.585 0.730 1.044 0.849 0.977
Conass 0.187 0.260 0.261 0.451 0.498 0.942 0.857 0.704 1.062
Coord 0.282 0.362 0.456 0.527 0.570 1.050 0.927 0.810 1.187
Phrasacc 0.268 0.132 0.761 0.849 0.610 1.215 0.920 0.868 1.049
Supersub 0.313 0.353 0.285 0.645 0.601 1.029 0.936 0.752 1.060
Table 2: Each cell contains the ratio of the performance of the corresponding model for the corresponding
downsampling ratio to the performance of the same model on the full graph. The higher ratio means the
less deterioration due to data sparseness.
the full graph are in Table 1, line 1. The SVD
model clearly outperforms the pure-vector based
approach and the graph-based approaches. Its per-
formance is above that of previous models trained
on the same corpus (Baroni and Lenci, 2009). The
best model that we report is based on web search
engine results (Chen et al, 2006). Among the
graph-based random walk models, flexible walk
with parameter 0.5 and fixed 1-step walk with in-
direct relatedness measures using dot product sim-
ilarity achieve the highest performance.
Concept categorization: Almuhareb (2006) pro-
posed a set of 402 nouns to be categorized into
21 classes of both concrete (animals, fruit. . . ) and
abstract (feelings, times. . . ) concepts. Our results
on this clustering task are given in Table 1 (line
2). The difference between SVD and pure-vector
models is negligible and they both obtain the best
performance in terms of both cluster entropy (not
shown in the table) and purity. Both models? per-
formances are comparable with the previously re-
ported studies, and above that of random walks.
Semantic priming: The next dataset comes
from Hodgson (1991) and it is of interest since
it requires capturing different forms of seman-
tic relatedness between prime-target pairs: syn-
onyms (synonym), coordinates (coord), antonyms
(antonym), free association pairs (conass), super-
and subordinate pairs (supersub) and phrasal as-
sociates (phrasacc). Following previous simula-
tions of this data-set (Pad?o and Lapata, 2007), we
measure the similarity of each related target-prime
pair, and we compare it to the average similar-
ity of the target to all the other primes instanti-
ating the same relation, treating the latter quan-
tity as our surrogate of an unrelated target-prime
pair. We report results in terms of differences be-
tween unrelated and related pairs, normalized to
t-scores, marking significance according to two-
tailed paired t-tests for the relevant degrees of free-
dom. Even though the SVD-based and pure-vector
models are among the top achievers in general, we
see that in different tasks different random walk
models achieve comparable or even better perfor-
mances. In particular, for phrasal associates and
conceptual associates, the best results are obtained
by random walks based on direct measures.
3.1 Robustness against data sparseness
So far, we reported only the results obtained on
the full graph. However, in order to see the re-
sponse of the models to using smaller corpora
52
we ran another set of experiments on artificially
down-sampled graphs as explained above. In this
case, we are not interested in the absolute perfor-
mance of the models per se but the relative per-
formance. Thus, for ease of comparison we fixed
each model?s performance on the full graph to 1
for each task and linearly scaled its performance
on smaller graphs. For example saying that the
SVD-based model achieves a score of 0.911 on
10% graph for the Rubenstein and Goodenough
dataset means that the ratio of the performance
of SVD-based model on 10% graph to the per-
formance of the same model on the full graph is
0.911. The results are given in Table 2, where the
only random walk model we report is dot 2, i.e., a
2-step random walk coupled with the dot product-
based indirect measure. This is by far the random
walk model most robust to downsampling. In the
10% graph, we see that on all tasks but one, dot 2
is the model least affected by the data reduction.
On the contrary, down-sampling has a positive ef-
fect on this model because on 6 tasks, it actually
performs better than it does on the full graph! The
same behavior is also observed on the 1% graph
- as an example, for phrasal associates relations,
dot 2 performance increases by a factor of around
1.2 when we use one hundredth of the graph in-
stead of the full one. For the smallest graph we
used, 0.1%, still dot 2 provides the highest relative
performance in 5 out of the 8 tasks.
4 Conclusion
We compared graph-based random walk models
and vector models. For this purpose, we showed
how corpus co-occurrences could be represented
both as a graph and a vector, and we identified
two different ways to calculate relatedness based
on the outcomes of random walks, by direct and
indirect measures. The experiments carried out
on 8 different tasks by using the full graph re-
vealed that SVD-based model performs very well
across all types of semantic relatedness. How-
ever, there is also evidence that -depending on
the particular relation- some random walk models
can achieve results as good as or even better than
those of SVD-based models. Our second ques-
tion was whether the random walk models would
be able to simulate the smoothing effects obtained
by SVD. While answering this question, we also
carried out a systematic comparison of plain and
SVD-based models on different tasks with differ-
ent sizes of data. One interesting result is that an
SVD-based model is not necessarily more robust
to data sparseness than the plain vector model.
The more interesting result is that a 2-step ran-
dom walk model, based on indirect measures with
dot product, consistently outperforms both SVD-
based and plain vector models in terms of relative
performance, thus it is able to achieve compara-
ble results on very small datasets. Actually, the
improvement on absolute performance measures
of this random walk by making the dataset even
smaller calls for future research.
References
A. Almuhareb. 2006. Attributes in lexical acquisition.
Dissertation, University of Essex.
M. Baroni and A. Lenci. 2009. One distributional
memory, many semantic spaces. In Proceedings of
GEMS, Athens, Greece.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.
2006. Novel association measures using web search
with double checking. In Proceedings of ACL, pages
1009?16.
J. Hodgson. 1991. Informational constraints on pre-
lexical priming. Language and Cognitive Processes,
6:169?205.
Einat Minkov and William W. Cohen. 2008. Learn-
ing graph walk based similarity measures for parsed
text. In Proceedings of EMNLP?08.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity: Measuring the relatedness of
concepts. In Proceedings of NAACL, pages 38?41.
Klaus Rothenh?ausler and Hinrich Sch?utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of GEMS, pages 17?
24.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
M. Sahlgren. 2006. The Word-Space Model. Disserta-
tion, Stockholm University.
H. Sch?utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford.
Dominic Widdows and Beate Dorow. 2002. A
graph model for unsupervised lexical acquisition.
In 19th International Conference on Computational
Linguistics, pages 1093?1099.
53
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474?482,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Dense Models of Query Similarity from User Click Logs
Fabio De Bona?
Friedrich Miescher Laboratory
of the Max Planck Society
Tu?bingen, Germany
fabio@tuebingen.mpg.de
Stefan Riezler
Google Research
Zu?rich, Switzerland
riezler@google.com
Keith Hall
Google Research
Zu?rich, Switzerland
kbhall@google.com
Massimiliano Ciaramita
Google Research
Zu?rich, Switzerland
massi@google.com
Amac? Herdag?delen?
University of Trento
Rovereto, Italy
amac@herdagdelen.com
Maria Holmqvist?
Linkopings University
Linkopings, Sweden
marho@ida.liu.se
Abstract
The goal of this work is to integrate query
similarity metrics as features into a dense
model that can be trained on large amounts
of query log data, in order to rank query
rewrites. We propose features that incorpo-
rate various notions of syntactic and semantic
similarity in a generalized edit distance frame-
work. We use the implicit feedback of user
clicks on search results as weak labels in train-
ing linear ranking models on large data sets.
We optimize different ranking objectives in a
stochastic gradient descent framework. Our
experiments show that a pairwise SVM ranker
trained on multipartite rank levels outperforms
other pairwise and listwise ranking methods
under a variety of evaluation metrics.
1 Introduction
Measures of query similarity are used for a wide
range of web search applications, including query
expansion, query suggestions, or listings of related
queries. Several recent approaches deploy user
query logs to learn query similarities. One set of ap-
proaches focuses on user reformulations of queries
that differ only in one phrase, e.g., Jones et al
(2006). Such phrases are then identified as candi-
date expansion terms, and filtered by various signals
such as co-occurrence in similar sessions, or log-
likelihood ratio of original and expansion phrase.
Other approaches focus on the relation of queries
and search results, either by clustering queries based
?The work presented in this paper was done while the au-
thors were visiting Google Research, Zu?rich.
on their search results, e.g., Beeferman and Berger
(2000), or by deploying the graph of queries and re-
sults to find related queries, e.g., Sahami and Heil-
man (2006).
The approach closest to ours is that of Jones et al
(2006). Similar to their approach, we create a train-
ing set of candidate query rewrites from user query
logs, and use it to train learners. While the dataset
used in Jones et al (2006) is in the order of a few
thousand query-rewrite pairs, our dataset comprises
around 1 billion query-rewrite pairs. Clearly, man-
ual labeling of rewrite quality is not feasible for our
dataset, and perhaps not even desirable. Instead, our
intent is to learn from large amounts of user query
log data. Such data permit to learn smooth mod-
els because of the effectiveness of large data sets to
capture even rare aspects of language, and they also
are available as in the wild, i.e., they reflect the ac-
tual input-output behaviour that we seek to automate
(Halevy et al, 2009). We propose a technique to au-
tomatically create weak labels from co-click infor-
mation in user query logs of search engines. The
central idea is that two queries are related if they
lead to user clicks on the same documents for a large
amount of documents. A manual evaluation of a
small subset showed that a determination of positive
versus negative rewrites by thresholding the number
of co-clicks correlates well with human judgements
of similarity, thus justifying our method of eliciting
labels from co-clicks.
Similar to Jones et al (2006), the features of our
models are not based on word identities, but instead
on general string similarity metrics. This leads to
dense rather than sparse feature spaces. The dif-
474
ference of our approach to Jones et al (2006) lies
in our particular choice of string similarity metrics.
While Jones et al (2006) deploy ?syntactic? fea-
tures such as Levenshtein distance, and ?semantic?
features such as log-likelihood ratio or mutual in-
formation, we combine syntactic and semantic as-
pects into generalized edit-distance features where
the cost of each edit operation is weighted by vari-
ous term probability models.
Lastly, the learners used in our approach are appli-
cable to very large datasets by an integration of lin-
ear ranking models into a stochastic gradient descent
framework for optimization. We compare several
linear ranking models, including a log-linear prob-
ability model for bipartite ranking, and pairwise and
listwise SVM rankers. We show in an experimen-
tal evaluation that a pairwise SVM ranker trained on
multipartite rank levels outperforms state-of-the-art
pairwise and listwise ranking methods under a vari-
ety of evaluation metrics.
2 Query Similarity Measures
2.1 Semantic measures
In several of the similarity measures we describe be-
low, we employ pointwise mutual information (PMI)
as a measure of the association between two terms or
queries. Let wi and wj be two strings that we want
to measure the amount of association between. Let
p(wi) and p(wj) be the probability of observing wi
and wj in a given model; e.g., relative frequencies
estimated from occurrence counts in a corpus. We
also define p(wi, wj) as the joint probability of wi
and wj ; i.e., the probability of the two strings occur-
ring together. We define PMI as follows:
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
. (1)
PMI has been introduced by Church and Hanks
(1990) as word assosiatio ratio, and since then
been used extensively to model semantic similar-
ity. Among several desirable properties, it correlates
well with human judgments (Recchia and Jones,
2009).
2.2 Taxonomic normalizations
As pointed out in earlier work, query transitions tend
to correlate with taxonomic relations such as gener-
alization and specialization (Lau and Horvitz, 1999;
Rieh and Xie, 2006). Boldi et al (2009) show how
knowledge of transition types can positively impact
query reformulation. We would like to exploit this
information as well. However, rather than building a
dedicated supervised classifier for this task we try to
capture it directly at the source. First, we notice how
string features; e.g., length, and edit distance already
model this phenomenon to some extent, and in fact
are part of the features used in Boldi et al (2009).
However, these measures are not always accurate
and it is easy to find counterexamples both at the
term level (e.g., ?camping? to ?outdoor activities? is
a generalization) and character level (?animal pic-
tures? to ?cat pictures? is a specialization). Sec-
ondly, we propose that by manipulating PMI we can
directly model taxonomic relations to some extent.
Rather than using raw PMI values we re-
normalize them. Notice that it is not obvious in our
context how to interpret the relation between strings
co-occurring less frequently than random. Such
noisy events will yield negative PMI values since
p(wi, wj) < p(wi)p(wj). We enforce zero PMI val-
ues for such cases. If PMI is thus constrained to
non-negative values, normalization will bound PMI
to the range between 0 and 1.
The first type of normalization, called joint nor-
malization, uses the negative log joint probability
and is defined as
PMI(J)(wi, wj) = PMI(wi, wj)/?log(p(wi, wj)).
The jointly normalized PMI(J) is a symmetric
measure between wi and wj in the sense that
PMI(J)(wi, wj) = PMI(J)(wj , wi). Intuitively it
is a measure of the amount of shared information
between the two strings relative to the sum of indi-
vidual strings information. The advantages of the
joint normalization of PMI have been noticed be-
fore (Bouma, 2009).
To capture asymmetries in the relation between
two strings, we introduce two non-symmetric nor-
malizations which also bound the measure between
0 and 1. The second normalization is called special-
ization normalization and is defined as
PMI(S)(wi, wj) = PMI(wi, wj)/? log(p(wi)).
The reason we call it specialization is that PMI(S)
favors pairs where the second string is a specializa-
475
tion of the first one. For instance, PMI(S) is at its
maximum when p(wi, wj) = p(wj) and that means
the conditional probability p(wi|wj) is 1 which is an
indication of a specialization relation.
The last normalization is called the generalization
normalization and is defined in the reverse direction
as
PMI(G)(wi, wj) = PMI(wi, wj)/? log(p(wj)).
Again, PMI(G) is a measure between 0 and 1 and is
at its maximum value when p(wj |wi) is 1.
The three normalizations provide a richer rep-
resentation of the association between two strings.
Furthermore, jointly, they model in an information-
theoretic sense the generalization-specialization di-
mension directly. As an example, for the query
transition ?apple? to ?mac os? PMI(G)=0.2917 and
PMI(S)=0.3686; i.e., there is more evidence for a
specialization. Conversely for the query transition
?ferrari models? to ?ferrari? we get PMI(G)=1 and
PMI(S)=0.5558; i.e., the target is a ?perfect? gener-
alization of the source1.
2.3 Syntactic measures
Let V be a finite vocabulary and ? be the null
symbol. An edit operation: insertion, deletion or
substitution, is a pair (a, b) ? {V ? {?} ? V ?
{?}} \ {(?, ?)}. An alignment between two se-
quences wi and wj is a sequence of edit oper-
ations ? = (a1, b1), ..., (an, bn). Given a non-
negative cost function c, the cost of an alignment is
c(?) =
?n
i=1 c(?i). The Levenshtein distance, or
edit distance, defined over V , dV (wi, wj) between
two sequences is the cost of the least expensive se-
quence of edit operations which transforms wi into
wj (Levenshtein, 1966). The distance computation
can be performed via dynamic programming in time
O(|wi||wj |). Similarity at the string, i.e., character
or term, level is an indicator of semantic similar-
ity. Edit distance captures the amount of overlap be-
tween the queries as sequences of symbols and has
been previously used in information retrieval (Boldi
et al, 2009; Jones et al, 2006).
We use two basic Levenshtein distance models.
The first, called Edit1 (E1), employs a unit cost func-
tion for each of the three operations. That is, given
1The values are computed from Web counts.
a finite vocabulary T containing all terms occurring
in queries:
?a, b ? T, cE1(a, b) = 1 if(a 6= b), 0 else.
The second, called Edit2 (E2), uses unit costs for
insertion and deletion, but computes the character-
based edit distance between two terms to decide on
the substitution cost. If two terms are very similar
at the character level, then the cost of substitution is
lower. Given a finite vocabulary T of terms and a
finite vocabulary A of characters, the cost function
is defined as:
?a, b ? T, cE2(a, b) = dA(a, b) ifa ? b 6= ?, 1 else.
where dA(a, b) is linearly scaled between 0 and 1
dividing by max(|a|, |b|).
We also investigate a variant of the edit distance
algorithm in which the terms in the input sequences
are sorted, alphabetically, before the distance com-
putation. The motivation behind this variant is the
observation that linear order in queries is not always
meaningful. For example, it seems reasonable to as-
sume that ?brooklyn pizza? and ?pizza brooklyn?
denote roughly the same user intent. However, the
pair has an edit distance of two (delete-insert), while
the distance between ?brooklyn pizza? and the less
relevant ?brooklyn college? is only one (substitute).
The sorted variant relaxes the ordering constraint.
2.4 Generalized measures
In this section we extend the edit distance frame-
work introduced in Section 2.3 with the semantic
similarity measures described in Section 2.1, using
the taxonomic normalizations defined in Section 2.2.
Extending the Levenshtein distance framework
to take into account semantic similarities between
terms is conceptually simple. As in the Edit2 model
above we use a modified cost function. We introduce
a cost matrix encoding individual costs for term sub-
stitution operations; the cost is defined in terms of
the normalized PMI measures of Section 2.2, recall
that these measures range between 0 and 1. Given a
normalized similarity measure f , an entry in a cost
matrix S for a term pair (wi, wj) is defined as:
s(wi, wj) = 2? 2f(wi, wj) + 
476
We call these models SEdit (SE), where S specifies
the cost matrix used. Given a finite term vocabulary
T and cost matrix S, the cost function is defined as:
?a, b ? T, cSE(a, b) = s(a, b) ifa ? b 6= ?, 1 else.
The cost function has the following properties.
Since insertion and deletion have unit cost, a term
is substituted only if a substitution is ?cheaper? than
deleting and inserting another term, namely, if the
similarity between the terms is not zero. The 
correction, coupled with unit insertion and deletion
cost, guarantees that for an unrelated term pair a
combination of insertion and deletion will always be
less costly then a substitution. Thus in the compu-
tation of the optimal alignment, each operation cost
ranges between 0 and 2.
As a remark on efficiency, we notice that here the
semantic similarities are computed between terms,
rather than full queries. At the term level, caching
techniques can be applied more effectively to speed
up feature computation. The cost function is imple-
mented as a pre-calculated matrix, in the next sec-
tion we describe how the matrix is estimated.
2.5 Cost matrix estimation
In our experiments we evaluated two different
sources to obtain the PMI-based cost matrices. In
both cases, we assumed that the cost of the substitu-
tion of a term with itself (i.e. identity substitution)
is always 0. The first technique uses a probabilis-
tic clustering model trained on queries and clicked
documents from user query logs. The second model
estimates cost matrices directly from user session
logs, consisting of approximately 1.3 billion U.S.
English queries. A session is defined as a sequence
of queries from the same user within a controlled
time interval. Let qs and qt be a query pair observed
in the session data where qt is issued immediately
after qs in the same session. Let q?s = qs \ qt and
q?t = qt \ qs, where \ is the set difference opera-
tor. The co-occurrence count of two terms wi and
wj from a query pair qs, qt is denoted by ni,j(qs, qt)
and is defined as:
ni,j(qs, qt) =
?
?
?
1 if wi = wj ? wi ? qs ? wj ? qt
1/(|q?s| |q
?
t|) if wi ? q
?
s ? wj ? q
?
t
0 else.
In other words, if a term occurs in both queries,
it has a co-occurrence count of 1. For all other term
pairs, a normalized co-occurrence count is computed
in order to make sure the sum of co-occurrence
counts for a term wi ? qs sums to 1 for a given
query pair. The normalization is an attempt to avoid
the under representation of terms occurring in both
queries.
The final co-occurrence count of two arbitrary
terms wi and wj is denoted by Ni,j and it is defined
as the sum over all query pairs in the session logs,
Ni,j =
?
qs,qt ni,j(qs, qt). Let N =
?
wi,wj
Ni,j be
the sum of co-occurrence counts over all term pairs.
Then we define a joint probability for a term pair as
p(wi, wj) =
Ni,j
N . Similarly, we define the single-
occurrence counts and probabilities of the terms
by computing the marginalized sums over all term
pairs. Namely, the probability of a termwi occurring
in the source query is p(i, ?) =
?
wj
Ni,j/N and
similarly the probability of a term wj occurring in
the target query is p(?, j) =
?
wi
Ni,j/N . Plugging
in these values in Eq. (1), we get the PMI(wi, wj)
for term pair wi and wj , which are further normal-
ized as described in Section 2.2.
More explanation and evaluation of the features
described in this section can be found in Ciaramita
et al (2010).
3 Learning to Rank from Co-Click Data
3.1 Extracting Weak Labels from Co-Clicks
Several studies have shown that implicit feedback
from clickstream data is a weaker signal than human
relevance judgements. Joachims (2002) or Agrawal
et al (2009) presented techniques to convert clicks
into labels that can be used for machine learning.
Our goal is not to elicit relevance judgments from
user clicks, but rather to relate queries by pivoting on
commonly clicked search results. The hypothesis is
that two queries are related if they lead to user clicks
on the same documents for a large amount of docu-
ments. This approach is similar to the method pro-
posed by Fitzpatrick and Dent (1997) who attempt
to measure the relatedness between two queries by
using the normalized intersection of the top 200 re-
trieval results. We add click information to this
setup, thus strengthening the preference for preci-
sion over recall in the extraction of related queries.
477
Table 1: Statistics of co-click data sets.
train dev test
number of queries 250,000 2,500 100
average number of
rewrites per query 4,500 4,500 30
percentage of rewrites
with ? 10 coclicks 0.2 0.2 43
In our experiments we created two ground-truth
ranking scenarios from the co-click signals. In a first
scenario, called bipartite ranking, we extract a set
of positive and a set of negative query-rewrite pairs
from the user logs data. We define positive pairs as
queries that have been co-clicked with at least 10 dif-
ferent results, and negative pairs as query pairs with
fewer than 10 co-clicks. In a second scenario, called
multipartite ranking, we define a hierarchy of levels
of ?goodness?, by combining rewrites with the same
number of co-clicks at the same level, with increas-
ing ranks for higher number of co-clicks. Statistics
on the co-click data prepared for our experiments are
given in Table 1.
For training and development, we collected
query-rewrite pairs from user query logs that con-
tained at least one positive rewrite. The training set
consists of about 1 billion of query-rewrite pairs; the
development set contains 10 million query-rewrite
pairs. The average number of rewrites per query is
around 4,500 for the training and development set,
with a very small amount of 0.2% positive rewrites
per query. In order to confirm the validity of our co-
click hypothesis, and for final evaluation, we held
out another sample of query-rewrite pairs for man-
ual evaluation. This dataset contains 100 queries for
each of which we sampled 30 rewrites in descending
order of co-clicks, resulting in a high percentage of
43% positive rewrites per query. The query-rewrite
pairs were annotated by 3 raters as follows: First the
raters were asked to rank the rewrites in descend-
ing order of relevance using a graphical user inter-
face. Second the raters assigned rank labels and bi-
nary relevance scores to the ranked list of rewrites.
This labeling strategy is similar to the labeling strat-
egy for synonymy judgements proposed by Ruben-
stein and Goodenough (1965). Inter-rater agree-
ments on binary relevance judgements, and agree-
ment between rounded averaged human relevance
scores and assignments of positive/negative labels
by the co-click threshold of 10 produced a Kappa
value of 0.65 (Siegel and Castellan, 1988).
3.2 Learning-to-Rank Query Rewrites
3.2.1 Notation
Let S = {(xq, yq)}nq=1 be a training sample
of queries, each represented by a set of rewrites
xq = {xq1, . . . , xq,n(q)}, and set of rank labels
yq = {yq1, . . . , yq,n(q)}, where n(q) is the num-
ber of rewrites for query q. For full rankings of
all rewrites for a query, a total order on rewrites is
assumed, with rank labels taking on values yqi ?
{1, . . . , n(q)}. Rewrites of equivalent rank can be
specified by assuming a partial order on rewrites,
where a multipartite ranking involves r < n(q) rele-
vance levels such that yqi ? {1, . . . , r} , and a bipar-
tite ranking involves two rank values yqi ? {1, 2}
with relevant rewrites at rank 1 and non-relevant
rewrites at rank 2.
Let the rewrites in xq be identified by the integers
{1, 2, . . . , n(q)}, and let a permutation piq on xq be
defined as a bijection from {1, 2, . . . , n(q)} onto it-
self. Let ?q denote the set of all possible permuta-
tions on xq, and let piqi denote the rank position of
xqi. Furthermore, let (i, j) denote a pair of rewrites
in xq and let Pq be the set of all pairs in xq.
We associate a feature function ?(xqi) with each
rewrite i = 1, . . . , n(q) for each query q. Further-
more, a partial-order feature map as used in Yue et
al. (2007) is created for each rewrite set as follows:
?(xq, piq) =
1
|Pq|
?
(i,j)?Pq
?(xqi)??(xqj)sgn(
1
piqi
?
1
piqj
).
The goal of learning a ranking over the rewrites
xq for a query q can be achieved either by sorting the
rewrites according to the rewrite-level ranking func-
tion f(xqi) = ?w, ?(xqi)?, or by finding the permu-
tation that scores highest according to a query-level
ranking function f(xq, piq) = ?w, ?(xq, piq)?.
In the following, we will describe a variety
of well-known ranking objectives, and extensions
thereof, that are used in our experiments. Optimiza-
tion is done in a stochastic gradient descent (SGD)
framework. We minimize an empirical loss objec-
tive
min
w
?
xq ,yq
`(w)
478
by stochastic updating
wt+1 = wt ? ?tgt
where ?t is a learning rate, and gt is the gradient
gt = ?`(w)
where
?`(w) =
?
?
?w1
`(w),
?
?w2
`(w), . . . ,
?
?wn
`(w)
?
.
3.2.2 Listwise Hinge Loss
Standard ranking evaluation metrics such as
(Mean) Average Precision (Manning et al, 2008)
are defined on permutations of whole lists and are
not decomposable over instances. Joachims (2005),
Yue et al (2007), or Chakrabarti et al (2008) have
proposed multivariate SVM models to optimize such
listwise evaluation metrics. The central idea is to
formalize the evaluation metric as a prediction loss
function L, and incorporate L via margin rescal-
ing into the hinge loss function, such that an up-
per bound on the prediction loss is achieved (see
Tsochantaridis et al (2004), Proposition 2).
The loss function is given by the following list-
wise hinge loss:
`lh(w) = (L(yq, pi
?
q )?
?
w, ?(xq, yq)? ?(xq, pi
?
q )
?
)+
where pi?q is the maximizer of the
maxpiq??q\yq L(yq, pi
?
q ) +
?
w, ?(xq, pi?q )
?
ex-
pression, (z)+ = max{0, z} and L(yq, piq) ? [0, 1]
denotes a prediction loss of a predicted ranking piq
compared to the ground-truth ranking yq.2
In this paper, we use Average Precision (AP) as
prediction loss function s.t.
LAP (yq, piq) = 1?AP (yq, piq)
where AP is defined as follows:
AP (yq, piq) =
?n(q)
j=1 Prec(j) ? (|yqj ? 2|)
?n(q)
j=1 (|yqj ? 2|)
,
P rec(j) =
?
k:piqk?piqj
(|yqk ? 2|)
piqj
.
2We slightly abuse the notation yq to denote the permutation
on xq that is induced by the rank labels. In case of full rankings,
the permutation piq corresponding to ranking yq is unique. For
multipartite and bipartite rankings, there is more than one pos-
sible permutation for a given ranking, so that we let piq denote
a permutation that is consistent with ranking yq .
Note that the ranking scenario is in this case bipartite
with yqi ? {1, 2}.
The derivatives for `lh are as follows:
?
?wk
`lh =
?
?
?
0 if
(?
w, ?(xq, yq)? ?(xq, pi?q )
?)
> L(yq, pi?q ),
?(?k(xq, yq)? ?k(xq, pi?q )) else.
SGD optimization involves computing pi?q for each
feature and each query, which can be done effi-
ciently using the greedy algorithm proposed by Yue
et al (2007). We will refer to this method as the
SVM-MAP model.
3.2.3 Pairwise Hinge Loss for Bipartite and
Multipartite Ranking
Joachims (2002) proposed an SVM method that
defines the ranking problem as a pairwise classifi-
cation problem. Cortes et al (2007) extended this
method to a magnitude-preserving version by penal-
izing a pairwise misranking by the magnitude of the
difference in preference labels. A position-sensitive
penalty for pairwise ranking SVMs was proposed
by Riezler and De Bona (2009) and Chapelle and
Keerthi (2010), and earlier for perceptrons by Shen
and Joshi (2005). In the latter approaches, the mag-
nitude of the difference in inverted ranks is accrued
for each misranked pair. The idea is to impose an
increased penalty for misrankings at the top of the
list, and for misrankings that involve a difference of
several rank levels.
Similar to the listwise case, we can view the
penalty as a prediction loss function, and incor-
porate it into the hinge loss function by rescaling
the margin by a pairwise prediction loss function
L(yqi, yqj). In our experiments we used a position-
sensitive prediction loss function
L(yqi, yqj) = |
1
yqi
?
1
yqj
|
defined on the difference of inverted ranks. The
margin-rescaled pairwise hinge loss is then defined
as follows:
`ph(w) =
?
(i,j)?Pq
(L(yqi, yqj)?
?w, ?(xqi)? ?(xqj)? sgn(
1
yqi
?
1
yqj
))+
479
Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,
SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.
MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5
Random 51.8 48.7 50.4 45.6 45.6 46.6
Best-feature 71.9 70.2 74.5 70.2 68.1 68.7
SVM-bipart. 73.7 73.7 74.7 79.4 70.1 70.1
SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0
Log-linear 74.7 75.1 75.7 75.3 72.2 71.3
SVM-pos.-sens. 75.7 76.0 76.6 82.5 72.9 73.0
SVM-multipart. 76.5 77.3 77.2 83.5 74.2 73.6
The derivative of `ph is calculated as follows:
?
?wk
`lp =
?
????
????
0 if (?w, ?(xqi)? ?(xqj)?
sgn( 1yqi ?
1
yqj
)) > L(yqi, yqj),
?(?k(xqi)? ?k(xqj))sgn( 1yqi ?
1
yqj
)
else.
Note that the effect of inducing a position-
sensitive penalty on pairwise misrankings applies
only in case of full rankings on n(q) rank levels,
or in case of multipartite rankings involving 2 <
r < n(q) rank levels. Henceforth we will refer to
margin-rescaled pairwise hinge loss for multipartite
rankings as the SVM-pos.-sens. method.
Bipartite ranking is a special case where
L(yqi, yqj) is constant so that margin rescaling does
not have the effect of inducing position-sensitivity.
This method will be referred to as the SVM-bipartite
model.
Also note that for full ranking or multipartite
ranking, predicting a low ranking for an instance
that is ranked high in the ground truth has a domino
effect of accruing an additional penalty at each
rank level. This effect is independent of margin-
rescaling. The method of pairwise hinge loss
for multipartite ranking with constant margin will
henceforth be referred to as the SVM-multipartite
model.
Computation in SGD optimization is dominated
by the number of pairwise comparisons |Pq| for
each query. For full ranking, a comparison of
|Pq| =
(n(q)
2
)
pairs has to be done. In the case
of multipartite ranking at r rank levels, each in-
cluding |li| rewrites, pairwise comparisons between
rewrites at the same rank level can be ignored.
This reduces the number of comparisons to |Pq| =
?r?1
i=1
?r
j=i+1 |li||lj |. For bipartite ranking of p
positive and n negative instances, |Pq| = p ? n com-
parisons are necessary.
3.2.4 Log-linear Models for Bipartite Ranking
A probabilistic model for bipartite ranking can be
defined as the conditional probability of the set of
relevant rewrites, i.e., rewrites at rank level 1, given
all rewrites at rank levels 1 and 2. A formalization in
the family of log-linear models yields the following
logistic loss function `llm that was used for discrim-
inative estimation from sets of partially labeled data
in Riezler et al (2002):
`llm(w) = ? log
?
xqi?xq |yqi=1
e?w,?(xqi)?
?
xqi?xq
e?w,?(xqi)?
.
The gradient of `llm is calculated as a difference be-
tween two expectations:
?
?wk
`llm = ?pw [?k|xq; yqi = 1] + pw [?k|xq] .
The SGD computation for the log-linear model is
dominated by the computation of expectations for
each query. The logistic loss for bipartite ranking is
henceforth referred to as the log-linear model.
4 Experimental Results
In the experiments reported in this paper, we trained
linear ranking models on 1 billion query-rewrite
pairs using 60 dense features, combined of the build-
ing blocks of syntactic and semantic similarity met-
rics under different estimations of cost matrices. De-
velopment testing was done on a data set that was
held-out from the training set. Final testing was car-
ried out on the manually labeled dataset. Data statis-
tics for all sets are given in Table 1.
480
Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.
Best-feature SVM-bipart. SVM-MAP Log-linear SVM-pos.-sens. SVM-multipart.
Best-feature - < 0.005 < 0.005 < 0.005 < 0.005 < 0.005
SVM-bipart. - - 0.324 < 0.005 < 0.005 < 0.005
SVM-MAP - - - 0.374 < 0.005 < 0.005
Log-linear - - - - 0.053 < 0.005
SVM-pos.-sens. - - - - - < 0.005
SVM-multipart. - - - - - -
Model selection was performed by adjusting
meta-parameters on the development set. We
trained each model at constant learning rates ? ?
{1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variant
after every fifth out of 100 passes over the training
set. The variant with the highest MAP score on the
development set was chosen and evaluated on the
test set. This early stopping routine also served for
regularization.
Evaluation results for the systems are reported in
Table 2. We evaluate all models according to the fol-
lowing evaluation metrics: Mean Average Precision
(MAP), Normalized Discounted Cumulative Gain
with a cutoff at rank 10 (NDCG@10), Area-under-
the-ROC-curve (AUC), Precision@n3. As baselines
we report a random permutation of rewrites (ran-
dom), and the single dense feature that performed
best on the development set (best-feature). The latter
is the log-probability assigned to the query-rewrite
pair by the probabilistic clustering model used for
cost matrix estimation (see Section 2.5). P-values
are reported in Table 3 for all pairwise compar-
isons of systems (except the random baseline) us-
ing an Approximate Randomization test where strat-
ified shuffling is applied to results on the query level
(see Noreen (1989)). The rows in Tables 2 and 3
are ranked according to MAP values of the systems.
SVM-multipartite outperforms all other ranking sys-
tems under all evaluation metrics at a significance
level ? 0.995. For all other pairwise comparisons
of result differences, we find result differences of
systems ranked next to each other to be not statis-
tically significant. All systems outperform the ran-
dom and best-feature baselines with statistically sig-
nificant result differences. The distinctive advantage
of the SVM-multipartite models lies in the possibil-
3For a definition of these metrics see Manning et al (2008)
ity to rank rewrites with very high co-click num-
bers even higher than rewrites with reasonable num-
bers of co-clicks. This preference for ranking the
top co-clicked rewrites high seems the best avenue
for transferring co-click information to the human
judgements encoded in the manually labeled test set.
Position-sensitive margin rescaling does not seem to
help, but rather seems to hurt.
5 Discussion
We presented an approach to learn rankings of query
rewrites from large amounts of user query log data.
We showed how to use the implicit co-click feed-
back about rewrite quality in user log data to train
ranking models that perform well on ranking query
rewrites according to human quality standards. We
presented large-scale experiments using SGD opti-
mization for linear ranking models. Our experimen-
tal results show that an SVM model for multipartite
ranking outperforms other linear ranking models un-
der several evaluation metrics. In future work, we
would like to extend our approach to other models,
e.g., sparse combinations of lexicalized features.
References
R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,
and P. Tsaparas. 2009. Generating labels from clicks.
In Proceedings of the 2nd ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain.
Doug Beeferman and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ?Dango? to ?Japanese cakes?: Query reformula-
481
tion models and patterns. In Proceedings of Web Intel-
ligence. IEEE Cs Press.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Proceedings of
GSCL.
Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and
Chiru Bhattacharayya. 2008. Structured learning for
non-smooth ranking losses. In Proceedings of the 14th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), Las Vegas, NV.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Massimiliano Ciaramita, Amac? Herdag?delen, Daniel
Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler,
and Enrique Alfonseca. 2010. Generalized syntactic
and semantic models of query reformulation. In Pro-
ceedings of the 33rd ACM SIGIR Conference, Geneva,
Switzerland.
Corinna Cortes, Mehryar Mohri, and Asish Rastogi.
2007. Magnitude-preserving ranking algorithms. In
Proceedings of the 24th International Conference on
Machine Learning (ICML?07), Corvallis, OR.
Larry Fitzpatrick and Mei Dent. 1997. Automatic feed-
back using past queries: Social searching? In Pro-
ceedings of the 20th Annual International ACM SIGIR
Conference, Philadelphia, PA.
Alon Halevy, Peter Norvig, and Fernando Pereira. 2009.
The unreasonable effectiveness of data. IEEE Intelli-
gent Systems, 24:8?12.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), New York, NY.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine Learn-
ing (ICML?05), Bonn, Germany.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th International World Wide Web
conference (WWW?06), Edinburgh, Scotland.
T. Lau and E. Horvitz. 1999. Patterns of search: analyz-
ing and modeling web query refinement. In Proceed-
ings of the seventh international conference on User
modeling, pages 119?128. Springer-Verlag New York,
Inc.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
G. Recchia and M.N. Jones. 2009. More data trumps
smarter algorithms: comparing pointwise mutual in-
formation with latent semantic analysis. Behavioral
Research Methods, 41(3):647?656.
S.Y. Rieh and H. Xie. 2006. Analysis of multiple query
reformulations on the web: the interactive information
retrieval context. Inf. Process. Manage., 42(3):751?
768.
Stefan Riezler and Fabio De Bona. 2009. Simple risk
bounds for position-sensitive max-margin ranking al-
gorithms. In Proceedings of the Workshop on Ad-
vances in Ranking at the 23rd Annual Conference
on Neural Information Processing Systems (NIPS?09),
Whistler, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 10(3):627?633.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th Inter-
national World Wide Web conference (WWW?06), Ed-
inburgh, Scotland.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73?96.
Sidney Siegel and John Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. Second Edition.
MacGraw-Hill, Boston, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning (ICML?04), Banff,
Canada.
Yisong Yue, Thomas Finley, Filip Radlinski, and
Thorsten Joachims. 2007. A support vector method
for optimizing average precision. In Proceedings of
the 30th Annual International ACM SIGIR Confer-
ence, Amsterdam, The Netherlands.
482
