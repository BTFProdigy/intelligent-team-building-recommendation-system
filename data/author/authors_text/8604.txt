Bootstrapping Parallel Treebanks
Martin VOLK and Yvonne SAMUELSSON
Stockholm University
Department of Linguistics
10691 Stockholm
Sweden
volk@ling.su.se
Abstract
This paper argues for the development of par-
allel treebanks. It summarizes the work done
in this area and reports on experiments for
building a Swedish-German treebank. And it
describes our approach for reusing resources
from one language while annotating another
language.
1 Introduction
Treebanks have become valuable resources in
natural language processing (NLP) in recent
years (Abeille?, 2003). A treebank is a collection
of syntactically annotated sentences in which
the annotation has been manually checked so
that the treebank can serve as training cor-
pus for natural language parsers, as repository
for linguistic research, or as evaluation corpus
for NLP systems. The current interest in tree-
banks is documented in international workshop
series like ?Linguistically Interpreted Corpora
(LINC)? or ?Treebanks and Linguistic Theo-
ries? (TLT). But also the recent international
CL conferences have seen a wide variety of pa-
pers that involved treebanks. Treebanks have
become a necessary resource for many research
activities in NLP.
On the other hand recent years have seen
an increasing interest in parallel corpora (of-
ten called bitexts). See for example (Melamed,
2001) or (Borin, 2002) for a broad picture of
this area.
But surprisingly little work has been reported
on combining these two areas: parallel tree-
banks. We define a parallel treebank as a bi-
text where the sentences of each language are
annotated with a syntactic tree, and the sen-
tences are aligned below the clause level. This
leaves room for various kinds of tree structure
(e.g. dependency structure trees or constituent
structure trees) and does not specify a precise
requirement for tree alignments but rather for
some sort of sub-clausal alignment (e.g. word
alignment or phrase alignment).
But why has there been so little work done
on parallel treebanks? The benefits of hav-
ing such a treebank for training statistical ma-
chine translation systems, experimenting with
example-based translation systems, or evalu-
ating word alignment programs seem so over-
whelming. We speculate that this scarcity is
mainly due to the expenses necessary for build-
ing a parallel treebank (in terms of time and
human resources). It is well known that the
manual labor involved in building a monolin-
gual treebank is high (For the Penn Treebank
(Taylor et al, 2003) report on 750 - 1000
words per hour for an experienced annotator,
which translates to 35 - 50 sentences per hour).
And the cross-language alignment requires ad-
ditional work. Therefore every approach to fa-
cilitate and speed up this process will be highly
welcome.
The goal of this paper is to summarize the
(little) work that has been done on parallel
treebanks and related areas such as annota-
tion projection. In particular we will report on
our experiments for building a Swedish-German
parallel treebank. As a side issue we investi-
gated whether the German treebank annota-
tion guidelines (from the NEGRA / TIGER
projects) can be applied to Swedish. We have
chosen Swedish and German because they are
our mother tongues, but also because they are
similar and still interestingly different.
2 Previous Work on Parallel
Treebanks
The field of parallel treebanks is only now evolv-
ing into a research field. (Cmejrek et al, 2003)
at the Charles University in Prague have built
a treebank for the specific purpose of machine
translation, the Czech-English Penn Treebank
with tectogrammatical dependency trees. They
have asked translators to translate part of the
Penn Treebank into Czech with the clear direc-
tive to translate every English sentence with one
in Czech and to stay as close as possible to the
original.
This directive seems strange at first sight but
it makes sense with regard to their objective.
Since they specifically construct the treebank
for training and evaluating machine translation
systems, a close human translation is a valid
starting point to get good automatic transla-
tions.
At the University of Mu?nster (Germany)
(Cyrus et al, 2003) have started working on
FuSe, a syntactically analyzed parallel corpus.
The goal is a treebank with English and German
texts (currently with examples from the Eu-
roparl corpus). The annotation is multi-layered
in that they use PoS-tags, constituent structure,
functional relations, predicate-argument struc-
ture and alignment information. However their
focus is on the predicate-argument structure.
The Nordic Treebank Network1 has started
an initiative to syntactically annotate the first
chapter of ?Sophie?s World?2 in the nordic lan-
guages. This text was chosen since it has been
translated into a vast number of languages and
since it includes interesting linguistic properties
such as direct speech. Currently a prototype
of this parallel treebank with the first 50 sen-
tences in Swedish, Norwegian, Danish, Estonian
and German has been finished. The challenge
in this project is that all involved researchers
annotate the Sophie sentences of their language
in their format of choice (ranging from depen-
dency structures for Danish and Swedish to con-
stituency structures for Estonian and German).
In order to make the results exchangeable and
comparable all results have been converted into
TIGER-XML so that TIGERSearch3 can be
used to display and search the annotated sen-
tences monolingually. The alignment across lan-
guages is still open.
3 Bootstrapping a German-Swedish
parallel treebank
We have built a small German-Swedish parallel
treebank with 25 sentence pairs taken from the
Europarl corpus. First, the German sentences
1The Nordic Treebank Network is headed by
Joakim Nivre. See www.masda.vxu.se/?nivre/research/
nt.html
2The Norwegian original is: Jostein Gaarder (1991):
Sofies verden: roman om filosofiens historie. Aschehoug.
3TIGERSearch is a treebank query tool developed at
the University of Stuttgart. See also section 5.2.
were tokenized and loaded into the Annotate
treebank editor4. Annotate includes Thorsten
Brants? Part-of-Speech Tagger and Chunker for
German. The PoS tagger employs the STTS, a
set of around 50 PoS-tags for German. The set
is so large because it incorporates some morpho-
syntactic features (e.g. it distinguishes between
finite and non-finite verb forms). The chun-
ker assigns a flat constituent structure with the
usual node labels (e.g. AP, NP, PP, S, VP), but
also special labels for coordinated phrases (e.g.
CAP, CNP, CPP, CS, CVP). In addition the
chunker suggests syntactic functions (like sub-
ject, object, head or modifier) as edge labels.
The human treebank annotator controls the
suggestions made by the tagger and the chun-
ker and modifies them where necessary. Tagger
and chunker help to speed up the annotation
process for German sentences enormously. The
upper tree in figure 1 shows the structure for
the following sentence (taken from Europarl):
(1) Doch sind Bu?rger einiger unserer
Mitgliedstaaten Opfer von schrecklichen
Naturkatastrophen geworden.
(EN: But citizens of some of our
member states have become victims of
terrible natural disasters.)
Now let us look at the resources available for
Swedish. First there is SUC (the Stockholm-
Ume?a-Corpus), a 1 million word corpus of writ-
ten Swedish designed as a representative corpus
along the lines of the Brown corpus. SUC con-
tains PoS-tags, morphological tags and lemmas
for all tokens as well as proper name classes.
All the information is hand-checked. So this
is proper training material for a PoS tagger.
Compared to the 50 tags of the STTS, the 22
SUC PoS-tags (e.g. only one verb tag) are rather
coarse-grained, but of course we can use the
combination of PoS-tags and morphological in-
formation to automatically derive a richer tag
set.
Training material for a Swedish chunker is
harder to come by. There are two early
Swedish treebanks, Mamba and SynTag (dating
back to the 1970s (!) and 1980s respectively),
but they are rather small (about 5000 sen-
tences each), very heterogeneously annotated
and somewhat faulty (cf. (Nivre, 2002)). There-
fore, the most serious attempt at training a
4Annotate is a treebank editor developed at the Uni-
versity of Saarbru?cken. See www.coli.uni-sb.de/sfb378/
negra-corpus/annotate.html
Figure 1: Parallel trees with lines showing the alignment.
chunker for Swedish was based on an automat-
ically created ?treebank? which of course con-
tained a certain error rate (Megyesi, 2002). Es-
sentially there exists no constituent structure
treebank for Swedish that could be used for
training a chunker with resulting structures cor-
responding to the German sentences.
Therefore we have worked with a different
approach (described in detail in (Samuelsson,
2004)). We first trained a PoS tagger on SUC
and used it to assign PoS-tags to our Swedish
sentences. We then converted the Swedish PoS-
tags in these sentences into the corresponding
German STTS tags.5 We loaded the Swedish
sentences into Annotate (now with STTS tags),
and we were then able to reuse the German
chunker to make structural decisions over the
Swedish sentences. This worked surprisingly
5An alternative approach could have been to map all
tags in the SUC to STTS and then train a Swedish tagger
on this converted material.
well due to the structural similarities of Swedish
and German. After the semi-automatic an-
notation of the syntactic structure, the PoS-
tags were converted back to the usual Swedish
tag set. This is a straight-forward example of
how resources for one language (in this case
German) can be reused to bootstrap linguis-
tic structure in another albeit related language
(here Swedish).
The lower tree in figure 1 shows the structure
for the Swedish sentence which corresponds to
the German sentence in example 1.
(2) Da?remot har inv?anarna i ett antal av
v?ara medlemsla?nder drabbats av
naturkatastrofer som verkligen varit
fo?rskra?ckliga.
(EN: However inhabitants of a number
of our member states were affected by
natural disasters which indeed were
terrible.)
Since the German STTS is more fine-grained
than the SUC tag set, the mapping from the
SUC tag set to STTS does not entail loosing
any information. When converting in this di-
rection the problem is rather which option to
choose. For example, the SUC tag set has one
tag for adjectives, but the STTS distinguishes
between attributive adjectives (ADJA) and ad-
verbial or predicative adjectives (ADJD). We
decided to map all Swedish adjectives to ADJA
since the information in SUC does not give us
any clue about the usage difference. The human
annotator then needs to correct the ADJA tag
to ADJD if appropriate, in order to enable the
chunker to work as intended.
Other tag mapping problems come with the
SUC tags for adverb, determiner, pronoun and
possessive all of which are marked as ?interrog-
ative or relative? in the guidelines. There is no
clear mapping of these tags to STTS. We de-
cided to use the mapping in table 1.
The benefit of using the German chunker for
annotating the Swedish sentences is hard to
quantify. A precise experiment would require
one group of annotators to work with this chun-
ker and another to work without it on the same
sentences for a comparison of the time needed.
We performed a small experiment to see how
often the German chunker suggests the correct
node labels and edge labels for the Swedish sen-
tences (when the children tags/nodes were man-
ually selected). In 100 trials we observed 89 cor-
rect node labels and 93% correct edge labels (for
305 edges). If we assume that manual inspec-
tion of correct suggestions takes about a third
of the time of manual annotation, and if we also
assume that the correction of erroneous sugges-
tions takes the same amount of time as manual
annotation, then the employment of the Ger-
man chunker for Swedish saves about 60% of
the annotation time.
Reusing a chunker for bootstrapping a paral-
lel treebank between closely related languages
like German and Swedish is only a first step to-
wards reusing annotation (be it automatic or
manual) in one language for another language.
But it points to a promising research direc-
tion. (Yarowsky et al, 2001) have reported
interesting results of an annotation-projection
technique for PoS tagging, named entities and
morphology. And (Cabezas et al, 2001) have
explored projecting syntactic dependency rela-
tions from English to Basque. This idea was
followed by (Hwa et al, 2002) who investi-
gated English to Chinese projections based on
the direct correspondence assumption. They
conclude that annotation projections are nearly
70% accurate (in terms of unlabelled dependen-
cies) when some linguistic knowledge is used.
We believe that annotation projection is a diffi-
cult field but even if we only succeed in a limited
number of cases, it will be valuable for increased
speed in the development of parallel treebanks.
3.1 Alignment
The alignment in our experimental treebank is
based on the nodes, not the edge labels. Fig-
ure 1 shows the phrase alignment as thick lines
across the trees. All of the alignment mapping
was done by hand.
We decided to make the alignment determin-
istic, i.e. a node in one language can only be
aligned with one node in the other language.
There are, of course, a lot of problems with
the alignment. We have looked at the meaning,
rather than the exact wording. Sometimes dif-
ferent words are used in an S or VP, but we still
feel that the meaning is the same, and therefore
we have aligned them. We might have align-
ment on one constituent level, while there are
differences (i.e. no alignment) on lower levels of
the tree. Therefore we consider it important to
make the parse trees sufficiently deep. We need
to be able to draw the alignment on as many
levels as possible.
Another problem arises when the sentences
are constructed in different ways, due to e.g.
passivisation or topicalisation. Although Ger-
man and Swedish are structurally close, there
are some clear differences.
? German separable prefix verbs (e.g. fangen
an = begin) do not have a direct corres-
pondence in Swedish. However, Swedish
has frequent particle verbs (e.g. ta upp =
bring up). But whereas the German sep-
arated verb prefix occupies a specific posi-
tion at the end of a clause (?Rechte Satzk-
lammer?), the Swedish verb particle occurs
at the end of the verb group.
? The general word order in Swedish subordi-
nate clauses is the same as in main clauses.
Unlike in German there is no verb-final or-
der in subordinate clauses.
? German uses accusative and dative case
endings to mark direct and indirect objects.
This is reflected in the German function
labels for accusative object (OA) and for
SUC tag STTS tag
HA int. or rel. adverb PWAV adverbial interrog. or relative pronoun
HD int. or rel. determiner PWS (stand-alone) interrog. pronoun
HP int. or rel. pronoun PRELS (stand-alone) relative pronoun
HS int. or rel. possessive PPOS (stand-alone) possessive pronoun
Table 1: Mapping of SUC tags to STTS
dative object (DO). Swedish has lost these
case endings and the labels therefore need
not reflect case but rather object function.
Our overall conclusion is that applying the
German treebank annotation guidelines to
Swedish works well when the few peculiarities
of Swedish are taken care of.
4 Corpus representation
After annotating the sentences in both lan-
guages with the Annotate treebank editor, the
tree structures were exported in the NEGRA
export format from the MySQL database. The
file in NEGRA format is easily loaded into
TIGERSearch via the TIGERRegistry which
provides an import filter for this format. This
import process creates a TIGER-XML file
which contains the same information as the NE-
GRA file. The difference is that the pointers in
the NEGRA format go from the tokens to the
pre-terminal nodes (and from nodes to parent
nodes) in a bottom-up fashion, whereas in the
TIGER-XML file the nodes point to their chil-
dren by listing their id numbers (idref) and their
edge label (in a top-down perspective).
In this file the tokens of the sentence (ter-
minals) are listed beneath each other with their
corresponding PoS-tag (PPER for personal pro-
noun, VVFIN for finite verb, APPRART for
contracted preposition etc.). The nodes (non-
terminals) are listed with their name and their
outgoing edges with labels such as HD for head,
NK for noun kernel, SB for subject etc.
<s id="s1">
<graph root="522">
<terminals>
<t id="1" word="Ich" pos="PPER" />
<t id="2" word="erkla?re" pos="VVFIN"/>
<t id="3" word="die" pos="ART" />
<t id="4" word="am" pos="APPRART"/>
<t id="5" word="Freitag" pos="NN" />
[...]
</terminals>
<nonterminals>
<nt id="500" cat="NP">
<edge label="HD" idref="1" />
</nt>
[...]
<nt id="522" cat="S">
<edge label="HD" idref="2" />
<edge label="SB" idref="500" />
<edge label="MO" idref="511" />
<edge label="OA" idref="521" />
</nt>
</nonterminals>
</graph>
</s>
Since all tokens and all nodes are uniquely
numbered, these numbers can be used for the
phrase alignment. For the representation of the
alignment we adapted a DTD that was devel-
oped for the Linko?ping Word Aligner (Ahren-
berg et al, 2002). The XML-file with the align-
ment information then looks like this. The
sentLink-tags each contain one sentence pair,
while each phraseLink represents one aligned
node pair.
<!DOCTYPE DeSv SYSTEM "align.dtd">
<DeSv fromDoc="De.xml" toDoc="Sv.xml">
<linkList>
<sentLink xtargets="1 ; 1">
<phraseLink xtargets="500; 500"/>
<phraseLink xtargets="501; 503"/>
[...]
</sentLink>
</linkList>
</DeSv>
This fragment first specifies the two involved
XML files for German (De.xml) and Swedish
(Sv.xml). It then states the phrase pairs for
the sentence pair 1 - 1 from these files. For
example, phrase number 501 from the German
sentence 1 is aligned with phrase number 503 of
the Swedish sentence.
5 Tools for Parallel Treebanks
Treebank tools are usually of two types. First
there are tools for producing the treebank, i.e.
for automatically adding information (taggers,
chunkers, parsers) and for manual inspection
and correction (treebank editors). On the other
hand we need tools for viewing and searching a
treebank.
5.1 Treebank Editors
Of course the tools for monolingual treebank
production can also be used for building the
language-specific parts of a parallel treebank.
Thus a treebank editor such as Annotate with
built-in PoS tagger and chunker is an invaluable
resource. But such a tool should include or be
complemented with a completeness and consis-
tency checker.
In addition the parallel treebank needs to be
aligned on the sub-sentence level. Automatic
word alignment systems will help ((Tiedemann,
2003) discusses some interesting approaches).
But tools for checking and correcting this align-
ment will be needed. For example the I*Link
system (Ahrenberg et al, 2002) could be used
for this task. I*Link comes with a graphical
user interface for creating and storing associa-
tions between segments in a bitext. I*Link is
aimed at word and phrase associations and re-
quires bitexts that are pre-aligned at the sen-
tence level.
5.2 Treebank Search Tools
With the announcement of the Penn Treebank,
some 10 years ago, came a search tool called
tgrep. It is a UNIX-based program that allows
querying a treebank specifying dominance and
precedence relations over trees (plus regular ex-
pressions and boolean operators). The search
results are bracketed trees in line-based or in-
dented format catering for the needs of different
users. For example, the following tgrep query
searches for a VP that dominates (not necessar-
ily directly) an NP which immediately precedes
a PP.
VP << (NP . PP)
More recently TIGERSearch was launched.
It is a Java-based program that comes with a
graphical user interface and a powerful feature-
value-oriented query language. The output
are graphical tree representations in which the
matched part of the tree is highlighted and fo-
cused. TIGERSearch?s ease of installation and
friendly user interface have made it the tool of
choice for many treebank researchers.
According to our knowledge no specific search
tools for parallel treebanks exist. In addition to
the above sketched search options of tgrep and
TIGERSearch a search tool for parallel tree-
banks will have to allow queries that combine
constraints over two trees. For example one
wants to issue queries such as ?Find a tree
in language 1 with a relative clause where the
parallel tree in language 2 uses a prepositional
phrase for the same content.?
5.3 Displaying Parallel Trees
There is currently no off-the-shelf tool that can
display parallel trees so that one could view two
phrase structure trees at the same time with
their alignment. Therefore we discuss possible
display options of such a future program.
One alternative is to show the two trees above
each other (as in figure 1). And there are
many ways to visualize the alignment: Either
by drawing lines between the nodes (as we did),
or by color marking the nodes, or by opening an-
other window where only chosen parallel nodes
are shown. The latter case corresponds to a
zoom function, but this also entails that the user
has to click on a node to view the alignment.
Another alternative would be a mirror imag-
ing. One language would have its tree with the
root at the top and the tree of the other lan-
guage would be below with the root at the bot-
tom. The alignment could be portrayed in the
same ways as above.
But then the display problem is mainly a
problem concerning the computer screens of to-
day, where a large picture partly lands outside
of the screen, while a smaller scale picture might
result in words that are too small to be read-
able. One solution could be to use two screens
(as is done in complex layout tasks), but then
we cannot have a solution with the trees above
each other, but rather next to each other, pos-
sibly with some kind of color marking of the
nodes.
A last alternative is to use vertical trees,
where the words are listed below each other,
showing phrase depth horizontally. Then the
alignment could be shown by having the nodes
side by side instead of above each other. This
is the least space consuming alternative, but it
is also the least intuitive one. Furthermore, this
is not a viable alternative if the trees contain
crossing branches.
We currently favor the first approach with
two trees above each other, and we have writ-
ten a program that takes the SVG (scalable
vector graphics) representation of two trees (as
exported from TIGERSearch), merges the two
graphs into a single graph and adds the phrase
alignment lines based on the information in the
alignment file.
6 Conclusions
We have reported on our experiments for build-
ing a German-Swedish parallel treebank. We
have shown that by mapping the German PoS
tag set to the Swedish tag set we were able
to reuse the German chunker for the semi-
automatic annotation of the Swedish sentences.
Our experiments have also shown that the Ger-
man annotation guidelines with minor adapta-
tions are well-suited for Swedish.
We have argued that tools for building mono-
lingual treebanks can be used for parallel tree-
banks as well, and that tools for sub-sentence
alignment are available but they are not enough
evaluated yet for aligning tree structures. Tools
for viewing and searching through parallel tree-
banks are missing.
7 Acknowledgements
We would like to thank the anonymous review-
ers for useful comments, the members of the
Nordic Treebank Network for many interesting
discussions, and David Hagstrand for handling
our annotation databases.
References
Anne Abeille?, editor. 2003. Building and Us-
ing Parsed Corpora, volume 20 of Text,
Speech and Language Technology. Kluwer,
Dordrecht.
Lars Ahrenberg, Magnus Merkel, and Mikael
Andersson. 2002. A system for incremental
and interactive word linking. In Proceedings
from The Third International Conference on
Language Resources and Evaluation (LREC-
2002), pages 485?490, Las Palmas.
Lars Borin, editor. 2002. Parallel Corpora,
Parallel Worlds. Selected Papers from a Sym-
posium on Parallel and Comparable Corpora
at Uppsala University, Sweden, 22-23 April,
1999., volume 43 of Language and Comput-
ers. Rodopi, Amsterdam.
Clara Cabezas, Bonnie Dorr, and Philip Resnik.
2001. Spanish language processing at Univer-
sity of Maryland: Building infrastructure for
multilingual applications. In Proceedings of
the Second International Workshop on Span-
ish Language Processing and Language Tech-
nologies (SLPLT-2), Jaen, Spain, September.
Martin Cmejrek, Jan Curin, and Jiri Havelka.
2003. Treebanks in machine translation. In
Proc. Of the 2nd Workshop on Treebanks and
Linguistic Theories, Va?xjo?, Sweden.
Lea Cyrus, Hendrik Feddes, and Frank Schu-
macher. 2003. FuSe - a multi-layered paral-
lel treebank. In Proc. Of the 2nd Workshop
on Treebanks and Linguistic Theories, Va?xjo?,
Sweden.
Rebecca Hwa, Philip Resnik, Amy Weinberg,
and Okan Kolak. 2002. Evaluating transla-
tional correspondence using annotation pro-
jection. In Proceedings of the 40th Annual
Meeting of the ACL, Philadelphia.
Bea?ta Megyesi. 2002. Data-Driven Syn-
tactic Analysis. Methods and Applications
for Swedish. Doctoral dissertation, Kungl.
Tekniska Ho?gskolan. Department of Speech,
Music and Hearing, Stockholm.
I. Dan Melamed. 2001. Empirical Methods for
Exploiting Parallel Texts. MIT Press, Cam-
bridge, MA.
Joakim Nivre. 2002. What kinds of trees grow
in Swedish soil? A comparison of four anno-
tation schemes for Swedish. In Proc. Of First
Workshop on Treebanks and Linguistic The-
ory, Sozopol, Bulgaria.
Yvonne Samuelsson. 2004. Parallel phrases.
Experiments towards a German-Swedish par-
allel treebank. C-uppsats, Stockholms Uni-
versitet.
Ann Taylor, Mitchell Marcus, and Beatrice
Santorini. 2003. The Penn Treebank: An
overview. In Anne Abeille?, editor, Build-
ing and Using Parsed Corpora, volume 20
of Text, Speech and Language Technology.
Kluwer, Dordrecht.
Jo?rg Tiedemann. 2003. Recycling Transla-
tions. Extraction of Lexical Data from Par-
allel Corpora and Their Application in Nat-
ural Language Processing. Acta universitatis
upsaliensis, Uppsala University.
D. Yarowsky, G. Ngai, and R. Wicentowski.
2001. Inducing multilingual text analysis
tools via robust projection across aligned cor-
pora. In Proceedings of HLT 2001, First In-
ternational Conference on Human Language
Technology Research.
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 81?88,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
How bad is the problem of PP-attachment? A comparison of English,
German and Swedish
Martin Volk
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
volk@ling.su.se
Abstract
The correct attachment of prepositional
phrases (PPs) is a central disambigua-
tion problem in parsing natural languages.
This paper compares the baseline situation
in English, German and Swedish based
on manual PP attachments in various tree-
banks for these languages. We argue that
cross-language comparisons of the disam-
biguation results in previous research is
impossible because of the different selec-
tion procedures when building the training
and test sets. We perform uniform tree-
bank queries and show that English has the
highest noun attachment rate followed by
Swedish and German. We also show that
the high rate in English is dominated by
the preposition of. From our study we de-
rive a list of criteria for profiling data sets
for PP attachment experiments.
1 Introduction
Any computer system for natural language
processing has to struggle with the problem of am-
biguities. If the system is meant to extract precise
information from a text, these ambiguities must
be resolved. One of the most frequent ambigu-
ities arises from the attachment of prepositional
phrases (PPs). Simply stated, a PP that follows
a noun (in English, German or Swedish) can be
attached to the noun or to the verb.
In the last decade various methods for the res-
olution of PP attachment ambiguities have been
proposed. The seminal paper by (Hindle and
Rooth, 1993) started a sequence of studies for
English. We investigated similar methods for Ger-
man (Volk, 2001; Volk, 2002). Recently other
languages (such as Dutch (Vandeghinste, 2002) or
Swedish (Aasa, 2004)) have followed.
In the PP attachment research for other lan-
guages there is often a comparison of the dis-
ambiguation accuracy with the English results.
But are the results really comparable across lan-
guages? Are we starting from the same base-
line when working on PP attachment in struc-
turally similar languages like English, German and
Swedish? Is the problem of PP attachment equally
bad (equally frequent and of equal balance) for
these three languages? These are the questions we
will discuss in this paper.
In order to find answers to these questions we
have taken a closer look at the training and test
data used in various experiments. And we have
queried the most important treebanks for the three
languages under investigation.
2 Background
(Hindle and Rooth, 1993) did not have access to a
large treebank. Therefore they proposed an unsu-
pervised method for resolving PP attachment am-
biguities. And they evaluated their method against
880 English triples verb-noun-preposition (V-N-P)
which they had extracted from randomly selected,
ambiguously located PPs in a corpus. For exam-
ple, the sentence ?Timex had requested duty-free
treatment for many types of watches? results in
the V-N-P triple (request, treatment, for). These
triples were manually annotated by both authors
with either noun or verb attachment based on the
complete sentence context. Interestingly, 586 of
these triples (67%) were judged as noun attach-
ments and only 33% as verb attachments. And
(Hindle and Rooth, 1993) reported on 80% at-
tachment accuracy, an improvement of 13% over
the baseline (i.e. guessing noun attachment in all
81
cases).
A year later (Ratnaparkhi et al, 1994) published
a supervised approach to the PP attachment prob-
lem. They had extracted quadruples V-N-P-N1
(plus the accompanying attachment decision) from
both an IBM computer manuals treebank (about
9000 tuples) and from the Wall Street Journal
(WSJ) section of the Penn treebank (about 24,000
tuples). The latter tuple set has been reused by
subsequent research, so let us focus on this one.2
(Ratnaparkhi et al, 1994) used 20,801 tuples for
training and 3097 tuples for evaluation. They re-
ported on 81.6% correct attachments.
But have they solved the same problem as (Hin-
dle and Rooth, 1993)? What was the initial bias
towards noun attachment in their data? It turns out
that their training set (the 20,801 tuples) contains
only 52% noun attachments, while their test set
(the 3097 tuples) contains 59% noun attachments.
The difference in noun attachments between these
two sets is striking, but (Ratnaparkhi et al, 1994)
do not discuss this (and we also do not have an
explanation for this). But it makes obvious that
(Ratnaparkhi et al, 1994) were tackling a prob-
lem different from (Hindle and Rooth, 1993) given
the fact that their baseline was at 59% guessing
noun attachment (rather than 67% in the Hindle
and Rooth experiments).3
Of course, the baseline is not a direct indica-
tor of the difficulty of the disambiguation task.
We may construct (artificial) cases with low base-
lines and a simple distribution of PP attachment
tendencies. For example, we may construct the
case that a language has 100 different prepositions,
where 50 prepositions always introduce noun at-
tachments, and the other 50 prepositions always
require verb attachments. If we also assume that
both groups occur with the same frequency, we
have a 50% baseline but still a trivial disambigua-
tion task.
But in reality the baseline puts the disambigua-
tion result into perspective. If, for instance, the
baseline is 60% and the disambiguation result is
80% correct attachments, then we will claim that
our disambiguation procedure is useful. Whereas
1The V-N-P-N quadruples also contain the head noun of
the NP within the PP.
2The Ratnaparkhi training and test sets were later distrib-
uted together with a development set of 4039 V-N-P-N tuples.
3It should be noted that important subsequent research,
e.g. by (Collins and Brooks, 1995; Stetina and Nagao, 1997),
used the Ratnaparkhi data sets and thus allowed for good
comparability.
if we have a baseline of 80% and the disambigua-
tion result is 75%, then the procedure can be dis-
carded.
So what are the baselines reported for other lan-
guages? And is it possible to use the same extrac-
tion mechanisms for V-N-P-N tuples in order to
come to comparable baselines?
We did an in-depth study on German PP at-
tachment (Volk, 2001). We compiled our own
treebank by annotating 3000 sentences from the
weekly computer journal ComputerZeitung. We
had first annotated a larger number of subsequent
sentences with Part-of-Speech tags, and based on
these PoS tags, we selected 3000 sentences that
contained at least one full verb plus the sequence
of a noun followed by a preposition. After annotat-
ing the 3000 sentences with complete syntax trees
we used a Prolog program to extract V-N-P-N tu-
ples with the accompanying attachment decisions.
This lead to 4562 tuples out of which 61% were
marked as noun attachments. We used the same
procedure to extract tuples from the first 10,000
sentences of the NEGRA treebank. This resulted
in 6064 tuples with 56% noun attachment (for a
detailed overview see (Volk, 2001) p. 86). Again
we observe a substantial difference in the baseline.
When our student Jo?rgen Aasa worked on repli-
cating our German experiments for Swedish, he
used a Swedish treebank from the 1980s for the
extraction of test data. He extracted V-N-P-N tu-
ples from SynTag, a treebank with 5100 newspa-
per sentences built by (Ja?rborg, 1986). And Aasa
was able to extract 2893 tuples out of which 73.8%
were marked as noun attachments (Aasa, 2004)
(p. 25). This was a surprisingly high figure, and
we wondered whether this indicated a tendency in
Swedish to avoid the PP in the ambiguous posi-
tion unless it was to be attached to the noun. But
again the extraction process was done with a spe-
cial purpose extraction program whose correctness
was hard to verify.
3 Querying Treebanks with
TIGER-Search
We therefore decided to check the attachment ten-
dencies of PPs in various treebanks for the three
languages in question with the same tool and with
queries that are as uniform as possible.
For English we used the WSJ section of the
Penn Treebank, for German we used our own
ComputerZeitung treebank (3000 sentences), the
82
NEGRA treebank (10,000 sentences) and the re-
cently released version of the TIGER treebank
(50,000 sentences). For Swedish we used the
SynTag treebank mentioned above and one sec-
tion of the Talbanken treebank (6100 sentences).
All these treebanks consist of constituent structure
trees, and they are in representation formats which
allow them to be loaded into TIGER-Search. This
enables us to query them all in similar manners
and to get a fairer comparison of the attachment
tendencies.
TIGER-Search is a powerful treebank query
tool developed at the University of Stuttgart
(Ko?nig and Lezius, 2002). Its query language
allows for feature-value descriptions of syntax
graphs. It is similar in expressiveness to tgrep (Ro-
hde, 2005) but it comes with graphical output and
highlighting of the syntax trees plus some nice sta-
tistics functions.
Our experiments for determining attachment
tendencies proceed along the following lines. For
each treebank we first query for all sequences of a
noun immediately followed by a PP (henceforth
noun+PP sequences). The dot being the prece-
dence operator, we use the query:
[pos="NN"] . [cat="PP"]
This query will match twice in the tree in fig-
ure 1. It gives us the frequency of all ambiguously
located PP. We disregard the fact that in certain
clause positions a PP in such a sequence cannot
be verb-attached and is thus not ambiguous. For
example, an English noun+PP sequence in subject
position is not ambiguous with respect to PP at-
tachment since the PP cannot attach to the verb.
Similar restrictions apply to German and Swedish.
In order to determine how many of these se-
quences are annotated as noun attachments, we
query for noun phrases that contain both a noun
and an immediately following PP. This query will
look like:
#np_mum:[cat="NP"] >
#np_child:[cat="NP"] &
#np_mum > #pp:[cat="PP"] &
#np_child >* #noun:[pos="NN"] &
#noun . #pp
All strings starting with # are variables and the
> symbol is the dominance operator. So, this
query says: Search for an NP (and call it np mum)
that immediately dominates another NP (np child)
AND that immediately dominates a PP, AND the
np child dominates a noun which is immediately
followed by the PP.
This query presupposes that a PP which is at-
tached to a noun is actually annotated with the
structure (NP (NP (... N)) (PP)) which is true for
the Penn treebank (compare to the tree in figure 1).
But the German treebanks represent this type of at-
tachment rather as (NP (... N) (PP)) which means
that the query needs to be adapted accordingly.4
Such queries give us the frequency of all
noun+PP sequences and the frequency of all such
sequences with noun attachments. These frequen-
cies allow us to calculate the noun attachment rate
(NAR) in our treebanks.
NAR = freq(noun+ PP, noun attachm)freq(noun+ PP )
We assume that all PPs in noun+PP sequences
which are not attached to a noun are attached to a
verb. This means we ignore the very few cases of
such PPs that might be attached to adjectives (as
for instance the second PP in ?due for revision in
1990?).
Different annotation schemes require modifica-
tions to these basic queries, and different noun
classes (regular nouns, proper names, deverbal
nouns etc.) allow for a more detailed investiga-
tion. We now present the results for each language
in turn.
3.1 Results for English
We used sections 0 to 12 of the WSJ part of the
Penn Treebank (Marcus et al, 1993) with a total
of 24,618 sentences for our experiments. Our start
query reveals that an ambiguously located PP (i.e.
a noun+PP sequence) occurs in 13,191 (54%) of
these sentences, and it occurs a total of 20,858
times (a rate of 0.84 occurrences per sentences
with respect to all sentences in the treebank).
Searching for noun attachments with the second
query described in section 3 we learn that 15,273
noun+PP sequences are annotated as noun attach-
ments. And we catch another 547 noun attach-
ments if we query for noun phrases that contain
two PPs in sequence.5 In these cases the sec-
ond PP is also attached to a noun, although not
4There are a few occurrences of this latter structure in the
Penn Treebank which should probably count as annotation
errors.
5See (Merlo et al, 1997) for a discussion of these cases
and an approach in automatically disambiguating them.
83
Figure 1: Noun phrase tree from the Penn Treebank
to the noun immediately preceding it (as for ex-
ample in the tree in figure 1). With some simi-
lar queries we located another 110 cases of noun
attachments (most of which are probably anno-
tation errors if the annotation guidelines are ap-
plied strictly). This means that we found a total
of 15,930 cases of noun attachment which corre-
sponds to a noun attachment rate of 76.4% (by
comparison to the 20,858 occurrences).
This is a surprisingly high number. Neither
(Hindle and Rooth, 1993) with 67% nor (Ratna-
parkhi et al, 1994) with 59% noun attachment
were anywhere close to this figure. What have we
done differently?
One aspect is that we only queried for singu-
lar nouns (NN) in the Penn Treebank where plural
nouns (NNS) and proper names (NNP and NNPS)
have separate PoS tags. Using analogous queries
for plural nouns we found that they exhibit a NAR
of 71.7%. Whereas the queries for proper names
(singular and plural names taken together) account
for a NAR of 54.5%.
Another reason for the discrepancy in the NAR
between Ratnaparkhi?s data and our calculations
certainly comes from the fact that we queried
for all sequences noun+PP as possibly ambiguous
whereas they looked only at such sequences within
verb phrases. But since we will do the same in
both German and Swedish, this is still worthwhile.
3.2 Results for German
The three German treebanks which we investigate
are all annotated in more or less the same man-
ner, i.e. according to the NEGRA guidelines which
were slightly refined for the TIGER project. This
enabled us to use the same set of queries for all
CZ NEGRA TIGER
size 3000 10,000 50,000
noun+PP seq 4355 6,938 39,634
occur rate 1.4 0.7 0.8
noun attachm 2743 4102 23,969
NAR 63.0% 59.1% 60.5%
Table 1: Results for the German treebanks
three of them. Since the German guidelines distin-
guish between node labels for coordinated phrases
(e.g. CNP and CPP) and non-coordinated phrases
(e.g. NP and PP), these distinctions needed to be
taken into account. Table 1 summarizes the re-
sults.
Our own ComputerZeitung treebank (CZ) has a
much higher occurrence rate of ambiguously lo-
cated PPs because the sentences were preselected
for this phenomenon. The general NEGRA and
TIGER treebanks have an occurrence rate that is
similar to English (0.8). The NAR varies between
59.1% for the NEGRA treebank and 63.0% for the
CZ treebank for regular nouns.
The German annotation also distinguishes be-
tween regular nouns and proper names. The
proper names show a much lower noun attach-
ment rate than the regular nouns. The NAR in the
CZ treebank is 22%, in the NEGRA treebank it is
20%, and in the TIGER treebank it is only 17%.
Here we suspect that the difference between the
CZ and the other treebanks is based on the differ-
ent text types. The computer journal CZ contains
more person names with affiliation (e.g. Stan Sug-
arman von der Firma Telemedia) and more com-
pany names with location (e.g. Aviso aus Finn-
84
land) than a regular newspaper (that was used in
the NEGRA and TIGER corpora).
As mentioned above, our previous experiments
in (Volk, 2001) were based on sets of extracted
tuples from both the CZ and NEGRA treebanks.
Our extracted data set from the CZ treebank had
a noun attachment rate of 61%, and the one from
the NEGRA treebank had a noun attachment rate
of 56%.
So why are our new results based on TIGER-
Search queries two to three percents higher? The
main reason is that our old data sets included
proper names (with their low noun attachment
rate). But our extraction procedure comprised also
a number of other idiosyncracies. In an attempt
to harvest as many interesting V-N-P-N tuples as
possible from our treebanks we exploited coordi-
nated phrases and pronominal PPs. Some exam-
ples:
1. If the PP was preceded by a coordinated
noun phrase, we created as many tuples
as there were head nouns in the coordina-
tion. For example, the phrase ?den Aus-
tausch und die gemeinsame Nutzung von
Daten . . . ermo?glichen? leads to the tuples
(ermo?glichen, Austausch, von, Daten) and
(ermo?glichen, Nutzung, von, Daten) both
with the decision ?noun attachment?.
2. If the PP was introduced by coordinated
prepositions (e.g. Die Argumente fu?r oder
gegen den Netzwerkcomputer), we created as
many tuples as there were prepositions.
3. If the verb group consists of coordinated
verbs (e.g. Infos fu?r Online-Dienste aufbe-
reiten und gestalten), we created as many tu-
ples as there were verbs.
4. We regarded pronominal adverbs (darin,
dazu, hieru?ber, etc.) and reciprocal pronouns
(miteinander, untereinander, voneinander,
etc.) as equivalent to PPs and created tu-
ples when such pronominals appeared imme-
diately after a noun. See (Volk, 2003) for a
more detailed discussion of these pronouns.
3.3 Results for Swedish
Currently there is no large-scale Swedish treebank
available. But there are some smaller treebanks
from the 80s which have recently been converted
to TIGER-XML so that they can also be queried
with TIGER-Search.
SynTag (Ja?rborg, 1986) is a treebank consist-
ing of around 5100 sentences. Its conversion to
TIGER-XML is documented in (Hagstro?m, 2004).
The treebank focuses on predicate-argument struc-
tures and some grammatical functions such as sub-
ject, head and adverbials. It is thus different from
the constituent structures that we find in the Penn
treebank or the German treebanks. We had to
adapt our queries accordingly. Since prepositional
phrases are not marked as such, we need to query
for constituents (marked as subject, as adverbial
or simply as argument) that start with a preposi-
tion. This results in a noun attachment rate of 73%
(which is very close to the rate reported by (Aasa,
2004)). Again this does not include proper names
which have a NAR of 44% in SynTag.
Let us compare these results to the second
Swedish treebank, Talbanken (first described by
(Telemann, 1974)). Talbanken was a remark-
able achievement in the 80s as it comes with two
written language parts (with a total of more than
10,000 sentences from student essays and from
newspapers) and two spoken language parts (with
another 10,000 trees from interviews and conver-
sations). We concentrated on the 6100 trees from
the written part taken from newspaper texts.
The occurrence rate in Talbanken is 0.76 (4658
noun+PP sequences in 6100 sentences), which is
similar to the rates observed for English and Ger-
man. The occurrence rate in SynTag is higher 0.93
(4737 noun+PP sequences in 5114 sentences).
Talbanken (in its converted form) is annotated
with constituent structure labels (NP, PP, VP etc.)
and also distinguishes coordinated phrases (CNP,
CPP, CVP etc.). The queries for determining the
noun attachment rate can thus be similar to the
queries over the German treebanks. In addition,
Talbanken comes with a rich set of grammatical
features as edge labels (e.g. there are different la-
bels for logical subject, dummy subject and other
subject).
We found that the NAR for regular nouns in
Talbanken is 60.5%. Talbanken distinguishes be-
tween regular nouns, deverbal nouns (often with
the derivation suffix -ing: tja?nstgo?ring, utbildning,
o?vning) and deadjectival nouns (mostly with the
derivation suffix -het: skyldighet, snabbhet, verk-
samhet). Not surprisingly, these special nouns
have higher NARs than the regular nouns. The
85
deadjectival nouns have a NAR of 69.5%, and the
deverbal nouns even have a NAR of 77%. Taken
together (i.e. regarding all regular, deadjectival
and deverbal nouns) this results in a NAR of 64%.
Thus, the NARs which we obtain from the two
Swedish treebanks (SynTag 73% and Talbanken
64%) differ drastically. It is unclear what this dif-
ference depends on. The text genre (newspapers)
is the same in both cases. We have noticed that
SynTag contains a number of annotation errors,
but we don?t see that these errors favor noun at-
tachment of PPs in a systematic way. One aspect
might be the annotation decision in Talbanken to
annotate PPs in light verb constructions.
These are disturbing cases where the PP is a
child node of the sentence node S (which means
that it is interpreted as a verb attachment) with
the edge label OA (objektadverbial). Nivre (2005,
personal communication) pointed out that ?OA is
what some theoreticians would call a ?preposi-
tional object? or a ?PP complement?, i.e. a com-
plement of the verb that semantically is close to
an object but which is realized as a prepositional
phrase.? In our judgement many of those cases
should be noun attachments (and thus be a child
of an NP).
For example, we looked at fo?rutsa?ttning fo?r (=
prerequisite for) which occurs 14 times, out of
which 2 are annotated as OO (Other object) + OA,
11 are annotated as noun attachments, and 1 is er-
roneously annotated. If we compare that to be-
tydelse fo?r (= significance for) which occurs 16
times out of which 13 are annotated as OO+OA
and 3 are annotated as noun attachments, we won-
der.
First, it is obvious that there are inconsistencies
in the treebank. We cannot see any reason why
the 2 cases of fo?rutsa?ttning fo?r are annotated dif-
ferently than the other 11 cases. The verbs do not
justify these discrepancies. For example, we have
skapa (= to create) with the verb attachments and
fo?rsvinna (= to disappear) with the noun attach-
ment cases. And we find ge (= to give) on both
sides.
Second, we find it hard to follow the argument
that the tendency for betydelse fo?r is stronger for
the OO+OA than for fo?rutsa?ttning fo?r. It might be
based on the fact that betydelse fo?r is often used
with the verb ha (= to have) and thus may count
as a light verb construction with a verb group con-
sisting of both ha plus betydelse and the fo?r-PP
being interpreted as an object of this complex verb
group.
Third, unfortunately not all cases of PPs anno-
tated as objektadverbial can be regarded as noun
attachments. But after having looked at some 70
occurrences of such PPs immediately following a
noun, we estimate that around 30% should be noun
attachments.
Concluding our observations on Swedish let us
mention that the very few cases of proper names
in Talbanken have a NAR of 24%.
4 Comparison of the results
For English we have computed a NAR of 76.4%
based on the Penn Treebank, for German we found
NARs between 59% and 63% based on three tree-
banks, and for Swedish we determined a puzzling
difference between 73% NAR in SynTag and 64%
NAR in Talbanken. So, why is the tendency of
a PP to attach to a preceding noun stronger in
English than in Swedish which in turn shows a
stronger tendency than German?
For English the answer is very clear. The strong
NAR is solely based on the dominance of the
preposition of. In our section of the Penn Tree-
bank we found 20,858 noun+PP sequences. Out
of these, 8412 (40% !!) were PPs with the prepo-
sition of. And 99% of all of-PPs are noun attach-
ments. So, the preposition of dominates the Eng-
lish NAR to the point that it should be treated sep-
arately.6
The Ratnaparkhi data sets (described above in
section 2) contain 30% tuples with the preposition
of in the test set and 27% of-tuples in the training
set. The higher percentage of of-tuples in the test
set may partially explain the higher NAR of 59%
(vs. 52% in the training set).
The dominance of of-tuples may also explain
the relatively high NAR for proper names in Eng-
lish (54.5%) in comparison to 17% - 22% in Ger-
man and similar figures for the Swedish Talbanken
corpus. The Penn Treebank represents names that
contain a PP (e.g. District of Columbia, American
Association of Individual Investors) with a regular
phrase structure. It turns out that 861 (35%) of the
2449 sequences ?proper name followed by PP? are
based on of-PPs. The dominance becomes even
more obvious if we consider that the following
6This is actually what has been done in some research on
English PP attachment disambiguation. (Ratnaparkhi, 1998)
first assumes noun attachment for all of-PPs and then applies
his disambiguation methods to all remaining PPs.
86
prepositions on the frequency ranks are in (with
only 485 occurrences) and for (246 occurrences).
The dominance of the preposition of is so strong
in English that we will get a totally different pic-
ture of attachment preferences if we omit of-PPs.
The Ratnaparkhi training set without of-tuples is
left with a NAR of 35% (!) and the test set has a
NAR of 42%. In other words, English has a clear
tendency of attaching PPs to verbs if we ignore the
dominating of-PPs.
Neither German nor Swedish has such a dom-
inating preposition. There are, of course, prepo-
sitions in both languages that exhibit a clear ten-
dency towards noun attachment or verb attach-
ment. But they are not as frequent as the prepo-
sition of in English. For example, clear temporal
prepositions like German seit (= since) are much
more likely as verb attachments.
Closest to the English of is the Swedish prepo-
sition av which has a NAR of 88% in the Tal-
banken corpus. But its overall frequency does not
dominate the Swedish ranking. The most frequent
preposition in ambiguous positions is i (frequency:
651 and NAR: 53%) followed by av (frequency:
564; NAR: 88%) and fo?r (frequency: 460; NAR:
42%).
5 Conclusion
The most important conclusion to be drawn from
the above experiments and observations is the im-
portance of profiling the data sets when working
and reporting on PP attachment experiments. The
profile should certainly answer the following ques-
tions:
1. What types of nouns where used when the tu-
ples were extracted? (regular nouns, proper
names, deverbal nouns, etc.)
2. Are there prepositions which dominate in fre-
quency and attachment rate (like the English
preposition of)? If so, how does the data set
look like without these dominating preposi-
tions?
3. What types of prepositions where regarded?
(regular prepositions, contracted prepositions
(e.g. in German am, im, zur), derived prepo-
sitions (e.g. English prepositions derived
from gerund verb forms following, including,
pending) etc.)
4. Is the extraction procedure restricted to
noun+PP sequences in the verb phrase, or
does it consider all such sequences?
5. What is the noun attachment rate in the data
set?
In order to find dominating prepositions we sug-
gest a data profiling that includes the frequency
and NARs of all prepositions in the data set. This
will also give an overall picture of the number of
prepositions involved.
Our experiments have also shown the advan-
tages of large treebanks for comparative linguistic
studies. Such treebanks are even more valuable
if they come in the same representation schema
(e.g. TIGER-XML) so that they can be queried
with the same tools. TIGER-Search has proven
to be a suitable treebank query tool for our exper-
iments although its statistics function broke down
on some frequency counts we tried on large tree-
banks. For example, it was not possible to get a
list of all prepositions with occurrence frequencies
from a 50,000 sentence treebank.
Another item on our TIGER-Search wish list is
a batch mode so that we could run a set of queries
and obtain a list of frequencies. Currently we have
to trigger each query manually and copy the fre-
quency results manually to an Excel file.
Other than that, TIGER-Search is a wonderful
tool which allows for quick sanity checks of the
queries with the help of the highlighted tree struc-
ture displays in its GUI.
We have compared noun attachment rates in
English, German and Swedish over treebanks
from various sources and with various annotation
schemes. Of course, the results would be even
better comparable if the treebanks were built on
the same translated texts, i.e. on parallel corpora.
Currently, there are no large parallel treebanks
available. But our group works on such a par-
allel treebank for English, German and Swedish.
Design decisions and first results were reported
in (Volk and Samuelsson, 2004) and (Samuels-
son and Volk, 2005). We believe that such par-
allel treebanks will allow a more focused and
more detailed comparison of phenomena across
languages.
6 Acknowledgements
We would like to thank Jo?rgen Aasa for discus-
sions on PP attachment in Swedish, and Joakim
87
Nivre, Johan Hall, Jens Nilsson at Va?xjo? Univer-
sity for making the Swedish Talbanken treebank
available. We also thank the anonymous review-
ers for their discerning comments.
References
Jo?rgen Aasa. 2004. Unsupervised resolution of PP at-
tachment ambiguities in Swedish. Master?s thesis,
Stockholm University. Combined C/D level thesis.
Michael Collins and James Brooks. 1995. Prepo-
sitional phrase attachment through a backed-off
model. In Proc. of the Third Workshop on Very
Large Corpora.
Bo Hagstro?m. 2004. A TIGER-XML version of Syn-
Tag. Master?s thesis, Stockhom University.
D. Hindle and M. Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19(1):103?120.
Jerker Ja?rborg. 1986. SynTag Dokumentation. Manual
fo?r SynTaggning. Technical report, Department of
Swedish, Go?teborg University.
Esther Ko?nig and Wolfgang Lezius. 2002. The TIGER
language - a description language for syntax graphs.
Part 1: User?s guidelines. Technical report.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330.
P. Merlo, M.W. Crocker, and C. Berthouzoz. 1997. At-
taching multiple prepositional phrases: generalized
backed-off estimation. In Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing. Brown University, RI.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase at-
tachment. In Proceedings of the ARPA Workshop
on Human Language Technology, Plainsboro, NJ,
March.
Adwait Ratnaparkhi. 1998. Statistical models for un-
supervised prepositional phrase attachment. In Pro-
ceedings of COLING-ACL-98, Montreal.
Douglas L. T. Rohde, 2005. TGrep2 User Man-
ual. MIT. Available from http://tedlab.mit.edu/
?dr/Tgrep2/.
Yvonne Samuelsson and Martin Volk. 2005. Presen-
tation and representation of parallel treebanks. In
Proc. of the Treebank-Workshop at Nodalida, Joen-
suu, May.
J. Stetina and M. Nagao. 1997. Corpus-based PP at-
tachment ambiguity resolution with a semantic dic-
tionary. In J. Zhou and K. Church, editors, Proc.
of the 5th Workshop on Very Large Corpora, pages
66?80, Beijing and Hongkong.
Ulf Telemann. 1974. Manual Fo?r Grammatisk
Beskrivning Av Talad Och Skriven Svenska. Inst. fo?r
nordiska spra?k, Lund.
Vincent Vandeghinste. 2002. Resolving PP attachment
ambiguities using the WWW (abstract). In Compu-
tational Linguistics in the Netherlands, Groningen.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proc. of Work-
shop on Linguistically Interpreted Corpora (LINC)
at COLING, Geneva.
Martin Volk. 2001. The automatic resolution of prepo-
sitional phrase attachment ambiguities in German.
Habilitationsschrift, University of Zurich.
Martin Volk. 2002. Combining unsupervised and su-
pervised methods for PP attachment disambiguation.
In Proc. of COLING-2002, Taipeh.
Martin Volk. 2003. German prepositions and their kin.
a survey with respect to the resolution of PP attach-
ment ambiguities. In Proc. of ACL-SIGSEM Work-
shop: The Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics For-
malisms and Applications, pages 77?88, Toulouse,
France, September. IRIT.
88
XML-based Phrase Alignment in Parallel Treebanks
Martin Volk, Sofia Gustafson-Capkova?, Joakim Lundborg,
Torsten Marek, Yvonne Samuelsson, Frida Tidstro?m
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
volk@ling.su.se
Abstract
This paper describes the usage of XML for
representing cross-language phrase align-
ments in parallel treebanks. We have de-
veloped a TreeAligner as a tool for interac-
tively inserting and correcting such align-
ments as an independent level of treebank
annotation.
1 Introduction
The combined research on treebanks and paral-
lel corpora has recently led to parallel treebanks.
A parallel treebank consists of syntactically anno-
tated sentences in two or more languages, taken
from translated (i.e. parallel) documents. In ad-
dition, the syntax trees of two corresponding sen-
tences are aligned on a sub-sentential level. This
means word level, phrase level and clause level,
but we will refer to it as phrase alignment since
it best represents the idea. Parallel treebanks can
be used as training or evaluation corpora for word
and phrase alignment, as input for example-based
machine translation (EBMT), as training corpora
for transfer rules, or for translation studies.
We are developing an English-German-Swedish
parallel treebank. In this paper we will focus on
the representation of the treebank and the align-
ment. We will briefly explain the steps for building
the parallel treebank and describe our new align-
ment tool. This paper is a follow-up and revision
of (Samuelsson and Volk, 2005) based on fresh in-
sights from this tool.
2 Building the treebanks
Our parallel treebank contains the first two chap-
ters of Jostein Gaarder?s novel ?Sofie?s World?
with about 500 sentences.1 In addition it contains
500 sentences from economy texts (a quarterly re-
port by a multinational company as well as part of
a bank?s annual report).
In creating the parallel treebank, we have
first annotated the monolingual treebanks with
the ANNOTATE treebank editor.2 It includes
Thorsten Brants? statistical Part-of-Speech Tagger
and Chunker. The chunker follows the TIGER
annotation guidelines for German (Brants and
Hansen, 2002), which gives a flat phrase structure
tree. This means, for instance, no unary nodes,
no ?unnecessary? NPs (noun phrases) within PPs
(prepositional phrases) and no finite VPs (verb
phrases).
Using a flat tree structure for manual treebank
annotation has two advantages for the human an-
notator: fewer annotation decisions, and a better
overview of the trees. This comes at the prize
of the trees not being complete from a linguistic
point of view. Moreover, flat syntax trees are also
problematic for node alignment in a parallel tree-
bank. We prefer to have ?deep trees? to be able to
draw the alignment on as many levels as possible;
in fact, the more detailed the sentence structure is,
the more expressive our alignment can become.
As an example, let us look at the work
flow for the German-Swedish parallel treebank.
We first annotated the German sentences semi-
automatically in the flat manner, and we then auto-
matically deepened the flat syntax trees (Samuels-
son and Volk, 2004).
1A prototype of the parallel treebank was developed by
Yvonne Samuelsson and contains the first chapter of the
novel in German and Swedish. Later, a French version was
added and aligned to the Swedish treebank by (Tidstro?m,
2005). We would like to thank Eckhard Bick, Declan Groves
and Jo?rg Tiedemann for their help.
2www.coli.uni-sb.de/sfb378/negra-corpus/annotate.html
93
We annotated the Swedish sentences by first
tagging them with a Part-of-Speech tagger trained
on SUC (the Stockholm-Umea? Corpus). Since we
did not have a Swedish treebank to train a Swedish
chunker, we used a trick to apply the German
chunker for Swedish sentences. We mapped the
Swedish Part-of-Speech tags in the Swedish sen-
tences to the corresponding German tags. Since
the German chunker works on these tags, it then
suggested constituents for the Swedish sentences,
assuming they were German sentences. These
experiments and the resulting time gain were re-
ported in (Volk and Samuelsson, 2004). Upon
completion of the Swedish treebank with flat syn-
tax trees, we applied the same deepening method
as for German, and we then converted the Part-of-
Speech labels back to the Swedish labels.
Finally, we annotated the English sentences ac-
cording to the Penn Treebank guidelines. We
trained the PoS tagger and the chunker on the Penn
Treebank and integrated them into ANNOTATE.
The English guidelines lead to complete trees so
that the deepening step is not needed.
3 XML Representation of the Trees
After finishing the monolingual treebanks with
ANNOTATE, the trees were exported from the
accompanying SQL database and converted into
TIGER-XML. TIGER-XML is a line-based (i.e.
not nested and thus database-friendly) representa-
tion for graph structures, which includes syntax
trees with node labels, edge labels, multiple fea-
tures on the word level and even crossing edges.3
In a TIGER-XML graph each leaf (= token) and
each node (= linguistic constituent) has a unique
identifier which is prefixed with the sentence num-
ber. Leaves are numbered from 1 to 499 and nodes
starting from 500 (under the plausible assumption
that no sentence will ever have more than 499 to-
kens). As can be seen in the following exam-
ple, node 500 in sentence 12 is of the category
PP (prepositional phrase). The phrase consists
of word number 4, which is the preposition in,
plus node 502 which in turn is marked as an NP
(noun phrase), consisting of the words 5 and 6. It
should be noted that the id attribute in the token
lines serves a dual purpose of identifier and order
marker. This makes it possible to represent cross-
ing branches.
<s id="s12">
3See www.ims.uni-stuttgart.de/projekte/TIGER
<graph root="s12_501">
<terminals>
<t id="s12_1" word="Jetzt" pos="ADV" />
<t id="s12_2" word="bog" pos="VVFIN" />
<t id="s12_3" word="sie" pos="PPER" />
<t id="s12_4" word="in" pos="APPR" />
<t id="s12_5" word="den" pos="ART" />
<t id="s12_6" word="Kl?verveien" pos="NE"/>
<t id="s12_7" word="ein" pos="PTKVZ" />
<t id="s12_8" word="." pos="$." />
</terminals>
<nonterminals>
<nt id="s12_500" cat="PP">
<edge label="HD" idref="s12_4" />
<edge label="NK" idref="s12_502" />
</nt>
<nt id="s12_502" cat="NP">
<edge label="NK" idref="s12_5" />
<edge label="HD" idref="s12_6" />
</nt>
[...]
</nonterminals>
</graph>
</s>
This means that the token identifiers and con-
stituent identifiers are used as pointers to represent
the nested tree structure. This example thus repre-
sents the upper tree in figure 1.
One might wonder why tree nesting is not di-
rectly mapped into XML nesting. But the require-
ment that the representation format must support
crossing edges rules out this option. TIGER-XML
is a powerful representation format and is typically
used with constituent symbols on the nodes and
functional information on the edge labels. This
constitutes a combination of constituent structure
and dependency structure information.
4 XML Representation of the Alignment
Phrase alignment can be regarded as an additional
layer of information on top of the syntax struc-
ture. We use the unique node identifiers for the
phrase alignment across parallel trees. We also
use an XML representation for storing the align-
ment. The alignment file first stores the names of
the treebank files and assigns identifiers to them.
Every single phrase alignment is then stored with
the tag align. Thus the entry in the following
example represents the alignment of node 505 in
sentence 13 of language one (German) to the node
506 in sentence 14 of language two (Swedish).
<treebanks>
<tbank file="Sofie_DE.xml" id="De"/>
<tbank file="Sofie_SV.xml" id="Sv"/>
</treebanks>
<align type="exact">
<node node_id="s13_505" tbank_id="De"/>
<node node_id="s14_506" tbank_id="Sv"/>
</align>
94
This representation allows phrase alignments
within m:n sentence alignments, which we have
used in our project. The XML also allows m:n
phrase alignments, which we however have not
used for reasons of simplicity and clarity. Two
nodes are aligned if the words which they span
convey the same meaning and could serve as trans-
lation units.
The alignment format allows alignments to be
specified between an arbitrary number of nodes,
for example nodes from three languages. And
it includes an attribute type which we currently
use to distinguish between exact and approximate
alignments.
5 Our Tree Alignment Tool
After finishing the monolingual trees we want to
align them on the phrase level. For this purpose
we have developed a ?TreeAligner?. This program
is a graphical user interface to insert (or correct)
alignments between pairs of syntax trees.4 The
TreeAligner can be seen in the line of tools such
as I*Link (Ahrenberg et al, 2002) or Cairo (Smith
and Jahr, 2000) but it is especially tailored to visu-
alize and align full syntax trees.
The TreeAligner requires three input files. One
TIGER-XML file with the trees from language
one, another TIGER-XML file with the trees from
language two, plus the alignment file as described
above. The alignment file might initially be empty
when we want to start manual alignment from
scratch, or it might contain automatically com-
puted alignments for correction. The TreeAligner
displays tree pairs with the trees in mirror orien-
tation (one top-up and one top-down). See fig-
ure 1 for an example. This has the advantage that
the alignment lines cross fewer parts of the lower
tree. The trees are displayed with node labels and
greyed-out edge labels. The PoS labels are omit-
ted in the display since they are not relevant for the
task.
Each alignment is displayed as a dotted line be-
tween one node (or word) from each tree. Clicking
on a node (or a word) in one tree and dragging the
mouse pointer to a node (or a word) in the other
tree inserts an alignment line. Figure 2 shows an
example of a tree pair with alignment lines. Cur-
rently the TreeAligner supports two types of align-
4The TreeAligner has been implemented in Python by
Joakim Lundborg and is freely available at www.ling.su.se/
DaLi/downloads/treealigner/index.htm
Figure 1: Tree pair German-Swedish in the
TreeAligner.
ment lines (displayed in different colors) which
are used to indicate exact translation correspon-
dence vs. approximate translation correspondence.
However, our experiments indicate that eventually
more alignment types will be needed to precisely
represent different translation deviations.
Often one tree needs to be aligned to two trees
in the other language. We therefore provide the
option to scroll the trees independently. For in-
stance, if we have aligned only a part of tree 20
from language one to tree 18 of language two, we
may scroll to tree 19 of language two in order to
align the remaining parts of tree 20.5
The TreeAligner is designed as a stand-alone
tool (i.e. it is not prepared for collaborative anno-
tation). It stores every alignment in an XML file
(in the format described above) as soon as the user
moves to a new tree pair. It has been tested on
parallel treebanks with several hundred trees each.
6 Conclusion
We have shown a straightforward way to tie in
XML-based phrase alignment information with
syntax trees represented in TIGER-XML. The
alignment information is stored independently
from the treebank files. This independence allows
for a modularization and separation of the anno-
tation but it entails that the synchronization of the
5The final result of an m:n tree alignment can be visual-
ized with an SVG-based display which we have described in
(Samuelsson and Volk, 2005). SVG (Scalable Vector Graph-
ics) describes vector graphics in XML.
95
Figure 2: Tree pair German-Swedish with alignment in the TreeAligner.
treebanks with the alignment needs to be guarded
separately. If any of the treebanks is modified, the
modification of the alignment needs to follow.
We have argued for the use of a graphical
TreeAligner to display and interactively modify
the alignment between parallel syntax trees. The
TreeAligner allows for m:n sentence alignment,
word alignment and node alignment. And it sup-
ports the distinction between exact and approxi-
mate alignments.
As a next step we plan to integrate a com-
ponent for automatic phrase alignment into the
TreeAligner. The user can then select a tree pair
and will get automatic phrase alignment predic-
tions. We have already experimented with the
projection of automatically computed word align-
ments to predict phrase alignment. Of course, the
automatic phrase alignment has to be manually
checked if we want to ensure high quality align-
ment data.
Another avenue of further research is the inclu-
sion of yet more levels of annotation. For exam-
ple, we are currently experimenting with the anno-
tation of semantic frames on top of the treebanks.
We use the SALSA tool developed at Saarbru?cken
University (Erk and Pado, 2004) which also as-
sumes TIGER-XML input. So, TIGER-XML has
become the lingua franca of treebank annotation
which allows for the addition of arbitrary layers.
References
Lars Ahrenberg, Magnus Merkel, and Mikael Anders-
son. 2002. A system for incremental and interactive
word linking. In Proc. of LREC-2002, pages 485?
490, Las Palmas.
Sabine Brants and Silvia Hansen. 2002. Developments
in the TIGER annotation scheme and their realiza-
tion in the corpus. In Proc. of LREC-2002, pages
1643?1649, Las Palmas.
Katrin Erk and Sebastian Pado. 2004. A powerful and
versatile XML format for representing role-semantic
annotation. In Proc. of LREC-2004, Lisbon.
Yvonne Samuelsson and Martin Volk. 2004. Au-
tomatic node insertion for treebank deepening. In
Proc. of 3rd Workshop on Treebanks and Linguistic
Theories, Tu?bingen, December.
Yvonne Samuelsson and Martin Volk. 2005. Presen-
tation and representation of parallel treebanks. In
Proc. of the Treebank-Workshop at Nodalida, Joen-
suu, May.
Noah A. Smith and Michael E. Jahr. 2000. Cairo:
An alignment visualization tool. In Proc. of LREC-
2000, Athens.
Frida Tidstro?m. 2005. Extending a parallel treebank
with data in French. C-uppsats, Department of Lin-
guistics, Stockholm University, April.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proc. of Work-
shop on Linguistically Interpreted Corpora (LINC)
at COLING, Geneva.
96
Proceedings of the Linguistic Annotation Workshop, pages 85?92,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Search Tool for Parallel Treebanks
Martin Volk, Joakim Lundborg and Mae?l Mettler
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
volk@ling.su.se
Abstract
This paper describes a tool for aligning and
searching parallel treebanks. Such treebanks
are a new type of parallel corpora that come
with syntactic annotation on both languages
plus sub-sentential alignment. Our tool al-
lows the visualization of tree pairs and the
comfortable annotation of word and phrase
alignments. It also allows monolingual and
bilingual searches including the specifica-
tion of alignment constraints. We show that
the TIGER-Search query language can eas-
ily be combined with such alignment con-
straints to obtain a powerful cross-lingual
query language.
1 Introduction
Recent years have seen a number of initiatives in
building parallel treebanks. Our group has partici-
pated in these efforts by building a tri-lingual paral-
lel treebank called SMULTRON (Stockholm MULti-
lingal TReebank).1 Our parallel treebank consists
of syntactically annotated sentences in three lan-
guages, taken from translated (i.e. parallel) docu-
ments. In addition, the syntax trees of corresponding
sentence pairs are aligned on a sub-sentential level.
This means they are aligned on word level or phrase
level (some phrases can be as large as clauses). Par-
allel treebanks can be used as training or evalua-
tion corpora for word and phrase alignment, as input
1We gratefully acknowledge financial support for the
SMULTRON project by Granholms stiftelse and Rausings stif-
telse.
for example-based machine translation (EBMT), as
training corpora for transfer rules, or for translation
studies.
Similar projects include Croco (Hansen-Schirra
et al, 2006) which is aimed at building a German-
English parallel treebank for translation stud-
ies, LinES an English-Swedish parallel treebank
(Ahrenberg, 2007), and the Czech-English parallel
dependency treebank built in Prague (Cmejrek et al,
2005).
SMULTRON is an English-German-Swedish par-
allel treebank (Samuelsson and Volk, 2006;
Samuelsson and Volk, 2007). It contains the first
two chapters of Jostein Gaarder?s novel ?Sofie?s
World? with about 500 sentences. In addition it
contains 500 sentences from economy texts (a quar-
terly report by a multinational company as well as
part of a bank?s annual report). We have (semi-
automatically) annotated the German sentences with
Part-of-Speech tags and phrase structure trees (incl.
edges labeled with functional information) follow-
ing the NEGRA/TIGER guidelines for German tree-
banking.
For English we have used the Penn Treebank
guidelines which are similar in that they also pre-
scribe phrase structure trees (with PoS tags, but only
partially annotated with functional labels). However
they differ from the German guidelines in many de-
tails. For example the German trees use crossing
edges for discontinuous units while the English trees
introduce symbols for empty tokens plus secondary
edges for the representation of such phenomena.
For Swedish there were no appropriate guidelines
available. Therefore we have adapted the German
85
guidelines to Swedish. The general annotation strat-
egy for Swedish was the same as for German: PoS
tags, phrase structure trees (incl. functional edge la-
bels) and crossing branches for discontinuous units.
But, of course, there are linguistic differences be-
tween German and Swedish that required special
attention (e.g. Swedish prepositions that introduce
sentences).
The treebanks for all three languages were anno-
tated separately with the help of the treebank editor
ANNOTATE. After finishing the monolingual tree-
banks, the trees were exported from the accompany-
ing SQL database and converted into an XML for-
mat as input to our alignment tool, the Stockholm
TreeAligner.
In this paper we will first describe this alignment
tool and then focus on its new search facility. To our
knowledge this is the first dedicated tool that com-
bines visualization, alignment and searching of par-
allel treebanks (although there are others who have
experimented with parallel corpus searches (Ny-
gaard and Johannesen, 2004; Petersen, 2006)).
2 The Stockholm TreeAligner
When our monolingual treebanks were finished, the
trees were exported from the editor system and con-
verted into TIGER-XML. TIGER-XML is a line-
based (i.e. not nested and thus database-friendly)
representation for graph structures which supports
crossing edges and secondary edges.2 TIGER-XML
has been defined as input format for TIGER-Search,
a query tool for monolingual treebanks (see section
3). We use this format also as input format for our
alignment tool, the Stockholm TreeAligner (Volk et
al., 2006).
The TreeAligner program is a graphical user in-
terface to specify (or correct) word and phrase
alignments between pairs of syntax trees.3 The
TreeAligner is roughly similar to alignment tools
such as I*Link (Ahrenberg et al, 2002) or Cairo
(Smith and Jahr, 2000) but it is especially tailored to
visualize and align full syntax trees (including trees
with crossing edges).
2For information about TIGER-XML see www.ims.uni-
stuttgart.de/projekte/TIGER
3The TreeAligner is freely available at www.ling.su.se/
DaLi/downloads/treealigner/index.htm
Figure 1: Tree pair German-English in the
TreeAligner.
The TreeAligner operates on an alignment file in
an XML format developed by us. This file describes
the alignments between two TIGER-XML treebanks
(specified in the alignment file) holding the trees
from language one and language two respectively.
For example the alignment between two nodes is
represented as:
<align type="exact">
<node id="s13_505" tb_id="DE"/>
<node id="s14_506" tb_id="EN"/>
</align>
This says that node 505 in sentence 13 of the Ger-
man treebank is aligned with node 506 in sentence
14 of the English treebank. The node identifiers re-
fer to the ids in the TIGER-XML treebanks. The
alignment is given the label ?exact? if the corre-
sponding token sequences are equivalent in mean-
ing.
The alignment file might initially be empty when
we want to start manual alignment from scratch, or
it might contain automatically computed alignments
for correction. The TreeAligner displays tree pairs
with the trees in mirror orientation (one top-up and
one top-down). See figure 1 for an example. The
trees are displayed with node labels and edge labels.
The PoS labels are omitted in the display since they
are not relevant for the alignment task.4
4During the development of our treebanks we discovered
86
Each alignment is displayed as a dotted line be-
tween two nodes (or words) across two trees. Click-
ing on a node (or a word) in one tree and drag-
ging the mouse pointer to a node (or a word) in the
other tree inserts an alignment line. Currently the
TreeAligner supports two types of alignment lines
(displayed in different colors) which are used to in-
dicate exact translation correspondence vs. approxi-
mate translation correspondence. However, our ex-
periments indicate that eventually more alignment
types will be needed to precisely represent differ-
ent translation differences. The alignment type at-
tribute can be used to describe many different lev-
els or types of alignment. These distinctions could
prove useful when exploiting the aligned treebanks
for Machine Translation and other applications.
Often one tree needs to be aligned to two (or
more) trees in the other language. The TreeAligner
therefore provides the option to browse the trees in-
dependently. For instance, if we have aligned only a
part of a tree Ti from language one to tree Tk of lan-
guage two, we may scroll to tree Tk+1 of language
two in order to align the remaining parts of Ti. Spe-
cial [Forward] and [Back] buttons are provided to
browse through the multiple-aligned trees systemat-
ically.
The TreeAligner is designed as a stand-alone tool
(i.e. it is not prepared for collaborative annotation).
It stores every alignment in an XML file (in the for-
mat described above) as soon as the user moves to a
new tree pair.
The TreeAligner was implemented in Python by
Joakim Lundborg. Python has become popular
in Language Technology in recent years. It is a
high level programming language that allows dif-
ferent programming styles including a good sup-
port for object-oriented programming. It is an in-
terpreted language that uses a dynamic type system.
It is therefore mostly compared to its siblings Perl,
Tcl and Ruby, even though the influence of other
languages like Smalltalk and Haskell are probably
stronger on a conceptual level.
One of Python?s strengths is the ease with which
that the TreeAligner is also useful for displaying different ver-
sions of the same treebank (e.g. before and after corrections, or
manually vs. automatically parsed). Therefore we plan to add a
tree-diff module which will highlight the differences between a
pair of trees over the same token sequence.
a programmer can manipulate primitive data types
like strings or numbers. Python?s string objects are
an excellent match to the needs of linguistic process-
ing. In addition to the primitive data types, Python
also features higher level data types: lists, tuples and
dictionaries. The combination of these built-in data
types, the vast standard library and the simple and
straightforward syntax make Python the perfect tool
for a wide range of scientific programming.
The TreeAligner served us well for creating the
alignments, but it soon became evident that we
needed suitable tools to explore and exploit the
aligned data. The most apparent need was a search
module for aligned trees. We decided to design our
search module after TIGER-Search.
3 TIGER-Search
TIGER-Search is a powerful treebank query tool de-
veloped at the University of Stuttgart by Wolfgang
Lezius (cf. (Ko?nig and Lezius, 2002; Lezius, 2002).
Its query language allows for feature-value descrip-
tions of syntax graphs. It is similar in expressiveness
to tgrep (Rohde, 2005) but it comes with graphical
output and highlighting of the syntax trees plus nice
frequency tables for objects identified in the query.
TIGER-Search has been implemented in Java and is
freely available for research purposes. Because of its
clearly defined input format (TIGER-XML) and its
powerful query language, it has become the corpus
query system of choice for many linguists.
The TIGER-Search query language is based on
feature-value descriptions of all linguistic objects
(tokens and constituents), dominance, precedence
and sibling relations in the tree, graph predicates
(e.g. with respect to token arity and continuity), vari-
ables for referencing objects, regular expressions
over values for varying the query precision, and
queries over secondary edges (which constitute a
secondary graph level).
A complex query might look like in the follow-
ing example (with > denoting direct dominance, >*
denoting general dominance, the dot denoting im-
mediate precedence, and the # symbol introducing
variables). This query is meant to find instances of
two ambiguously located PPs that are both attached
to the first noun (as illustrated by the example tree in
figure 2).
87
Figure 2: Noun phrase tree from the Penn Treebank
#np:[cat="NP"] >* #n1:[pos="NN"]&
#np > #pp1:[cat="PP"] &
#n1 . #pp1 &
#pp1 >* #n2:[pos="NN"] &
#np > #pp2:[cat="PP"] &
#n2 . #pp2
This query says: Search for an NP (call it #np)
that dominates a noun #n1 (line 1) and two PPs (lines
2 and 5). #pp1 must follow immediately after the
noun #n1 (line 3), and #pp2 must follow immedi-
ately after the noun within the #pp1 (lines 4 and 6).
TIGER-Search handles such queries efficiently
based on a intricate indexing scheme. It finds all
matching instances in a given treebank and allows
to browse (and to export) the resulting trees. The
matching objects in the resulting trees are high-
lighted.
TIGER-Search is limited in that it only allows
manually entered queries (rather than processing a
batch of queries from a file). Furthermore it is lim-
ited with regard to negation. The TIGER-Search
query language includes a negation operator but this
is of limited usefulness. The reason is that ?For
the sake of computational simplicity and tractabil-
ity, the universal quantifier is (currently) not part
of the TIGER language? (quoted from the TIGER-
Search online help manual). This means that typical
negated queries such as ?Find all VPs which do not
contain any NP? are not possible.
And clearly TIGER-Search is a tool for querying
monolingual treebanks and thus needed to be ex-
tended for our purposes, i.e. querying parallel tree-
banks.
4 The TreeAligner Search Module
(Merz and Volk, 2005) had listed the requirements
for a parallel treebank search tool. Based on
these we have now re-implemented TIGER-Search
for parallel treebanks and integrated it into the
TreeAligner.
The idea is to allow the power of TIGER-Search
queries on both treebanks plus additional alignment
constraints. For example, a typical query could ask
for a verb phrase VP dominating a prepositional
phrase PP in treebank one. This query can be com-
bined with the constraint that the VP in treebank one
is aligned to a sentence S in treebank two which also
dominates a PP. Such a query would be expressed in
3 lines as:
#t1:[cat="VP"] > [cat="PP"]
#t2:[cat="S"] > [cat="PP"]
#t1 * #t2
These three lines are entered into three separate
input fields in the user interface (cf. the three in-
put fields in the bottom left in figure 3). Lines 1
and 2 contain the queries over the monolingual tree-
banks 1 and 2. And line 3 contains the alignment
constraint. Note that the treebank queries 1 and 2
closely follow the TIGER-Search syntax. In par-
ticular they allow the binding of variables (marked
with #) to specific linguistic objects in the query.
And these variables are used in the alignment con-
straint in line 3. The reuse of the variables is the cru-
88
Figure 3: Screenshot of the TreeAligner with the Search Module.
cial idea which enabled a clear design of the Search
Module by keeping the alignment constraints sepa-
rate from the queries over the two treebanks.
So the above query will find the tree pair in figure
3 because it matches the alignment between the Eng-
lish VP closed the front door behind her and the el-
liptical Swedish sentence sta?ngde do?rren bakom sig
(which lacks the subject, but is still annotated as S).
The Search Module has recently been added to the
TreeAligner. It is intended to be used with any par-
allel treebank where the monolingual treebanks can
be converted into TIGER-XML and where the align-
ment information can be converted to the SMUL-
TRON alignment format. The separation of these
parts makes it possible to query each treebank sepa-
rately as well. The system is divided into a monolin-
gual query facility and an alignment query facility
that makes use of the former to perform its job. This
design choice made it necessary to (re)implement
the following in Python:
1. TIGER-Search
2. The alignment query facility
3. The integration into the TreeAligner
The choice of reimplementing TIGER-Search in
Python influenced the feature set. Even though
the implementation of TIGER-Search is well doc-
umented (in (Lezius, 2002) among others) and the
source codes are available under an Open Source li-
cense, this is still a non-trivial task. In order to nar-
row down the amount of work in a first phase, it was
decided to restrict the implementation to a subset of
the TIGER-Search query language. The implemen-
tation of negation within the queries was therefore
postponed (with the exception of negations used in
regular expressions within a feature definition). As
discussed in section 3, negations are limited even in
TIGER-Search, and we plan to implement a com-
prehensive support for negation at a later stage. The
code already has hooks for this extension.
The language for the alignment constraints is
kept simple as well. The user can specify that
two linguistic objects must be aligned (with exact
89
alignment or approximate alignment). And such
constraints can be combined with AND statements
into more complex constraints. Currently, we can-
not foresee exactly how a parallel treebank will be
queried. We have therefore focused on a clear de-
sign of the Search Module rather than overloading it
with features. This will facilitate the integration of
more features as they are requested by users.
4.1 Implementation Details
The implementation of the Search Module started
as a close re-implementation of the TIGER-Search
system described in (Lezius, 2002). During the de-
velopment it became apparent that some of Lezius?
design choices did not translate well into Python.
Moreover, the advancements concerning speed and
memory in computer hardware in recent years have
made it possible for us to deviate from the original
design towards a more Python-oriented and simpler
code with less considerations for resource limita-
tions (see (Mettler, 2007)).
This code base can be divided into four types
of functionality classes: helper, index, parser and
processor. The helper classes are the smallest pieces
of code and perform trivial tasks like sorting or set
operations and are called from the other classes.
The query system as such consists of the index, the
parsers and processors. The parsers are used to
transform a string such as the TIGER-XML files or
the queries into objects. These parse objects are then
used to create the index or are passed to a processor
object to get the results of a query.
The index consists of four classes. The Cor-
pus class governs the three others which are used
to store the data for the graphs and the attribute
value register that is defined in the TIGER-XML
head. Each graph is contained within its own ob-
ject. The attribute value register consists of one ob-
ject that governs a range of attribute value lookup
tables. There are three parser classes and one parser
method. Each of these parser classes handles a dif-
ferent input. The first parses TIGER-XML, the sec-
ond parses the node definitions within a TIGER-
Search query (contained within the square brackets),
and the third parser class uses them to parse com-
plete TIGER-Search queries. As the syntax for the
alignment constraints is simple, this was done within
a method of the parallel query processor class. This
is likely to change with the increasing feature set for
parallel queries.
The last part of the system consists of two proces-
sor classes. The first is the class used for monolin-
gual queries. On instantiation the class takes an in-
dex object and a query parser object as arguments.
When the object?s query method is called with a
query string, the object lets the query parser produce
a parse object from the string. The parse object is
then processed to produce an object that contains the
matching graph parts using the index. The processor
for parallel queries works similarly. On instantiation
a monolingual processor for each language is passed
as arguments to the object. When the query method
is called, the parallel processor objects gets the re-
sults from the monolingual processors first and then
parses and processes the parallel query using the re-
sults from the monolingual processing step. The re-
sult of a query is a list with the two aligned sentence
IDs.
4.2 Evaluation of the Search Module
The TreeAligner Search module was first tested by
running a set of representative queries over a part
of our English-German parallel treebank (500 tree
pairs). This test set included:
? dominance relations (direct dominance, gen-
eral dominance, labeled dominance, right and
left corner dominance)
? precedence relations (immediate precedence,
general precedence, sibling precedence, prece-
dence distance)
? queries over secondary edges
? graph predicates (root, arity, tokenarity)
For the monolingual queries we checked whether
the number of hits in our TreeAligner Search cor-
responded to the number of hits in TIGER-Search.
This worked nicely. For bilingual queries we manu-
ally checked the correctness of the results.
We also tested the system for robustness and scal-
ability. Since we currently do not have a large paral-
lel treebank, we took the German NEGRA treebank
with 10,000 trees and used it for both language one
and language two in our TreeAligner. This means
we used each tree aligned to a copy of itself as the
90
basic data. This treebank contains around 81,000
nodes. We automatically generated an alignment file
that contains each node aligned to its copy in the
corresponding tree. This means we were using an
alignment file with 81,000 alignments.
Unfortunately the time for loading this data set
into the TreeAligner was prohibitively long (while
loading a monolingual treebank with 10,000 trees
into TIGER-Search takes less than a minute for in-
dexing it once, plus few seconds for loading the in-
dex before starting the searches). Obviously, we
need to improve the scalability of the TreeAligner.
When we redid the experiment with 1000 trees
from the NEGRA treebank (with 35,756 align-
ments), it worked fine. Loading takes about one
minute, and queries like the one given in the exam-
ple above are processed in less than one minute. The
system is currently not optimized for speed. It is
a proof-of-concept system to demonstrate that the
(monolingual) TIGER-Search query language can
be elegantly extended with alignment constraints for
parallel treebank searches.
Lately we have tested the use of serialized in-
dexes. We have observed that they are much faster,
but that the speed-up factor decreases with increas-
ing file size. It seems that eventually we will have
to switch to a custom binary format as was done
in TIGER-Search, if we want to provide a smooth
work experience with parallel treebanks of 10,000
and more trees.
5 Conclusions
We have built a TreeAligner for displaying and
searching parallel aligned trees. The tool is writ-
ten in Python and freely available. In particular it
allows to align nodes and words across languages
by drawing lines. We distinguish between exact
and approximate alignment types. The search mod-
ule which was recently added supports queries over
both treebanks in combination with alignment con-
straints. The query language follows TIGER-Search
(though negation is not included yet). The alignment
constraints use the variables bound to linguistic ob-
jects in the monolingual queries.
In the future we will improve the TreeAligner in
three directions: features, usability and evaluation.
The feature part consists of providing full support
for TIGER-Search queries (in particular the imple-
mentation of negation) and improving the parallel
query facilities (with a variety of alignment con-
straints).
Moreover we are in the process of extending the
TreeAligner to handling dependency trees. The
TreeAligner currently imports only treebanks in
TIGER-XML. This format is well suited for rep-
resenting phrase structure trees but less for depen-
dency trees. We will therefore extend the support to
appropriate XML import formats.
Usability is the broadest group and aims at im-
provements like creating an installation routine for
all operating systems, improving speed and making
sure that UTF8 support works properly.
Finally, more systematic evaluations are needed.
We plan to enlarge our standard set of queries to
cover all possible combinations. This query set
could then be used to test the speed and performance
of our system (and for the comparison with other
systems). We hope that the TreeAligner will gain a
broad user community which will help to drive im-
provements in alignment and querying.
References
Lars Ahrenberg, Magnus Merkel, and Mikael Andersson.
2002. A system for incremental and interactive word
linking. In Proc. of LREC-2002, pages 485?490, Las
Palmas.
Lars Ahrenberg. 2007. LinES: An English-Swedish par-
allel treebank. In Proc. of Nodalida, Tartu.
Martin Cmejrek, Jan Cur??n, and Jir?? Havelka. 2005.
Prague Czech-English dependency treebank. Resource
for structure-based MT. In Proceedings of EAMT 10th
Annual Conference, Budapest.
Silvia Hansen-Schirra, Stella Neumann, and Mihaela
Vela. 2006. Multi-dimensional annotation and align-
ment in an English-German translation corpus. In Pro-
ceedings of the EACL Workshop on Multidimensional
Markup in Natural Language Processing (NLPXML-
2006), pages 35? 42, Trento.
Esther Ko?nig and Wolfgang Lezius. 2002. The TIGER
language - a description language for syntax graphs.
Part 1: User?s guidelines. Technical report.
Wolfgang Lezius. 2002. Ein Suchwerkzeug fu?r syn-
taktisch annotierte Textkorpora. Ph.D. thesis, IMS,
University of Stuttgart, December. Arbeitspapiere des
91
Instituts fu?r Maschinelle Sprachverarbeitung (AIMS),
volume 8, number 4.
Charlotte Merz and Martin Volk. 2005. Requirements
for a parallel treebank search tool. In Proceedings of
GLDV-Conference, Sprache, Sprechen und Computer
/ Computer Studies in Language and Speech, Bonn,
March. Peter Lang Verlag.
Mae?l Mettler. 2007. Parallel treebank search - the imple-
mentation of the Stockholm TreeAligner search. C-
uppsats, Stockholm University, March.
Lars Nygaard and Janne Bondi Johannesen. 2004.
SearchTree - a user-friendly treebank search interface.
In Proc. of 3rd Workshop on Treebanks and Linguistic
Theories, pages 183?189, Tu?bingen, December.
Ulrik Petersen. 2006. Querying both parallel and tree-
bank corpora: Evaluation of a corpus query system. In
Proc. of LREC, Genua.
Douglas L. T. Rohde, 2005. TGrep2 User Manual. MIT.
Available from http://tedlab.mit.edu/ ?dr/Tgrep2/.
Yvonne Samuelsson and Martin Volk. 2006. Phrase
alignment in parallel treebanks. In Jan Hajic and
Joakim Nivre, editors, Proc. of the Fifth Workshop
on Treebanks and Linguistic Theories, pages 91?102,
Prague, December.
Yvonne Samuelsson and Martin Volk. 2007. Alignment
tools for parallel treebanks. In Proceedings of GLDV
Fru?hjahrstagung 2007.
Noah A. Smith and Michael E. Jahr. 2000. Cairo: An
alignment visualization tool. In Proc. of LREC-2000,
Athens.
Martin Volk, Sofia Gustafson-Capkova?, Joakim Lund-
borg, Torsten Marek, Yvonne Samuelsson, and Frida
Tidstro?m. 2006. XML-based phrase alignment in par-
allel treebanks. In Proc. of EACL Workshop on Multi-
dimensional Markup in Natural Language Processing,
Trento, April.
92
Combining unsupervised and supervised methods for PP
attachment disambiguation
Martin Volk
University of Zurich
Scho?nberggasse 9
CH-8001 Zurich
vlk@zhwin.ch
Abstract
Statistical methods for PP attachment fall into
two classes according to the training material
used: first, unsupervised methods trained on
raw text corpora and second, supervised meth-
ods trained on manually disambiguated exam-
ples. Usually supervised methods win over un-
supervised methods with regard to attachment
accuracy. But what if only small sets of manu-
ally disambiguated material are available? We
show that in this case it is advantageous to in-
tertwine unsupervised and supervised methods
into one disambiguation algorithm that outper-
forms both methods used alone.1
1 Introduction
Recently, numerous statistical methods for
prepositional phrase (PP) attachment disam-
biguation have been proposed. They can
broadly be divided into unsupervised and su-
pervised methods. In the unsupervised methods
the attachment decision is based on information
derived from large corpora of raw text. The text
may be automatically processed (e.g. by shallow
parsing) but not manually disambiguated. The
most prominent unsupervised methods are the
Lexical Association score by Hindle and Rooth
(1993) and the cooccurrence values by Ratna-
parkhi (1998). They resulted in up to 82% cor-
rect attachments for a set of around 3000 test
cases from the Penn treebank. Pantel and Lin
(2000) increased the training corpus, added a
collocation database and a thesaurus which im-
proved the accuracy to 84%.
In contrast, the supervised methods are based
on information that the program learns from
manually disambiguated cases. These cases
1This research was supported by the Swiss National
Science Foundation under grant 12-54106.98.
are usually extracted from a treebank. Su-
pervised methods are as varied as the Back-
off approach by Collins and Brooks (1995)
and the Transformation-based approach by
Brill and Resnik (1994). Back-off scored
84% correct attachments and outperformed the
Transformation-based approach (80%). Even
better results were reported by Stetina and Na-
gao (1997) who used the WordNet thesaurus
with a supervised learner and achieved 88% ac-
curacy.
All these accuracy figures were reported for
English. We have evaluated both unsupervised
and supervised methods for PP attachment dis-
ambiguation in German. This work was con-
strained by the availability of only a small Ger-
man treebank (10,000 sentences). Under this
constraint we found that an intertwined combi-
nation of using information from unsupervised
and supervised learning leads to the best re-
sults. We believe that our results are relevant to
many languages for which only small treebanks
are available.
2 Our training resources
We used the NEGRA treebank (Skut et al,
1998) with 10,000 sentences from German news-
papers and extracted 4-tuples (V,N1, P,N2)
whenever a PP with the preposition P and the
core noun N2 immediately followed a noun N1
in a clause headed by the verb V . For example,
the sentence
In Deutschland ist das Gera?t u?ber die Bad
Homburger Ergos zu beziehen.
[In Germany the appliance may be ordered from Er-
gos based in Bad Homburg.]
leads to the 4-tuple (beziehen, Gera?t, u?ber,
Ergos). In this way we obtained 5803 4-tuples
with the human judgements about the attach-
ment of the PP (42% verb attachments and 58%
noun attachments). We call this the NEGRA
test set.
As raw corpus for unsupervised training we
used four annual volumes (around 5.5 mil-
lion words) of the ?Computer-Zeitung? (CZ), a
weekly computer science magazine. This corpus
was subjected to a number of processing steps:
sentence recognition, proper name recognition
for persons, companies and geographical loca-
tions (cities and countries), part-of-speech tag-
ging, lemmatization, NP/PP chunking, recog-
nition of local and temporal PPs, and finally
clause boundary recognition.
3000 sentences of the CZ corpus each contain-
ing at least one PP in an ambiguous position
were set aside for manual disambiguation. An-
notation was done according to the same guide-
lines as for the NEGRA treebank. From these
manually annotated sentences we obtained a
second test set (which we call the CZ test set)
of 4469 4-tuples from the same domain as our
raw training corpus.
3 Results for the unsupervised
methods
We explored various possibilities to extract PP
disambiguation information from the automat-
ically annotated CZ corpus. We first used it to
gather frequency data on the cooccurrence of
pairs: nouns + prepositions and verbs + prepo-
sitions.
The cooccurrence value is the ratio of the bi-
gram frequency count freq(word, preposition)
divided by the unigram frequency freq(word).
For our purposes word can be the verb V or
the reference noun N1. The ratio describes
the percentage of the cooccurrence of word +
preposition against all occurrences of word. It
is thus a straightforward association measure for
a word pair. The cooccurrence value can be seen
as the attachment probability of the preposition
based on maximum likelihood estimates. We
write:
cooc(W,P ) = freq(W,P )/freq(W )
with W ? {V,N1}. The cooccurrence values
for verb V and noun N1 correspond to the prob-
ability estimates in (Ratnaparkhi, 1998) except
that Ratnaparkhi includes a back-off to the uni-
form distribution for the zero denominator case.
We will add special precautions for this case
in our disambiguation algorithm. The cooccur-
rence values are also very similar to the proba-
bility estimates in (Hindle and Rooth, 1993).
We started by computing the cooccurrence
values over word forms for nouns, preposi-
tions, and verbs based on their part-of-speech
tags. In order to compute the pair frequen-
cies freq(N1, P ), we search the training corpus
for all token pairs in which a noun is immedi-
ately followed by a preposition. The treatment
of verb + preposition cooccurrences is different
from the treatment of N+P pairs since verb and
preposition are seldom adjacent to each other in
a German sentence. On the contrary, they can
be far apart from each other, the only restric-
tion being that they cooccur within the same
clause. We use the clause boundary information
in our training corpus to enforce this restriction.
For computing the cooccurrence values we ac-
cept only verbs and nouns with a occurrence
frequency of more than 10.
With the N+P and V+P cooccurrence values
for word forms we did a first evaluation over
the CZ test set with the following simple dis-
ambiguation algorithm.
if ( cooc(N1,P) && cooc(V,P) ) then
if ( cooc(N1,P) >= cooc(V,P) ) then
noun attachment
else
verb attachment
We found that we can only decide 57% of the
test cases with an accuracy of 71.4% (93.9% cor-
rect noun attachments and 55.0% correct verb
attachments). This shows a striking imbalance
between the noun attachment accuracy and the
verb attachment accuracy. Obviously, the cooc-
currence values favor verb attachment. The
comparison of the verb cooccurrence value and
the noun cooccurrence value too often leads to
verb attachment, and only the clear cases of
noun attachment remain. This points to an in-
herent imbalance between the cooccurrence val-
ues for verbs and nouns. We will flatten out this
imbalance with a noun factor.
The noun factor is supposed to strengthen
the N+P cooccurrence values and thus to at-
tract more noun attachment decisions. What
is the rationale behind the imbalance between
noun cooccurrence value and verb cooccurrence
value? One influence is certainly the well-known
fact that verbs bind their complements stronger
than nouns.
The imbalance between noun cooccurrence
values and verb cooccurrence values can be
quantified by comparing the overall tendency of
nouns to cooccur with a preposition to the over-
all tendency of verbs to cooccur with a prepo-
sition. We compute the overall tendency as the
cooccurrence value of all nouns with all prepo-
sitions.
cooc(all N, all P ) =
?
(N1,P ) freq(N1, P )?
N1 freq(N1)
The computation for the overall verb cooc-
currence tendency is analogous. For example,
in our training corpus we have found 314,028
N+P pairs (tokens) and 1.72 million noun to-
kens. This leads to an overall noun cooccur-
rence value of 0.182. The noun factor (nf) is
then the ratio of the overall verb cooccurrence
tendency divided by the overall noun cooccur-
rence tendency:
nf = cooc(all V, all P )cooc(all N, all P )
In our training corpus this leads to a noun fac-
tor of 0.774/0.182 = 4.25. In the disambigua-
tion algorithm we multiply the noun cooccur-
rence value with this noun factor before compar-
ing the product to the verb cooccurrence value.
This move leads to an improvement of the over-
all attachment accuracy to 81.3% (83.1% cor-
rect noun attachments and 76.9% correct verb
attachments).
We then went on to increase the attachment
coverage, the number of decidable cases, by
using lemmas, decompounding (i.e. using only
the last component of a noun compound), and
proper name classes. These measures increased
the coverage from 57% to 86% of the test cases.
For the remaining test cases we used a thresh-
old comparison if either of the needed cooc-
currence values (cooc(N1, P ) or cooc(V, P )) has
been computed from our training corpus. This
raises the coverage to 90%. While coverage in-
creased, accuracy suffered slightly and at this
stage was at 78.3%.
This is a surprising result given the fact that
we counted all PPs during the training phases.
No disambiguation was attempted so far, we
counted ambiguous and non-ambiguous PPs in
the same manner. We then added this distinc-
tion in the training, counting one point for a
PP in a non-ambiguous position and only half a
point for an ambiguous PP, in this way splitting
the PP?s contribution to verb and noun attach-
ment. This move increased the accuracy rate by
2% (to 80.5%).
So far we have used bigram frequencies over
word pairs, (V, P ) and (N1, P ), to compute
the cooccurrence values. Some of the previous
research (e.g. (Collins and Brooks, 1995) and
(Pantel and Lin, 2000)) has shown that it is ad-
vantageous to include the noun from within the
PP (called N2) in the calculation. But mov-
ing from pair frequencies to triple frequencies
will increase the sparse data problem. Therefore
we computed the pair frequencies and triple fre-
quencies in parallel and used a cascaded disam-
biguation algorithm to exploit the triple cooc-
currence values and the pair cooccurrence val-
ues in sequence.
In analogy to the pair cooccurrence value, the
triple cooccurrence value is computed as:
cooc(W,P,N2) = freq(W )/freq(W,P,N2)
with W ? {V,N1}. With the triple informa-
tion (V, P,N2) we were able to identify support
verb units (such as in Angriff nehmen, unter
Beweis stellen) which are clear cases of verb
attachment. We integrated this and the triple
cooccurrence values into the disambiguation al-
gorithm in the following manner.
if ( support_verb_unit(V,P,N2) )
then verb attachment
elsif (cooc(N1,P,N2) && cooc(V,P,N2))
then if ((cooc(N1,P,N2) * nf)
>= cooc(V,P,N2))
then noun attachment
else verb attachment
elsif (cooc(N1,P) && cooc(V,P)) then
if ((cooc(N1,P) * nf) >= cooc(V,P))
then noun attachment
else verb attachment
elsif (cooc(N1,P) > threshold(N))
then noun attachment
elsif (cooc(V,P) > threshold(V))
then verb attachment
The noun factors for triple comparison and
factor correct incorrect accuracy threshold
noun attachment 5.47; 5.97 2213 424 83.92% 0.020
verb attachment 1077 314 77.43% 0.109
total 3290 738 81.67%
decidable test cases 4028 (of 4469) coverage: 90.13%
Table 1: Attachment accuracy for the CZ test set using cooccurrence values
from unsupervised learning.
decision level number coverage accuracy
support verb units 97 2.2% 100.00%
triple comparison 953 21.3% 84.36%
pair comparison 2813 62.9% 79.95%
cooc(N1, P ) > threshold 74 1.7% 85.13%
cooc(V, P ) > threshold 91 2.0% 84.61%
total 4028 90.1% 81.67%
Table 2: Attachment accuracy for the cooc. method split on decision levels.
pair comparison are computed separately. The
noun factor for pairs is 5.47 and for triples 5.97.
The attachment accuracy is improved to
81.67% by the integration of the triple cooc-
currence values (see table 1). A split on the
decision levels reveals that triple comparison is
4.41% better than pair comparison (see table 2).
The 84.36% for triple comparison demon-
strates what we can expect if we enlarge our cor-
pus and consequently increase the percentage of
test cases that can be disambiguated based on
triple cooccurrence values.
The accuracy of 81.67% reported in table 1 is
computed over the decidable cases. If we force
a default decision (noun attachment) on the re-
maining cases, the overall accuracy is at 79.14%.
4 Results for the supervised
methods
One of the most successful supervised methods
is the Back-off model as introduced by Collins
and Brooks (1995). This model is based on
the idea of using the best information available
and backing off to the next best level when-
ever an information level is missing. For the
PP attachment task this means using the at-
tachment tendency for the complete quadruple
(V,N1, P,N2) if the quadruple has been seen in
the training data. If not, the algorithm backs
off to the attachment tendency of triples. All
triples that contain the preposition are consid-
ered: (V,N1, P ); (V, P,N2); (N1, P,N2). The
triple information is used if any of the triples
has been seen in the training data. Else, the
algorithm backs off to pairs, then to the prepo-
sition alone, and finally to default attachment.
The attachment tendency on each level is
computed as the ratio of the relative frequency
to the absolute frequency. Lacking a large tree-
bank we had to use our test sets in turn as
training data for the supervised learning. In a
first experiment we used the NEGRA test set as
training material and evaluated against the CZ
test set. Both test sets were subjected to the
following restrictions to reduce the sparse data
problem.
1. Verbs, nouns and contracted prepositions
were substituted by their base forms. Com-
pound nouns were substituted by the base
form of their last component.
2. Proper names were substituted by their
name class tag (person, location, com-
pany).
3. Pronouns and numbers (in PP complement
position) were substituted by a pronoun
tag or number tag respectively.
This means we used 5803 NEGRA quadruples
with their given attachment decisions as train-
ing material for the Back-off model. We then
correct incorrect accuracy
noun attachment 2291 677 77.19%
verb attachment 1015 486 67.62%
total 3306 1163 73.98%
decidable test cases 4469 (of 4469) coverage: 100%
Table 3: Attachment accuracy for the CZ test set using supervised learning
over the NEGRA test set based on the Back-off method.
decision level number coverage accuracy
quadruples 8 0.2% 100.00%
triples 329 7.3% 88.75%
pairs 3040 68.0% 75.66%
preposition 1078 24.1% 64.66%
default 14 0.3% 64.29%
total 4469 100.0% 73.98%
Table 4: Attachment accuracy for the Back-off method split on decision levels.
applied the Back-off decision algorithm to de-
termine the attachments for the 4469 test cases
in the CZ test set. Table 3 shows the results.
Due to the default attachment step in the algo-
rithm, the coverage is 100%. The accuracy is
close to 74%, with noun attachment accuracy
being 10% better than verb attachment.
A closer look reveals that the attachment
accuracy for quadruples (100%) and triples
(88.7%) is highly reliable (cf. table 4) but only
7.5% of the test cases can be resolved in this
way. The overall accuracy is most influenced by
the accuracy of the pairs (that account for 68%
of all attachments with an accuracy of 75.66%)
and by the attachment tendency of the preposi-
tion alone which resolves 24.1% of the test cases
but results in a low accuracy of 64.66%.
We suspected that the size of the training cor-
pus has a strong impact on the disambiguation
quality. Since we did not have access to any
larger treebank for German, we used cross vali-
dation on the CZ test set in a third experiment.
We evenly divided this test corpus in 5 parts
of 894 test cases each. We added 4 of these
parts to the NEGRA test set as training ma-
terial. The training material thus consists of
5803 quadruples from the NEGRA test set plus
3576 quadruples from the CZ test set. We then
evaluated against the remaining part of 894 test
cases. We repeated this 5 times with the differ-
ent parts of the CZ test set and summed up the
correct and incorrect attachment decisions.
The result from cross validation is 5% better
than using the NEGRA corpus alone as train-
ing material. This could be due to the enlarged
training set or to the domain overlap of the test
set with part of the training set. We therefore
did another cross validation experiment taking
only the 4 parts of the CZ test set as training
material. If the improved accuracy were a result
of the increased corpus size, we would expect a
worse accuracy for this small training set. But
in fact, training with this small set resulted in
around 77% attachment accuracy. This is bet-
ter than training on the NEGRA test set alne.
This indicates that the domain overlap is the
most influential factor.
5 Intertwining unsupervised and
supervised methods
Now, that we have seen the advantages of the
supervised approaches, but lack a sufficiently
large treebank for training, we suggest combin-
ing the unsupervised and supervised informa-
tion. With the experiments on cooccurrence
values and the Back-off method we have worked
out the quality of the various decision levels
within these approaches, and we will now order
the decision levels according to the reliability of
the information sources.
We reuse the triple and pair cooccurrence val-
ues that we have computed for the experiments
with our unsupervised method. That means
that we will also reuse the respective noun fac-
tors and thresholds. In addition, we use the
NEGRA test set as supervised training corpus
for the Back-off method.
The disambiguation algorithm will now work
in the following manner. It starts off with the
support verb units as level 1, since they are
known to be very reliable. As long as no at-
tachment decision is taken, the algorithm pro-
ceeds to the next level. Next is the application
of supervised quadruples (level 2), followed by
supervised triples (level 3). In section 4 we had
seen that there is a wide gap between the accu-
racy of supervised triples and pairs. We fill this
gap by accessing unsupervised information, i.e.
triple cooccurrence values followed by pair cooc-
currence values (level 4 and 5). Even threshold
comparisons based on one cooccurrence value
are usually more reliable than supervised pairs
and therefore constitute levels 6 and 7. If still no
decision has been reached, the algorithm contin-
ues with supervised pair probabilities followed
by pure preposition probabilities. The left-over
cases are handled by default attachment. Be-
low is the complete disambiguation algorithm
in pseudo-code:
if ( support_verb_unit(V,P,N2) )
then verb attachment
### level 2 ###
elsif ( supervised(V,N1,P,N2) ) then
if ( prob(noun_att | V,N1,P,N2) >= 0.5)
then noun attachment
else verb attachment
### level 3 ###
elsif ( supervised(triple) ) then
if ( prob(noun_att | triple) >= 0.5 )
then noun attachment
else verb attachment
### level 4 ###
elsif ( cooc(N1,P,N2) && cooc(V,P,N2) )
then
if ((cooc(N1,P,N2)*nf) >= cooc(V,P,N2))
then noun attachment
else verb attachment
### level 5 ###
elsif ( cooc(N1,P) && cooc(V,P) ) then
if ((cooc(N1,P) * nf) >= cooc(V,P))
then noun attachment
else verb attachment
### levels 6 / 7 ###
elsif ( cooc(N1,P) > threshold(N) )
then noun attachment
elsif ( cooc(V,P) > threshold(V) )
then verb attachment
### level 8 ###
elsif ( supervised(pair) ) then
if ( prob(noun_attach | pair) >= 0.5)
then noun attachment
else verb attachment
### level 9 ###
elsif ( supervised(P) ) then
if ( prob(noun_attach | P) >= 0.5 )
then noun attachment
else verb attachment
### level 10 ###
else default verb attachment
And indeed, this combination of unsuper-
vised and supervised information leads to an
improved attachment accuracy. For complete
coverage we get an accuracy of 80.98% (cf. ta-
ble 5). This compares favorably to the accuracy
of the cooccurrence experiments plus default at-
tachment (79.14%) reported in section 3 and to
the Back-off results (73.98%) reported in table
3. We obviously succeeded in combining the
best of both worlds into an improved behavior
of the disambiguation algorithm.
The decision levels in table 6 reveal that the
bulk of the attachment decisions still rests with
the cooccurrence values, mostly pair value com-
parisons (59.9%) and triple value comparisons
(18.9%). But the high accuracy of the super-
vised triples and, equally important, the grace-
ful degradation in stepping from threshold com-
parison to supervised pairs (resolving 202 test
cases with 75.74% accuracy) help to improve the
overall attachment accuracy.
We also checked whether the combination of
unsupervised and supervised approaches leads
to an improvement for the NEGRA test set. We
exchanged the corpus for the supervised train-
ing (now the CZ test set) and evaluated over the
NEGRA test set. This results in an accuracy of
71.95% compared to 68.29% for pure applica-
tion of the supervised Back-off method. That
means, the combination leads to an improve-
ment of 3.66% in accuracy.
6 Conclusions
We have shown that unsupervised approaches
to PP attachment disambiguation are about as
factor correct incorrect accuracy threshold
noun attachment 5.47; 5.97 2400 469 83.65% 0.020
verb attachment 1219 381 76.19% 0.109
total 3619 850 80.98%
decidable test cases 4469 (of 4469) coverage: 100%
Table 5: Attachment accuracy for the combination of Back-off and cooccur-
rence values for the CZ test set (based on training over the NEGRA test set).
decision level number coverage accuracy
1 support verb units 97 2.2% 100.00%
2 supervised quadruples 6 0.1% 100.00%
3 supervised triples 269 6.0% 86.62%
4 cooccurrence triples 845 18.9% 84.97%
5 cooccurrence pairs 2677 59.9% 80.39%
6 cooc(N1, P ) > threshold 71 1.6% 85.51%
7 cooc(V, P ) > threshold 81 1.8% 82.72%
8 supervised pairs 202 4.5% 75.74%
9 supervised prepositions 210 4.7% 60.48%
10 default 11 0.3% 54.55%
total 4469 100.0% 80.98%
Table 6: Attachment accuracy split on decision levels for the combination of
Back-off and cooccurrence values.
good as supervised approaches over small man-
ually disambiguated training sets. If only small
manually disambiguated training sets are avail-
able, the intertwined combination of unsuper-
vised and supervised information sources leads
to the best results.
In another vein of this research we have
demonstrated that cooccurrence frequencies ob-
tained through WWW search engines are useful
for PP attachment disambiguation (Volk, 2001).
In the future we want to determine at which de-
cision level such frequencies could be integrated.
References
E. Brill and P. Resnik. 1994. A rule-based
approach to prepositional phrase attachment
disambiguation. In Proceedings of COLING,
pages 1198?1204, Kyoto. ACL.
M. Collins and J. Brooks. 1995. Preposi-
tional phrase attachment through a backed-
off model. In Proc. of the Third Workshop
on Very Large Corpora.
D. Hindle and M. Rooth. 1993. Structural am-
biguity and lexical relations. Computational
Linguistics, 19(1):103?120.
P. Pantel and D. Lin. 2000. An unsupervised
approach to prepositional phrase attachment
using contextually similar words. In Proc. of
ACL-2000, Hongkong.
Adwait Ratnaparkhi. 1998. Statistical models
for unsupervised prepositional phrase attach-
ment. In Proceedings of COLING-ACL-98,
Montreal.
W. Skut, T. Brants, B. Krenn, and H. Uszko-
reit. 1998. A linguistically interpreted cor-
pus of German newspaper text. In Proc. of
ESSLLI-98 Workshop on Recent Advances in
Corpus Annotation, Saarbru?cken.
J. Stetina and M. Nagao. 1997. Corpus
based PP attachment ambiguity resolution
with a semantic dictionary. In J. Zhou and
K. Church, editors, Proc. of the 5th Work-
shop on Very Large Corpora, Beijing and
Hongkong.
Martin Volk. 2001. Exploiting the WWW as
a corpus to resolve PP attachment ambigu-
ities. In Proc. of Corpus Linguistics 2001,
Lancaster, March.
231
232
233
234
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 51?57
Manchester, August 2008
Human Judgements in Parallel Treebank Alignment
Martin Volk and Torsten Marek
University of Zurich
Institute of Computational Linguistics
8050 Zurich, Switzerland
volk@cl.uzh.ch
Yvonne Samuelsson
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
yvonne.samuelsson@ling.su.se
Abstract
We have built a parallel treebank that
includes word and phrase alignment.
The alignment information was manually
checked using a graphical tool that al-
lows the annotator to view a pair of trees
from parallel sentences. We found the
compilation of clear alignment guidelines
to be a difficult task. However, experi-
ments with a group of students have shown
that we are on the right track with up to
89% overlap between the student annota-
tion and our own. At the same time these
experiments have helped us to pin-point
the weaknesses in the guidelines, many of
which concerned unclear rules related to
differences in grammatical forms between
the languages.
1 Introduction
Establishing translation correspondences is a dif-
ficult task. This task is traditionally called align-
ment and is usually performed on the paragraph
level, sentence level and word level. Alignment
answers the question: Which part of a text in lan-
guage L1 corresponds in meaning to which part of
a text in language L2 (under the assumption that
the two texts represent the same meaning in differ-
ent languages). This may mean that one text is the
translation of the other or that both are translations
derived from a third text.
There is considerable interest in automating the
alignment process. Automatic sentence alignment
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of legacy translations helps to fill translation mem-
ories. Automatic word alignment is a crucial step
in training statistical machine translation systems.
Both sentence and word alignment have to deal
with 1:many alignments, i.e. sometimes a sentence
in one language is translated as two or three sen-
tences in the other language.
In other respects sentence alignment and word
alignment are fundamentally different. It is rela-
tively safe to assume the same sentence order in
both languages when computing sentence align-
ment. But such a monotonicity assumption is not
possible for word alignment which needs to allow
for word order differences and thus for crossing
alignments. And while algorithms for sentence
alignment usually focus on length comparisons (in
terms of numbers of characters), word alignment
algorithms use cross-language cooccurrence fre-
quencies as a key feature.
Our work focuses on word alignment and on an
intermediate alignment level which we call phrase
alignment. Phrase alignment encompasses the
alignment from simple noun phrases and preposi-
tional phrases all the way to complex clauses. For
example, on the word alignment level we want to
establish the correspondence of the German ?verb
form plus separated prefix? fing an with the Eng-
lish verb form began. While in phrase alignment
we mark the correspondence of the verb phrases
ihn in den Briefkasten gesteckt and dropped it in
the mail box.
We regard phrase alignment as alignment be-
tween linguistically motivated phrases, in con-
trast to some work in statistical machine trans-
lation where phrase alignment is defined as the
alignment between arbitrary word sequences. Our
phrase alignment is alignment between nodes in
constituent structure trees. See figure 1 for an ex-
51
ample of a tree pair with word and phrase align-
ment.
We believe that such linguistically motivated
phrase alignment provides useful phrase pairs for
example-based machine translation, and provides
interesting insights for translation science and
cross-language comparisons. Phrase alignments
are particularly useful for annotating correspon-
dences of idiomatic or metaphoric language use.
2 The Parallel Treebank
We have built a trilingual parallel treebank in Eng-
lish, German and Swedish. The treebank consists
of around 500 trees from the novel Sophie?s World
and 500 trees from economy texts (an annual re-
port from a bank, a quarterly report from an inter-
national engineering company, and the banana cer-
tification program of the Rainforest Alliance). The
sentences in Sophie?s World are relatively short
(14.8 tokens on average in the English version),
while the sentences in the economy texts are much
longer (24.3 tokens on average; 5 sentences in the
English version have more than 100 tokens).
The treebanks in English and German consist of
constituent structure trees that follow the guide-
lines of existing treebanks, the NEGRA/TIGER
guidelines for German and the Penn treebank
guidelines for English. There were no guidelines
for Swedish constituent structure trees. We have
therefore adapted the German treebank guidelines
for Swedish. Both German trees and Swedish trees
are annotated with flat structures but subsequently
automatically deepened to result in richer and lin-
guistically more plausible tree structures.
When the monolingual treebanks were finished,
we started with the word and phrase alignment.
For this purpose we have developed a special tool
called the Stockholm TreeAligner (Lundborg et
al., 2007) which displays two trees and allows the
user to draw alignment lines by clicking on nodes
and words. This tool is similar to word alignment
tools like ILink (Ahrenberg et al, 2003) or Cairo
(Smith and Jahr, 2000). As far as we know our tool
is unique in that it allows the alignments of lin-
guistically motivated phrases via node alignments
in parallel constituent structure trees (cf. (Samuels-
son and Volk, 2007)).
After having solved the technical issues, the
challenge was to compile precise and comprehen-
sive guidelines to ensure smooth and consistent
alignment decisions. In (Samuelsson and Volk,
2006) we have reported on a first experiment to
evaluate inter-annotator agreement from our align-
ment tasks.
In this paper we report on another recently con-
ducted experiment in which we tried to identify
the weaknesses in our alignment guidelines. We
asked 12 students to alignment 20 tree pairs (Eng-
lish and German) taken from our parallel treebank.
By comparing their alignments to our Gold Stan-
dard and to each other we gained valuable insights
into the difficulty of the alignment task and the
quality of our guidelines.
3 Related Research
Our research on word and phrase alignment is re-
lated to previous work on word alignment as e.g.
in the Blinker project (Melamed, 1998) or in the
UPLUG project (Ahrenberg et al, 2003). Align-
ment work on parallel treebanks is rare. Most
notably there is the Prague Czech-English tree-
bank (Kruijff-Korbayova? et al, 2006) and the
Linko?ping Swedish-English treebank (Ahrenberg,
2007). There has not been much work on the align-
ment of linguistically motivated phrases. Tinsley
et al (2007) and Groves et al (2004) report on
semi-automatic phrase alignment as part of their
research on example-based machine translation.
Considering the fact that the alignment task is
essentially a semantic annotation task, we may
also compare our results to other tasks in seman-
tic corpus annotation. For example, we may con-
sider the methods for resolving annotation con-
flicts and the figures for inter-annotator agreement
in frame-semantic annotation as found in the Ger-
man SALSA project (cf. (Burchardt et al, 2006)).
4 Our Alignment Guidelines
We have compiled alignment guidelines for word
and phrase alignment between annotated syntax
trees. The guidelines consist of general principles,
concrete rules and guiding principles.
The most important general principles are:
1. Align items that can be re-used as units in a
machine translation system.
2. Align as many items (i.e. words and phrases)
as possible.
3. Align as close as possible to the tokens.
The first principle is central to our work. It
defines the general perspective for our alignment.
52
Figure 1: Tree pair German-English with word and phrase alignments.
We do not want to know which part of a sentence
has possibly given rise to which part of the cor-
respondence sentence. Instead our perspective is
on whether a phrase pair is general enough to be
re-used as translation unit in a machine translation
system. For example, we do not want to align die
Verwunderung u?ber das Leben with their astonish-
ment at the world although these two phrases were
certainly triggered by the same phrase in the orig-
inal and both have a similar function in the two
corresponding sentences. These two phrases seen
in isolation are too far apart in meaning to license
their re-use. We are looking for correspondences
like was fu?r eine seltsame Welt and what an ex-
traordinary world which would make for a good
translation in many other contexts.
Some special rules follow from this principle.
For example, we have decided that a pronoun in
one language shall never be aligned with a full
noun in the other, since such a pair is not directly
useful in a machine translation system.
Principles 2 and 3 are more technical. Princi-
ple 2 tells our annotators that alignment should be
exhaustive. We want to re-use as much as pos-
sible from the treebank, so we have to look for
as many alignments as possible. And principle 3
says that in case of doubt the alignment should go
to the node that is closest to the terminals. For
example, our German treebank guidelines require
a multi-word proper noun to first be grouped in
a PN phrase which is a daughter node of a noun
phrase [[Sofie Amundsen]PN ]NP whereas
the English guidelines only require the NP node
[Sophie Amundsen]NP. When we align the
two names, principle 3 tells us to draw the align-
ment line between the German PN node and the
English NP node since the PN node is closer to the
tokens than the German NP node.
Often we are confronted with phrases that are
not exact translation correspondences but approx-
imate translation correspondences. Consider the
phrases mehr als eine Maschine and more than a
piece of hardware. This pair does not represent the
closest possible translation but it represents a pos-
sible translation in many contexts. In a way we
could classify this pair as the ?second-best? trans-
lation. To allow for such distinctions we provide
our annotators with a choice between exact transla-
tion correspondences and approximate correspon-
dences. We also use the term fuzzy correspon-
dence to refer to and give an intuitive picture of
these approximate correspondences. The option to
53
distinguish between different alignment strengths
sounded very attractive at the start but it turned out
to be the source for some headaches later. Where
and how can we draw the line between exact and
fuzzy translation correspondences?
We have formulated some clear-cut rules:
1. If an acronym is to be aligned with a spelled-
out term, it is always an approximate align-
ment. For example, in our economy reports
the English acronym PT stands for Power
Technology and is aligned to the German En-
ergietechnik as a fuzzy correspondence.
2. Proper names shall be aligned as exact align-
ments (even if they are spelled differently
across languages; e.g. Sofie vs. Sophie).
But many open questions persist. Is einer der
ersten Tage im Mai an exact or rather a fuzzy trans-
lation correspondence of early May? We decided
that it is not an exact correspondence. How shall
we handle zu dieser Jahreszeit vs. at this time of
the year where a literal translation would be in this
season? We decided that the former is still an exact
correspondence. These examples illustrate the dif-
ficulties that make us wonder how useful the dis-
tinction between exact and approximate translation
correspondence really is.
Automatically ensuring the overall consistency
of the alignment decisions is a difficult task.
But we have used a tool to ensure the consis-
tency within the exact and approximate alignment
classes. The tool computes the token span for each
alignment and checks if the same tokens pairs have
always received the same alignment type. For ex-
ample, if the phrase pair mit einer blitzschnellen
Bewegung and with a lightning movement is once
annotated as exact alignment, then it should always
be annotated as exact alignment. Figure 1 shows
approximate alignments between the PPs in der
Hand and in her hand. It was classified as approxi-
mate rather than exact alignment since the German
PP lacks the possessive determiner.
Currently our alignment guidelines are 6 pages
long with examples for English-German and
English-Swedish alignments.
5 Experiments with Student Annotators
In order to check the inter-annotator agreement for
the alignment task we performed the following ex-
periment. We gave 20 tree pairs in German and
English to 12 advanced undergraduate students in
a class on ?Machine Translation and Parallel Cor-
pora?. Half of the tree pairs were taken from our
Sophie?s World treebank and the other half from
our Economy treebank. We made sure that there
was one 1:2 sentence alignment in the sample. The
students did not have access to the Gold Standard
alignment.
In class we demonstrated the alignment tool to
the students and we introduced the general align-
ment principles to them. Then the students were
given a copy of the alignment guidelines. We
asked them to do the alignments independently of
each other and to the best of their knowledge ac-
cording to the guidelines.
In our own annotation of the 20 tree pairs (= the
Gold Standard alignment) we have the following
numbers of alignments:
type exact fuzzy total
Sophie part word 75 3 78
phrase 46 12 58
Economy part word 159 19 178
phrase 62 9 71
In the Sophie part of the experiment treebank we
have 78 word-to-word alignments and 58 phrase-
to-phrase alignments. Note that some phrases con-
sist only of one word and thus the same alignment
information is represented twice. We have deliber-
ately kept this redundancy.
The alignments in the Sophie part consist of
125 times 1:1 alignments, 4 times 1:2 alignments
and one 1:3 alignment (wa?re vs. would have been)
when viewed from the German side. There are 3
times 1:2 alignments (e.g. introducing vs. stellte
vor) and no other 1:many alignment when viewed
from the English side.
In the Economy part the picture is similar. The
vast majority are 1:1 alignments. There are 207
times 1:1 alignments and 21 times 1:2 alignments
(many of which are German compound nouns)
when viewed from German. And there are 235
times 1:1 alignments, plus 4 times 1:2 alignments,
plus 2 times 1:3 alignments when viewed from
English (e.g. the Americas was aligned to the three
tokens Nord- und Su?damerika).
The student alignments showed a huge vari-
ety in terms of numbers of alignments. In the
Sophie part they ranged from 125 alignments to
bare 47 alignments (exact alignments and fuzzy
alignments taken together). In the Economy part
the variation was between 259 and 62 alignments.
54
On closer inspection we found that the student
with the lowest numbers works as a translator
and chose to use a very strict criterion of transla-
tion equivalence rather than translation correspon-
dence. Three other students at the end of the list are
not native speakers of either German and English.
We therefore decided to exclude these 4 students
from the following comparison.
The student alignments allow for the investiga-
tion of a number of interesting questions:
1. How did the students? alignments differ from
the Gold Standard?
2. Which were the alignments done by all stu-
dents?
3. Which were the alignments done by single
students only?
4. Which alignments varied most between exact
and fuzzy alignment?
When we compared each student?s alignments
to the Gold Standard alignments, we computed
three figures:
1. How often did the student alignment and the
Gold Standard alignment overlap?
2. How many Gold Standard alignments did the
student miss?
3. How many student alignments were not in the
Gold Standard?
The remaining 8 students reached between 81%
and 48% overlap with the Gold Standard on the
Sophie part, and between 89% and 66% overlap
with the Gold Standard on the Economy texts. This
can be regarded as their recall values if we assume
that the Gold Standard represents the correct align-
ments. These same 8 students additionally had
between 2 and 22 own alignments in the Sophie
part and between 12 and 55 own alignments in the
Economy part.
So the interesting question is: What kind of
alignments have they missed, and which were
the additional own alignments that they suggested
(alignments that are not in the gold standard)? We
first checked the students with the highest numbers
of own alignments. We found that some of these
alignments were due to the fact that students had
ignored the rule to align as close to the tokens as
possible (principle 3 above).
Another reason was that students sometimes
aligned a word (or some words) with a node.
For example, one student had aligned the word
natu?rlich to the phrase of course instead of to the
word sequence of course. Our alignment tool al-
lows that, but the alignment guidelines discour-
age such alignments. There might be exceptional
cases where a word-to-phrase alignment is neces-
sary in order to keep valuable information, but in
general we try to stick to word-to-word and phrase-
to-phrase alignments.
Another discrepancy occurred when the stu-
dents aligned a German verb group with a single
verb form in English (e.g. ist zuru?ckzufu?hren vs.
reflecting). We have decided to only align the full
verb to the full verb (independent of the inflection).
This means that we align only zuru?ckzufu?hren to
reflecting in this example.
The uncertainties on how to deal with different
grammatical forms led to the most discrepancies.
Shall we align the definite NP die Umsa?tze with
the indefinite NP revenues since it is much more
common to drop the article in an English plural NP
than in German? Shall we align a German genitive
NP with an of-PP in English (der beiden Divisio-
nen vs. of the two divisions)? We have decided to
give priority to form over function and thus to align
the NP der beiden Divisionen with the NP the two
divisions. But of course this choice is debatable.
When we compute the intersection of the align-
ments done by all students (ignoring the difference
between exact and fuzzy alignments), we find that
about 50% of the alignments done by the student
with the smallest number of alignments is shared
by all other students. All of the alignments in the
intersection are in our Gold Standard file. This in-
dicates that there is a core of alignments that are
obvious and uncontroversial. Most of them are
word alignments.
When we compute the union of the alignments
done by all students (again ignoring the difference
between exact and fuzzy alignments), we find that
the number of alignments in the union is 40% to
50% higher than the number of alignments done by
the student with the highest number of alignments.
It is also about 40% to 50% higher than the number
of alignments in the Gold Standard. This means
that there is considerable deviation from the Gold
Standard.
Comparing the union of the students? align-
ments to the Gold Standard points to some weak-
55
nesses of the guidelines. For example, one align-
ment in the Gold Standard that was missed by all
students concerns the alignment of a German pro-
noun (wenn sie die Hand ausstreckte) to an empty
token in English (herself shaking hands). Our
guidelines recommend to align such cases as fuzzy
alignments, but of course it is difficult to determine
that the empty token really corresponds to the Ger-
man word.
Other discrepancies concern cases of differing
grammatical forms, e.g. a German definite singu-
lar noun phrase (die Hand) that was aligned to an
English plural noun phrase (Hands) in the Gold
Standard but missed by all students. Finally there
are a few cases where obvious noun phrase cor-
respondences were simply overlooked by all stu-
dents (sich - herself ) although the tokens them-
selves were aligned. Such cases should be handled
by an automated process in the alignment tool that
projects from aligned tokens to their mother nodes
(in particular in cases of single token phrases).
We also investigated how many exact align-
ments and how many fuzzy alignments the stu-
dents had used. The following table gives the fig-
ures.
exact fuzzy overlap total
Sophie part 152 106 69 189
Economy part 296 188 119 366
The alignments done by all students resulted in a
union set of 189 alignments for the Sophie part and
366 alignments for the Economy part. The align-
ments in the Sophie part consisted of 152 exact
alignments and 106 fuzzy alignments. This means
that 69 alignments were marked as both exact and
fuzzy. In other words, in 69 cases at least one stu-
dent has marked an alignment as fuzzy while at
least one other student has marked the same align-
ment as good. So there is still considerable con-
fusion amongst the annotators on how to decide
between exact and fuzzy alignments. And in case
of doubt many students have decided in favor of
fuzzy alignments.
6 Conclusions
We have shown the difficulties in creating cross-
language word and phrase alignments. Experi-
ments with a group of students have helped to iden-
tify the weaknesses in our alignment guidelines
and in our Gold Standard alignment. We have re-
alized that the guidelines need to contain a host
of fine-grained alignment rules and examples that
will clarify critical cases.
In order to evaluate a set of alignment experi-
ments with groups of annotators it is important to
have good visualization tools to present the results.
We have worked with Perl scripts for the compar-
ison and with our own TreeAligner tool for the vi-
sualization. For example we have used two colors
to visualize a student?s alignment overlap with the
Gold Standard in one color and his own alignments
(that are not in the Gold Standard) in another color.
In order to visualize the agreements of the whole
group it would be desirable to have the option to in-
crease the alignment line width in proportion to the
number of annotators that have chosen a particular
alignment link. This would give an intuitive im-
pression of strong alignment links and weak align-
ment links.
Another option for future extension of this work
is an even more elaborate classification of the
alignment links. (Hansen-Schirra et al, 2006) have
demonstrated how a fine-grained distinction be-
tween different alignment types could look like.
Annotating such a corpus will be labor-intensive
but provide for a wealth of cross-language obser-
vations.
References
Ahrenberg, Lars, Magnus Merkel, and Michael Petter-
stedt. 2003. Interactive word alignment for language
engineering. In Proc. of EACL-2003, Budapest.
Ahrenberg, Lars. 2007. LinES: An English-Swedish
parallel treebank. In Proc. of Nodalida, Tartu.
Burchardt, A., K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. The SALSA corpus: A Ger-
man corpus resource for lexical semantics. In Pro-
ceedings of LREC 2006, pages 969?974, Genoa.
Groves, Declan, Mary Hearne, and Andy Way. 2004.
Robust sub-sentential alignment of phrase-structure
trees. In Proceedings of Coling 2004, pages 1072?
1078, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Hansen-Schirra, Silvia, Stella Neumann, and Mihaela
Vela. 2006. Multi-dimensional annotation and
alignment in an English-German translation corpus.
In Proceedings of the EACL Workshop on Multidi-
mensional Markup in Natural Language Processing
(NLPXML-2006), pages 35? 42, Trento.
Kruijff-Korbayova?, Ivana, Kla?ra Chva?talova?, and Oana
Postolache. 2006. Annotation guidelines for the
Czech-English word alignment. In Proceedings of
LREC, Genova.
56
Lundborg, Joakim, Torsten Marek, Mae?l Mettler,
and Martin Volk. 2007. Using the Stockholm
TreeAligner. In Proc. of The 6th Workshop on Tree-
banks and Linguistic Theories, Bergen, December.
Melamed, Dan. 1998. Manual annotation of transla-
tional equivalence: The blinker project. Technical
Report 98-06, IRCS, Philadelphia PA.
Samuelsson, Yvonne and Martin Volk. 2006. Phrase
alignment in parallel treebanks. In Hajic, Jan and
Joakim Nivre, editors, Proc. of the Fifth Workshop on
Treebanks and Linguistic Theories, pages 91?102,
Prague, December.
Samuelsson, Yvonne and Martin Volk. 2007. Align-
ment tools for parallel treebanks. In Proceedings of
GLDV Fru?hjahrstagung 2007.
Smith, Noah A. and Michael E. Jahr. 2000. Cairo:
An alignment visualization tool. In Proc. of LREC-
2000, Athens.
Tinsley, John, Ventsislav Zhechev, Mary Hearne, and
Andy Way. 2007. Robust language pair-independent
sub-tree alignment. In Machine Translation Summit
XI Proceedings, Copenhagen.
57
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 192?196,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Combining Parallel Treebanks and Geo-Tagging
Martin Volk, Anne Go?hring, Torsten Marek
University of Zurich, Institute of Computational Linguistics
volk@cl.uzh.ch
Abstract
This paper describes a new kind of seman-
tic annotation in parallel treebanks. We
build French-German parallel treebanks of
mountaineering reports, a text genre that
abounds with geographical names which
we classify and ground with reference to
a large gazetteer of Swiss toponyms. We
discuss the challenges in obtaining a high
recall and precision in automatic ground-
ing, and sketch how we represent the
grounding information in our treebank.
1 Introduction
Treebanks have become valuable resources in nat-
ural language processing as training corpora for
natural language parsers, as repositories for lin-
guistic research, or as evaluation corpora for dif-
ferent NLP systems. We define a treebank as
a collection of syntactically annotated sentences.
The annotation can vary from constituent to de-
pendency or tecto-grammatical structures. The
term treebank is mostly used to denote manually
checked collections, but recently it has been ex-
tended to also refer to automatically parsed cor-
pora.
We have built manually checked treebanks for
various text genres (see section 3): economy texts,
a popular science philosophy novel, and technical
user manuals. We are now entering a new genre,
mountaineering reports, with the goal to link tex-
tual to spatial information. We build French and
German treebanks of translated texts from the
Swiss Alpine Club. This genre contains a multi-
tude of geographical names (e.g. mountains and
valleys, glaciers and rivers). Therefore we need to
include the identification and grounding of these
toponyms as part of the annotation process.
In this paper we first describe our corpus of
alpine texts, then our work on creating paral-
lel treebanks which includes aligning the parallel
trees on word and phrase level. We sketch the dif-
ficulties in disambiguating the toponyms and de-
scribe our integration of the toponym identifiers as
a special kind of semantic annotation in the tree-
bank.
2 Our Text+Berg Corpus
In our project Text+Berg1 we digitize alpine her-
itage literature from various European countries.
Currently our group digitizes all yearbooks of the
Swiss Alpine Club (SAC) from 1864 until today.
Each yearbook consists of 300 to 600 pages and
contains reports on mountain expeditions, culture
of mountain peoples, as well as the flora, fauna
and geology of the mountains.
The corpus preparation presented interesting
challenges in automatic OCR correction, language
identification, and text structure recognition which
we have described in (Volk et al, 2010).
As of March 2010 we have scanned and OCR-
converted 142 books from 1864 to 1982, corre-
sponding to nearly 70,000 pages. This resulted in
a multilingual corpus of 6101 articles in German,
2659 in French, 155 in Italian, 13 in Romansch,
and 3 in Swiss-German. The parallel part of our
corpus currently contains 701 translated articles
amounting to 2.6 million tokens in French and 2.3
million tokens in German.
3 Parallel Treebanks
In recent years the combined research on tree-
banks and parallel corpora has led to parallel tree-
banks. We have built a parallel treebank (En-
glish, German, Swedish) which contains 1500 sen-
tences in three languages: 500 sentences each
from Jostein Gaarder?s novel ?Sophie?s World?,
from economy texts (e.g. business reports from
mechanical engineering company ABB and from
the bank SEB), and from a technical manual with
1See www.textberg.ch.
192
usage instructions for a DVD player (Go?hring,
2009).
We have annotated the English sentences
according to the well-established Penn Tree-
bank guidelines. For German we followed the
TIGER annotation guidelines, and we adapted
these guidelines also for Swedish (see (Volk
and Samuelsson, 2004)). For French treebank-
ing we are looking for inspiration from the Le
Monde treebank (Abeille? et al, 2003) and from
L?Arboratoire (Bick, 2010). The Le Monde tree-
bank is a constituent structure treebank partially
annotated with functional labels. L?Arboratoire is
based on constraint grammar analysis but can also
output constituent trees.
3.1 Our Tree Alignment Tool
After finishing the monolingual trees we aligned
them on the word level and phrase level. For
this purpose we have developed the TreeAligner
(Lundborg et al, 2007). This program comes with
a graphical user interface to insert or modify align-
ments between pairs of syntax trees.2
The TreeAligner displays tree pairs with the
trees in mirror orientation (one top-up and one top-
down). This has the advantage that the alignment
lines cross fewer parts of the lower tree. Figure 1
shows an example of a tree pair with alignment
lines. The lines denote translation equivalence.
Both trees are constituent structure trees, but the
edge labels contain function labels (like subject,
object, attribute) which can be used to easily con-
vert the trees to dependency structures (cf. (Marek
et al, 2009)).
Recently we have extended the TreeAligner?s
functionality from being solely an alignment tool
to also being a powerful search tool over parallel
treebanks (Volk et al, 2007; Marek et al, 2008).
This enables our annotators to improve the align-
ment quality by cross-checking previous align-
ments. This functionality makes the TreeAligner
also attractive to a wider user base (e.g. linguists,
translation scientists) who are interested in search-
ing rather than building parallel treebanks.
3.2 Similar Treebanking Projects
Parallel treebanks have evolved into an active re-
search field in the last decade. Cmejrek et al
2The TreeAligner has been implemented in Python by
Joakim Lundborg and Torsten Marek and is freely available
at http://kitt.cl.uzh.ch/kitt/treealigner.
(2003) have built a parallel treebank for the spe-
cific purpose of machine translation, the Czech-
English Penn Treebank with tecto-grammatical
dependency trees. Other parallel treebank projects
include Croco (Hansen-Schirra et al, 2006) which
is aimed at building a English-German tree-
bank for translation studies, LinES an English-
Swedish parallel treebank (Ahrenberg, 2007), and
the English-French HomeCentre treebank (Hearne
and Way, 2006), a hand-crafted parallel treebank
consisting of 810 sentence pairs from a Xerox
printer manual.
Some researchers have tried to exploit parallel
treebanks for example-based or statistical machine
translation (Tinsley et al, 2009). Since manually
created treebanks are too small for this purpose,
various researchers have worked on automatically
parsing and aligning parallel treebanks. Zhechev
(2009) and Tiedemann and Kotze? (2009) have
presented methods for automatic cross-language
phrase alignment.
There have been various attempts to enrich
treebanks with semantic information. For exam-
ple, the Propbank project has assigned semantic
roles to Penn treebank sentences (Kingsbury et al,
2002). Likewise the SALSA project has added
frame-semantic annotations on top of syntax trees
from the German TIGER treebank (Burchardt et
al., 2006). Frame-semantics was extended to par-
allel treebanks by (Pado?, 2007) and (Volk and
Samuelsson, 2007). To our knowledge a treebank
with grounded toponym information has not been
created yet.
4 Geo-Tagging
Named entity recognition is an important aspect of
information extraction. But it has also been recog-
nized as important for the access to heritage data.
In a previous project we have investigated meth-
ods for named entity recognition in newspaper
texts (Volk and Clematide, 2001). In that work
we had only distinguished two types of geograph-
ical names: city names and country names. This
was sufficient for texts that dealt mostly with facts
like a company being located in a certain coun-
try or having started business in a certain city.
In contrast to that, our alpine text corpus deals
with much more fine-grained location informa-
tion: mountains and valleys, glaciers and climb-
ing routes, cabins and hotels, rivers and lakes. In
fact the description of movements (e.g. in moun-
193
MonteNE RosaNEderART RundtourNN umAPPR denARTDasART klassischeADJA Endst?ckNN desART Schwarzberg-WeisstorsNEderART GletscherpassNNausAPZR istVAFINvonAPPR ZermattNN .$.
.PCT_SestV leD_def colN_C glaciaireA_qual deP Schwarzberg-WeisstorN_PLaD_def derni?reA_qual ?tapeN_C ?P partirV deP ZermattN_PduP tourN_C duP MontN_P RoseN_P
HDNPPNC PNCPN NK HDNP
NK HD AGNPHD HDNKPPNK HDNP
HD NKPP
NK HD MNRMNRNP
NK NK HD AGNP
HD SPSP S
HD
HD
HD
HDNP
PPMONP
SP
HD
PHD
HDNP
PP
MO
HD
HD
HD
PNC PNCNP
PPMONP
PPMO NPSP S
Figure 1: German-French tree pair with alignments in the TreeAligner.
tains) requires all kinds of intricate references to
positions and directions in three dimensions.
In order to recognize the geographical names in
our corpus we have acquired a large list of Swiss
toponyms.
4.1 The SwissTopo Name List
The Swiss Federal Office of Topography (www.
swisstopo.ch) maintains a database of all
names that appear on its topographical maps. We
have obtained a copy of this database which con-
tains 156,755 names in 61 categories. Categories
include settlements (10 categories ranging from
large cities to single houses), bodies of water (13
categories from major rivers to ponds and wells),
mountains (7 categories from mountain ranges to
small hills), valleys, mountain passes, streets and
man-made facilities (e.g. bridges and tunnels), and
single objects like hotels, mountain cabins, monu-
ments etc. Some objects are subclassified accord-
ing to size. For example, cities are subdivided into
main, large, middle and small cities according to
their number of inhabitants.
Every name is listed in the SwissTopo database
with its coordinates, its altitude (if applicable and
available), the administrative unit to which it be-
longs (usually the name of a nearby town), and the
canton.
4.2 A First Experiment: Finding Mountain
Names
We selected an article from the SAC yearbook
of 1900 to check the precision and recall of au-
tomatically identifying mountain names based on
the SwissTopo name list. The article is titled
?Bergfahrten im Clubgebiet (von Dr. A. Walker)?.
It is an article in German with a wealth of French
mountain names since the author reports about his
hikes in the French speaking part of Switzerland.
We took the article after OCR without any further
manual correction. After our tokenization (incl.
the splitting of punctuation symbols) it consisted
of 9380 tokens.
We used the SwissTopo mountain names classi-
fied as ?Massiv, HGipfel, GGipfel, and KGipfel?
i.e. the 4 highest mountain classes. They consist of
5588 mountain names. This leads to a recall of 54
mountain names (20 different mountain names) at
194
the expense of erroneously marking 6 nouns Gen-
darm, Haupt, Kamm, Stand, Stein, Turm as moun-
tain names.
How many mountain names have we missed
to identify? A manual inspection showed that
there are another 92 mountain names (35 differ-
ent mountain names) missing. So recall of the
naive exact matching is below 40% despite the
large gazetteer. We have reported on a number of
reasons for missed names in (Volk et al, 2010).
We found that spelling variations and partial co-
references account for the majority of recall prob-
lems. In addition we need to disambiguate be-
tween name-noun and name-name homographs.
This leaves the issue on how to represent the geo-
tagging information in our treebank.
5 Geonames in Treebanks
Named entity classification can be divided into
name recognition, disambiguation and grounding.
The first two steps are applicable to all kinds of
names. The final step of grounding the names is
different depending on the name types. A per-
son name may be grounded by refering to the per-
son?s Wikipedia page. The same could be done
for a geographical name. The obvious disadvan-
tage are changing URLs and missing Wikipedia
pages. The goal of grounding must be to link
the name to the most stable and most reliable
?ground?. Therefore toponyms are often linked to
their geographical coordinates. We have chosen to
link the toponyms from our alpine texts to unique
identifiers in the SwissTopo database. This works
well for Swiss names and particularly well for par-
allel French-German sentence pairs. The cross-
language alignment assures that the names are rec-
ognized in either language and the classification
information can then automatically be transfered
to the other language.
In our example in figure 1, the mountain name
?Monte Rosa? is listed in SwissTopo with its al-
titude (4633 m) and its location close to Zermatt.
Since ?Zermatt? itself occurs in the sentence, this
is strong evidence that we have identified the cor-
rect mountain, and we will attach its SwissTopo
identification number in our treebank. Technically
this means we add a reference to the gazetteer and
to the identifier within the gazetteer into the XML
representation of the linguistic object.
In our German example sentence ?Monte Rosa?
is annotated as a proper name (PN). This occur-
rence is phrase 502 in sentence 311 of our tree-
bank. The grounding id (g id) is taken from Swis-
sTopo which then allows us to access the geo-
graphical coordinates, the altitude and neighbor-
hood information.
<nt id="s311_502"
cat="PN"
g_source="SwissTopo"
g_id="7355873" >
Instead of integrating the grounding pointers di-
rectly in the XML file of the treebank, it is possible
to use stand-off annotation by connecting the iden-
tifier of the geo-name with the identifier from the
gazetteer in a separate file.
The alignments in our parallel treebank lead
to the advantage that the grounding information
needs to be saved only once. In our example, the
corresponding mountain name ?Mont Rose? in the
French translation is listed in SwissTopo only as a
building in the municipality of Genthod in the can-
ton Geneva. Since we have strong evidence from
the German sentence, we can rule out this option.
Zermatt itself occurs in both the French and
German sentences in our example. It is listed in
SwissTopo with its altitude (1616 m) and classi-
fied as mid-sized municipality (2000 to 10,000 in-
habitants). Zermatt is a unique name in SwissTopo
and therefore is grounded via its SwissTopo identi-
fier. Likewise we ground ?Schwarzberg Weisstor?
(spelled without hyphen in SwissTopo) which is
listed as foot pass in the municipality of Saas-
Almagell. In case of doubt we could verify
that Saas-Almagell and Zermatt are neighboring
towns, which indeed they are.
6 Conclusions
Grounding toponyms in parallel treebanks repre-
sents a new kind of semantic annotation. We have
sketched the issues in automatic toponym classi-
fication and disambiguation. We are working on
a French-German parallel treebank of alpine texts
which contain a multitude of toponyms that de-
scribe way-points on climbing or hiking routes but
also panorama views. We are interested in iden-
tifying all toponyms in order to enable treebank
access via geographical maps. In the future we
want to automatically compute and display climb-
ing routes from the textual descriptions. The an-
notated treebank will then serve as a gold standard
for the evaluation of the automatic geo-tagging.
195
References
Anne Abeille?, Lionel Cle?ment, and Francois Toussenel.
2003. Building a Treebank for French. In Anne
Abeille?, editor, Building and Using Parsed Corpora,
volume 20 of Text, Speech and Language Technol-
ogy, chapter 10, pages 165?187. Kluwer, Dordrecht.
Lars Ahrenberg. 2007. LinES: An English-Swedish
parallel treebank. In Proc. of Nodalida, Tartu.
Eckhard Bick. 2010. FrAG, a hybrid constraint gram-
mar parser for French. In Proceedings of LREC,
Malta.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. The SALSA corpus: A Ger-
man corpus resource for lexical semantics. In Pro-
ceedings of LREC 2006, pages 969?974, Genoa.
Martin Cmejrek, Jan Curin, and Jiri Havelka. 2003.
Treebanks in machine translation. In Proc. Of the
2nd Workshop on Treebanks and Linguistic Theo-
ries, pages 209?212, Va?xjo?.
Anne Go?hring. 2009. Spanish expansion of a parallel
treebank. Lizentiatsarbeit, University of Zurich.
Silvia Hansen-Schirra, Stella Neumann, and Mihaela
Vela. 2006. Multi-dimensional annotation and
alignment in an English-German translation corpus.
In Proceedings of the EACL Workshop on Multidi-
mensional Markup in Natural Language Processing
(NLPXML-2006), pages 35? 42, Trento.
Mary Hearne and Andy Way. 2006. Disambiguation
strategies for data-oriented translation. In Proceed-
ings of the 11th Conference of the European Asso-
ciation for Machine Translation (EAMT), pages 59?
68, Oslo.
P. Kingsbury, M. Palmer, and M. Marcus. 2002.
Adding semantic annotation to the Penn TreeBank.
In Proceedings of the Human Language Technology
Conference (HLT?02), San Diego.
Joakim Lundborg, Torsten Marek, Mae?l Mettler, and
Martin Volk. 2007. Using the Stockholm
TreeAligner. In Proc. of The 6th Workshop on Tree-
banks and Linguistic Theories, Bergen, December.
Torsten Marek, Joakim Lundborg, and Martin Volk.
2008. Extending the TIGER query language with
universal quantification. In Proceeding of KON-
VENS, pages 3?14, Berlin.
Torsten Marek, Gerold Schneider, and Martin Volk.
2009. A framework for constituent-dependency
conversion. In Proceedings of the 8th Workshop on
Treebanks and Linguistic Theories, Milano, Decem-
ber.
Sebastian Pado?. 2007. Cross-Lingual Annotation
Projection Models for Role-Semantic Information.
Ph.D. thesis, Saarland University, Saarbru?cken.
Jo?rg Tiedemann and Gideon Kotze?. 2009. Building
a large machine-aligned parallel treebank. In Pro-
ceedings of the 8th International Workshop on Tree-
banks and Linguistic Theories, pages 197?208, Mi-
lano.
John Tinsley, Mary Hearne, and Andy Way. 2009.
Exploiting parallel treebanks to improve phrase-
based statistical machine translation. In Computa-
tional Linguistics and Intelligent Text Processing.
Springer.
Martin Volk and Simon Clematide. 2001. Learn-
filter-apply-forget. Mixed approaches to named en-
tity recognition. In Ana M. Moreno and Reind P.
van de Riet, editors, Applications of Natural Lan-
guage for Information Systems. Proc. of 6th Inter-
national Workshop NLDB?01, volume P-3 of Lec-
ture Notes in Informatics (LNI) - Proceedings, pages
153?163, Madrid.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proc. of Work-
shop on Linguistically Interpreted Corpora (LINC)
at COLING, Geneva.
Martin Volk and Yvonne Samuelsson. 2007. Frame-
semantic annotation on a parallel treebank. In Proc.
of Nodalida Workshop on Building Frame Semantics
Resources for Scandinavian and Baltic Languages,
Tartu.
Martin Volk, Joakim Lundborg, and Mae?l Mettler.
2007. A search tool for parallel treebanks. In Proc.
of Workshop on Linguistic Annotation at ACL, pages
85?92, Prague.
Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya
Bangerter, Lenz Furrer, and Beni Ruef. 2010. Chal-
lenges in building a multilingual alpine heritage cor-
pus. In Proceedings of LREC, Malta.
Ventsislav Zhechev. 2009. Automatic Generation of
Parallel Treebanks. An Efficient Unsupervised Sys-
tem. Ph.D. thesis, School of Computing at Dublin
City University.
196
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 112?120,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Mining for Domain-specific Parallel Text from Wikipedia
Magdalena Plamada?, Martin Volk
Institute of Computational Linguistics, University of Zurich
Binzmu?hlestrasse 14, 8050 Zurich
{plamada, volk}@cl.uzh.ch
Abstract
Previous attempts in extracting parallel
data from Wikipedia were restricted by the
monotonicity constraint of the alignment
algorithm used for matching possible can-
didates. This paper proposes a method for
exploiting Wikipedia articles without wor-
rying about the position of the sentences in
the text. The algorithm ranks the candidate
sentence pairs by means of a customized
metric, which combines different similar-
ity criteria. Moreover, we limit the search
space to a specific topical domain, since
our final goal is to use the extracted data
in a domain-specific Statistical Machine
Translation (SMT) setting. The precision
estimates show that the extracted sentence
pairs are clearly semantically equivalent.
The SMT experiments, however, show that
the extracted data is not refined enough to
improve a strong in-domain SMT system.
Nevertheless, it is good enough to boost
the performance of an out-of-domain sys-
tem trained on sizable amounts of data.
1 Introduction
A high-quality Statistical Machine Translation
(SMT) system can only be built with large quan-
tities of parallel texts. Moreover, systems special-
ized in specific domains require in-domain train-
ing data. A well-known problem of SMT systems
is that existing parallel corpora cover a small per-
centage of the possible language pairs and very
few domains. We therefore need a language-
independent approach for discovering parallel sen-
tences in the available multilingual resources.
This idea was explored intensively in the last
decade with different text sources, generically
called comparable corpora, such as news feeds,
encyclopedias or even the entire Web. The first
approaches focused merely on news corpora and
were either based on IBM alignment models (Zhao
and Vogel, 2002; Fung and Cheung, 2004) or em-
ploying machine learning techniques (Munteanu
and Marcu, 2005; Abdul Rauf and Schwenk,
2011).
The multilingual Wikipedia is another source
of comparable texts, not yet thoroughly explored.
Adafre and de Rijke (2006) describe two meth-
ods for identifying parallel sentences across it
based on monolingual sentence similarity (MT
and respectively, lexicon based). Fung et al
(2010) approach the problem by combining recall-
and precision-oriented methods for sentence align-
ment, such as the DK-vec algorithm or algorithms
based on cosine similarities. Both approaches
have achieved good results in terms of precision
and recall.
However, we are interested in real applica-
tion scenarios, such as SMT systems. The fol-
lowing approaches report significant performance
improvements when using the extracted data as
training material for SMT: Smith et al (2010)
use a maximum entropy-based classifier with
various feature functions (e.g. alignment cover-
age, word fertility, translation probability, distor-
tion). S?tefa?nescu et al (2012) propose an algo-
rithm based on cross-lingual information retrieval,
which also considers similarity features equivalent
to the ones mentioned in the previous paper.
The presented approaches extract general pur-
pose sentences, but we are interested in a spe-
cific topical domain. We have previously tackled
the problem (Plamada and Volk, 2012) and en-
countered two major bottlenecks: the alignment
algorithm for matching possible candidates and
the similarity metric used to compare them. To
our knowledge, existing sentence alignment al-
gorithms (including the one we have employed
in the first place) have a monotonic order con-
straint, meaning that crossing alignments are not
112
allowed. But this phenomenon occurs often in
Wikipedia, because its articles in different lan-
guages are edited independently, without respect-
ing any guidelines. Moreover, the string-based
comparison metric proved to be unreliable for
identifying parallel sentences.
In this paper we propose an improved approach
for selecting parallel sentences in Wikipedia arti-
cles which considers all possible sentence pairs,
regardless of their position in the text. The selec-
tion will be made by means of a more informed
similarity metric, which rates different aspects
concerning the correspondences between two sen-
tences. Although the approach is language and
domain-independent, the present paper reports re-
sults obtained through querying the German and
French Wikipedia for Alpine texts (i.e. moun-
taineering reports, hiking recommendations, arti-
cles on the biology and the geology of mountain-
ous regions). Moreover, we report preliminary re-
sults regarding the use of the extracted corpus for
SMT training.
2 Finding candidate articles
The general architecture of our parallel sentence
extraction process is shown in Figure 1. We
applied the approach only to the language pair
German-French, as these are the languages we
have expertise in. In the project Domain-specific
Statistical Machine Translation1 we have built an
SMT system for the Alpine domain and for this
language pair. The training data comes from the
Text+Berg corpus2, which contains the digitized
publications of the Swiss Alpine Club (SAC) from
1864 until 2011, in German and French. This
SMT system will generate the automatic transla-
tions required in the extraction process.
The input consists of German and French
Wikipedia dumps3, available in the MediaWiki
format 4. Therefore our workflow requires a pre-
processing step, where the MediaWiki contents
are transformed to XML and then to raw text.
Preprocessing is based on existing tools, such as
WikiPrep5, but further customization is needed in
order to correctly convert localized MediaWiki el-
ements (namespaces, templates, date and number
formats etc.). We then identify Wikipedia articles
1http://www.cl.uzh.ch/research en.html
2See www.textberg.ch
3Accessed in September 2011
4http://www.mediawiki.org/wiki/MediaWiki
5http://sourceforge.net/projects/wikiprep/
Figure 1: The extraction workflow
available in both languages by means of the inter-
language links provided by Wikipedia itself. This
reliable information is a good basis for the extrac-
tion workflow, since we do not have to worry about
the document alignment.
Upon completion of this step, we have extracted
a bilingual corpus of approximately 400 000 ar-
ticles per language. The corpus is subsequently
used for information retrieval (IR) queries aiming
to identify the articles belonging to the Alpine do-
main. The input queries contain the 100 most fre-
quent mountaineering keywords in the Text+Berg
corpus (e.g. Alp, Gipfel, Berg, Route in German
and montagne, sommet, voie, cabane in French).
This filter reduces the search space to 40 000
articles. Although we have refined our search
terms by discarding the ones occurring frequently
in other text types (e.g. meter, day, year, end),
we were not able to avoid a small percentage of
false positives. Once we extract the Alpine com-
parable corpus, we proceed to the extraction of
113
parallel sentences, which will be thoroughly dis-
cussed in the following section. See (Plamada and
Volk, 2012) for more details about the extraction
pipeline.
3 Finding parallel segments in Wikipedia
articles
The analysis of our previous results brought into
attention many ?parallel? sentence pairs of dif-
ferent lengths, meaning that the shared trans-
lated content does not span over the whole sen-
tence. As an example, consider the following sen-
tences which have been retrieved by the extraction
pipeline. Although they both contain information
about the valleys connected by the Turini pass, the
German sentence contains a fragment about its po-
sition, which has not been translated into French.
DE: Der Pass liegt in der a?usseren, besiedelten
Zone des Nationalpark Mercantour und stellt den
U?bergang zwischen dem Tal der Be?ve?ra und dem
Tal der Ve?subie dar.
FR: Le col de Turini relie la valle?e de la Ve?subie
a` la valle?e de la Be?ve?ra.
If this sentence pair would be used for MT
training, it would most probably confuse the sys-
tem, because noisy word alignments are to be ex-
pected. Our solution to this problem is to split
the sentences into smaller entities (e.g. clauses)
and then to find the alignments on this granular-
ity level. The clause boundary detection is per-
formed independently for German and French, re-
spectively, following the approach developed by
Volk (2001). The general idea is to split the sen-
tences into clauses containing a single full verb.
Our alignment algorithm, unlike previous ap-
proaches, ignores the position of the clauses in the
texts. Although Wikipedia articles are divided into
sections, their structure is not synchronized across
the language variants, since articles are edited in-
dependently. We have encountered, for example,
cases where one section in the French article was
included in the general introduction of the Ger-
man article. If we would have considered sec-
tions boundaries as anchor points, we would have
missed useful clause pairs. We therefore decided
to use an exhaustive matching algorithm, in order
to cover all possible combinations.
For the sake of simplicity, we reduce the prob-
lem to a monolingual alignment task by using an
intermediary machine translation of the source ar-
ticle. We decided that German articles should al-
ways be considered the source because we expect
a better automatic translation quality from German
to French. The translation is performed by our in-
house SMT system trained on Alpine texts. The
algorithm generates all possible clause pairs be-
tween the automatic translation and the targeted
article and computes for each of them a similarity
score. Subsequently it reduces the search space by
keeping only the 3 best-scoring alignment candi-
dates for each clause. Finally the algorithm returns
the alignment pair which maximizes the similarity
score and complies with the injectivity constraint.
In the end we filter the results by allowing only
clause pairs above the set threshold.
We defined our similarity measure as a
weighted sum of feature functions, which returns
values in the range [0,1]. The similarity score
models two comparison criteria:
? METEOR score
We used the METEOR similarity metric be-
cause, unlike other string-based metrics (e.g.
BLEU (Papineni et al, 2002)), it considers
not only exact matches, but also word stems,
synonyms, and paraphrases (Denkowski and
Lavie, 2011). Suppose that we compute the
similarity between the following sentences in
French: j? aimerais bien vous voir and je
voudrais vous voir (both meaning I would
like to see you). BLEU, which is a string-
based metric, would assign a similarity score
of 52.5. This value could hardly be con-
sidered reliable, given that the sentence ta
voiture vous voir (paired with the first sen-
tence) would get the same BLEU score, al-
though the latter sentence (EN: your car see
you) is obviously nonsense. On the other
hand, METEOR would return a score of 90.3
for the original sentence pair, since it can
appreciate that the two pronouns (je and j?)
are both variations of the first person singular
in French and that the predicates convey the
same meaning.
? Number of aligned content words
However, METEOR scores can also be mis-
leading, since they rely on automatic word
alignments. Two sentences are likely to re-
ceive a high similarity score when they share
many aligned words. However, the align-
ments are not always reliable. We often saw
114
sentence pairs with a decent Meteor score
where only some determiners, punctuation
signs or simple word collocations (e.g. de
la montagne (EN: of the mountain)) were
matched. As an illustration, consider the fol-
lowing sentence pair and its corresponding
alignment:
Hyp: les armoiries , le de?sir de la ville de
breslau par ferdinand i. le 12 mars 1530 a
Ref: le 19 juin 1990 , le conseil municipal
re?tablit le blason original de la ville
2-4 3-5 5-12 6-13 7-14 13-0
Although the sentences are obviously not se-
mantically equivalent (a fact also suggested
by the sparse word alignments), the pair re-
ceives a METEOR score of 0.23. We decided
to compensate for this by counting only the
aligned pairs which link content words and
dividing them by the total number of words
in the longest sentence from the considered
pair. In the example above, only one valid
alignment (7-14) can be identified, therefore
the sentence pair will get a partial score of
1/18. In this manner we can ensure the de-
crease of the initial similarity score.
Additionally, we have defined a token ratio fea-
ture to penalize the sentence length differences.
Although a length penalty is already included in
the METEOR score, we still found false candidate
pairs with exceedingly different lengths. There-
fore we decided to use this criterion as a selec-
tion filter rather than including it in the similarity
function, in order to increase the chances of other
candidates with similar length. Even if no other
candidate will pass all the filters, at least we ex-
pect the precision to increase, since we will have
one false positive less.
The final formula for the similarity score be-
tween two clauses src in the source language and,
respectively trg in the target language is:
score(src, trg) = w1 ? s1 + (1? w1) ? s2 (1)
where s1 represents the METEOR score and s2 the
alignment score.
The weights, as well as the final threshold are
tuned to maximize the correlation with human
judgments. We modeled the task as a minimiza-
tion problem, where the function value increases
by 1 for each correctly selected clause pair and
decreases by 1 for each wrong pair. The solu-
tion (consisting of the individual weights and the
threshold) is found using a brute force approach,
for which we employed the scipy.optimize
package from Python. The training set consists
of an article with 1300 clause pairs, 25 of which
are parallel and the rest non-parallel. We chose
this distribution of the useful/not useful clauses
because this corresponds to the real distribution
observed in Wikipedia articles. In the best con-
figuration, we retrieve 23 good clause pairs and 1
wrong. This corresponds to a precision of 95%
and a recall of 92% on this small test set.
However, we can influence the quantity of ex-
tracted parallel clauses by manually adjusting the
final filter threshold. Figure 2 depicts the size vari-
ations of the resulting corpus at different thresh-
olds, where the relative frequency is represented
on a logarithmic scale. We notice that the rate of
decrease is linear in the log scale of the number
of extracted clause pairs. We start at a similarity
score of 0.2 because the pairs below this thresh-
old are too noisy. The data between 0.2 and 0.3
is already mixed, as it will be shown in the fol-
lowing sections. However, since this data segment
contains approximately twice as much data as the
summed superior ones, we decided to include it in
the corpus.
Figure 2: The distribution of the extracted clause
pairs at different thresholds
Table 1 presents German-French clause pairs
with their corresponding similarity scores. On
the top we can find rather short clauses (up to
10 words) with perfectly aligned words. One ex-
pects that the decrease of the values implies that
115
Nr. French clause German clause Score
1 mcnish e?crit dans son journal : mcnish schrieb in sein tagebuch : 1.0
2 son journal n? a pas e?te? retrouve? sein tagebuch wurde nie gefunden 0.950
3 elle travailla pendant plusieurs se-
maines avec lui
wa?hrend mehrerer wochen arbeitete sie
mit ihm zusammen
0.840
4 en 1783, il fait une premie`re tentative
infructueuse avec marc the?odore bourrit
paccard startete 1783 zusammen mit marc
theodore bourrit einen ersten, erfolglosen
besteigungsversuch
0.717
5 en 1962, les bavarois toni kinshofer,
siegfried lo?w et anderl mannhardt
re?ussirent pour la premie`re fois l? as-
cension par la face du diamir
1962 durchstiegen die bayern toni
kinshofer, siegfried lo?w und anderl
mannhardt erstmals die diamir-flanke
0.623
6 le 19 aou?t 1828 il tenta, avec les deux
guides jakob leuthold et johann wahren
l? ascension du finsteraarhorn
august 1828 versuchte er zusammen mit
den beiden bergfu?hrern jakob leuthold
und johann wa?hren das finsteraarhorn zu
besteigen
0.519
7 le parc prote`ge le mont robson, le
plus haut sommet des rocheuses cana-
diennes
das 2248 km2 gro?e schutzgebiet er-
streckt sich um den 3954 m hohen mount
robson, dem ho?chsten berg der kanadis-
chen rocky mountains
0.470
8 la plupart des e?difices volcaniques du
haut eifel sont des do?mes isole?s plus ou
moins aplatis
die meisten der vulkanbauten der
hocheifel sind als isolierte kuppen
vereinzelt oder in reihen der mehr oder
minder flachen hochfla?che aufgesetzt
0.379
9 le site, candidat au patrimoine mondial,
se compose d? esplanades-autels faits
de pierres
die sta?tte, ein kandidat fu?r das unesco-
welterbe, besteht aus altarplattformen aus
steinen und erde, gestu?tzt auf einer un-
terirdischen konstruktion aus bemalten
ton-pfeilern
0.259
10 qu? un cas mineur ayant un effet limite?
sur la sante?
wie sich diese substanzen auf die gesund-
heit auswirken,
0.200
Table 1: Examples of extracted clause pairs
the clauses contain less or even no translated frag-
ments. A manual inspection of the extracted pairs
showed that this is not always the case. We have
found clause pairs with almost perfect 1-1 word
correspondences and a similarity score of only
0.51. The ?low? score is due to the fact that we
are comparing human language to automatic trans-
lations, which are not perfect.
On the other hand, a comparable score can be
achieved by a pair in which one of the clauses
contains some extra information (e.g. pair num-
ber 7). The extra parts in the German variant
(2248 km2 gro?e - EN: with an area of 2248 km2;
3954 m hohen - EN: 3954 m high) cannot be
separated by means of clause boundary detection,
since they don?t contain any verbs. This finding
would motivate the idea of splitting the phrases
into subsentential segments (linguistically moti-
vated or not) and aligning the segments, similar
to what Munteanu (2006) proposed. Nevertheless,
we consider this pair a good candidate for the par-
allel corpus.
Pair number 8 has the same coordinates (i.e. an
extra tail in the German variant), yet it receives a
lower score, which might disqualify it for the final
list, if we only look at the numbers. In this case,
the low score is caused by the German compounds
(Vulkanbauten, Hocheifel), which are unknown to
the SMT system, therefore they are left untrans-
lated and cannot be aligned. However, we argue
that this clause pair should also be part of the ex-
tracted corpus.
116
Score Average sentence length
range German French
[0.9? 1.0] 4 4.26
[0.8? 0.9) 4.87 5.04
[0.7? 0.8) 6.47 6.65
[0.6? 0.7) 10.78 10.71
[0.5? 0.6) 12.09 11.51
[0.4? 0.5) 11.91 11.80
[0.3? 0.4) 11.28 11.22
[0.2? 0.3) 11.22 11.01
Table 2: The average sentence length for different
score ranges
The last pair is definitely a bad candidate for a
parallel corpus, since the clauses do not convey the
same meaning, although they share many words
(avoir un effet - auswirken, sur la sante? - auf die
Gesundheit). A subsentential approach would al-
low us to extract the useful segments in this case,
as well. There are, of course, pairs with similar
scores and poorer quality, therefore 0.2 is the low-
est threshold which can provide useful candidate
pairs. At the other end of the scale, we consider
pairs above 0.4 as parallel and everything below
as comparable. As a general rule, a high threshold
ensures a high accuracy of the extraction pipeline.
Table 2 presents the average length (number
of tokens) of the extracted clauses for different
ranges of the similarity score. We notice that the
best ranked clauses tend to be very short, whereas
the last ranked are longer, as the examples in ta-
ble 1 confirm. However, the average length over
the whole extracted corpus is below 10 words, a
small value compared to the results reported on
Wikipedia articles by S?tefa?nescu and Ion (2013).
This finding is due to the fact that we are aligning
clauses instead of whole sentences.
We expected the German sentences to be usu-
ally shorter than the French ones (or at least have
a similar number of words), since they are more
likely to contain compounds. This fact is con-
firmed by the first part of the table. A turnaround
occurs in the range (0.5,0.6), where the German
sentences become slightly longer than the French
ones, since they tend to contain extra information
(see also table 1).
4 Experiments and Results
The conducted experiments have focused only on
the extraction of parallel clauses and their use in a
SMT scenario. For this purpose, we have used as
input the articles selected and preprocessed in the
previous development phase (Plamada and Volk,
2012). Specifically, the data set consists of 39 000
parallel articles with approximately 6 million Ger-
man clauses and 2.7 million French ones. We were
able to extract 225 000 parallel clause pairs out of
them, by setting the final filter threshold to 0.2.
This means that roughly 4% of the German clauses
have an French equivalent (and 8% when reporting
to the French clauses), figures comparable to our
previous results on a different sized data set. How-
ever, the quality of the extracted data is higher than
in our previous approaches.
To evaluate the quality of the parallel data ex-
tracted, we manually checked a set of 200 au-
tomatically aligned clauses with similarity scores
above 0.25. For this test set, 39% of the ex-
tracted data represent perfect translations, 26% are
translations with an extra segment (e.g. a noun
phrase) on one side and 35% represent misalign-
ments. However, given the high degree of paral-
lelism between the clauses from the middle class,
we consider them as true positives, achieving a
precision of 65%. Furthermore, 40% of the false
positives have been introduced by matching proper
names, 32% contain matching subsentential seg-
ments (word sequences longer than 3 words) and
27% represent failures in the alignment process.
4.1 SMT Experiments
In addition to the manual evaluation discussed in
the previous subsection, we have run preliminary
investigations with regard to the usefulness of the
extracted corpus for SMT. In this evaluation sce-
nario, we use only pairs with a similarity score
above 0.35. The results discussed in this sec-
tion refer only to the translation direction German-
French. The SMT systems are trained with the
Moses toolkit (Koehn et al, 2007), according to
the WMT 2011 guidelines6. The translation per-
formance was measured using the BLEU evalua-
tion metric on a single reference translation. We
also report statistical significance scores, in order
to indicate the validity of the comparisons between
the MT systems (Riezler and Maxwell, 2005). We
consider the BLEU score difference significant if
the computed p-value is below 0.05.
We compare two baseline MT systems and sev-
eral systems with different model mixtures (trans-
6http://www.statmt.org/wmt11/baseline.html
117
lation models, language models or both). The first
baseline system is an in-domain one, trained on the
Text+Berg corpus and is the same used for the au-
tomatic translations required in the extraction step
(see section 3). The second system is purely out-
of-domain and it is trained on Europarl, a collec-
tion of parliamentary proceedings (Koehn, 2005).
The development set and the test set contain in-
domain data, held out from the Text+Berg corpus.
Table 3 lists the sizes of the data sets used for the
SMT experiments.
Data set Sentences DE Words FR Words
SAC 220 000 4 200 000 4 700 000
Europarl 1 680 000 37 000 000 43 000 000
Wikipedia 120 000 1 000 000 1 000 000
Dev set 1424 30 000 33 000
Test set 991 19 000 21 000
Table 3: The size of the German-French data sets
Our first intuition was to add the extracted sen-
tences to the existing in-domain training corpus
and to evaluate the performance of the system. In
the second scenario, we added the extracted data
to an SMT system for which no in-domain paral-
lel data was available. For this purpose, we exper-
imented with different combinations of the mod-
els involved in the translation process, namely the
German-French translation model (responsible for
the translation variants) and the French language
model (ensures the fluency of the output). Besides
of the models trained on the parallel data available
in each of the data sets, we also built combined
models with optimized weights for each of the in-
volved data sets. The optimization was performed
with the tools provided by Sennrich (2012) as part
of the Moses toolkit. We also want to compare
several language models, some trained on the indi-
vidual data sets, others obtained by linearly inter-
polating different data sets, all optimized for min-
imal perplexity on the in-domain development set.
The results are summarized in table 4.
A first remark is that an out-of-domain lan-
guage model (LM) adapted with in-domain data
(extracted from Wikipedia and/or SAC data) sig-
nificantly improves on top of a baseline system
trained with out-of-domain texts (Europarl, EP)
with up to 1.7 BLEU points. And this improve-
ment can be achieved with only a small quantity
of additional data compared to the size of the orig-
inal training data (120k or 220k versus 1680k sen-
tence pairs). When replacing the out-of-domain
Translation Language model BLEU
model score
Europarl TM EP LM 9.45
Europarl TM EP+Wiki LM 10.39
EP+Wiki TM EP+Wiki LM 10.37
Europarl TM EP+Wiki+SAC LM 11.22
EP+Wiki TM EP+Wiki+SAC LM 11.74
EP+WMix TM EP+Wiki+SAC LM 10.40
SAC TM SAC LM 16.71
SAC+Wiki TM SAC+Wiki LM 16.51
SAC+WMix TM SAC+Wiki LM 16.37
Table 4: SMT results for German-French
translation model with a combined one (includ-
ing the Wikipedia data set) and keeping only the
adapted language models, we can observe two ten-
dencies. In the first case (using a combination
of out-of-domain and Wikipedia-data for the lan-
guage model), the BLEU score remains approxi-
mately at the same level (10.37-10.39), the differ-
ence not being statistically significant (p-value =
0.387).
The addition of quality in-domain data for the
LM from the previous configuration brings an im-
provement of 0.5 BLEU points on top of the best
Europarl system (11.22 BLEU points). Given that
all other factors are kept constant, this improve-
ment can be attributed to the additional transla-
tion model (TM) trained on Wikipedia data. More-
over, the statistical significance tests confirm that
the improved system performs better than the pre-
vious one (p-value = 0.005). To demonstrate
that these results are not accidental, we replaced
the Wikipedia extracted sentences with a random
combination thereof (referred to as WMix) and re-
trained the system. Under these circumstances,
the performance of the system dropped to 10.40
BLEU points. These findings demonstrate the ef-
fect of a small in-domain data set on the perfor-
mance of an out-of-domain system trained on big
amounts of data. If the data is of good quality, it
can improve the performance of the system, other-
wise it significantly deteriorates it.
We notice that the performance of a strong in-
domain baseline system (SAC) cannot be heav-
ily influenced (either positively or negatively) by
translation and language model mixtures combin-
ing existing in-domain data with Wikipedia data.
In terms of BLEU points, the mixture models
trained with ?good? Wikipedia data cause a perfor-
118
mance drop of 0.2, but the significance test shows
that the difference is not statistically significant (p-
value = 0.08). On the other hand, the TM includ-
ing shuffled Wikipedia sentences causes a perfor-
mance drop of 0.34 BLEU points, which is statis-
tically significant (p-value = 0.013). We can con-
clude that the quantity of the data is not the deci-
sive factor for the performance change, but rather
the quality of the data. The Wikipedia extracted
data set maintains the good performance, whereas
a random mixture of the Wikipedia data set causes
a performance decrease. Therefore the focus of
future work should be on obtaining high quality
data, regardless of its amount.
5 Conclusions and Outlook
In this paper we presented a method for extract-
ing domain-specific parallel data from Wikipedia
articles. Based on previous experiments, we fo-
cus on clause level alignments rather than on full-
sentence extraction methods. Moreover, the rank-
ing of the candidates is based on a metric com-
bining different similarity criteria, which we de-
fined ourselves. The precision estimates show that
the extracted sentence pairs are clearly semanti-
cally equivalent. The SMT experiments, however,
show that the extracted data is not refined enough
to improve a strong in-domain SMT system. Nev-
ertheless, it is good enough to overtake an out-of-
domain system trained on 10 times bigger amounts
of data.
Since our extraction system is merely a proto-
type, there are several ways to improve its per-
formance, including better filtering for in-domain
articles, finer grained alignment and more so-
phisticated similarity metrics. For example, the
selection of domain-specific articles can be im-
proved by means of an additional filter based on
Wikipedia categories. The accuracy of the extrac-
tion procedure can be improved by means of a
more informed similarity metric, weighting more
feature functions. Moreover, we can bypass the
manual choice of thresholds by employing a clas-
sifier (e.g. SVMlight (Joachims, 2002)). Addi-
tionally, we could try to align even shorter sen-
tence fragments (not necessarily linguistically mo-
tivated).
We are confident that Wikipedia can be seen as
a useful resource for SMT, but further investiga-
tion is needed in order to find the best method to
exploit the extracted data in a SMT scenario. For
this purpose, quality data should be preferred over
sizable data. We would therefore like to experi-
ment with different ratio combinations of the data
sets (Wikipedia extracted and in-domain data) un-
til we find a combination which outperforms our
in-domain baseline system.
References
Sadaf Abdul Rauf and Holger Schwenk. 2011. Paral-
lel sentence generation from comparable corpora for
improved SMT. Machine Translation, 25:341?375.
Sisay Fissaha Adafre and Maarten de Rijke. 2006.
Finding similar sentences across multiple languages
in Wikipedia. Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 62?69.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and EM. In Proceed-
ings of EMNLP.
Pascale Fung, Emmanuel Prochasson, and Simon Shi.
2010. Trillions of comparable documents. In Pro-
ceedings of the the 3rd workshop on Building and
Using Comparable Corpora (BUCC?10), Malta.
Thorsten Joachims. 2002. Learning to classify text
using Support Vector Machines. Kluwer Academic
Publishers.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79?86.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31:477?504, December.
Dragos Stefan Munteanu. 2006. Exploiting compara-
ble corpora. Ph.D. thesis, University Of Southern
California.
119
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Magdalena Plamada and Martin Volk. 2012. Towards
a Wikipedia-extracted alpine corpus. In Proceed-
ings of the Fifth Workshop on Building and Using
Comparable Corpora, Istanbul, May.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 539?
549, Avignon, France. Association For Computa-
tional Linguistics.
Jason Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
403?411, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dan S?tefa?nescu and Radu Ion. 2013. Parallel-Wiki:
A collection of parallel sentences extracted from
Wikipedia. In Proceedings of the 14th Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing 2013).
Dan S?tefa?nescu, Radu Ion, and Sabine Hunsicker.
2012. Hybrid parallel sentence mining from com-
parable corpora. In Mauro Cettolo, Marcello Fed-
erico, Lucia Specia, and AndyEditors Way, editors,
Proceedings of the 16th Conference of the European
Association for Machine Translation EAMT 2012,
pages 137?144.
Martin Volk. 2001. The automatic resolution of prepo-
sitional phrase - attachment ambiguities in German.
Habilitation thesis, University of Zurich.
Bing Zhao and Stephan Vogel. 2002. Adaptive parallel
sentences mining from web bilingual news collec-
tion. In Proceedings of the 2002 IEEE International
Conference on Data Mining, ICDM ?02, pages 745?
748, Washington, DC, USA. IEEE Computer Soci-
ety.
120
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 11?19,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building a German/Simple German Parallel Corpus
for Automatic Text Simplification
David Klaper Sarah Ebling Martin Volk
Institute of Computational Linguistics, University of Zurich
Binzm?hlestrasse 14, 8050 Zurich, Switzerland
david.klaper@uzh.ch, {ebling|volk}@cl.uzh.ch
Abstract
In this paper we report our experiments
in creating a parallel corpus using Ger-
man/Simple German documents from the
web. We require parallel data to build a
statistical machine translation (SMT) sys-
tem that translates from German into Sim-
ple German. Parallel data for SMT sys-
tems needs to be aligned at the sentence
level. We applied an existing monolingual
sentence alignment algorithm. We show
the limits of the algorithm with respect to
the language and domain of our data and
suggest ways of circumventing them.
1 Introduction
Simple language (or, ?plain language?, ?easy-to-
read language?) is language with low lexical and
syntactic complexity. It provides access to infor-
mation to people with cognitive disabilities (e.g.,
aphasia, dyslexia), foreign language learners, Deaf
people,1 and children. Text in simple language
is obtained through simplification. Simplification
is a text-to-text generation task involving multiple
operations, such as deletion, rephrasing, reorder-
ing, sentence splitting, and even insertion (Coster
and Kauchak, 2011a). By contrast, paraphrasing
and compression, two other text-to-text generation
tasks, involve merely rephrasing and reordering
(paraphrasing) and deletion (compression). Text
simplification also shares common ground with
grammar and style checking as well as with con-
trolled natural language generation.
Text simplification approaches exist for vari-
ous languages, including English, French, Span-
ish, and Swedish. As Matausch and Nietzio (2012)
write, ?plain language is still underrepresented in
1It is an often neglected fact that Deaf people tend to ex-
hibit low literacy skills (Gutjahr, 2006).
the German speaking area and needs further devel-
opment?. Our goal is to build a statistical machine
translation (SMT) system that translates from Ger-
man into Simple German.
SMT systems require two corpora aligned at the
sentence level as their training, development, and
test data. The two corpora together can form a
bilingual or a monolingual corpus. A bilingual
corpus involves two different languages, while a
monolingual corpus consists of data in a single
language. Since text simplification is a text-to-
text generation task operating within the same lan-
guage, it produces monolingual corpora.
Monolingual corpora, like bilingual corpora,
can be either parallel or comparable. A parallel
corpus is a set of two corpora in which ?a no-
ticeable number of sentences can be recognized as
mutual translations? (Tom?s et al, 2008). Paral-
lel corpora are often compiled from the publica-
tions of multinational institutions, such as the UN
or the EU, or of governments of multilingual coun-
tries, such as Canada (Koehn, 2005). In contrast, a
comparable corpus consists of two corpora created
independently of each other from distinct sources.
Examples of comparable documents are news ar-
ticles written on the same topic by different news
agencies.
In this paper we report our experiments in cre-
ating a monolingual parallel corpus using Ger-
man/Simple German documents from the web. We
require parallel data to build an SMT system that
translates from German into Simple German. Par-
allel data for SMT systems needs to be aligned at
the sentence level. We applied an existing mono-
lingual sentence alignment algorithm. We show
the limits of the algorithm with respect to the lan-
guage and domain of our data and suggest ways of
circumventing them.
The remainder of this paper is organized as fol-
lows: In Section 2 we discuss the methodologies
pursued and the data used in previous work deal-
11
ing with automatic text simplification. In Section 3
we describe our own approach to building a Ger-
man/Simple German parallel corpus. In particu-
lar, we introduce the data obtained from the web
(Section 3.1), describe the sentence alignment al-
gorithm we used (Section 3.2), present the results
of the sentence alignment task (Section 3.3), and
discuss them (Section 3.4). In Section 4 we give
an overview of the issues we tackled and offer an
outlook on future work.
2 Approaches to Text Simplification
The task of simplifying text automatically can be
performed by means of rule-based, corpus-based,
or hybrid approaches. In a rule-based approach,
the operations carried out typically include replac-
ing words by simpler synonyms or rephrasing rel-
ative clauses, embedded sentences, passive con-
structions, etc. Moreover, definitions of difficult
terms or concepts are often added, e.g., the term
web crawler is defined as ?a computer program
that searches the Web automatically?. Gasperin et
al. (2010) pursued a rule-based approach to text
simplification for Brazilian Portuguese within the
PorSimples project,2 as did Brouwers et al (2012)
for French.
As part of the corpus-based approach, machine
translation (MT) has been employed. Yatskar et al
(2010) pointed out that simplification is ?a form of
MT in which the two ?languages? in question are
highly related?.
As far as we can see, Zhu et al (2010) were the
first to use English/Simple English Wikipedia data
for automatic simplification via machine transla-
tion.3 They assembled a monolingual compara-
ble corpus4 of 108,016 sentence pairs based on
the interlanguage links in Wikipedia and the sen-
tence alignment algorithm of Nelken and Shieber
(2006) (cf. Section 3.2). Their system applies a
?tree-based simplification model? including ma-
chine translation techniques. The system learns
probabilities for simplification operations (substi-
tution, reordering, splitting, deletion) offline from
2http://www2.nilc.icmc.usp.br/wiki/
index.php/English
3English Wikipedia: http://en.wikipedia.
org/; Simple English Wikipedia: http://simple.
wikipedia.org/.
4We consider this corpus to be comparable rather than
parallel because not every Simple English Wikipedia article
is necessarily a translation of an English Wikipedia article.
Rather, Simple English articles can be added independently
of any English counterpart.
the comparable Wikipedia data. At runtime, an in-
put sentence is parsed and zero or more simplifica-
tion operations are carried out based on the model
probabilities.
Specia (2010) used the SMT system Moses
(Koehn et al, 2007) to translate from Brazilian
Portuguese into a simpler version of this language.
Her work is part of the PorSimples project men-
tioned above. As training data she used 4483 sen-
tences extracted from news texts that had been
manually translated into Simple Brazilian Por-
tuguese.5 The results, evaluated automatically
with BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002) as well as manually, show that
the system performed lexical simplification and
sentence splitting well, while it exhibited prob-
lems in reordering phrases and producing subject?
verb?object (SVO) order. To further improve her
system Specia (2010) suggested including syntac-
tic information through hierarchical SMT (Chi-
ang, 2005) and part-of-speech tags through fac-
tored SMT (Hoang, 2007).
Coster and Kauchak (2011a; 2011b) trans-
lated from English into Simple English using En-
glish/Simple English Wikipedia data. Like Spe-
cia (2010), they applied Moses as their MT sys-
tem but in addition to the default configuration al-
lowed for phrases to be empty. This was moti-
vated by their observation that 47% of all Simple
English Wikipedia sentences were missing at least
one phrase compared to their English Wikipedia
counterparts. Coster and Kauchak (2011a; 2011b)
used four baselines to evaluate their system: in-
put=output,6 two text compression systems, and
vanilla Moses. Their system, Moses-Del, achieved
higher automatic MT evaluation scores (BLEU)
than all of the baselines. In particular, it outper-
formed vanilla Moses (lacking the phrase deletion
option).
Wubben et al (2012) also worked with En-
glish/Simple English Wikipedia data and Moses.
They added a post-hoc reranking step: Follow-
ing their conviction that the output of a simplifi-
cation system has to be a modified version of the
input,7 they rearranged the 10-best sentences out-
put by Moses such that those differing from the
5Hence, the corpus as a whole is a monolingual parallel
corpus.
6The underlying assumption here was that not every sen-
tence needs simplification.
7Note that this runs contrary to the assumption Coster and
Kauchak (2011a; 2011b) made.
12
input sentences were given preference over those
that were identical. Difference was calculated on
the basis of the Levenshtein score (edit distance).
Wubben et al (2012) found their system to work
better than that of Zhu et al (2010) when evalu-
ated with BLEU, but not when evaluated with the
Flesch-Kincaid grade level, a common readability
metric.
Bott and Saggion (2011) presented a monolin-
gual sentence alignment algorithm, which uses a
Hidden Markov Model for alignment. In contrast
to other monolingual alignment algorithms, Bott
and Saggion (2011) introduced a monotonicity re-
striction, i.e., they assumed the order of sentences
to be the same for the original and simplified texts.
Apart from purely rule-based and purely
corpus-based approaches to text simplification,
hybrid approaches exist. For example, Bott et al
(2012) in their Simplext project for Spanish8 let a
statistical classifier decide for each sentence of a
text whether it should be simplified (corpus-based
approach). The actual simplification was then per-
formed by means of a rule-based approach.
As has been shown, many MT approaches to
text simplification have used English/Simple En-
glish Wikipedia as their data. The only excep-
tion we know of is Specia (2010), who together
with her colleagues in the PorSimples project built
her own parallel corpus. This is presumably be-
cause there exists no Simple Brazilian Portuguese
Wikipedia. The same is true for German: To date,
no Simple German Wikipedia has been created.
Therefore, we looked for data available elsewhere
for our machine translation system designated to
translate from German to Simple German. We dis-
covered that German/Simple German parallel data
is slowly becoming available on the web. In what
follows, we describe the data we harvested and re-
port our experience in creating a monolingual par-
allel corpus from this data.
3 Building a German/Simple German
Parallel Corpus from the Web
3.1 Data
As mentioned in Section 1, statistical machine
translation (SMT) systems require parallel data.
A common approach to obtain such material is
to look for it on the web.9 The use of already
8http://www.simplext.es/
9Resnik (1999) was the first to discuss the possibility of
collecting parallel corpora from the web.
available data offers cost and time advantages.
Many websites, including that of the German gov-
ernment,10 contain documents in Simple German.
However, these documents are often not linked to a
single corresponding German document; instead,
they are high-level summaries of multiple German
documents.
A handful of websites exist that offer articles
in two versions: a German version, often called
Alltagssprache (AS, ?everyday language?), and
a Simple German version, referred to as Leichte
Sprache (LS, ?simple language?). Table 1 lists the
websites we used to compile our corpus. The num-
bers indicate how many parallel articles were ex-
tracted. The websites are mainly of organizations
that support people with disabilities. We crawled
the articles with customized Python scripts that lo-
cated AS articles and followed the links to their LS
correspondents. A sample sentence pair from our
data is shown in Example 1.
(1) German:
Wir freuen uns ?ber Ihr Interesse an unserer
Arbeit mit und f?r Menschen mit
Behinderung.
(?We appreciate your interest in our work
with and for people with disabilities.?)
Simple German:
Sch?n, dass Sie sich f?r unsere Arbeit
interessieren.
Wir arbeiten mit und f?r Menschen mit
Behinderung.
(?Great that you are interested in our work.
We work with and for people with
disabilities.?)
The extracted data needed to be cleaned from
HTML tags. For our purpose, we considered text
and paragraph structure markers as important in-
formation; therefore, we retained them. We subse-
quently tokenized the articles. The resulting cor-
pus consisted of 7755 sentences, which amounted
to 82,842 tokens. However, caution is advised
when looking at these numbers: Firstly, the tok-
enization module overgenerated tokens. Secondly,
some of the LS articles were identical, either be-
cause they summarized multiple AS articles or be-
cause they were generic placeholders. Hence, the
10http://www.bundesregierung.de/Webs/
Breg/DE/LeichteSprache/leichteSprache_
node.html (last accessed 15th April 2013)
13
Short name URL No. of parallel art.
ET www.einfach-teilhaben.de 51
GWW www.gww-netz.de 65
HHO www.os-hho.de 34
LMT www.lebenshilfe-main-taunus.de 47
OWB www.owb.de 59
Table 1: Websites and number of articles extracted
actual numbers were closer to 7000 sentences and
70,000 tokens.
SMT systems usually require large amount of
training data. Therefore, this small experimen-
tal corpus is certainly not suitable for large-scale
SMT experiments. However, it can serve as proof
of concept for German sentence simplification.
Over time more resources will become available.
SMT systems rely on data aligned at the sen-
tence level. Since the data we extracted from the
web was aligned at the article level only, we had
to perform sentence alignment. For this we split
our corpus into a training set (70% of the texts),
development set (10%), and test set (20%). We
manually annotated sentence alignments for all of
the data. Example 2 shows an aligned AS/LS sen-
tence pair.
(2) German:
In den Osnabr?cker Werkst?tten (OW) und
OSNA-Techniken sind rund 2.000 Menschen
mit einer Behinderung besch?ftigt.
(?In the Osnabr?ck factories and
OSNA-Techniken, about 2.000 people with
disability are employed.?)
Simple German:
In den Osnabr?cker Werkst?tten und den
Osna-Techniken arbeiten zweitausend
Menschen mit Behinderung.
(?Two thousand people with disability work
in the Osnabr?ck factories and
Osna-Techniken.?)
To measure the amount of parallel sentences
in our data, we calculated the alignment di-
versity measure (ADM) of Nelken and Shieber
(2006). ADM measures how many sentences are
aligned. It is calculated as 2?matches(T1,T2)|T1|+|T2| , where
matches is the number of alignments between the
two texts T1 and T2. ADM is 1.0 in a perfectly
parallel corpus, where every sentence from one
text is aligned to exactly one sentence in another
text.
ADM for our corpus was 0.786, which means
that approximately 78% of the sentences were
aligned. This is a rather high number compared to
the values reported by Nelken and Shieber (2006):
Their texts (consisting of encyclopedia articles and
gospels) resulted in an ADM of around 0.3. A pos-
sible explanation for the large difference in ADM
is the fact that most simplified texts in our corpus
are solely based on the original texts, whereas the
simple versions of the encyclopedia articles might
have been created by drawing on external informa-
tion in addition.
3.2 Sentence Alignment Algorithm
Sentence alignment algorithms differ according to
whether they have been developed for bilingual or
monolingual corpora. For bilingual parallel cor-
pora many?typically length-based?algorithms
exist. However, our data was monolingual. While
the length of a regular/simple language sentence
pair might be different, an overlap in vocabulary
can be expected. Hence, monolingual sentence
alignment algorithms typically exploit lexical sim-
ilarity.
We applied the monolingual sentence alignment
algorithm of Barzilay and Elhadad (2003). The al-
gorithm has two main features: Firstly, it uses a
hierarchical approach by assigning paragraphs to
clusters and learning mapping rules. Secondly,
it aligns sentences despite low lexical similarity
if the context suggests an alignment. This is
achieved through local sequence alignment, a dy-
namic programming algorithm.
The overall algorithm has two phases, a train-
ing and a testing phase. The training phase in turn
consists of two steps: Firstly, all paragraphs of the
texts of one side of the parallel corpus (henceforth
referred to as ?AS texts?) are clustered indepen-
dently of all paragraphs of the texts of the other
14
side of the parallel corpus (henceforth termed ?LS
texts?), and vice versa. Secondly, mappings be-
tween the two sets of clusters are calculated, given
the reference alignments.
As a preprocessing step to the clustering pro-
cess, we removed stopwords, lowercased all
words, and replaced dates, numbers, and names
by generic tags. Barzilay and Elhadad (2003) ad-
ditionally considered every word starting with a
capital letter inside a sentence to be a proper name.
In German, all nouns (i.e., regular nouns as well as
proper names) are capitalized; thus, this approach
does not work. We used a list of 61,228 first names
to remove at least part of the proper names.
We performed clustering with scipy (Jones et
al., 2001). We adapted the hierarchical complete-
link clustering method of Barzilay and Elhadad
(2003): While the authors claimed to have set a
specific number of clusters, we believe this is not
generally possible in hierarchical agglomerative
clustering. Therefore, we used the largest num-
ber of clusters in which all paragraph pairs had a
cosine similarity strictly greater than zero.
Following the formation of the clusters, lex-
ical similarity between all paragraphs of corre-
sponding AS and LS texts was computed to es-
tablish probable mappings between the two sets
of clusters. Barzilay and Elhadad (2003) used
the boosting tool Boostexter (Schapire and Singer,
2000). All possible cross-combinations of para-
graphs from the parallel training data served as
training instances. An instance consisted of the
cosine similarity of the two paragraphs and a string
combining the two cluster IDs. The classifica-
tion result was extracted from the manual align-
ments. In order for an AS and an LS paragraph
to be aligned, at least one sentence from the LS
paragraph had to be aligned to one sentence in the
AS paragraph. Like Barzilay and Elhadad (2003),
we performed 200 iterations in Boostexter. After
learning the mapping rules, the training phase was
complete.
The testing phase consisted of two additional
steps. Firstly, each paragraph of each text in the
test set was assigned to the cluster it was clos-
est to. This was done by calculating the cosine
similarity of the word frequencies in the clusters.
Then, every AS paragraph was combined with all
LS paragraphs of the parallel text, and Boostexter
was used in classification mode to predict whether
the two paragraphs were to be mapped.
Secondly, within each pair of paragraphs
mapped by Boostexter, sentences with very high
lexical similarity were aligned. In our case, the
threshold for an alignment was a similarity of 0.5.
For the remaining sentences, proximity to other
aligned or similar sentences was used as an indi-
cator. This was implemented by local sequence
alignment. We set the mismatch penalty to 0.02,
as a higher mismatch penalty would have reduced
recall. We set the skip penalty to 0.001 conform-
ing to the value of Barzilay and Elhadad (2003).
The resulting alignments were written to files. Ex-
ample 3 shows a successful sentence alignment.
(3) German:
Die GWW ist in den Landkreisen B?blingen
und Calw aktiv und bietet an den folgenden
Standorten Wohnm?glichkeiten f?r
Menschen mit Behinderung an ? ganz in
Ihrer N?he!
(?The GWW is active in the counties of
B?blingen and Calw and offers housing
options for people with disabilities at the
following locations ? very close to you!?)
Simple German:
Die GWW gibt es in den Landkreisen Calw
und B?blingen.
Wir haben an den folgenden Orten
Wohn-M?glichkeiten f?r Sie.
(?The GWW exists in the counties of Calw
and B?blingen. We have housing options for
you in the following locations.?)
The algorithm described has been modified in
various ways. Nelken and Shieber (2006) used
TF/IDF instead of raw term frequency, logistic re-
gression on the cosine similarity instead of cluster-
ing, and an extended version of the local alignment
recurrence. Both Nelken and Shieber (2006) and
Quirk et al (2004) found that the first sentence
of each document is likely to be aligned. We ob-
served the same for our corpus. Therefore, in our
algorithm we adopted the strategy of uncondition-
ally aligning the first sentence of each document.
3.3 Results
Table 2 shows the results of evaluating the algo-
rithm described in the previous section with re-
spect to precision, recall, and F1 measure. We in-
troduced two baselines:
15
Method Precision Recall F1
Adapted algorithm of Barzilay and Elhadad (2003) 27.7% 5.0% 8.5%
Baseline I: First sentence 88.1% 4.8% 9.3%
Baseline II: Word in common 2.2% 8.2% 3.5%
Table 2: Alignment results on test set
1. Aligning only the first sentence of each text
(?First sentence?)
2. Aligning every sentence with a cosine simi-
larity greater than zero (?Word in common?)
As can be seen from Table 2, by applying the
sentence alignment algorithm of Barzilay and El-
hadad (2003) we were able to extract only 5%
of all reference alignments, while precision was
below 30%. The rule of aligning the first sen-
tences performed well with a precision of 88%.
Aligning all sentences with a word in common
clearly showed the worst performance; this is be-
cause many sentences have a word in common.
Nonetheless, recall was only slightly higher than
with the other methods.
In conclusion, none of the three approaches
(adapted algorithm of Barzilay and Elhadad
(2003), two baselines ?First sentence? and ?Word
in common?) performed well on our test set. We
analyzed the characteristics of our data that ham-
pered high-quality automatic alignment.
3.4 Discussion
Compared with the results of Barzilay and El-
hadad (2003), who achieved 77% precision at
55.8% recall for their data, our alignment scores
were considerably lower (27.7% precision, 5% re-
call). We found two reasons for this: language
challenges and domain challenges. In what fol-
lows, we discuss each reason in more detail.
While Barzilay and Elhadad (2003) aligned En-
glish/Simple English texts, we dealt with Ger-
man/Simple German data. As mentioned in Sec-
tion 3.2, in German nouns (regular nouns as well
as proper names) are capitalized. This makes
named entity recognition, a preprocessing step to
clustering, more difficult. Moreover, German is
an example of a morphologically rich language:
Its noun phrases are marked with case, leading
to different inflectional forms for articles, pro-
nouns, adjectives, and nouns. English morphol-
ogy is poorer; hence, there is a greater likelihood
of lexical overlap. Similarly, compounds are pro-
ductive in German; an example from our corpus
is Seniorenwohnanlagen (?housing complexes for
the elderly?). In contrast, English compounds are
multiword units, where each word can be accessed
separately by a clustering algorithm. Therefore,
cosine similarity is more effective for English than
it is for German. One way to alleviate this problem
would be to use extensive morphological decom-
position and lemmatization.
In terms of domain, Barzilay and Elhadad
(2003) used city descriptions from an encyclope-
dia for their experiments. For these descriptions
clustering worked well because all articles had the
same structure (paragraphs about culture, sports,
etc.). The domain of our corpus was broader:
It included information about housing, work, and
events for people with disabilities as well as infor-
mation about the organizations behind the respec-
tive websites.
Apart from language and domain challenges we
observed heavy transformations from AS to LS in
our data (Figure 1 shows a sample article in AS
and LS). As a result, LS paragraphs were typi-
cally very short and the clustering process returned
many singleton clusters. Example 4 shows an
AS/LS sentence pair that could not be aligned be-
cause of this.
(4) German:
Der Beauftragte informiert ?ber die
Gesetzeslage, regt Rechts?nderungen an,
gibt Praxistipps und zeigt M?glichkeiten der
Eingliederung behinderter Menschen in
Gesellschaft und Beruf auf.
(?The delegate informs about the legal
situation, encourages revisions of laws, gives
practical advice and points out possibilities
of including people with disabilities in
society and at work.?)
Simple German:
Er gibt ihnen Tipps und Infos.
16
Figure 1: Comparison of AS and LS article from http://www.einfach-teilhaben.de
(?He provides them with advice and
information.?)
Figure 2 shows the dendrogram of the cluster-
ing of the AS texts. A dendrogram shows the re-
sults of a hierarchical agglomerative clustering. At
the bottom of the dendrogram every paragraph is
marked by an individual line. At the points where
two vertical paths join, the corresponding clusters
are merged to a new larger cluster. The Y-axis is
the dissimilarity value of the two clusters. In our
experiment the resulting clusters are the clusters
at dissimilarity 1 ? 1?10. Geometrically this is a
horizontal cut just below dissimilarity 1.0. As can
be seen from Figure 2, many of the paragraphs
in the left half of the picture are never merged
to a slightly larger cluster but are directly con-
nected to the universal cluster that merges every-
thing. This is because they contain only stopwords
or only words that do not appear in all paragraphs
of another cluster. Such an unbalanced clustering,
where many paragraphs are clustered to one clus-
ter and many other paragraphs remain singleton
clusters, reduces the precision of the hierarchical
approach.
4 Conclusion and Outlook
In this paper we have reported our experiments in
creating a monolingual parallel corpus using Ger-
man/Simple German documents from the web. We
have shown that little work has been done on au-
tomatic simplification of German so far. We have
described our plan to build a statistical machine
translation (SMT) system that translates form Ger-
man into Simple German. SMT systems require
parallel corpora. The process of creating a parallel
corpus for use in machine translation involves sen-
tence alignment. Sentence alignment algorithms
for bilingual corpora differ from those for mono-
lingual corpora. Since all of our data was from
the same language, we applied the monolingual
sentence alignment approach of Barzilay and El-
hadad (2003). We have shown the limits of the al-
gorithm with respect to the language and domain
of our data. For example, named entity recogni-
tion, a preprocessing step to clustering, is harder
for German than for English, the language Barzi-
lay and Elhadad (2003) worked with. Moreover,
German features richer morphology than English,
which leads to less lexical overlap when working
on the word form level.
17
Figure 2: Dendrogram of AS clusters
The domain of our corpus was also broader than
that of Barzilay and Elhadad (2003), who used city
descriptions from an encyclopedia for their exper-
iments. This made it harder to identify common
article structures that could be exploited in clus-
tering.
As a next step, we will experiment with other
monolingual sentence alignment algorithms. In
addition, we will build a second parallel corpus for
German/Simple German: A person familiar with
the task of text simplification will produce simple
versions of German texts. We will use the result-
ing parallel corpus as data for our experiments in
automatically translating from German to Simple
German. The parallel corpus we compiled as part
of the work described in this paper can be made
available to interested parties upon request.
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
MTTG ?11, pages 20?26, Stroudsburg, PA, USA.
Stefan Bott, Horacio Saggion, and David Figueroa.
2012. A Hybrid System for Spanish Text Simpli-
fication. In Proceedings of the Third Workshop on
Speech and Language Processing for Assistive Tech-
nologies, pages 75?84, Montr?al, Canada, June.
Laetitia Brouwers, Delphine Bernhard, Anne-Laure
Ligozat, and Thomas Fran?ois. 2012. Simplifica-
tion syntaxique de phrases pour le fran?ais. In Actes
de la conf?rence conjointe JEP-TALN-RECITAL
2012, volume 2: TALN, pages 211?224.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In ACL-
05: 43rd Annual Meeting of the Association for
Computational Linguistics, pages 263?270, Univer-
sity of Michigan, Ann Arbor, Michigan, USA.
William Coster and David Kauchak. 2011a. Learn-
ing to simplify sentences using Wikipedia. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, MTTG ?11, pages 1?9, Strouds-
burg, PA, USA.
William Coster and David Kauchak. 2011b. Simple
English Wikipedia: a new text simplification task.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 665?669, Stroudsburg, PA, USA.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
occurrence Statistics. In HLT 2002: Human Lan-
guage Technology Conference, Proceedings of the
Second International Conference on Human Lan-
guage Technology Research, pages 138?145, San
Diego, California.
18
Caroline Gasperin, Erick Maziero, and Sandra M.
Aluisio. 2010. Challenging choices for text sim-
plification. In Computational Processing of the Por-
tuguese Language. Proceedings of the 9th Interna-
tional Conference, PROPOR 2010, volume 6001
of Lecture Notes in Artificial Intelligence (LNAI),
pages 40?50, Porto Alegre, RS, Brazil. Springer.
A. Gutjahr. 2006. Lesekompetenz Geh?rloser: Ein
Forschungs?berblick. Universit?t Hamburg.
Hieu Hoang. 2007. Factored Translation Models.
In EMNLP-CoNLL 2007: Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 868?876, Prague, Czech
Republic.
Eric Jones, Travis Oliphant, Pearu Peterson, et al
2001. SciPy: Open Source Scientific Tools for
Python.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 177?180, Prague, Czech Republic.
Kerstin Matausch and Annika Nietzio. 2012. Easy-to-
read and plain language: Defining criteria and re-
fining rules. http://www.w3.org/WAI/RD/
2012/easy-to-read/paper11/.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In Proceedings of 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 161?168.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In 40th Annual
Meeting of the Association for Computational Lin-
guistics, Proceedings of the Conference, pages 311?
318, Philadelphia, PA, USA.
Chris Quirk, Chris Brocket, and William Dolan. 2004.
Monolingual machine translation for paraphrase
generation. In Proceedings Empirical Methods in
Natural Language Processing.
Philip Resnik. 1999. Mining the Web for Bilingual
Text. In 37th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the Con-
ference, pages 527?534, University of Maryland,
College Park, Maryland, USA.
Robert E. Schapire and Yoram Singer. 2000. BoosTex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2?3):135?168.
Lucia Specia. 2010. Translating from complex to
simplified sentences. In Computational Process-
ing of the Portuguese Language. Proceedings of the
9th International Conference, PROPOR 2010, vol-
ume 6001 of Lecture Notes in Artificial Intelligence
(LNAI), pages 30?39, Porto Alegre, RS, Brazil.
Springer.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 1015?1024, Jeju Island, Korea.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Proceedings of the An-
nual Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
365?368.
Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A
monolingual tree-based translation model for sen-
tence simplification. In Proceedings of the Inter-
national Conference on Computational Linguistics,
pages 1353?1361.
19
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 24?33,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Detecting Code-Switching in a Multilingual Alpine Heritage Corpus
Martin Volk and Simon Clematide
University of Zurich
Institute of Computational Linguistics
volk|siclemat@cl.uzh.ch
Abstract
This paper describes experiments in de-
tecting and annotating code-switching in
a large multilingual diachronic corpus of
Swiss Alpine texts. The texts are in En-
glish, French, German, Italian, Romansh
and Swiss German. Because of the mul-
tilingual authors (mountaineers, scientists)
and the assumed multilingual readers, the
texts contain numerous code-switching
elements. When building and annotating
the corpus, we faced issues of language
identification on the sentence and sub-
sentential level. We present our strategy
for language identification and for the an-
notation of foreign language fragments
within sentences. We report 78% precision
on detecting a subset of code-switches
with correct language labels and 92% un-
labeled precision.
1 Introduction
In the Text+Berg project we have digitized the
yearbooks of the Swiss Alpine Club (SAC) from
its first edition in 1864 until today. They contain
articles about mountain expeditions, the flora and
fauna of the Alpes and other mountain regions,
glacier and climate observations, geology and his-
tory papers, book reviews, accident and security
reports, as well as the protocols of the annual
club gatherings. The texts are in the four official
languages of Switzerland French, German, Italian
and Romansh
1
plus a few in English and Swiss
German dialects.
Because of the multilinguality of the authors
and readers, many articles are mixed-language
texts with inter-sentential and intra-sentential
1. Romansh is the 4th official language in Switzerland. It
is spoken by around 25,000 people in the mountainous South-
Eastern canton of Graub?unden.
code-switching. This poses a challenge for auto-
matically processing the texts. When we apply
Part-of-Speech (PoS) tagging, named entity recog-
nition or parsing, our systems need to know the
language that they are dealing with. Therefore we
had used a language identifier from the start of the
project to mark the language of each sentence. We
report on our experiences with sentence-based lan-
guage identification in section 3. Figure 1 shows
an example of a French text with an English ap-
pendix title plus an English quote from this book.
Lately we discovered that our corpus also
contains many intra-sentential code-switches. For
example, we find sentences like
... und ich finde es

very nice and de-
lightful

einen Vortrag halten zu d?urfen.
(Die Alpen, 1925) (EN : ... and I find it
very nice and delightful to be allowed to
give a talk.)
where the German sentence contains an English
phrase in quotation marks. Obviously, a German
PoS tagger will produce nonsense tags for the En-
glish phrase as the words will be unknown to it.
PoS taggers are good at tagging single unknown
words based on the surrounding context, but most
taggers fail miserably when a sequence of two
or more words is unknown. The upper half of fi-
gure 2 shows the PoS tagger output for the above
example. The words very, nice, delightful are sen-
selessly tagged as proper names (NE), only and is
tagged as foreign word (FM).
Our goal is to detect all intra-sentential code-
switches and to annotate them as exemplified in
the lower half of figure 2. They shall be framed
with the TEI-conformant tag <foreign> which
also shall specify the language of the foreign lan-
guage segment. All tokens in the segment shall be
tagged as foreign words (e.g. FM in the German
STTS tag set, ET in the French Le Monde tag set
(Abeill?e et al., 2003)), and each lemma shall get
24
the special symbol @fn@ to set it apart from lem-
mas of the surrounding sentence. In this paper we
report on our experiments towards this goal and
suggest an algorithm for detecting code-switching.
We adopt a wide definition of code-switching.
We are interested in detecting all instances where
a text is in a dominant language and contains
words, phrases and sentences in another language.
Though our definition is broad, it is clearly more
restricted than others, as e.g. the definition by
Kracht and Klein (2014) which includes special
purpose codes like bank account numbers or shoe
sizes.
In this paper we will give an overview of the
language mix in the yearbooks of the Swiss Al-
pine Club over the 150 years, and we will illus-
trate how we identified inter-sentential and intra-
sentential code-switching. We will give a quanti-
tative overview of the number of code-switching
candidates that we automatically located.
2 The Text+Berg Corpus
The Text+Berg corpus comprises the annual pu-
blications of the Swiss Alpine Club (SAC) from
its first edition in 1864 until 2013. From the start
until 1923 the official yearbook was called ?Jahr-
buch des Schweizer Alpen-Club? (EN : yearbook
of the Swiss Alpine Club), and it typically consis-
ted of 500 to 700 pages. The articles of these first
60 years were mostly in German (with 86% of
the words), but some also in French (13% of the
words) and few in Italian and Romansh (Volk et
al., 2010).
Interestingly, the German articles contained
passages in French and sometimes other languages
(e.g. English, Swiss German, Latin) without trans-
lations, and vice versa. Obviously, the article au-
thors and yearbook editors assumed that the rea-
ders of the yearbook were polyglott at least in En-
glish, French, German and Latin during that time.
In fact, the members of the SAC in the 19th cen-
tury came from an academic elite. Mountain ex-
ploration was a past-time of the rich and educated.
Still, during that same time the French-speaking
sections of the Swiss Alpine Club published their
own yearbook in parallel to the official yearbook
and called it ?Echo des Alpes?. It started shortly
after the official yearbook in the late 1860s and
continued until 1923. Each ?Echo des Alpes? year-
book contained between 300 to 600 pages adding
up to a total of 22,582 pages with 7.4 million to-
kens, almost all in French with rare quotes in Ger-
man.
As of 1925 the official SAC yearbook and the
?Echo des Alpes? were merged into a new publi-
cation called ?Die Alpen. Les Alpes. Le Alpi? (in
German, French, Italian) which has been publi-
shed ever since. Over the years it sometimes ap-
peared as quarterly and sometimes as monthly ma-
gazine. Today it appears 12 times per year in ma-
gazine format. For the sake of simplicity we conti-
nue to call each annual volume a yearbook.
The merger in 1925 resulted in a higher per-
centage of French texts in the new yearbook. For
example, the 1925 yearbook had around 143,000
words in German and 112,000 in French (56% to
44%). The ratio varied somewhat but was still at
64% to 36% in 1956.
From 1957 onwards, the SAC has published pa-
rallel (i.e. translated) French and German versions
of the yearbooks. At the start of this new era only
half of the articles were translated, the rest was
printed in the original language in identical ver-
sions in the two language copies.
Over the next decade the number of translations
increased and as of 1983 the yearbooks were com-
pletely translated between German and French.
Few Italian articles were still published verbatim
in both the French and German yearbooks. As of
2012 the SAC has launched an Italian language
version of its monthly magazine so that now it pro-
duces French, German and Italian parallel texts.
In its latest release the Text+Berg corpus (com-
prising the SAC yearbooks, the ALPEN maga-
zine and the Echo des Alpes) contains around
45.8 million tokens (after tokenization). French
and German account for around 22 million tokens
each, Italian accounts for 0.8 million tokens. The
remainder goes to English, Latin, Romansh and
Swiss German. The corpus is freely available for
research purposes upon request.
3 Language Identification in the
Text+Berg Corpus
We compiled the Text+Berg corpus by scanning
all SAC yearbooks from 1864 until 2000 (around
100,000+ pages). Afterwards we employed com-
mercial OCR software to convert the scan images
into electronic text. We developed and applied
techniques to automatically reduce the number of
OCR errors (Volk et al., 2011).
We obtained the yearbooks from 2001 to
25
FIGURE 1 ? Example of an English title and an English quote in a French text (Die Alpen, 1955)
FIGURE 2 ? Example of an annotated German sentence with English segment, before and after code-
switch detection (Die Alpen, 1925)
26
2009 as PDF documents which we automatically
converted to text. The subsequent yearbooks from
2010 until 2013 we received as XML files from
the SAC.
We have turned the whole corpus into a uniform
XML format. For this, the OCR output texts as
well as the texts converted from PDF and XML
are structured and annotated by automatically mar-
king article boundaries, by tokenization, language
identification, Part-of-Speech tagging and lemma-
tization. Our processing pipeline also includes to-
ponym recognition and geo-coding of mountains,
glaciers, cabins, valleys, lakes and towns. Further-
more we recognize and co-reference person names
(Ebling et al., 2011), and we annotate temporal
expressions (date, time, duration and set) with a
variant of HeidelTime (Rettich, 2013). Finally we
analyze the parallel parts of our corpus and pro-
vide sentence alignment information that is com-
puted via BLEUalign (Sennrich and Volk, 2011).
In order to process our texts with language-
specific tools (e.g. PoS tagging and person name
recognition) we employed automatic language
identification on the sentence level. We used
Lingua-Ident
2
(developed by Michael Piotrowski)
to determine for each sentence in our corpus whe-
ther it is in English, French, German, Italian or Ro-
mansh. Lingua-Ident is a statistical language iden-
tifier based on letter n-gram frequencies. For long
sentences it reliably distinguishes between the lan-
guages. Unfortunately it often misclassifies short
sentences. Therefore we decided to use it only for
sentences with more than 40 characters. Shorter
sentences are assigned the language of the article.
This can be problematic for mixed language ar-
ticles. An alternative strategy would be to assign
the language of the previous sentence to short sen-
tences.
For sentences that Lingua-Ident judges as Ger-
man we run a second classifier that distinguishes
between Standard German and Swiss German dia-
lect text. Since there are no writing rules for Swiss
German dialects, they come in a variety of spel-
lings. We have compiled a list of typical Swiss
German words (e.g. Swiss-German : chli, chlii,
chlini, chline = German : klein, kleine = English :
small) that are not used in Standard German in or-
der to identify Swiss German sentences.
3
2. http ://search.cpan.org/dist/Lingua-Ident/
3. We are aware that the Text+Berg corpus contains also
occasional sentences (or sentence fragments) in other Ger-
man dialects (e.g. Austrian German, Bavarian German) and
Based on the language tag of each sentence
we are able to investigate coarse-grained code-
switching. Whenever the language of a sentence
deviates from the language of the article, we have a
candidate for code-switching. For example, in the
yearbook 1867 we find a German text (describing
the activities of the club) with a French quote :
Der Berichterstatter bemerkt dar?uber :
?On peut remarquer `a cette occasion
qu?il est rare que par un effort de l?es-
prit on puisse mettre du brouillard en
bouteille, et . . . ? Die etwas ?altere Sek-
tion Diablerets, deren Steuer Herr Au-
gust Bernus mit kundiger Hand . . .
Most code-switching occurs with direct speech,
quotes and book titles. The communicative goal is
obviously to make the text more authentic.
4 Related Work on Detection of
Code-Switching
Most previous work on automatically detecting
code-switching focused on the switches between
two known languages (whereas we have to deal
with a mix of 6 languages).
Solorio and Liu (2008) worked on real-time
prediction of code-switching points in Spanish-
English conversations. This means that the judge-
ment whether the current word is in a different lan-
guage than the language of the matrix clause can
only be based on the previous words. They use the
PoS tag and its probability plus the lemma as pro-
vided by both the Spanish and the English Tree-
Tagger as well as the position of the word in the
Beginning-Inside-Outside scheme as features for
making the decision. In order to keep the number
of experiments manageable they restricted their
history to one or two preceding words. As an inter-
esting experiment they generated code-switching
sentences Spanish-English based on their different
predictors and asked human judges to rate the na-
turalness of the resulting sentences. This helped
them to identify the most useful code-switching
predictor.
Vu et al. (2013) and Adel et al. (2013) consi-
der English-Mandarin code-switching in speech
recognition. They investigate recurrent neural net-
work language models and factored language mo-
dels to the task in an attempt to integrate syntac-
tic features. For the experiments they use SEAME,
in old German spellings. Since these varieties are rare in the
corpus, we do not deal with them explicitly.
27
the South East Asia Mandarin-English speech cor-
pus compiled from Singaporean and Malaysian
speakers. It consists of spontaneous interviews
and conversations. The transcriptions were clea-
ned and each word was manually tagged as En-
glish, Mandarin or other. The data consists of an
intensive mix of the two languages with the ave-
rage duration of both English and Mandarin seg-
ments to be less than a second ( !). In order to as-
sign PoS tags to this mixed language corpus, the
authors applied two monolingual taggers and com-
bined the results.
Huang and Yates (2014) also work on the de-
tection of English-Chinese code-switching but not
on speech but rather on web forum texts produ-
ced by Chinese speakers living in the US. They
use statistical word alignment and a Chinese lan-
guage model to substitute English words in Chi-
nese sentences with suitable Chinese words. Pre-
paring the data in this way significantly improved
Machine Translation quality. Their approach is li-
mited to two known languages and to very short
code-switching phrases (typically only one word).
Tim Baldwin and his group (Hughes et al.,
2006) have surveyed the approaches to language
identification at the time. They found a number of
missing issues, such as language identification for
minority languages, open class language identifi-
cation (in contrast to identification within a fixed
set of languages), sparse training data, varying
encodings, and multilingual documents. Subse-
quently they (Lui and Baldwin, 2011) introduced a
system for language identification of 97 languages
trained on a mixture of corpora from different
domains. They claim that their system Langid is
particularly well suited for classifying short input
strings (as in Twitter messages). We therefore tes-
ted Langid in our experiments for code-switching
detection.
5 Exploratory Experiments with the
SAC Yearbook 1925
In order to assess the performance of Langid
for the detection of code-switching we performed
an exploratory experiment with the SAC yearbook
1925. We extracted all word sequences between
pairs of quotation marks where at least one token
had been assigned the ?unknown? lemma by our
PoS tagger. The ?unknown? lemma indicates that
this word sequence may come from a different lan-
guage.
The word sequence had to be at least 4 cha-
racters long, thus skipping single letters and ab-
breviations. In this way we obtained 333 word
sequences that are potential candidates for intra-
sentential code-switching. We then ran these word
sequences through the Langid language identifica-
tion system with the restriction that we expect the
word sequences only to be either English, French,
German, Italian or Latin (Romansh and Swiss Ger-
man are not included in Langid). For a given string
Langid delivers the most likely language together
with a confidence score.
We then compared the language predicted by
the Langid system with the (automatically) com-
puted language of the complete sentence. In 189
out of the 333 sentences the Langid output pre-
dicted a code-switch. We then manually graded all
Langid judgements and found that 225 language
judgements (67.5%) were correct. But only 89 of
the 189 predicted code-switches came with the
correct language. 40 of the 100 incorrect judge-
ments were actually code-switches but with a dif-
ferent language. The remaining ones should have
been classified with the same language as the sur-
rounding sentence and are thus no examples of
code-switching.
A closer inspection of the results revealed that
the book contained not only code-switches in the
expected 5 languages, but also into Romansh (6),
Spanish (4) and Swiss-German (13). Obviously all
of these were incorrectly classified. Most (8) of
the Swiss-German word sequences were classified
as German which could count as half correct, but
the others were misclassified as English (among
them a variant of the popular Swiss German fare-
well phrase uf Wiederluege spelled as uf?s Wieder-
luege).
The Langid system has a tendency to classify
word sequences as English. Many of the short, in-
correctly classified word sequences were judged
as English. It turns out that Langid judges even the
empty string as English with a score of 9.06. The-
refore all judgements with this score are dubious.
We found that 56 short word sequences were clas-
sified as English with this score, out of which 35
were erroneously judged as English. Only strings
with a length of 15 and more characters that are
classified as English should be trusted. All others
need to be discarded.
In general, if precision is the most important as-
pect, then Langid should only be used for strings
28
SAC yearbooks candidates predicted code-sw correct wrong lang no code-sw
1868 to 1878 388 121 88 33 13
1926 to 1935 792 335 266 69 23
Total 1180 456 354 102 36
TABLE 1 ? Recognition of code-switches in the Text+Berg corpus
with 20 or more characters. In our test set only 4
strings that were longer than 20 characters were
incorrectly classified within the selected language
set. Among the errors was the famous Latin phrase
conditio sine qua non (length : 21 characters inclu-
ding blanks) which Langid incorrectly classified
as Italian.
Another reason for the considerable number of
misclassifications can be repeated occurrences of
a word sequence. Our error count is a token-based
count and thus prone to misclassified recurring
phrases. In our experiment, Langid misclassified
the French book name Echo des Alpes as Italian.
Unfortunately this name occurs 18 times in our
test set and thus accounts for 18 errors. We suspect
that an -o at the end of a word is a strong indicator
for Italian. In a short string like Echo des Alpes (14
characters), this can make the difference.
Another interesting observation is that hyphens
speak for German. Our test set contains the hy-
phenated French string vesse-de-neige which Lan-
gid misclassifies as German with a clear margin
over French. When the same string is analyzed
without hyphens, then Langid correctly computes
a preference for French over German. A similar
observation comes from the Swiss German phrase
uf?s Wiederluege being classified as English when
spelled with the apostrophe (which is less frequent
in German than in English). Without the apos-
trophe Langid would count the string as German.
With short strings like this, special symbols have a
visible impact on the language identification.
We also observed that Langid is sensitive to
all-caps capitalization. For example, AUS DEM
LEBEN DER GEBIRGSMUNDARTEN (EN : The
Lives of Mountain Dialects) is misclassified as En-
glish (with the default score) while Aus dem Le-
ben der Gebirgsmundarten is correctly classified
as German.
Overall, we found that code-switching within
the same article rarely targets different languages.
For example, if the article is in German and
contains code-switches into English, then it hardly
ever contains code-switches into other languages.
In analogy to the one-sense-per-discourse hypo-
thesis we might call this the one-code-switch-
language-per-discourse hypothesis.
6 Detecting Intra-sentential
Code-Switching
Based on exploratory studies and observations
we decided on the following algorithm for detec-
ting and annotating intra-sentential foreign lan-
guage segments in the Text+Berg corpus. We
search for sub-sentential token sequences (possi-
bly of length 1) that are framed by a pair of quota-
tion marks and that contain at least one ?unknown?
lemma. There must be at least two tokens outside
of the quotation marks in the same sentence. As
a compromise we restrict our detection to strings
longer than 15 characters so that we get relati-
vely reliable language judgements by Langid. The
strings may consist of one token that is longer than
15 characters (e.g. Matterhornhochtourist) or a se-
quence of tokens whose sum of characters inclu-
ding blanks is more than 15. We feed these can-
didate strings to Langid for language identifica-
tion and compare the output language with the lan-
guage attribute of the surrounding sentence. If the
languages are different, then we regard the token
sequence as code-switch and mark it accordingly
in XML as shown in figure 2.
In order to determine the precision of this al-
gorithm, we checked 10 yearbooks from 1868 to
1878 (there was no yearbook in 1870) and from
1926 to 1935. The results are in table 1. From
the 1180 code-switch candidates that we compu-
ted based on the above restrictions, Langid predic-
ted 456 code-switches (39%). This means that in
39% of the cases Langid predicted a language that
was different from the language of the surrounding
sentence.
We manually evaluated all 456 predicted code-
switches and found that 354 of them (78%) were
correctly classified and labeled. These segments
were indeed in a different language than the sur-
rounding sentence and their language was cor-
rectly determined. For example, the French seg-
29
SAC yearbooks
> 15 characters
without unknowns
? 15 characters
all sample : TN/FN all sample : TN/FP
1868 to 1878 322 20/1 404 15/8
1926 to 1935 1944 78/1 1136 54/23
Total 2266 (2%) 98/2 1540 (31%) 69/31
TABLE 2 ? Estimation of the loss of recall due to the filtering approach based on a random sample of 100
quotations for each filtering category (TN : true negatives, FN : false negatives)
ment in the following German sentence is cor-
rectly detected and classified :
Anschliessend f?uhrte Ambros dasselbe
Bergsteigertrio

dans des circonstances
tr`es d?efavorables

auf den Monte Rosa
... (Die Alpen, 1935) (EN : After-
wards Ambros led the same 3 mountai-
neers

under very unfavorable condi-
tions

onto Monte Rosa.)
Out of the 102 segments whose language
was wrongly classified, only 36 were no code-
switches. For example, the Latin segment cum
grano salis africani is indeed a code-switch in
a German sentence although Langid incorrectly
classifies it as English. In fact, our evaluation sho-
wed that Langid is ?reluctant? to classify strings as
Latin. Latin strings are often misclassified as En-
glish or Italian.
Overall this means that only 8% of the predicted
code-switches are no code-switches. Therefore we
can safely add the module for code-switch detec-
tion into our processing and annotation pipeline.
In order to estimate the recall of our quota-
tion filtering approach we manually evaluated a
sample of the quotations that our algorithm exclu-
ded. Table 2 presents the numbers for the two time
periods for two cases : first for sequences that are
longer than 15 characters and contain only known
lemmas, second for sequences that are shorter than
16 characters and contain at least one ?unknown?
lemma. For both cases we checked 100 instances.
The evaluation for the quotations with more
than 15 characters but with all known lemmas (no
?unknown? lemma) shows only 2 false negatives.
Therefore, we can conclude safely that most of the
code-switches with more than 15 characters were
included in our candidate set.
Table 2 also shows that there were 1540 quota-
tions with 15 or less characters. The manual ins-
pection of 100 randomly selected quotations re-
vealed that 31 indeed include foreign material.
Some of these quotations are geographic names,
e.g. the valley Bergell (EN/IT : Val Bregaglia),
where it is difficult to decide whether this should
be regarded as a code-switch. For this evaluation,
we sticked to the principle that a foreign geogra-
phic name in quotation marks counts as a code-
switch. The number of missed code-switches is
high (31%). However, due to the limited preci-
sion of Langid (and other character-based lan-
guage identifiers) for short character sequences,
we still consider our length threshold appropriate.
A different approach to language identification is
needed to reliably classify these short quotes.
7 Discussion
The correctly marked code-switches in our test
periods can be split by language of the matrix sen-
tence and the language of the sub-sentential seg-
ment (= the code-switch segment). Table 3 gives
an overview of the types of code-switches for the
two periods under investigation. We see clearly
that code-switches from German to English were
rare in the 19th century (8 out of 89 = 9%) but be-
came much more popular in the 1920s and 1930s
(61 out of 265 = 23%). This came at the cost of
French which lost ground from 54% (48 out of 89)
to 40% (106 out of 265).
One can only compare the code-switch num-
bers from German with the corresponding num-
bers from French after normalizing the numbers
in relation to the overall amount of text in Ger-
man and French. During the first period (1868 to
1878) we count roughly 200,000 tokens in French
and 1.4 million tokens in German, whereas in the
second period (1926 to 1935) we have around 1
million tokens in French and again 1.4 million
tokens in German. For the first period we find
87 code-switches (triggered by quotation marks)
in the 1.4 million German tokens compared to
189 code-switches in the second period. The num-
30
sent
lang
segm
lang
1868 to
1878
1926 to
1935
de en 8 61
de fr 48 106
de it 24 19
de la 7 3
fr de 2 35
fr en - 20
fr it - 11
fr la - 2
it de - 3
it en - 2
it fr - 3
Total 89 265
TABLE 3 ? Correctly detected code-switches in the
Text+Berg corpus
sent
lang
segm
lang
1868 to
1878
1926 to
1935
de en 13 23
de fr 9 5
de it 8 12
de la 2 1
fr de - 7
fr en 1 10
fr it - 8
fr la - 1
it en - 2
Total 33 69
TABLE 4 ? Incorrectly labeled code-switches in
the Text+Berg corpus
ber of code-switches have clearly increased. For
French we observe the same trend with 2 code-
switches in 200?000 words in the first period com-
pared to 68 code-switches in the 1 million tokens
in the second period.
There is also a striking difference between
French and German with many more code-
switches in German than in French. For instance,
for German we find 135 code-switches per 1 mil-
lion tokens in the second period vs. 68 code-
switches per 1 million tokens for French.
One surprising finding were the code-switches
into Latin. We had not noticed them before, since
our corpus does not contain longer passages of La-
tin text. But this study shows that code-switches
correct
segm Langid prediction
lang en it fr la de Total
la 15 12 3 1 31
de 7 5 5 1 18
fr 7 3 10
it 6 6
es 3 1 2 6
rm 1 2 3
ru 1 1
id 1 1
Total 40 22 10 3 1 76
TABLE 5 ? Confusion matrix for incorrectly labe-
led code-switches in the periods 1868 to 1878 and
1926 to 1935
into Latin persisted into the 1920s (3 out of Ger-
man and 2 out of French).
On the negative side (cf. table 4), misclassi-
fying segments as English is the most frequent
cause for a wrong language assignment in both
periods. Table 5 shows the confusion matrix which
contrasts the manually determined segment lan-
guage with the incorrect language predicted by
Langid. This confirms that Langid has a tendency
to classify short text segments as English. But
there are also a number of errors for Latin being
mistaken for Italian, and German being mistaken
for Italian or French.
As a general remark, it should be noted that an
n-gram-based language identifier has advantages
over a lexicon-based language identifier in the face
of OCR errors. In the yearbook 1926 we observed
the rare case of a whole English sentence having
been contracted to one token Ilovetobemothered.
Still, our code-switch detector recognizes this as
an English string.
4
8 Conclusions
We have described our efforts in language iden-
tification in a multilingual corpus of Alpine texts.
As part of corpus annotation we have identified
the language of each corpus sentence amongst En-
glish, French, Standard German, Swiss German,
4. The complete sentence is : Un long Anglais, avec le-
quel, dans le hall familial, je m?essaie `a ?echanger laborieu-
sement quelques impressions `a ce sujet, me dit :

I love to be
mothered.

31
Italian and Romansh. Furthermore we have de-
veloped an algorithm to identify intra-sentential
code-switching by analyzing sentence parts in
quotation marks that contain ?unknown? lemmas.
We have shown that token sequences that
amount to 15 or more characters can be judged by
a state-of-the-art language identifier and will result
in 78% correctly labeled code-switches. Another
14% are code-switches but with a language dif-
ferent from the auto-assigned language. Only 8%
are not code-switches at all.
There are many ways to continue and extend
this research. We have not included language iden-
tification for Swiss German nor for Romansh in
the intra-sentential code-switch experiments re-
ported in this paper. We will train language models
for these two languages and add them to Langid
to check the impact on the recognition accuracy.
Since code-switches into Romansh are rare, and
since Romansh can easily be confused with Ita-
lian, it is questionable whether the addition of this
language model will have a positive influence.
We have used the ?general-purpose? language
identifier Langid in these experiments. It will be
interesting to investigate language identifiers that
are optimized for short text fragments as discus-
sed by Vatanen et al. (2010). Given the relati-
vely high number of short quotations (31%) that
contain code-switches, recall could improve consi-
derably.
In this paper we have focused solely on code-
switching candidates that are triggered by pairs of
quotation marks. In order to increase the recall we
will certainly enlarge the set of triggers to other in-
dicators such as parentheses or commas. We have
briefly looked at parentheses as trigger symbols
and found them clearly less productive than quo-
tation marks. To also find code-switches that have
no overt marker remains the ultimate goal.
Finally, we will exploit the parallel parts of our
corpus. If a sentence in German contains a French
segment, then it is likely that this French segment
occurs verbatim in the parallel French sentence.
Based on sentence and word alignment we will
search for identical phrases in both language ver-
sions. We hope that this will lead to high accuracy
code-switch data that we can use as training mate-
rial for machine learning experiments.
Acknowledgments
We would like to thank Michi Amsler and
Don Tuggener for useful comments on literature
and tools for language identification and code-
switching, as well as Patricia Scheurer for com-
ments and suggestions on the language use in
the SAC corpus. This research was supported
by the Swiss National Science Foundation under
grant CRSII2 147653/1 through the project ?MO-
DERN : Modelling discourse entities and relations
for coherent machine translation?.
References
Anne Abeill?e, Lionel Cl?ement, and Francois Tousse-
nel. 2003. Building a Treebank for French. In Anne
Abeill?e, editor, Building and Using Parsed Corpora,
volume 20 of Text, Speech and Language Tech-
nology, chapter 10, pages 165?187. Kluwer, Dor-
drecht.
Heike Adel, Ngoc Thang Vu, and Tanja Schultz. 2013.
Combination of recurrent neural networks and fac-
tored language models for code-switching language
modeling. In Proceedings of the 51st Annual Mee-
ting of the Association for Computational Linguis-
tics (ACL), Sofia.
Sarah Ebling, Rico Sennrich, David Klaper, and Martin
Volk. 2011. Digging for names in the mountains :
Combined person name recognition and reference
resolution for German alpine texts. In Proceedings
of The 5th Language & Technology Conference :
Human Language Technologies as a Challenge for
Computer Science and Linguistics, Poznan.
Fei Huang and Alexander Yates. 2014. Improving
word alignment using linguistic code switching data.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 1?9, G?oteborg.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew Mackinlay. 2006. Reconsi-
dering language identification for written language
resources. In Proceedings of LREC 2006, pages
485?488, Genoa.
Marcus Kracht and Udo Klein. 2014. The grammar
of code switching. Journal of Logic, Language and
Information, 23(3) :313?329.
Marco Lui and Timothy Baldwin. 2011. Cross-
domain feature selection for language identification.
In Proceedings of 5th International Joint Conference
on Natural Language Processing, pages 553?561,
Chiang Mai, Thailand. Asian Federation of Natural
Language Processing.
Katrin Rettich. 2013. Automatische Annotation
von deutschen und franz?osischen temporalen Aus-
dr?ucken im Text+Berg-Korpus. Master thesis, Uni-
versit?at Z?urich, Institut f?ur Computerlinguistik.
Rico Sennrich and Martin Volk. 2011. Itera-
tive, MT-based sentence alignment of parallel texts.
32
In Proceedings of The 18th International Nordic
Conference of Computational Linguistics (Noda-
lida), Riga.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981, Honolulu. Asso-
ciation for Computational Linguistics.
Tommi Vatanen, Jaakko J. V?ayrynen, and Sami Vir-
pioja. 2010. Language identification of short text
segments with n-gram models. In Proceedings of
LREC, pages 3423?3430, Malta.
Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya
Bangerter, Lenz Furrer, and Beni Ruef. 2010. Chal-
lenges in building a multilingual alpine heritage cor-
pus. In Proceedings of LREC, Valletta, Malta.
Martin Volk, Lenz Furrer, and Rico Sennrich. 2011.
Strategies for reducing and correcting OCR errors.
In C. Sporleder, A. van den Bosch, and K. Zerva-
nou, editors, Language Technology for Cultural He-
ritage : Selected Papers from the LaTeCH Work-
shop Series, Theory and Applications of Natural
Language Processing, pages 3?22. Springer-Verlag,
Berlin.
Ngoc Thang Vu, Heike Adel, and Tanja Schultz.
2013. An investigation of code-switching atti-
tude dependent language modeling. In Statistical
Language and Speech Processing, pages 297?308.
Springer.
33
