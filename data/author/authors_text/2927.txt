Robust Sub-Sentential Alignment of Phrase-Structure Trees
Declan Groves
School of Computing
Dublin City University
Dublin 9, Ireland
dgroves@computing.dcu.ie
Mary Hearne
School of Computing
Dublin City University
Dublin 9, Ireland
mhearne@computing.dcu.ie
Andy Way
School of Computing
Dublin City University
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Data-Oriented Translation (DOT), based on Data-
Oriented Parsing (DOP), is a language-independent
MT engine which exploits parsed, aligned bitexts
to produce very high quality translations. How-
ever, data acquisition constitutes a serious bottleneck
as DOT requires parsed sentences aligned at both
sentential and sub-structural levels. Manual sub-
structural alignment is time-consuming, error-prone
and requires considerable knowledge of both source
and target languages and how they are related. Au-
tomating this process is essential in order to carry out
the large-scale translation experiments necessary to
assess the full potential of DOT.
We present a novel algorithm which automatically in-
duces sub-structural alignments between context-free
phrase structure trees in a fast and consistent fash-
ion requiring little or no knowledge of the language
pair. We present results from a number of experi-
ments which indicate that our method provides a se-
rious alternative to manual alignment.
1 Introduction
Approaches to Machine Translation (MT) using
Data-Oriented Parsing (DOP: (Bod, 1998; Bod et
al., 2003)) require ?source,target? tree fragments
aligned at sentential and sub-sentential levels. In
previous approaches to Data-Oriented Translation
(DOT: (Poutsma, 2000; Hearne and Way, 2003)),
such fragments were produced manually. This is
time-consuming, error-prone, and requires consid-
erable expertise of both source and target languages
as well as how they are related. The obvious solu-
tion, therefore, is to automate the process of sub-
sentential alignment. However, while there are
many approaches to sentential alignment e.g. (Kay
and Ro?scheisen, 1993; Gale & Church, 1993), no
methods exist for aligning non-isomorphic phrase-
structure (PS) tree fragments at sub-sentential level
for use in MT. (Matsumoto et al, 1993) align
?source,target? dependency trees, with a view to re-
solve parsing ambiguities, but their approach can-
not deal with complex or compound sentences.
Other researchers (Imamura, 2001) also use phrase-
alignment in parsing but in DOT the translation
fragments are already in the form of parse-trees.
(Eisner, 2003) outlines a computationally expensive
structural manipulation tool which he has used for
intra-lingual translation but has yet to apply to inter-
lingual translation. (Gildea, 2003) performs tree-to-
tree alignment, but treats it as part of a generative
statistical translation model, rather than a seperate
task. The method of (Ding et al, 2003) can cope
with a limited amount of non-isomorphism, but the
algorithm is only suitable for use with dependency
trees.
We develop a novel algorithm which automati-
cally aligns translationally equivalent tree fragments
in a fast and consistent fashion, and which requires
little or no knowledge of the language pair. Our ap-
proach is similar to that of (Menezes and Richard-
son, 2003), who use a best-first approach to align
dependency-type tree structures.
We conduct a number of experiments on the
English-French section of the Xerox HomeCentre
corpus. Using the manual alignment of (Hearne
and Way, 2003) as a ?gold standard?, we show that
our algorithm identifies sub-structural translational
equivalences with 73.7% precision and 67.84% re-
call. Furthermore, we replicate previous DOT ex-
periments performed using manually aligned data.
However, we use data aligned by our novel al-
gorithm and evaluate the output translations. We
demonstrate that while coverage decreases by 10%,
the translations output are of comparable quality.
These results indicate that our automatic alignment
algorithm provides a serious alternative to manual
alignment.
The remainder of this paper is organised as fol-
lows: in section 2, we discuss related research in
more detail, while in section 3, we provide an over-
iew of DOT. We present our algorithm in section 4,
and in section 5 describe the experiments conducted
together with the results obtained. Finally, we con-
clude and provide avenues for further research.
2 Related Research
Several approaches to sub-structural alignment of
tree representations have been proposed.
(Matsumoto et al, 1993) and (Imamura, 2001)
focus on using alignments to help resolve pars-
ing ambiguities. As we wish to develop an align-
ment process for use in MT rather than parsing, this
makes their approaches unsuitable for our use.
(Eisner, 2003) presents a tree-mapping method
for use on dependency trees which he claims can be
adapted for use with PS trees. He uses dynamic pro-
gramming to break tree pairs into pairs of aligned
elementary trees, similar to DOT. However, he aims
to estimate a translation model from unaligned data,
whereas we wish to align our data off-line. Cur-
rently, he has used his algorithm to perform intra-
lingual translation but has yet to develop and apply
real models to inter-lingual MT.
(Gildea, 2003) outlines an algorithm for use in
syntax-based statistical models of MT, applying a
statistical TSG with probabilities parameterized to
generate the target tree conditioned on the structure
of the source tree. His approach is unsuitable for
DOT as it involves altering the shape of trees in or-
der to impose isomorphism and the algorithm does
not always generate a complete target tree structure.
However, unlike (Gildea, 2003), we treat the prob-
lem of alignment as a seperate task rather than as
part of a generative translation model.
(Ding et al, 2003) and (Menezes and Richard-
son, 2003) also present approaches to the alignment
of tree structures. Both deal with dependency struc-
tures rather than PS trees. (Ding et al, 2003) out-
line an algorithm to extract word-level alignments
using structural information taken from parallel de-
pendency trees. They fix the nodes of tree pairs
based on word alignments deduced statistically and
then proceed by partitioning the tree into treelet
pairs with the fixed nodes as their roots. Their al-
gorithm relies on the fact that, in dependency trees,
subtrees are headed by words rather than syntactic
labels, making it unsuitable for our use.
(Menezes and Richardson, 2003) employ a best-
first strategy and use a small alignment grammar
to extract transfer mappings from bilingual corpora
for use in translation. They use a bilingual dictio-
nary and statistical techniques to supply translation
pair candidates and to identify multi-word terms.
Lexical correspondences are established using the
lexicon of 98,000 translation pairs and a deriva-
tional morphology component to match other lexi-
cal items. Nodes are then aligned using these lexical
correspondences along with structural information.
Our algorithm uses a similar methodology. How-
ever, (Menezes and Richardson, 2003) use logical
forms, which constitute a variation of dependency
trees that normalize both the lexical and syntactic
form of examples, whereas we align PS trees.
Although the methods outlined above have
achieved promising results, only the approach of
(Menezes and Richardson, 2003) seems relevant
to our goal, even though they deal with abstract
dependency-type structures rather than PS trees.
3 Data-Oriented Translation
Data-Oriented Translation (DOT) (Poutsma, 2000;
Hearne and Way, 2003), which is based on Data-
Oriented Parsing (DOP) (Bod, 1998; Bod et al,
2003), comprises a context-rich, experience-based
approach to translation, where new translations
are derived with reference to grammatical analy-
ses of previous translations. DOT exploits bilin-
gual treebanks comprising linguistic representations
of previously seen translation pairs, as well as ex-
plicit links which map the translational equivalences
present within these pairs at sub-sentential level ?
an example of such a linked translation pair can be
seen in Figure 1(a). Analyses and translations of
the input are produced simultaneously by combin-
ing source and target language fragment pairs de-
rived from the treebank trees.
3.1 Fragmentation
The tree fragment pairs used in Tree-DOT are
called subtree pairs and are extracted from bilingual
aligned treebank trees. The two decomposition op-
erators, which are similar to those used in Tree-DOP
but are refined to take the translational links into ac-
count, are as follows:
? the root operator which takes any pair of linked
nodes in a tree pair to be the roots of a subtree pair
and deletes all nodes except these new roots and all
nodes dominated by them;
? the frontier operator which selects a (possibly
empty) set of linked node pairs in the newly cre-
ated subtree pairs, excluding the roots, and deletes
all subtree pairs dominated by these nodes.
Allowing the root operator to select the root nodes
of the original treebank tree pair and then the fron-
tier operator to select an empty set of node pairs
ensures that the original treebank tree pair is al-
ways included in the fragment base ? in Figure 1,
fragment (a) exactly matches the original treebank
tree pair from which fragments (a) ? (f) were de-
rived. Fragments (b) and (f) were also derived by
allowing the frontier operator to select the empty
set; the root operation selected node pairs <A,N>
and <NPadj,NPdet> respectively. Fragments (c),
(d) and (e) were derived by selecting all further pos-
sible combinations of node pairs by root and fron-
tier.
(a) (b)
VPv
V NPadj
clearing A N
paper jams
NPpp
N PP
resolution P NPdet
de D NPap
les N N
incidents papier
A
paper
N
papier
(c) (d)
VPv
V NPadj
clearing A N
jams
NPpp
N PP
resolution P NPdet
de D NPap
les N N
incidents
NPadj
A N
jams
NPdet
D NPap
les N N
incidents
(e) (f)
VPv
V NPadj
clearing
NPpp
N PP
resolution P NPdet
de
NPadj
A N
paper jams
NPdet
D NPap
les N N
incidents papier
Figure 1: DOT fragments generated via root and frontier
3.2 Translation
The DOT composition operator is defined as fol-
lows. The composition of tree pairs <s1,t1> and
<s2,t2> (<s1,t1> ? <s2,t2>) is only possible if
? the leftmost non-terminal frontier node of s1 is of
the same syntactic category (e.g. S, NP, VP) as the
root node of s2, and
? the leftmost non-terminal frontier node of s1?s
linked counterpart in t1 is of the same syntactic cat-
egory as the root node of t2.
The resulting tree pair consists of a copy of s1 where
s2 has been inserted at the leftmost frontier node and
a copy of t1 where t2 has been inserted at the node
linked to s1?s leftmost frontier node, as illustrated in
Figure 2.
The DOT probability of a translation derivation is
the joint probability of choosing each of the subtree
pairs involved in that derivation. The probability of
selecting a subtree pair is its number of occurrences
in the corpus divided by the number of pairs in the
corpus with the same root nodes as it:
P (< es, et >) =
|<es,et>|
?
<us,ut>:r(<us,ut>)=r(<es,et>) |<us,ut>|
The probability of a derivation in DOT is the prod-
uct of the probabilities of the subtree pairs involved
ROOT
VPv PERIOD
V N .
click
LISTITEM
VPverb PERIOD
V PP .
cliquez P N
sur
?
N
Save
N
Enregistrer
=
ROOT
VPv PERIOD
V N .
click Save
LISTITEM
VPverb PERIOD
V PP .
cliquez P N
sur Enregistrer
Figure 2: The DOT composition operation
in building that derivation. Thus, the probability of
derivation <s1,t1> ? ... ? <sn,tn> is given by
P (< s1, t1 > ?...? < sn, tn >) =
?
i
P (< si, ti >)
Again, a translation can be generated by many dif-
ferent derivations, so the probability of a transla-
tion ws ??wt is the sum of the probabilities of its
derivations:
P (< ws, wt >) =
?
<tsi ,tti> yields <ws,wt>
P (< tsi , tti >)
Selection of the most probable translation via Monte
Carlo sampling involves taking a random sample of
derivations and outputting the most frequently oc-
curring translation in the sample.
4 Our Algorithm
The operation of a DOT system is dependent on the
availability of bilingual treebanks aligned at senten-
tial and sub-sentential level. Our novel algorithm
attempts to fully automate sub-sentential alignment
using an approach inspired by that of (Menezes and
Richardson, 2003). The algorithm takes as input a
pair of ?source,target? PS trees and outputs a map-
ping between the nodes of the tree pair.
As with the majority of previous approaches, the
algorithm starts by finding lexical correspondences
between the source and target trees. Our lexicon
is built automatically using a previously developed
word aligner based on the k-vec aligner as outlined
by (Fung & Church, 1994). This lexical aligner uses
a combination of automatically extracted cognate
information, mutual information and probabilistic
measures to obtain one-to-one lexical correspon-
dences between the source and target strings. Dur-
ing lexical alignment, function words are excluded
because, as they are the most common words in a
language, they tend to co-occur frequently with the
content words they precede. This can lead to the
incorrect alignment of content words with function
words.
The algorithm then proceeds from the aligned
lexical terminal nodes in a bottom-up fashion, us-
ing a mixture of node label matching and structural
information to perform language-independent link-
ing between all ?source,target? node pairs within the
trees. As with (Menezes and Richardson, 2003),
it uses a best-first approach. After each step, new
linked node pairs are added to the current list of
linked nodes. The links made between the nodes
are fixed, thus restricting the freedom of alignment
for the remaining unaligned nodes in the tree pair.
The methods of the algorithm are applied to each
new linked node pair in turn until no new node pairs
can be added. The algorithm consists of five main
methods which are performed on each linked node
pair in the list:
Verb + Object Align (Figure 3): We have a linked source-
target node pair ?s,t?. s and t are both verbs, are the
leftmost children in their respective trees, both have VP
parent nodes and they have the same number of siblings
which have similar syntactic labels. We align the corre-
sponding siblings of s and t. This aligns the objects of
the source verb with the equivalent objects of the target
verb. We also align the parents of s and t.
S
PRON VPaux
you MODAL VPv
can V NPadj
scan A N
entire pages
S[dec]
PRON VPverb
vous MODAL VPverb[main]
pouvez V NPdet
nume?riser D NPap
des N A
pages entie`res
Figure 3: Verb + Object Align: the dashed lines represent
the links made by Verb + Object Align when the current linked
node pair is ?MODAL,MODAL?.
Parent Align (Figure 4): We have a current linked source-
target node pair ?s,t? with unlinked parents pars and part
respectively. All the sister nodes of s are aligned with
sister nodes of t. We link pars and part. If s and t each
have one unlinked sister, but the remaining sisters of s are
aligned with sister nodes of t, link the unlinked sisters and
link pars with part.
NP/VP Align (Figure 5): We have a linked source-target
node pair ?s,t? and s and t are both nouns. Traverse up
the source tree to find the topmost NP node nps dominat-
ing s and traverse up the target tree to find the topmost
Nmod
A NP
color print head
NPap
NP N
te?te d?impression couleur
Figure 4: Parent Align: The dashed lines are the links made
by Parent Align, when ?color,couleur? is the current linked node
pair.
target NP node npt dominating t. We link nps and npt.
We then traverse down from nps and npt to the leftmost
leaf nodes ( ls and lt) in the source and target subtrees
rooted at nps and npt. If ls and lt have similar labels,
we link them. This helps to preserve the scope of noun-
phrase modifiers. If s and t are both verbs, we perform a
similar method, this time linking the topmost VP nodes
in the source and target trees.
NP
D NPadj
a A N
pending document
NPdet
D NPpp
un N PP
document P NPpp
en N PP
attenteP N
de impression
Figure 5: NP Align: the dashed lines represent the links made
by NP Align when the current linked node pair is ?N,N?.
Child Align (Figure 6): This method is similar to that of Par-
ent Align. We have a current linked source-target node
pair ?s,t?. Each node has the same number of children
and these children have similar node labels. We link their
corresponding children.
S
NP VPcop
PRON N Vcop NP
is PRON NPadj
your imagination your A N
only limitation
S
NPdet VPcop
D N Vcop NPdet
est D NPap
votre imagination votre A N
seule limite
Figure 6: Child Align: the dashed lines represent the links
made by Child Align when the current linked node pair is ?S,S?.
Subtree Align: We have a linked source-target node pair ?s,t?.
If the subtrees rooted at s and at t are fully isomorphic, we
link the corresponding nodes within the subtrees. This
accounts for the fact that trees may not be completely
isomorphic from their roots but may be isomorphic at
subtree level.1
1Originally we used a method isomophic which checked for
Once lexical correspondences have been estab-
lished, the methods outlined above use structural in-
formation to align the ?source,target? nodes. The
comparison of ?source,target? node labels during
alignment ensures that sub-structures with corre-
sponding syntactic categories are aligned. If the
algorithm fails to find any alignments between the
source and target tree pairs, due to the absence
of initial lexical correspondences, we align the
?source,target? root nodes.
5 Experiments and results
Previous DOT experiments (Hearne and Way, 2003)
were carried out on a subset of the HomeCentre
corpus consisting of 605 English-French sentence
pairs from Xerox documentation parsed into LFG
c(onstituent)- and f(unctional)-structure representa-
tions and aligned at sentence level. This bilingual
treebank constitutes a linguistically complex frag-
ment base containing many ?hard? translation ex-
amples, including cases of nominalisations, pas-
sivisation, complex coordination and combinations
thereof. Accordingly, the corpus would appear to
present a challenge to any MT system.
The insertion of the links denoting translational
equivalence for the set of tree pairs used in the pre-
vious experiments was performed manually. We
have applied our automatic sub-structural alignment
algorithm to this same set of 605 tree pairs and
evaluated performance using two distinct methods.
Firstly, we used the manual alignments as a ?gold
standard? against which we evaluated the output of
the alignment algorithm in terms of precision, recall
and f-score. The results of this evaluation are pre-
sented in Section 5.1. Secondly, we repeated the
DOT experiments described in (Hearne and Way,
2003) using the automatically generated alignments
in place of those determined manually. We evalu-
ated the output translations in terms of IBM Bleu
scores, precision, recall and f-score and present
these results in Section 5.2.
5.1 Evaluation of alignment quality
Using the manually aligned tree pairs as a ?gold
standard?, we evaluated the performance of each
of the five methods which constitute the alignment
algorithm both individually and in combination.
These evaluations are summarised in Figures 7 and
8 respectively.
The alignment process is always initialised by
finding word correspondences between the source
isomorphism from the roots downwards, assuming a root-root
correspondence. However, this significantly decreased the per-
formance of the aligner.
PRECISION RECALL F-SCORE
Lex 0.6800 0.3057 0.4212
Par 0.7471 0.4983 0.5978
NP/VP 0.7206 0.4879 0.5819
Child 0.7045 0.3856 0.4984
Verb + Object 0.6843 0.3191 0.4352
Figure 7: Individual evaluation of alignment methods
PRECISION RECALL F-SCORE
Par + Child 0.7525 0.5588 0.6414
Par + NP/VP 0.7373 0.6106 0.6680
Par + Child + NP/VP 0.7411 0.6587 0.6974
All 0.7430 0.6686 0.7039
All + Subtree 0.7370 0.6784 0.7064
Figure 8: Evaluation of combined alignment methods
and target trees, meaning that lexical alignment is
carried out regardless of which other method or
combination of methods is included. The low rate of
recall achieved by the lexical alignment process of
0.3057, shown in Figure 7, can be largely attributed
to the fact that it does not align function words. We
achieve high precision relative to recall ? as is gen-
erally preferred for automatic procedures ? indicat-
ing that the alignments induced are more likely to
be ?partial? than incorrect.
When evaluated individually, the Parent Align
method performs best, achieving an f-score of
0.5978. Overall, the highest f-score of 0.7064 is
achieved by using all methods, including the addi-
tional subtree method, in combination.
5.2 Evaluation of translation quality
In order to evaluate the impact of using automat-
ically generated alignments on translation quality,
we repeated the DOT experiments described in
(Hearne and Way, 2003) using these alignments in
place of manually determined translational equiva-
lences.
In order to ensure that differences in the results
achieved could be attributed solely to the differ-
ent sub-structural alignments imposed, we used pre-
cisely the same 8 training/test set splits as before,
where each training set contained 545 parsed sen-
tence pairs, each test set 60 sentences, and all words
occurring in the source side of the test set alo oc-
curred in the source side of the training set (but not
necessarily with the same lexical category). As be-
fore, all translations carried out were from English
into French and the number of samples taken during
the disambiguation process was limited to 5000.
Due to constraints on time and memory, data-
oriented language processing applications gener-
ally limit the size of the fragment base by exclud-
Bleu/Auto Bleu/Man F-Score/Aut. F-Score/Man
LD1 0.0605 0.2627 0.3558 0.5506
LD2 0.1902 0.3018 0.4867 0.5870
LD3 0.1983 0.3235 0.4957 0.6045
LD4 0.214 0.3235 0.5042 0.6069
Figure 9: Evaluation over all translations
ing larger fragments. In these experiments, we in-
creased the size of the fragment base incrementally
by initially allowing only fragments of link depth
(LD) 1 and then including those of LD 2, 3 and 4. 2
We evaluated the output translations in terms of
IBM Bleu scores using the NIST MT Evaluation
Toolkit3 and in terms of precision, recall and f-score
using the NYU General Text Matcher.4 We sum-
marise our results and reproduce and extend those
of (Hearne and Way, 2003)5 in Figures 9, 10 and
11.
Results over the full set of output translations,
summarised in Figure 9, show that using the man-
ually linked fragment base results in significantly
better overall performance at all link depths (LD1
- LD4) than using the automatic alignments. How-
ever, both metrics used assign score 0 in all in-
stances where no translation was output by the sys-
tem. The comparatively poor scores achieved us-
ing the automatically induced alignments reflect the
fact that these alignments give poorer coverage at all
depths than those determined manually (47.71% vs.
66.46% at depth 1, 56.39% vs. 67.92% at depths 2
- 4).
The results in Figure 10 include scores only
where a translation was produced. Here, transla-
tions produced using manual alignments score bet-
ter only at LD 1; better performance is achieved at
LD 2 - 4 using the automatically linked fragment
base. Again, this may ? at least in part ? be an issue
of coverage: many of the sentences for which only
the manually aligned fragment base produces trans-
lations are translationally complex and, therefore,
more likely to be only partially correct and achieve
poor scores.
Finally, we determined the subset of sentences
for which translations were produced both when
the manually aligned fragment bases were used and
2The link depth of a fragment pair is defined as greatest
number of steps taken which depart from a linked node to get
from the root node to any frontier nodes (Hearne and Way,
2003).
3http://www.nist.gov/speech/tests/mt/mt2001/resource/
4http://nlp.cs.nyu.edu/GTM/
5The Bleu scores shown here differ from those published in
(Hearne and Way, 2003) as a result of recent modifications to
the NIST MT Evaluation Kit.
Bleu/Auto Bleu/Man F-Score/Auto F-Score/Man
LD1 0.6118 0.6591 0.7900 0.8090
LD2 0.7519 0.7144 0.8751 0.8446
LD3 0.7790 0.7610 0.8887 0.8688
LD4 0.7940 0.7611 0.8930 0.8736
Figure 10: Evaluation over translations produced
Bleu/Auto Bleu/Man F-Score/Auto F-Score/Man
LD1 0.5945 0.6363 0.7918 0.7989
LD2 0.7293 0.7382 0.8823 0.8629
LD3 0.7700 0.7930 0.8938 0.8913
LD4 0.7815 0.7940 0.8964 0.8933
Figure 11: Evaluation of sentences translated by both
alignment methods
when the automatically linked ones were used. Fig-
ure 11 summarises the results achieved when eval-
uating only these translations. In terms of Bleu
scores, translations produced using manual align-
ments score slightly better at all depths. However,
as link depth increases the gap narrows consistently
and at depth 4 the difference in scores is reduced
to just 0.0125. In terms of f-scores, the translations
produced using automatic alignments actually score
better than those produced using manual alignments
at depths 2 - 4.
5.3 Discussion
Our first evaluation method (Section 5.1) is, per-
haps, the obvious one to use when evaluating align-
ment performance. However, the results of this
evaluation, which show best f-scores of 70%, pro-
vide no insight into the effect using these align-
ments has on translation accuracy. Evaluating these
alignments in context ? by using them in the DOT
system for which they were intended ? gives us a
true picture of their worth. Crucially, in Section 5.2
we showed that using automatic rather than manual
alignments results in translations of extremely high
quality, comparable to those produced using manual
alignments.
In many cases, translations produced using au-
tomatic alignments contain fewer errors involving
local syntactic phenomena than those produced us-
ing manual alignment. This suggests that, as links
between function words are infrequent in the au-
tomatic alignments, we achieve better modelling
of phenomena such as determiner-noun agreement
because the determiner fragments do not gener-
ally occur without context. For example, there are
relatively few instances of ?D?the? aligned with
?D?le/la/l?/les? in the automatic alignment com-
pared to the manual alignment.
On the other hand, we achieve 10% less coverage
when translating using the automatic alignments.
The automatic alignments are less likely to identify
non-local phenomena such as long-distance depen-
dencies. Consequently, the sentences only trans-
lated when using manual alignments are generally
longer and more complex than those translated by
both. While a degree of trade-off between coverage
and accuracy is to be expected, we would like to
increase coverage while maintaining or improving
translation quality. Improvements to lexical align-
ment should prove valuable in this regard. While
we expect translation quality to improve as depth
increases, experiments using the automatical align-
ment show disproportionately poor performance at
depth 1. The majority of links in the depth 1 frag-
ment base are inserted using the lexical aligner, in-
dicating that these are less than satisfactory. We ex-
pect improvements to the lexical aligner to signifi-
cantly improve the overall performance of the align-
ment algorithm and, consequently, the quality of the
translations produced. Lexical alignment is crucial
in identifying complex phenomena such as long dis-
tance dependencies. Using machine-readable bilin-
gual dictionaries or, alternatively, manually estab-
lished word-alignments to intiate the automatic sub-
structural alignment algorithm may provide more
accurate results.
6 Conclusions and future work
We have presented an automatic algorithm which
aligns bilingual context-free phrase-structure trees
at sub-structural level and applied this algorithm to
a subset of the English-French section of the Home-
Centre corpus. We have outlined detailed eval-
uations of our algorithm. They show that while
translation coverage was 10% lower using the au-
tomatically aligned data, the quality of the trans-
lations produced is comparable to the quality of
those produced using manual alignments. While
DOT systems produce very high quality transla-
tions in reasonable time, resource acquisition re-
mains an issue. Manual sub-structural alignment is
time-consuming, error-prone and requires consider-
able linguistic expertise. Our alignment method, on
the other hand, is efficient, consistent and language-
independent, constituting a viable alternative to
manual sub-structural alignment; thus solving the
data acquisition problem.
We intend to apply our automatic alignment
methodology to the full English-French section of
the HomeCentre corpus, as well as the English-
German and French-German sections, and perform
experiments to further validate the the language-
independent nature of both our alignment algorithm
and the data-oriented approach to translation. We
also plan to automatically parse existing bitexts,
thus creating further resources for use with our
DOT system and, together with our aligner, en-
abling much larger-scale DOT-based translation ex-
periments than have been performed to date.
7 Aknowledgements
The work presented in this paper is partly supported
by an IRCSET 6 PhD Fellowship Award.
References
Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI, Stanford, CA.
Rens Bod, Remko Scha and Khalil Sima?an. (eds.) 2003.
Data-Oriented Parsing. CSLI, Stanford CA.
Yuan Ding, Dan Gildea and Martha Palmer. 2003. An
Algorithm for Word-Level Alignment of Parallel De-
pendency Trees. MT Summit IX. New Orleans, LO.,
pp.95?101.
Jason Eisner. 2003. Learning Non-Isomorphic Tree
Mappings for Machine Translation. In Proceedings of
the 41st COLING, Sapporo, Japan.
Pascale Fung & Ken W. Church. 1994. K-vec: A New
Approach for Aligning Parallel Texts. In Proceedings
of COLING 94, Kyoto, Japan, pp.1096-1102.
William A. Gale & Ken W. Church. 1993. A program
for aligning sentences in bilingual corpora. Computa-
tional Linguistics 19(1):75?102.
Daniel Gildea. 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of the 41st ACL.
Sapporo, Japan, pp.80?87.
Mary Hearne and Andy Way. 2003. Seeing the Wood
for the Trees: Data-Oriented Translation. MT Summit
IX. New Orleans, LO., pp.165?172.
Kenji Imamura. 2001. Hierarchical Phrase Alignment
Harmonized With Parsing. In Proceedings of the
Sixth Natural Language Processing Pacific Rim Sym-
posium. Tokyo, Japan, pp.377?384.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics
19(1):121?142.
Yuji Matsumoto, Ishimoto Hiroyuki and Takehito Ut-
suro. 1993. Structural Matching of Parallel Texts. In
Proceedings of the 31st ACL. Columbus, OH., pp.23?
30.
Arul Menezes and Stephen D. Richardson. 2003. A
Best-First Alignment Algorithm for Automatic Ex-
traction of Transfer Mappings from Bilingual Cor-
pora. In M. Carl & A. Way (eds.) Recent Advances
in Example-Based Machine Translation. Kluwer
Academic Publishers, Dordrecht, The Netherlands,
pp.421?442.
Arjen Poutsma. 2000. Data-Oriented Translation. In
18th COLING, Saarbru?cken, Germany, pp.635?641.
6http://www.ircset.ie
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 183?190,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Hybrid Example-Based SMT: the Best of Both Worlds?
Declan Groves
School of Computing
Dublin City University
Dublin 9, Ireland
dgroves@computing.dcu.ie
Andy Way
School of Computing
Dublin City University
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
(Way and Gough, 2005) provide an in-
depth comparison of their Example-Based
Machine Translation (EBMT) system with
a Statistical Machine Translation (SMT)
system constructed from freely available
tools. According to a wide variety of au-
tomatic evaluation metrics, they demon-
strated that their EBMT system outper-
formed the SMT system by a factor of two
to one.
Nevertheless, they did not test their EBMT
system against a phrase-based SMT sys-
tem. Obtaining their training and test
data for English?French, we carry out a
number of experiments using the Pharaoh
SMT Decoder. While better results are
seen when Pharaoh is seeded with Giza++
word- and phrase-based data compared to
EBMT sub-sentential alignments, in gen-
eral better results are obtained when com-
binations of this ?hybrid? data is used
to construct the translation and probabil-
ity models. While for the most part the
EBMT system of (Gough & Way, 2004b)
outperforms any flavour of the phrase-
based SMT systems constructed in our
experiments, combining the data sets au-
tomatically induced by both Giza++ and
their EBMT system leads to a hybrid sys-
tem which improves on the EBMT system
per se for French?English.
1 Introduction
(Way and Gough, 2005) provide what are to our
knowledge the first published results comparing
Example-Based and Statistical models of Machine
Translation (MT). Given that most MT research car-
ried out today is corpus-based, it is somewhat sur-
prising that until quite recently no qualitative re-
search existed on the relative performance of the two
approaches. This may be due to a number of factors:
the relative unavailability of EBMT systems, the
lack of participation of EBMT researchers in com-
petitive evaluations or the dominance in the MT re-
search community of the SMT approach?whenever
one paradigm finds favour with the clear majority of
MT practitioners, the assumption made by most of
the community is that this way of doing things is
clearly better than the alternatives.
Like (Way and Gough, 2005), we find this regret-
table: the only basis on which such views should
be allowed to permeate our field is following exten-
sive testing and evaluation. Nonetheless, given that
no EBMT systems are freely available, very few re-
search groups are in the position of being able to
carry out such work.
This paper extends the work of (Way and Gough,
2005) by testing EBMT against phrase-based mod-
els of SMT, rather than the word-based models used
in this previous work. In so doing, it provides a
more complete evaluation of the main question at
hand, namely whether an SMT system outperforms
an EBMT system on reasonably large training and
test sets.
We obtained the same training and test data used
183
in (Way and Gough, 2005), and evaluated a num-
ber of SMT systems which use the Pharaoh decoder1
against the Marker-Based EBMT system of (Gough
& Way, 2004b), for French?English and English?
French. We provide results using a range of au-
tomatic evaluation metrics: BLEU (Papineni et al,
2002), Precision and Recall (Turian et al, 2003), and
Word- and Sentence Error Rates. (Way and Gough,
2005) observe that EBMT tends to outperform a
word-based SMT model, and our experiments show
that a number of different phrase-based SMT sys-
tems still tend to fall short of the quality obtained
via EBMT for these evaluation metrics. However,
when Pharaoh is seeded with the data sets automati-
cally induced by both Giza++ and their EBMT sys-
tem, better results are seen for French?English than
for the EBMT system per se.
The remainder of the paper is constructed as fol-
lows. In section 2, we summarize the main ideas be-
hind typical models of SMT and EBMT, as well as
the EBMT system of (Gough & Way, 2004b) used in
our experiments. In section 3, we revisit the exper-
iments and results carried out by (Way and Gough,
2005). In section 4, we describe our extensions to
their work, and compare their findings to ours, and
in section 5, present a number of hybrid SMT mod-
els. Finally, we conclude and offer some thoughts
for future work in section 6, and in section 7 present
some further comments on the narrowing gap be-
tween EBMT and phrase-based SMT.
2 Example-Based and Statistical Models of
Translation
A sine qua non for both EBMT and SMT is a set of
sentences in one language aligned with their trans-
lations in another. Although similar in that both
models of translation automatically induce transla-
tion knowledge from this resource, there are signifi-
cant differences regarding both the type of informa-
tion learnt and how this is brought to bear in dealing
with new input.
2.1 EBMT
Given a new input string, EBMT models use three
separate processes in order to derive translations:
1http://www.isi.edu/licensed-sw/pharaoh/
1. Searching the source side of the bitext for
?close? matches and their translations;
2. Determining the sub-sentential translation links
in those retrieved examples;
3. Recombining relevant parts of the target trans-
lation links to derive the translation.
Searching for the best matches involves determin-
ing a similarity metric based on word occurrences
and part-of-speech labels, generalised templates and
bilingual dictionaries. The recombination process
depends on the nature of the examples used in
the first place, which may include aligning phrase-
structure (sub-)trees (Hearne & Way, 2003) or de-
pendency trees (Watanabe et al, 2003), or using
placeables (Brown, 1999) as indicators of chunk
boundaries.
Another method?and the one used in the EBMT
system used in our experiments?is to use a set
of closed-class words to segment aligned source
and target sentences and to derive an additional set
of lexical and phrasal resources. (Gough & Way,
2004b) base their work on the ?Marker Hypothe-
sis? (Green, 1979), a universal psycholinguistic con-
straint which posits that languages are ?marked?
for syntactic structure at surface level by a closed
set of specific lexemes and morphemes. In a pre-
processing stage, (Gough & Way, 2004b) use 7 sets
of marker words for English and French (e.g. de-
terminers, quantifiers, conjunctions etc.), which to-
gether with cognate matches and mutual information
scores are used to derive three new data sources: sets
of marker chunks, generalised templates and a lexi-
con.
In order to describe this in more detail, we revisit
an example from (Gough & Way, 2004a), namely:
(1) each layer has a layer number =?chaque
couche a un nombre de la couche
From the sentence pair in (1), the strings in (2)
are generated, where marker words are automati-
cally tagged with their marker categories:
184
(2) <QUANT> each layer has <DET> a
layer number =?<QUANT> chaque
couche a <DET> un nombre <PREP>
de la couche
Taking into account marker tag information (label,
and relative sentence position), and lexical similar-
ity, the marker chunks in (3) are automatically gen-
erated from the marker-tagged strings in (2):
(3) a. <QUANT> each layer has: <QUANT>
chaque couche a
b. <DET> a layer number: <DET> un
nombre de la couche
(3b) shows that n:m alignments are possible (the two
French marker chunks un nombre and de la couche
are absorbed into one following the lexical similari-
ties between layer and couche and number and nom-
bre, respectively) given the sub-sentential alignment
algorithm of (Gough & Way, 2004b).
By generalising over the marker lexicon, a set
of marker templates is produced by replacing the
marker word by its relevant tag. From the examples
in (3), the generalised templates in (4) are derived:
(4) a. <QUANT> layer has: <QUANT>
couche a
b. <DET> layer number: <DET> nombre
de la couche
These templates increase the robustness of the sys-
tem and make the matching process more flexible.
Now any marker word can be inserted after the rele-
vant tag if it appears with its translation in the lexi-
con, so that (say) the layer number can now be han-
dled by the generalised template in (4b) and insert-
ing a (or all) translation(s) for the in the system?s
lexicon.
2.2 Word- and Phrase-Based SMT
SMT systems require two large probability tables in
order to generate translations of new input:
1. a translation model induced from a large
amount of bilingual data;
2. a target language model induced from a(n even)
large(r) quantity of separate monolingual text.
Essentially, the translation model establishes the
set of target language words (and more recently,
phrases) which are most likely to be useful in trans-
lating the source string, while the language model
tries to assemble these words (and phrases) in the
most likely target word order. The language model
is trained by determining all bigram and/or trigram
frequency distributions occurring in the training
data, while the translation model takes into account
source and target word (and phrase) co-occurrence
frequencies, sentence lengths and the relative sen-
tence positions of source and target words.
Until quite recently, SMT models of translation
were based on the simple word alignment models
of (Brown et al, 1990). Nowadays, however, SMT
practitioners also get their systems to learn phrasal
as well as lexical alignments (e.g. (Koehn et al,
2003); (Och, 2003)). Unsurprisingly, the quality
obtained by today?s phrase-based SMT systems is
considerably better than that obtained by the poorer
word-based models.
3 Comparing EBMT and Word-Based
SMT
(Way and Gough, 2005) obtained a large translation
memory from Sun Microsystems containing 207,468
English?French sentence pairs, of which 3,939 sen-
tence pairs were randomly extracted as a test set,
with the remaining 203,529 sentences used as train-
ing data. The average sentence length for the En-
glish test set was 13.1 words and 15.2 words for the
corresponding French test set. The EBMT system
used was their Marker-based system as described in
section 2.1 above. In order to create the necessary
SMT language and translation models, they used:
? Giza++ (Och & Ney, 2003);2
? the CMU-Cambridge statistical toolkit;3
? the ISI ReWrite Decoder.4
Translation was performed from English?French
and French?English, and the resulting translations
were evaluated using a range of automatic metrics:
BLEU (Papineni et al, 2002), Precision and Recall
2http://www.isi.edu/?och/Giza++.html
3http://mi.eng.cam.ac.uk/?prc14/toolkit.html
4http://www.isi.edu/licensed-sw/rewrite-decoder/
185
(Turian et al, 2003), and Word- and Sentence Error
Rates. In order to see whether the amount of train-
ing data affected the (relative) performance of the
EBMT and SMT systems, (Way and Gough, 2005)
split the training data into three sets, of 50K (1.1M
words), 100K (2.4M words) and 203K (4.8M words)
sentence pairs (TS1?TS3 in what follows).
3.1 English?French Results
Table 1: Comparing the EBMT system of (Gough &
Way, 2004b) with a Word-Based SMT (WB-SMT) system for
English?French.
BLEU Prec. Recall WER SER
TS1 WB-SMT .2971 .6739 .5912 54.9 90.8
EBMT .3318 .6525 .6183 54.3 89.2
TS2 WB-SMT .3375 .6824 .5962 51.1 89.9
EBMT .4534 .7355 .6983 44.8 77.5
TS3 WB-SMT .3223 .6513 .5704 53.5 89.1
EBMT .4409 .6727 .6877 52.4 65.6
The results obtained by (Gough & Way, 2004b)
for English?French for their EBMT system and
word-based SMT (WB-SMT) are given in Table 1.
Essentially, all the automatic evaluation metrics bar
one (Precision) suggest that EBMT can outperform
SMT from English?French. Surprisingly, however,
apart from SER, all evaluation scores are higher us-
ing 100K sentence pairs as training data rather than
the full 203K sentences. It is generally assumed that
increasing the size of the training data for corpus-
based MT systems will improve the quality of the
output translations. (Way and Gough, 2005) observe
that while this dip in performance may be due to a
degree of over?fitting, they intend to carry out some
variance analysis on these results (e.g. performing
bootstrap-resampling on the test set (Koehn, 2004)),
or re-test with different sample test sets in order
to investigate whether the same phenomenon is ob-
served.
With respect to SER, however, for both SMT and
EBMT, the figures improve as more training data is
made available. However, the improvement is much
more significant for EBMT (20.6%) than for SMT
(0.1%). While the WER scores are much the same,
indicating that both systems are identifying reason-
able target vocabulary that should appear in the out-
put translation, the vast differences in SER using
TS3 indicate that a system containing essentially no
information about target syntax has very little hope
of arranging these target words in the right order.
On the contrary, even a system containing some ba-
sic knowledge of how phrases fit together such as
the Marker-based EBMT system of (Gough & Way,
2004b) will generate translations of far higher qual-
ity.
3.2 French?English Results
Table 2: Comparing the EBMT system of (Gough & Way,
2004b) with a WB-SMT system for French?English.
BLEU Prec. Recall WER SER
TS1 WB-SMT .3794 .7096 .7355 52.5 86.5
EBMT .2571 .5419 .6314 69.7 89.2
TS2 WB-SMT .3924 .7206 .7433 46.2 81.3
EBMT .4262 .6731 .7962 55.2 66.2
TS3 WB-SMT .4462 .7035 .7240 46.8 80.8
EBMT .4611 .6782 .7441 50.8 51.2
The results obtained by (Way and Gough, 2005)
for French?English translations are presented in Ta-
ble 2. Translating in this language direction is inher-
ently ?easier? than for English?French as far fewer
agreement errors and cases of boundary friction are
likely. Accordingly, all WB-SMT results in Table 2
are better than for the reverse direction, while for
EBMT, improved results are to be seen for BLEU,
Recall and SER.
While the majority of metrics obtained for
English?French indicate that EBMT outperforms
WB-SMT, the results for French?English are by no
means as conclusive. Of the 15 tests, WB-SMT out-
performs EBMT in nine.
4 Comparing EBMT and Phrase-Based
SMT
From the results in the previous sections for French?
English and for English?French, (Way and Gough,
2005) observe that EBMT outperforms WB-SMT in
the majority of tests. If we are to treat each of the
metrics as being equally significant, it can be said
that EBMT appears to outperform WB-SMT by a
factor of two to one. In fact, the only metric for
which EBMT seems to consistently underperform
is precision for French?English which, when we
examine WER, indicates that the EBMT system?s
knowledge of word correspondences is incomplete
and not as comprehensive as that of the WB-SMT
system.
186
However, it has been apparent for some time now
that phrase-based SMT outperforms previous sys-
tems using word-based models. The results obtained
by (Way and Gough, 2005) for SER also indicate
that if phrase-based SMT were used, then improve-
ments in translation quality ought to be seen.
Accordingly, in this section we describe a set
of experiments which extends the work of (Way
and Gough, 2005) by evaluating the Marker-based
EBMT system of (Gough & Way, 2004b) against a
phrase-based SMT system built using the following
components:
? Giza++, to extract the word-level correspon-
dences;
? The Giza++ word alignments are then refined
and used to extract phrasal alignments ((Och &
Ney, 2003); or (Koehn et al, 2003) for a more
recent implementation);
? Probabilities of the extracted phrases are calcu-
lated from relative frequencies;
? The resulting phrase translation table is passed
to the Pharaoh phrase-based SMT decoder
which along with SRI language modelling
toolkit5 performs translation.
4.1 English?French Results
Table 3: Seeding Pharaoh with Giza++ and EBMT sub-
sentential alignments for English?French.
BLEU Prec. Recall WER SER
TS3 GIZA-DATA .3753 .6598 .5879 58.5 86.82
EBMT-DATA .3643 .6661 .5759 61.33 87.99
We seeded the phrase-based SMT system con-
structed from the publicly available resources listed
above with the word- and phrase-alignments derived
via both Giza++ and the Marker-Based EBMT sys-
tem of (Gough & Way, 2004b). Using the full 203K
training set of (Gough & Way, 2004b), and testing
on their near 4K test set, the results are given in Ta-
ble 3. It is clear to see that the Giza++ alignments
obtain better scores than the EBMT sub-sentential
data. Before one considers the full impact of these
results, one should take into account that the size of
5http://www.speech.sri.com/projects/srilm/
the EBMT data set (word- and phrase-alignments)
is 403,317, while there are over four times as many
SMT sub-sentential alignments (1,732,715).
Comparing these results with those in Table 1,
we can see that for the same training-test data,
the phrase-based SMT system outperforms the WB-
SMT system on most metrics, considerably so with
respect to BLEU score (.3753 vs. .3223). WER,
however, is somewhat worse (.585 vs. .535), and
SER remains disappointingly high. Compared to
the EBMT system of (Gough & Way, 2004b), the
phrase-based SMT system still falls well short with
respect to BLEU score (.4409 for EBMT vs. .3573
for SMT), and again, notably for SER (.656 EBMT,
.868 SMT).
4.2 French?English Results
Table 4: Seeding Pharaoh with Giza++ and EBMT sub-
sentential alignments for French?English.
BLEU Prec. Recall WER SER
TS3 GIZA-DATA .4198 .6527 .7100 62.93 82.84
EBMT-DATA .3952 .6151 .6643 74.77 86.21
Again, the phrase-based SMT system was seeded
with the Giza++ and EBMT alignments, trained on
the full 203K training set, and tested on the 4K test
set. The results are given in Table 4. As for English?
French, the Giza++ alignments obtain better scores
than when the EBMT sub-sentential data is used.
Comparing these results with those in Table 2, we
see that the phrase-based SMT system actually does
worse than WB-SMT, which is an unexpected re-
sult6. As expected, therefore, the results for phrase-
based SMT here are worse still compared to EBMT.
5 Towards Hybridity: Merging SMT and
EBMT Alignments
We decided to experiment further by combining
parts of the EBMT sub-sentential alignments with
parts of the data induced by Giza++. In the follow-
ing sections, for both English?French and French?
English, we seed the Pharaoh phrase-based SMT
system with:
6The Pharaoh system is untuned, so as to provide an easily
replicable baseline for other similar research. It is quite possible
that with tuning the phrase-based SMT system will outperform
the word-based system.
187
1. the EBMT phrase-alignments with the Giza++
word-alignments;
2. all the EBMT and Giza++ sub-sentential align-
ments (both words and phrases).
5.1 Giza++ Words and EBMT Phrases
Here we seeded Pharaoh with the word-alignments
induced by Giza++ and the EBMT phrasal chunks
only (i.e. no Giza++ phrases and no EBMT lexical
alignments).
5.1.1 English?French Results
Table 5: Seeding Pharaoh with Giza++ word and EBMT
phrasal alignments for English?French.
BLEU Prec. Recall WER SER
TS3 .3962 .6773 .5913 59.32 85.43
Using the full 203K training set of (Gough &
Way, 2004b), and testing on their near 4K test set,
the results are given in Table 5. Comparing these
figures to those in Table 3, we can see that all au-
tomatic evaluation metrics improve with this hybrid
system configuration. Note that the data set size is
430,336, compared to 1.73M for the phrase-based
SMT system seeded solely with Giza++ alignments.
With respect to the EBMT system per se in Table 1,
these results remain slightly below those figures (ex-
cept for precision).
5.1.2 French?English Results
Table 6: Seeding Pharaoh with Giza++ word and EBMT
phrasal alignments for French?English.
BLEU Prec. Recall WER SER
TS3 .4265 .6424 .6918 68.05 83.40
Running the same experimental set up for the re-
verse language direction gives the results in Table 6.
While recall drops slightly, all the other metrics
show a slight increase compared to the performance
obtained when Pharaoh is seeded with Giza++ word-
and phrase-alignments (cf. Table 4).
5.2 Merging All Data
The following two experiments were carried out by
seeding Pharaoh with all the EBMT and Giza++
sub-sentential alignments, i.e. both words and
phrases.
5.2.1 English?French Results
Table 7: Seeding Pharaoh with all Giza++ and EBMT sub-
sentential alignments for English?French.
BLEU Prec. Recall WER SER
TS3 .4259 .7026 .6099 54.26 83.63
Inserting all Giza++ and EBMT data into
Pharaoh?s knowledge sources gives the results in Ta-
ble 7. These are considerably better than the scores
for the ?semi-hybrid? system described in section
5.1.1. This indicates that a phrase-based SMT sys-
tem is likely to perform better when EBMT word-
and phrase-alignments are used in the calculation of
the translation and target language probability mod-
els. Note, however, that the size of the data set in-
creases to over 2M items. Despite this, compared to
the results for the EBMT system of (Gough & Way,
2004b) shown in Table 1, these results for the ?fully
hybrid? SMT system still fall somewhat short (ex-
cept for Precision: .6727 vs. .7026).
5.2.2 French?English Results
Table 8: Seeding Pharaoh with all Giza++ and EBMT sub-
sentential alignments for French?English.
BLEU Prec. Recall WER SER
TS3 .4888 .6927 .7173 56.37 78.42
Carrying out a similar experiment for the reverse
language direction gives the results in Table 8. This
time this hybrid SMT system does outperform the
EBMT system of (Gough & Way, 2004b), with re-
spect to BLEU score (.4888 vs .4611) and Precision
(.6927 vs. 6782), but the EBMT system still wins
out where Recall, WER and SER are concerned. Re-
garding this latter, it seems that the correlation be-
tween low SER and high BLEU score is not as im-
portant as is claimed in (Way and Gough, 2005).
6 Conclusions
(Way and Gough, 2005) carried out a number of ex-
periments designed to test their large-scale Marker-
Based EBMT system described in (Gough & Way,
2004b) against a WB-SMT system constructed from
publicly available tools. While the results were a lit-
tle mixed, the EBMT system won out overall.
188
Nonetheless, WB-SMT has long been abandoned
in favour of phrase-based models. We extended
the work of (Way and Gough, 2005) by performing
a range of experiments using the Pharaoh phrase-
based decoder. Our main observations are as fol-
lows:
? Seeding Pharaoh with word- and phrase-
alignments induced via Giza++ generates bet-
ter results than if EBMT sub-sentential data is
used.
? Seeding Pharaoh with a ?hybrid? dataset of
Giza++ word alignments and EBMT phrases
improves over the baseline phrase-based SMT
system primed solely with Giza++ data. This
would appear to indicate that the quality of the
EBMT phrases is better than the SMT phrases,
and that SMT practitioners should use EBMT
phrasal data in the calculating of their language
and translation models, if available.
? Seeding Pharaoh with all data induced by
Giza++ and the EBMT system leads to the best-
performing hybrid SMT system: for English?
French, as well as EBMT phrasal data, EBMT
word alignments also contribute positively, but
the EBMT system per se still wins out (except
for Precision); for French?English, however,
our hybrid Example-Based SMT system out-
performs the EBMT system of (Gough & Way,
2004b) (cf. Table 9).
Table 9: Comparing the hybrid phrase-based SMT system us-
ing both the full Giza++ and full EBMT data against the EBMT
system of (Gough & Way, 2004b) for the full training set (TS3).
BLEU Prec. Recall WER SER
EN-FR HYBRID .2971 .6739 .5912 54.9 90.8
EBMT .3318 .6525 .6183 54.3 89.2
FR-EN HYBRID .2971 .6739 .5912 54.9 90.8
EBMT .3318 .6525 .6183 54.3 89.2
A number of avenues of further work remain open
to us. We would like to extend our investigations
into hybrid example-based statistical approaches to
machine translation by experiment with seeding the
Marker-Based system of (Gough & Way, 2004b)
with the SMT data, and combinations thereof with
the EBMT sub-sentential alignments, to investigate
the effect on translation quality. Given our find-
ings here, we are optimistic that ?hybrid statistical
EBMT? will outperform the baseline EBMT system,
and that our findings will prompt EBMT practition-
ers to augment their data resources with SMT align-
ments, something which to our knowledge is cur-
rently not done. In addition, we intend to continue
this line of research on different and larger data sets,
and for other language pairs.
7 Final Remarks
Finally, as (Way and Gough, 2005) observe, it is dif-
ficult to explain why to this day SMT practitioners
have not made full use of the large body of existing
work on EBMT, from (Nagao, 1984) to (Carl & Way,
2003) and beyond, which has contributed greatly to
the field of corpus-based MT.
From its very inception EBMT has made use of a
range of sub-sentential data ? both phrasal and lexi-
cal ? to perform translations whereas, until quite re-
cently, SMT models of translation were based on the
relatively simple word alignment models of (Brown
et al, 1990). With the advent of phrase-based SMT
systems the line between EBMT and SMT has be-
come significantly blurred, yet we are still unaware
of any papers on SMT which acknowledge their
debt to EBMT or which describe their approach as
?example?based?.
Despite it becoming increasingly difficulty to dis-
tinguish between EBMT and (phrase?based) SMT
models of translation, some differences still exist.
Rather than using models of syntax in a post hoc
fashion, as is the case with most SMT systems, an
EBMT model of translation builds in syntax at its
core. Given this, a phrase?based SMT system is
more likely to ?learn? chunks that an EBMT sys-
tem would not, as the system learns n-gram se-
quences rather than syntactically-motivated phrases
per se. Furthermore, our research here has demon-
strated quite clearly that if available, merging SMT
and EBMT data improves the quality of the result-
ing hybrid SMT system, as phrases extracted by both
methods that are more likely to function as syntac-
tic units (and therefore be more beneficial during
the translation process) are given a higher statistical
significance. Conversely, the probabilities of those
?less useful? SMT n-grams that are not also gener-
189
ated by the EBMT system are reduced. Essentially,
the EBMT data helps the SMT system to make the
best use of phrase alignments during translation.
Moreover, we see the fact that it is becoming in-
creasingly difficult to describe the differences be-
tween EBMT and SMT as a good thing, and that
as here, this convergence can lead to hybrid systems
capable of outperforming leading EBMT systems as
well as state-of-the-art phrase-based SMT.
We hope that the research presented here,
together with that begun by (Way and Gough,
2005), will lead to new areas of collaboration
between both sets of researchers, to the clear benefit
of the MT research community and the wider public.
Acknowledgements
We would like to thank Nano Gough for sup-
plying us with our EBMT training data. Thanks also
to three anonymous reviewers for their insightful
comments. The work presented in this paper is
partly supported by an IRCSET7 PhD Fellowship
Award.
References
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Fred Jelinek, Robert Mercer, and
Paul Roossin. 1990. A statistical approach to machine
translation Computational Linguistics 16:79?85.
Ralf Brown. 1999. Adding Linguistic Knowledge to a
Lexical Example-based Translation System. In In Pro-
ceedings of the 8th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation (TMI-99), Chester, England, pp.22?32.
Michael Carl and Andy Way (eds). 2003. Recent
Advances in Example-Based Machine Translation.
Kluwer, Dordrecht, The Netherlands.
Nano Gough and Andy Way. 2004. Example-Based Con-
trolled Translation. In Proceedings of the Ninth EAMT
Workshop, Valetta, Malta, pp.73?81.
Nano Gough and Andy Way. 2004. Robust Large-Scale
EBMT with Marker-Based Segmentation. In Pro-
ceedings of the Tenth Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-
04), Baltimore, MD., pp.95?104.
7http://www.ircset.ie
Thomas Green. 1979. The Necessity of Syntax Markers.
Two experiments with artificial languages. Journal of
Verbal Learning and Behavior 18:481?496.
Mary Hearne and Andy Way. 2003. Seeing the Wood for
the Trees: Data-Oriented Translation. In MT Summit
IX, New Orleans, LA., pp.165?172.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2004), Barcelona,
Spain, pp.388?395.
Philipp Koehn, Franz Och, and Dan Marcu. 2003. Sta-
tistical Phrase-Based Translation. Human Language
Technology Conference, (HLT-NAACL), Edmonton,
Canada, pp.48?54.
Makoto Nagao. 1984. A Framework of a Mechanical
Translation between Japanese and English by Analogy
Principle. In A. Elithorn and R. Banerji (eds.) Artifi-
cial and Human Intelligence, North-Holland, Amster-
dam, The Netherlands, pp.173?180.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
41stAnnual Meeting of the Association for Computa-
tional Linguistics (ACL-03), Sapporo, Japan, pp.160?
167.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics 29:19?51.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-02), Philadelphia, PA.,
pp.311?318.
Joseph Turian, Luke Shen and Dan Melamed. 2003.
Evaluation of Machine Translation and its Evaluation.
In MT Summit IX, New Orleans, LA., pp.386?393.
Hideo Watanabe, Sadao Kurohashi and Eiji Aramaki.
2003. Finding Translation Patterns from Paired Source
and Target Dependency Structures. In M. Carl & A.
Way (eds.) Recent Advances in Example-Based Ma-
chine Translation, Kluwer Academic Publishers, Dor-
drecht, The Netherlands, pp.397?420.
Andy Way and Nano Gough. 2005. Comparing
Example-Based and Statistical Machine Translation.
Natural Language Engineering [in press].
190
Proceedings of the Workshop on Statistical Machine Translation, pages 86?93,
New York City, June 2006. c?2006 Association for Computational Linguistics
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation 
 
Karolina Owczarzak Declan Groves Josef Van Genabith Andy Way 
National Centre for Language Technology 
School of Computing 
Dublin City University 
Dublin 9, Ireland 
{owczarzak,dgroves,josef,away}@computing.dcu.ie 
 
 
Abstract 
In this paper we present a novel method 
for deriving paraphrases during automatic 
MT evaluation using only the source and 
reference texts, which are necessary for 
the evaluation, and word and phrase 
alignment software. Using target language 
paraphrases produced through word and 
phrase alignment a number of alternative 
reference sentences are constructed auto-
matically for each candidate translation. 
The method produces lexical and low-
level syntactic paraphrases that are rele-
vant to the domain in hand, does not use 
external knowledge resources, and can be 
combined with a variety of automatic MT 
evaluation system. 
1 Introduction 
Since their appearance, BLEU (Papineni et al, 
2002) and NIST (Doddington, 2002) have been the 
standard tools used for evaluating the quality of 
machine translation. They both score candidate 
translations on the basis of the number of n-grams 
it shares with one or more reference translations 
provided. Such automatic measures are indispen-
sable in the development of machine translation 
systems, because they allow the developers to con-
duct frequent, cost-effective, and fast evaluations 
of their evolving models.  
These advantages come at a price, though: an 
automatic comparison of n-grams measures only 
the string similarity of the candidate translation to 
one or more reference strings, and will penalize 
any divergence from them. In effect, a candidate 
translation expressing the source meaning accu-
rately and fluently will be given a low score if the 
lexical choices and syntactic structure it contains, 
even though perfectly legitimate, are not present in 
at least one of the references. Necessarily, this 
score would not reflect a much more favourable 
human judgment that such a translation would re-
ceive. 
The limitations of string comparison are the 
reason why it is advisable to provide multiple ref-
erences for a candidate translation in the BLEU- or 
NIST-based evaluation in the first place. While 
(Zhang and Vogel, 2004) argue that increasing the 
size of the test set gives even more reliable system 
scores than multiple references, this still does not 
solve the inadequacy of BLEU and NIST for sen-
tence-level or small set evaluation. On the other 
hand, in practice even a number of references do 
not capture the whole potential variability of the 
translation. Moreover, often it is the case that mul-
tiple references are not available or are too difficult 
and expensive to produce: when designing a statis-
tical machine translation system, the need for large 
amounts of training data limits the researcher to 
collections of parallel corpora like Europarl 
(Koehn, 2005), which provides only one reference, 
namely the target text; and the cost of creating ad-
ditional reference translations of the test set, usu-
ally a few thousand sentences long, often exceeds 
the resources available. Therefore, it would be de-
sirable to find a way to automatically generate le-
gitimate translation alternatives not present in the 
reference(s) already available. 
86
In this paper, we present a novel method that 
automatically derives paraphrases using only the 
source and reference texts involved in for the 
evaluation of French-to-English Europarl transla-
tions produced by two MT systems: statistical 
phrase-based Pharaoh (Koehn, 2004) and rule-
based Logomedia.1 In using what is in fact a minia-
ture bilingual corpus our approach differs from the 
mainstream paraphrase generation based on mono-
lingual resources. We show that paraphrases pro-
duced in this way are more relevant to the task of 
evaluating machine translation than the use of ex-
ternal lexical knowledge resources like thesauri or 
WordNet2, in that our paraphrases contain both 
lexical equivalents and low-level syntactic vari-
ants, and in that, as a side-effect, evaluation bitext-
derived paraphrasing naturally yields domain-
specific paraphrases. The paraphrases generated 
from the evaluation bitext are added to the existing 
reference sentences, in effect creating multiple ref-
erences and resulting in a higher score for the can-
didate translation. Our hypothesis, confirmed by 
the experiments in this paper, is that the scores 
raised by additional references produced in this 
way will correlate better with human judgment 
than the original scores. 
The remainder of this paper is organized as fol-
lows: Section 2 describes related work; Section 3 
describes our method and presents examples of 
derived paraphrases; Section 4 presents the results 
of the comparison between the BLUE and NIST 
scores for a single-reference translation and the 
same translation using the paraphrases automati-
cally generated from the bitext, as well as the cor-
relations between the scores and human judgment; 
Section 5 discusses ongoing work; Section 6 con-
cludes. 
2 
2.1 
                                                          
Related work 
Word and phrase alignment 
Several researchers noted that the word and 
phrase alignment used in training translation mod-
els in Statistical MT can be used for other purposes 
as well. (Diab and Resnik, 2002) use second lan-
guage alignments to tag word senses. Working on 
an assumption that separate senses of a L1 word 
2.2 
1 http://www.lec.com/ 
2 http://wordnet.princeton.edu/ 
can be distinguished by its different translations in 
L2, they also note that a set of possible L2 transla-
tions for a L1 word may contain many synonyms. 
(Bannard and Callison-Burch, 2005), on the other 
hand, conduct an experiment to show that para-
phrases derived from such alignments can be se-
mantically correct in more than 70% of the cases. 
Automatic MT evaluation 
The insensitivity of BLEU and NIST to per-
fectly legitimate variation has been raised, among 
others, in (Callison-Burch et al, 2006), but the 
criticism is widespread. Even the creators of BLEU 
point out that it may not correlate particularly well 
with human judgment at the sentence level (Pap-
ineni et al, 2002), a problem also noted by (Och et 
al., 2003) and (Russo-Lassner et al, 2005). A side 
effect of this phenomenon is that BLEU is less re-
liable for smaller data sets, so the advantage it pro-
vides in the speed of evaluation is to some extent 
counterbalanced by the time spent by developers 
on producing a sufficiently large test data set in 
order to obtain a reliable score for their system.  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic machine translation metrics. Some of 
them concentrate mainly on the word reordering 
aspect, like Maximum Matching String (Turian et 
al., 2003) or Translation Error Rate (Snover et al, 
2005). Others try to accommodate both syntactic 
and lexical differences between the candidate 
translation and the reference, like CDER (Leusch 
et al, 2006), which employs a version of edit dis-
tance for word substitution and reordering; 
METEOR (Banerjee and Lavie, 2005), which uses 
stemming and WordNet synonymy; and a linear 
regression model developed by (Russo-Lassner et 
al., 2005), which makes use of stemming, Word-
Net synonymy, verb class synonymy, matching 
noun phrase heads, and proper name matching. 
A closer examination of these metrics suggests 
that the accommodation of lexical equivalence is 
as difficult as the appropriate treatment of syntactic 
variation, in that it requires considerable external 
knowledge resources like WordNet, verb class da-
tabases, and extensive text preparation: stemming, 
tagging, etc. The advantage of our method is that it 
produces relevant paraphrases with nothing more 
than the evaluation bitext and a widely available 
word and phrase alignment software, and therefore 
can be used with any existing evaluation metric. 
87
3 Contextual bitext-derived paraphrases 
The method presented in this paper rests on a 
combination of two simple ideas. First, the compo-
nents necessary for automatic MT evaluation like 
BLEU or NIST, a source text and a reference text, 
constitute a miniature parallel corpus, from which 
word and phrase alignments can be extracted 
automatically, much like during the training for a 
statistical machine translation system. Second, tar-
get language words ei1, ?,  ein aligned as the likely 
translations to a source language word fi are often 
synonyms or near-synonyms of each other. This 
also holds for phrases: target language phrases epi1, 
?, epin aligned with a source language phrase fpi 
are often paraphrases of each other. For example, 
in our experiment, for the French word question 
the most probable automatically aligned English 
translations are question, matter, and issue, which 
in English are practically synonyms. Section 3.2 
presents more examples of such equivalent expres-
sions.  
3.1 
3.2 
                                                          
Experimental design 
For our experiment, we used two test sets, 
each consisting of 2000 sentences, drawn ran-
domly from the test section of the Europarl parallel 
corpus. The source language was French and the 
target language was English. One of the test sets 
was translated by Pharaoh trained on 156,000 
French-English sentence pairs. The other test set 
was translated by Logomedia, a commercially 
available rule-based MT system. Each test set con-
sisted therefore of three files: the French source 
file, the English translation file, and the English 
reference file. 
Each translation was evaluated by the BLEU 
and NIST metrics first with the single reference, 
then with the multiple references for each sentence 
using the paraphrases automatically generated 
from the source-reference mini corpus. A subset of 
a 100 sentences was randomly extracted from each 
test set and evaluated by two independent human 
judges with respect to accuracy and fluency; the 
human scores were then compared to the BLEU 
and NIST scores for the single-reference and the 
automatically generated multiple-reference files. 
Word alignment and phrase extraction 
We used the GIZA++ word alignment soft-
ware3 to produce initial word alignments for our 
miniature bilingual corpus consisting of the source 
French file and the English reference file, and the 
refined word alignment strategy of (Och and Ney, 
2003; Koehn et al, 2003; Tiedemann, 2004) to 
obtain improved word and phrase alignments. 
For each source word or phrase fi that is 
aligned with more than one target words or 
phrases, its possible translations ei1, ..., ein were 
placed in a list as equivalent expressions (i.e. 
synonyms, near-synonyms, or paraphrases of each 
other). A few examples are given in (1). 
 
(1) agreement - accordance 
adopted - implemented 
matter - lot - case 
funds - money 
arms - weapons 
area - aspect  
question ? issue ? matter 
we would expect - we cer-
tainly expect 
bear on - are centred 
around 
 
Alignment divides target words and 
phrases into equivalence sets; each set corresponds 
to one source word/phrase that was originally 
aligned with the target elements. For example, for 
the French word citoyens three English words were 
deemed to be the most appropriate translations: 
people, public, and citizens; therefore these three 
words constitute an equivalence set. Another 
French word population was aligned with two 
English translations: population and people; so the 
word people appears in two equivalence set (this 
gives rise to the question of equivalence transitiv-
ity, which will be discussed in Section 3.3). From 
the 2000-sentence evaluation bitext we derived 769 
equivalence sets, containing in total 1658 words or 
phrases. Each set contained on average two or 
three elements. In effect, we produced at least one 
equivalent expression for 1658 English words or 
phrases. 
An advantage of our method is that the tar-
get paraphrases and words come ordered with re-
3 http://www.fjoch.com/GIZA++ 
88
spect to their likelihood of being the translation of 
the source word or phrase ? each of them is as-
signed a probability expressing this likelihood, so 
we are able to choose only the most likely transla-
tions, according to some experimentally estab-
lished threshold. The experiment reported here was 
conducted without such a threshold, since the word 
and phrase alignment was of a very high quality. 
3.3 
3.4 
3.5 
Domain-specific lexical and syntactic 
paraphrases 
It is important to notice here how the para-
phrases produced are more appropriate to the task 
at hand than synonyms extracted from a general-
purpose thesaurus or WordNet. First, our para-
phrases are contextual - they are restricted to only 
those relevant to the domain of the text, since they 
are derived from the text itself. Given the context 
provided by our evaluation bitext, the word area in 
(1) turns out to be only synonymous with aspect, 
and not with land, territory, neighbourhood, divi-
sion, or other synonyms a general-purpose thesau-
rus or WordNet would give for this entry. This 
allows us to limit our multiple references only to 
those that are likely to be useful in the context pro-
vided by the source text. Second, the phrase align-
ment captures something neither a thesaurus nor 
WordNet will be able to provide: a certain amount 
of syntactic variation of paraphrases. Therefore, we 
know that a string such as we would expect in (1), 
with the sequence noun-aux-verb, might be para-
phrased by we certainly expect, a sequence of 
noun-adv-verb. 
Open and closed class items 
One important conclusion we draw from 
analysing the synonyms obtained through word 
alignment is that equivalence is limited mainly to 
words that belong to open word classes, i.e. nouns, 
verbs, adjectives, adverbs, but is unlikely to extend 
to closed word classes like prepositions or pro-
nouns. For instance, while the French preposition ? 
can be translated in English as to, in, or at, depend-
ing on the context, it is not the case that these three 
prepositions are synonymous in English. The divi-
sion is not that clear-cut, however: within the class 
of pronouns, he, she, and you are definitely not 
synonymous, but the demonstrative pronouns this 
and that might be considered equivalent for some 
purposes. Therefore, in our experiment we exclude 
prepositions and in future work we plan to examine 
the word alignments more closely to decide 
whether to exclude any other words. 
Creating multiple references 
After the list of synonyms and paraphrases is 
extracted from the evaluation bitext, for each 
reference sentence a string search replaces every 
eligible word or phrase with its equivalent(s) from 
the paraphrase list, one at a time, and the resulting 
string is added to the array of references. The 
original string is added to the array as well. This 
process results in a different number of reference 
sentences for every test sentence, depending on 
whether there was anything to replace in the refer-
ence and how many paraphrases we have available 
for the original substring. One example of this 
process is shown in (2). 
 
(2) Original reference: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 1: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 2: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to 
that  
Paraphrase 3: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
Transitivity 
As mentioned before, an interesting question 
that arises here is the potential transitivity of our 
automatically derived synonyms/paraphrases. It 
could be argued that if the word people is equiva-
lent to public according to one set from our list, 
and to the word population according to another 
set, then public can be thought of as equivalent to 
population. In this case, the equivalence is not con-
troversial. However, consider the following rela-
tion: if sure in one of the equivalence sets is 
synonymous to certain, and certain in a different 
89
set is listed as equivalent to some, then treating 
sure and some as synonyms is a mistake. In our 
experiment we do not allow synonym transitivity; 
we only use the paraphrases from equivalence sets 
containing the word/phrase we want to replace.  
Multiple simultaneous substitution 
Note that at the moment the references we are 
producing do not contain multiple simultaneous 
substitutions of equivalent expressions; for exam-
ple, in (2) we currently do not produce the follow-
ing versions: 
 
(3) Paraphrase 4:  
i admire the reply mrs parly 
gave this morning however we 
have turned a blind eye to 
that 
Paraphrase 5: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to it 
Paraphrase 6: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
This can potentially prevent higher n-grams being 
successfully matched if two or more equivalent 
expressions find themselves within the range of n-
grams being tested by BLEU and NIST. To avoid 
combinatorial problems, implementing multiple 
simultaneous substitutions could be done using a 
lattice, much like in (Pang et al, 2003). 
4 Results 
As expected, the use of multiple references 
produced by our method raises both the BLEU and 
NIST scores for translations produced by Pharaoh 
(test set PH) and Logomedia (test set LM). The 
results are presented in Table 1. 
 
 BLEU NIST 
PH single ref 0.2131 6.1625 
PH multi ref 0.2407 7.0068 
LM single ref 0.1782 5.5406 
LM multi ref 0.2043 6.3834 
 
Table 1. Comparison of single-reference and multi-
reference scores for test set PH and test set LM 
 
The hypothesis that the multiple-reference 
scores reflect better human judgment is also con-
firmed. For 100-sentence subsets (Subset PH and 
Subset LM) randomly extracted from our test sets 
PH and LM, we calculated Pearson?s correlation 
between the average accuracy and fluency scores 
that the translations in this subset received from 
two human judges (for each subset) and the single-
reference and multiple-reference sentence-level 
BLEU and NIST scores.  
There are two issues that need to be noted at 
this point. First, BLEU scored many of the sen-
tences as zero, artificially leveling many of the 
weaker translations.4 This explains the low, al-
though still statistically significant (p value < 
0.015) correlation with BLEU for both single and 
multiple reference translations. Using a version of 
BLEU with add-one smoothing we obtain consid-
erably higher correlations. Table 2 shows Pear-
son?s correlation coefficient for BLEU, BLEU 
with add-one smoothing, NIST, and human judg-
ments for Subsets PH. Multiple paraphrase refer-
ences produced by our method consistently lead to 
a higher correlation with human judgment for 
every metric.6 
 
                           Subset PH 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.297 0.307 
H & BLEU smoothed 0.396 0.404 
H & NIST  0.323 0.355 
 
Table 2. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset PH (of test set PH)  
 
The second issue that requires explanation is 
the lower general scores Logomedia?s translation 
received on the full set of 2000 sentences, and the 
extremely low correlation of its automatic evalua-
tion with human judgment, irrespective of the 
number of references. It has been noticed (Calli-
                                                          
4 BLEU uses a geometric average while calculating the sen-
tence-level score and will score a sentence as 0 if it does not 
have at least one 4-gram.  
5 A critical value for Pearson?s correlation coefficient for the 
sample size between 90 and 100 is 0.267, with p < 0.01. 
6 The significance of the rise in scores was confirmed in a 
resampling/bootstrapping test, with p < 0.0001. 
90
son-Burch et al, 2006) that BLEU and NIST fa-
vour n-gram based MT models such as Pharaoh, so 
the translation produced by Logomedia scored 
lower on the automatic evaluation, even though the 
human judges rated Logomedia output higher than 
Pharaoh?s translation. Both human judges consis-
tently gave very high scores to most sentences in 
subset LM (Logomedia), and as a consequence 
there was not enough variation in the scores as-
signed by them to create a good correlation with 
the BLEU and NIST scores. The average human 
scores for the subsets PH and LM and the coeffi-
cients of variation are presented in Table 3. It is 
easy to see that Logomedia?s translation received a 
higher mean score (on a scale 0 to 5) from the hu-
man judges and with less variance than Pharaoh. 
 
 Mean score  Variation 
Subset PH 3.815 19.1% 
Subset LM 4.005 16.25% 
 
Table 3. Human judgment mean scores and coeffi-
cients of variation for Subset PH and Subset LM 
 
As a result of the consistently high human scores 
for Logomedia, none of the Pearson?s correlations 
computed for Subset LM is high enough to be sig-
nificant. The values are lower than the critical 
value 0.164 corresponding to p < 0.10. 
 
                          Subset LM 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.046* 0.067* 
H & BLEU smoothed 0.163* 0.151* 
H & NIST  0.078* 0.116* 
 
Table 4. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset LM (of test set LM). * denotes values with p >  
0.10. 
5 Current and future work 
We would like to experiment with the way in 
which the list of equivalent expressions is pro-
duced. One possible development would be to de-
rive the expressions from a very large training 
corpus used by a statistical machine translation 
system, following (Bannard and Callison-Burch, 
2005), for instance, and use it as an external wider-
purpose knowledge resource (rather than a current 
domain-tailored resource as in our experiment), 
which would be nevertheless improve on a thesau-
rus in that it would also include phrase equivalents 
with some syntactic variation. According to (Ban-
nard and Callison-Burch, 2005), who derived their 
paraphrases automatically from a corpus of over a 
million German-English Europarl sentences, the 
baseline syntactic and semantic accuracy of the 
best paraphrases (those with the highest probabil-
ity) reaches 48.9% and 64.5%, respectively. That 
is, by replacing a phrase with its one most likely 
paraphrase the sentence remained syntactically 
well-formed in 48.9% of the cases and retained its 
meaning in 65% of the cases. 
In a similar experiment we generated para-
phrases from a French-English Europarl corpus of 
700,000 sentences. The data contained a consid-
erably higher level of noise than our previous ex-
periment on the 2000-sentence test set, even 
though we excluded any non-word entities from 
the results. Like (Bannard and Callison-Burch, 
2005), we used the product of probabilities p(fi|ei1) 
and p(ei2|fi) to determine the best paraphrase for a 
given English word ei1. We then compared the ac-
curacy across four samples of data. Each sample 
contained 50 randomly drawn words/phrases and 
their paraphrases. For the first two samples, the 
paraphrases were derived from the initial 2000-
sentence corpus; for the second two, the para-
phrases were derived from the 700,000-sentence 
corpus. For each corpus, one of the two samples 
contained only one best paraphrase for each entry, 
while the other listed all possible paraphrases. We 
then evaluated the quality of each paraphrase with 
respect to its syntactic and semantic accuracy. In 
terms of syntax, we considered the paraphrase ac-
curate either if it had the same category as the 
original word/phrase; in terms of semantics, we 
relied on human judgment of similarity. Tables 5 
and 6 summarize the syntactic and semantic accu-
racy levels in the samples. 
 
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 59% 60% 
700,000-sent. corpus 70% 48% 
 
Table 5. Syntactic accuracy of paraphrases 
 
 
91
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 83% 74% 
700,000-sent. corpus 76% 68% 
 
Table 6. Semantic accuracy of paraphrases 
 
Although it has to be kept in mind that these 
percentages were taken from relatively small sam-
ples, an interesting pattern emerges from compar-
ing the results. It seems that the average syntactic 
accuracy of all paraphrases decreases with in-
creased corpus size, but the syntactic accuracy of 
the one best paraphrase improves. This reflects the 
idea behind word alignment: the bigger the corpus, 
the more potential alignments there are for a given 
word, but at the same time the better their order in 
terms of probability and the likelihood to obtain 
the correct translation. Interestingly, the same pat-
tern is not repeated for semantic accuracy, but 
again, these samples are quite small. In order to 
address this issue, we plan to repeat the experiment 
with more data. 
Additionally, it should be noted that certain 
expressions, although not completely correct syn-
tactically, could be retained in the paraphrase lists 
for the purposes of machine translation evaluation. 
Consider the case where our equivalence set looks 
like this: 
 
(4) abandon ? abandoning ? 
abandoned 
 
The words in (4) are all inflected forms of the verb 
abandon, and although they would produce rather 
ungrammatical paraphrases, those ungrammatical 
paraphrases still allow us to score our translation 
higher in terms of BLEU or NIST if it contains one 
of the forms of abandon than when it contains 
some unrelated word like piano instead. This is 
exactly what other scoring metrics mentioned in 
Section 2 attempt to obtain with the use of stem-
ming or prefix matching. 
6 Conclusions 
In this paper we present a novel combination 
of existing ideas from statistical machine transla-
tion and paraphrase generation that leads to the 
creation of multiple references for automatic MT 
evaluation, using only the source and reference 
files that are required for the evaluation. The 
method uses simple word and phrase alignment 
software to find possible synonyms and para-
phrases for words and phrases of the target text, 
and uses them to produce multiple reference sen-
tences for each test sentence, raising the BLEU and 
NIST evaluation scores and reflecting human 
judgment better. The advantage of this method 
over other ways to generate paraphrases is that (1) 
unlike other methods, it does not require extensive 
parallel monolingual paraphrase corpora, but it 
extracts equivalent expressions from the miniature 
bilingual corpus of the source and reference 
evaluation files; (2) unlike other ways to accom-
modate synonymy in automatic evaluation, it does 
not require external lexical knowledge sources like 
thesauri or WordNet; (3) it extracts only synonyms 
that are relevant to the domain in hand; and (4) the 
equivalent expressions it produces include a certain 
amount of syntactic paraphrases.  
The method is general and it can be used with 
any automatic evaluation metric that supports mul-
tiple references. In our future work, we plan to ap-
ply it to newly developed evaluation metrics like 
CDER and TER that aim to allow for syntactic 
variation between the candidate and the reference, 
therefore bringing together solutions for the two 
shortcomings of automatic evaluation systems: 
insensitivity to allowable lexical differences and 
syntactic variation. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Proceed-
ings of the ACL 2005 Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or Sum-
marization: 65-73. 
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proceed-
ings of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL 2005): 597-604. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. To appear in Pro-
ceedings of EACL-2006. 
Mona Diab and Philip Resnik. 2002. An unsupervised 
Method for Word Sense Tagging using Parallel Cor-
pora. Proceedings of the 40th Annual Meeting of the 
92
Association for Computational Linguistics, Philadel-
phia, PA. 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. Pro-
ceedings of Human Language Technology Confer-
ence 2002: 138?145. 
Philipp Koehn, Franz Och and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. Proceedings of 
the  Human Language Technology Conference (HLT-
NAACL 2003): 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. Machine translation: From real users to re-
search. 6th Conference of the Association for 
Machine Translation in the Americas (AMTA 2004): 
115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. To appear in Proceedings of the 11th 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2006). 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19?51. 
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. 
Syntax for statistical machine translation. Technical 
report, Center for Language and Speech Processing, 
John Hopkins University, Baltimore, MD.  
Bo Pang, Kevin Knight and Daniel Marcu. 2003. Syn-
tax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences. 
Proceedings of Human Language Technology-North 
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL) 2003: 181?188. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
ACL: 311-318. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-
TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-
versity of Maryland, College Park, MD. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula and Ralph Weischedel. 
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. Technical Report LAMP-
TR-126, CS-TR-4755, UMIACS-TR-2005-58, Uni-
versity of Maryland, College Park. MD. 
J?rg Tiedemann. 2004. Word to word alignment strate-
gies. Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004): 
212-218. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. 
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. TMI-2004: Proceedings of the 10th 
Conference on Theoretical and Methodological Is-
sues in Machine Translation: 85-94. 
93
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
TMTprime: A Recommender System for MT and TM Integration
Aswarth Dara?, Sandipan Dandapat??, Declan Groves? and Josef van Genabith?
? Centre for Next Generation Localisation, School of Computing
Dublin City University, Dublin, Ireland
? Department of Computer Science and Engineering
IIT-Guwahati, Assam, India
{adara, dgroves, josef}@computing.dcu.ie, sdandapat@iitg.ernet.in
Abstract
TMTprime is a recommender system that fa-
cilitates the effective use of both transla-
tion memory (TM) and machine translation
(MT) technology within industrial language
service providers (LSPs) localization work-
flows. LSPs have long used Translation Mem-
ory (TM) technology to assist the translation
process. Recent research shows how MT sys-
tems can be combined with TMs in Computer
Aided Translation (CAT) systems, selecting
either TM or MT output based on sophis-
ticated translation quality estimation without
access to a reference. However, to date there
are no commercially available frameworks for
this. TMTprime takes confidence estimation
out of the lab and provides a commercially vi-
able platform that allows for the seamless inte-
gration of MT with legacy TM systems to pro-
vide the most effective (least effort/cost) trans-
lation options to human translators, based on
the TMTprime confidence score.
1 Introduction
Within the LSP community there is growing interest
in the use of MT as a means to increase automation
and reduce overall localisation project cost. When
high-quality MT output is available, translators see
significant productivity gains over translation from
scratch, but poor MT quality leads to frustration
and wasted time as suggested translations are dis-
carded in favour of providing a translation from
scratch. We present a commercially-relevant soft-
ware platform providing a translation confidence es-
timation metric and, based on this, a mechanism for
effectively integrating MT with TMs in localisation
workflows. The confidence metric ensures that only
?Author did this work during his post doctoral research at
CNGL.
those MT outputs that are guaranteed to require less
post-editing effort than the best corresponding TM
match are presented to the post-editor (He et al,
2010a). The MT is integrated seamlessly, and es-
tablished localisation cost estimation models based
on TM technologies still apply as upper bounds.
2 Related Work
MT confidence estimation and its relation to existing
TM scoring methods, together with how to make the
most effective use of both technologies, is an active
area of research.
(Specia, 2011) and (Specia et al, 2009, 2010) pro-
pose a confidence estimator that relates specifically
to the post-editing effort of translators. This re-
search uses regression on both the automatic scores
assigned to the MT and scores assigned by post-
editors and aims to model post-editors? judgements
of the translation quality between good and bad, or
among three levels of post-editing effort.
Our work is an extension of (He et al, 2010a,b,c),
and uses outputs and features relevant to the TM
and MT systems. We focus on using system exter-
nal features. This is important for cases where the
internals of the MT system are not available, as in
the use of MT as a service in a localisation work-
flow.1 Furthermore, instead of having to solve a
regression problem, our approach is based on solv-
ing an easier binary prediction problem (using Sup-
port Vector Machines) and can be easily integrated
into TMs. (He et al, 2010b) present a MT/TM seg-
ment recommender, (He et al, 2010c) a MT/TM n-
best list segment re-ranker and (He et al, 2010a) a
MT/TM integration method that can use matching
sub-segments in MT/TM combination. Importantly,
1(Specia et al, 2009) note that using glass-box features
when available, in addition to black-box features, offer only
small gains and also incur significant computational effort.
10
translators can tune the models for precision without
retraining the models.
Related research by (Simard and Isabelle., 2009)
focuses on combining TM information into an SMT
system for improving the performance of the MT
when a close match already exists within the TM.
(Koehn and Haddow, 2009) presents a post-editing
environment using information from the phrase-
based SMT system Moses.2 (Guerberof, 2009) com-
pares the post-editing effort required for TM and
MT output, respectively. (Tatsumi, 2009) studies the
correlation between automatic evaluation scores and
post-editing effort.
3 Translation Recommender
Figure 1: TMTprime Workflow
The workflow of the translation recommender is
shown in Figure 1. We train MT systems using a
significant portion of the training data and use these
models as well as TM outputs to obtain a recommen-
dation development data set. MT systems can be
either in-house, e.g. a Moses-based system, or ex-
ternally available systems, such as Microsoft Bing3
or Google Translate.4 For each sentence in the de-
velopment data set, we have access to the reference
as well as to the outputs for each of the MT and TM
systems. We then select the best MT (or TM) output
as the translation with the lowest TER score with
respect to the reference and label the data accord-
ingly. System-independent features for each trans-
lation output are fed as input to the SVM classi-
fier (Cortes and Vapnik, 1995). The SVM classi-
fier outputs class labels and the class labels are con-
verted into confidence scores using the techniques
given in (Lin et al, 2007). Relying on system inde-
pendent black-box features has allowed us to build
2http://www.statmt.org/moses/
3http://www.bing.com/translator
4http://translate.google.com/
a fully extendable platform that will allow any num-
ber of MT systems (or indeed TM systems) to be
plugged into the recommender with little effort.
4 Demo Description
Using the Amazon EC25 deployment as a back-end,
we have developed a front-end GUI for the system
(Figure 2). The interface allows the user to select
which of the available translation systems (whether
they be MT or TM) they wish to use within the rec-
ommender system. The user can input their own
pre-established estimated cost of post-editing, based
on error ranges. Typically the costs for post-editing
those translations which have a lower-error rate (i.e.
fewer errors) is less than the cost for post-editing
translations which have a greater number of errors,
as they are of lower quality. The user is requested to
upload a file for translation to the system.
Figure 2: TMTprime GUI
Once the user has selected their desired options,
the TMTprime platform provides various analysis
measures based on its recommendation engine, such
as how many segments from the input file are recom-
mended for translation by the various selected trans-
lation engines or TMs available. Based on the input
costs, it provides a visualisation of overall estimated
cost of either using an individual translation system
on its own, or using the recommender selecting the
best performing system on a segment-by-segment
basis. The TMTprime system is an implementa-
tion of a segment-based system selector selecting
the most appropriate available translation/TM sys-
tem for a given input. A snapshot of the results pro-
duced by TMTprime is given in Figure 3: the pie-
chart shows what percentage of segments are rec-
ommended from each of the translation systems; the
5http://aws.amazon.com/ec2/
11
bar-graph gives an estimated cost of using a single
translation system alone and the estimated cost when
using TMTprime?s combined recommendation. The
estimated cost using TMTprime is lower when com-
pared to using a single MT or TM system alone
(in the worst case, it will be the same as the best-
performing single translation engine or TM system).
This estimated cost includes both the cost for trans-
lation (currently uniform cost for each translation
system) and the cost required for post-editing. For
example, if the MT is an in-house system the cost
of translation will be (close to) zero whereas there is
potentially an additional base cost for using an exter-
nal MT engine. Finally, the interface provides statis-
tics related to various confidence levels for different
translation outputs across the various translation and
TM systems.
Figure 3: Results shown by TMTprime system
5 Experiments and Results
Evaluation targets two objectives and is described
below.
5.1 Correlation with Automatic Metrics
TER and METEOR are widely-used automatic met-
rics (Snover et al, 2006; Denkowski and Lavie,
2011) that calculate the quality of translation out-
put by comparing it against a human translation,
known as the reference translation. Our data sets
for the experiment consist of English-French trans-
lation memories from the IT domain. In all instances
MT was carried out for English-French translations.
As we have access to the reference target language
translations for our test set, we are able to calculate
the TER and METEOR scores for the three trans-
lation outputs (here TM, MaTrEx (Dandapat et al,
2010) and Microsoft Bing). For each sentence in the
test set, TMTprime recommends a particular transla-
tion output with a certain estimated confidence level
without access to a reference. We measure Pearson?s
correlation coefficient (Hollander and Wolfe, 1999)
between the recommendation scores, TER scores
and METEOR scores (for all system outputs) in or-
der to determine how well the TMTprime prediction
score correlates with the widely used automatic eval-
uation metrics. Results of these experiments are pro-
vided in Table 1 which shows there is a negative cor-
relation between TMTprime scores and TER scores.
This shows that both TMTprime scores and TER
scores are moving in opposite directions, supporting
the claim that the higher the recommendation scores,
the lower the TER scores. As TER is an error score,
the lower the TER score, the higher the quality of
the machine translation output compared to its refer-
ence. On the other hand, TMTprime scores are pos-
itively correlated with METEOR scores which sup-
ports the claim that the higher the recommendation
scores, the higher the METEOR scores.
Pearson?s r TER METEOR
TMTprime -0.402 0.447
Table 1: Correlation with automatic metrics
The evaluation has been performed on a test data
set of 2,500 sentences. Both the correlations are sig-
nificant at the (p<0.01) level.
5.2 Correlation with Post-Editing time
This is the most important and crucial metric for the
evaluation. For this experiment we made use of post-
editing data captured during a real-world translation
task, for English-French in the IT domain.
Pearson?s r TER METEOR PE Time
TMTprime -0.122 0.129 -0.132
Table 2: Correlation with Post-Editing times
For testing, we collect the post-editing times for
MT outputs from two different translators using a
commercial computer-aided translation (CAT tool)
in a real-world production scenario. The data set
consists of 1113 samples and is different from the
one used in the correlation with automatic metrics.
12
Post-editing times provide a real measure of the
amount of post-editing effort required to perfect the
output of the MT system. For this experiment, we
took the output of the MT system used in the task to-
gether with the post-editing times and measured the
Pearsons correlation coefficient between the TMT-
prime recommendation scores and the post-editing
(PE) times (only for MT output from a single sys-
tem since this data set does contain PE times for
other translation outputs). In addition, we also re-
peated the previous experiment setup for finding the
correlation between the TMTprime scores and the
automatically-produced TER, METEOR scores for
this data set. The results are given in Table 2.
The results show that the confidence scores do
correlate with automatic evaluation metrics and
post-editing times. Although the correlations do not
seem as strong as before, the results are statistically
significant (p<0.01).
6 Conclusions and Future Work
We present a commercially viable translation recom-
mender system which selects the best output from
multiple TM/MT outputs. We have shown that our
confidence score correlates with automatic metrics
and post-editing times. For future work, we are
looking into extending and evaluating the system for
different language pairs and data sets.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grants SFI11-TIDA-B2040 and 07/CE/I1142) as
part of the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We would also
like to thank Symantec, Autodesk and Welocalize for
their support and provision of data sets used in our ex-
periments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-vector
networks. In Machine Learning. pages 273?297.
Dandapat, Sandipan, Mikel L. Forcada, Declan Groves, Ser-
gio Penkale, John Tinsley, and Andy Way. 2010. OpenMa-
TrEx: A free/open-source marker-driven example-based ma-
chine translation system. In Proceedings of the 7th interna-
tional conference on Advances in natural language process-
ing. Springer-Verlag, Berlin, Heidelberg, IceTAL?10, pages
121?126.
Denkowski, Michael and Alon Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation of ma-
chine translation systems. In Proceedings of the EMNLP
2011 Workshop on Statistical Machine Translation. Edin-
burgh, UK.
Guerberof, Ana. 2009. Productivity and quality in mt post-
editing. In Proceedings of Machine Translation Summit XII
- Workshop: Beyond Translation Memories: New Tools for
Translators. Ottawa, Canada.
He, Yifan, Yanjun Ma, J Roturier, Andy Way, and Josef van
Genabith. 2010a. Improving the post-editing experience us-
ing translation recommendation: A user study. In Proceed-
ings of the Ninth Conference of the Association for Ma-
chine Translation in the Americas. Denver, Colorado, AMTA
2010, pages 247?256.
He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way.
2010b. Bridging smt and tm with translation recommenda-
tion. In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for Com-
putational Linguistics, Uppsala, Sweden, ACL 2010, pages
622?630.
He, Yifan, Yanjun Ma, Andy Way, and Josef van Genabith.
2010c. Integrating n-best smt outputs into a tm system. In
Proceedings of the 23rd International Conference on Com-
putational Linguistics: Posters. Association for Computa-
tional Linguistics, Beijing, China, COLING 2010, pages
374?382.
Hollander, Myles and Douglas A. Wolfe. 1999. Nonparametric
Statistical Methods. John Wiley and Sons.
Koehn, Philip and Barry Haddow. 2009. Interactive assis-
tance to human translators using statistical machine trans-
lation methods. In Proceedings of MT Summit XII. Ottawa,
Canada, pages 73?80.
Lin, Hsuan-Tien, Chih-Jen Lin, and Ruby C. Weng. 2007. A
note on platt?s probabilistic outputs for support vector ma-
chines. Machine Learning 68(3):267?276.
Simard, Michael and Pierre Isabelle. 2009. Phrase-based ma-
chine translation in a computer-assisted translation environ-
ment. In Proceedings of Machine Translation Summit XII.
Ottawa, Canada, pages 120?127.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings of Asso-
ciation for Machine Translation in the Americas. Cambridge,
MA, pages 223?231.
Specia, Lucia. 2011. Exploiting objective annotations for mea-
suring translation post-editing effort. In Proceedings of the
15th Annual Conference of the European Association for
Machine Translation. Leuven, Belgium, EAMT 2011, pages
73?80.
Specia, Lucia, Nicola Cancedda, and Marc Dymetman. 2010. A
dataset for assessing machine translation evaluation metrics.
In Proceedings of LREC 2010. Valletta, Malta.
Specia, Lucia, Marco Turqui, Zhuoran Wang, John Shawe-
Taylor, and Craig Saunders. 2009. Improving the confidence
of machine translation quality estimates. In Proceedings
of Machine Translation Summit XII. Ottawa, Canada, pages
136?143.
Tatsumi, Midori. 2009. Correlation between automatic evalua-
tion scores, post-editing speed and some other factors. In
Proceedings of Machine Translation Summit XII. Ottawa,
Canada, pages 332?339.
13
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 20?23,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
A Web Application for the Diagnostic Evaluation of Machine Translation
over Specific Linguistic Phenomena
Antonio Toral Sudip Kumar Naskar Joris Vreeke Federico Gaspari Declan Groves
School of Computing
Dublin City University
Ireland
{atoral, snaskar, fgaspari, dgroves}@computing.dcu.ie joris.vreeke@dcu.ie
Abstract
This paper presents a web application and a
web service for the diagnostic evaluation of
Machine Translation (MT). These web-based
tools are built on top of DELiC4MT, an open-
source software package that assesses the per-
formance of MT systems over user-defined
linguistic phenomena (lexical, morphological,
syntactic and semantic). The advantage of the
web-based scenario is clear; compared to the
standalone tool, the user does not need to carry
out any installation, configuration or mainte-
nance of the tool.
1 Automatic Evaluation of Machine
Translation beyond Overall Scores
Machine translation (MT) output can be evaluated
using different approaches, which can essentially be
divided into human and automatic, both of which,
however, present a number of shortcomings. Hu-
man evaluation tends to be more reliable in a num-
ber of ways and can be tailored to a variety of situ-
ations, but is rather expensive (both in terms of re-
sources and time) and is difficult to replicate. On
the other hand, standard automatic MT evaluation
metrics such as BLEU (Papineni et al, 2002) and
METEOR (Banerjee and Lavie, 2005) are consid-
erably cheaper and provide faster results, but return
rather crude scores that are difficult to interpret for
MT users and developers alike. Crucially, current
standard automatic MT evaluation metrics also lack
any diagnostic value, i.e. they cannot identify spe-
cific weaknesses in the MT output. Diagnostic in-
formation can be extremely valuable for MT devel-
opers and users, e.g. to improve the performance of
the system or to decide which output is more suited
for particular scenarios.
An interesting alternative to the traditional MT
evaluation metrics is to evaluate the performance
of MT systems over specific linguistic phenomena.
While retaining the main advantage of automatic
metrics (low cost), this approach provides more fine-
grained linguistically-motivated evaluation. The lin-
guistic phenomena, also referred to as linguistic
checkpoints, can be defined in terms of linguistic in-
formation at different levels (lexical, morphological,
syntactic, semantic, etc.) that appear in the source
language. Examples of such linguistic checkpoints,
what translation information they can represent, and
their relevance for MT are provided in Table 1.
Checkpoint Relevance for MT
Lexical Words that can have multiple translations in
the target. For example, the preposition ?de?
in Spanish can be translated into English as
?of? or ?from? depending on the context.
Syntactic Syntactic constructs that are difficult to trans-
late. E.g., a checkpoint containing the se-
quence a noun (noun1) followed by the
preposition ?de?, followed by another noun
(noun2) when translating from Spanish to
English. The equivalent English construct
would be noun2?s noun1, the translation thus
involving some reordering.
Semantic Words with multiple meanings, which possi-
bly correspond to different translations in the
target language. Polysemous words can be
collected from electronic dictionaries such as
WordNet (Miller, 1995).
Table 1: Linguistic Checkpoints
Checkpoints can also be built by combining el-
20
ements from different categories. For example, by
combining lexical and syntantic elements, we could
define a checkpoint for prepositional phrases (syn-
tactic element) which start with the preposition ?de?
(lexical element).
Woodpecker (Zhou et al, 2008) is a tool that per-
forms diagnostic evaluation of MT systems over lin-
guistic checkpoints for English?Chinese. Probably
due to its limitation to one language pair, its pro-
prietary nature as well as rather restrictive licensing
conditions, Woodpecker does not seem to have been
widely used in the community, in spite of its ability
to support diagnostic evaluation.
DELiC4MT1 is an open-source software that fol-
lows the same approach as Woodpecker. However,
DELiC4MT is easily portable to any language pair2
and provides additional functionality such as filter-
ing of noisy checkpoint instances and support for
statistical significance tests. This paper focuses on
the usage of this tool through a web application and
a web service from the user?s perspective. Details
regarding its implementation, evaluation, etc. can
be found in (Toral et al, 2012; Naskar et al, 2011).
2 Web Services for Language Technology
Tools
There exist many freely available language pro-
cessing tools, some of which are distributed under
open-source licenses. In order to use these tools,
they need to be downloaded, installed, configured
and maintained, which results in high cost both in
terms of manual effort and computing resources.
The requirement for in-depth technical knowledge
severely limits the usability of these tools amongst
non-technical users, particularly in our case amongst
translators and post-editors.
Web services introduce a new paradigm in the
way we use software tools where only providers
of the tools are required to have knowledge re-
garding their installation, configuration and mainte-
nance. This enables wider adoption of the tools and
reduces the learning curve for users as the only infor-
mation needed is basic knowledge of the functional-
1http://www.computing.dcu.ie/?atoral/
delic4mt/
2It has already been tested on language pairs involving
the following languages: Arabic, Bulgarian, Dutch, English,
French, German, Hindi, Italian, Turkish and Welsh.
ity and input/output parameters (which can be easily
included, e.g. as part of an online tutorial). While
this paradigm is rather new in the field of compu-
tational linguistics, it is quite mature and successful
in other fields such as bioinformatics (Oinn et al,
2004; Labarga et al, 2007).
Related work includes two web applications in the
area of MT evaluation. iBLEU (Madnani, 2011) or-
ganises BLEU scoring information in a visual man-
ner. Berka et al (2012) perform automatic error de-
tection and classification of MT output.
Figure 1: Web interface for the web service.
3 Demo
The demo presented in this paper consists of a
web service and a web application built on top of
DELiC4MT that allow to assess the performance of
MT systems on different linguistic phenomena de-
21
Figure 2: Screenshot of the web application (visualisation of results).
fined by the user. The following subsections detail
both parts of the demo.
3.1 Web Service
A SOAP-compliant web service3 has been built on
top of DELiC4MT. It receives the following input
parameters (see Figure 1):
1. Word alignment between the source and target
sides of the testset, in the GIZA++ (Och and
Ney, 2003) output format.
2. Linguistic checkpoint defined as a Ky-
bot4 (Vossen et al, 2010) profile.
3. Output of the MT system to be evaluated, in
plain text, tokenised and one sentence per line.
4. Source and target sides of the testset (or
gold standard), in KAF format (Bosma et al,
2009).5
The tool then evaluates the performance of the
MT system (input parameter 3) on the linguistic phe-
nomenon (parameter 2) by following this procedure:
3http://registry.elda.org/services/301
4Kybot profiles can be understood as regular expressions
over KAF documents, http://kyoto.let.vu.nl/svn/
kyoto/trunk/modules/mining_module/
5An XML format for text analysis based on representation
standards from ISO TC37/SC4.
? Occurrences of the linguistic phenomenon (pa-
rameter 2) are identified in the source side of
the testset (parameter 4).
? The equivalent tokens of these occurrences in
the target side (parameter 5) are found by using
word alignment information (parameter 1).
? For each checkpoint instance, the tool checks
how many of the n-grams present in the refer-
ence of the checkpoint instance are contained
in the output produced by the MT system (pa-
rameter 3).
3.2 Web Application
The web application builds a graphical interface on
top of the web service. It allows the user to visualise
the results in a fine-grained manner, the user can see
the performance of the MT system for each single
occurrence of the linguistic phenomenon.
Sample MT output for the ?noun? checkpoint for
the English to French language direction is shown
in Figure 2. Two occurrences of the checkpoint are
shown. The first one regards the source noun ?mr.?
and its translation in the reference ?monsieur?, iden-
tified through word alignments. The alignment (4-
4) indicates that both the source and target tokens
appear at the fifth position (0-based index) in the
sentence. The reference token (?monsieur?) is not
found in the MT output and thus a score of 0/1
22
(0 n-gram matches out of a total of 1 possible n-
gram) is assigned to the MT system for this noun in-
stance. Conversely, the score for the second occur-
rence (?speaker?) is 1/1 since the MT output con-
tains the 1-gram of the reference translation (?ora-
teur?).
The recall-based overall score is shown at the bot-
tom of the figure (0.5025). This is calculated by
summing up the scores (matching n-grams) for all
the occurrences (803) and dividing the result by the
total number of possible n-grams (1598).
4 Conclusions
In this paper we have presented a web applica-
tion and a web service for the diagnostic evalua-
tion of MT output over linguistic phenomena using
DELiC4MT. The tool allows users and developers
of MT systems to easily receive fine-grained feed-
back on the performance of their MT systems over
linguistic checkpoints of their interest. The applica-
tion is open-source, freely available and adaptable to
any language pair.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 under
grant agreements FP7-ICT-4-248531 and PIAP-GA-
2012-324414 and through Science Foundation Ire-
land as part of the CNGL (grant 07/CE/I1142)
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Intrin-
sic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, Proceedings of the
ACL-05 Workshop, pages 65?72, University of Michi-
gan, Ann Arbor, Michigan, USA.
Jan Berka, Ondej Bojar, Mark Fishel, Maja Popovi, and
Daniel Zeman. 2012. Automatic MT Error Anal-
ysis: Hjerson Helping Addicter. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), Istanbul, Turkey.
European Language Resources Association (ELRA).
W. E. Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic
semantic annotation format. In Proceedings of the
GL2009 Workshop on Semantic Annotation, Septem-
ber.
Alberto Labarga, Franck Valentin, Mikael Andersson,
and Rodrigo Lopez. 2007. Web services at the euro-
pean bioinformatics institute. Nucleic Acids Research,
35(Web-Server-Issue):6?11.
Nitin Madnani. 2011. iBLEU: Interactively Debugging
and Scoring Statistical Machine Translation Systems.
In Proceedings of the 2011 IEEE Fifth International
Conference on Semantic Computing, ICSC ?11, pages
213?214, Washington, DC, USA. IEEE Computer So-
ciety.
George A. Miller. 1995. WordNet: a lexical database for
English. Commun. ACM, 38(11):39?41, November.
Sudip Kumar Naskar, Antonio Toral, Federico Gaspari,
and Andy Way. 2011. A Framework for Diagnostic
Evaluation of MT based on Linguistic Checkpoints. In
Proceedings of the 13th Machine Translation Summit,
pages 529?536, Xiamen, China, September.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51, March.
Tom Oinn, Matthew Addis, Justin Ferris, Darren Marvin,
Martin Senger, Mark Greenwood, Tim Carver, Kevin
Glover, Matthew R. Pocock, Anil Wipat, and Peter Li.
2004. Taverna: a tool for the composition and en-
actment of bioinformatics workflows. Bioinformatics,
20(17):3045?3054, November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Antonio Toral, Sudip Kumar Naskar, Federico Gaspari,
and Declan Groves. 2012. DELiC4MT: A Tool for
Diagnostic MT Evaluation over User-defined Linguis-
tic Phenomena. The Prague Bulletin of Mathematical
Linguistics, pages 121?132.
Piek Vossen, German Rigau, Eneko Agirre, Aitor Soroa,
Monica Monachini, and Roberto Bartolini. 2010. KY-
OTO: an open platform for mining facts. In Proceed-
ings of the 6th Workshop on Ontologies and Lexical
Resources, pages 1?10, Beijing, China.
Ming Zhou, Bo Wang, Shujie Liu, Mu Li, Dongdong
Zhang, and Tiejun Zhao. 2008. Diagnostic evalu-
ation of machine translation systems using automati-
cally constructed linguistic check-points. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics - Volume 1, COLING ?08, pages
1121?1128, Stroudsburg, PA, USA. Association for
Computational Linguistics.
23
