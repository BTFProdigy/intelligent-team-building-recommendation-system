Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 262?270,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Compiling a Massive, Multilingual Dictionary via Probabilistic Inference
Mausam Stephen Soderland Oren Etzioni
Daniel S. Weld Michael Skinner* Jeff Bilmes
University of Washington, Seattle *Google, Seattle
{mausam,soderlan,etzioni,weld,bilmes}@cs.washington.edu mskinner@google.com
Abstract
Can we automatically compose a large set
of Wiktionaries and translation dictionar-
ies to yield a massive, multilingual dic-
tionary whose coverage is substantially
greater than that of any of its constituent
dictionaries?
The composition of multiple translation
dictionaries leads to a transitive inference
problem: if word A translates to word
B which in turn translates to word C,
what is the probability that C is a trans-
lation of A? The paper introduces a
novel algorithm that solves this problem
for 10,000,000 words in more than 1,000
languages. The algorithm yields PANDIC-
TIONARY, a novel multilingual dictionary.
PANDICTIONARY contains more than four
times as many translations than in the
largest Wiktionary at precision 0.90 and
over 200,000,000 pairwise translations in
over 200,000 language pairs at precision
0.8.
1 Introduction and Motivation
In the era of globalization, inter-lingual com-
munication is becoming increasingly important.
Although nearly 7,000 languages are in use to-
day (Gordon, 2005), most language resources are
mono-lingual, or bi-lingual.1 This paper investi-
gates whether Wiktionaries and other translation
dictionaries available over the Web can be auto-
matically composed to yield a massive, multilin-
gual dictionary with superior coverage at compa-
rable precision.
We describe the automatic construction of a
massive multilingual translation dictionary, called
1The English Wiktionary, a lexical resource developed by
volunteers over the Internet is one notable exception that con-
tains translations of English words in about 500 languages.
Figure 1: A fragment of the translation graph for two senses
of the English word ?spring?. Edges labeled ?1? and ?3? are
for spring in the sense of a season, and ?2? and ?4? are for
the flexible coil sense. The graph shows translation entries
from an English dictionary merged with ones from a French
dictionary.
PANDICTIONARY, that could serve as a resource
for translation systems operating over a very
broad set of language pairs. The most immedi-
ate application of PANDICTIONARY is to lexical
translation?the translation of individual words or
simple phrases (e.g., ?sweet potato?). Because
lexical translation does not require aligned cor-
pora as input, it is feasible for a much broader
set of languages than statistical Machine Transla-
tion (SMT). Of course, lexical translation cannot
replace SMT, but it is useful for several applica-
tions including translating search-engine queries,
library classifications, meta-data tags,2 and recent
applications like cross-lingual image search (Et-
zioni et al, 2007), and enhancing multi-lingual
Wikipedias (Adar et al, 2009). Furthermore,
lexical translation is a valuable component in
knowledge-based Machine Translation systems,
e.g., (Bond et al, 2005; Carbonell et al, 2006).
PANDICTIONARY currently contains over 200
million pairwise translations in over 200,000 lan-
guage pairs at precision 0.8. It is constructed from
information harvested from 631 online dictionar-
ies and Wiktionaries. This necessitates match-
2Meta-data tags appear in community Web sites such as
flickr.com and del.icio.us.
262
ing word senses across multiple, independently-
authored dictionaries. Because of the millions of
translations in the dictionaries, a feasible solution
to this sense matching problem has to be scalable;
because sense matches are imperfect and uncer-
tain, the solution has to be probabilistic.
The core contribution of this paper is a princi-
pled method for probabilistic sense matching to in-
fer lexical translations between two languages that
do not share a translation dictionary. For exam-
ple, our algorithm can conclude that Basque word
?udaherri? is a translation of Maori word ?koanga?
in Figure 1. Our contributions are as follows:
1. We describe the design and construction of
PANDICTIONARY?a novel lexical resource
that spans over 200 million pairwise transla-
tions in over 200,000 language pairs at 0.8
precision, a four-fold increase when com-
pared to the union of its input translation dic-
tionaries.
2. We introduce SenseUniformPaths, a scal-
able probabilistic method, based on graph
sampling, for inferring lexical translations,
which finds 3.5 times more inferred transla-
tions at precison 0.9 than the previous best
method.
3. We experimentally contrast PANDIC-
TIONARY with the English Wiktionary and
show that PANDICTIONARY is from 4.5 to
24 times larger depending on the desired
precision.
The remainder of this paper is organized as fol-
lows. Section 2 describes our earlier work on
sense matching (Etzioni et al, 2007). Section 3
describes how the PANDICTIONARY builds on and
improves on their approach. Section 4 reports on
our experimental results. Section 5 considers re-
lated work on lexical translation. The paper con-
cludes in Section 6 with directions for future work.
2 Building a Translation Graph
In previous work (Etzioni et al, 2007) we intro-
duced an approach to sense matching that is based
on translation graphs (see Figure 1 for an exam-
ple). Each vertex v ? V in the graph is an or-
dered pair (w, l) where w is a word in a language
l. Undirected edges in the graph denote transla-
tions between words: an edge e ? E between (w1,
l1) and (w2, l2) represents the belief that w1 and
w2 share at least one word sense.
Construction: The Web hosts a large num-
ber of bilingual dictionaries in different languages
and several Wiktionaries. Bilingual dictionaries
translate words from one language to another, of-
ten without distinguishing the intended sense. For
example, an Indonesian-English dictionary gives
?light? as a translation of the Indonesian word ?en-
teng?, but does not indicate whether this means il-
lumination, light weight, light color, or the action
of lighting fire.
The Wiktionaries (wiktionary.org) are sense-
distinguished, multilingual dictionaries created by
volunteers collaborating over the Web. A transla-
tion graph is constructed by locating these dictio-
naries, parsing them into a common XML format,
and adding the nodes and edges to the graph.
Figure 1 shows a fragment of a translation
graph, which was constructed from two sets of
translations for the word ?spring? from an English
Wiktionary, and two corresponding entries from
a French Wiktionary for ?printemps? (spring sea-
son) and ?ressort? (flexible spring). Translations of
the season ?spring? have edges labeled with sense
ID=1, the flexible coil sense has ID=2, translations
of ?printemps? have ID=3, and so forth.3
For clarity, we show only a few of the actual
vertices and edges; e.g., the figure doesn?t show
the edge (ID=1) between ?udaherri? and ?primav-
era?.
Inference: In our previous system we had
a simple inference procedure over translation
graphs, called TRANSGRAPH, to find translations
beyond those provided by any source dictionary.
TRANSGRAPH searched for paths in the graph be-
tween two vertices and estimated the probability
that the path maintains the same word sense along
all edges in the path, even when the edges come
from different dictionaries. For example, there are
several paths between ?udaherri? and ?koanga? in
Figure 1, but all shift from sense ID 1 to 3. The
probability that the two words are translations is
equivalent to the probability that IDs 1 and 3 rep-
resent the same sense.
TRANSGRAPH used two formulae to estimate
these probabilities. One formula estimates the
probability that two multi-lingual dictionary en-
tries represent the same word sense, based on the
proportion of overlapping translations for the two
entries. For example, most of the translations of
3Sense-distinguished multi-lingual entries give rise to
cliques all of which share a common sense ID.
263
French ?printemps? are also translations of the sea-
son sense of ?spring?. A second formula is based
on triangles in the graph (useful for bilingual dic-
tionaries): a clique of 3 nodes with an edge be-
tween each pair of nodes. In such cases, there is
a high probability that all 3 nodes share a word
sense.
Critique: While TRANSGRAPH was the first
to present a scalable inference method for lexical
translation, it suffers from several drawbacks. Its
formulae operate only on local information: pairs
of senses that are adjacent in the graph or triangles.
It does not incorporate evidence from longer paths
when an explicit triangle is not present. Moreover,
the probabilities from different paths are com-
bined conservatively (either taking the max over
all paths, or using ?noisy or? on paths that are
completely disjoint, except end points), thus lead-
ing to suboptimal precision/recall.
In response to this critique, the next section
presents an inference algorithm, called SenseUni-
formPaths (SP), with substantially improved recall
at equivalent precision.
3 Translation Inference Algorithms
In essence, inference over a translation graph
amounts to transitive sense matching: if word A
translates to word B, which translates in turn to
word C, what is the probability that C is a trans-
lation of A? If B is polysemous then C may not
share a sense with A. For example, in Figure 2(a)
if A is the French word ?ressort? (the flexible-
coil sense of spring) and B is the English word
?spring?, then Slovenian word ?vzmet? may or may
not be a correct translation of ?ressort? depending
on whether the edge (B,C) denotes the flexible-
coil sense of spring, the season sense, or another
sense. Indeed, given only the knowledge of the
path A ? B ? C we cannot claim anything with
certainty regarding A to C.
However, if A, B, and C are on a circuit that
starts at A, passes through B and C and re-
turns to A, there is a high probability that all
nodes on that circuit share a common word sense,
given certain restrictions that we enumerate later.
Where TRANSGRAPH used evidence from circuits
of length 3, we extend this to paths of arbitrary
lengths.
To see how this works, let us begin with the sim-
plest circuit, a triangle of three nodes as shown in
Figure 2(b). We can be quite certain that ?vzmet?
shares the sense of coil with both ?spring? and
?ressort?. Our reasoning is as follows: even
though both ?ressort? and ?spring? are polysemous
they share only one sense. For a triangle to form
we have two choices ? (1) either ?vzmet? means
spring coil, or (2) ?vzmet? means both the spring
season and jurisdiction, but not spring coil. The
latter is possible but such a coincidence is very un-
likely, which is why a triangle is strong evidence
for the three words to share a sense.
As an example of longer paths, our inference
algorithms can conclude that in Figure 2(c), both
?molla? and ?vzmet? have the sense coil, even
though no explicit triangle is present. To show
this, let us define a translation circuit as follows:
Definition 1 A translation circuit from v?1 with
sense s? is a cycle that starts and ends at v?1 with
no repeated vertices (other than v?1 at end points).
Moreover, the path includes an edge between v?1
and another vertex v?2 that also has sense s
?.
All vertices on a translation circuit are mutual
translations with high probability, as in Figure
2(c). The edge from ?spring? indicates that ?vzmet?
means either coil or season, while the edge from
?ressort? indicates that ?molla? means either coil
or jurisdiction. The edge from ?vzmet? to ?molla?
indicates that they share a sense, which will hap-
pen if all nodes share the sense season or if either
?vzmet? has the unlikely combination of coil and
jurisdiction (or ?molla? has coil and season).
We also develop a mathematical model of
sense-assignment to words that lets us formally
prove these insights. For more details on the the-
ory please refer to our extended version. This pa-
per reports on our novel algorithm and experimen-
tal results.
These insights suggest a basic version of our al-
gorithm: ?given two vertices, v?1 and v
?
2 , that share
a sense (say s?) compute all translation circuits
from v?1 in the sense s
?; mark all vertices in the
circuits as translations of the sense s??.
To implement this algorithm we need to decide
whether a vertex lies on a translation circuit, which
is trickier than it seems. Notice that knowing
that v is connected independently to v?1 and v
?
2
doesn?t imply that there exists a translation circuit
through v, because both paths may go through a
common node, thus violating of the definition of
translation circuit. For example, in Figure 2(d) the
Catalan word ?ploma? has paths to both spring and
ressort, but there is no translation circuit through
264
spring
English
ressort
French
vzmet
Slovenian
spring
English
ressort
French
vzmet
Slovenian
spring
English
vzmet
Slovenian
ressort
French
molla
Italian
spring
English
ressort
French
ploma
Catalan
Feder
German
???? 
Russian
spring
English
ressort
French
fj?der
Swedish
penna
Italian
Feder
German
(a)                         (b)                                   (c)                                (d)                     (e)
season
coil
jurisdiction
coil
s* s*
s* s*
s*
? ? ?
? ?
feather
coil
?
?
Figure 2: Snippets of translation graphs illustrating various inference scenarios. The nodes in question mark represent the
nodes in focus for each illustration. For all cases we are trying to infer translations of the flexible coil sense of spring.
it. Hence, it will not be considered a transla-
tion. This example also illustrates potential errors
avoided by our algorithm ? here, German word
?Feder? mean feather and spring coil, but ?ploma?
means feather and not the coil.
An exhaustive search to find translation circuits
would be too slow, so we approximate the solution
by a random walk scheme. We start the random
walk from v?1 (or v
?
2) and choose random edges
without repeating any vertices in the current path.
At each step we check if the current node has an
edge to v?2 (or v
?
1). If it does, then all the ver-
tices in the current path form a translation circuit
and, thus, are valid translations. We repeat this
random walk many times and keep marking the
nodes. In our experiments for each inference task
we performed a total of 2,000 random walks (NR
in pseudo-code) of max circuit length 7. We chose
these parameters based on a development set of 50
inference tasks.
Our first experiments with this basic algorithm
resulted in a much higher recall than TRANS-
GRAPH, albeit, at a significantly lower precision.
A closer examination of the results revealed two
sources of error ? (1) errors in source dictionary
data, and (2) correlated sense shifts in translation
circuits. Below we add two new features to our
algorithm to deal with each of these error sources,
respectively.
3.1 Errors in Source Dictionaries
In practice, source dictionaries contain mistakes
and errors occur in processing the dictionaries to
create the translation graph. Thus, existence of a
single translation circuit is only limited evidence
for a vertex as a translation. We wish to exploit
the insight that more translation circuits constitute
stronger evidence. However, the different circuits
may share some edges, and thus the evidence can-
not be simply the number of translation circuits.
We model the errors in dictionaries by assigning
a probability less than 1.0 to each edge4 (pe in the
4In our experiments we used a flat value of 0.6, chosen by
pseudo-code). We assume that the probability of
an edge being erroneous is independent of the rest
of the graph. Thus, a translation graph with pos-
sible data errors converts into a distribution over
accurate translation graphs.
Under this distribution, we can use the proba-
bility of existence of a translation circuit through a
vertex as the probability that the vertex is a trans-
lation. This value captures our insights, since a
larger number of translation circuits gives a higher
probability value.
We sample different graph topologies from our
given distribution. Some translation circuits will
exist in some of the sampled graphs, but not in
others. This, in turn, means that a given vertex v
will only be on a circuit for a fraction of the sam-
pled graphs. We take the proportion of samples in
which v is on a circuit to be the probability that v
is in the translation set. We refer to this algorithm
as Unpruned SenseUniformPaths (uSP).
3.2 Avoiding Correlated Sense-shifts
The second source of errors are circuits that in-
clude a pair of nodes sharing the same polysemy,
i.e., having the same pair of senses. A circuit
might maintain sense s? until it reaches a node that
has both s? and a distinct si. The next edge may
lead to a node with si, but not s?, causing an ex-
traction error. The path later shifts back to sense
s? at a second node that also has s? and si. An ex-
ample for this is illustrated in Figure 2(e), where
both the German and Swedish words mean feather
and spring coil. Here, Italian ?penna? means only
the feather and not the coil.
Two nodes that share the same two senses oc-
cur frequently in practice. For example, many
languages use the same word for ?heart? (the or-
gan) and center; similarly, it is common for lan-
guages to use the same word for ?silver?, the metal
and the color. These correlations stem from com-
parameter tuning on a development set of 50 inference tasks.
In future we can use different values for different dictionaries
based on our confidence in their accuracy.
265
Figure 3: The set {B, C} has a shared ambiguity - each
node has both sense 1 (from the lower clique) and sense 2
(from the upper clique). A circuit that contains two nodes
from the same ambiguity set with an intervening node not in
that set is likely to create translation errors.
mon metaphor and the shared evolutionary roots
of some languages.
We are able to avoid circuits with this type of
correlated sense-shift by automatically identifying
ambiguity sets, sets of nodes known to share mul-
tiple senses. For instance, in Figure 2(e) ?Feder?
and ?fj?der? form an ambiguity set (shown within
dashed lines), as they both mean feather and coil.
Definition 2 An ambiguity set A is a set of ver-
tices that all share the same two senses. I.e.,
?s1, s2, with s1 6= s2 s.t. ?v ? A, sense(v, s1)?
sense(v, s2), where sense(v, s) denotes that v has
sense s.
To increase the precision of our algorithm we
prune the circuits that contain two nodes in the
same ambiguity set and also have one or more in-
tervening nodes that are not in the ambiguity set.
There is a strong likelihood that the intervening
nodes will represent a translation error.
Ambiguity sets can be detected from the graph
topology as follows. Each clique in the graph rep-
resents a set of vertices that share a common word
sense. When two cliques intersect in two or more
vertices, the intersecting vertices share the word
sense of both cliques. This may either mean that
both cliques represent the same word sense, or that
the intersecting vertices form an ambiguity set. A
large overlap between two cliques makes the for-
mer case more likely; a small overlap makes it
more likely that we have found an ambiguity set.
Figure 3 illustrates one such computation.
All nodes of the clique V1, V2, A,B,C,D share
a word sense, and all nodes of the clique
B,C,E, F,G,H also share a word sense. The set
{B,C} has nodes that have both senses, forming
an ambiguity set. We denote the set of ambiguity
sets by A in the pseudo-code.
Having identified these ambiguity sets, we mod-
ify our random walk scheme by keeping track of
whether we are entering or leaving an ambiguity
set. We prune away all paths that enter the same
ambiguity set twice. We name the resulting algo-
rithm SenseUniformPaths (SP), summarized at a
high level in Algorithm 1.
Comparing Inference Algorithms Our evalua-
tion demonstrated that SP outperforms uSP. Both
these algorithms have significantly higher recall
than TRANSGRAPH algorithm. The detailed re-
sults are presented in Section 4.2. We choose SP
as our inference algorithm for all further research,
in particular to create PANDICTIONARY.
3.3 Compiling PanDictionary
Our goal is to automatically compile PANDIC-
TIONARY, a sense-distinguished lexical transla-
tion resource, where each entry is a distinct word
sense. Associated with each word sense is a list of
translations in multiple languages.
We use Wiktionary senses as the base senses
for PANDICTIONARY. Recall that SP requires two
nodes (v?1 and v
?
2) for inference. We use the Wik-
tionary source word as v?1 and automatically pick
the second word from the set of Wiktionary trans-
lations of that sense by choosing a word that is
well connected, and, which does not appear in
other senses of v?1 (i.e., is expected to share only
one sense with v?1).
We first run SenseUniformPaths to expand the
approximately 50,000 senses in the English Wik-
tionary. We further expand any senses from the
other Wiktionaries that are not yet covered by
PANDICTIONARY, and add these to PANDIC-
TIONARY. This results in the creation of the
world?s largest multilingual, sense-distinguished
translation resource, PANDICTIONARY. It con-
tains a little over 80,000 senses. Its construction
takes about three weeks on a 3.4 GHz processor
with a 2 GB memory.
Algorithm 1 S.P.(G, v?1, v
?
2,A)
1: parameters NG: no. of graph samples, NR: no. of ran-
dom walks, pe: prob. of sampling an edge
2: createNG versions ofG by sampling each edge indepen-
dently with probability pe
3: for all i = 1..NG do
4: for all vertices v : rp[v][i] = 0
5: perform NR random walks starting at v?1 (or v
?
2 ) and
pruning any walk that enters (or exits) an ambiguity
set in A twice. All walks that connect to v?2 (or v
?
1 )
form a translation circuit.
6: for all vertices v do
7: if(v is on a translation circuit) rp[v][i] = 1
8: return
?
i
rp[v][i]
NG
as the prob. that v is a translation
266
4 Empirical Evaluation
In our experiments we investigate three key ques-
tions: (1) which of the three algorithms (TG, uSP
and SP) is superior for translation inference (Sec-
tion 4.2)? (2) how does the coverage of PANDIC-
TIONARY compare with the largest existing mul-
tilingual dictionary, the English Wiktionary (Sec-
tion 4.3)? (3) what is the benefit of inference over
the mere aggregation of 631 dictionaries (Section
4.4)? Additionally, we evaluate the inference algo-
rithm on two other dimensions ? variation with the
degree of polysemy of source word, and variation
with original size of the seed translation set.
4.1 Experimental Methodology
Ideally, we would like to evaluate a random sam-
ple of the more than 1,000 languages represented
in PANDICTIONARY.5 However, a high-quality
evaluation of translation between two languages
requires a person who is fluent in both languages.
Such people are hard to find and may not even
exist for many language pairs (e.g., Basque and
Maori). Thus, our evaluation was guided by our
ability to recruit volunteer evaluators. Since we
are based in an English speaking country we were
able to recruit local volunteers who are fluent in
a range of languages and language families, and
who are also bilingual in English.6
The experiments in Sections 4.2 and 4.3 test
whether translations in a PANDICTIONARY have
accurate word senses. We provided our evalua-
tors with a random sample of translations into their
native language. For each translation we showed
the English source word and gloss of the intended
sense. For example, a Dutch evaluator was shown
the sense ?free (not imprisoned)? together with the
Dutch word ?loslopende?. The instructions were
to mark a word as correct if it could be used to ex-
press the intended sense in a sentence in their na-
tive language. For experiments in Section 4.4 we
tested precision of pairwise translations, by having
informants in several pairs of languages discuss
whether the words in their respective languages
can be used for the same sense.
We use the tags of correct or incorrect to com-
pute the precision: the percentage of correct trans-
5The distribution of words in PANDICTIONARY is highly
non-uniform ranging from 182,988 words in English to 6,154
words in Luxembourgish and 189 words in Tuvalu.
6The languages used was based on the availability of na-
tive speakers. This varied between the different experiments,
which were conducted at different times.
Figure 4: The SenseUniformPaths algorithm (SP) more
than doubles the number of correct translations at precision
0.95, compared to a baseline of translations that can be found
without inference.
lations divided by correct plus incorrect transla-
tions. We then order the translations by probabil-
ity and compute the precision at various probabil-
ity thresholds.
4.2 Comparing Inference Algorithms
Our first evaluation compares our SenseUniform-
Paths (SP) algorithm (before and after pruning)
with TRANSGRAPH on both precision and num-
ber of translations.
To carry out this comparison, we randomly sam-
pled 1,000 senses from English Wiktionary and
ran the three algorithms over them. We evalu-
ated the results on 7 languages ? Chinese, Danish,
German, Hindi, Japanese, Russian, and Turkish.
Each informant tagged 60 random translations in-
ferred by each algorithm, which resulted in 360-
400 tags per algorithm7. The precision over these
was taken as a surrogate for the precision across
all the senses.
We compare the number of translations for each
algorithm at comparable precisions. The baseline
is the set of translations (for these 1000 senses)
found in the source dictionaries without inference,
which has a precision 0.95 (as evaluated by our
informants).8
Our results are shown in Figure 4. At this high
precision, SP more than doubles the number of
baseline translations, finding 5 times as many in-
ferred translations (in black) as TG.
Indeed, both uSP and SP massively outperform
TG. SP is consistently better than uSP, since it
performs better for polysemous words, due to its
pruning based on ambiguity sets. We conclude
7Some translations were marked as ?Don?t know?.
8Our informants tended to underestimate precision, often
marking correct translations in minor senses of a word as in-
correct.
267
0.5
0.6
0.7
0.8
0.9
1
0.0 4.0 8.0 12.0 16.0
Pr
ec
is
io
n
Translations in Millions
PanDictionary
English Wiktionary
Figure 5: Precision vs. coverage curve for PANDIC-
TIONARY. It quadruples the size of the English Wiktionary at
precision 0.90, is more than 8 times larger at precision 0.85
and is almost 24 times the size at precision 0.7.
that SP is the best inference algorithm and employ
it for PANDICTIONARY construction.
4.3 Comparison with English Wiktionary
We now compare the coverage of PANDIC-
TIONARY with the English Wiktionary at varying
levels of precision. The English Wiktionary is the
largest Wiktionary with a total of 403,413 transla-
tions. It is also more reliable than some other Wik-
tionaries in making word sense distinctions. In this
study we use only the subset of PANDICTIONARY
that was computed starting from the English Wik-
tionary senses. Thus, this subsection under-reports
PANDICTIONARY?s coverage.
To evaluate a huge resource such as PANDIC-
TIONARY we recruited native speakers of 14 lan-
guages ? Arabic, Bulgarian, Danish, Dutch, Ger-
man, Hebrew, Hindi, Indonesian, Japanese, Ko-
rean, Spanish, Turkish, Urdu, and Vietnamese. We
randomly sampled 200 translations per language,
which resulted in about 2,500 tags. Figure 5
shows the total number of translations in PANDIC-
TIONARY in senses from the English Wiktionary.
At precision 0.90, PANDICTIONARY has 1.8 mil-
lion translations, 4.5 times as many as the English
Wiktionary.
We also compare the coverage of PANDIC-
TIONARY with that of the English Wiktionary in
terms of languages covered. Table 1 reports, for
each resource, the number of languages that have
a minimum number of distinct words in the re-
source. PANDICTIONARY has 1.4 times as many
languages with at least 1,000 translations at pre-
cision 0.90 and more than twice at precision 0.7.
These observations reaffirm our faith in the pan-
lingual nature of the resource.
PANDICTIONARY?s ability to expand the lists
of translations provided by the EnglishWiktionary
is most pronounced for senses with a small num-
0.75
0.8
0.85
0.9
0.95
1 2 3,4 >4
Pre
cis
ion
Avg precision 0.90
Avg precision 0.85
Polysemy of the English source word
3-4
Figure 6: Variation of precision with the degree of poly-
semy of the source English word. The precision decreases as
polysemy increases, still maintaining reasonably high values.
ber of translations. For example, at precision 0.90,
senses that originally had 3 to 6 translations are in-
creased 5.3 times in size. The increase is 2.2 times
when the original sense size is greater than 20.
For closer analysis we divided the English
source words (v?1) into different bins based on the
number of senses that English Wiktionary lists for
them. Figure 6 plots the variation of precision with
this degree of polysemy. We find that translation
quality decreases as degree of polysemy increases,
but this decline is gradual, which suggests that SP
algorithm is able to hold its ground well in difficult
inference tasks.
4.4 Comparison with All Source Dictionaries
We have shown that PANDICTIONARY has much
broader coverage than the English Wiktionary, but
how much of this increase is due to the inference
algorithm versus the mere aggregation of hundreds
of translation dictionaries in PANDICTIONARY?
Since most bilingual dictionaries are not sense-
distinguished, we ignore the word senses and
count the number of distinct (word1, word2) trans-
lation pairs.
We evaluated the precision of word-word trans-
lations by a collaborative tagging scheme, with
two native speakers of different languages, who
are both bi-lingual in English. For each sug-
gested translation they discussed the various
senses of words in their respective languages
and tag a translation correct if they found some
sense that is shared by both words. For this
study we tagged 7 language pairs: Hindi-Hebrew,
# languages with distinct words
? 1000 ? 100 ? 1
English Wiktionary 49 107 505
PanDictionary (0.90) 67 146 608
PanDictionary (0.85) 75 175 794
PanDictionary (0.70) 107 607 1066
Table 1: PANDICTIONARY covers substantially more lan-
guages than the English Wiktionary.
268
050
100
150
200
250
EW 631D PD(0.9) PD(0.85) PD(0.8)
Inferred transl. Direct transl.
Tra
nsl
ati
on
s(i
n m
illio
ns)
Figure 7: The number of distinct word-word translation
pairs from PANDICTIONARY is several times higher than the
number of translation pairs in the English Wiktionary (EW)
or in all 631 source dictionaries combined (631 D). A major-
ity of PANDICTIONARY translations are inferred by combin-
ing entries from multiple dictionaries.
Japanese-Russian, Chinese-Turkish, Japanese-
German, Chinese-Russian, Bengali-German, and
Hindi-Turkish.
Figure 7 compares the number of word-word
translation pairs in the English Wiktionary (EW),
in all 631 source dictionaries (631 D), and in PAN-
DICTIONARY at precisions 0.90, 0.85, and 0.80.
PANDICTIONARY increases the number of word-
word translations by 73% over the source dictio-
nary translations at precision 0.90 and increases it
by 2.7 times at precision 0.85. PANDICTIONARY
also adds value by identifying the word sense of
the translation, which is not given in most of the
source dictionaries.
5 Related Work
Because we are considering a relatively new prob-
lem (automatically building a panlingual transla-
tion resource) there is little work that is directly re-
lated to our own. The closest research is our previ-
ous work on TRANSGRAPH algorithm (Etzioni et
al., 2007). Our current algorithm outperforms the
previous state of the art by 3.5 times at precision
0.9 (see Figure 4). Moreover, we compile this in a
dictionary format, thus considerably reducing the
response time compared to TRANSGRAPH, which
performed inference at query time.
There has been considerable research on meth-
ods to acquire translation lexicons from either
MRDs (Neff and McCord, 1990; Helmreich et
al., 1993; Copestake et al, 1994) or from par-
allel text (Gale and Church, 1991; Fung, 1995;
Melamed, 1997; Franz et al, 2001), but this has
generally been limited to a small number of lan-
guages. Manually engineered dictionaries such as
EuroWordNet (Vossen, 1998) are also limited to
a relatively small set of languages. There is some
recent work on compiling dictionaries from mono-
lingual corpora, which may scale to several lan-
guage pairs in future (Haghighi et al, 2008).
Little work has been done in combining mul-
tiple dictionaries in a way that maintains word
senses across dictionaries. Gollins and Sanderson
(2001) explored using triangulation between alter-
nate pivot languages in cross-lingual information
retrieval. Their triangulation essentially mixes
together circuits for all word senses, hence, is un-
able to achieve high precision.
Dyvik?s ?semantic mirrors? uses translation
paths to tease apart distinct word senses from
inputs that are not sense-distinguished (Dyvik,
2004). However, its expensive processing and
reliance on parallel corpora would not scale to
large numbers of languages. Earlier (Knight and
Luk, 1994) discovered senses of Spanish words by
matching several English translations to a Word-
Net synset. This approach applies only to specific
kinds of bilingual dictionaries, and also requires a
taxonomy of synsets in the target language.
Random walks, graph sampling and Monte
Carlo simulations are popular in literature, though,
to our knowledge, none have applied these to our
specific problems (Henzinger et al, 1999; Andrieu
et al, 2003; Karger, 1999).
6 Conclusions
We have described the automatic construction of
a unique multilingual translation resource, called
PANDICTIONARY, by performing probabilistic in-
ference over the translation graph. Overall, the
construction process consists of large scale in-
formation extraction over the Web (parsing dic-
tionaries), combining it into a single resource (a
translation graph), and then performing automated
reasoning over the graph (SenseUniformPaths) to
yield a much more extensive and useful knowl-
edge base.
We have shown that PANDICTIONARY has
more coverage than any other existing bilingual
or multilingual dictionary. Even at the high preci-
sion of 0.90, PANDICTIONARY more than quadru-
ples the size of the English Wiktionary, the largest
available multilingual resource today.
We plan to make PANDICTIONARY available
to the research community, and also to the Wik-
tionary community in an effort to bolster their ef-
forts. PANDICTIONARY entries can suggest new
translations for volunteers to add to Wiktionary
entries, particularly if combined with an intelli-
gent editing tool (e.g., (Hoffmann et al, 2009)).
269
Acknowledgments
This research was supported by a gift from the
Utilika Foundation to the Turing Center at Uni-
versity of Washington. We acknowledge Paul
Beame, Nilesh Dalvi, Pedro Domingos, Rohit
Khandekar, Daniel Lowd, Parag, Jonathan Pool,
Hoifung Poon, Vibhor Rastogi, Gyanit Singh for
fruitful discussions and insightful comments on
the research. We thank the language experts who
donated their time and language expertise to eval-
uate our systems. We also thank the anynomous
reviewers of the previous drafts of this paper for
their valuable suggestions in improving the evalu-
ation and presentation.
References
E. Adar, M. Skinner, and D. Weld. 2009. Information
arbitrage in multi-lingual Wikipedia. In Procs. of
Web Search and Data Mining(WSDM 2009).
C. Andrieu, N. De Freitas, A. Doucet, and M. Jor-
dan. 2003. An Introduction to MCMC for Machine
Learning. Machine Learning, 50:5?43.
F. Bond, S. Oepen, M. Siegel, A. Copestake, and
D D. Flickinger. 2005. Open source machine trans-
lation with DELPH-IN. In Open-Source Machine
Translation Workshop at MT Summit X.
J. Carbonell, S. Klein, D. Miller, M. Steinbaum,
T. Grassiany, and J. Frey. 2006. Context-based ma-
chine translation. In AMTA.
A. Copestake, T. Briscoe, P. Vossen, A. Ageno,
I. Castellon, F. Ribas, G. Rigau, H. Rodriquez, and
A. Samiotou. 1994. Acquisition of lexical trans-
lation relations from MRDs. Machine Translation,
3(3?4):183?219.
H. Dyvik. 2004. Translation as semantic mirrors: from
parallel corpus to WordNet. Language and Comput-
ers, 49(1):311?326.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer.
2007. Lexical translation with application to image
search on the Web. In Machine Translation Summit
XI.
M. Franz, S. McCarly, and W. Zhu. 2001. English-
Chinese information retrieval at IBM. In Proceed-
ings of TREC 2001.
P. Fung. 1995. A pattern matching method for finding
noun and proper noun translations from noisy paral-
lel corpora. In Proceedings of ACL-1995.
W. Gale and K.W. Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-1991.
T. Gollins and M. Sanderson. 2001. Improving cross
language retrieval with triangulated translation. In
SIGIR.
Raymond G. Gordon, Jr., editor. 2005. Ethnologue:
Languages of the World (Fifteenth Edition). SIL In-
ternational.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL.
S. Helmreich, L. Guthrie, and Y. Wilks. 1993. The
use of machine readable dictionaries in the Pangloss
project. In AAAI Spring Symposium on Building
Lexicons for Machine Translation.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 1999. Measuring index
quality using random walks on the web. In WWW.
R. Hoffmann, S. Amershi, K. Patel, F. Wu, J. Foga-
rty, and D. S. Weld. 2009. Amplifying commu-
nity content creation with mixed-initiative informa-
tion extraction. In ACM SIGCHI (CHI2009).
D. R. Karger. 1999. A randomized fully polynomial
approximation scheme for the all-terminal network
reliability problem. SIAM Journal of Computation,
29(2):492?514.
K. Knight and S. Luk. 1994. Building a large-scale
knowledge base for machine translation. In AAAI.
I.D. Melamed. 1997. A Word-to-Word Model of
Translational Equivalence. In Proceedings of ACL-
1997 and EACL-1997, pages 490?497.
M. Neff and M. McCord. 1990. Acquiring lexical data
from machine-readable dictionary resources for ma-
chine translation. In 3rd Intl Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion of Natural Language.
P. Vossen, editor. 1998. EuroWordNet: A multilingual
database with lexical semantic networds. Kluwer
Academic Publishers.
270
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 68?76,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Structuring Operative Notes using Active Learning
Kirk Roberts
?
National Library of Medicine
National Institutes of Health
Bethesda, MD 20894
kirk.roberts@nih.gov
Sanda M. Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75080
sanda@hlt.utdallas.edu
Michael A. Skinner
University of Texas Southwestern Medical Center
Children?s Medical Center of Dallas
Dallas, TX 75235
michael.skinner@childrens.com
Abstract
We present an active learning method for
placing the event mentions in an operative
note into a pre-specified event structure.
Event mentions are first classified into ac-
tion, peripheral action, observation, and
report events. The actions are further clas-
sified into their appropriate location within
the event structure. We examine how uti-
lizing active learning significantly reduces
the time needed to completely annotate a
corpus of 2,820 appendectomy notes.
1 Introduction
Operative reports are written or dictated after ev-
ery surgical procedure. They describe the course
of the operation as well as any abnormal find-
ings in the surgical process. Template-based and
structured methods exist for recording the opera-
tive note (DeOrio, 2002), and in many cases have
been shown to increase the completeness of sur-
gical information (Park et al., 2010; Gur et al.,
2011; Donahoe et al., 2012). The use of natural
language, however, is still preferred for its expres-
sive power. This unstructured information is typi-
cally the only vehicle for conveying important de-
tails of the procedure, including the surgical in-
struments, incision techniques, and laparoscopic
methods employed.
The ability to represent and extract the infor-
mation found within operative notes would enable
?
Most of this work was performed while KR was at the
University of Texas at Dallas.
powerful post-hoc reasoning methods about surgi-
cal procedures. First, the completeness problem
may be alleviated by indicating gaps in the sur-
gical narrative. Second, deep semantic similarity
methods could be used to discover comparable op-
erations across surgeons and institutions. Third,
given information on the typical course and find-
ings of a procedure, abnormal aspects of an oper-
ation could be identified and investigated. Finally,
other secondary use applications would be enabled
to study the most effective instruments and tech-
niques across large amounts of surgical data.
In this paper, we present an initial method for
aligning the event mentions within an operative
note to the overall event structure for a procedure.
A surgeon with experience in a particular proce-
dure first describes the overall event structure. A
supervised method enhanced by active learning is
then employed to rapidly build an information ex-
traction model to classify event mentions into the
event structure. This active learning paradigm al-
lows for rapid prototyping while also taking ad-
vantage of the sub-language characteristics of op-
erative notes and the common structure of opera-
tive notes reporting the same type of procedure. A
further goal of this method is to aid in the eval-
uation of unsupervised techniques that can auto-
matically discover the event structure solely from
the narratives. This would enable all the objectives
outlined above for leveraging the unstructured in-
formation within operative notes.
This paper presents a first attempt at this ac-
tive learning paradigm for structuring appendec-
tomy reports. We intentionally chose a well-
understood and relatively simple procedure to en-
68
sure a straight-forward, largely linear event struc-
ture where a large amount of data would be eas-
ily available. Section 3 describes a generic frame-
work for surgical event structures and the particu-
lar structure chosen for appendectomies. Section 4
details the data used in this study. Section 5 de-
scribes the active learning experiment for filling in
this event structure for operative notes. Section 6
reports the results of this experiment. Section 7
analyzes the method and proposes avenues for fur-
ther research. First, however, we outline the small
amount of previous work in natural language pro-
cessing on operative notes.
2 Previous Work
An early tool for processing operative notes was
proposed by Lamiell et al. (1993). They develop
an auditing tool to help enforce completeness in
operative notes. A syntactic parser converts sen-
tences in an operative note into a graph structure
that can be queried to ensure the necessary surgical
elements are present in the narrative. For appen-
dectomies, they could determine whether answers
were specified for questions such as ?What was
the appendix abnormality?? and ?Was cautery or
drains used??. Unlike what we propose, they did
not attempt to understand the narrative structure of
the operative note, only ensure that a small num-
ber of important elements were present. Unfortu-
nately, they only tested their rule-based system on
four notes, so it is difficult to evaluate the robust-
ness and generalizability of their method.
More recently, Wang et al. (2014) proposed a
machine learning (ML) method to extract patient-
specific values from operative notes written in
Chinese. They specifically extract tumor-related
information from patients with hepatic carcinoma,
such as the size/location of the tumor, and whether
the tumor boundary is clear. In many ways this is
similar in purpose to Lamiell et al. (1993) in the
sense that there are operation-specific attributes to
extract. However, while the auditing function pri-
marily requires knowing whether particular items
were stated, their method extracts the particular
values for these items. Furthermore, they em-
ploy an ML-based conditional random field (CRF)
trained and tested on 114 operative notes. The pri-
mary difference between the purpose of these two
methods and the purpose of our method lies in the
attempt to model all the events that characterize a
surgery. Both the work of Lamiell et al. (1993)
and Wang et al. (2014) can be used for complete-
ness testing, and Wang et al. (2014) can be used
to find similar patients. The lack of understand-
ing of the event structure, however, prevents these
methods from identifying similar surgical methods
or unexpected surgical techniques, or from accom-
plishing many other secondary use objectives.
In a more similar vein to our own approach,
Wang et al. (2012) studies actions (a subset of
event mentions) within an operative note. They
note that various lexico-syntactic constructions
can be used to specify an action (e.g., incised, the
incision was carried, made an incision). Like our
approach, they observed sentences can be catego-
rized into actions, perceptions/reports, and other
(though we make this distinction at the event men-
tion level). They adapted the Stanford Parser
(Klein and Manning, 2003) with the Specialist
Lexicon (Browne et al., 1993) similar to Huang
et al. (2005). They do not, however, propose any
automatic system for recognizing and categoriz-
ing actions. Instead, they concentrate on evalu-
ating existing resources. They find that many re-
sources, such as UMLS (Lindberg et al., 1993) and
FrameNet (Baker et al., 1998) have poor coverage
of surgical actions, while Specialist and WordNet
(Fellbaum, 1998) have good coverage.
A notable limitation of their work is that they
only studied actions at the sentence level, look-
ing at the main verb of the independent clause.
We have found in our study that multiple actions
can occur within a sentence, and we thus study ac-
tions at the event mention level. Wang et al. (2012)
noted this shortcoming and provide the following
illustrative examples:
? The patient was taken to the operating room
where general anesthesia was administered.
? After the successful induction of spinal anes-
thesia, she was placed supine on the operat-
ing table.
The second event mention in the first sentence
(administered) and the first event mention in the
second sentence (induction) are ignored in Wang
et al. (2012)?s study. Despite the fact that they
are stated in dependent clauses, these mentions
may be more semantically important to the narra-
tive than the mentions in the independent clauses.
This is because a grammatical relation does not
necessarily imply event prominence. In a further
study, Wang et al. (2013) work toward the creation
of an automatic extraction system by annotating
69
PropBank (Palmer et al., 2005) style predicate-
argument structures on thirty common surgical ac-
tions.
3 Event Structures in Operative Notes
Since operations are considered to be one of the
riskier forms of clinical treatment, surgeons fol-
low strict procedures that are highly structured and
require significant training and oversight. Thus,
a surgeon?s description of a particular operation
should be highly similar with a different descrip-
tion of the same type of operation, even if writ-
ten by a different surgeon at a different hospital.
For instance, the two examples below were writ-
ten by two different surgeons to describe the event
of controlling the blood supply to the appendix:
? The 35 mm vascular Endo stapler device was
fired across the mesoappendix...
? The meso appendix was divided with electro-
cautery...
In these two examples, the surgeons use different
lexical forms (fired vs. divided), syntactic forms
(mesoappendix to the right or left of the EVENT),
different semantic predicate-argument structures
(INSTRUMENT-EVENT-ANATOMICALOBJECT
vs. ANATOMICALOBJECT-EVENT-METHOD),
and even different surgical techniques (stapling or
cautery). Still, these examples describe the same
step in the operation and thus can be mapped to
the same location in the event structure.
In order to recognize the event structure in op-
erative notes, we start by specifying an event
structure to a particular operation (e.g., mastec-
tomy, appendectomy, heart transplant) and create
a ground-truth structure based on expert knowl-
edge. Our goal is then to normalize the event men-
tions within a operative note to the specific surgi-
cal actions in the event structure. While the lex-
ical, syntactic, and predicate-argument structures
vary greatly across the surgeons in our data, many
event descriptions are highly consistent within
notes written by the same surgeon. This is es-
pecially true of events with little linguistic vari-
ability, typically largely procedural but necessary
events that are not the focus of the surgeon?s de-
scription of the operation. An example of low-
variability is the event of placing the patient on
the operating table, as opposed to the event of ma-
nipulating the appendix to prepare it for removal.
Additionally, while there is considerable lexical
variation in how an event is mentioned, the ter-
minology for event mentions is fairly limited, re-
sulting in reasonable similarity between surgeons
(e.g., the verbal description used for the dividing
of the mesoappendix is typically one of the fol-
lowing mentions: fire, staple, divide, separate, re-
move).
3.1 Event Structure Representation
Operative notes contain event mentions of many
different event classes. Some classes correspond
to actions performed by the surgeon, while oth-
ers describe findings, provide reasonings, or dis-
cuss interactions with patients or assistants. These
distinctions are necessary to recognizing the event
structure of an operation, in which we are primar-
ily concerned with surgical actions. We consider
the following event types:
? ACTION: the primary types of events in an
operation. These typically involve physi-
cal actions taken by the surgeon (e.g., cre-
ating/closing an incision, dividing tissue), or
procedural events (e.g., anesthesia, transfer
to recovery). With limited exceptions, AC-
TIONs occur in a strict order and the i
th
AC-
TION can be interpreted as enabling the (i +
1)
th
ACTION.
? P ACTION: the peripheral actions that are
optional, do not occur within a specific place
in the chain of ACTIONs, and are not consid-
ered integral to the event structure. Examples
include stopping unexpected bleeding and re-
moving benign cysts un-connected with the
operation.
? OBSERVATION: an event that denotes the
act of observing a given state. OBSERVA-
TIONs may lead to ACTION (e.g., the ap-
pendix is perforated and therefore needs to
be removed) or P ACTIONs (e.g., a cyst is
found). They may also be elaborations to pro-
vide more details about the surgical method
being used.
? REPORT: an event that denotes a verbal in-
teraction between the surgeon and a patient,
guardian, or assistant (such as obtaining con-
sent for an operation).
The primary class of events that we are interested
in here are ACTIONs. Abstractly, one can view a
type of operation as a directed graph with specified
start and end states. The nodes denote the events,
while the edges denote enablements. An instance
of an operation then can be represented as some
70
Figure 1: Graphical representation of a surgical
procedure with ACTIONs A, B, C , D, E, and F ,
OBSERVATION O, and P ACTION G. (a) strict
surgical graph (only actions), (b) surgical graph
with an observation invoking an action, (c) surgi-
cal graph with an observation invoking a periph-
eral action.
path between the start and end nodes.
In its simplest form, a surgical graph is com-
posed entirely of ACTION nodes (see Figure 1(a)).
It is possible to add expected OBSERVATIONs
that might trigger a different ACTION path (Fig-
ure 1(b)). Finally, P ACTIONs can be represented
as optional nodes in the surgical graph, which may
or may not be triggered by OBSERVATIONs (Fig-
ure 1(c)). This graphical model is simply a con-
ceptual aid to help design the action types. The
model currently plays no role in the automatic
classification. For the remainder of this section
we focus on a relatively limited surgical proce-
dure that can be interpreted as a linear chain of
ACTIONs.
3.2 Appendectomy Representation
Acute appendicitis is a common condition requir-
ing surgical management, and is typically treated
by removing the appendix, either laparoscopically
or by using an open technique. Appendectomies
are the most commonly performed urgent surgi-
cal procedure in the United States. The procedure
is relatively straight-forward, and the steps of the
procedure exhibit little variation between differ-
ent surgeons. The third author (MS), a surgeon
with more than 20 years of experience in pedi-
atric surgery, provided the following primary AC-
TIONs:
? APP01: transfer patient to operating room
? APP02: place patient on table
? APP03: anesthesia
? APP04: prep
? APP05: drape
? APP06: umbilical incision
? APP07: insert camera/telescope
? APP08: insert other working ports
? APP09: identify appendix
? APP10: dissect appendix away from other
structures
? APP11: divide blood supply
? APP12: divide appendix from cecum
? APP13: place appendix in a bag
? APP14: remove bag from body
? APP15: close incisions
? APP16: wake up patient
? APP17: transfer patient to post-anesthesia
care unit
In the laparoscopic setting, each of these actions is
a necessary part of the operation, and most should
be recorded in the operative note. Additionally,
any number of P ACTION, OBSERVATION, and
REPORT events may be interspersed.
4 Data
In accordance with generally accepted medical
practice and to comply with requirements of The
Joint Commission, a detailed report of any surgical
procedure is placed in the medical record within
24 hours of the procedure. These notes include the
preoperative diagnosis, the post-operative diagno-
sis, the procedure name, names of surgeon(s) and
assistants, anesthetic method, operative findings,
complications (if any), estimated blood loss, and a
detailed report of the conduct of the procedure. To
ensure accuracy and completeness, such notes are
typically dictated and transcribed shortly after the
procedure by the operating surgeon or one of the
assistants.
To obtain the procedure notes for this study,
The Children?s Medical Center (CMC) of Dal-
las electronic medical record (EMR) was queried
for operative notes whose procedure contained the
word ?appendectomy? (CPT codes 44970, 44950,
44960) for a preoperative diagnosis of ?acute ap-
pendicitis? (ICD9 codes 541, 540.0, 540.1). At
the time of record acquisition, the CMC EMR had
been in operation for about 3 years, and 2,820
notes were obtained, having been completed by 12
pediatric surgeons. In this set, there were 2,757
71
Surgeon Notes Events Words
surgeon
1
8 291 2,305
surgeon
2
311 16,379 134,748
surgeon
3
143 6,897 57,797
surgeon
4
400 8,940 62,644
surgeon
5
391 15,246 114,684
surgeon
6
307 9,880 77,982
surgeon
7
397 10,908 74,458
surgeon
8
34 2,401 20,391
surgeon
9
2 100 973
surgeon
10
355 9,987 89,085
surgeon
11
380 14,211 135,215
surgeon
12
92 2,417 19,364
Total 2,820 97,657 789,646
Table 1: Overview of corpus by surgeon.
laparoscopic appendectomies and 63 open proce-
dures. The records were then processed automat-
ically to remove any identifying information such
as names, hospital record numbers, and dates. For
the purposes of this investigation, only the sur-
geon?s name and the detailed procedure note were
collected for further study. Owing to the complete
anonymity of the records, the study received an ex-
emption from the University of Texas Southwest-
ern Medical Center and CMC Institutional Review
Boards. Table 1 contains statistics about the distri-
bution of notes by surgeon in our dataset.
5 Active Learning Framework
Active learning is becoming a more and more
popular framework for natural language annota-
tion in the biomedical domain (Hahn et al., 2012;
Figueroa et al., 2012; Chen et al., 2013a; Chen et
al., 2013b). In an active learning setting, instead of
performing manual annotation separate from auto-
matic system development, an existing ML classi-
fier is employed to help choose which examples
to annotate. Thus, human annotators can focus on
examples that would prove difficult for a classifier,
which can dramatically reduce overall annotation
time. However, active learning is not without pit-
falls, notably sampling bias (Dasgupta and Hsu,
2008), re-usability (Tomanek et al., 2007), and
class imbalance (Tomanek and Hahn, 2009). In
our work, the purpose of utilizing an active learn-
ing framework is to produce a fully-annotated cor-
pus of labeled event mentions in as small a period
of time as possible. To some extent, the goal of
full-annotation alleviates some of the active learn-
ing issues discussed above (re-usability and class
imbalance), but sampling bias could still lead to
significantly longer annotation time.
Our goal is to (1) distinguish event mentions in
one of the four classes introduced in Section 3.1
(event type annotation), and (2) further classify ac-
tions into their appropriate location in the event
structure (on this data, appendectomy type anno-
tation). While most active learning methods are
used with the intention of only manually labeling
a sub-set of the data, our goal is to annotate every
event mention so that we may ultimately evaluate
unsupervised techniques on this data. Our active
learning experiment thus proceeds in two paral-
lel tracks: (i) a traditional active learning process
where the highest-utility unlabeled event mentions
are classified by a human annotator, and (ii) a
batch annotation process where extremely simi-
lar, ?easy? examples are annotated in large groups.
Due to small intra-surgeon language variation, and
relatively small inter-surgeon variation due to the
limited terminology, this second process allows us
to annotate large numbers of unlabeled examples
at a time. The batch labeling largely annotates un-
labeled examples that would not be selected by the
primary active learning module because they are
too similar to the already-labeled examples. After
a sufficient amount of time being spent in tradi-
tional active learning, the batch labeling is used
to annotate until the batches produced are insuf-
ficiently similar and/or wrong classifications are
made. After a sufficent number of annotations are
made with the active learning method, the choice
of when to use the active learning or batch anno-
tation method is left to the discretion of the anno-
tator. This back-and-forth is then repeated itera-
tively until all the examples are annotated.
For both the active learning and batch labeling
processes, we use a multi-class support vector ma-
chine (SVM) using a simple set of features:
F1. Event mention?s lexical form (e.g., identified)
F2. Event mention?s lemma (identify)
F3. Previous words (3-the, 2-appendix, 1-was)
F4. Next words (1-and, 2-found, 3-to, 4-be,
5-ruptured)
F5. Whether the event is a gerund (false)
Features F3 and F4 were constrained to only return
words within the sentence.
To sample event mentions for the active learner,
we combine several sampling techniques to ensure
a diversity of samples to label. This meta-sampler
chooses from 4 different samplers with differing
probability p:
1. UNIFORM: Choose (uniformly) an unlabeled
instance (p = 0.1). Formally, let L be the
72
set of manually labeled instances. Then, the
probability of selecting an event e
i
is:
P
U
(e
i
) ? ?(e
i
/? L)
Where ?(x) is the delta function that returns
1 if the condition x is true, and 0 otherwise.
Thus, an unlabeled event has an equal prob-
ability of being selected as every other unla-
beled event.
2. JACCARD: Choose an unlabeled instance bi-
ased toward those whose word context is least
similar to the labeled instances using Jac-
card similarity (p = 0.2). This sampler pro-
motes diversity to help prevent sampling bias.
Let W
i
be the words in e
i
?s sentence. Then
the probability of selecting an event with the
JACCARD sampler is:
P
J
(e
i
) ? ?(e
i
/? L) min
e
j
?L
[(
1?
W
i
?W
j
W
i
?W
j
)
?
]
Here, ? is a parameter to give more weight to
dissimilar sentences (we set ? = 2).
3. CLASSIFIER: Choose an unlabeled instance
biased toward those the SVM assigned low
confidence values (p = 0.65). Formally, let
f
c
(e
i
) be the confidence assigned by the clas-
sifier to event e
i
. Then, the probability of se-
lecting an event with the CLASSIFIER sam-
pler is:
P
C
(e
i
) ? ?(e
i
/? L)(1? f
c
(e
i
))
The SVM we use provides confidence values
largely in the range (-1, 1), but for some very
confident examples this value can be larger.
We therefore constrain the raw confidence
value f
r
(e
i
) and place it within the range [0,
1] to achieve the modified confidence f
c
(e
i
)
above:
f
c
(e
i
) =
max(min(f
r
(e
i
), 1),?1) + 1
2
In this way, f
c
(e
i
) can be guaranteed to be
within [0, 1] and can thus be interpreted as a
probability.
4. MISCLASSIFIED: Choose (uniformly) a la-
beled instance that the SVM mis-classifies
during cross-validation (p = 0.05). Let f(e
i
)
be the classifier?s guess and L(e
i
) be the
manual label for event e
i
. Then the proba-
bility of selecting an event is:
P
M
(e
i
) ? ?(e
i
? L)?(f(e
i
) 6= L(e
i
))
Event Type Precision Recall F
1
ACTION 0.79 0.90 0.84
NOT EVENT 0.75 0.82 0.79
OBSERVATION 0.71 0.57 0.63
P ACTION 0.66 0.40 0.50
REPORT 1.00 0.58 0.73
Active Learning Accuracy: 76.4%
Batch Annotation Accuracy: 99.5%
Table 2: Classification results for event types. Ex-
cept when specified, results are for data annotated
using the active learning method, while the batch
annotation results include all data.
The first annotation was made using the UNIFORM
sampler. For every new annotation, the meta-
sampler chooses one of the above sampling meth-
ods according to the above p values, and that sam-
pler selects an example to annotate. For each se-
lected sample, it is first assigned an event type. If it
is assigned as an ACTION, the annotator further as-
signs its appropriate action type. The CLASSIFIER
and MISCLASSIFIED samplers alternate between
the event type and action type classifiers. These
four samplers were chosen to balance the tra-
ditional active learning approach (CLASSIFIER),
while trying to prevent classifier bias (UNIFORM
and JACCARD), while also allowing mis-labeled
data to be corrected (MISCLASSIFIED). An eval-
uation of the utility of the individual samplers is
beyond the scope of this work.
6 Results
For event type annotation, two annotators single-
annotated 1,014 events with one of five event types
(ACTION, P ACTION, OBSERVATION, REPORT,
and NOT EVENT). The classifier?s accuracy on
this data was 75.9% (see Table 2 for a breakdown
by event type). However, the examples were cho-
sen because they were very different from the cur-
rent labeled set, and thus we would expect them to
be more difficult than a random sampling. When
one includes the examples annotated using batch
labeling, the overall accuracy is 99.5%.
For action type annotation, the same two anno-
tators labeled 626 ACTIONs with one of the 17 ac-
tion types (APP01?APP17). The classifier?s accu-
racy on this data was again a relatively low 72.2%
(see Table 3 for a breakdown by action type).
However, again, these examples were expected to
be difficult for the classifier. When one includes
the examples annotated using batch labeling, the
overall accuracy is 99.4%.
73
Action Type Precision Recall F
1
APP01 0.91 0.77 0.83
APP02 1.00 0.67 0.80
APP03 1.00 0.67 0.80
APP04 0.95 0.95 0.95
APP05 1.00 1.00 1.00
APP06 0.79 0.72 0.76
APP07 0.58 0.58 0.58
APP08 0.65 0.75 0.70
APP09 0.82 0.93 0.87
APP10 0.63 0.73 0.68
APP11 0.50 0.50 0.50
APP12 0.61 0.56 0.58
APP13 0.94 0.94 0.94
APP14 0.71 0.73 0.72
APP15 0.84 0.79 0.82
APP16 0.93 0.81 0.87
APP17 0.84 0.89 0.86
Active Learning Accuracy: 71.4%
Batch Annotation Accuracy: 99.4%
Table 3: Classification results for action types.
7 Discussion
The total time allotted for annotation was approxi-
mately 12 hours, split between two annotators (the
first author and a computer science graduate stu-
dent). Prior to annotation, both annotators were
given a detailed description of an appendectomy,
including a video of a procedure to help asso-
ciate the actual surgical actions with the narrative
description. After annotation, 1,042 event types
were annotated using the active learning method,
90,335 event types were annotated using the batch
method, and 6,279 remained un-annotated. Sim-
ilarly, 658 action types were annotated using the
active learning method, 35,799 action types were
annotated using the batch method, and 21,151 re-
mained un-annotated. A greater proportion of ac-
tions remained un-annotated due to the lower clas-
sifier confidence associated with the task. Event
and action types were annotated in unison, but we
estimate during the active learning process it took
about 25 seconds to annotate each event (both the
event type and the action type if classified as an
ACTION). The batch process enabled the annota-
tion of an average of 3 event mentions per second.
This rapid annotation was made possible by
the repetitive nature of operative notes, especially
within an individual surgeon?s notes. For exam-
ple, the following statements were repeated over
100 times in our corpus:
? General anesthesia was induced.
? A Foley catheter was placed under sterile
conditions.
? The appendix was identified and seemed to
be acutely inflamed.
The first example was used by an individual sur-
geon in 95% of his/her notes, and only used three
times by a different surgeon. In the second exam-
ple, the sentence is used in 77% of the surgeon?s
notes while only used once by another surgeon.
The phrase ?Foley catheter was placed?, however,
was used 133 times by other surgeons. In the con-
text of an appendectomy, this action is unambigu-
ous, and so only a few annotations are needed to
recognize the hundreds of actual occurrences in
the data. Similarly, with the third example, the
phrase ?the appendix was identified? was used in
over 600 operative notes by 10 of the 12 surgeons.
After a few manual annotations to achieve suffi-
cient classification confidence, the batch process
can identify duplicate or near-duplicate events that
can be annotated at once, greatly reducing the time
needed to achieve full annotation.
Unfortunately, the most predictable parts of a
surgeon?s language are typically the least inter-
esting from the perspective of understanding the
critical points in the narrative. As shown in the
examples above, the highest levels of redundancy
are found in the most routine aspects of the op-
eration. The batch annotation, therefore, is quite
biased and the 99% accuracies it achieves cannot
be expected to hold up once the data is fully an-
notated. Conversely, the active learning process
specifically chooses examples that are different
from the current labeled set and thus are more dif-
ficult to classify. Active learning is more likely to
sample from the ?long tail? than the most frequent
events and actions, so the performance on the cho-
sen sample is certainly a lower bound on the per-
formance of a completely annotated data set. If
one assumes the remaining un-annotated data will
be of similar difficulty to the data sampled by the
active learner, one could project an overall event
type accuracy of 97% and an overall action type
accuracy of 89%. This furthermore assumes no
improvements are made to the machine learning
method based on this completed data.
One way to estimate the potential bias in batch
annotation is by observing the differences in the
distributions of the two data sets. Figure 2 shows
the total numbers of action types for both the
active learning and batch annotation portions of
the data. For the most part, the distributions
are similar. APP08 (insert other working ports),
APP10 (dissect appendix away from other struc-
tures), APP11 (divide blood supply), APP12 (di-
74
Figure 2: Frequencies of action types in the active learning (AL) portion of the data set (left vertical axis)
and the batch annotation (BA) portion of the data set (right vertical axis).
vide appendix from cecum), and APP14 (remove
bag from body) are the most under-represented in
the batch annotation data. This confirms our hy-
pothesis that some of the most interesting events
have the greatest diversity in expression.
In Section 2 we noted that a limitation of the an-
notation method of Wang et al. (2012) was that a
sentence could only have one action. We largely
overcame this problem by associating a single sur-
gical action with an event mention. This has one
notable limitation, however, as occasionally a sin-
gle event mention corresponds to more than one
action. In our data, APP11 and APP12 are com-
monly expressed together:
? Next, the mesoappendix and appendix is
stapled
APP11/APP12
and then the appendix is
placed
APP13
in an endobag.
Here, a coordination (?mesoappendix and ap-
pendix?) is used to associate two events (the sta-
pling of the mesoappendix and the stapling of
the appendix) with the same event mention. In
the event extraction literature, this is a well-
understood occurrence, as for instance TimeML
(Pustejovsky et al., 2003) can represent more than
one event with a single event mention. In practice,
however, few automatic TimeML systems handle
such phenomena. Despite this, for our purpose the
annotation structure should likely be amended so
that we can account for all the important actions
in the operative note. This way, gaps in our event
structure will correspond to actual gaps in the nar-
rative (e.g., dividing the blood supply is a critical
step in an appendectomy and therefore needs to fit
within the event structure).
Finally, the data in our experiment comes from
a relatively simple procedure (an appendectomy).
It is unclear how well this method would general-
ize to more complex operations. Most likely, the
difficulty will lie in actions that are highly ambigu-
ous, such as if more than one incision is made.
In this case, richer semantic information will be
necessary, such as the spatial argument that indi-
cates where a particular event occurs (Roberts et
al., 2012).
8 Conclusion
With the increasing availability of electronic oper-
ative notes, there is a corresponding need for deep
analysis methods to understand the note?s narra-
tive structure to enable applications for improving
patient care. In this paper, we have presented a
method for recognizing how event mentions in an
operative note fit into the event structure of the ac-
tual operation. We have proposed a generic frame-
work for event structures in surgical notes with a
specific event structure for appendectomy opera-
tions. We have described a corpus of 2,820 opera-
tive notes of appendectomies performed by 12 sur-
geons at a single institution. With the ultimate goal
of fully annotating this data set, which contains al-
most 100,000 event mentions, we have shown how
an active learning method combined with a batch
annotation process can quickly annotate the ma-
jority of the corpus. The method is not without
its weaknesses, however, and further annotation is
likely necessary.
Beyond finishing the annotation process, our ul-
timate goal is to develop unsupervised methods
for structuring operative notes. This would en-
able expanding to new surgical procedures without
human intervention while also leveraging the in-
creasing availability of this information. We have
shown in this work how operative notes have lin-
guistic characteristics that result in parallel struc-
tures. It is our goal to leverage these characteris-
tics in developing unsupervised methods.
75
Acknowledgments
The authors would like to thank Sanya Peshwani
for her help in annotating the data.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of ACL/COLING.
Allen C. Browne, Alexa T. McCray, and Suresh Srini-
vasan. 1993. The SPECIALIST Lexicon. Tech-
nical Report NLM-LHC-93-01, National Library of
Medicine.
Yukun Chen, Hongxin Cao, Qiaozhu Mei, Kai Zheng,
and Hua Xu. 2013a. Applying active learning to su-
pervised word sense disambiguation in MEDLINE.
J Am Med Inform Assoc, 20:1001?1006.
Yukun Chen, Robert Carroll, Eugenia R. McPeek Hinz,
Anushi Shah, Anne E. Eyler, Joshua C. Denny, ,
and Hua Xu. 2013b. Applying active learning
to high-throughput phenotyping algorithms for elec-
tronic health records data. J Am Med Inform Assoc,
20:e253?e259.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
Sampling for Active Learning. In Proceedings of the
International Conference on Maching Learning.
J.K. DeOrio. 2002. Surgical templates for orthopedic
operative reports. Orthopedics, 25(6):639?642.
Laura Donahoe, Sean Bennett, Walley Temple, Andrea
Hilchie-Pye, Kelly Dabbs, EthelMacIntosh, and Ge-
off Porter. 2012. Completeness of dictated oper-
ative reports in breast cancer?the case for synoptic
reporting. J Surg Oncol, 106(1):79?83.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Rosa L. Figueroa, Qing Zeng-Treitler, Long H. Ngo,
Sergey Goryachev, and Eduardo P. Wiechmann.
2012. Active learning for clinical text classification:
is it better than random sampling? J Am Med Inform
Assoc, 19:809?816.
I. Gur, D. Gur, and J.A. Recabaren. 2011. The com-
puterized synoptic operative report: A novel tool in
surgical residency education. Arch Surg, pages 71?
74.
Udo Hahn, Elena Beisswanger, Ekaterina Buyko, and
Erik Faessler. 2012. Active Learning-Based Corpus
Annotation ? The PATHOJEN Experience. In Pro-
ceedings of the AMIA Symposium, pages 301?310.
Yang Huang, Henry J Lowe, Dan Klein, and Rus-
sell J Cucina. 2005. Improved Identification of
Noun Phrases in Clinical Radiology Reports Using
a High-Performance Statistical Natural Language
Parser Augmented with the UMLS Specialist Lex-
icon. J Am Med Inform Assoc, 12:275?285.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL,
pages 423?430.
James M Lamiell, Zbigniew M Wojcik, and John
Isaacks. 1993. Computer Auditing of Surgical Op-
erative Reports Written in English. In Proc Annu
Symp Comput Appl Med Care, pages 269?273.
Donald A.B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Jason Park, Venu G. Pillarisetty, Murray F. Brennan,
and et al. 2010. Electronic Synoptic Operative Re-
porting: Assessing the Reliability and Completeness
of Synoptic Reports for Pancreatic Resection. J Am
Coll Surgeons, 211(3):308?315.
James Pustejovsky, Jose? Castano, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir Radev. 2003. TimeML: Ro-
bust Specification of Event and Temporal Expres-
sions in Text. In Proceedings of the Fifth Interna-
tional Workshop on Computational Semantics.
Kirk Roberts, Bryan Rink, Sanda M. Harabagiu,
Richard H. Scheuermann, Seth Toomay, Travis
Browning, Teresa Bosler, and Ronald Peshock.
2012. A Machine Learning Approach for Identi-
fying Anatomical Locations of Actionable Findings
in Radiology Reports. In Proceedings of the AMIA
Symposium.
Katrin Tomanek and Udo Hahn. 2009. Reducing Class
Imbalance during Active Learning for Named Entity
Annotation. In Proceedings of KCAP.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An Approach to Text Corpus Construc-
tion which Cuts Annotation Costs and Maintains
Reusability of Annotated Data. In Proceedings of
EMNLP/CoNLL, pages 486?495.
Yan Wang, Serguei Pakhomov, Nora E. Burkart,
James O. Ryan, and Genevieve B. Melton. 2012.
A Study of Actions in Operative Notes. In Proceed-
ings of the AMIA Symposium, pages 1431?1440.
Yan Wang, Serguei Pakhomov, and Genevieve B
Melton. 2013. Predicate Argument Structure
Frames for Modeling Information in Operative
Notes. In Studies in Health Technology and Infor-
matics (MEDINFO), pages 783?787.
Hui Wang, Weide Zhang, Qiang Zeng, Zuofeng Li,
Kaiyan Feng, and Lei Liu. 2014. Extracting impor-
tant information from Chinese Operation Notes with
natural language processing methods. J Biomed In-
form.
76
