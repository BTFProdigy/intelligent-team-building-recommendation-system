Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 169?174,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Annotations for the Google Books Ngram Corpus
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman and Slav Petrov?
Google Inc.
{yurilin,jbmichel,drerez,orwant,brockman,slav}@google.com
Abstract
We present a new edition of the Google Books
Ngram Corpus, which describes how often
words and phrases were used over a period
of five centuries, in eight languages; it reflects
6% of all books ever published. This new edi-
tion introduces syntactic annotations: words
are tagged with their part-of-speech, and head-
modifier relationships are recorded. The an-
notations are produced automatically with sta-
tistical models that are specifically adapted to
historical text. The corpus will facilitate the
study of linguistic trends, especially those re-
lated to the evolution of syntax.
1 Introduction
The Google Books Ngram Corpus (Michel et al,
2011) has enabled the quantitative analysis of lin-
guistic and cultural trends as reflected in millions
of books written over the past five centuries. The
corpus consists of words and phrases (i.e., ngrams)
and their usage frequency over time. The data is
available for download, and can also be viewed
through the interactive Google Books Ngram Viewer
at http://books.google.com/ngrams.
The sheer quantity of and broad historical scope
of the data has enabled a wide range of analyses
(Michel et al, 2011; Ravallion, 2011). Of course,
examining raw ngram frequencies is of limited util-
ity when studying many aspects of linguistic change,
particularly the ones related to syntax. For instance,
most English verbs are regular (their past tense is
formed by adding -ed), and the few exceptions,
known as irregular verbs, tend to regularize over the
? Corresponding author.
1800 1850 1900 1950 2000
Rel
ativ
e F
req
uen
cy
burntburnt_VERBburnt_ADJburnedburned_VERBburned_ADJ
Figure 1: Usage frequencies of burned and burnt over
time, showing that burned became the dominant spelling
around 1880. Our new syntactic annotations enable a
more refined analysis, suggesting that the crossing-point
for the verb usage (burned VERB vs. burnt VERB) was
decades earlier.
centuries (Lieberman et al, 2007). Figure 1 illus-
trates how burned gradually overtook burnt, becom-
ing more frequent around 1880. Unfortunately, as a
study of verb regularization, this analysis is skewed
by a significant confound: both words can serve
as either verbs (e.g., the house burnt) or adjectives
(e.g., the burnt toast). Because many words have
multiple syntactic interpretations, such confounds
often limit the utility of raw ngram frequency data.
In this work we provide a new edition of the
Google Books Ngram Corpus that contains over 8
million books, or 6% of all books ever published (cf.
Section 3). Moreover, we include syntactic anal-
ysis in order to facilitate a fine-grained analysis of
the evolution of syntax. Ngrams are annotated with
part-of-speech tags (e.g., in the phrase he burnt the
toast, burnt is a verb; in the burnt toast, burnt is an
adjective) and head-modifier dependencies (e.g., in
the phrase the little black book, little modifies book).
The annotated ngrams are far more useful for ex-
169
amining the evolution of grammar and syntax. For
our study of the regularization of the verb burn,
the availability of syntactic annotations resolves the
verb vs. adjective ambiguity in the original data, al-
lowing us to only examine instances where burnt
and burned appear as verbs. This more refined anal-
ysis suggests a crossover date for the frequency of
the verb forms that is several decades earlier than
the overall (verbs and adjectives) crossover.
We use state-of-the-art statistical part-of-speech
taggers and dependency parsers to produce syntac-
tic annotations for eight languages in the Google
Books collection. The annotations consist of 12 lan-
guage universal part-of-speech tags and unlabeled
head-modifier dependencies. Section 4 describes the
models that we used and the format of the annota-
tions in detail. We assess the expected annotation
accuracies experimentally and discuss how we adapt
the taggers and parsers to historical text in Section 5.
The annotated ngrams are available as a new edition
of the Google Books Ngram Corpus; we provide
some examples from the new corpus in Figure 3.
2 Related Work
Michel et al (2011) described the construction of
the first edition of the Google Books Ngram Corpus
and used it to quantitatively analyze a variety of top-
ics ranging from language growth to public health.
The related Ngram Viewer has become a popular
tool for examining language trends by experts and
non-experts alike.
In addition to studying frequency patterns in the
data, researchers have also attempted to analyze the
grammatical function of the ngrams (Davies, 2011).
Such endeavors are hampered by the fact that the
Ngram Corpus provides only aggregate statistics in
the form of ngram counts and not the full sen-
tences. Furthermore, only ngrams that pass certain
occurrence thresholds are publicly available, making
any further aggregation attempt futile: in heavy tail
distributions like the ones common in natural lan-
guages, the counts of rare events (that do not pass
the frequency threshold) can have a large cumula-
tive mass.
In contrast, because we have access to the full
text, we can annotate ngrams to reflect the particu-
lar grammatical functions they take in the sentences
Language #Volumes #Tokens
English 4,541,627 468,491,999,592
Spanish 854,649 83,967,471,303
French 792,118 102,174,681,393
German 657,991 64,784,628,286
Russian 591,310 67,137,666,353
Italian 305,763 40,288,810,817
Chinese 302,652 26,859,461,025
Hebrew 70,636 8,172,543,728
Table 1: Number of volumes and tokens for each lan-
guage in our corpus. The total collection contains more
than 6% of all books ever published.
they were extracted from, and can also account for
the contribution of rare ngrams to otherwise frequent
grammatical functions.
3 Ngram Corpus
The Google Books Ngram Corpus has been avail-
able at http://books.google.com/ngrams
since 2010. This work presents new corpora that
have been extracted from an even larger book collec-
tion, adds a new language (Italian), and introduces
syntactically annotated ngrams. The new corpora
are available in addition to the already existing ones.
3.1 Books Data
The new edition of the Ngram Corpus supports the
eight languages shown in Table 1. The book vol-
umes were selected from the larger collection of all
books digitized at Google following exactly the pro-
cedure described in Michel et al (2011). The new
edition contains data from 8,116,746 books, or over
6% of all books ever published. The English cor-
pus alone comprises close to half a trillion words.
This collection of books is much larger than any
other digitized collection; its generation required a
substantial effort involving obtaining and manually
scanning millions of books.
3.2 Raw Ngrams
We extract ngrams in a similar way to the first edi-
tion of the corpus (Michel et al, 2011), but with
some notable differences. Previously, tokenization
was done on whitespace characters and all ngrams
occurring on a given page were extracted, includ-
ing ones that span sentence boundaries, but omitting
170
Tag English Spanish French German Russian1 Italian Chinese Hebrew
ADJ other, such mayor, gran tous, me?me anderen, ersten vse,to