Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 385?396,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Evaluating Dependency Parsing:
Robust and Heuristics-Free Cross-Annotation Evaluation
Reut Tsarfaty
Uppsala University
Sweden
Joakim Nivre
Uppsala University
Sweden
Evelina Andersson
Uppsala University
Sweden
Abstract
Methods for evaluating dependency parsing
using attachment scores are highly sensitive
to representational variation between depen-
dency treebanks, making cross-experimental
evaluation opaque. This paper develops a ro-
bust procedure for cross-experimental eval-
uation, based on deterministic unification-
based operations for harmonizing different
representations and a refined notion of tree
edit distance for evaluating parse hypothe-
ses relative to multiple gold standards. We
demonstrate that, for different conversions of
the Penn Treebank into dependencies, perfor-
mance trends that are observed for parsing
results in isolation change or dissolve com-
pletely when parse hypotheses are normalized
and brought into the same common ground.
1 Introduction
Data-driven dependency parsing has seen a consid-
erable surge of interest in recent years. Dependency
parsers have been tested on parsing sentences in En-
glish (Yamada and Matsumoto, 2003; Nivre and
Scholz, 2004; McDonald et al, 2005) as well as
many other languages (Nivre et al, 2007a). The
evaluation metric traditionally associated with de-
pendency parsing is based on scoring labeled or
unlabeled attachment decisions, whereby each cor-
rectly identified pair of head-dependent words is
counted towards the success of the parser (Buchholz
and Marsi, 2006). As it turns out, however, such
evaluation procedures are sensitive to the annotation
choices in the data on which the parser was trained.
Different annotation schemes often make differ-
ent assumptions with respect to how linguistic con-
tent is represented in a treebank (Rambow, 2010).
The consequence of such annotation discrepancies is
that when we compare parsing results across differ-
ent experiments, even ones that use the same parser
and the same set of sentences, the gap between re-
sults in different experiments may not reflect a true
gap in performance, but rather a difference in the an-
notation decisions made in the respective treebanks.
Different methods have been proposed for making
dependency parsing results comparable across ex-
periments. These methods include picking a single
gold standard for all experiments to which the parser
output should be converted (Carroll et al, 1998; Cer
et al, 2010), evaluating parsers by comparing their
performance in an embedding task (Miyao et al,
2008; Buyko and Hahn, 2010), or neutralizing the
arc direction in the native representation of depen-
dency trees (Schwartz et al, 2011).
Each of these methods has its own drawbacks.
Picking a single gold standard skews the results in
favor of parsers which were trained on it. Trans-
forming dependency trees to a set of pre-defined la-
beled dependencies, or into task-based features, re-
quires the use of heuristic rules that run the risk of
distorting correct information and introducing noise
of their own. Neutralizing the direction of arcs is
limited to unlabeled evaluation and local context,
and thus may not cover all possible discrepancies.
This paper proposes a new three-step protocol for
cross-experiment parser evaluation, and in particu-
lar for comparing parsing results across data sets
that adhere to different annotation schemes. In the
385
first step all structures are brought into a single for-
mal space of events that neutralizes representation
peculiarities (for instance, arc directionality). The
second step formally computes, for each sentence
in the data, the common denominator of the differ-
ent gold standards, containing all and only linguistic
content that is shared between the different schemes.
The last step computes the normalized distance from
this common denominator to parse hypotheses, mi-
nus the cost of distances that reflect mere annotation
idiosyncrasies. The procedure that implements this
protocol is fully deterministic and heuristics-free.
We use the proposed procedure to compare de-
pendency parsing results trained on Penn Treebank
trees converted into dependency trees according to
five different sets of linguistic assumptions. We
show that when starting off with the same set of
sentences and the same parser, training on differ-
ent conversion schemes yields apparently significant
performance gaps. When results across schemes are
normalized and compared against the shared linguis-
tic content, these performance gaps decrease or dis-
solve completely. This effect is robust across parsing
algorithms. We conclude that it is imperative that
cross-experiment parse evaluation be a well thought-
through endeavor, and suggest ways to extend the
protocol to additional evaluation scenarios.
2 The Challenge: Treebank Theories
Dependency treebanks contain information about
the grammatically meaningful elements in the utter-
ance and the grammatical relations between them.
Even if the formal representation in a dependency
treebank is well-defined according to current stan-
dards (Ku?bler et al, 2009), there are different ways
in which the trees can be used to express syntactic
content (Rambow, 2010). Consider, for instance, al-
gorithms for converting the phrase-structure trees in
the Penn Treebank (Marcus et al, 1993) into depen-
dency structures. Different conversion algorithms
implicitly make different assumptions about how to
represent linguistic content in the data. When mul-
tiple conversion algorithms are applied to the same
data, we end up with different dependency trees for
the same sentences (Johansson and Nugues, 2007;
Choi and Palmer, 2010; de Marneffe et al, 2006).
Some common cases of discrepancies are as follows.
Lexical vs. Functional Head Choice. In linguis-
tics, there is a distinction between lexical heads and
functional heads. A lexical head carries the seman-
tic gist of a phrase while a functional one marks its
relation to other parts of the sentence. The two kinds
of heads may or may not coincide in a single word
form (Zwicky, 1993). Common examples refer to
prepositional phrases, such as the phrase ?on Sun-
day?. This phrase has two possible analyses, one se-
lects a lexical head (1a) and the other selects a func-
tional one (1b), as depicted below.
(1a) Sunday
on
(1b) on
Sunday
Similar choices are found in phrases which contain
functional elements such as determiners, coordina-
tion markers, subordinating elements, and so on.
Multi-Headed Constructions. Some phrases are
considered to have multiple lexical heads, for in-
stance, coordinated structures. Since dependency-
based formalisms require us to represent all con-
tent as binary relations, there are different ways we
could represent such constructions. Let us consider
the coordination of nominals below. We can choose
between a functional head (1a) and a lexical head
(2b, 2c). We can further choose between a flat rep-
resentation in which the first conjunct is a single
head (2b), or a nested structure where each con-
junct/marker is the head of the following element
(2c). All three alternatives empirically exist. Exam-
ple (2a) reflects the structures in the CoNLL 2007
shared task data (Nivre et al, 2007a). Johansson
and Nugues (2007) use structures like (2b). Exam-
ple (2c) reflects the analysis of Mel?c?uk (1988).
(2a) and
earth wind fire
(2b) earth
wind and fire
(2c) earth
wind
and
fire
Periphrastic Marking. When a phrase includes
periphrastic marking ? such as the tense and modal
marking in the phrase ?would have worked? below
? there are different ways to consider its division
into phrases. One way to analyze this phrase would
be to choose auxiliaries as heads, as in (3a). Another
alternative would be to choose the final verb as the
prep pobj
con
j con
j
conj cc
coord
conj
coordconj
conj
386
Experiment Gold Parse
#1 arrive
on
Sunday
arrive
on
Sunday
#2 arrive
Sunday
on
arrive
Sunday
on
Gold: #1 # 2
Parse
#1 1.0 0.0
#2 0.0 1.0
Figure 1: Calculating cross-experiment LAS results
main head, and let the auxiliaries create a verb chain
with different levels of projection. Each annotation
decision dictates a different direction of the arcs and
imposes its own internal division into phrases.
(3a) would
have
worked
(3b) worked
have
would
In standard settings, an experiment that uses
a data set which adheres to a certain annotation
scheme reports results that are compared against the
annotation standard that the parser was trained on.
But if parsers were trained on different annotation
standards, the empirical results are not comparable
across experiments. Consider, for instance, the ex-
ample in Figure 1. If parse1 and parse2 are com-
pared against gold2 using labeled attachment scores
(LAS), then parse1 results are lower than the results
of parse2, even though both parsers produced lin-
guistically correct and perfectly useful output.
Existing methods for making parsing results com-
parable across experiments include heuristics for
converting outputs into dependency trees of a prede-
fined standard (Briscoe et al, 2002; Cer et al, 2010)
or evaluating the performance of a parser within an
embedding task (Miyao et al, 2008; Buyko and
Hahn, 2010). However, heuristic rules for cross-
annotation conversion are typically hand written and
error prone, and may not cover all possible discrep-
ancies. Task-based evaluation may be sensitive to
the particular implementation of the embedding task
and the procedures that extract specific task-related
features from the different parses. Beyond that,
conversion heuristics and task-based procedures are
currently developed almost exclusively for English.
Other languages typically lack such resources.
A recent study by Schwartz et al (2011) takes
a different approach towards cross-annotation eval-
uation. They consider different directions of
head-dependent relations (such as on?Sunday
and Sunday?on) and different parent-child and
grandparent-child relations in a chain (such as
arrive?on and arrive?sunday in ?arrive on sun-
day?) as equivalent. They then score arcs that fall
within corresponding equivalence sets. Using these
new scores Schwartz et al (2011) neutralize certain
annotation discrepancies that distort parse compar-
ison. However, their treatment is limited to local
context and does not treat structures larger than two
sequential arcs. Additionally, since arcs in differ-
ent directions are typically labeled differently, this
method only applies for unlabeled dependencies.
What we need is a fully deterministic and for-
mally precise procedure for comparing any set of la-
beled or unlabeled dependency trees, by consolidat-
ing the shared linguistic content of the complete de-
pendency trees in different annotation schemes, and
comparing parse hypotheses through sound metrics
that can take into account multiple gold standards.
3 The Proposal: Cross-Annotation
Evaluation in Three Simple Steps
We propose a new protocol for cross-experiment
parse evaluation, consisting of three fundamental
components: (i) abstracting away from annotation
peculiarities, (ii) generalizing theory-specific struc-
tures into a single linguistically coherent gold stan-
dard that contains all and only consistent informa-
tion from all sources, and (iii) defining a sound met-
ric that takes into account the different gold stan-
dards that are being considered in the experiments.
In this section we first define functional trees as
the common space of formal objects and define a de-
terministic conversion procedure from dependency
trees to functional trees. Next we define a set of for-
mal operations on functional trees that compute, for
every pair of corresponding trees of the same yield, a
single gold tree that resolves inconsistencies among
gold standard alternatives and combines the infor-
mation that they share. Finally, we define scores
based on tree edit distance, refined to consider the
distance from parses to the overall gold tree as well
as the different annotation alternatives.
vg vg
vgvg
tmod
pobj pobj
prepprep
tmod
tmod
tmod
387
Preliminaries. Let T be a finite set of terminal
symbols and let L be a set of grammatical relation
labels. A dependency graph d is a directed graph
which consists of nodes Vd and arcs Ad ? Vd ? Vd.
We assume that all nodes in Vd are labeled by ter-
minal symbols via a function labelV : Vd ? T . A
well-formed dependency graph d = (Vd, Ad) for a
sentence S = t1, t2, ..., tn is any dependency graph
that is a directed tree originating out of a node v0
labeled t0 = ROOT , and spans all terminals in
the sentence, that is, for every ti ? S there exists
vj ? Vd labeled labelV (vj) = ti. For simplicity we
assume that every node vj is indexed according to
the position of the terminal label, i.e., that for each
ti labeling vj , i always equals j. In a labeled de-
pendency tree, arcs in Ad are labeled by elements
of L via a function labelA : Ad ? L that encodes
the grammatical relation between the terminals la-
beling the connected nodes. We define two auxiliary
functions on nodes in dependency trees. The func-
tion subtree : Vd ? P(Vd) assigns to every node
v ? Vd the set of nodes accessible by it through
the reflexive transitive closure of the arc relation Ad.
The function span : Vd ? P(T ) assigns to every
node v ? Vd a set of terminals such that span(v) =
{t ? T |t = labelV (u) and u ? subtree(v)}.1
Step 1: Functional Representation Our first goal
is to define a representation format that keeps all
functional relationships that are represented in the
dependency trees intact, but remains neutral with
respect to the directionality of the head-dependent
relations. To do so we define functional trees
? linearly-ordered labeled trees which, instead of
head-to-head binary relations, represent the com-
plete functional structure of a sentence. Assuming
the same sets of terminal symbols T and grammat-
ical relation labels L, and assuming extended sets
of nodes V and arcs A ? V ? V , a functional tree
pi = (V,A) is a directed tree originating from a sin-
gle root v0 ? V where all non-terminal nodes in
pi are labeled with grammatical relation labels that
signify the grammatical function of the chunk they
dominate inside the tree via labelNT : V ? L. All
1If a dependency tree d is projective, than for all v ? Vd the
terminals in span(v) form a contiguous segment of S. The cur-
rent discussion assumes that all trees are projective. We com-
ment on non-projective dependencies in Section 4.
terminal nodes in pi are labeled with terminal sym-
bols via a labelT : V ? T function. The function
span : V ? P(V ) now picks out the set of ter-
minal labels of the terminal nodes accessible by a
node v ? V via A. We obtain functional trees from
dependency trees using the following procedure:
? Initialize the set of nodes and arcs in the tree.
V := Vd
A := Ad
? Label each node v ? V with the label of its
incoming arc.
labelNT (v) = labelA(u, v)
? In case |span(v)| > 1 add a new node u as a
daughter designating the lexical head, labeled
with the wildcard symbol *:
V := V ? {u}
A := A ? {(v, u)}
labelNT (u) = ?
? For each node v such that |span(v)| = 1, add a
new node u as a daughter, labeled with its own
terminal:
V := V ? {u}
A := A ? {(v, u)}
if (labelNT (v) = ?)
labelT (u) := labelV (v)
else
labelT (u) := labelV (parent(v))
That is to say, we label all nodes with spans
greater than 1 with the grammatical function of their
head, and for each node we add a new daughter u
designating the head word, labeled with its gram-
matical function. Wildcard labels are compatible
with any, more specific, grammatical function of the
word inside the phrase. This gives us a constituency-
like representation of dependency trees labeled with
functional information, which retains the linguis-
tic assumptions reflected in the dependency trees.
When applying this procedure, examples (1)?(3) get
transformed into (4)?(6) respectively.
(4a) ...
prep
on
*
Sunday
(4b) ...
*
on
pobj
Sunday
388
(5a) ...
conj
earth
conj
wind
*
and
conj
fire
(5b) ...
*
earth
conj
wind
cc
and
conj
fire
(5c) ...
*
earth
coord
*
wind
coord
*
and
conj
fire
(6a) ...
*
would
vg
*
have
vg
worked
(6b) ...
vg
vg
would
*
have
*
worked
Considering the functional trees resulting from
our procedure, it is easy to see that for tree pairs
(4a)?(4b) and (5a)?(5b) the respective functional
trees are identical modulo wildcards, while tree pairs
(5b)?(5c) and (6a)?(6b) end up with different tree
structures that realize different assumptions con-
cerning the internal structure of the tree. In order
to compare, combine or detect inconsistencies in the
information inherent in different functional trees, we
define a set of formal operations that are inspired by
familiar notions from unification-based formalisms
(Shieber (1986) and references therein).
Step 2: Formal Operations on Trees The intu-
ition behind the formal operations we define is sim-
ple. A completely flat tree over a span is the most
general structural description that can be given to it.
The more nodes dominate a span, the more linguis-
tic assumptions are made with respect to its struc-
ture. If an arc structure in one tree merely elaborates
an existing flat span in another tree, the theories un-
derlying the schemes are compatible, and their in-
formation can be combined. Otherwise, there exists
a conflict in the linguistic assumptions, and we need
to relax some of the assumptions, i.e., remove func-
tional nodes, in order to obtain a coherent structure
that contains the information on which they agree.
Let pi1, pi2 be functional trees over the same yield
t1, .., tn. Let the function span(v) pick out the ter-
minals labeling terminal nodes that are accessible
via a node v ? V in the functional tree through the
relation A. We define first the tree subsumption re-
lation for comparing the amount of information in-
herent in the arc-structure of two trees.2
T-Subsumption, denoted t, is a relation be-
tween trees which indicates that a tree pi1 is
consistent with and more general than tree
pi2. Formally: pi1 t pi2 iff for every node
n ? pi1 there exists a node m ? pi2 such
that span(n) = span(m) and label(n) =
label(m).
Looking at the functional trees of (4a)?(4b) we
see that their unlabeled skeletons mutually subsume
each other. In their labeled versions, however, each
tree contains labeling information that is lacking in
the other. In the functional trees (5b)?(5c) a flat
structure over a span in (5b) is more elaborated in
(5c). In order to combine information in trees with
compatible arc structures, we define tree unification.
T-Unification, denoted unionsqt, is the operation that
returns the most general tree structure pi3 that
is subsumed by both pi1, pi2 if such exists, and
fails otherwise. Formally: pi1 unionsqt pi2 = pi3 iff
pi1 t pi3 and pi2 t pi3, and for all pi4 such that
pi1 t pi4 and pi2 t pi4 it holds that pi3 t pi4.
Tree unification collects the information from two
trees into a single result if they are consistent, and
detects an inconsistency otherwise. In case of an
inconsistency, as is the case in the functional trees
(6a) and (6b), we cannot unify the structures due
to a conflict concerning the internal division of an
expression into phrases. However, we still want to
generalize these two trees into one tree that contains
all and only the information that they share. For that
we define the tree generalization operation.
T-Generalization, denoted t, is the operation
that returns the most specific tree that is more
general than both trees. Formally, pi1 t pi2 =
pi3 iff pi3 t pi1 and pi3 t pi2, and for every pi4
such that pi4 t pi1 and pi4 t pi2 it holds that
pi4 t pi3.
2Note that the wildcard symbol * is equal to any other sym-
bol. In case the node labels consist of complex feature structures
made of attribute-value lists, we replace label(n) = label(m)
in the subsumption definition with label(n)  label(m) in the
sense of (Shieber, 1986).
389
Unlike unification, generalization can never fail.
For every pair of trees there exists a tree that is more
general than both: in the extreme case, pick the com-
pletely flat structure over the yield, which is more
general than any other structure. For (6a)?(6b), for
instance, we get that (6a)t(6b) is a flat tree over
pre-terminals where ?would? and ?have? are labeled
with ?vg? and ?worked? is the head, labeled with ?*?.
The generalization of two functional trees pro-
vides us with one structure that reflects the common
and consistent content of the two trees. These struc-
tures thus provide us with a formally well-defined
gold standard for cross-treebank evaluation.
Step 3: Measuring Distances. Our functional
trees superficially look like constituency-based
trees, so a simple proposal would be to use Parse-
val measures (Black et al, 1991) for comparing the
parsed trees against the new generalized gold trees.
Parseval scores, however, have two significant draw-
backs. First, they are known to be too restrictive
with respect to some errors and too permissive with
respect to others (Carroll et al, 1998; Ku?bler and
Telljohann, 2002; Roark, 2002; Rehbein and van
Genabith, 2007). Secondly, F1 scores would still
penalize structures that are correct with respect to
the original gold, but are not there in the generalized
structure. Here we propose to adopt measures that
are based on tree edit distance (TED) instead. TED-
based measures are, in fact, an extension of attach-
ment scores for dependency trees. Consider, for in-
stance, the following operations on dependency arcs.
reattach-arc remove arc (u, v) ? Ad and add
an arc Ad ? {(w, v)}.
relabel-arc relabel arc l1(u, v) as l2(u, v)
Assuming that each operation is assigned a cost,
the attachment score of comparing two dependency
trees is simply the cost of all edit operations that are
required to turn a parse tree into its gold standard,
normalized with respect to the overall size of the de-
pendency tree and subtracted from a unity.3 Here
we apply the idea of defining scores by TED costs
normalized relative to the size of the tree and sub-
stracted from a unity, and extend it from fixed-size
dependency trees to ordered trees of arbitrary size.
3The size of a dependency tree, either parse or gold, is al-
ways fixed by the number of terminals.
Our formalization follows closely the formulation
of the T-Dice measure of Emms (2008), building on
his thorough investigation of the formal and empir-
ical differences between TED-based measures and
Parseval. We first define for any ordered and labeled
tree pi the following operations.
relabel-node change the label of node v in pi
delete-node delete a non-root node v in pi with
parent u, making the children of v the children
of u, inserted in the place of v as a subsequence
in the left-to-right order of the children of u.
insert-node insert a node v as a child of u in
pi making it the parent of a consecutive subse-
quence of the children of u.
An edit script ES(pi1, pi2) = {e0, e1....ek} between
pi1 and pi2 is a set of edit operations required for turn-
ing pi1 into pi2. Now, assume that we are given a cost
function defined for each edit operation. The cost of
ES(pi1, pi2) is the sum of the costs of the operations
in the script. An optimal edit script is an edit script
between pi1 and pi2 of minimum cost.
ES?(pi1, pi2) = argminES(pi1,pi2)
?
e?ES(pi1,pi2)
cost(e)
The tree edit distance problem is defined to be the
problem of finding the optimal edit script and com-
puting the corresponding distance (Bille, 2005).
A simple way to calculate the error ? of a parse
would be to define it as the edit distance between
the parse hypothesis pi1 and the gold standard pi2.
?(pi1, pi2) = cost(ES?(pi1, pi2))
However, in such cases the parser may still get pe-
nalized for recovering nodes that are lacking in the
generalization. To solve this, we refine the distance
between a parse tree and the generalized gold tree
to discard edit operations on nodes that are there in
the native gold tree but are eliminated through gen-
eralization. We compute the intersection of the edit
script turning the parse tree into the generalize gold
with the edit script turning the native gold tree into
the generalized gold, and discard its cost. That is, if
parse1 and parse2 are compared against gold1 and
gold2 respectively, and if we set gold3 to be the re-
sult of gold1tgold2, then ?new is defined as:
390
Figure 2: The evaluation pipeline. Different versions of the treebank go into different experiments, resulting in
different parse and gold files. All trees are transformed into functional trees. All gold files enter generalization to
yield a new gold. The different ? arcs represent the different tree distances used for calculating the TED-based scores.
?new(parse1, gold1,gold3) =
?(parse1,gold3)
?cost(ES?(parse1,gold3)?ES?(gold1,gold3))
Now, if gold1 and gold3 are identi-
cal, then ES?(gold1,gold3)=? and we fall
back on the simple tree edit distance score
?new(parse1,gold1,gold3)=?(parse1, gold3).
When parse1 and gold1 are identical,
i.e., the parser produced perfect out-
put with respect to its own scheme, then
?new(parse1,gold1,gold3)=?new(gold1,gold1,gold3)
=?(gold1,gold3)? cost(ES?(gold1,gold3))=0, and
the parser does not get penalized for recovering a
correct structure in gold1 that is lacking in gold3.
In order to turn distances into accuracy measures
we have to normalize distances relative to the maxi-
mal number of operations that is conceivable. In the
worst case, we would have to remove all the internal
nodes in the parse tree and add all the internal nodes
of the generalized gold, so our normalization factor
? is defined as follows, where |pi| is the size4 of pi.
?(parse1,gold3) = |parse1| + |gold3|
We now define the score of parse1 as follows:5
1? ?new(parse1,gold1,gold3)?(parse1,gold3)
Figure 2 summarizes the steps in the evalu-
ation procedure we defined so far. We start
off with two versions of the treebank, TB1 and
TB2, which are parsed separately and provide their
own gold standards and parse hypotheses in a la-
beled dependencies format. All dependency trees
4Following common practice, we equate size |pi| with the
number of nodes in pi, discarding the terminals and root node.
5If the trees have only root and leaves, ? = 0, score := 1.
are then converted into functional trees, and we
compute the generalization of each pair of gold
trees for each sentence in the data. This pro-
vides the generalized gold standard for all exper-
iments, here marked as gold3.6 We finally com-
pute the distances ?new(parse1,gold1,gold3) and
?new(parse2,gold2,gold3) using the different tree
edit distances that are now available, and we repeat
the procedure for each sentence in the test set.
To normalize the scores for an entire test set of
size n we can take the arithmetic mean of the scores.
?|test-set|
i=1 score(parse1i,gold1i,gold3i)
|test-set|
Alternatively we can globally average of all edit dis-
tance costs, normalized by the maximally possible
edits on parse trees turned into generalized trees.
1?
?|test-set|
i=1 ?new(parse1i,gold1i,gold3i)?|test-set|
i=1 ?(parse1i,gold3i)
The latter score, global averaging over the entire test
set, is the metric we use in our evaluation procedure.
4 Experiments
We demonstrate the application of our procedure to
comparing dependency parsing results on different
versions of the Penn Treebank (Marcus et al, 1993).
The Data We use data from the PTB, converted
into dependency structures using the LTH soft-
ware, a general purpose tool for constituency-to-
dependency conversion (Johansson and Nugues,
2007). We use LTH to implement the five different
annotation standards detailed in Table 3.
6Generalization is an associative and commutative opera-
tion, so it can be extended for n experiments in any order.
TB1 parse1.dep
gold1.dep
parse2.dep
gold2.dep
parse1
gold3
gold1
parse2
gold2TB2
parse
parse
? (parse1,gold3)
? (gold1,gold3)
? (pars
e2,gold
3)
? (gold2,gold
3)
parse transform generalizeparse
391
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9142 0.6077 0.7772
LAS 0.8820 0.4801 0.6454
U-TED 0.9488 0.8926 0.9237
L-TED 0.9241 0.7811 0.8441
Old LTH UAS 0.6053 0.8955 0.6508
LAS 0.4816 0.8644 0.5771
U-TED 0.8931 0.9564 0.9092
L-TED 0.7811 0.9317 0.8197
CoNLL07 UAS 0.7734 0.6474 0.8917
LAS 0.6479 0.5722 0.8736
U-TED 0.9260 0.9097 0.9474
L-TED 0.8480 0.8204 0.9233
Default-OldLTH U-TED 0.9500 0.9543
L-TED 0.9278 0.9324
Default-CoNLL07 U-TED 0.9444? 0.9453?
L-TED 0.9266? 0.9260?
oldLTH-CoNLL07 U-TED 0.9519 0.9490
L-TED 0.9323 0.9283
default-oldLTH-CoNLL U-TED 0.9464? 0.9515 0.9471?
L-TED 0.9281? 0.9336 0.9280?
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8917 0.8054 0.6986
LAS 0.8736 0.7895 0.6831
U-TED 0.9474 0.9357 0.9237
L-TED 0.9233 0.8960 0.8606
Functional UAS 0.8040 0.8970 0.6110
LAS 0.7873 0.8793 0.5977
U-TED 0.9347 0.9466 0.9107
L-TED 0.8948 0.9239 0.8316
Lexical UAS 0.7013 0.6138 0.8823
LAS 0.6875 0.6022 0.8635
U-TED 0.9252 0.9132 0.9500
L-TED 0.8623 0.8345 0.9266
CoNLL07-Functional U-TED 0.9473? 0.9473?
L-TED 0.9233 0.9247
CoNLL07-Lexical U-TED 0.9490? 0.9500?
L-TED 0.9253? 0.9266?
Functional-Lexical U-TED 0.9489? 0.9501?
L-TED 0.9266? 0.9267?
CoNLL07-Functional-Lexical U-TED 0.9489? 0.9489? 0.9501?
L-TED 0.9254? 0.9266? 0.9267?
Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan-
dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported
in the same row. The ? sign marks pairwise results where the difference is not statistically significant.
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9173 0.6085 0.7709
LAS 0.8833 0.4780 0.6414
U-TED 0.9513 0.8903 0.9236
L-TED 0.9249 0.7727 0.8424
Old LTH UAS 0.6078 0.8952 0.6415
LAS 0.4809 0.8471 0.5669
U-TED 0.8960 0.9550 0.9096
L-TED 0.7823 0.9224 0.8170
CoNLL07 UAS 0.7767 0.6517 0.8991
LAS 0.6504 0.5725 0.8709
U-TED 0.9289 0.9087 0.9479
L-TED 0.8502 0.8159 0.9208
Default-oldLTH U-TED 0.9533 0.9515
L-TED 0.9289 0.9224
Default-CoNLL U-TED 0.9474? 0.9460?
L-TED 0.9281 0.9238
OldLTH-CoNLL U-TED 0.9479 0.9493
L-TED 0.9234 0.9258
Default-OldLTH-CoNLL U-TED 0.9492? 0.9461 0.9480?
L-TED 0.9298 0.9241? 0.9258?
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8991 0.8077 0.7018
LAS 0.8709 0.7902 0.6804
U-TED 0.9479 0.9373 0.9221
L-TED 0.9208 0.8955 0.8505
Functional UAS 0.8083 0.8978 0.6150
LAS 0.7895 0.8782 0.5975
U-TED 0.9356 0.9476 0.9092
L-TED 0.8929 0.9226 0.8218
Lexical UAS 0.6997 0.6161 0.8826
LAS 0.6835 0.6034 0.8491
U-TED 0.9259 0.9152 0.9483
L-TED 0.8593 0.8340 0.9160
CoNLL-Functional U-TED 0.9479? 0.9487?
L-TED 0.9209 0.9237
CoNLL-Lexical U-TED 0.9497 0.9483
L-TED 0.9228 0.9161
Functional-Lexical U-TED 0.9504 0.9483
L-TED 0.9258 0.9161
CoNLL-Functional-Lexical U-TED 0.9498 0.9504? 0.9483?
L-TED 0.9229 0.9258 0.9161
Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We
report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results
reported in the same row. The ? sign marks pairwise results where the difference is not statistically significant.
ID Description
Default The LTH conversion default settings
OldLTH The conversion used in Johansson and Nugues (2007)
CoNLL07 The conversion used in the CoNLL shared task (Nivre et al, 2007a)
Lexical Same as CoNLL, but selecting only lexical heads when a choice exists
Functional Same as CoNLL, but selecting only functional heads when a choice exists
Table 3: LTH conversion schemes used in the experiments. The LTH conversion settings in terms of the complete
feature-value pairs associated with the LTH parameters in different schemes are detailed in the supplementary material.
392
The Default, OldLTH and CoNLL schemes
mainly differ in their coordination structure, and the
Functional and Lexical schemes differ in their selec-
tion of a functional and a lexical head, respectively.
All schemes use the same inventory of labels.7 The
LTH parameter settings for the different schemes are
elaborated in the supplementary material.
The Setup We use two different parsers: (i) Malt-
Parser (Nivre et al, 2007b) with the arc eager algo-
rithm as optimized for English in (Nivre et al, 2010)
and (ii) MSTParser with the second-order projec-
tive model of McDonald and Pereira (2006). Both
parsers were trained on the different instances of
sections 2-21 of the PTB obeying the different an-
notation schemes in Table 3. Each trained model
was used to parse section 23. All non-projective de-
pendencies in the training and gold sets were projec-
tivized prior to training and parsing using the algo-
rithm of Nivre and Nilsson (2005). A more princi-
pled treatment of non-projective dependency trees is
an important topic for future research. We evaluated
the parses using labeled and unlabeled attachment
scores, and using our TEDEVAL software package.
Evaluation Our TEDEVAL software package im-
plements the pipeline described in Section 3. We
convert all parse and gold trees into functional
trees using the algorithm defined in Section 3, and
for each pair of parsing experiments we calculate
a shared gold standard using generalization deter-
mined through a chart-based greedy algorithm.8 Our
scoring procedure uses the TED algorithm defined
by Zhang and Shasha (1989).9 The unlabeled score
is obtained by assigning cost(e) = 0 for every e re-
labeling operation. To calculate pairwise statistical
significance we use a shuffling test with 10,000 it-
erations (Cohen, 1995). A sample of all files in the
evaluation pipeline for a subset of 10 PTB sentences
is available in the supplementary materials.10
7In case the labels are not taken from the same inventory,
e.g., subjects in one scheme are marked as SUB and in the other
marked as SBJ, it is possible define a a set of zero-cost operation
types ? in such case, to the operation relabel(SUB,SBJ) ? in
order not to penalize string label discrepancies.
8Our algorithm has space and runtime complexity ofO(n2).
9Available via http://web.science.mq.edu.au/
?swan/howtos/treedistance/
10The TEDEVAL software package is available via http:
//stp.lingfil.uu.se/?tsarfaty/unipar
Results Table 1 reports the results for the inter-
and cross-experiment evaluation of parses produced
by MaltParser. The left hand side of the table
presents the parsing results for a set of experiments
in which we compare parsing results trained on the
Default, OldLTH and CoNLL07 schemes. In a sec-
ond set of experiments we compare the CoNLL07,
Lexical and Functional schemes. Table 2 reports the
evaluation of the parses produced by MSTParser for
the same experimental setup. Our goal here is not to
compare the parsers, but to verify that the effects of
switching from LAS to TEDEVAL are robust across
parsing algorithms.
In each of the tables, the top three groups of four
rows compare results of parsed dependency trees
trained on a particular scheme against gold trees of
the same and the other schemes. The next three
groups of two rows report the results for compar-
ing pairwise sets of experiments against a general-
ized gold using our proposed procedure. In the last
group of two rows we compare all parsing results
against a single gold obtained through a three-way
generalization.
As expected, every parser appears to perform at
its best when evaluated against the scheme it was
trained on. This is the case for both LAS and TEDE-
VAL measures and the performance gaps are statis-
tically significant. When moving to pairwise evalu-
ation against a single generalized gold, for instance,
when comparing CoNLL07 to the Default settings,
there is still a gap in performance, e.g., between
OldLTH and CoNLL07, and between OldLTH and
Default. This gap is however a lot smaller and is not
always statistically significant. In fact, when evalu-
ating the effect of linguistically disparate annotation
variations such as Lexical and Functional on the per-
formance of MaltParser, Table 1 shows that when
using TEDEVAL and a generalized gold the perfor-
mance gaps are small and statistically insignificant.
Moreover, observed performance trends when
evaluating individual experiments on their original
training scheme may change when compared against
a generalized gold. The Default scheme, for Malt-
Parser, appears better than OldLTH when both are
evaluated against their training schemes. But look-
ing at the pairwise-evaluated experiments, it is the
other way round (the difference is smaller, but statis-
tically significant). In evaluating against a three-way
393
generalization, all the results obtained for different
training schemes are on a par with one another, with
minor gaps in performance, rarely statistically sig-
nificant. This suggests that apparent performance
trends between experiments when evaluating with
respect to the training schemes may be misleading.
These observations are robust across parsing algo-
rithms. In each of the tables, results obtained against
the training schemes show significant differences
whereas applying our cross-experimental procedure
shows small to no gaps in performance across dif-
ferent schemes. Annotation variants which seem to
have crucial effects have a relatively small influence
when parsed structures are brought into the same
formal and theoretical common ground for compar-
ison. Of course, it may be the case that one parser is
better trained on one scheme while the other utilizes
better another scheme, but objective performance
gaps can only be observed when they are compared
against shared linguistic content.
5 Discussion and Extensions
This paper addresses the problem of cross-
experiment evaluation. As it turns out, this prob-
lem arises in NLP in different shapes and forms;
when evaluating a parser against different annota-
tion schemes, when evaluating parsing performance
across parsers and different formalisms, and when
comparing parser performance across languages.
We consider our contribution successful if after
reading it the reader develops a healthy suspicion to
blunt comparison of numbers across experiments, or
better yet, across different papers. Cross-experiment
comparison should be a careful and well thought-
through endeavor, in which we retain as much infor-
mation as we can from the parsed structures, avoid
lossy conversions, and focus on an object of evalua-
tion which is agreed upon by all variants.
Our proposal introduces one way of doing so in
a streamlined, efficient and formally worked out
way. While individual components may be further
refined or improved, the proposed setup and imple-
mentation can be straightforwardly applied to cross-
parser and cross-framework evaluation. In the fu-
ture we plan to use this procedure for comparing
constituency and dependency parsers. A conversion
from constituency-based trees into functional trees
is straightforward to define: simply replace the node
labels with the grammatical function of their domi-
nating arc ? and the rest of the pipeline follows.
A pre-condition for cross-framework evaluation
is that all representations encode the same set of
grammatical relations by, e.g., annotating arcs in de-
pendency trees or decorating nodes in constituency
trees. For some treebanks this is already the case
(Nivre and Megyesi, 2007; Skut et al, 1997; Hin-
richs et al, 2004) while for others this is still lack-
ing. Recent studies (Briscoe et al, 2002; de Marn-
effe et al, 2006) suggest that evaluation through a
single set of grammatical relations as the common
denominator is a linguistically sound and practically
useful way to go. To guarantee extensions for cross-
framework evaluation it would be fruitful to make
sure that resources use the same set of grammatical
relation labels across different formal representation
types. Moreover, we further aim to inquire whether
we can find a single set of grammatical relation la-
bels that can be used across treebanks for multiple
languages. This would then pave the way for the de-
velopment of cross-language evaluation procedures.
6 Conclusion
We propose an end-to-end procedure for compar-
ing dependency parsing results across experiments
based on three steps: (i) converting dependency trees
to functional trees, (ii) generalizing functional trees
to harmonize information from different sources,
and (iii) using distance-based metrics that take the
different sources into account. When applied to
parsing results of different dependency schemes,
dramatic gaps observed when comparing parsing re-
sults obtained in isolation decrease or dissolve com-
pletely when using our proposed pipeline.
Acknowledgments We thank the developers of
the LTH and TED software who made their code
available for our use. We thank Richard Johansson
for providing us with the LTH parameter settings of
existing dependency schemes. We thank Ari Rap-
poport, Omri Abend, Roy Schwartz and members of
the NLP lab at the Hebrew University of Jerusalem
for stimulating discussion. We finally thank three
anonymous reviewers for useful comments on an
earlier draft. The research reported in the paper was
partially funded by the Swedish Research Council.
394
References
Philip Bille. 2005. A survey on tree edit distance
and related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Claudia
Gdaniec, Ralph Grishman, P. Harrison, Donald Hin-
dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-
vans, Mark Liberman, Mitchell P. Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
1991. Procedure for quantitatively comparing the syn-
tactic coverage of English grammars. In E. Black, ed-
itor, Proceedings of the workshop on Speech and Nat-
ural Language, HLT, pages 306?311. Association for
Computational Linguistics.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes.
In Proceedings of LREC Workshop?Beyond Parseval
? Towards improved evaluation measures for parsing
systems?.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the impact of alternative dependency graph encodings
on solving event extraction tasks. In Proceedings of
EMNLP, pages 982?992.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In
Proceedings of LREC, pages 447?454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC.
Jinho D. Choi and Martha Palmer. 2010. Robust
constituent-to-dependency conversion for English. In
Proceedings of TLT.
Paul Cohen. 1995. Empirical Methods for Artificial In-
telligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, pages 449?454.
Martin Emms. 2008. Tree-distance and some other vari-
ants of evalb. In Proceedings of LREC.
Erhard Hinrichs, Sandra Ku?bler, Karin Naumann, Heike
Telljohan, and Julia Trushkina. 2004. Recent develop-
ment in linguistic annotations of the Tu?Ba-D/Z Tree-
bank. In Proceedings of TLT.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA.
Sandra Ku?bler and Heike Telljohann. 2002. Towards
a dependency-oriented evaluation for partial parsing.
In Proceedings of LREC Workshop?Beyond Parseval
? Towards improved evaluation measures for parsing
systems?.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of ACL, pages 46?54.
Joakim Nivre and Beata Megyesi. 2007. Bootstrapping
a Swedish Treebank using cross-corpus harmonization
and annotation projection. In Proceedings of TLT.
Joakim Nivre and Jens Nilsson. 2005. Pseudo projective
dependency parsing. In Proceeding of ACL, pages 99?
106.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
COLING, pages 64?70.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
JoakimNivre, Laura Rimell, RyanMcDonald, and Carlos
Go?mez-Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. pages 813?821.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In Proceedings of HLT-ACL, pages 337?
340.
Ines Rehbein and Josef van Genabith. 2007. Why is it so
difficult to compare treebanks? Tiger and Tu?Ba-D/Z
revisited. In Proceedings of TLT, pages 115?126.
395
Brian Roark. 2002. Evaluating parser accuracy us-
ing edit distance. In Proceedings of LREC Work-
shop?Beyond Parseval ? Towards improved evaluation
measures for parsing systems?.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of ACL, pages 663?672.
Stuart M. Shieber. 1986. An Introduction to Unification-
Based Grammars. Center for the Study of Language
and Information.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for free
word-order languages. In Proceedings of the fifth con-
ference on Applied natural language processing, pages
88?95.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceeding of IWPT, pages 195?206.
Kaizhong Zhang and Dennis Shasha. 1989. Simple fast
algorithms for the editing distance between trees and
related problems. In SIAM Journal of Computing, vol-
ume 18, pages 1245?1262.
Arnold M. Zwicky. 1993. Heads, bases, and functors.
In G.G. Corbett, N. Fraser, and S. McGlashan, editors,
Heads in Grammatical Theory, pages 292?315. Cam-
bridge University Press.
396
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44?54,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Cross-Framework Evaluation for Statistical Parsing
Reut Tsarfaty Joakim Nivre Evelina Andersson
Uppsala University, Box 635, 75126 Uppsala, Sweden
tsarfaty@stp.lingfil.uu.se,{joakim.nivre,evelina.andersson}@lingfil.uu.se
Abstract
A serious bottleneck of comparative parser
evaluation is the fact that different parsers
subscribe to different formal frameworks
and theoretical assumptions. Converting
outputs from one framework to another is
less than optimal as it easily introduces
noise into the process. Here we present a
principled protocol for evaluating parsing
results across frameworks based on func-
tion trees, tree generalization and edit dis-
tance metrics. This extends a previously
proposed framework for cross-theory eval-
uation and allows us to compare a wider
class of parsers. We demonstrate the useful-
ness and language independence of our pro-
cedure by evaluating constituency and de-
pendency parsers on English and Swedish.
1 Introduction
The goal of statistical parsers is to recover a for-
mal representation of the grammatical relations
that constitute the argument structure of natural
language sentences. The argument structure en-
compasses grammatical relationships between el-
ements such as subject, predicate, object, etc.,
which are useful for further (e.g., semantic) pro-
cessing. The parses yielded by different parsing
frameworks typically obey different formal and
theoretical assumptions concerning how to rep-
resent the grammatical relationships in the data
(Rambow, 2010). For example, grammatical rela-
tions may be encoded on top of dependency arcs
in a dependency tree (Mel?c?uk, 1988), they may
decorate nodes in a phrase-structure tree (Marcus
et al 1993; Maamouri et al 2004; Sima?an et
al., 2001), or they may be read off of positions in
a phrase-structure tree using hard-coded conver-
sion procedures (de Marneffe et al 2006). This
diversity poses a challenge to cross-experimental
parser evaluation, namely: How can we evaluate
the performance of these different parsers relative
to one another?
Current evaluation practices assume a set of
correctly annotated test data (or gold standard)
for evaluation. Typically, every parser is eval-
uated with respect to its own formal representa-
tion type and the underlying theory which it was
trained to recover. Therefore, numerical scores
of parses across experiments are incomparable.
When comparing parses that belong to different
formal frameworks, the notion of a single gold
standard becomes problematic, and there are two
different questions we have to answer. First, what
is an appropriate gold standard for cross-parser
evaluation? And secondly, how can we alle-
viate the differences between formal representa-
tion types and theoretical assumptions in order to
make our comparison sound ? that is, to make sure
that we are not comparing apples and oranges?
A popular way to address this has been to
pick one of the frameworks and convert all
parser outputs to its formal type. When com-
paring constituency-based and dependency-based
parsers, for instance, the output of constituency
parsers has often been converted to dependency
structures prior to evaluation (Cer et al 2010;
Nivre et al 2010). This solution has vari-
ous drawbacks. First, it demands a conversion
script that maps one representation type to another
when some theoretical assumptions in one frame-
work may be incompatible with the other one.
In the constituency-to-dependency case, some
constituency-based structures (e.g., coordination
44
and ellipsis) do not comply with the single head
assumption of dependency treebanks. Secondly,
these scripts may be labor intensive to create, and
are available mostly for English. So the evalua-
tion protocol becomes language-dependent.
In Tsarfaty et al(2011) we proposed a gen-
eral protocol for handling annotation discrepan-
cies when comparing parses across different de-
pendency theories. The protocol consists of three
phases: converting all structures into function
trees, for each sentence, generalizing the different
gold standard function trees to get their common
denominator, and employing an evaluation mea-
sure based on tree edit distance (TED) which dis-
cards edit operations that recover theory-specific
structures. Although the protocol is potentially
applicable to a wide class of syntactic represen-
tation types, formal restrictions in the procedures
effectively limit its applicability only to represen-
tations that are isomorphic to dependency trees.
The present paper breaks new ground in the
ability to soundly compare the accuracy of differ-
ent parsers relative to one another given that they
employ different formal representation types and
obey different theoretical assumptions. Our solu-
tion generally confines with the protocol proposed
in Tsarfaty et al(2011) but is re-formalized to
allow for arbitrary linearly ordered labeled trees,
thus encompassing constituency-based as well as
dependency-based representations. The frame-
work in Tsarfaty et al(2011) assumes structures
that are isomorphic to dependency trees, bypass-
ing the problem of arbitrary branching. Here we
lift this restriction, and define a protocol which
is based on generalization and TED measures to
soundly compare the output of different parsers.
We demonstrate the utility of this protocol by
comparing the performance of different parsers
for English and Swedish. For English, our
parser evaluation across representation types al-
lows us to analyze and precisely quantify previ-
ously encountered performance tendencies. For
Swedish we show the first ever evaluation be-
tween dependency-based and constituency-based
parsing models, all trained on the Swedish tree-
bank data. All in all we show that our ex-
tended protocol, which can handle linearly-
ordered labeled trees with arbitrary branch-
ing, can soundly compare parsing results across
frameworks in a representation-independent and
language-independent fashion.
2 Preliminaries: Relational Schemes for
Cross-Framework Parse Evaluation
Traditionally, different statistical parsers have
been evaluated using specially designated evalu-
ation measures that are designed to fit their repre-
sentation types. Dependency trees are evaluated
using attachment scores (Buchholz and Marsi,
2006), phrase-structure trees are evaluated using
ParsEval (Black et al 1991), LFG-based parsers
postulate an evaluation procedure based on f-
structures (Cahill et al 2008), and so on. From a
downstream application point of view, there is no
significance as to which formalism was used for
generating the representation and which learning
methods have been utilized. The bottom line is
simply which parsing framework most accurately
recovers a useful representation that helps to un-
ravel the human-perceived interpretation.
Relational schemes, that is, schemes that en-
code the set of grammatical relations that con-
stitute the predicate-argument structures of sen-
tences, provide an interface to semantic interpre-
tation. They are more intuitively understood than,
say, phrase-structure trees, and thus they are also
more useful for practical applications. For these
reasons, relational schemes have been repeatedly
singled out as an appropriate level of representa-
tion for the evaluation of statistical parsers (Lin,
1995; Carroll et al 1998; Cer et al 2010).
The annotated data which statistical parsers are
trained on encode these grammatical relationships
in different ways. Dependency treebanks provide
a ready-made representation of grammatical rela-
tions on top of arcs connecting the words in the
sentence (Ku?bler et al 2009). The Penn Tree-
bank and phrase-structure annotated resources en-
code partial information about grammatical rela-
tions as dash-features decorating phrase structure
nodes (Marcus et al 1993). Treebanks like Tiger
for German (Brants et al 2002) and Talbanken
for Swedish (Nivre and Megyesi, 2007) explic-
itly map phrase structures onto grammatical rela-
tions using dedicated edge labels. The Relational-
Realizational structures of Tsarfaty and Sima?an
(2008) encode relational networks (sets of rela-
tions) projected and realized by syntactic cate-
gories on top of ordinary phrase-structure nodes.
Function trees, as defined in Tsarfaty et al
(2011), are linearly ordered labeled trees in which
every node is labeled with the grammatical func-
45
(a) -ROOT- John loves Marysbj objroot ? root
sbj
John
hd
loves
obj
Mary
(b) S-root
NP-sbj
NN-hd
John
VP-prd
V-hd
loves
NP-obj
NN-hd
Mary
? root
sbj
hd
John
prd
hd
loves
obj
hd
Mary
(c) S
{sbj,prd,obj}
sbj
NP
{hd}
hd
NN
John
prd
V
loves
obj
NP
{hd}
hd
NN
Mary
? root
sbj
hd
John
prd
loves
obj
hd
Mary
Figure 1: Deterministic conversion into function trees.
The algorithm for extracting a function tree from a de-
pendency tree as in (a) is provided in Tsarfaty et al
(2011). For a phrase-structure tree as in (b) we can re-
place each node label with its function (dash-feature).
In a relational-realizational structure like (c) we can re-
move the projection nodes (sets) and realization nodes
(phrase labels), which leaves the function nodes intact.
tion of the dominated span. Function trees ben-
efit from the same advantages as other relational
schemes, namely that they are intuitive to under-
stand, they provide the interface for semantic in-
terpretation, and thus may be useful for down-
stream applications. Yet they do not suffer from
formal restrictions inherent in dependency struc-
tures, for instance, the single head assumption.
For many formal representation types there ex-
ists a fully deterministic, heuristics-free, proce-
dure mapping them to function trees. In Figure 1
we illustrate some such procedures for a simple
transitive sentence. Now, while all the structures
at the right hand side of Figure 1 are of the same
formal type (function trees), they have different
tree structures due to different theoretical assump-
tions underlying the original formal frameworks.
(t1) root
f1
f2
w
(t2) root
f2
f1
w
(t3) root
{f1,f2}
w
Figure 2: Unary chains in function trees
Once we have converted framework-specific
representations into function trees, the problem of
cross-framework evaluation can potentially be re-
duced to a cross-theory evaluation following Tsar-
faty et al(2011). The main idea is that once
all structures have been converted into function
trees, one can perform a formal operation called
generalization in order to harmonize the differ-
ences between theories, and measure accurately
the distance of parse hypotheses from the gener-
alized gold. The generalization operation defined
in Tsarfaty et al(2011), however, cannot handle
trees that may contain unary chains, and therefore
cannot be used for arbitrary function trees.
Consider for instance (t1) and (t2) in Figure 2.
According to the definition of subsumption in
Tsarfaty et al(2011), (t1) is subsumed by (t2)
and vice versa, so the two trees should be identi-
cal ? but they are not. The interpretation we wish
to give to a function tree such as (t1) is that the
word w has both the grammatical function f1 and
the grammatical function f2. This can be graphi-
cally represented as a set of labels dominating w,
as in (t3). We call structures such as (t3) multi-
function trees. In the next section we formally de-
fine multi-function trees, and then use them to de-
velop our protocol for cross-framework and cross-
theory evaluation.
3 The Proposal: Cross-Framework
Evaluation with Multi-Function Trees
Our proposal is a three-phase evaluation proto-
col in the spirit of Tsarfaty et al(2011). First,
we obtain a formal common ground for all frame-
works in terms of multi-function trees. Then we
obtain a theoretical common ground by means
of tree-generalization on gold trees. Finally, we
calculate TED-based scores that discard the cost
of annotation-specific edits. In this section, we
define multi-function trees and update the tree-
generalization and TED-based metrics to handle
multi-function trees that reflect different theories.
46
Figure 3: The Evaluation Protocol. Different formal frameworks yield different parse and gold formal types.
All types are transformed into multi-function trees. All gold trees enter generalization to yield a new gold for
each sentence. The different ? arcs represent the different edit scripts used for calculating the TED-based scores.
3.1 Defining Multi-Function Trees
An ordinary function tree is a linearly ordered tree
T = (V,A) with yield w1, ..., wn, where internal
nodes are labeled with grammatical function la-
bels drawn from some set L. We use span(v)
and label(v) to denote the yield and label, respec-
tively, of an internal node v. A multi-function tree
is a linearly ordered tree T = (V,A) with yield
w1, ..., wn, where internal nodes are labeled with
sets of grammatical function labels drawn from L
and where v 6= v? implies span(v) 6= span(v?)
for all internal nodes v, v?. We use labels(v) to
denote the label set of an internal node v.
We interpret multi-function trees as encoding
sets of functional constraints over spans in func-
tion trees. Each node v in a multi-function tree
represents a constraint of the form: for each
l ? labels(v), there should be a node v? in the
function tree such that span(v) = span(v?) and
label(v?) = l. Whenever we have a conversion for
function trees, we can efficiently collapse them
into multi-function trees with no unary produc-
tions, and with label sets labeling their nodes.
Thus, trees (t1) and (t2) in Figure 2 would both
be mapped to tree (t3), which encodes the func-
tional constraints encoded in either of them.
For dependency trees, we assume the conver-
sion to function trees defined in Tsarfaty et al
(2011), where head daughters always get the la-
bel ?hd?. For PTB style phrase-structure trees, we
replace the phrase-structure labels with functional
dash-features. In relational-realization structures
we remove projection and realization nodes. De-
terministic conversions exist also for Tiger style
treebanks and frameworks such as LFG, but we
do not discuss them here.1
1All the conversions we use are deterministic and are
defined in graph-theoretic and language-independent terms.
We make them available at http://stp.lingfil.uu.
se/?tsarfaty/unipar/index.html.
3.2 Generalizing Multi-Function Trees
Once we obtain multi-function trees for all the
different gold standard representations in the sys-
tem, we feed them to a generalization operation
as shown in Figure 3. The goal of this opera-
tion is to provide a consensus gold standard that
captures the linguistic structure that the different
gold theories agree on. The generalization struc-
tures are later used as the basis for the TED-based
evaluation. Generalization is defined by means of
subsumption. A multi-function tree subsumes an-
other one if and only if all the constraints defined
by the first tree are also defined by the second tree.
So, instead of demanding equality of labels as in
Tsarfaty et al(2011), we demand set inclusion:
T-Subsumption, denoted vt, is a relation
between multi-function trees that indicates
that a tree pi1 is consistent with and more
general than tree pi2. Formally: pi1 vt pi2
iff for every node n ? pi1 there exists a node
m ? pi2 such that span(n) = span(m) and
labels(n) ? labels(m).
T-Unification, denoted unionsqt, is an operation
that returns the most general tree structure
that contains the information from both input
trees, and fails if such a tree does not exist.
Formally: pi1 unionsqt pi2 = pi3 iff pi1 vt pi3 and
pi2 vt pi3, and for all pi4 such that pi1 vt pi4
and pi2 vt pi4 it holds that pi3 vt pi4.
T-Generalization, denoted ut, is an opera-
tion that returns the most specific tree that
is more general than both trees. Formally,
pi1utpi2 = pi3 iff pi3 vt pi1 and pi3 vt pi2, and
for every pi4 such that pi4 vt pi1 and pi4 vt pi2
it holds that pi4 vt pi3.
The generalization tree contains all nodes that ex-
ist in both trees, and for each node it is labeled by
47
the intersection of the label sets dominating the
same span in both trees. The unification tree con-
tains nodes that exist in one tree or another, and
for each span it is labeled by the union of all label
sets for this span in either tree. If we generalize
two trees and one tree has no specification for la-
bels over a span, it does not share anything with
the label set dominating the same span in the other
tree, and the label set dominating this span in the
generalized tree is empty. If the trees do not agree
on any label for a particular span, the respective
node is similarly labeled with an empty set. When
we wish to unify theories, then an empty set over
a span is unified with any other set dominating the
same span in the other tree, without altering it.
Digression: Using Unification to Merge Infor-
mation From Different Treebanks In Tsarfaty
et al(2011), only the generalization operation
was used, providing the common denominator of
all the gold structures and serving as a common
ground for evaluation. The unification operation
is useful for other NLP tasks, for instance, com-
bining information from two different annotation
schemes or enriching one annotation scheme with
information from a different one. In particular,
we can take advantage of the new framework to
enrich the node structure reflected in one theory
with grammatical functions reflected in an anno-
tation scheme that follows a different theory. To
do so, we define the Tree-Labeling-Unification
operation on multi-function trees.
TL-Unification, denoted unionsqtl, is an opera-
tion that returns a tree that retains the struc-
ture of the first tree and adds labels that ex-
ist over its spans in the second tree. For-
mally: pi1 unionsqtl pi2 = pi3 iff for every node
n ? pi1 there exists a node m ? pi3 such
that span(m) = span(n) and labels(m) =
labels(n) ? labels(pi2, span(n)).
Where labels(pi2, span(n)) is the set of labels of
the node with yield span(n) in pi2 if such a node
exists and ? otherwise. We further discuss the TL-
Unification and its use for data preparation in ?4.
3.3 TED Measures for Multi-Function Trees
The result of the generalization operation pro-
vides us with multi-function trees for each of the
sentences in the test set representing sets of con-
straints on which the different gold theories agree.
We would now like to use distance-based met-
rics in order to measure the gap between the gold
and predicted theories. The idea behind distance-
based evaluation in Tsarfaty et al(2011) is that
recording the edit operations between the native
gold and the generalized gold allows one to dis-
card their cost when computing the cost of a parse
hypothesis turned into the generalized gold. This
makes sure that different parsers do not get penal-
ized, or favored, due to annotation specific deci-
sions that are not shared by other frameworks.
The problem is now that TED is undefined with
respect to multi-function trees because it cannot
handle complex labels. To overcome this, we
convert multi-function trees into sorted function
trees, which are simply function trees in which
any label set is represented as a unary chain of
single-labeled nodes, and the nodes are sorted ac-
cording to the canonical order of their labels.2 In
case of an empty set, a 0-length chain is created,
that is, no node is created over this span. Sorted
function trees prevent reordering nodes in a chain
in one tree to fit the order in another tree, since it
would violate the idea that the set of constraints
over a span in a multi-function tree is unordered.
The edit operations we assume are add-
node(l, i, j) and delete-node(l, i, j) where l ? L
is a grammatical function label and i < j define
the span of a node in the tree. Insertion into a
unary chain must confine with the canonical order
of the labels. Every operation is assigned a cost.
An edit script is a sequence of edit operations that
turns a function tree pi1 into pi2, that is:
ES(pi1, pi2) = ?e1, . . . , ek?
Since all operations are anchored in spans, the se-
quence can be determined to have a unique order
of traversing the tree (say, DFS). Different edit
scripts then only differ in their set of operations
on spans. The edit distance problem is finding the
minimal cost script, that is, one needs to solve:
ES?(pi1, pi2) = min
ES(pi1,pi2)
?
e?ES(pi1,pi2)
cost(e)
In the current setting, when using only add and
delete operations on spans, there is only one edit
script that corresponds to the minimal edit cost.
So, finding the minimal edit script entails finding
a single set of operations turning pi1 into pi2.
2The ordering can be alphabetic, thematic, etc.
48
We can now define ? for the ith framework, as
the error of parsei relative to its native gold stan-
dard goldi and to the generalized gold gen. This
is the edit cost minus the cost of the script turning
parsei into gen intersected with the script turning
goldi into gen. The underlying intuition is that
if an operation that was used to turn parsei into
gen is used to discard theory-specific information
from goldi, its cost should not be counted as error.
?(parsei, goldi, gen) = cost(ES
?(parsei, gen))
?cost(ES?(parsei, gen) ? ES
?(goldi, gen))
In order to turn distance measures into parse-
scores we now normalize the error relative to the
size of the trees and subtract it from a unity. So
the Sentence Score for parsing with framework i
is:
score(parsei, goldi, gen) =
1?
?(parsei, goldi,gen)
|parsei|+ |gen|
Finally, Test-Set Average is defined by macro-
avaraging over all sentences in the test-set:
1?
?|testset|
j=1 ?(parseij , goldij , genj)
?|testset|
j=1 |parseij |+ |genj |
This last formula represents the TEDEVAL metric
that we use in our experiments.
A Note on System Complexity Conversion of
a dependency or a constituency tree into a func-
tion tree is linear in the size of the tree. Our
implementation of the generalization and unifica-
tion operation is an exact, greedy, chart-based al-
gorithm that runs in polynomial time (O(n2) in
n the number of terminals). The TED software
that we utilize builds on the TED efficient algo-
rithm of Zhang and Shasha (1989) which runs in
O(|T1||T2|min(d1, n1)min(d2, n2)) time where
di is the tree degree (depth) and ni is the number
of terminals in the respective tree (Bille, 2005).
4 Experiments
We validate our cross-framework evaluation pro-
cedure on two languages, English and Swedish.
For English, we compare the performance of
two dependency parsers, MaltParser (Nivre et al
2006) and MSTParser (McDonald et al 2005),
and two constituency-based parsers, the Berkeley
parser (Petrov et al 2006) and the Brown parser
(Charniak and Johnson, 2005). All experiments
use Penn Treebank (PTB) data. For Swedish,
we compare MaltParser and MSTParser with two
variants of the Berkeley parser, one trained on
phrase structure trees, and one trained on a vari-
ant of the Relational-Realizational representation
of Tsarfaty and Sima?an (2008). All experiments
use the Talbanken Swedish Treebank (STB) data.
4.1 English Cross-Framework Evaluation
We use sections 02?21 of the WSJ Penn Tree-
bank for training and section 00 for evaluation and
analysis. We use two different native gold stan-
dards subscribing to different theories of encoding
grammatical relations in tree structures:
? THE DEPENDENCY-BASED THEORY is the
theory encoded in the basic Stanford Depen-
dencies (SD) scheme. We obtain the set of
basic stanford dependency trees using the
software of de Marneffe et al(2006) and
train the dependency parsers directly on it.
? THE CONSTITUENCY-BASED THEORY is
the theory reflected in the phrase-structure
representation of the PTB (Marcus et al
1993) enriched with function labels compat-
ible with the Stanford Dependencies (SD)
scheme. We obtain trees that reflect this
theory by TL-Unification of the PTB multi-
function trees with the SD multi-function
trees (PTBunionsqtlSD) as illustrated in Figure 4.
The theory encoded in the multi-function trees
corresponding to SD is different from the one
obtained by our TL-Unification, as may be seen
from the difference between the flat SD multi-
function tree and the result of the PTBunionsqtlSD in
Figure 4. Another difference concerns coordina-
tion structures, encoded as binary branching trees
in SD and as flat productions in the PTBunionsqtlSD.
Such differences are not only observable but also
quantifiable, and using our redefined TED metric
the cross-theory overlap is 0.8571.
The two dependency parsers were trained using
the same settings as in Tsarfaty et al(2011), using
SVMTool (Gime?nez and Ma`rquez, 2004) to pre-
dict part-of-speech tags at parsing time. The two
constituency parsers were used with default set-
tings and were allowed to predict their own part-
of-speech tags. We report three different evalua-
tion metrics for the different experiments:
49
(PTB) S
NP
NN
John
VP
V
loves
NP
NN
Mary
?
John loves
Mary
? ?
?
John
?
?
loves
?
Mary
(SD) -ROOT- John loves Marysbj objroot ? root
sbj
John
hd
loves
obj
Mary
? {root}
{sbj}
John
{hd}
loves
{obj}
Mary
(PTB) unionsqtl (SD) = {root}
{sbj}
John
?
{hd}
loves
{obj}
Mary
Figure 4: Conversion of PTB and SD tree to multi-
function trees, followed by TL-Unification of the trees.
Note that some PTB nodes remain without an SD label.
? LAS/UAS (Buchholz and Marsi, 2006)
? PARSEVAL (Black et al 1991)
? TEDEVAL as defined in Section 3
We use LAS/UAS for dependency parsers that
were trained on the same dependency theory. We
use ParseEval to evaluate phrase-structure parsers
that were trained on PTB trees in which dash-
features and empty traces are removed. We
use our implementation of TEDEVAL to evaluate
parsing results across all frameworks under two
different scenarios:3 TEDEVAL SINGLE evalu-
ates against the native gold multi-function trees.
TEDEVAL MULTIPLE evaluates against the gen-
eralized (cross-theory) multi-function trees. Un-
labeled TEDEVAL scores are obtained by sim-
ply removing all labels from the multi-function
nodes, and using unlabeled edit operations. We
calculate pairwise statistical significance using a
shuffling test with 10K iterations (Cohen, 1995).
Tables 1 and 2 present the results of our cross-
framework evaluation for English Parsing. In the
left column of Table 1 we report ParsEval scores
for constituency-based parsers. As expected, F-
Scores for the Brown parser are higher than the
F-Scores of the Berkeley parser. F-Scores are
however not applicable across frameworks. In
the rightmost column of Table 1 we report the
LAS/UAS results for all parsers. If a parser yields
3Our TedEval software can be downloaded at
http://stp.lingfil.uu.se/?tsarfaty/
unipar/download.html.
a constituency tree, it is converted to and evalu-
ated on SD. Here we see that MST outperforms
Malt, though the differences for labeled depen-
dencies are insignificant. We also observe here a
familiar pattern from Cer et al(2010) and others,
where the constituency parsers significantly out-
perform the dependency parsers after conversion
of their output into dependencies.
The conversion to SD allows one to compare
results across formal frameworks, but not with-
out a cost. The conversion introduces a set of an-
notation specific decisions which may introduce
a bias into the evaluation. In the middle column
of Table 1 we report the TEDEVAL metrics mea-
sured against the generalized gold standard for all
parsing frameworks. We can now confirm that
the constituency-based parsers significantly out-
perform the dependency parsers, and that this is
not due to specific theoretical decisions which are
seen to affect LAS/UAS metrics (Schwartz et al
2011). For the dependency parsers we now see
that Malt outperforms MST on labeled dependen-
cies slightly, but the difference is insignificant.
The fact that the discrepancy in theoretical as-
sumptions between different frameworks indeed
affects the conversion-based evaluation procedure
is reflected in the results we report in Table 2.
Here the leftmost and rightmost columns report
TEDEVAL scores against the own native gold
(SINGLE) and the middle column against the gen-
eralized gold (MULTIPLE). Had the theories
for SD and PTBunionsqtlSD been identical, TEDEVAL
SINGLE and TEDEVAL MULTIPLE would have
been equal in each line. Because of theoretical
discrepancies, we see small gaps in parser perfor-
mance between these cases. Our protocol ensures
that such discrepancies do not bias the results.
4.2 Cross-Framework Swedish Parsing
We use the standard training and test sets of the
Swedish Treebank (Nivre and Megyesi, 2007)
with two gold standards presupposing different
theories:
? THE DEPENDENCY-BASED THEORY is the
dependency version of the Swedish Tree-
bank. All trees are projectivized (STB-Dep).
? THE CONSTITUENCY-BASED THEORY is
the standard Swedish Treebank with gram-
matical function labels on the edges of con-
stituency structures (STB).
50
Formalism PS Trees MF Trees Dep Trees
Theory PTB unionsqlt SD (PTB unionsqlt SD) SD
ut SD
Metrics PARSEVAL TEDEVAL ATTSCORES
MALT N/A U: 0.9525L: 0.9088
U: 0.8962
L: 0.8772
MST N/A U: 0.9549L: 0.9049
U: 0.9059
L: 0.8795
BERKELEY F-Scores0.9096
U: 0.9677
L: 0.9227
U: 0.9254
L: 0.9031
BROWN F-Scores0.9129
U: 0.9702
L: 0.9264
U: 0.9289
L: 0.9057
Table 1: English cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are highest in their column. Italic scores
are the highest for dependency parsers in their column.
Formalism PS Trees MF Trees Dep Trees
Theory PTB unionsqlt SD (PTB unionsqlt SD) SD
ut SD
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9525L: 0.9088
U: 0.9524
L: 0.9186
MST N/A U: 0.9549L: 0.9049
U: 0.9548
L: 0.9149
BERKELEY U: 0.9645L: 0.9271
U: 0.9677
L: 0.9227
U: 0.9649
L: 0.9324
BROWN U: 0.9667L: 0.9301
U: 9702
L: 9264
U: 0.9679
L: 0.9362
Table 2: English cross-framework evaluation: TEDE-
VAL scores against gold and generalized gold. Bold-
face scores are highest in their column. Italic scores
are highest for dependency parsers in their column.
Because there are no parsers that can out-
put the complete STB representation including
edge labels, we experiment with two variants of
this theory, one which is obtained by simply re-
moving the edge labels and keeping only the
phrase-structure labels (STB-PS) and one which
is loosely based on the Relational-Realizational
scheme of Tsarfaty and Sima?an (2008) but ex-
cludes the projection set nodes (STB-RR). RR
trees only add function nodes to PS trees, and
it holds that STB-PSutSTB-RR=STB-PS. The
overlap between the theories expressed in multi-
function trees originating from STB-Dep and
STB-RR is 0.7559. Our evaluation protocol takes
into account such discrepancies while avoiding
biases that may be caused due to these differences.
We evaluate MaltParser, MSTParser and two
versions of the Berkeley parser, one trained on
STB-PS and one trained on STB-RR. We use
predicted part-of-speech tags for the dependency
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics PARSEVAL TEDEVAL ATTSCORE
MALT N/A U: 0.9266L: 0.8225
U: 0.8298
L: 0.7782
MST N/A U: 0.9275L: 0.8121
U: 0.8438
L: 0.7824
BKLY/STB-RR F-Score0.7914
U: 0.9281
L: 0.7861 N/A
BKLY/STB-PS F-Score0.7855 N/A N/A
Table 3: Swedish cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are the highest in their column.
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9266L: 0.8225
U: 0.9264
L: 0.8372
MST N/A U: 0.9275L: 0.8121
U: 0.9272
L: 0.8275
BKLY-STB-RR U: 0.9239L: 0.7946
U: 0.9281
L: 0.7861 N/A
Table 4: Swedish cross-framework evaluation: TEDE-
VAL scores against the native gold and the generalized
gold. Boldface scores are the highest in their column.
parsers, using the HunPoS tagger (Megyesi,
2009), but let the Berkeley parser predict its own
tags. We use the same evaluation metrics and pro-
cedures as before. Prior to evaluating RR trees
using ParsEval we strip off the added function
nodes. Prior to evaluating them using TedEval we
strip off the phrase-structure nodes.
Tables 3 and 4 summarize the parsing results
for the different Swedish parsers. In the leftmost
column of table 3 we present the constituency-
based evaluation measures. Interestingly, the
Berkeley RR instantiation performs better than
when training the Berkeley parser on PS trees.
These constituency-based scores however have a
limited applicability, and we cannot use them to
compare constituency and dependency parsers. In
the rightmost column of Table 3 we report the
LAS/UAS results for the two dependency parsers.
Here we see higher performance demonstrated by
MST on both labeled and unlabeled dependen-
cies, but the differences on labeled dependencies
are insignificant. Since there is no automatic pro-
cedure for converting bare-bone phrase-structure
Swedish trees to dependency trees, we cannot use
51
LAS/UAS to compare across frameworks, and we
use TEDEVAL for cross-framework evaluation.
Training the Berkeley parser on RR trees which
encode a mapping of PS nodes to grammatical
functions allows us to compare parse results for
trees belonging to the STB theory with trees obey-
ing the STB-Dep theory. For unlabeled TEDE-
VAL scores, the dependency parsers perform at the
same level as the constituency parser, though the
difference is insignificant. For labeled TEDEVAL
the dependency parsers significantly outperform
the constituency parser. When considering only
the dependency parsers, there is a small advantage
for Malt on labeled dependencies, and an advan-
tage for MST on unlabeled dependencies, but the
latter is insignificant. This effect is replicated in
Table 4 where we evaluate dependency parsers us-
ing TEDEVAL against their own gold theories. Ta-
ble 4 further confirms that there is a gap between
the STB and the STB-Dep theories, reflected in
the scores against the native and generalized gold.
5 Discussion
We presented a formal protocol for evaluating
parsers across frameworks and used it to soundly
compare parsing results for English and Swedish.
Our approach follows the three-phase protocol of
Tsarfaty et al(2011), namely: (i) obtaining a for-
mal common ground for the different representa-
tion types, (ii) computing the theoretical common
ground for each test sentence, and (iii) counting
only what counts, that is, measuring the distance
between the common ground and the parse tree
while discarding annotation-specific edits.
A pre-condition for applying our protocol is the
availability of a relational interpretation of trees in
the different frameworks. For dependency frame-
works this is straightforward, as these relations
are encoded on top of dependency arcs. For con-
stituency trees with an inherent mapping of nodes
onto grammatical relations (Merlo and Musillo,
2005; Gabbard et al 2006; Tsarfaty and Sima?an,
2008), a procedure for reading relational schemes
off of the trees is trivial to implement.
For parsers that are trained on and parse into
bare-bones phrase-structure trees this is not so.
Reading off the relational structure may be more
costly and require interjection of additional theo-
retical assumptions via manually written scripts.
Scripts that read off grammatical relations based
on tree positions work well for configurational
languages such as English (de Marneffe et al
2006) but since grammatical relations are re-
flected differently in different languages (Postal
and Perlmutter, 1977; Bresnan, 2000), a proce-
dure to read off these relations in a language-
independent fashion from phrase-structure trees
does not, and should not, exist (Rambow, 2010).
The crucial point is that even when using ex-
ternal scripts for recovering a relational scheme
for phrase-structure trees, our protocol has a clear
advantage over simply scoring converted trees.
Manually created conversion scripts alter the the-
oretical assumptions inherent in the trees and thus
may bias the results. Our generalization operation
and three-way TED make sure that theory-specific
idiosyncrasies injected through such scripts do
not lead to over-penalizing or over-crediting
theory-specific structural variations.
Certain linguistic structures cannot yet be eval-
uated with our protocol because of the strict as-
sumption that the labeled spans in a parse form a
tree. In the future we plan to extend the protocol
for evaluating structures that go beyond linearly-
ordered trees in order to allow for non-projective
trees and directed acyclic graphs. In addition, we
plan to lift the restriction that the parse yield is
known in advance, in order to allow for evalua-
tion of joint parse-segmentation hypotheses.
6 Conclusion
We developed a protocol for comparing parsing
results across different theories and representa-
tion types which is framework-independent in the
sense that it can accommodate any formal syntac-
tic framework that encodes grammatical relations,
and it is language-independent in the sense that
there is no language specific knowledge encoded
in the procedure. As such, this protocol is ad-
equate for parser evaluation in cross-framework
and cross-language tasks and parsing competi-
tions, and using it across the board is expected
to open new horizons in our understanding of the
strengths and weaknesses of different parsers in
the face of different theories and different data.
Acknowledgments We thank David McClosky,
Marco Khulmann, Yoav Goldberg and three
anonymous reviewers for useful comments. We
further thank Jennifer Foster for the Brown parses
and parameter files. This research is partly funded
by the Swedish National Science Foundation.
52
References
Philip Bille. 2005. A survey on tree edit distance and
related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Clau-
dia Gdaniec, Ralph Grishman, P. Harrison, Don-
ald Hindle, Robert Ingria, Frederick Jelinek, Ju-
dith L. Klavans, Mark Liberman, Mitchell P. Mar-
cus, Salim Roukos, Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for quantitatively
comparing the syntactic coverage of English gram-
mars. In Proceedings of the DARPA Workshop on
Speech and Natural Language, pages 306?311.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The Tiger
treebank. In Proceedings of TLT.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Computa-
tional Linguistics, 34(1):81?124.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: A survey and a new pro-
posal. In Proceedings of LREC, pages 447?454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Pars-
ing to Stanford Dependencies: Trade-offs between
speed and accuracy. In Proceedings of LREC.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL.
Paul Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the Penn treebank. In Proceed-
ing of HLT-NAACL, pages 184?191.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of LREC.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420?1425.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference
on Arabic Language Resources and Tools.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT ?05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
Beata Megyesi. 2009. The open source tagger Hun-
PoS for Swedish. In Proceedings of the 17th Nordic
Conference of Computational Linguistics (NODAL-
IDA), pages 239?241.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Paola Merlo and Gabriele Musillo. 2005. Accurate
function parsing. In Proceedings of EMNLP, pages
620?627.
Joakim Nivre and Beata Megyesi. 2007. Bootstrap-
ping a Swedish Treebank using cross-corpus har-
monization and annotation projection. In Proceed-
ings of TLT.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216?2219.
Joakim Nivre, Laura Rimell, Ryan McDonald, and
Carlos Go?mez-Rodr??guez. 2010. Evaluation of de-
pendency parsers on unbounded dependencies. In
Proceedings of COLING, pages 813?821.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL.
Paul M. Postal and David M. Perlmutter. 1977. To-
ward a universal characterization of passivization.
In Proceedings of the 3rd Annual Meeting of the
Berkeley Linguistics Society, pages 394?417.
Owen Rambow. 2010. The simple truth about de-
pendency and phrase structure representations: An
opinion piece. In Proceedings of HLT-ACL, pages
337?340.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proceedings of ACL, pages
663?672.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique
des Langues.
53
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP.
Kaizhong Zhang and Dennis Shasha. 1989. Sim-
ple fast algorithms for the editing distance between
trees and related problems. In SIAM Journal of
Computing, volume 18, pages 1245?1262.
54
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 6?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Evaluation of Morphological Segmentation and Syntactic Parsing
Reut Tsarfaty Joakim Nivre Evelina Andersson
Box 635, 751 26, Uppsala University, Uppsala, Sweden
tsarfaty@stp.lingfil.uu.se, {joakim.nivre, evelina.andersson}@lingfil.uu.se
Abstract
We present novel metrics for parse evalua-
tion in joint segmentation and parsing sce-
narios where the gold sequence of terminals
is not known in advance. The protocol uses
distance-based metrics defined for the space
of trees over lattices. Our metrics allow us
to precisely quantify the performance gap be-
tween non-realistic parsing scenarios (assum-
ing gold segmented and tagged input) and re-
alistic ones (not assuming gold segmentation
and tags). Our evaluation of segmentation and
parsing for Modern Hebrew sheds new light
on the performance of the best parsing systems
to date in the different scenarios.
1 Introduction
A parser takes a sentence in natural language as in-
put and returns a syntactic parse tree representing
the sentence?s human-perceived interpretation. Cur-
rent state-of-the-art parsers assume that the space-
delimited words in the input are the basic units of
syntactic analysis. Standard evaluation procedures
and metrics (Black et al, 1991; Buchholz and Marsi,
2006) accordingly assume that the yield of the parse
tree is known in advance. This assumption breaks
down when parsing morphologically rich languages
(Tsarfaty et al, 2010), where every space-delimited
word may be effectively composed of multiple mor-
phemes, each of which having a distinct role in the
syntactic parse tree. In order to parse such input the
text needs to undergo morphological segmentation,
that is, identifying the morphological segments of
each word and assigning the corresponding part-of-
speech (PoS) tags to them.
Morphologically complex words may be highly
ambiguous and in order to segment them correctly
their analysis has to be disambiguated. The multiple
morphological analyses of input words may be rep-
resented via a lattice that encodes the different seg-
mentation possibilities of the entire word sequence.
One can either select a segmentation path prior to
parsing, or, as has been recently argued, one can let
the parser pick a segmentation jointly with decoding
(Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg
and Tsarfaty, 2008; Green and Manning, 2010). If
the selected segmentation is different from the gold
segmentation, the gold and parse trees are rendered
incomparable and standard evaluation metrics break
down. Evaluation scenarios restricted to gold input
are often used to bypass this problem, but, as shall be
seen shortly, they present an overly optimistic upper-
bound on parser performance.
This paper presents a full treatment of evaluation
in different parsing scenarios, using distance-based
measures defined for trees over a shared common
denominator defined in terms of a lattice structure.
We demonstrate the informativeness of our metrics
by evaluating joint segmentation and parsing perfor-
mance for the Semitic language Modern Hebrew, us-
ing the best performing systems, both constituency-
based and dependency-based (Tsarfaty, 2010; Gold-
berg, 2011a). Our experiments demonstrate that, for
all parsers, significant performance gaps between re-
alistic and non-realistic scenarios crucially depend
on the kind of information initially provided to the
parser. The tool and metrics that we provide are
completely general and can straightforwardly apply
to other languages, treebanks and different tasks.
6
(tree1) TOP
PP
IN
0B1
?in?
NP
NP
DEF
1H2
?the?
NP
NN
2CL3
?shadow?
PP
POSS
3FL4
of
PRN
4HM5
?them?
ADJP
DEF
5H6
?the?
JJ
6NEIM7
?pleasant?
(tree2) TOP
PP
IN
0B1
?in?
NP
NP
NN
1CL2
?shadow?
PP
POSS
2FL3
?of?
PRN
3HM4
?them?
VB
4HNEIM5
?made-pleasant?
Figure 1: A correct tree (tree1) and an incorrect tree (tree2) for ?BCLM HNEIM?, indexed by terminal boundaries.
Erroneous nodes in the parse hypothesis are marked in italics. Missing nodes from the hypothesis are marked in bold.
2 The Challenge: Evaluation for MRLs
In morphologically rich languages (MRLs) substan-
tial information about the grammatical relations be-
tween entities is expressed at word level using in-
flectional affixes. In particular, in MRLs such as He-
brew, Arabic, Turkish or Maltese, elements such as
determiners, definite articles and conjunction mark-
ers appear as affixes that are appended to an open-
class word. Take, for example the Hebrew word-
token BCLM,1 which means ?in their shadow?. This
word corresponds to five distinctly tagged elements:
B (?in?/IN), H (?the?/DEF), CL (?shadow?/NN), FL
(?of?/POSS), HM (?they?/PRN). Note that morpho-
logical segmentation is not the inverse of concatena-
tion. For instance, the overt definite article H and
the possessor FL show up only in the analysis.
The correct parse for the Hebrew phrase ?BCLM
HNEIM? is shown in Figure 1 (tree1), and it pre-
supposes that these segments can be identified and
assigned the correct PoS tags. However, morpholog-
ical segmentation is non-trivial due to massive word-
level ambiguity. The word BCLM, for instance, can
be segmented into the noun BCL (?onion?) and M (a
genitive suffix, ?of them?), or into the prefix B (?in?)
followed by the noun CLM (?image?).2 The multi-
tude of morphological analyses may be encoded in a
lattice structure, as illustrated in Figure 2.
1We use the Hebrew transliteration in Sima?an et al (2001).
2The complete set of analyses for this word is provided in
Goldberg and Tsarfaty (2008). Examples for similar phenom-
ena in Arabic may be found in Green and Manning (2010).
Figure 2: The morphological segmentation possibilities
of BCLM HNEIM. Double-circles are word boundaries.
In practice, a statistical component is required to
decide on the correct morphological segmentation,
that is, to pick out the correct path through the lat-
tice. This may be done based on linear local context
(Adler and Elhadad, 2006; Shacham and Wintner,
2007; Bar-haim et al, 2008; Habash and Rambow,
2005), or jointly with parsing (Tsarfaty, 2006; Gold-
berg and Tsarfaty, 2008; Green and Manning, 2010).
Either way, an incorrect morphological segmenta-
tion hypothesis introduces errors into the parse hy-
pothesis, ultimately providing a parse tree which
spans a different yield than the gold terminals. In
such cases, existing evaluation metrics break down.
To understand why, consider the trees in Figure 1.
Metrics like PARSEVAL (Black et al, 1991) cal-
culate the harmonic means of precision and recall
on labeled spans ?i, label, j? where i, j are termi-
nal boundaries. Now, the NP dominating ?shadow
of them? has been identified and labeled correctly
in tree2, but in tree1 it spans ?2,NP, 5? and in tree2
it spans ?1,NP, 4?. This node will then be counted
as an error for tree2, along with its dominated and
dominating structure, and PARSEVAL will score 0.
7
A generalized version of PARSEVAL which con-
siders i, j character-based indices instead of termi-
nal boundaries (Tsarfaty, 2006) will fail here too,
since the missing overt definite article H will cause
similar misalignments. Metrics for dependency-
based evaluation such as ATTACHMENT SCORES
(Buchholz and Marsi, 2006) suffer from similar
problems, since they assume that both trees have the
same nodes ? an assumption that breaks down in
the case of incorrect morphological segmentation.
Although great advances have been made in pars-
ing MRLs in recent years, this evaluation challenge
remained unsolved.3 In this paper we present a solu-
tion to this challenge by extending TEDEVAL (Tsar-
faty et al, 2011) for handling trees over lattices.
3 The Proposal: Distance-Based Metrics
Input and Output Spaces We view the joint task
as a structured prediction function h : X ? Y from
input space X onto output space Y . Each element
x ? X is a sequence x = w1, . . . , wn of space-
delimited words from a setW . We assume a lexicon
LEX, distinct fromW , containing pairs of segments
drawn from a set T of terminals and PoS categories
drawn from a set N of nonterminals.
LEX = {?s, p?|s ? T , p ? N}
Each word wi in the input may admit multiple
morphological analyses, constrained by a language-
specific morphological analyzer MA. The morpho-
logical analysis of an input word MA(wi) can be
represented as a lattice Li in which every arc cor-
responds to a lexicon entry ?s, p?. The morpholog-
ical analysis of an input sentence x is then a lattice
L obtained through the concatenation of the lattices
L1, . . . , Ln where MA(w1) = L1, . . . , MA(wn) =
Ln. Now, let x = w1, . . . , wn be a sentence with
a morphological analysis lattice MA(x) = L. We
define the output space YMA(x)=L for h (abbreviated
YL), as the set of linearly-ordered labeled trees such
that the yield of LEX entries ?s1, p1?,. . . ,?sk, pk? in
each tree (where si ? T and pi ? N , and possibly
k 6= n) corresponds to a path through the lattice L.
3A tool that could potentially apply here is SParseval (Roark
et al, 2006). But since it does not respect word-boundaries, it
fails to apply to such lattices. Cohen and Smith (2007) aimed to
fix this, but in their implementation syntactic nodes internal to
word boundaries may be lost without scoring.
Edit Scripts and Edit Costs We assume a
set A={ADD(c, i, j),DEL(c, i, j),ADD(?s, p?, i, j),
DEL(?s, p?, i, j)} of edit operations which can add
or delete a labeled node c ? N or an entry ?s, p? ?
LEX which spans the states i, j in the lattice L. The
operations in A are properly constrained by the lat-
tice, that is, we can only add and delete lexemes that
belong to LEX, and we can only add and delete them
where they can occur in the lattice. We assume a
function C(a) = 1 assigning a unit cost to every op-
eration a ? A, and define the cost of a sequence
?a1, . . . , am? as the sum of the costs of all opera-
tions in the sequence C(?a1, ..., am?) =
?m
i=1 C(ai).
An edit script ES(y1, y2) = ?a1, . . . , am? is a se-
quence of operations that turns y1 into y2. The tree-
edit distance is the minimum cost of any edit script
that turns y1 into y2 (Bille, 2005).
TED(y1, y2) = min
ES(y1,y2)
C(ES(y1, y2))
Distance-Based Metrics The error of a predicted
structure p with respect to a gold structure g is now
taken to be the TED cost, and we can turn it into a
score by normalizing it and subtracting from a unity:
TEDEVAL(p, g) = 1?
TED(p, g)
|p|+ |g| ? 2
The term |p| + |g| ? 2 is a normalization factor de-
fined in terms of the worst-case scenario, in which
the parser has only made incorrect decisions. We
would need to delete all lexemes and nodes in p and
add all the lexemes and nodes of g, except for roots.
An Example Both trees in Figure 1 are contained
in YL for the lattice L in Figure 2. If we re-
place terminal boundaries with lattice indices from
Figure 2, we need 6 edit operations to turn tree2
into tree1 (deleting the nodes in italic, adding the
nodes in bold) and the evaluation score will be
TEDEVAL(tree2,tree1) = 1? 614+10?2 = 0.7273.
4 Experiments
We aim to evaluate state-of-the-art parsing architec-
tures on the morphosyntactic disambiguation of He-
brew texts in three different parsing scenarios: (i)
Gold: assuming gold segmentation and PoS-tags,
(ii) Predicted: assuming only gold segmentation,
and (iii) Raw: assuming unanalyzed input text.
8
SEGEVAL PARSEVAL TEDEVAL
Gold PS U: 100.00 U: 94.35
L: 100.00 L: 88.75 L: 93.39
Predicted PS U: 100.00 U: 92.92
L: 90.85 L: 82.30 L: 86:26
Raw PS U: 96.42 U: 88.47
L: 84.54 N/A L: 80.67
Gold RR U: 100.00 U: 94.34
L: 100.00 L: 83.93 L: 92.45
Predicted RR U: 100.00 U: 92.82
L: 91.69 L: 78.93 L: 85.83
Raw RR U: 96.03 U: 87.96
L: 86.10 N/A L: 79.46
Table 1: Phrase-Structure based results for the Berke-
ley Parser trained on bare-bone trees (PS) and relational-
realizational trees (RR). We parse all sentences in the dev
set. RR extra decoration is removed prior to evaluation.
SEGEVAL ATTSCORES TEDEVAL
Gold MP 100.00 U: 83.59 U: 91.76
Predicted MP 100.00 U: 82.00 U: 91.20
Raw MP 95.07 N/A U: 87.03
Gold EF 100.00 U: 84.68 U: 92.25
Predicted EF 100.00 U: 83.97 U: 92:02
Raw EF 95.07 N/A U: 87.75
Table 2: Dependency parsing results by MaltParser (MP)
and EasyFirst (EF), trained on the treebank converted into
unlabeled dependencies, and parsing the entire dev-set.
For constituency-based parsing we use two mod-
els trained by the Berkeley parser (Petrov et al,
2006) one on phrase-structure (PS) trees and one
on relational-realizational (RR) trees (Tsarfaty and
Sima?an, 2008). In the raw scenario we let a lattice-
based parser choose its own segmentation and tags
(Goldberg, 2011b). For dependency parsing we use
MaltParser (Nivre et al, 2007b) optimized for He-
brew by Ballesteros and Nivre (2012), and the Easy-
First parser of Goldberg and Elhadad (2010) with the
features therein. Since these parsers cannot choose
their own tags, automatically predicted segments
and tags are provided by Adler and Elhadad (2006).
We use the standard split of the Hebrew tree-
bank (Sima?an et al, 2001) and its conversion into
unlabeled dependencies (Goldberg, 2011a). We
use PARSEVAL for evaluating phrase-structure trees,
ATTACHSCORES for evaluating dependency trees,
and TEDEVAL for evaluating all trees in all scenar-
ios. We implement SEGEVAL for evaluating seg-
mentation based on our TEDEVAL implementation,
replacing the tree distance and size with string terms.
Table 1 shows the constituency-based parsing re-
sults for all scenarios. All of our results confirm
that gold information leads to much higher scores.
TEDEVAL allows us to precisely quantify the drop
in accuracy from gold to predicted (as in PARSE-
VAL) and than from predicted to raw on a single
scale. TEDEVAL further allows us to scrutinize the
contribution of different sorts of information. Unla-
beled TEDEVAL shows a greater drop when moving
from predicted to raw than from gold to predicted,
and for labeled TEDEVAL it is the other way round.
This demonstrates the great importance of gold tags
which provide morphologically disambiguated in-
formation for identifying phrase content.
Table 2 shows that dependency parsing results
confirm the same trends, but we see a much smaller
drop when moving from gold to predicted. This is
due to the fact that we train the parsers for predicted
on a treebank containing predicted tags. There is
however a great drop when moving from predicted
to raw, which confirms that evaluation benchmarks
on gold input as in Nivre et al (2007a) do not pro-
vide a realistic indication of parser performance.
For all tables, TEDEVAL results are on a simi-
lar scale. However, results are not yet comparable
across parsers. RR trees are flatter than bare-bone
PS trees. PS and DEP trees have different label
sets. Cross-framework evaluation may be conducted
by combining this metric with the cross-framework
protocol of Tsarfaty et al (2012).
5 Conclusion
We presented distance-based metrics defined for
trees over lattices and applied them to evaluating
parsers on joint morphological and syntactic dis-
ambiguation. Our contribution is both technical,
providing an evaluation tool that can be straight-
forwardly applied for parsing scenarios involving
trees over lattices,4 and methodological, suggesting
to evaluate parsers in all possible scenarios in order
to get a realistic indication of parser performance.
Acknowledgements
We thank Shay Cohen, Yoav Goldberg and Spence
Green for discussion of this challenge. This work
was supported by the Swedish Science Council.
4The tool can be downloaded http://stp.ling.uu.
se/?tsarfaty/unipar/index.html
9
References
Meni Adler and Michael Elhadad. 2006. An unsuper-
vised morpheme-based HMM for Hebrew morpholog-
ical disambiguation. In Proceedings of COLING-ACL.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: A system for MaltParser optimization. Istan-
bul.
Roy Bar-haim, Khalil Sima?an, and Yoad Winter. 2008.
Part-of-speech tagging of Modern Hebrew text. Natu-
ral Language Engineering, 14(2):223?251.
Philip Bille. 2005. A survey on tree-edit distance
and related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Claudia
Gdaniec, Ralph Grishman, P. Harrison, Donald Hin-
dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-
vans, Mark Liberman, Mitchell P. Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
1991. A procedure for quantitatively comparing the
syntactic coverage of English grammars. In Proceed-
ings of the DARPA Workshop on Speech and Natural
Language.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL, pages 208?217.
Yoav Goldberg and Michael Elhadad. 2010. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of NAACL/HLT workshop on Statistical Parsing
of Morphologically Rich Languages.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL.
Yoav Goldberg. 2011a. Automatic Syntactic Processing
of Modern Hebrew. Ph.D. thesis, Ben-Gurion Univer-
sity of the Negev.
Yoav Goldberg. 2011b. Joint morphological segmen-
tation and syntactic parsing using a PCFGLA lattice
parser. In Proceedings of ACL.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Brian Roark, Mary Harper, Eugene Charniak, Bon-
nie Dorr C, Mark Johnson D, Jeremy G. Kahn
E, Yang Liu F, Mari Ostendorf E, John Hale
H, Anna Krasnyanskaya I, Matthew Lease D,
Izhak Shafran J, Matthew Snover C, Robin Stewart K,
and Lisa Yung J. 2006. Sparseval: Evaluation metrics
for parsing speech. In Proceesings of LREC.
Danny Shacham and Shuly Wintner. 2007. Morpholog-
ical disambiguation of Hebrew: A case study in clas-
sifier combination. In Proceedings of the 2007 Joint
Conference of EMNLP-CoNLL, pages pages 439?447.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, San-
dra Kuebler, Marie Candito, Jennifer Foster, Yan-
nick Versley, Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing for morphologically rich language
(SPMRL): What, how and whither. In Proceedings of
the first workshop on Statistical Parsing of Morpho-
logically Rich Languages (SPMRL) at NA-ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In Proceedings of EACL.
Reut Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
10
