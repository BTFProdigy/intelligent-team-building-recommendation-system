Using Roget?s Thesaurus for Fine-grained Emotion Recognition 
Saima Aman 
School of Information Technology 
and Engineering 
University of Ottawa, Ottawa, Canada 
 
 
saman071@site.uottawa.ca 
Stan Szpakowicz 
School of Information Technology 
and Engineering 
University of Ottawa, Ottawa, Canada 
ICS, Polish Academy of Sciences 
Warszawa, Poland 
szpak@site.uottawa.ca 
 
 
Abstract 
Recognizing the emotive meaning of text 
can add another dimension to the under-
standing of text. We study the task of 
automatically categorizing sentences in a 
text into Ekman?s six basic emotion cate-
gories. We experiment with corpus-based 
features as well as features derived from 
two emotion lexicons. One lexicon is 
automatically built using the classification 
system of Roget?s Thesaurus, while the 
other consists of words extracted from 
WordNet-Affect. Experiments on the data 
obtained from blogs show that a combina-
tion of corpus-based unigram features with 
emotion-related features provides superior 
classification performance. We achieve F-
measure values that outperform the rule-
based baseline method for all emotion 
classes. 
1 Introduction 
Recognizing emotions conveyed by a text can pro-
vide an insight into the author?s intent and senti-
ment, and can lead to better understanding of the 
text?s content. Emotion recognition in text has re-
cently attracted increased attention of the NLP 
community (Alm et al, 2005; Liu et al 2003; Mi-
halcea and Liu, 2006); it is also one of the tasks at 
Semeval-20071.  
Automatic recognition of emotions can be ap-
plied in the development of affective interfaces for 
                                                
1 Affective Text: Semeval Task at the 4th International Work-
shop on Semantic Evaluations, 2007, Prague 
(nlp.cs.swarthmore.edu/semeval/tasks/task14/summary.shtml).  
Computer-Mediated Communication and Human-
Computer Interaction. Other areas that can poten-
tially benefit from automatic emotion analysis are 
personality modeling and profiling (Liu and Maes, 
2004), affective interfaces and communication sys-
tems (Liu et al 2003; Neviarouskaya et al, 2007a) 
consumer feedback analysis, affective tutoring in 
e-learning systems (Zhang et al, 2006), and text-
to-speech synthesis (Alm et al, 2005). 
In this study, we address the task of automati-
cally assigning an emotion label to each sentence 
in the given dataset, indicating the predominant 
emotion type expressed in the sentence. The possi-
ble labels are happiness, sadness, anger, disgust, 
surprise, fear and no-emotion. Those are Ekman?s 
(1992) six basic emotion categories, and an addi-
tional label to account for the absence of a clearly 
discernible emotion. 
We experiment with two types of features for 
representing text in emotion classification based on 
machine learning (ML). Features of the first type 
are a corpus-based unigram representation of text. 
Features of the second type comprise words that 
appear in emotion lexicons. One such lexicon con-
sists of words that we automatically extracted from 
Roget?s Thesaurus (1852). We chose words for 
their semantic similarity to a basic set of terms that 
represent each emotion category. Another lexicon 
builds on lists of words for each emotion category, 
extracted from WordNet-Affect (Strapparava and 
Valitutti, 2004). 
We compare the classification results for groups 
of features of these two types. We get good results 
when the features are combined in a series of ML 
experiments. 
312
2 Related Work 
Research in emotion recognition has focused on 
discerning emotions along the dimensions of va-
lence (positive / negative) and arousal (calm / ex-
cited), and on recognizing distinct emotion catego-
ries. We focus on the latter.  
Liu et al (2003) use a real-world commonsense 
knowledge base to classify sentences into Ekman?s 
(1992) basic emotion categories. They use an en-
semble of rule-based affect models to determine 
the emotional affinity of individual sentences. 
Neviarouskaya et al (2007b) also use rules to de-
termine the emotions in sentences in blog posts; 
their analysis relies on a manually prepared data-
base of words, abbreviations and emoticons la-
beled with emotion categories. 
Since these papers do not report conventional 
performance metrics such as precision and recall, 
the effectiveness of their methods cannot be judged 
empirically. They also disregard statistical learning 
methods as ineffective for emotion recognition at 
sentence level. They surmise that the small size of 
the text input (a sentence) gives insufficient data 
for statistical analysis, and that statistical methods 
cannot handle negation. In this paper, we show that 
ML-based approach with the appropriate combina-
tion of features can be applied to distinguishing 
emotions in text. 
Previous work has used lexical resources such as 
WordNet to automatically acquire emotion-related 
words for emotion classification experiments. 
Starting from a set of primary emotion adjectives, 
Alm et al (2005) retrieve similar words from 
WordNet utilizing all senses of all words in the 
synsets that contain the adjectives. They also ex-
ploit the synonym and hyponym relations in 
WordNet to manually find words similar to nomi-
nal emotion words. Kamps and Marx (2002) use 
WordNet?s synset relations to determine the affec-
tive meaning of words. They assign multi-
dimensional scores to individual words based on 
the minimum path length between them and a pair 
of polar words (such as ?good? and ?bad?) in 
WordNet?s structure. 
There is also a corpus-driven method of deter-
mining the emotional affinity of words: learn prob-
abilistic affective scores of words from large cor-
pora. Mihalcea and Liu (2006) have used this 
method to assign a happiness factor to words de-
pending on the frequency of their occurrences in 
happy-labeled blogposts compared to their total 
frequency in the corpus. 
In this paper, we study a new approach to auto-
matically acquiring a wide variety of words that 
express emotions or emotion-related concepts, us-
ing Roget?s Thesaurus (1852). 
3 Emotion-Labeled Data 
We have based our study on data collected from 
blogs. We chose blogs as data source because they 
are potentially rich in emotion content, and contain 
good examples of real-world instances of emotions 
expressed in text. Additionally, text in blogs does 
not conform to the style of any particular genre per 
se, and thus offers a variety in writing styles, 
choice and combination of words, as well as topics. 
So, the methods learned for discerning emotion 
using blog data are quite general and therefore 
applicable to a variety of genres rather than to 
blogs only. 
We retrieved blogs using seed words for all 
emotion categories. Four human judges manually 
annotated the blog posts with emotion-related 
information - every sentence received two 
judgments. The annotators were required to mark 
each sentence with one of the eight labels: 
happiness, sadness, anger, disgust, surprise, fear, 
mixed-emotion, and no-emotion. The mixed-
emotion label was included to handle those 
sentences that had more than one type of emotion 
or whose emotion content could not fit into any of 
the given emotion categories. Sample sentences 
from the annotated corpus are shown in Fig. 1. 
We measured the inter-annotator agreement us-
ing Cohen?s (1960) kappa. The average pair-wise 
agreement for different emotion categories ranged 
from 0.6 to 0.79. In the experiments reported in 
this paper, we use only those sentences for which 
there was agreement between both judgments (to 
form a benchmark for the evaluation of the results 
of automatic classification). The distribution of 
emotion categories in the corpus used in our ex-
periments is shown in Table 1. 
313
 
Emotion Class Number of sentences 
Happiness 536 
Sadness 173 
Anger 179 
Disgust 172 
Surprise 115 
Fear 115 
No-emotion 600 
Table 1. Distribution of emotion classes 
4 A Baseline Approach 
We are interested in investigating if emotion in text 
can be discerned on the basis of its lexical content. 
A na?ve approach to determining the emotional 
orientation of text is to look for obvious emotion 
words, such as ?happy?, ?afraid? or ?astonished?. 
The presence of one or more words of a particular 
emotion category in a sentence provides a good 
premise for interpreting the overall emotion of the 
sentence. This approach relies on a list of words 
with prior information about their emotion type, 
and uses it for sentence-level classification. The 
obvious advantage is that no training data are re-
quired. 
For evaluation purposes, we took this approach 
to develop a baseline system that counts the num-
ber of emotion words of each category in a sen-
tence, and then assigns this sentence the category 
with the largest number of words. Ties were re-
solved by choosing the emotion label according to 
an arbitrarily predefined ordering of emotion 
classes. A sentence containing no emotion word of 
any type was assigned the no emotion category. 
This system worked with word lists 2  extracted 
                                                
2  Emotion words from WordNet-Affect 
(http://www.cse.unt.edu/~rada/affectivetext/data/WordNet
AffectEmotionLists.tar.gz)  
from WordNet-Affect (Strapparava and Valitutti, 
2004) for six basic emotion categories. 
Table 2 shows the precision, recall, and F-
measure values for the baseline system. As we 
have seven classes in our experiments, the class 
imbalance makes accuracy values less relevant 
than precision, recall and F-measure. That is why 
we do not report accuracy values in our results. 
The baseline system shows precision values 
above 50% for all but two classes. This shows the 
usefulness of this approach. This method, however, 
fails in the absence of obvious emotion words in 
the sentence, as indicated by low recall values. 
Thus, in order to improve recall, we need to in-
crease the ambit of words that are considered emo-
tion-related. An alternative approach is to use ML 
to learn automatically rules that classify emotion in 
text. 
 
Class Precision Recall F-Measure 
Happiness 0.589 0.390 0.469 
Sadness 0.527 0.283 0.368 
Anger 0.681 0.262 0.379 
Disgust 0.944 0.099 0.179 
Surprise 0.318 0.296 0.306 
Fear 0.824 0.365 0.506 
No-emotion 0.434 0.867 0.579 
Table 2. Performance metrics of the base-
line system 
5 Approach Based on Machine Learning  
We study two types of features: corpus-based fea-
tures and features based on emotion lexicons. 
5.1 Corpus-based features 
The corpus-based features exploit the statistical 
characteristics of the data on the basis of the n-
gram distribution. In our experiments, we take uni-
grams (n=1) as features. Unigram models have 
been previously shown to give good results in sen-
timent classification tasks (Kennedy and Inkpen, 
2006; Pang et al, 2002): unigram representations 
can capture a variety of lexical combinations and 
distributions, including those of emotion words. 
This is particularly important in the case of blogs, 
whose language is often characterized by frequent 
use of new words, acronyms (such as ?lol?), ono-
matopoeic words (?haha?, ?grrr?), and slang, most 
of which can be captured in a unigram representa-
This was the best summer I have 
ever experienced.  (happiness) 
I don?t feel like I ever have that 
kind of privacy where I can talk 
to God and cry and figure things 
out. (sadness) 
Finally, I got fed up. (disgust) 
I can?t believe she is finally here! (surprise) 
Fig 1. Sample sentences from the corpus 
314
tion. Another advantage of a unigram representa-
tion is that it does not require any prior knowledge 
about the data under investigation or the classes to 
be identified. 
For our experiments, we selected all unigrams 
that occur more than three times in the corpus. This 
eliminates rare words, as well as foreign-language 
words and spelling mistakes, which are quite 
common in blogs. We also excluded words that 
occur in a list of stopwords - primarily function 
words that do not generally have emotional conno-
tations. We used the SMART list of stopword3, 
with minor modifications. For instance, we re-
moved from the stop list words such as ?what? and 
?why?, which may be used in the context of ex-
pressing surprise. 
5.2 Features derived from Roget?s Thesaurus 
We utilized Roget?s Thesaurus (Jarmasz and 
Szpakowicz, 2001) to automatically build a lexicon 
                                                
3 SMART stopwords list. Used with the SMART information 
retrieval system at Cornell University 
(ftp://ftp.cs.cornell.edu/pub/smart/english.stop) 
of emotion-related words. The features based on an 
emotion lexiconrequire prior knowledge about 
emotion relatedness of words. We extracted this 
knowledge from the classification system in 
Roget?s, which groups related concepts into vari-
ous levels of a hierarchy. For a detailed account of 
this classification structure, see Jarmasz and 
Szpakowicz (2001). 
Roget?s structure allows the calculation of se-
mantic relatedness between words, based on the 
path length between the nodes in the structure that 
represent those words. In case of multiple paths, 
the shortest path is considered. Jarmasz and Szpak-
owicz (2004) have introduced a similarity measure 
derived from path length, which assigns scores 
ranging from a maximum of 16 to most semanti-
cally related words to a minimum of 0 to least 
related words. They have shown that on semantic 
similarity tests this measure outperforms several 
other methods. 
To build a lexicon of emotion-related words utiliz-
ing Roget?s structure, we need first to make two 
decisions: select a primary set of emotion words 
starting with which we can extract other similar 
Similarity 
Score 
Happiness Sadness Anger Disgust Surprise Fear 
16 
family, home, 
friends, life, 
house, loving, 
partying, bed, 
pleasure, rest, 
close, event, 
lucks, times 
crying, lost, 
wounds, bad, 
pills, falling, 
messed, spot, 
unhappy, 
pass, black, 
events, hurts, 
shocked 
pride, fits, 
stormed, 
abandoned, 
bothered, 
mental, an-
ger, feelings, 
distractions 
shock, dis-
gust, dislike, 
loathing 
plans, catch, 
expected, 
early, slid, 
slipped, ear-
lier, caught, 
act 
nervous, cry, 
terror, panic, 
feelings, run, 
fog, fire, turn, 
police, faith, 
battle, war, 
sounds 
14 
love, like, 
feel, pretty, 
lovely, better, 
smiling, nice, 
beautiful, 
hope, cutest 
celebrations, 
warm, desires 
ill, bored, 
feeling, ruin, 
blow, down, 
wrong, awful, 
evil, worry, 
crushing, 
bug, death, 
trouble, dark 
hate, burn, 
upset, dislike, 
wrong, blood, 
ill, flaws, bar, 
defects, bit-
ter, growled, 
black, slow 
hate, pain, 
horrifying, ill, 
pills, sad, 
wear, blood, 
appalling, 
end, work, 
weighed, 
regrets, bad 
left, swing, 
noticed, 
worry, times, 
amazing, 
stolen, break, 
interesting, 
attention 
falling, life, 
stunned, pay, 
broken, hate, 
blast, times, 
hanging, 
hope, broken, 
blood, blue 
12 
gift, treats, 
adorable, fun, 
hug, kidding, 
bigger, great, 
lighting, won, 
stars, enjoy, 
favourite, 
social, divine 
defeat, nasty, 
boring, ugly, 
loser, end, 
victim, sick, 
hard, serious, 
aggravating, 
bothering, 
burning 
lose, throw, 
offended, hit, 
power, feel, 
flaring, pills, 
broken, life, 
forgot, rant-
ing 
feel, fun, lies, 
drawn, lose, 
missed, de-
prived, lack, 
sighs, defeat, 
down, hurt, 
tears, insulted 
realize, pick, 
wake, sense, 
jumped, new, 
late, magic, 
omen, forget, 
popped, feel, 
question, late, 
throw 
fearful, spy, 
night, upset, 
feel, chased, 
hazardous, 
tomorrow, 
victim, grim, 
terrorists, 
apprehensive 
Table 3. Emotion-related words automatically extracted from Roget?s Thesaurus 
315
words, and choose an appropriate similarity score 
to serve as cutoff for determining semantic relat-
edness between words. 
The primary set of words that we selected con-
sists of one word for each emotion category, repre-
senting the base form of the name of the category: 
{happy, sad, anger, disgust, surprise, fear}. 
Experiments performed on Miller and Charles 
similarity data (1991), reported in Jarmasz and 
Szpakowicz (2004), have shown that pairs of 
words with a semantic similarity value of 16 have 
high similarity, while those with a score of 12 to 
14 have intermediate similarity. Therefore, we se-
lect the score of 12 as cutoff, and include in the 
lexicon all words that have similarity scores of 12 
or higher with respect to the words in the primary 
set. This selection of cutoff therefore serves as a 
form of feature selection. In Table 3, we present 
sample words from the lexicon with similarity 
scores of 16, 14, and 12 for each emotion category. 
These words represent three different levels of re-
latedness to each emotion category. We are able to 
identify a large variety of emotion-related words 
belonging to different parts of speech that go well 
beyond the stereotypical words associated with 
different emotions. We particularly note some ge-
neric neutral words, such as ?feel?, ?life?, and 
?times? associated with many emotion categories, 
indicating their conceptual relevance to emotions. 
5.3 Features derived from WordNet-Affect 
WordNet-Affect is an affective lexical resource that 
assigns a variety of affect-related labels to a subset 
Model Class Precision Recall F-Measure Baseline F-Measure 
Happiness 0.840 0.675 0.740 0.469 
Sadness 0.619 0.301 0.405 0.368 
Anger 0.634 0.358 0.457 0.379 
Disgust 0.772 0.453 0.571 0.179 
Surprise 0.813 0.339 0.479 0.306 
Fear 0.889 0.487 0.629 0.506 
Unigrams 
No-emotion 0.581 0.342 0.431 0.579 
Happiness 0.772 0.562 0.650 0.469 
Sadness 0.574 0.225 0.324 0.368 
Anger 0.638 0.246 0.355 0.379 
Disgust 0.729 0.297 0.421 0.179 
Surprise 0.778 0.243 0.371 0.306 
Fear 0.857 0.470 0.607 0.506 
Roget?s Thesaurus 
(RT) Features 
No-emotion 0.498 0.258 0.340 0.579 
Happiness 0.809 0.705 0.754 0.469 
Sadness 0.577 0.370 0.451 0.368 
Anger 0.636 0.419 0.505 0.379 
Disgust 0.686 0.471 0.559 0.179 
Surprise 0.717 0.374 0.491 0.306 
Fear 0.831 0.513 0.634 0.506 
Unigrams + 
RT Features 
No-emotion 0.586 0.512 0.546 0.579 
Happiness 0.813 0.698 0.751 0.469 
Sadness 0.605 0.416 0.493 0.368 
Anger 0.650 0.436 0.522 0.379 
Disgust 0.672 0.488 0.566 0.179 
Surprise 0.723 0.409 0.522 0.306 
Fear 0.868 0.513 0.645 0.506 
Unigrams + 
RT Features + 
WNA Features 
No-emotion 0.587 0.625 0.605 0.579 
* Highest precision, recall, and F-measure values for each class are shown in bold 
Table 4 ML Classification Results 
316
of WordNet synsets comprising affective concepts. 
We used lists of words extracted from it for each of 
the six emotion categories. 
6 Experiments and Results 
We train classifiers with unigram features for each 
emotion class using Support Vector Machine 
(SVM) for predicting the emotion category of the 
sentences in our corpus. SVM has been shown to 
be useful for text classification tasks (Joachims, 
1998), and has previously given good performance 
in sentiment classification experiments (Kennedy 
and Inkpen, 2006; Mullen and Collier, 2004; Pang 
and Lee, 2004; Pang et al, 2002). In Table 4, we 
report results from ten-fold cross-validation ex-
periments conducted using the SMO implementa-
tion of SVM in Weka (Witten and Frank, 2005). In 
each experiment, we represent a sentence by a vec-
tor indicating the number of times each feature oc-
curs. 
In the first experiment, we use only corpus-
based unigram features. We obtain high precision 
values for all emotion classes (as shown in Table 
4), and the recall and F-measure values surpass 
baseline values for all classes except no-emotion. 
This validates our premise that unigrams can help 
learn lexical distributions well to accurately predict 
emotion categories. 
Next, we use as features all words in the emo-
tion lexicon acquired from Roget?s Thesaurus 
(RT). The F-measure scores beat the baseline for 
four out of seven classes. When we combine both 
corpus-based unigrams with RT features, we can 
increase recall values across all seven classes. 
Finally, we add features from WordNet-Affect 
to the feature set containing corpus unigrams and 
RT features. This leads to further improvement in 
overall performance. Combining all features, we 
achieve highest recall values across all but one 
class. The resulting F-measure values (ranging 
from 0.493 to 0.751) surpass the baseline values 
across all seven classes. This increase was found to 
be statistically significant (paired t-test, p=0.05). 
7 Discussion 
We observe that corpus-based features and emo-
tion-related features together contribute to im-
proved performance, better than given by any one 
type of feature group alone. 
Any automatic way of recognizing emotion 
should inevitably take into account a wide variety 
of words that are semantically connected to emo-
tions. While some words are obviously affective, 
many more are only potentially affective. The lat-
ter derive their affective property from their asso-
ciations with emotional concepts. For instance, 
words like ?family?, ?friends?, ?home? are not in-
herently emotional, but because of their well-
known semantic association with emotion con-
cepts, their presence in a sentence can be taken as 
an indicator of emotion expression in the sentence. 
We can interpret the results as indicators of how 
much correlation the classifiers can find between 
the features and the predicted class. Considering 
our best results using all features, we find that this 
correlation is highest for the ?happy? class, indi-
cated by a precision of 0.813 and recall of 0.698, 
the highest among all classes. We can therefore 
conclude that it is easier to discern happiness in 
text than Ekman?s other basic emotions.  
8 Conclusions 
Working on a corpus of blog sentences anno-
tated with emotion labels, we were able to demon-
strate that a combination of corpus-based unigram 
features and features derived from emotion lexi-
cons can help automatically distinguish basic emo-
tion categories in written text. When used together 
in an SVM-based learning environment, these fea-
tures increased recall in all cases and the resulting 
F-measure values significantly surpassed the base-
line scores for all emotion categories. 
In addition, we described a method of building 
an emotion lexicon derived from Roget?s Thesau-
rus on the basis of semantic relatedness of words 
to a set of basic emotion words for each emotion 
category. The effectiveness of this emotion lexicon 
was demonstrated in the emotion classification 
tasks. 
 
References 
Cecilia O. Alm, Dan Roth, and Richard Sproat, Emo-
tions from text: machine learning for text-based emo-
tion prediction. In Proceedings of Joint Conference 
on HLT/EMNLP, pages 579-586, Vancouver, Can-
ada, Oct 2005. 
M.M. Bradley and P.J. Lang, Affective norms for Eng-
lish words (ANEW): Instruction manual and affective 
317
ratings, Technical Report C-1, The Center for Re-
search in Psychophysiology, University of Florida, 
1999. 
J. Cohen, A coefficient of agreement for nominal scales, 
Educational and Psychological Measurement, 1960, 
20 (1): 37?46. 
Paul Ekman, An Argument for Basic Emotions, Cogni-
tion and Emotion, 6, 1992, 169-200. 
Mario Jarmasz and Stan Szpakowicz, The Design and 
Implementation of an Electronic Lexical Knowledge 
Base. In Proceedings of the 14th Biennial Confer-
ence of the Canadian Society for Computational 
Studies of Intelligence (AI 2001), Ottawa, Canada, 
June 2001, 325-333. 
Mario Jarmasz and Stan Szpakowicz, Roget's Thesaurus 
and Semantic Similarity. N. Nicolov, K. Bontcheva, 
G. Angelova, R. Mitkov (eds.) Recent Advances in 
Natural Language Processing III: Selected Papers 
from RANLP 2003, John Benjamins, Amster-
dam/Philadelphia, Current Issues in Linguistic The-
ory, 260, 2004, 111-120. 
Thorsten Joachims, Text categorization with support 
vector machines: Learning with many relevant fea-
tures. In Proceedings of the European Conference on 
Machine Learning (ECML-98), pages 137?142. 
Jaap Kamps, Maarten Marx, Robert J. Mokken, and 
Marten de Rijke, Words with attitude, In Proceedings 
of the 1st International Conference on Global Word-
Net, pages 332-341, Mysore, India, 2002. 
Alistair Kennedy and Diana Inkpen, Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence 
Shifters. Computational Intelligence, 2006, 
22(2):110-125. 
Hugo Liu, Henry Lieberman, and Ted Selker, A model 
of textual affect sensing using real-world knowledge. 
In Proceedings of the ACM Conference on Intelligent 
User Interfaces, 2003, 125?132. 
Hugo Liu, and P. Maes, What Would They Think? A 
Computational Model of Attitudes. In Proceedings of 
the ACM International Conference on Intelligent 
User Interfaces, IUI 2004, 38-45, ACM Press. 
Rada Mihalcea and Hugo Liu, A corpus-based approach 
to finding happiness, In Proceedings of the AAAI 
Spring Symposium on Computational Approaches for 
Analysis of Weblogs, Stanford, CA, USA, March 
2006. 
G. Miller and W. Charles. Contextual correlates of se-
mantic similarity. Language and Cognitive Proc-
esses, 6(1):1-28, 1991. 
T Mullen and N Collier. Sentiment analysis using sup-
port vector machines with diverse information 
sources. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-2004), 
pages 412?418, Barcelona, Spain. 
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru 
Ishizuka. Analysis of affect expressed through the 
evolving language of online communication. In Pro-
ceedings of the 12th International Conference on In-
telligent User Interfaces (IUI-07), pages 278-281, 
Honolulu, Hawaii, USA, 2007a. 
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru 
Ishizuka, Narrowing the Social Gap among People 
involved in Global Dialog: Automatic Emotion De-
tection in Blog Posts, In Proceedings of the Interna-
tional Conference on Weblogs and Social Media 
(ICWSM 2007), pages 293-294, Boulder, CO, USA, 
March 2007b. 
A. Ortony, G.L. Clore, and A. Collins, The cognitive 
structure of emotions. New York: Cambridge Uni-
versity Press, 1988 
Bo Pang and Lillian Lee, 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL?04), Barcelona, 
Spain, pages 271-278. 
Bo Pang, Lillian Lee, and S. Vaithyanathan, Thumbs 
up? Sentiment classification using machine learning 
techniques, In Proceedings of the 2002 Conference 
on Empirical Methods in Natural Language Process-
ing, Philadelphia, PA, 2002, 79?86. 
Peter Mark Roget, Roget?s Thesaurus of English Words 
and Phrases. Harlow, Essex, England: Longman 
Group Limited, 1852. 
Carlo Strapparava and A Valitutti, WordNet-Affect: an 
affective extension of WordNet. In Proceedings of 
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), Lisbon, 2004, 
1083-1086. 
Ian H. Witten and Eibe Frank. Data Mining: Practical 
Machine Learning Tools and Techniques (2nd ed.), 
Morgan Kaufmann, San Francisco, 2005. 
(www.cs.waikato.ac.nz/ml/weka/) 
L. Zhang, J. Barnden, R. Hendley, and A. Wallington, 
Exploitation in Affect Detection in Open-Ended Im-
provisational Text. In Proceedings of the ACL Work-
shop on Sentiment and Subjectivity in Text, 2006, 
pages 47-54, Sydney, Australia. 
318
Vocabulary Usage in Newswire Summaries 
Terry COPECK 
School of IT and Engineering 
University of Ottawa 
Ottawa, Ontario Canada 
terry@site.uottawa.ca 
Stan SZPAKOWICZ 
School of IT and Engineering 
University of Ottawa 
Ottawa, Ontario Canada 
szpak@site.uottawa.ca 
Abstract 
Analysis of 9000 manually written summaries of 
newswire stories used in four Document Under-
standing Conferences indicates that approximately 
40% of their lexical items do not occur in the source 
document. A further comparison of different sum-
maries of the same document shows agreement on 
28% of their vocabulary. It can be argued that these 
relationships establish a performance ceiling for 
automated summarization systems which do not 
perform syntactic and semantic analysis on the 
source document. 
 
1 Introduction 
Automatic summarization systems rely on manually 
prepared summaries for training data, heuristics and 
evaluation. Generic summaries are notoriously hard 
to standardize; biased summaries, even in a most 
restricted task or application, also tend to vary be-
tween authors. It is unrealistic to expect one perfect 
model summary, and the presence of many, poten-
tially quite diverse, models introduces considerable 
uncertainty into the summarization process. In addi-
tion, many summarization systems tacitly assume 
that model summaries are somehow close to the 
source documents. 
We investigate this assumption, and study the va-
riability of manually produced summaries. We first 
describe the collection of documents with 
summaries which has been accumulated over sev-
eral years of participation in the Document Under-
standing Conference (DUC) evaluation exercises 
sponsored by the National Institute of Science and 
Technology (NIST). We then present our methodol-
ogy, discuss the rather pessimistic results, and fi-
nally draw a few simple conclusions. 
2 The Corpus 
2.1 General Organization 
The authors have assembled a corpus of manually 
written summaries of texts from their archive of ma-
terials provided to participants in the DUC confer-
ences, held annually since 2001. It is available at the 
DUC Web site to readers who are qualified to ac-
cess the DUC document sets on application to 
NIST. To help interested parties assess it for their 
purposes we provide more detail than usual on its 
organization and contents. 
Most summaries in the corpus are abstracts, writ-
ten by human readers of the source document to best 
express its content without restriction in any manner 
save length (words or characters). One method of 
performing automatic summarization is to construct 
the desired amount of output by concatenating rep-
resentative sentences from the source document, 
which reduces the task to one of determining most 
adequately what ?representative? means. Such sum-
maries are called extracts. In 2002, recognizing that 
many participants summarize by extraction, NIST 
produced versions of documents divided into indi-
vidual sentences and asked its author volunteers to 
compose their summaries similarly. Because we use 
a sentence-extraction technique in our summariza-
tion system, this data is of particular interest to us. It 
is not included in the corpus being treated here and 
will be discussed in a separate paper. 
The DUC corpus contains 11,867 files organized 
in a three-level hierarchy of directories totalling 
62MB. The top level identifies the source year and 
exists simply to avoid the name collision which oc-
curs when different years use same-named subdirec-
tories. The middle 291 directories identify the 
document clusters; DUC reuses collections of 
newswire stories assembled for the TREC and TDT 
research in itiatives which report on a common topic 
or theme. Directories on the lowest level contain 
SGML-tagged and untagged versions of 2,781 indi-
vidual source documents, and between one and five 
summaries of each, 9,086 summaries in total. In 
most cases the document involved is just that: a sin-
gle news report originally published in a newspaper. 
552 directories, approximately 20% of the corpus, 
represent multi-document summaries?ones which 
the author has based on all the files in a cluster of 
related documents. For these summaries a source 
document against which to compare them has been 
constructed by concatenating the individual docu-
ments in a cluster into one file. Concatenation is 
done in directory order, though the order of docu-
ments does not matter here. 
2.2 The Corpus in Detail 
The Document Understanding Conference has 
evolved over the four years represented in our cor-
pus, and this is reflected in the materials which are 
available for our purposes. Table 1 classifies these 
files by year and by target size of summary; the 
rightmost column indicates the ratio of summaries 
to source documents, that is, the average number of 
summaries per document. Totals appear in bold. The 
following factors of interest can be identified in its 
data: 
? Size . Initially DUC targeted summaries of 50, 
100 and 200 words. The following year 10-word 
summaries were added, and in 2003 only 10- 
and 100-word summaries were produced; 
? Growth. Despite the high cost of producing 
manual summaries, the number of documents 
under consideration has doubled over the four 
years under study while the number of summa-
ries has tripled; 
? Ratio. On average, three manual summaries are 
available for each source document; 
? Formation. While longer summaries are rou-
tinely composed of well-formed sentences, sub-
sentential constructs such as headlines are ac-
ceptable 10-word summaries, as are lists of key 
words and phrases. 
? Author. Although the 2004 DUC source docu-
ments include machine translations of foreign 
language news stories, in each case a parallel 
human translation was available. Only source 
documents written or translated by human be-
ings appear in the corpus. 
3 The Evaluation Model 
Figure 1 shows the typical contents of a third-level 
source document directory. Relations we wish to 
investigate are marked by arrows. There are two: the 
relationship between the vocabulary used in the 
source document and summaries of it, and that 
among the vocabulary used in summaries them-
selves. The first is marked by white arrows, the sec-
ond by grey.  
The number of document-summary relations in 
the corpus is determined by the larger cardinality set 
involved, which here is the number of summaries: 
thus 9,086 instances. For every document with N 
summaries, we consider all C(N, 2) pairs of summa-
ries. In total there are 11,441 summary-summary 
relationships.  
We ask two questions: to what degree do summa-
ries use words appearing in the source document? 
and, to what degree do different summaries use the 
same vocabulary? 
3.1 Measures 
To answer our two questions we decided to compute 
statistics on two types of elements of each pair of 
test documents: their phrases, and ultimately, their 
 DOCUMENTS   SUMMARIES  D : S 
 10 50 100 200 ?   10 50 100 200 ?   
2001  28 316 56 400   84 946 168 1198 1 : 3 
2002 59 59 626 59 803  116 116 1228 116 1576 1 : 2 
2003 624  90  714  2496  360  2856 1 : 4 
2004 740  124  864  2960  496  3455 1 : 4 
?  1423 87 1156 115 2781  5572 200 3030 284 9086 1 : 3 
Table 1: Number of documents and summaries by size and by year, and ratios 
Source 
Text 
DOCUMENT ? SUMMARY SUMMARY ? SUMMARY 
 
B 
?
??
? ?
?
 
C 
 
A
Figure 1: Files and relationships investigated 
individual tokens. Phrases were extracted by apply-
ing a 987-item stop list developed by the authors 
(Copeck and Szpakowicz 2003) to the test docu-
ments. Each collocation separated by stop words is 
taken as a phrase1. Test documents were tokenized 
by breaking the text on white space and trimming 
off punctuation external to the token. Instances of 
each sort of item were recorded in a hash table and 
written to file.  
Tokens are an obvious and unambiguous baseline 
for lexical agreement, one used by such summary 
evaluation systems as ROUGE (Lin and Hovy, 
2003). On the other hand, it is important to explain 
what we mean by units we call phrases; they should 
not be confused with syntactically correct constitu-
ents such as noun phrases or verb phrases. Our units 
often are not syntactically well-formed. Adjacent 
constituents not separated by a stop word are uni-
fied, single constituents are divided on any embed-
ded stop word, and those composed entirely of stop 
words are simply missed.  
Our phrases, however, are not n-grams. A 10-
word summary has precisely 9 bigrams but, in this 
study, only 3.4 phrases on average (Table 2). On the 
continuum of grammaticality these units can thus be 
seen as lying somewhere between generated blindly 
n-grams and syntactically well-formed phrasal con-
stituents. We judge them to be weakly syntactically 
motivated2 and only roughly analogous to the fac-
toids identified by van Halteren and Teufel (2003) 
in the sense that they also express semantic con-
structs. Where van Halteren and Teufel identified 
factoids in 50 summaries, we sacrificed accuracy for 
automation in order to process 9000. 
We then assessed the degree to which a pair of 
documents for comparison shared vocabulary in 
terms of these units. This was done by counting 
matches between the phrases. Six different kinds of 
match were identified and are listed here in what we 
deem to be decreasing order of stringency. While 
the match types are labelled and described in terms 
of summary and source document for clarity, they 
apply equally to summary pairs. Candidate phrases 
are underlined and matching elements tinted in the 
examples; headings used in the results table (Table 
2) appear in SMALL CAPS. 
                                                 
1 When analysis of a summary indicated that it was a 
list of comma- or semicolon-delimited phrases, the phras-
ing provided by the summary author was adopted, includ-
ing any stopwords present. Turkey attacks Kurds in Iraq, 
warns Syria, accusations fuel tensions, Mubarak inter-
cedes is thus split into four phrases with the first retaining 
the stopword in. There are 453 such summaries. 
2 While the lexical units in question might be more ac-
curately labelled syntactically motivated ngrams, for sim-
plicity we use phrase in the discussion. 
? Exact match. The most demanding, requires 
candidates agree in all respects.  EXACT 
after Mayo Clinic stay ? 
Mayo Clinic group 
? Case-insensitive exact match relaxes the re-
quirement for agreement in case.  EXACT CI 
concerning bilateral relations ? 
Bilateral relations with 
? Head of summary phrase in document re-
quires only that the head of the candidate appear 
in the source document phrase. The head is the 
rightmost word in a phrase.  HEAD DOC 
calls Sharon disaster ? 
deemed tantamount to disaster 
? Head of document phrase in summary is the 
previous test in reverse.  HEAD SUM 
? Summary phrase is substring of document 
phrase. True if the summary phrase appears 
anywhere in the document phrase.  SUB DOC 
has identified Iraqi agent as ? 
the Iraqi agent defection 
? Document phrase is substring of summary 
phrase reverses the previous test. SUB SUM 
Tests for matches between the tokens of two 
documents are more limited because only single 
lexical items are involved. Exact match can be sup-
plemented by case insensitivity and by stemming to 
identify any common root shared by two tokens. 
The Porter stemmer was used. 
The objective of all these tests is to capture any 
sort of meaningful resemblance between the vo-
cabularies employed in two texts. Without question, 
additional measures can and should be identified 
and implemented to correct, expand, and refine the 
analysis. 
3.2 Methodology 
The study was carried out in three stages. A pre-
study determined the ?lie of the land??what the 
general character of results was likely to be, the 
most appropriate methodology to realize them, and 
so on. In particular this initial investigation alerted 
us to the fact that so few phrases in any two texts 
under study matched exactly as to provide little use-
ful data, leading us to add more relaxed measures of 
lexical agreement. This initial investigation made it 
clear that there was no point in attempting to find a 
subset of vocabulary used in a number of summa-
ries?it would be vanishingly small?and we there-
fore confined ourselves in the main study to 
pairwise comparisons. The pre-study also suggested 
that summary size would be a significant factor in 
lexical agreement while source document size 
would be less so, indications which were not en-
tirely borne out by the strength of the results ult i-
mately observed. 
The main study proceeded in two phases. After 
the corpus had been organized as described in Sec-
tion 2 and untagged versions of the source docu-
ments produced for the analysis program to work 
on, that process traversed the directory tree, decom-
posing each text file into its phrases and tokens. 
These were stored in hash tables and written to file 
to provide an audit point on the process. The hash 
tables were then used to test each pair of test docu-
ments for matches?the source document to each 
summary, and all combinations of summaries. The 
resulting counts for all comparisons together with 
other data were then written to a file with results, 
one line per source document in a comma-delimited 
format suitable for importation to a spreadsheet pro-
gram. 
The second phase of the main study involved or-
ganizing the spreadsheet data into a format permit-
ting the calculation of statistics on various cate-
gorizations of documents they describe. Because the 
source document record was variable-length in itself 
and also contained a varying number of variable-
length sub-records of document pair comparisons, 
this was a fairly time-consuming clerical task. It did 
however provide the counts and averages presented 
in Table 2 and subsequently allowed the user to re-
categorize the data fairly easily.  
A post-study was then conducted to validate the 
computation of measures by reporting these to the 
user for individual document sets, and applied to a  
 
  
  
AFA19981230.1000.0058:  X <> W  exact: 2, exactCI: 2, partSum2: 2, partSum1 2, token-
Match: 6 
  X: Jordanian King Hussein to meet with Clinton concerning bilateral relations  
  W: King Hussein to meet with Clinton after visiting Mayo Clinic  
2 exact:  meet,Clinton 
2 exactCI:  meet,clinton 
2 headSum1:  clinton,meet 
2 headSum2:  meet,clinton 
6 tokMatch:  hussein,meet,clinton,to,king,with 
Figure 2: Text and matches for two summaries of AFA19981230.1000.0058 
 DOCUMENT  - SUMMARY 
 SUMMARY  PHRASES  TOKENS 
 
COUNT TOKENS PHRASES  EXACT EXACT CI 
HEAD 
DOC 
HEAD 
SUM 
SUB 
DOC 
SUB 
SUM 
 EXACT STEM CI 
10 5572 10.0 3.4  0.8 1.0 1.4 0.9 2.3 2.7  5.4 6.3 
50 200 47.4 15.5  5.5 5.7 8.8 4.9 11.8 12.0  30.6 32.6 
100 3030 95.6 30.5  12.1 12.5 14.9 10.1 22.3 20.5  52.7 54.8 
200 284 157.5 48.6  19.7 20.4 28.3 17.1 38.4 35.3  82.9 85.8 
ALL 9086 44.0 14.1  5.2 5.5 6.9 8.4 10.3 28.2  24.2 25.5 
              
10     22% 29% 43% 27% 69% 79%  55% 63% 
50     35% 37% 57% 31% 76% 77%  65% 69% 
100     39% 41% 49% 34% 78% 74%  55% 58% 
200     40% 42% 56% 35% 79% 73%  51% 53% 
ALL     37% 39% 49% 33% 73% 70%  55% 58% 
              
 SUMMARY  -  SUMMARY 
              
10 8241 10.0 3.4  0.17 0.21 0.24 0.24    2.82 3.13 
50 141 47.4 15.5  0.71 0.84 1.09 1.06    10.89 11.77 
100 2834 95.6 30.5  4.21 4.39 4.76 4.82    28.16 29.66 
200 225 157.5 48.6  4.26 4.52 6.24 5.93    35.16 37.14 
ALL 11441 44.0 14.1  1.26 1.34 1.5 1.5    9.8 10.48 
              10     5% 6% 7% 7%    28% 31% 
50     5% 5% 7% 7%    23% 25% 
100     14% 14% 16% 16%    29% 31% 
200     9% 9% 13% 12%    22% 24% 
ALL     9% 10% 11% 11%    22% 24% 
Table 2: Counts and percentages of vocabulary agreement, by size and total 
small random sample of text pairs. Figure 2 shows 
the comparison of two summaries of source docu-
ment AFA19981230.1000.0058. A secondary 
objective of the post-study was to inspect the ac-
tual data. Were there factors in play in the data 
that had escaped us? None were made evident be-
yond the all-too-familiar demonstration of the 
wide variety of language use in play. The log file 
of document phrase hash tables provided an addi-
tional snapshot of the kind of materials with 
which the automated computation had been work-
ing. 
4  Results 
4.1 Data Averages  
Table 2 illustrates the degree to which summaries 
in the DUC corpus employ the same vocabulary 
as the source documents on which they are based 
and the degree to which they resemble each other 
in wording. The table, actually a stack of four ta-
bles which share common headings, presents data 
on the document-summary relationship followed 
by inter-summary data, giving counts and then 
percentages for each relationship. Statistics on the 
given relationship appear in the first three col-
umns on the left; counts and averages are classi-
fied by summary size. The central group of six 
columns presents from left to right, in decreasing 
order of strictness, the average number of phrase 
matches found for the size category. The final two 
columns on the right present parallel match data 
for tokens. Thus for example the column entitled 
STEM CI shows the average number of stemmed, 
case-insensitive token matches in a pair of test 
documents of the size category indicated. Each 
table in the stack ends with a boldface row that 
averages statistics across all size categories.  
Inspection of the results in Table 2 leads to 
these general observations: 
? With the exception of 200-word summaries 
falling somewhat short (157 words), each cate-
gory approaches its target size quite closely; 
? Phrases average three tokens in length regard-
less of summary size; 
? The objective of relaxing match criteria in the 
main study was achieved. With few exceptions, 
each less strict match type produces more hits 
than its more stringent neighbors; 
? The much smaller size of the now discontinued 
50- and 200-word categories argues against in-
vesting much confidence in their data; 
? Finally, while no effect was found for source 
document size (and results for that categorization 
are therefore not presented), the percentage tables 
suggest summary size has some limited impact on 
vocabulary agreement. This effect occurs solely 
on the phrasal level, most strongly on its strictest 
measures; token values are effectively flat.  
We are uncertain why this last situation is so. 
Consider only the well-populated 10-word and 
100-word summary classes. The effect cannot be 
accounted for a preponderance of multiple -
document summaries in either class which might 
provide more opportunities for matches. Despite 
many more of these being among the 100-word 
summaries than the 10-word (1974 single : 1056 
multi, versus 116 single : 5456 multi), the per-
centage of exact phrasal matches is essentially the 
same in each subcategorization of these classes. 
We speculate that authors may compose the 
sentences in 100-word summaries in terms of 
phrases from the source document, while 10-word 
summaries, which more closely resemble terse 
headlines, cannot be composed by direct reuse of 
source document phrases. 50- and 200-word 
summaries are also composed of sentences. Their 
exact match percentages approach those of 100-
word summaries, lending support to this interpre-
tation.  
Figure 3: Percentages of summary vocabulary 
agreement for all source documents, by measure 
4.2 Data Variance 
Whether count or percentage, exclusively average 
data is presented in Table 2. While measures of 
central tendency are an important dimension of 
any population, a full statistical description also 
requires some indication of measures of variance. 
These appear in Figure 3 which shows, for each of 
the six phrasal and two token measures, what per-
centage of the total number of summaries falls 
into each tenth of the range of possible values. For 
example, a summary in which 40% of the phrases 
were exactly matched in the source document 
would be represented in the figure by the vertical 
position of the frontmost band over the extent of 
the decade labeled ?4??24%. The figure?s three-
dimensional aspect allows the viewer to track 
which decades have the greatest number of in-
stances as measures move from more strict to 
more relaxed, front to back.   
However, the most striking message communi-
cated by Figure 3 is that large numbers of summa-
ries have zero values for the stricter measures, 
EXACT, EXACT CI and PART SUM in particular and 
PART DOC to a lesser degree. These same meas-
ures have their most frequent values around the 
50% decade, with troughs both before and after. 
To understand why this is so requires some expla-
nation. Suppose a summary contains two phrases.  
If none are matched in the source its score is 0%.  
If one is matched its score is 50%; if  both, 100%.  
A summary with three phrases has four possible 
percentage values: 0%, 33%, 66% and 100%.  The 
'hump' of partial matching is thus around the fifty 
percent level because most summaries are ten 
words, and have only 1 or 2 candidates to be 
matched. The ranges involved in the stricter 
measures are not large. 
That acknowledged, we can see that the modal 
or most frequent decade does indeed tend in an 
irregular way to move from left to right, from zero 
to 100 percent, as measures become less strict. In 
making this observation, note that the two back-
most bands represent measures on tokens, a dif-
ferent syntactic element than the phrase. The 
information about the distribution of summary 
measures shown in this figure is not unexpected. 
4.3 Key Findings 
The central fact that these data communicate quite 
clearly is that summaries do not employ many of 
the same phrases their source documents do, and 
even fewer than do other summaries. In particular, 
on average only 37% of summary phrases appear 
in the source document, while summaries share 
only 9% of their phrases. This becomes more un-
derstandable when we note that on average only 
55% of the individual words used in summaries, 
both common vocabulary terms and proper names, 
appear in the source document; and between 
summaries, on average only 22% are found in 
both. 
It may be argued that the lower counts for inter-
summary vocabulary agreement can be explained 
thus: since a summary is so much smaller than its 
source document, lower counts should result. One 
reply to that argument is that, while acknowledg-
ing that synonymy, generalization and specializa-
tion would augment the values found, the essence 
of a generic summary is to report the pith, the gist, 
the central points, of a document and that these 
key elements should not vary so widely from one 
summary to the next. 
5 Pertinent Research 
Previous research addressing summary vocabulary 
is limited, and most has been undertaken in con-
nection with another issue: either with the prob-
lem of evaluating summary quality (Mani, 2001; 
Lin and Hovy, 2002) or to assess sentence element 
suitability for use in a summary (Jing and McKe-
own, 1999). In such a case results arise as a by-
product of the main line of research and conclu-
sions about vocabulary must be inferred from 
other findings.  
Mani (2001) reports that ?previous studies, most 
of which have focused on extracts, have shown 
evidence of low agreement among humans as to 
which sentences are good summary sentences.? 
Lin and Hovy?s (2002) discovery of low inter-
rater agreement in single (~40%) and multiple 
(~29%) summary evaluation may also pertain to 
our findings. It stands to reason that individua ls 
who disagree on sentence pertinence or do not rate 
the same summary highly are not likely to use the 
same words to write the summary. In the very 
overt rating situation they describe, Lin and Hovy 
were also able to identify human error and quan-
tify it as a significant factor in rater performance. 
This reality may introduce variance as a conse-
quence of suboptimal performance: a writer may 
simply fail to use the mot juste . 
In contrast, Jing, McKeown, Barzilay and Elha-
dad (1998) found human summarizers to be ?quite 
consistent? as to what should be included, a result 
they acknowledge to be ?surprisingly high?. Jing 
et al note that agreement drops off with summary 
length, that their experience is somewhat at vari-
ance with that of other researchers, and that this 
may be accounted for in part by regularity in the 
structure of the documents summarized. 
Observing that ?expert summarizers often reuse 
the text in the original document to produce a 
summary? Jing and McKeown (1999) analyzed 
300 human written summaries of news articles 
and found that ?a significant portion (78%) of 
summary sentences produced by humans are ba-
sed on cut-and-paste?, where ?cut-and-paste? indi-
cates vocabulary agreement. This suggests that 
22% of summary sentences are not produced in 
this way; and the authors report that 315 (19%) 
sentences do not match any sentence in the docu-
ment. 
In their 2002 paper, Lin and Hovy examine the 
use of multiple gold standard summaries for 
summarization evaluation, and conclude ?we need 
more than one model summary although we can-
not estimate how many model summaries are re-
quired to achieve reliable automated summary 
evaluation?.  
Attempting to answer that question, van Hal-
teren and Teufel (2003) conclude that 30 to 40 
manual summaries should be sufficient to estab-
lish a stable consensus model summary. Their re-
search, which directly explores the differences and 
similarities between various human summaries to 
establish a basis for such an estimate, finds great 
variation in summary content as reflected in fac-
toids3. This variation does not fall off with the 
number of summaries and accordingly no two 
summaries correlate highly. Although factoid 
measures did not correlate highly with those of 
unigrams (tokens), the former did clearly demon-
strate an importance hierarchy which is an essen-
tial condition if a consensus model summary is to 
be constructed. Our work can thus be seen as con-
firming that, in large measure, van Halteren and 
Teufel?s findings apply to the DUC corpus of 
manual summaries. 
6 Discussion 
We began this study to test two hypotheses. The 
first is this: automatic summarization is made dif-
ficult to the degree that manually-written summa-
ries do not limit themselves to the vocabulary of 
the source document. For a summarization system 
                                                 
3 A factoid is an atomic semantic unit corresponding 
to an expression in first-order predicate logic. As al-
ready noted we approximate phrases to factoids. 
to incorporate words which do not appear in the 
source document requires at a minimum that it has 
a capacity to substitute a synonym of some word 
in the text, and some justification for doing so. 
More likely it would involve constructing a repre-
sentation of the text?s meaning and reasoning 
(generalization, inferencing) on the content of that 
representation. The latter are extremely hard tasks. 
Our second hypothesis is that automatic sum-
marization is made difficult to the degree that 
manually written summaries do not agree among 
themselves. While the variety of possible dis-
agreements are multifarious, the use of different 
vocabulary is a fundamental measure of semantic 
heterogeneity. Authors cannot easily talk of the 
same things if they do not use words in common. 
Unfortunately, our study of the DUC manual 
summaries and their source documents provides 
substantial evidence that summarization of these 
documents remains difficult indeed.  
7 Conclusion 
Previous research on the degree of agreement be-
tween documents and summaries, and between 
summaries, has generally indicated that there are 
significant differences in the vocabulary used by 
authors of summaries and the source document. 
Our study extends the investigation to a corpus 
currently popular in the text summarization re-
search community and finds the majority opinion 
to be borne out there. In addition, our data sug-
gests that summaries resemble the source docu-
ment more closely than they do each other. The 
limited number of summaries available for any 
individual source document prevents us from 
learning any characteristics of the population of 
possible summaries. Would more summaries dis-
tribute themselves evenly throughout the semantic 
space defined by the source document?s vocabu-
lary? Would clumps and clusters show them-
selves, or a single cluster as van Halteren and 
Teufel suggest? If the latter, such a grouping 
would have a good claim to call itself a consensus 
summary of the document and a true gold stan-
dard would be revealed. 
References 
Copeck, Terry and Stan Szpakowicz. 2003. Pick-
ing phrases, picking sentences. In DUC Work-
shop at HLT/NAACL-2003 Workshop on 
Automatic Summarization. 
Jing, Hongyan, Regina Barzilay, Kathleen McKe-
own and Michael Elhadad. 1998. Summariza-
tion evaluation methods: Experiments and 
analysis. In 1998 AAAI Spring Symposium on 
Intelligent Text Summarization, AAAI Technical 
Report SS-98-06. 
Jing, Hongyan. and Kathleen McKeown. 1999. 
The decomposition of human-written summary 
sentences. Proceedings of the 22nd Interna-
tional Conference on Research and Develop-
ment in Information Retrieval (SIGIR?99). 
Lin, Chin-Yew and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. Proceedings of 2003 Lan-
guage Technology Conference (HLT-NAACL 
2003). 
Lin, Chin-Yew and Eduard Hovy. 2002. Manual 
and automatic evaluation of summaries. Pro-
ceedings of Workshop on Automatic Summari-
zation, 2002 ACL (WAS/ACL-02). 
Mani, Inderjeet. 2001. Summarization evaluation: 
An overview. Proceedings of the Second NTCIR 
Workshop on Research in Chinese & Japanese 
Text Retrieval and Text Summarization. 
Van Halteren, Hans, and Simone Teufel. 2003. 
Examining the consensus between human sum-
maries: initial experiments with factoid analysis. 
Proceedings of Workshop on Automatic Sum-
marization, 2003 Language Technology Confer-
ence (WAS/HLT-NAACL-2003). 
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 8?15,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Challenges in Evaluating Summaries of Short Stories 
Anna Kazantseva School of Information Technology and Engineering, University of Ottawa, Ottawa, Canada ankazant@site.uottawa.ca 
Stan Szpakowicz School of Information Technology and Engineering, University of Ottawa, Ottawa, Canada Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland szpak@site.uottawa.ca  Abstract This paper presents experiments with the evaluation of automatically produced summaries of literary short stories. The summaries are tailored to a particular purpose of helping a reader decide whether she wants to read the story. The evaluation procedure includes extrinsic and intrinsic measures, as well as subjective and factual judgments about the summaries pronounced by human subjects. The experiments confirm the experience of summarizing more conventional genres: sentence overlap between human- and machine-made summaries is not a complete picture of the quality of a summary. In fact, in our case, sentence overlap does not correlate well with human judgment. We explain the evaluation procedures and discuss several challenges of evaluating summaries of works of fiction. 1 Introduction In recent years the automatic text summarization community has increased its focus on reliable evaluation. The much used evaluation methods based on sentence overlap with reference summaries have been called into question (Mani 2001) as they provide only a rough approximation of semantic similarity between summaries. A number of deeper, more semantically-motivated approaches have been proposed, such as the factoid method (van Halteren 
and Teufel, 2003) and the pyramid method (Nenkova and Passonneau 2004). These methods measure similarity between reference and generated summaries more reliably but, unfortunately, have a disadvantage of being very labour-intensive. This paper describes experiments in evaluating automatically produced summaries of literary short stories. It presents an approach that evaluates summaries from two different perspectives: comparing computer-made summaries to those produced by humans based on sentence-overlap and measuring usefulness and informativeness of the summaries by themselves ? a step critical when creating and evaluating summaries of a relatively unexplored genre. The paper also points out several challenges specific to evaluating summaries of fiction such as questionable suitability of traditional metrics (those based on sentence overlap), unavailability of clearly defined criteria to judge ?goodness? of a summary and a higher degree of redundancy in such texts. We achieve these goals by performing a two-step evaluation of our summaries. Initially, for each story in the test set we compare sentence overlap between summaries which the system generates and those produced by three human subjects. These experiments reveal that inter-rater agreement measures tend to be pessimistic where fiction is concerned. This seems due to a higher degree of redundancy and paraphrasing in such texts. The second stage of the evaluation process seeks to measure usefulness of the summaries in a more tangible way. To this end, three subjects answered a number of questions, first after 
8
Figure 1. Example of a summary produced by the system. A MATTER OF MEAN ELEVATION. By O. Henry (1862-1910). On the camino real along the beach the two saddle mules and the four pack mules of Don Se?or Johnny Armstrong stood, patiently awaiting the crack of the whip of the arriero, Luis. These articles Don Johnny traded to the interior Indians for the gold dust that they washed from the Andean streams and stored in quills and bags against his coming. It was a profitable business, and Se?or Armstrong expected soon to be able to purchase the coffee plantation that he coveted. Armstrong stood on the narrow sidewalk, exchanging garbled Spanish with old Peralto, the rich native merchant who had just charged him four prices for half a gross of pot-metal hatchets, and abridged English with Rucker, the little German who was Consul for the United States. [?]  Armstrong, waved a good-bye and took his place at the tail of the procession. Armstrong concurred, and they turned again upward toward Tacuzama. [?] Peering cautiously inside, he saw, within three feet of him, a woman of marvellous, imposing beauty, clothed in a splendid loose robe of leopard skins. The hut was packed close to the small space in which she stood with the squatting figures of Indians. [?] I am an American. If you need assistance tell me how I can render it. [?] The woman was worthy of his boldness. Only by a sudden flush of her pale cheek did she acknowledge understanding of his words. [?] " I am held a prisoner by these Indians. God knows I need help. [?] look, Mr. Armstrong, there is the sea!   reading only the summary and then after reading the complete story. The set included both factual questions (e.g. can you tell where this story takes place?) and subjective questions (e.g. how readable did you find this summary?). Finally, we compare the two types of results with a surprising discovery: overlap-based measures and human judgment do not correlate well in our case. This paper is organized in the following manner. Section 2 briefly describes our summarizer of short stories. Section 3.1 discusses experiments comparing generated summaries to reference ones based on sentence overlap. The experiments involving human judgment of the summaries are presented in Section 3.2 and the two types of experiments are compared in Section 3.3. Section 4 draws conclusions and outlines possible directions for future work. 2 Background: System Description A detailed description of our summarizer of short stories is outside the scope of this paper. For completeness, this section gives an overview of the system?s inner workings. An interested reader is referred to our previous work (Kazantseva 2006) for more information. The system is designed to create a particular type of indicative generic summaries ? namely, summaries that would help readers decide whether they would like to read a given story. Because of this, a summary, as defined here, is not meant to summarize the plot of a story. It is intended 
to raise adequate expectations and to enable a reader to make informed decisions based on a summary only. We achieve this goal by identifying the salient portions of the original texts that lay out the setting of a story, namely, location and main characters. The present prototype of our system creates summaries by extracting sentences from original documents. An example summary produced by the system appears in Figure 1. The system works in two stages. First it attempts to identify important entities in stories (locations and characters). Next, sentences that are descriptive and set out the background of a story are separated from those that relate events of the plot. Finally, the system selects summary-worthy sentences in a way that favours descriptive ones that focus on important entities and occur early in the text. The identification of important entities is achieved by processing the stories using a gazetteer. Pronominal and noun phrase anaphora are very common in fiction, so we resolve anaphoric expressions of these two types. The anaphora resolution module is restricted to resolving singular anaphoric expressions that denote animate entities (people and, sometimes, animals). The main characters are then identified using normalized frequency counts. The next stage of the process attempts to identify sentences that set out the background in each story. The stories are parsed using the Connexor Machinese Syntax Parser (Tapanainen and J?rvinen 1997) and sentences are split into clauses. 
9
Each clause is represented as a vector of features that approximate its aspectual type. The features are designed to help identify state clauses (John was a tall man) and serial situations (John always drops things) (Huddleston and Pullum 2002, p. 123-124). Four groups of features represent each clause: character-related, location-related, aspect-related and others. Character-related features capture such information as the presence of a mention of one of the main characters in a clause, its syntactic function, how early in the text this mention occurs, etc. Location-related features state whether a clause contains a location name and whether this name is embedded in a prepositional phrase. Aspect-related features reflect a number of properties of a clause that influence its aspectual type. They include the main verb?s lexical aspect, the tense, the presence and the type of temporal expressions, voice, and the presence of modal verbs. In our experiments we create two separate representations for each clause: fine-grained and coarse-grained. Both contain features from all four feature groups. The difference between them is only in the number of features and in the cardinality of the set of possible values. Two different procedures achieve the actual selection process. The first procedure performs decision tree induction using C5.0 (Quinlan 1992) to select the most likely candidate sentences. The training data for this process consists of short stories annotated at the clause-level by the first author of this paper. The second procedure applies a set of manually created rules to select summary-worthy sentences. The corpus for the experiments contains 47 short stories from Project Gutenberg (http://www.gutenberg.org) divided into a training set (27 stories) and a test set (20 stories). These are classical works written in English or translated into English by authors including O.Henry, Jerome K. Jerome, Katherine Mansfield and Anton Chekhov. They have on average 3,333 tokens and 244 sentences (4.5 letter-sized pages). The target compression rate was set at 6% counted in 
sentences. This rate was selected because it corresponded to the compression rate achieved by the first author when creating initial training and test data. 3 Evaluation: Experimental Setup We designed our evaluation procedure to have easily interpreted, meaningful results, and keep the amount of labour reasonable. We worked with six subjects (different than the authors of this paper) who performed two separate tasks. In Task 1 each subject was asked to read a story and create its summary by selecting 6% of the sentences. The subjects were explained that their summaries were to raise expectations about the story, but not to reveal what happens in it. In Task 2 the subjects made a number of judgments about the summaries before and after reading the original stories. The subjects read a summary similar to the one shown in Figure 1. Next, they were asked six questions, three of which were factual in nature and three others were subjective. The subjects had to answer these questions using the summary as the only source of information. Subsequently, they read the original story and answered almost the same questions (see Section 4). This process allowed us to understand how informative the summaries were by themselves, without access to the originals, and also whether they were misleading or incomplete. The experiments were performed on a test set of 20 stories and involved six participants divided into two groups of three people. Group 1 performed Task 1 on stories 1-10 of the testing set and Group 2 performed this task on stories 11-20. During Task 2 Group 1 worked on stories 11-20 and Group 2 ? on stories 1-10. By adjusting a number of system parameters, we produced four different summaries per story. All four versions were compared with human-made summaries using sentence overlap-based measures. However, because the experiments are rather time consuming, it was not possible to evaluate more than one set of summaries using human judgments (Task 2). That is 
10
Table 1. Inter-judge agreement. Statistic Group 1 Group 2 Average  Cohen (4) 0.50 0.34 0.42 Cohen (3) 0.51 0.34 0.42 PABAK (4) 0.88 0.85 0.87 PABAK (3) 0.89 0.86 0.87 ICC (4) 0.80 (0.78, 0.82) 0.67 (0.64, 0.70) 0.73 (0.71, 0.76) ICC (3) 0.76 (0.74, 0.80) 0.6 (0.56, 0.64) 0.68 (0.65, 0.72)  
why only summaries generated using the coarse-grained dataset and manually composed rules were evaluated in Task 2. We selected this version because the differences between this set of summaries and gold-standard summaries are easiest to interpret. That is to say, decisions based on a set of rules employing a smaller number of parameters are easier to track than those taken using machine learning or more elaborate rules. On average, the subjects reported that completing both tasks required between 15 and 35 hours of work. Four out of six subjects were native speakers of English. Two others had a near-native and very good levels of English respectively. The participants were given the data in form of files and had four weeks to complete the tasks. 3.1 Creating Gold-Standard Summaries: Task 1 During this task each participant had to create extract-based summaries for 10 different stories. The criteria (making a summary indicative rather than informative) were explained and one example of an annotated story shown. The instructions for these experiments are available at <http://www.site.uottawa.ca/~ankazant/instructions.zip>. Table 1 presents several measures of agreement between judges within each group and with the first author of this paper (included in the agreement figures because this person created the initial training data and test data for the preliminary experiments). The measurement names are displayed in the first column of Table 1. Cohen denotes Cohen?s kappa (Cohen 1960). PABAK denotes Prevalence and Bias Adjusted Kappa (Bland and Altman 1986). ICC denotes Intra-class Correlation Coefficient (Shrout and Fleiss 1979). The numbers 3 and 4 state whether the statistic is computed only for 3 subjects participating in the evaluation or for 4 subjects (including the first author of the paper). 
As can be seen in Table 1, the agreement statistics are computed for each group separately. This is because the sets of stories that they annotated are disjoint. The column Average provides an average of these figures to give a better overall idea. Cohen?s kappa in its original form can only be computed for a pair of raters. For this reason we computed it for each possible pair-wise combination of raters within a group and then the numbers were averaged. The PABAK statistic was computed in the same manner using Cohen?s kappa as its basis. ICC is the statistic that measures inter-rater agreement and can be computed for more than 2 judges. It was computed for all 3 or 4 raters at the same time. ICC was computed for a two-way mixed model and measures the average reliability of ratings taken together. The numbers in parentheses are confidence intervals for 99% confidence.  We compute three different agreement measures because each of these statistics has its weakness and distorts the results in a different manner. Cohen?s kappa is known to be a pessimistic measurement in the presence of a severe class imbalance, as is the case in our setting (Sim and Wright 2005). PABAK is a measure that takes class imbalance into account, but it is too optimistic because it artificially removes class imbalance present in the original setting. ICC has weaknesses similar to Cohen?s kappa (sensitivity to class imbalance). Besides, it assumes that the sample of targets to be rated (sentences in our case) is a random sample of targets drawn from a larger population. This is not 
11
Table 2. Sentence overlap between computer- and human-made summaries. Majority gold-standard. Dataset Prec. Rec. F LEAD  25.09 30.49 27.53 LEAD CHAR  28.14 33.18 30.45 Rules, coarse-grained 34.14 44.39 38.60 Rules, fine-gr. 39.27 50.00 43.99 Machine learning, coarse-gr. 35.55 40.81 38.00 ML, fine-gr. 37.97 50.22 43.22  
Figure 2. Fragments of summaries produced by 3 annotators for The Cost of Kindness by Jerome K Jerome. Annotator A. The Rev. Augustus Cracklethorpe would be quitting Wychwood-on-the-Heaththe the following Monday, never to set foot [?] in the neighbourhood again. The Rev. Augustus Cracklethorpe, M.A., might possibly have been of service to his Church in, say, [?] some mission station far advanced amid the hordes of heathendom. In picturesque little Wychwood-on-the-Heath [?] these qualities made only for scandal and disunion. Churchgoers who had not visited St. Jude's for months had promised themselves the luxury of feeling they were listening to the Rev. Augustus Cracklethorpe for the last time. The Rev. Augustus Cracklethorpe had prepared a sermon that for plain speaking and directness was likely to leave an impression. Annotator B. The Rev. Augustus Cracklethorpe would be quitting Wychwood-on-the-Heaththe the following Monday, never to set foot [?] in the neighbourhood again. The Rev. Augustus Cracklethorpe, M.A., might possibly have been of service to his Church in, say, [..] some mission station far advanced amid the hordes of heathendom. What marred the entire business was the impulsiveness of little Mrs. Pennycoop. Mr. Pennycoop, carried away by his wife's eloquence, added a few halting words of his own. Other ladies felt it their duty to show to Mrs. Pennycoop that she was not the only Christian in Wychwood-on-the-Heath. Annotator C. The Rev. Augustus Cracklethorpe would be quitting Wychwood-on-the-Heath the following Monday, never to set foot [?] in the neighbourhood again. The Rev. Augustus Cracklethorpe, M.A., might possibly have been of service to his Church in, say, [?] some mission station far advanced amid the hordes of heathendom. For the past two years the Rev. Cracklethorpe's parishioners [?] had sought to impress upon him, [..] their cordial and daily-increasing dislike of him, both as a parson and a man. The Rev. Augustus Cracklethorpe had prepared a sermon that for plain speaking and directness was likely to leave an impression. The parishioners of St. Jude's, Wychwood-on-the-Heath, had their failings, as we all have. The Rev. Augustus flattered himself that he had not missed out a single one, and was looking forward with pleasurable anticipation to the sensation that his remarks, from his "firstly" to his "sixthly and lastly," were likely to create.     
necessarily the case as the corpus was not compiled randomly. We hope that these three measures, although insufficient individually, provide an adequate understanding of inter-rater agreement in our evaluation. We note that the average overlap (intersection) between judges in each group is 1.8% out of 6% of summary-worthy sentences. All of these agreement measures and, in fact, all measures based on computing sentence overlap are inherently incomplete where fiction is concerned because any two different sentences are not necessarily ?equally different?. The matter is exemplified in Figure 2. It displays 
segments of summaries produced for the same story by three different annotators. Computing Cohen?s kappa between these fragments gives agreement of 0.521 between annotators A and B and 0.470 between annotators A and C. However, a closer look at these fragments reveals that there are more differences between summaries A and B than between summaries A and C. This is because many of the sentences in summaries A and C describe the same information (personal qualities of Rev. Cracklethorpe) even though they do not overlap. On the other hand, sentences from summaries A and B are not only distinct; they ?talk? about different facts. This problem is not unique to fiction, but in this context it is more acute because literary texts exhibit more redundancy. Tables 2-4 show the results of comparing four different versions of computer-made summaries against gold-standard summaries produced by humans. The tables also display the results of two baseline algorithms. The LEAD baseline refers to the version of summaries produced by selecting the first 6% of sentences in each story. LEAD CHAR baseline is obtained by selecting first 
12
Table 3. Sentence overlap between computer- and human-made summaries. Union gold-standard. Dataset Prec. Rec. F LEAD 36.53 17.97 24.09 LEAD CHAR 44.49 21.23 28.75 Rules, coarse-grained 52.41 30.96 38.92 Rules, fine-gr. 56.77 31.22 40.28 Machine learning, coarse-gr. 51.17 23.76 32.47 ML, fine-gr. 55.59 29.76 38.77  
Table 4. Sentence overlap between computer- and human-made summaries. Intersection gold-standard. Dataset Prec. Rec. F LEAD 12.55 37.36 18.78 LEAD CHAR 15.97 46.14 23.73 Rules, coarse-grained 19.66 62.64 29.92 Rules, fine-gr. 23.10 76.92 35.53 Machine learning, coarse-gr. 19.14 53.85 28.24 ML, fine-gr. 21.36 69.23 32.64  
Table 5. Answers to factual questions. Id Question After summary only After reading the original  Mean Std. dev Mean Std. dev. Q1, Q7 Please list up to 3 main characters in this story, in the order of importance (scale: -1 to 3) 2.28 0.64 2.78 0.45 Q2, Q8 State where this story takes place. Be as specific as possible (scale: -1 to 3) 1.78 1.35 2.60 0.91 Q3, Q9 Select a time period when this story takes place.(scale: 0 or 1) 0.53 0.50 0.70 0.46   
6% of sentences that contain a mention of an important character. The improvements over the baselines are significant with 99% confidence in all cases. By combining summaries created by human annotators in different ways we create three distinct gold-standard summaries. The majority gold-standard summary contains all sentences that were selected by at least two judges. It is the most commonly accepted way of creating gold-standard summaries and it is best suited to give an overall picture of how similar computer-made summaries are to man-made ones. The union gold standard is obtained by considering all sentences that were judged summary-worthy by at least one judge. Union summaries provide a more relaxed measurement. Precision for the union gold standard gives one an idea of how many irrelevant sentences a given summary contains (sentences not selected by any of three judges are more likely to prove irrelevant). The intersection summaries are obtained by combining sentences that all three judges deemed to be important. Intersection gold standard is the strictest way to measure the 
goodness of a summary. Recall for intersection gold standard tells one how many of the most important sentences were included in summaries by the system (sentences selected by all three judges are likely to be the most important ones). It should be noted, however, that the numbers in Tables 2-4 do not give a complete picture of the quality of the summaries for the same reason that the agreement measures do not reveal fully the extent of inter-judge agreement: sentences that are not part of the reference summaries are not necessarily equally unsuitable for inclusion in the summary. 3.2 Human Judgment of Computer-Made Summaries: Task 2 In order to evaluate one summary in Task 2, a participant had to read it and to answer six questions using the summary as the only source of information. The participant was then required to read the original story and to answer another six questions. The questions asked before and after reading the original were the same with one exception: question Q4 was replaced by Q11 (see Table 6.) The subjects were asked not to correct the answers after the fact. 
13
Table 8. ANOVA F-values between sentence overlap measures and human judgments. Question Prec. Rec. F Q1(main characters) 0.60 0.61 0.58 Q2(location) 2.58 1.94 2.36 Q3(time) 1.11 0.67 0.97 Q4(readability) 2.10 0.90 1.60 Q5(relevance) 4.55 3.75 4.28 Q10 (relevance) 6.33 3.46 5.15 Q11(completeness) 3.11 4.22 3.43 Q12(helpfulness) 4.53 2.54 3.72  
Table 7. Spearman rank correlation coefficient between sentence overlap measures and human judgments. Question Prec. Rec. F Q1(main characters) 0.09 0.29 0.17 Q2(location) 0.21 0.18 0.22 Q3(time) 0.38 0.28 0.34 Q4(readability) 0.47 0.31 0.50 Q5(relevance) 0.31 0.19 0.34 Q10(relevance) 0.60 0.40 0.59 Q11(completeness) 0.40 0.29 0.40 Q12(helpfulness) 0.59 0.41 0.61  
Table 6. Answers to subjective questions. Id Question (scale: 1 to 6) After summary only After reading the original   Mean Std. dev Mean Std. dev. Q4 How readable do you find this summary? 4.43 1.39 N/A N/A Q5, Q10 How much irrelevant information does this summary contain? 4.27 1.41 4.51 1.16 Q11 How complete is the summary? N/A N/A 4.53 1.25 Q6, Q12 How helpful was this summary for deciding whether you would like to read the story or not? 4.52 1.37 4.6 1.21    Three of the questions were factual and three others ? subjective. Table 5 displays the factual questions along with the resulting answers. The participants had to answer questions Q1 and Q2 in their own words and question Q3 was a multiple-choice question where a participant selected the century when the story took place. Q1 and Q2 were ranked on a scale from -1 to 3. A score of 3 means that the answer was complete and correct, 2 ? slightly incomplete, 1 ? very incomplete, 0 ? a subject could not find the answer in the text and -1 if the person answered incorrectly. Q3 was ranked on a binary scale (0 or 1). Questions Q3-Q7 asked the participants to pronounce a subjective judgment on a summary. These were multiple-choice questions where a participant needed to select a score from 1 to 6, with 1 indicating a strong negative property and 6 indicating a strong positive property. The questions and results appear in Table 6. The results displayed in Tables 5 and 6 suggest that the subjects can answer simple questions based on the summaries alone. They also seem to indicate that the subjects found the summaries quite helpful. It is interesting to note that even after reading 
complete stories the subjects are not always capable of answering the factual questions with perfect precision. 3.3 Putting Sentence Overlap and Human Judgment Together In order to check whether the two types of statistics measure the same or different qualities of the summaries, we explored whether the two are correlated. Table 7 displays the values of Spearman rank correlation coefficient between median values of answers for questions from Task 2 and measurements obtained by comparing computer-made summaries against the majority gold-standard summaries. All questions, except Q10 (relevance) and Q11 (completeness) are those asked and answered using the summary as the only source of information. Sentence overlap values (F-score, precision and recall) were discretized (banded) in order to be used in this test. These results are based on the values obtained for 20 stories in the test set ? a relatively small sample ? which prohibits drawing definite conclusions. However, in most cases the correlation coefficient between human opinions and sentence overlap measurements is below the cut-off 
14
value with 99% confidence, which is 0.57 (the exceptions are highlighted). This suggests that in our case the measurements using sentence overlap as their basis are not correlated with the opinions of subjects about the summaries. We also performed a one-way ANOVA test using human judgments as independent factors and sentence-overlap based measures as dependent variables. The results are in line with those obtained using Spearman coefficient. They are shown in Table 8. The F-values which are statistically significant with 99% confidence are highlighted (the cut-off value for questions Q4-Q12 is 4.89, for Q1 and Q2 ? 6.11 and for Q3 ? 8.29). 4 Conclusions and Future Work This paper presented an experimental way of evaluating automatically produced summaries of literary short stories. In the course of our experiments we have remarked a few issues pertinent to evaluating summaries of short fiction. Firstly, higher degree of redundancy of sentences in texts makes measures based on sentence overlap not very enlightening when evaluating extracted summaries. Secondly, at least in our corpus, the sentence overlap-based measures do not correlate well with those measuring opinions of humans about summaries. This work is exploratory, and as such raises more questions than it answers. In order to evaluate summaries of literary works in a meaningful and reliable way one needs to define criteria which make such summaries suitable or not suitable for a particular purpose. We will explore this issue in our future work. We also intend to apply the pyramid method of evaluating summaries to extracted summaries produced by the human annotators. 5 Acknowledgements The authors would like to express their gratitude to Connexor Oy and especially to Atro Voutilainen for their kind permission to use Connexor Machinese Syntax parser free of charge for research purposes. 
References J. Bland and D. Altman. 1986. Statistical methods for assessing agreement between two methods of clinical measurement. Lancet. 1986; 1(8476):307-310. J. Cohen, 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement. 1960; 20:37-46.. R. Huddleston and G. Pullum. 2002. The Cambridge Grammar of the English Language Usage, 123-124. Cambridge University Press. A. Kazantseva. 2006. Proc Student Research Workshop at EACL 2006, 55-63. I. Mani. 2001. Automatic Summarization. John Benjamins B.V. A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. Proc Human Language Technology Conference and NAACL. J. Quinlan. 1992. C4.5: Programs for Machine Learning, Morgan Kaufmann Pub., San Mateo, CA. P. Shrout and J. Fleiss. Intraclass correlations: uses in assessing rater reliability. Psychological Bulletin 1979; 86:420?428 J. Sim and C. Wright. 2005. The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements. Physical Therapy, 2005(85-3): 257-268. P. Tapanainen and T. J?rvinen. 1997. A non-projective dependency parser. Proc 5th Conference on Applied Natural Language Processing, 64-71. H. Van Halteren and S. Teufel. 2003. Examining the consensus between human summaries: initial experiments with factoid analysis. HLT/NAACL-2003 Workshop on Automatic Summarization, 57-64. 
15
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 13?18,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 04:
Classification of Semantic Relations between Nominals
Roxana Girju
Univ. of Illinois
at Urbana-Champaign
Urbana, IL 61801
girju@uiuc.edu
Preslav Nakov
Univ. of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Vivi Nastase
EML Research gGmbH
Heidelberg, Germany 69118
nastase@eml-research.de
Stan Szpakowicz
University of Ottawa
Ottawa, ON K1N 6N5
szpak@site.uottawa.ca
Peter Turney
National Research Council of Canada
Ottawa, ON K1A 0R6
peter.turney@nrc-cnrc.gc.ca
Deniz Yuret
Koc? University
Istanbul, Turkey 34450
dyuret@ku.edu.tr
Abstract
The NLP community has shown a renewed
interest in deeper semantic analyses, among
them automatic recognition of relations be-
tween pairs of words in a text. We present an
evaluation task designed to provide a frame-
work for comparing different approaches to
classifying semantic relations between nom-
inals in a sentence. This is part of SemEval,
the 4th edition of the semantic evaluation
event previously known as SensEval. We de-
fine the task, describe the training/test data
and their creation, list the participating sys-
tems and discuss their results. There were
14 teams who submitted 15 systems.
1 Task Description and Related Work
The theme of Task 4 is the classification of semantic
relations between simple nominals (nouns or base
noun phrases) other than named entities ? honey
bee, for example, shows an instance of the Product-
Producer relation. The classification occurs in the
context of a sentence in a written English text. Al-
gorithms for classifying semantic relations can be
applied in information retrieval, information extrac-
tion, text summarization, question answering and so
on. The recognition of textual entailment (Tatu and
Moldovan, 2005) is an example of successful use of
this type of deeper analysis in high-end NLP appli-
cations.
The literature shows a wide variety of methods
of nominal relation classification. They depend as
much on the training data as on the domain of ap-
plication and the available resources. Rosario and
Hearst (2001) classify noun compounds from the
domain of medicine, using 13 classes that describe
the semantic relation between the head noun and
the modifier in a given noun compound. Rosario
et al (2002) classify noun compounds using the
MeSH hierarchy and a multi-level hierarchy of se-
mantic relations, with 15 classes at the top level.
Nastase and Szpakowicz (2003) present a two-level
hierarchy for classifying noun-modifier relations in
base noun phrases from general text, with 5 classes
at the top and 30 classes at the bottom; other re-
searchers (Turney and Littman, 2005; Turney, 2005;
Nastase et al, 2006) have used their class scheme
and data set. Moldovan et al (2004) propose a 35-
class scheme to classify relations in various phrases;
the same scheme has been applied to noun com-
pounds and other noun phrases (Girju et al, 2005).
Chklovski and Pantel (2004) introduce a 5-class set,
designed specifically for characterizing verb-verb
semantic relations. Stephens et al (2001) propose
17 classes targeted to relations between genes. La-
pata (2002) presents a binary classification of rela-
tions in nominalizations.
There is little consensus on the relation sets and
algorithms for analyzing semantic relations, and it
seems unlikely that any single scheme could work
for all applications. For example, the gene-gene re-
lation scheme of Stephens et al (2001), with rela-
tions like X phosphorylates Y, is unlikely to be trans-
ferred easily to general text.
We have created a benchmark data set to allow the
evaluation of different semantic relation classifica-
tion algorithms. We do not presume to propose a sin-
gle classification scheme, however alluring it would
13
Relation Training data Test data Agreement Example
positive set size positive set size (independent tagging)
Cause-Effect 52.1% 140 51.3% 80 86.1% laugh (cause) wrinkles (effect)
Instrument-Agency 50.7% 140 48.7% 78 69.6% laser (instrument) printer (agency)
Product-Producer 60.7% 140 66.7% 93 68.5% honey (product) bee (producer)
Origin-Entity 38.6% 140 44.4% 81 77.8% message (entity) from outer-space (origin)
Theme-Tool 41.4% 140 40.8% 71 47.8% news (theme) conference(tool)
Part-Whole 46.4% 140 36.1% 72 73.2% the door (part) of the car (whole)
Content-Container 46.4% 140 51.4% 74 69.1% the apples (content) in the basket (container)
Table 1: Data set statistics
be to try to design a unified standard ? it would be
likely to have shortcomings just as any of the others
we have just reviewed. Instead, we have decided to
focus on separate semantic relations that many re-
searchers list in their relation sets. We have built an-
notated data sets for seven such relations. Every data
set supports a separate binary classification task.
2 Building the Annotated Data Sets
Ours is a new evaluation task, so we began with data
set creation and annotation guidelines. The data set
that Nastase and Szpakowicz (2003) created had re-
lation labels and part-of-speech and WordNet sense
annotations, to facilitate classification. (Moldovan
et al, 2004; Girju et al, 2005) gave the annotators
an example of each phrase in a sentence along with
WordNet senses and position of arguments. Our
annotations include all these, to support a variety
of methods (since we work with relations between
nominals, the part of speech is always noun). We
have used WordNet 3.0 on the Web and sense index
tags.
We chose the following semantic relations:
Cause-Effect, Content-Container, Instrument-
Agency, Origin-Entity, Part-Whole, Product-
Producer and Theme-Tool. We wrote seven detailed
definitions, including restrictions and conventions,
plus prototypical positive and near-miss negative
examples. For each relation separately, we based
data collection on wild-card search patterns that
Google allows. We built the patterns manually,
following Hearst (1992) and Nakov and Hearst
(2006). Instances of the relation Content-Container,
for example, come up in response to queries such as
?* contains *?, ?* holds *?, ?the * in the *?. Fol-
lowing the model of the Senseval-3 English Lexical
Sample Task, we set out to collect 140 training and
at least 70 test examples per relation, so we had a
number of different patterns to ensure variety. We
also aimed to collect a balanced number of positive
and negative examples. The use of heuristic patterns
to search for both positive and negative examples
should naturally result in negative examples that
are near misses. We believe that near misses are
more useful for supervised learning than negative
examples that are generated randomly.
?Among the contents of the <e1>vessel</e1>
were a set of carpenter?s <e2>tools</e2>, sev-
eral large storage jars, ceramic utensils, ropes and
remnants of food, as well as a heavy load of ballast
stones.?
WordNet(e1) = ?vessel%1:06:00::?,
WordNet(e2) = ?tool%1:06:00::?,
Content-Container(e2, e1) = ?true?,
Query = ?contents of the * were a?
Figure 1: Annotations illustrated
Figure 1 illustrates the annotations. We tag the
nominals, so parsing or chunking is not necessary.
For Task 4, we define a nominal as a noun or base
noun phrase, excluding names entities. A base noun
phrase, e.g., lawn or lawn mower, is a noun with pre-
modifiers. We also exclude complex noun phrases
(e.g., with attached prepositional phrases ? the en-
gine of the lawn mower).
The procedure was the same for each relation.
One person gathered the sample sentences (aim-
ing approximately for a similar number of positive
and negative examples) and tagged the entities; two
other people annotated the sentences with WordNet
senses and classified the relations. The detailed re-
lation definitions and the preliminary discussions of
positive and negative examples served to maximize
the agreement between the annotators. They first
classified the data independently, then discussed ev-
ery disagreement and looked for consensus. Only
the agreed-upon examples went into the data sets.
Next, we split each data set into 140 training and
no fewer than 70 test examples. (We published the
training set for the Content-Container relation as de-
velopment data two months before the test set.) Ta-
ble 1 shows the number of positive and negative ex-
14
amples for each relation.1
The average inter-annotator agreement on rela-
tions (true/false) after the independent annotation
step was 70.3%, and the average agreement on
WordNet sense labels was 71.9%. In the process of
arriving at a consensus between annotators, the def-
inition of each relation was revised to cover explic-
itly cases where there had been disagreement. We
expect that these revised definitions would lead to
much higher levels of agreement than the original
definitions did.
3 The Participants
The task of classifying semantic relations between
nominals has attracted the participation of 14 teams
who submitted 15 systems. Table 4 lists the sys-
tems, the authors and their affiliations, and brief de-
scriptions. The systems? performance information
in terms of precision, recall, F -measure and accu-
racy, macroaveraged over all relations, appears in
Table 3. We computed these measures as described
in Lewis (1991).
We distinguish four categories of systems based
on the type of information used ? WordNet senses
and/or Google queries:
A ? WordNet = NO & Query = NO;
B ? WordNet = YES & Query = NO;
C ? WordNet = NO & Query = YES;
D ? WordNet = YES & Query = YES.
WordNet = ?YES? or WordNet = ?NO? tells us
only whether a system uses the WordNet sense la-
bels in the data sets. A system may use WordNet
internally for varied purposes, but ignore our sense
labels; such a system would be in category A or C .
Based on the input variation, each submitted system
may have up to 4 variations ? A,B,C,D.
Table 2 presents three baselines for a relation.
Majority always guesses either ?true? or ?false?,
whichever is the majority in the test set (maximizes
accuracy). Alltrue always guesses ?true? (maxi-
mizes recall). Probmatch randomly guesses ?true?
(?false?) with the probability matching the distribu-
tion of ?true? (?false?) in the test dataset (balances
precision and recall).
We present the results in Table 3 grouped by cat-
egory, to facilitate system comparison.
1As this paper serves also as a documentation of the data set,
the order of relations in the table is the same as in the data set.
Type P R F Acc
majority 81.3 42.9 30.8 57.0
alltrue 48.5 100.0 64.8 48.5
probmatch 48.5 48.5 48.5 51.7
Table 2: Baselines: precision, recall, F -measure and
accuracy averaged over the 7 binary classifications.
Team P R F Acc
A ? WordNet = NO & Query = NO
UCD-FC 66.1 66.7 64.8 66.0
ILK 60.5 69.5 63.8 63.5
UCB? 62.7 63.0 62.7 65.4
UMELB-B 61.5 55.7 57.8 62.7
UTH 56.1 57.1 55.9 58.8
UC3M 48.2 40.3 43.1 49.9
avg?stdev 59.2?6.3 58.7?10.5 58.0?8.1 61.1?6.0
B ? WordNet = YES & Query = NO
UIUC? 79.7 69.8 72.4 76.3
FBK-IRST 70.9 73.4 71.8 72.9
ILK 72.8 70.6 71.5 73.2
UCD-S1 69.9 64.6 66.8 71.4
UCD-PN 62.0 71.7 65.4 67.0
UC3M 66.7 62.8 64.3 67.2
CMU-AT 55.7 66.7 60.4 59.1
UCD-FC 66.4 58.1 60.3 63.6
UMELB-A 61.7 56.8 58.7 62.5
UVAVU 56.8 56.3 56.1 57.7
LCC-SRN 55.9 57.8 51.4 53.7
avg ? stdev 65.3?7.7 64.4?6.5 63.6?6.9 65.9?7.2
C ? WordNet = NO & Query = YES
UCB? 64.2 66.5 65.1 67.0
UCD-FC 66.1 66.7 64.8 66.0
UC3M 49.4 43.9 45.3 50.1
avg?stdev 59.9?9.1 59.0?13.1 58.4?11.3 61.0?9.5
D ? WordNet = YES & Query = YES
UTD-HLT-CG 67.3 65.3 62.6 67.2
UCD-FC 66.4 58.1 60.3 63.6
UC3M 60.9 57.8 58.8 62.3
avg?stdev 64.9?3.5 60.4?4.2 60.6?1.9 64.4?2.5
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 3: System performance grouped by category.
Precision, recall, F -measure and accuracy macro-
averaged over each system?s performance on all 7
relations.
4 Discussion
The highest average accuracy on Task 4 was 76.3%.
Therefore, the average initial agreement between an-
notators (70.3%), before revising the definitions, is
not an upper bound on the accuracy that can be
achieved. That the initial agreement between anno-
tators is not a good indicator of the accuracy that can
be achieved is also supported by the low correlation
15
System Institution Team Description System Type
UVAVU Univ. of Amsterdam
TNO Science & Industry
Free Univ. Amsterdam
Sophia Katrenko
Willem Robert van
Hage
similarity measures in WordNet; syn-
tactic dependencies; lexical patterns;
logical combination of attributes
B
CMU -AT Carnegie Mellon Univ. Alicia Tribble
Scott E. Fahlman
WordNet; manually-built ontologies;
Scone Knowledge Representation Lan-
guage; semantic distance
B
ILK Tilburg University Caroline Sporleder
Roser Morante
Antal van den Bosch
semantic clusters based on noun simi-
larity; WordNet supersenses; grammat-
ical relation between entities; head of
sentence; WEKA
A, B
FBK-IRST Fondazione Bruno
Kessler - IRST
Claudio Giuliano
Alberto Lavelli
Daniele Pighin
Lorenza Romano
shallow and deep syntactic information;
WordNet synsets and hypernyms; ker-
nel methods; SVM
B
LCC-SRN Language Computer
Corp.
Adriana Badulescu named entity recognition; lexical, se-
mantic, syntactic features; decision tree
and semantic scattering
B
UMELB-A Univ. of Melbourne Su Kim
Timothy Baldwin
sense collocations; similarity of con-
stituents; extending training and testing
data using similar words
B
UMELB-B Univ. of Melbourne Su Kim
Timothy Baldwin
similarity of nearest-neighbor matching
over the union of senses for the two
nominals; cascaded tagging with de-
creasing thresholds
A
UCB? Univ. of California at
Berkeley
Preslav Nakov
Marti Hearst
VSM; joining terms; KNN-1 A, C
UC3M Univ. Carlos III of Madrid Isabel Segura Bedmar
Doaa Sammy
Jose? Luis Mart??nez
Ferna?ndez
WordNet path; syntactic features; SVM A, B, C, D
UCD-S1 Univ. College Dublin Cristina Butnariu
Tony Veale
lexical-semantic categories from Word-
Net; syntactic patterns from corpora,
SVM
B
UCD-FC Univ. College Dublin Fintan Costello WordNet; additional noun compounds
tagged corpus; Naive Bayes
A, B, C, D
UCD-PN Univ. College Dublin Paul Nulty WordNet supersenses; web-based fre-
quency counts for specific joining
terms; WEKA (SMO)
B
UIUC? Univ. of Illinois at Urbana
Champaign
Roxana Girju
Brandon Beamer
Suma Bhat
Brant Chee
Andrew Fister
Alla Rozovskaya
features based on WordNet, NomLex-
PLUS, grammatical roles, lexico-
syntactic patterns, semantic parses
B
UTD-HLT-CG Univ. of Texas at Dallas Cristina Nicolae
Garbiel Nicolae
Sanda Harabagiu
lexico-semantic features from Word-
Net, VerbNet; semantic features from a
PropBank parser; dependency features
D
UTH Univ. of Tokio Eiji Aramaki
Takeshi Imai
Kengo Miyo
Kazuhiko Ohe
joining phrases; physical size for enti-
ties; web-mining; SVM
A
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 4: Short description of the teams and the participating systems.
16
Relation Team Type P R F Acc Test size Base-F Base-Acc Avg. rank
Cause-Effect UIUC B4 69.5 100.0 82.0 77.5 80 67.8 51.2 3.4
Instrument-Agency FBK-IRST B4 76.9 78.9 77.9 78.2 78 65.5 51.3 3.4
Product-Producer UCD-S1 B4 80.6 87.1 83.7 77.4 93 80.0 66.7 1.7
Origin-Entity ILK B3 70.6 66.7 68.6 72.8 81 61.5 55.6 6.0
Theme-Tool ILK B4 69.0 69.0 69.0 74.6 71 58.0 59.2 6.0
Part-Whole UC3M B4 72.4 80.8 76.4 81.9 72 53.1 63.9 4.5
Content-Container UIUC B4 93.1 71.1 80.6 82.4 74 67.9 51.4 3.1
Table 5: The best results per relation. Precision, recall, F -measure and accuracy macro-averaged over each
system?s performance on all 7 relations. Base-F shows the baseline F -measure (alltrue), Base-Acc ? the
baseline accuracy score (majority). The last column shows the average rank for each relation.
of 0.15 between the Acc column in Table 5 and the
Agreement column in Table 1.
We performed various analyses of the results,
which we summarize here in four questions. We
write Xi to refer to four possible system categories
(Ai, Bi, Ci, and Di) with four possible amounts of
training data (X1 for training examples 1 to 35, X2
for 1 to 70, X3 for 1 to 105, and X4 for 1 to 140).
Does more training data help?
Overall, the results suggest that more training data
improves the performance. There were 17 cases in
which we had results for all four possible amounts
of training data. All average F -measure differences,
F (X4)?F (Xi) where X = A to D, i = 1 to 3, for
these 17 sets of results are statistically significant:
F (X4)?F (X1): N = 17, avg = 8.3, std = 5.8, min =
1.1, max = 19.6, t-value = ?5.9, p-value = 0.00001.
F (X4)?F (X2): N = 17, avg = 4.0, std = 3.7, min =
?3.5, max = 10.5, t-value = 4.5, p-value = 0.0002.
F (X4)?F (X3): N = 17, avg = 0.9, std = 1.7, min =
?2.6, max = 4.7, t-value = 2.1, p-value = 0.03.
Does WordNet help?
The statistics show that WordNet is important, al-
though the contribution varies across systems. Three
teams submitted altogether 12 results both for A1?
A4 and B1?B4. The average F -measure difference,
F (Bi)?F (Ai), i = 1 to 4, is significant:
F (Bi)?F (Ai): N = 12, avg = 6.1, std = 8.4, min =
?4.5, max = 21.2, t-value = ?2.5, p-value = 0.01.
The results of the UCD-FC system actually went
down when WordNet was used. The statistics for the
remaining two teams, however, are a bit better:
F (Bi)?F (Ai): N = 8, avg = 10.4, std = 6.7, min =
?1.0, max = 21.2, t-value = ?4.4, p-value = 0.002.
Does knowing the query help?
Overall, knowing the query did not seem to improve
the results. Three teams submitted 12 results both
for A1?A4 and C1?C4. The average F -measure dif-
ference, F (Ci)?F (Ai) , i = 1 to 4, is not significant:
F (Ci)?F (Ai): N = 12, avg = 0.9, std = 1.8, min =
?2.0, max = 5.0, t-value = ?1.6, p-value = 0.06.
Again, the UCD-FC system differed from the
other systems in that the A and C scores were iden-
tical, but even averaging over the remaining two sys-
tems and 8 cases does not show a statistically signif-
icant advantage:
F (Ci)?F (Ai): N = 8, avg = 1.3, std = 2.2, min =
?2.0, max = 5.0, t-value = ?1.7, p-value = 0.07.
Are some relations harder to classify?
Table 5 shows the best results for each relation in
terms of precision, recall, and F -measure, per team
and system category. Column Base-F presents the
baseline F -measure (alltrue), while Base-Acc the
baseline accuracy score (majority). For all seven re-
lations, the best team significantly outperforms the
baseline. The category of the best-scoring system
in almost every case is B4 (only the ILK B4 system
scored second on the Origin-Entity relation).
Table 5 suggests that some relations are more dif-
ficult to classify than others. The best F -measure
ranges from 83.7 for Product?Producer to 68.6 for
Origin?Entity. The difference between the best F -
measure and the baseline F -measure ranges from
23.3 for Part-Whole to 3.7 for Product-Producer.
The difference between the best accuracy and the
baseline accuracy ranges from 31.0 for Content-
Container to 10.7 for Product-Producer.
The F column shows the best result for each rela-
tion, but similar differences among the relations may
be observed when all results are pooled. The Avg.
rank column computes the average rank of each re-
lation in the ordered list of relations generated by
each system. For example, Product?Producer is of-
ten listed as the first or the second easiest relation
(with an average rank of 1.7), while Origin?Entity
and Theme?Tool are identified as the most difficult
17
relations to classify (with average ranks of 6.0).
5 Conclusion
This paper describes a new semantic evaluation task,
Classification of Semantic Relations between Nom-
inals. We have accomplished our goal of providing
a framework and a benchmark data set to allow for
comparisons of methods for this task. The data in-
cluded different types of information ? lexical se-
mantic information, context, query used ? meant to
facilitate the analysis of useful sources of informa-
tion for determining the semantic relation between
nominals. The results that the participating systems
have reported show successful approaches to this
difficult task, and the advantages of using lexical se-
mantic information.
The success of the task ? measured in the inter-
est of the community and the results of the partici-
pating systems ? shows that the framework and the
data are useful resources. By making this collection
freely accessible, we encourage further research into
this domain and integration of semantic relation al-
gorithms in high-end applications.
Acknowledgments
We thank Eneko Agirre, Llu??s Ma`rquez and Richard
Wicentowski, the organizers of SemEval 2007, for
their guidance and prompt support in all organiza-
tional matters. We thank Marti Hearst for valu-
able advice throughout the task description and de-
bates on semantic relation definitions. We thank the
anonymous reviewers for their helpful comments.
References
T. Chklovski and P. Pantel. 2004. Verbocean: Mining the
web for fine-grained semantic verb relations. In Proc.
Conf. on Empirical Methods in Natural Language Pro-
cessing, EMNLP-04, pages 33?40, Barcelona, Spain.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19:479?496.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. 14th International
Conf. on Computational Linguistics (COLING-92),
pages 539?545.
M. Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
D.D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the Speech and Natural Language
Workshop, pages 312?318, Asilomar.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and
R. Girju. 2004. Models for the semantic classification
of noun phrases. In Proc. Computational Lexical Se-
mantics Workshop at HLT-NAACL 2004, pages 60?67,
Boston, MA.
P. Nakov and M. Hearst. 2006. Using verbs to char-
acterize noun-noun relations. In Proc. Twelfth Inter-
national Conf. in Artificial Intelligence (AIMSA-06),
pages 233?244, Varna,Bulgaria.
V. Nastase and S. Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301, Tilburg, The Netherlands.
V. Nastase, J. Sayyad-Shirabad, M. Sokolova, and S. Sz-
pakowicz. 2006. Learning noun-modifier semantic
relations with corpus-based and WordNet-based fea-
tures. In Proc. 21st National Conf. on Artificial Intel-
ligence (AAAI 2006), pages 781?787, Boston, MA.
B. Rosario and M. Hearst. 2001. Classifying the seman-
tic relations in noun-compounds via domain-specific
lexical hierarchy. In Proc. 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), pages 82?90.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The de-
scent of hierarchy, and selection in relational seman-
tics. In Proc. 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages 417?
424, Philadelphia, PA.
M. Stephens, M. Palakal, S. Mukhopadhyay, and R. Raje.
2001. Detecting gene relations from MEDLINE ab-
stracts. In Proc. Sixth Annual Pacific Symposium on
Biocomputing, pages 483?496.
M. Tatu and D. Moldovan. 2005. A semantic approach to
recognizing textual entailment. In Proc. Human Lan-
guage Technology Conf. and Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP
2005), pages 371?378, Vancouver, Canada.
P.D. Turney and M.L. Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60(1-3):251?278.
P.D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. Nineteenth Interna-
tional Joint Conf. on Artificial Intelligence (IJCAI-05),
pages 1136?1141, Edinburgh, Scotland.
18
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 100?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
We present a brief overview of the main
challenges in understanding the semantics of
noun compounds and consider some known
methods. We introduce a new task to be
part of SemEval-2010: the interpretation of
noun compounds using paraphrasing verbs
and prepositions. The task is meant to provide
a standard testbed for future research on noun
compound semantics. It should also promote
paraphrase-based approaches to the problem,
which can benefit many NLP applications.
1 Introduction
Noun compounds (NCs) ? sequences of two or more
nouns acting as a single noun,1 e.g., colon cancer
tumor suppressor protein ? are abundant in English
and pose a major challenge to the automatic anal-
ysis of written text. Baldwin and Tanaka (2004)
calculated that 3.9% and 2.6% of the tokens in
the Reuters corpus and the British National Corpus
(BNC), respectively, are part of a noun compound.
Compounding is also an extremely productive pro-
cess in English. The frequency spectrum of com-
pound types follows a Zipfian or power-law distribu-
tion (O? Se?aghdha, 2008), so in practice many com-
pound tokens encountered belong to a ?long tail?
of low-frequency types. For example, over half of
the two-noun NC types in the BNC occur just once
(Lapata and Lascarides, 2003). Even for relatively
frequent NCs that occur ten or more times in the
BNC, static English dictionaries give only 27% cov-
erage (Tanaka and Baldwin, 2003). Taken together,
1We follow the definition in (Downing, 1977).
the factors of high frequency and high productiv-
ity mean that achieving robust NC interpretation is
an important goal for broad-coverage semantic pro-
cessing. NCs provide a concise means of evoking a
relationship between two or more nouns, and natu-
ral language processing (NLP) systems that do not
try to recover these implicit relations from NCs are
effectively discarding valuable semantic informa-
tion. Broad coverage should therefore be achieved
by post-hoc interpretation rather than pre-hoc enu-
meration, since it is impossible to build a lexicon of
all NCs likely to be encountered.
The challenges presented by NCs and their se-
mantics have generated significant ongoing interest
in NC interpretation in the NLP community. Repre-
sentative publications include (Butnariu and Veale,
2008; Girju, 2007; Kim and Baldwin, 2006; Nakov,
2008b; Nastase and Szpakowicz, 2003; O? Se?aghdha
and Copestake, 2007). Applications that have been
suggested include Question Answering, Machine
Translation, Information Retrieval and Information
Extraction. For example, a question-answering sys-
tem may need to determine whether headaches in-
duced by caffeine withdrawal is a good paraphrase
for caffeine headaches when answering questions
about the causes of headaches, while an information
extraction system may need to decide whether caf-
feine withdrawal headache and caffeine headache
refer to the same concept when used in the same
document. Similarly, a machine translation system
facing the unknown NC WTO Geneva headquarters
might benefit from the ability to paraphrase it as
Geneva headquarters of the WTO or as WTO head-
quarters located in Geneva. Given a query like can-
100
cer treatment, an information retrieval system could
use suitable paraphrasing verbs like relieve and pre-
vent for page ranking and query refinement.
In this paper, we introduce a new task, which will
be part of the SemEval-2010 competition: NC inter-
pretation using paraphrasing verbs and prepositions.
The task is intended to provide a standard testbed
for future research on noun compound semantics.
We also hope that it will promote paraphrase-based
approaches to the problem, which can benefit many
NLP applications.
The remainder of the paper is organized as fol-
lows: Section 2 presents a brief overview of the
existing approaches to NC semantic interpretation
and introduces the one we will adopt for SemEval-
2010 Task 9; Section 3 provides a general descrip-
tion of the task, the data collection, and the evalua-
tion methodology; Section 4 offers a conclusion.
2 Models of Relational Semantics in NCs
2.1 Inventory-Based Semantics
The prevalent view in theoretical and computational
linguistics holds that the semantic relations that im-
plicitly link the nouns of an NC can be adequately
enumerated via a small inventory of abstract re-
lational categories. In this view, mountain hut,
field mouse and village feast all express ?location
in space?, while the relation implicit in history book
and nativity play can be characterized as ?topicality?
or ?aboutness?. A sample of some of the most influ-
ential relation inventories appears in Table 1.
Levi (1978) proposes that complex nominals ?
a general concept grouping together nominal com-
pounds (e.g., peanut butter), nominalizations (e.g.,
dream analysis) and non-predicative noun phrases
(e.g., electric shock) ? are derived through the com-
plementary processes of recoverable predicate dele-
tion and nominalization; each process is associated
with its own inventory of semantic categories. Table
1 lists the categories for the former.
Warren (1978) posits a hierarchical classifica-
tion scheme derived from a large-scale corpus study
of NCs. The top-level relations in her hierar-
chy are listed in Table 1, while the next level
subdivides CONSTITUTE into SOURCE-RESULT,
RESULT-SOURCE and COPULA; COPULA is then
further subdivided at two additional levels.
In computational linguistics, popular invento-
ries of semantic relations have been proposed by
Nastase and Szpakowicz (2003) and Girju et al
(2005), among others. The former groups 30 fine-
grained relations into five coarse-grained super-
categories, while the latter is a flat list of 21 re-
lations. Both schemes are intended to be suit-
able for broad-coverage analysis of text. For spe-
cialized applications, however, it is often useful
to use domain-specific relations. For example,
Rosario and Hearst (2001) propose 18 abstract rela-
tions for interpreting NCs in biomedical text, e.g.,
DEFECT, MATERIAL, PERSON AFFILIATED,
ATTRIBUTE OF CLINICAL STUDY.
Inventory-based analyses offer significant advan-
tages. Abstract relations such as ?location? and ?pos-
session? capture valuable generalizations about NC
semantics in a parsimonious framework. Unlike
paraphrase-based analyses (Section 2.2), they are
not tied to specific lexical items, which may them-
selves be semantically ambiguous. They also lend
themselves particularly well to automatic interpreta-
tion methods based on multi-class classification.
On the other hand, relation inventories have been
criticized on a number of fronts, most influentially
by Downing (1977). She argues that the great vari-
ety of NC relations makes listing them all impos-
sible; creative NCs like plate length (?what your
hair is when it drags in your food?) are intuitively
compositional, but cannot be assigned to any stan-
dard inventory category. A second criticism is that
restricted inventories are too impoverished a repre-
sentation scheme for NC semantics, e.g., headache
pills and sleeping pills would both be analyzed as
FOR in Levi?s classification, but express very differ-
ent (indeed, contrary) relationships. Downing writes
(p. 826): ?These interpretations are at best reducible
to underlying relationships. . . , but only with the loss
of much of the semantic material considered by sub-
jects to be relevant or essential to the definitions.?
A further drawback associated with sets of abstract
relations is that it is difficult to identify the ?correct?
inventory or to decide whether one proposed classi-
fication scheme should be favored over another.
2.2 Interpretation Using Verbal Paraphrases
An alternative approach to NC interpretation asso-
ciates each compound with an explanatory para-
101
Author(s) Relation Inventory
Levi (1978) CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT
Warren (1978) POSSESSION, LOCATION, PURPOSE, ACTIVITY-ACTOR, RESEMBLANCE, CONSTITUTE
Nastase and CAUSALITY (cause, effect, detraction, purpose),
Szpakowicz PARTICIPANT (agent, beneficiary, instrument, object property,
(2003) object, part, possessor, property, product, source, whole, stative),
QUALITY (container, content, equative, material, measure, topic, type),
SPATIAL (direction, location at, location from, location),
TEMPORALITY (frequency, time at, time through)
Girju et al (2005) POSSESSION, ATTRIBUTE-HOLDER, AGENT, TEMPORAL, PART-WHOLE, IS-A, CAUSE,
MAKE/PRODUCE, INSTRUMENT, LOCATION/SPACE, PURPOSE, SOURCE, TOPIC, MANNER,
MEANS, THEME, ACCOMPANIMENT, EXPERIENCER, RECIPIENT, MEASURE, RESULT
Lauer (1995) OF, FOR, IN, AT, ON, FROM, WITH, ABOUT
Table 1: Previously proposed inventories of semantic relations for noun compound interpretation. The first two come
from linguistic theories; the rest have been proposed in computational linguistics.
phrase. Thus, cheese knife and kitchen knife can be
expanded as a knife for cutting cheese and a knife
used in a kitchen, respectively. In the paraphrase-
based paradigm, semantic relations need not come
from a small set; it is possible to have many sub-
tle distinctions afforded by the vocabulary of the
paraphrasing language (in our case, English). This
paradigm avoids the problems of coverage and rep-
resentational poverty, which Downing (1977) ob-
served in inventory-based approaches. It also re-
flects cognitive-linguistic theories of NC semantics,
in which compounds are held to express underlying
event frames and whose constituents are held to de-
note event participants (Ryder, 1994).
Lauer (1995) associates NC semantics with
prepositional paraphrases. As Lauer only consid-
ers a handful of prepositions (about, at, for,
from, in, of, on, with), his model is es-
sentially inventory-based. On the other hand, noun-
preposition co-occurrences can easily be identified
in a corpus, so an automatic interpretation can be
implemented through simple unsupervised methods.
The disadvantage of this approach is the absence of a
one-to-one mapping from prepositions to meanings;
prepositions can be ambiguous (of indicates many
different relations) or synonymous (at, in and on
all express ?location?). This concern arises with all
paraphrasing models, but it is exacerbated by the re-
stricted nature of prepositions. Furthermore, many
NCs cannot be paraphrased adequately with prepo-
sitions, e.g., woman driver, honey bee.
A richer, more flexible paraphrasing model is af-
forded by the use of verbs. In such a model, a honey
bee is a bee that produces honey, a sleeping pill
is a pill that induces sleeping and a headache pill
is a pill that relieves headaches. In some previous
computational work on NC interpretation, manually
constructed dictionaries provided typical activities
or functions associated with nouns (Finin, 1980; Is-
abelle, 1984; Johnston and Busa, 1996). It is, how-
ever, impractical to build large structured lexicons
for broad-coverage systems; these methods can only
be applied to specialized domains. On the other
hand, we expect that the ready availability of large
text corpora should facilitate the automatic mining
of rich paraphrase information.
The SemEval-2010 task we present here builds on
the work of Nakov (Nakov and Hearst, 2006; Nakov,
2007; Nakov, 2008b), where NCs are paraphrased
by combinations of verbs and prepositions. Given
the problem of synonymy, we do not provide a sin-
gle correct paraphrase for a given NC but a prob-
ability distribution over a range of candidates. For
example, highly probable paraphrases for chocolate
bar are bar made of chocolate and bar that tastes
like chocolate, while bar that eats chocolate is very
unlikely. As described in Section 3.3, a set of gold-
standard paraphrase distributions can be constructed
by collating responses from a large number of hu-
man subjects.
In this framework, the task of interpretation be-
comes one of identifying the most likely paraphrases
for an NC. Nakov (2008b) and Butnariu and Veale
(2008) have demonstrated that paraphrasing infor-
mation can be collected from corpora in an un-
supervised fashion; we expect that participants in
102
SemEval-2010 Task 9 will further develop suitable
techniques for this problem. Paraphrases of this kind
have been shown to be useful in applications such as
machine translation (Nakov, 2008a) and as an inter-
mediate step in inventory-based classification of ab-
stract relations (Kim and Baldwin, 2006; Nakov and
Hearst, 2008). Progress in paraphrasing is therefore
likely to have follow-on benefits in many areas.
3 Task Description
The description of the task we present below is pre-
liminary. We invite the interested reader to visit the
official Website of SemEval-2010 Task 9, where up-
to-date information will be published; there is also a
discussion group and a mailing list.2
3.1 Preliminary Study
In a preliminary study, we asked 25-30 human sub-
jects to paraphrase 250 noun-noun compounds us-
ing suitable paraphrasing verbs. This is the Levi-
250 dataset (Levi, 1978); see (Nakov, 2008b) for de-
tails.3 The most popular paraphrases tend to be quite
apt, while some less frequent choices are question-
able. For example, for chocolate bar we obtained
the following paraphrases (the number of subjects
who proposed each one is shown in parentheses):
contain (17); be made of (16); be made
from (10); taste like (7); be composed
of (7); consist of (5); be (3); have (2);
smell of (2); be manufactured from (2);
be formed from (2); melt into (2); serve
(1); sell (1); incorporate (1); be made with
(1); be comprised of (1); be constituted
by (1); be solidified from (1); be flavored
with (1); store (1); be flavored with (1); be
created from (1); taste of (1)
3.2 Objective
We propose a task in which participating systems
must estimate the quality of paraphrases for a test
set of NCs. A list of verb/preposition paraphrases
will be provided for each NC, and for each list a
participating system will be asked to provide aptness
2Please follow the Task #9 link at the SemEval-2010 home-
page http://semeval2.fbk.eu
3This dataset is available from http://sourceforge.
net/projects/multiword/
scores that correlate well (in terms of frequency dis-
tribution) with the human judgments collated from
our test subjects.
3.3 Datasets
Trial/Development Data. As trial/development
data, we will release the previously collected para-
phrase sets for the Levi-250 dataset (after further
review and cleaning). This dataset consists of 250
noun-noun compounds, each paraphrased by 25-30
human subjects (Nakov, 2008b).
Test Data. The test data will consist of approx-
imately 300 NCs, each accompanied by a set of
paraphrasing verbs and prepositions. Following the
methodology of Nakov (2008b), we will use the
Amazon Mechanical Turk Web service4 to recruit
human subjects. This service offers an inexpensive
way to recruit subjects for tasks that require human
intelligence, and provides an API which allows a
computer program to easily run tasks and collate
the responses from human subjects. The Mechanical
Turk is becoming a popular means to elicit and col-
lect linguistic intuitions for NLP research; see Snow
et al (2008) for an overview and a discussion of is-
sues that arise.
We intend to recruit 100 annotators for each NC,
and we will require each annotator to paraphrase
at least five NCs. Annotators will be given clear
instructions and will be asked to produce one or
more paraphrases for a given NC. To help us filter
out subjects with an insufficient grasp of English or
an insufficient interest in the task, annotators will
be asked to complete a short and simple multiple-
choice pretest on NC comprehension before pro-
ceeding to the paraphrasing step.
Post-processing. We will manually check the
trial/development data and the test data. Depending
on the quality of the paraphrases, we may decide to
drop the least frequent verbs.
License. All data will be released under the Cre-
ative Commons Attribution 3.0 Unported license5.
3.4 Evaluation
Single-NC Scores. For each NC, we will compare
human scores (our gold standard) with those pro-
posed by each participating system. We have con-
4http://www.mturk.com
5http://creativecommons.org/licenses/by/3.0/
103
sidered three scores: (1) Pearson?s correlation, (2)
cosine similarity, and (3) Spearman?s rank correla-
tion.
Pearson?s correlation coefficient is a standard
measure of the correlation strength between two dis-
tributions; it can be calculated as follows:
? = E(XY ) ? E(X)E(Y )?
E(X2) ? [E(X)]2?E(Y 2) ? [E(Y )]2
(1)
where X = (x1, . . . , xn) and Y = (y1, . . . , yn) are
vectors of numerical scores for each paraphrase pro-
vided by the humans and the competing systems, re-
spectively, n is the number of paraphrases to score,
and E(X) is the expectation of X .
Cosine correlation coefficient is another popu-
lar alternative and was used by Nakov and Hearst
(2008); it can be seen as an uncentered version of
Pearson?s correlation coefficient:
? = X.Y?X??Y ? (2)
Spearman?s rank correlation coefficient is suit-
able for comparing rankings of sets of items; it is
a special case of Pearson?s correlation, derived by
considering rank indices (1,2,. . . ) as item scores . It
is defined as follows:
? = n
?xiyi ? (?xi)(? yi)?
n?x2i ? (
?xi)2
?
n? y2i ? (
? yi)2
(3)
One problem with using Spearman?s rank coef-
ficient for the current task is the assumption that
swapping any two ranks has the same effect. The
often-skewed nature of paraphrase frequency distri-
butions means that swapping some ranks is intu-
itively less ?wrong? than swapping others. Consider,
for example, the following list of human-proposed
paraphrasing verbs for child actor, which is given in
Nakov (2007):
be (22); look like (4); portray (3); start as
(1); include (1); play (1); have (1); involve
(1); act like (1); star as (1); work as (1);
mimic (1); pass as (1); resemble (1); be
classified as (1); substitute for (1); qualify
as (1); act as (1)
Clearly, a system that swaps the positions for
be (22) and look like (4) for child actor will
have made a significant error, while swapping con-
tain (17) and be made of (16) for chocolate bar (see
Section 3.1) would be less inappropriate. However,
Spearman?s coefficient treats both alterations iden-
tically since it only looks at ranks; thus, we do not
plan to use it for official evaluation, though it may
be useful for post-hoc analysis.
Final Score. A participating system?s final score
will be the average of the scores it achieves over all
test examples.
Scoring Tool. We will provide an automatic eval-
uation tool that participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
4 Conclusion
We have presented a noun compound paraphrasing
task that will run as part of SemEval-2010. The goal
of the task is to promote and explore the feasibility
of paraphrase-based methods for compound inter-
pretation. We believe paraphrasing holds some key
advantages over more traditional inventory-based
approaches, such as the ability of paraphrases to rep-
resent fine-grained and overlapping meanings, and
the utility of the resulting paraphrases for other ap-
plications such as Question Answering, Information
Extraction/Retrieval and Machine Translation.
The proposed paraphrasing task is predicated on
two important assumptions: first, that paraphrasing
via a combination of verbs and prepositions pro-
vides a powerful framework for representing and in-
terpreting the meaning of compositional nonlexical-
ized noun compounds; and second, that humans can
agree amongst themselves about what constitutes a
good paraphrase for any given NC. As researchers in
this area and as proponents of this task, we believe
that both assumptions are valid, but if the analysis
of the task were to raise doubts about either assump-
tion (e.g., by showing poor agreement amongst hu-
man annotators), then this in itself would be a mean-
ingful and successful output of the task. As such,
we anticipate that the task and its associated dataset
will inspire further research, both on the theory and
development of paraphrase-based compound inter-
pretation and on its practical applications.
104
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it
right. In Proceedings of the ACL 2004 Workshop on
Multiword Expressions: Integrating Processing, pages
24?31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
81?88.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Timothy Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.D. Dissertation, University
of Illinois, Urbana, Illinois.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
568?575.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceedings of the 10th International Con-
ference on Computational Linguistics, pages 509?516.
Michael Johnston and Frederica Busa. 1996. Qualia
structure and the compositional interpretation of com-
pounds. In Proceedings of the ACL 1996 Workshop on
Breadth and Depth of Semantic Lexicons, pages 77?
88.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb
semantics. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL 2006) Main Conference Poster Ses-
sions, pages 491?498.
Mirella Lapata and Alex Lascarides. 2003. Detecting
novel compounds: the role of distributional evidence.
In Proceedings of the 10th conference of the European
chapter of the Association for Computational Linguis-
tics (EACL 2003), pages 235?242.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Preslav Nakov andMarti A. Hearst. 2006. Using verbs to
characterize noun-noun relations. In LNCS vol. 4183:
Proceedings of the 12th international conference on
Artificial Intelligence: Methodology, Systems and Ap-
plications (AIMSA 2006), pages 233?244. Springer.
Preslav Nakov and Marti A. Hearst. 2008. Solving re-
lational similarity problems using the web as a cor-
pus. In Proceedings of the 46th Annual Meeting of the
Association of Computational Linguistics (ACL 2008),
pages 452?460.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence (ECAI?2008), pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In LNAI
vol. 5253: Proceedings of the 13th international con-
ference on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA 2008), pages 103?117.
Springer.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 57?64.
Diarmuid O? Se?aghdha. 2008. Learning Compound Noun
Semantics. Ph.D. thesis, University of Cambridge.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 82?90.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), pages 254?263.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: a feasibility
study on shallow processing. In Proceedings of the
ACL 2003 workshop on Multiword expressions, pages
17?24.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
105
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx? , Su Nam Kim? , Zornitsa Kozareva? , Preslav Nakov? ,
Diarmuid O? Se?aghdha?, Sebastian Pado?? , Marco Pennacchiotti??,
Lorenza Romano??, Stan Szpakowicz??
Abstract
We present a brief overview of the main
challenges in the extraction of semantic
relations from English text, and discuss the
shortcomings of previous data sets and shared
tasks. This leads us to introduce a new
task, which will be part of SemEval-2010:
multi-way classification of mutually exclusive
semantic relations between pairs of common
nominals. The task is designed to compare
different approaches to the problem and to
provide a standard testbed for future research,
which can benefit many applications in
Natural Language Processing.
1 Introduction
The computational linguistics community has a con-
siderable interest in robust knowledge extraction,
both as an end in itself and as an intermediate step
in a variety of Natural Language Processing (NLP)
applications. Semantic relations between pairs of
words are an interesting case of such semantic
knowledge. It can guide the recovery of useful facts
about the world, the interpretation of a sentence, or
even discourse processing. For example, pears and
bowl are connected in a CONTENT-CONTAINER re-
lation in the sentence ?The bowl contained apples,
?University of Antwerp, iris.hendrickx@ua.ac.be
?University of Melbourne, snkim@csse.unimelb.edu.au
?University of Alicante, zkozareva@dlsi.ua.es
?National University of Singapore, nakov@comp.nus.edu.sg
?University of Cambridge, do242@cl.cam.ac.uk
?University of Stuttgart, pado@stanford.edu
??Yahoo! Inc., pennacc@yahoo-inc.com
??Fondazione Bruno Kessler, romano@fbk.eu
??University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
pears, and oranges.?, while ginseng and taste are in
an ENTITY-ORIGIN relation in ?The taste is not from
alcohol, but from the ginseng.?.
The automatic recognition of semantic relations
can have many applications, such as information
extraction (IE), document summarization, machine
translation, or construction of thesauri and seman-
tic networks. It can also facilitate auxiliary tasks
such as word sense disambiguation, language mod-
eling, paraphrasing or recognizing textual entail-
ment. For example, semantic network construction
can benefit from detecting a FUNCTION relation be-
tween airplane and transportation in ?the airplane
is used for transportation? or a PART-WHOLE rela-
tion in ?the car has an engine?. Similarly, all do-
mains that require deep understanding of text rela-
tions can benefit from knowing the relations that de-
scribe events like ACQUISITION between named en-
tities in ?Yahoo has made a definitive agreement to
acquire Flickr?.
In this paper, we focus on the recognition of se-
mantic relations between pairs of common nomi-
nals. We present a task which will be part of the
SemEval-2010 evaluation exercise and for which we
are developing a new benchmark data set. This data
set and the associated task address three significant
problems encountered in previous work: (1) the def-
inition of a suitable set of relations; (2) the incorpo-
ration of context; (3) the desire for a realistic exper-
imental design. We outline these issues in Section
2. Section 3 describes the inventory of relations we
adopted for the task. The annotation process, the
design of the task itself and the evaluation method-
ology are presented in Sections 4-6.
94
2 Semantic Relation Classification: Issues
2.1 Defining the Relation Inventory
A wide variety of relation classification schemes ex-
ist in the literature, reflecting the needs and granular-
ities of various applications. Some researchers only
investigate relations between named entities or in-
ternal to noun-noun compounds, while others have a
more general focus. Some schemes are specific to a
domain such as biomedical text.
Rosario and Hearst (2001) classify noun com-
pounds from the domain of medicine into 13 classes
that describe the semantic relation between the head
noun and the modifier. Rosario et al (2002) classify
noun compounds using the MeSH hierarchy and a
multi-level hierarchy of semantic relations, with 15
classes at the top level. Stephens et al (2001) pro-
pose 17 very specific classes targeting relations be-
tween genes. Nastase and Szpakowicz (2003) ad-
dress the problem of classifying noun-modifier rela-
tions in general text. They propose a two-level hier-
archy, with 5 classes at the first level and 30 classes
at the second one; other researchers (Kim and Bald-
win, 2005; Nakov and Hearst, 2008; Nastase et al,
2006; Turney, 2005; Turney and Littman, 2005)
have used their class scheme and data set. Moldovan
et al (2004) propose a 35-class scheme to classify
relations in various phrases; the same scheme has
been applied to noun compounds and other noun
phrases (Girju et al, 2005). Lapata (2002) presents a
binary classification of relations in nominalizations.
Pantel and Pennacchiotti (2006) concentrate on five
relations in an IE-style setting. In short, there is little
agreement on relation inventories.
2.2 The Role of Context
A fundamental question in relation classification is
whether the relations between nominals should be
considered out of context or in context. When one
looks at real data, it becomes clear that context does
indeed play a role. Consider, for example, the noun
compound wood shed : it may refer either to a shed
made of wood, or to a shed of any material used to
store wood. This ambiguity is likely to be resolved
in particular contexts. In fact, most NLP applica-
tions will want to determine not all possible relations
between two words, but rather the relation between
two instances in a particular context. While the in-
tegration of context is common in the field of IE (cf.
work in the context of ACE1), much of the exist-
ing literature on relation extraction considers word
pairs out of context (thus, types rather than tokens).
A notable exception is SemEval-2007 Task 4 Clas-
sification of Semantic Relations between Nominals
(Girju et al, 2007; Girju et al, 2008), the first to of-
fer a standard benchmark data set for seven semantic
relations between common nouns in context.
2.3 Style of Classification
The design of SemEval-2007 Task 4 had an im-
portant limitation. The data set avoided the chal-
lenge of defining a single unified standard classifi-
cation scheme by creating seven separate training
and test sets, one for each semantic relation. That
made the relation recognition task on each data set
a simple binary (positive / negative) classification
task.2 Clearly, this does not easily transfer to prac-
tical NLP settings, where any relation can hold be-
tween a pair of nominals which occur in a sentence
or a discourse.
2.4 Summary
While there is a substantial amount of work on re-
lation extraction, the lack of standardization makes
it difficult to compare different approaches. It is
known from other fields that the availability of stan-
dard benchmark data sets can provide a boost to the
advancement of a field. As a first step, SemEval-
2007 Task 4 offered many useful insights into the
performance of different approaches to semantic re-
lation classification; it has also motivated follow-
up research (Davidov and Rappoport, 2008; Ka-
trenko and Adriaans, 2008; Nakov and Hearst, 2008;
O? Se?aghdha and Copestake, 2008).
Our objective is to build on the achievements of
SemEval-2007 Task 4 while addressing its short-
comings. In particular, we consider a larger set of
semantic relations (9 instead of 7), we assume a
proper multi-class classification setting, we emulate
the effect of an ?open? relation inventory by means
of a tenth class OTHER, and we will release to the
research community a data set with a considerably
1http://www.itl.nist.gov/iad/mig/tests/
ace/
2Although it was not designed for a multi-class set-up, some
subsequent publications tried to use the data sets in that manner.
95
larger number of examples than SemEval-2007 Task
4 or other comparable data sets. The last point is cru-
cial for ensuring the robustness of the performance
estimates for competing systems.
3 Designing an Inventory of Semantic Re-
lations Between Nominals
We begin by considering the first of the problems
listed above: defining of an inventory of semantic
relations. Ideally, it should be exhaustive (should al-
low the description of relations between any pair of
nominals) and mutually exclusive (each pair of nom-
inals in context should map onto only one relation).
The literature, however, suggests no such inventory
that could satisfy all needs. In practice, one always
must decide on a trade-off between these two prop-
erties. For example, the gene-gene relation inven-
tory of Stephens et al (2001), with relations like X
phosphorylates Y, arguably allows no overlaps, but
is too specific for applications to general text.
On the other hand, schemes aimed at exhaus-
tiveness tend to run into overlap issues, due
to such fundamental linguistic phenomena as
metaphor (Lakoff, 1987). For example, in the sen-
tence Dark clouds gather over Nepal., the relation
between dark clouds and Nepal is literally a type of
ENTITY-DESTINATION, but in fact it refers to the
ethnic unrest in Nepal.
We seek a pragmatic compromise between the
two extremes. We have selected nine relations with
sufficiently broad coverage to be of general and
practical interest. We aim at avoiding ?real? overlap
to the extent that this is possible, but we include two
sets of similar relations (ENTITY-ORIGIN/ENTITY-
DESTINATION and CONTENT-CONTAINER/COM-
PONENT-WHOLE/MEMBER-COLLECTION), which
can help assess the models? ability to make such
fine-grained distinctions.3
As in Semeval-2007 Task 4, we give ordered two-
word names to the relations, where each word de-
scribes the role of the corresponding argument. The
full list of our nine relations follows4 (the definitions
we show here are intended to be indicative rather
than complete):
3COMPONENT-WHOLE and MEMBER-COLLECTION are
proper subsets of PART-WHOLE, one of the relations in
SemEval-2007 Task 4.
4We have taken the first five from SemEval-2007 Task 4.
Cause-Effect. An event or object leads to an effect.
Example: Smoking causes cancer.
Instrument-Agency. An agent uses an instrument.
Example: laser printer
Product-Producer. A producer causes a product to
exist. Example: The farmer grows apples.
Content-Container. An object is physically stored
in a delineated area of space, the container. Ex-
ample: Earth is located in the Milky Way.
Entity-Origin. An entity is coming or is derived
from an origin (e.g., position or material). Ex-
ample: letters from foreign countries
Entity-Destination. An entity is moving towards a
destination. Example: The boy went to bed.
Component-Whole. An object is a component of a
larger whole. Example: My apartment has a
large kitchen.
Member-Collection. A member forms a nonfunc-
tional part of a collection. Example: There are
many trees in the forest.
Communication-Topic. An act of communication,
whether written or spoken, is about a topic. Ex-
ample: The lecture was about semantics.
We add a tenth element to this set, the pseudo-
relation OTHER. It stands for any relation which
is not one of the nine explicitly annotated relations.
This is motivated by modelling considerations. Pre-
sumably, the data for OTHER will be very nonho-
mogeneous. By including it, we force any model of
the complete data set to correctly identify the deci-
sion boundaries between the individual relations and
?everything else?. This encourages good generaliza-
tion behaviour to larger, noisier data sets commonly
seen in real-world applications.
3.1 Semantic Relations versus Semantic Roles
There are three main differences between our task
(classification of semantic relations between nomi-
nals) and the related task of automatic labeling of
semantic roles (Gildea and Jurafsky, 2002).
The first difference is to do with the linguistic
phenomena described. Lexical resources for theo-
ries of semantic roles such as FrameNet (Fillmore et
96
al., 2003) and PropBank (Palmer et al, 2005) have
been developed to describe the linguistic realization
patterns of events and states. Thus, they target pri-
marily verbs (or event nominalizations) and their de-
pendents, which are typically nouns. In contrast,
semantic relations may occur between all parts of
speech, although we limit our attention to nominals
in this task. Also, semantic role descriptions typi-
cally relate an event to a set of multiple participants
and props, while semantic relations are in practice
(although not necessarily) binary.
The second major difference is the syntactic con-
text. Theories of semantic roles usually developed
out of syntactic descriptions of verb valencies, and
thus they focus on describing the linking patterns of
verbs and their direct dependents, phenomena like
raising and noninstantiations notwithstanding (Fill-
more, 2002). Semantic relations are not tied to
predicate-argument structures. They can also be es-
tablished within noun phrases, noun compounds, or
sentences more generally (cf. the examples above).
The third difference is that of the level of gen-
eralization. FrameNet currently contains more than
825 different frames (event classes). Since the se-
mantic roles are designed to be interpreted at the
frame level, there is a priori a very large number
of unrelated semantic roles. There is a rudimen-
tary frame hierarchy that defines mappings between
roles of individual frames,5 but it is far from com-
plete. The situation is similar in PropBank. Prop-
Bank does use a small number of semantic roles, but
these are again to be interpreted at the level of in-
dividual predicates, with little cross-predicate gen-
eralization. In contrast, all of the semantic relation
inventories discussed in Section 1 contain fewer than
50 types of semantic relations. More generally, se-
mantic relation inventories attempt to generalize re-
lations across wide groups of verbs (Chklovski and
Pantel, 2004) and include relations that are not verb-
centered (Nastase and Szpakowicz, 2003; Moldovan
et al, 2004). Using the same labels for similar se-
mantic relations facilitates supervised learning. For
example, a model trained with examples of sell re-
lations should be able to transfer what it has learned
to give relations. This has the potential of adding
5For example, it relates the BUYER role of the COM-
MERCE SELL frame (verb sell ) to the RECIPIENT role of the
GIVING frame (verb give).
1. People in Hawaii might be feeling
<e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.
2. My new <e1>apartment</e1> has a
<e2>large kitchen</e2>.
Figure 1: Two example sentences with annotation
crucial robustness and coverage to analysis tools in
NLP applications based on semantic relations.
4 Annotation
The next step in our study will be the actual annota-
tion of relations between nominals. For the purpose
of annotation, we define a nominal as a noun or a
base noun phrase. A base noun phrase is a noun and
its pre-modifiers (e.g., nouns, adjectives, determin-
ers). We do not include complex noun phrases (e.g.,
noun phrases with attached prepositional phrases or
relative clauses). For example, lawn is a noun, lawn
mower is a base noun phrase, and the engine of the
lawn mower is a complex noun phrase.
We focus on heads that are common nouns. This
emphasis distinguishes our task from much work in
IE, which focuses on named entities and on consid-
erably more fine-grained relations than we do. For
example, Patwardhan and Riloff (2007) identify cat-
egories like Terrorist organization as participants in
terror-related semantic relations, which consists pre-
dominantly of named entities. We feel that named
entities are a specific category of nominal expres-
sions best dealt with using techniques which do not
apply to common nouns; for example, they do not
lend themselves well to semantic generalization.
Figure 1 shows two examples of annotated sen-
tences. The XML tags <e1> and <e2> mark the
target nominals. Since all nine proper semantic re-
lations in this task are asymmetric, the ordering of
the two nominals must be taken into account. In
example 1, CAUSE-EFFECT(e1, e2) does not hold,
although CAUSE-EFFECT(e2, e1) would. In exam-
ple 2, COMPONENT-WHOLE(e2, e1) holds.
We are currently developing annotation guide-
lines for each of the relations. They will give a pre-
cise definition for each relation and some prototypi-
cal examples, similarly to SemEval-2007 Task 4.
The annotation will take place in two rounds. In
the first round, we will do a coarse-grained search
97
for positive examples for each relation. We will
collect data from the Web using a semi-automatic,
pattern-based search procedure. In order to ensure
a wide variety of example sentences, we will use
several dozen patterns per relation. We will also
ensure that patterns retrieve both positive and nega-
tive example sentences; the latter will help populate
the OTHER relation with realistic near-miss negative
examples of the other relations. The patterns will
be manually constructed following the approach of
Hearst (1992) and Nakov and Hearst (2008).6
The example collection for each relation R will
be passed to two independent annotators. In order to
maintain exclusivity of relations, only examples that
are negative for all relations but R will be included
as positive and only examples that are negative for
all nine relations will be included as OTHER. Next,
the annotators will compare their decisions and as-
sess inter-annotator agreement. Consensus will be
sought; if the annotators cannot agree on an exam-
ple it will not be included in the data set, but it will
be recorded for future analysis.
Finally, two other task organizers will look for
overlap across all relations. They will discard any
example marked as positive in two or more relations,
as well as examples in OTHER marked as positive in
any of the other classes. The OTHER relation will,
then, consist of examples that are negatives for all
other relations and near-misses for any relation.
Data sets. The annotated data will be divided into
a training set, a development set and a test set. There
will be 1000 annotated examples for each of the
ten relations: 700 for training, 100 for development
and 200 for testing. All data will be released under
the Creative Commons Attribution 3.0 Unported Li-
cense7. The annotation guidelines will be included
in the distribution.
5 The Classification Task
The actual task that we will run at SemEval-2010
will be a multi-way classification task. Not all pairs
of nominals in each sentence will be labeled, so the
gold-standard boundaries of the nominals to be clas-
sified will be provided as part of the test data.
6Note that, unlike in Semeval 2007 Task 4, we will not re-
lease the patterns to the participants.
7http://creativecommons.org/licenses/by/
3.0/
In contrast with Semeval 2007 Task 4, in which
the ordering of the entities was provided with each
example, we aim at a more realistic scenario in
which the ordering of the labels is not given. Par-
ticipants in the task will be asked to discover both
the relation and the order of the arguments. Thus,
the more challenging task is to identify the most
informative ordering and relation between a pair
of nominals. The stipulation ?most informative?
is necessary since with our current set of asym-
metrical relations that includes OTHER, each pair
of nominals that instantiates a relation in one di-
rection (e.g., REL(e1, e2)), instantiates OTHER in
the inverse direction (OTHER (e2, e1)). Thus, the
correct answers for the two examples in Figure 1
are CAUSE-EFFECT (earthquake, aftershocks) and
COMPONENT-WHOLE (large kitchen, apartment).
Note that unlike in SemEval-2007 Task 4, we will
not provide manually annotated WordNet senses,
thus making the task more realistic. WordNet senses
did, however, serve for disambiguation purposes in
SemEval-2007 Task 4. We will therefore have to
assess the effect of this change on inter-annotator
agreement.
6 Evaluation Methodology
The official ranking of the participating systems will
be based on their macro-averaged F-scores for the
nine proper relations. We will also compute and re-
port their accuracy over all ten relations, including
OTHER. We will further analyze the results quan-
titatively and qualitatively to gauge which relations
are most difficult to classify.
Similarly to SemEval-2007 Task 4, in order to
assess the effect of varying quantities of training
data, we will ask the teams to submit several sets of
guesses for the labels for the test data, using varying
fractions of the training data. We may, for example,
request test results when training on the first 50, 100,
200, 400 and all 700 examples from each relation.
We will provide a Perl-based automatic evalua-
tion tool that the participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
7 Conclusion
We have introduced a new task, which will be part of
SemEval-2010: multi-way classification of semantic
98
relations between pairs of common nominals. The
task will compare different approaches to the prob-
lem and provide a standard testbed for future re-
search, which can benefit many NLP applications.
The description we have presented here should
be considered preliminary. We invite the in-
terested reader to visit the official task web-
site http://semeval2.fbk.eu/semeval2.
php?location=tasks\#T11, where up-to-
date information will be published; there is also a
discussion group and a mailing list.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. EMNLP 2004, pages 33?40.
Dmitry Davidov and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals using
pattern clusters. In Proc. ACL-08: HLT, pages 227?
235.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Charles J. Fillmore. 2002. FrameNet and the linking be-
tween semantic and syntactic relations. In Proc. COL-
ING 2002, pages 28?36.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-
tohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. 4th Semantic Eval-
uation Workshop (SemEval-2007).
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2008.
Classification of semantic relations between nominals.
Language Resources and Evaluation. In print.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING
92, pages 539?545.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
types of some generic relation arguments: Detection
and evaluation. In Proc. ACL-08: HLT, Short Papers,
pages 185?188.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCAI, pages 945?956.
George Lakoff. 1987. Women, fire, and dangerous
things. University of Chicago Press, Chicago, IL.
Maria Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28:357?388.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 60?67.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. ACL-08: HLT, pages 452?460.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and WordNet-
based features. In Proc. AAAI, pages 781?787.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. COLING 2008, pages 649?656.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. COLING/ACL, pages
113?120.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proc. EMNLP-CoNLL), pages
717?727.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proc. EMNLP 2001,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in re-
lational semantics. In Proc. ACL-02, pages 247?254.
Matthew Stephens, Mathew Palakal, Snehasis
Mukhopadhyay, Rajeev Raje, and Javed Mostafa.
2001. Detecting gene relations from Medline ab-
stracts. In Pacific Symposium on Biocomputing, pages
483?495.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Peter D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. IJCAI, pages 1136?
1141.
99
Workshop on TextGraphs, at HLT-NAACL 2006, pages 29?32,
New York City, June 2006. c?2006 Association for Computational Linguistics
A Study of Two Graph Algorithms in Topic-driven Summarization
Vivi Nastase1 and Stan Szpakowicz1,2
1 School of Information Technology and Engineering,
University of Ottawa, Ottawa, Canada
2 Institute of Computer Science,
Polish Academy of Sciences, Warsaw, Poland
{vnastase, szpak}@site.uottawa.ca
Abstract
We study how two graph algorithms ap-
ply to topic-driven summarization in the
scope of Document Understanding Con-
ferences. The DUC 2005 and 2006 tasks
were to summarize into 250 words a col-
lection of documents on a topic consist-
ing of a few statements or questions.
Our algorithms select sentences for ex-
traction. We measure their performance
on the DUC 2005 test data, using the Sum-
mary Content Units made available after
the challenge. One algorithm matches a
graph representing the entire topic against
each sentence in the collection. The
other algorithm checks, for pairs of open-
class words in the topic, whether they can
be connected in the syntactic graph of
each sentence. Matching performs bet-
ter than connecting words, but a combi-
nation of both methods works best. They
also both favour longer sentences, which
makes summaries more fluent.
1 Introduction
The DUC 2005 and 2006 summarization challenges
were motivated by the desire to make summariza-
tion relevant to real users. The task was focussed by
specifying an information need as a topic: one or a
few statements or questions (Dang, 2005). Systems
usually employ such data as a source of key words
or phrases which then help rank document sentences
by relevance to the topic.
We explore other information that can be ex-
tracted from a topic description. In particular, we
look at connections between open-class words. A
dependency parser, MiniPar (Lin, 1998), builds a
dependency relation graph for each sentence. We
apply such graphs in two ways. We match a graph
that covers the entire topic description against the
graph for each sentence in the collection. We also
extract all pairs of open-class words from the topic
description, and check whether they are connected
in the sentence graphs. Both methods let us rank
sentences; the top-ranking ones go into a summary
of at most 250 words. We evaluate the summaries
with the summary content units (SCU) data made
available after DUC 2005 (Nenkova and Passon-
neau, 2004; Copeck and Szpakowicz, 2005). The
experiments show that using more information than
just keywords leads to summaries with more SCUs
(total and unique) and higher SCU weight.
We present related work in section 2, and the data
and the representation we work with in section 3.
Section 4 shows the algorithms in more detail. We
describe the experiments and their results in section
5, and draw a few conclusions in section 6.
2 Related work
Erkan and Radev (2004), Mihalcea (2004), Mihal-
cea and Tarau (2004) introduced graph methods
for summarization, word sense disambiguation and
other NLP applications.
The summarization graph-based systems imple-
ment a form of sentence ranking, based on the idea
of prestige or centrality in social networks. In this
case the network consists of sentences, and signifi-
cantly similar sentences are interconnected. Various
measures (such as node degree) help find the most
central sentences, or to score each sentence.
In topic-driven summarization, one or more sen-
tences or questions describe an information need
which the summaries must address. Previous sys-
tems extracted key words or phrases from topics
and used them to focus the summary (Fisher et al,
2005).
Our experiments show that there is more to topics
than key words or phrases. We will experiment with
using grammatical dependency relations for the task
of extractive summarization.
In previous research, graph-matching using gram-
matical relations was used to detect textual entail-
ment (Haghighi et al, 2005).
3 Data
3.1 Topics
We work with a list of topics from the test data in
the DUC 2005 challenge. A topic has an identifier,
category (general/specific), title and a sequence of
statements or questions, for example:
29
d307b
specific
New Hydroelectric Projects
What hydroelectric projects are planned
or in progress and what problems are
associated with them?
We apply MiniPar to the titles and contents
of the topics, and to all documents. The out-
put is post-processed to produce dependency pairs
only for open-class words. The dependency pairs
bypass prepositions and subordinators/coordinators
between clauses, linking the corresponding open-
class words. After post-processing, the topic will
be represented like this:
QUESTION NUMBER: d307b
LIST OF WORDS:
associate, hydroelectric, in, plan,
problem, progress, project, new, them
LIST OF PAIRS:
relation(project, hydroelectric)
relation(project, new)
relation(associate, problem)
relation(plan, project)
relation(in, progress)
relation(associate, them)
The parser does not always produce perfect
parses. In this example it did not associate the phrase
in progress with the noun projects, so we missed the
connection between projects and progress.
In the next step, we expand each open-class word
in the topic with all its WordNet synsets and one-step
hypernyms and hyponyms. We have two variants
of the topic file: with all open-class words from the
topic description Topicsall, and only with nouns andverbs TopicsNV .
3.2 Documents
For each topic, we summarize a collection of up to
50 news items. In our experiments, we build a file
with all documents for a given topic, one sentence
per line, cleaned of XML tags. We process each file
with MiniPar, and post-process the output similarly
to the topics. For documents we keep the list of de-
pendency relations but not a separate list of words.
This processing also gives one file per topic, each
sentence followed by its list of dependency relations.
3.3 Summary Content Units
The DUC 2005 summary evaluation included an
analysis based on Summary Content Units. SCUs
are manually-selected topic-specific summary-
worthy phrases which the summarization systems
are expected to include in their output (Nenkova and
Passonneau, 2004; Copeck and Szpakowicz, 2005).
The SCUs for 20 of the test topics became available
after the challenge. We use the SCU data to mea-
sure the performance of our graph-matching and
path-search algorithms: the total number, weight
and number of unique SCUs per summary, and
the number of negative SCU sentences, explicitly
marked as not relevant to the summary.
4 Algorithms
4.1 Topic?sentence graph matching (GM)
We treat a sentence and a topic as graphs. The nodes
are the open-class words in the sentence or topic (we
also refer to them as keywords), and the edges are
the dependency relations extracted from MiniPar?s
output. In order to maximize the matching score, we
replace a word wS in the sentence with wQ from thequery, if wS appears in the WordNet expansion ofwords in wQ.To score a match between a sentence and a graph,
we compute and then combine two partial scores:
SN (node match score) the node (keyword) overlapbetween the two text units. A keyword count
is equal to the number of dependency pairs it
appears with in the document sentence;
SE (edge match score) the edge (dependency rela-tion) overlap.
The overall score is S = SN +
WeightFactor ? SE , where WeightFactor ?
{0, 1, 2, ..., 15, 20, 50, 100}. Varying the weight
factor allows us to find various combinations
of node and edge score matches which work
best for sentence extraction in summarization.
When WeightFactor = 0, the sentence scores
correspond to keyword counts.
4.2 Path search for topic keyword pairs (PS)
Here too we look at sentences as graphs. We only
take the list of words from the topic representation.
For each pair of those words, we check whether they
both appear in the sentence and are connected in
the sentence graph. We use the list of WordNet-
expanded terms again, to maximize matching. The
final score for the sentence has two components:
the node-match score SN , and SP , the number ofword pairs from the topic description connected by
a path in the sentence graph. The final score is
S = SN +WeightFactor?SP . WeightFactor, inthe same range as previously, is meant to boost the
contribution of the path score towards the final score
of the sentence.
5 Experiments and results
We produce a summary for each topic and each ex-
perimental configuration. We take the most highly
ranked (complete) sentences for which the total
number of words does not exceed the 250-word
limit. Next, we gather SCU data for each sentence in
each summary from the SCU information files. For
a specific experimental configuration ? topic repre-
sentation, graph algorithm ? we produce summaries
for the 20 documents with the weight factor values
0, 1, 2, ..., 15, 20, 50, 100. Each experimental con-
figuration generates 19 sets of average results, one
30
 10
 11
 12
 13
 14
 15
 16
 17
 18
 0  5  10  15  20A
ve
ra
ge
 (p
os
itiv
e) 
SC
U 
we
igh
ts/
su
mm
ary
Pair/connections weight factor
Graph match method
NV
all
 10
 11
 12
 13
 14
 15
 16
 0  5  10  15  20A
ve
ra
ge
 (p
os
itiv
e) 
SC
U 
we
igh
ts/
su
mm
ary
Pair/connections weight factor
Path search method
NV
all
Figure 1: Average SCU weights for graph matching
(GM) and path search (PS) with different topic rep-
resentations
per weight factor. For one weight factor, we gen-
erate summaries for the 20 topics, and then average
their SCU statistics, including SCU weight, number
of unique SCUs and total number of SCUs. In the re-
sults which follow we present average SCU weight
per summary. The number of unique SCUs and
the number of SCUs closely follow the presented
graphs. The overlap of SCUs (number of SCUs
/ number of unique SCUs) reaches a maximum of
1.09. There was no explicit redundancy elimination,
mostly because the SCU overlap was so low.
We compare the performance of the two algo-
rithms, GM and PS, on the two topic representations
? with all open-class words and only with nouns and
verbs. Figure 1 shows the performance of the meth-
ods in terms of average SCU weights per summary
for each weight factor considered 1.
The results allow us to make several observations.
? Keyword-only match performs worse that ei-
ther GM or PS. The points corresponding to
keyword (node) match only are the points for
which the weight factor is 0. In this case the
dependency pairs match and paths found in the
graph do not contribute to the overall score.
? Both graph algorithms achieve better perfor-
mance for only the nouns and verbs from the
1The summary statistics level off above a certain weight fac-tor, so we include only the non-flat part of the graph.
topic than for all open-class words. If, how-
ever, the topic requests entities or events with
specific properties, described by adjectives or
adverbs, using only nouns and verbs may pro-
duce worse results.
? GM performs better than PS for both types of
topic descriptions. In other words, looking at
the same words that appear in the topic, con-
nected in the same way, leads to better results
than finding pairs of words that are ?somehow?
connected.
? Higher performance for higher weight factors
further supports the point that looking for word
connections, instead of isolated words, helps
find sentences with information content more
related to the topic.
For the following set of experiments, we use the
topics with the word list containing only nouns and
verbs. We want to compare graph matching and
path search further. One issue that comes to mind is
whether a combination of the two methods will per-
form better than each of them individually. Figure 2
plots the average of SCU weights per summary.
 11
 12
 13
 14
 15
 16
 17
 18
 19
 0  5  10  15  20
Av
er
ag
e 
(po
sit
ive
) S
CU
 w
eig
hts
/su
mm
ary
Pair/connections weight factor
GM & PS
GM
PS
Figure 2: Graph matching, path search and their
combination
We observe that the combination of graph match-
ing and path search gives better results than ei-
ther method alone. The sentence score com-
bines the number of edges matched and the num-
ber of connections found with equal weight fac-
tors for the edge match and path score. This
raises the question whether different weights for
the edge match and path would lead to better
scores. Figure 3 plots the results produced us-
ing the score computation formula S = SN +
WeightFactorE ? SE + WeightFactorP ? SP ,where both WeightFactorE and WeightFactorPare integers from 0 to 30.
The lowest scores are for the weight factors 0,
when sentence score depends only on the keyword
score. There is an increase in average SCU weights
31
 0
 5
 10
 15
Pair weight factor  5
 10
 15
 20
 25
 30
Path weight factor
 15.5
 16
 16.5
 17
 17.5
 18
 18.5
 19
 19.5
Avg weight of SCUs
Figure 3: Graph match and path search combined
with different weight factors
towards higher values of weight factors. A transpar-
ent view of the 3D graph shows that graph match has
higher peaks toward higher weight factors than path
search, and higher also than the situation when path
search and graph match have equal weights.
The only sentences in the given documents tagged
with SCU information are those which appeared in
the summaries generated by the competing teams
in 2005. Our results are therefore actually a lower
bound ? more of the sentences selected may include
relevant information. A manual analysis of the sum-
maries generated using only keyword counts showed
that, for these summaries, the sentences not contain-
ing SCUs were not informative. We cannot check
this for all the summaries generated in these ex-
periments, because the number is very large, above
1000. An average summary had 8.24 sentences, with
3.19 sentences containing SCUs. We cannot say
much about the sentences that do not contain SCUs.
This may raise doubts about our results. Support
for the fact that the results reflect a real increase in
performance comes from the weights of the SCUs
added: the average SCU weight increases from 2.5
when keywords are used to 2.75 for path search al-
gorithm, and 2.91 for graph match and the combina-
tion of path search and graph match. This shows that
by increasing the weight of graph edges and paths
in the scoring of a sentence, the algorithm can pick
more and better SCUs, SCUs which more people see
as relevant to the topic. It would be certainly in-
teresting to have a way of assessing the ?SCU-less?
sentences in the summary. We leave that for future
work, and possibly future developments in SCU an-
notation.
6 Conclusions
We have studied how two algorithms influence sum-
marization by sentence extraction. They match the
topic description and sentences in a document. The
results show that using connections between the
words in the topic description improves the accu-
racy of sentence scoring compared to simple key-
word match. Finding connections between query
words in a sentence depends on finding the corre-
sponding words in the sentence. In our experiments,
we have used one-step extension in WordNet (along
IS-A links) to find such correspondences. It is, how-
ever, a limited solution, and better word matches
should be attempted, such as for example word sim-
ilarity scores in WordNet.
In summarization by sentence extraction, other
scores affect sentence ranking, for example position
in the document and paragraph or proximity to other
high-ranked sentences. We have analyzed the effect
of connections in isolation, to reduce the influence of
other factors. A summarization system would com-
bine all these scores, and possibly produce better re-
sults. Word connections or pairs could also be used
just as keywords were, as part of a feature descrip-
tion of documents, to be automatically ranked using
machine learning.
References
Terry Copeck and Stan Szpakow-
icz. 2005. Leveraging pyramids.
http://duc.nist.gov/pubs/2005papers/uottawa.copeck2.pdf.
Hoa Trang Dang. 2005. Overview of DUC 2005.
http://duc.nist.gov/pubs/2005papers/OVERVIEW05.pdf.
Gu?nes? Erkan and Dragomir Radev. 2004. LexRank: Graph-
based centrality as salience in text summarization. Journal
of Artificial Intelligence Research, (22).
Seeger Fisher, Brian Roark, Jianji Yang, and Bill
Hersh. 2005. Ogi/ohsu baseline query-directed
multi-document summarization system for duc-2005.
http://duc.nist.gov/pubs/2005papers/ohsu.seeger.pdf.
Aria Haghighi, Andrew Ng, and Christopher Manning. 2005.
Robust textual inference via graph matching. In Proc. of
HLT-EMNLP 2005, Vancouver, BC, Canada.
Dekang Lin. 1998. Dependency-based evaluation of MiniPar.
In Workshop on the Evaluation of Parsing Systems, Granada,
Spain.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order
into texts. In Proc. of EMNLP 2004, Barcelona, Spain.
Rada Mihalcea. 2004. Graph-based ranking algorithms for sen-
tence extraction, applied to text summarization. In Proc. of
ACL 2004, Barcelona, Spain.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. In
Proc. of NAACL-HLT 2004.
32
The Telling Tail:
Signals of Success in Electronic Negotiation Texts
Marina Sokolova
IRO, Universite? de Montre?al
Montre?al, Que?bec, Canada
sokolovm@iro.umontreal.ca
Vivi Nastase
EML Research, gGmbH
Heidelberg,Germany
nastase@eml-research.de
Stan Szpakowicz
SITE, University of Ottawa
Ottawa, Ontario, Canada
ICS, Polish Academy of Sciences
Warsaw, Poland
szpak@site.uottawa.ca
Abstract
We analyze the linguistic behaviour of par-
ticipants in bilateral electronic negotiations,
and discover that particular language char-
acteristics are in contrast with face-to-face
negotiations. Language patterns in the later
part of electronic negotiation are highly in-
dicative of the successful or unsuccessful
outcome of the process, whereas in face-to-
face negotiations, the first part of the nego-
tiation is more useful for predicting the out-
come. We formulate our problem in terms of
text classification on negotiation segments
of different sizes. The data are represented
by a variety of linguistic features that cap-
ture the gist of the discussion: negotiation-
or strategy-related words. We show that,
as we consider ever smaller final segments
of a negotiation transcript, the negotiation-
related words become more indicative of the
negotiation outcome, and give predictions
with higher Accuracy than larger segments
from the beginning of the process.
1 Introduction
We use language every day to convince, explain, ma-
nipulate and thus reach our goals. This aspect of
language use is even more obvious in the context
of negotiations. The parties must reach an agree-
ment on the partitioning or sharing of a resource,
while each party usually wants to leave the negotia-
tion table with the larger piece of the pie. These ten-
dencies become stronger when negotiators use only
electronic means to communicate, that is to say, par-
ticipate in electronic negotiations. In face-to-face
contact, prosody and body language often have a
crucial role in conveying attitudes and feelings. E-
negotiators, on the other hand, must rely only on
texts. We perform automatic analysis of the textual
data in e-negotiations. We identify linguistic expres-
sions of such negotiation-specific behaviour that are
indicative of the final outcome of the process ? suc-
cess or failure ? and observe how powerful a tool
language is in helping people get what they want.
In this paper we focus on the negotiation as an on-
going process. We analyze the linguistic features of
messages exchanged at various points in the course
of the negotiation, to determine the time frame in
which the outcome becomes decided. From our ex-
perimental point of view, we determine the segment
of the negotiation which is most predictive of the
outcome. There is an imposed three-week deadline
in the electronic negotiations that we analyze. We
hypothesize that the pressure of the deadline is re-
flected in the messages exchanged. The messages
written later in the process are more indicative of
the outcome of the process. Our empirical results
support this hypothesis; an analysis of the linguis-
tic features that make this prediction possible shows
what the negotiators? main concerns are as the dead-
line draws near.
Here is what our results contribute to the field
of text analysis. Research on text records of face-
to-face negotiations suggests that the language pat-
terns used in the first half of a negotiation predict
the negotiation outcome better than those in the sec-
ond half (Simons, 1993). The explanation was that
257
in the first phase people establish contact, exchange
personal information and engage in general polite
conversation, creating a foundation of trust between
partners. No numerical data, however, supported
this diagnosis, and there was no distinction between
the prediction of successful and unsuccessful out-
comes. When it comes to text classification, our
hypothesis says that the classification of the second
parts of e-negotiation texts is more accurate with re-
spect to the outcome than the classification of the
first parts. This makes e-negotiation texts different
from newsgroup messages, newspaper articles and
other documents classified by Blatak et al (2004),
where texts showe d better classification Accuracy
on their initial parts. We report the results of several
sets of Machine Learning (ML) experiments. Per-
formed on varying-size text data segments, they sup-
port our hypothesis.
We worked with a collection of transcripts of
negotiations conducted over the Internet using the
Web-based negotiation support system Inspire (Ker-
sten and Zhang, 2003). Kersten and Zhang (2003)
and Nastase (2006) classified e-negotiation out-
comes using non-textual data. Classification based
on texts is discussed in (Sokolova et al 2005;
Sokolova and Szpakowicz, 2006). None of those ex-
periments considered segmenting the data, although
Sokolova and Szpakowicz (2006) analyzed the im-
portance of the first part of e-negotiations. The work
we present here is the first attempt to investigate the
effect of parts of e-negotiation textual data on classi-
fication quality. In this study we do not report types
of expressions that are relevant to success and failure
of negotiations. These expressions have been pre-
sented and analyzed in (Sokolova and Szpakowicz,
2005).
In section 2 we take a brief look at other work on
the connection between behaviour and language. In
section 3 we present our data and their representa-
tion for ML experiments, and we further motivate
our work. Section 4 describes the experiments. We
discuss the results in Section 5. Section 6 draws con-
clusions and discusses a few ideas for future work.
2 Background Review
Young (1991) discusses the theory that the situation
in which language is used affects the way in which it
is used. This theory was illustrated with a particular
example of academic speech.
The field of neuro-linguistic programming in-
vestigates how to program our language (among
other things) to achieve a goal. In the 1980s,
Rodger Bailey developed the Language and Be-
haviour Profile based on 60 meta-programs. Charvet
(1997) presents a simplified approach with 14 meta-
programs. This profile proposes that people?s lan-
guage patterns are indicators of behavioural pref-
erences. In the study of planning dialogues (Chu-
Carroll and Carberry, 2000), Searle?s theory of
speech acts used through the discourse analysis also
supports the fact that language carries much of peo-
ple?s behaviour and emotions. Reitter and Moore
(2007) studied repetitions in task-oriented conver-
sations. They demonstrated that a speaker?s short-
term ability to copy the interlocutor?s syntax is au-
tonomous from the success of the task, whereas
long-term adaptation varies with such success.
We consider a negotiation to be a communication
in which the participants want to reach an agreement
relative to the splitting/sharing of resources. Lan-
guage is one of the tools used to reach the goal. We
propose that not all messages exchanged throughout
a negotiation have the same effect on the negotiation
outcome. To test this hypothesis, we take an ever
smaller segment of the negotiation, and see howwell
we can predict the outcome of the process, based
only on the messages in this fragment.
We encountered several challenges in predict-
ing e-negotiation outcomes using the messages ex-
changed. First, electronic negotiations usually do
not have a sequential-stage model of behaviour
(Koeszegi et al 2007), which is common in face-
to-face negotiations (Adair and Brett, 2005). Here is
an example of behavioural phases in face-to-face ne-
gotiations: Perform Relational Positioning ? Iden-
tify the Problem ? Generate Solutions ? Reach
Agreement. Unexpected turns and moves ? typical
of human behaviour ? make prediction of the ne-
gotiation outcome difficult. In case of electronic
negotiation, the absence of the usual negotiation
structure further complicates the outcome predic-
tion. This distinguishes e-negotiations from agent-
customer phone conversations studied in (Takeuchi
et al 2007), where an agent follows the call flow
pre-defined by his company?s policy.
258
The longer an e-negotiation takes, the more elab-
orate the structure of the e-negotiation process be-
comes. Simpler e-negotiation may involve an ex-
change of well-structured business documents such
as pre-defined contract or retail transactions. A
more complex process comprises numerous offers
and counter-offers and has a high degree of uncer-
tainty because of the possible unpredictability of ne-
gotiation moves.
The next challenge stems from the limitations im-
posed by the use of electronic means. This overloads
text messages with various tasks: negotiation issues
themselves, introductions and closures traditional in
negotiations, and even socializing. On the other
hand, electronic means make the contacts less for-
mal, allowing people to communicate more freely.
As a result, the data have a high volume of informal-
ity such as abbreviations or slang.
The last challenge is specific to text analysis. E-
negotiations usually involve a multi-cultural audi-
ence of varied background, many of whom are not
native English speakers. While communicating in
English, they introduce a fair amount of spelling and
grammatical mistakes.
3 Textual Data in Electronic Negotiations
Participants in a negotiation assume well-defined
roles (such as buyers/sellers in some business nego-
tiations, or facilitators in legal disputes), have goals,
and adopt specific behaviour to achieve those goals
(Koeszegi et al 2007). These circumstances are re-
flected in the language of texts exchanged in negoti-
ations, and distinguish this type of texts from casual
e-mail exchange and postings on discussion groups
and chat boards. We claim that the language cap-
tured in e-negotiation textual data changes as a nego-
tiation progresses, and that this is clearly detectable,
even though it does not follow a sequential-stage
model common in face-to-face-negotiations (Adair
and Brett, 2005) or an agent-customer interaction
call flow recommended by a company (Takeuchi et
al, 2007). To support the language change hypothe-
sis, we have conducted a series of ML experiments
on negotiation segments of varying size and posi-
tion, using the largest available data of electronic ne-
gotiations.
Our data come from the Web-based negoti-
ation support system Inspire. Inspire has been
used in business courses to teach students about
e-negotiations and give them a chance to practice
bilateral business negotiations conducted in a lightly
controlled environment. For many users, conducting
negotiations has been a business/ course assignment.
Other users wanted to develop their English skills
by participating in an Inspire-enabled negotiation.
A negotiation would last up to three weeks, after
which, if an agreement has not been reached, the
systems would terminate the negotiation and record
it as unsuccessful. The following is an example of a
negotiation message (with the original spelling):
Dear Georg, I hope you are doig well. I send you this message
to ask you what happened to our offer. Just be aware that
we will not be indifinitely waiting on your response. As I
told you during our last meeting, Itex Manufacturing needs
a partnership. So it is important to me to know if you are
ready to negotiate with us. We can not afford losing so much
precious time. We give you now five more days to answer
our offer (1st of december 1997, 2400 Swiss time). After this
dead line, will propose our services to your concurrence. I
still believe in a good partnership and relationship between
our two societies. Let me know if you think so. For Itex
Manufacturing. Rookie.
Among the wealth of data gathered by Inspire, we
have focussed on the accompanying text messages,
extracted from the transcripts of 2557 negotiations.
Each negotiation had two different participants, and
one person participated in only one negotiation. The
total number of contributors was over 5000; most
of them were not native English speakers. The data
contain 1, 514, 623 word tokens and 27, 055 types.
Compared with benchmark corpora, for example the
Brown or the Wall Street Journal corpus (Francis
and Kucera, 1997; Paul and Baker, 1992), this col-
lection has a lower type-token ratio and a higher
presence of content words among the most frequent
words (this is typical of texts on a specific topic), and
a high frequency of singular first- and second-person
pronouns (this is typical of dialogues).
We considered all messages from one negotiation
to be a single negotiation text. We concatenated the
messages in chronological order, keeping the punc-
tuation and spelling unedited. Each negotiation had
a unique label, either positive or negative, and was
a training example in one of two classes ? success-
259
Features Split NB SVM DT
Acc F P R Acc F P R Acc F P R
negotiation-related 1/2 and 1/2 68.1 70.4 73.0 68.0 73.6 76.8 75.4 78.2 73.9 78.8 72.1 86.8
negotiation-related 3/4 and 1/4 69.1 71.3 74.1 68.7 73.7 77.0 75.5 78.5 75.4 79.4 73.8 86.0
Table 1: Accuracy and corresponding F ? score , Precision and Recall . Classifying all negotiations as successful gives a
baseline Accuracy of 55%.
ful or unsuccessful. Inspire assigned a negotiation
to the right class automatically. 55% of negotiations
in our data set were successful, i.e. ended up with
agreement.
We represented a complete negotiation, or text as
we consider it, as a combined bag of words. We
matched the tokens in the messages with an inven-
tory of domains from Longman Dictionary of Con-
temporary English (Procter, 1978). This allowed us
to select those terms that refer to negotiation specific
issues ? we call them negotiation-related words. We
select strategic words based on words and patterns
that literature shows to express the intentions, influ-
ence, self-obligations and motivations of the negoti-
ation participants. In classifying successful and un-
successful negotiations, subsets of these two types
of features provided better Accuracy than statisti-
cally selected features, e.g. most frequent unigrams
and unigrams with a higher log-likelihood values
calculated between positive and negative classes
(Sokolova et al 2005).
We halved each text, that is to say, the complete
record of a negotiation. For each half we built a
bag of 123 negotiation-related words ? more on this
in section 4. The binary attributes represented the
presence or absence of the word in its half of the
text. We concatenated the two bags, and labelled
the resulting bag by the outcome of the whole ne-
gotiation: positive if the negotiation was successful,
negative otherwise. We repeated this procedure for
the split of the negotiation text into 34 and
1
4 . Our
ML tools were Weka?s (Witten and Frank, 2005)
NAIVE BAYES (NB), the sequential minimal optimiza-
tion (SVM) version of SUPPORT VECTOR MACHINE, and
DECISION TREE (DT). In Table 1 we report Accuracy
and Precision (P ), Recall (R) and F ? score (F ).
P , R, F are calculated on the positive class. For
every classifier, the best Accuracy and correspond-
ing P , R, F are reporte d; we performed an exhaus-
tive search on adjustable parameters; the evaluation
method was tenfold cross-validation. Our Accuracy
results are comparable with those reported in pre-
vious studies (Kersten and Zhang, 2003; Nastase,
2006; Sokolova and Szpakowicz, 2006).
We used the paired t-test to generalize the results
on both splits.1 The two-tailed P value was 0.0102.
By conventional criteria, this difference is consid-
ered to be statistically significant.
Accuracy and, especially, Precision results show
that DECISION TREE is sensitive to the positions of
words in different parts of the negotiations. SUPPORT
VECTOR MACHINE and NAIVE BAYES change Accuracy
only slightly. The Precision and Recall results
give a better picture of the performance. The
presence/absence of words recorded for different
splits of negotiations influences the identification of
true positive examples (successful negotiations) and
true negative examples (unsuccessful negotiations).
Recall displays that DT classifies successful negoti-
ations better when the negotiations are split 12 and
1
2 .
Precision andRecall together imply that unsuccess-
ful negotiations have a higher rate of true classifica-
tion achieved by NB, when the split is 34 and
1
4 . This
split lets us improve the worst rates of true classifi-
cations ? unsuccessful negotiations for DT and suc-
cessful negotiations for NB. Generally, the unequal
split al lows us to reduce the difference between true
positive and true negative classification results, and
thus makes the classification of negotiations more
balanced than the equal split. For all the three clas-
sifiers, Accuracy and F ? score are better on the 34
and 14 split.
4 The Empirical Set-up
We wanted to determine the placement of the seg-
ment of a negotiation most important in deciding
whether the outcome is positive: at the beginning
or at the end of the process. To do that, we split each
negotiation in half, and built two parallel data sets,
corresponding to the two halves. We classified each
1Results on the same data require the paired version of t-test.
260
part using various ML tools. Next, we repeated the
same classification tasks using smaller and smaller
final segments, in order to monitor the variation in
performance. Thus each negotiation text N con-
sisted of the head segment (h) and the tail segment
(t): N = h
?
t, h
?
t = ?, where |t| = |N |i and t was
the segment at the end of N , and |h| = (i?1)|N |i cov-
ering the beginning of the negotiation. We stopped
when for two consecutive splits two classifiers had
better Accuracy on the head than on the tail. Each
segment got the same class label as the whole nego-
tiation.
For these experiments, as briefly explained in sec-
tion 3, we took the textual negotiation data repre-
sented as bags of words. Because of the large num-
ber of word features (27, 055 tokens), we performed
lexical feature selection.
Statistical analysis of the corpus built from the
Inspire negotiation messages has revealed that the
issues discussed in these messages can be grouped
into a small set of topics. The particular topic
or domain to which a word belongs derives from
the most frequent bigram and trigram meanings;
for instance, the second most frequent trigram
with the word delivery is payment upon delivery, so
we assign delivery to the domain negotiation process.
The data come from negotiations on a specific
topic (sale/purchase of bicycle parts), so a likely
candidate subset would be words related to it. We
select such negotiation-related words as the first
set of features. We show a text sample with the
negotiation-related words in bold:
Dear Georg, I hope you are doig well. I send you this message
to ask you what happened to our offer. Just be aware that
we will not be indifinitely waiting on your response. As I
told you during our last meeting, Itex Manufacturing needs
a partnership. So it is important to me to know if you are
ready to negotiatewith us. We can not afford losing so much
precious time. We give you now five more days to answer our
offer (1st of december 1997, 2400 Swiss time). After this
dead line, we will propose our services to your concurrence.
I still believe in a good partnership and relationship between
our two societies. Let me know if you think so. For Itex
Manufacturing. Rookie.
Strategies which the negotiators adopt (promises,
threats, exchange of information, argumentation,
and so on) affect the outcome (Sokolova and
Szpakowicz, 2006). Since the messages are dense,
short and grammatically simple, the expression of
strategies through language is straightforward and
concentrates on communicating the main goal. The
word categories that convey negotiators? strategies
are modals, personal pronouns, volition verbs,
mental verbs; we refer to them as strategic words.
Strategic words constitute the second set of features.
Our text sample with strategic words in bold looks
as follows:
Dear Georg, I hope you are doig well. I send you this mes-
sage to ask you what happened to our offer. Just be aware
that we will not be indifinitely waiting on your response. As
I told you during our last meeting, Itex Manufacturing needs
a partnership. So it is important to me to know if you are
ready to negotiate with us. We can not afford losing so much
precious time. We give you now five more days to answer
our offer (1st of december 1997, 2400 Swiss time). After
this dead line, we will propose our services to your concur-
rence. I still believe in a good partnership and relationship
between our two societies. Let me know if you think so.
For Itex Manufacturing. Rookie.
We work with kernel (SVM), decision-based (DT)
and probabilistic (NB) classifiers. Applying classi-
fiers with different working paradigms allow us to
capture and understand different aspects of the data,
as the results and our discussion in section 5 will
show. For each classifier, we used tenfold cross-
validation and exhaustive search on adjustable pa-
rameters in model selection. The best results, in par-
ticular with high overall Accuracy , appear in Fig-
ure 1.
When the data are represented using negotiation-
related words, the tail segments give more accurate
outcome classification than the head segments. This
holds for all splits and all classifiers; see Figure 1.
The increase in Accuracy when the head segments
grow was to be expected, although it does not hap-
pen with DT and SVM ? only with NB. At the same
time, there is no monotonic decline in Accuracy
when the length of the tail segments decreases. On
the contrary, NB constantly improves the Accuracy
of the classification. We note the fact that NB in-
creases theAccuracy on both head and tail segments
and makes the basic assumption of the conditional
independence of features. We explain the NB re-
sults by the decreased dependence between the pres-
261
1. DT
 71
 71.5
 72
 72.5
 73
 73.5
 74
 74.5
 75
 75.5
 1  2  3  4  5  6
Ac
cur
acy
Inverse size of segment
head part, negotiation-related wordstail part, negotiation-related wordshead part, strategic wordstail part, strategic words
2. SVM
 71
 71.5
 72
 72.5
 73
 73.5
 74
 74.5
 75
 75.5
 1  2  3  4  5  6
Ac
cur
acy
Inverse size of segment
head part, negotiation-related wordstail part, negotiation-related wordshead part, strategic wordstail part, strategic words
3. NB
 67.5
 68
 68.5
 69
 69.5
 70
 70.5
 71
 71.5
 1  2  3  4  5  6
Ac
cur
acy
Inverse size of segment
head part, negotiation-related wordstail part, negotiation-related wordshead part, strategic wordstail part, strategic words
Figure 1: The classification Accuracy with DT, SVM and
NB, for negotiation-related and strategic words.
ence/absence of negotiation-related words when the
negotiations move to the second part of the process.
The results on the strategic-word representation
are slightly different for the three classifiers; see
Classifier tail s1 s2 s1 s2 s3
DT 74.4 71.9 74.9 72.5 71.9 73.9
SVM 75.3 70.5 73.5 70.8 69.9 74.6
NB 68.8 68.5 70.1 68.7 68.9 70.9
Negotiation-related words
Classifier tail s1 s2 s1 s2 s3
DT 73.8 73.8 73.4 71.7 71.4 72.9
SVM 73.8 70.9 72.8 72.0 71.3 73.4
NB 60.8 70.6 69.5 69.2 69.3 68.7
Strategic words
Table 2: The Accuracy of the negotiation outcome classifica-
tion on 2 and 3 splits of the second half of the negotiation ? the
tail segment. Classifying all negotiations as successful gives a
baseline Accuracy of 55%.
Figure 1. SVM classifies all tail segments better
than head segments, DT classifies tail segments bet-
ter than head segments up to the 45/
1
5 split, and NB
classifies the tail segment better than the head seg-
ment only for the half-and-half split. The Accuracy
results are unstable for all three classifiers, with the
Accuracy on the head segments decreasing when
the segments grow and the Accuracy on the tail
segments increasing when the tail segments shrink.
The performance of the classifiers indicate that, as
the deadline approaches, negotiation-related words
reflect the negotiation process better than strategic
words.
To investigate which part of the tail segments is
more important for classifying the outcomes, we in-
troduced additional splits in the tail segments. We
divided the second half of each text into 2 and 3
parts and repeated the classification procedures for
every new split. The results appear at the top of
Table 2, where tail shows the classification results
when the second half of the text was classified, and
the other columns report the results on the tail splits;
both splits satisfy the conditions tail =
?
i si, where
si
?
sj = ? for every i 6= j.
The results show that adding splits in the tail seg-
ments emphasizes the importance of the last part of
a negotiation. For negotiation-related word repre-
sentation, the classification of the outcome on the
last part of the tail is more accurate than on its other
parts. This holds for all three classifiers. For the
strategic-word representation the same is true for
SVM and partially for DT, but not for NB; see the
bottom of Table 2. NB classifies the negotiation out-
comes more accurately on s1 than on s2 and on s2
rather than s3.
262
Classifier 1/3 1/4 1/5 1/6 1/7 1/8 1/9
P R P R P R P R P R P R P R
DT 74.2 85.3 74.2 84.3 75.2 82.3 73.61 83.0 74.5 82.4 72.1 81.6 74.0 81.3
SVM 76.1 78.1 76.3 76.3 77.0 75.3 78.3 75.3 77.2 73.4 76.9 72.3 77.6 71.6
NB 73.8 71.8 71.8 73.9 74.8 71.9 74.9 72.0 71.3 72.2 70.8 72.5 70.5 74.3
Table 3: Precision and Recall on the tail segments; negotiation-related words. Precision and Recall are calculated on the
positive class.
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 1  2  3  4  5  6  7  8  9
Ac
cur
acy
Inverse size of segment
C5.0SMONaive Bayes
Figure 2: The evolution of the success and failure classifica-
tion Accuracy with decreasing segment sizes.
5 Segmentation Results
Taking into account the results reported in section
4, we chose negotiation-related words as the feature
set. We selected for further analysis the half that per-
formed better for a majority of the tools used. We
focussed on the last part of the negotiation, and we
extracted a gradually smaller fragment (12 ?
1
9 ; 9 is
the average number of text messages in one negotia-
tion). Figure 2 plots the results of the experiments
performed with decreasing segment sizes. As we
see, the tail segment of the length 17 gave a decline
of the Accuracy for SVM and NB, with a slight im-
provement on smaller tail segments.
A more detailed analysis comes from consider-
ing the Precision and Recall results on the seg-
ments; see Table 3. On 17 and
1
9 tail segments a
higher Precision indicates that all classifiers have
improved the identification of true negatives (unsuc-
cessful negotiations). This means that the trends in
the class of unsuccessful negotiations become more
noticeable for the classifiers when the deadline ap-
proaches. The 18 split is an exception, with the
abrupt drop of true negative classification by DE-
CISION TREE. The correct classification of positive
examples (successful negotiations), however, dimin-
ishes when splits become smaller; this applies to the
performance of all three classifiers. This means that
at the end of the negotiations the class of success-
ful negotiations becomes more diverse and, subse-
quently, multi-modal, and the trends are more diffi-
cult to capture by the classifiers.
As in the previous experiments, NB?s Accuracy
on the tail segments is higher than on the complete
data. The opposite is true for SVM and DT: their
Accuracy on the tail segments is lower than on the
complete data. We explain this by the fact that the
sizes of tail segments in the last splits do not give
these two classifiers sufficient information.
6 Discussion and Future Work
We have analyzed textual messages exchanged in the
course of electronic negotiations. The results sup-
port our hypothesis that texts of electronic negoti-
ation have different characteristics than records of
face-to-face negotiation. In particular, messages ex-
changed later in the process are more informative
with regard to the negotiation outcome than mes-
sages exchanged at the beginning.
We represented textual records of negotiations by
two types of word features. These features cap-
ture the important aspects of the negotiation process
? negotiation-related concepts and indicators of the
strategies employed. We performed extensive exper-
iments with different types of ML algorithms and
segments of varying sizes from the beginning and
the end of the negotiation, on a collection of over
2500 electronic negotiations. Our study shows that
words expressing negotiation-related concepts are
more useful for distinguishing successful and failed
negotiations, especially towards the end of negotia-
tions. We also have shown that there is no linear de-
pendency between the segment sizes and Accuracy
of classification of the negotiation success and fail-
ure.
263
Our research plans include a continuation of the
investigation of the negotiators? behaviour in elec-
tronic negotiations and its reflection in language. To
see whether dialogue analysis improves prediction
of the negotiation outcomes, we will look at negotia-
tions as dialogues between participants and take into
account their roles, e.g. buyer and seller. We will
split a negotiation at message boundaries to avoid
arbitrary splits of the negotiation process.
Acknowledgments
Partial support for this work came from the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
W. Adair, J. M. Brett. 2005. The negotiation dance: time,
culture, and behavioral sequences in negotiation. Or-
ganization Science. 16(1): 33-51.
J. Blata?k, E. Mra?kova?, L. Popel??nsky. 2004. Fragments
and Text Categorization. Proceedings of the 42th An-
nual Meeting of the Association of Computational Lin-
guistics (ACL 2004). Companion Volume: 226-229,
Association for Computational Linguistics.
J. M. Brett. 2001. Negotiating Globally. Jossey-Bass.
S. R. Charvet. 1997. Words that Change Minds: Master-
ing the Language of Influence. Kendall/Hunt.
J. Chu-Carroll, S. Carberry. 2000. Conflict Resolution in
Collaborative Planning Dialogues. International Jour-
nal of HumanComputer Studies. 53(6): 969-1015.
W. N. Francis, H. Kuc?era. 1967. Computational Analysis
of Present-Day American English, Brown University
Press.
G. E. Kersten, G. Zhang. 2003. Mining Inspire Data
for the Determinants of Successful Internet Negotia-
tions. Central European Journal of Operational Re-
search. 11(3): 297-316.
S. Koeszegi, E.-M. Pesendorfer, R. Vetschera. 2007.
Data-driven Episodic Phase Analysis of E-negotiation.
Group Decision and Negotiation 2007. 2: 11?130.
V. Nastase. 2006. Concession curve analysis for Inspire
negotiations. Group Decision and Negotiation. 15:
18?193.
D. B. Paul and J. M. Baker 1992 The Design for the
Wall Street Journal-based CSR Corpus. Proceedings
of the 2nd International Conference on Spoken Lan-
guage Processing (ICSLP?92), 357-361.
P. Procter. 1978. Longman Dictionary of Contemporary
English. Longman Group Ltd. Essex, UK.
D. Reitter, J. Moore. 2007. Predicting Success in Dia-
logue. Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
808-815, Association for Computational Linguistics.
T. Simons. 1993. Speech Patterns and the Concept of
Utility in Cognitive Maps: the Case of Integrative Bar-
gaining. Academy of Management Journal. 38(1):
139-156.
M. Sokolova, V. Nastase, M. Shah, S. Szpakowicz.
2005. Feature Selection in Electronic Negotiation
Texts. Proceedings of Recent Advances in Natural
Language Processing (RANLP?2005), 518-524, In-
coma Ltd, Bulgaria.
M. Sokolova, S. Szpakowicz. 2006. Language Patterns
in the Learning of Strategies from Negotiation Texts.
Proceedings of the 19th Canadian Conference on Ar-
tificial Intelligence (AI?2006), 288-299, Springer.
M. Sokolova and S. Szpakowicz. 2005 Analysis and
Classification of Strategies in Electronic Negotiations.
Proceedings of the 18th Canadian Conference on Ar-
tificial Intelligence (AI?2005), 145-157, Springer.
, H. Takeuchi, L. Subramaniam, T. Nasukawa, S. Roy.
2007 Automatic Identification of Important Segments
and Expressions for Mining of Business-Oriented
Conversations at Contact Centers. Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), 458?467, As-
sociation for Computational Linguistics.
I. Witten, E. Frank. 2005. Data Mining, 2nd ed.. Morgan
Kaufmann. www.cs.waikato.ac.nz/ml/weka/
L. Young. 1991. Language as Behaviour, Language as
Code: A Study of Academic English. John Benjamins.
264
Proceedings of ACL-08: HLT, pages 416?424,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating Roget?s Thesauri
Alistair Kennedy
School of Information Technology
and Engineering
University of Ottawa
Ottawa, Ontario, Canada
akennedy@site.uottawa.ca
Stan Szpakowicz
School of Information Technology
and Engineering
University of Ottawa
Ottawa, Ontario, Canada
and
Institute of Computer Science
Polish Academy of Sciences
Warsaw, Poland
szpak@site.uottawa.ca
Abstract
Roget?s Thesaurus has gone through many re-
visions since it was first published 150 years
ago. But how do these revisions affect Ro-
get?s usefulness for NLP? We examine the
differences in content between the 1911 and
1987 versions of Roget?s, and we test both ver-
sions with each other and WordNet on prob-
lems such as synonym identification and word
relatedness. We also present a novel method
for measuring sentence relatedness that can be
implemented in either version of Roget?s or in
WordNet. Although the 1987 version of the
Thesaurus is better, we show that the 1911 ver-
sion performs surprisingly well and that often
the differences between the versions of Ro-
get?s and WordNet are not statistically signif-
icant. We hope that this work will encourage
others to use the 1911 Roget?s Thesaurus in
NLP tasks.
1 Introduction
Roget?s Thesaurus, first introduced over 150 years
ago, has gone through many revisions to reach its
current state. We compare two versions, the 1987
and 1911 editions of the Thesaurus with each other
and with WordNet 3.0. Roget?s Thesaurus has a
unique structure, quite different from WordNet, of
which the NLP community has yet to take full ad-
vantage. In this paper we demonstrate that although
the 1911 version of the Thesaurus is very old, it can
give results comparable to systems that use WordNet
or newer versions of Roget?s Thesaurus.
The main motivation for working with the 1911
Thesaurus instead of newer versions is that it is in
the public domain, along with related NLP-oriented
software packages. For applications that call for an
NLP-friendly thesaurus, WordNet has become the
de-facto standard. Although WordNet is a fine re-
sources, we believe that ignoring other thesauri is
a serious oversight. We show on three applications
how useful the 1911 Thesaurus is. We ran the well-
established tasks of determining semantic related-
ness of pairs of terms and identifying synonyms (Jar-
masz and Szpakowicz, 2004). We also proposed
a new method of representing the meaning of sen-
tences or other short texts using either WordNet or
Roget?s Thesaurus, and tested it on the data set pro-
vided by Li et al (2006). We hope that this work
will encourage others to use Roget?s Thesaurus in
their own NLP tasks.
Previous research on the 1987 version of Roget?s
Thesaurus includes work of Jarmasz and Szpakow-
icz (2004). They propose a method of determin-
ing semantic relatedness between pairs of terms.
Terms that appear closer together in the Thesaurus
get higher weights than those farther apart. The
experiments aimed at identifying synonyms using
a modified version of the proposed semantic sim-
ilarity function. Similar experiments were carried
out using WordNet in combination with a variety of
semantic relatedness functions. Roget?s Thesaurus
was found generally to outperform WordNet on these
problems. We have run similar experiments using
the 1911Thesaurus.
Lexical chains have also been developed using the
1987 Roget?s Thesaurus (Jarmasz and Szpakowicz,
2003). The procedure maps words in a text to the
Head (a Roget?s concept) from which they are most
likely to come. Although we did not experiment
416
with lexical chains here, they were an inspiration for
our sentence relatedness function.
Roget?s Thesaurus does not explicitly label the
relations between its terms, as WordNet does. In-
stead, it groups terms together with implied rela-
tions. Kennedy and Szpakowicz (2007) show how
disambiguating one of these relations, hypernymy,
can help improve the semantic similarity functions
in (Jarmasz and Szpakowicz, 2004). These hyper-
nym relations were also put towards solving analogy
questions.
This is not the first time the 1911 version of Ro-
get?s Thesaurus has been used in NLP research. Cas-
sidy (2000) used it to build the semantic network
FACTOTUM. This required significant (manual) re-
structuring, so FACTOTUM cannot really be con-
sidered a true version of Roget?s Thesaurus.
The 1987 data come from Penguin?s Roget?s The-
saurus (Kirkpatrick, 1987). The 1911 version is
available from Project Gutenberg1. We use WordNet
3.0, the latest version (Fellbaum, 1998). In the ex-
periments we present here, we worked with an inter-
face to Roget?s Thesaurus implemented in Java 5.02.
It is built around a large index which stores the lo-
cation in the thesaurus of each word or phrase; the
system individually indexes all words within each
phrase, as well as the phrase itself. This was shown
to improve results in a few applications, which we
will discuss later in the paper.
2 Content comparison of the 1911 and
1987 Thesauri
Although the 1987 and 1911 Thesauri are very sim-
ilar in structure, there are a few differences, among
them, the number of levels and the number of parts-
of-speech represented. For example, the 1911 ver-
sion contains some pronouns as well as more sec-
tions dedicated to phrases.
There are nine levels in Roget?s Thesaurus hierar-
chy, from Class down to Word. We show them in
Table 1 along with the counts of instances of each
level. An example of a Class in the 1911 Thesaurus
is ?Words Expressing Abstract Relations?, a Section
in that Class is ?Quantity? with a Subsection ?Com-
parative Quantity?. Heads can be thought of as the
heart of the Thesaurus because it is at this level that
1http://www.gutenberg.org/ebooks/22
2http://rogets.site.uottawa.ca/
Hierarchy 1911 1987
Class 8 8
Section 39 39
Subsection 97 95
Head Group 625 596
Head 1044 990
Part-of-speech 3934 3220
Paragraph 10244 6443
Semicolon Group 43196 59915
Total Words 98924 225124
Unique Words 59768 100470
Table 1: Frequencies of each level of the hierarchy in the
1911 and 1987 Thesauri.
the lexical material, organized into approximately a
thousand concepts, resides. Head Groups often pair
up opposites, for example Head #1 ?Existence? and
Head #2 ?Nonexistence? are found in the same Head
Group in both versions of the Thesaurus. Terms in
the Thesaurus may be labelled with cross-references
to other words in different Heads. We did not use
these references in our experiments.
The part-of-speech level is a little confusing, since
clearly no such grouping contains an exhaustive list
of all nouns, all verbs etc. We will write ?POS? to in-
dicate a structure in Roget?s and ?part-of-speech? to
indicate the word category in general. The four main
parts-of-speech represented in a POS are nouns,
verbs, adjectives and adverbs. Interjections are also
included in both the 1911 and 1987 thesauri; they are
usually phrases followed by an exclamation mark,
such as ?for God?s sake!? and ?pshaw!?. The Para-
graph and Semicolon Group are not given names,
but can often be represented by the first word.
The 1911 version also contains phrases (mostly
quotations), prefixes and pronouns. There are only
three prefixes ? ?tri-?, ?tris-?, ?laevo-? ? and six pro-
nouns ? ?he?, ?him?, ?his?, ?she?, ?her?, ?hers?.
Table 2 shows the frequency of paragraphs, semi-
colon groups and both total and unique words in a
given type of POS. Many terms occur both in the
1911 and 1987 Thesauri, but many more are unique
to either. Surprisingly, quite a few 1911 terms do not
appear in the 1987 data, as shown in Table 3; many
of them may have been considered obsolete and thus
dropped from the 1987 version. For example ?in-
grafted? appears in the same semicolon group as
417
POS Paragraph Semicolon Grp
1911 1987 1911 1987
Noun 4495 2884 19215 31174
Verb 2402 1499 10838 13958
Adjective 2080 1501 9097 12893
Adverb 594 499 2028 1825
Interjection 108 60 149 65
Phrase 561 0 1865 0
Total Word Unique Words
1911 1987 1911 1987
Noun 46308 114473 29793 56187
Verb 25295 55724 15150 24616
Adjective 20447 48802 12739 21614
Adverb 4039 5720 3016 4144
Interjection 598 405 484 383
Phrase 2228 0 2038 0
Table 2: Frequencies of paragraphs, semicolon groups,
total words and unique words by their part of speech; we
omitted prefixes and pronouns.
POS Both Only 1911 Only 1987
All 35343 24425 65127
N. 18685 11108 37502
Vb. 8618 6532 15998
Adj. 8584 4155 13030
Adv. 1684 1332 2460
Int. 68 416 315
Phr. 0 2038 0
Table 3: Frequencies of terms in either the 1911 or 1987
Thesaurus, and in both; we omitted prefixes and pro-
nouns.
?implanted? in the older but not the newer version.
Some mismatches may be due to small changes in
spelling, for example, ?Nirvana? is capitalized in the
1911 version, but not in the 1987 version.
The lexical data in Project Gutenberg?s 1911 Ro-
get?s appear to have been somewhat added to. For
example, the citation ?Go ahead, make my day!?
from the 1971 movie Dirty Harry appears twice (in
Heads #715-Defiance and #761-Prohibition) within
the Phrase POS. It is not clear to what extent new
terms have been added to the original 1911 Roget?s
Thesaurus, or what the criteria for adding such new
elements could have been.
In the end, there are many differences between the
1987 and 1911 Roget?s Thesauri, primarily in con-
tent rather than in structure. The 1987 Thesaurus is
largely an expansion of the 1911 version, with three
POSs (phrases, pronouns and prefixes) removed.
3 Comparison on applications
In this section we consider how the two versions of
Roget?s Thesaurus and WordNet perform in three ap-
plications ? measuring word relatedness, synonym
identification, and sentence relatedness.
3.1 Word relatedness
Relatedness can be measured by the closeness of the
words or phrases ? henceforth referred to as terms ?
in the structure of the thesaurus. Two terms in the
same semicolon group score 16, in the same para-
graph ? 14, and so on (Jarmasz and Szpakowicz,
2004). The score is 0 if the terms appear in differ-
ent classes, or if either is missing. Pairs of terms get
higher scores for being closer together. When there
are multiple senses of two terms A and B, we want
to select senses a ? A and b ? B that maximize the
relatedness score. We define a distance function:
semDist(A,B) = max
a?A,b?B
2 ? (depth(lca(a, b)))
lca is the lowest common ancestor and depth is the
depth in the Roget?s hierarchy; a Class has depth 0,
Section 1, ..., Semicolon Group 8. If we think of the
function as counting edges between concepts in the
Roget?s hierarchy, then it could also be written as:
semDist(A,B) = max
a?A,b?B
16?edgesBetween(a, b)
We do not count links between words in the same
semicolon group, so in effect these methods find
distances between semicolon groups, that is to say,
these two functions will give the same results.
The 1911 and 1987 Thesauri were compared
with WordNet 3.0 on the three data sets contain-
ing pairs of words with manually assigned similarity
scores: 30 pairs (Miller and Charles, 1991), 65 pairs
(Rubenstein and Goodenough, 1965) and 353 pairs3
(Finkelstein et al, 2001). We assume that all terms
are nouns, so that we can have a fair comparison
of the two Thesauri with WordNet. We measure the
correlation with Pearson?s Correlation Coefficient.
3http://www.cs.technion.ac.il/?gabr/resources/data/
wordsim353/wordsim353.html
418
Year Miller & Rubenstein & Finkelstein
Charles Goodenough et. al
Index words and phrase
1911 0.7846 0.7313 0.3449
1987 0.7984 0.7865 0.4214
Index phrase only
1911 0.7090 0.7168 0.3373
1987 0.7471 0.7777 0.3924
Table 4: Pearson?s coefficient values when not breaking /
breaking phrases up.
A preliminary experiment set out to determine
whether there is any advantage to indexing the words
in a phrase separately, for example, whether the
phrase ?change of direction? should be indexed only
as a whole, or as all of ?change?, ?of?, ?direction?
and ?change of direction?. The outcome of this ex-
periment appears in Table 4. There is a clear im-
provement: breaking phrases up gives superior re-
sults on all three data sets, for both versions of Ro-
get?s. In the remaining experiments, we have each
word in a phrase indexed.
We compare the results for the 1911 and 1987
Roget?s Thesauri with a variety of WordNet-based
semantic relatedness measures ? see Table 5. We
consider 10 measures, noted in the table as J&C
(Jiang and Conrath, 1997), Resnik (Resnik, 1995),
Lin (Lin, 1998), W&P (Wu and Palmer, 1994),
L&C (Leacock and Chodorow, 1998), H&SO (Hirst
and St-Onge, 1998), Path (counts edges between
synsets), Lesk (Banerjee and Pedersen, 2002), and
finally Vector and Vector Pair (Patwardhan, 2003).
The latter two work with large vectors of co-
occurring terms from a corpus, so WordNet is only
part of the system. We used Pedersen?s Semantic
Distance software package (Pedersen et al, 2004).
The results suggest that neither version of Ro-
get?s is best for these data sets. In fact, the Vector
method is superior on all three sets, and the Lesk
algorithm performs very closely to Roget?s 1987.
Even on the largest set (Finkelstein et al, 2001),
however, the differences between Roget?s Thesaurus
and the Vector method are not statistically signifi-
cant at the p < 0.05 level for either thesaurus on
a two-tailed test4. The difference between the 1911
Thesaurus and Vector would be statistically signifi-
4http://faculty.vassar.edu/lowry/rdiff.html
Method Miller & Rubenstein & Finkelstein
Charles Goodenough et. al
1911 0.7846 0.7313 0.3449
1987 0.7984 0.7865 0.4214
J&C 0.4735 0.5755 0.2273
Resnik 0.8060 0.8224 0.3531
Lin 0.7388 0.7264 0.2932
W&P 0.7641 0.7973 0.2676
L&C 0.7792 0.8387 0.3094
H&SO 0.6668 0.7258 0.3548
Path 0.7550 0.7842 0.3744
Lesk 0.7954 0.7780 0.4220
Vector 0.8645 0.7929 0.4621
Vct Pair 0.5101 0.5810 0.3722
Table 5: Pearson?s coefficient values for three data sets
on a variety of relatedness functions.
cant at p < 0.07.
On the (Miller and Charles, 1991) and (Ruben-
stein and Goodenough, 1965) data sets the best sys-
tem did not show a statistically significant improve-
ment over the 1911 or 1987 Roget?s Thesauri, even
at p < 0.1 for a two-tailed test. These data sets are
too small for a meaningful comparison of systems
with close correlation scores.
3.2 Synonym identification
In this problem we take a term q and we seek the
correct synonym s from a setC. There are two steps.
We used the system from (Jarmasz and Szpakowicz,
2004) for identifying synonyms with Roget?s. First
we find a set of terms B ? C with the maximum
relatedness between q and each term x ? C:
B = {x | argmax
x?C
semDist(x, q)}
Next, we take the set of terms A ? B where each
a ? A has the maximum number of shortest paths
between a and q.
A = {x | argmax
x?B
numberShortestPaths(x, q)}
If s ? A and |A| = 1, the correct synonym has been
selected. Often the sets A and B will contain just
one item. If s ? A and |A| > 1, there is a tie. If
s /? A then the selected synonyms are incorrect. If
a multi-word phrase c ? C of length n is not found,
419
ESL
Method Yes Tie No QNF ANF ONF
1911 27 3 20 0 3 3
1987 36 6 8 0 0 1
J&C 30 4 16 4 4 10
Resnik 26 6 18 4 4 10
Lin 31 5 14 4 4 10
W&P 31 6 13 4 4 10
L&C 29 11 10 4 4 10
H&SO 34 4 12 0 0 0
Path 30 11 9 4 4 10
Lesk 38 0 12 0 0 0
Vector 39 0 11 0 0 0
VctPair 40 0 10 0 0 0
TOEFL
1911 52 3 25 10 5 25
1987 59 7 14 4 4 17
J&C 34 37 9 33 31 90
Resnik 37 37 6 33 31 90
Lin 33 41 6 33 31 90
W&P 39 36 5 33 31 90
L&C 38 36 6 33 31 90
H&SO 60 16 4 1 0 1
Path 38 36 6 33 31 90
Lesk 70 1 9 1 0 1
Vector 69 1 10 1 0 1
VctPair 65 2 13 1 0 1
RDWP
1911 157 13 130 57 13 76
1987 198 17 85 22 5 17
J&C 100 146 54 62 58 150
Resnik 114 114 72 62 58 150
Lin 94 160 46 62 58 150
W&P 147 87 66 62 58 150
L&C 149 93 58 62 58 150
H&SO 170 82 48 4 6 5
Path 148 96 56 62 58 150
Lesk 220 7 73 4 6 5
Vector 216 7 73 4 6 5
VctPair 187 10 103 4 6 5
Table 6: Synonym selection experiments.
it is replaced by each of its words c1, c2..., cn, and
each of these words is considered in turn. The ci
that is closest to q is chosen to represent c. When
searching for a word in Roget?s or WordNet, we look
for all forms of the word.
The results of these experiments appear in Ta-
ble 6. ?Yes? indicates correct answers, ?No? ? in-
correct answers, and ?Tie? is for ties. QNF stands
for ?Question word Not Found?, ANF for ?Answer
word Not Found? and ONF for ?Other word Not
Found?. We used three data sets for this applica-
tion: 80 questions taken from the Test of English as a
Foreign Language (TOEFL) (Landauer and Dumais,
1997), 50 questions ? from the English as a Second
Language test (ESL) (Turney, 2001) and 300 ques-
tions ? from the Reader?s Digest Word Power Game
(RDWP) (Lewis, 2000 and 2001).
Lesk and the Vector-based systems perform bet-
ter than all others, including Roget?s 1911 and 1987.
Even so, both versions of Roget?s Thesaurus per-
formed well, and were never worse than the worst
WordNet systems. In fact, six of the ten Word-
Net-based methods are consistently worse than the
1911 Thesaurus. Since the two Vector-based sys-
tems make use of additional data beyond WordNet,
Lesk is the only completely WordNet-based system
to outperform Roget?s 1987. One advantage of Ro-
get?s Thesaurus is that both versions generally have
fewer missing terms than WordNet, though Lesk,
Hirst & St-Onge and the two vector based methods
had fewer missing terms than Roget?s. This may be
because the other WordNet methods will only work
for nouns and verbs.
3.3 Sentence relatedness
Our final experiment concerns sentence relatedness.
We worked with a data set from (Li et al, 2006)5.
They took a subset of the term pairs from (Ruben-
stein and Goodenough, 1965) and chose sentences
to represent these terms; the sentences are defini-
tions from the Collins Cobuild dictionary (Sinclair,
2001). Thirty people were then asked to assign re-
latedness scores to these sentences, and the average
of these similarities was taken for each sentence.
Other methods of determining sentence seman-
tic relatedness expand term relatedness functions to
5http://www.docm.mmu.ac.uk/STAFF/D.McLean/
SentenceResults.htm
420
create a sentence relatedness function (Islam and
Inkpen, 2007; Mihalcea et al, 2006). We propose
to approach the task by exploiting in other ways the
commonalities in the structure of Roget?s Thesaurus
and of WordNet. We use the OpenNLP toolkit6 for
segmentation and part-of-speech tagging.
We use a method of sentence representation that
involves mapping the sentence into weighted con-
cepts in either Roget?s or WordNet. We mean a
concept in Roget?s to be either a Class, Section, ...,
Semicolon Group, while a concept in WordNet is any
synset. Essentially a concept is a grouping of words
from either resource. Concepts are weighted by two
criteria. The first is how frequently words from the
sentence appear in these concepts. The second is the
depth (or specificity) of the concept itself.
3.3.1 Weighting based on word frequency
Each word and punctuation mark w in a sentence
is given a score of 1. (Naturally, only open-category
words will be found in the thesaurus.) If w has n
word senses w1, ..., wn, each sense gets a score of
1/n, so that 1/n is added to each concept in the
Roget?s hierarchy (semicolon group, paragraph, ...,
class) or WordNet hierarchy that contains wi. We
weight concepts in this way simply because, unable
to determine which sense is correct, we assume that
all senses are equally probable. Each concept in Ro-
get?s Thesaurus and WordNet gets the sum of the
scores of the concepts below it in its hierarchy.
We will define the scores recursively for a concept
c in a sentence s and sub-concepts ci. For example,
in Roget?s if the concept c were a Class, then each ci
would be a Section. Likewise, in WordNet if c were
a synset, then each ci would be a hyponym synset of
c. Obviously if c is a word sense wi (a word in either
a synset or a Semicolon Group), then there can be no
sub-concepts ci. When c = wi, the score for c is the
sum of all occurrences of the word w in sentence s
divided by the number of senses of the word w.
score(c, s) =
{
instancesOf(w,s)
sensesOf(w) if c = wi?
ci?c
score(ci, s) otherwise
See Table 7 for an example of how this sentence
representation works. The sentence ?A gem is a
jewel or stone that is used in jewellery.? is repre-
sented using the 1911 Roget?s. A concept is identi-
6http://opennlp.sourceforge.net
fied by a name and a series of up to 9 numbers that
indicate where in the thesaurus it appears. The first
number represents the Class, the second the Sec-
tion, ..., the ninth the word. We only show con-
cepts with weights greater than 1.0. Words not in
the thesaurus keep a weight of 1.0, but this weight
will not increase the weight of any concepts in Ro-
get?s or WordNet. Apart from the function words
?or?, ?in?, ?that? and ?a? and the period, only the
word ?jewellery? had a weight above 1.0. The cat-
egories labelled 6, 6.2 and 6.2.2 are the only an-
cestors of the word ?use? that ended up with the
weights above 1.0. The words ?gem?, ?is?, ?jewel?,
?stone? and ?used? all contributed weight to the cat-
egories shown in Table 7, and to some categories
with weights lower than 1.0, but no sense of the
words themselves had a weight greater than 1.0.
It is worth noting that this method only relies on
the hierarchies in Roget?s and WordNet. We do not
take advantage of other WordNet relations such as
hyponymy, nor do we use any cross-reference links
that exist in Roget?s Thesaurus. Including such re-
lations might improve our sentence relatedness sys-
tem, but that has been left for future work.
3.3.2 Weighting based on specificity
To determine sentence relatedness, one could, for
example, flatten the structures like those in Table 7
into vectors and measure their closeness by some
vector distance function such as cosine similarity.
There is a problem with this, though. A concept in-
herits the weights of all its sub-concepts, so the con-
cepts that appear closer to the root of the tree will far
outweigh others. Some sort of weighting function
should be used to re-adjust the weights of particular
concepts. Were this an Information Retrieval task,
weighting schemes such as tf.idf for each concept
could apply, but for sentence relatedness we propose
an ad hoc weighting scheme based on assumptions
about which concepts are most important to sentence
representation. This weighting scheme is the second
element of our sentence relatedness function.
We weight a concept in Roget?s and in WordNet
by how many words in a sentence give weight to it.
We need to re-weight it based on how specific it is.
Clearly, concepts near the leaves of the hierarchy are
more specific than those close to the root of the hier-
archy. We define specificity as the distance in levels
between a given word and each concept found above
421
Identifier Concept Weight
6 Words Relating to the Voluntary Powers - Individual Volition 2.125169028274
6.2 Prospective Volition 1.504066255252
6.2.2 Subservience to Ends 1.128154077172
8 Words Relating to the Sentiment and Moral Powers 3.13220884041
8.2 Personal Affections 1.861744448402
8.2.2 Discriminative Affections 1.636503978149
8.2.2.2 Ornament/Jewelry/Blemish [Head Group] 1.452380952380
8.2.2.2.886 Jewelry [Head] 1.452380952380
8.2.2.2.886.1 Jewelry [Noun] 1.452380952380
8.2.2.2.886.1.1 jewel [Paragraph] 1.452380952380
8.2.2.2.886.1.1.1 jewel [Semicolon Group] 1.166666666666
8.2.2.2.886.1.1.1.3 jewellery [Word Sense] 1.0
or - 1.0
in - 1.0
that - 1.0
a - 2.0
. - 1.0
Table 7: ?A gem is a jewel or stone that is used in jewellery.? as represented using Roget?s 1911.
it in the hierarchy. In Roget?s Thesaurus there are ex-
actly 9 levels from the term to the class. In WordNet
there will be as many levels as a word has ances-
tors up the hypernymy chain. In Roget?s, a term has
specificity 1, a Semicolon Group 2, a Paragraph 3,
..., a Class 9. In WordNet, the specificity of a word
is 1, its synset ? 2, the synset?s hypernym ? 3, its
hypernym ? 4, and so on. Words not found in the
Thesaurus or in WordNet get specificity 1.
We seek a function that, given s, assigns to
all concepts of specificity s a weight progressively
larger than to their neighbours. The weights in this
function should be assigned based on specificity, so
that all concepts of the same specificity receive the
same score. Weights will differ depending on a com-
bination of specificity and how frequently words that
signal the concepts appear in a sentence. The weight
of concepts with specificity s should be the highest,
of those with specificity s? 1 ? lower, of those with
specificity s ? 2 lower still, and so on. In order to
achieve this effect, we weight the concepts using a
normal distribution, where the mean is s:
f(x) =
1
?
?
2pi
e
?
? (x?s)
2
2?2
?
Since the Head is often considered the main cat-
egory in Roget?s, we expect a specificity of 5 to be
best, but we decided to test the values 1 through 9
as a possible setting for specificity. We do not claim
that this weighting scheme is optimal; other weight-
ing schemes might do better. For the purpose of
comparing the 1911 and 1987 Thesauri and Word-
Net, however, this method appears sufficient.
With this weighting scheme, we determine the
distance between two sentences using cosine simi-
larity:
cosSim(A,B) =
?
ai ? bi
??
a2i ?
??
b2i
For this problem we used the MIT Java WordNet In-
terface version 1.1.17.
3.3.3 Sentence similarity results
We used this method of representation for Roget?s
of 1911 and of 1987, as well as for WordNet 3.0 ?
see Figure 1. For comparison, we also implemented
a baseline method that we refer to as Simple: we
built vectors out of words and their count.
It can be seen in Figure 1 that each system is su-
perior for at least one of the nine specificities. The
Simple method is best at a specificity of 1, 8 and 9,
Roget?s Thesaurus 1911 is best at 6, Roget?s The-
saurus 1987 is best at 4, 5 and 7, and WordNet is
best at 2 and 3. The systems based on Roget?s and
WordNet more or less followed a bell-shaped curve,
with the curves of the 1911 and 1987 Thesauri fol-
lowing each other fairly closely and peaking close
together. WordNet clearly peaked first and then fell
the farthest.
7http://www.mit.edu/?markaf/projects/wordnet/
422
The best correlation result for the 1987 Roget?s
Thesaurus is 0.8725 when the mean is 4, the POS.
The maximum correlation for the 1911 Thesaurus is
0.8367, where the mean is 5, the Head. The max-
imum for WordNet is 0.8506, where the mean is 3,
or the first hypernym synset. This suggests that the
POS and Head are most important for representing
text in Roget?s Thesaurus, while the first hypernym
is most important for representing text using Word-
Net. For the Simple method, we found a more mod-
est correlation of 0.6969.
Figure 1: Correlation data for all four systems.
Several other methods have given very good
scores on this data set. For the system in (Li et
al., 2006), where this data set was first introduced, a
correlation of 0.816 with the human annotators was
achieved. The mean of all human annotators had a
score of 0.825, with a standard deviation of 0.072.
In (Islam and Inkpen, 2007), an even better system
was proposed, with a correlation of 0.853.
Selecting the mean that gives the best correlation
could be considered as training on test data. How-
ever, were we simply to have selected a value some-
where in the middle of the graph, as was our original
intuition, it would have given an unfair advantage
to either version of Roget?s Thesaurus over Word-
Net. Our system shows good results for both ver-
sions of Roget?s Thesauri and WordNet. The 1987
Thesaurus once again performs better than the 1911
version and than WordNet. Much like (Miller and
Charles, 1991), the data set used here is not large
enough to determine if any system?s improvement is
statistically significant.
4 Conclusion and future work
The 1987 version of Roget?s Thesaurus performed
better than the 1911 version on all our tests, but we
did not find the differences to be statistically signifi-
cant. It is particularly interesting that the 1911 The-
saurus performed as well as it did, given that it is al-
most 100 years old. On problems such as semantic
word relatedness, the 1911 Thesaurus performance
was fairly close to that of the 1987 Thesaurus, and
was comparable to many WordNet-based measures.
For problems of identifying synonyms both versions
of Roget?s Thesaurus performed relatively well com-
pared to most WordNet-based methods.
We have presented a new method of sentence
representation that attempts to leverage the struc-
ture found in Roget?s Thesaurus and similar lexi-
cal ontologies (among them WordNet). We have
shown that given this style of text representation
both versions of Roget?s Thesaurus work compara-
bly to WordNet. All three perform fairly well com-
pared to the baseline Simple method. Once again,
the 1987 version is superior to the 1911 version, but
the 1911 version still works quite well.
We hope to investigate further the representation
of sentences and other short texts using Roget?s
Thesaurus. These kinds of measurements can help
with problems such as identifying relevant sentences
for extractive text summarization, or possibly para-
phrase identification (Dolan et al, 2004). Another
? longer-term ? direction of future work could be
merging Roget?s Thesaurus with WordNet.
We also plan to study methods of automatically
updating the 1911 Roget?s Thesaurus with modern
words. Some work has been done on adding new
terms and relations to WordNet (Snow et al, 2006)
and FACTOTUM (O?Hara and Wiebe, 2003). Sim-
ilar methods could be used for identifying related
terms and assigning them to a correct semicolon
group or paragraph.
Acknowledgments
Our research is supported by the Natural Sciences
and Engineering Research Council of Canada and
the University of Ottawa. We thank Dr. Di-
ana Inkpen, Anna Kazantseva and Oana Frunza for
many useful comments on the paper.
423
References
S. Banerjee and T. Pedersen. 2002. An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In Proc. CICLing 2002, pages 136?145.
P. Cassidy. 2000. An investigation of the semantic rela-
tions in the roget?s thesaurus: Preliminary results. In
Proc. CICLing 2000, pages 181?204.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: ex-
ploiting massively parallel news sources. In Proc.
COLING 2004, pages 350?356, Morristown, NJ.
C. Fellbaum. 1998. A semantic network of english verbs.
In C. Fellbaum, editor, WordNet: An Electronic Lexi-
cal Database, pages 69?104. MIT Press, Cambridge,
MA.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In Proc.
10th International Conf. on World Wide Web, pages
406?414, New York, NY, USA. ACM Press.
G. Hirst and D. St-Onge. 1998. Lexical chains as rep-
resentation of context for the detection and correc-
tion malapropisms. In C. Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 305?322. MIT
Press, Cambridge, MA.
A. Islam and D. Inkpen. 2007. Semantic similarity of
short texts. In Proc. RANLP 2007, pages 291?297,
September.
M. Jarmasz and S. Szpakowicz. 2003. Not as easy as it
seems: Automating the construction of lexical chains
using roget?s thesaurus. In Proc. 16th Canadian Conf.
on Artificial Intelligence, pages 544?549.
M. Jarmasz and S. Szpakowicz. 2004. Roget?s thesaurus
and semantic similarity. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP 2003, Current Issues in Linguistic The-
ory, volume 260, pages 111?120. John Benjamins.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. 10th International Conf. on Research on Com-
putational Linguistics, pages 19?33.
A. Kennedy and S. Szpakowicz. 2007. Disambiguating
hypernym relations for roget?s thesaurus. In Proc. TSD
2007, pages 66?75.
B. Kirkpatrick, editor. 1987. Roget?s Thesaurus of En-
glish Words and Phrases. Penguin, Harmondsworth,
Middlesex, England.
T. Landauer and S. Dumais. 1997. A solution to Plato?s
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104:211?240.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet sense similiarity for word sense
disambiguation. In C. Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 265?284. MIT
Press, Cambridge, MA.
M. Lewis, editor. 2000 and 2001. Readers Digest,
158(932, 934, 935, 936, 937, 938, 939, 940), 159(944,
948). Readers Digest Magazines Canada Limited.
Y. Li, D. McLean, Z. A. Bandar, J. D. O?Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?
1150.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proc. 21st National Conf. on
Artificial Intelligence, pages 775?780. AAAI Press.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Process, 6(1):1?28.
T. P. O?Hara and J. Wiebe. 2003. Classifying functional
relations in factotum via wordnet hypernym associa-
tions. In Proc. CICLing 2003), pages 347?359.
S. Patwardhan. 2003. Incorporating dictionary and cor-
pus information into a vector measure of semantic re-
latedness. Master?s thesis, University of Minnesota,
Duluth, August.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of the 19th National Conference
on Artificial Intelligence., pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity. In Proc. 14th International Joint
Conf. on Artificial Intelligence, pages 448?453.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communication of the ACM,
8(10):627?633.
J. Sinclair. 2001. Collins Cobuild English Dictionary for
Advanced Learners. Harper Collins Pub.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proc COLING/ACL 2006, pages 801?808.
P. Turney. 2001. Mining the web for synonyms: Pmi-ir
versus lsa on toefl. In Proc. 12th European Conf. on
Machine Learning, pages 491?502.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. 32nd Annual Meeting of the
ACL, pages 133?138, New Mexico State University,
Las Cruces, New Mexico.
424
Workshop on TextGraphs, at HLT-NAACL 2006, pages 81?88,
New York City, June 2006. c?2006 Association for Computational Linguistics
Matching Syntactic-Semantic Graphs for Semantic Relation Assignment
Vivi Nastase1 and Stan Szpakowicz1,2
1 School of Information Technology and Engineering,
University of Ottawa, Ottawa, Canada
2 Institute of Computer Science,
Polish Academy of Sciences, Warsaw, Poland
{vnastase,szpak}@site.uottawa.ca
Abstract
We present a graph-matching algorithm
for semantic relation assignment. The al-
gorithm is part of an interactive text analy-
sis system. The system automatically ex-
tracts pairs of syntactic units from a text
and assigns a semantic relation to each
pair. This is an incremental learning algo-
rithm, in which previously processed pairs
and user feedback guide the process. Af-
ter each assignment, the system adds to its
database a syntactic-semantic graph cen-
tered on the main element of each pair of
units. A graph consists of the main unit
and all syntactic units with which it is syn-
tactically connected. An edge contains in-
formation both about syntax and about se-
mantic relations for use in further process-
ing. Syntactic-semantic graph matching is
used to produce a list of candidate assign-
ments for 63.75% of the pairs analysed,
and in 57% of situations the correct rela-
tions is one of the system?s suggestions;
in 19.6% of situations it suggests only the
correct relation.
1 Introduction
When analysing texts, it is essential to see how el-
ements of meaning are interconnected. This is an
old idea. The first chronicled endeavour to con-
nect text elements and organise connections between
them goes back to the 5th century B.C. and the work
of Panini1. He was a grammarian who analysed San-
skrit (Misra, 1966). The idea resurfaced forcefully
at several points in the more recent history of lin-
guistic research (Tesnie`re, 1959; Gruber, 1965; Fill-
more, 1968). Now it has the attention of many re-
searchers in natural language processing, as shown
by recent research in semantic parsing and semantic
1The sources date his work variously between the 5th and
7th century.
role labelling (Baker et al, 1998; Kipper et al, 2000;
Carreras and Marquez, 2004; Carreras and Marquez,
2005; Atserias et al, 2001; Shi and Mihalcea, 2005).
Graph-like structures are a natural way of or-
ganising one?s impressions of a text seen from
the perspective of connections between its simpler
constituents of varying granularity, from sections
through paragraphs, sentences, clauses, phrases,
words to morphemes.
In this work we pursue a well-known and of-
ten tacitly assumed line of thinking: connections at
the syntactic level reflect connections at the seman-
tic level (in other words, syntax carries meaning).
Anecdotal support for this stance comes from the
fact that the grammatical notion of case is the basis
for semantic relations (Misra, 1966; Gruber, 1965;
Fillmore, 1968). Tesnie`re (1959), who proposes a
grouping of verb arguments into actants and circum-
stances, gives a set of rules to connect specific types
of actants ? for example, agent or instrument ? to
such grammatical elements as subject, direct object,
indirect object. This idea was expanded to include
nouns and their modifiers through verb nominaliza-
tions (Chomsky, 1970; Quirk et al, 1985).
We work with sentences, clauses, phrases and
words, using syntactic structures generated by a
parser. Our system incrementally processes a text,
and extracts pairs of text units: two clauses, a verb
and each of its arguments, a noun and each of its
modifiers. For each pair of units, the system builds a
syntactic graph surrounding the main element (main
clause, head verb, head noun). It then tries to find
among the previously processed instances another
main element with a matching syntactic graph. If
such a graph is found, then the system maps pre-
viously assigned semantic relations onto the current
syntactic graph. We have a list of 47 relations that
manifest themselves in compound clauses, inside a
simple clause or in noun phrases. The list, a syn-
thesis of a number of relation lists cited in the lit-
erature, has been designed to be general, domain-
independent (Barker et al, 1997a).
Section 2 overviews research in semantic relation
analysis. Section 3 describes the text we used in ex-
81
periments, and the semantic relation list. Section 4
looks in detail at the graph-matching heuristic. Sec-
tion 5 describes the experimental setting and shows
how often the heuristic was used when processing
the input text. We show in detail our findings about
syntactic levels (how often graph matching helped
assign a relation between two clauses, a verb and its
arguments, or a noun and its modifier) and about the
accuracy of the suggestion. Discussion and conclu-
sions appear in Section 6.
2 Related Work
Some methods of semantic relation analysis rely on
predefined templates filled with information from
processed texts (Baker et al, 1998). In other meth-
ods, lexical resources are specifically tailored to
meet the requirements of the domain (Rosario and
Hearst, 2001) or the system (Gomez, 1998). Such
systems extract information from some types of syn-
tactic units (clauses in (Fillmore and Atkins, 1998;
Gildea and Jurafsky, 2002; Hull and Gomez, 1996);
noun phrases in (Hull and Gomez, 1996; Rosario et
al., 2002)). Lists of semantic relations are designed
to capture salient domain information.
In the Rapid Knowledge Formation Project (RKF)
a support system was developed for domain experts.
It helps them build complex knowledge bases by
combining components: events, entities and modi-
fiers (Clark and Porter, 1997). The system?s inter-
face facilitates the expert?s task of creating and ma-
nipulating structures which represent domain con-
cepts, and assigning them relations from a relation
dictionary.
In current work on semantic relation analysis, the
focus is on semantic roles ? relations between verbs
and their arguments. Most approaches rely on Verb-
Net (Kipper et al, 2000) and FrameNet (Baker et
al., 1998) to provide associations between verbs and
semantic roles, that are then mapped onto the cur-
rent instance, as shown by the systems competing in
semantic role labelling competitions (Carreras and
Marquez, 2004; Carreras and Marquez, 2005) and
also (Gildea and Jurafsky, 2002; Pradhan et al,
2005; Shi and Mihalcea, 2005).
These systems share two ideas which make them
different from the approach presented here: they all
analyse verb-argument relations, and they all use
machine learning or probabilistic approaches (Prad-
han et al, 2005) to assign a label to a new instance.
Labelling every instance relies on the same previ-
ously encoded knowledge (see (Carreras and Mar-
quez, 2004; Carreras and Marquez, 2005) for an
overview of the systems in the semantic role la-
belling competitions from 2004 and 2005). Pradhan
et al (2005) combine the outputs of multiple parsers
to extract reliable syntactic information, which is
translated into features for a machine learning ex-
periment in assigning semantic roles.
Our system analyses incrementally pairs of units
coming from three syntactic levels ? clause (CL),
intra-clause (or verb-argument, IC), noun-phrase
(NP). There are no training and testing data sets. In-
stead of using previously built resources, the system
relies on previously processed examples to find the
most appropriate relation for a current pair. Because
the system does not rely on previously processed
or annotated data, it is flexible. It allows the user
to customize the process for a specific domain by
choosing the syntactic units of interest and her own
list of relations that best fit the domain.
It is also interesting to assess, using the current
system configuration, the effect of syntactic infor-
mation and incremental learning on semantic analy-
sis. This is described in section 5.
Because of these differences in the type of data
used, and in the learning approach, the results we
obtain cannot be compared to previous approaches.
In order to show that the system does learn, we show
that the number of examples for which it provides
the correct answer increases with the number of ex-
amples previously analysed.
3 Input data and semantic relations
3.1 Input data
We work with a semi-technical text on meteorolog-
ical phenomena (Larrick, 1961), meant for primary
school students. The text gradually introduces con-
cepts related to precipitation, and explains them. Its
nature makes it appropriate for the semantic analy-
sis task in an incremental approach. The system will
mimic the way in which a human reader accumu-
lates knowledge and uses what was written before to
process ideas introduced later in the text.
The text contains 513 sentences, with an average
length of 9.13 words. There are 4686 word tokens
and 969 types. The difference between the num-
ber of types (2850) and tokens (573) in the extracted
pairs (which contain only open-class words) shows
that the same concepts recur, as expected in a didac-
tic text.
The syntactic structures of the input data are
produced by a parser with good coverage and de-
tailed syntactic information, DIPETT (Delisle and
Szpakowicz, 1995). The parser, written in Prolog,
implements a classic constituency English grammar
from Quirk et al (1985). Pairs of syntactic units
connected by grammatical relations are extracted
from the parse trees. A dependency parser would
82
produce a similar output, but DIPETT also provides
verb subcategorization information (such as, for ex-
ample, subject-verb-object or subject-verb-object-
indirect object), which we use to select the (best)
matching syntactic structures.
To find pairs, we use simple structural informa-
tion. If a unit is directly embedded in another unit,
we assume a subordinate relation between them; if
the two units are coordinate, we assume a coordinate
relation. These assumptions are safe if the parse is
correct. A modifier is subordinate to its head noun,
an argument to its head verb, and a clause perhaps to
the main clause in the sentence.
If we conclude that two units should interact, we
seek an appropriate semantic relation to describe this
interaction. The system uses three heuristics to find
one or more semantic relation candidates for the cur-
rent pair.
1. Word match ? the system will propose the se-
mantic relation(s) that have previously been as-
signed to a pair containing the same lemmas.
2. Syntactic graph match ? we elaborate this
heuristic in Section 4.
3. Marker ? the system uses a manually built dic-
tionary of markers (prepositions, coordinators,
subordinators) associated with the semantic re-
lations they indicate. The dictionary contains
325 markers, and a total of 662 marker-relation
associations.
If neither of the three heuristics yield results, the
system will present an empty list, and expect the user
to input the appropriate relation. When at least one
relation is proposed, the user can accept a unique
relation, choose among several options, or supply
a new one. The system records which action took
place, as well as the heuristic that generated the op-
tions presented to the user. The pair is also analysed
to determine the syntactic level from which it came,
to allow for a more detailed analysis of the behaviour
of the system.
3.2 Semantic relations
The list of semantic relations with which we work
is based on extensive literature study (Barker et al,
1997a). Three lists of relations for three syntactic
levels ? inter-clause, intra-clause (case) and noun-
modifier relations ? were next combined based on
syntactic and semantic phenomena. The resulting
list is the one used in the experiments we present
in this paper. The relations are grouped by general
similarity into 6 relation classes (H denotes the head
of a base NP, M denotes the modifier).
1. CAUSAL groups relations enabling or oppos-
ing an occurrence. Examples:
cause - H causes M: u virus;
effect - H is the effect (was caused by) M:
exam anxiety;
purpose - H is for M: concert hall;
2. CONJUNCTIVE includes relations that describe
the conjunction or disjunction of occurrences
(events/act/actions/states/activities), entities or
attributes:
conjunction - both H and M occur or exist
(and nothing more can be said about that
from the point of view of causality or
temporality): running and swimming (are
good for you);
disjunction - either one or both H and M occur
or exist: painting or drawing;
3. PARTICIPANT groups relations between an oc-
currence and its participants or circumstances.
Examples:
agent - M performs H: student protest;
object - M is acted upon by H: metal separa-
tor;
beneficiary - M benefits from H: student dis-
count;
4. SPATIAL groups relations that place an occur-
rence at an absolute or relative point in space.
Examples:
direction - H is directed towards M: outgoing
mail;
location - H is the location of M: home town;
location at - H is located at M: desert storm;
5. TEMPORAL groups relations that place an oc-
currence at an absolute or relative point in time.
Examples:
frequency - H occurs every time M occurs:
weekly game;
time at - H occurs when M occurs: morning
coffee;
time through - H existed while M existed: 2-
hour trip;
6. QUALITY groups the remaining relations be-
tween a verb or noun and its arguments. Exam-
ples:
manner - H occurs as indicated by M: stylish
writing;
83
material - H is made of M: brick house;
measure - M is a measure of H: heavy rock;
There is no consensus in the literature on a list of
semantic relations that would work in all situations.
This is, no doubt, because a general list of relations
such as the one we use would not be appropriate for
the semantic analysis of texts in a specific domain,
such as for example medical texts. All the relations
in the list we use were necessary, and sufficient, for
the analysis of the input text.
4 Syntactic-semantic graph-matching
Our system begins operation with a minimum of
manually encoded knowledge, and accumulates in-
formation as it processes the text. This design idea
was adopted from TANKA (Barker et al, 1997b).
The only manually encoded knowledge is a dictio-
nary of markers (subordinators, coordinators, prepo-
sitions). This resource does not affect the syntactic-
semantic graph-matching heuristic.
Because the system gradually accumulates
knowledge as it goes through the input text, it uses a
form of memory-based learning to make predictions
about the semantic relation that fits the current pair.
The type of knowledge that it accumulates consists
of previously analysed pairs, together with the
semantic relation assigned, and a syntactic-semantic
graph centered on each word in a sentence which
appears as the main element in a processed pair.
To process a pair P not encountered previously,
the system builds a graph centered on the main ele-
ment (often the head) of P . This idea was inspired
by Delisle et al (1993), who used a list of argu-
ments surrounding the main verb together with the
verb?s subcategorization information and previously
processed examples to analyse semantic roles (case
relations). In recent approaches, syntactic informa-
tion is translated into features which, together with
information from FrameNet, WordNet or VerbNet,
will be used with ML tools to make predictions for
each example in the test set (Carreras and Marquez,
2004; Carreras and Marquez, 2005).
Our system builds a (simple) graph surrounding
a head word (which may be a verb ? representing
the predicate of a sentence, or representing a clause
? or noun), and matches it with previously analysed
examples.
A graph G(w) centered on word w consists of
the following: a node for w; a set of nodes for
each of the words wi in the sentence with which wis connected by a grammatical relation (including
situations when w is a modifier/argument); edges
that connect w with each wi, tagged with gram-
matical relation GR (such as subject, object, com-
plement) and connective information Con (preposi-
tions, coordinators, subordinators, or nil). The nodes
also contain part-of-speech information for the cor-
responding word, and other information from the
parser (such as subcategorization structure for the
verb, if it is available).
Graph matching starts with the central node, and
continues with edge matching. If G(w) is the graph
centered on word w whose pairs are currently being
processed, the system selects from the collection of
previously stored graphs, a set of graphs {G(wi)},
which satisfy the following conditions:
? The central nodes match. The matching is
guided by a set of contraints. We choose the
graphs centered on the nodes that satisfy the
most constraints, presented here in the order of
their importance:
? w and wi must have the same part of
speech.
? w and wi have the same syntactic proper-ties. If w and wi are verbs, they must have
the same subcategorization structure.
? w and wi are the same lemma. We empha-size that a graph centered on a different
lemma, but with the same subcategoriza-
tion structure is preferred to a graph with
the same lemma, but a different subcate-
gorization structure.
? The edge representing the word pair to which
we want to assign a semantic relation has a
match in G(wi). From all graphs that com-ply with this constraint, the ones that have the
lowest distance ? corresponding to the high-
est matching score ? are chosen. The graphs
are matched edge by edge. Two edges match
if the grammatical relation and the connectives
match. Figure 1 shows the formula that com-
putes the distance between two graphs. We
note that edge matching uses only edge infor-
mation ? grammatical and connective informa-
tion. Using the node information as is (lemmas
and their part of speech) is too restrictive. We
are looking into using word similarity as a so-
lution of node matching.
If no matching graph has been found, the system
searches for a simpler match, in which the current
word pair is matched against previously processed
pairs, using the same formula as for edge distance,
and preferring the pairs that have the same modifier.
This algorithm will retrieve the set of graphs
{G(wi)}, which give the same score when matched
84
Definition of a graph centered on w:
G(w) = {wi, edge(w,wi) or edge(wi, w)|wiappears in sent. S, and is connected to w}
edge(w,wi) = {GRi, Coni} ; GRi ? {subject, object, complement, ...}
Coni ? {at, in, on,with, for, ...}
Distance metric between two graphs:
dist(G(w1), G(w2)) =
N
?
k=1
d(edge1k , edge2k); edgeik ? G(wi), N is the number of edges in G(wi)
d(edge1k , edge2k)=d({GR1k , Con1k}, {GR2k , Con2k})
=d1(GR1k, GR2k) + d1(Con1k, Con2k)
d1(x, y) =
{ 0 : x = y
1 : x 6= y
Figure 1: Distance between two graphs
with the current graph. The set of possible semantic
relations presented to the user consists of the seman-
tic relation on the edge of each G(wi) that matchesthe edge (of the current graph) corresponding to the
word pair which we are analysing.
To the sentence:
When you breathe out on a cold day, you make a
cloud.
corresponds the following syntactic graph:
(compl,nil)
(subord,when)out (v, sv)
(com
pl,nil)
(v,svo)
breathe make
cold
day
you you
(su
bj,
nil
)
(subj,nil)
(c
om
pl
,on
)
cloud
When we focus on the graph centered on a spe-
cific word, such as breathe, we look only at the node
corresponding to the word breathe, and the nodes
adjacent to it.
To process a pair P = (wH , wM ), the system firstbuilds G(wH), and then searches through previouslystored graphs for those which have the same center
wH , or have the same part of speech as wH assigned
to its central node. For each graph found, we com-
pute a distance that gives a measure of the match
between the two graphs. The best match will have
the smallest distance.
For example, for the sentence:
Weathermen watch the clouds day and night.
the system builds the following network centered
on the predicate watch2:
cloudweatherman
watch
(v, svo)
(subj,nil)
(c
om
pl
,n
il)
day and night
(co
mp
l,n
il)
The system locates among previously stored net-
works those centered around verbs3. For the sen-
tence above, the system uses the following graph,
2The nil value on the edges means that no preposition or
other connective explicitly links the two words or the corre-
sponding syntactic structures.
3If more detailed information is available, the system will
choose only networks associated with verbs that have the same
subcategorisation pattern (svo, svoi and so on).
85
built from the immediately preceding sentence in the
text:
Air pilots know that clouds can bring rain, hail,
sleet and snow.
(v, svo)
(subj,nil)
know
air pilots bring
(co
mp
l,n
il)
AGENT
OB
JE
CT
According to the metric, the networks match and
the pairs (watch, weatherman) and (know, air pi-
lots) match, so the semantic relation for the pair
(know, air pilots) is proposed as a possible relation
for pair (watch, weatherman) .
5 Experiments
The system processes the 513 sentences interac-
tively. It begins by running the DIPETT parser.
Next, it extracts syntactic units (clauses, phrases,
words) and pairs them up according to the informa-
tion in the parse tree. Each unit is represented by
its head word. Next, the system checks if the same
pair of word lemmas has already been processed, to
propose the same relation(s) to the user as options.
If not, the system builds a graph centered on the
head word, and proceeds with the matching on pre-
viously encountered instances, as described in sec-
tion 4. When a set of candidates has been found, the
system goes through a dialogue with the user.
The system generated 2020 pairs from the 513
sentences. The experiment was run in 5 interactive
sessions of approximately 3 hours each. The total
net processing time was 6 hours, 42 minutes and 52
seconds4 . While it would have been instructive to
run the system several times with different users, it
was not feasible. The experiment was run once, with
two cooperating users. They were instructed on the
set of semantic relations, and told how the system
works. They discussed the semantic relation assign-
ment and, once agreed, compared the system?s sug-
gestion with their decision.
DIPETT did not produce a complete parse for all
sentences. When a complete parse (correct or incor-
rect) was not possible, DIPETT produced fragmen-
tary parses. The semantic analyser extracted units
even from tree fragments, although sometimes the
fragments were too small to accommodate any pairs.
Of the 513 input sentences, 441 had a parse tree that
allowed the system to extract pairs.
4The time difference accounts for system processing times,
and user interaction for other steps of the analysis.
# of analyzed examples 1475
level statistics CL IC NP
64 978 433
user actions accept choose supply
459 393 623
avg. # of suggestions 2.81
graph-matching usage 933
level/action statistics CL IC NP
accept 183 (19.61%) 9 141 33
choose 349 (37.41%) 23 314 12
supply 401 (42.98%) 27 316 58
Table 1: Summary of semantic analysis
Of 2020 pairs generated, the users discarded 545
in the dialogue step. An example of an erroneous
pair comes from the sentence:
Tiny clouds drift across like feathers on parade.
The semantic analyser produced the pair
(drift,parade), because of a parsing error: pa-
rade was parsed as a complement of drift, instead
of a post-modifier for feathers. The correct pairing
(feather,parade) is missing, because it cannot be
inferred from the parse tree.
Table 1 gives a summary of the processing statis-
tics. We observe that graph-matching was used to
process a clear majority of the total pairs extracted ?
63.25% (933/1475) , leaving the remaining 36.75%
for the other two heuristics and for cases where no
suggestion could be made. In 57.02% of the situa-
tions when graph-matching was used, the system?s
suggestion contained the correct answer (user?s ac-
tion was either accept or choose), and in 19.61% of
the situations a single correct semantic relation was
proposed (user action was accept).
When the system presents multiple suggestions to
the user, including the correct one, the average num-
ber of suggestions is 3.75. The small number of
suggestions shows that the system does not simply
add to the list relations that it has previously encoun-
tered, but it learns from past experience and graph-
matching helps it make good selections. Figure 2
plots the difference between the number of exam-
ples for which the system gives the correct answer
(possibly among other suggestions) and the number
of examples when the user must supply the correct
relation, from the first example processed until the
end of the experiment. We observe a steady increase
in the number of correctly processed examples.
Our system does not differentiate between syntac-
tic levels, but based on the structures of the syntac-
tic units in each pair we can decide which syntactic
level it pertains to. For a more in-depth analysis, we
have separated the results for each syntactic level,
86
-20
 0
 20
 40
 60
 80
 100
 120
 140
 0  100  200  300  400  500  600  700  800  900  1000
D
iff
er
en
ce
 in
 n
um
be
r o
f e
xa
m
pl
es
Examples processed
Figure 2: Difference between the number of situa-
tions in which the user accepts or chooses from the
system?s suggestions, and when it must supply the
correct relation
and present them for comparison in Figure 3.
We observe that the intra-clause level ? verbs
and their arguments ? makes the best use of graph-
matching, with the curve showing the cumulative
number of situations in which the system makes cor-
rect predictions becoming steeper as more text is
processed. At the same time, the curve that plots the
cumulative number of cases in which the user has to
supply a correct answer begins to level off. As ex-
pected, at the noun-phrase level where the syntactic
structures are very simple, often consisting of only
the noun and its modifier (without even a connec-
tive), the graph-matching algorithm does not help as
much. At the inter-clause level the heuristic helps,
as shown by the marginally higher curve for cumula-
tive accept/choose user actions, compared to supply
actions.
6 Conclusions
We have shown through the results gathered from an
interactive and incremental text processing system
that syntactic-semantic graph-matching can be used
with good results for semantic analysis of texts. The
graph-matching heuristic clearly dominates other
heuristics used, and it learns to make better predic-
tions as more examples accumulate.
Graph-matching is most useful for assigning se-
mantic relations between verbs and their arguments,
but it also gives good results for inter-clause rela-
tions. At the noun-phrase level, we could only tackle
noun-modifier pairs that exhibit a modicum of syn-
tactic structure ? a connective. For base NPs there
is practically nothing that syntactic information can
bring to the semantic analysis process.
The graph-matching process could be improved
by bringing into play freely available lexical re-
1. All syntactic levels
 0
 100
 200
 300
 400
 500
 600
 0  100  200  300  400  500  600  700  800  900  1000
Cu
m
m
ul
at
ive
 n
um
be
r o
f e
xa
m
pl
es
Examples processed
accept+choose
supply
2. Clause level (CL)
 0
 5
 10
 15
 20
 25
 30
 35
 0  10  20  30  40  50  60
Cu
m
m
ul
at
ive
 n
um
be
r o
f e
xa
m
pl
es
Examples processed
accept+choose
supply
3. Intra-clause level (IC)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
 0  100  200  300  400  500  600  700  800
Cu
m
m
ul
at
ive
 n
um
be
r o
f e
xa
m
pl
es
Examples processed
accept+choose
supply
4. Noun phrase level
(NP)
 0
 10
 20
 30
 40
 50
 60
 0  20  40  60  80  100  120
Cu
m
m
ul
at
ive
 n
um
be
r o
f e
xa
m
pl
es
Examples processed
accept+choose
supply
Figure 3: Graph-matching for different syntactic
levels
87
sources. For now, the actual words in the graph
nodes are not used at all. We could use WordNet
to compute word similarities, to select closer match-
ing graphs. VerbNet or FrameNet information could
help choose graphs centered on verbs with simi-
lar syntactic behaviour, as captured by Levin?s verb
groups (Levin, 1993) which are the basis of VerbNet.
References
Jordi Atserias, L. Padro?, and German Rigau. 2001. Integrating
multiple knowledge sources for robust semantic parsing. In
Proceedings of RANLP - 2001, Tsigov Czark, Bulgaria.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In COLING-ACL, pages
86?90, Montreal, Canada.
Ken Barker, Terry Copeck, Sylvain Delisle, and Stan Szpakow-
icz. 1997a. Systematic construction of a versatile case sys-
tem. Journal of Natural Language Engineering, 3(4):279?
315.
Ken Barker, Sylvain Delisle, and Stan Szpakowicz. 1997b.
Test-driving TANKA: Evaluating a semi-automatic system
of text analysis for knowledge acquisition. In Proceedings
of CAI 1997, pages 60?71, Vancouver, BC, Canada.
Xavier Carreras and Lluis Marquez, editors. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role Labelling.
Boston, MA, USA.
Xavier Carreras and Lluis Marquez, editors. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role Labelling.
Ann Arbour, MI, USA.
Noam Chomsky. 1970. Remarks on nominalizations. In Rod-
erick Jacobs and Peter Rosenbaum, editors, Readings in En-
glish Transformational Grammar, pages 184?221. Ginn and
Co., Waltham, MA, USA.
Peter Clark and Bruce Porter. 1997. Building concept reprezen-
tations from reusable components. In AAAI, pages 367?376,
Providence, Rhode Island.
Sylvain Delisle and Stan Szpakowicz. 1995. Realistic pars-
ing: Practical solutions of difficult problems. In PACLING,
Brisbane, Queensland, Australia.
Sylvain Delisle, Terry Copeck, Stan Szpakowicz, and Ken
Barker. 1993. Pattern matching for case analysis: A com-
putational definition of closeness. In ICCI, pages 310?315,
Sudbury, ON, Canada.
Charles Fillmore and Beryl T. Atkins. 1998. FrameNet and
lexicographic relevance. In Proceedings of the 1st Interna-
tional Conference on Language Resources and Evaluation,
Granada, Spain.
Charles Fillmore. 1968. The case for case. In Emmond Bach
and Robert T. Harms, editors, Universals in Linguistic The-
ory, pages 1?88. Holt, Rinehart and Winston.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3):245?
288.
Fernando Gomez. 1998. A representation of complex events
and processes for the acquisition of knowledge from text.
Kowledge-Based Systems, 10(4):237?251.
Jeffrey Gruber. 1965. Studies in Lexical Relations. Ph.D.
thesis, MIT, Cambridge, MA. Reprinted in Jeffrey Gru-
ber. 1976. Lexical Structures in Syntax and Semantics. Part
I. North-Holland Publishing Company, Amsterdam.
Richard D. Hull and Fernando Gomez. 1996. Semantic inter-
pretation of nominalizations. In 13th National Conference
on Artificial Intelligence, pages 1062?1068, Portland, Ore-
gon, USA.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In AAAI/IAAI,
pages 691?696.
Nancy Larrick. 1961. Junior Science Book of Rain, Hail, Sleet
and Snow. Garrard Publishing Company, Champaign, Illi-
nois.
Beth Levin. 1993. English Verb Classes and Alternations. Uni-
versity of Chicago Press.
Vidya Niwas Misra. 1966. The Descriptive Technique of
Panini. Mouton, The Hague.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Mar-
tin, and Daniel Jurafsky. 2005. Semantic role labelling us-
ing different syntactic views. In Proceedings of ACL 2005,
pages 581?588, Ann Arbour, MI, USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan
Svartvik. 1985. A Comprehensive Grammar of the English
Language. Longman, London and New York.
Barbara Rosario and Marti Hearst. 2001. Classifying the se-
mantic relations in noun-compounds via a domain specific
hierarchy. In EMNLP, pages 82?90, Pittsburg, PA, USA.
Barbara Rosario, Marti Hearst, and Charles Fillmore. 2002.
The descent of hierarchy and selection in relational seman-
tics. In ACL, Philadelphia, PA, USA.
Lei Shi and Rada Mihalcea. 2005. Putting pieces together:
Combining framenet, verbnet and wordnet for robust seman-
tic parsing. In Proceedings of CICLing 2005, pages 100?
111, Mexico City, Mexico.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe structurale. C.
Klincksieck, Paris.
88
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 37?47, Dublin, Ireland, August 23-29 2014.
Hierarchical Topical Segmentation with Affinity Propagation
Anna Kazantseva & Stan Szpakowicz
School of Electrical Engineering and Computer Science
University of Ottawa
Ottawa, Ontario, Canada
{ankazant,szpak}@eecs.uottawa.ca
Abstract
We present a hierarchical topical segmenter for free text. Hierarchical Affinity Propagation for
Segmentation (HAPS) is derived from a clustering algorithm Affinity Propagation. Given a doc-
ument, HAPS builds a topical tree. The nodes at the top level correspond to the most prominent
shifts of topic in the document. Nodes at lower levels correspond to finer topical fluctuations.
For each segment in the tree, HAPS identifies a segment centre ? a sentence or a paragraph which
best describes its contents. We evaluate the segmenter on a subset of a novel manually segmented
by several annotators, and on a dataset of Wikipedia articles. The results suggest that hierarchical
segmentations produced by HAPS are better than those obtained by iteratively running several
one-level segmenters. An additional advantage of HAPS is that it does not require the ?gold
standard? number of segments in advance.
1 Introduction
When an NLP application works with a document, it may benefit from knowing something about this
document?s high-level structure. Text summarization (Haghighi and Vanderwende, 2009), question an-
swering (Oh et al., 2007) and information retrieval (Ponte and Croft, 1998) are some of the examples
of such applications. Topical segmentation is a lightweight form of such structural analysis: given a
sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by
a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the
author, such as speech transcripts, meeting notes or literature.
The past decade has witnessed significant progress in the area of text segmentation. Most of the topical
segmenters (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008; Kazantseva and Szpakowicz,
2011; Misra et al., 2011; Du et al., 2013) can only produce single-level segmentation, a worthy endeavour
in and of itself. Yet, to view the structure of a document linearly, as a sequence of segments, is in certain
discord with most theories of discourse structure, where it is more customary to consider documents as
trees (Mann and Thompson, 1988; Marcu, 2000; Hernault et al., 2010; Feng and Hirst, 2012) or graphs
(Wolf and Gibson, 2006). Regardless of the theory, we hypothesize that it may be useful to have an idea
about fluctuations of topic in documents beyond the coarsest level. It is the contribution of this work that
we develop such a hierarchical segmenter, implement it and do our best to evaluate it.
The segmenter described here is HAPS ? Hierarchical Affinity Propagation for Segmentation. It is
closely based on a graphical model for hierarchical clustering called Hierarchical Affinity Propagation
(Givoni et al., 2011). It is a similarity-based segmenter. It takes as input a matrix of similarities between
atomic units of text in the sequence to be segmented (sentences or paragraphs), the desired number of
levels in the topical tree and a preference value for each data point and each level. This value captures
a priori belief about how likely it is that this data point is a segment centre at that level. The preference
values also control the granularity of segmentation: how many segments are to be identified at each level.
The output is a topical tree. For each segment at every level, HAPS also finds a segment centre, a data
point which best describes the segment.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
37
The objective function maximized by the segmenter is net similarity ? the sum of similarities between
all segment centres and their children for all levels of the tree. This function is similar to the objective
function of the well-known k-means algorithm, except that here it is computed hierarchically.
It is not easy to evaluate HAPS. We are not aware of comparable hierarchical segmenters other than
that in (Eisenstein, 2009) which, unfortunately, is no longer publicly available. Therefore we compared
the trees built by HAPS to the results of running iteratively two state-of-the-art flat segmenters. The
results are compared on two datasets. A set of Wikipedia articles was automatically compiled by Carroll
(2010). The other set, created to evaluate HAPS, consists of nine chapters from the novel Moonstone by
Wilkie Collins. Each chapter was annotated for hierarchical structure by 3-6 people.
The evaluation is based on two metrics, windowDiff (Pevzner and Hearst, 2002) and evalHDS (Car-
roll, 2010). Both metrics are less then ideal. They do not give a complete picture of the quality of
topical segmentations, but the preliminary results suggest that running a global model for hierarchical
segmentation produces better results then iteratively running flat segmenters. Compared to the baseline
segmenters, HAPS has an important practical advantage. It does not require the number of segments as
an input; this requirement is customary for most flat segmenters.
We also made a rough attempt to evaluate the quality of the segment centres identified by HAPS. Using
20 chapters from several novels of Jane Austen, we compared the centres identified for each chapter
against summaries produces by a recent automatic summarizer CohSum (Smith et al., 2012). The basis
of comparison was the ROUGE metric (Lin, 2004). While far from conclusive, the results suggest that
segment centres identified by HAPS are rather comparable with the summaries produced by an automatic
summarizer.
A Java implementation of HAPS and the corpus of hierarchical segmentations for nine chapters of
Moonstone are publicly available. We consider these to be the main contributions of this research.
2 Related work
Most work on topical text segmentation has been done for single-level segmentation. Contemporary
approaches usually rely on the idea that topic shifts can be identified by finding shifts in the vocabulary
(Youmans, 1991). We can distinguish between local and global models for topical text segmentation.
Local algorithms have a limited view of the document. For example, TextTiling (Hearst, 1997) operates
by sliding a window through the input sequence and computing similarity between adjacent units. By
identifying ?valleys? in similarities, TextTiling identifies topic shifts. More recently, Marathe (2010)
used lexical chains and Blei and Moreno (2001) used Hidden Markov Models. Such methods are usually
very fast, but can be thrown off by small digressions in the text.
Among global algorithms, we can distinguish generative probabilistic models and similarity-based
models. Eisenstein and Barzilay (2008) model a document as a sequence of segments generated by latent
topic variables. Misra et al. (2011) and Du et al. (2013) have similar models. Malioutov and Barzilay
(2006) and (Kazantseva and Szpakowicz, 2011) use similarity-based representations. Both algorithms
take as input a matrix of similarities between sentences of the input document; the former uses graph
cuts to find cohesive segments, while the latter modifies a clustering algorithm to perform segmentation.
Research on hierarchical segmentation has been more scarce. Yaari (1997) produced hierarchical
segmentation by agglomerative clustering. Eisenstein (2009) used a Bayesian model to create topical
trees, but the system is regrettably no longer publicly available. Song et al. (2011) develop an algorithm
for hierarchical segmentation which iteratively splits a document in two at a place where cohesion links
are the weakest. A second pass transforms a deep binary tree into a shallow and broad structure.
Any flat segmenter can certainly be used iteratively to create trees of segments by subdividing each
segment, but this may be problematic. Topical segmenters are not perfect, so running them iteratively is
likely to compound the error. Most segmenters also require the number of segments as an input. This
estimate is feasible for flat segmentation. To know in advance the number of segments and sub-segments
at each level is not a realistic requirement when building a tree.
This work describes a hierarchical model of text segmentation. It takes a global view of the document
and of the topical hierarchy. Each iteration attempts to find the best assignment of segments for the
38
whole tree. It does not need to know the exact number of segments. Instead, it takes a more abstract
parameter, preference values, to specify the granularity of segmentation at each level. For each segment
it also outputs a segment centre, a unit of text which best captures the contents of the segment.
3 Creating a corpus of hierarchical segmentations
Before embarking on the task of building a hierarchical segmenter, we wanted to study how people
perform such a task. We also needed a benchmark corpus which could be used to evaluate the quality of
segmentations produced by HAPS.
To this end, we annotated nine chapters of the novel Moonstone for hierarchical structure. We settled
on these data because it is a subset of a publicly available dataset for flat segmentation (Kazantseva
and Szpakowicz, 2012). In our study, each chapter was annotated by 3-6 people (4.8 on average). The
annotators, undergraduate students of English, were paid $50 dollars each.
Procedure. The instructions asked the annotator to read the chapter and split it into top-level segments
according to where there is a perceptible shift of topic. She had to provide a one-sentence description of
what the segment is about. The procedure had to be repeated for each segment all the way down to the
level of individual paragraphs. Effectively, the annotators were building a detailed hierarchical outline
for each chapter.
Metrics. Two different metrics helped estimate the quality of our hierarchical dataset: windowDiff
(Pevzner and Hearst, 2002) and S (Fournier and Inkpen, 2012).
windowDiff is computed by sliding a window across the input sequence and checking, for each window
position, whether the number of reference breaks is the same as the number of breaks in the hypothetical
segmentation. The number of erroneous windows is then normalized by the total number of windows. In
Equation 1, N is the length of the input sequence and k is the size of the sliding window.
windowDiff =
1
N ? k
N?k
?
i=1
(|ref ? hyp| 6= 0) (1)
windowDiff is designed to compare sequences of segments, not trees. That is why we compute it for
each level between each pair of annotators who worked on the same chapter. It should be noted that
windowDiff is a penalty metric: higher values indicate less agreement (windowDiff= 0 corresponds to
two identical segmentations).
The S metric allows us to compare trees and take into account situations when the segmenter places a
boundary at a correct position but at a wrong level. S is an edit-distance metric. It computes the number
of operations necessary to turn one segmentation into another. There are three types of editing operations:
add/delete, transpose and substitute (change the level in the tree). The sum is normalized by the number
of possible boundaries in the sequence. S has an unfortunate downside of being too optimistic, but it
allows the breakdown of error types and it explicitly compares trees.
Unlike windowDiff, S is a similarity metric: higher values correspond to more similar segmentations.
The value of S between two identical segmentations is 1.
S(bs
a
, bs
b
, n) =
1? |boundary distance(bs
a
, bs
b
, n)|
pb(D)
(2)
Here boundary distance(bs
a
, bs
b
, n) is the total number of edit operations needed to turn a segmen-
tation bs
a
into bs
b
, n is the threshold defining the maximum distance of transpositions. pb(D) is the
maximum possible number of edits. Segmentations bs
a
and bs
a
are represented as strings of sets of
boundary positions. For example bs
a
= ({2}, {1,2}, {1,2}) corresponds to a hierarchical segmentation of
a three-unit sequence in the following manner: a segment boundary at level 1 after the first unit, segment
boundaries at levels 1 and 2 after the second unit and the third unit.
Corpus Analysis. On average, the annotators took 3.5 hours to complete the task (? = 1.6). The
average depth of the tree is 3.00 levels (? = 0.65), suggesting that the annotators prefer shallow but broad
structures. Table 1 reports the average breadth of the tree at different levels. In the Table and further
39
in this paper we refer to the bottom level of the tree (i.e., the leaves of the tree or the most fine-grained
level of segmentation) as level 1. In Table 1, level 4 refers to the top level of the tree (the coarsest
segmentations). The values were computed using only the breaks explicitly specified by the annotators
(i.e., we did not assume that a break at a coarse level implies a break at a more detailed level).
The average breadth of the trees at the bottom (level 1) is lower than that at level 2, indicating that only
a small percentage of the entire tree was annotated more than three levels deep. The table also shows the
average values of windowDiff computed for each possible pair of annotators. The values worsen toward
the bottom of the tree, suggesting that the annotators agree more about top-level segments and less and
less about finer fluctuations of topic.
We hypothesize that these shallow broad structures are due to the fact that it is difficult for people to
create deep recursive structures in their mental representations. We do not, however, have any hard data
to support this hypothesis. Many of the annotators specifically commented on the difficulty of the task. 9
out of 23 people included comments ranging from notes about specific places to general comments about
their lack of confidence. 4 annotators found several (specific) passages they had trouble with.
The average value of pairwise S is 0.79. We have noted earlier that the S metric tends to be optimistic
(that is due to its normalization factor) but it provides a breakdown of disagreements between the anno-
tators. According to S, 46.14% of disagreements are errors of omission (some of the annotators did not
include segment breaks where others did), 47.56% are disagreements about the level of segmentation
(the annotators placed boundaries in the same place but at different levels) and only 6.31% are errors
of transposition (the annotators do not agree about the exact placement but place boundaries within 1
position of each other). This distribution is more interesting than the overall value of S. Among other
things, it shows why it is so important to take into account adjacent levels when evaluating topical trees.
4 The HAPS algorithm
1
4.1 Factor graphs
The HAPS segmenter is based on factor graphs, a unifying formalism for such graphical models as
Markov or Bayesian networks. A factor graph is a bi-partite graph with two types of nodes, factor or
function nodes and variable nodes. Each factor node is connected to those variable nodes which are
its arguments. Running the well-known Max-Sum algorithm (Bishop, 2006) on a factor graph finds a
configuration of variables which maximizes the sum of all component functions. This is a message-
passing algorithm. All variable nodes send messages to their factor neighbours (functions in which those
nodes are variables) and all factor nodes send messages to their variable neighbours (their arguments).
A message ?
x?f
sent from a variable node x to a function node f is computed as a sum of all incoming
messages to x, except the message from the recipient function f :
?
x?f
=
?
f
?
?N(x)\f
?
f
?
?x
(3)
N(x) is the set of all function nodes which are x?s neighbours. Intuitively, the message reflects evi-
dence about the distribution of x from all functions which have x as an argument, except the function
corresponding to the receiving node f . A message ?
f(x,...)?x
sent from the factor node f(x, ...) to the
1
The derivation of the HAPS algorithm, quite involved, is unlikely to interest many readers. We only present the bare
minimum of facts about the algorithm, the framework of factor graphs and the derivation of HAPS from the underlying model
of Affinity Propagation. A detailed account appears in (Kazantseva, 2014).
Table 1: Average breadth of manually created topical trees and windowDiff value across different levels
Level Average breadth windowDiff
4 (top) 6.53 0.35
3 17.55 0.46
2 17.63 0.47
1 (bottom) 8.80 0.50
40
Cl?1
1
C
l?1
i
C
l?1
N
e
l?1
1
e
l?1
i
e
l?1
N
E
l?1
1
E
i
l ? 1
E
l?1
N
I
l?1
1
I
l?1
i
I
l?1
N
c
l?1
11
c
l?1
1i
c
l?1
1N
c
l?1
i1
c
l?1
ii
c
l?1
iN
c
l?1
N1
c
l?1
Ni
c
l?1
NN
S
11
l?1
S
l?1
1i
S
1N
l?1
S
l?1
i1
S
l?1
ii
S
l?1
iN
S
l?1
N1
S
l?1
Ni
S
l?1
NN
Level l - 1
C
l
1
C
l
j
C
l
N
e
l
1
e
l
j
e
l
N
E
l
1
E
l
j
E
l
N
I
l
1
I
l
i
I
l
N
c
l
11
c
l
1j
c
l
1N
c
l
i1
c
l
ij
c
l
iN
c
l
N1
c
l
Nj
c
l
NN
S
11
l
S
l
1j
S
1N
l
S
l
i1
S
l
ij
S
l
iN
S
l
N1
S
l
Nj
S
l
NN
Level l
(a) Fragment of the factor graph for levels l ? 1 and l
(b) Types of messages
sent in the HAPS model
Figure 1: Factor graph for HAPS ? Hierarchical Affinity Propagation for Segmentation
variable node x is computed as a maximum of the value of f(x) plus all messages incoming to f(x, ...)
other than the message from the recipient node x:
?
f?x
= max
N(f)\x
(f(x
1
, . . . , x
m
) +
?
x
?
?N(f)\x
?
x
?
?f
) (4)
N(f) is the set of all variable nodes which are f ?s neighbours. The message reflects the evidence about
the distribution of x from function f and its neighbours other than x.
4.2 Hierarchical Affinity Propagation for Segmentation
This work aims to build trees of topical segments. Each segment is characterized by a centre which best
describes its content. The objective function is net similarity, the sum of similarities between all centres
and the data points which they exemplify. The complete sequence of data points is to be segmented at
each level of the tree, subject to the following constraint: centres at each level l, l > 1, must be a subset
of the centres from the previous level l ? 1. Figure 1a shows a fragment of the factor graph describing
HAPS corresponding to levels l and l?1. The tree has L levels, from the root (l = L) down to the leaves
(l = 1). The superscripts of factor and variable nodes denote the level.
At each level, there areN
2
variable nodes c
l
ij
andN variable nodes e
l
j
(N is the number of data points
in the sequence to segment). A variable?s value is 0 or 1: c
l
ij
= 1? the data point i at level l belongs to
the segment centred around data point j; e
l
j
= 1? there is a segment centred around j at level l.
Four types of factor nodes in Figure 1a are I , E, C and S. The I factors ensure that each data point
is assigned to exactly one segment and that segment centres at level l are a subset of those from level
l ? 1. The E nodes ensure that segments are centred around the segment centres in solid blocks (rather
than unordered clusters). The values of I and E are 0 for valid configurations and -? otherwise. The S
factors capture similarities between data points. S
l
ij
= sim(i, j) if c
l
ij
= 1; S
l
ij
= 0 if c
l
ij
= 0.
2
The C
factors handle preferences in an analogous manner. Running the Max-Sum algorithm on the factor graph
in Figure 1a maximizes the net similarity between all segment centres and their children at all levels:
max
{c
l
ij
},{e
l
j
}
S({c
l
ij
}, {e
l
j
}) =
?
i,j,l
S
l
i,j
(c
l
ij
) +
?
i,l
I
l
i
(c
l
i1
, . . . , c
l
iN
, e
l?1
i
) +
?
j,l
E
l
j
(c
l
1j
, . . . , c
l
Nj
, e
l
j
) +
?
j,l
C
l
j
(e
l
j
) (5)
2
The value sim(i, j) is specified in the input matrix.
41
Figure 1b shows a close-up view of the messages that must be sent to find the optimizing configuration
of variables. Messages ?, ?, ?? do not need to be sent explicitly: their values are subsumed by other types
of messages. We only need to compute explicitly and send four types of messages: ?, ?, ? and ? .
Algorithm 1 shows the pseudo-code for the HAPS algorithm.
3
Intuitively, different parts of the update
messages in Algorithm 1 correspond to likelihood ratios between two hypotheses: whether a data point i
is or is not part of a segment centred around another data point j at a given level l. For example, here is
the availability (?) message sent from a potential segment centre j to itself at level l:
?
l
ij
= p
l
j
+ ?
l
j
+
j
max
s=1
(
j?1
?
k=s
?
l
kj
) +
N
max
e=j
(
e
?
k=j+1
?
l
kj
) (6)
Here p
l
j
incorporates the information about the preference value for the data point j at the level l. ?
l
j
brings in the information from the coarser level of the tree. The summand max
j
s=1
(
?
j?1
k=s
?
l
kj
) encodes
the likelihood that there is a segment starting before j given the values of responsibility messages for all
data points i such that i < j ? hence the information from a more detailed level of the tree as well as
the similarities between all data points i (i < j) and j. The summand max
N
e=j
(
?
e
k=j+1
?
l
kj
) does the
same for the tail-end of the segment (all data points i such that i > j).
Complexity analysis. The HAPS model contains N
2
c
l
ij
nodes at each level. In practice, however, the
matrix of similarities SIM does not need to be fully specified. It is customary to compute this matrix
with a large sliding window; the size should be at least twice the anticipated average length. On each
iteration, we need to send L*M*N messages ? and ?, resulting in the complexity O(L*M*N). Here L is
the number of levels, N is the number of data points in the sequence and M (M ? N ) is the size of the
sliding window used for computing similarities. The computation of ? and ? messages is independent
for each row and column respectively, so the algorithm would be easy to parallelize.
Parameter settings. An important advantage of HAPS is that it does not require the number of
segments in advance. Instead, the user needs to set the preference values for each level. However, HAPS
is fairly resistant to changes in preferences and this generic parameter is a convenient knob for fine-tuning
the desired granularity of segmentation, as opposed to specifying the exact number of segments at each
level of the tree. In this work we set preferences uniformly, but it is possible to incorporate additional
knowledge through more discriminative settings.
In all our experiments, preference values are set uniformly for each level of the tree, so effectively
all data points are equally likely to be chosen as segment centres at each level. As a starting point,
the preference value for the most detailed level of the tree should be about approximately equal to the
median similarity value (as specified in the input matrix). A near-zero preference value tends to result in
a medium number of segments and is thus suitable to the middle levels of the tree. A negative preference
value results in a small number of segments and is appropriate for identifying the most pronounced
segment breaks.
5 Experimental evaluation
In order to evaluate the quality of topical trees produced by HAPS, we ran the system on two datasets.
We compared the results obtained by HAPS against topical trees obtained by iteratively running two
high-performance single-level segmenters.
Datasets. We used the Moonstone corpus described in Section 2, and the Wikipedia dataset com-
piled by Carroll (2010). Created automatically from metadata on Web pages, the dataset consists of 66
Wikipedia entries on various topics; the annotations and the results concern sentences. In the Moonstone
corpus we work with paragraphs. To simplify evaluation and interpretation, we produced three-tier trees.
This is in line with the average depths of manual annotations in the Moonstone data.
3
It is not possible to include a detailed derivation of the new update messages in the space allowed here. The interested reader
can find these details in (Kazantseva, 2014). The derivation follows the same logic as (Givoni et al., 2011) and (Kazantseva and
Szpakowicz, 2011).
42
Algorithm 1 Hierarchical Affinity Propagation for Segmentation
1: input: 1) L pairwise similarity matrices {SIM
l
(i, j)}
(i,j)?{1,...,N}
2 ; 2) L preferences p
l
(one per
level l) indicating a priori likelihood of point i being a segment centre at level l
2: initialization: ?i, j : ?
ij
= 0 (set all availabilities to 0)
3: repeat
4: iteratively update ?, ?, ? and ? messages
5:
?i, l : ?
l?1
i
= max[0, ?
ii
?max
k 6=i
(s
l
ik
+ ?
l
ik
)]
6:
?i, j, l : ?
l
ij
=
?
?
?
?
?
min(0, ?
l
i
)?max
k 6=i
(s
l
ik
+ ?
l
ik
) if i = j
s
l
ij
+ min[max(0,??
l
i
)? ?
l
ii
,?max
k*i,j
(s
l
ik
+ ?
l
ik
)] if i 6= j
7:
?i, j, l : ?
l
ij
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
p
l
j
+ ?
l
j
+
j
max
s=1
(
j?1
?
k=s
?
l
kj
) +
N
max
e=j
(
e
?
k=j+1
?
l
kj
) if i = j
?
l
ij,i<j
= min[(
i
max
s=1
i?1
?
k=s
?
l
kj
+
j
?
k=i+1
?
l
kj
+
N
max
e=j
e
?
k=j+1
?
l
kj
) + p
l
j
+ ?
l
j
,
i
max
s=1
i?1
?
k=s
?
l
kj
+
j
min
s=i+1
s?1
?
k=i+1
?
l
kj
] if i < j
min[(
j
max
s=1
j?1
?
k=s
?
l
kj
+
i?1
?
k=j
?
l
kj
+
N
max
e=i
e
?
k=i+1
?
l
kj
) + p
l
j
+ ?
l
j
,
i?1
min
e=j
i?1
?
k=e+1
?
l
kj
+
N
max
e=i
e
?
k=i+1
?
l
kj
]
8:
?j, l : ?
l+1
j
= p
l
(j) + ?
l
jj
+
j
max
s=1
(
j?1
?
k=s
?
l
kj
) +
N
max
e=j
(
e
?
k=j+1
?
l
kj
)
9: until convergence
10: compute optimal configuration: ?i, j i is in the segment centred around j iff ?
ij
+ ?
ij
> 0
11: output: segment centres and segment boundaries
Baselines. Regrettably, we are not aware of another publicly available hierarchical segmenter. That is
why we used as baselines two recent flat segmenters: MCSeg (Malioutov and Barzilay, 2006) and BSeg
(Eisenstein and Barzilay, 2008). Both were first run to produce top-level segmentations. Each segment
thus computed was a new input document for segmentation. We repeated the procedure twice to obtain
three-tiered trees. MCSeg cannot be run without knowing the number of segments in advance. Therefore,
on each iteration, we had to specify the correct number of segments in the reference segmentation. BSeg
does not need the exact number of segments, so we had two settings: with and without knowing the
number of segments.
Evaluation metrics. We did our best to obtain a realistic picture of the results, but each metric has
its shortcomings. We compared topical trees using windowDiff and evalHDS (Carroll, 2010). Both
metrics are penalties: the higher the values, the worse the hypothetical segmentation. evalHDS computes
windowDiff for each level of the tree in isolation and weighs the errors according to their prominence in
43
the tree. We computed evalHDS using the publicly available Python implementation (Carroll, 2010).
4
When computing windowDiff, we treated each level of the tree as a separate segmentation and com-
pared each hypothetical level against a corresponding level in the reference segmentation.
To ensure that evaluations are well-defined at all levels, we propagated the more pronounced reference
breaks to lower levels (in both annotations and in the results). In effect, the whole sequence is segmented
at each level ? otherwise windowDiff would not be not well-defined. Conceptually this means that if
there is a topical shift of noticeable magnitude (e.g., at the top level), there must be at least a shift of less
pronounced magnitude (e.g., at an intermediate level).
The Moonstone dataset has on average 4.8 annotations per chapter. It is not obvious how to combine
these multiple annotations. We evaluated separately each hypothetical segmentation against each avail-
able gold standard. We report the averages across all annotators ? for both evalHDS and windowDiff ?
per level.
Preprocessing. The representations used by HAPS and the MCSeg are very similar. Both systems
compute a matrix of similarities between atomic units of the document (sentences or paragraphs). Each
unit was represented as a bag of words. The vectors were further weighted by the tf.idf value of the term
and also smoothed in the same manner as in (Malioutov and Barzilay, 2006). We computed cosine simi-
larity between vectors corresponding to each sentence or paragraph. We used tenfold cross-validation on
the Wikipedia dataset and fourfold cross-validation on the smaller Moonstone data.
The quality of the segment centres. In addition to finding topical shifts, HAPS identifies segment
centres ? sentences or paragraphs which best capture what each segment is about. In order to get a rough
estimate of the quality of the centres, we extracted paragraphs identified as segment centres at the second
(middle) level of HAPS trees. These pseudo-summaries were then compared to summaries created by
an automatic summarizer CohSum. We used ROUGE-1 and ROUGE-L metrics (Lin, 2004) as a basis
for comparison. CohSum identifies the most salient sentences in a document by running a variant of the
TextRank algorithm (Mihalcea and Tarau, 2004) on the entire document. In addition to using lexical
similarity, the summarizer takes into account coreference links between sentences. We ran CohSum at
10% compression rate.
The summarization experiment was performed on the Moonstone corpus. We also collected 20 chap-
ters from several other XIX century novels and used it in a separate experiment. The ROUGE package
requires manually written summaries to compare with the automatically created ones. We obtained the
summaries from the SparkNotes website.
5
6 Results and discussion
Table 2 shows the results of comparing HAPS with two baseline segmenters using windowDiff and
evalHDS. HAPS was run without knowing the number of segments. MCSeg required that the exact
number be specified. BSeg was tested with and without that parameter. Therefore, rows 3 and 4 in
Table 2 correspond to baselines considerably more informed than HAPS. This is especially true of the
bottom levels where sometimes knowing the exact number of segments unambiguously determines the
only possible segmentation.
The results suggest that HAPS performs well on the Moonstone data even when compared to more
informed baselines. This applies to both metrics, windowDiff and evalHDS. BSeg performs slightly
better at the bottom levels of the tree when it has the information about the exact number of segments.
We hypothesize that the advantage may be due to this additional information, especially when segmenting
already small segments at level 1 into a predefined number of segments. Another explanation may be
that when using windowDiff as the evaluation metric, HAPS was fine-tuned so as to maximize the value
of windowDiff at the top level, effectively disregarding lower levels of segmentation.
4
When working with the Moonstone dataset, we realized that the software produces very low values, almost too good to be
true. That is because the bottommost annotations are very fine-grained. Sometimes each paragraph corresponds to a separate
segment. This causes problems for the software. So, when we report evalHDS values for the Moonstone dataset, we only
consider two top levels of the tree, disregarding the leaves. We also remove the ?too good to be true? outliers, though the ?bad?
tail is left intact. We applied the same procedure to all three segmenters, only for the Moonstone dataset.
5
http://www.sparknotes.com/
44
Level Moonstone Wikipedia Moonstone Wikipedia
windowDiff windowDiff evalHDS evalHDS
HAPS
3 (top) 0.337 (? 0.060) 0.421 (? 0.060) 0.353 0.450
2 (middle) 0.422 (? 0.060) 0.447 (? 0.070) (? 0.072) (? 0.015)
1 (bottom) 0.556 (? 0.070) 0.617 (? 0.080)
MinCutSeg-iter.
3 (top) 0.375 0.440 (? 0.075) 0.377 0.444
2 (middle) 0.541 0.424 (? 0.064) (? 0.002) (? 0.002)
segm. known 1 (bottom) 0.601 0.471 (? 0.057)
BayesSeg-iter.
3 (top) 0.353 (? 0.071) 0.391 (? 0.070) 0.367 0.370
2 (middle) 0.406 (? 0.053) 0.344 (? 0.033) (? 0.089) (? 0.019)
segm. known 1 (bottom) 0.504 (? 0.064) 0.354 (? 0.033)
BayesSeg-iter.
3 (top) 0.600 (? 0.071) 0.637 (? 0.070) 0.453 0.437
2 (middle) 0.447 (? 0.053) 0.877 (? 0.033) (? 0.089) (? 0.022)
segm. unknown 1 (bottom) 0.545 (? 0.064) 0.952 (? 0.033)
Table 2: Evaluation of HAPS and iterative versions of APS, MCSeg and BSeg using windowDiff per level
(mean windowDiff and standard deviation for cross-validation)
Moonstone corpus Austen corpus
ROUGE-1 ROUGE-L ROUGE-1 ROUGE-L
Segment centres 0.341 0.321 0.291 0.301
(0.312, 0.370) (0.298, 0.346) (0.272, 0.311) (0.293, 0.330)
CohSum 0.294 0.269 0.305 0.307
summaries (0.243, 0.334) (0.226, 0.306) (0.290, 0.320) (0.287, 0.327)
Table 3: HAPS segment centres compared to CohSum summaries: ROUGE scores and 95% confidence
intervals
All segmenters perform worse on the Wikipedia dataset. Using that scale, informed BSeg performs the
best, but it is interesting to note a significant drop in performance when the number of segments is not
specified.
Overall, HAPS appears to perform better than, or comparably to, the more informed baselines, and
much better than the baseline not given information about the number of segments.
We also made a preliminary attempt to evaluate the quality of segment centres by comparing them to
the summaries created by the CohSum summarizer. In addition to working with the Moonstone corpus,
we collected a corpus of 20 chapters from various novels by Jane Austen.
Table 3 shows the results. They are not conclusive because there is no evidence that ROUGE scores
correlate with the quality of automatically created summaries for literature. According to the scores in
Table 3, however, the summaries created by CohSum cannot be distinguished from simple summaries
composed of segment centres identified by HAPS. We interpret this as a sign that the centres identified
by HAPS are approximately as informative as those created by an automatic summarizer.
7 A brief conclusion
This paper presented HAPS, a hierarchical segmenter for free text. Given an input document, HAPS
creates a topical tree and identifies a segment centre for each segment. One of the advantages of HAPS
is that it does not require the exact number of segments in advance. Instead, it estimates the number
of segments given information on generic preferences with regard to segmentation granularity. We also
created a corpus of hierarchical segmentations which has been annotated by 3-6 people per chapter.
A Java implementation of HAPS and the Moonstone corpus are publicly available.
6
Acknowledgements
We thank Chris Fournier (for computing S values using a beta version of SegEval software for hierar-
chical datasets), Lucien Carrol (for help and discussion of the evalHDS software and representation) and
Christian Smith (for allowing us to use his implementation of CohSum).
6
http://www.eecs.uottawa.ca/
?
ankazant/
45
References
Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.
David Blei and Pedro Moreno. 2001. Topic segmentation with an aspect hidden Markov Model. In Proceedings of
the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 343?348.
Lucien Carroll. 2010. Evaluating Hierarchical Discourse Segmentation. In Human Language Technologies: The
2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 993?1001.
Lan Du, Wray Buntine, and Mark Johnson. 2013. Topic Segmentation with a Structured Topic Model. In
Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 190?200, Atlanta, Georgia.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian Unsupervised Topic Segmentation. In Proceedings of the
2008 Conference on Empirical Methods in Natural Language Processing, pages 334?343, Honolulu, Hawaii.
Jacob Eisenstein. 2009. Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion. In Proceedings of
the 2009 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 353?361. The Association for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level Discourse Parsing with Rich Linguistic Features. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 60?68, Jeju Island, Korea, July. Association for Computational Linguistics.
Chris Fournier and Diana Inkpen. 2012. Segmentation Similarity and Agreement. In Proceedings of the 2012
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 152?161, Montr?eal, Canada.
Inmar E. Givoni, Clement Chung, and Brendan J. Frey. 2011. Hierarchical Affinity Propagation. In Uncertainty
in AI, Proceedings of the Twenty-Seventh Conference (2011), pages 238?246.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring Content Models for Multi-Document Summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 362?370, Boulder, Colorado, June.
Marti A. Hearst. 1997. TextTiling: segmenting text into multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Hugo Hernault, Helmut Prendinger, David A. duVerlea, and Mitsuru Ishizuka. 2010. HILDA: A Discourse Parser
Using Support Vector Machine Classification. Dialogue and Discourse, 3:1?33.
Anna Kazantseva and Stan Szpakowicz. 2011. Linear Text Segmentation Using Affinity Propagation. In Proceed-
ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293, Edinburgh,
Scotland.
Anna Kazantseva and Stan Szpakowicz. 2012. Topical Segmentation: a Study of Human Performance and a New
Measure of Quality. In Proceedings of the 2012 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 211?220, Montr?eal, Canada.
Anna Kazantseva. 2014. Topical Structure in Long Informal Documents. Ph.D. thesis, University of Ottawa.
?http://www.eecs.uottawa.ca/
?
ankazant/?.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of summaries. In Text Summarization
Branches Out, Proceedings of the ACL Workshop, pages 74?81, Barcelona, Spain.
Igor Malioutov and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, Sydney, Australia.
William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a functional theory of
text organization. Text, 8(3):243?281.
Meghana Marathe. 2010. Lexical Chains Using Distributional Measures of Concept Distance. Master?s thesis,
University of Toronto.
46
Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge,
Mass.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Dekang Lin and Dekai Wu, editors,
Proceedings of the Conference on Empirical Methods in Natural Language Processing 2004, pages 404?411,
Barcelona, Spain.
Hemant Misra, Franc?ois Yvon, Olivier Capp?e, and Joemon M. Jose. 2011. Text segmentation: A topic modeling
perspective. Information Processing and Management, 47(4):528?544.
Hyo-Jung Oh, Sung Hyon Myaeng, and Myung-Gil Jang. 2007. Semantic passage segmentation based on sentence
topics for question answering. Information Sciences, an International Journal, 177:3696?3717.
Lev Pevzner and Marti A. Hearst. 2002. A Critique and Improvement of an Evaluation Metric for Text Segmenta-
tion. Computational Linguistics, 28(1):19?36.
Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. In SIGIR
?98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 275?281, Melbourne, Australia.
Christian Smith, Henrik Danielsson, and Arne Jnsson. 2012. A more cohesive summarizer. In 24th International
Conference on Computational Linguistics, Proceedings of COLING 2012: Posters, pages 1161?1170, Mumbai,
India.
Fei Song, William M. Darling, Adnan Duric, and Fred W. Kroon. 2011. An iterative approach to text segmentation.
In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR?11, pages 629?
640, Berlin, Heidelberg. Springer-Verlag.
Florian Wolf and Edward Gibson. 2006. Coherence in Natural Language: Data Structures and Applications. MIT
Press, Cambridge, MA.
Yaakov Yaari. 1997. Segmentation of Expository Texts by Hierarchical Agglomerative Clustering. In Proceedings
of International Conference on Recent Advances in Natural Language Processing RANLP97, pages 59?65,
Tzigov Chark, Bulgaria.
Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language,
67(4):763?789.
47
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 476?485, Dublin, Ireland, August 23-29 2014.
Measuring Lexical Cohesion: Beyond Word Repetition
Anna Kazantseva & Stan Szpakowicz
School of Electrical Engineering and Computer Science
University of Ottawa
Ottawa, Ontario, Canada
{ankazant,szpak}@eecs.uottawa.ca
Abstract
This paper considers the problem of finding topical shifts in documents and in particular at what
information can be leveraged to identify them. Recent research on topical segmentation usually
assumes that topical shifts in discourse are signalled by changes in vocabulary. This information,
however, is not always a sufficient indicator of a topical shift, especially for certain genres. This
paper explores an additional source of information. Our hypothesis is that the type of a referring
expression is an indicator of how accessible its antecedent is. The shorter and less informative
the expression (e.g., a personal pronoun versus a lengthy post-modified noun phrase), the more
accessible the antecedent is likely to be and the more likely it is that the topic under discussion has
remained constant between the two mentions. We explore how this information can be used to
augment a lexically-based topical segmenter. We test our hypothesis on two types of data, literary
narratives and lecture notes. The results suggest that our similarity metric is useful: depending on
the settings it either slightly improves the performance or leaves it unchanged. They also suggest
that certain types of referring expressions are more useful than others.
1 Introduction
In the past 10 years, research on topical segmentation has mostly centred on using surface vocabulary to
identify topical shifts. The intuition is that if the vocabulary changes perceptibly, so does the topic under
discussion. One popular way to model this assumption is by probabilistic graphical models. A document
may be modelled as a sequence of strings (e.g., sentences) generated by a latent topic variable, where
the topic variables correspond to distributions over a finite vocabulary. Similarity-based methods are
an alternative methodology. The segmenter explicitly measures the amount of lexical similarity between
sentences. Places where similarity is low are likely to indicate shifts of topic. The common thread among
these approaches is that they rely almost exclusively on the explicitly mentioned words.
The idea that vocabulary shifts indicate topical shifts dates back to Youmans (1991). Indeed, by and
large, introducing new concepts almost necessarily requires that the concepts be named and described.
How densely the concepts are explicitly mentioned and how often the mentions are repeated depends
to a large degree on the genre and on the cognitive complexity of the document. In scientific papers or
legal documents clarity is paramount, so the author will endeavour to state things explicitly and avoid
ambiguity. The less complicated the document, however, the less it is necessary to explicitly repeat
terminology. In literature, for example, word repetition is not only uncommon, but it is usually a sign
of bad writing. In casual conversations, the topic can easily be never mentioned explicitly. How can we
identify topical shifts in a document whose author does not ?hold the reader?s hand??
It turns out that lexical cohesion (or, put simply, word repetition) is only one of several devices of
cohesion (Halliday and Hasan, 1976, p. 29) Other possibilities are reference, substitution, ellipsis and
conjunction. In this paper we mainly explore referential cohesion.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
476
Figure 1: An example dialogue from the Moonstone corpus
?What?s wrong now?? I said once more.
?Rosanna?s late again for dinner,? says Nancy. ?And I?m sent to fetch her in. All the hard work falls on my shoulders in
this house. Let me alone, Mr. Betteredge!?
The person here mentioned as Rosanna was our second housemaid. ?Where is she?? I inquired. [. . . ]
?At the sands, of course!? says Nancy, with a toss of her head. ?She had another of her fainting fits this morning, and she
asked to go out and get a breath of fresh air. I have no patience with her!?
?Go back to your dinner, my girl,? I said. ?I have patience with her, and I?ll fetch her in.?
Figure 1 shows a snippet of a dialogue from the publicly available Moonstone corpus (Kazantseva and
Szpakowicz, 2012). The two speakers discuss a specific person, Rosanna, yet her name is mentioned
explicitly only twice. In the remainder of the dialogue the author uses pronouns to refer to this person,
whose identity is evident from the context. Running an automatic segmenter on such a document would
likely be challenging since focal concepts ? characters ? are often referred to by pronouns or definite
noun phrases (NPs) instead of explicit repetition.
The focal entity Rosanna is introduced once and then it is referred to by nominal and pronominal
anaphora, not by explicit repetition. Simplifying things somewhat, we can say that merely by the virtue
of encountering a referring expression (e.g., she or the person), we know that it refers to something that
must be clear from the context. The type of the referring expression also contains information about the
availability of the antecedent. A she implies that the ?she? in question is rather obvious, that is to say,
the antecedent is nearby and, more important for our purposes, the topical thread continues. A more
verbose referring expression (e.g., the woman in red) is more likely in situations where the antecedent is
less obvious and the reader needs additional information to disambiguate the expression.
The idea that the type of referring expression tells a lot about the accessibility of its antecedent dates
back to Giv?on (1981). He postulated that the more informative the referring expression is, the less acces-
sible the antecedent will be. Figure 2 shows the list of expressions from the least to the most informative.
Projecting this information onto our task, we can say that the more informative the expression is, the less
continuity there will be in the topic.
The main contribution of this work is to show how such information can be used to improve the
quality of text segmentation. We extract NPs and classify them by informativeness. This is achieved
with the help of a syntactic parser, but a lighter form of processing might do, perhaps even if it captured
personal pronouns. Using this information, we augment and correct a matrix of lexical similarities
between sentences, a structure frequently used as an input to a topical segmenter.
The results of using coreferential similarity are evaluated on a dataset of manually segmented chapters
from a novel (Kazantseva and Szpakowicz, 2012) and on transcripts of lectures in Artificial Intelligence
(Malioutov and Barzilay, 2006). We try the new similarity matrix on two publicly available similarity-
based segmenters APS (Kazantseva and Szpakowicz, 2011) and MinCutSeg (Malioutov and Barzilay,
2006). The results suggest that the new matrix never hurts, and in several case improves, the performance
of the segmenter, especially for the novel. We also check whether this metric would still be useful if
instead of the traditionally used lexical similarity we used a similarity metric which took synonymy into
account. In this case, the margin of improvement is lower, but still the coreferential similarity metric
never hurts the performance and often improves it.
Section 2 of the paper gives an overview of related work. Section 3 describes our similarity metric and
how we compute it. Section 4 shows the details of the experiments, while Section 5 discusses the results.
We conclude in Section 6 with a discussion of how our metric can be improved and simplified.
2 Background and related work
Much of research on topical segmentation of text is based on the idea that changes of topic are usually
accompanied by vocabulary changes. Introduced by Youmans (1991), it has since formed the backbone
477
of research on topical segmentation. We now briefly review recent work on text segmentation. Since
the focus of this research is on what information is useful for text segmentation, this review emphasizes
representations rather than algorithms.
Perhaps the simplest way of estimating topical similarity between sentences is to measure cosine sim-
ilarity between corresponding feature vectors. It has been used extensively in text segmentation. Hearst
(1994; 1997) describes TextTiling, an algorithm which identifies topical shifts by sliding a window
through the document and measures cosine similarity between adjacent windows. The drops in simi-
larity signal shifts of topic. More recently, Malioutov and Barzilay (2006) as well as Kazantseva and
Szpakowicz (2011) use graph cuts and factor graph clustering for text segmentation. Both systems rely
on cosine similarity between bag-of-word vectors as an underlying representation.
While cosine similarity between vectors is easy to compute, it is hardly a reliable metric of topical
similarity. Several researchers have used lexical chains ? first introduced by Halliday and Hasan (1976)
? to improve the performance of topical segmenters.
1
The intuition behind using lexical chains for text
segmentation is that the beginning and end of a chain tend to correspond to the beginning and end of
a topically cohesive segment. One version of TextTiling (Hearst, 1997) uses lexical chains manually
constructed using Roget?s Thesaurus. Okumura and Honda (1994) apply automatically created lexical
chains to segment a small set of documents in Japanese. More recently, Marathe (2010) tried to build
lexical chains using distributional semantics and apply the method to text segmentation.
Other proposals to move beyond word repetition in topical segmentation include the use of bigram
overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer
and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010).
It should be noted that much of the recent work on topical segmentation revolves around generative
models. For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misra
et al. (2011) and Du et al. (2013) use higher-order models. We do not review this work in detail here
because it centers on algorithms for text segmentation and not on the information supplied to those algo-
rithms, which is the focus of this research. Fundamentally, the text is modelled as a sequence of tokens
generated by latent topic variables. Although probabilistic segmenters can be extended to use addi-
tional information (e.g., Eisenstein and Barzilay (2008) augment their segmenter with information about
discourse markers), it is not trivial to change these models to include information such as synonymy,
co-reference and so on. That is why we do not review them in detail here.
As this brief review shows, a number of approaches have been proposed to measure cohesion between
sentences, that is to say, to describe to what extent a pair of sentences is ?about the same thing?. Most
of them have a common denominator: they use explicit lexical information, sometimes augmented by
semantic relations from thesauri or ontologies.
Lexical resources, such as ontologies and knowledge-bases, may help improve the quality of segmen-
tations, but such resources are not always available. They also may cause problems with precision. More
important, however, they do not solve a more fundamental problem: a text may be highly cohesive and
coherent without being tightly bound by either lexical cohesion or synonymy.
The main ideas developed in this work originate in (Giv?on, 1981). The author looks at the functional
domain of topical accessibility. A number of coding devices affect this property. They are listed in
Figure 2, ordered from the devices used to mark the most continuous topics to those which mark the least
continuous topics. The order in Figure 2 is governed by a simple principle: the more accessible the topic
is, the less information is used to code it. The author argues that the continuum is applicable in many
languages. He also mentions that while the exact values of the phenomenon in question are difficult to
predict or even estimate, their relative order can be predicted with certainty, even if some devices are
unavailable in some languages.
In a similar spirit, Ariel (2014) groups non-initial NPs into expressions with low accessibility (definite
NPs and proper names), those with intermediate accessibility (personal and demonstrative pronouns) and
those with high accessibility (pronouns).
In this work, we propose to leverage the presence and type of co-referential relations to improve
1
Very simply put, a lexical chain is a sequence of related words in a text.
478
the results of two recent similarity-based segmenters. Instead of resolving anaphoric references, we
assume that their mere presence often indicates topic continuity. With this augmented model, we segment
fiction and spoken lecture transcripts, the two types of data where low rates of lexical cohesion preclude
achieving segmentation of good quality using only surface information about token types.
3 Estimating coreferential similarity
In order to see whether knowledge about types of referential expressions is useful for measuring topi-
cal similarity, we incorporate this information into two publicly available similarity-based topical seg-
menters, MCSeg (Malioutov and Barzilay, 2006) and APS (Kazantseva and Szpakowicz, 2011). Nor-
mally, both MCSeg and APS measure similarity between sentences by computing cosine similarity be-
tween the vectors corresponding to bag-of-words representation for each sentence:
sim(s
1
, s
2
) =
s
1
? s
2
||s
1
|| ? ||s
2
||
(1)
Each atomic unit of text is represented as a vector of features corresponding the occurrences of each
token type. The vectors are weighted using tf.idf values for each token type. Next, a segmenter measures
cosine similarity between vectors according to Equation 1. That is the fundamental representation in both
MCSeg and APS. MCSeg identifies segment boundaries by creating a weighted cyclic graph and cutting
it so as to maximize the sum of edges within segments and to minimize the sum of severed edges. APS
segments the sequence by finding segment centres ? points which best capture the content of a segment
? and assigning data points to best segment centres so as to maximize net similarity.
The proposed similarity metric relies on the following idea: in order to measure how many concepts
two sentences share, we do not need to resolve anaphoric expressions in full, but only to map them onto
sentences which contain their most recent antecedent (without actually naming the antecedents). We do
that by parsing the documents with the Connexor parser (Tapanainen and J?arvinen, 1997) and extracting
all NPs with their constituents. Next, we attempt to classify the NPs into categories which would roughly
correspond to those listed in Figure 2 and to those in (Ariel, 2014).
A manual study by Brown (1983) suggests that the average referential distance for animate and inan-
imate entities differs widely within the same document.
2
That is why it makes sense to distinguish
between these two types. In the end, then, we classify each identified NP into one of the categories listed
in Figure 3. The list is not exhaustive and in some cases an NP may belong to more than one type. In
practice, however, an NP is always assigned a single type dictated by the implementation.
2
Brown (1983, pp. 323-324) compares referring expressions which denote human and non-human entities. She uses three
measurements: average distance to the nearest antecedent, average ambiguity and persistence. On all three counts, human and
non-human entities appear to have different distributions.
Figure 2: Linguistic coding devices which signal topic accessibility (Giv?on, 1981)
Most continuous (least surprising)
1. zero anaphora
2. unstressed pronouns (e.g., He was speaking loudly.)
3. right-dislocated definite noun phrases (NPs) (e.g., It is no good, that book.)
4. neutral-ordered definite NPs (e.g., That book is no good.)
5. left-dislocated definite NPs (e.g., That book, it is no good.)
6. Y-moved NP?s (e.g., The book they read in turns.)
7. cleft/focus constructions (e.g., It was that book, that was on her mind for weeks.)
8. referential indefinite NPs (e.g., He picked up a book and left.)
Least continuous (most surprising)
479
Figure 3: Categories of noun phrases taken into account when computing coreferential similarity
1. animate personal pronouns (he, she, they)
2. inanimate pronouns (it)
3. demonstrative pronouns (that, those)
4. animate proper names (John Hernecastle)
5. inanimate proper names (London)
6. animate definite noun phrases (the man)
7. inanimate definite noun phrases (the jewel)
8. animate indefinite noun phrases (a man)
9. inanimate indefinite noun phrases (a jewel)
Finally, coreferential similarity between sentences S
i
and S
j
is measured as follows:
coref sim(S
i
, S
j
) = (
?
|T |
t=0
count
S
j
t
? weight
t
|S
1
| ? |S
2
|
)
(j?i?1)?decayFactor
(2)
T is the set of of all types of referring expressions which we consider ? those given in Figure 3.
count
S
j
t
is the number of times when an expression of type t appears in the most recent sentence, S
j
.
Note that we only consider the referring expressions in the most recent sentence, because a referring
expression, by its nature, must refer to something previously mentioned. The ?tightness? of the link is
controlled by setting weight
t
for each expression type t. weight
t
effectively specifies how likely it is
that the antecedent for an expression of a type t appears in sentence s
i
. The values of the weights are
set experimentally on the holdout data. They can almost certainly be further fine-tuned. Intuitively, the
settings of the weights reflect the logic behind Giv?on?s theory. Consider an example vector of weights
for expressions, where a higher weight corresponds to a more accessible antecedent (for animate and
inanimate entities respectively).
<personal pronouns anim: 4, demonstr pronouns anim: 2, proper names anim: 1,
def np anim: 0.5, indef np anim: 0, pronouns inanim: 2, demonstr pronouns inanim: 2,
proper names inanim: 0, def np inanim: 0, indef np inanim: 0>
The denominator of Equation 2 normalizes the value by the product of the lengths of sentences S
1
and
S
2
. The exponent (j ? i ? 1) ? decayFactor is responsible for decreasing similarity as the distance
between sentence S
i
and S
j
increases. The decay factor, 0 < decayFactor < 1, is set experimentally,
and j ? i is the distance between sentences S
i
and S
j
, i < j.
Figure 4 contains a walk-through example of computing referential similarity between two sentences.
The coreferential similarity as defined by Equation 2 is rather limited. The first limitation is the range:
it can only measure similarity between nearby sentences or paragraphs, because it only makes sense
between the closest occurrences of an antecedent and a subsequent referring expression. For example,
it does not make sense to measure coreferential similarity between sentences that are several paragraphs
apart. Even if they indeed talk about the same entities, the topic has most likely been re-introduced
several times in between. That is why we only compute coreferential similarity for sentences no more
than decayWindow sentences apart. The value of decayWindow is usually between 2 and 6 and it is
set experimentally on the holdout set for each corpus.
The values of coref
s
im are usually quite small and the information used is rather one-sided. We
use it, therefore, in addition to, not instead of, lexical similarity. In our experiments, we first compute
lexical similarity between sentences (or paragraphs) and then modify the lexical matrix by adding to it
the matrix of coreferential similarity.
480
Figure 4: An example of computing coreferential similarity
coref sim(S
i
, S
j
) = (
?
|T |
t=0
count
S
j
t
? weight
t
|S
1
| ? |S
2
|
)
(j?i?1)?decayFactor
S1: ?At the sands, of course!? says Nancy, with a toss of her head.
S2: ?She had another of her fainting fits this morning, and she asked to go out and get a breath of fresh air.?
Expression counts:
personal pronouns anim: 2 (she, she)
demonstr pronouns anim: 0
proper names anim: 1
def np anim: 0
indef np anim: 0
pronouns inanim: 0
demonstr pronouns inanim: 1
proper names inanim: 0
def np inanim: 2 (this morning, fainting fits)
indef np inanim: 1 (a breath)
Weights:
4
2
1
0.5
0
2
2
0
0
0
coref sim(S
2
, S
1
) =
2? 4 + 1? 1 + 1? 2
21? 22
(2?1?1)?0.5
= 0.0234
4 Experimental results
The effectiveness of coreferential similarity metric has been tested in practice. A set of experiments
compared how much the metric improves the quality of topical segmentations. To this end, we ran
APS and MCSeg with and without adding coreferential similarity to lexical similarity, and compared
the results. We chose these segmenters for comparison because coreferential similarity can only be
naturally incorporated into a similarity-based segmenter.
Data. In our experiments we used two publicly available datasets. The first one is a set of lectures on
Artificial Intelligence (Malioutov and Barzilay, 2006). The dataset contains 22 documents which were
manually annotated for the presence of topical shifts. The second dataset is the Moonstone dataset de-
scribed in (Kazantseva and Szpakowicz, 2012). It contains 20 chapters from Wilkie Collins?s novel, each
annotated by 4-6 people. To reconcile these multiple reference annotations, we create a majority gold
standard. It only contains segment breaks which were marked by at least 30% of the annotators. Both
segmenters are compared against this gold standard. There is a fair amount of disagreement between the
annotators of this dataset. The average inter-annotator windowDiff is 0.38 (Kazantseva and Szpakow-
icz, 2012, pp. 215-216), but if one takes into account near-hits, then at least 50% of the boundaries are
marked by more than two annotators.
Both datasets are quite challenging. The lecture dataset contain a lot of rather informal speech and
there is not as much lexical repetition as would be in a more formal text. The Moonstone dataset is an
example of literary language, full of small digressions, dialogue and so on.
The first dataset is annotated at the level of individual sentences. The second dataset is annotated at the
level of paragraphs. We segment both datasets at the level of the gold-standard annotations (sentences
for lectures, paragraphs for the novel).
When working with paragraphs, coref sim is computed slightly differently:
coref sim(p
i
, p
j
) = (
?
|T |
t=0
count
p
j
t
? weight
t
|p
1
| ? |p
2
|
)
(j?i?1)?decayFactor
(3)
In this case, count
p
j
t
refers to the number of occurrences of expression of type t in the first
paragraphCutOff sentences of the paragraph p
j
, instead of the whole paragraph. The rationale behind
this heuristic is that the referring expressions in the opening sentences of the paragraph are likely to refer
481
to entities from the previous paragraph, while expressions in the middle or the end of the paragraph are
likely to refer to entities introduced inside the paragraph.
Segmenters and baselines. We use two publicly available topical segmenters in our experiments:
MCSeg and APS. The default version of each segmenter computes a similarity matrix between sentence
in the input document. The values in the matrix correspond to cosine similarity (Equation 1) computed
after the removal of stop words and weighting the bag-of-word vectors by tf.idf . The results obtained
using these default matrices are our first baseline.
In our experiments, we modify this matrix by adding to it the matrix of coreferential similarities. The
values of coreferential similarities are rather small and most modifications are localized. That is because
the value of decayWindow is set between 2 and 6 (see Section 3).
In addition to the matrices based on cosine similarity, we wanted to see if using a more intelligent
measure of topical similarity improves the results. We built one more flavour of similarity matrices using
the DKPro Similarity framework (B?ar et al., 2013). The framework contains a model of textual similarity
which has been used by the winning system at the SemEval Textual Similarity 2012 shared evaluation.
We use this model (further STS-2012) as a more competitive baseline for computing topical similarity.
The STS-2012 baseline consists of a log-linear regression model trained on the SemEval 2012 train-
ing data. It combines an assortment of measures of textual similarity to come up with its judgments.
The metrics include n-gram overlap, semantic similarity measures (based on both corpora and lexical
resources) and several measures of stylistic similarity. We chose to use this relatively complicated met-
ric because of its competitive performance at SemEval 2012. The system, however, was not designed
to measure topical similarity per se, especially between many sentences coming from the same source
document. By default, the STS-2012 baseline outputs values between 1 and 5. These were normalized to
be between 0 and 1.
Similarly to the experimental design with cosine similarity matrices, we try running the segmenters
using STS-2012 with and without adding coreferential similarity matrix to it.
On both datasets we set the weights for various types of referential expressions using hold-out sets
of two files. When setting the weights, we were guided by the principle captured in Figure 2: personal
pronouns suggest the tightest link, followed by demonstrative pronouns, proper names, and so on.
It should be noted that because we had to modify the native representation of both segmenters by sup-
plying a matrix computed using non-native code, we could not use the proper training scripts which come
with the segmenters. In effect, the results are likely to be lower than they could have been. Even so, this is
acceptable for our purposes because we are interested in the improvement gained by using coreferential
similarity, not in obtaining the best possible segmentation via the setting of the best parameters.
Processing. We computed the underlying lexical similarity matrices using the same procedure as
described in (Malioutov and Barzilay, 2006; Kazantseva and Szpakowicz, 2011), but using our own
code. In other words, we built a matrix of cosine similarities after removing stop words and weighting
the underlying vectors by tf.idf values.
In order to compute coreferential similarity, all documents were parsed using the Connexor parser
(Tapanainen and J?arvinen, 1997). The parser was chosen because it produces high-quality partial parses
of long sentences often encountered in the Moonstone dataset. We also tagged named entities and labelled
NPs as animate or inanimate using the Stanford Core NLP suite.
3
Metrics. We compare topical segmentations using the windowDiff metric:
winDiff =
1
N ? k
N?k
?
i=1
(|ref ? hyp| 6= 0) (4)
windowDiff slides a window of size k through the input sequence of length N . At every position of the
window, the metric compares the number of boundaries in the reference sequence and in the hypothetical
sequence. The number of erroneous windows is normalized by the total number of windows to obtain
the final value. windowDiff is a penalty metric: lower values correspond to better segmentations.
3
http://nlp.stanford.edu/software/corenlp.shtml
482
AI Lectures Moonstone
APS 0.420 (? 0.014) 0.441 (? 0.075)
APS-coref sim 0.411 (? 0.025) 0.391 (? 0.060)
APS-STS 0.428 (? 0.049) 0.479 (? 0.041)
APS-STS -coref sim 0.429 (? 0.020) 0.478 (? 0.035)
MCSeg 0.431 (? 0.045) 0.470 (? 0.095)
MCSeg-coref
s
im 0.410 (? 0.060) 0.413 (? 0.030)
MCSeg-STS 0.451 (? 0.023) 0.441 (? 0.051)
MCSeg-STS-coref sim 0.433 (? 0.070) 0.430 (? 0.025)
Table 1: Results of comparing APS and MCSeg using four different matrix types (windowDiff values and
standard deviation)
5 Evaluation
Table 1 presents the results of running APS and MCSeg using four different input matrices each. The
first column shows the combination of the name of the segmenter and the specific input matrix. APS and
MCSeg refer to the cases where both segmenters were run using simple cosine similarity matrices. STS
refers to matrices computed using STS-2012 from the DKPro Similarity framework. coref sim refers to
cosine similarity matrices modified by adding a matrix with coreferential similarities. STS?coref sim
are matrices computed using STS-2012 which had coreferential similarity added to them.
In all experiments, we set the weights for different types of referring expressions on two hold-out files.
The remainder of the data is divided into five folds. Standard deviation reported in the tables is computed
across folds.
Coreferential similarity improves the results of the cosine matrix for both segmenters, but the improve-
ment on the AI dataset is rather small (1% for APS and 2% for MCSeg).
It is interesting to see that in most cases using STS matrices slightly hurts the performance of the
segmenters compared to using simple cosine similarity matrices. The only exception is running MCSeg
on the Moonstone dataset which improves the performance by 3%.
Adding a matrix of coreferential similarities to STS matrices slightly improves the performance on
the Moonstone dataset and leaves it practically unchanged on the dataset of AI lectures.
It is somewhat surprising that using STS-2012 for similarity computation does not improve, and occa-
sionally worsens, the results compared to using simple cosine similarity. Coreferential similarity, on the
other hand, produces a small but consistent improvement.
We have examined the vectors of weights used in these experiments (set using hold-out data). On
the Moonstone dataset, the results are the best when personal animate pronouns get the highest weight,
followed by demonstrative animate pronouns, as well as inanimate pronouns, both regular and demon-
strative. Other expression types are assigned either a very small weight or the value 0, effectively making
them inconsequential. We hypothesize that this is due to the fact that the novel discusses people, their
relations and interactions, making animate entities central for estimating topical links.
The vectors used on the AI lecture dataset are similar, except that here the highest weights are given
to demonstrative and regular inanimate pronouns. These are followed by demonstrative and then regular
animate pronouns. This distribution is likely due to the fact that the lecture dataset discusses abstract
concepts, while people are likely to be noted more tangentially. We are not sure how to explain the fact
that in this dataset demonstrative pronouns have a slightly higher weight than the regular ones.
Identifying and categorizing noun phrases requires either high-quality NP-tagging or parsing. On the
other hand, most pronouns can be captured very easily, perhaps even using a list of words. It is interesting
to note that the most gain is due to these ?cheap? types of referring expressions. In the future, we plan to
implement a lighter version of the coreferential similarity metric which only considers pronouns.
6 Conclusions and future work
This paper has presented a method for improving the quality of topical segmentations by using infor-
mation about referential expressions in nearby sentences. The method slightly improves the quality of
segmentations and, what is even more important, seems never to worsen the results.
483
The necessity to perform complete parsing of the input document is a drawback of the current ap-
proach. We note in Section 5, however, that the only types of referential expressions which improve
performance are personal and demonstrative pronouns. Those can be easily captured without parsing. In
the near future we plan to investigate such a light-weight version of coref sim metric.
Another way to improve our current implementation would be a more objective method of setting the
weights for different types of referring expressions. At present, the expressions are set by hand on a
small hold-out set of documents. This is far from ideal. We plan to investigate if using logistic regression
or expectation maximization would make the system more robust.
References
Mira Ariel. 2014. Accessing Noun-Phrase Antecedents. Routledge, London and New York.
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013. DKPro Similarity: An Open Source Framework for Text
Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System
Demonstrations, pages 121?126, Sofia, Bulgaria.
David Blei and Pedro Moreno. 2001. Topic segmentation with an aspect hidden Markov Model. In Proceedings of
the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 343?348.
Elizabeth Brown. 1983. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993?1022.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,
pages 109?117, Pittsburgh, Pennsylvania.
Lan Du, Wray Buntine, and Mark Johnson. 2013. Topic Segmentation with a Structured Topic Model. In
Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 190?200, Atlanta, Georgia.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian Unsupervised Topic Segmentation. In Proceedings of the
2008 Conference on Empirical Methods in Natural Language Processing, pages 334?343, Honolulu, Hawaii.
Talmy Giv?on. 1981. Typology and Functional Domains. Studies in Language, 5(2):163?193.
M.A.K Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London and New York.
Marti A. Hearst. 1994. Multi-paragraph Segmentation of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguistics, ACL ?94, pages 9?16, Las Cruces, New Mexico.
Marti A. Hearst. 1997. TextTiling: segmenting text into multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Amanda C. Jobbins and Lindsay J. Evett. 1998. Text Segmentation Using Reiteration and Collocation. In Pro-
ceedings of the 17th International Conference on Computational Linguistics - Volume 1, COLING ?98, pages
614?618, Montr?eal, Qu?ebec.
Anna Kazantseva and Stan Szpakowicz. 2011. Linear Text Segmentation Using Affinity Propagation. In Proceed-
ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293, Edinburgh,
Scotland.
Anna Kazantseva and Stan Szpakowicz. 2012. Topical Segmentation: a Study of Human Performance and a New
Measure of Quality. In Proceedings of the 2012 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 211?220, Montr?eal, Canada.
Thomas K Landauer and Susan T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological Review, pages 211?240.
Igor Malioutov and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, Sydney, Australia.
Meghana Marathe. 2010. Lexical Chains Using Distributional Measures of Concept Distance. Master?s thesis,
University of Toronto.
484
Hemant Misra, Franc?ois Yvon, Olivier Capp?e, and Joemon M. Jose. 2011. Text segmentation: A topic modeling
perspective. Information Processing and Management, 47(4):528?544.
Manabu Okumura and Takeo Honda. 1994. Word Sense Disambiguation and Text Segmentation Based On Lexical
Cohesion. In COLING 1994 Volume 2: The 15th International Conference on Computational Linguistics, pages
775?761, Kyoto, Japan.
Andrew Olney and Zhiqiang Cai. 2005. An Orthonormal Basis for Topic Segmentation in Tutorial Dialogue. In
Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language
Processing ?HLT ?05, pages 971?978, Vancouver, Canada.
Jeffrey C. Reynar. 1999. Statistical Models of Text Segmentation. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages 357?364.
Martin Scaiano, Diana Inkpen, Robert Lagani`ere, and Adele Reinhartz. 2010. Automatic Text Segmentation for
Movie Subtitles. In Advances in Artificial Intelligence, volume 6085 of Lecture Notes in Computer Science,
pages 295?298. Springer.
Pasi Tapanainen and Timo J?arvinen. 1997. A non-projective dependency parser. Proceedings of the 5th Confer-
ence on Applied Natural Language Processing, pages 64?71.
Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language,
67(4):763?789.
485
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Linear Text Segmentation Using Affinity Propagation
Anna Kazantseva
School of Electrical Engineering
and Computer Science,
University of Ottawa
ankazant@site.uottawa.ca
Stan Szpakowicz
School of Electrical Engineering
and Computer Science,
University of Ottawa &
Institute of Computer Science,
Polish Academy of Sciences
szpak@site.uottawa.ca
Abstract
This paper presents a new algorithm for lin-
ear text segmentation. It is an adaptation of
Affinity Propagation, a state-of-the-art clus-
tering algorithm in the framework of factor
graphs. Affinity Propagation for Segmenta-
tion, or APS, receives a set of pairwise simi-
larities between data points and produces seg-
ment boundaries and segment centres ? data
points which best describe all other data points
within the segment. APS iteratively passes
messages in a cyclic factor graph, until conver-
gence. Each iteration works with information
on all available similarities, resulting in high-
quality results. APS scales linearly for realistic
segmentation tasks. We derive the algorithm
from the original Affinity Propagation formu-
lation, and evaluate its performance on topi-
cal text segmentation in comparison with two
state-of-the art segmenters. The results sug-
gest that APS performs on par with or outper-
forms these two very competitive baselines.
1 Introduction
In complex narratives, it is typical for the topic to
shift continually. Some shifts are gradual, others ?
more abrupt. Topical text segmentation identifies the
more noticeable topic shifts. A topical segmenter?s
output is a very simple picture of the document?s
structure. Segmentation is a useful intermediate step
in such applications as subjectivity analysis (Stoy-
anov and Cardie, 2008), automatic summarization
(Haghighi and Vanderwende, 2009), question an-
swering (Oh, Myaeng, and Jang, 2007) and others.
That is why improved quality of text segmentation
can benefit other language-processing tasks.
We present Affinity Propagation for Segmenta-
tion (APS), an adaptation of a state-of-the-art clus-
tering algorithm, Affinity Propagation (Frey and
Dueck, 2007; Givoni and Frey, 2009).1 The origi-
nal AP algorithm considerably improved exemplar-
based clustering both in terms of speed and the qual-
ity of solutions. That is why we chose to adapt it to
segmentation. At its core, APS is suitable for seg-
menting any sequences of data, but we present it in
the context of segmenting documents. APS takes as
input a matrix of pairwise similarities between sen-
tences and, for each sentence, a preference value
which indicates an a priori belief in how likely
a sentence is to be chosen as a segment centre.
APS outputs segment assignments and segment cen-
tres ? data points which best explain all other points
in a segment. The algorithm attempts to maximize
net similarity ? the sum of similarities between all
data points and their respective segment centres.
APS operates by iteratively passing messages in
a factor graph (Kschischang, Frey, and Loeliger,
2001) until a good set of segments emerges. Each
iteration considers all similarities ? takes into ac-
count all available information. An iteration in-
cludes sending at most O(N2) messages. For the
majority of realistic segmentation tasks, however,
the upper bound is O(MN) messages, where M
is a constant. This is more computationally ex-
pensive than the requirements of locally informed
segmentation algorithms such as those based on
HMM or CRF (see Section 2), but for a globally-
informed algorithm the requirements are very rea-
sonable. APS is an instance of loopy-belief propaga-
tion (belief propagation on cyclic graphs) which has
1An implementation of APS in Java, and the data sets, can be
downloaded at ?www.site.uottawa.ca/?ankazant?.
284
been used to achieved state-of-the-art performance
in error-correcting decoding, image processing and
data compression. Theoretically, such algorithms
are not guaranteed to converge or to maximize the
objective function. Yet in practice they often achieve
competitive results.
APS works on an already pre-compiled similaritiy
matrix, so it offers flexibility in the choice of simi-
larity metrics. The desired number of segments can
be set by adjusting preferences.
We evaluate the performance of APS on three
tasks: finding topical boundaries in transcripts of
course lectures (Malioutov and Barzilay, 2006),
identifying sections in medical textbooks (Eisen-
stein and Barzilay, 2008) and identifying chapter
breaks in novels. We compare APS with two recent
systems: the Minimum Cut segmenter (Malioutov
and Barzilay, 2006) and the Bayesian segmenter
(Eisenstein and Barzilay, 2008). The comparison
is based on the WindowDiff metric (Pevzner and
Hearst, 2002). APS matches or outperforms these
very competitive baselines.
Section 2 of the paper outlines relevant research
on topical text segmentation. Section 3 briefly cov-
ers the framework of factor graphs and outlines the
original Affinity Propagation algorithm for cluster-
ing. Section 4 contains the derivation of the new
update messages for APSeg. Section 5 describes the
experimental setting, Section 6 reports the results,
Section 7 discusses conclusions and future work.
2 Related Work
This sections discusses selected text segmentation
methods and positions the proposed APS algorithm
in that context.
Most research on automatic text segmentation re-
volves around a simple idea: when the topic shifts,
so does the vocabulary (Youmans, 1991). We can
roughly subdivide existing approaches into two cat-
egories: locally informed and globally informed.
Locally informed segmenters attempt to identify
topic shifts by considering only a small portion of
complete document. A classical approach is Text-
Tiling (Hearst, 1997). It consists of sliding two ad-
jacent windows through text and measuring lexical
similarity between them. Drops in similarity corre-
spond to topic shifts. Other examples include text
segmentation using Hidden Markov Models (Blei
and Moreno, 2001) or Conditional Random Fields
(Lafferty, McCallum, and Pereira, 2001). Locally
informed methods are often very efficient because
of lean memory and CPU time requirements. Due to
a limited view of the document, however, they can
easily be thrown off by short inconsequential digres-
sions in narration.
Globally informed methods consider ?the big pic-
ture? when determining the most likely location of
segment boundaries. Choi (2000) applies divisive
clustering to segmentation. Malioutov and Barzilay
(2006) show that the knowledge about long-range
similarities between sentences improves segmenta-
tion quality. They cast segmentation as a graph-
cutting problem. The document is represented as a
graph: nodes are sentences and edges are weighted
using a measure of lexical similarity. The graph is
cut in a way which maximizes the net edge weight
within each segment and minimizes the net weight
of severed edges. Such Minimum Cut segmentation
resembles APS the most among others mentioned in
this paper. The main difference between the two is
in different objective functions.
Another notable direction in text segmentation
uses generative models to find segment boundaries.
Eisenstein and Barzilay (2008) treat words in a sen-
tence as draws from a multinomial language model.
Segment boundaries are assigned so as to maximize
the likelihood of observing the complete sequence.
Misra et al (2009) use a Latent Dirichlet aloca-
tion topic model (Blei, Ng, and Jordan, 2003) to find
coherent segment boundaries. Such methods output
segment boundaries and suggest lexical distribution
associated with each segment. Generative models
tend to perform well, but are less flexible than the
similarity-based models when it comes to incorpo-
rating new kinds of information.
Globally informed models generally perform bet-
ter, especially on more challenging datasets such as
speech recordings, but they have ? unsurprisingly ?
higher memory and CPU time requirements.
The APS algorithm described in this paper com-
bines several desirable properties. It is unsupervised
and, unlike most other segmenters, does not require
specifying the desired number of segments as an in-
put parameter. On each iteration it takes into account
the information about a large portion of the docu-
285
ment (or all of it). Because APS operates on a pre-
compiled matrix of pair-wise sentence similarities, it
is easy to incorporate new kinds of information, such
as synonymy or adjacency. It also provides some in-
formation as to what the segment is about, because
each segment is associated with a segment centre.
3 Factor graphs and affinity propagation
for clustering
3.1 Factor graphs and the max-sum algorithm
The APS algorithm is an instance of belief propa-
gation on a cyclic factor graph. In order to explain
the derivation of the algorithm, we will first briefly
introduce factor graphs as a framework.
Many computational problems can be reduced to
maximizing the value of a multi-variate function
F (x1, . . . , xn) which can be approximated by a sum
of simpler functions. In Equation 1, H is a set of
discrete indices and fh is a local function with argu-
ments Xh ? {x1, . . . , xn}:
F (x1, . . . , xn) =
?
h?H
fh(Xh) (1)
Factor graphs offer a concise graphical represen-
tation for such problems. A global function F which
can be decomposed into a sum of M local function
fh can be represented as a bi-partite graph with M
function nodes and N variable nodes (M = |H|).
Figure 1 shows an example of a factor graph for
F (x1, x2, x3, x4) = f1(x1, x2, x3)+ f2(x2, x3, x4).
The factor (or function) nodes are dark squares, the
variable nodes are light circles.
The well-known max-sum algorithm (Bishop,
2006) seeks a configuration of variables which max-
imizes the objective function. It finds the maximum
in acyclic factor graphs, but in graphs with cycles
neither convergence nor optimality are guaranteed
(Pearl, 1982). Yet in practice good approximations
can be achieved. The max-sum algorithm amounts
to propagating messages from function nodes to
variable nodes and from variable nodes to function
nodes. A message sent from a variable node x to a
function node f is computed as a sum of the incom-
ing messages from all neighbours of x other than f
(the sum is computed for each possible value of x):
?x?f =
?
f ??N(x)\f
?f ??x (2)
Figure 1: Factor graph for F (x1, x2, x3, x4)
= f1(x1, x2, x3) + f2(x2, x3, x4).
f1 f2
x1 x2 x3 x4
N(x) is the set of all function nodes which are x?s
neighbours. The message reflects the evidence about
the distribution of x from all functions which have x
as an argument, except for the function correspond-
ing to the receiving node f .
A message ?f?x from function f to variable x is
computed as follows:
?f?x = max
N(f)\x
(f(x1, . . . , xm) +
?
x??N(f)\x
?x??f )
(3)
N(f) is the set of all variable nodes which are f ?s
neighbours. The message reflects the evidence about
the distribution of x from function f and its neigh-
bours other than x.
A common message-passing schedule on cyclic
factor graphs is flooding: iteratively passing all
variable-to-function messages, then all function-to-
variable messages. Upon convergence, the summary
message reflecting final beliefs about the maximiz-
ing configuration of variables is computed as a sum
of all incoming function-to-variable messages.
3.2 Affinity Propagation
The APS algorithm described in this paper is a mod-
ification of the original Affinity Propagation algo-
rithm intended for exemplar-based clustering (Frey
and Dueck, 2007; Givoni and Frey, 2009). This sec-
tion describes the binary variable formulation pro-
posed by Givoni and Frey, and lays the groundwork
for deriving the new update messages (Section 4).
Affinity Propagation for exemplar-based cluster-
ing is formulated as follows: to cluster N data
points, one must specify a matrix of pairwise sim-
ilarities {SIM(i, j)}i,j?{1,...,N},i 6=j and a set of
self-similarities (so-called preferences) SIM(j, j)
which reflect a priori beliefs in how likely each data
point is to be selected as an exemplar. Preference
values occupy the diagonal of the similarity matrix.
The algorithm then assigns each data point to an ex-
emplar so as to maximize net similarity ? the sum of
286
Figure 2: Factor graph for affinity propagation.
E1 Ej EN
I1
Ii
IN
c11 c1j c1N
ci1 cij ciN
cN1 cNj cNN
S11 S1j S1N
Si1 Sij SiN
SN1 SNj SNN
similarities between all points and their respective
exemplars; this is expressed by Equation 7. Figure 2
shows a schematic factor graph for this problem,
with N2 binary variables. cij = 1 iff point j is an
exemplar for point i. Function nodes Ej enforce a
coherence constraint: a data point cannot exemplify
another point unless it is an exemplar for itself:
Ej(c1j , . . . , cNj) =
?
??
??
?? if cjj = 0 ? cij = 1
for some i 6= j
0 otherwise
(4)
An I node encodes a single-cluster constraint: each
data point must belong to exactly one exemplar ?
and therefore to one cluster:
Ii(ci1, . . . , ciN ) =
{
?? if ?j cij 6= 1
0 otherwise (5)
An S node encodes user-defined similarities
between data-points and candidate exemplars
(SIM(i, j) is the similarity between points i and
j):
Sij(cij) =
{
SIM(i, j) if cij = 1
0 otherwise (6)
Equation 7 shows the objective function which we
want to maximize: a sum of similarities between
data points and their exemplars, subject to the two
constraints (coherence and single-cluster per point).
S(c11, . . . , cNN ) =
?
i,j
Si,j(cij) +
?
i
Ii(ci1, . . . , ciN )
(7)
+
?
j
Ej(c1j , . . . , cNj)
According to Equation 3, the computation of a sin-
gle factor-to-variable message involves maximizing
over 2n configurations. E and I , however, are bi-
nary constraints and evaluate to ?? for most con-
figurations. This drastically reduces the number of
configurations which can maximize the message val-
ues. Given this simple fact, Givoni and Frey (2009)
show how to reduce the necessary update messages
to only two types of scalar ones: availabilities (?)
and responsibilities (?).2
A responsibility message ?ij , sent from a variable
node cij to function node Ej , reflects the evidence
of how likely j is to be an exemplar for i given all
other potential exemplars:
?ij = SIM(i, j)?maxk 6=j (SIM(i, k) + ?ik) (8)
An availability message ?ij , sent from a function
node Ej to a variable node cij , reflects how likely
point j is to be an exemplar for i given the evidence
from all other data points:
?ij =
?
????
????
?
k 6=j
max[?kj , 0] if i = j
min[0, ?jj +
?
k/?{i,j}
max[?kj , 0]] if i 6= j
(9)
Let ?ij(l) be the message value corresponding to set-
ting variable cij to l, l ? {0, 1}. Instead of sending
two-valued messages (corresponding to the two pos-
sible values of the binary variables), we can send
the difference for the two possible configurations:
?ij = ?ij(1)? ?ij(0) ? effectively, a log-likelihood
ratio.
2Normally, each iteration of the algorithm sends five types
of two-valued messages: to and from functions E and I and
a message from functions S. Fortunately, the messages sent
to and from E factors to the variable nodes subsume the three
other message types and it is not necessary to compute them
explicitly. See (Givoni and Frey, 2009, p.195) for details.
287
Figure 3: Examples of valid configuration of hidden
variables {cij} for clustering and segmentation.
(a) Clustering (b) Segmentation
The algorithm converges when the set of points
labelled as exemplars remains unchanged for a pre-
determined number of iterations. When the al-
gorithm terminates, messages to each variable are
added together. A positive final message indicates
that the most likely value of a variable cij is 1 (point
j is an exemplar for i), a negative message indicates
that it is 0 (j is not an exemplar for i).
4 Affinity Propagation for Segmentation
This section explains how we adapt the Affinity
Propagation clustering algorithm to segmentation.
In this setting, sentences are data points and we
refer to exemplars as segment centres. Given a doc-
ument, we want to assign each sentence to a segment
centre so as to maximize net similarity.
The new formulation relies on the same underly-
ing factor graph (Figure 2). A binary variable node
cij is set to 1 iff sentence j is the segment centre for
sentence i. When clustering is the objective, a clus-
ter may consist of points coming from anywhere in
the data sequence. When segmentation is the ob-
jective, a segment must consist of a solid block of
points around the segment centre. Figure 3 shows,
for a toy problem with 5 data points, possible valid
configurations of variables {cij} for clustering (3a)
and for segmentation (3b).
To formalize this new linearity requirement, we
elaborate Equation 4 into Equation 10. Ej evaluates
to ?? in three cases. Case 1 is the original coher-
ence constraint. Case 2 states that no point k may
be in the segment with a centre is j, if k lies before
the start of the segment (the sequence c(s?1)j = 0,
csj = 1 necessarily corresponds to the start of the
segment). Case 3 handles analogously the end of
the segment.
Ej =
?
??????????
??????????
?? 1. if cjj = 0 ? cij = 1 for some i 6= j
2. if cjj = 1 ? csj = 1 ? c(s?1)j = 0
? ckj = 1 for some s < j, k < s? 1
3. if cjj = 1 ? cej = 1 ? c(e+1)j = 0
? ckj = 1 for some e > j, k > e+ 1
0 otherwise
(10)
The E function nodes are the only changed part of
the factor graph, so we only must re-derive ? mes-
sages (availabilities) sent from factors E to variable
nodes. A function-to-variable message is computed
as shown in Equation 11 (elaborated Equation 3),
and the only incoming messages to E nodes are re-
sponsibilities (? messages):
?f?x = max
N(f)\x
(f(x1, . . . , xm) +
?
x??N(f)\x
?x??f ) =
(11)
max
cij , i 6=j
((Ej(c1j , . . . , cNj) +
?
cij , i 6=j
?ij(cij)))
We need to compute the message values for the
two possible settings of binary variables ? denoted
as ?ij(1) and ?ij(0) ? and propagate the difference
?ij = ?ij(1) - ?ij(0).
Consider the case of factor Ej sending an ? mes-
sage to the variable node cjj (i.e., i = j). If cjj = 0
then point j is not its own segment centre and the
only valid configuration is to set al other cij to 0:
?jj(0) = maxcij ,i 6=j
(Ej(c1j , . . . , cNj) +
?
cij ,i 6=j
?ij(cij))
(12)
=
?
i 6=j
?ij(0)
To compute ?ij(1) (point j is its own segment
centre), we only must maximize over configurations
which will not correspond to cases 2 and 3 in Equa-
tion 10 (other assignments are trivially non-optimal
because they would evaluate Ej to ??). Let the
start of a segment be s, 1 ? s < j and the end of
the segment be e, j + 1 < e ? N . We only need to
consider configurations such that all points between
s and e are in the segment while all others are not.
288
The following picture shows a valid configuration.3
1 s j e N
To compute the message ?ij(1), i = j, we have:
?jj(1) =
jmax
s=1
[
s?1?
k=1
?kj(0) +
j?1?
k=s
?kj(1)]+ (13)
Nmax
e=j
[
e?
k=j+1
?kj(1) +
N?
k=e+1
?kj(0)]
Subtracting Equation 12 from Equation 13, we get:
?jj = ?jj(1)? ?jj(0) = (14)
jmax
s=1
(
j?1?
k=s
?kj) +
Nmax
e=j
(
e?
k=j+1
?kj)
Now, consider the case of factor Ej sending an ?
message to a variable node cij other than segment
exemplar j (i.e., i 6= j). Two subcases are possible:
point i may lie before the segment centre j (i < j),
or it may lie after the segment centre (i > j).
The configurations which may maximize ?ij(1)
(the message value for setting the hidden variable
to 1) necessarily conform to two conditions: point
j is labelled as a segment centre (cjj = 1) and all
points lying between i and j are in the segment.
This corresponds to Equation 15 for i < j and to
Equation 16 for i > j. Pictorial examples of corre-
sponding valid configurations precede the equations.
1 s i j e N
?ij, i<j(1) =
imax
s=1
[
s?1?
k=1
?kj(0) +
i?1?
k=s
?kj(1)]+
(15)
j?
k=i+1
?kj(1) +
Nmax
e=j
[
e?
k=j+1
?kj(1) +
N?
k=e+1
?kj(0)]
3Variables cij set to 1 are shown as shaded circles, to 0 ? as
white circles. Normally, variables form a column in the factor
graph; we transpose them to save space.
1 s j i e N
?ij, i>j(1) =
jmax
s=1
[
s?1?
k=1
?kj(0) +
j?1?
k=s
?kj(1)]+
(16)
i?1?
k=j
?kj(1) +
Nmax
e=i
[
e?
k=i+1
?kj(1) +
N?
k=e+1
?kj(0)]
To compute the message value for setting the
hidden variable cij to 0, we again distinguish
between i < j and i > j and consider whether cjj
= 1 or cjj = 0 (point j is / is not a segment centre).
For cjj = 0 the only optimal configuration is cij = 0
for all i 6= j. For cjj = 1 the set of possible optimal
configurations is determined by the position of point
i with respect to point j. Following the same logic
as in the previous cases we get Equation 17 for
i < j and Equation 18 for i > j.
1 i s j e N
?ij(0) = max(
?
k/?i,j
?kj(0), (17)
i?1?
k=1
?kj(0) +
jmax
s=i+1
[
s?1?
k=i+1
?kj(0) +
j?1?
k=s
?kj(1)]+
?jj(1) +
Nmax
e=j
[
e?
k=j+1
?kj(1) +
N?
k=e+1
?kj(0)])
1 s j e i N
?ij(0) = max(
?
k/?i,j
?kj(0), (18)
jmax
s=1
[
s?1?
k=1
?kj(0) +
j?1?
k=s
?kj(1)]+
?jj(1) +
i?1max
e=j
[
e?
k=j+1
?kj(1) +
i?1?
k=e+1
?kj(0)]
N?
k=i+1
?kj(0))
Due to space constraints, we will omit the details
of subtracting Equation 17 from 15 and Equation 18
from 16. The final update rules for both i < j and
289
Algorithm 1 Affinity Propagation for Segmentation
1: input: 1) a set of pairwise similarities {SIM(i, j)}(i,j)?{1,...,N}2 , SIM(i, j) ? R; 2) a set of prefer-
ences (self-similarities) {SIM(i, i)}i?{1,...,N} indicating a priori likelihood of point i being a segment
centre
2: initialization: ?i, j : ?ij = 0 (set al availabilities to 0)
3: repeat
4: iteratively update responsibilities (?) and availabilities (?)
5:
?i, j : ?ij = SIM(i, j) + maxk 6=j (SIM(i, k)? ?ik)
6:
?i, j : ?ij =
?
?????????????????????????
?????????????????????????
jmax
s=1
(
j?1?
k=s
?kj) +
Nmax
e=j
(
e?
k=j+1
?kj) if i = j
min[ imax
s=1
i?1?
k=s
?kj +
j?
k=i+1
?kj +
Nmax
e=j
e?
k=j+1
?kj ,
imax
s=1
i?1?
k=s
?kj +
j
min
s=i+1
s?1?
k=i+1
?kj ] if i < j
min[ jmax
s=1
j?1?
k=s
?kj +
i?1?
k=j
?kj +
Nmax
e=i
e?
k=i+1
?kj ,
i?1
min
e=j
i?1?
k=e+1
?kj +
Nmax
e=i
e?
k=i+1
?kj ] if i > j
7: until convergence
8: compute the final configuration of variables: ?i, j j is the exemplar for i iff ?ij + ?ij > 0
9: output: exemplar assignments
i > j appear in Algorithm 1, where we summarize
the whole process.
The equations look cumbersome but they are triv-
ial to compute. Every summand corresponds to find-
ing the most likely start or end of the segment, tak-
ing into account fixed information. When computing
messages for any given sender node, we can remem-
ber the maximizing values for neighbouring recipi-
ent nodes. For example, after computing the avail-
ability message from factor Ej to cij , we must only
consider one more responsibility value when com-
puting the message from Ej to variable c(i+1)j . The
cost of computing a message is thus negligible.
When the matrix is fully specified, each iteration
requires passing 2N2 messages, so the algorithm
runs in O(N2) time and requires O(N2) memory
(to store the similarities, the availabilities and the
responsibilities). When performing segmentation,
however, the user generally has some idea about
the average or maximum segment length. In such
more realistic cases, the input matrix of similarities
is sparse ? it is constructed by sliding a window of
size M . M usually needs to be at least twice the
maximum segment length or thrice the average seg-
ment length. Each iteration, then, involves sending
2MN messages and the storage requirements are
also O(MN).
As is common in loopy belief propagation algo-
rithms, both availability and responsibility messages
are dampened to avoid overshooting and oscillating.
The dampening factor is ? where 0.5 ? ? < 1.
newMsg = ? ? oldMsg+ (1? ?)newMsg (19)
The APS algorithm is unsupervised. It only benefits
290
from a small development set to fine-tune a few pa-
rameters: preference values and the dampening fac-
tor. APS does not require (nor allow) specifying the
number of segments beforehand. The granularity of
segmentation is adjusted through preference values;
this reflect how likely each sentence is to be selected
as a segment centre. (This translates into the cost of
adding a segment.)
Because each message only requires the knowl-
edge about one column or row of the matrix, the al-
gorithm can be easily parallelized.
5 Experimental Setting
Datasets. We evaluate the performance of the
APS algorithm on three datasets. The first, com-
piled by Malioutov and Barzilay (2006), consists
of manually transcribed and segmented lectures on
Artificial Intelligence, 3 development files and 19
test files. The second dataset consists of 227 chap-
ters from medical textbooks (Eisenstein and Barzi-
lay, 2008), 5 of which we use for development. In
this dataset the gold standard segment boundaries
correspond to section breaks specified by the au-
thors. The third dataset consists of 85 works of fic-
tion downloaded from Project Gutenberg, 3 of which
are used for development. The segment boundaries
correspond to chapter breaks or to breaks between
individual stories. They were inserted automatically
using HTML markup in the downloaded files.
The datasets exhibit different characteristics. The
lecture dataset and the fiction dataset are challeng-
ing because they are less cohesive than medical text-
books. The textbooks are cognitively more difficult
to process and the authors rely on repetition of ter-
minology to facilitate comprehension. Since lexical
repetition is the main source of information for text
segmentation, we expect a higher performance on
this dataset. Transcribed speech, on the other hand,
is considerably less cohesive. The lecturer makes an
effort to speak in ?plain language? and to be com-
prehensible, relying less on terminology. The use of
pronouns is very common, as is the use of examples.
Repeated use of the same words is also uncom-
mon in fiction. In addition, the dataset was compiled
automatically using HTML markup. The markup
is not always reliable and occasionally the e-book
proofreaders skip it altogether, which potentially
adds noise to the dataset.
Baselines. We compare the performance of
APS with that of two state-of-the-art segmenters: the
Minimum Cut segmenter (Malioutov and Barzilay,
2006) and the Bayesian segmenter (Eisenstein and
Barzilay, 2008). The authors have made Java imple-
mentations publicly available. For the Minimum Cut
segmenter, we select the best parameters using the
script included with that distribution. The Bayesian
segmenter automatically estimates all necessary pa-
rameters from the data.
Preprocessing and the choice of similarity met-
ric. As described in Section 4, the APS algorithm
takes as inputs a matrix of pairwise similarities be-
tween sentences in the document and also, for each
sentence, a preference value.
This paper focuses on comparing globally in-
formed segmentation algorithms, and leaves for fu-
ture work the exploration of best similarity metrics.
To allow fair comparison, then, we use the same
metric as the Minimum Cut segmenter, cosine sim-
ilarity. Each sentence is represented as a vector of
token-type frequencies. Following (Malioutov and
Barzilay, 2006), the frequency vectors are smoothed
by adding counts of words from the adjacent sen-
tences and then weighted using a tf.idf metric (for
details, see ibid.) The similarity between sentence
vectors s1 and s2 is computed as follows:
cos(s1, s2) =
s1 ? s2
||s1|| ? ||s2||
(20)
The representation used by the Bayesian segmenter
is too different to be incorporated into our model di-
rectly, but ultimately it is based on the distribution
of unigrams in documents. This is close enough to
our representation to allow fair comparison.
The fiction dataset consists of books: novels or
collections of short stories. Fiction is known to ex-
hibit less lexical cohesion. That is why ? when
working on this dataset ? we work at the paragraph
level: the similarity is measured not between sen-
tences but between paragraphs. We use this repre-
sentation with all three segmenters.
All parameters have been fine-tuned on the devel-
opment portions of the datasets. For APS algorithm
per se we needed to set three parameters: the size of
the sliding window for similarity computations, the
dampening factor ? and the preference values. The
291
BayesSeg MinCutSeg APS
AI 0.443 0.437 0.404
Clinical 0.353 0.382 0.371
Fiction 0.377 0.381 0.350
Table 1: Results of segmenting the three datasets us-
ing the Bayesian segmenter, the Minimum Cut seg-
menter and APS.
parameters for the similarity metric (best variation
of tf.idf, the window size and the decay factor for
smoothing) were set using the script provided in the
Minimum Cut segmenter?s distribution.
Evaluation metric. We have measured the per-
formance of the segmenters with the WindowDiff
metric (Pevzner and Hearst, 2002). It is computed
by sliding a window through reference and through
segmentation output and, at each window position,
comparing the number of reference breaks to the
number of breaks inserted by the segmenter (hypo-
thetical breaks). It is a penalty measure which re-
ports the number of windows where the reference
and hypothetical breaks do not match, normalized
by the total number of windows. In Equation 21,
ref and hyp denote the number of reference and hy-
pothetical segment breaks within a window.
winDiff = 1N ? k
N?k?
i=1
(|ref ? hyp| 6= 0) (21)
6 Experimental Results and Discussion
Table 1 compares the performance of the three seg-
menters using WindowDiff values. On the lecture
and fiction datasets, the APS segmenter outperforms
the others by a small margin, around 8% over the
better of the two. It is second-best on the clinical
textbook dataset. According to a one-tailed paired
t-test with 95% confidence cut-off, the improvement
is statistically significant only on the fiction dataset.
All datasets are challenging and the baselines are
very competitive, so drawing definitive conclusions
is difficult. Still, we can be fairly confident that
APS performs at least as well as the other two seg-
menters. It also has certain advantages.
One important difference between APS and the
other segmenters is that APS does not require the
number of segments as an input parameter. This is
very helpful, because such information is generally
unavailable in any realistic deployment setting. The
parameters are fine-tuned to maximize WindowDiff
values, so this results in high-precision, low-recall
segment assignments; that is because WindowDiff
favours missing boundaries over near-hits.
APS also outputs segment centres, thus providing
some information about a segment?s topic. We have
not evaluated how descriptive the segment centres
are; this is left for future work.
APS performs slightly better than the other seg-
menters but not by much. We hypothesize that one
of the reasons is that APS relies on the presence of
descriptive segment centres which are not necessar-
ily present for large, coarse-grained segments such
as chapters in novels. It is possible for APS to have
an advantage performing fine-grained segmentation.
7 Conclusions and Future Work
In this paper we have presented APS ? a new algo-
rithm for linear text segmentation. APS takes into
account the global structure of the document and
outputs segment boundaries and segment centres. It
scales linearly in the number of input sentences, per-
forms competitively with the state-of-the-art and is
easy to implement. We also provide a Java imple-
mentation of the APS segmenter.
We consider two main directions for future work:
using more informative similarity metrics and mak-
ing the process of segmentation hierarchical. We
chose to use cosine similarity primarily to allow fair
comparison and to judge the algorithm itself, in iso-
lation from the information it uses. Cosine similarity
is a very simple metric which cannot provide an ad-
equate picture of topic fluctuations in documents. It
is likely that dictionary-based or corpus-based simi-
larity measures would yield a major improvement in
performance.
Reliance on descriptive segment centres may
handicap APS?s performance when looking for
coarse-grained segments. One possible remedy is to
look for shorter segments first and then merge them.
One can also modify the algorithm to perform hier-
archical segmentation: consider net similarity with
low-level segment centres as well as with high-level
ones. We plan to explore both possibilities.
292
Acknowledgements
We thank Inmar Givoni for explaining the details
of binary Affinity Propagation and for comment-
ing on our early ideas in this project. Many thanks
to Yongyi Mao for a helpful discussion on the use
Affinity Propagation for text segmentation.
References
Bishop, Christopher M. 2006. Pattern Recognition and
Machine Learning. Springer.
Blei, David and Pedro Moreno. 2001. Topic Segmenta-
tion with an Aspect Hidden Markov Model. In Pro-
ceedings of the 24th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 343?348. ACM Press.
Blei, David M., Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Choi, Freddy Y. Y. 2000. Advances in Domain Inde-
pendent Linear Text Segmentation. In Proceedings of
NAACL, pages 26?33.
Eisenstein, Jacob and Regina Barzilay. 2008. Bayesian
Unsupervised Topic Segmentation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 334?343, Honolulu,
Hawaii, October.
Frey, Brendan J. and Delbert Dueck. 2007. Clustering
by Passing Messages Between Data Points. Science,
315:972?976.
Givoni, Inmar E. and Brendan J. Frey. 2009. A Binary
Variable Model for Affinity Propagation. Neural Com-
putation, 21:1589?1600.
Haghighi, Aria and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-Document Summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 362?370, Boulder, Colorado, June.
Hearst, Marti A. 1997. TextTiling: segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23:33?64, March.
Kschischang, Frank R., Brendan J. Frey, and Hans-A
Loeliger. 2001. Factor graphs and the sum-product
algorithm. In IEEE Transactions on Information The-
ory, Vol 47, No 2, pages 498?519, February.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of ICML-01, pages 282?289.
Malioutov, Igor and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?
32, Sydney, Australia, July.
Misra, Hemant, Franc?ois Yvon, Joemon M. Jose, and
Olivier Cappe?. 2009. Text segmentation via topic
modeling: an analytical study. In 18th ACM Con-
ference on Information and Knowledge Management,
pages 1553?1556.
Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-Gil
Jang. 2007. Semantic passage segmentation based on
sentence topics for question answering. Information
Sciences, an International Journal, 177:3696?3717,
September.
Pearl, Judea. 1982. Reverend Bayes on inference en-
gines: A distributed hierarchical approach. In Pro-
ceedings of the American Association of Artificial In-
telligence National Conference on AI, pages 133?136,
Pittsburgh, PA.
Pevzner, Lev and Marti A. Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 28(1):19?36.
Stoyanov, Veselin and Claire Cardie. 2008. Topic identi-
fication for fine-grained opinion analysis. In COLING
?08 Proceedings of the 22nd International Conference
on Computational Linguistics - Volume 1, pages 817?
824.
Youmans, Gilbert. 1991. A new tool for discourse anal-
ysis: The vocabulary-management profile. Language,
67(4):763?789.
293
Summarizing Short Stories
Anna Kazantseva?
University of Ottawa
Stan Szpakowicz??
University of Ottawa
Polish Academy of Sciences
We present an approach to the automatic creation of extractive summaries of literary short
stories. The summaries are produced with a specific objective in mind: to help a reader decide
whether she would be interested in reading the complete story. To this end, the summaries give
the user relevant information about the setting of the story without revealing its plot. The system
relies on assorted surface indicators about clauses in the short story, the most important of which
are those related to the aspectual type of a clause and to the main entities in a story. Fifteen judges
evaluated the summaries on a number of extrinsic and intrinsic measures. The outcome of this
evaluation suggests that the summaries are helpful in achieving the original objective.
1. Introduction
In the last decade, automatic text summarization has become a popular research topic
with a curiously restricted scope of applications. A few innovative research directions
have emerged, including headline generation (Soricut and Marcu 2007), summarization
of books (Mihalcea and Ceylan 2007), personalized summarization (D??az and Gerva?s
2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), sum-
marization of speech (Fuentes et al 2005), dialogues (Zechner 2002), evaluative text
(Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks
2007). In addition, more researchers have been venturing past purely extractive sum-
marization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By
and large, however, most research in text summarization still revolves around texts
characterized by rigid structure. The better explored among such texts are news articles
(Barzilay and McKeown 2005), medical documents (Elhadad et al 2005), legal docu-
ments (Moens 2007), and papers in the area of computer science (Teufel and Moens
2002; Mei and Zhai 2008). Although summarizing these genres is a formidable challenge
in itself, it excludes a continually increasing number of informal documents available
electronically. Such documents, ranging from novels to personal Web pages, offer a
wealth of information that merits the attention of the text summarization community.
? School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave.,
Ottawa, Ontario K1N 6N5, Canada. E-mail: ankazant@site.uottawa.ca.
?? School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave.,
Ottawa, Ontario K1N 6N5, Canada. E-mail: szpak@site.uottawa.ca.
Submission received: 3 April 2007; revised submission received: 20 January 2009; accepted for publication:
29 July 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
We attempt to make a step in this direction by devising an approach to summarizing a
relatively unexplored genre: literary short stories.
Well-structured documents, such as news articles, exhibit a number of character-
istics that help identify some of the important passages without performing in-depth
semantic analysis. These characteristics include predictable location of typical items in
a document and in its well-delineated parts, cue words, and template-like structure
that often characterizes a genre (e.g., scientific papers). This is not the case in literature.
Quite the contrary?to write fiction in accordance with a template is a sure way to write
poor prose. One also cannot expect to find portions of text that summarize the main
idea behind a story, and even less so to find them in the same location. In addition, the
variety of literary devices (the widespread use of metaphor and figurative language,
leaving things unsaid and relying on the reader?s skill of reading between the lines,
frequent use of dialogue, etc.) makes summarizing fiction a very distinct task. It is
a contribution of this work to demonstrate that summarizing short fiction is feasible
using state-of-the-art tools in natural language technology. In the case of our corpus,
this is also done without deep semantic resources or knowledge bases, although such
resources would be of great help. We leverage syntactic information and shallow se-
mantics (provided by a gazetteer) to produce indicative summaries of short stories that
people find helpful and that outperform naive baselines and two state-of-the-art generic
summarizers.
We have restricted the scope of this potentially vast project in several ways. In
the course of this work we concentrate on producing summaries of short stories suit-
able for a particular purpose: to help a reader form adequate expectations about the
complete story and decide whether she would be interested in reading it. To this end,
the summary includes important elements of the setting of a story, such as the place
and the main characters, presented as excerpts from the complete story. The assump-
tion behind this definition is this: If a reader knows when and where the story takes
place and who its main characters are, she should be able to make informed decisions
about it.
With such a definition of short story summaries, re-telling the plot in the summary
is not among the objectives of this work; in fact, doing so is undesirable. We have
introduced this limitation for two reasons. There is an ?ideological? side of the decision:
Not many people want to know what happens in a story before reading it, even if this
may help them decide that the story is worth reading. There also is a practical side,
namely the complexity of the problem: Summarizing the plot would be considerably
more difficult (see Section 2 for a review of related work). We hope to tackle this
issue in the future. For now, creating indicative summaries of short stories is challenge
enough.
The summaries in Figures 1?3 illustrate our approach in the context of a naive
lead baseline and a ceiling. Figure 1 shows an example of an automatically produced
summary that meets the aforementioned criteria. A reader can see that the story is set in
a restaurant where the customers are tended to by two waitresses: the fair Aileen who
?wins hearts? and ?the-bag-o?-meal? plain-faced Tildy. If the reader chooses to pursue
the story, she will find the description of an accident of paramount importance to Tildy:
One day she is kissed by a customer in public! The event is more than flattering to
usually under-appreciated Tildy. It causes a complete change in how she views herself.
The story then unfolds to reveal that the customer was drunk on the day in question
and that he returned to apologize several days later. This apology is a severe blow to
Tildy and an abrupt end of many a dream that the incident had spurred in her head. The
story ends with Aileen trying to comfort her crying friend by saying ?He ain?t anything
72
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 1
Example of a summary produced by the system.
of a gentleman or he wouldn?t ever of apologized.? Yet, the summary in Figure 1 does
not reveal these facts. For comparison, Figure 2 shows a summary obtained by taking
the same number of sentences from the beginning of the story. As the reader can see,
such a trivial approach is not sufficient to create a useful summary. Figure 3 shows a
manually created ?ideal? summary.
We experimented with a corpus of 47 stories from the 19th and early 20th century
written by renowned writers, including O. Henry, Jerome K. Jerome, Anton Chekhov,
and Guy de Maupassant. The stories, with the exception of a few fairy tales, are clas-
sical examples of short social fiction. The corpus was collected from Project Gutenberg
(www.gutenberg.org) and only contains stories in English. The average length of a story
is 3,333 tokens and the target compression rate expressed in the number of sentences
is 94%.
In order to create summaries of short stories that satisfy our stated criteria (hence-
forth indicative summaries), the system searches each story for sentences that focus
on important entities and relate the background of the story (as opposed to events).
Correspondingly, processing has two stages. Initially, the summarizer identifies two
types of important entities: main characters and locations. This is achieved using a
gazetteer, resolving anaphoric expressions and then identifying frequently mentioned
Figure 2
Example of a lead baseline summary.
73
Computational Linguistics Volume 36, Number 1
Figure 3
Example of a manual summary.
entities. Next, the system selects sentences that set out the background of the story
and focus on one of the important entities. In order to separate the background of a
story from the plot (i.e., events), we rely on the notion of aspect.1 We approximate the
aspectual type of a clause using either machine learning or manually produced rules.
This is achieved by relying on an array of verb-related features, such as tense, lexical
aspect of the main verb, presence of temporal expressions, and so on. Finally, the system
composes a summary out of the selected sentences.
Our task remains a significant challenge despite its limited scope. To produce such
indicative summaries successfully, one needs to consider many facets of the problem.
An informative data representation, computational complexity, and usability of the final
product are only some of them. Because the project is at the stage of an advanced
feasibility study, it has not been possible to do justice to all aspects of the problem.
Therefore, we concentrated on several specific issues and left many more to future work
and to fellow researchers.
Firstly, we sought to identify characteristics of short stories that could be helpful in
creating summaries. We devised an informative and practical data representation that
could be reproduced without too much cost or effort. Secondly, we restricted ourselves
to identifying the most informative portions of the stories and paid much less attention
to readability and coherence of the resulting summaries. Even though readability is an
important property, we hypothesized that informativeness is yet more important. Once
the task of identifying informative passages has been accomplished, one can work on
achieving coherence and readability. In the end, the emphasis was on the creation of
extractive summaries using established tools and methods and on the identification of
genre-specific properties that can help summarization.
The novelty of the task and the absence of agreed-upon measures for evaluating
summaries of literary prose call for a thorough evaluation using a variety of metrics.
That is why we conduct three distinct evaluation experiments. The summaries are
1 The term aspect is defined and explained in detail in Section 4. For now it suffices to say that by aspect
we mean a characteristic of a clause that gives readers an idea about the temporal flow of an event or a
state described in it. For example, the aspectual type of the sentence He likes to run is a state. The aspectual
type of He has run a marathon is an event.
74
Kazantseva and Szpakowicz Summarizing Short Stories
evaluated both extrinsically and intrinsically. They are also compared with two naive
baselines (lead and random) and two state-of-the-art summarization systems designed
for summarizing newswire (henceforth baseline summarizers).2
In the first experiment, 15 people read a mix of machine-made, random, and manual
summaries, and answer questions about them. Some questions are factual in nature
(e.g., list the main characters of the story), and others are subjective (e.g., rate the readability
of the summary). The results show that the machine-made summaries are significantly
better than the random baseline but they fall far short of the quality of the manual
summaries.
During the second evaluative experiment, the machine-made summaries are com-
pared against extracts created by people using sentence co-selection measures (pre-
cision, recall, and F-score). By sentence co-selection we mean measuring how many
sentences found in manually created extracts are selected for inclusion in automatically
produced summaries. The results suggest that our system outperforms all baselines,
including state-of-the-art summarizers.
The third part of the evaluation uses two ROUGE metrics (Lin 2004) to compare
the machine-made and the baseline summaries with the model abstracts. The results
suggest that these measures are not well suited for evaluating extractive indicative
summaries of short stories.
This paper is organized in the following manner. Section 2 gives a brief overview of
the body of research in automatic story comprehension. Section 3 describes the process
of identifying important entities in short stories. Section 4 introduces the notion of
aspect, gives an overview of the system?s design, and discusses the linguistic motivation
behind it. Section 5 describes the classification procedures (the use of machine learning
and manual rule creation) that distinguish between the descriptive elements of a story
and the passages that describe events. Section 6 reports on the evaluation of summaries
which our system produces. Section 7 draws conclusions and outlines directions for
future work.
2. Related Work
Summarization of literary prose is a relatively unexplored topic. There exists, however,
a substantial body of research tackling the problem of story comprehension. During
the 1970s and 1980s, a number of researchers in artificial intelligence built story-
understanding systems that relied in one way or another on contemporary research
in psychology and discourse processing.
Much of that line of research relied on an assumption that stories exhibit global
cognitive structure (known as macrostructure [van Dijk 1980] or schema [Bartlett 1932])
and that they can be decomposed into a finite number of cognitive units. According
to this view, diversity in stories is not due to an infinite number of plots, but to an
infinite number of combinations of a (relatively) small number of cognitive units. This
direction was pioneered in 1928 by Vladimir Propp (1968) with his detailed analysis of
100 Russian folk tales. After a period of oblivion, these ideas regained popularity: van
Dijk and Kintsch (1978) demonstrated the existence of macrostructures and their role
in story comprehension and recall; Rumelhart (1975), Thorndyke (1975), and Mandler
2 These systems are GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, O?Leary,
and Conroy 2008; Conroy, Schlesinger, and O?Leary 2007). See Section 6 for details.
75
Computational Linguistics Volume 36, Number 1
(1987) developed sets of cognitive schemas (story grammars) that could be applied to
certain types of stories; and Lehnert (1982) proposed to represent action-based stories
in terms of a finite number of plot units?configurations of affect (or emotional) states
of its characters.
The developments in psychology and discourse processing had gone hand in hand
with those in artificial intelligence and resulted in a score of story-understanding
and question-answering systems. Many of these relied on a set of manually encoded
schemas and chose the most appropriate one for a given story (Cullingford 1978; Dyer
1983; Leake 1989). For example, a system called BORIS (Dyer 1983) processed stories
word-by-word to create a very rich semantic representation of them using Memory
Organization Packets (MOPs) and Thematic Affect Units (TAUs). These knowledge
structures were activated by means of a very detailed lexicon where each lexeme was
associated with MOPs and TAUs it could invoke.
Systems such as BORIS could not process stories that did not conform to schemas
already at their disposal. Charniak and Goldman (1988) and Norvig (1989) at-
tempted to circumvent this problem by learning to recognize more general structures.
FAUSTUS (Norvig 1989) recognized six general classes of inferences by finding patterns
of connectivity in a semantic network. It could be adapted to new kinds of documents by
extending its knowledge base and not the underlying algorithm or patterns. Research in
automatic story comprehension offered a number of important solutions for subsequent
developments in artificial intelligence. No less important, it pointed out a number of
challenges. All these systems required a formidable amount of semantic knowledge and
a robust and efficient way of building a semantic representation of texts. In addition,
systems such as BORIS or SAM (Cullingford 1978) also needed a set of schemas or
schema-like scenarios. With such labor intensity, these requirements prohibit using
schema-based approaches for real-life stories (e.g., fiction) and only allow the processing
of artificially created examples.
In this historical context, our current approach to summarization of short fiction
appears rather modest: Our system does not ?understand? stories, nor does it retell
their plot. Instead, it offers the reader hints?important information about the story?s
setting?which should help her guess what type of story is to come. This assumption
appears reasonable because it has been shown that comprehension and recall of dis-
course are strongly influenced by the reader?s familiarity with the type of schema (van
Dijk and Kintsch 1978). Because our system is tailored to work with classics of the genre,
it was our expectation that the gist of the story?s setting offered to the reader in the
wording of the original would give her an idea about the story?s themes and likely plot
developments. The results of our experiments appear to back this assumption.
In addition, given the original objective, it seems reasonable that elements other
than the plot would have a strong influence on the reader?s decision to read or not to
read the story. The setting of the story, its characters, and style are some of the important
factors. Many outstanding literary works differ not so much in plot as in their setting or
moral.3 Our system attempts to capture important elements of the setting explicitly and
we expect that some elements of style may be captured implicitly due to the extractive
nature of the summaries.
3 Consider, for example, Goethe?s Faust and Bulgakov?s Master and Margarita. Although they both revolve
around the protagonist entering into a pact with the devil?albeit for different reasons?the latter takes
place in Moscow around 1930s and the two works are dramatically different.
76
Kazantseva and Szpakowicz Summarizing Short Stories
3. Identifying Important Entities
During the first stage of summary production the system identifies important entities
in stories. Initially, we planned to identify three types of entities: people, locations, and
time stamps. During a preliminary exploration of the corpus, we analyzed 14 stories for
the presence of surface indicators of characters, locations, and temporal anchors.4 We
employed the GATE Gazetteer (Cunningham et al 2002), and only considered entities
it recognized automatically.
The experiment revealed that the stories in the corpus contained multiple mentions
of characters (on average, 64 mentions per story, excluding pronouns). On the other
hand, the 14 stories contained only 22 location markers, mostly street names. Four
stories had no identifiable location markers. Finally, merely four temporal anchors
were identified in all 14 stories: two absolute (such as year) and two relative (e.g.,
Christmas). These findings support the intuitive idea that short stories revolve around
their characters, even if the ultimate goal is to show a larger social phenomenon. They
also suggest that looking for time stamps in short stories is unlikely to prove productive,
because such information is not included in these texts explicitly. That is why our system
does not attempt to identify them.
Because characters appear to be central to short stories, we designed our system
to maximize the amount of information available about them. It contains an anaphora
resolution module that resolves pronominal and noun phrase anaphoric references to
animate entities. The term anaphora, as used in this work, can be explained as a way
of mentioning a previously encountered entity without naming it explicitly. Consider
Examples 1a, 1b, and 1c from ?A Matter of Mean Elevation? by O. Henry. The noun
phrase Mlle. Giraud from Example 1a is an antecedent and the pronouns her and she from
Example 1c are anaphoric expressions or referents. Example 1c illustrates pronominal
anaphora, and Example 1b illustrates noun phrase anaphora. Here the noun phrase
the woman is the anaphoric expression which refers to the antecedent Mlle. Giraud from
Example 1a.
(1a) John Armstrongent1 and Mlle. Giraudent2 rode among the Andean peaks, enveloped
in their greatness and sublimity.
(1b) To Armstrongent1 the womanent2 seemed almost a holy thing.
(1c) Never yet since herent2 rescue had sheent2 smiled.
The anaphora resolution module only handles first and third person singular per-
sonal pronouns (I, me, my, he, his ...) and singular definite noun phrases that denote
animate entities (e.g., the man, but not men). It is implemented in Java, within the GATE
framework, using the Connexor Machinese Syntax parser (Tapanainen and Ja?rvinen
1997).
The system resolves anaphoric expressions in the following manner. Initially, the
documents are parsed with the Connexor Machinese Syntax parser. The parsed data are
then forwarded to the Gazetteer in GATE, which recognizes nouns denoting locations
and persons. The original version of the Gazetteer only recognizes named entities and
professions, but we extended it to include 137 common animate nouns such as man,
woman, soldier, or baby. During the next stage, pronominal anaphoric expressions are
4 The stories used in this exploration were later included in the training part of the data. They were never
used for testing.
77
Computational Linguistics Volume 36, Number 1
Table 1
Results of anaphora resolution.
Type of anaphora All Correct Incorrect Error rate, %
Pronominal 597 507 90 15.07
Nominal 152 96 56 36.84
Both 749 603 146 19.49
resolved using an implementation of the algorithm proposed by Lappin and Leass
(1994).5 Subsequently, anaphoric noun phrases are identified using the rules outlined by
Vieira and Poesio (2000). Finally, anaphoric noun phrases are resolved using a modified
version of the Lappin and Leass algorithm, adjusted to finding antecedents of nouns.
The implementation is described in detail in Kazantseva (2006).
A thorough evaluation of the anaphora resolution module would be prohibitively
labor-intensive. We estimated the performance of the module by manually verifying
the results it achieved on two short stories of the training set (Table 1). The error rates
for pronominal anaphora resolution are significantly lower than those for noun phrase
anaphora resolution (15.07% vs. 36.84%). This is not unexpected because resolving noun
phrase anaphora is known to be a very challenging task (Vieira and Poesio 2000). The
results also reveal that referring to characters by pronouns is much more frequent
than by noun phrases?in our case, the ratio of pronominal to nominal expressions is
almost 4:1. This suggests that resolving pronominal anaphoric expressions is crucial to
summarizing short stories.
The GATE Gazetteer, part of this module, also annotates the stories for the pres-
ence of expressions denoting locations. After resolving anaphoric expressions, char-
acters central to each story are selected based on normalized frequency counts taking
anaphoric expressions into account. The output of this module consists of short stories
annotated for the presence of location markers and main character mentions.
4. Selecting Descriptive Sentences Using Aspectual Information
4.1 Linguistic Definition of Aspect
We rely on aspect to select salient sentences that set out the background of a story. In
this paper, the term aspect denotes the same concept as what Huddleston and Pullum
(2002, page 118) call the situation type. The term refers to ?different ways of viewing
the internal temporal consistency of a situation? (Comrie 1976, page 3). Informally, the
aspect of a clause suggests the temporal flow of an event or a state and the speaker?s
position with respect to it.
A general aspectual classification based on Huddleston and Pullum (2002) appears
in Figure 4, with examples for each type.
5 Lappin and Leass (1994) present a rule-based algorithm for resolving pronominal anaphora. The
algorithm suggests the most likely antecedent after taking into account the candidates? syntactic function,
recency, and absence or presence of parallelism and cataphora with the referent. It also enforces
agreement between referent?antecedent pairs.
78
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 4
Aspectual hierarchy after Huddleston and Pullum (2002).
The first distinction is between states and events. Events are processes that go on in
time and consist of successive phases (Vendler 1967, page 99). For instance, an event of
writing an essay consists of writing separate words, correcting, pausing between words,
and so on. A state of understanding each other, on the other hand, does not imply such
compositionality: It remains unchanged throughout the whole period when it is true.
In other words, the meaning of events exhibits a dynamic component, whereas that of
states does not.
Events are further categorized by whether a particular situation lasts for some time
or occurs momentarily. The latter type of events are referred to as achievements, and
events that imply duration are known as processes. For example, the nature of events
such as dropping, stumbling, or recognizing is that they occur instantaneously and,
therefore, are achievements. On the other hand, events such as playing golf or writing
an essay last for some time, so they are processes.
Processes are classified into accomplishments and activities depending on whether
a situation implies an ending (Vendler 1967, page 100). This property is known as
telicity. Reading a book in the context of Example 2a implies that the person finished
reading it: the overall situation is telic. We cannot say that she has read the book in the
first 15 minutes of doing so because the implied ending was not achieved (i.e., the book
has not been read). Such situations are referred to as accomplishments. On the other
hand, playing golf or talking on the phone does not imply that the process must end
with a specific conclusion and the situation is atelic. Such situations are called activities.
In addition, the aspectual type of a clause may be altered by multiplicity, for exam-
ple, repetitions. Consider Examples 2a and 2b.
(2a) She read a book.
(2b) She read a book a day.
Example 2b is referred to as a serial situation (Huddleston and Pullum 2002, page 123).
It is considered to be a state, even though a single act of reading a book would constitute
an event.
79
Computational Linguistics Volume 36, Number 1
Intuitively, stative?and especially serial?situations are more likely to be associ-
ated with descriptions, that is to say, with things that are, or things that were happening
for an extended period (consider He was a tall man vs. He opened the window). The
remainder of Section 4 describes how we identify single and serial stative clauses and
use them to construct summaries.
4.2 Overall System Design
Several system components are responsible for selecting salient background sentences.
A story, annotated for the presence of important entities (as outlined in Section 3), is
parsed with the Connexor Machinese Syntax parser. The sentences are then recursively
split into clauses based on the results of parsing. For the purposes of this project, a
clause is defined as a main verb as identified by the parser (whether finite or non-finite)
with all its complements, including subject, modifiers, and their constituents.
Next, each clause is represented as a vector of features describing its characteristics.
The system offers a choice: a fine-grained or coarse-grained representation. The main
difference between the two is in the level of detail at which each clause is represented.
For instance, a fine-grained feature vector has three different features with seven possi-
ble values to carry tense-related information: tense, is progressive, and is perfect, whereas
a coarse-grained vector carries only one binary feature, is simple past or present.6
Finally, the system selects salient descriptive sentences. Regardless of the granular-
ity of the representation, one may choose between two different procedures for sentence
selection. The first procedure employs machine learning techniques, namely the C5.0
decision tree induction (Quinlan 1992). The second procedure applies a set of manually
created rules that guide the classification process. Section 4.3 gives a motivation for fea-
tures used in each data set. Sections 5.1?5.3 describe the experimental setting. Section 6
presents the results.
The part of the system that selects descriptive sentences is implemented in Python.
4.3 Feature Selection: Description and Motivation
There are two criteria for the selection of features for both representations:
(Criterion 1) a clause should ?talk? about important things, such as characters or
locations
(Criterion 2) a clause should contain background descriptions rather then events
We hypothesize that sentences which satisfy both criteria are good candidates for inclu-
sion in indicative summaries. In other words, a summary that consists of such sentences
would familiarize the reader with important elements of the setting of the story, but
would not reveal the plot.
The features that contribute towards Criterion 1 can be divided into character-
related and location-related. We have designed character-related features to help iden-
tify sentences that focus on characters, not just mention them in passing. These features
are modeled so as to help identify sentences that contain at least one mention of an
important character with a salient grammatical function (e.g., subject). Location-related
6 Furthermore, in this article we refer to a data set annotated with the fine-grained features as the
fine-grained data set, and to the one annotated with the coarse-grained features as the coarse-grained
data set.
80
Kazantseva and Szpakowicz Summarizing Short Stories
Table 2
Description of the features in both data sets.
Fine-grained data set Coarse-grained data set
Type of Number of Number of Number of Number of
features features values features values
Character-related 10 18 4 6
Aspect-related 14 48 6 15
Location-related 2 4 2 4
Other 3 7 3 4
All 29 77 15 29
features are intended to help identify sentences where named entities tagged as loca-
tions by the Gazetteer indeed refer to location names.
Criterion 2 has been introduced to ensure that the selected sentences are back-
ground sentences (as opposed to those relating events) and are therefore suitable for
inclusion in indicative summaries. To this end, the features that contribute towards
Criterion 2 are designed to identify stative clauses and clauses that describe serial situa-
tions. A single unambiguous indicator of aspectual type does not exist, but a number of
verb-related characteristics of the clause may signal or limit its possible aspectual type.
These characteristics include the lexical aspect of the main verb, tense, the presence of
temporal expressions, voice, and certain properties of the direct object. The verb-related
features capture this information in our representation.7
The remainder of this section contains a detailed description of the various types of
features and motivates their inclusion. Table 2 shows how many features contribute
to each criterion, and how many discrete values they have. Appendix A contains a
complete list of features used in both representations, explains how they are computed,
and shows the cardinality of the sets of possible values.
Character-related features.Character-related features help ensure that selected sen-
tences are about one of the important characters in the story. So, this group of features
describes whether a clause contains a character mention and what its grammatical
function is (subject, object, indirect object, or other). Mentions of characters early in the
text tend to contain more salient background information. That is why character-related
features reflect the position of a parent sentence8 relative to the sentence where the
character is introduced. In addition, these features capture the presence of a character
mention that is premodified by a noun phrase. The interest in such mentions is inspired
by the fact that these constructions?appositions?often introduce new entities into the
discourse (Vieira and Poesio 2000). For the same reasons, the system also establishes
whether a character mention is nominal or pronominal (e.g., Jack vs. he), whether it is
used in the genitive case (e.g., Jack?s) and, for common nouns, whether the mention is
accompanied by an indefinite article.
7 It must be mentioned that several researchers have looked into automatically determining various
semantic properties of verbs (Siegel 1998b; Merlo et al 2002). These approaches, however, attempt to
determine properties of verbs viewed in isolation and do not deal with particular usages in the context of
concrete sentences. That is why we cannot directly re-apply that research in determining the aspectual
type of clauses.
8 By parent sentence we mean the sentence from which the clause is taken.
81
Computational Linguistics Volume 36, Number 1
Table 3
Privative featural identification of aspectual classes after Dorr and Olsen (1997).
Aspectual class Telic Dynamic Durative Examples
State + know, believe
Activity + + paint, walk
Accomplishment + + + destroy
Achievement + + notice, win
Location-related features. In discourse such as fiction, not all tokens that the
Gazetteer recognizes as markers of location denote locations. Location-related features
help identify mentions of locations in each clause and verify that these mentions indeed
denote a place. These features describe whether a clause contains a mention of a location
and whether it is embedded in a prepositional phrase. The rationale for these features is
that true location mentions are more likely to occur inside prepositional phrases, such
as from Chicago or to China.
Verb-related features. Verb-related features model the characteristics of a clause
that help determine its aspectual type.
Lexical aspect of a verb. Lexical aspect refers to a property of a verb when viewed
in isolation, without regard to the context provided by a particular clause. Just as for
clauses, a verb may be a state (or stative) verb (e.g., believe), or an event verb (e.g., run).
Event verbs are further subdivided into verbs of activity (e.g., read), accomplishment
(e.g., take a test), and achievement (e.g., drop).
The relation between the lexical aspect of a verb and the aspect of a clause has been
discussed by Vendler (1967), Dorr and Olsen (1997), and Huddleston and Pullum (2002,
pages 118?123). Dorr and Olsen have proposed a privative model of this relation?see
Table 3. The model states that verbs are categorized into aspectual classes based on
whether they exhibit one or more of the following properties: dynamicity, durativity,
and telicity. Dorr and Olsen speculate that, depending on the context of usage, verbs
may form clauses that have more of these properties than the main verb viewed in
isolation, but that it is impossible for a verb to ?shed? one of its properties. We illustrate
this in Examples 3a and 3b. In Example 3a the state verb know participates in an
accomplishment clause; the clause is telic, although the verb by itself is not. On the other
hand, an attempt to deprive the accomplishment verb destroy of its telic meaning when
constructing a clause of type activity fails to create an acceptable clause (Example 3b).
(3a) He knew it that very moment. (accomplishment)
(3b) *He was destroying it for an hour. (activity) 9
The lexical aspect of a verb influences the aspect of a clause. Several features in our
system capture this information. The fine-grained data set contains three features with
six possible values that show whether the main verb of a clause is durative, dynamic, or
telic. The coarse-grained data set contains a single feature with four possible values (the
lexical aspect of a verb according to the model in Table 3). We derive this information
from a manually compiled database of Lexical Conceptual Structures (Dorr and Olsen
1997), which contains these properties for 4,432 English verbs.
9 Throughout this paper, the asterisk (*) denotes incorrect or marginally correct usage.
82
Kazantseva and Szpakowicz Summarizing Short Stories
Grammatical tense. The grammatical tense used in a particular clause places a
number of constraints on its aspectual type. For instance, simple tenses are more
likely to be used in stative or habitual situations than progressive or perfect tenses.
It is also commonly accepted (Dowty 1979; Huddleston and Pullum 2002, page 119)
that stative clauses cannot be realized using progressive tenses (see Examples 4a and
4b). Huddleston and Pullum (2002, page 121) stipulate that it is also the case with
achievement clauses (see Example 4c).
(4a) John is running. (event, activity)
(4b) *John is knowing the answer. (state)
(4c) *John was recognizing her. (event, accomplishment)
Among the constraints that grammatical tense imposes there is the special relation
between simple present tense and event clauses. As a rule, clauses realized in simple
present tense cannot denote events, but only states (Huddleston and Pullum 2002,
page 119). The matter is illustrated in Examples 5a through 7b.
(5a) She knew history well. (state)
(5b) She knows history well. (state)
(6a) She fell off a chair. (event)
(6b) *She falls off a chair. (event)
(7a) She danced (last night). (event)
(7b) She dances. (state)
In the fine-grained data set the information related to tense is expressed using three
features with seven possible values (whether a clause is in present, past, or future tense;
whether it is progressive; and whether it is perfective). In the coarse-grained data set,
this information is expressed using one binary feature: whether a clause is in simple,
past, or present tense.
Temporal expressions. Temporal markers (often referred to as temporal adverbials),
such as usually, never, suddenly, at that moment, and many others, are widely employed
to mark the aspectual type of a sentence (Dowty 1979; Harkness 1987; By 2002). Such
markers provide a wealth of information and often unambiguously signal aspectual
type. For example:
(8a) She read a lot tonight.
(8b) She always read a lot. (or She used to read a lot.)
Such expressions are not easy to capture automatically, however. In order to use the
information expressed in temporal adverbials, we analyzed the training part of the
corpus for the presence of such expressions. There were 295 occurrences in 10 stories.
It turns out that this set can be reduced to 95 templates. For example, the expressions
this year, next year, that long year can all be reduced to a template ?some expression year?.
Possible values of ?time expression? are further restricted to allow only valid modifiers
(e.g., last, next, but not yellow). The system captures temporal expressions using a cas-
cade of regular expression. It first identifies the least ambiguous unit (in this example
83
Computational Linguistics Volume 36, Number 1
year) and then attempts to find the boundaries of the expression. The complete list of
regular expressions used appears in Kazantseva (2006).
Three features characterize each template: the type of the temporal expression
(location, duration, frequency, or enactment) (Harkness 1987); magnitude (year, day,
etc.); and plurality (year vs. years). The fine-grained data set contains three such fea-
tures with 14 possible values (type of expression, its magnitude, and plurality). The
coarse-grained data set contains one binary feature (whether a clause contains an ex-
pression that denotes a long period of time).
Voice. Usually, clauses in passive voice only occur with events (Siegel 1998b,
page 51). Both data sets contain one binary feature that describes this information.
Properties of the direct object. For some verbs, properties of the direct object help
determine whether a given clause is stative or dynamic.
(9a) She wrote a book. (event)
(9b) She wrote books. (state)
It is of particular interest whether the direct object follows a definite or indefinite
determiner and whether it is used in a singular or plural form. Two binary features
that describe this information are included in the fine-grained data set.
Several additional features in both data sets describe the overall characteristics of
a clause and its parent sentence, such as whether these were affirmative statements,
exclamations, or questions; their index in the text; and a few others. The fine-grained
data set contains three such features with seven possible values. The coarse-grained
data set contains three features with four values.
4.4 Handling Clauses with the Verb Have
The preceding section notes that the same verb may form clauses of different aspectual
types depending on its context. A verb with a particularly ambiguous aspect is the
verb have (when used as the main verb and not an auxiliary). Its meaning is strongly
influenced by what kind of direct object it takes. That is why determining its aspectual
type is a very challenging task. This issue is illustrated in Examples 10a?10c.
(10a) She had lunch. (event).
(10b) She had a friend. (state).
(10c) She had an accident. (event).
Due to the high degree of ambiguity, our system handles clauses with have as the
main verb in a manner different from all other clauses. This machinery remains the
same regardless of what options are used for the granularity of representation and for
sentence selection procedures.
In order to handle have-clauses, our system contains an implementation of an ap-
proach proposed by Siegel (1998a). The solution relies on WordNet (Fellbaum 1998) and
contains a set of rules that determine the aspectual type of a have-clause based on the
top WordNet category of the direct object. For instance, the direct object lunch from
Example 11a belongs to the category food and, according to rules from Siegel (1998a),
the aspectual type of a clause is event. The direct object friend from Example 11b belongs
to the category person, so the aspectual type of the clause is state. Siegel (1998a) used
WordNet 1.6, whereas we work with a newer version, WordNet 2.0. The structure of
84
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 5
Pseudo-code for determining the type of have-clauses based on the WordNet category of direct
object (Siegel 1998b).
this newer ontology is different from that of version 1.6. For this reason, we consider
all parent categories in the hypernym tree, not only the top category. For the sake of
completeness, Figure 5 shows the pseudo-code for this procedure. The system judges a
have-clause to be summary-worthy if two conditions are fulfilled: the clause contains a
mention of one or more important characters and it is a state clause.
5. Experiments
5.1 Experimental Setting
The final version of the summarizer proceeds as follows. First of all, the stories are
parsed with the Connexor parser and named entities are recognized using the GATE
Gazetteer. Then the system resolves anaphoric references and identifies important char-
acters and locations. During the next stage, the summarizer splits all source sentences
into clauses and creates coarse- and fine-grained representations for each clause. A
clause is modeled as a vector of character-, location- and verb-related features. Finally,
the system employs two alternative procedures to select summary-worthy sentences:
manually designed rules and machine learning.
We performed a number of experiments to find out how successful our system is
in creating summaries of short stories. The experimental corpus consisted of 47 short
stories split into a training set of 27 stories and a test set of 20 stories. The average length
of a story in the corpus was 3,333 tokens, 244 sentences, or approximately 4.5 U.S.-letter-
sized pages. The corpus contains stories written by 17 different authors. It was split
manually so that its training and test portions contained approximately an equal num-
ber of stories by the same writer. The first author of this paper annotated each clause of
every story for summary-worthiness and achieved the compression rate of 6%, counted
in sentences. This rate was the target compression rate in all further experiments.
The training data set consisted of 10,525 clauses, 506 of which were annotated as
summary-worthy and all others as not summary-worthy. The test data set contained
7,890 clauses, 406 of them summary-worthy.
We fine-tuned the system and used the training portion of the data set to iden-
tify the best settings. Then we ran two sets of experiments on the test portion. In the
first set of experiments, we applied a manually designed set of rules that select sen-
tences for possible inclusion in summaries. These experiments are described in Sec-
tion 5.2. The second set of experiments relied on using machine-learning techniques
to create summaries. It is described in Section 5.3. After the completion of the experi-
ments, the summaries were evaluated by six judges. They were also compared against
85
Computational Linguistics Volume 36, Number 1
Figure 6
Examples of manually designed rules.
extractive summaries produced by three people. Section 6 discusses the evaluation
procedures in detail and reports the results.
5.2 Experiments with Manually Designed Rules
The first classification procedure applies manually designed rules to a clause-level
representation of the original stories to produce descriptive summaries. The rules are
designed using the same features as those used for machine learning and described in
Section 4.3 and in Appendix A.
The first author created two sets of rules to guide the sentence classification process:
one for the coarse-grained and another for the fine-grained representation. The rules
operate at clause level. If a clause is deemed summary-worthy, the complete parent
sentence is included in the summary. Figure 6 displays a few examples of rules for the
fine-grained data set (a clause is considered to be summary-worthy if a rule returns
True). The first rule attempts to select clauses that talk about one of the main characters
and contain temporal expressions of type enactment. The rationale for this rule is that
such clauses are likely to describe habitual activities of protagonists (e.g., He always
smoked.) The second rule follows the same rationale but the stativity of the situation
is signaled by the main stative verb. The third rule rejects clauses in progressive tense
because such clauses are unlikely to contain background information.
The set of rules for the fine-grained representation has a tree-like structure. It
processes the features of a clause and outputs a binary prediction. The rules for the
coarse-grained representation function differently. Each clause is assigned a score based
on the values of its features. The system then selects 6% of sentences that contain clauses
with the highest scores. The scores attributed to the particular feature values were
assigned and fine-tuned manually using linguistic knowledge described in Section 4.3.
The reasons why the procedures for the two data sets differ are as follows. Assigning
and fine-tuning the scores is a more flexible process and it is easier to perform manu-
ally. Ideally, we would apply score-based rules to both representations, but assigning
and fine-tuning the scores manually for the fine-grained data set is excessively labor-
intensive: there are too many features with too many values. For instance, one may
want to reward clauses in simple past or present tenses, reflecting the fact that such
clauses are more likely to be descriptive than those in perfect or progressive tenses.
This information is expressed in the coarse-grained data set using one binary feature
simple past present and fine-tuning the score is trivial. On the other hand, the same
86
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 7
High-level overview of the rules for the fine-grained data set.
information in the fine-grained data set is distributed over three features with a total
of seven values: is perf (yes, no), is progressive (yes, no), and tense (past, present. future).
Distributing the ?reward? among three independent features is far less obvious.
The rules in both data sets, as well as the set of weights used for the coarse-grained
representation, were selected and fine-tuned empirically using the training portion of
the corpus as a guide. Once the parameters had been adjusted, the system produced
two sets of summaries for the test portion of the corpus (one for each representation).
The detailed algorithms for both data sets are too long for inclusion in this article.
Figures 7 and 8 show the rationale for the algorithms. The interested reader is referred
to Kazantseva (2006) for pseudo-code.
5.3 Experiments with Machine Learning
As an alternative to rule construction, in the second set of experiments we performed
decision tree induction with C5.0 (Quinlan 1992) to select salient descriptive sentences.
C5.0 was our choice mainly because of the readability of its output.
The training and test data sets exhibited an almost 1:17 class imbalance (i.e., only 6%
of all annotated clauses belonged to the positive class). Because the corpus was rather
small, we applied a number of techniques to correct class imbalance in the training data
set. These techniques included classification costs, undersampling (randomly removing
instances of the majority class), oversampling (randomly duplicating instances of the
minority class), and synthetic example generation (Chawlar et al 2002). Using tenfold
cross-validation on the training data set and original annotations by the first author,
Figure 8
High-level overview of the rules for the coarse-grained data set.
87
Computational Linguistics Volume 36, Number 1
we selected the best class-imbalance correction techniques for each representation and
also fine-tuned the learning parameters available in C5.0. These experiments brought
the best results when using classification costs for the coarse-grained data set and
undersampling for the fine-grained data set.
In order to see what features were most informative in each data set, we conducted
a small experiment. We removed one feature at a time from the training set and used the
decrease in F-score as a measure of informativeness. The experiment showed that in the
coarse-grained data set the following features were the most informative: the presence
of a character in a clause, the difference between the index of the current sentence and
the sentence where the character was first mentioned, syntactic function of a character
mention, index of the sentence, and tense. In the fine-grained data set the findings are
similar: the index of the sentence, whether a character mention is a subject, the presence
of a character mention in the clause, and whether the character mention is a pronoun
are more important than the other features.
After selecting the best parameters on the training data set using tenfold cross-
validation, the system produced two sets of summaries for the test data set.
6. Evaluation
6.1 Overview
We are not aware of any agreed-upon metrics for evaluating summaries of short fiction.
In fact, it is not wholly clear what makes one summary better than another even for
manual ones. That is why we evaluate our summaries using a variety of metrics and
baselines, hoping to obtain a stereoscopic view of their quality.
The first evaluation experiment aims to measure the informativeness and the use-
fulness of the summaries. It is designed so as to compare the machine-made summaries
with a random baseline and also with a ?ceiling?? manual abstracts (henceforth model
summaries). To achieve this, we engaged 15 evaluators to read the summaries and
the stories of the test set and to answer two types of questions about them: factual
(e.g., list main characters of the story) and subjective (e.g., rank readability of the summary).
Such experimental design allowed us to evaluate extrinsically the informativeness of
the summaries and intrinsically their usefulness. Both types of questions were asked
first after reading the summary alone and then after reading the complete story. The
summaries are an anonymous mix of random, machine-made, and model ones (i.e., the
evaluators did not know whether the summaries were produced by programs or by
people). Section 6.2 describes the experiment in detail.
The second round of evaluation aimed to evaluate the summaries by measuring
sentence co-selection with the manually created extracts. It was designed to allow
the comparison of machine-made summaries with two naive baselines and with two
state-of-the-art generic summarizers (baseline summarizers). Section 6.3 contains the
description of this experiment.
The third evaluation experiment compared the machine-made and the baseline
summaries with the manually created abstracts using ROUGE (Lin 2004)?a package
for automatically evaluating summaries. This experiment is described in Section 6.4.
6.2 Evaluating Informativeness and Usefulness of the Summaries
We define the objectives of this experiment as measuring the informativeness, the
usefulness and, to some extent, the linguistic quality of the summaries that our system
88
Kazantseva and Szpakowicz Summarizing Short Stories
produces. The informativeness is measured indirectly?by asking people factual
questions about the story. The linguistic quality and the usefulness are evaluated
intrinsically?by asking people to rank specific characteristics of the summaries.
Fifteen unbiased evaluators answer both types of questions twice, first after reading
the summary alone and then again after reading the complete story?repeating the
procedure for all 20 test stories. Asking the questions after reading the summary
alone measures the informativeness and the usefulness of the summaries in a realistic
situation: To an evaluator, the summary is the only source of information about the
original story. Repeating the procedure after reading the complete story evaluates the
summaries in a situation where the evaluator has the complete information about
the source. Each evaluator works with a mix of machine-made, random, and model
summaries with six or seven summaries of each kind. This allows comparing the
performance of our summarizer with a baseline and a ceiling.
Our summarizer produces four different flavors of summaries.10 The labor intensity
of the process prohibits asking the subjects to evaluate all four summary types. That is
also why it is not possible to use more than one baseline or the summaries created
by the baseline systems.11 Restricted to evaluating only one type of the machine-made
summaries, we opt for the coarse-grained rule-based ones, mainly because the coarse-
grained representation and the rules make it easier to trace why the system selects
specific sentences.
Conditions to be tested. We evaluate three factual and four subjective characteristics
of the summaries.
Factual
How well the reader can name
 the main characters
 the location
 the time
when the summary is the only source of information about the story.
Subjective
 How readable the summaries are.
 How much irrelevant information they contain.
 How complete they are.
 How useful they are for deciding whether to read the complete story.
Evaluating these facets of the summaries reveals whether we achieve the objective of
producing informative summaries. The focus of the system was not on readability. Still,
we evaluate how readable the summaries are, because severe lack of coherence may
prevent people from correctly interpreting the available information. We have provided
10 The options are as follows: either the coarse-grained or the fine-grained representation, and selecting
sentences using either rules or machine learning.
11 Each additional summary type would have required approximately 67 extra man-hours.
89
Computational Linguistics Volume 36, Number 1
no definitions or criteria to the subjects apart from the questions shown in Tables 4
and 5.
Baselines. We compare the machine-made summaries with a baseline and a ceiling.
The baseline consists of randomly selected sentences. Both the machine-made and the
random summaries contain the same number of sentences. The ceiling consists of the
summaries written by two human subjects. The summary writers were instructed to
write 20 summaries of short stories in a way that does not reveal all of the plot. They
received one example summary and were allowed to reuse sentences from the stories,
to employ metaphor and any other literary devices they found useful.
Metrics. The evaluators answered the factual questions about the main characters
and the location of the story in their own words. The first author rated the answers
on the scale of ?1 to 3. A score of 3 means that the answer is complete and correct,
2 = slightly incomplete, 1 = very incomplete, 0 = the subject cannot find the answer
in the text, and ?1 = the answer is incorrect. The question asking to identify the time
frame of the story is a multiple-choice one: select the century when the story takes place.
The answers to this question are rated on a binary scale (1 if the answer is correct,
0 if it is not or if the subject cannot infer time from the text). We calculate the mean
answers for each question and compare them across summary types using the Kruskal?
Wallis test and the Mann?Whitney test (also known as the Wilcoxon Rank?Sum test).
The tests are appropriate when the response variable is ordinal and the dependent
variable is categorical. Both tests are based on assigning ranks to the available data
points.
The Kruskal?Wallis test is a nonparametric test used to determine whether several
samples come from the same population. It is based on calculating the K statistic which
follows ?2 distribution for sample sizes of five or larger. Given i samples containing
ti data points each with Ri being the sum of ranks of all data points in sample ti, K is
calculated as follows (Leach 1979, page 150):
K = 12
n(n+ 1)
?
ti(
Ri
ti
? n+ 1
2
)2 (1)
In order to make pairwise comparisons between samples and to establish the locus
of the difference, we rely on the Mann?Whitney test. The test is based on calculating the
S statistic. For large sample sizes the distribution of S can be approximated using the
normal distribution.
S = 2R? t2(n+ 1) (2)
where t2 is the size of the smaller sample, n is the size of both samples together, and R
is the sum of ranks in the smaller sample. We use the Kruskal?Wallis test with 0.01 con-
fidence level. In order to avoid increasing the chance of Type I error when performing
pairwise comparisons, we set per-comparison confidence level for the Mann?Whitney
test at ? = ?/c where ? is the desired per-experiment confidence level and c is the
number of comparisons (Leach 1979, page 161). In our case ? = 0.0033.
All subjective questions are multiple-choice questions. An evaluator has to select a
score of 1 to 6, with 1 indicating a strong negative property and 6 indicating a strong
positive property. We opt for a scale with an even number of available values so as
to avoid the evaluators? giving excessive preference to the middle rank. We measure
the mean ranks for each question and compare them across summary types using the
Kruskal?Wallis and Mann?Whitney tests. The inter-annotator agreement is computed
90
Kazantseva and Szpakowicz Summarizing Short Stories
using Krippendorff?s ? (Krippendorff 2004, pp. 221?236) (henceforth ?). ? measures
disagreement between annotators corrected for chance disagreement.
? = 1 ?
Disagreementobserved
Disagreementexpected
= 1 ?
Average metric ?2 within all categories
Average metric ?2 within all items
(3)
Unlike other coefficients of inter-coder agreement, ? allows taking into account the
magnitude of disagreement by specifying a distance metric ?2. This property is crucial
in our case: a situation when raters disagree whether to give a summary a rank of 1 or 6
should be penalized more heavily than a situation when they do not agree between
the ranks of 5 and 6. When computing ?, we use the distance metric suggested by
Krippendorff for ordinal data (Krippendorff 2004, page 223):
ordinal ?2 = (
nc
2
+
g<k
?
g>c
ng +
nk
2
)2 (4)
where c and k, c < k, are the two ranks.
For all questions, the computation of ? is based on the following parameters: N =
300, n = 15, and c = 6, where N is the total number of items (i.e., summary?story pairs
ranked), n is the number of raters, and c is the number of available categories.
Subjects. The participants for the experiment were recruited by the means of adver-
tising at the Department of Linguistics at the University of Ottawa. Most of them are
third- and fourth-year undergraduate students of linguistics. The only requirement for
participation was to be a native speaker of English. We hired two people to create model
summaries for the 20 stories of the test set. The summary writers worked approximately
15?20 hours each. Fifteen people were hired to evaluate the summaries (i.e., to read
the summary?story pairs and answer the questions). The task of evaluating a sum-
mary required approximately 12?15 hours of labor per person. All participants were
paid. The instructions for summary writers are available at www.site.uottawa.ca/
?ankazant/instructions-writers.zip. The instructions for evaluators can be found
at www.site.uottawa.ca/?ankazant/instructions eval.zip.
Material. Each evaluator received 20 summary?story pairs. Because some questions
sought to measure the informativeness of the summary, every evaluator worked on 20
distinct stories of the test set and no one worked with the same story more than once.
The summaries were a randomly selected mix of random, machine-made, and model
summaries.
Procedure. The experiment was conducted remotely. The summary writers received
the test set of stories and the instructions and had seven working days to submit their
abstracts. A week later, we sent randomly generated packages of summary?story pairs
to the evaluators. The packages contained between six and seven summaries of each
kind (random, machine-made, and model). Each evaluator worked with exactly one
summary for each story, reading a total of 20 pairs. Every summary was evaluated by
five subjects. The evaluators had seven working days to complete the task.
Results. Informativeness. Table 4 shows the results of comparing the mean an-
swers between the machine-made, the baseline, and the model summaries using the
Kruskal?Wallis and Mann?Whitney tests. The column Groups shows homogeneous
groups, identified using the Mann?Whitney test with 99.67% confidence (recall that
per-comparison confidence level ? = 0.0033). The groups are denoted using distinct
literals (e.g., A, B, C).
91
Computational Linguistics Volume 36, Number 1
Table 4
Answers to factual questions.
Summary type After reading the summaries only After reading the stories
Mean rank Groups Std. Dev Mean rank Groups Std. Dev
Question: Name three main characters in the order of importance.
Model 2.24 A 0.73 2.73 A 0.49
Machine 2.21 A 0.69 2.71 A 0.56
Random 1.42 B 1.04 2.67 A 0.62
Question: Name the location of the story.
Model 2.1 A 1.25 2.62 A 0.93
Machine 1.39 B 1.33 2.79 A 0.62
Random 0.71 C 1.18 2.43 A 0.98
Question: Select the century when the story takes place.
Model 0.5 A 0.5 0.69 A 0.46
Machine 0.29 B 0.46 0.76 A 0.43
Random 0.19 B 0.39 0.7 A 0.54
The differences between the machine-made summaries and the random ones
are significant for the questions about characters and the location of the story. This
shows that in these respects the machine-made summaries are?rather predictably?
consistently more informative than the random ones. The difference between the
machine-made and the random summaries is not statistically significant for the question
asking to name the time of the story. Keeping in mind how rare absolute temporal
anchors are in short stories, this is not surprising. The manual summaries, however,
are ranked higher with statistical significance. This may suggest that the machine-made
summaries are not as coherent as the model ones, which prevents the reader from
finding implicit cues about timeframe available in the summaries.
The differences between the machine-made and the model summaries are signifi-
cant for the questions about the time and the place of the story, but not for the questions
about the main characters. This suggests that the machine-made summaries are almost
as informative as the model ones when it comes to informing the reader whom the story
is about. They cannot, however, give the reader as good an idea about the time and the
place of the story as the model summaries can.
All summary types are less informative than the complete story; that is, the differ-
ences between answers obtained after reading the summary alone and after reading the
complete story are significant in all cases.
Usefulness and linguistic quality. Table 5 shows mean ranks for the three summary
types, along with the homogeneous groups identified using the Mann?Whitney test,
with 99.67% confidence. The request to rank readability was made only once?after
reading the summary; the request to evaluate the completeness was made only after
reading the complete story. (The corresponding cells in Table 5 are empty.)
The results reveal that the evaluators consistently rank the model summaries as
best, the machine-made summaries as second-best, and the random ones as worst. The
differences between summary types are significant in all cases.
The readability of the machine-made summaries is ranked as slightly better than
average (3.28 on the scale of 1 to 6). For all other questions (relevance, completeness, and
92
Kazantseva and Szpakowicz Summarizing Short Stories
Table 5
Subjective rankings.
Summary type After reading the summaries only After reading the stories
Mean rank Groups Alpha Mean rank Groups Alpha
Question: How readable do you find the summary?
(Is it coherent and easy to read, or confusing and does not make sense?) (scale: 1 to 6)
Model 5.44 A 0.74
Machine 3.28 B
Random 1.89 C
Question: How much irrelevant information does the summary contain?
(useless, confusing information, fragments that do not make sense) (scale: 1 to 6)
Model 5.10 A 0.61 5.24 A 0.62
Machine 2.83 B 2.82 B
Random 1.93 C 1.85 C
Having read the story, do you find that a lot of important information is missing?
Rate how complete you find the summary. (scale: 1 to 6)
Model 5.18 A 0.69
Machine 2.81 B
Random 1.65 C
Imagine that this summary should help you decide whether you would like to read the complete
story. How helpful was the summary for this purpose? (scale: 1 to 6)
Model 5.22 A 0.60 5.11 A 0.63
Machine 2.81 B 2.81 B
Random 1.88 C 1.65 C
usefulness), the machine-made summaries are ranked as slightly worse than average
(around 2.81). This shows that even though the summaries are somewhat useful and
consistently outperform the random baseline, they fall short of the quality of the manual
abstracts. This is hardly surprising given the inherent difficulty of summarizing fiction
and the exploratory nature of this work. It is worth remarking that even the model
summaries do not appear to be perfect: The evaluators ranked them around 5.20, even
though they had significantly worse summaries to compare against. This may suggest
that the task is not easy even for people, let alne for a computer program.
The column labelled Alpha in Table 5 shows the results of measuring the extent to
which the evaluators agree when answering the subjective questions.12 The agreement
is measured using Krippendorff?s ?. The results show substantial agreement but fall
short of the reliability cut-off point of 0.8 suggested by Krippendorff. The failure to
reach such high agreement is hardly surprising: the task of ranking the quality of
the summaries is highly subjective. Instead of asking the subjects to bin items into
a predefined number of categories, the task calls for discretizing a concept which is
12 We have not measured the agreement for factual questions because those questions were answered in the
evaluators? own words and the answers were scored by the first author. To give the reader an idea of
variability of the answers, Table 4 reports standard deviation from the mean.
93
Computational Linguistics Volume 36, Number 1
continuous in nature: the quality of a summary. That is why we interpret the level of
agreement as sufficient for the purpose of evaluating the quality of the summaries.
6.3 Comparing the Machine-Made Summaries and the Manually Created Extracts
Measuring sentence co-selection between extractive summaries created by humans and
those created by automatic summarizers has a long tradition in the text summarization
community (Lin and Hovy 2000; Marcu 2000), but this family of measures has a number
of well-known shortcomings. As many have remarked on previous occasions (Mani
2001; Radev et al 2003), co-selection measures do not provide a complete assessment of
the quality of a summary. First of all, when a summary in question contains sentences
that do not appear in any of the model extracts, one may not be sure that those sentences
are uninformative or inappropriate for inclusion in a summary. In addition, documents
have internal discourse structure and sentences are often inter-dependent. Therefore,
even if a summary contains sentences found in one or more reference summaries, it
does not always mean that it is advisable to include those sentences in the summary in
question.
Sentence co-selection does not explicitly measure the quality of a summary. It does,
however, measure a quality that is objective and easy to pin down: how many sentences
that humans judge summary-worthy are included in the machine-made summary.
Such a metric is a useful complement to the results reported in Section 6.2. It has the
advantage of being easy to interpret and comprehend. It also has a long tradition of
usage which allows us to compare our summarizer?on a familiar scale?with other
summarization systems. That is why we chose co-selection as the basis for comparing
the summaries that our system produces with manually created extracts.13
Overview. The experiment involves six annotators divided into two groups of three.
Each annotator is asked to read 10 short stories and to select 6% of sentences that, in their
opinion, constitute a good indicative summary. In this manner three people annotate
each story of the test set for summary-worthy sentences. We used their annotations as a
gold standard and compared the machine-made summaries against them. In addition,
we used the same gold standard and metrics to evaluate the quality of two baseline
summaries and of two summaries produced by state-of-the art summarization systems.
Conditions to be tested. The purpose of the experiment is defined as measuring how
many sentences found in our system?s summaries and the baseline summaries occur in
the extractive summaries created by the human annotators. We are also interested in
finding out whether our summarizer outperforms the trivial baseline algorithms and
the existing state-of-the-art summarizers fine-tuned to summarizing newswire.
Baselines. In order to evaluate our summarizer comparatively, we defined two naive
baselines and a ceiling. Intuitively, when a person wishes to decide whether to read
a book, she opens it and flips through several pages at the beginning. Imitating this
process, we computed a simple lead baseline consisting of the first 6% of the sentences in
a story. The second baseline consists of 6% of sentences of the story selected at random.
The ceiling consists of all sentences deemed summary-worthy by one of the human
annotators.
13 We decided against using deeper approaches, such as the Pyramid method (Nenkova and Passonneau
2004), factoids (van Halteren and Teufel 2003), and relative utility (Radev and Tam 2003). The reason is
practical: These approaches have an unfortunate disadvantage of being considerably more
labor-intensive than the measures based on sentence co-selection.
94
Kazantseva and Szpakowicz Summarizing Short Stories
It is also necessary to see whether our genre-specific approach shows any improve-
ments over the existing generic state-of-the-art systems put to work on fiction. To this
end, we compared our summarizer with two systems that were top performers in the
Document Understanding Conference (henceforth DUC) 2007, the annual ?competi-
tion? for automatic summarizers. In DUC competitions the summarization systems
are evaluated on a variety of metrics: manually assigned scores (ranking readability,
grammaticality, non-redundancy, referential clarity, focus, and coherence), the pyramid
method (Nenkova and Passonneau 2004), and ROUGE scores (Lin 2004). There is no
unified ranking of the systems? performance, and selecting the best summarizer is
not straightforward. We chose two systems among the top performers in DUC 2007?
GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, O?Leary,
and Conroy 2008; Conroy, Schlesinger, and O?Leary 2007). GISTexter appears to be
the best summarizer according to the scores assigned by the human judges. Apart
from baselines, it is consistently ranked as the best or the second-best system on most
characteristics evaluated by the judges (the only exception is non-redundancy where
GISTexter is ranked eighth). CLASSY, on the other hand, is one of the four top systems
according to ROUGE scores. The scores it received from the human judges are also
quite good.
The main task in DUC 2007 called for creating 250-word summaries from a collec-
tion of newswire articles on a specific topic. Each input collection was accompanied by
a topic statement that briefly explained what the summaries should cover. Therefore,
both CLASSY and GISTexter are geared towards multi-document query-based sum-
marization of newswire?a task dramatically different from that of summarizing short
fiction.14 No adjustments were made to either system to make them more suitable for
summarizing stories. Therefore, the comparison is not wholly fair, but?in the absence
of systems similar to ours?it was the only possibility to compare our summarizer with
the state-of-the-art in the community and to verify whether genre-specific methods are
useful in summarizing fiction.
Evaluation metrics. By combining summaries created by the annotators in several
ways we create three distinct gold-standard summaries. The majority gold-standard
summary contains all sentences selected by at least two judges. It is best suited to give
an overall picture of how similar the machine-made summaries are to the man-made
ones. The union gold standard is obtained by considering all sentences that are judged
summary-worthy by at least one judge. Union summaries provide a more relaxed
measurement. Precision computed on the basis of the union gold standard gives an idea
of how many irrelevant sentences a given summary contains (sentences not selected by
any of the three judges are more likely to prove irrelevant). The intersection summaries
are obtained by combining sentences that all three judges deemed to be important.
Recall measured on the basis of the intersection gold standard says how many of the
most important sentences are included in summaries produced by the system (sentences
selected by all three judges are likely to be the most important ones). All summaries
are compared against each gold-standard summary using precision (P), recall (R), and
equally-weighted F-score (F).
Recall = TP
TP+ FN
(5)
14 GISTexter in particular relies on extensive syntactic and semantic query decomposition and, thus, is at a
disadvantage when no informative query is provided.
95
Computational Linguistics Volume 36, Number 1
Precision = TP
TP+ FP
(6)
F = 2TP
2TP+ FP+ FN
(7)
TP denotes true positives, FP = false positives, TN = true negatives, and FN = false
negatives.
The statistical significance of the differences between various types of summaries
is established using the one-way Analysis Of Variance test (ANOVA) and the Tukey
Honestly Significant Differences test (henceforth Tukey HSD).
ANOVA tests whether the differences between sample means are significant by
comparing variance between samples with total variance within samples:
f =
S 2between/(p? 1)
S 2within/(n? 1)
(8)
where p is the number of samples, n is the total number of observations, Sbetween is the
sum of squared deviations between sample means and the total mean, and Swithin is
the total sum of squared deviations within samples. The f statistic is distributed as F
when the null hypothesis is true (i.e., the differences between sample means are not
significant).
The power of ANOVA extends as far as verifying whether the differences between
sample means are significant overall, but the test does not say anything about differ-
ences between particular pairs of sample means. Tukey HSD is a test that does just that.
It measures q, the studentized range statistic:
q =
Ml ?Ms
SE
(9)
where Ml is the larger of the sample means, Ms is the smaller one, and SE is the standard
error of the data in question. In accordance with our interpretation of the three types of
gold standards, we use the most meaningful measurement for each standard: F-score
for the majority, precision for the union, and recall for the intersection. We also use the
same measurement to set the ceiling for each standard (i.e., by choosing the manually
created extract that compares best on that scale).
Before combining the extracts created by the annotators into gold-standard sum-
maries, we measure how well these people agree among themselves. We estimate
the agreement using Scott?s ? (Scott 1955).15 This coefficient measures the observed
agreement between judges corrected for chance agreement.
? =
Agreementobserved ? Agreementexpected
1 ? Agreementexpected
(10)
15 The coefficient is also known as Siegel and Castellan?s ? (Siegel and Castellan 1988).
96
Kazantseva and Szpakowicz Summarizing Short Stories
Table 6
Inter-annotator agreement on selecting summary-worthy sentences.
Statistic Group 1 Group 2 Average
?(4) 0.52 0.34 0.43
?(3) 0.50 0.34 0.42
Agreementobserved =
1
ic(c? 1)
?
i?I
?
k?K
nik(nik ? 1) (11)
Agreementexpected =
1
(ic)2
?
k?K
n2k (12)
where i is the number of items to be classified in set I, k is the number of available
categories in set K, c is the number of coders, nik is the number of coders who assign
item i to category k, and nk is the total number of items assigned to category k by all
annotators (Artstein and Poesio 2008, pp. 562?563).
Subjects. Six human subjects participated in annotating the test set of stories for the
presence of summary-worthy sentences. These people are colleagues and acquaintances
of the first author. At the time of the experiment none of them was familiar with the
design of the system. Four annotators are native speakers of English and the remaining
two have a very good command of the language.
Materials. The material for the experiment consisted of the 20 stories of the test set.
Three annotators created extractive summaries for each story. In addition, there were
eight distinct automatically produced summaries per story: four summaries produced
by our system, two baseline summaries, and two summaries created by the baseline
systems from DUC.
Procedure. The experiment was conducted by e-mail. The annotators received the
stories and had two weeks to annotate them. The participants reported having taken
10?20 hours to complete the task.
Results. Agreement. Table 6 reports the agreement between the judges within each
group and with the first author of this article. The agreement with the first author is
reported because she created the initial training and test data for experiments. The num-
bers 3 and 4 state whether the statistic is computed only for three subjects participating
in the evaluation or for four subjects (including the first author). As can be seen from
Table 6, the agreement statistics are computed for each group separately. This is because
the sets of stories that they annotated are disjoint. The ?Average? column shows an
average of these figures, to give a better overall idea.
The agreement values in Table 6 are rather low. They fall well below the 0.8 cut-off
point specified by Krippendorff (2004). On a less demanding scale, Landis and Koch
(1977) interpret values in the range of 0.21?0.4 as fair agreement and in the range of
0.41?0.6 as moderate agreement.16 Weak agreement is not surprising: Many researchers
16 Krippendorff?s suggestion refers to ?, rather than Scott?s ?, and Landis and Koch?s scale was created for
Cohen?s ? (Cohen 1960). In our setting, however, the values of ?,? and ? are almost the same.
97
Computational Linguistics Volume 36, Number 1
report that people do not agree well on what sentences constitute a good summary of
a document (Rath, Resnick, and Savage 1961; Salton et al 1997; Lin and Hovy 2003). In
most cases the agreement corresponding to ? of 0.42 would not be sufficient for creating
a resource, but we interpret this level of agreement as acceptable for evaluating a single
facet of the summaries that are also evaluated in other ways.
Co-selection. Tables 7?9 show the results of comparing eight different versions of the
computer-made summaries against the gold-standard summaries produced by people.
In each table, the entry HUMAN corresponds to the summaries created by the annotator
who achieves the highest scores for the corresponding standard. The ?Groups (metric)?
column reports homogeneous groups identified using Tukey HSD with 95% confidence
for the specified metric.
Our system outperforms both baseline algorithms and the baseline summarizers,
but it always falls short of the performance of the best human summary. The improve-
ment margins between the random and the baseline systems? summaries and those
produced by our system are rather wide. The weaker performance of the baseline
summarizers strongly suggests the need for genre-specific methods when summarizing
short fiction.
The differences between the lead summaries and the system-made ones are also
statistically significant, yet they are much narrower. We interpret this as an indication
that the lead baseline is more demanding than the random one when creating indicative
summaries of short fiction.
Table 7
Sentence co-selection between computer- and human-made summaries. Majority gold standard.
Data set Precision Recall F Groups (F)
HUMAN 64.95 84.18 72.41 A
Rules, fine-grained 39.20 53.35 44.47 B
Machine-learning, fine-grained 36.36 49.44 41.26 B
Rules, coarse-grained 35.42 44.39 38.94 B
Machine learning, coarse-grained 35.31 41.81 36.90 B
LEAD 25.50 31.18 27.57 C
CLASSY 7.14 9.08 7.70 D
GISTexter 9.67 6.30 6.91 D
RANDOM 4.10 5.40 4.57 D
Table 8
Sentence co-selection between computer- and human-made summaries. Union gold standard.
Data set Precision Recall F Groups (P)
HUMAN 1 55.61 71.05 A
Rules, fine-grained 56.10 32.11 40.56 BC
Rules, coarse-grained 53.98 29.77 38.04 BC
Machine-learning, fine-grained 52.39 30.66 38.35 C
Machine learning, coarse-grained 49.62 25.06 32.78 C
LEAD 36.76 18.53 24.50 DEF
GISTexter 22.70 6.18 9.28 EFG
CLASSY 19.74 10.23 13.05 FG
RANDOM 12.41 6.40 8.40 G
98
Kazantseva and Szpakowicz Summarizing Short Stories
Table 9
Sentence co-selection between computer- and human-made summaries. Intersection gold
standard.
Data set Precision Recall F Groups (R)
HUMAN 31.32 1.00 45.21 A
Rules, fine-grained 23.11 79.25 34.02 AB
Rules, coarse-grained 21.70 65.42 30.55 BC
Machine learning, coarse-grained 19.66 57.29 26.80 BC
Machine-learning, fine-grained 16.41 56.19 24.00 BC
LEAD 12.37 38.83 17.84 DE
CLASSY 3.58 8.04 4.89 F
GISTexter 3.20 6.06 3.83 F
RANDOM 0.79 1.67 1.06 F
The results also suggest that automatically produced summaries bear some resem-
blance to manual ones. There is no straightforward way to interpret these results as
good or bad in the context of other summarization systems. Firstly, the task is new and
no comparable results exist. Secondly, even though sentence co-selection metrics have
been widely used for evaluating summaries of other genres, different compression rates,
different gold standards, and availability of naturally occurring competitive baselines
(e.g., lead baseline in newswire summarization) make fair comparison difficult. For
example, Marcu (2000, page 214) reports achieving F-score of 76.04 when creating sum-
maries of newswire articles at 10% of their original length. The lead baseline achieves
F-score of 71.89. When summarizing dialogues, Zechner (2002, page 479) reports
weighted accuracy of 0.614 compared to the lead baseline?s performance of 0.438 (the
numbers are averages over five different summary sizes of 5%, 10%, 15%, 20%, and
25%). In this context we interpret the results in Tables 7?9 as suggesting that our genre-
specific system outperforms the naive baselines and two generic summarizers.
6.4 Evaluating Summaries using Lexical Overlap
ROUGE (Lin 2004) is a package for automatically evaluating summaries. Given one
or more gold-standard summaries (usually written by people), ROUGE offers several
metrics for evaluating the summary in question. The metrics reward lexical overlap
between the model summaries and the candidate one. Depending on the metric, the
lexical units taken into consideration are n-grams, word sequences, and word pairs.
Since 2004, ROUGE scores have been among the measures used for evaluating au-
tomatic summarizers at DUC. Following this tradition, we ran ROUGE to evaluate our
summaries and to compare them to the baselines (including CLASSY and GISTexter).
Conditions to be tested. The objective of the experiment was to establish how
much lexical overlap exists between the machine-made and the model summaries. We
achieved this by computing ROUGE-2 and ROUGE-SU4 scores.17
Baselines and material. We evaluated eight types of summaries: four types created
by our summarizer, the lead and the random baselines, and the summaries created by
17 ROUGE-2 and ROUGE-SU4 are two measures that were used at DUC 2007.
99
Computational Linguistics Volume 36, Number 1
GISTexter and CLASSY. In addition, we included a ceiling by computing ROUGE scores
for the model summaries.
Metrics. ROUGE-2 score measures the bigram recall between the reference summary
and the candidate one. It is computed according to the following formula:
ROUGE-2 =
?
s?S
?
b?s Countmatch(b)
?
s?S
?
b?s Count(b)
(13)
where S is the set of reference summaries, b is a bigram in the reference summary s,
Countmatch(b) is the number of bigrams that both summaries share, and Count(b) is the
total number of bigrams in the reference summary s.
ROUGE-S measures the similarity of a pair of summaries based on how many
skip-bigrams they have in common. A skip-bigram is any pair of words in a sentence,
allowing for arbitrary gaps.
ROUGE-S =
SKIP2(X,Y)
C(m, 2)
(14)
where X is the reference summary of length m, Y is the candidate summary, SKIP2(X,Y)
is the number of skip-bigram matches between X and Y, and C is the combination
function.
ROUGE-SU4 is an extension of ROUGE-S that also rewards matching unigrams.
The maximum gap allowed by skip-bigrams is 4 (hence SU4).
In order to compare the automatically produced summaries with those created by
humans we implemented the following leave-one-out procedure. At first, we computed
ROUGE scores by comparing all automatically produced summaries (i.e., those created
by our system and the baseline ones) and one of the model summaries against the
second available model summary. Next, the procedure was repeated but the model
summaries were switched. The significance of the differences was tested using ANOVA
and Tukey HSD for 95% confidence level. When calculating ANOVA and Tukey HSD,
we used the scores obtained from both runs.
Results. Tables 10 and 11 show ROUGE-2 and ROUGE-SU4 scores for all automati-
cally produced and model summaries. The results are inconclusive.
When using ROUGE-2 as a guide, the only summaries consistently different from
the rest with 95% confidence are the randomly generated ones. The scores of all other
summaries are too close to reject the hypothesis that the differences are due to chance.
This is the case even with the differences between the model and the automatically
produced summaries. A possible interpretation could be that all summaries are of
very high quality that is indistinguishable from that of the model summaries. This
hypothesis, however, can be easily dismissed: The results reported in Sections 6.2 and
6.3 clearly show that the quality of the summaries produced by our system is well below
the ceiling.
The situation is similar with ROUGE-SU4 scores, if not so dramatic. There are three
distinct groups of summaries. Group A includes the rule-based fine-grained summaries
and those produced by CLASSY. The second group includes the lead baseline, three
types of summaries created by our summarizer, the model summaries, and those cre-
ated by GISTexter. The last group contains the random and the lead baselines. Even
though ROUGE-SU4 measurement seems to have more discriminative power, it is at
least puzzling that it cannot distinguish between the model and the automatically
100
Kazantseva and Szpakowicz Summarizing Short Stories
Table 10
ROUGE-2 recall scores.
System ROUGE-2 Groups
HUMAN-1 0.0874 A
HUMAN-2 0.0807 A
Rules, fine-grained 0.0981 A
Machine learning, fine-grained 0.0905 A
GISTexter 0.0829 A
CLASSY 0.0826 A
Rules, coarse-grained 0.0816 A
Machine learning, coarse-grained 0.0808 A
LEAD 0.0572 AB
RANDOM 0.038 B
Table 11
ROUGE-SU4 recall scores.
System ROUGE-2 Groups
Rules, fine-grained 0.1684 A
CLASSY 0.1654 A
GISTexter 0.1607 AB
Rules, coarse-grained 0.1564 AB
HUMAN-1 0.1540 AB
Machine learning, coarse-grained 0.1468 AB
HUMAN-2 0.1426 AB
Machine learning, fine-grained 0.1584 AB
LEAD 0.127 BC
RANDOM 0.0956 C
produced summaries. In particular, placing the rule-based coarse-grained summaries
and the model ones in the same group directly contradicts the results reported in
Section 6.2?that people find the model summaries far superior to this particular type
of summary produced by our summarizer.
We interpret these results as suggesting that the ROUGE-2 and ROUGE-SU4 scores
are not well suited for evaluating indicative summaries of short stories. An explana-
tion could be that when people summarize fiction?rather than newswire or scientific
papers?they seem to use fewer sentences and clauses verbatim and, by and large, in-
troduce more generalization and abstraction. (We have made this informal observation
when processing the model summaries used in this experiment.) This results in little
lexical overlap with the source text and hence with extractive summaries of any flavor.
This hypothesis, however, is only preliminary and requires further investigation.18
18 It is possible that significantly increasing the number of model summaries would alleviate the problem.
Unfortunately, obtaining so many model summaries was prohibitively expensive in our case. To move in
this direction, we ran ROUGE without jackknifing to enable the use of two model summaries for
comparison. The results were similar to those reported in Tables 10 and 11: Only the random summaries
are consistently significantly worse than the rest.
101
Computational Linguistics Volume 36, Number 1
7. Conclusions
We presented an approach to summarizing literary short stories. The text sum-
marization community has not yet seriously explored this genre, except for early sem-
inal work on story understanding. In contrast with the story-understanding systems
proposed in the 1970s and 1980s, our system does not require labor-intensive semantic
resources?knowledge-bases and schemas?and it works on real-life stories, namely,
short fiction.
The summaries that the system produces, limited in scope, are intended to help
readers form adequate expectations about the original story. We have demonstrated
that such summaries can be produced without deep semantic resources, only relying
on syntax and the information about important entities in the story. According to the
judges who evaluated the summaries, our summaries are somewhat useful for their
original purpose, even if their quality falls far short of the quality of manual abstracts.
Our summaries appear better than the naive baselines and than two state-of-the-art
summarizers fine-tuned for working with newswire.
In the course of this work we have made a number of observations about automatic
summarization of short stories. First of all, we confirmed informally that characters
tend to be a central element of short fiction. Character mentions provide a wealth
of information that can be leveraged in automatic summarization. This finding was
also reflected in the approach proposed by Lehnert (1982). In addition, it appears that
position in text is important, as can be seen from the analysis of the usefulness of
features in Section 5.3. Besides, relatively high performance of the lead baselines also
suggests that position in text is a good indicator of salience, even though it plays a
lesser role than in more structured documents.
We view this work as a small step towards creating tools for searching, summa-
rizing, and otherwise processing fiction available electronically. The current system
accomplishes with some success a limited task of producing indicative summaries or
short stories, but much more work is needed to create high-quality flexible summaries of
literary works suitable for more than one purpose. Perhaps the most obvious extension
to the current system would be summarizing the plot of short stories. Although this is
not useful given our original criterion (forming adequate expectations about the story,
without ?spoilers?), the ability to handle plot would allow the creation of different
types of summaries. We also hope to explore the possibility of establishing structure
within stories: Knowing that certain portions of a story lay out the setting while
others describe events or the culmination would be a significant step towards better
summarization.
Evaluation of summaries of literary work is yet another dimension of the task that
needs to be considered. We have concentrated thus far on summary production rather
than on establishing the criteria that define the quality of the summary. Evaluation
of summaries remains an issue even where well-structured factual documents are
concerned. In fiction, it is far less clear what contributes towards the quality of the
summary: The facts, for instance, are likely to be less important than in scientific papers
or news items. Other candidate qualities may include closeness to the language or the
tone of the original story, the information about the author, the time period, or ideology
behind a certain work of fiction. This remains an open question, the answer to which
may well lie outside the field of computational linguistics.
102
Kazantseva and Szpakowicz Summarizing Short Stories
Appendix A: Features Used in the Coarse- and the Fine-Grained
Clause Representations
The appendix lists features computed to represent a clause in the fine-grained data set
(Table 12) and in the coarse-grained data set (Table 13). Prior to constructing feature
vectors, the stories are parsed with the Connexor Machinese Parser. All syntactic infor-
mation is computed on the basis of the parser output. The ?Category? column shows
whether a feature is character-related (C), location-related (L), aspect-related (A ), or
other (O). LCS refers to the database of Lexical Conceptual Structures (Dorr and Olsen
1997).
Features representing a clause in the fine-grained data set.
Table 12
Name Category Possible Description Default
values value
char if ind obj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is indirect object
no
char if obj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is direct object
no
char if subj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is subject
no
char in sent C yes, no yes if the parent sentence contains a
mention of a character
no
char indef C def, indef def if the clause contains a mention
of a character and a) it is a proper
name or b) it is modified by a
definite determiner or a pronoun;
indef if the mention is modified by
an indefinite determiner
n/a
char is attr C yes, no yes if the mention of a character is in
the genitive case
n/a
char mention C yes, no yes if the clause contains a mention
of a character
no
char modified C yes, no yes if the mention of a character is
modified by a noun phrase
n/a
char pronoun C 1st, 3rd 1st if the clause contains a
pronominal mention of a character
and it is in 1st person (e.g., I); 3rd if
the pronominal mention is in 3rd
person (e.g., he)
n/a
nbr after
first mention
C continuous an integer that reflects the difference
between the index of the current
sentence and the sentence where the
character is first mentioned (it is
only defined for clauses containing
mentions of characters)
-1
loc in prep L yes, no yes if the clause contains a mention
of a location and is embedded in a
prepositional clause
no
103
Computational Linguistics Volume 36, Number 1
Table 12
(continued)
Name Category Possible Description Default
values value
loc present L yes, no yes if the clause contains a mention
of a location
no
durative A yes, no yes if the main verb of the clause is
durative; this information is
computed using LCS
no
dynamic A yes, no yes if the main verb of the clause is
dynamic; this information is
computed using LCS
no
modal A can, could,
shall, should,
would, must,
may, might,
dare, need, will,
ought, canst
a modal verb from the list, if it
appears in the clause
n/a
neg A yes, no yes if the main verb of the clause is
negated
no
obj def A yes, no no if the direct object of the main
verb is modified by an indefinite
determiner; yes in all other cases
where a direct object is present
n/a
obj plur A yes, no yes if the direct object of the verb is
in plural; no in all other cases where
a direct object is present
n/a
passive A yes, no yes if the clause is realized in passive
voice
no
perf A yes, no yes if the clause is realized in a
perfect tense
no
progr A yes, no yes if the clause is realized in a
progressive tense
no
telic A yes, no yes if the main verb of the clause is
telic; this information is computed
using LCS
no
tense A past, present,
future
the tense used in the clause n/a
tmp magn A min, hour, day,
week, month,
year, year plus
the magnitude of the core temporal
unit in the expression (defined for
clauses containing temporal
expressions and assigned using a set
of manually designed templates):
min if the core unit denotes a period
of no more than a minute (e.g., in a
few seconds, that moment); hour if it
denotes a period of no more than an
hour (e.g., during those hours, at
10 am); the values day through year
are assigned analogously, and
year plus denotes periods longer
than a year (e.g., for decades)
n/a
tmp plur A yes, no yes if the core temporal unit in the
expression is in plural (e.g., during
those years), no if it is singular
(e.g., that day); defined for clauses
containing temporal expressions
n/a
104
Kazantseva and Szpakowicz Summarizing Short Stories
Table 12
(continued)
Name Category Possible Description Default
values value
tmp type A location,
duration,
frequency,
enactment,
temporal
manner
the type of the expression (defined
for clauses containing temporal
expressions, and assigned using a
set of manually designed templates):
all values except temporal manner are
assigned according to the
classification of temporal
expressions available in the
linguistic literature (Harkness 1987),
for example today (location), during
those hours (duration), every day
(frequency), never (enactment);
temporal manner is a separate
pseudo-category defined to include
expressions such as immediately,
instantly etc.
n/a
clause type O assertive,
imperative,
infinitive,
subjunctive
the form of the main verb in the
clause as output by the parser:
imperative for clauses realized in the
imperative mood, subjunctive for
those realized in subjunctive,
infinitive for infinitival clauses (e.g.,
He decided to go), assertive otherwise
assertive
nbr of sent O continuous the index of the parent sentence in
text
-1
sent type O exclaim,
question,
assert
exclaim for clauses that are
exclamations, question for those
that are questions, and assert for all
others
assert
Features representing a clause in the coarse-grained data set.
Table 13
Name Category Possible Description Default
values value
char in clause C yes, no yes if the clause contains a mention
of a character
no
is subj obj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is subject or direct object
no
modified by np C yes, no yes if the mention of a character is
present in the clause and it is
modified by a noun phrase
n/a
nbr after
first mention
C continuous an integer that reflects the difference
between the index of the current
sentence and the sentence where the
character is first mentioned (only
defined for clauses containing
mentions of characters)
-1
105
Computational Linguistics Volume 36, Number 1
Table 13
(continued)
Name Category Possible Description Default
values value
loc in prep L yes, no yes if the clause contains a mention
of a location embedded in a
prepositional clause
no
loc present L yes, no yes if the clause contains a mention
of a location
no
default aspect A state, activity,
accomp, achieve
default lexical aspect of the main
verb in the clause; computed
according to the privative model
defined in (Dorr and Olsen 1997)
n/a
has modal A yes, no yes if the clause contains a modal
verb
no
past perfect A yes, no yes if the clause is realized in past
perfect tense
no
politeness
with be
A yes, no yes if the clause contains one of the
following expressions: to be sorry, to
be delighted, to be glad, to be sad; the
feature is designed to help capture
politeness expressions (e.g., I am glad
to see you)
no
simple past
present
A yes, no yes if the clause is realized in simple
present or past tense
no
tmp exp
long duration
A no, long, short long if the clause contains a temporal
expression denoting a long period of
time, short if it contains an
expression denoting a short period
of time and no otherwise
no
is assertive
clause
O yes, no no if the clause is not an assertion yes
is assertive
sent
O yes, no no if the parent sentence is not
an assertion
yes
nbr of sent O continuous the index of the parent sentence
in text
-1
Acknowledgments
We are grateful to Connexor Oy and
especially to Atro Voutilainen for permission
to use the Connexor Machinese Syntax
parser free of charge for research purposes.
We thank John Conroy and Judith
Schlesinger for running CLASSY on our test
set, and Andrew Hickl for doing it with
GISTexter. Ana Arregui helped us recruit
students for the evaluation. Many thanks to
the annotators, summary writers, and raters,
who helped evaluate our summarizer. A
special thank-you goes to the anonymous
reviewers for Computational Linguistics for all
their incisive, insightful, and immensely
helpful comments. Support for this work
comes from the Natural Sciences and
Engineering Research Council of Canada.
References
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for computational
linguistics (survey article). Computational
Linguistics, 34(4):555?596.
Bartlett, Frederic C. 1932. Remembering: A
Study in Experimental and Social Psychology.
Cambridge University Press, London.
Barzilay, Regina and Kathleen R. McKeown.
2005. Sentence fusion for multidocument
news summarization. Computational
Linguistics, 31(3):297?239.
Branavan, S. R. K., Pawan Deshpande, and
Regina Barzilay. 2007. Generating a
table-of-contents. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 544?551,
Prague.
106
Kazantseva and Szpakowicz Summarizing Short Stories
By, Thomas. 2002. Tears in the Rain. Ph.D.
thesis, University of Sheffield.
Carenini, Giuseppe, Raymond Ng, and
Adam Pauls. 2006. Multi-document
summarization of evaluative text. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 305?313,
Trento.
Charniak, Eugene and Robert Goldman.
1988. A logic for semantic interpretation.
In Proceedings of the 26th Annual Meeting of
the Association for Computational Linguistics,
pages 87?94, State University of New York
at Buffalo, Buffalo, NY.
Chawlar, Nitesh V., Kevin W. Bowyer,
Lawrence O. Hall, and W. Philip
Kegelmeyer. 2002. SMOTE: synthetic
minority over-sampling techniques.
Journal of Artificial Intelligence Research,
16:321?357.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Comrie, Bernard. 1976. Aspect. Cambridge
University Press, London.
Conroy, John M., Judith D. Schlesinger, and
Diane O. O?Leary. 2007. CLASSY 2007 at
DUC 2007. In Proceedings of the Document
Understanding Conference 2007, New York.
Available at http://duc.nist.gov/pubs/
2007papers/ida-umd.final.pdf.
Cullingford, R. E. 1978. Script Application:
Computer Understanding of Newspaper
Stories. Ph.D. thesis, Department of
Computer Science, Yale University.
Cunningham, Hamish, Diana Maynard,
Kalina Bontcheva, and Valentin Tablan.
2002. GATE: an Architecture for
Development of Robust HLT
applications. In Proceedings of the 40th
Anniversary Meeting of the Association for
Computational Linguistics, pages 168?175,
Philadelphia, PA.
D??az, Alberto and Pablo Gerva?s. 2007.
User-model based personalized
summarization. Information Processing and
Management, 43(6):1715?1734.
Dorr, Bonnie J. and Mari Broman Olsen.
1997. Deriving verbal and compositonal
lexical aspect for NLP applications. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics and
8th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 151?158, Madrid.
Dowty, David. 1979. Word Meaning and
Montague Grammar. D. Reidel Publishing
Company, Dordrecht.
Dyer, Michael G. 1983. In-Depth
Understanding: A Computer Model of
Integrated Processing for Narrative
Comprehension. MIT Press,
Cambridge, MA.
Elhadad, Noemie, Min-Yen Kan, Judith
Klavans, and Kathleen McKeown. 2005.
Customization in a unified framework for
summarizing medical literature. Journal of
Artificial Intelligence in Medicine,
33(2):179?198.
Fellbaum, Christiane. 1998. WordNet: An
Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fuentes, Maria, Edgar Gonza`lez, Horacio
Rodr??gue, Jordi Turmo, and Laura Alonso
i Alemany. 2005. Summarizing
spontaneous speech using general text
properties. In Proceedings of International
Workshop Crossing Barriers in Text
Summarization Research, at Recent Advances
in Natural Language Processing 2005,
pages 10?18, Borovetz.
Harabagiu, Sandra, Andrew Hickl, and
Finley Lacatusu. 2007. Satisfying
information needs with multi-document
summaries. Information Processing and
Management, 43(6):1619?1642.
Harkness, Janet. 1987. Time adverbials in
English and reference time. In Alfred
Schopf, editor, Essays on Tensing in English,
Vol. I: Reference Time, Tense and Adverbs.
Max Niemeyer, Tu?bingen, pages 71?110.
Huddleston, Rodney D. and Geoffrey K.
Pullum. 2002. The Cambridge Grammar of
the English Language. Cambridge
University Press, New York.
Kazantseva, Anna. 2006. Automatic
summarization of short stories. Master?s
thesis, University of Ottawa. Available at
www.site.uottawa.ca/?ankazant/pubs/
thesis.tar.gz.
Krahmer, Emiel, Erwin Marsi, and Paul van
Pelt. 2008. Query-based sentence fusion is
better defined and leads to more preferred
results than generic sentence fusion. In
Proceedings of ACL-08: HLT, Short Papers,
pages 193?196, Columbus, OH.
Krippendorff, Klaus. 2004. Content Analysis.
An Introduction to Its Methodology. Sage
Publications, Thousand Oaks, CA.
Landis, J. Richards and Garry G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics,
33(1):159?174.
Lappin, Shalom and Herbert Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):535?561.
107
Computational Linguistics Volume 36, Number 1
Leach, Chris. 1979. Introduction to Statistics.
A Nonparametric Approach for the Social
Sciences. John Wiley and Sons, New York.
Leake, David. 1989. Anomaly detection
strategies for schema-based story
understanding. In Proceedings of the
Eleventh Annual Conference of the
Cognitive Science Society, pages 490?497,
Ann Arbor, MI.
Lehnert, Wendy G. 1982. Plot units: A
narrative summarization strategy. In
Wendy G. Lehnert and Martin H. Ringle,
editors, Strategies for Natural Language
Processing. Erlbaum, Hillsdale, NJ,
pages 375?414.
Lin, Chin-Yew. 2004. ROUGE: A package
for automatic evaluation of summaries.
In Marie-Francine Moens and Stan
Szpakowicz, editors, Text Summarization
Branches Out: Proceedings of the ACL-04
Workshop, pages 74?81, Barcelona.
Lin, Chin-Yew and Eduard Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings of
the 18th Conference on Computational
Linguistics, pages 495?501, Morristown, NJ.
Lin, Chin-Yew and Eduard Hovy. 2003.
Automatic evaluation of summaries using
n-gram co-occurrence statistics. In Marti
Hearst and Mari Ostendorf, editors,
HLT-NAACL 2003: Main Proceedings,
pages 150?157, Edmonton.
Mandler, George. 1987. Determinants of
recognition. In E. van Der Meer and
J. Hoffman, editors, Knowledge-Aided
Information Processing. North Holland,
Amsterdam.
Mani, Indejeet. 2001. Automatic
Summarization. John Benjamins B.V.,
Amsterdam.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
The MIT Press, Cambridge, MA.
McDonald, Ryan. 2006. Discriminative
sentence compression with soft syntactic
evidence. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics,
pages 297?305, Trento.
Mei, Qiaozhu and ChengXiang Zhai. 2008.
Generating impact-based summaries
for scientific literature. In Proceedings
of ACL-08: HLT, pages 816?824,
Columbus, OH.
Merlo, Paola, Suzanne Stevenson, Vivian
Tsang, and Gianluca Allaria. 2002. A
multilingual paradigm for automatic
verb classification. In Proceedings
of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 207?214, University of
Pennsylvania, Philadelphia, PA.
Mihalcea, Rada and Hakan Ceylan. 2007.
Explorations in automatic book
summarization. In Proceedings of the
2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL), pages 380?389,
Prague.
Moens, Marie-Francine. 2007. Summarizing
court decisions. Information Processing and
Management, 43(6):1748?1764.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The Pyramid Method.
In Proceedings of the Human Language
Technology Conference and North American
Chapter of the Association for Computational
Linguistics Annual Meeting, pages 145?152,
Boston, MA.
Nomoto, Tadashi. 2007. Discriminative
sentence compression with random
conditional fields. Information Processing
and Management, 43(6):1571?1587.
Norvig, Peter. 1989. Marker passing as a
weak method for text inferencing.
Cognitive Science, 13(4):569?620.
Propp, Vladimir. 1968. Morphology of the
Folk Tale. Indiana University Press,
Bloomington, IN, 2nd edition.
Quinlan, J. Ross. 1992. C4. 5: Programs for
Machine Learning. Morgan Kaufmann,
San Mateo, CA.
Radev, Dragomir and Daniel Tam. 2003.
Summarization evaluation using relative
utility. In Proceedings of the 12th
International Conference on Information and
Knowledge Management, pages 508?511,
New York, NY.
Radev, Dragomir R., Simone Teufel, Horacio
Saggion, Wai Lam, John Blitzer, Hong Qi,
Arda C?elebi, Danyu Liu, and Elliott
Drabek. 2003. Evaluation challenges in
large-scale document summarization. In
Erhard Hinrichs and Dan Roth, editors,
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 375?382, Sapporo.
Rath, G. J., A. Resnick, and T. R. Savage.
1961. The formation of abstracts by the
selection of sentences. American
Documentation, 2(12):139?143.
Reeve, Lawrence H., Hyoil Han, and Ari D.
Brooks. 2007. The use of domain-specific
concepts in biomedical text
summarization. Information Processing and
Management, 43(6):1765?1776.
108
Kazantseva and Szpakowicz Summarizing Short Stories
Rumelhart, David E. 1975. Notes on a schema
for stories. In Daniel G. Bobrow and Allan
Collins, editors, Representation and
Understanding. Studies in Cognitive Science,
pages 221?237. Academic Press, New York.
Salton, Gerard, Amit Singhal, Mandar Mitra,
and Chris Buckley. 1997. Automatic text
structuring and summarization.
Information Processing and Management,
33(2):193?207.
Schlesinger, Judith D., Dianne P. O?Leary,
and John M. Conroy. 2008. Arabic/English
multi-document summarization with
CLASSY - The past and the future. In
Computational Linguistics and Intelligent Text
Processing, 9th International Conference,
CICLing 2008, pages 568?581, Haifa.
Scott, William. 1955. Reliability of content
analysis: The case of nominal scale coding.
Public Opinion Quarterly, 19(3):321?325.
Siegel, Eric V. 1998a. Disambiguating verbs
with the WordNet category of the direct
object. In Usage of WordNet in Natural
Language Processing Systems Workshop,
pages 9?15, Universite? de Montre?al.
Siegel, Eric V. 1998b. Linguistic Indicators for
Language Understanding: Using Machine
Learning Methods to Combine Corpus-Based
Indicators for Aspectual Classification of
Clauses. Ph.D. thesis, Columbia University,
New York.
Siegel, Sidney and John. N. Castellan, Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. McGraw Hill,
Boston, MA.
Soricut, Radu and Daniel Marcu. 2007.
Abstractive headline generation using
wild-expressions. Information Processing
and Management, 43(6):1536?1548.
Tapanainen, Pasi and Timo Ja?rvinen. 1997.
A non-projective dependency parser.
Proceedings of the 5th Conference on Applied
Natural Language Processing, pages 64?71,
Washington, DC.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and rhetorical
status. Computational Linguistics,
28(4):409?445.
Thorndyke, Perry W. 1975. Cognitive
Structures in Human Story Comprehension
and Memory. Ph.D. thesis, Stanford
University.
van Dijk, Teun A. 1980. Macrostructures. An
Interdisciplinary Study of Global Structures
in Discourse, Interaction, and Cognition.
Laurence Erlbaum Associates,
Hillsdale, NJ.
van Dijk, Teun A. and Walter Kintsch. 1978.
Cognitive psychology and discourse:
Recalling and summarizing stories. In
Wolfgang U. Dressler, editor, Current
Trends in Textlinguistics. Walter de Gruyter,
New York, pages 61?79.
van Halteren, Hans and Simone Teufel.
2003. Examining the consensus between
human summaries: initial experiments
with factoid analysis. In Dragomir
Radev and Simone Teufel, editors,
HLT?NAACL 2003 Workshop: Text
Summarization (DUC03), pages 57?64,
Edmonton.
Vendler, Zeno. 1967. Linguistics in Philosophy.
Cornell University Press, Ithaca, NY.
Vieira, Renata and Massimo Poesio. 2000. An
empirically based system for processing
definite descriptions. Computational
Linguistics, 26(4):539?593.
Zechner, Klaus. 2002. Automatic
summarization of open-domain
multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
109

Last Words
Failure Is an Orphan (Let?s Adopt)
Stan Szpakowicz?
SITE, University of Ottawa &
ICS, Polish Academy of Sciences
Sweet Smell of Success
We like to think that the progress of science follows an exponential curve, boldly shoot-
ing upwards, ever more speedily. In reality, there is a firmament of darkness (that?s our
daily slog) sparsely spottedwith dim lights (those are discoveries). Fittingly, discoverers
are also few, just as in every field of human endeavor. For every great leader, artist, or
scientist there is a multitude of followers, and this is as it should be. There can only be
so many winners?and only so much winning research. Anecdotal evidence has it that
one project in a hundred gives back: Its results justify the investment.1
And yet, our cocky attitude: We tend to count ourselves among that one hundredth.
We need success. We simply must excel. That is, after all, how scientists are judged.
Those of us who have sat on hiring or promotion committees know how evaluation
works; surely, we all get to be at the receiving end. One needs many publications for
a thriving career, and we publish if we show how we go one better on someone else?s
result. So, we cannot afford a setback, a path wrongly taken, a poor result. There is no
room for that in our line of work.
This already sounds dismal, but it gets worse. People have come to equate every
negative result with failure. Suppose you have set up an experiment carefully and
in good faith, but still it comes up short. That?s not a positive outcome. Maybe your
intuition has let you down. Maybe this cannot work. Wait, maybe you can prove that it
cannot work? No, forget it. You have already failed. Don?t waste any more time. Cut
your losses and move on.
Well, reconsider! You can miss a truly fundamental lesson. In 1887, Michelson and
Morley set out to prove the existence of ether, thought to be a conduit for light. They
could not prove it: they failed. But?their null result prompted a line of research which
culminated in the theory of special relativity.
House of Peers
Peer review breedsmediocrity. Galileos, Brunos, Modiglianis, and van Goghs go against
the grain; the scorn of their peers and the indifference of the public crush them.
An obliging self-promoter will fare much better. Peer review also censures failure. A
? School of Information Technology and Engineering, University of Ottawa, 800 King Edward Avenue,
Ottawa, Ontario, K1N 6N5, Canada. Institute of Computer Science, Polish Academy of Sciences, Ordona 21,
01-237 Warszawa, Poland. E-mail: szpak@site.uottawa.ca.
1 As Einstein famously said: ?I think and think for months and years. Ninety-nine times, the conclusion is
false. The hundredth time I am right.?
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
forthright admission of the inferiority of one?s results?despite the integrity or novelty
of the work?is a kiss of death: no publication. There must be improvement, however
minuscule.2 By the ?law of averages,? caution tends to trump adventurousness, and
certainly conformance to reviewers? expectations is an asset. Indeed, we write so they
are likely to accept.
Now, reviewers are us. If we the authors believe that negative results are always and
inescapably a bad thing, so do we the reviewers. If we the authors relentlessly strive for
success, we the reviewers check papers for signs of success. Serves us all right: If we
have lost, we can?t win.
Can?t we?
Philosopher?s Stone
Good science engenders good results. An experiment carefully thought out, a systematic
procedure, an honest evaluation?these are the ingredients. It is not mandatory for the
results to be positive, though it certainly lifts one up if they are.
In areas where empirical methods dominate?computational linguistics is such a
discipline?it is a given that people try things which fail at the experimental stage.
This may be due to lack of rigor, but often there are deeper, unexpected, and intriguing
reasons. We can learn a lot if we analyze scientifically why an intuitive and plausible
experiment did not work. We know how important counter-examples are in rejecting
conjectures. And if a negative result points to an interesting and important open prob-
lem, it ought to be explored. Then again, to know what leads to dead ends in research
surely can warn others off paths which take us nowhere.
An interesting negative result may arise from a compelling idea proven wrong in a
thorough and well-conducted experiment. This is valuable to the community if you can
explain confidently why you have tried out this idea, and if people can replicate your
experiment; the setback is an educational experience insofar as you convince others that
the expected outcome just could not materialize. Simply put, a negative result can be a
useful lesson.3
Acknowledgments
This note owes a great deal to Vivi Nastase.
Not only did she put me on the path toward
it, but she kept me on my toes. Her many
incisive comments helped this little paper
take its present shape.
2 A score will double if just a hundred researchers increase it, one by one, by a mere 0.7%.
3 Not everyone is loath to accept negative results. Journal of Negative Results in BioMedicine (www.jnrbm.com)
welcomes articles which contradict current tenets, to better support scientists and physicians in making
clinical decisions. Journal of Negative Results ? Ecology and Evolutionary Biology (www.jnr-eeb.org) offers
an outlet to studies which seem uneventful (as a reaction to publications in the field which only accept
studies with noteworthy results) to avoid missing important phenomena in nature. Journal of Articles in
Support of the Null Hypothesis (www.jasnh.com) publishes papers on experiments which do not pass the
traditional significance level threshold, to avoid replicating empirical work already examined. Closer to
home, there is WWWW: the AAAI Spring Symposia hosted in 2006 and 2008 workshops dedicated to
?What Went Wrong and Why: Lessons from AI Research and Applications? (www.aaai.org/Press/
Reports/Symposia/Spring/ss-06-08.php), www.aaai.org/Press/Reports/Workshops/ws-08-14.php.
And if you want to publish a serious, worthwhile negative result in computational linguistics, there is
Journal of Interesting Negative Results in NLP and ML (www.jinr.org).
158
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 211?220,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Topical Segmentation: a Study of Human Performance
and a New Measure of Quality
Anna Kazantseva
School of Electrical Engineering
and Computer Science,
University of Ottawa
ankazant@eecs.uottawa.ca
Stan Szpakowicz
School of Electrical Engineering
and Computer Science,
University of Ottawa &
Institute of Computer Science,
Polish Academy of Sciences
szpak@eecs.uottawa.ca
Abstract
In a large-scale study of how people find top-
ical shifts in written text, 27 annotators were
asked to mark topically continuous segments
in 20 chapters of a novel. We analyze the re-
sulting corpus for inter-annotator agreement
and examine disagreement patterns. The re-
sults suggest that, while the overall agree-
ment is relatively low, the annotators show
high agreement on a subset of topical breaks
? places where most prominent topic shifts
occur. We recommend taking into account
the prominence of topical shifts when evalu-
ating topical segmentation, effectively penal-
izing more severely the errors on more impor-
tant breaks. We propose to account for this in a
simple modification of the windowDiff metric.
We discuss the experimental results of evaluat-
ing several topical segmenters with and with-
out considering the importance of the individ-
ual breaks, and emphasize the more insightful
nature of the latter analysis.
1 Introduction
Topical segmentation is a useful intermediate step
in many high-level NLP applications such as in-
formation retrieval, automatic summarization and
question answering. It is often necessary to split a
long document into topically continuous segments.
Segmentation may be particularly beneficial when
working with documents without overt structure:
speech transcripts (Malioutov and Barzilay, 2006),
newswire (Misra et al, 2011) or novels (Kazantseva
and Szpakowicz, 2011). The customary approach
is to cast text segmentation as a binary problem: is
there a shift of topic between any two adjacent tex-
tual units (e.g., sentences or paragraphs)? While
necessary, this simplification is quite crude. Topic in
discourse usually changes continually; some shifts
are subtle, others ? more prominent.
The evaluation of text segmentation remains an
open research problem. It is a tradition to compile a
gold-standard segmentation reference using one or
more annotations created by humans. If an auto-
matic segmenter agrees with the reference, it is re-
warded, otherwise it is penalized (see Section 4 for
details). The nature of the task, however, is such that
creating and applying a reference segmentation is far
from trivial. The identification of topical shifts re-
quires discretization of a continuous concept ? how
much the topic changes between two adjacent units.
That is why annotators often operate at different lev-
els of granularity. Some people mark only the most
prominent topic fluctuations, while others also in-
clude finer changes. The task is also necessarily
under-defined. In addition to topic changes per se,
annotators effectively must classify some rhetorical
and pragmatic phenomena ? exactly how much it is
depends on the document genre. For simplicity we
do not directly address the latter problem here; we
concentrate on the former.
To study how people identify topical shifts in
written text, we asked 27 annotators to segment into
episodes 20 chapters of the novel The Moonstone
by Wilkie Collins. Each chapter was annotated by
4-6 people. An episode roughly corresponds to a
topically continuous segment ? the term is defined
211
in Section 3. The analysis of the resulting corpus
reveals that while the overall inter-annotator agree-
ment is quite low and is not uniform throughout each
chapter. Some topical shifts are marked by most or
all annotators, others ? by one or by a minority. In
fact, only about 50% of all annotated topical shifts
are supported by at least 50% of annotators (includ-
ing near-hits), while the other half is only marked by
a minority. In this work we take the agreement about
a certain topical shift as a measure of its prominence,
and show how this measure can be simply utilized
for the purpose of evaluation.
The main claim of this paper is perhaps the fol-
lowing: when evaluating the performance of auto-
matic segmenters, it is important to consider not
only the overall similarity between human and ma-
chine segmentations, but also to examine the regions
of disagreement. When a program misses or mis-
places a prominent topic shift ? a segment bound-
ary marked by all annotators ? it should be penal-
ized more than if it was mistaken about a boundary
marked by one person. Similarly, a false positive
in the region where none of the annotators found a
change in topic is worse than a boundary inserted in
a place where at least one person perceived a topic
change. We suggest that it is important to use all
available reference segmentations instead of com-
piling them into a single gold standard. We show
how a small modification to the popular windowD-
iff (Pevzner and Hearst, 2002) metric can allow con-
sidering multiple annotations at once.
To demonstrate the increased interpretive power
of such evaluation we run and evaluate several state-
of-the art segmenters on the corpus described in this
work. We evaluate their performance first in a con-
ventional manner ? by combining all available ref-
erences into one ? and then by using the proposed
modification. Comparing the results suggests that
the information provided by this method differs from
what existing methods provide.
Section 2 gives a brief background on text seg-
mentation. Section 3 describes the corpus and how
it was collected. Section 4 contain quantitative and
qualitative analysis of the corpus and its interpreta-
tions. Section 5 proposes a modified version of win-
dowDiff and motivates it. Section 6 compares eval-
uation of three segmenters in several different ways.
Section 7 contains the conclusions and outlines di-
rections for future work.
2 Background and Related Work
The goal of topical text segmentation is to identify
segments within which the topic under discussion
remains relatively constant. A flip-side of this def-
inition is identifying topic shifts ? places where the
topic shifts significantly or abruptly. In the context
of this paper we allow ourselves to use these two def-
initions interchangeably, sometimes talking about
identifying topic shifts, at other times ? about identi-
fying topically continuous segments. While the the-
oretical correctness of such usage remains question-
able, it is sufficient for the purpose of our discussion,
and it is in line with the literature on the topic.
There is a number of corpora annotated for the
presence of topical shifts by one or more annotators.
Passonneau and Litman (1997) describe an experi-
ment where seven untrained annotators were asked
to find discourse segments in a corpus of transcribed
narratives about a movie. While the authors show
that the agreement is significant, they also note that
people include segment boundaries at different rates.
Gruenstein, Niekrasz, and Purver (2005) describe
the process of annotating parts of two corpora of
meeting transcripts: ICSI (Janin et al, 2003) and
ISL (Burger, MacLaren, and Yu, 2002). Two peo-
ple annotated the texts at two levels: major and mi-
nor, corresponding to the more and less important
topic shifts. Topical shifts were to be annotated so
as to allow an outsider to glance at the transcript
and get the gist of what she missed. Not unlike
our work, the authors report rather low overall inter-
annotator agreement. Galley et al (2003) also com-
piled a layer of annotation for topical shifts for part
of the ICSI corpus, using a somewhat different pro-
cedure with three annotators. Malioutov and Barzi-
lay (2006) created a corpus of course lectures seg-
mented by four annotators, noting that the annota-
tors operated at different levels of granularity. In
these three projects, manual annotations were com-
piled into a single gold standard reference for use in
evaluating and fine-tuning automatic segmenters.
The work described in this paper is different in
several ways. To the best of our knowledge, this is
212
the first attempt to annotate literary texts for topical
shifts. Because we collected relatively many anno-
tations for each chapter (four to six), we can make
some generalizations as to the nature of the process.
In addition to compiling and describing the corpus,
we analyze disagreement patterns between annota-
tors. We claim that even though the annotators may
not agree on granularity, they do agree at some level,
at least with respect to most prominent breaks. We
propose that instead of compiling a single reference
from multiple annotations it may be more useful to
evaluate automatic segmenters against several anno-
tations at once. We will show how to do that.
3 The Overview of the Corpus
Our current work on text segmentation is part of a
larger project on automatic summarization of fic-
tion, which is why we chose a XIX century novel,
The Moonstone by Wilkie Collins, as the text to
be annotated. We used two chapters for a pilot
study and then another 20 for the large-scale experi-
ment. The annotators worked with individual chap-
ters and were required to align segment boundaries
with paragraph breaks.
Objectives. The main question behind this study
was this: ?How do people identify topical shifts in
literature?? This vague question can be mapped to
several more specific objectives. First, we sought
to verify that topical segmentation of literature was
a sensible task from the viewpoint of an untrained
annotator. Next, it was important to examine inter-
annotator agreement to make sure that the annota-
tors in fact worked on the same phenomena and that
the resulting corpus is a reasonable approximation of
how people segment literature in general. Third, in
addition to analyzing the overall agreement we also
took a close look at the type of common disagree-
ments, in search of patterns and insights to evaluate
automatic segmenters.
Subjects. The participants were undergraduate
students of an English department at the University
of Ottawa, recruited by email. They received $50
each for their participation. Everyone had to anno-
tate four chapters from The Moonstone, not neces-
sarily consecutive ones. The chapters were divided
so as to ensure an approximately equal workload.
We had planned six independent annotations for
each chapter of the novel.1 The annotators were di-
vided into five groups, each group asked to read and
annotate four distinct chapters. In the end we had
three groups with six people, one group with five
and one group with four.
Procedure. The experiment was conducted re-
motely. The students received email packages with
detailed instructions and an example of a segmented
chapter from a different novel. They had two weeks
to annotate the first two chapters and then two more
weeks to annotate another two chapters.
The annotators were instructed to read each chap-
ter and split it into episodes ? topically continuous
spans of text demarcated by the most perceptible
shifts of topic in the chapter. We asked the anno-
tators to provide a brief one-sentence description of
each episode, effectively creating a chapter outline.
The students were also asked to record places they
found challenging and to note the time it takes to
complete the task.
Because even short chapters of most traditional
novels are rather lengthy, we chose to use paragraphs
as the basic unit of annotation (sentences are more
common in text segmentation literature).
4 Corpus Analysis
Time. On average, an annotator required 137.9 min-
utes to complete both tasks. The standard devia-
tion was ? = 98.32 minutes appropriately reflecting
the fact that some students are very fast readers and
besides have already read the novel in one of their
classes, while others are quite slow.
The average chapter has 53.85 paragraphs (? =
29.31), the average segment length across all anno-
tators is 9.25 paragraphs (? = 9.77). On average the
annotators identified 5.80 episodes (? = 2.45) per
chapter. Figure 1 shows the distribution of the num-
ber of segments identified in each chapter. An indi-
vidual box plot is compiled using all available anno-
tations for that chapter ? six for most, four or five
for several. The data are plotted for individual chap-
ters, so the only source of variance is the disagree-
ment between annotators as to what is the appropri-
ate level of detail for the task. Figure 1 confirms
1We hired 30 students. Three did not complete the task.
213
Figure 1: Distribution of segment counts across chapters.
other researchers? findings: people find topical shifts
at different levels of granularity (Malioutov and
Barzilay, 2006; Gruenstein, Niekrasz, and Purver,
2005). We take this investigation further and explore
whether there are patterns to this disagreement and
how they can be interpreted and leveraged.
4.1 Inter-annotator Agreement
In order to make sure that our guidelines are suffi-
ciently clear and the annotators in fact annotate the
same phenomenon, it is important to measure inter-
annotator agreement (Artstein and Poesio, 2008).
This is particularly important given the fact that the
resulting corpus is intended as a benchmark dataset
for evaluation of automatic segmenters.
When looking at inter-annotator agreement inde-
pendently of the domain, the most commonly used
metrics are coefficients of agreement ? ? (Krippen-
dorff, 2004), ? (Cohen, 1960; Shrout and Fleiss,
1979), pi (Scott, 1955) and several others. In this
work we use a multi-annotator version of pi, also
known in the CL community as Fleiss?s ? (Shrout
and Fleiss, 1979; Siegel and Castellan, 1988) .
Fleiss?s ? is computed as follows:
? =
Agreementobserved ? Agreementexpected
1? Agreementexpected
(1)
Agreementobserved =
1
ic(c? 1)
X
i?I
X
k?K
nik(nik ? 1) (2)
Agreementexpected =
1
(ic)2
X
k?K
n2k (3)
where i is the number of items to be classified in set
I, k is the number of available categories in set K, c is
the number of annotators, nik is the number of anno-
tators who assign item i to category k, nk is the total
number of items assigned to category k by all anno-
tators (Artstein and Poesio, 2008, pp. 562-563). Ef-
fectively ?measures how much the annotators agree
above what can be expected by chance. The value
of ? is 0 where there is no agreement above chance
and 1 where the annotators agree completely.
While we report ? values for our dataset, it is
important to note that ? is ill-suited to measuring
agreement in segmentation. The main problem is its
insensitivity to near-hits. When asked to segment
a document, the annotators often disagree about the
exact placement of the boundary but agree that there
is a boundary somewhere in the region (e.g., con-
sider paragraphs 9-11 in segmentations in Figure 2).
It is desirable to give partial credit to such near-hits
instead of dismissing them as utter disagreement.
This cannot be achieved with ?. The second prob-
lem is the independence assumption: the label for
each item must be independent from the labels of all
other items. In our case, this would amount to claim-
ing, highly unrealistically, that the probability of a
topical shift between two sentences is independent
of the topical landscape of the rest of the document.
Two other commonly used agreement metrics are
Pk (Beeferman, Berger, and Lafferty, 1999) and win-
dowDiff (Pevzner and Hearst, 2002), both designed
to compare a hypothetical segmentation to a refer-
ence, not to measure agreement per se. A com-
mon feature of both metrics is that they award partial
credit to near-hits by sliding a fixed-length window
through the sequence and comparing the reference
segmentation and hypothetical segmentation at each
window position. The window size is generally set
at half the average segment length.
Pk (Equation 4) measures the probability that two
units randomly drawn from a document are correctly
classified as belonging to the same topical segment.
Pk has been criticized for penalizing false negatives
less than false positives and for being altogether in-
sensitive to certain types of error; see (Pevzner and
214
Hearst, 2002, pp. 22-26) for details. Despite its
shortcomings, Pk is widely used. We report it for
comparison with other corpora.
Pk(ref, hyp) =
X
1?i?j?n
D(i, j)(?ref (i, j) XNOR ?hyp(i, j))
(4)
Functions ?hyp and ?ref indicate whether the two
segment endpoints i and j belong to the same seg-
ment in the hypothetical segmentation and reference
segmentation respectively.
windowDiff was designed to remedy some of Pk?s
shortcomings. It counts erroneous windows in the
hypothetical sequence normalized by the total num-
ber of windows. A window is judged erroneous if
the boundary counts in the reference segmentation
and hypothetical segmentation differ; that is (|ref -
hyp| 6= 0) in Equation 5).
winDiff =
1
N ? k
N?kX
i=1
(|ref ? hyp| 6= 0) (5)
Both Pk and windowDiff produce penalty scores be-
tween 0 and 1, with 1 corresponding to all windows
being in error, and 0 ? to a perfect segmentation.
Table 1 reports Pk, windowDiff and ? values for
our corpus. Pk and windowDiff are computed pair-
wise for all annotators within one group and then
averaged. We set the window size to half the aver-
age segment length as measured across all annota-
tors who worked on a given chapter. The values are
computed for each group separately; Table 1 shows
the averages across five groups.
Even by most relaxed standards, e.g., (Landis and
Koch, 1977), the ? value of 0.38 corresponds to low
agreement. This is not surprising, since it only in-
cludes the cases when the annotators agree exactly
where the boundary should be. For the purpose of
our task, such a definition is too strict.
The values of windowDiff and Pk are more rea-
sonable; windowDiff = 0.34 means that on aver-
age a pair of annotators disagrees on 34% of win-
dows. windowDiff was originally designed to com-
pare only two segmentations. Our strategy of com-
puting its values pairwise is perhaps not optimal but
in the absence of another metric allowing to account
for near-hits we are practically forced to use it as a
primary means of inter-annotator agreement.
Table 1: Overview of inter-annotator agreement.
Mean Std. dev.
? 0.29 0.15
Pk 0.33 0.17
windowDiff 0.38 0.09
Figure 2: Example segmentation for Chapter 1.
4.2 Patterns of Disagreement
Figure 2 shows the segmentation of the shortest
chapter in the dataset. The overall agreement is
quite low (windowDiff=0.38, ? = 0.28). This is not
surprising, since annotators 1 and 3 found two seg-
ments, annotator 3 ? five segments, and annotator 4
? four. Yet al annotators agree on certain things: ev-
eryone found that there was a significant change of
topic between paragraphs 9 and 11 (though they dis-
agree on its exact placement). It is therefore likely
that the topical shift between paragraphs 9 and 11 is
quite prominent. Annotators 2 and 4 chose to place
a segment boundary after paragraph 2, while anno-
tators 1 and 3 did not place one there. It is likely that
the topical shift occurring there is less prominent, al-
though perceptible. According to these annotations,
the least perceptible topic shifts in the chapter oc-
cur after paragraph 4 (marked only by annotator 2)
and possibly after paragraph 11 (marked only by an-
notator 1). Overall, glancing at these segmentations
suggests that there is a prominent topical shift be-
tween paragraphs 9-11, three significant ones (after
2, 10 and 12) and several minor fluctuations (after 3
and possibly after 10 and 11).
Looking at the segmentations in Figure 2 it seems
likely that the disagreements between annotators 2
and 4 are due to granularity, while the annotators
1 and three disagree more fundamentally on where
the topic changes. When measuring agreement, we
would like to be able to distinguish between dis-
215
Figure 3: Quality of segment boundaries.
agreements due to granularity and disagreements
due to true lack of agreement (annotator 1 and 3).
We would also like to leverage this information for
the evaluation of automatic segmenters.
Distinguishing between true disagreement and
different granularity while taking into account near-
hits is not trivial, especially since we are working
with multiple annotations simultaneously and there
is no one correct segmentation.
In order to estimate the quality of individual
boundaries and look inside the segmented sequence,
we approximate the quality of each suggested seg-
ment boundary by the percentage of annotators who
marked it. Since the annotators may disagree on the
exact placement of the boundaries, our measurement
must be relaxed to allow for near-hits.
Figure 3 shows the distribution of segment bound-
aries using three different standards of quality. We
consider all segment boundaries introduced by at
least one annotator. Then, for each suggested bound-
ary we compute how much support there is from
peer annotators: what percentage of annotators in-
cluded this boundary in their segmentation. The left-
most box plot in Figure 3 corresponds to the most
strict standard. When computing support we only
consider perfect matches: segment boundaries spec-
ified in exactly the same location (window size =
0). The middle box plot is more relaxed: we con-
sider boundaries found within half of a windowD-
iff window size of the boundary under inspection.
The rightmost box plot corresponds to the inclusion
of boundaries found within a full windowDiff win-
dow size of the boundary under inspection.
Looking at exact matches (the leftmost box plot),
we observe that at least a half of segment bound-
aries were specified by less than 25% of annotators
(which corresponds to one person). It explains why
? values in Table 1 are so low: this is the only sort
of agreement ? captures. Also one can notice that
at most 25% of the boundaries have the support of
more than 50% of the annotators.
The picture changes if we consider all boundaries
within a tight window around the candidate bound-
ary (the middle box plot). This standard is twice
as strict as the regular windowDiff evaluation. Here
50% of all boundaries are marked by at least 35% at
and most 80% of annotators. Only 25% of bound-
aries are marked by less than 30% of the annotators.
The rightmost plot looks even better. If we con-
sider the support found within a window size of any
candidate boundary, then 50% of all boundaries are
supported by over 70% of annotators. However, we
find this way of measuring support too optimistic.
The reason is, again, the difference in the granu-
larity of segmentations. The window size used for
these measurements is based on the average segment
length across all annotations. For example, the aver-
age segment length for segmentation shown in Fig-
ure 2 is 4, making the window size 2. This size is
too relaxed for annotators 2 and 3, who were very
detailed. Due to the excessively large window there
will almost always be a boundary where fine-grained
annotations are concerned, but those boundaries will
not correspond to the same phenomena. That is why
we think that a stricter standard is generally more
appropriate. This is especially the case since we
work with paragraphs, not sentences. A distance of
2-3 sentences is quite tolerable, but a distance of 2-3
paragraphs is considerable, and it is far more likely
that a stricter notion of near-hits must be considered.
5 Proposed Modification to windowDiff
WindowDiff compares two segmentations by taking
into account near-hits ? penalizing them proportion-
ally to how far a hypothetical segment boundary is
216
from a reference boundary. Section 4.2 argued that
some boundaries are more prominent. We aim to
modify windowDiff so the prominence of the bound-
aries matters in evaluating automatic segmenters.
Recall that to compute windowDiff we slide a
window through the reference and the hypotheti-
cal segmentation and check whether the number of
boundaries is equal at each window position. The
number of erroneous windows is then normalized:
winDiff =
1
N ? k
N?kX
i=1
(|refi ? hypi| 6= 0) (6)
refi and hypoi are the counts of boundaries in a
given window in the reference and the hypothetical
sequence, N is the length of the complete sequence,
k is the window size (so there are N - k windows).
The prominence of a boundary can be approxi-
mated by how many annotators specified it in their
segmentations. One simple way to take prominence
into account is to slide a window through all avail-
able segmentations, not just one. A straighforward
modification to equation (6) achieves that:
winDiff ? =
1
h(N ?m)
hX
a=1
N?mX
i=1
(|refai ? hypi| 6= 0) (7)
A is the set of all available annotations and h is
their total number. Effectively, for each position of
the window the hypothetical output is penalized as
many times as there are reference annotations with
which it disagrees. Note that the window size m is
different from that used for pair-wise comparisons.
Following the convention, we recommend setting it
to half of the size of an average segment length (av-
eraged over all available references). The size of
the window effectively specifies a tolerance thresh-
old for what is an acceptable near-hit (as opposed to
a plain miss), and can be modified accordingly.
windowDiff and Pk range from 0 to 1, with 0
corresponding to an ideal segmentation. The upper
and lower bounds for Equation 7 are different and
depend on how much the reference segmentations
agree between themselves.2
2We find that the upper bound corresponds to the worst-case,
and the lower bound to the best-case scenario. To avoid confu-
sion, we talk of the best-case bound and the worst-case bound.
Let us refer to the most popular opinion for a
given position of the window as the majority opin-
ion. Then, for each window, the smallest possible
penalty is assigned if the hypothetical segmentation
correctly ?guesses? the majority opinion (the win-
dow then receives a penalty equal to the number of
annotators disagreeing with the majority opinion):
best case =
1
N ?m
N?mX
i=1
(h?majority support) (8)
Here majority support is the number of annota-
tors who support the most frequent opinion.
Conversely, to merit the highest penalty, a hypo-
thetical segmentation must ?guess? the least popu-
lar opinion (possibly an opinion not supported by
any annotators) at each window position. In Equa-
tion 9, unpopular support is the number of anno-
tators who agree with the least popular opinion.
worst case =
1
N ?m
N?mX
i=1
(h? unpopular support) (9)
In order to have a multi-annotator version of win-
dowDiff interpretable within the familiar [0, 1] in-
terval, we normalize Equation 7:
multWinDiff =
(
Ph
a=1
PN?m
i=1 (|refa ? hyp| 6= 0))? best case
h(N ?m)(worst case? best case)
(10)
The best and the worst-case bounds serve as indi-
cators of how much agreement there can be between
reference segmentations and so as indicators of how
difficult to segment a given document is.
The multWinDiff metric in Equation 10 has the
same desirable properties as the original metric,
namely it takes into account near hits and penal-
izes according to how far the reference and hypo-
thetical boundaries are. Additionally, for each win-
dow position it takes into account how much a hy-
pothetical segmentation is similar to all available an-
notations, thus penalizing mistakes according to the
prominence of boundaries (or to the certainty that
there are no boundaries).3
3Java code to compute multWinDiff is available as a part of
the APS segmenter. The corpus and the software can be down-
loaded at ?www.eecs.uottawa.ca/?ankazant?.
217
6 Experiments
In order to illustrate why using a single gold-
standard reference segmentation can be problem-
atic, we evaluate three publicly available seg-
menters, MinCutSeg (Malioutov and Barzilay,
2006), BayesSeg (Eisenstein and Barzilay, 2008)
and APS (Kazantseva and Szpakowicz, 2011), us-
ing several different gold standards and then using
all available annotations. The corpus used for eval-
uation is The Moonstone corpus described in Sec-
tions 3-4. We withheld the first four chapters for de-
velopment and used the remaining 16 for testing. We
also compared the segmenters to a random baseline
which consisted of randomly selecting a number of
boundaries equal to the average number of segments
across all available annotations.
None of the segmenters requires training in the
conventional sense, but APS and MinCutSeg seg-
meters come with scripts allowing to fine-tune sev-
eral parameters. We selected the best parameters for
these two segmenters using the first four chapters of
the corpus. BayesSeg segmeter, a probabilistic seg-
menter, does not require setting any parameters.
Table 2 sums up the results. Each row corre-
sponds to one reference segmentation and metric ?
regular windowDiff in the first six rows. We com-
piled several flavours of consensus reference seg-
mentations: 1) all boundaries marked by ? 50% of
the annotators (windowDiff ? 50%), 2) all boundaries
marked by ? 30% of the annotators (windowDiff ?
30%), 3) all boundaries marked by at least one an-
notator (windowDiff union). To illustrate why com-
paring against a single annotation is unreliable, we
report comparisons against three single-person an-
notations (windowDiff annotator 1, 4, 2). multWinDiff
is the proposed multi-annotator version from Equa-
tion 10. The best-case bound for multWinDiff is 0.21
and the worst-case bound is 1.0.
Each segmenter produced just one segmentation,
so the numbers in the Table 2 differ only depending
on the mode of evaluation. The cells are coloured.
The lightest shade correspond to the best perfor-
mance, darker shades ? to poorer performance. The
actual values for the first six rows are rather low, but
what is more bothersome is the lack of consistency
in the ranking of segmenters. Only the random base-
APS Bayes MinCut Rand.
windowDiff
?50%
0.60. 0.66 0.73 0.73
windowDiff
?30%
0.61 0.52 0.69 0.61
windowDiff
union
0.6 0.53 0.63 0.65
windowDiff
annotator 1
0.66 0.57 0.74 0.76
windowDiff
annotator 4
0.62 0.7 0.69 0.74
windowDiff
annotator 2
0.61 0.6 0.66 0.69
multWinDiff 0.23 0.28 0.31 0.41
Table 2: The three segmenters and a random baseline
compared using different references for computing win-
dowDiff. windowDiff ?50%: the gold standard consists
of all boundaries specified by at least 50% of the anno-
tators; windowDiff ?30%: all boundaries specified by
at least 30% of the annotators; windowDiff union: all
boundaries specified by at least one person; windowD-
iff annotator a: comparisons against individual annota-
tors. multWinDiff is multi-annotator windowDiff from
equation (10).
line remains the worst in most cases. The APS and
BayesSeg segmenters tend to appear better than the
MinCutSeg but it is not always the case and the rank-
ings among the three are not consistent.
The last row reports multi-annotator windowD-
iff which takes into account all available references
and also the best-case and the worst-case bounds. In
principle, there is no way to prove that the metric is
better than using windowDiff and a single reference
annotation. It does, however, take into account all
available information and provides a different, if not
unambiguously more true, picture of the compara-
tive performance of automatic segmenters.
7 Conclusions and Future Work
We have described a new corpus which can be used
in research on topical segmentation. The corpus is
compiled for fiction, a genre for which no such cor-
pus exists. It contains a reasonable number of anno-
tations per chapter to allow an in-depth analysis of
topical segmentation as performed by humans.
218
Our analysis of the corpus confirms the hypothe-
sis that when asked to find topical segments, people
operate at different levels of granularity. We show
that only a small percentage of segment boundaries
is agreed upon by all or almost all annotators. If,
however, near-hits are considered, suggested seg-
ment boundaries can be ranked by their prominence
using the information about how many people in-
clude each boundary in their annotation.
We propose a simple modification to windowD-
iff which allows for taking into account more than
one reference segmentation, and thus rewards or pe-
nalizes the output of automatic segmenters by con-
sidering the severity of their mistakes. The proposed
metric is not trouble-free. It is a window-based met-
ric so its value depends on the choice of the window
size. While it has become a convention to set the
window size to half of the average segment length in
the reference segmentation, it is not obvious that the
same logic applies in case of multi-annotator win-
dowDiff. The metric also hides whether false posi-
tives or false negatives are the main source of error.
All these shortcomings notwithstanding, the met-
ric offers an advantage of allowing the evaluation of
hypothetical segmentations with more subtlety than
those using a single gold standard reference. When
using regular windowDiff and a single reference seg-
mentation, one is restricted to an evaluation based
on binary comparisons: whether a given hypothet-
ical boundary is similar to the gold standard seg-
mentation (e.g., the majority opinion). Divergent
segmentations are penalized even if they are simi-
lar to minority opinions (and thus feasible, though
maybe less likely) or if they are completely different
from anything created by humans (and thus proba-
bly genuinely erroneous). Our version of windowD-
iff, however, takes into account multiple annotations
and gives partial reward to segmentations based on
how similar there are to any human segmentation,
not just the majority opinion (while giving prefer-
ence to high agreement with the majority opinion).
To evaluate the output of topical segmenters is
hard. There is disagreement between the annota-
tors about the appropriate level of granularity and
about the exact placement of segment boundaries.
The task itself is also a little vague. Just as it is
the case in automatic text summarization, generation
and other advanced NLP tasks, there is no single cor-
rect answer and the goal of a good evaluation met-
ric is to reward plausible hypotheses and to penalize
improbable ones. It is quite possible that a better
metric than the one proposed here can be devised;
see, for example, (Fournier and Inkpen, 2012)(Sca-
iano and Inkpen, 2012). We feel, however, that any
reliable metric for evaluating segmentations must ?
in one manner or another ? take into account more
than one annotation and the prominence of segment
breaks.
Acknowledgments
We thank William Klement and Chris Fournier for
commenting on an early draft of this paper. The first
author also thanks Chris Fournier and Martin Sca-
iano for insightful discussions about the evaluation
of topical segmenters. This work is partially funded
by the National Sciences and Engineering Research
Council of Canada and by the Ontario Graduate
Scholarship program.
References
Artstein, Ron and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555?596.
Beeferman, Doug, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34:177?210, February.
Burger, Susanne, Victoria MacLaren, and Hua Yu. 2002.
The ISL meeting corpus: the impact of meeting type
on speech style. In INTERSPEECH?02.
Cohen, Jacob. 1960. A coefficient of agreement for
nominal scales . Educational and Psychological Mea-
surement, 20:37?46.
Eisenstein, Jacob and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 334?343, Honolulu,
Hawaii, October.
Fournier, Chris and Diana Inkpen. 2012. Segmentation
similarity and agreement. In Proceedings of NAACL-
HLT 2012 (this volume), Montre?al, Canada, June.
Galley, Michel, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation
of multi-party conversation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 562?
219
569, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Gruenstein, Alexander, John Niekrasz, and Matthew
Purver. 2005. Meeting Structure Annotation: Data
and Tools. In In Proceedings of the SIGdial Workshop
on Discourse and Dialogue, pages 117?127.
Janin, Adam, Don Baron, Jane Edwards, D. Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, A.ndreas Stolcke, and Chuck
Wooters. 2003. The ICSI Meeting Corpus. In
Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP-
03), volume 1, pages 364?367, April.
Kazantseva, Anna and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 284?
293, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Krippendorff, Klaus. 2004. Content Analysis. An Intro-
duction to Its Methodology. Sage Publications.
Landis, J. Richards and Garry G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174.
Malioutov, Igor and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?
32, Sydney, Australia, July.
Misra, Hemant, Franc?ois Yvon, Olivier Cappe?, and Joe-
mon M. Jose. 2011. Text segmentation: A topic mod-
eling perspective. Information Processing and Man-
agement, 47(4):528?544.
Passonneau, Rebecca J. and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103?139, March.
Pevzner, Lev and Marti A. Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 28(1):19?36.
Scaiano, Martin and Diana Inkpen. 2012. Getting
more from segmentation evaluation. In Proceedings
of NAACL-HLT 2012 (this volume), Montre?al, Canada,
June.
Scott, William. 1955. Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Shrout, Patrick E. and Joseph L. Fleiss. 1979. Intraclass
correlations: uses in assessing rater reliability. Psy-
chological Bulletin, 86(2):420?428.
Siegel, Sidney and John. N. Jr. Castellan. 1988. Non-
parametric statistics for the behavioral sciences. Mc-
Graw Hill, Boston, MA.
220
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39?44,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid
?
O S
?
eaghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
Previous research has shown that the mean-
ing of many noun-noun compounds N
1
N
2
can be approximated reasonably well by
paraphrasing clauses of the form ?N
2
that
. . . N
1
?, where ?. . . ? stands for a verb
with or without a preposition. For exam-
ple, malaria mosquito is a ?mosquito that
carries malaria?. Evaluating the quality of
such paraphrases is the theme of Task 9 at
SemEval-2010. This paper describes some
background, the task definition, the process
of data collection and the task results. We
also venture a few general conclusions be-
fore the participating teams present their
systems at the SemEval-2010 workshop.
There were 5 teams who submitted 7 sys-
tems.
1 Introduction
Noun compounds (NCs) are sequences of two or
more nouns that act as a single noun,1 e.g., stem
cell, stem cell research, stem cell research organi-
zation, etc. Lapata and Lascarides (2003) observe
that NCs pose syntactic and semantic challenges for
three basic reasons: (1) the compounding process
is extremely productive in English; (2) the seman-
tic relation between the head and the modifier is
implicit; (3) the interpretation can be influenced by
contextual and pragmatic factors. Corpus studies
have shown that while NCs are very common in
English, their frequency distribution follows a Zip-
fian or power-law distribution and the majority of
NCs encountered will be rare types (Tanaka and
Baldwin, 2003; Lapata and Lascarides, 2003; Bald-
win and Tanaka, 2004; ?O S?eaghdha, 2008). As a
consequence, Natural Language Processing (NLP)
1
We follow the definition in (Downing, 1977).
applications cannot afford either to ignore NCs or
to assume that they can be handled by relying on a
dictionary or other static resource.
Trouble with lexical resources for NCs notwith-
standing, NC semantics plays a central role in com-
plex knowledge discovery and applications, includ-
ing but not limited to Question Answering (QA),
Machine Translation (MT), and Information Re-
trieval (IR). For example, knowing the (implicit)
semantic relation between the NC components can
help rank and refine queries in QA and IR, or select
promising translation pairs in MT (Nakov, 2008a).
Thus, robust semantic interpretation of NCs should
be of much help in broad-coverage semantic pro-
cessing.
Proposed approaches to modelling NC seman-
tics have used semantic similarity (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004; Kim and
Baldwin, 2005; Nastase and Szpakowicz, 2006;
Girju, 2007; ?O S?eaghdha and Copestake, 2007)
and paraphrasing (Vanderwende, 1994; Kim and
Baldwin, 2006; Butnariu and Veale, 2008; Nakov
and Hearst, 2008). The former body of work seeks
to measure the similarity between known and un-
seen NCs by considering various features, usually
context-related. In contrast, the latter group uses
verb semantics to interpret NCs directly, e.g., olive
oil as ?oil that is extracted from olive(s)?, drug
death as ?death that is caused by drug(s)?, flu shot
as a ?shot that prevents flu?.
The growing popularity ? and expected direct
utility ? of paraphrase-based NC semantics has
encouraged us to propose an evaluation exercise
for the 2010 edition of SemEval. This paper gives
a bird?s-eye view of the task. Section 2 presents
its objective, data, data collection, and evaluation
method. Section 3 lists the participating teams.
Section 4 shows the results and our analysis. In
Section 5, we sum up our experience so far.
39
2 Task Description
2.1 The Objective
For the purpose of the task, we focused on two-
word NCs which are modifier-head pairs of nouns,
such as apple pie or malaria mosquito. There are
several ways to ?attack? the paraphrase-based se-
mantics of such NCs.
We have proposed a rather simple problem: as-
sume that many paraphrases can be found ? perhaps
via clever Web search ? but their relevance is up in
the air. Given sufficient training data, we seek to es-
timate the quality of candidate paraphrases in a test
set. Each NC in the training set comes with a long
list of verbs in the infinitive (often with a prepo-
sition) which may paraphrase the NC adequately.
Examples of apt paraphrasing verbs: olive oil ?
be extracted from, drug death ? be caused by, flu
shot ? prevent. These lists have been constructed
from human-proposed paraphrases. For the train-
ing data, we also provide the participants with a
quality score for each paraphrase, which is a simple
count of the number of human subjects who pro-
posed that paraphrase. At test time, given a noun
compound and a list of paraphrasing verbs, a partic-
ipating system needs to produce aptness scores that
correlate well (in terms of relative ranking) with
the held out human judgments. There may be a
diverse range of paraphrases for a given compound,
some of them in fact might be inappropriate, but
it can be expected that the distribution over para-
phrases estimated from a large number of subjects
will indeed be representative of the compound?s
meaning.
2.2 The Datasets
Following Nakov (2008b), we took advantage of
the Amazon Mechanical Turk2 (MTurk) to acquire
paraphrasing verbs from human annotators. The
service offers inexpensive access to subjects for
tasks which require human intelligence. Its API
allows a computer program to run tasks easily and
collate the subjects? responses. MTurk is becoming
a popular means of eliciting and collecting linguis-
tic intuitions for NLP research; see Snow et al
(2008) for an overview and a further discussion.
Even though we recruited human subjects,
whom we required to take a qualification test,3
2
www.mturk.com
3We soon realized that we also had to offer a version of
our assignments without a qualification test (at a lower pay
rate) since very few people were willing to take a test. Overall,
data collection was time-consuming since many
annotators did not follow the instructions. We had
to monitor their progress and to send them timely
messages, pointing out mistakes. Although the
MTurk service allows task owners to accept or re-
ject individual submissions, rejection was the last
resort since it has the triply unpleasant effect of
(1) denying the worker her fee, (2) negatively af-
fecting her rating, and (3) lowering our rating as
a requester. We thus chose to try and educate our
workers ?on the fly?. Even so, we ended up with
many examples which we had to correct manu-
ally by labor-intensive post-processing. The flaws
were not different from those already described by
Nakov (2008b). Post-editing was also necessary to
lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we
released as trial data the previously collected para-
phrase sets (Nakov, 2008b) for the Levi-250 dataset
(after further review and cleaning). This dataset
consisted of 250 noun-noun compounds form (Levi,
1978), each paraphrased by 25-30 MTurk workers
(without a qualification test).
Training Data. The training dataset was an ex-
tension of the trial dataset. It consisted of the same
250 noun-noun compounds, but the number of an-
notators per compound increased significantly. We
aimed to recruit at least 30 additional MTurk work-
ers per compound; for some compounds we man-
aged to get many more. For example, when we
added the paraphrasing verbs from the trial dataset
to the newly collected verbs, we had 131 different
workers for neighborhood bars, compared to just
50 for tear gas. On the average, we had 72.7 work-
ers per compound. Each worker was instructed
to try to produce at least three paraphrasing verbs,
so we ended up with 191.8 paraphrasing verbs per
compound, 84.6 of them being unique. See Table 1
for more details.
Test Data. The test dataset consisted of 388
noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003) dataset;
and (2) the Lauer (1995) dataset. The former
contains 328 noun-noun compounds (there are
also a number of adjective-noun and adverb-noun
pairs), while the latter contains 266 noun-noun
compounds. Since these datasets overlap between
themselves and with the training dataset, we had
to exclude some examples. In the end, we had 388
we found little difference in the quality of work of subjects
recruited with and without the test.
40
Training: 250 NCs Testing: 388 NCs All: 638 NCs
Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg
MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0
Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3
Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1
Table 1: Statistics about the the training/test datasets. Shown are the total number of verbs proposed as
well as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.
unique noun-noun compounds for testing, distinct
from those used for training. We aimed for 100
human workers per testing NC, but we could only
get 68.3, with a minimum of 57 and a maximum of
96; there were 185.0 paraphrasing verbs per com-
pound, 70.9 of them being unique, which is close
to what we had for the training data.
Data format. We distribute the training data as
a raw text file. Each line has the following tab-
separated format:
NC paraphrase frequency
where NC is a noun-noun compound (e.g., ap-
ple cake, flu virus), paraphrase is a human-
proposed paraphrasing verb optionally followed
by a preposition, and frequency is the number
of annotators who proposed that paraphrase. Here
is an illustrative extract from the training dataset:
flu virus cause 38
flu virus spread 13
flu virus create 6
flu virus give 5
flu virus produce 5
...
flu virus be made up of 1
flu virus be observed in 1
flu virus exacerbate 1
The test file has a similar format, except that the
frequency is not included and the paraphrases for
each noun compound appear in random order:
...
chest pain originate
chest pain start in
chest pain descend in
chest pain be in
...
License. All datasets are released under the Cre-
ative Commons Attribution 3.0 Unported license.4
4
creativecommons.org/licenses/by/3.0
2.3 Evaluation
All evaluation was performed by computing an ap-
propriate measure of similarity/correlation between
system predictions and the compiled judgements of
the human annotators. We did it on a compound-by-
compound basis and averaged over all compounds
in the test dataset. Section 4 shows results for three
measures: Spearman rank correlation, Pearson cor-
relation, and cosine similarity.
Spearman Rank Correlation (?) was adopted
as the official evaluation measure for the competi-
tion. As a rank correlation statistic, it does not use
the numerical values of the predictions or human
judgements, only their relative ordering encoded
as integer ranks. For a sample of n items ranked
by two methods x and y, the rank correlation ? is
calculated as follows:
? =
n
?
x
i
y
i
? (
?
x
i
)(
?
y
i
)
?
n
?
x
2
i
? (
?
x
i
)
2
?
n
?
y
2
i
? (
?
y
i
)
2
(1)
where x
i
, y
i
are the ranks given by x and y to the
ith item, respectively. The value of ? ranges be-
tween -1.0 (total negative correlation) and 1.0 (total
positive correlation).
Pearson Correlation (r) is a standard measure
of correlation strength between real-valued vari-
ables. The formula is the same as (1), but with
x
i
, y
i
taking real values rather than rank values;
just like ?, r?s values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to
compare numerical vectors:
cos =
?
n
i
x
i
y
i
?
?
n
i
x
2
i
?
n
i
y
2
i
(2)
For non-negative data, the cosine similarity takes
values between 0.0 and 1.0. Pearson?s r can be
viewed as a version of the cosine similarity which
performs centering on x and y.
Baseline: To help interpret these evaluation mea-
sures, we implemented a simple baseline. A dis-
tribution over the paraphrases was estimated by
41
System Institution Team Description
NC-INTERP International Institute of
Information Technology,
Hyderabad
Prashant
Mathur
Unsupervised model using verb-argument frequen-
cies from parsed Web snippets and WordNet
smoothing
UCAM University of Cambridge Clemens Hepp-
ner
Unsupervised model using verb-argument frequen-
cies from the British National Corpus
UCD-GOGGLE-I University College
Dublin
Guofu Li Unsupervised probabilistic model using pattern fre-
quencies estimated from the Google N-Gram corpus
UCD-GOGGLE-II Paraphrase ranking model learned from training
data
UCD-GOGGLE-III Combination of UCD-GOGGLE-I and UCD-
GOGGLE-II
UCD-PN University College
Dublin
Paul Nulty Scoring according to the probability of a paraphrase
appearing in the same set as other paraphrases pro-
vided
UVT-MEPHISTO Tilburg University Sander
Wubben
Supervised memory-based ranker using features
from Google N-Gram Corpus and WordNet
Table 2: Teams participating in SemEval-2010 Task 9
summing the frequencies for all compounds in the
training dataset, and the paraphrases for the test ex-
amples were scored according to this distribution.
Note that this baseline entirely ignores the identity
of the nouns in the compound.
3 Participants
The task attracted five teams, one of which (UCD-
GOGGLE) submitted three runs. The participants
are listed in Table 2 along with brief system de-
scriptions; for more details please see the teams?
own description papers.
4 Results and Discussion
The task results appear in Table 3. In an evaluation
by Spearman?s ? (the official ranking measure),
the winning system was UVT-MEPHISTO, which
scored 0.450. UVT also achieved the top Pear-
son?s r score. UCD-PN is the top-scoring system
according to the cosine measure. One participant
submitted part of his results after the official dead-
line, which is marked by an asterisk.
The participants used a variety of information
sources and estimation methods. UVT-MEPHISTO
is a supervised system that uses frequency informa-
tion from the Google N-Gram Corpus and features
from WordNet (Fellbaum, 1998) to rank candidate
paraphrases. On the other hand, UCD-PN uses
no external resources and no supervised training,
yet came within 0.009 of UVT-MEPHISTO in the
official evaluation. The basic idea of UCD-PN ?
that one can predict the plausibility of a paraphrase
simply by knowing which other paraphrases have
been given for that compound regardless of their
frequency ? is clearly a powerful one. Unlike the
other systems, UCD-PN used information about the
test examples (not their ranks, of course) for model
estimation; this has similarities to ?transductive?
methods for semi-supervised learning. However,
post-hoc analysis shows that UCD-PN would have
preserved its rank if it had estimated its model on
the training data only. On the other hand, if the task
had been designed differently ? by asking systems
to propose paraphrases from the set of all possi-
ble verb/preposition combinations ? then we would
not expect UCD-PN?s approach to work as well as
models that use corpus information.
The other systems are comparable to UVT-
MEPHISTO in that they use corpus frequencies
to evaluate paraphrases and apply some kind of
semantic smoothing to handle sparsity. How-
ever, UCD-GOGGLE-I, UCAM and NC-INTERP
are unsupervised systems. UCAM uses the 100-
million word BNC corpus, while the other systems
use Web-scale resources; this has presumably ex-
acerbated sparsity issues and contributed to a rela-
tively poor performance.
The hybrid approach exemplified by UCD-
GOGGLE-III combines the predictions of a sys-
tem that models paraphrase correlations and one
that learns from corpus frequencies and thus at-
tains better performance. Given that the two top-
scoring systems can also be characterized as using
these two distinct information sources, it is natu-
ral to consider combining these systems. Simply
normalizing (to unit sum) and averaging the two
sets of prediction values for each compound does
42
Rank System Supervised? Hybrid? Spearman ? Pearson r Cosine
1 UVT-MEPHISTO yes no 0.450 0.411 0.635
2 UCD-PN no no 0.441 0.361 0.669
3 UCD-GOGGLE-III yes yes 0.432 0.395 0.652
4 UCD-GOGGLE-II yes no 0.418 0.375 0.660
5 UCD-GOGGLE-I no no 0.380 0.252 0.629
6 UCAM no no 0.267 0.219 0.374
7 NC-INTERP* no no 0.186 0.070 0.466
Baseline yes no 0.425 0.344 0.524
Combining UVT and UCD-PN yes yes 0.472 0.431 0.685
Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).
indeed give better scores: Spearman ? = 0.472,
r = 0.431, Cosine = 0.685.
The baseline from Section 2.3 turns out to be
very strong. Evaluating with Spearman?s ?, only
three systems outperform it. It is less competitive
on the other evaluation measures though. This
suggests that global paraphrase frequencies may
be useful for telling sensible paraphrases from bad
ones, but will not do for quantifying the plausibility
of a paraphrase for a given noun compound.
5 Conclusion
Given that it is a newly-proposed task, this initial
experiment in paraphrasing noun compounds has
been a moderate success. The participation rate
has been sufficient for the purposes of comparing
and contrasting different approaches to the role
of paraphrases in the interpretation of noun-noun
compounds. We have seen a variety of approaches
applied to the same dataset, and we have been able
to compare the performance of pure approaches to
hybrid approaches, and of supervised approaches
to unsupervised approaches. The results reported
here are also encouraging, though clearly there is
considerable room for improvement.
This task has established a high baseline for sys-
tems to beat. We can take heart from the fact that
the best performance is apparently obtained from a
combination of corpus-derived usage features and
dictionary-derived linguistic knowledge. Although
clever but simple approaches can do quite well on
such a task, it is encouraging to note that the best
results await those who employ the most robust
and the most informed treatments of NCs and their
paraphrases. Despite a good start, this is a chal-
lenge that remains resolutely open. We expect that
the dataset created for the task will be a valuable
resource for future research.
Acknowledgements
This work is partially supported by grants from
Amazon and from the Bulgarian National Science
Foundation (D002-111/15.12.2008 ? SmartBook).
References
Timothy Baldwin and Takaaki Tanaka. 2004. Trans-
lation by Machine of Compound Nominals: Getting
it Right. In Proceedings of the ACL-04 Workshop
on Multiword Expressions: Integrating Processing,
pages 24?31, Barcelona, Spain.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
08), pages 81?88, Manchester, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Roxana Girju. 2007. Improving the Interpretation
of Noun Phrases with Cross-linguistic Information.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL-07),
pages 568?575, Prague, Czech Republic.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 945?956, Jeju Island, South Ko-
rea.
Su Nam Kim and Timothy Baldwin. 2006. Inter-
preting Semantic Relations in Noun Compounds via
Verb Semantics. In Proceedings of the COLING-
ACL-06 Main Conference Poster Sessions, pages
491?498, Sydney, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional evi-
dence. In Proceedings of the 10th Conference of the
43
European Chapter of the Association for Computa-
tional Linguistics (EACL-03), pages 235?242, Bu-
dapest, Hungary.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, NY.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on Computa-
tional Lexical Semantics, pages 60?67, Boston, MA.
Preslav Nakov and Marti A. Hearst. 2008. Solving
Relational Similarity Problems Using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL-08), pages 452?460, Columbus, OH.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence (ECAI-08), pages 338?342, Patras,
Greece.
Preslav Nakov. 2008b. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study.
In Proceedings of the 13th International Confer-
ence on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA-08), pages 103?117,
Varna, Bulgaria.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics (IWCS-03), pages 285?301, Tilburg, The
Netherlands.
Vivi Nastase and Stan Szpakowicz. 2006. Matching
syntactic-semantic graphs for semantic relation as-
signment. In Proceedings of the 1st Workshop on
Graph Based Methods for Natural Language Pro-
cessing (TextGraphs-06), pages 81?88, New York,
NY.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proceedings of the ACL-07 Workshop
on A Broader Perspective on Multiword Expressions
(MWE-07), pages 57?64, Prague, Czech Republic.
Diarmuid
?
O S?eaghdha. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, University of Cam-
bridge.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-08), pages 254?263, Honolulu, HI.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of the
ACL-03 Workshop on Multiword Expressions (MWE-
03), pages 17?24, Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics (COLING-94), pages 782?788,
Kyoto, Japan.
44
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 138?143, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 4: Free Paraphrases of Noun Compounds
Iris Hendrickx
Radboud University Nijmegen &
Universidade de Lisboa
iris@clul.ul.pt
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Stan Szpakowicz
University of Ottawa &
Polish Academy of Sciences
szpak@eecs.uottawa.ca
Zornitsa Kozareva
University of Southern California
kozareva@isi.edu
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
In this paper, we describe SemEval-2013 Task
4: the definition, the data, the evaluation and
the results. The task is to capture some of the
meaning of English noun compounds via para-
phrasing. Given a two-word noun compound,
the participating system is asked to produce
an explicitly ranked list of its free-form para-
phrases. The list is automatically compared
and evaluated against a similarly ranked list
of paraphrases proposed by human annota-
tors, recruited and managed through Ama-
zon?s Mechanical Turk. The comparison of
raw paraphrases is sensitive to syntactic and
morphological variation. The ?gold? ranking
is based on the relative popularity of para-
phrases among annotators. To make the rank-
ing more reliable, highly similar paraphrases
are grouped, so as to downplay superficial dif-
ferences in syntax and morphology. Three
systems participated in the task. They all beat
a simple baseline on one of the two evalua-
tion measures, but not on both measures. This
shows that the task is difficult.
1 Introduction
A noun compound (NC) is a sequence of nouns
which act as a single noun (Downing, 1977), as in
these examples: colon cancer, suppressor protein,
tumor suppressor protein, colon cancer tumor sup-
pressor protein, etc. This type of compounding is
highly productive in English. NCs comprise 3.9%
and 2.6% of all tokens in the Reuters corpus and the
British National Corpus (BNC), respectively (Bald-
win and Tanaka, 2004).
The frequency spectrum of compound types fol-
lows a Zipfian distribution (O? Se?aghdha, 2008), so
many NC tokens belong to a ?long tail? of low-
frequency types. More than half of the two-noun
types in the BNC occur exactly once (Kim and Bald-
win, 2006). Their high frequency and high produc-
tivity make robust NC interpretation an important
goal for broad-coverage semantic processing of En-
glish texts. Systems which ignore NCs may give up
on salient information about the semantic relation-
ships implicit in a text. Compositional interpretation
is also the only way to achieve broad NC coverage,
because it is not feasible to list in a lexicon all com-
pounds which one is likely to encounter. Even for
relatively frequent NCs occurring 10 times or more
in the BNC, static English dictionaries provide only
27% coverage (Tanaka and Baldwin, 2003).
In many natural language processing applications
it is important to understand the syntax and seman-
tics of NCs. NCs often are structurally similar,
but have very different meaning. Consider caffeine
headache and ice-cream headache: a lack of caf-
feine causes the former, an excess of ice-cream ? the
latter. Different interpretations can lead to different
inferences, query expansion, paraphrases, transla-
tions, and so on. A question answering system may
have to determine whether protein acting as a tumor
suppressor is an accurate paraphrase for tumor sup-
pressor protein. An information extraction system
might need to decide whether neck vein thrombosis
and neck thrombosis can co-refer in the same doc-
ument. A machine translation system might para-
phrase the unknown compound WTO Geneva head-
quarters as WTO headquarters located in Geneva.
138
Research on the automatic interpretation of NCs
has focused mainly on common two-word NCs. The
usual task is to classify the semantic relation under-
lying a compound with either one of a small number
of predefined relation labels or a paraphrase from an
open vocabulary. Examples of the former take on
classification include (Moldovan et al, 2004; Girju,
2007; O? Se?aghdha and Copestake, 2008; Tratz and
Hovy, 2010). Examples of the latter include (Nakov,
2008b; Nakov, 2008a; Nakov and Hearst, 2008; But-
nariu and Veale, 2008) and a previous NC paraphras-
ing task at SemEval-2010 (Butnariu et al, 2010),
upon which the task described here builds.
The assumption of a small inventory of prede-
fined relations has some advantages ? parsimony and
generalization ? but at the same time there are lim-
itations on expressivity and coverage. For exam-
ple, the NCs headache pills and fertility pills would
be assigned the same semantic relation (PURPOSE)
in most inventories, but their relational semantics
are quite different (Downing, 1977). Furthermore,
the definitions given by human subjects can involve
rich and specific meanings. For example, Down-
ing (1977) reports that a subject defined the NC
oil bowl as ?the bowl into which the oil in the en-
gine is drained during an oil change?, compared to
which a minimal interpretation bowl for oil seems
very reductive. In view of such arguments, linguists
such as Downing (1977), Ryder (1994) and Coulson
(2001) have argued for a fine-grained, essentially
open-ended space of interpretations.
The idea of working with fine-grained para-
phrases for NC semantics has recently grown in pop-
ularity among NLP researchers (Butnariu and Veale,
2008; Nakov and Hearst, 2008; Nakov, 2008a). Task
9 at SemEval-2010 (Butnariu et al, 2010) was de-
voted to this methodology. In that previous work,
the paraphrases provided by human subjects were
required to fit a restrictive template admitting only
verbs and prepositions occurring between the NC?s
constituent nouns. Annotators recruited through
Amazon Mechanical Turk were asked to provide
paraphrases for the dataset of NCs. The gold stan-
dard for each NC was the ranked list of paraphrases
given by the annotators; this reflects the idea that a
compound?s meaning can be described in different
ways, at different levels of granularity and capturing
different interpretations in the case of ambiguity.
For example, a plastic saw could be a saw made
of plastic or a saw for cutting plastic. Systems par-
ticipating in the task were given the set of attested
paraphrases for each NC, and evaluated according to
how well they could reproduce the humans? ranking.
The design of this task, SemEval-2013 Task 4,
is informed by previous work on compound anno-
tation and interpretation. It is also influenced by
similar initiatives, such as the English Lexical Sub-
stitution task at SemEval-2007 (McCarthy and Nav-
igli, 2007), and by various evaluation exercises in
the fields of paraphrasing and machine translation.
We build on SemEval-2010 Task 9, extending the
task?s flexibility in a number of ways. The restric-
tions on the form of annotators? paraphrases was re-
laxed, giving us a rich dataset of close-to-freeform
paraphrases (Section 3). Rather than ranking a set of
attested paraphrases, systems must now both gener-
ate and rank their paraphrases; the task they perform
is essentially the same as what the annotators were
asked to do. This new setup required us to innovate
in terms of evaluation measures (Section 4).
We anticipate that the dataset and task will be of
broad interest among those who study lexical se-
mantics. We believe that the overall progress in the
field will significantly benefit from a public-domain
set of free-style NC paraphrases. That is why our
primary objective is the challenging endeavour of
preparing and releasing such a dataset to the re-
search community. The common evaluation task
which we establish will also enable researchers to
compare their algorithms and their empirical results.
2 Task description
This is an English NC interpretation task, which ex-
plores the idea of interpreting the semantics of NCs
via free paraphrases. Given a noun-noun compound
such as air filter, the participating systems are asked
to produce an explicitly ranked list of free para-
phrases, as in the following example:
1 filter for air
2 filter of air
3 filter that cleans the air
4 filter which makes air healthier
5 a filter that removes impurities from the air
. . .
139
Such a list is then automatically compared and
evaluated against a similarly ranked list of para-
phrases proposed by human annotators, recruited
and managed via Amazon?s Mechanical Turk. The
comparison of raw paraphrases is sensitive to syn-
tactic and morphological variation. The ranking
of paraphrases is based on their relative popular-
ity among different annotators. To make the rank-
ing more reliable, highly similar paraphrases are
grouped so as to downplay superficial differences in
syntax and morphology.
3 Data collection
We used Amazon?s Mechanical Turk service to
collect diverse paraphrases for a range of ?gold-
standard? NCs.1 We paid the workers a small fee
($0.10) per compound, for which they were asked to
provide five paraphrases. Each paraphrase should
contain the two nouns of the compound (in sin-
gular or plural inflectional forms, but not in an-
other derivational form), an intermediate non-empty
linking phrase and optional preceding or following
terms. The paraphrasing terms could have any part
of speech, so long as the resulting paraphrase was a
well-formed noun phrase headed by the NC?s head.
We gave the workers feedback during data col-
lection if they appeared to have misunderstood the
nature of the task. Once raw paraphrases had been
collected from all workers, we collated them into a
spreadsheet, and we merged identical paraphrases
in order to calculate their overall frequencies. Ill-
formed paraphrases ? those violating the syntactic
restrictions described above ? were manually re-
moved following a consensus decision-making pro-
cedure; every paraphrase was checked by at least
two task organizers. We did not require that the
paraphrases be semantically felicitous, but we per-
formed minor edits on the remaining paraphrases if
they contained obvious typos.
The remaining well-formed paraphrases were
sorted by frequency separately for each NC. The
most frequent paraphrases for a compound are as-
signed the highest rank 0, those with the next-
highest frequency are given a rank of 1, and so on.
1Since the annotation on Mechanical Turk was going slowly,
we also recruited four other annotators to do the same work,
following exactly the same instructions.
Total Min / Max / Avg
Trial/Train (174 NCs)
paraphrases 6,069 1 / 287 / 34.9
unique paraphrases 4,255 1 / 105 / 24.5
Test (181 NCs)
paraphrases 9,706 24 / 99 / 53.6
unique paraphrases 8,216 21 / 80 / 45.4
Table 1: Statistics of the trial and test datasets: the total
number of paraphrases with and without duplicates, and
the minimum / maximum / average per noun compound.
Paraphrases with a frequency of 1 ? proposed for
a given NC by only one annotator ? always occupy
the lowest rank on the list for that compound.
We used 174+181 noun-noun compounds from
the NC dataset of O? Se?aghdha (2007). The trial
dataset, which we initially released to the partici-
pants, consisted of 4,255 human paraphrases for 174
noun-noun pairs; this dataset was also the training
dataset. The test dataset comprised paraphrases for
181 noun-noun pairs. The ?gold standard? contained
9,706 paraphrases of which 8,216 were unique for
those 181 NCs. Further statistics on the datasets are
presented in Table 1.
Compared with the data collected for the
SemEval-2010 Task 9 on the interpretation of noun
compounds, the data collected for this new task have
a far greater range of variety and richness. For ex-
ample, the following (selected) paraphrases for work
area vary from parsimonious to expansive:
? area for work
? area of work
? area where work is done
? area where work is performed
? . . .
? an area cordoned off for persons responsible for
work
? an area where construction work is carried out
? an area where work is accomplished and done
? area where work is conducted
? office area assigned as a work space
? . . .
140
4 Scoring
Noun compounding is a generative aspect of lan-
guage, but so too is the process of NC interpretation:
human speakers typically generate a range of possi-
ble interpretations for a given compound, each em-
phasizing a different aspect of the relationship be-
tween the nouns. Our evaluation framework reflects
the belief that there is rarely a single right answer
for a given noun-noun pairing. Participating systems
are thus expected to demonstrate some generativity
of their own, and are scored not just on the accu-
racy of individual interpretations, but on the overall
breadth of their output.
For evaluation, we provided a scorer imple-
mented, for good portability, as a Java class. For
each noun compound to be evaluated, the scorer
compares a list of system-suggested paraphrases
against a ?gold-standard? reference list, compiled
and rank-ordered from the paraphrases suggested
by our human annotators. The score assigned to
each system is the mean of the system?s performance
across all test compounds. Note that the scorer re-
moves all determiners from both the reference and
the test paraphrases, so a system is neither punished
for not reproducing a determiner or rewarded for
producing the same determiners.
The scorer can match words identically or non-
identically. A match of two identical words Wgold
and Wtest earns a score of 1.0. There is a partial
score of (2 |P | / (|PWgold| + |PWtest|))2 for a
match of two words PWgold and PWtest that are
not identical but share a common prefix P , |P | > 2,
e.g., wmatch(cutting, cuts) = (6/11)2 = 0.297.
Two n-grams Ngold = [GW1, . . . , GWn] and
Ntest = [TW1, . . . , TWn] can be matched if
wmatch(GWi, TWi) > 0 for all i in 1..n. The
score assigned to the match of these two n-grams is
then
?
i wmatch(GWi, TWi). For every n-gram
Ntest = [TW1, . . . , TWn] in a system-generated
paraphrase, the scorer finds a matching n-gram
Ngold = [GW1, . . . , GWn] in the reference para-
phrase Paragold which maximizes this sum.
The overall n-gram overlap score for a reference
paraphrase Paragold and a system-generated para-
phrase Paratest is the sum of the score calculated
for all n-grams in Paratest, where n ranges from 1
to the size of Paratest.
This overall score is then normalized by dividing
by the maximum value among the n-gram overlap
score for Paragold compared with itself and the n-
gram overlap score for Paratest compared with it-
self. This normalization step produces a paraphrase
match score in the range [0.0 ? 1.0]. It punishes a
paraphrase Paratest for both over-generating (con-
taining more words than are found in Paragold)
and under-generating (containing fewer words than
are found in Paragold). In other words, Paratest
should ideally reproduce everything in Paragold,
and nothing more or less.
The reference paraphrases in the ?gold standard?
are ordered by rank; the highest rank is assigned to
the paraphrases which human judges suggested most
often. The rank of a reference paraphrase matters
because a good participating system will aim to re-
produce the top-ranked ?gold-standard? paraphrases
as produced by human judges. The scorer assigns
a multiplier of R/(R + n) to reference paraphrases
at rank n; this multiplier asymptotically approaches
0 for the higher values of n of ever lower-ranked
paraphrases. We choose a default setting of R = 8,
so that a reference paraphrase at rank 0 (the highest
rank) has a multiplier of 1, while a reference para-
phrase at rank 5 has a multiplier of 8/13 = 0.615.
When a system-generated paraphrase Paratest is
matched with a reference paraphrase Paragold, their
normalized n-gram overlap score is scaled by the
rank multiplier attaching to the rank of Paragold rel-
ative to the other reference paraphrases provided by
human judges. The scorer automatically chooses the
reference paraphrase Paragold for a test paraphrase
Paratest so as to maximize this product of normal-
ized n-gram overlap score and rank multiplier.
The overall score assigned to each system for
a specific compound is calculated in two differ-
ent ways: using isomorphic matching of suggested
paraphrases to the ?gold-standard?s? reference para-
phrases (on a one-to-one basis); and using non-
isomorphic matching of system?s paraphrases to the
?gold-standard?s? reference paraphrases (in a poten-
tially many-to-one mapping).
Isomorphic matching rewards both precision and
recall. It rewards a system for accurately reproduc-
ing the paraphrases suggested by human judges, and
for reproducing as many of these as it can, and in
much the same order.
141
In isomorphic mode, system?s paraphrases are
matched 1-to-1 with reference paraphrases on a first-
come first-matched basis, so ordering can be crucial.
Non-isomorphic matching rewards only preci-
sion. It rewards a system for accurately reproducing
the top-ranked human paraphrases in the ?gold stan-
dard?. A system will achieve a higher score in a non-
isomorphic match if it reproduces the top-ranked hu-
man paraphrases as opposed to lower-ranked human
paraphrases. The ordering of system?s paraphrases
is thus not important in non-isomorphic matching.
Each system is evaluated using the scorer in both
modes, isomorphic and non-isomorphic. Systems
which aim only for precision should score highly
on non-isomorphic match mode, but poorly in iso-
morphic match mode. Systems which aim for pre-
cision and recall will face a more substantial chal-
lenge, likely reflected in their scores.
A na??ve baseline
We decided to allow preposition-only paraphrases,
which are abundant in the paraphrases suggested
by human judges in the crowdsourcing Mechanical
Turk collection process. This abundance means that
the top-ranked paraphrase for a given compound is
often a preposition-only phrase, or one of a small
number of very popular paraphrases such as used for
or used in. It is thus straightforward to build a na??ve
baseline generator which we can expect to score
reasonably on this task, at least in non-isomorphic
matching mode. For each test compound M H,
the baseline system generates the following para-
phrases, in this precise order: H of M, H in M, H
for M, H with M, H on M, H about M, H has M, H to
M, H used for M, H used in M.
This na??ve baseline is truly unsophisticated. No
attempt is made to order paraphrases by their corpus
frequencies or by their frequencies in the training
data. The same sequence of paraphrases is generated
for each and every test compound.
5 Results
Three teams participated in the challenge, and all
their systems were supervised. The MELODI sys-
tem relied on semantic vector space model built
from the UKWAC corpus (window-based, 5 words).
It used only the features of the right-hand head noun
to train a maximum entropy classifier.
Team isomorphic non-isomorphic
SFS 23.1 17.9
IIITH 23.1 25.8
MELODI-Primary 13.0 54.8
MELODI-Contrast 13.6 53.6
Naive Baseline 13.8 40.6
Table 2: Results for the participating systems; the base-
line outputs the same paraphrases for all compounds.
The IIITH system used the probabilities of the
preposition co-occurring with a relation to identify
the class of the noun compound. To collect statis-
tics, it used Google n-grams, BNC and ANC.
The SFS system extracted templates and fillers
from the training data, which it then combined with
a four-gram language model and a MaxEnt reranker.
To find similar compounds, they used Lin?s Word-
Net similarity. They further used statistics from the
English Gigaword and the Google n-grams.
Table 2 shows the performance of the partici-
pating systems, SFS, IIITH and MELODI, and the
na??ve baseline. The baseline shows that it is rela-
tively easy to achieve a moderately good score in
non-isomorphic match mode by generating a fixed
set of paraphrases which are both common and
generic: two of the three participating systems,
SFS and IIITH, under-perform the na??ve baseline
in non-isomorphic match mode, but outperform it
in isomorphic mode. The only system to surpass
this baseline in non-isomorphic match mode is the
MELODI system; yet, it under-performs against the
same baseline in isomorphic match mode. No par-
ticipating team submitted a system which would out-
perform the na??ve baseline in both modes.
6 Conclusions
The conclusions we draw from the experience of or-
ganizing the task are mixed. Participation was rea-
sonable but not large, suggesting that NC paraphras-
ing remains a niche interest ? though we believe it
deserves more attention among the broader lexical
semantics community and hope that the availabil-
ity of our freeform paraphrase dataset will attract a
wider audience in the future.
142
We also observed a varied response from our an-
notators in terms of embracing their freedom to gen-
erate complex and rich paraphrases; there are many
possible reasons for this including laziness, time
pressure and the fact that short paraphrases are often
very appropriate paraphrases. The results obtained
by our participants were also modest, demonstrating
that compound paraphrasing is both a difficult task
and a novel one that has not yet been ?solved?.
Acknowledgments
This work has partially supported by a small but ef-
fective grant from Amazon; the credit allowed us
to hire sufficiently many Turkers ? thanks! And a
thank-you to our additional annotators Dave Carter,
Chris Fournier and Colette Joubarne for their com-
plete sets of paraphrases of the noun compounds in
the test data.
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of complex nominals: Getting it right.
Proc. ACL04 Workshop on Multiword Expressions: In-
tegrating Processing, Barcelona, Spain, 24-31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK, 81-
88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. Proc. 5th International ACL Workshop on Se-
mantic Evaluation, Uppsala, Sweden, 39-44.
Seana Coulson. 2001. Semantic Leaps: Frame-Shifting
and Conceptual Blending in Meaning Construction.
Cambridge University Press, Cambridge, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4): 810-842.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. Proc.
45th Annual Meeting of the Association of Computa-
tional Linguistics, Prague, Czech Republic, 568-575.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
semantic relations in noun compounds via verb seman-
tics. Proc. ACL-06 Main Conference Poster Session,
Sydney, Australia, 491-498.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. Proc.
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, 48-53.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Dan Moldovan
and Roxana Girju, eds., HLT-NAACL 2004: Workshop
on Computational Lexical Semantics, Boston, MA,
USA, 60-67.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the Web as a corpus.
Proc. 46th Annual Meeting of the Association for Com-
putational Linguistics ACL-08, Columbus, OH, USA,
452-460.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. Proc. 18th
European Conference on Artificial Intelligence ECAI-
08, Patras, Greece, 338-342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. Proc.
13th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications AIMSA-
08, Varna, Bulgaria, Lecture Notes in Computer Sci-
ence 5253, Springer, 103-117.
Diarmuid O? Se?aghdha. 2007. Designing and Evaluating
a Semantic Annotation Scheme for Compound Nouns.
In Proceedings of the 4th Corpus Linguistics Confer-
ence, Birmingham, UK.
Diarmuid O? Se?aghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Report
735.
Diarmuid O? Se?aghdha and Ann Copestake. 2009. Using
lexical and relational similarity to classify semantic re-
lations. Proc. 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
EACL-09, Athens, Greece, 621-629.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA, USA.
Takaaki Tanaka and Tim Baldwin. 2003. Noun-noun
compound machine translation: A feasibility study
on shallow processing. Proc. ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, 17-24.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. Proc. 48th Annual Meeting of the As-
sociation for Computational Linguistics ACL-10, Up-
psala, Sweden, 678-687.
143
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 140?146,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Hierarchical versus Flat Classification of Emotions in Text 
 
Diman Ghazi (a), Diana Inkpen (a), Stan Szpakowicz (a, b) 
(a) School of Information Technology and Engineering, University of Ottawa 
(b) Institute of Computer Science, Polish Academy of Sciences 
{dghaz038,diana,szpak}@site.uottawa.ca 
 
 
 
Abstract 
We explore the task of automatic classifica-
tion of texts by the emotions expressed. Our 
novel method arranges neutrality, polarity and 
emotions hierarchically. We test the method 
on two datasets and show that it outperforms 
the corresponding ?flat? approach, which does 
not take into account the hierarchical informa-
tion. The highly imbalanced structure of most 
of the datasets in this area, particularly the two 
datasets with which we worked, has a dramat-
ic effect on the performance of classification. 
The hierarchical approach helps alleviate the 
effect. 
1 Introduction 
Computational approaches to emotion analysis 
have focused on various emotion modalities, but 
there was only limited effort in the direction of 
automatic recognition of emotion in text (Aman, 
2007). 
Oleveres et al(1998), as one of the first works in 
emotion detection in text, uses a simple Natural 
Language Parser for keyword spotting, phrase 
length measurement and emoticon identification. 
They apply a rule-based expert system to construct 
emotion scores based on the parsed text and con-
textual information. However their simple word-
level analysis system is not sufficient when the 
emotion is expressed by more complicated phrases 
and sentences. 
More advanced systems for textual emotion recog-
nition performed sentence-level analysis.  Liu et al 
(2003), proposed an approach aimed at understand-
ing the underlying semantics of language using 
large-scale real-world commonsense knowledge to 
classify sentences into ?basic? emotion categories. 
They developed a commonsense affect model 
enabling the analysis of the affective qualities of 
text in a robust way.  
In SemEval 2007, one of the tasks was carried out 
in an unsupervised setting and the emphasis was on 
the study of emotion in lexical semantics (Strappa-
rava and Mihalcea, 2008; Chaumartin, 2007; Koza-
reva et al, 2007; Katz et al, 2007). Neviarouskaya 
et al(2009) applied a rule-based approach to affect 
recognition from a blog text. However, statistical 
and machine learning approaches have became a 
method of choice for constructing a wide variety of 
NLP applications (Wiebe et al, 2005). 
There has been previous work using statistical 
methods and supervised machine learning, includ-
ing (Aman, 2007; Katz et al, 2007; Alm, 2008; 
Wilson et al, 2009). Most of that research concen-
trated on feature selections and applying lexical 
semantics rather than on different learning 
schemes. In particular, only flat classification has 
been considered. 
According to Kiritchenko et al (2006), ?Hierar-
chical categorization deals with categorization 
problems where categories are organized in hierar-
chies?. Hierarchical text categorization places new 
items into a collection with a predefined hierar-
chical structure. The categories are partially or-
dered, usually from more generic to more specific.  
Koller and Sahami (1997) carried out the first 
proper study of a hierarchical text categorization 
problem in 1997. More work in hierarchical text 
categorization has been reported later. Keshtkar 
and Inkpen (2009) applied a hierarchical approach 
to mood classification: classifying blog posts into 
132 moods. The connection with our work is only 
indirect, because ? even though moods and emo-
tions may seem similar ? their hierarchy structure 
and the classification task are quite different. The 
work reported in (Kiritchenko et al, 2006) is more 
general. It explores two main aspects of hierarchic-
140
al text categorization: learning algorithms and per-
formance evaluation.  
In this paper, we extend our preliminary work 
(Ghazi et al, 2010) on hierarchical classification. 
Hierarchical classification is a new approach to 
emotional analysis, which considers the relation 
between neutrality, polarity and emotion of a text. 
The main idea is to arrange these categories and 
their interconnections into a hierarchy and leverage 
it in the classification process. 
We categorize sentences into six basic emotion 
classes; there also may, naturally, be no emotion in 
a sentence. The emotions are happiness, sadness, 
fear, anger, disgust, and surprise (Ekman, 1992). 
In one of the datasets we applied, we did consider 
the class non-emotional. 
For these categories, we have considered two 
forms of hierarchy for classification, with two or 
three levels. In the two-level method, we explore 
the effect of neutral instances on one dataset and 
the effect of polarity on the other dataset. In the 
three-level hierarchy, we consider neutrality and 
polarity together. 
Our experiments on data annotated with emotions 
show performance which exceeds that of the corre-
sponding flat approach. 
Section 2 of this paper gives an overview of the 
datasets and feature sets. Section 3 describes both 
hierarchical classification methods and their 
evaluation with respect to flat classification results. 
Section 4 discusses future work and presents a few 
conclusions. 
2 Data and Feature Sets 
2.1 Datasets 
The statistical methods typically require training 
and test corpora, manually annotated with respect 
to each language-processing task to be learned 
(Wiebe et al, 2005). One of the datasets in our 
experiments is a corpus of blog sentences anno-
tated with Ekman?s emotion labels (Aman, 2007). 
The second dataset is a sentence-annotated corpus 
resource divided into three parts for large-scale 
exploration of affect in children?s stories (Alm, 
2008). 
In the first dataset, each sentence is tagged by a 
dominant emotion in the sentence, or labelled as 
non-emotional. The dataset contains 173 weblog 
posts annotated by two judges. Table 1 shows the 
details of the dataset. 
In the second dataset, two annotators have anno-
tated 176 stories. The affects considered are the 
same as Ekman?s six emotions, except that the 
surprise class is subdivided into positive surprise 
and negative surprise. We run our experiments on 
only sentences with high agreement- sentences 
with the same affective labels annotated by both 
annotators. That is the version of the dataset which 
merged angry and disgusted instances and com-
bined the positive and negative surprise classes. 
The resulting dataset, therefore, has only five 
classes (Alm, 2008). Table 1 presents more details 
about the datasets, including the range of frequen-
cies for the class distribution (Min is the proportion 
of sentences with the most infrequent class, Max is 
the proportion for sentences with the most frequent 
class.) The proportion of the most frequent class 
also gives us a baseline for the accuracies of our 
classifiers (since the poorest baseline classifier 
could always choose the most frequent class).  
Table 1. Datasets specifications. 
 Domain Size # classes Min-Max% 
Aman?s 
Data set 
Weblogs 2090 7 6-38 % 
Alm?s 
Data set 
Stories 1207 5 9-36% 
2.2 Feature sets 
In (Ghazi et al, 2010), three sets of features ? one 
corpus-based and two lexically-based ? are com-
pared on Aman?s datasets. The first experiment is a 
corpus-based classification which uses unigrams 
(bag-of-words). In the second experiment, classifi-
cation was based on features derived from the 
Prior-Polarity lexicon1 (Wilson et al 2009); the 
features were the tokens common between the 
prior-polarity lexicon and the chosen dataset. In the 
last experiment, we used a combination of the 
emotional lists of words from Roget?s Thesaurus2 
(Aman and Szpakowicz, 2008) and WordNet Af-
fect3 (Strapparava and Valitutti, 2004); we call it 
the polarity feature set.  
                                                 
1 www.cs.pitt.edu/mpqa 
2 The 1987 Penguin?s Roget?s Thesaurus was used. 
3
 www.cse.unt.edu/~rada/affective 
text/data/WordNetAffectEmotioLists.tar.gz 
141
Based on the results and the discussion in (Ghazi et 
al., 2010), we decided to use the polarity feature 
set in our experiments. This feature set has certain 
advantages. It is quite a bit smaller than the uni-
gram features, and we have observed that they ap-
pear to be more meaningful. For example, the 
unigram features include (inevitably non-
emotional) names of people and countries. It is 
also possible to have misspelled tokens in uni-
grams, while the prior-polarity lexicon features are 
well-defined words usually considered as polar. 
Besides, lexical features are known to be more 
domain- and corpus-independent. Last but not 
least, our chosen feature set significantly outper-
forms the third set. 
2.3 Classification 
As a classification algorithm, we use the support 
vector machines (SVM) algorithm with tenfold 
cross-validation as a testing option. It is shown that 
SVM obtains good performance in text classifica-
tion: it scales well to the large numbers of features 
(Kennedy and Inkpen, 2006; Aman, 2007).  
We apply the same settings at each level of the 
hierarchy for our hierarchical approach classifica-
tion.  
In hierarchical categorization, categories are organ-
ized into levels (Kiritchenko et al, 2006). We use 
the hierarchical categories to put more knowledge 
into our classification method as the category hier-
archies are carefully composed manually to repre-
sent our knowledge of the subject. We will achieve 
that in two forms of hierarchy. A two-level hierar-
chy represents the relation of emotion and neutral-
ity in text, as well as the relation of positive and 
negative polarity. These two relations are exam-
ined in two different experiments, each on a sepa-
rate dataset. 
A three-level hierarchy is concerned with the rela-
tion between polarity and emotions along with the 
relation between neutrality and emotion. We as-
sume that, of Ekman's six emotions, happiness be-
longs to the positive polarity class, while the other 
five emotions have negative polarity. This is quite 
similar to the three-level hierarchy of affect labels 
used by Alm (2008). In her diagram, she considers 
happiness and positive surprise as positive, and the 
rest as negative emotions. She has not, however, 
used this model in the classification approach: 
classification experiments were only run at three 
separate affect levels. She also considers positive 
and negative surprise as one Surprise class. 
For each level of our proposed hierarchy, we run 
two sets of experiments. In the first set, we assume 
that all the instances are correctly classified at the 
preceding levels, so we only need to be concerned 
with local mistakes. Because we do not have to 
deal with instances misclassified at the previous 
level, we call these results reference results.  
In the second set of experiments, the methodology 
is different than in (Ghazi et al 2010). In that work 
both training and testing of subsequent levels is 
based on the results of preceding levels. A question 
arises, however: once we have good data available, 
why train on incorrect data which result from mis-
takes at the preceding level? That is why we de-
cided to train on correctly-labelled data and when 
testing, to compute global results by cumulating 
the mistakes from all the levels of the hierarchical 
classification. In other words, classification mis-
takes at one level of the hierarchy carry on as mis-
takes at the next levels. Therefore, we talk of 
global results because we compute the accuracy, 
precision, recall and F-measure globally, based on 
the results at all levels. These results characterize 
the hierarchical classification approach when test-
ing on new sentences: the classifiers are applied in 
a pipeline order: level 1, then level 2 on the results 
of the previous level (then level 3 if we are in the 
three-level setting).   
In the next section, we show the experiments and 
results on our chosen datasets. 
 
3 Results and discussions 
3.1 Two-level classification 
This section has two parts. The main goal of the 
first part is to find out how the presence of neutral 
instances affects the performance of features for 
distinguishing between emotional classes in 
Aman?s dataset. This was motivated by a similar 
work in polarity classification (Wilson et al, 
2009). 
In the second part, we discuss the effect of consid-
ering positive and negative polarity of emotions for 
five affect classes in Alm?s dataset. 
142
3.1.1 Neutral-Emotional 
At the first level, emotional versus non-emotional 
classification tries to determine whether an in-
stance is neutral or emotional. The second step 
takes all instances which level 1 classified as emo-
tional, and tries to classify them into one of Ek-
man's six emotions. Table 2 presents the result of 
experiments and, for comparison, the flat classifi-
cation results. A comparison of the results in both 
experiments with flat classification shows that in 
both cases the accuracy of two-level approach is 
significantly better than the accuracy of flat classi-
fication. 
One of the results worth discussing further is the 
precision of the non-emotional class: it increases 
while recall decreases. We will see the same pat-
tern in further experiments. This happens to the 
classes which used to dominate in flat classifica-
tion but they no longer dominate in hierarchical 
classification. Classifiers tends to give priority to a 
dominant class, so more instances are placed in 
this class; thus, classification achieves low preci-
sion and high recall. Hierarchical methods tend to 
produce higher precision. 
The difference between precision and recall of the 
happiness class in the flat approach and the two-
level approach cannot be ignored. It can be ex-
plained as follows: at the second level there are no 
more non-emotional instances, so the happiness 
class dominates, with 42% of all the instances. As 
explained before, this gives high recall and low 
precision for the happiness class. We hope to ad-
dress this big gap between precision and recall of 
the happiness class in the next experiments, three-
level classification. It separates happiness from the 
other five emotions, so it makes the number of in-
stances of each level more balanced. 
Our main focus is comparing hierarchical and flat 
classification, assuming all the other parameters 
are fixed. We mention, however, the best previous 
results achieved by Aman (2007) on the same data-
set. Her best result was obtained by combining 
corpus-based unigrams, features derived from 
emotional lists of words from Roget?s Thesaurus 
(explained in 2.2) and common words between the 
dataset and WordNetAffect. She also applied SVM 
with tenfold cross validation. The results appear in 
Table 3. 
     Table 3. Aman?s best results on her data set. 
 Precision Recall F-Measure 
happiness 0.813  0.698  0.751  
sadness  0.605  0.416  0.493  
fear  0.868  0.513  0.645  
surprise  0.723  0.409  0.522  
disgust  0.672  0.488  0.566  
anger  0.650  0.436  0.522  
non-emo 0.587  0.625  0.605  
 
 
 
Table 2. Two-level emotional classification on Aman?s dataset (the highest precision, recall, and F-measure val-
ues for each class are shown in bold). The results of the flat classification are repeated for convenience. 
Two-level classification Flat classification 
 Precision Recall F-measure Precision Recall F-measure 
1st level emo non-emo 
0.88 
0.88 
0.85 
0.81 
0.86 
0.84 
-- 
0.54 
-- 
0.87 
-- 
0.67 
2nd level 
reference results 
 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.59 
0.77 
0.91 
0.75 
0.66 
0.72 
0.95 
0.49 
0.49 
0.32 
0.35 
0.33 
0.71 
0.60 
0.63 
0.45 
0.45 
0.46 
0.74 
0.69 
0.82 
0.64 
0.68 
0.67 
0.60 
0.42 
0.49 
0.27 
0.31 
0.26 
0.66 
0.52 
0.62 
0.38 
0.43 
0.38 
Accuracy   68.32%   61.67%  
2-level experi-
ment 
global results 
non-emo 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.88 
0.56 
0.64 
0.75 
0.56 
0.52 
0.55 
0.81 
0.86 
0.42 
0.43 
0.29 
0.29 
0.27 
0.84 
0.68 
0.51 
0.55 
0.38 
0.37 
0.36 
0.54 
0.74 
0.69 
0.82 
0.64 
0.68 
0.67
0.87 
0.60 
0.42 
0.49 
0.27 
0.31 
0.26 
0.67 
0.66 
0.52 
0.62 
0.38 
0.43 
0.38 
Accuracy   65.50%   61.67%  
 
143
 By comparing the reference results in Table 2 with 
Aman?s result shown in Table 3, our results on two  
classes, non-emo and sadness are significantly bet-
ter. Even though recall of our experiments is high-
er for happiness class, the precision makes the F-
measure to be lower. The reason behind the differ-
ence between the precisions is the same as their 
difference between in our hierarchical and flat 
comparisons. As it was also mentioned there we 
hope to address this problem in three-level classifi-
cation. Both precision and recall of the sadness in 
our experiments is higher than Aman?s results. We 
have a higher precision for fear, but recall is 
slightly lower. For the last three classes our preci-
sion is higher while recall is significantly lower.  
 
The size of these three classes, which are the smal-
lest classes in the dataset, appears to be the reason. 
It is possible that the small set of features that we 
are using will recall fewer instances of these 
classes comparing to the bigger feature sets used 
by Aman (2007).  
3.1.2 Negative-Positive polarity 
These experiments have been run on Alm?s dataset 
with five emotion classes. This part is based on the 
assumption that the happiness class is positive and 
the remaining four classes are negative.  
 
 
At the first level, positive versus negative classifi-
cation tries to determine whether an instance bears 
a positive emotion. The second step takes all in-
stances which level 1 classified as negative, and 
tries to classify them into one of the four negative 
classes, namely sadness, fear, surprise and anger-
disgust. The results show a higher accuracy in ref-
erence results while it is slightly lower for global 
results. In terms of precision and recall, however, 
there is a high increase in precision of positive 
(happiness) class while the recall decreases. 
The results show a higher accuracy in reference 
results while it is slightly lower for global results. 
In terms of precision and recall, however, there is a 
high increase in precision of positive (happiness) 
class while the recall decreases. 
We also see a higher F-measure for all classes in 
the reference results. That confirms the consistency 
between the result in Table 2 and Table 4. 
In the global measurements, recall is higher for all 
the classes at the second level, but the F-measure is 
higher only for three classes. 
Here we cannot compare our results with the best 
previous results achieved by Alm (2008), because 
the datasets and the experiments are not the same. 
She reports the accuracy of the classification re-
sults for three sub-corpora separately. She random-
ly selected neutral instances from the annotated 
data and added them to the dataset, which makes it 
Table 4. Two-level emotional classification on Alm?s dataset (the highest precision, recall, and F-measure val-
ues for each class are shown in bold). 
Two-level classification Flat classification 
 Precision Recall F-measure Precision Recall F-measure 
1st level neg pos 
0.81 
0.84 
0.93 
0.64 
0.87 
0.72 
-- 
0.56 
-- 
0.86 
-- 
0.68 
2nd level 
reference results 
 
sadness 
fear 
surprise 
anger 
0.65 
0.59 
0.45 
0.49 
0.68 
0.40 
0.21 
0.73 
0.66 
0.47 
0.29 
0.59 
0.67 
0.59 
0.35 
0.54 
0.53 
0.38 
0.10 
0.43 
0.59 
0.46 
0.16 
0.48 
Accuracy   59.07%   57.41%  
2-level experiment 
global results 
happiness 
sadness 
fear 
surprise 
anger 
0.84 
0.55 
0.45 
0.27 
0.43 
0.64 
0.61 
0.39 
0.21 
0.68 
0.72 
0.58 
0.42 
0.19 
0.53 
0.56 
0.67 
0.59 
0.35 
0.54 
0.86 
0.53 
0.38 
0.10 
0.43 
0.68 
0.59 
0.46 
0.16 
0.48 
Accuracy   56.57%   57.41%  
 
144
different than the data set we used in our experi-
ments.  
3.2 Three-level classification 
In this approach, we go even further: we break the 
seven-class classification task into three levels. 
The first level defines whether the instance is emo-
tional. At the second level the instances defined as 
emotional by the first level will be classified on 
their polarity. At the third level, we assume that the 
instances of happiness class have positive polarity 
and the other five emotions negative polarity. That 
is why we take the negative instances from the 
second level and classify them into the five nega-
tive emotion classes. Table 5 presents the results of  
this classification. The results show that the accu-
racy of both reference results and global results are 
higher than flat classification, but the accuracy of 
the global results is not significantly better. 
At the first and second level, the F-measure of no-
emotion and happiness classes is significantly bet-
ter. At the third level, except in the class disgust, 
we see an increase in the F-measure of all classes 
in comparison with both the two-level and flat 
classification. 
 
Table 5. Three-level emotional classification on Aman?s data-
set (the highest precision, recall, and F-measure values for 
each class are shown in bold) 
?
Three-level Classification 
 Precision Recall F 
1st level emo 
non-emo 
0.88 
0.88 
0.85 
0.81 
0.86 
0.84 
2nd level 
reference results 
positive 
negative 
0.89 
0.79 
0.65 
0.94 
0.75 
0.86 
3rd level 
reference results 
 
 
sadness 
fear 
surprise 
disgust 
anger 
0.63 
0.88 
0.79 
0.42 
0.38 
0.54 
0.52 
0.37 
0.38 
0.71 
0.59 
0.65 
0.50 
0.40 
0.49 
Accuracy   65.5%  
 
3-level experi-
ment 
global results 
non-emo 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.88 
0.77 
0.43 
0.52 
0.46 
0.31 
0.35 
0.81 
0.62 
0.49 
0.4 
0.32 
0.31 
0.55 
0.84 
0.69 
0.46 
0.45 
0.38 
0.31 
0.43 
Accuracy   62.2%  
Also, as shown by the two-level experiments, the 
results of the second level of the reference results 
approach an increase in the precision of the happi-
ness class. That makes the instances defined as 
happiness more precise. 
By comparing the results with Table 3, which is 
the best previous results, we see an increase in the 
precision of happiness class and its F-measure 
consequently; therefore in these results we get a 
higher F-measure for three classes, non-emo, sad-
ness and fear. We get the same F-measure for hap-
piness and slightly lower F-measure for surprise 
but we still have a lower F-measure for the other 
two classes, namely, disgust and anger. The other 
difference is the high increase in the recall value 
for fear. 
4 Conclusions and Future Work 
The focus of this study was a comparison of the 
hierarchical and flat classification approaches to 
emotional analysis and classification. In the emo-
tional classification we noticed that having a 
dominant class in the dataset degrades the results 
significantly. A classifier trained on imbalanced 
data gives biased results for the classes with more 
instances. Our results, based on a novel method, 
shows that the hierarchical classification approach 
is better at dealing with the highly imbalanced 
data. We also saw a considerable improvement in 
the classification results when we did not deal with 
the errors from previous steps and slightly better 
results when we evaluated the results globally. 
In the future, we will consider different levels of 
our hierarchy as different tasks which could be 
handled differently. Each of the tasks has its own 
specification. We can, therefore, definitely benefit 
from analyzing each task separately and defining 
different sets of features and classification methods 
for each task rather than using the same method for 
every task. 
145
References 
Alm, C.: ?Affect in text and speech?, PhD disserta-
tion, University of Illinois at Urbana-
Champaign, Department of Linguistics (2008) 
Aman, S.: ?Identifying Expressions of Emotion in 
Text?, Master's thesis, University of Ottawa, Ot-
tawa, Canada (2007) 
Aman, S., Szpakowicz, S.: ?Using Roget?s Thesau-
rus for Fine-grained Emotion Recognition?. 
Proc. Conf. on Natural Language Processing 
(IJCNLP), Hyderabad, India, 296-302 (2008) 
Chaumartin. F.: ?Upar7: A knowledge-based sys-
tem for headline sentiment tagging?, Proc. Se-
mEval-2007, Prague, Czech Republic, June 
(2007) 
Ekman, P.: ?An Argument for Basic Emotions?, 
Cognition and Emotion, 6, 169-200 (1992) 
Ghazi, D., Inkpen, D., Szpakowicz, S.: ?Hierar-
chical approach to emotion recognition and clas-
sification in texts?, A. Farzindar and V. Keselj 
(eds.), Proc. 23rd Canadian Conference on Ar-
tificial Intelligence, Ottawa, ON.  Lecture Notes 
in Artificial Intelligence 6085, Springer Verlag,  
40?50 (2010) 
Katz, P., Singleton, M., Wicentowski, R.: ?Swat-
mp: the semeval-2007 systems for task 5 and 
task 14?, Proc. SemEval-2007, Prague, Czech 
Republic, June (2007) 
Kennedy, A., Inkpen, D.: ?Sentiment classification 
of movie reviews using contextual valence shif-
ter?, Computational Intelligence 22. 110-125 
(2006) 
Keshtkar, F., Inkpen, D.: ?Using Sentiment Orien-
tation Features for Mood Classification in Blog 
Corpus?, IEEE International Conf. on NLP and 
KE, Dalian, China, Sep. 24-27 (2009) 
Kiritchenko, S., Matwin, S., Nock, R., Famili, 
F.: ?Learning and Evaluation in the Presence of 
Class Hierarchies: Application to Text Categori-
zation?, Lecture Notes in Artificial Intelligence 
4013, Springer, 395-406 (2006) 
Koller, D., Sahami, M.: ?Hierarchically Classify-
ing Documents Using Very Few Words?. Proc. 
International Conference on Machine Learning, 
170-178 (1997) 
Kozareva, Z., Navarro, B., Vazquez, S., Montoyo, 
A.: ?UA-ZBSA: A headline emotion classifica-
tion through web information?, Proc. SemEval-
2007, Prague, Czech Republic, June (2007) 
Liu, H., Lieberman, H., Selker, T.: ?A Model of 
Textual Affect Sensing using Real-World 
Knowledge?. In Proc. IUI 2003, 125-132 (2003) 
Neviarouskaya, A., Prendinger, H., and Ishizuka, 
M.: ?Compositionality Principle in Recognition 
of Fine-Grained Emotions from Text?, In: Pro-
ceedings of Third International Conference on 
Weblogs and Social Media (ICWSM?09), 
AAAI, San Jose, California, US, 278-281 (2009) 
Olveres, J., Billinghurst, M., Savage, J., Holden, 
A.: ?Intelligent, Expressive Avatars?. In Proc. of 
the WECC?98, 47-55 (1998) 
 Strapparava, C., Mihalcea, R.: ?SemEval-2007 
Task 14: Affective Text? (2007) 
Strapparava, C., Mihalcea, R.: ?Learning to Identi-
fy Emotions in Text?, Proc. ACM Symposium 
on Applied computing, Fortaleza, Brazil, 1556-
1560 (2008) 
Wilson, T., Wiebe, J., Hoffmann, P.: ?Recognizing 
contextual polarity: an exploration of features 
for phrase-level sentiment analysis?, Computa-
tional Linguistics 35(3), 399-433 (2009) 
 Wiebe, J., Wilson, T., Cardie, C.: ?Annotating 
Expressions of Opinions and Emotions in Lan-
guage?, Language Resources and Evaluation 39, 
165-210 (2005) 
146
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70?78,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Prior versus Contextual Emotion of a Word in a Sentence
Diman Ghazi
EECS, University of Ottawa
dghaz038@uottawa.ca
Diana Inkpen
EECS, University of Ottawa
diana@eecs.uottawa.ca
Stan Szpakowicz
EECS, University of Ottawa &
ICS, Polish Academy of Sciences
szpak@eecs.uottawa.ca
Abstract
A set of words labelled with their prior emo-
tion is an obvious place to start on the auto-
matic discovery of the emotion of a sentence,
but it is clear that context must also be con-
sidered. No simple function of the labels on
the individual words may capture the overall
emotion of the sentence; words are interre-
lated and they mutually influence their affect-
related interpretation. We present a method
which enables us to take the contextual emo-
tion of a word and the syntactic structure of the
sentence into account to classify sentences by
emotion classes. We show that this promising
method outperforms both a method based on
a Bag-of-Words representation and a system
based only on the prior emotions of words.
The goal of this work is to distinguish auto-
matically between prior and contextual emo-
tion, with a focus on exploring features impor-
tant for this task.
1 Introduction
Recognition, interpretation and representation of af-
fect have been investigated by researchers in the
field of affective computing (Picard 1997). They
consider a wide range of modalities such as affect in
speech, facial display, posture and physiological ac-
tivity. It is only recently that there has been a grow-
ing interest in automatic identification and extraction
of sentiment, opinions and emotions in text.
Sentiment analysis is the task of identifying posi-
tive and negative opinions, emotions and evaluations
(Wilson, Wiebe, and Hoffmann, 2005). Most of the
current work in sentiment analysis has focused on
determining the presence of sentiment in the given
text, and on determining its polarity ? the positive or
negative orientation. The applications of sentiment
analysis range from classifying positive and nega-
tive movie reviews (Pang, Lee, and Vaithyanathan,
2002; Turney, 2002) to opinion question-answering
(Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie,
and Wiebe, 2005). The analysis of sentiment must,
however, go beyond differentiating positive from
negative emotions to give a systematic account of
the qualitative differences among individual emo-
tion (Ortony, Collins, and Clore, 1988).
In this work, we deal with assigning fine-grained
emotion classes to sentences in text. It might seem
that these two tasks are strongly tied, but the higher
level of classification in emotion recognition task
and the presence of certain degrees of similarities
between some emotion labels make categorization
into distinct emotion classes more challenging and
difficult. Particularly notable in this regard are two
classes, anger and disgust, which human annotators
often find hard to distinguish (Aman and Szpakow-
icz, 2007). In order to recognize and analyze affect
in written text ? seldom explicitly marked for emo-
tions ? NLP researchers have come up with a variety
of techniques, including the use of machine learn-
ing, rule-based methods and the lexical approach
(Neviarouskaya, Prendinger, and Ishizuka, 2011).
There has been previous work using statistical
methods and supervised machine learning applied to
corpus-based features, mainly unigrams, combined
with lexical features (Alm, Roth, and Sproat, 2005;
Aman and Szpakowicz, 2007; Katz, Singleton, and
Wicentowski, 2007). The weakness of such methods
70
is that they neglect negation, syntactic relations and
semantic dependencies. They also require large (an-
notated) corpora for meaningful statistics and good
performance. Processing may take time, and anno-
tation effort is inevitably high. Rule-based meth-
ods (Chaumartin, 2007; Neviarouskaya, Prendinger,
and Ishizuka, 2011) require manual creation of rules.
That is an expensive process with weak guaran-
tee of consistency and coverage, and likely very
task-dependent; the set of rules of rule-based af-
fect analysis task (Neviarouskaya, Prendinger, and
Ishizuka, 2011) can differ drastically from what un-
derlies other tasks such as rule-based part-of-speech
tagger, discourse parsers, word sense disambigua-
tion and machine translation.
The study of emotions in lexical semantics was
the theme of a SemEval 2007 task (Strapparava and
Mihalcea, 2007), carried out in an unsupervised set-
ting (Strapparava and Mihalcea, 2008; Chaumartin,
2007; Kozareva et al, 2007; Katz, Singleton, and
Wicentowski, 2007). The participants were encour-
aged to work with WordNet-Affect (Strapparava and
Valitutti, 2004) and SentiWordNet (Esuli and Sebas-
tiani, 2006). Word-level analysis, however, will not
suffice when affect is expressed by phrases which re-
quire complex phrase- and sentence-level analyses:
words are interrelated and they mutually influence
their affect-related interpretation. On the other hand,
words can have more than one sense, and they can
only be disambiguated in context. Consequently, the
emotion conveyed by a word in a sentence can differ
drastically from the emotion of the word on its own.
For example, according to the WordNet-Affect lex-
icon, the word ?afraid? is listed in the ?fear? cate-
gory, but in the sentence ?I am afraid it is going to
rain.? the word ?afraid? does not convey fear.
We refer to the emotion listed for a word in an
emotion lexicon as the word?s prior emotion. A
word?s contextual emotion is the emotion of the sen-
tence in which that word appears, taking the context
into account.
Our method combines several way of tackling the
problem. First, we find keywords listed in WordNet-
Affect and select the sentences which include emo-
tional words from that lexicon. Next, we study the
syntactic structure and semantic relations in the text
surrounding the emotional word. We explore fea-
tures important in emotion recognition, and we con-
happi- sad- anger dis- sur- fear total
ness ness gust prise
398 201 252 53 71 141 1116
Table 1: The distribution of labels in the WordNet-
Affect Lexicon.
sider their effect on the emotion expressed by the
sentence. Finally, we use machine learning to clas-
sify the sentences, represented by the chosen fea-
tures, by their contextual emotion.
We categorize sentences into six basic emotions
defined by Ekman (1992); that has been the choice
of most of previous related work. These emotions
are happiness, sadness, fear, anger, disgust and sur-
prise. There also may, naturally, be no emotion in a
sentence; that is tagged as neutral/non-emotional.
We evaluate our results by comparing our method
applied to our set of features with Support Vec-
tor Machine (SVM) applied to Bag-of-Words, which
was found to give the best performance among su-
pervised methods (Yang and Liu, 1999; Pang, Lee,
and Vaithyanathan, 2002; Aman and Szpakowicz,
2007; Ghazi, Inkpen, and Szpakowicz, 2010). We
show that our method is promising and that it out-
performs both a system which works only with prior
emotions of words, ignoring context, and a system
which applies SVM to Bag-of-Words.
Section 2 of this paper describes the dataset and
resources used. Section 3 discusses the features
which we use for recognizing contextual emotion.
Experiments and results are presented in Section 4.
In Section 5, we conclude and discuss future work.
2 Dataset and Resources
Supervised statistical methods typically require
training data and test data, manually annotated
with respect to each language-processing task to be
learned. In this section, we explain the dataset and
lexicons used in our experiments.
WordNet-Affect Lexicon (Strapparava and Vali-
tutti, 2004). The first resource we require is an
emotional lexicon, a set of words which indicate
the presence of a particular emotion. In our exper-
iments, we use WordNet-Affect, which contains six
lists of words corresponding to the six basic emo-
tion categories. It is the result of assigning a variety
71
Neutral Negative Positive Both
6.9% 59.7% 31.1% 0.3%
Table 2: The distribution of labels in the Prior-Polarity
Lexicon.
of affect labels to each synset in WordNet. Table 1
shows the distribution of words in WordNet-Affect.
Prior-Polarity Lexicon (Wilson, Wiebe, and
Hoffmann, 2009). The prior-polarity subjectivity
lexicon contains over 8000 subjectivity clues col-
lected from a number of sources. To create this
lexicon, the authors began with the list of subjec-
tivity clues extracted by Riloff (2003). The list
was expanded using a dictionary and a thesaurus,
and adding positive and negative word lists from
the General Inquirer.1 Words are grouped into
strong subjective and weak subjective clues; Table 2
presents the distribution of their polarity.
Intensifier Lexicon (Neviarouskaya, Prendinger,
and Ishizuka, 2010). It is a list of 112 modifiers (ad-
verbs). Two annotators gave coefficients for inten-
sity degree ? strengthening or weakening, from 0.0
to 2.0 ? and the result was averaged.
Emotion Dataset (Aman and Szpakowicz,
2007). The main consideration in the selection of
data for emotional classification task is that the data
should be rich in emotion expressions. That is why
we chose for our experiments a corpus of blog sen-
tences annotated with emotion labels, discussed by
Aman and Szpakowicz (2007). Each sentence is
tagged by its dominant emotion, or as non-emotional
if it does not include any emotion. The annotation is
based on Ekman?s six emotions at the sentence level.
The dataset contains 4090 annotated sentences, 68%
of which were marked as non-emotional. The highly
unbalanced dataset with non-emotional sentences as
by far the largest class, and merely 3% in the fear
and surprise classes, prompted us to remove 2000 of
the non-emotional sentences. We lowered the num-
ber of non-emotional sentences to 38% of all the
sentences, and thus reduced the imbalance. Table 3
shows the details of the chosen dataset.
1www.wjh.harvard.edu/?inquirer/
hp sd ag dg sr fr ne total
536 173 179 172 115 115 800 2090
Table 3: The distribution of labels in Aman?s modified
dataset. The labels are happiness, sadness, anger, dis-
gust, surprise, fear, no emotion.
3 Features
The features used in our experiments were motivated
both by the literature (Wilson, Wiebe, and Hoff-
mann, 2009; Choi et al, 2005) and by the explo-
ration of contextual emotion of words in the anno-
tated data. All of the features are counted based on
the emotional word from the lexicon which occurs in
the sentence. For ease of description, we group the
features into four distinct sets: emotion-word fea-
tures, part-of-speech features, sentence features and
dependency-tree features.
Emotion-word features. This set of features are
based on the emotion-word itself.
? The emotion of a word according to WordNet-
Affect (Strapparava and Valitutti, 2004).
? The polarity of a word according to the prior-
polarity lexicon (Wilson, Wiebe, and Hoff-
mann, 2009).
? The presence of a word in a small list of modi-
fiers (Neviarouskaya, Prendinger, and Ishizuka,
2010).
Part-of-speech features. Based on the Stanford
tagger?s output (Toutanova et al, 2003), every word
in a sentence gets one of the Penn Treebank tags.
? The part-of-speech of the emotional word it-
self, both according to the emotion lexicon and
Stanford tagger.
? The POS of neighbouring words in the same
sentence. We choose a window of [-2,2], as it
is usually suggested by the literature (Choi et
al., 2005).
Sentence features. For now we only consider the
number of words in the sentence.
Dependency-tree features. For each emotional
word, we create features based on the parse tree and
its dependencies produced by the Stanford parser
(Marneffe, Maccartney, and Manning, 2006). The
72
dependencies are all binary relations: a grammati-
cal relation holds between a governor (head) and a
dependent (modifier).
According to Mohammad and Turney (2010),2
adverbs and adjectives are some of the most
emotion-inspiring terms. This is not surprising con-
sidering that they are used to qualify a noun or a
verb; therefore to keep the number of features small,
among all the 52 different type of dependencies, we
only chose the negation, adverb and adjective modi-
fier dependencies.
After parsing the sentence and getting the de-
pendencies, we count the following dependency-tree
Boolean features for the emotional word.
? Whether the word is in a ?neg? dependency
(negation modifier): true when there is a nega-
tion word which modifies the emotional word.
? Whether the word is in a ?amod? dependency
(adjectival modifier): true if the emotional
word is (i) a noun modified by an adjective or
(ii) an adjective modifying a noun.
? Whether the word is in a ?advmod? depen-
dency (adverbial modifier): true if the emo-
tional word (i) is a non-clausal adverb or adver-
bial phrase which serves to modify the meaning
of a word, or (ii) has been modified by an ad-
verb.
We also have several modification features based
on the dependency tree. These Boolean features cap-
ture different types of relationships involving the cue
word.3 We list the feature name and the condition on
the cue word w which makes the feature true.
? Modifies-positive: w modifies a positive word
from the prior-polarity lexicon.
? Modifies-negative: w modifies a negative word
from the prior-polarity lexicon.
? Modified-by-positive: w is the head of the de-
pendency, which is modified by a positive word
from the prior-polarity lexicon.
? Modified-by-negative: w is the head of the
dependency, which is modified by a negative
word from the prior-polarity lexicon.
2In their paper, they also explain how they created an emo-
tion lexicon by crowd-sourcing, but ? to the best of our knowl-
edge ? it is not publicly available yet.
3The terms ?emotional word? and ?cue word? are used in-
terchangeably.
hp sd ag dg sr fr ne total
part 1 196 64 64 63 36 52 150 625
part 2 51 18 22 18 9 14 26 158
part 1+ 247 82 86 81 45 66 176 783
part 2
Table 4: The distribution of labels in the portions of
Aman?s dataset used in our experiments, named part 1,
part 2 and part 1+part 2. The labels are happiness, sad-
ness, anger, disgust, surprise, fear, no emotion.
? Modifies-intensifier-strengthen: w modifies a
strengthening intensifier from the intensifier
lexicon.
? Modifies-intensifier-weaken: w modifies a
weakening intensifier from the intensifier lex-
icon.
? Modified-by-intensifier-strengthen: w is the
head of the dependency, which is modified by
a strengthening intensifier from the intensifier
lexicon.
? Modified-by-intensifier-weaken: w is the head
of the dependency, which is modified by a
weakening intensifier from the intensifiers lex-
icon.
4 Experiments
In the experiments, we use the emotion dataset pre-
sented in Section 2. Our main consideration is to
classify a sentence based on the contextual emotion
of the words (known as emotional in the lexicon).
That is why in the dataset we only choose sentences
which contain at least one emotional word accord-
ing to WordNet-Affect. As a result, the number of
sentences chosen from the dataset will decrease to
783 sentences, 625 of which contain only one emo-
tional word and 158 sentences which contain more
than one emotional word. Their details are shown in
Table 4.
Next, we represent the data with the features pre-
sented in Section 3. Those features, however, were
defined for each emotional word based on their con-
text, so we will proceed differently for sentences
with one emotional word and sentences with more
than one emotional word.
? In sentences with one emotional word, we as-
sume the contextual emotion of the emotional
73
word is the same as the emotion assigned to the
sentence by the human annotators; therefore all
the 625 sentences with one emotional word are
represented with the set of features presented
in Section 3 and the sentence?s emotion will be
considered as their contextual emotion.
? For sentences with more than one emotional
word, the emotion of the sentence depends on
all emotional words and their syntactic and se-
mantic relations. We have 158 sentences where
no emotion can be assigned to the contextual
emotion of their emotional words, and all we
know is the dominant emotion of the sentence.
We will, therefore, have two different sets of ex-
periments. For the first set of sentences, the data are
all annotated, so we will take a supervised approach.
For the second set of sentences, we combine super-
vised and unsupervised learning. We train a clas-
sifier on the first set of data and we use the model
to classify the emotional words into their contextual
emotion in the second set of data. Finally, we pro-
pose an unsupervised method to combine the con-
textual emotion of all the emotional words in a sen-
tence and calculate the emotion of the sentence.
For evaluation, we report precision, recall, F-
measure and accuracy to compare the results. We
also define two baselines for each set of experiments
to compare our results with. The experiments are
presented in the next two subsections.
4.1 Experiments on sentences with one
emotional word
In these experiments, we explain first the baselines
and then the results of our experiments on the sen-
tences with only one emotional word.
Baseline
We develop two baseline systems to assess the dif-
ficulty of our task. The first baseline labels the sen-
tences the same as the most frequent class?s emo-
tion, which is a typical baseline in machine learning
tasks (Aman and Szpakowicz, 2007; Alm, Roth, and
Sproat, 2005). This baseline will result in 31% ac-
curacy.
The second baseline labels the emotion of the sen-
tence the same as the prior emotion of the only emo-
tional word in the sentence. The accuracy of this
Precision Recall F
SVM +
Bag-of-
Words
Happiness 0.59 0.67 0.63
Sadness 0.38 0.45 0.41
Anger 0.40 0.31 0.35
Surprise 0.41 0.33 0.37
Disgust 0.51 0.43 0.47
Fear 0.55 0.50 0.52
Non-emo 0.49 0.48 0.48
Accuracy 50.72%
SVM
+ our
features
Happiness 0.68 0.78 0.73
Sadness 0.49 0.58 0.53
Anger 0.66 0.48 0.56
Surprise 0.61 0.31 0.41
Disgust 0.43 0.38 0.40
Fear 0.67 0.63 0.65
Non-emo 0.51 0.53 0.52
Accuracy 58.88%
Logistic
Regres-
sion + our
features
Happiness 0.78 0.82 0.80
Sadness 0.53 0.64 0.58
Anger 0.69 0.62 0.66
Surprise 0.89 0.47 0.62
Disgust 0.81 0.41 0.55
Fear 0.71 0.71 0.71
Non-emo 0.53 0.64 0.58
Accuracy 66.88%
Table 5: Classification experiments on the dataset with
one emotional word in each sentence. Each experiment
is marked by the method and the feature set.
experiment is 51%, remarkably higher than the first
baseline?s accuracy. The second baseline is particu-
larly designed to address the emotion of the sentence
only based on the prior emotion of the emotional
words; therefore it will allow us to assess the dif-
ference between the emotion of the sentence based
on the prior emotion of the words in the sentence
versus the case when we consider the context and its
effect on the emotion of the sentence.
Learning Experiments
In this part, we use two classification algorithms,
Support Vector Machines (SVM) and Logistic Re-
gression (LR), and two different set of features,
the set of features from Section 3 and Bag-of-
Words (unigram). Unigram models have been
widely used in text classification and shown to pro-
vide good results in sentiment classification tasks.
In general, SVM has long been a method of
choice for sentiment recognition in text. SVM has
74
been shown to give good performance in text clas-
sification experiments as it scales well to the large
numbers of features (Yang and Liu, 1999; Pang, Lee,
and Vaithyanathan, 2002; Aman and Szpakowicz,
2007). For the classification, we use the SMO al-
gorithm (Platt, 1998) from Weka (Hall et al, 2009),
setting 10-fold cross validation as a testing option.
We compare applying SMO to two sets of features,
(i) Bag-of-Words, which are binary features defin-
ing whether a unigram exists in a sentence and (ii)
our set of features. In our experiments we use uni-
grams from the corpus, selected using feature selec-
tion methods from Weka.
We also compare those two results with the third
experiment: apply SimpleLogistic (Sumner, Frank,
and Hall, 2005) from Weka to our set of features,
again setting 10-fold cross validation as a testing op-
tion. Logistic regression is a discriminative prob-
abilistic classification model which operates over
real-valued vector inputs. It is relatively slow to train
compared to the other classifiers. It also requires ex-
tensive tuning in the form of feature selection and
implementation to achieve state-of-the-art classifica-
tion performance. Logistic regression models with
large numbers of features and limited amounts of
training data are highly prone to over-fitting (Alias-
i, 2008). Besides, logistic regression is really slow
and it is known to only work on data represented
by a small set of features. That is why we do not
apply SimpleLogistic to Bag-of-Words features. On
the other hand, the number of our features is rela-
tively low, so we find logistic regression to be a good
choice of classifier for our representation method.
The classification results are shown in Table 5.
We note consistent improvement. The results of
both experiments using our set of features signifi-
cantly outperform (on the basis of a paired t-test,
p=0.005) both the baselines and SVM applied to
Bag-of-Words features. We get the best result, how-
ever, by applying logistic regression to our feature
set. The number of our features and the nature of
the features we introduce make them an appropriate
choice of data representation for logistic regression
methods.
4.2 Experiments on sentences with more than
one emotional word
In these experiments, we combine supervised and
unsupervised learning. We train a classifier on the
first set of data, which is annotated, and we use the
model to classify the emotional words in the sec-
ond group of sentences. We propose an unsuper-
vised method to combine the contextual emotion of
the emotional words and calculate the emotion of the
sentence.
Baseline
We develop two baseline systems. The first base-
line labels all the sentences the same: as the emo-
tion of the most frequent class, giving 32% accu-
racy. The second baseline labels the emotion of the
sentence the same as the most frequently occurring
prior-emotion of the emotional words in the sen-
tence. In the case of a tie, we randomly pick one
of the emotions. The accuracy of this experiment
is 45%. Again, as a second baseline we choose a
baseline that is based on the prior emotion of the
emotional words so that we can compare it with the
results based on contextual emotion of the emotional
words in the sentence.
Learning Experiments
For sentences with more than one emotional
word, we represent each emotional word and its con-
text by the set of features explained in section 3. We
do not have the contextual emotion label for each
emotional word, so we cannot train the classifier on
these data. Consequently, we train the classifier on
the part of the dataset which only includes sentences
with one emotional word. In these sentences, each
emotional word is labeled with their contextual emo-
tion ? the same as the sentence?s emotion.
Once we have the classifier model, we get the
probability distribution of emotional classes for each
emotional word (calculated by the logistic regres-
sion function learned from the annotated data). We
add up the probabilities of each class for all emo-
tional words. Finally, we select the class with the
maximum probability. The result, shown in Table 6,
is compared using supervised learning, SVM, with
Bag-of-Words features, explained in previous sec-
tion, with setting 10-fold cross validation as a testing
75
Precision Recall F
SVM +
Bag-of-
Words
Happiness 0.52 0.60 0.54
Sadness 0.35 0.33 0.34
Anger 0.30 0.27 0.29
Surprise 0.14 0.11 0.12
Disgust 0.30 0.17 0.21
Fear 0.44 0.29 0.35
Non-emo 0.23 0.35 0.28
Accuracy 36.71%
Logistic
Regres-
sion +
unsu-
pervised
+ our
features
Happiness 0.63 0.71 0.67
Sadness 0.67 0.44 0.53
Anger 0.50 0.41 0.45
Surprise 1.00 0.22 0.36
Disgust 0.80 0.22 0.34
Fear 0.60 0.64 0.62
Non-emo 0.37 0.69 0.48
Accuracy 54.43%
Table 6: Classification experiments on the dataset with
more than one emotional word in each sentence. Each
experiment is marked by the method and the feature set.
option.4
By comparing the results in Table 6, we can see
that the result of learning applied to our set of fea-
tures significantly outperforms (on the basis of a
paired t-test, p=0.005) both baselines and the result
of SVM algorithm applied to Bag-of-Words features.
4.3 Discussion
We cannot directly compare our results with the pre-
vious results achieved by Aman and Szpakowicz
(2007), because the datasets differ. F-measure, pre-
cision and recall for each class are reported on the
whole dataset, but we only used part of that dataset.
To show how hard this task is, and to see where we
stand, the best result from (Aman and Szpakowicz,
2007) is shown in Table 7.
In our experiments, we showed that our approach
and our features significantly outperform the base-
lines and the SVM result applied to Bag-of-Words.
For the final conclusion, we add one more compar-
ison. As we can see from Table 6, the accuracy
result of applying SVM to Bag-of-Words is really
low. Because supervised methods scale well on large
datasets, one reason could be the size of the data we
use in this experiment; therefore we try to compare
4Since SVM does not return a distribution probability, we
cannot apply SVM to our features in this set of experiments.
Precision Recall F
Happiness 0.813 0.698 0.751
Sadness 0.605 0.416 0.493
Anger 0.650 0.436 0.522
Surprise 0.723 0.409 0.522
Disgust 0.672 0.488 0.566
Fear 0.868 0.513 0.645
Non-emo 0.587 0.625 0.605
Table 7: Aman?s best result on the dataset explained in
Section 2.
the results of the two experiments on all 758 sen-
tences with at least one emotional word.
For this comparison, we apply SVM with Bag-of-
Words features to all of 758 sentences and we get
an accuracy of 55.17%. Considering our features
and methodology, we cannot apply logistic regres-
sion with our features to the whole dataset; therefore
we calculate its accuracy by counting the percent-
age of correctly classified instances in both parts of
the dataset, used in the two experiments, and we get
an accuracy of 64.36%. We also compare the re-
sults with the baselines. The first baseline, which
is the percentage of most frequent class (happiness
in this case), results in 31.5% accuracy. The second
baseline based on the prior emotion of the emotional
words results in 50.13% accuracy. It is notable that
the result of applying LR to our set of features is
still significantly better than the result of applying
SVM to Bag-of-Words and both baselines; this sup-
ports our earlier conclusion. It is hard to compare
the results mentioned thus far, so we have combined
all the results in Figure 1, which displays the accu-
racy obtained by each experiment.
We also looked into our results and assessed the
cases where the contextual emotion is different from
the prior emotion of the emotional word. Consider
the sentence ?Joe said it does not happen that often
so it does not bother him.? Based on the emotion
lexicon, the word ?bother? is classified as angry; so
is the emotion of the sentence if we only consider
the prior emotion of words. In our set of features,
however, we consider the negation in the sentence,
so the sentence is classified as non-emotional rather
than angry. Another interesting sentence is the rather
simple ?You look like her I guess.? Based on the lex-
icon, the word ?like? is in the happy category, while
76
Figure 1: The comparison of accuracy results of all ex-
periments for sentences with one emotional word (part
1), sentences with more than one emotional words (part
2), and sentences with at least one emotional word (part
1+part 2).
the sentence is non-emotional. In this case, the part-
of-speech features play an important role and they
catch the fact that ?like" is not a verb here; it does
not convey a happy emotion and the sentence is clas-
sified as non-emotional.
We also analyzed the errors, and we found some
common errors due to:
? complex sentences or unstructured sentences
which will cause the parser to fail or return in-
correct data, resulting in incorrect dependency-
tree information;
? limited coverage of the emotion lexicon.
These are some of the issues which we would like
to address in our future work.
5 Conclusion and Future Directions
The focus of this study was a comparison of prior
emotion of a word with its contextual emotion, and
their effect on the emotion expressed by the sen-
tence. We also studied features important in recog-
nizing contextual emotion. We experimented with
a wide variety of linguistically-motivated features,
and we evaluated the performance of these fea-
tures using logistic regression. We showed that
our approach and features significantly outperform
the baseline and the SVM result applied to Bag-of-
Words.
Even though the features we presented did quite
well on the chosen dataset, in the future we would
like to show the robustness of these features by ap-
plying them to different datasets.
Another direction for future work will be to ex-
pand our emotion lexicon using existing techniques
for automatically acquiring the prior emotion of
words. Based on the number of instances in each
emotion class, we noticed there is a tight relation
between the number of words in each emotion list
in the emotion lexicon and the number of sentences
that are derived for each emotion class. It follows
that a larger lexicon will have a greater coverage of
emotional expressions.
Last but not least, one of the weaknesses of our
approach was the fact that we could not use all the
instances in the dataset. Again, the main reason was
the low coverage of the emotion lexicon that was
used. The other reason was the limitation of our
method: we had to only choose the sentences that
have one or more emotional words. As future work,
we would like to relax the restriction by using the
root of the sentence (based on the dependency tree
result) as a cue word rather than the emotional word
from the lexicon. So, for sentences with no emo-
tional word, we can calculate all the features regard-
ing the root word rather than the emotional word.
References
Alias-i. 2008. Lingpipe 4.1.0., October.
Alm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.
2005. Emotions from Text: Machine Learning for
Text-based Emotion Prediction. In HLT/EMNLP.
Aman, Saima and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Proc. 10th Inter-
national Conf. Text, Speech and Dialogue, pages 196?
205. Springer-Verlag.
Chaumartin, Fran?ois-Regis. 2007. UPAR7: a
knowledge-based system for headline sentiment tag-
ging. In Proc. 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 422?425.
Choi, Yejin, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proc. Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 355?362.
Ekman, Paul. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3):169?200.
Esuli, Andrea and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Resource
77
for Opinion Mining. In Proc. 5th Conf. on Language
Resources and Evaluation LREC 2006, pages 417?
422.
Ghazi, Diman, Diana Inkpen, and Stan Szpakowicz.
2010. Hierarchical approach to emotion recognition
and classification in texts. In Canadian Conference on
AI, pages 40?50.
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Katz, Phil, Matthew Singleton, and Richard Wicen-
towski. 2007. SWAT-MP: the SemEval-2007 systems
for task 5 and task 14. In Proc. 4th International Work-
shop on Semantic Evaluations, SemEval ?07, pages
308?313.
Kozareva, Zornitsa, Borja Navarro, Sonia V?zquez, and
Andr?s Montoyo. 2007. UA-ZBSA: a headline emo-
tion classification through web information. In Proc.
4th International Workshop on Semantic Evaluations,
SemEval ?07, pages 334?337.
Marneffe, Marie-Catherine De, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Proc.
LREC 2006.
Mohammad, Saif M. and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In Proc.
NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in
Text, CAAGET ?10, pages 26?34.
Neviarouskaya, Alena, Helmut Prendinger, and Mitsuru
Ishizuka. 2010. AM: textual attitude analysis model.
In Proc. NAACL HLT 2010 Workshop on Computa-
tional Approaches to Analysis and Generation of Emo-
tion in Text, pages 80?88.
Neviarouskaya, Alena, Helmut Prendinger, and Mitsuru
Ishizuka. 2011. Affect Analysis Model: novel rule-
based approach to affect sensing from text. Natural
Language Engineering, 17(1):95?135.
Ortony, Andrew, Allan Collins, and Gerald L. Clore.
1988. The cognitive structure of emotions. Cambridge
University Press.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proc. ACL-02 confer-
ence on Empirical methods in natural language pro-
cessing - Volume 10, EMNLP ?02, pages 79?86.
Platt, John C. 1998. Sequential Minimal Optimization:
A Fast Algorithm for Training Support Vector Ma-
chines.
Riloff, Ellen. 2003. Learning extraction patterns for sub-
jective expressions. In Proc. 2003 Conf. on Empirical
Methods in Natural Language Processing, pages 105?
112.
Stoyanov, Veselin, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proc. Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 923?930.
Strapparava, Carlo and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proc. Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 70?74, Prague, Czech Republic, June.
Strapparava, Carlo and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proc. 2008 ACM sym-
posium on Applied computing, SAC ?08, pages 1556?
1560.
Strapparava, Carlo and Alessandro Valitutti. 2004.
WordNet-Affect: an Affective Extension of Word-
Net. In Proc. 4th International Conf. on Language
Resources and Evaluation, pages 1083?1086.
Sumner, Marc, Eibe Frank, and Mark A. Hall. 2005.
Speeding Up Logistic Model Tree Induction. In Proc.
9th European Conference on Principles and Practice
of Knowledge Discovery in Databases, pages 675?
683.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proc. HLT-NAACL, pages 252?259.
Turney, Peter D. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. 40th Annual Meeting on
Association for Computational Linguistics, ACL ?02,
pages 417?424.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proc. HLT-EMNLP, pages 347?
354.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: An Explo-
ration of Features for Phrase-Level Sentiment Analy-
sis. Computational Linguistics, 35(3):399?433.
Yang, Yiming and Xin Liu. 1999. A re-examination
of text categorization methods. In Proc. 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?99, pages 42?49.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proc. 2003 conference on Empirical
methods in natural language processing, EMNLP ?03,
pages 129?136.
78
