Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 17?20,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Interactive Humanoid Robot Exhibiting Flexible Sub-Dialogues?
Heriberto Cuaya?huitl
DFKI GmbH
hecu01@dfki.de
Ivana Kruijff-Korbayova?
DFKI GmbH
ivana.kruijff@dfki.de
Abstract
We demonstrate a conversational humanoid
robot that allows users to follow their own
dialogue structures. Our system uses a hi-
erarchy of reinforcement learning dialogue
agents, which support transitions across
sub-dialogues in order to relax the strict-
ness of hierarchical control and therefore
support flexible interactions. We demon-
strate our system with the Nao robot play-
ing two versions of a Quiz game. Whilst
language input and dialogue control is au-
tonomous or wizarded, language output is
provided by the robot combining verbal and
non-verbal contributions. The novel fea-
tures in our system are (a) the flexibility
given to users to navigate flexibly in the in-
teraction; and (b) a framework for investi-
gating adaptive and flexible dialogues.
1 Introduction
Hierarchical Dialogue Control (HDC) consists of
behaviours or discourse segments at different lev-
els of granularity executed from higher to lower
level. For example, a dialogue agent can invoke a
sub-dialogue agent, which can also invoke a sub-
sub-dialogue agent, and so on. Task-oriented di-
alogues have shown evidence of following hierar-
chical structures (Grosz and Sidner, 1986; Litman
and Allen, 1987; Clark, 1996). Practically speak-
ing, HDC offers the following benefits. First,
modularity helps to specify sub-dialogues that
may be easier to specify than the entire full dia-
logues. Second, sub-dialogues may include only
relevant dialogue knowledge (e.g. subsets of dia-
logue acts), thus reducing significantly their com-
?*Funding by the EU-FP7 project ALIZ-E (ICT-248116)
is gratefully acknowledged.
(a) strict hierachical 
     dialogue control
Dialogue
Sub-dialogue1 Sub-dialogue2
(b) flexible hierachical 
  dialogue control
Dialogue
Sub-dialogue1 Sub-dialogue2
Figure 1: Hierarchies of dialogue agents with strict
(top down) and flexible control (partial top down).
plexity. Third, sub-dialogues can be reused when
dealing with new behaviours. In this paper we dis-
tinguish two types of hierarchical dialogue con-
trol: strict and flexible. These two forms of dia-
logue control are shown in Figure 1. It can be ob-
served that strict HDC is based on a pure top down
execution, and flexible HDC is based on a com-
bined hierarchical and graph-based execution.
The main limitation of strict HDC is that
human-machine interactions are rigid, i.e. the
user cannot change the imposed dialogue struc-
ture. A more natural way of interaction is by re-
laxing the dialogue structure imposed by the con-
versational machine. The advantage of flexible
HDC is that interactions become less rigid be-
cause it follows a partially specified hierarchical
control, i.e. the user is allowed to navigate across
the available sub-dialogues. In addition, another
important property of the latter form of HDC is
that we can model flexible dialogue structures not
only driven by the user but also by the machine.
The latter requires the machine to learn the dia-
logue structure in order to behave in an adaptive
way. The rest of the paper describes a demo sys-
tem exhibiting both types of behaviour, based on
a reinforcement learning dialogue framework.
17
2 Hierarchical Reinforcement Learning
Dialogue Agents with Flexible Control
Our dialogue controllers use hierarchical rein-
forcement learning as in (Cuaya?huitl et al, 2010).
We extend such a formalization through a hierar-
chy of dialogue agents defined with the following
tuples: M ij = <Sij , Aij , T ij , Rij , Lij , U ij , ?ij , ?ij>,
where Sij is a set of states, Aij is a set of actions,
T ij is a stochastic state transition function, Rij is
a reward function, Lij is a grammar that specifies
tree-based state representations, U ij is a finite set
of user actions (e.g. user dialogue acts), ?ij is a
finite set of models that subtask M ij is being al-
lowed to transition to, and ?ij = P (m? ? ?ij |m ?
?ij , u ? U ij) is a stochastic model transition func-
tion1 that specifies the next model m? given model
m and user action u. Although the hierarchy of
agents can be fully-connected when all models
are allowed to transition from a given particu-
lar model (avoiding self-transitions), in practice,
we may want our hierarchy of agents partially-
connected, i.e. when ?ij is a subset of subtasks
that agent M ij is allowed to transition to.
We implemented a modified version of the
HSMQ-Learning algorithm (Dietterich, 2000) to
simultaneously learn a hierarchy of policies piij .
This algorithm uses a stack of subtasks and op-
erates as illustrated in Figure 2. If during the ex-
ecution of a subtask the user decides to jump to
another subtask, i.e. to change to another sub-
dialogue, the flexible execution of subtasks allows
each subtask to be interrupted in two ways. In the
first case, we check whether the new (active) sub-
task is already on the stack of subtasks to execute.
This would be the case if it was a parent of the
current subtask. In this case, we terminate exe-
cution of all intervening subtasks until we reach
the parent subtask, which would be the new ac-
tive subtask. Notice that termination of all inter-
vening subtasks prevents the stack from growing
infinitely. In the second case, the current subtask
is put on hold, and if the new active subtask is
not already on the stack of subtasks to execute, it
is pushed onto the stack and control is passed to
it. Once the new subtask terminates its execution,
control is transferred back to the subtask on hold.
1This is a very relevant feature in dialogue agents in order
to allow users to say and/or do anything at anytime, and the
learning agents have to behave accordingly.
Initial
stack
Pushing
'dialogue'
Pushing
'sub-dialogue1'
Pushing
'sub-dialogue2'
(two siblings 
in the stack)
Popping
'sub-dialogue2'
Popping
'sub-dialogue1'
Popping
'dialogue'
dialogue dialogue dialogue dialogue dialogue
sub-
dialogue1
sub-
dialogue1
sub-
dialogue2
sub-
dialogue1
Figure 2: Hypothetical operations of stack-based hier-
archical dialogue controllers. Whilst the fourth opera-
tion from left to right is not allowed in strict HDC, all
stack operations are allowed in flexible HDC.
These kinds of transitions can be seen as high-
level transitions in the state space. They can also
be seen as the mechanism to transition from any
state to any other in the hierarchy. To do that we
maintain an activity status for each subtask M ij ,
where only one subtask is allowed to be active at
a time. We maintain a knowledge-rich state that
keeps the dialogue history in order to initialize
or reinitialize states of each subtask accordingly.
Since there is learning when new subtasks are in-
voked and no learning when they are interrupted,
this algorithm maintains its convergence proper-
ties to optimal context-independent policies.
3 A Hierarchy of Dialogue Agents for
Playing Quiz Games
We use a small hierarchy of dialogue agents?
for illustration purposes?with one parent agent
and two children agents (?robot asks? and ?user
asks?). Thus, the hierarchy of agents can ask the
user questions, and vice-versa, the user can ask
the robot questions (described in the next section).
Both conversants can play multiple rounds with a
predefined number of questions.
Due to space restrictions, we describe the hi-
erarchy of agents only briefly. The set of states
and actions use relational representations (they
can be seen as trees) in order to specify the
state-action space compactly, which can grow as
more features or games are integrated. Dialogue
and game features are included so as to inform
the agents of possible situations in the interac-
tion. The action sets use constrained spaces, i.e.
only a subset of actions is available at each state
based on the relational representations. For ex-
ample, the action Request(PlayGame) ? x0
is valid for the dialogue state x0 expressed as
Salutation(greeting)?UserName(known)?
PlayGame(unknown). The sets of primitive
actions (80 in total) assume verbal behaviours
18
with a mapping to non-verbal ones, some sam-
ple dialogue act types are as follows: requests,
apologies, confirmations, provide information,
acknowledgements, feedback, non-verbal expres-
sions, game-related actions. The transition func-
tions use pre-defined parameters, their training
from data is left as future work. The reward func-
tion addresses efficient and effective interactions
by penalizing dialogue length and encouraging to
continue playing. The dialogue agents learnt their
behaviour by interacting with a stochastic simu-
lated user, where the user responses eventually
required transitions across agents. A sample dia-
logue with flexible interaction is shown in Fig. 3.
4 A Humanoid Robot Integrated System
Figure 4 shows the robot?s integrated system,
which equips the robot with the following capa-
bilities: listening, talking, seeing and moving.2 A
sample interaction assuming wizarded behaviour
is as follows. The user says something to the robot
(e.g. ?ask me a question?) and the wizard selects
a user dialogue act for the corresponding utter-
ance such as ?Request(Question)?. Then the di-
alogue manager selects and executes the learned
action by querying/updating the game move gen-
erator and user model components, and by pass-
ing a dialogue act to the language generation and
motor control components. The former provides
the corresponding text to the speech synthesizer
and the latter provides the corresponding non-
verbal behaviour. Then the dialogue manager ob-
serves its new dialogue state, and suggests the
next actions?based on its dialogue policies and
user simulations?in the wizard-of-Oz GUI, see
Figure 5. From this setting, if we remove the
wizard then the system can behave autonomously,
where user inputs are derived from the language
and gesture understanding components.
The attractiveness of this kind of set-up is the
smooth transition from purely wizarded conver-
sational behaviour to fully autonomous behaviour,
where the dialogue policies can be tested and eval-
2We use off-the-shelf technologies such as Sphinx /
Julius / HTK for speech recognition, OpenCV for ges-
ture recognition, Acapela and Mary for speech synthe-
sis, HRL-based dialogue management (as described above),
OpenCCG for language parsing and language generation,
Weka and JavaBayes for maintaining a probabilistic and per-
sonalized user profile of asked and answered questions, the
URBI middleware (www.gostai.com), and the NAO hu-
manoid robot (www.aldebaran-robotics.com).
SYS Hi, my name is NAO, and you?
USR I am Peter
SYS Nice to meet you Peter
Do you want to Play a Quiz game?
USR Ask me questions [jump to ?robot asks?]
SYS The first question is.
What is a typical magic instrument?
Your choices are: One, guitar. Two, drums.
Three ,violin. Four, flute. What is your choice?
USR Number two
SYS That is not quite right. Do you have another choice?
USR Maybe number three
SYS The correct answer is number four, flute.
The next question is: which one of these is
an important artificial channel of the central America?
Your choices are: One, channel five. Two, Panama canal.
Three, Suez canal. Four, English channel.
What is your choice?
USR Now I want to ask you questions [jump to ?user asks?]
SYS Ask me the first question.
USR What does water become at the temperature of
zero degrees?
SYS What are my choices?
USR Your choices are:
One, condensation. Two, ice. Three, steam.
SYS I choose number one
USR That is incorrect.
SYS So what is the right answer?
USR The correct answer is number two.
SYS Okay, ask me another question.
USR The next question is: through which breach did
the Italian army entered into Rome in 1870?
Your choices are: One, porta venezia. Two, porta pia.
Three, porta pazienza. Four, porta nuova.
What is your choice?
SYS Paris
USR Let?s stop playing. [jump to ?root?]
SYS Did you like playing the Quiz Game?
USR Yes
SYS I am glad to hear that.
SYS It was nice playing with you, see you soon. Bye!
Figure 3: Sample dialogue with non-strict hierarchical
control. The groups of utterances represent activity
in different sub-dialogues in the hierarchy of agents.
Note that the user is allowed to override the imposed
dialogue structure. For instance, when the user says
?Ask me questions? the agent interrupts the execution
of the root subtask and transitions to the subtask ?robot
asks?. Similarly, the sub-dialogues do not need to follow
their imposed structure and the user is allowed to talk
about previous and unadressed sub-dialogues such as
the sudden switch from ?robot asks? to ?user asks?.
Middleware
Speech Recognizer,
Voice Act. Detector,
Audio Front End
Gesture
Recognizer
Motor
Control
Speech
Synthesizer
Game Move
Generator
Dialogue Manager
Parser,
Dialogue Act
Classifier
Language
Generator
User
Model
Wizard-of-Oz
GUI
ASR
result
Gesture
act
System
Dialogue
act
Text
ASR
result
Gesture
Act
System
dialogue
Act
Text
User
dialogue
act
Dialogue acts System
dialogue
act
user,
game
results
query,
questions,
answers
Figure 4: High-level architecture of our talking robot.
19
Figure 5: Screen shot of the wizard-of-Oz GUI, where
the dialogue policies and user simulations suggest
highlighted actions to the wizard. This setting allows
fully-wizarded and (semi-) autonomous behaviour.
Figure 6: The Nao robot greeting a user prior to play-
ing a Quiz game. The pieces of paper on the table are
the Quiz questions the child asks the robot.
uated with (semi-) autonomous behaviour. We use
this framework to investigate long-term human-
robot interaction, in particular child-robot inter-
actions for educational purposes. Figure 6 shows
a scene from a pilot evaluation, where the robot
and a child are visibly engaged with each other. A
complete evaluation with simulated and real dia-
logues will be reported in a forthcoming paper.
5 Discussion and Summary
Typically, conversational interfaces impose a di-
alogue structure on the user. Even in dialogue
systems with mixed-initiative interaction that give
flexibility to the user in terms of providing more
than one piece of information at a time, the
user is hardly allowed to navigate flexibly during
the interaction. Notable exceptions without dia-
logue optimization are (Rudnicky and Wu, 1999;
Lemon et al, 2001; Larsson, 2002; Foster et al,
2006). We believe that Hierarchical Reinforce-
ment Learning with global state transitions is an
interesting method to optimize (sub-) dialogues at
different levels of granularity, where the design of
action selection might not be easy to hand-craft.
On the one hand, our HDCs can be applied to
dialogues with user-driven topic shift, where the
user can take control of the interaction by navigat-
ing across sub-dialogues and the system has to re-
spond accordingly. On the other hand, our HDCs
can be applied to dialogues with system-driven
topic shift, where the system can itself terminate a
sub-dialogue, perhaps by inferring the user?s emo-
tional and/or situational state, and the system has
to switch itself to another sub-dialogue.
We have described a conversational humanoid
robot that allows users to follow their own dia-
logue structures. The novelty in our system is
its flexible hierarchical dialogue controller, which
extends strict hierarchical control with transitions
across sub-controllers. Suggested future work
consists in training and evaluating our humanoid
robot from real interactions using either partially
specified or fully learnt dialogue structures.
References
H. Clark. 1996. Using Language. Cambridge Univer-
sity Press.
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2010. Evaluation of a hierarchical rein-
forcement learning spoken dialogue system. Com-
puter Speech and Language, 24(2):395?429.
T. Dietterich. 2000. An overview of MAXQ hi-
erarchical reinforcement learning. In Symposium
on Abstraction, Reformulation, and Approximation
(SARA), pages 26?44.
M. E. Foster, T. By, M. Rickert, and A. Knoll. 2006.
Human-robot dialogue for joint construction tasks.
In ICMI, pages 68?71.
B. Grosz and C. Sidner. 1986. Attention, intentions
and the structure of discourse. Computational Lin-
guistics, 12(3):175?204.
S. Larsson. 2002. Issue-Based Dialogue Manage-
ment. Ph.D. thesis, University of Goteborg.
O. Lemon, A. Bracy, A. Gruenstein, and S. Peters.
2001. The WITAS multi-modal dialogue system I.
In EUROSPEECH, Aalborg, Denmark.
D. Litman and J. Allen. 1987. A plan recognition
model for subdialogues in conversations. Cognitive
Science, 11:163?200.
A. Rudnicky and W. Wu. 1999. An agenda-based
dialogue management architecture for spoken lan-
guage systems. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU),
pages 337?340, Keystone, Colorado, USA, Dec.
20
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 654?659,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Hierarchical Reinforcement Learning and Hidden Markov Models for
Task-Oriented Natural Language Generation
Nina Dethlefs
Department of Linguistics,
University of Bremen
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
German Research Centre for Artificial Intelligence
(DFKI), Saarbru?cken
heriberto.cuayahuitl@dfki.de
Abstract
Surface realisation decisions in language gen-
eration can be sensitive to a language model,
but also to decisions of content selection. We
therefore propose the joint optimisation of
content selection and surface realisation using
Hierarchical Reinforcement Learning (HRL).
To this end, we suggest a novel reward func-
tion that is induced from human data and is
especially suited for surface realisation. It is
based on a generation space in the form of
a Hidden Markov Model (HMM). Results in
terms of task success and human-likeness sug-
gest that our unified approach performs better
than greedy or random baselines.
1 Introduction
Surface realisation decisions in a Natural Language
Generation (NLG) system are often made accord-
ing to a language model of the domain (Langkilde
and Knight, 1998; Bangalore and Rambow, 2000;
Oh and Rudnicky, 2000; White, 2004; Belz, 2008).
However, there are other linguistic phenomena, such
as alignment (Pickering and Garrod, 2004), consis-
tency (Halliday and Hasan, 1976), and variation,
which influence people?s assessment of discourse
(Levelt and Kelter, 1982) and generated output (Belz
and Reiter, 2006; Foster and Oberlander, 2006).
Also, in dialogue the most likely surface form may
not always be appropriate, because it does not cor-
respond to the user?s information need, the user is
confused, or the most likely sequence is infelicitous
with respect to the dialogue history. In such cases, it
is important to optimise surface realisation in a uni-
fied fashion with content selection. We suggest to
use Hierarchical Reinforcement Learning (HRL) to
achieve this. Reinforcement Learning (RL) is an at-
tractive framework for optimising a sequence of de-
cisions given incomplete knowledge of the environ-
ment or best strategy to follow (Rieser et al, 2010;
Janarthanam and Lemon, 2010). HRL has the ad-
ditional advantage of scaling to large and complex
problems (Dethlefs and Cuaya?huitl, 2010). Since
an HRL agent will ultimately learn the behaviour
it is rewarded for, the reward function is arguably
the agent?s most crucial component. Previous work
has therefore suggested to learn a reward function
from human data as in the PARADISE framework
(Walker et al, 1997). Since PARADISE-based re-
ward functions typically rely on objective metrics,
they are not ideally suited for surface realisation,
which is more dependend on linguistic phenomena,
e.g. frequency, consistency, and variation. However,
linguistic and psychological studies (cited above)
show that such phenomena are indeed worth mod-
elling in an NLG system. The contribution of this
paper is therefore to induce a reward function from
human data, specifically suited for surface genera-
tion. To this end, we train HMMs (Rabiner, 1989)
on a corpus of grammatical word sequences and use
them to inform the agent?s learning process. In addi-
tion, we suggest to optimise surface realisation and
content selection decisions in a joint, rather than iso-
lated, fashion. Results show that our combined ap-
proach generates more successful and human-like
utterances than a greedy or random baseline. This is
related to Angeli et al (2010), who also address in-
terdependent decision making, but do not use an opt-
misation framework. Since language models in our
approach can be obtained for any domain for which
corpus data is available, it generalises to new do-
mains with limited effort and reduced development
654
Utterance
string=?turn around and go out?, time=?20:54:55?
Utterance type
content=?orientation,destination? [straight, path, direction]
navigation level=?low? [high]
User
user reaction=?perform desired action?
[perform undesired action, wait, request help]
user position=?on track? [off track]
Figure 1: Example annotation: alternative values for at-
tributes are given in square brackets.
time. For related work on using graphical models
for language generation, see e.g., Barzilay and Lee
(2002), who use lattices, or Mairesse et al (2010),
who use dynamic Bayesian networks.
2 Generation Spaces
We are concerned with the generation of navigation
instructions in a virtual 3D world as in the GIVE
scenario (Koller et al, 2010). In this task, two peo-
ple engage in a ?treasure hunt?, where one partici-
pant navigates the other through the world, pressing
a sequence of buttons and completing the task by
obtaining a trophy. The GIVE-2 corpus (Gargett et
al., 2010) provides transcripts of such dialogues in
English and German. For this paper, we comple-
mented the English dialogues of the corpus with a
set of semantic annotations,1 an example of which
is given in Figure 1. This example also exempli-
fies the type of utterances we generate. The input to
the system consists of semantic variables compara-
ble to the annotated values, the output corresponds
to strings of words. We use HRL to optimise deci-
sions of content selection (?what to say?) and HMMs
for decisions of surface realisation (?how to say it?).
Content selection involves whether to use a low-, or
high-level navigation strategy. The former consists
of a sequence of primitive instructions (?go straight?,
?turn left?), the latter represents contractions of se-
quences of low-level instructions (?head to the next
room?). Content selection also involves choosing a
level of detail for the instruction corresponding to
the user?s information need. We evaluate the learnt
content selection decisions in terms of task success.
For surface realisation, we use HMMs to inform
the HRL agent?s learning process. Here we address
1The annotations are available on request.
the one-to-many relationship arising between a se-
mantic form (from the content selection stage) and
its possible realisations. Semantic forms of instruc-
tions have an average of 650 surface realisations,
including syntactic and lexical variation, and deci-
sions of granularity. We refer to the set of alterna-
tive realisations of a semantic form as its ?generation
space?. In surface realisation, we aim to optimise the
tradeoff between alignment and consistency (Picker-
ing and Garrod, 2004; Halliday and Hasan, 1976) on
the one hand, and variation (to improve text quality
and readability) on the other hand (Belz and Reiter,
2006; Foster and Oberlander, 2006) in a 50/50 dis-
tribution. We evaluate the learnt surface realisation
decisions in terms of similarity with human data.
Note that while we treat content selection and
surface realisation as separate NLG tasks, their op-
timisation is achieved jointly. This is due to a
tradeoff arising between the two tasks. For exam-
ple, while surface realisation decisions that are opti-
mised solely with respect to a language model tend
to favour frequent and short sequences, these can
be inappropriate according to the user?s information
need (because they are unfamiliar with the naviga-
tion task, or are confused or lost). In such situa-
tions, it is important to treat content selection and
surface realisation as a unified whole. Decisions of
both tasks are inextricably linked and we will show
in Section 5.2 that their joint optimisation leads to
better results than an isolated optimisation as in, for
example, a two-stage model.
3 NLG Using HRL and HMMs
3.1 Hierarchical Reinforcement Learning
The idea of language generation as an optimisa-
tion problem is as follows: given a set of genera-
tion states, a set of actions, and an objective reward
function, an optimal generation strategy maximises
the objective function by choosing the actions lead-
ing to the highest reward for every reached state.
Such states describe the system?s knowledge about
the generation task (e.g. content selection, naviga-
tion strategy, surface realisation). The action set
describes the system?s capabilities (e.g. ?use high
level navigation strategy?, ?use imperative mood?,
etc.). The reward function assigns a numeric value
for each action taken. In this way, language gen-
655
Figure 2: Hierarchy of learning agents (left), where shaded agents use an HMM-based reward function. The top three
layers are responsible for content selection (CS) decisions and use HRL. The shaded agents in the bottom use HRL
with an HMM-based reward function and joint optimisation of content selection and surface realisation (SR). They
provide the observation sequence to the HMMs. The HMMs represent generation spaces for surface realisation. An
example HMM, representing the generation space of ?destination? instructions, is shown on the right.
eration can be seen as a finite sequence of states,
actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},
where the goal is to find an optimal strategy auto-
matically. To do this we use RL with a divide-and-
conquer approach to optimise a hierarchy of genera-
tion policies rather than a single policy. The hierar-
chy of RL agents consists of L levels and N models
per level, denoted as M ij , where j ? {0, ..., N ? 1}
and i ? {0, ..., L ? 1}. Each agent of the hierar-
chy is defined as a Semi-Markov Decision Process
(SMDP) consisting of a 4-tuple < Sij, Aij , T ij , Rij >.
Sij is a set of states, Aij is a set of actions, T ij is
a transition function that determines the next state
s? from the current state s and the performed ac-
tion a, and Rij is a reward function that specifies
the reward that an agent receives for taking an ac-
tion a in state s lasting ? time steps. The random
variable ? represents the number of time steps the
agent takes to complete a subtask. Actions can be
either primitive or composite. The former yield sin-
gle rewards, the latter correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optimal policy that max-
imises the reward for each visited state, according to
pi?ij(s) = arg maxa?Aij Q
?i
j(s, a), where Q?ij (s, a)
specifies the expected cumulative reward for execut-
ing action a in state s and then following policy pi?ij .
We use HSMQ-Learning (Dietterich, 1999) to learn
a hierarchy of generation policies.
3.2 Hidden Markov Models for NLG
The idea of representing the generation space of
a surface realiser as an HMM can be roughly de-
fined as the converse of POS tagging, where an in-
put string of words is mapped onto a hidden se-
quence of POS tags. Our scenario is as follows:
given a set of (specialised) semantic symbols (e.g.,
?actor?, ?process?, ?destination?),2 what is the most
likely sequence of words corresponding to the sym-
bols? Figure 2 provides a graphic illustration of this
idea. We treat states as representing words, and se-
quences of states i0...in as representing phrases or
sentences. An observation sequence o0...on consists
of a finite set of semantic symbols specific to the in-
struction type (i.e., ?destination?, ?direction?, ?orien-
tation?, ?path?, ?straight?). Each symbol has an ob-
servation likelihood bi(ot), which gives the proba-
bility of observing o in state i at time t. The tran-
sition and emission probabilities are learnt during
training using the Baum-Welch algorithm. To de-
sign an HMM from the corpus data, we used the
ABL algorithm (van Zaanen, 2000), which aligns
strings based on Minimum Edit Distance, and in-
duces a context-free grammar from the aligned ex-
amples. Subsequently, we constructed the HMMs
from the CFGs, one for each instruction type, and
trained them on the annotated data.
2Utterances typically contain five to ten semantic categories.
656
3.3 An HMM-based Reward Function Induced
from Human Data
Due to its unique function in an RL framework, we
suggest to induce a reward function for surface re-
alisation from human data. To this end, we create
and train HMMs to represent the generation space
of a particular surface realisation task. We then use
the forward probability, derived from the Forward
algorithm, of an observation sequence to inform the
agent?s learning process.
r =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0 for reaching the goal state
+1 for a desired semantic choice or
maintaining an equal distribution
of alignment and variation
-2 for executing action a and remain-
ing in the same state s = s?
P (w0...wn) for for reaching a goal state corres-
ponding to word sequence w0...wn
-1 otherwise.
Whenever the agent has generated a word sequence
w0...wn, the HMM assigns a reward corresponding
to the likelihood of observing the sequence in the
data. In addition, the agent is rewarded for short
interactions at maximal task success3 and optimal
content selection (cf. Section 2). Note that while re-
ward P (w0...wn) applies only to surface realisation
agents M30...4, the other rewards apply to all agents
of the hierarchy.
4 Experimental Setting
We test our approach using the (hand-crafted) hierar-
chy of generation subtasks in Figure 2. It consists of
a root agent (M00 ), and subtasks for low-level (M20 )
and high-level (M21 ) navigation strategies (M11 ), and
for instruction types ?orientation? (M30 ), ?straight?
(M31 ), ?direction? (M32 ), ?path? (M33 ) and destina-
tion? (M34 ). Models M30...4 are responsible for sur-
face generation. They will be trained using HRL
with an HMM-based reward function induced from
human data. All other agents use hand-crafted re-
wards. Finally, subtask M10 can repair a previous
system utterance. The states of the agent contain
all situational and linguistic information relevant to
its decision making, e.g., the spatial environment,
3Task success is addressed by that each utterance needs to
be ?accepted? by the user (cf. Section 5.1).
discourse history, and status of grounding.4 Due to
space constraints, please see Dethlefs et al (2011)
for the full state-action space. We distinguish prim-
itive actions (corresponding to single generation de-
cisions) and composite actions (corresponding to
generation subtasks (Fig. 2)).
5 Experiments and Results
5.1 The Simulated Environment
The simulated environment contains two kinds of
uncertainties: (1) uncertainty regarding the state of
the environment, and (2) uncertainty concerning the
user?s reaction to a system utterance. The first as-
pect is represented by a set of contextual variables
describing the environment, 5 and user behaviour.6
Altogether, this leads to 115 thousand different con-
textual configurations, which are estimated from
data (cf. Section 2). The uncertainty regarding
the user?s reaction to an utterance is represented by
a Naive Bayes classifier, which is passed a set of
contextual features describing the situation, mapped
with a set of semantic features describing the utter-
ance.7 From these data, the classifier specifies the
most likely user reaction (after each system act) of
perform desired action, perform undesired action, wait
and request help.8 The classifier was trained on the
annotated data and reached an accuracy of 82% in a
cross-corpus validation using a 60%-40% split.
5.2 Comparison of Generation Policies
We trained three different generation policies. The
learnt policy optimises content selection and sur-
face realisation decisions in a unified fashion, and
is informed by an HMM-based generation space
reward function. The greedy policy is informed
only by the HMM and always chooses the most
4An example for the state variables of model M11 are the
annotation values in Fig. 1 which are used as the agent?s
knowledge base. Actions are ?choose easy route?, ?choose short
route?, ?choose low level strategy?, ?choose high level strategy?.
5previous system act, route length, route status
(known/unknown), objects within vision, objects within
dialogue history, number of instructions, alignment(proportion)
6previous user reaction, user position, user wait-
ing(true/false), user type(explorative/hesitant/medium)
7navigation level(high / low), abstractness(implicit / ex-
plicit), repair(yes / no), instruction type(destination / direction /
orientation / path / straight)
8User reactions measure the system?s task success.
657
likely sequence independent of content selection.
The valid sequence policy generates any grammat-
ical sequence. All policies were trained for 20000
episodes.9 Figure 3, which plots the average re-
wards of all three policies (averaged over ten runs),
shows that the ?learnt? policy performs best in terms
of task success by reaching the highest overall re-
wards over time. An absolute comparison of the av-
erage rewards (rescaled from 0 to 1) of the last 1000
training episodes of each policy shows that greedy
improves ?any valid sequence? by 71%, and learnt
improves greedy by 29% (these differences are sig-
nificant at p < 0.01). This is due to the learnt policy
showing more adaptation to contextual features than
the greedy or ?valid sequence? policies. To evaluate
human-likeness, we compare instructions (i.e. word
sequences) using Precision-Recall based on the F-
Measure score, and dialogue similarity based on the
Kulback-Leibler (KL) divergence (Cuaya?huitl et al,
2005). The former shows how the texts generated by
each of our generation policies compare to human-
authored texts in terms of precision and recall. The
latter shows how similar they are to human-authored
texts. Table 1 shows results of the comparison of
two human data sets ?Real1? vs ?Real2? and both of
them together against our different policies. While
the greedy policy receives higher F-Measure scores,
the learnt policy is most similar to the human data.
This is due to variation: in contrast to greedy be-
haviour, which always exploits the most likely vari-
ant, the learnt policy varies surface forms. This leads
to lower F-Measure scores, but achieves higher sim-
ilarity with human authors. This ultimately is a de-
sirable property, since it enhances the quality and
naturalness of our instructions.
6 Conclusion
We have presented a novel approach to optimising
surface realisation using HRL. We suggested to
inform an HRL agent?s learning process by an
HMM-based reward function, which was induced
9For training, the step-size parameter ? (one for each
SMDP) , which indicates the learning rate, was initiated with
1 and then reduced over time by ? = 11+t , where t is the time
step. The discount rate ?, which indicates the relevance of fu-
ture rewards in relation to immediate rewards, was set to 0.99,
and the probability of a random action ? was 0.01. See Sutton
and Barto (1998) for details on these parameters.
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
x 104
?250
?200
?150
?100
?50
0
50
Av
er
ag
e 
Re
wa
rd
s
Episodes
 
 
Valid Sequence
Greedy
Learnt
Figure 3: Performance of ?learnt?, ?greedy?, and ?any
valid sequence? generation behaviours (average rewards).
Compared Policies F-Measure KL-Divergence
Real1 - Real2 0.58 1.77
Real - ?Learnt? 0.40 2.80
Real - ?Greedy? 0.49 4.34
Real - ?Valid Seq.? 0.0 10.06
Table 1: Evaluation of generation behaviours with
Precision-Recall and KL-divergence.
from data and in which the HMM represents the
generation space of a surface realiser. We also
proposed to jointly optimise surface realisation
and content selection to balance the tradeoffs of
(a) frequency in terms of a language model, (b)
alignment/consistency vs variation, (c) properties
of the user and environment. Results showed that
our hybrid approach outperforms two baselines in
terms of task success and human-likeness: a greedy
baseline acting independent of content selection,
and a random ?valid sequence? baseline. Future
work can transfer our approach to different domains
to confirm its benefits. Also, a detailed human
evaluation study is needed to assess the effects
of different surface form variants. Finally, other
graphical models besides HMMs, such as Bayesian
Networks, can be explored for informing the surface
realisation process of a generation system.
Acknowledgments
Thanks to the German Research Foundation DFG
and the Transregional Collaborative Research Cen-
tre SFB/TR8 ?Spatial Cognition? and the EU-FP7
project ALIZ-E (ICT-248116) for partial support of
this work.
658
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 502?512.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th Conference on Computa-
tional Linguistics (ACL) - Volume 1, pages 42?48.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 164?171.
Anja Belz and Ehud Reiter. 2006. Comparing Automatic
and Human Evaluation of NLG Systems. In Proc. of
the European Chapter of the Association for Compu-
tational Linguistics (EACL), pages 313?320.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 1:1?26.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-Computer Dia-
logue Simulation Using Hidden Markov Models. In
Proc. of ASRU, pages 290?295.
Nina Dethlefs and Heriberto Cuaya?huitl. 2010. Hi-
erarchical Reinforcement Learning for Adaptive Text
Generation. Proceeding of the 6th International Con-
ference on Natural Language Generation (INLG).
Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.
2011. Optimising Natural Language Generation De-
cision Making for Situated Dialogue. In Proc. of the
12th Annual SIGdial Meeting on Discourse and Dia-
logue.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227?303.
Mary Ellen Foster and Jon Oberlander. 2006. Data-
driven generation of emphatic facial displays. In Proc.
of the European Chapter of the Association for Com-
putational Linguistic (EACL), pages 353?360.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus of
giving instructions in virtual environments. In LREC.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In Proc. of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 69?78.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010. The first challenge on generat-
ing instructions in virtual environments. In M. The-
une and E. Krahmer, editors, Empirical Methods
on Natural Language Generation, pages 337?361,
Berlin/Heidelberg, Germany. Springer.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
704?710.
W J M Levelt and S Kelter. 1982. Surface form and
memory in question answering. Cognitive Psychol-
ogy, 14.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1552?1561.
Alice H. Oh and Alexander I. Rudnicky. 2000. Stochas-
tic language generation for spoken dialogue systems.
In Proceedings of the 2000 ANLP/NAACL Workshop
on Conversational systems - Volume 3, pages 27?32.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistc psychology of dialog. Behavioral and
Brain Sciences, 27.
L R Rabiner. 1989. A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition.
In Proceedings of IEEE, pages 257?286.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proc. of the Annual Meeting of
the Association for Computational Lingustics (ACL),
pages 1009?1018.
Richard S Sutton and Andrew G Barto. 1998. Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA.
Menno van Zaanen. 2000. Bootstrapping syntax and
recursion using alginment-based learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning (ICML), pages 1063?1070, San
Francisco, CA, USA.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,
and Alicia Abella. 1997. PARADISE: A framework
for evaluating spoken dialogue agents. In Proc. of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 271?280.
Michael White. 2004. Reining in CCG chart realization.
In Proc. of the International Conference on Natural
Language Generation (INLG), pages 182?191.
659
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1254?1263,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Conditional Random Fields for Responsive Surface Realisation using
Global Features
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl and Oliver Lemon
Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh
n.s.dethlefs | h.hastie | h.cuayahuitl | o.lemon@hw.ac.uk
Abstract
Surface realisers in spoken dialogue sys-
tems need to be more responsive than con-
ventional surface realisers. They need to
be sensitive to the utterance context as well
as robust to partial or changing generator
inputs. We formulate surface realisation as
a sequence labelling task and combine the
use of conditional random fields (CRFs)
with semantic trees. Due to their extended
notion of context, CRFs are able to take
the global utterance context into account
and are less constrained by local features
than other realisers. This leads to more
natural and less repetitive surface realisa-
tion. It also allows generation from partial
and modified inputs and is therefore ap-
plicable to incremental surface realisation.
Results from a human rating study confirm
that users are sensitive to this extended no-
tion of context and assign ratings that are
significantly higher (up to 14%) than those
for taking only local context into account.
1 Introduction
Surface realisation typically aims to produce out-
put that is grammatically well-formed, natural and
cohesive. Cohesion can be characterised by lexical
or syntactic cues such as repetitions, substitutions,
ellipses, or connectives. In automatic language
generation, such properties can sometimes be dif-
ficult to model, because they require rich context-
awareness that keeps track of all (or much) of what
was generated before, i.e. a growing generation
history. In text generation, cohesion can span over
the entire text. In interactive settings such as gen-
eration within a spoken dialogue system (SDS), a
challenge is often to keep track of cohesion over
several utterances. In addition, since interactions
are dynamic, generator inputs from the dialogue
manager can sometimes be partial or subject to
subsequent modification. This has been addressed
by work on incremental processing (Schlangen
and Skantze, 2009). Since dialogue acts are passed
on to the generation module as soon as possible,
this can sometimes lead to incomplete generator
inputs (because the user is still speaking), or in-
puts that are subject to later modification (because
of an initial ASR mis-recognition).
In this paper, we propose to formulate surface
realisation as a sequence labelling task. We use
conditional random fields (Lafferty et al, 2001;
Sutton and McCallum, 2006), which are suitable
for modelling rich contexts, in combination with
semantic trees for rich linguistic information. This
combination is able to keep track of dependen-
cies between syntactic, semantic and lexical fea-
tures across multiple utterances. Our model can
be trained from minimally labelled data, which re-
duces development time and may (in the future)
facilitate an application to new domains.
The domain used in this paper is a pedestrian
walking around a city looking for information and
recommendations for local restaurants from an
SDS. We describe here the module for surface re-
alisation. Our main hypothesis is that the use of
global context in a CRF with semantic trees can
lead to surface realisations that are better phrased,
more natural and less repetitive than taking only
local features into account. Results from a human
rating study confirm this hypothesis. In addition,
we compare our system with alternative surface
realisation methods from the literature, namely, a
rank and boost approach and n-grams.
Finally, we argue that our approach lends itself
1254
to surface realisation within incremental systems,
because CRFs are able to model context across
full as well as partial generator inputs which may
undergo modifications during generation. As a
demonstration, we apply our model to incremen-
tal surface realisation in a proof-of-concept study.
2 Related Work
Our approach is most closely related to Lu et
al. (2009) who also use CRFs to find the best
surface realisation from a semantic tree. They
conclude from an automatic evaluation that using
CRF-based generation which takes long-range de-
pendencies into account outperforms several base-
lines. However, Lu et al?s generator does not take
context beyond the current utterance into account
and is thus restricted to local features. Further-
more, their model is not able to modify generation
results on the fly due to new or updated inputs.
In terms of surface realisation from graphical
models (and within the context of SDSs), our ap-
proach is also related to work by Georgila et al
(2002) and Dethlefs and Cuaya?huitl (2011b), who
use HMMs, Dethlefs and Cuaya?huitl (2011a) who
use Bayes Nets, and Mairesse et al (2010) who
use Dynamic Bayes Nets within an Active Learn-
ing framework. The last approach is also con-
cerned with generating restaurant recommenda-
tions within an SDS. Specifically, their system op-
timises its performance online, during the interac-
tion, by asking users to provide it with new textual
descriptions of concepts, for which it is unsure of
the best realisation. In contrast to these related
approaches, we use undirected graphical models
which are useful when the natural directionality
between the input variables is unknown.
In terms of surface realisation for SDSs, Oh and
Rudnicky (2000) present foundational work in us-
ing an n-gram-based system. They train a surface
realiser based on a domain-dependent language
model and use an overgeneration and ranking ap-
proach. Candidate utterances are ranked accord-
ing to a penalty function which penalises too long
or short utterances, repetitious utterances and ut-
terances which either contain more or less infor-
mation than required by the dialogue act. While
their approach is fast to execute, it has the dis-
advantage of not being able to model long-range
dependencies. They show that humans rank their
output equivalently to template-based generation.
Further, our approach is related to the SPaRKy
sentence generator (Walker et al, 2007). SPaRKy
was also developed for the domain of restaurant
recommendations and was shown to be equivalent
to or better than a carefully designed template-
based generator which had received high human
ratings in the past (Stent et al, 2002). It generates
sentences in two steps. First, it produces a ran-
domised set of alternative realisations, which are
then ranked according to a mapping from sentence
plans to predicted human ratings using a boosting
algorithm. As in our approach, SPaRKy distin-
guishes local and global features. Local features
take only information of the current tree node into
account, including its parents, siblings and chil-
dren, while global features take information of the
entire utterance into account. While SPaRKy is
shown to reach high output quality in compari-
son to a template-based baseline, the authors ac-
knowledge that generation with SPaRKy is rather
slow when applied in a real-time SDS. This could
present a problem in incremental settings, where
generation speed is of particular importance.
The SPaRKy system is also used by Rieser et
al. (2011), who focus on information presentation
strategies for restaurant recommendations, sum-
maries or comparisons within an SDS. Their sur-
face realiser is informed by the highest ranked
SPaRKy outputs for a particular information pre-
sentation strategy and will constitute one of our
baselines in the evaluation.
More work on trainable realisation for SDSs
generally includes Bulyko and Ostendorf (2002)
who use finite state transducers, Nakatsu and
White (2006) who use supervised learning, Varges
(2006) who uses chart generation, and Konstas
and Lapata (2012) who use weighted hypergraphs,
among others.
3 Cohesion across Utterances
3.1 Tree-based Semantic Representations
The restaurant recommendations we generate can
include any of the attributes shown in Table 1.
It is then the task of the surface realiser to find
the best realisation, including whether to present
them in one or several sentences. This often is
a sentence planning decision, but in our approach
it is handled using CRF-based surface realisation.
The semantic forms underlying surface realisation
can be produced in many ways. In our case, they
are produced by a reinforcement learning agent
which orders semantic attributes in the tree ac-
1255
Timing and Ordering
Surface Realisation
User
Interaction 
Micro-turn dialogue 
act, inform(food=Thai)
Semantic tree
String of words
intervening modules
speech
semantics of 
user utterance
(synthesised)
Manager
Figure 1: Architecture of our SDS with a focus on
the NLG components. While the user is speaking,
the dialogue manager sends dialogue acts to the
NLG module, which uses reinforcement learning
to order semantic attributes and produce a seman-
tic tree (see Dethlefs et al (2012b)). This paper fo-
cuses on surface realisation from these trees using
a CRF as shown in the surface realisation module.
Slot Example
ADDRESS The venue?s address is . . .
AREA It is located in . . .
FOOD The restaurant serves . . . cuisine.
NAME The restaurant?s name is . . .
PHONE The venue?s phone number is . . .
POSTCODE The postcode is . . .
QUALITY This is a . . . venue.
PRICE It is located in the . . . price range.
SIGNATURE The venue specialises in . . .
VENUE This venue is a . . .
Table 1: Semantic slots required for our domain
along with example realisations. Attributes can be
combined in all possible ways during generation.
cording to their confidence in the dialogue. This
is because SDSs can often have uncertainties with
regard to the user?s actual desired attribute values
due to speech recognition inaccuracies. We there-
fore model all semantic slots as probability distri-
butions, such as inform(food=Indian, 0.6) or in-
form(food=Italian, 0.4) and apply reinforcement
learning to finding the optimal sequence for pre-
sentation. Please see Dethlefs et al (2012b) for
details. Here, we simply assume that a semantic
form has been produced by a previous processing
module.
As shown in the architecture diagram in Fig-
ure 1, a CRF surface realiser takes a semantic
tree as input. We represent these as context-free
trees which can be defined formally as 4-tuples
Lexical
features
Syntactic
features
Semantic
features
The Beluga is a great Italian restaurant
y0 y1 y2
root
inform(
name=
Beluga)
The Beluga
root
inform(
venue=
Restaurant)
is a great Italian
inform(
type=
Italian)
root
restaurant
(a)
(b)
The 
Beluga
is a great
Italian
restaurant
other
phrases
(c)
Figure 2: (a) Graphical representation of a linear-
chain Conditional Random Field (CRF), where
empty nodes correspond to the labelled sequence,
shaded nodes to linguistic observations, and dark
squares to feature functions between states and ob-
servations; (b) Example semantic trees that are up-
dated at each time step in order to provide linguis-
tic features to the CRF (only one possible surface
realisation is shown and parse categories are omit-
ted for brevity); (c) Finite state machine of phrases
(labels) for this example.
{S, T,N,H}, where S is a start symbol, typically
the root node of the tree; T = {t0, t1, t2 . . . t|T |}
is a set of terminal symbols, corresponding to sin-
gle phrases; N = {n0, n1, n2 . . . n|N |} is a set of
non-terminal symbols corresponding to semantic
categories, and H = {h0, h1, h2 . . . h|H|} is a set
of production rules of the form n ? ?, where
n ? N , ? ? T ? N . The production rules rep-
resent alternatives at each branching node where
the CRF is consulted for the best available expan-
sion from the subset of possible ones. All nodes
in the tree are annotated with a semantic concept
(obtained from the semantic form) as well as their
parse category.
3.2 Conditional Random Fields for
Phrase-Based Surface Realisation
The main idea of our approach is to treat surface
realisation as a sequence labelling task in which a
sequence of semantic inputs needs to be labelled
with appropriate surface realisations. The task is
therefore to find a mapping between (observed)
1256
lexical, syntactic and semantic features and a (hid-
den) best surface realisation.
We use the linear-chain Conditional Random
Field (CRF) model for statistical phrase-based sur-
face realisation, see Figure 2 (a). This probabilis-
tic model defines the posterior probability of la-
bels (surface realisation phrases) y={y1, . . . , y|y|}
given features x={x1, . . . , x|x|} (informed by a se-
mantic tree, see Figure 2 (b)), as
P (y|x) = 1Z(x)
T?
t=1
exp
{ K?
k=1
?k?k(yt, yt?1, xt)
}
,
where Z(x) is a normalisation factor over all pos-
sible realisations (i.e. labellings) of x such that the
sum of all terms is one. The parameters ?k are
weights corresponding to feature functions ?k(.),
which are real values describing the label state y
at time t based on the previous label state yt?1 and
features xt. For example: from Figure 2 (c), ?k
might have the value ?k = 1.0 for the transition
from ?The Beluga? to ?is a great Italian?, and 0.0
elsewhere. The parameters ?k are set to maximise
the conditional likelihood of phrase sequences in
the training data set. They are estimated using the
gradient ascent algorithm.
After training, labels can be predicted for new
sequences of observations. The most likely phrase
sequence is expressed as
y ? = argmax
y
P (y|x),
which is computed using the Viterbi algorithm.
We use the Mallet package1 (McCallum, 2002) for
parameter learning and inference.
3.3 Feature Selection and Training
The following features define the generation con-
text used during training of the CRF. The genera-
tion context includes everything that has been gen-
erated for the current utterance so far. All features
can be obtained from a semantic input tree.
? Lexical items of parents and siblings,
? Semantic types in expansion,
? Semantic types of parents and siblings,
? Parse category of expansion,
? Parse categories of parents and siblings.
We use the StanfordParser2 (Marneffe et al, 2006)
to obtain the parse category for each tree node.
1http://mallet.cs.umass.edu/
2http://nlp.stanford.edu/software/
lex-parser.shtml
The semantics for each node are derived from the
input dialogue acts (these are listed in Table 1) and
are associated with nodes. The lexical items are
present in the generation context and are mapped
to semantic tree nodes.
As an example, for generating an utterance (la-
bel sequence) such as The Beluga is a great restau-
rant. It is located in the city centre., each gen-
eration step needs to take the features of the en-
tire generation history into account. This includes
all individual lexical items generated, the seman-
tic types used and the parse categories for each
tree node involved. For the first constituent, The
Beluga, this corresponds to the features {? BE-
GIN NAME} indicating the beginning of a sentence
(where empty features are omitted), the beginning
of a new generation context and the next semantic
slot required. For the second constituent, is a great
restaurant, the features are {THE BELUGA NAME
NP VENUE}, i.e. including the generation history
(with lexical items and parse category added for
the first constituent) and the semantics of the next
required slot, VENUE. In this way, a sequence of
surface form constituents is generated correspond-
ing to latent states in the CRF.
Since global utterance features capture the full
generation context (i.e. beyond the current ut-
terance), we are also able to model phenomena
such as co-references and pronouns. This is useful
for longer restaurant recommendations which may
span over more than one utterance. If the genera-
tion history already contains a semantic attribute,
e.g. the restaurant name, the CRF may afterwards
choose a pronoun, e.g. it, which has a higher like-
lihood than using the proper name again. Simi-
larly, the CRF may decide to realise a new attribute
as constituents of different order, such as a sen-
tence or PP, depending on the length, number and
parse categories of previously generated output. In
this way, our approach implicitly treats sentence
planning decisions such as the distribution of con-
tent over a set of messages in the same way as (or
as part of) surface realisation. A further capabil-
ity of our surface realiser is that it can generate
complete phrases from full as well as partial dia-
logue acts. This is useful in interactive contexts,
where we need as much robustness as possible. A
demonstration of this is given in Section 5 in an
application to incremental surface realisation.
To train the CRF, we used a data set of 552
restaurant recommendations from the website The
1257
List.3 The data contains recommendations such as
Located in the city centre, Beluga is a stylish yet
laid-back restaurant with a smart menu of modern
European cuisine.
3.4 Grammar Induction
The grammar g of surface realisation candidates
is obtained through an automatic grammar induc-
tion algorithm which can be run on unlabelled
data and requires only minimal human interven-
tion. This grammar defines the surface realisa-
tion space for the CRFs. We provide the human
corpus of restaurant recommendations from Sec-
tion 3.3 as input to grammar induction. The al-
gorithm is shown in Algorithm 1. It first identi-
fies all semantic attributes of interest in an utter-
ance, in our case those specified in Table 1, and re-
places them by a variable. These attributes include
food types, such as Mexican, Chinese, particular
parts of town, prices, etc. About 45% of them can
be identified based on heuristics. The remainder
needs to be hand-annotated at the moment, which
includes mainly attributes like restaurant names or
quality attributes, such as delicate, exquisite, etc.
Subsequently, all utterances are parsed using the
Stanford parser to obtain constituents and are inte-
grated into the grammar under construction. The
non-terminal symbols are named after the auto-
matically annotated semantic attributes contained
in their expansion, e.g. NAME QUALITY ? The
$name$ is of $quality$ quality. In this way, each
non-terminal symbol has a semantic representa-
tion and an associated parse category. In total, our
induced grammar contains more than 800 rules.
4 Evaluation
To evaluate our approach, we focus on a sub-
jective human rating study which aims to deter-
mine whether CRF-based surface realisation that
takes the full generation context into account,
called CRF (global), is perceived better by human
judges than one that uses a CRF but just takes local
context into account, called CRF (local). While
CRF (global) uses features from the entire genera-
tion history, CRF (local) uses only features from
the current tree branch. We assume that cohe-
sion can be identified by untrained judges as natu-
ral, well-phrased and non-repetitive surface forms.
To examine differences in methodology between
3http://www.list.co.uk
Algorithm 1 Grammar Induction.
1: function FINDGRAMMAR(utterances u, semantic at-
tributes a) return grammar
2: for each utterance u do
3: if u contains a semantic attribute from a, such as
venue, cuisine, etc. then
4: Find and replace the attribute by its semantic
variable, e.g. $venue$.
5: end if
6: Parse the sentence and induce a set of rules ??
?, where ? is a semantic variable and ? is its parse.
7: Traverse the parse tree in a top-down, depth-first
search and
8: if expansion ? exists then
9: continue
10: else if non-terminal ? exists then
11: add new expansion ? to ?.
12: else write new rule ?? ?.
13: end if
14: Write grammar.
15: end for
16: end function
CRFs and other state-of-the-art methods, we also
compare our system to two other baselines:
? CLASSiC corresponds to the system re-
ported in Rieser et al (2011),4 which gen-
erates restaurant recommendations based on
the SPaRKy system (Walker et al, 2007), and
has received high ratings in the past. SPaRKy
uses global utterance features.
? n-grams represents a simple 5-gram baseline
that is similar to Oh and Rudnicky (2000)?s
system. We will sample from the most likely
slot realisations that do not contain a repeti-
tion and include exactly the required slot val-
ues. Local context only is taken into account.
4.1 Human Rating Study
We carried out a user rating study on the Crowd-
Flower crowd sourcing platform.5 Each partici-
pant was shown part of a real human-system dia-
logue that emerged as part of the CLASSiC project
evaluation (Rieser et al, 2011). All dialogues
and data are freely available from http://www.
classic-project.org. Each dialogue contained
two variations for one of the utterances. These
variations were generated from two out of the four
systems described above. The order that these
were presented to the participant was counterbal-
anced. Table 2 gives an example of a dialogue seg-
ment presented to the participants.
4In Rieser et al (2011), this system is referred to as the
TIP system, which generates summaries, comparisons or rec-
ommendations for restaurants. For the present study, we com-
1258
SYS Thank you for calling the Cambridge Information
system. Your call will be recorded for research pur-
poses. You may ask for information about a place
to eat, such as a restaurant, a pub, or a cafe. How
may I help you?
USR I want to find an American restaurant which is in
the very expensive area.
SYS
A
The restaurant Gourmet Burger is an outstanding,
expensive restaurant located in the central area.
SYS
B
Gourmet Burger is a smart and welcoming restau-
rant. Gourmet Burger provides an expensive dining
experience with great food and friendly service. If
you?re looking for a central meal at an expensive
price.
USR What is the address and phone number?
SYS Gourmet Burger is on Regent Street and its phone
number is 01223 312598.
USR Thank you. Good bye.
Table 2: Example dialogue for participants to
compare alternative outputs in italics, USR=user,
SYS A=CRF (global), SYS B=CRF(local).
System Natural Phrasing Repetit.
CRF global 3.65 3.64 3.65
CRF local 3.10? 3.19? 3.13?
CLASSiC 3.53? 3.59 3.48?
n-grams 3.01? 3.09? 3.32?
Table 3: Subjective user ratings. Significance with
CRF (global) at p<0.05 is indicated as ?.
44 participants gave a total of 1,830 ratings of
utterances produced across the four systems. Flu-
ent speakers of English only were requested and
the participants were from the United States. They
were asked to rate each utterance on a 5 point Lik-
ert scale in response to the following questions
(where 5 corresponds to totally agree and 1 cor-
responds to totally disagree):
? The utterance was natural, i.e. it could have
been produced by a human. (Natural)
? The utterance was phrased well. (Phrasing)
? The utterance was repetitive. (Repetitive)
4.2 Results
We can see from Table 3 that across all the cate-
gories, the CRF (global) gets the highest overall
ratings. This difference is significant for all cat-
egories compared with CRF (local) and n-grams
(using a 1-sided Mann Whitney U-test, p < 0.001).
pare only with the subset of recommendations.
5http://www.crowdflower.com
Possibly this is because the local context taken
into account by both systems was not enough to
ensure cohesion across surface phrases. It is not
possible, e.g., to cover co-references within a lo-
cal context only or discourse markers that refer be-
yond the current utterance. This can lead to short
and repetitive phrases, such as Make your way to
Gourmet Burger. The food quality is outstanding.
The prices are expensive. generated by the n-gram
baseline.
The CLASSiC baseline, based on SPaRKy, was
the most competitive system in our comparison.
None-the-less CRF (global) is rated higher across
categories and significantly so for Natural (p <
0.05) and Repetitive (p < 0.005). For Phrasing,
there is a trend but not a significant difference (p
< 0.16). All comparisons are based on a 1-sided
Mann Whitney U-test. A qualitative comparison
between the CRF (global) and CLASSiC outputs
showed the following. CLASSiC utterances tend
to be longer and contain more sentences than CRF
(global) utterances. While CRF (global) often de-
cides to aggregate attributes into one sentence,
such as the Beluga is an outstanding restaurant
in the city centre, CLASSiC tends to rely more on
individual messages, such as The Beluga is an out-
standing restaurant. It is located in the city cen-
tre. A possible reason is that while CRF (global)
is able to take features beyond an utterance into
account, CLASSiC/SPaRKy is restricted to global
features of the current utterance.
We can further compare our results with Rieser
et al (2011) and Mairesse et al (2010) who also
generate restaurant recommendations and asked
similar questions to participants as we did. Rieser
et al (2011)?s system received an average rating
of 3.586 in terms of Phrasing which compares to
our 3.64. This difference is not significant, and
in line with the user ratings we observed for the
CLASSiC system above (3.59). Mairesse et al
(2010) achieved an average score of 4.05 in terms
of Natural in comparison to our 3.65. This differ-
ence is significant at p<0.05. Possibly their better
performance is due to the data set being more ?in
domain? than ours. They collected data from hu-
mans that was written specifically for the task that
the system was tested on. In contrast, our system
was trained on freely available data that was writ-
ten by professional restaurant reviewers. Unfortu-
nately, we cannot compare across other categories,
6This was rescaled from a 1-6 scale.
1259
USR1 I?m looking for a nice restaurant in the centre.
SYS1 inform(area=centre [0.2], food=Thai [0.3])
inform(name=Bangkok [0.3])
So you?re looking for a Thai . . .
USR2 [barges in] No, I?m looking for a restaurant
with good quality food.
SYS2 inform(quality=good [0.6], name=Beluga [0.6])
Oh sorry, so a nice restaurant located . . .
USR3 [barges in] . . . in the city centre.
SYS3 inform(area=centre [0.8])
Table 4: Example dialogue where the dialogue
manager needs to send incremental updates to the
NLG. Incremental surface realisation from seman-
tic trees for this dialogue is shown in Figure 3.
because the authors tested only for Phrasing and
Natural, respectively.
5 Incremental Surface Realisation
Recent years have seen increased interest in
incremental dialogue processing (Skantze and
Schlangen, 2009; Schlangen and Skantze, 2009).
The main characteristic of incremental architec-
tures is that instead of waiting for the end of a user
turn, they begin to process the input stream as soon
as possible, updating their processing hypotheses
as more information becomes available. From a
dialogue perspective, they can be said to work on
partial rather than full dialogue acts.
With respect to surface realisation, incremen-
tal NLG systems have predominantly relied on
pre-defined templates (Purver and Otsuka, 2003;
Skantze and Hjalmarsson, 2010; Dethlefs et al,
2012a), which limits the flexibility and quality of
output generation. Buschmeier et al (2012) have
presented a system which systematically takes
the user?s acoustic understanding problems into
account by pausing, repeating or re-phrasing if
necessary. Their approach is based on SPUD
(Stone et al, 2003), a constraint satisfaction-based
NLG architecture and marks important progress
towards more flexible incremental surface realisa-
tion. However, given the human labour involved in
constraint specification, cohesion is often limited
to a local context. Especially for long utterances
or such that are separated by user turns, this may
lead to surface form increments that are not well
connected and lack cohesion.
5.1 Application to Incremental SR
This section will discuss a proof-of-concept appli-
cation of our approach to incremental surface re-
alisation. Table 4 shows an example dialogue be-
tween a user and system that contains a number
of incremental phenomena that require hypothe-
sis updates, system corrections and user barge-
ins. Incremental surface realisation for this dia-
logue is shown in Figure 3, where processing steps
are indicated as bold-face numbers and are trig-
gered by partial dialogue acts that are sent from
the dialogue manager, such as inform(area=centre
[0.2]). The numbers in square brackets indicate
the system?s confidence in the attribute-value pair.
Once a dialogue act is observed by the NLG sys-
tem, a reinforcement learning agent determines the
order of attributes and produces a semantic tree, as
described in Section 3.1. Since the semantic forms
are constructed incrementally, new tree nodes can
be attached to and deleted from an existing tree,
depending on what kind of update is required.
In the dialogue in Table 4, the user first asks
for a nice restaurant in the centre. The dialogue
manager constructs a first attribute-value slot, in-
form(area=centre [0.2], . . . ), and passes it on to
NLG.7 In Figure 3, we can observe the corre-
sponding NLG action, a first tree is created with
just a root node and a node representing the area
slot (step 1). In a second step, the semantically
annotated node gets expanded into a surface form
that is chosen from a set of candidates (shown in
curly brackets). The CRF is responsible for this
last step. Since there is no preceding utterance, the
best surface form is chosen based on the semantics
alone. Active tree nodes, i.e. those currently under
generation, are indicated as asterisks in Figure 3.
Currently inactive nodes are shown as circles.
Step 3 then further expands the current tree
adding a node for the food type and the name of
a restaurant that the dialogue manager had passed.
We see here that attributes can either be primitive
or complex. Primitive attributes contain a single
semantic type, such as area, whereas complex at-
tributes contain multiple types, such as food, name
and need to be decomposed in a later processing
step (see steps 4 and 6). Step 5 again uses the CRF
7Note here that the information passed on to the NLG is
distinct from the dialogue manager?s own actions. In the ex-
ample, the NLG is asked to generate a recommendation, but
the dialogue manager actually decides to clarify the user?s
preferences due to low confidence. This scenario is an exam-
ple of generator inputs that may get revised afterwards.
1260
root
(1) inform
(area=centre)
(2) Right in the city centre,
{located in $area$, if 
you're looking to eat 
in $area$, in $area$, ...} 
inform(area=
centre)
(3) inform(food=Thai
        name=Bangkok)
Right in the city centre, 
root
(6) inform
(food=Thai)
(4) inform(name=
              Bangkok)
(5) Bangkok
{the $name$, 
it is called $name$,  ...}
root
inform(area=
centre)
Right in the city centre, 
inform(food=Thai, 
name=Bangkok)
root
inform(area=
centre)
Right in the city centre, 
(7) inform(quality=very
good, name=Beluga)
inform(name=
        Bangkok)
inform
(food=Thai)
Bangkok
root
inform(area=
centre)
inform(quality=nice, 
name=Beluga)
Right in the city centre, 
(8) inform(name=
Beluga)
(10) inform(quality=
very good)
(9) the Beluga
{$name$, the venue 
called $name$, ...}
(11) is of very good quality. 
{is a $quality$ venue, if you want $quality$ 
food, $quality$, a $quality$ place ...}
*
*
*
*
*
* *
**
*
**
*
Figure 3: Example of incremental surface realisation, where each generation step is indicated by a num-
ber. Active generation nodes are shown as asterisks and deletions are shown as crossed out. Lexical and
semantic features are associated with their respective nodes. Syntactic information in the form of parse
categories are also taken into account for surface realisation, but have been omitted in this figure.
to obtain the next surface realisation that connects
with the previous one (so that a sequence of real-
isation ?labels? appears: Right in the city centre
and Bangkok). It takes the full generation context
into account to ensure a globally optimal choice.
This is important, because the local context would
otherwise be restricted to a partial dialogue act,
which can be much smaller than a full dialogue
act and thus lead to short, repetitive sentences.
The dialogue continues as the system implicitly
confirms the user?s preferred restaurant (SYS1).
At this point, we encounter a user barge-in correct-
ing the desired choice. As a consequence, the dia-
logue manager needs to update its initial hypothe-
ses and communicate this to NLG. Here, the last
three tree nodes need to be deleted from the tree
because the information is no longer valid. This
update and the deletion is shown in step 7. After-
wards, the dialogue continues and NLG involves
mainly expanding the current tree into a full se-
quence of surface realisations for partial dialogue
acts which come together into a full utterance.
This example illustrates three incremental pro-
cessing steps: expansions, updates and deletions.
Expansions are the most frequent operation. They
add new partial dialogue acts to the semantic tree.
They also consult the CRF for the best surface
realisation. Since CRFs are not restricted by the
Markov condition, they are less constrained by lo-
cal context than other models and can take non-
local dependencies into account. For our applica-
tion, the maximal context is 9 semantic attributes
(for a surface form that uses all possible 10 at-
tributes). While their extended context aware-
ness can often make CRFs slow to train, they are
fast at execution and therefore very applicable to
the incremental scenario. For applications involv-
ing longer-spanning alternatives, such as texts or
paragraphs, the context of the CRF would likely
have to be constrained. Updates are triggered by
the hypothesis updates of the dialogue manager.
Whenever a new attribute comes in, it is checked
against the generator?s existing knowledge. If it
is inconsistent with previous knowledge, an up-
date is triggered and often followed by a deletion.
Whenever generated output needs to be modified,
old expansions and surface forms are deleted first,
before new ones can be expanded in their place.
5.2 Updates and Processing Speed Results
Since fast responses are crucial in incremental sys-
tems, we measured the average time our system
took for a surface realisation. The time is 100ms
on a MacBook Intel Core 2.6 Duo with 8GB in
1261
RAM. This is slightly better than other incremen-
tal systems (Skantze and Schlangen, 2009) and
much faster than state-of-the-art non-incremental
systems such as SPaRKy (Walker et al, 2007).
In addition, we measured the number of neces-
sary generation updates in comparison to a non-
incremental setting. Since updates take effect di-
rectly on partial dialogue acts, rather than the full
generated utterance, we require around 50% less
updates as if generating from scratch for every
changed input hypothesis. A qualitative analysis
of the generated outputs showed that the quality is
comparable to the non-incremental case.
6 Conclusion and Future Directions
We have presented a novel technique for surface
realisation that treats generation as a sequence la-
belling task by combining a CRF with tree-based
semantic representations. An essential property
of interactive surface realisers is to keep track of
the utterance context including dependencies be-
tween linguistic features to generate cohesive ut-
terances. We have argued that CRFs are well
suited for this task because they are not restricted
by independence assumptions. In a human rating
study, we confirmed that judges rated our output
as better phrased, more natural and less repetitive
than systems that just take local features into ac-
count. This also holds for a comparison with state-
of-the-art rank and boost or n-gram approaches.
Keeping track of the global context is also impor-
tant for incremental systems since generator inputs
can be incomplete or subject to modification. In a
proof-of-concept study, we have argued that our
approach is applicable to incremental surface real-
isation. This was supported by preliminary results
on the speed, number of updates and quality dur-
ing generation. As future work, we plan to test
our model in a task-based setting using an end-to-
end SDS in an incremental and non-incremental
setting. This study will contain additional evalu-
ation categories, such as the understandability or
informativeness of system utterances. In addition,
we may compare different sequence labelling al-
gorithms for surface realisation (Nguyen and Guo,
2007) or segmented CRFs (Sarawagi and Cohen,
2005) and apply our method to more complex sur-
face realisation domains such as text generation or
summarisation. Finally, we would like to explore
methods for unsupervised data labelling so as to
facilitate portability across domains further.
Acknowledgements
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE).
References
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533?550.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Incremental Language Generation and Incremental
Speech Synthesis. In Proceedings of the 13th An-
nual SigDial Meeting on Discourse and Dialogue
(SIGdial), Seoul, South Korea.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011a. Com-
bining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Genera-
tion in Situated Dialogue. In Proceedings of the 13th
European Workshop on Natural Language Genera-
tion (ENLG), Nancy, France.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011b.
Hierarchical Reinforcement Learning and Hidden
Markov Models for Task-Oriented Natural Lan-
guage Generation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), Portland, Oregon, USA.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of the International Con-
ference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
Kallirroi Georgila, Nikos Fakotakis, and George
Kokkinakis. 2002. Stochastic Language Modelling
for Recognition and Generation in Dialogue Sys-
tems. TAL (Traitement automatique des langues)
Journal, 43(3):129?154.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text Generation via Discriminative Reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378, Jeju Island, Korea.
John D. Lafferty, Andrew McCallum, and Fer-
nando C.N. Pereira. 2001. Conditional Random
1262
Fields: Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing (ICML), pages 282?289.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009.
Natural Language Generation with Tree Conditional
Random Fields. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
Franc?ois Mairesse, Filip Jurc???c?ek, Simon Keizer,
Blaise Thomson, Kai Yu, and Steve Young. 2010.
Phrase-Based Statistical Language Generation Us-
ing Graphical Models and Active Learning. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL), Uppsala,
Sweden.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Crystal Nakatsu and Michael White. 2006. Learning
to Say It Well: Reranking Realizations by Predicted
Synthesis Quality. In In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL) 2006, pages 1113?1120,
Sydney, Australia.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of Sequence Labeling Algorithms and Extensions.
In Proceedings of the International Conference on
Machine Learning (ICML), Corvallis, OR, USA.
Alice Oh and Alexander Rudnicky. 2000. Stochas-
tic Language Generation for Spoken Dialogue Sys-
tems. In Proceedings of the ANLP/NAACL Work-
shop on Conversational Systems, pages 27?32, Seat-
tle, Washington, USA.
Matthew Purver and Masayuki Otsuka. 2003. In-
cremental Generation by Incremental Parsing. In
In Proceedings of the 6th UK Special-Interesting
Group for Computational Linguistics (CLUK) Col-
loquium.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Sunita Sarawagi and William Cohen. 2005. Semi-
Markov Conditional Random Fields for Information
Extraction. Advances in Neural Information Pro-
cessing.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual SigDial
Meeting on Discourse and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Athens, Greece.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored Generation
for Spoken Dialogue: An Experiment. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with Communicative Intentions: The SPUD
System. Computational Intelligence, 19:311?381.
Charles Sutton and Andrew McCallum. 2006. Intro-
duction to Conditional Random Fields for Relational
Learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Sebastian Varges. 2006. Overgeneration and Ranking
for Spoken Dialogue Systems. In Proceedings of the
Fourth International Natural Language Generation
Conference (INLG), Sydney, Australia.
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
1263
Hierarchical Reinforcement Learning for Adaptive Text Generation
Nina Dethlefs
University of Bremen, Germany
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
University of Bremen, Germany
heriberto@uni-bremen.de
Abstract
We present a novel approach to natural lan-
guage generation (NLG) that applies hierar-
chical reinforcement learning to text genera-
tion in the wayfinding domain. Our approach
aims to optimise the integration of NLG tasks
that are inherently different in nature, such
as decisions of content selection, text struc-
ture, user modelling, referring expression gen-
eration (REG), and surface realisation. It
also aims to capture existing interdependen-
cies between these areas. We apply hierar-
chical reinforcement learning to learn a gen-
eration policy that captures these interdepen-
dencies, and that can be transferred to other
NLG tasks. Our experimental results?in a
simulated environment?show that the learnt
wayfinding policy outperforms a baseline pol-
icy that takes reasonable actions but without
optimization.
1 Introduction
Automatic text generation involves a number of sub-
tasks. (Reiter and Dale, 1997) list the following as
core tasks of a complete NLG system: content se-
lection, discourse planning, sentence planning, sen-
tence aggregation, lexicalisation, referring expres-
sion generation and linguistic realisation. However,
decisions made for each of these core tasks are not
independent of each other. The value of one gen-
eration task can change the conditions of others,
as evidenced by studies in corpus linguistics, and
it can therefore be undesirable to treat them all as
isolated modules. In this paper, we focus on inter-
related decision making in the areas of content se-
lection, choice of text structure, referring expression
and surface form. Concretely, we generate route in-
structions that are tailored specifically towards dif-
ferent user types as well as different environmental
features. In addition, we aim to balance the degree
of variation and alignment in texts and produce lex-
ical and syntactic patterns of co-occurrence that re-
semble those of human texts of the same domain.
Evidence for the importance of this is provided by
(Halliday and Hasan, 1976) who note the way that
lexical cohesive ties contribute to text coherence as
well as by the theory of interactive alignment. Ac-
cording to (Pickering and Garrod, 2004) we would
expect significant traces of lexical and syntactic self-
alignment in texts.
Approaches to NLG in the past have been ei-
ther rule-based (Reiter and Dale, 1997) or statisti-
cal (Langkilde and Knight, 1998). However, the for-
mer relies on a large number of hand-crafted rules,
which makes it infeasible for controlling a large
number of interrelated variables. The latter typi-
cally requires training on a large corpus of the do-
main. While these approaches may be better suitable
for larger domains, for limited domains such as our
own, we propose to overcome these drawbacks by
applying Reinforcement Learning (RL)?with a hi-
erarchical approach. Previous work that has used RL
for NLG includes (Janarthanam and Lemon, 2009)
who employed it for alignment of referring expres-
sions based on user models. Also, (Lemon, 2008;
Rieser and Lemon, 2009) used RL for optimising
information presentation styles for search results.
While both approaches displayed significant effects
of adaptation, they focused on a single area of opti-
misation. For larger problems, however, such as the
one we are aiming to solve, flat RL will not be appli-
cable due to the large state space. We therefore sug-
gest to divide the problem into a number of subprob-
lems and apply hierarchical reinforcement learning
(HRL) (Barto and Mahadevan, 2003) to solve it.
We describe our problem in more detail in Sec-
tion 2, our proposed HRL architecture in Sections
3 and 4 and present some results in Section 5. We
show that our learnt policies outperform a baseline
that does not adapt to contextual features.
2 Generation tasks
Our experiments are all drawn from an indoor
navigation dialogue system which provides users
with route instructions in a university building and
is described in (Cuaya?huitl et al, 2010). We aim
to optimise generation within the areas of content
selection, text structure, referring expression gener-
ation and surface realisation.
Content Selection Content selection decisions
are subject to different user models. We distin-
guish users who are familiar with the navigation
environment and users who are not. In this way,
we can provide different routes for these users
corresponding to their particular information need.
Specifically, we provide more detail for unfamiliar
than familiar users by adding any or several of
the following: (a) landmarks at decision points,
(b) landmarks lying on long route segments, (c)
specifications of distance.
Text Structure Depending on the type of user
and the length of the route, we choose among three
different text generation strategies to ease the cogni-
tive load of the user. Examples of all strategies are
displayed in Table 1. All three types resulted from
an analysis of a corpus of 24 human-written driving
route instructions. We consider the first type (se-
quential) most appropriate for long or medium-long
routes and both types of user. The second type (tem-
poral) is appropriate for unfamiliar users and routes
of short or medium length. It divides the route into
an explicit sequence of consecutive actions. The
third type (schematic) is used in the remaining cases.
Referring Expression Generation We dis-
tinguish three types of referring expressions:
common names, familiar names and descriptions.
In this way, entities can be named according to
the users? prior knowledge. For example, one
and the same room can be called either ?the
student union room?, ?room A3530? or ?the room
right at the corner beside the entrance to the terrace?.
Surface Realisation For surface realisation, we
aim to generate texts that display a natural balance
of (self-)alignment and variation. While it is a rule
of writing that texts should typically contain varia-
tion of surface forms in order not to appear repetitive
and stylistically poor, there is evidence that humans
also get influenced by self-alignment processes dur-
ing language production. Specifically, (Garrod and
Anderson, 1987; Pickering and Garrod, 2004) ar-
gue that the same mental representations are used
during language production and comprehension, so
that alignment occurs regardless of whether the last
utterance was made by another person or by the
speaker him- or herself (for experimental evidence
see (Branigan et al, 2000; Bock, 1986)). We can
therefore hypothesise that coherent texts will, be-
sides variation, also display a certain degree of self-
alignment. In order to determine a proper balance
of alignment and variation, we computed the degree
of lexical repetition from our corpus of 24 human
route descriptions. This analysis was based on (Hirst
and St-Onge, 1998) who retrieve lexical chains from
texts by identifying a number of relations between
lexical items. We focus here exclusively on Hirst
& St-Onge?s ?extra-strong? relations, since these can
be computed from shallow properties of texts and do
not require a large corpus of the target domain. In
order to make a fair comparison between the human
texts and our own, we used a part-of-speech (POS)
tagger (Toutanova and Manning, 2000)1 to extract
those grammatical categories that we aim to control
within our framework, i.e. nouns, verbs, preposi-
tions, adjectives and adverbs. Based on these cat-
egories, we compute the proportion of tokens that
are members in lexical chains, the ?alignment score?
(AS), according to the following equation:
AS = Lexical tokens in chains
Total number of tokens ? 100. (1)
We obtained an average alignment score of 43.3%
for 24 human route instructions. In contrast, the
1http://nlp.stanford.edu/software/tagger.shtml
Table 1: Different text generation strategies for the same underlying route.
Type 1: Sequential Type 2: Temporal Type 3: Schematic
Turn around, and go straight First, turn around. Second, - Turn around.
to the glass door in front of go straight to the glass door - Go straight until the glass door in front
you. Turn right, then follow in front of you. Third, turn of you. (20 m)
the corridor until the lift. It right. Fourth, follow the - Turn right
will be on your left-hand corridor until the lift. It will - Follow the corridor until the lift. (20 m)
side. be on your left-hand side. - It will be on your left-hand side.
same number of instructions generated by Google
Maps yielded 78.7%, i.e. an almost double amount
of repetition. We will therefore train our agent
to generate texts with an about medium alignment
score.
3 Hierarchical Reinforcement Learning
for NLG
The idea of text generation as an optimization
problem is as follows: given a set of genera-
tion states, a set of actions, and an objective
reward function, an optimal generation strategy
maximizes the objective function by choosing the
actions leading to the highest reward for every
reached state. Such states describe the system?s
knowledge about the generation task (e.g. con-
tent selection, text structure, REG, surface realiza-
tion). The action set describes the system?s ca-
pabilities (e.g. expand sequential aggregation, ex-
pand schematic aggregation, expand lexical items,
etc.). The reward function assigns a numeric value
for each taken action. In this way, text generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, where
the goal is to find an optimal strategy automatically.
To do that we use hierarchical reinforcement learn-
ing in order to optimize a hierarchy of text genera-
tion policies rather than a single policy.
The hierarchy of RL agents consists of L lev-
els and N models per level, denoted as M = M ij ,
where j ? {0, ..., N ? 1} and i ? {0, ..., L ? 1}.
Each agent of the hierarchy is defined as a Semi-
Markov Decision Process (SMDP) consisting of a
4-tuple < Sij, Aij , T ij , Rij >. Sij is a set of states, Aij
is a set of actions, and T ij is a transition function that
determines the next state s? from the current state
s and the performed action a with a probability of
P (s?|s, a). Rij(s?, ? |s, a) is a reward function that
specifies the reward that an agent receives for taking
an action a in state s at time ? . Since SMDPs allow
for temporal abstraction, that is, actions may take a
variable number of time steps to complete, the ran-
dom variable ? represents this number of time steps.
Actions can be either primitive or composite. The
former yield single rewards, the latter (executed us-
ing a stack mechanism) correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optional policy ?? that max-
imises the reward for each visited state, according to
??ij(s) = arg maxa?A Q
?i
j(s, a). (2)
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing ??. For learning a generation policy, we
use hierarchical Q-Learning (HSMQ) (Dietterich,
1999). The dynamics of SMDPs are as follows:
when an SMDP terminates its execution, it is popped
off the stack of models to execute, and control is
transferred to the next available SMDP in the stack,
and so on until popping off the root SMDP. An
SMDP terminates when it reaches one of its termi-
nal states. This algorithm is executed until the Q-
values of the root agent stabilize. The hierarchical
decomposition allows to find context-independent
policies with the advantages of policy reuse and fa-
cilitation for state-action abstraction. This hierarchi-
cal approach has been applied successfully to dia-
logue strategy learning (Cuayahuitl et al, 2010).
4 Experimental Setting
4.1 Hierarchy of SMDPs
The hierarchy consists of 15 agents. It is depicted
in Figure 1. The root agent is responsible for deter-
Figure 1: Hierarchy of agents for learning adaptive text generation strategies in the wayfinding domain
mining a route instruction type for a navigation situ-
ation. We distinguish turning, passing, locating, go-
ing and following instructions. It also chooses a text
generation strategy and the information structure of
the clause (i.e., marked or unmarked theme (Hall-
iday and Matthiessen, 2004)). Leaf agents are re-
sponsible for expanding constituents in which varia-
tion or alignment can occur, e.g. the choice of verb
or prepositional phrase.
4.2 State and action sets
We distinguish three kinds of state representations,
displayed in Table 2. The first (M010 and M10 ) en-
codes information on the spatial environment and
user type so that texts can be tailored towards these
variables. These variables play a major part in our
simulated environment (Section 5.1). The second
representation (M11 - M15 and M23 ) controls sentence
structure and ensures that all required constituents
for a message have been realised. The third (all re-
maining models) encodes variants of linguistic sur-
face structure and represents the degree of alignment
of all variants. We address the way that these align-
ment values are computed in Section 4.4. Actions
can be either primitive or composite. Whereas the
former expand a logical form directly, the latter cor-
respond to SMDPs at different levels of the hierar-
chy. All parent agents have both types of actions,
only the leaf agents have exclusively primitive ac-
tions. The set of primitive actions is displayed in Ta-
ble 2, all composite actions, corresponding to mod-
els, are shown in Figure 1. The average number of
state-action pairs for a model is |S ? A| = 77786.
While in the present work, the action set was de-
termined manually, future work can aim at learning
hierarchies of SMDPs automatically from data.
4.3 Prior Knowledge
Agents contain prior knowledge of two sorts. First,
the root agents and agents at the first level of the hi-
erarchy contain prior probabilities of executing cer-
tain actions. For example, given an unfamiliar user
and a long route, model M10 , text strategy, is initi-
ated with a higher probability of choosing a sequen-
tial text strategy than a schematic or temporal strat-
egy. Second, leaf agents of the hierarchy are initi-
ated with values of a hand-crafted language model.
These values indicate the probabilities of occurrence
of the different surface forms of the leaf agents listed
in Table 2. Both types of prior probabilities are used
by the reward functions described below.
4.4 Reward functions
We use two types of reward function, both of which
are directly motivated by the principles we stated in
Section 2. The first addresses interaction length (the
shorter the better) and the choice of actions tailored
towards the user model and spatial environment.
R =
?
?
?
0 for reaching the goal state
-10 for an already invoked subtask
p(a) otherwise
(3)
p(a) corresponds to the probability of the last ac-
tion given the current state, described above as prior
knowledge. The second reward function addresses
Table 2: State and action sets for learning adaptive text generation strategies in the wayfinding domain
Model State Variables Action Set
M00 text strategy (FV), info structure (FV), instruction (FV), expand text strategy (M10 ), turning (M23 ),
slot in focus(0=action, 1=landmark), user type(0=unfamiliar, going (M21 ), passing (M25 ), following (M22 ),
1=familiar) subtask termination(0=continue, 1=halt) locating instr.(M24 ), expand unmarked theme
M10 end(0=continue, 1=halt), text strategy (FV), route length expand schematic aggregation, expand sequen-
(0=short, 1=medium, 2=long), user type(0=unfam., 1=fam.) ce aggregation, expand temporal aggregation
M11 going vp (FV), limit (FV), SV expand going vp (M20 ), expand limit
M12 following vp (FV), SV, limit (FV) expand following vp (M21 ), expand limit
M13 turning location (FV), turning vp (FV), expand turning vp (M32 ), expand turning loc.,
SV, turning direction (FV) expand turning direction (M34 )
M14 np locatum (FV), locating vp (FV), expand np locatum, expand locating vp (M25 ),
static direction (FV), SV expand static dir. (M26 )
M15 np locatum (FV), passing vp (FV), SV, static direction (FV) expand pass. vp (M26 ), expand static dir. (M26 )
M20 vp go straight ahead, vp go straight, vp move straight ahead, Actions correspond to expansions of
vp walk straight ahead, vp walk straight (all AS) lexemes
M21 vp follow, vp go over, vp walk down, vp go down, Actions correspond to expansions of
vp go up, vp walk up, vp walk over (all AS) lexemes
M22 vp walk, vp veer , vp hang, vp bear (all AS), vp go, vp head, Actions correspond to expansions of
vp turn (all AS) lexemes
M23 identifiability(0=not id.,1=id.), user type(0=un-, expand relatum id., expand relatum, not id.,
fam.,1=fam,, relatum identifiability (FV), relatum name (FV) expand descriptive, expand common name
M24 pp nonphoric, pp nonphoric handedness, Actions correspond to expansions of
pp nonphoric poss, pp phoric pp nonphoric side (all AS) lexemes
M25 vp be, vp be located at, vp get to, vp see (all AS) Actions correspond to expansions of lexemes
M26 direction on, direction poss, direction to (all AS) Actions correspond to expansions of lexemes
M27 vp move past, vp pass, vp pass by, vp walk past (all AS) Actions correspond to expansions of lexemes
(FV = filling status): 0=unfilled, 1=filled. (SV = shared variables): the variables np actor (FV), relatum (FV),
sentence (FV) and information need (0=low, 1=high) are shared by several subagents; the same applies to their
corresponding expansion actions. (AS = alignment score): 0=unaligned, 1=low AS, 2=medium AS, 3=high AS.
the tradeoff between alignment and variation:
R =
?
?
?
0 for reaching the goal state
p(a) for medium alignment
-0.1 otherwise
(4)
Whilst the former reward function is used by the root
and models M10 - M15 and M22 , the latter is used by
models M20 - M21 and M23 - M27 . It rewards the agent
for a medium alignment score, which corresponds
to the score of typical human texts we computed
in Section 2. The alignment status of a constituent
is computed by the Constituent Alignment Score
(CAS) as follows, where MA stands for ?medium
alignment?.
CAS(a) = Count of occurrences(a)Occurences of a without MA (5)
From this score, we can determine the degree of
alignment of a constituent by assigning ?no align-
ment? for a constituent with a score of less than
0.25, ?low alignment? for a score between 0.25 and
0.5, ?medium alignment? for a score between 0.5 and
0.75 and ?high alignment? above. On the whole thus,
the agent?s task consists of finding a balance be-
tween choosing the most probable action given the
language model and choosing an action that aligns
with previous utterances.
5 Experiments and Results
5.1 Simulated Environment
The simulated environment encodes information on
the current user type (un-/familiar with the environ-
ment) and corresponding information need (low or
high), the length of the current route (short, medium-
long, long), the next action to perform (turn, go
straight, follow a path, pass a landmark or take note
of a salient landmark) and the current focus of at-
tention (the action to be performed or some salient
landmark nearby). Thus, there are five different state
variables with altogether 120 combinations, sam-
pled from a uniform distribution. This simple form
of stochastic behaviour is used in our simulated en-
vironment. Future work can consider inducing a
learning environment from data.
5.2 Comparison of learnt and baseline policies
In order to test our framework, we designed a sim-
ulated environment that simulates different naviga-
tional situations, routes of different lengths and dif-
ferent user types. We trained our HRL agent for
10.000 episodes with the following learning param-
eters: the step-size parameter ? was initiated with 1
and then reduced over time by ? = 11+t , t being the
time step. The discount rate parameter ? was 0.99
and the probability of random action ? was 0.01 (see
(Sutton and Barto, 1998) for details on these param-
eters). Figure 2 compares the learnt behaviour of
our agent with a baseline (averaged over 10 runs)
that chooses actions at random in models M10 and
M20 - M27 (i.e., the baseline does not adapt its text
strategy to user type or route length and neither per-
forms adaptation of referring expressions or align-
ment score). The user study reported in (Cuaya?huitl
et al, 2010) provided users with instruction using
this baseline generation behaviour. The fact that
users had a user satisfaction score of 90% indicates
that this is a sensible baseline, producing intelligi-
ble instructions. We can observe that after a certain
number of episodes, the performance of the trained
agent begins to stabilise and it consistently outper-
forms the baseline.
6 Example of generation
As an example, Figure 3 shows in detail the genera-
tion steps involved in producing the clause ?Follow
102 103 104
?65
?60
?55
?50
?45
?40
?35
?30
?25
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Learnt Behaviour
Baseline
Figure 2: Comparison of learnt and baseline behaviour in
the generation of route descriptions
the corridor until the copyroom? for an unfamiliar
user and a route of medium length. Generation starts
with the root agent in state (0,0,0,0,0,0), which in-
dicates that text strategy, info structure and instruc-
tion are unfilled slots, the slot in focus of the sen-
tence is an action, the status of subtask termination
is ?continue? and the user type is unfamiliar. After
the primitive action expand unmarked theme was
executed, the state is updated to (0,1,0,0,0,0), in-
dicating the filled slot. Next, the composite action
text strategy is executed, corresponding to model
M10 . The initial state (1,0,0) indicates a route
of medium length, an unfilled text strategy slot
and an unfamiliar user. After the primitive ac-
tion expand sequential text was chosen, the ter-
minal state is reached and control is returned to
the root agent. Here, the next action is follow-
ing instruction corresponding to model M12 . The
initial state (0,1,0,0,0,0) here indicates unfilled slots
for following vp, np actor, sentence, path, limit
and relatum, as well as a high information need
of the current user. The required constituents
are expanded in turn. First, the primitive actions
expand limit, expand np actor, expand s and ex-
pand path cause their respective slots in the state
representation to be filled. Next, the composite ac-
tion expand relatum is executed with an initial state
(0,1,0,0) representing an identifiable landmark, un-
filled slots for a determiner and a referring expres-
sion for the landmark and an unfamiliar user. Two
primitive actions, expand relatum identifiable and
expand relatum common name, cause the agent to
reach its terminal state. The generated referring ex-
pression thus treats the referenced entity as either
known or easily recoverable. Finally, model M21
executes the composite action expand following vp,
which is initialised with a number of variables cor-
responding to the alignment status of different verb
forms. Since this is the first time this agent is called,
none of them shows traces of alignment (i.e., all val-
ues are 0). Execution of the primitive action ex-
pand following vp causes the respective slot to be
updated and the agent to terminate. After this sub-
task, model M12 has also reached its terminal state
and control is returned to the root agent.
As a final step towards surface generation, all cho-
sen actions are transformed into an SPL (Kasper,
1989). The type ?following instruction? leads to the
initialisation of a semantically underspecified scaf-
fold of an SPL, all other actions serve to supplement
this scaffold to preselect specific syntactic structures
or lexical items. For example, the choice of ?ex-
pand following vp? leads to the lexical item ?fol-
low? being inserted. Similarly, the choice of ?ex-
pand path? leads to the insertion of ?the corridor?
into the SPL to indicate the path the user should fol-
low. ?expand limit?, in combination with the choice
of referring expression, leads to the insertion of the
PP ?until the copy room?. For generation of more
than one instruction, aggregation has to take place.
This is done by iterating over all instructions of a
text and inserting them into a larger SPL that re-
alises the aggregation. Finally, the constructed SPL
is passed to the KPML surface generator (Bateman,
1997) for string realisation.
7 Discussion
We have argued in this paper that HRL is an es-
pecially suited framework for generating texts that
are adaptive to different users, to environmental fea-
tures and properties of surface realisation such as
alignment and variation. While the former tasks ap-
pear intuitively likely to contribute to users? com-
prehension of texts, it is often not recognised that
the latter task can have the same effect. Differing
surface forms of identical concepts in texts without
motivation can lead to user confusion and deterio-
rate task success. This is supported by Clark?s ?prin-
ciple of contrast? (Clark, 1987), according to which
new expressions are only introduced into an interac-
tion when the speaker wishes to contrast them with
other entities already present in the discourse. Si-
miliarly, a study by (Clark and Wilkes-Gibbs, 1986)
showed that interlocutors tend to align their referring
expressions and thereby achieve more efficient and
successful dialogues. We tackled the integration of
different NLG tasks by applying HRL and presented
results, which showed to be promising. As an al-
ternative to RL, other machine learning approaches
may be conceivable. However, supervised learning
requires a large amount of training data, which may
not always be available, and may also produce un-
predictable behaviour in cases where a user deviates
from the behaviour covered by the corpus (Levin
et al, 2000). Both arguments are directly trans-
ferable to NLG. If an agent is able to act only on
grounds of what it has observed in a training cor-
pus, it will not be able to react flexibly to new state
representations. Moreover, it has been argued that
a corpus for NLG cannot be regarded as an equiv-
alent gold standard to the ones of other domains of
NLP (Belz and Reiter, 2006; Scott and Moore, 2006;
Viethen and Dale, 2006). The fact that an expres-
sion for a semantic concept does not appear in a cor-
pus does not mean that it is an unsuited or impos-
sible expression. Another alternative to pure RL is
to apply semi-learnt behaviour, which can be help-
ful for tasks with very large state-action spaces. In
this way, the state-action space is reduced to only
sensible state-action pairs by providing the agent
with prior knowledge of the domain. All remain-
ing behaviour continues to be learnt. (Cuaya?huitl,
2009) suggests such an approach for learning dia-
logue strategies, but again the principle is transfer-
able to NLG. While there is room for exploration
of different RL methods, it is clear that neither tra-
ditional rule-based accounts of generation, nor n-
gram-based generators can achieve the same flexible
generation behaviour given a large, and partially un-
known, number of state variables. Since state spaces
are typically very large, specifying rules for each
single condition is at best impractical. Especially for
tasks such as achieving a balanced alignment score,
as we have shown in this paper, decisions depend on
very fine-grained textual cues such as patterns of co-
occurrence which are hard to pin down accurately
by hand. On the other hand, statistical approaches
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M00 (0, 0, 0, 0, 0, 0)
{action = expand unmarked
theme}
M00 (0, 1, 0, 0, 0, 0)
{action = text strategy}
?
?
M10 (1, 0, 0)
{action = expand sequential text}
M10 (1, 1, 0), (terminalstate)
?
?
M00 , (1, 1, 0, 0, 0, 0),
{action = following instruction}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M21 (0, 1, 0, 0, 0, 0)
{action = expand limit}
M21 (0, 1, 1, 0, 0, 0)
{action = expand np actor}
M21 (0, 1, 1, 1, 0, 0)
{action = expand s}
M21 (0, 1, 1, 0, 1, 0)
{action = expand path}
M21 (0, 1, 1, 1, 1, 0)
{action = expand relatum}
?
?
?
?
?
?
?
?
?
M23 (0, 0, 0, 0)
{action = expand relatum
identifiable}
M23 (0, 1, 0, 0)
{action = expand relatum
common name}
M23 (0, 1, 1, 0), (terminalstate)
?
?
?
?
?
?
?
?
?
M21 (0, 1, 1, 1, 1, 0)
{action = expand following
vp}
?
?
?
?
M21 (0, 0, 0, 0, 0, 0, 0, 0, 0)
{action = follow}
M21 (1, 0, 0, 0, 0, 0, 0, 0, 0)
(terminalstate)
?
?
?
?
M21 (1, 1, 1, 1, 1, 1)(terminalstate)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M00 (1, 1, 1, 0, 1, 0)(terminalstate)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Example of generation for the clause ?Follow the corridor until the copy room?. This example shows decision
making for a single instruction, adaptation and alignment occurs over longer sequences of text.
to generation that are based on n-grams focus on the
frequency of constructions in a corpus without tak-
ing contextual variables such as user type or environ-
mental properties into account. Further, they share
the problem of supervised learning approaches dis-
cussed above, namely, that it can act only on grounds
of what it has observed in the past, and are not well
able to adapt to novel situations. For a more de-
tailed account of statistical and trainable approaches
to NLG as well as their advantages and drawbacks,
see (Lemon, 2008).
8 Conclusion
We presented a novel approach to text generation
that applies hierarchical reinforcement learning to
optimise the following interrelated NLG tasks: con-
tent selection, choice of text structure, referring ex-
pressions and surface structure. Generation deci-
sions in these areas were learnt based on three differ-
ent variables: the type of user, the properties of the
spatial environment and the proportion of alignment
and variation in texts. Based on a simulated envi-
ronment, we compared the results of different poli-
cies and demonstrated that the learnt policy outper-
forms a baseline that chooses actions without taking
contextual variables into account. Future work can
transfer our approach to different domains of appli-
cation or to other NLG tasks. In addition, our pre-
liminary simulation results should be confirmed in
an evaluation study with real users.
Acknowledgements
This work was partly supported by DFG SFB/TR8
?Spatial Cognition?.
References
Barto, A. G. and Mahadevan, S. (2003). Recent Ad-
vances in Hierarchical Reinforcement Learning. Dis-
crete Event Dynamic Systems, 13:2003.
Bateman, J. A. (1997). Enabling technology for multi-
lingual natural language generation: the KPML devel-
opment environment. Natural Language Engineering,
3(1):15?55.
Belz, A. and Reiter, E. (2006). Comparing automatic and
human evaluation of nlg systems. In In Proc. EACL06,
pages 313?320.
Bock, K. (1986). Syntactic persistence in language pro-
duction. Cognitive Psychology, 18.
Branigan, H. P., Pickering, M. J., and Cleland, A. (2000).
Syntactic coordination in dialogue. Cognition, 75.
Clark, E. (1987). The principle of contrast: A constraint
on language acquisition. In MacWhinney, B., edi-
tor, Mechanisms of Language Acquisition, pages 1?33.
Lawrence Erlbaum Assoc., Hillsdale, NJ.
Clark, H. H. and Wilkes-Gibbs, D. (1986). Referring as
a colloborative process. Cognition, 22.
Cuaya?huitl, H. (2009). Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD thesis,
School of Informatics, University of Edinburgh.
Cuaya?huitl, H., Dethlefs, N., Richter, K.-F., Tenbrink, T.,
and Bateman, J. (2010). A dialogue system for indoor
wayfinding using text-based natural language. In-
ternational Journal of Computational Linguistics and
Applications, ISSN 0976-0962.
Cuayahuitl, H., Renals, S., Lemon, O., and Shimodaira,
H. (2010). Evaluation of a hierarchical reinforcement
learning spoken dialogue system. Computer Speech
and Language, 24(2):395?429.
Dietterich, T. G. (1999). Hierarchical reinforcement
learning with the maxq value function decomposition.
Journal of Artificial Intelligence Research, 13:227?
303.
Garrod, S. and Anderson, A. (1987). Saying What You
Mean in Dialogue: A Study in conceptual and seman-
tic co-ordination. Cognition, 27.
Halliday, M. A. K. and Hasan, R. (1976). Cohesion in
English. Longman, London.
Halliday, M. A. K. and Matthiessen, C. M. I. M. (2004).
An Introduction to Functional Grammar. Edward
Arnold, London, 3rd edition.
Hirst, G. and St-Onge, D. (1998). Lexical chains as rep-
resentations of context for the detection and correction
of malapropisms. In Fellbaum, C., editor, WordNet:
An Electronic Database and Some of its Applications,
pages 305?332. MIT Press.
Janarthanam, S. and Lemon, O. (2009). Learning lexi-
cal alignment policies for generating referring expres-
sions in spoken dialogue systems. In ENLG ?09: Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 74?81, Morristown, NJ,
USA.
Kasper, R. (1989). SPL: A Sentence Plan Language for
text generation. Technical report, USC/ISI.
Langkilde, I. and Knight, K. (1998). Generation that ex-
ploits corpus-based statistical knowledge. In ACL-36:
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
pages 704?710.
Lemon, O. (2008). Adaptive Natural Language Gener-
ation in Dialogue using Reinforcement Learning. In
SemDial.
Levin, E., Pieraccini, R., and Eckert, W. (2000). A
stochastic model of computer-human interaction for
learning dialogue strategies. IEEE Transactions on
Speech and Audio Processing, 8.
Pickering, M. J. and Garrod, S. (2004). Toward a mecha-
nistc psychology of dialog. Behavioral and Brain Sci-
ences, 27.
Reiter, E. and Dale, R. (1997). Building applied natural
language generation systems. Natural Language En-
gineering, 3(1):57?87.
Rieser, V. and Lemon, O. (2009). Natural language gen-
eration as planning under uncertainty for spoken dia-
logue systems. In EACL ?09: Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 683?691,
Morristown, NJ, USA.
Scott, D. and Moore, J. (2006). An NLG evaluation com-
petition? eight reasons to be cautious. Technical re-
port.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA, USA.
Toutanova, K. and Manning, C. D. (2000). Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natural
language processing and very large corpora, pages
63?70, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Viethen, J. and Dale, R. (2006). Towards the evaluation
of referring expression generation. In In Proceedings
of the 4th Australiasian Language Technology Work-
shop, pages 115?122.
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 78?87,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Optimising Natural Language Generation Decision Making
For Situated Dialogue
Nina Dethlefs
Department of Linguistics,
University of Bremen
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
German Research Centre
for Artificial Intelligence (DFKI)
heriberto.cuayahuitl@dfki.de
Jette Viethen
Centre for Language Technology
acquarie University
jviethen@ics.mq.edu.au
Abstract
Natural language generators are faced with a
multitude of different decisions during their
generation process. We address the joint opti-
misation of navigation strategies and referring
expressions in a situated setting with respect to
task success and human-likeness. To this end,
we present a novel, comprehensive framework
that combines supervised learning, Hierarchi-
cal Reinforcement Learning and a hierarchical
Information State. A human evaluation shows
that our learnt instructions are rated similar
to human instructions, and significantly better
than the supervised learning baseline.
1 Introduction
Natural Language Generation (NLG) systems are
typically faced with a multitude of decisions dur-
ing their generation process due to nondeterminacy
between a semantic input to a generator and its re-
alised output. This is especially true in situated set-
tings, where sudden changes of context can occur
at anytime. Sources of uncertainty include (a) the
situational context, such as visible objects, or task
complexity, (b) the user, including their behaviour
and reactions, and (c) the dialogue history, includ-
ing shared knowledge or patterns of linguistic con-
sistency (Halliday and Hasan, 1976) and alignment
(Pickering and Garrod, 2004).
Previous work on context-sensitive generation in
situated domains includes Stoia et al (2006) and
Garoufi and Koller (2010). Stoia et al present a
supervised learning approach for situated referring
expression generation (REG). Garoufi and Koller
use techniques from AI planning for the combined
generation of navigation instructions and referring
expressions (RE). More generally, the NLG prob-
lem of non-deterministic decision making has been
addressed from many different angles, including
PENMAN-style choosers (Mann and Matthiessen,
1983), corpus-based statistical knowledge (Langk-
ilde and Knight, 1998), tree-based stochastic models
(Bangalore and Rambow, 2000), maximum entropy-
based ranking (Ratnaparkhi, 2000), combinatorial
pattern discovery (Duboue and McKeown, 2001),
instance-based ranking (Varges, 2003), chart gen-
eration (White, 2004), planning (Koller and Stone,
2007), or probabilistic generation spaces (Belz,
2008) to name just a few.
More recently, there have been several approaches
towards using Reinforcement Learning (RL) (Rieser
et al, 2010; Janarthanam and Lemon, 2010) or Hi-
erarchical Reinforcement Learning (HRL) (Deth-
lefs and Cuaya?huitl, 2010) for NLG decision mak-
ing. All of these approaches have demonstrated that
HRL/RL offers a powerful mechanism for learn-
ing generation policies in the absence of complete
knowledge about the environment or the user. It
overcomes the need for large amounts of hand-
crafted knowledge or data in rule-based or super-
vised learning accounts. On the other hand, RL
can have difficulties to find an optimal policy in a
large search space, and is therefore often limited to
small-scale applications. Pruning the search space
of a learning agent by including prior knowledge is
therefore attractive, since it finds solutions faster, re-
duces computational demands, incorporates expert
knowledge, and scales to complex problems. Sug-
78
gestions to use such prior knowledge include Lit-
man et al (2000) and Singh et al (2002), who
hand-craft rules of prior knowledge obvious to the
system designer. Cuaya?huitl (2009) suggests us-
ing Hierarchical Abstract Machines to partially pre-
specify dialogue strategies, and Heeman (2007) uses
a combination of RL and Information State (IS)
to also pre-specify dialogue strategies. Williams
(2008) presents an approach of combining Partially-
Observable Markov Decision Processes with con-
ventional dialogue systems. The Information State
approach is well-established in dialogue manage-
ment (e.g., Bohlin et al (1999) and Larsson and
Traum (2000)). It allows the system designer to
specify dialogue strategies in a principled and sys-
tematic way. A disadvantage is that random design
decisions need to be made in cases where the best
action, or sequence of actions, is not obvious.
The contribution of this paper consists in a com-
prehensive account of constrained Hierarchical Re-
inforcement Learning through a combination with
a hierarchical Information State (HIS), which is in-
formed by prior knowledge induced from decision
trees. We apply our framework to the generation
of navigation strategies and referring expressions in
a situated setting, jointly optimised for task suc-
cess and linguistic consistency. An evaluation shows
that humans prefer our learnt instructions to the su-
pervised learning-based instructions, and rate them
equal to human instructions. Simulation-based re-
sults show that our semi-learnt approach learns more
quickly than the fully-learnt baseline, which makes
it suitable for large and complex problems. Our ap-
proach differs from Heeman?s in that we transfer it
to NLG and to a hierarchical setting. Although Hee-
man was able to show that his combined approach
learns more quickly than pure RL, it is limited to
small-scale systems. Our ?divide-and-conquer? ap-
proach, on the other hand, scales up to large search
spaces and allows us to address complex problems.
2 The Generation Tasks
2.1 The GIVE-2 Domain
Our domain is the generation of navigation instruc-
tions and referring expressions in a virtual 3D world
in the GIVE scenario (Koller et al, 2010). In this
task, two people engage in a ?treasure hunt?, where
an instruction giver (IG) navigates an instruction fol-
lower (IF) through the world, pressing a sequence of
buttons and completing the task by obtaining a tro-
phy. Pairs take part in three dialogues (in three dif-
ferent worlds); after the first dialogue, they switch
roles. The GIVE-2 corpus (Gargett et al, 2010) pro-
vides transcripts of such dialogues in English and
German. For this paper, we complemented the En-
glish dialogues of the corpus with a set of seman-
tic annotations.1 The feature set is organised in five
groups (Table 1). The first two groups cover manip-
ulation instructions (i.e., instructions to press a but-
ton), including distractors2 and landmarks (Gargett
et al, 2010). The third group describes high- and
low-level navigation, the fourth group describes the
user. The fifth group finally contains grammatical
information.
2.2 Navigation and Manipulation Instructions
Navigation instructions can take many forms, even
for the same route. For example, a way to another
room can be described as ?go to the room with the
lamp?, ?go left and through the door?, or ?turn 90
degrees, left, straight?. Choosing among these vari-
ants is a highly context- and speaker-dependent task.
Figure 1 shows the six user strategies we identified
from the corpus based on an analysis of the combi-
nation of navigation level (?high? vs. ?low?) and con-
tent (?destination?, ?direction?, ?orientation?, ?path?,
?straight?). User models are based on the navigation
level and content decisions made in a sequence of in-
structions, so that different sequences, with a certain
distribution, lead to different user model classifica-
tions. The proportions are shown in Figure 1. We
found that 75% of all speakers use the same strat-
egy in consecutive rounds/games. 62.5% of pairs
are consistent over all three dialogues, indicating
inter-speaker alignment. These high measures of
human consistency suggest that this phenomenon
is worth modelling in a learning agent, and there-
fore provides the motivation of including linguis-
tic consistency in our agent?s behaviour. Manipula-
tion instructions were treated as an REG task, which
needs to be sensitive to the properties of the referent
and distractors (e.g, size, colour, or spatial relation
1The annotations are available on request.
2Distractors are objects of the same type as the referent.
79
ID Feature Type Description
f1 absolute property(referent) boolean Is the colour of the referent mentioned?
f2 absolute property(distractor) boolean Is the colour of the distractor mentioned?
f3 discriminative colour(referent) boolean Is the colour of the referent discriminating?
f4 discriminative colour(distractor) boolean Is the colour of the distractor discriminating?
f5 mention(distractor) boolean Is a distractor mentioned?
f6 first mention(referent) boolean Is this the first reference to the referent?
f7 mention(macro landmark) boolean Is a macro (non-movable) landmark mentioned?
f8 mention(micro landmark) boolean Is a micro (movable) landmark mentioned?
f9 num(distractors) integer How many distractors are present?
f10 num(micro landmarks) integer How many micro landmarks are present?
f11 spatial rel(referent,obj) string Which spatial relation(s) are used in the RE?
f12 taxonomic property(referent) boolean Is the type of the distractor mentioned?
f13 within field of vision(referent) boolean Is the referent within the user?s field of vision?
f14 mention(colour, lm) boolean Is the colour of a macro- / micro lm mentioned?
f15 mention(size, lm) boolean Is the size of a macro- / micro lm mentioned?
f16 abstractness(nav instruction) string Is the instruction explicit or implicit?
f17 content(nav instruction) string Vals: destination, direction, orientation, path, straight
f18 level(nav instruction) string Is the instruction high- or low-level?
f19 position(user) string Is the user on track or off track?
f20 reaction(user) string Vals: take action, take wrong action, wait, req help
f21 type(user) string Vals: likes waiting, likes exploring, in between
f22 waits(user) boolean Is the user waiting for the next instruction?
f23 model(user) string User model/navig. strategy used (cf. Fig.1)?
f24 actor(instruction) boolean Is the actor of the instruction inserted?
f25 mood(instruction) boolean Is the mood of the instruction inserted?
f26 process(instruction) boolean Is the process of the instruction inserted?
f27 locational phrase(instruction) boolean Is the loc. phrase (path, straight, etc.) inserted?
Table 1: Corpus annotation features that were used as knowledge of the learning agent and the Information State. Fea-
tures are presented in groups, describing the properties of referents in the environment (f1...f13) and their distractors
(f14...f15), features of high- and low-level navigation (f16...f18), the user (f19...f23), and grammatical information
about constituents (f24...f27).
with respect to the referent) to be natural and dis-
tinguishing. We also considered the visual salience
of objects, and the type of spatial relation involved,
since recent studies indicate the potential relevance
of these features (Viethen and Dale, 2008). Given
these observations, we aim to optimise the task suc-
cess and linguistic consistency of instructions. Task
success is measured from user reactions after each
instruction (Section 5.1). Linguistic consistency is
achieved by rewarding the agent for generating in-
structions that belong to the same user model as the
previous one. The agent has the same probability
for choosing any pattern, but is then rewarded for
consistency. Table 3 (in Section 5.2) presents an ex-
ample dialogue generated by our system.
3 Constrained Hierarchical Reinforcement
Learning for NLG
3.1 Hierarchical Reinforcement Learning
Our idea of language generation as an optimisa-
tion problem is as follows: given a set of genera-
tion states, a set of actions, and an objective reward
function, an optimal generation strategy maximises
the objective function by choosing the actions lead-
ing to the highest reward for every reached state.
Such states describe the system?s knowledge about
80
Figure 1: Decision tree for the classification of user
models (UM) defined by the use of navigation level and
content. UM 0=high-level, UM 1=low-level (LL), UM
2=orientation-based LL, UM 3=orientation-based mix-
ture (M), UM 4=path-based M, UM 5=pure M.
the generation task (e.g. navigation strategy, or re-
ferring expressions). The action set describes the
system?s capabilities (e.g. ?use high level naviga-
tion strategy?, ?mention colour of referent?, etc.).
The reward function assigns a numeric value for
each action taken. In this way, language generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, where
the goal is to find an optimal strategy automatically.
To do this we use RL with a divide-and-conquer ap-
proach in order to optimise a hierarchy of generation
policies rather than a single policy. The hierarchy of
RL agents consists of L levels and N models per
level, denoted as M ij , where j ? {0, ..., N ? 1}
and i ? {0, ..., L ? 1}. Each agent of the hierar-
chy is defined as a Semi-Markov Decision Process
(SMDP) consisting of a 4-tuple < Sij, Aij , T ij , Rij >.
Sij is a set of states, Aij is a set of actions, T ij is
a transition function that determines the next state
s? from the current state s and the performed ac-
tion a, and Rij is a reward function that specifies
the reward that an agent receives for taking an ac-
tion a in state s lasting ? time steps. The random
variable ? represents the number of time steps the
agent takes to complete a subtask. Actions can be
either primitive or composite. The former yield sin-
gle rewards, the latter correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optimal policy that max-
imises the reward for each visited state, according to
??ij(s) = arg maxa?Aij Q
?i
j(s, a), where Q?ij (s, a)
specifies the expected cumulative reward for exe-
cuting action a in state s and then following pol-
icy ??ij . We use HSMQ-Learning (Dietterich, 1999)
for learning a hierarchy of generation policies. This
hierarchical approach has been applied successfully
to dialogue strategy learning by Cuaya?huitl et al
(2010).
3.2 Information State
The notion of an Information State has traditionally
been applied to dialogue, where it encodes all infor-
mation relevant to the current state of the dialogue.
This includes, for example, the context of the in-
teraction, participants and their beliefs, and the sta-
tus of grounding. An IS consists of a set of infor-
mational components, encoding the information of
the dialogue, formal representations of these com-
ponents, a set of dialogue moves leading to the up-
date of the IS, a set of update rules which govern the
update, and finally an update strategy, which speci-
fies which update rule to apply in case more than one
applies (Larsson and Traum (2000), p. 2-3). In this
paper, we apply the theory of IS to language gener-
ation. For this purpose we define the informational
components of an IS to represent the (situational and
linguistic) knowledge of the generator (Section 4.2).
Update rules are triggered by generator actions, such
as the decision to insert a new constituent into the
current logical form, or the decision to prefer one
word order sequence over another. We use the DIP-
PER toolkit (Bos et al, 2003)3 for our implementa-
tion of the IS.
3.3 Combining Hierarchical Reinforcement
Learning and Information State
Previous work has suggested the HSMQ-Learning
algorithm for optimizing text generation strategies
(Dethlefs and Cuaya?huitl, 2010). Because such an
algorithm uses all available actions in each state,
an important extension is to constrain the actions
available with some prior expert knowledge, aim-
ing to combine behaviour specified by human de-
signers and behaviour automatically inferred by re-
inforcement learning agents. To that end, we sug-
3http://www.ltg.ed.ac.uk/dipper
81
Figure 2: (Left:) Hierarchy of learning agents executed from top to bottom for generating instructions. (Right:) State
representations for the agents shown in the hierarchy on the left. The features f1...f27 refer back to the features used
in the annotation given in the first column of Table 1. Note that agents can share information across levels.
gest combining the Information State approach with
hierarchical reinforcement learning. We therefore
re-define the characterisation of each Semi-Markov
Decision Process (SMDP) in the hierarchy as a 5-
tuple model M ij =< Sij, Aij , T ij , Rij , Iij >, where
Sij , Aij , T ij and Rij are as before, and the additional
element Iij is an Information State used as knowl-
edge base and rule-based decision maker. In this ex-
tended model, action selection is based on a con-
strained set of actions provided by the IS update
rules. We assume that the names of update rules
in Iij represent the agent actions Aij . The goal of
each SMDP is then to find an optimal policy that
maximises the reward for each visited state, accord-
ing to ??ij(s) = arg maxa?Aij?Iij Q
?i
j(s, a), where
Q?ij (s, a) specifies the expected cumulative reward
for executing constrained action a in state s and then
following ??ij thereafter. For learning such poli-
cies we use a modified version of HSMQ-Learning.
This algorithm receives subtask M ij and Information
State Iij used to initialise state s, performs similarly
to Q-Learning for primitive actions, but for compos-
ite actions it invokes recursively with a child sub-
task. In contrast to HSMQ-Learning, this algorithm
chooses actions from a subset derived by applying
the IS update rules to the current state of the world.
When the subtask is completed, it returns a cumu-
lative reward rt+? , and continues its execution until
finding a goal state for the root subtask. This process
iterates until convergence occurs to optimal context-
independent policies, as in HSMQ-Learning.
4 Experimental Setting
4.1 Hierarchy of Agents
Figure 2 shows a (hand-crafted) hierarchy of learn-
ing agents for navigating and acting in a situated en-
vironment. Each of these agents represents an indi-
vidual generation task. Model M00 is the root agent
and is responsible for ensuring that a set of naviga-
tion instructions guide the user to the next referent,
where an RE is generated. Model M10 is responsible
for the generation of the RE that best describes an
intended referent. Subtasks M20 ... M22 realise sur-
face forms of possible distractors, or macro- / micro
landmarks. Model M12 is responsible for the gener-
ation of navigation instructions which smoothly fit
into the linguistic consistency pattern chosen. Part
of this task is choosing between a low-level (model
M23 ) and a high-level (model M24 ) instruction. Sub-
tasks M30 ...M34 realise the actual instructions, des-
tination, direction, orientation, path, and ?straight?,
respectively.4 Finally, model M11 can repair previ-
ous system utterances.
4Note that navigation instructions and REs correspond to se-
quences of actions, not to a single one.
82
Model(s) Actions
M00 navigation, manipulation, confirmation, stop, repair system act, repair no system act
M10 insert distractor, insert no distractor, insert no absolute property, insert micro relatum, insert macro relatum
insert no taxonomic property, insert absolute property, insert no macro relatum, insert taxonomic property
M12 choose high level, choose low level, get route, choose easy route, choose short route
M20 ... M22 exp head, exp no head, insert colour, insert no colour, insert size, insert no size, exp spatial relation
M23 choose explicit abstractness, choose implicit abstractness, destination instruction, path instruction
M24 choose explicit abstractness, choose implicit abstractness, direction instr, orientation instr, straight instr
M30 ... M34 exp actor, exp no actor, exp mood, exp loc phrase, exp no loc phrase, exp process, exp no process
Table 2: Action set of the learning agents and Information States.
4.2 State and Action Sets
The HRL agent?s knowledge base consists of all sit-
uational and linguistic knowledge the agent needs
for decision making. Figure 2 shows the hierarchy
of learning agents together with the knowledge base
of the learning agent with respect to the semantic
features shown in Table 1 that were used for the an-
notation of the GIVE-2 corpus dialogues. The first
column of the table in Figure 2 indicates the respec-
tive model, also referred to as agent, or subtask, and
the second column refers to the knowledge variable
it uses (in the form of the feature index given in the
first column of Table 1). In the agent, boolean values
and strings were represented as integers. The HIS
shares all information of the learning agent, but has
an additional set of relational feature-value pairs for
each slot. For example, if the agent knows that the
slot content(nav instruction) has value 1 (mean-
ing ?filled?), the HIS knows also which value it was
filled with, such as path. Such additional knowledge
is required for the supervised learning baseline (Sec-
tion 5). The action set of the hierarchical learning
agent and the hierarchical information state is given
in Table 2. The state-action space size of a flat learn-
ing agent would be |S ?A| = 1011, the hierarchical
setting has a state-action space size of 2.4 ? 107.
The average state-action space size of all subtasks is
|S ? A|/14 = 1.7 ? 107. Generation actions can
be primitive or composite. While the former corre-
spond to single generation decisions, the latter rep-
resent separate generation subtasks (Fig. 2).
4.3 Prior Knowledge
Prior knowledge can include decisions obvious to
the system designer, expert knowledge, or general
intuitions. In our case, we use a supervised learn-
ing approach to induce prior knowledge into our
HRL agent. We trained decision trees on our anno-
tated corpus data using Weka?s (Witten and Frank,
2005) J48 decision tree classifer. A separate tree
was trained for each semantic attribute (cf. Table
1). The obtained decision trees represent our super-
vised learning baseline. They achieved an accuracy
of 91% in a ten-fold cross-validation. For our semi-
learnt combination of HRL and HIS, we performed a
manual analysis of the resulting rules to assess their
impact on a learning agent.5 In the end, the fol-
lowing rules were used to constrain the agent?s be-
haviour: (1) In REs, always use a referent?s colour,
except in cases of repair when colour is not discrim-
inating; (2) mention a distractor or micro landmark,
if the colour of the referent is not discriminating;
(3) in navigation, always make orientation instruc-
tions explicit. All remaining behaviour was subject
to learning.
4.4 Reward Function
We use the following reward function to train the hi-
erarchy of policies of our HRL agent. It aims to re-
duce discourse length at maximal task success6 us-
ing a consistent navigation strategy.
R =
?
?
?
?
?
?
?
?
?
0 for reaching the goal state
-2 for an already invoked subtask
+1 for generating instruction u con-
sistent with instruction u?1
-1 otherwise.
5We excluded rules that always choose the same value, since
they would work against our aim of generating consistent, but
variable instructions.
6Task success is addressed by that the user has to ?accept?
each instruction for a state transition.
83
The third reward that encourages consistency of in-
structions rewards a sequence of actions that allow
the last generated instruction to be classified as be-
longing to the same navigation strategy/user model
as the previously generated instruction (cf. 2.2).
5 Experiments and Results
5.1 The Simulated Environment
The simulated environment contains two kinds of
uncertainties: (1) uncertainty regarding the state of
the environment, and (2) uncertainty concerning the
user?s reaction to a system utterance. The first aspect
is represented by a set of contextual variables de-
scribing the environment, 7 and user behaviour.8 Al-
together, this leads to 115 thousand different contex-
tual configurations, which are estimated from data
(cf. Section 2.1). The uncertainty regarding the
user?s reaction to an utterance is represented by a
Naive Bayes classifier, which is passed a set of
contextual features describing the situation, mapped
with a set of semantic features describing the utter-
ance.9 From these data, the classifier specifies the
most likely user reaction (after each system act) of
perform desired action, perform undesired action, wait
and request help.10 The classifier was trained on the
annotated data and reached an accuracy of 82% in a
ten-fold cross validation.
5.2 Learnt Policies
With respect to REs, the fully-learnt policy (only
HRL) uses colour when it is discriminating, and a
distractor or micro landmark otherwise. The semi-
learnt policy (HRL with HIS) behaves as defined in
Section 4.3. The supervised learning policy (only
HIS) uses the rules learnt by the decision trees. Both
learnt policies learn to maximise task success, and
to generate consistent navigation strategies.11 The
7previous system act, route length, route status
(known/unknown), objects within vision, objects within
dialogue history, number of instructions, alignment(proportion)
8previous user reaction, user position, user wait-
ing(true/false), user type(explorative/hesitant/medium)
9navigation level(high / low), abstractness(implicit / ex-
plicit), repair(yes / no), instruction type(destination / direction /
orientation / path / straight)
10User reactions measure the system?s task success.
11They thereby also learn to adapt their semantic choices to
those most frequently made by humans.
101 102 103 104
?80
?70
?60
?50
?40
?30
?20
?10
0
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Deterministic
Semi?Learnt
Fully?Learnt
Figure 3: Comparison of fully-learnt, semi-learnt, and su-
pervised learning (deterministic) behaviours.
supervised learning policy generates successful in-
structions from the start. Note that we are not ac-
tually learning dialogue strategies, but rather gen-
eration strategies using dialogue features. There-
fore the described policies, fully-learnt, semi-learnt
and supervised-learning, exclusively guide the sys-
tem?s behaviour in the interaction with the simulated
user. An example dialogue is shown in Table 3. We
can observe that the agent starts using a low level
navigation strategy, and then switches to high level.
When the user gets confused, the system temporar-
ily switches back to low level. For referring expres-
sions, it first attempts to locate the referent by ref-
erence to a distractor, and then repairs by using a
micro landmark. The surface forms of instructions
were realised from templates, since the NLG system
so far only generates a sequence of content selection
decisions. We address surface realisation in Dethlefs
and Cuaya?huitl (2011).
We compared our semi-learnt policy against a
fully-learnt, and a supervised learning baseline. All
policies were trained for 40 thousand episodes. For
training, the step-size parameter ?, which indicates
the learning rate, was initiated with 1 and then re-
duced over time by ? = 11+t , where t is the time step.
The discount rate ?, which indicates the relevance of
future rewards in relation to immediate rewards, was
set to 0.99, and the probability of a random action ?
was 0.01. See (Sutton and Barto, 1998) for details
on these parameters.
5.3 Comparison of Policies
Figure 3 compares the rewards averaged over 10
runs. In terms of learning speed, we can see that
84
Agent Policy Action (composite in italics) Utterance
Usr request route How do I get to the trophy?
Sys ?00, ?12 root, navigation, dontRepair, orientation
?23, ?30 low level navigation, add direction+degrees First, turn 90 degrees left.
?23, ?31 low level navigation, ?straight? Then go straight ahead.
Usr perform desired action
Sys ?00 confirmation OK.
?24, ?34, ?32 high level navigation, destination, direction,
?24, ?34, ?32 addLocation Turn left at the end of the hallway.
Usr wait
Sys ?11 repair system act, change navigation strategy
?23, ?32 low level navigation, direction Turn right
?24, ?33 high level navigation, path and go down the hallway.
Usr perform desired action
Sys ?23, ?32 low level navigation, direction Now turn left.
Usr perform desired action
Sys ?00 confirmation OK.
?24, ?34 high level navigation, destination,
?24, ?34 addLocation Enter to the room in front of you.
Usr perform desired action
Sys ?10, ?21 manipulation, use colour, use distractor Press the red button beside the blue.
Usr request help
Sys ?11 repair system act, change granularity
?24, ?10 high level navigation, manipulation
?34, ?22 destination, use colour, use micro landmark Go to the red button beside the chair.
Usr perform desired action
Sys ?10 manipulation Press it.
Usr perform desired action
Sys ?00 confirmation Well done.
Table 3: Sample dialogue in the GIVE-2 scenario showing the dynamics of generation policies. See Figure 2 for the
corresponding hierarchy models, and Table 2 for the action set. See Section 5.2 for an explantation of the dialogue.
while the semi-learnt behaviour is able to follow a
near-optimal policy from the beginning, the fully-
learnt policy takes about 40 thousand episodes to
reach the same performance. In terms of simulated
task success, we see that while the supervised learn-
ing behaviour follows a good policy from the start,
it is eventually beaten by the learnt policies.
5.4 Human Evaluation Study
We asked 11 participants12 to rate altogether 132
sets of instructions, where each set contained a spa-
tial graphical scene containing a person, mapped
with one human, one learnt, and one supervised
126 female, 5 male with an age average of 26.4.
learning instruction. Instructions consisted of a nav-
igation instruction followed by a referring expres-
sion. Subjects were asked to rate instructions on a
1-5 Likert scale (where 5 is the best) for their help-
fulness on guiding the displayed person from its ori-
gin to pressing the intended button. We selected
six different scenarios for the evaluation: (a) only
one button is present, (b) two buttons are present,
the referent and a distractor of the same colour as
the referent, (c) two buttons are present, the referent
and a distractor of a different colour than the refer-
ent, (d) one micro landmark is present and one dis-
tractor of the same colour as the referent, (e) one
micro landmark is present and one distractor of a
different colour than the referent. All scenarios oc-
85
Figure 4: Example scenario of the human evaluation study.
curred twice in each evaluation sheet, their specific
instances were drawn from the GIVE-2 corpus at
random. Scenes and instructions were presented in
a randomised order. Figure 4 presents an example
evaluation scene. Finally, we asked subjects to cir-
cle the object they thought was the intended refer-
ent. Subjects rated the human instructions with an
average of 3.82, the learnt instructions with an aver-
age of 3.55, and the supervised learning instructions
with an average of 2.39. The difference between hu-
man and learnt is not significant. The difference be-
tween learnt and supervised learning is significant at
p < 0.003, and the difference between human and
supervised learning is significant at p < 0.0002. In
96% of all cases, users were able to identify the in-
tended referent.
6 Conclusion and Discussion
We have presented a combination of HRL with a hi-
erarchical IS, which was informed by prior knowl-
edge from decision trees. Such a combined frame-
work has the advantage that it allows us to system-
atically pre-specify (obvious) generation strategies,
and thereby find solutions faster, reduce computa-
tional demands, scale to complex domains, and in-
corporate expert knowledge. By applying HRL to
the remaining (non-obvious) action set, we are able
to learn a flexible, generalisable NLG policy, which
will take the best action even under uncertainty. As
an application of our approach and its generalisabil-
ity across domains, we have presented the joint op-
timisation of two separate NLG tasks, navigation in-
structions and referring expressions, in situated dia-
logue under the aspects of task success and linguis-
tic consistency. Based on an evaluation in a simu-
lated environment estimated from data, we showed
that our semi-learnt behaviour outperformed a fully-
learnt baseline in terms of learning speed, and a su-
pervised learning baseline in terms of average re-
wards. Human judges rated our instructions signif-
icantly better than the supervised learning instruc-
tions, and close to human quality. The study re-
vealed a task success rate of 96%. Future work
can transfer our approach to different applications to
confirm its benefits, and induce the agent?s reward
function from data to test in a more realistic setting.
Acknowledgments
Thanks to the German Research Foundation DFG
and the Transregional Collaborative Research Cen-
tre SFB/TR8 ?Spatial Cognition? and the EU-FP7
project ALIZ-E (ICT-248116) for partial support of
this work. Also, thanks to John Bateman for com-
ments on an earlier draft of this paper.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th conference on Computa-
tional linguistics - Volume 1, pages 42?48.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 1:1?26.
86
Peter Bohlin, Robin Cooper, Elisabet Engdahl, and
Staffan Larsson. 1999. Information states and di-
alogue move engines. In IJCAI-99 Workshop on
Knowledge and Reasoning in Practical Dialogue Sys-
tems.
Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi Oka.
2003. DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Architec-
ture. In 4th SIGDial Workshop on Discourse and Dia-
logue, pages 115?124.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395?429.
Heriberto Cuaya?huitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. thesis,
School of Informatics, University of Edinburgh.
Nina Dethlefs and Heriberto Cuaya?huitl. 2010. Hi-
erarchical Reinforcement Learning for Adaptive Text
Generation. Proceedings of INLG ?10.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hier-
archical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Genera-
tion. In Proceedings of ACL-HLT 2011, Portland, OR.
Thomas G. Dietterich. 1999. Hierarchical reinforce-
ment learning with the maxq value function decom-
position. Journal of Artificial Intelligence Research,
13:227?303.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In ACL ?01, pages 172?179.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The give-2 corpus of
giving instructions in virtual environments. In LREC.
Konstantina Garoufi and Alexander Koller. 2010. Au-
tomated planning for situated natural language gener-
ation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1573?1582, July.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Peter Heeman. 2007. Combining reinforcement learning
with information-state update rules. In Human Tech-
nology Conference (HLT), pages 268?275.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In ACL ?10,
pages 69?78.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of ACL-07.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010. The first challenge on generat-
ing instructions in virtual environments. In M. The-
une and E. Krahmer, editors, Empirical Methods
on Natural Language Generation, pages 337?361,
Berlin/Heidelberg, Germany. Springer.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
ACL-36, pages 704?710.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Nat. Lang. Eng., 6(3-
4):323?340.
Diane J. Litman, Michael S. Kearns, Satinder Singh, and
Marilyn A. Walker. 2000. Automatic optimization of
dialogue management. In Proceedings of the 18th con-
ference on Computational linguistics, pages 502?508.
William Mann and Christian M I M Matthiessen. 1983.
NIGEL: A systemic grammar for text generation.
Technical report, ISI/RR-85-105.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistc psychology of dialog. Behavioral and
Brain Sciences, 27.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL, pages 194?201.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In ACL ?10, pages 1009?1018.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105?133.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase gen-
eration for situated dialogs. In Proceedings of INLG
?06, pages 81?88.
Richard S Sutton and Andrew G Barto. 1998. Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA.
Sebastian Varges. 2003. Instance-based Natural Lan-
guage Generation. Ph.D. thesis, School of Informat-
ics, University of Edinburgh.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of INLG ?08, INLG ?08, pages 59?67.
Michael White. 2004. Reining in CCG chart realization.
In In Proc. INLG-04, pages 182?191.
Jason Williams. 2008. The best of both worlds: Uni-
fying conventional dialog systems and POMDPs. In
Interspeech, Brisbane.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. 2. edi-
tion.
87
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 7?8,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Dialogue Systems Using Online Learning: Beyond Empirical Methods ?
Heriberto Cuaya?huitl
German Research Center for Artificial Intelligence
Saarbru?cken, Germany
hecu01@dfki.de
Nina Dethlefs
Heriot-Watt University
Edinburgh, Scotland
n.s.dethlefs@hw.ac.uk
Abstract
We discuss a change of perspective for train-
ing dialogue systems, which requires a shift
from traditional empirical methods to online
learning methods. We motivate the applica-
tion of online learning, which provides the
benefit of improving the system?s behaviour
continuously often after each turn or dialogue
rather than after hundreds of dialogues. We
describe the requirements and advances for di-
alogue systems with online learning, and spec-
ulate on the future of these kinds of systems.
1 Motivation
Important progress has been made in empirical
methods for training spoken or multimodal dialogue
systems over the last decade. Nevertheless, a differ-
ent perspective has to be embraced if we want dia-
logue systems to learn on the spot while interacting
with real users. Typically, empirical methods op-
erate cyclically as follows: collect data, provide the
corresponding annotations, train a statistical or other
machine learning model, evaluate the performance
of the learned model, and if satisfactory, deploy the
trained model in a working system. The disadvan-
tage of this approach is that while data is still be-
ing collected subsequent to deployment, the system
does not optimize its behaviour anymore (cf. step-
wise learning, the solid blue line in Fig. 1). In con-
trast, dialogue systems with online learning tackle
this limitation by learning a machine learning model
?This research was funded by the EC?s FP7 programmes
under grant agreement no. ICT-248116 (ALIZ-E) and under
grant agreement no. 287615 (PARLANCE).
Tr
ai
ni
ng
 a
ct
iv
ity
Online
learning
Offline
learning
Collected dialogues
Figure 1: Learning approaches for dialogue systems.
Whilst offline learning aims for discontinuous learning,
online learning aims for continuous learning while inter-
acting with users in a real environment.
continuously often from unlabeled or minimally la-
beled data (cf. dotted red line in Fig. 1). So whilst
empirical methods train models after hundreds of di-
alogues, online learning methods refine the system
models after each user turn or each dialogue. In the
rest of the paper we discuss the requirements, ad-
vances and potential future of these kind of systems.
2 Online Learning Systems: Requirements
Several requirements arise for the development of
successful online learning systems. First of all, they
need to employ methods that are scalable for real-
world systems and the modelling of knowledge in
sufficient detail. Second, efficient learning is a pre-
requisite for learning from an ongoing interaction
without causing hesitations or pauses for the user.
Third, learnt models should satisfy a stability crite-
rion that guarantees that the learning agent?s perfor-
mance does not deteriorate over time, e.g. over the
course of a number of interactions, due to the newly
accumulated knowledge and behaviours. Fourth,
7
systems should employ a knowledge transfer ap-
proach in which they master new tasks they are con-
fronted with over their life span by transferring gen-
eral knowledge gathered in previous tasks. Fifth, on-
line learning sytems should adopt a lifelong learn-
ing approach, arguably without stopping learning.
This implies making use of large data sets, which
can be unlabeled or partially labeled due to the costs
that they imply. Finally, in the limit of updating the
learned models after every user turn, the online and
offline learning methods could be the same as long
as they meet the first three requirements above.
3 Online Learning Systems: Advances
Several authors have recognised the potential bene-
fits of online learning methods in previous work.
Thrun (1994) presents a robot for lifelong learn-
ing that learns to navigate in an unknown office en-
vironment by suggesting to transfer general purpose
knowledge across tasks. Bohus et al (2006) de-
scribe a spoken dialogue system that learns to op-
timise its non-understanding recovery strategies on-
line through interactions with human users based on
pre-trained logistic regression models. Cuaya?huitl
and Dethlefs (2011) present a dialogue system in the
navigation domain that is based on hierarchical rein-
forcement learning and Bayesian Networks and re-
learns its behaviour after each user turn, using indi-
rect feedback from the user?s performance. Gas?ic? et
al. (2011) present a spoken dialogue system based
on Gaussian Process-based Reinforcement Learn-
ing. It learns directly from binary feedback that
users assign explicitly as rewards at the end of each
dialogue and that indicate whether users were happy
or unhappy with the system?s performance. From
these previous investigations, we can observe that
online learning systems can take both explicit and/or
implicit feedback to refine their trained models.
4 Online Learning Systems: Future
While previous work has made important steps, the
problem of lifelong learning for spoken dialogue
systems is far from solved. Especially the follow-
ing challenges will need to receive attention: (a) fast
learning algorithms that can retrain behaviours after
each user turn with stable performance; and (b) scal-
able methods for optimizing multitasked behaviours
at different levels and modalities of communication.
In addition, we envision online learning systems
with the capability of transfering knowledge across
systems and domains. For example: a dialogue act
classifier, an interaction strategy, or a generation
strategy can be made transferable to similar tasks.
This could involve reasoning mechanisms to infer
what is known/unknown based on past experiences.
The idea of learning from scratch every time a new
system is constructed will thus be avoided. In this
regard, the role of the system developer in these
kinds of systems is not only to specify the system?s
tasks and learning environment, but to constrain and
bootstrap the system behaviour for faster learning.
All of these capabilities will be possible using on-
line learning with a lifelong learning perspective.
5 Tools and Data
Currently there are software tools for training mod-
els but they are more suitable for offline learning.1
Software tools for online learning remain to be de-
veloped and shared with the community. In addi-
tion, since building a dialogue system typically re-
quires a tremendous amount of effort, researchers
working on learning approaches should agree on
standards to facilitate system development. Finally,
since dialogue data is an often lacking resource in
the community, the online learning perspective may
contribute towards reducing the typical chicken and
egg problem, due to dialogue knowledge being more
readily transferable across domains, subject to on-
line adaption towards particular domains.
References
Dan Bohus, Brian Langner, Antoine Raux, Alan Black,
Maxine Eskenazi, and Alexander Rudnicky. 2006.
Online Supervised Learning of Non-Understanding
Recovery Policies. In Proc. IEEE SLT.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011. Optimiz-
ing Situated Dialogue Management in Unknown Envi-
ronments. In Proc. INTERSPEECH.
Milica Gas?ic?, Filip Jurc???c?ek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-line policy optimisation
of spoken dialogue systems via interaction with human
subjects. In Proc. IEEE ASRU.
Sebastian Thrun. 1994. A Lifelong Learning Perspective
for Mobile Robot Control. In Proc. IEEE/RSJ/GI.
1www.cs.waikato.ac.nz/ml/weka/
8
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the SIGDIAL 2013 Conference, pages 314?318,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Impact of ASR N-Best Information on Bayesian Dialogue Act Recognition
Heriberto Cuaya?huitl, Nina Dethlefs, Helen Hastie, Oliver Lemon
School of Mathematical and Computer Sciences,
Heriot-Watt University, Edinburgh, UK
{h.cuayahuitl,n.s.dethlefs,h.hastie,o.lemon}@hw.ac.uk
Abstract
A challenge in dialogue act recognition
is the mapping from noisy user inputs to
dialogue acts. In this paper we describe
an approach for re-ranking dialogue act
hypotheses based on Bayesian classifiers
that incorporate dialogue history and Au-
tomatic Speech Recognition (ASR) N-best
information. We report results based on
the Let?s Go dialogue corpora that show
(1) that including ASR N-best information
results in improved dialogue act recogni-
tion performance (+7% accuracy), and (2)
that competitive results can be obtained
from as early as the first system dialogue
act, reducing the need to wait for subse-
quent system dialogue acts.
1 Introduction
The primary challenge of a Dialogue Act Recog-
niser (DAR) is to find the correct mapping be-
tween a noisy user input and its true dialogue
act. In standard ?slot-filling? dialogue sys-
tems a dialogue act is generally represented as
DialogueActType(attribute-value pairs), see Sec-
tion 3. While a substantial body of research has
investigated different types of models and meth-
ods for dialogue act recognition in spoken dia-
logue systems (see Section 2), here we focus on
re-ranking the outputs of an existing DAR for eval-
uation purposes. In practice the re-ranker should
be part of the DAR itself. We propose to use mul-
tiple Bayesian classifiers to re-rank an initial set
of dialogue act hypotheses based on information
from the dialogue history as well as ASR N-best
lists. In particular the latter type of information
helps us to learn mappings between dialogue acts
and common mis-recognitions. We present exper-
imental results based on the Let?s Go dialogue cor-
pora which indicate that re-ranking hypotheses us-
ing ASR N-best information can lead to improved
recognition. In addition, we compare the recogni-
tion accuracy over time and find that high accuracy
can be obtained with as little context as one system
dialogue act, so that there is often no need to take
a larger context into account.
2 Related Work
Approaches to dialogue act recognition from spo-
ken input have explored a wide range of meth-
ods. (Stolcke et al, 2000) use HMMs for dialogue
modelling, where sequences of observations cor-
respond to sequences of dialogue act types. They
also explore the performance with decision trees
and neural networks and report their highest ac-
curacy at 65% on the Switchboard corpus. (Zim-
mermann et al, 2005) also use HMMs in a joint
segmentation and classification model. (Grau et
al., 2004) use a combination of Naive Bayes and
n-grams with different smoothing methods. Their
best models achieve an accuracy of 66% on En-
glish Switchboard data and 89% on a Spanish cor-
pus. (Sridhar et al, 2009; Wright et al, 1999)
both use a maximum entropy classifier with n-
grams to classify dialogue acts using prosodic fea-
tures. (Sridhar et al, 2009) report an accuracy of
up to 74% on Switchboard data and (Wright et al,
1999) report an accuracy of 69% on the DCIEM
Maptask Corpus. (Bohus and Rudnicky, 2006)
maintain an N-best list of slot values using logis-
tic regression. (Surendran and Levow, 2006) use
a combination of linear support vector machines
(SVMs) and HMMs. They report an accuracy of
65.5% on the HCRC MapTask corpus and con-
clude that SVMs are well suited for sparse text and
dense acoustic features. (Gamba?ck et al, 2011)
use SVMs within an active learning framework.
They show that while passive learning achieves an
accuracy of 77.8% on Switchboard data, the ac-
tive learner achieves up to 80.7%. (Henderson et
al., 2012) use SVMs for dialogue act recognition
from ASR word confusion networks.
314
Speech 
Recognizer
Dialogue Act 
Recognizer Dialogue Act Re-Ranker
n-best 
list
n-best 
list
n-best 
list
(e.g. Let's Go parser)
speech
scored re-scored
  Following 
Components
Figure 1: Pipeline architecture for dialogue act recognition and re-ranking component. Here, the input
is a list of dialogue acts with confidence scores, and the output is the same list of dialogue acts but with
recomputed confidence scores. A dialogue act is represented as DialogueActType(attribute-value pairs).
Several authors have presented evidence in
favour of Bayesian methods. (Keizer and op den
Akker, 2007) have shown that Bayesian DARs
can outperform baseline classifiers such as deci-
sion trees. More generally, (Ng and Jordan, 2001)
show that generative classifiers (e.g. Naive Bayes)
reach their asymptotic error faster than discrimina-
tive ones. As a consequence, generative classifiers
are less data intensive than discriminative ones.
In addition, several authors have investigated
dialogue belief tracking. While our approach
is related to belief tracking, we focus here on
spoken language understanding under uncertainty
rather than estimating user goals. (Williams, 2007;
Thomson et al, 2008) use approximate inference
to improve the scalability of Bayes nets for be-
lief tracking and (Lison, 2012) presents work on
improving their scalability through abstraction.
(Mehta et al, 2010) model user intentions through
the use of probabilistic ontology trees.
Bayes nets have also been applied to other
dialogue-related tasks, such as surface realisa-
tion within dialogue (Dethlefs and Cuaya?huitl,
2011) or multi-modal dialogue act recognition
(Cuaya?huitl and Kruijff-Korbayova?, 2011). In the
following, we will explore a dialogue act recogni-
tion technique based on multiple Bayesian classi-
fiers and show that re-ranking with ASR N-best in-
formation can improve recognition performance.
3 Re-Ranking Dialogue Acts Using
Multiple Bayesian Networks
Figure 1 shows an illustration of our dialogue act
re-ranker within a pipeline architecture. Here, pro-
cessing begins with the user?s speech being inter-
preted by a speech recogniser, which produces a
first N-best list of hypotheses. These hypotheses
are subsequently passed on and interpreted by a
dialogue act recogniser, which in our case is rep-
resented by the Let?s Go parser. The parser pro-
duces a first set of dialogue act hypotheses, based
on which our re-ranker becomes active. A full
dialogue act in our scenario consists of three el-
ements: dialogue act types, attributes (or slots),
and slot values. An example dialogue act is in-
form(from=Pittsburgh Downtown). The dialogue
act re-ranker thus receives a list of hypotheses
in the specified form (triples) from its preceding
module (a DAR or in our case the Let?s Go parser)
and its task is to generate confidence scores that
approximate true label (i.e. the dialogue act really
spoken by a user) as closely as possible.
We address this task by using multiple Bayesian
classifiers: one for classifying a dialogue act type,
one for classifying a set of slots, and the rest for
classifying slot values. The use of multiple classi-
fiers is beneficial for scalability purposes; for ex-
ample, assuming 10 dialogue act types, 10 slots,
10 values per slot, and no other dialogue con-
text results in a joint distribution of 1011 parame-
ters. Since a typical dialogue system is required to
model even larger joint distributions, our adopted
approach is to factorize them into multiple inde-
pendent Bayesian networks (with combined out-
puts). A multiple classifier system is a power-
ful solution to complex classification problems in-
volving a large set of inputs and outputs. This
approach not only decreases training time but has
also been shown to increase the performance of
classification (Tax et al, 2000).
A Bayesian Network (BN) models a joint prob-
ability distribution over a set of random variables
and their dependencies, see (Bishop, 2006) for
an introduction to BNs. Our motivation for us-
ing multiple BNs is to incorporate a fairly rich di-
alogue context in terms of what the system and
user said at lexical and semantic levels. In con-
trast, using a single BN for all slots with rich di-
alogue context faces scalability issues, especially
for slots with large numbers of domain values,
and is therefore not an attractive option. We
denote our set of Bayesian classifiers as ? =
{?dat, ?att, ..., ?val(i)}, where BN ?dat is used to
rank dialogue act types, BN ?att is used to rank
attributes, and the other BNs (?val(i)) are used to
315
rank values for each slot i. The score of a user
dialogue act (< d, a, v >) is computed as:
P (d, a, v) = 1Z
?
P (d|pad)P (a|paa)P (v|pav),
where d is a dialogue act type, a is an attribute
(or slot), v is a slot value, pax is a parent random
variable, andZ is a normalising constant. This im-
plies that the score of a dialogue act is the product
of probabilities of dialogue act type and slot-value
pairs. For dialogue acts including multiple slot-
value pairs, the product above can be extended ac-
cordingly. The best and highest ranked hypothesis
(from space H) can be obtained according to:
< d, a, v >?= arg max
<d,a,v>?H
P (d, a, v).
In the following, we describe our experimental
setting. Here, the structure and parameters of our
classifiers will be estimated from a corpus of spo-
ken dialogues, and we will use the equations above
for re-ranking user dialogue acts. Finally, we re-
port results comparing Bayesian classifiers that
make use of ASR N-best information and dialogue
context against Bayesian classifiers that make pre-
dictions based on the dialogue context alone.
4 Experiments and Results
4.1 Data
Our experiments are based on the Let?s Go corpus
(Raux et al, 2005). Let?s Go contains recorded in-
teractions between a spoken dialogue system and
human users who make enquiries about the bus
schedule in Pittsburgh. Dialogues are driven by
system-initiative and query the user sequentially
for five slots: an optional bus route, a departure
place, a destination, a desired travel date, and a
desired travel time. Each slot needs to be explic-
itly (or implicity) confirmed by the user. Our anal-
yses are based on a subset of this data set contain-
ing 779 dialogues with 7275 turns, collected in the
Summer of 2010. From these dialogues, we used
70% for training our classifiers and the rest for
testing (with 100 random splits). Briefly, this data
set contains 12 system dialogue act types1, 11 user
dialogue act types2, and 5 main slots with varia-
tions3. The number of slot values ranges between
1ack, cant help, example, expl conf, go back, hello,
impl conf, more buses, request, restart, schedule, sorry.
2affirm, bye, go back, inform, negate, next bus, prevbus,
repeat, restart, silence, tellchoices.
3date.absday, date.abmonth, date.day, date.relweek, from,
route, time.ampm, time.arriveleave, time.hour, time.minute,
time.rel, to.
*
Figure 2: Bayesian network for probabilistic rea-
soning of locations (variable ?from desc?), which
incorporates ASR N-best information in the vari-
able?from desc nbest? and dialogue history in-
formation in the remaining random variables.
102 and 103 so that the combination of all possi-
ble dialogue act types, attributes and values leads
to large amounts of triplets. While the majority
of user inputs contain one user dialogue act, the
average number of system dialogue acts per turn
is 4.2. Note that for the user dialogue act types,
we also model silence explicitly. This is often not
considered in dialogue act recognisers: since the
ASR will always try to recognise something out
of any input (even background noise), typical dia-
logue act recognisers will then try to map the ASR
output onto a semantic interpretation.
4.2 Bayesian Networks
We trained our Bayesian networks in a supervised
learning manner and used 43 discrete features (or
random variables) plus a class label (also discrete).
The feature set is described by three main subsets:
25 system-utterance-level binary features4 derived
from the system dialogue act(s) in the last turn; 17
user-utterance-level binary features5 derived from
(a) what the user heard prior to the current turn,
or (b) what keywords the system recognised in its
4System utterance features: heardAck, heardCantHelp,
heardExample, heardExplConf, heardGoBackDAT, heard-
Hello, heardImplConf, heardMoreBuses, heardRequest,
heardRestartDAT, heardSchedule, heardSorry, heardDate,
heardFrom, heardRoute, heardTime, heardTo, heardNext,
heardPrevious, heardGoBack, heardChoices, heardRestart,
heardRepeat, heardDontKnow, lastSystemDialActType.
5User utterance features: hasRoute, hasFrom, hasTo, has-
Date, hasTime, hasYes, hasNo, hasNext, hasPrevious, has-
GoBack, hasChoices, hasRestart, hasRepeat, hasDontKnow,
hasBye, hasNothing, duration in secs. (values=0,1,2,3,4,>5).
316
list of speech recognition hypotheses; and 1 word-
level non-binary feature (* nbest) corresponding
to the slot values in the ASR N-best lists.
Figure 2 shows the Bayes net corresponding to
the classifier used to rank location names. The
random variable from desc is the class label, the
random variable from desc nbest (marked with an
asterisk) incorporates slot values from the ASR
N-best lists, and the remaining variables model
dialogue history context. The structure of our
Bayesian classifiers were derived from the K2 al-
gorithm6, and their parameters were derived from
maximum likelihood estimation. In addition, we
performed probabilistic inference using the Junc-
tion tree algorithm7. Based on these data and
tools, we trained 14 Bayesian classifiers: one for
scoring dialogue act types, one for scoring at-
tributes (slots), and the rest for scoring slot values.
4.3 Experimental Results
We compared 7 different dialogue act recognisers
in terms of classification accuracy. The compar-
ison was made against gold standard data from
a human-labelled corpus. (Semi-Random) is a
recogniser choosing a random dialogue act from
the Let?s Go N-best parsing hypotheses. (Inci) is
our proposed approach considering a context of i
system dialogue acts, and (Ceiling) is a recogniser
choosing the correct dialogue act from the Let?s
Go N-best parsing hypotheses. The latter was used
as a gold standard from manual annotations, which
reflects the proportion of correct labels in the N-
best parsing hypotheses.
We also assessed the impact of ASR N-best in-
formation on probabilistic inference. To this end,
we compared Bayes nets with a focus on the ran-
dom variable ?* nbest?, which in one case con-
tains induced distributions from data and in the
other case contains an equal distribution of slot
values. Our hypothesis is that the former setting
will lead to better performance.
Figure 3 shows the classification accuracy of
our dialogue act recognisers. The first point to no-
tice is that the incorporation of ASR N-best infor-
mation makes an important difference. The per-
formance of recogniser IncK (K being the num-
ber of system dialogue acts) is 66.9% without
ASR N-best information and 73.9% with ASR N-
best information (the difference is significant8 at
6www.cs.waikato.ac.nz/ml/weka/
7www.cs.cmu.edu/?javabayes/Home/8Based on a two-sided Wilcoxon Signed-Rank test.
Semi?Random Inc0 Inc1 Inc2 Inc3 IncK Ceiling40
45
50
55
60
65
70
75
80
85
90
Dialogue Act Recogniser
Clas
sifica
tion 
Accu
racy
 (%)
 
 
Without ASR N?Best InformationWith ASR N?Best Information
Figure 3: Bayesian dialogue act recognisers show-
ing the impact of ASR N-best information.
p < 0.05). The latter represents a substantial im-
provement over the semi-random baseline (62.9%)
and Lets Go dialogue act recognizer (69%), both
significant at p < 0.05. A second point to notice is
that the differences between Inci (? i>0) recognis-
ers were not significant. We can say that the use of
one system dialogue act as context is as competi-
tive as using a larger set of system dialogue acts.
This suggests that dialogue act recognition carried
out at early stages (e.g. after the first dialogue act)
in an utterance does not degrade recognition per-
formance. The effect is possibly domain-specific
and generalisations remain to be investigated.
Generally, we were able to observe that more
than half of the errors made by the Bayesian clas-
sifiers were due to noise in the environment and
caused by the users themselves, which interfered
with ASR results. Detecting when users do not
convey dialogue acts to the system is therefore still
a standing challenge for dialogue act recognition.
5 Conclusion and Future Work
We have described a re-ranking approach for user
dialogue act recognition. Multiple Bayesian clas-
sifiers are used to rank dialogue acts from a set of
dialogue history features and ASR N-best infor-
mation. Applying our approach to the Let?s Go
data we found the following: (1) that including
ASR N-best information results in improved di-
alogue act recognition performance; and (2) that
competitive results can be obtained from as early
as the first system dialogue act, reducing the need
to include subsequent ones.
Future work includes: (a) a comparison of our
317
Bayesian classifiers with other probabilistic mod-
els and forms of training (for example by us-
ing semi-supervised learning), (b) training dia-
logue act recognisers in different (multi-modal and
multi-task) domains, and (c) dealing with random
variables that contain very large domain values.
6 Acknowledgements
This research was funded by the EC FP7 pro-
gramme under grant agreement no. 287615 (PAR-
LANCE) and no. 270019 (SPACEBOOK).
Sample Re-Ranked User Inputs
User input: ?forty six d?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(route=46a) 3.33E-4 1.9236763E-6
inform(route=46b) 1.0E-6 1.5243509E-16
inform(route=46d) 0.096107 7.030841E-4
inform(route=46k) 0.843685 4.9941495E-10
silence() NA 0
User input: ?um jefferson hills to mckeesport?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(from=mill street) 7.8E-4 3.5998527E-16
inform(from=mission street) 0.015577 3.5998527E-16
inform(from=osceola street) 0.0037 3.5998527E-16
inform(from=robinson township) 0.007292 3.5998527E-16
inform(from=sheraden station) 0.001815 3.1346254E-8
inform(from=brushton) 2.45E-4 3.5998527E-16
inform(from=jefferson) 0.128727 0.0054255757
inform(from=mckeesport) 0.31030 2.6209198E-4
silence() NA 0
References
[Bishop2006] Christopher M. Bishop. 2006. Pattern Recog-
nition and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secaucus, NJ,
USA.
[Bohus and Rudnicky2006] D. Bohus and A. Rudnicky.
2006. A k hypotheses + other? belief updating model. In
AAAI Workshop on Statistical and Empirical Approaches
to Spoken Dialogue Systems.
[Cuaya?huitl and Kruijff-Korbayova?2011] H. Cuaya?huitl and
I. Kruijff-Korbayova?. 2011. Learning human-robot di-
alogue policies combining speech and visual beliefs. In
IWSDS, pages 133?140.
[Dethlefs and Cuaya?huitl2011] Nina Dethlefs and Heriberto
Cuaya?huitl. 2011. Combining Hierarchical Reinforce-
ment Learning and Bayesian Networks for Natural Lan-
guage Generation in Situated Dialogue. In ENLG, Nancy,
France.
[Gamba?ck et al2011] Bjo?rn Gamba?ck, Fredrik Olsson, and
Oscar Ta?ckstro?m. 2011. Active Learning for Dialogue
Act Classification. In INTERSPEECH, pages 1329?1332.
[Grau et al2004] Sergio Grau, Emilio Sanchis, Maria Jose
Castro, and David Vilar. 2004. Dialogue Act Classifi-
cation Using a Bayesian Approach. In SPECOM.
[Henderson et al2012] Matthew Henderson, Milica Gasic,
Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve
Young. 2012. Discriminative spoken language under-
standing using word confusion networks. In SLT, pages
176?181.
[Keizer and op den Akker2007] Simon Keizer and Rieks
op den Akker. 2007. Dialogue Act Recognition Under
Uncertainty Using Bayesian Networks. Natural Language
Engineering, 13(4):287?316.
[Lison2012] Pierre Lison. 2012. Probabilistic dialogue mod-
els with prior domain knowledge. In SIGDIAL Confer-
ence, pages 179?188.
[Mehta et al2010] Neville Mehta, Rakesh Gupta, Antoine
Raux, Deepak Ramachandran, and Stefan Krawczyk.
2010. Probabilistic ontology trees for belief tracking in
dialog systems. In SIGDIAL Conference, pages 37?46.
[Ng and Jordan2001] Andrew Y. Ng and Michael I. Jordan.
2001. On discriminative vs. generative classifiers: A com-
parison of logistic regression and naive bayes. In NIPS,
pages 841?848.
[Raux et al2005] Antoine Raux, Brian Langner, Dan Bohus,
Alan W. Black, and Maxine Eskenazi. 2005. Let?s
go public! Taking a Spoken Dialog System to the Real
World. In INTERSPEECH, pages 885?888.
[Sridhar et al2009] Vivek Kumar Rangarajan Sridhar, Srini-
vas Bangalore, and Shrikanth Narayanan. 2009. Com-
bining Lexical, Syntactic and Prosodic Cues for Improved
Online Dialog Act Tagging. Computer Speech & Lan-
guage, 23(4):407?422.
[Stolcke et al2000] Andreas Stolcke, Klaus Ries, Noah Coc-
caro, Elizabeth Shriberg, Rebecca A. Bates, Daniel Juraf-
sky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialog Act Modeling for Auto-
matic Tagging and Recognition of Conversational Speech.
Computational Linguistics, 26(3):339?373.
[Surendran and Levow2006] Dinoj Surendran and Gina-Anne
Levow. 2006. Dialog Act Tagging with Support Vec-
tor Machines and Hidden Markov Models. In INTER-
SPEECH.
[Tax et al2000] David M. Tax, Martijn van Breukelen,
Robert P. Duin, and Josef Kittler. 2000. Combining mul-
tiple classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475?1485, September.
[Thomson et al2008] Blaise Thomson, Jost Schatzmann, and
Steve Young. 2008. Bayesian update of dialogue state for
robust dialogue systems. In ICASSP, pages 4937?4940.
[Williams2007] Jason D. Williams. 2007. Using particle fil-
ters to track dialogue state. In ASRU, pages 502?507.
[Wright et al1999] H. Wright, Massimo Poesio, and Stephen
Isard. 1999. Using high level dialogue information for
dialogue act recognition using prosodic features. In Pro-
ceedings of an ESCA Tutorial and Research Workshop on
Dialogue and Prosody, pages 139?143, Eindhoven, The
Netherlands.
[Zimmermann et al2005] Matthias Zimmermann, Yang Liu,
Elizabeth Shriberg, and Andreas Stolcke. 2005. Toward
Joint Segmentation and Classification of Dialog Acts in
Multiparty Meetings. In MLMI, pages 187?193.
318
