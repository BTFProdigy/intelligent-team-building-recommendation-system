Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237?1249,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Exact Decoding for Phrase-Based Statistical Machine Translation
Wilker Aziz
?
Marc Dymetman
?
Lucia Specia
?
?
Department of Computer Science, University of Sheffield, UK
W.Aziz@sheffield.ac.uk
L.Specia@sheffield.ac.uk
?
Xerox Research Centre Europe, Grenoble, France
Marc.Dymetman@xrce.xerox.com
Abstract
The combinatorial space of translation
derivations in phrase-based statistical ma-
chine translation is given by the intersec-
tion between a translation lattice and a tar-
get language model. We replace this in-
tractable intersection by a tractable relax-
ation which incorporates a low-order up-
perbound on the language model. Exact
optimisation is achieved through a coarse-
to-fine strategy with connections to adap-
tive rejection sampling. We perform ex-
act optimisation with unpruned language
models of order 3 to 5 and show search-
error curves for beam search and cube
pruning on standard test sets. This is the
first work to tractably tackle exact opti-
misation with language models of orders
higher than 3.
1 Introduction
In Statistical Machine Translation (SMT), the task
of producing a translation for an input string x =
?x
1
, x
2
, . . . , x
I
? is typically associated with find-
ing the best derivation d
?
compatible with the in-
put under a linear model. In this view, a derivation
is a structured output that represents a sequence of
steps that covers the input producing a translation.
Equation 1 illustrates this decoding process.
d
?
= argmax
d?D(x)
f(d) (1)
The set D(x) is the space of all derivations com-
patible with x and supported by a model of trans-
lational equivalences (Lopez, 2008). The func-
tion f(d) = ? ? H(d) is a linear parameteri-
sation of the model (Och, 2003). It assigns a
real-valued score (or weight) to every derivation
d ? D(x), where ? ? R
m
assigns a relative
importance to different aspects of the derivation
independently captured by m feature functions
H(d) = ?H
1
(d), . . . ,H
m
(d)? ? R
m
.
The fully parameterised model can be seen as
a discrete weighted set such that feature func-
tions factorise over the steps in a derivation. That
is, H
k
(d) =
?
e?d
h
k
(e), where h
k
is a (local)
feature function that assesses steps independently
and d = ?e
1
, e
2
, . . . , e
l
? is a sequence of l steps.
Under this assumption, each step is assigned the
weightw(e) = ? ??h
1
(e), h
2
(e), . . . , h
m
(e)?. The
setD is typically finite, however, it contains a very
large number of structures ? exponential (or even
factorial, see ?2) with the size of x ? making
exhaustive enumeration prohibitively slow. Only
in very restricted cases combinatorial optimisation
techniques are directly applicable (Tillmann et al.,
1997; Och et al., 2001), thus it is common to resort
to heuristic techniques in order to find an approxi-
mation to d
?
(Koehn et al., 2003; Chiang, 2007).
Evaluation exercises indicate that approximate
search algorithms work well in practice (Bojar
et al., 2013). The most popular algorithms pro-
vide solutions with unbounded error, thus pre-
cisely quantifying their performance requires the
development of a tractable exact decoder. To
date, most attempts were limited to short sentences
and/or somewhat toy models trained with artifi-
cially small datasets (Germann et al., 2001; Igle-
sias et al., 2009; Aziz et al., 2013). Other work
has employed less common approximations to the
model reducing its search space complexity (Ku-
mar et al., 2006; Chang and Collins, 2011; Rush
and Collins, 2011). These do not answer whether
or not current decoding algorithms perform well at
real translation tasks with state-of-the-art models.
We propose an exact decoder for phrase-based
SMT based on a coarse-to-fine search strategy
(Dymetman et al., 2012). In a nutshell, we re-
lax the decoding problem with respect to the Lan-
guage Model (LM) component. This coarse view
is incrementally refined based on evidence col-
1237
lected via maximisation. A refinement increases
the complexity of the model only slightly, hence
dynamic programming remains feasible through-
out the search until convergence. We test our de-
coding strategy with realistic models using stan-
dard data sets. We also contribute with optimum
derivations which can be used to assess future im-
provements to approximate decoders. In the re-
maining sections we present the general model
(?2), survey contributions to exact optimisation
(?3), formalise our novel approach (?4), present
experiments (?5) and conclude (?6).
2 Phrase-based SMT
In phrase-based SMT (Koehn et al., 2003), the
building blocks of translation are pairs of phrases
(or biphrases). A translation derivation d is an
ordered sequence of non-overlapping biphrases
which covers the input text in arbitrary order gen-
erating the output from left to right.
1
f(d) = ?(y) +
l
?
i=1
?(e
i
) +
l?1
?
i=1
?(e
i
, e
i?1
) (2)
Equation 2 illustrates a standard phrase-based
model (Koehn et al., 2003): ? is a weighted tar-
get n-gram LM component, where y is the yield
of d; ? is a linear combination of features that
decompose over phrase pairs directly (e.g. back-
ward and forward translation probabilities, lexi-
cal smoothing, and word and phrase penalties);
and ? is an unlexicalised penalty on the num-
ber of skipped input words between two adjacent
biphrases. The weighted logic program in Figure
1 specifies the fully parameterised weighted set of
solutions, which we denote ?D(x), f(d)?.
2
A weighted logic program starts from its ax-
ioms and follows exhaustively deducing new items
by combination of existing ones and no deduction
happens twice. In Figure 1, a nonteminal item
summarises partial derivation (or hypotheses). It is
denoted by [C, r, ?] (also known as carry), where:
C is a coverage vector, necessary to impose the
non-overlapping constraint; r is the rightmost po-
sition most recently covered, necessary for the
computation of ?; and ? is the last n ? 1 words
1
Preventing phrases from overlapping requires an expo-
nential number of constraints (the powerset of x) rendering
the problem NP-complete (Knight, 1999).
2
Weighted logics have been extensively used to describe
weighted sets (Lopez, 2009), operations over weighted sets
(Chiang, 2007; Dyer and Resnik, 2010), and a variety of dy-
namic programming algorithms (Cohen et al., 2008).
ITEM
[
{0, 1}
I
, [0, I + 1],?
n?1
]
GOAL
[
1
I
, I + 1, EOS
]
AXIOM
?BOS? BOS?
[0
I
, 0, BOS] : ?(BOS)
EXPAND
[
C, r, y
j?1
j?n+1
] ?
x
i
?
i
?
r
??? y
j
?
j
?
[
C
?
, i
?
, y
j
?
j
?
?n+2
]
: w
?
i
?
k=i
c
k
=
?
0
where c
?
k
= c
k
if k < i or k > i
?
else
?
1
w = ?
r
? ?(r, i)? ?(y
j
?
j
|y
j?1
j?n+1
)
ACCEPT [
1
I
, r, ?
]
[1
I
, I + 1, EOS] : ?(r, I + 1)? ?(EOS|?)
r ? I
Figure 1: Specification for the weighted set of
translation derivations in phrase-based SMT with
unconstrained reordering.
in the yield, necessary for the LM component. The
program expands partial derivations by concatena-
tion with a translation rule
?
x
i
?
i
?
r
??? y
j
?
j
?
, that is, an
instantiated biphrase which covers the span x
i
?
i
and
yields y
j
?
j
with weight ?
r
. The side condition im-
poses the non-overlapping constraint (c
k
is the kth
bit in C). The antecedents are used to compute the
weight of the deduction, and the carry is updated
in the consequent (item below the horizontal line).
Finally, the rule ACCEPT incorporates the end-of-
sentence boundary to complete items.
3
It is perhaps illustrative to understand the set of
weighted translation derivations as the intersection
between two components. One that is only locally
parameterised and contains all translation deriva-
tions (a translation lattice or forest), and one that
re-ranks the first as a function of the interactions
between translation steps. The model of transla-
tional equivalences parameterised only with ? is
an instance of the former. An n-gram LM compo-
nent is an instance of the latter.
2.1 Hypergraphs
A backward-hypergraph, or simply hypergraph,
is a generalisation of a graph where edges have
multiple origins and one destination (Gallo et al.,
1993). They can represent both finite-state and
context-free weighted sets and they have been
widely used in SMT (Huang and Chiang, 2007).
A hypergraph is defined by a set of nodes (or ver-
3
Figure 1 can be seen as a specification for a weighted
acyclic finite-state automaton whose states are indexed by
[l, C, r] and transitions are labelled with biphrases. However,
for generality of representation, we opt for using acyclic hy-
pergraphs instead of automata (see ?2.1).
1238
tices) V and a weighted set of edges ?E,w?. An
edge e connects a sequence of nodes in its tail
t[e] ? V
?
under a head node h[e] ? V and has
weight w(e). A node v is a terminal node if it
has no incoming edges, otherwise it is a nontermi-
nal node. The node that has no outgoing edges,
is called root, with no loss of generality we can
assume hypergraphs to have a single root node.
Hypergraphs can be seen as instantiated logic
programs. In this view, an item is a template
for the creation of nodes, and a weighted deduc-
tion rule is a template for edges. The tail of
an edge is the sequence of nodes associated with
the antecedents, and the head is the node associ-
ated with the consequent. Even though the space
of weighted derivations in phrase-based SMT is
finite-state, using a hypergraph as opposed to a
finite-state automaton makes it natural to encode
multi-word phrases using tails. We opt for rep-
resenting the target side of the biphrase as a se-
quence of terminals nodes, each of which repre-
sents a target word.
3 Related Work
3.1 Beam filling algorithms
Beam search (Koehn et al., 2003) and cube prun-
ing (Chiang, 2007) are examples of state-of-the-art
approximate search algorithms. They approximate
the intersection between the translation forest and
the language model by expanding a limited beam
of hypotheses from each nonterminal node. Hy-
potheses are organised in priority queues accord-
ing to common traits and a fast-to-compute heuris-
tic view of outside weights (cheapest way to com-
plete a hypothesis) puts them to compete at a fairer
level. Beam search exhausts a node?s possible ex-
pansions, scores them, and discards all but the k
highest-scoring ones. This process is wasteful in
that k is typically much smaller than the number of
possible expansions. Cube pruning employs a pri-
ority queue at beam filling and computes k high-
scoring expansions directly in near best-first order.
The parameter k is known as beam size and it con-
trols the time-accuracy trade-off of the algorithm.
Heafield et al. (2013a) move away from us-
ing the language model as a black-box and build
a more involved beam filling algorithm. Even
though they target approximate search, some of
their ideas have interesting connections to ours
(see ?4). They group hypotheses that share partial
language model state (Li and Khudanpur, 2008)
reasoning over multiple hypotheses at once. They
fill a beam in best-first order by iteratively vis-
iting groups using a priority queue: if the top
group contains a single hypothesis, the hypothesis
is added to the beam, otherwise the group is parti-
tioned and the parts are pushed back to the queue.
More recently, Heafield et al. (2014) applied their
beam filling algorithm to phrase-based decoding.
3.2 Exact optimisation
Exact optimisation for monotone translation has
been done using A
?
search (Tillmann et al., 1997)
and finite-state operations (Kumar et al., 2006).
Och et al. (2001) design near-admissible heuris-
tics for A
?
and decode very short sentences (6-
14 words) for a word-based model (Brown et al.,
1993) with a maximum distortion strategy (d = 3).
Zaslavskiy et al. (2009) frame phrase-based de-
coding as an instance of a generalised Travel-
ling Salesman Problem (TSP) and rely on ro-
bust solvers to perform decoding. In this view,
a salesman graph encodes the translation options,
with each node representing a biphrase. Non-
overlapping constraints are imposed by the TSP
solver, rather than encoded directly in the sales-
man graph. They decode only short sentences
(17 words on average) using a 2-gram LM due to
salesman graphs growing too large.
4
Chang and Collins (2011) relax phrase-based
models w.r.t. the non-overlapping constraints,
which are replaced by soft penalties through La-
grangian multipliers, and intersect the LM com-
ponent exhaustively. They do employ a maximum
distortion limit (d = 4), thus the problem they
tackle is no longer NP-complete. Rush and Collins
(2011) relax a hierarchical phrase-based model
(Chiang, 2005)
5
w.r.t. the LM component. The
translation forest and the language model trade
their weights (through Lagrangian multipliers) so
as to ensure agreement on what each component
believes to be the maximum. In both approaches,
when the dual converges to a compliant solution,
the solution is guaranteed to be optimal. Other-
4
Exact decoding had been similarly addressed with Inte-
ger Linear Programming (ILP) in the context of word-based
models for very short sentences using a 2-gram LM (Ger-
mann et al., 2001). Riedel and Clarke (2009) revisit that for-
mulation and employ a cutting-plane algorithm (Dantzig et
al., 1954) reaching 30 words.
5
In hierarchical translation, reordering is governed by a
synchronous context-free grammar and the underlying prob-
lem is no longer NP-complete. Exact decoding remains in-
feasible because the intersection between the translation for-
est and the target LM is prohibitively slow.
1239
wise, a subset of the constraints is explicitly added
and the dual optimisation is repeated. They handle
sentences above average length, however, resort-
ing to compact rulesets (10 translation options per
input segment) and using only 3-gram LMs.
In the context of hierarchical models, Aziz et
al. (2013) work with unpruned forests using up-
perbounds. Their approach is the closest to ours.
They also employ a coarse-to-fine strategy with
the OS
?
framework (Dymetman et al., 2012), and
investigate unbiased sampling in addition to op-
timisation. However, they start from a coarser
upperbound with unigram probabilities, and their
refinement strategies are based on exhaustive in-
tersections with small n-gram matching automata.
These refinements make forests grow unmanage-
able too quickly. Because of that, they only deal
with very short sentences (up to 10 words) and
even then decoding is very slow. We design bet-
ter upperbounds and a more efficient refinement
strategy. Moreover, we decode long sentences us-
ing language models of order 3 to 5.
6
4 Approach
4.1 Exact optimisation with OS
?
Dymetman et al. (2012) introduced OS
?
, a unified
view of optimisation and sampling which can be
seen as a cross between adaptive rejection sam-
pling (Robert and Casella, 2004) and A
?
optimisa-
tion (Hart et al., 1968). In this framework, a com-
plex goal distribution is upperbounded by a sim-
pler proposal distribution for which optimisation
(and sampling) is feasible. This proposal is incre-
mentally refined to be closer to the goal until the
maximum is found (or until the sampling perfor-
mance exceeds a certain level).
Figure 2 illustrates exact optimisation with OS
?
.
Suppose f is a complex target goal distribution,
such that we cannot optimise f , but we can as-
sess f(d) for a given d. Let g
(0)
be an upper-
bound to f , i.e., g
(0)
(d) ? f(d) for all d ? D(x).
Moreover, suppose that g
(0)
is simple enough to
be optimised efficiently. The algorithm proceeds
by solving d
0
= argmax
d
g
(0)
(d) and comput-
6
The intuition that a full intersection is wasteful is also
present in (Petrov et al., 2008) in the context of approximate
search. They start from a coarse distribution based on au-
tomatic word clustering which is refined in multiple passes.
At each pass, hypotheses are pruned a posteriori on the basis
of their marginal probabilities, and word clusters are further
split. We work with upperbounds, rather than word clusters,
with unpruned distributions, and perform exact optimisation.
f
g
(0)
d
0
D(x)
g
(1)
d
1
d
*
f
1
f
0
f
*
Figure 2: Sequence of incrementally refined up-
perbound proposals.
ing the quantity r
0
=
f(d
0
)
/g
(0)
(d
0
). If r
0
were
sufficiently close to 1, then g
(0)
(d
0
) would be
sufficiently close to f(d
0
) and we would have
found the optimum. However, in the illustration
g
(0)
(d
0
)  f(d
0
), thus r
0
 1. At this point
the algorithm has concrete evidence to motivate
a refinement of g
(0)
that can lower its maximum,
bringing it closer to f
?
= max
d
f(d) at the cost
of some small increase in complexity. The re-
fined proposal must remain an upperbound to f .
To continue with the illustration, suppose g
(1)
is
obtained. The process is repeated until eventually
g
(t)
(d
t
) = f(d
t
), where d
t
= argmax
d
g
(t)
(d),
for some finite t. At which point d
t
is the opti-
mum derivation d
?
from f and the sequence of
upperbounds provides a proof of optimality.
7
4.2 Model
We work with phrase-based models in a standard
parameterisation (Equation 2). However, to avoid
having to deal with NP-completeness, we con-
strain reordering to happen only within a limited
window given by a notion of distortion limit. We
require that the last source word covered by any
biphrase must be within d words from the leftmost
uncovered source position (Lopez, 2009). This is
a widely used strategy and it is in use in the Moses
toolkit (Koehn et al., 2007).
8
Nevertheless, the problem of finding the best
7
If d is a maximum from g and g(d) = f(d), then it is
easy to show by contradiction that d is the actual maximum
from f : if there existed d
?
such that f(d
?
) > f(d), then it
follows that g(d
?
) ? f(d
?
) > f(d) = g(d), and hence d
would not be a maximum for g.
8
A distortion limit characterises a form of pruning that
acts directly in the generative capacity of the model leading
to induction errors (Auli et al., 2009). Limiting reordering
like that lowers complexity to a polynomial function of I and
an exponential function of the distortion limit.
1240
derivation under the model remains impractica-
ble due to nonlocal parameterisation (namely,
the n-gram LM component). The weighted set
?D(x), f(d)?, which represents the objective, is
a complex hypergraph which we cannot afford
to construct. We propose to construct instead a
simpler hypergraph for which optimisation by dy-
namic programming is feasible. This proxy rep-
resents the weighted set
?
D(x), g
(0)
(d)
?
, where
g
(0)
(d) ? f(d) for every d ? D(x). Note that
this proposal contains exactly the same translation
options as in the original decoding problem. The
simplification happens only with respect to the pa-
rameterisation. Instead of intersecting the com-
plete n-gram LM distribution explicitly, we im-
plicitly intersect a simpler upperbound view of it,
where by simpler we mean lower-order.
g
(0)
(d) =
l?
i=1
?(y[e
i
]) +
l?
i=1
?(e
i
) +
l?1?
i=1
?(e
i
, e
i?1
) (3)
Equation 3 shows the model we use as a proxy
to perform exact optimisation over f . In compar-
ison to Equation 2, the term
?
l
i=1
?(y[e
i
]) replaces
?(y) = ?
?
p
LM
(y). While ? weights the yield y
taking into account all n-grams (including those
crossing the boundaries of phrases), ? weights
edges in isolation. Particularly, ?(y[e
i
]) =
?
?
q
LM
(y[e
i
]), where y[e
i
] returns the sequence of
target words (a target phrase) associated with the
edge, and q
LM
(?) is an upperbound on the true LM
probability p
LM
(?) (see ?4.3). It is obvious from
Equation 3 that our proxy model is much simpler
than the original ? the only form of nonlocal pa-
rameterisation left is the distortion penalty, which
is simple enough to represent exactly.
The program in Figure 3 illustrates the con-
struction of
?
D(x), g
(0)
(d)
?
. A nonterminal item
[l, C, r] stores: the leftmost uncovered position l
and a truncated coverage vector C (together they
track d input positions); and the rightmost position
r most recently translated (necessary for the com-
putation of the distortion penalty). Observe how
nonterminal items do not store the LM state.
9
The
rule ADJACENT expands derivations by concate-
nation with a biphrase
?
x
i
?
i
? y
j
?
j
?
starting at the
leftmost uncovered position i = l. That causes
the coverage window to move ahead to the next
leftmost uncovered position: l
?
= l + ?
1
(C) + 1,
9
Drawing a parallel to (Heafield et al., 2013a), a nontermi-
nal node in our hypergraph groups derivations while exposing
only an empty LM state.
ITEM
[
[1, I + 1], {0, 1}
d?1
, [0, I + 1]
]
GOAL [I, ?, I + 1]
AXIOMS
?BOS? BOS?
[1, 0
d?1
, 0] : ?(BOS)
ADJACENT
[l, C, r]
?
x
i
?
i
?
r
??? y
j
?
j
?
[l
?
, C
?
, i
?
] : ?
r
? ?(r, i
?
)? ?(y
j
?
j
)
i = l
?
i
?
?l
k=i?l
c
k
=
?
0
where l
?
= l + ?
1
(C) + 1
C
?
 ?
1
(C) + 1
NON-ADJACENT
[l, C, r]
?
x
i
?
i
?
r
??? y
j
?
j
?
[l, C
?
, i
?
] : ?
r
? ?(r, i
?
)? ?(y
j
?
j
)
i > l
?
i
?
?l
k=i?l
c
k
=
?
0
|r ? i+ 1| ? d
|i
?
? l + 1| ? d
where c
?
k
= c
k
if k < i? l or k > i
?
? l else
?
1
ACCEPT
[I + 1, C, r]
[I + 1, ?, I + 1] : ?(r, I + 1)? ?(EOS)
r ? I
Figure 3: Specification of the initial proposal hy-
pergraph. This program allows the same reorder-
ings as (Lopez, 2009) (see logic WLd), however,
it does not store LM state information and it uses
the upperbound LM distribution ?(?).
where ?
1
(C) returns the number of leading 1s in
C, and C
?
 ?
1
(C) + 1 represents a left-shift.
The rule NON-ADJACENT handles the remaining
cases i > l provided that the expansion skips at
most d input words |r ? i+ 1| ? d. In the conse-
quent, the window C is simply updated to record
the translation of the input span i..i
?
. In the non-
adjacent case, a gap constraint imposes that the
resulting item will require skipping no more than
d positions before the leftmost uncovered word is
translated |i
?
? l + 1| ? d.
10
Finally, note that
deductions incorporate the weighted upperbound
?(?), rather than the true LM component ?(?).
11
4.3 LM upperbound and Max-ARPA
Following Carter et al. (2012) we compute an
upperbound on n-gram conditional probabilities
by precomputing max-backoff weights stored in
a ?Max-ARPA? table, an extension of the ARPA
format (Jurafsky and Martin, 2000).
A standard ARPA table T stores entries
10
This constraint prevents items from becoming dead-ends
where incomplete derivations require a reordering step larger
than d. This is known to prevent many search errors in beam
search (Chang and Collins, 2011).
11
Unlike Aziz et al. (2013), rather than unigrams only, we
score all n-grams within a translation rule (including incom-
plete ones).
1241
?Z,Z.p,Z.b?, where Z is an n-gram equal to the
concatenation Pz of a prefix P with a word z, Z.p
is the conditional probability p(z|P), and Z.b is
a so-called ?backoff? weight associated with Z.
The conditional probability of an arbitrary n-gram
p(z|P), whether listed or not, can then be recov-
ered from T by the simple recursive procedure
shown in Equation 4, where tail deletes the first
word of the string P.
p(z|P) =
?
?
?
p(z| tail(P)) Pz 6? T and P 6? T
p(z| tail(P))? P.b Pz 6? T and P ? T
Pz.p Pz ? T
(4)
The optimistic version (or ?max-backoff?) q of
p is defined as q(z|P) ? max
H
p(z|HP), where
H varies over all possible contexts extending the
prefix P to the left. The Max-ARPA table allows to
compute q(z|P) for arbitrary values of z and P. It
is constructed on the basis of the ARPA table T by
adding two columns to T : a column Z.q that stores
the value q(z|P) and a column Z.m that stores an
optimistic version of the backoff weight.
These columns are computed offline in two
passes by first sorting T in descending order of
n-gram length.
12
In the first pass (Algorithm 1),
we compute for every entry in the table an opti-
mistic backoff weight m. In the second pass (Algo-
rithm 2), we compute for every entry an optimistic
conditional probability q by maximising over 1-
word history extensions (whose .q fields are al-
ready known due to the sorting of T ).
The following Theorem holds (see proof be-
low): For an arbitrary n-gram Z = Pz, the prob-
ability q(z|P) can be recovered through the proce-
dure shown in Equation 5.
q(z|P) =
?
?
?
p(z|P) Pz 6? T and P 6? T
p(z|P)? P.m Pz 6? T and P ? T
Pz.q Pz ? T
(5)
Note that, if Z is listed in the table, we return its
upperbound probability q directly. When the n-
gram is unknown, but its prefix is known, we take
into account the optimistic backoff weight m of the
prefix. On the other hand, if both the n-gram and
its prefix are unknown, then no additional context
could change the score of the n-gram, in which
case q(z|P) = p(z|P).
In the sequel, we will need the following defini-
tions. Suppose ? = y
J
I
is a substring of y = y
M
1
.
12
If an n-gram is listed in T , then all its substrings must
also be listed. Certain pruning strategies may corrupt this
property, in which case we make missing substrings explicit.
Then p
LM
(?) ?
?
J
k=I
p(y
k
|y
k?1
1
) is the contribu-
tion of ? to the true LM score of y. We then ob-
tain an upperbound q
LM
(?) to this contribution by
defining q
LM
(?) ? q(y
I
|)
?
J
k=I+1
q(y
k
|y
k?1
I
).
Proof of Theorem. Let us first suppose that the length
of P is strictly larger than the order n of the language
model. Then for any H, p(z|HP) = p(z|P); this is be-
cause HP /? T and P /? T , along with all intermedi-
ary strings, hence, by (4), p(z|HP) = p(z| tail(HP)) =
p(z| tail(tail(HP))) = . . . = p(z|P). Hence q(z|P) =
p(z|P), and, because Pz /? T and P /? T , the theorem
is satisfied in this case.
Having established the theorem for |P| > n, we
now assume that it is true for |P| > m and prove by
induction that it is true for |P| = m. We use the
fact that, by the definition of q, we have q(z|P) =
max
x??
q(z|xP). We have three cases to consider.
First, suppose that Pz /? T and P /? T . Then
xPz /? T and xP /? T , hence by induction q(z|xP) =
p(z|xP) = p(z|P) for any x, therefore q(z|P) =
p(z|P). We have thus proven the first case.
Second, suppose that Pz /? T and P ? T . Then, for
any x, we have xPz /? T , and:
q(z|P) = max
x??
q(z|xP)
= max( max
x??, xP/?T
q(z|xP), max
x??, xP?T
q(z|xP)).
For xP /? T , by induction, q(z|xP) = p(z|xP) =
p(z|P), and therefore max
x??, xP/?T
q(z|xP) =
p(z|P). For xP ? T , we have q(z|xP) = p(z|xP) ?
xP.m = p(z|P)? xP.b? xP.m. Thus, we have:
max
x??, xP?T
q(z|xP) = p(z|P)? max
x??, xP?T
xP.b?xP.m.
But now, because of lines 3 and 4 of Algorithm
1, P.m = max
x??, xP?T
xP.b ? xP.m, hence
max
x??, xP?T
q(z|xP) = p(z|P) ? P.m. Therefore,
q(z|P) = max(p(z|P), p(z|P)?P.m) = p(z|P)?P.m,
where we have used the fact that P.m ? 1 due to line 1
of Algorithm 1. We have thus proven the second case.
Finally, suppose that Pz ? T . Then, again,
q(z|P) = max
x??
q(z|xP)
= max(
max
x??, xPz/?T, xP/?T
q(z|xP),
max
x??, xPz/?T, xP?T
q(z|xP),
max
x??, xPz?T
q(z|xP) ).
For xPz /? T, xP /? T , we have q(z|xP) =
p(z|xP) = p(z|P) = Pz.p, where the last equality is
due to the fact that Pz ? T . For xPz /? T, xP ? T , we
have q(z|xP) = p(z|xP)? xP.m = p(z|P)? xP.b?
xP.m = Pz.p? xP.b? xP.m. For xPz ? T , we have
q(z|xP) = xPz.q. Overall, we thus have:
q(z|P) = max( Pz.p,
max
x??, xPz/?T, xP?T
Pz.p? xP.b? xP.m,
max
x??, xPz?T
xPz.q ).
Note that xPz ? T ? xP ? T , and then one can
check that Algorithm 2 exactly computes Pz.q as this
maximum over three maxima, hence Pz.q = q(z|P).
1242
Algorithm 1 Max-ARPA: first pass
1: for Z ? T do
2: Z.m? 1
3: for x ? ? s.t xZ ? T do
4: Z.m? max(Z.m,xZ.b? xZ.m)
5: end for
6: end for
Algorithm 2 Max-ARPA: second pass
1: for Z = Pz ? T do
2: Pz.q? Pz.p
3: for x ? ? s.t xP ? T do
4: if xPz ? T then
5: Pz.q? max(Pz.q,xPz.q)
6: else
7: Pz.q? max(Pz.q,Pz.p? xP.b? xP.m)
8: end if
9: end for
10: end for
4.4 Search
The search for the true optimum derivation is il-
lustrated in Algorithm 3. The algorithm takes as
input the initial proposal distribution g
(0)
(d) (see
?4.2, Figure 3) and a maximum error  (which we
set to a small constant 0.001 rather than zero, to
avoid problems with floating point precision). In
line 3 we find the optimum derivation d in g
(0)
(see ?4.5). The variable g
?
stores the maximum
score w.r.t. the current proposal, while the vari-
able f
?
stores the maximum score observed thus
far w.r.t. the true model (note that in line 5 we as-
sess the true score of d). In line 6 we start a loop
that runs until the error falls below . This error is
the difference (in log-domain) between the proxy
maximum g
?
and the best true score observed thus
far f
?
.
13
In line 7, we refine the current proposal
using evidence from d (see ?4.6). In line 9, we
update the maximum derivation searching through
the refined proposal. In line 11, we keep track of
the best score so far according to the true model,
in order to compute the updated gap in line 6.
4.5 Dynamic Programming
Finding the best derivation in a proposal hyper-
graph is straightforward with standard dynamic
programming. We can compute inside weights
in the max-times semiring in time proportional
13
Because g
(t)
upperbounds f everywhere, in optimisation
we have a guarantee that the maximum of f must lie in the
interval [f
?
, g
?
) (see Figure 2) and the quantity g
?
? f
?
is
an upperbound on the error that we incur if we early-stop the
search at any given time t. This bound provides a principled
criterion in trading accuracy for performance (a direction that
we leave for future work). Note that most algorithms for ap-
proximate search produce solutions with unbounded error.
Algorithm 3 Exact decoding
1: function OPTIMISE(g
(0)
, )
2: t? 0 . step
3: d? argmax
d
g
(t)
(d)
4: g
?
? g
(t)
(d)
5: f
?
? f(d)
6: while (q
?
? f
?
? ) do .  is the maximum error
7: g
(t+1)
? refine(g
(t)
,d) . update proposal
8: t? t+ 1
9: d? argmax
d
g
(t)
(d) . update argmax
10: g
?
? g
(t)
(d)
11: f
?
? max(f
?
, f(d)) . update ?best so far?
12: end while
13: return g
(t)
, d
14: end function
to O(|V | + |E|) (Goodman, 1999). Once inside
weights have been computed, finding the Viterbi-
derivation starting from the root is straightforward.
A simple, though important, optimisation con-
cerns the computation of inside weights. The in-
side algorithm (Baker, 1979) requires a bottom-up
traverse of the nodes in V . To do that, we topolog-
ically sort the nodes in V at time t = 0 and main-
tain a sorted list of nodes as we refine g throughout
the search ? thus avoiding having to recompute the
partial ordering of the nodes at every iteration.
4.6 Refinement
If a derivation d = argmax
d
g
(t)
(d) is such that
g
(t)
(d) f(d), there must be in d at least one n-
gram whose upperbound LM weight is far above
its true LM weight. We then lower g
(t)
locally by
refining only nonterminal nodes that participate in
d. Nonterminal nodes are refined by having their
LM states extended one word at a time.
14
For an illustration, assume we are perform-
ing optimisation with a bigram LM. Suppose
that in the first iteration a derivation d
0
=
argmax
d
g
(0)
(d) is obtained. Now consider an
edge in d
0
[l, C, r, ] ?y
1
w
?? [l
0
, C
0
, r
0
, ]
where an empty LM state is made explicit (with an
empty string ) and ?y
1
represents a target phrase.
We refine the edge?s head [l
0
, C
0
, r
0
, ] by creating
a node based on it, however, with an extended LM
state, i.e., [l
0
, C
0
, r
0
, y
1
]. This motivates a split
of the set of incoming edges to the original node,
such that, if the target projection of an incoming
14
The refinement operation is a special case of a general
finite-state intersection. However, keeping its effect local to
derivations going through a specific node is non-trivial using
the general mechanism and justifies a tailored operation.
1243
edge ends in y
1
, that edge is reconnected to the
new node as below.
[l, C, r, ] ?y
1
w
?? [l
0
, C
0
, r
0
, y
1
]
The outgoing edges from the new node are
reweighted copies of those leaving the original
node. That is, outgoing edges such as
[l
0
, C
0
, r
0
, ] y
2
?
w
??
[
l
?
, C
?
, r
?
, ?
?
]
motivate edges such as
[l
0
, C
0
, r
0
, y
1
] y
2
?
w?w
?
????
[
l
?
, C
?
, r
?
, ?
?
]
where w
?
= ?
?
q
LM
(y
1
y
2
)
/q
LM
(y
2
) is a change in LM
probability due to an extended context.
Figure 4 is the logic program that constructs the
refined hypergraph in the general case. In com-
parison to Figure 3, items are now extended to
store an LM state. The input is the original hy-
pergraph G = ?V,E? and a node v
0
? V to be
refined by left-extending its LM state ?
0
with the
word y. In the program,
?
u?
w
?? v
?
with u,v ? V
and ? ? ?
?
represents an edge in E. An item
[l, C, r, ?]
v
(annotated with a state v ? V ) rep-
resents a node (in the refined hypergraph) whose
signature is equivalent to v (in the input hyper-
graph). We start with AXIOMS by copying the
nodes in G. In COPY, edges from G are copied
unless they are headed by v
0
and their target pro-
jections end in y?
0
(the extended context). Such
edges are processed by REFINE, which instead of
copying them, creates new ones headed by a re-
fined version of v
0
. Finally, REWEIGHT contin-
ues from the refined node with reweighted copies
of the edges leaving v
0
. The weight update repre-
sents a change in LM probability (w.r.t. the upper-
bound distribution) due to an extended context.
5 Experiments
We used the dataset made available by the Work-
shop on Statistical Machine Translation (WMT)
(Bojar et al., 2013) to train a German-English
phrase-based system using the Moses toolkit
(Koehn et al., 2007) in a standard setup. For
phrase extraction, we used both Europarl (Koehn,
2005) and News Commentaries (NC) totalling
about 2.2M sentences.
15
For language modelling,
in addition to the monolingual parts of Europarl
15
Pre-processing: tokenisation, truecasing and automatic
compound-splitting (German only). Following Durrani et al.
(2013), we set the maximum phrase length to 5.
INPUT
G = ?V,E?
v
0
= [l
0
, C
0
, r
0
, ?
0
] ? V where ?
0
? ?
?
y ? ?
ITEM [l, C, r, ? ? ?
?
]
AXIOMS
[l, C, r, ?]
v
v ? V
COPY
[l, C, r, ?]
u
?
u?
w
?? v
?
[l
?
, C
?
, r
?
, ?
?
]
v
: w
v 6= v
0
? ?? 6= ?y?
0
?, ?
?
, ?, ? ? ?
?
REFINE
[l, C,R, ?]
u
?
u?
w
?? v
0
?
[l
0
, C
0
, r
0
, y?
0
] : w
?? = ?y?
0
?, ?, ? ? ?
?
REWEIGHT
[l
0
, C
0
, r
0
, y?
0
]
?
v
0
?
w
?? v
?
[l, C, r, ?]
v
: w ? w
?
?, ? ? ?
?
where w
?
= ?
?
q
LM
(y?
0
)
q
LM
(?
0
)
Figure 4: Local intersection via LM right state re-
finement. The input is a hypergraph G = ?V,E?,
a node v
0
? V singly identified by its carry
[l
0
, C
0
, r
0
, ?
0
] and a left-extension y for its LM
context ?
0
. The program copies most of the edges?
u?
w
?? v
?
? E. If a derivation goes through v
0
and the string under v
0
ends in y?
0
, the program
refines and reweights it.
and NC, we added News-2013 totalling about 25M
sentences. We performed language model interpo-
lation and batch-mira tuning (Cherry and Foster,
2012) using newstest2010 (2,849 sentence pairs).
For tuning we used cube pruning with a large beam
size (k = 5000) and a distortion limit d = 4. Un-
pruned language models were trained using lmplz
(Heafield et al., 2013b) which employs modified
Kneser-Ney smoothing (Kneser and Ney, 1995).
We report results on newstest2012.
Our exact decoder produces optimal translation
derivations for all the 3,003 sentences in the test
set. Table 1 summarises the performance of our
novel decoder for language models of order n = 3
to n = 5. For 3-gram LMs we also varied the dis-
tortion limit d (from 4 to 6). We report the average
time (in seconds) to build the initial proposal, the
total run time of the algorithm, the number of it-
erations N before convergence, and the size of the
hypergraph in the end of the search (in thousands
of nodes and thousands of edges).
16
16
The size of the initial proposal does not depend on LM
order, but rather on distortion limit (see Figure 3): on aver-
age (in thousands) |V
0
| = 0.6 and |E
0
| = 27 with d = 4,
|V
0
| = 1.3 and |E
0
| = 70 with d = 5, and |V
0
| = 2.5 and
1244
n d build (s) total (s) N |V | |E|
3 4 1.5 21 190 2.5 159
3 5 3.5 55 303 4.4 343
3 6 10 162 484 8 725
4 4 1.5 50 350 4 288
5 4 1.5 106 555 6.1 450
Table 1: Performance of the exact decoder in
terms of: time to build g
(0)
, total decoding time in-
cluding build, number of iterations (N), and num-
ber of nodes and edges (in thousands) at the end of
the search.
It is insightful to understand how different as-
pects of the initial proposal impact on perfor-
mance. Increasing the translation option limit (tol)
leads to g
(0)
having more edges (this dependency
is linear with tol). In this case, the number of
nodes is only minimally affected ? due to the pos-
sibility of a few new segmentations. The maxi-
mum phrase length (mpl) introduces in g
(0)
more
configurations of reordering constraints ([l, C] in
Figure 3). However, not many more, due to C
being limited by the distortion limit d. In prac-
tice, we observe little impact on time performance.
Increasing d introduces many more permutations
of the input leading to exponentially many more
nodes and edges. Increasing the order n of the LM
has no impact on g
(0)
and its impact on the overall
search is expressed in terms of a higher number of
nodes being locally intersected.
An increased hypergraph, be it due to addi-
tional nodes or additional edges, necessarily leads
to slower iterations because at each iteration we
must compute inside weights in timeO(|V |+|E|).
The number of nodes has the larger impact on the
number of iterations. OS
?
is very efficient in ig-
noring hypotheses (edges) that cannot compete for
an optimum. For instance, we observe that run-
ning time depends linearly on tol only through the
computation of inside weights, while the number
of iterations is only minimally affected.
17
An in-
|E
0
| = 178 with d = 6. Observe the exponential depen-
dency on distortion limit, which also leads to exponentially
longer running times.
17
It is possible to reduce the size of the hypergraph
throughout the search using the upperbound on the search
error g
?
? f
?
to prune hypotheses that surely do not stand
a chance of competing for the optimum (Graehl, 2005). An-
other direction is to group edges connecting the same nonter-
minal nodes into one partial edge (Heafield et al., 2013a) ?
this is particularly convenient due to our method only visiting
the 1-best derivation from g(d) at each iteration.
n
Nodes at level m LM states at level m
0 1 2 3 4 1 2 3 4
3 0.4 1.2 0.5 - - 113 263 - -
4 0.4 1.6 1.4 0.3 - 132 544 212 -
5 0.4 2.1 2.4 0.7 0.1 142 790 479 103
Table 2: Average number of nodes (in thousands)
whose LM state encode an m-gram, and average
number of unique LM states of order m in the fi-
nal hypergraph for different n-gram LMs (d = 4
everywhere).
creased LM order, for a fixed distortion limit, im-
pacts much more on the number of iterations than
on the average running time of a single iteration.
Fixing d = 4, the average time per iteration is 0.1
(n = 3), 0.13 (n = 4) and 0.18 (n = 5). Fixing a
3-gram LM, we observe 0.1 (d = 4), 0.17 (d = 5)
and 0.31 (d = 6). Note the exponential growth
of the latter, due to a proposal encoding exponen-
tially many more permutations.
Table 2 shows the average degree of refine-
ment of the nodes in the final proposal. Nodes
are shown by level of refinement, where m indi-
cates that they store m words in their carry. The
table also shows the number of unique m-grams
ever incorporated to the proposal. This table il-
lustrates well how our decoding algorithm moves
from a coarse upperbound where every node stores
an empty string to a variable-order representation
which is sufficient to prove an optimum derivation.
In our approach a complete derivation is opti-
mised from the proxy model at each iteration. We
observe that over 99% of these derivations project
onto distinct strings. In addition, while the opti-
mum solution may be found early in the search, a
certificate of optimality requires refining the proxy
until convergence (see ?4.1). It turns out that most
of the solutions are first encountered as late as in
the last 6-10% of the iterations.
We use the optimum derivations obtained with
our exact decoder to measure the number of search
errors made by beam search and cube pruning with
increasing beam sizes (see Table 3). Beam search
reaches optimum derivations with beam sizes k ?
500 for all language models tested. Cube prun-
ing, on the other hand, still makes mistakes at
k = 1000. Table 4 shows translation quality
achieved with different beam sizes for cube prun-
ing and compares it to exact decoding. Note that
for k ? 10
4
cube pruning converges to optimum
1245
kBeam search Cube pruning
3 4 5 3 4 5
10 938 1294 1475 2168 2347 2377
10
2
19 60 112 613 999 1126
10
3
0 0 0 29 102 167
10
4
0 0 0 0 4 7
Table 3: Beam search and cube pruning search er-
rors (out of 3,003 test samples) by beam size using
LMs of order 3 to 5 (d = 4).
order 3 4 5
k d = 4 d = 5 d = 6 d = 4 d = 4
10 20.47 20.13 19.97 20.71 20.69
10
2
21.14 21.18 21.08 21.73 21.76
10
3
21.27 21.34 21.32 21.89 21.91
10
4
21.29 21.37 21.37 21.92 21.93
OS
?
21.29 21.37 21.37 21.92 21.93
Table 4: Translation quality in terms of BLEU as
a function of beam size in cube pruning with lan-
guage models of order 3 to 5. The bottom row
shows BLEU for our exact decoder.
derivations in the vast majority of the cases (100%
with a 3-gram LM) and translation quality in terms
of BLEU is no different from OS
?
. However, with
k < 10
4
both model scores and translation quality
can be improved. Figure 5 shows a finer view on
search errors as a function of beam size for LMs
of order 3 to 5 (fixed d = 4). In Figure 6, we fix
a 3-gram LM and vary the distortion limit (from 4
to 6). Dotted lines correspond to beam search and
dashed lines correspond to cube pruning.
6 Conclusions and Future Work
We have presented an approach to decoding with
unpruned hypergraphs using upperbounds on the
language model distribution. The algorithm is an
instance of a coarse-to-fine strategy with connec-
tions to A
?
and adaptive rejection sampling known
as OS
?
. We have tested our search algorithm us-
ing state-of-the-art phrase-based models employ-
ing robust language models. Our algorithm is able
to decode all sentences of a standard test set in
manageable time consuming very little memory.
We have performed an analysis of search errors
made by beam search and cube pruning and found
that both algorithms perform remarkably well for
phrase-based decoding. In the case of cube prun-
ing, we show that model score and translation
102 103 104[log] Beam size
100
101
102
103
104
[log
] Se
arch
 err
ors
Search errors in newstest2012
CP 3-gramCP 4-gramCP 5-gramBS 3-gramBS 4-gramBS 5-gram
Figure 5: Search errors made by beam search and
cube pruning as a function of beam-size.
102 103 104[log] Beam size
100
101
102
103
104
[log
] Se
arch
 err
ors
Search errors in newstest2012 (3-gram LM)
CP d=4CP d=5CP d=6BS d=4BS d=5BS d=6
Figure 6: Search errors made by beam search and
cube pruning as a function of the distortion limit
(decoding with a 3-gram LM).
quality can be improved for beams k < 10, 000.
There are a number of directions that we intend
to investigate to speed up our decoder, such as: (1)
error-safe pruning based on search error bounds;
(2) use of reinforcement learning to guide the de-
coder in choosing which n-gram contexts to ex-
tend; and (3) grouping edges into partial edges,
effectively reducing the size of the hypergraph and
ultimately computing inside weights in less time.
Acknowledgments
The work of Wilker Aziz and Lucia Specia was
supported by EPSRC (grant EP/K024272/1).
1246
References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of transla-
tion model search spaces. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 224?232, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wilker Aziz, Marc Dymetman, and Sriram Venkatapa-
thy. 2013. Investigations in exact inference for hi-
erarchical translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
472?483, Sofia, Bulgaria, August. Association for
Computational Linguistics.
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547?550, Boston, MA, June.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311, June.
Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact sampling and decoding in
high-order hidden Markov models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125?1134, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
Lagrangian relaxation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 26?37, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 427?436, Stroudsburg, PA,
USA. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201?228.
Shay B. Cohen, Robert J. Simmons, and Noah A.
Smith. 2008. Dynamic programming algorithms as
products of weighted logic programs. In Maria Gar-
cia de la Banda and Enrico Pontelli, editors, Logic
Programming, volume 5366 of Lecture Notes in
Computer Science, pages 114?129. Springer Berlin
Heidelberg.
G Dantzig, R Fulkerson, and S Johnson. 1954. So-
lution of a large-scale traveling-salesman problem.
Operations Research, 2:393?410.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 114?121, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 858?
866, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Marc Dymetman, Guillaume Bouchard, and Simon
Carter. 2012. Optimization and sampling for NLP
from a unified viewpoint. In Proceedings of the
First International Workshop on Optimization Tech-
niques for Human Language Technology, pages 79?
94, Mumbai, India, December. The COLING 2012
Organizing Committee.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and
applications. Discrete Applied Mathematics, 42(2-
3):177?201, April.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ?01, pages 228?
235, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Joshua Goodman. 1999. Semiring parsing. Comput.
Linguist., 25(4):573?605, December.
Jonathan Graehl. 2005. Relatively useless pruning.
Technical report, USC Information Sciences Insti-
tute.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.
1968. A formal basis for the heuristic determina-
tion of minimum cost paths. IEEE Transactions On
Systems Science And Cybernetics, 4(2):100?107.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013a. Grouping language model boundary words
1247
to speed k-best extraction from hypergraphs. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
958?968, Atlanta, Georgia, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013b. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 690?696, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Kenneth Heafield, Michael Kayser, and Christopher D.
Manning. 2014. Faster Phrase-Based decoding by
refining feature state. In Proceedings of the Associa-
tion for Computational Linguistics, Baltimore, MD,
USA, June.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 380?388, Athens,
Greece, March. Association for Computational Lin-
guistics.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Series in Artificial In-
telligence. Prentice Hall, 1 edition.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Ac-
coustics, Speech, and Signal Processing, 1:181?184.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25(4):607?615, December.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit, pages 79?86.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine transla-
tion. Natural Language Engineering, 12(1):35?75,
March.
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the Second Workshop on Syntax
and Structure in Statistical Translation, SSST ?08,
pages 10?18, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):8:1?8:49, August.
Adam Lopez. 2009. Translation as weighted de-
duction. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, EACL ?09, pages 532?540,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statisti-
cal machine translation. In Proceedings of the work-
shop on Data-driven methods in machine translation
- Volume 14, DMMT ?01, pages 1?8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1 of ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 108?116, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sebastian Riedel and James Clarke. 2009. Revisit-
ing optimal decoding for machine translation IBM
model 4. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, NAACL-Short ?09, pages 5?8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
1248
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods (Springer Texts in Statis-
tics). Springer-Verlag New York, Inc., Secaucus,
NJ, USA.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
Lagrangian relaxation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 72?82, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
and A. Zubiaga. 1997. A DP based search using
monotone alignments in statistical translation. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, EACL ?97, pages 289?296, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine
translation as a traveling salesman problem. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ?09, pages 333?
341, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1249
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117?122,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
USPwlv and WLVusp: Combining Dictionaries and Contextual
Information for Cross-Lingual Lexical Substitution
Wilker Aziz
University of S?ao Paulo
S?ao Carlos, SP, Brazil
wilker.aziz@usp.br
Lucia Specia
University of Wolverhampton
Wolverhampton, UK
l.specia@wlv.ac.uk
Abstract
We describe two systems participating
in Semeval-2010?s Cross-Lingual Lexical
Substitution task: USPwlv and WLVusp.
Both systems are based on two main com-
ponents: (i) a dictionary to provide a num-
ber of possible translations for each source
word, and (ii) a contextual model to select
the best translation according to the con-
text where the source word occurs. These
components and the way they are inte-
grated are different in the two systems:
they exploit corpus-based and linguistic
resources, and supervised and unsuper-
vised learning methods. Among the 14
participants in the subtask to identify the
best translation, our systems were ranked
2nd and 4th in terms of recall, 3rd and 4th
in terms of precision. Both systems out-
performed the baselines in all subtasks ac-
cording to all metrics used.
1 Introduction
The goal of the Cross-Lingual Lexical Substitu-
tion task in Semeval-2010 (Mihalcea et al, 2010)
is to find the best (best subtask) Spanish transla-
tion or the 10-best (oot subtask) translations for
100 different English source words depending on
their context of occurrence. Source words include
nouns, adjectives, adverbs and verbs. 1, 000 oc-
currences of such words are given along with a
short context (a sentence).
This task resembles that of Word Sense Dis-
ambiguation (WSD) within Machine Translation
(MT). A few approaches have recently been pro-
posed using standard WSD features to learn mod-
els using translations instead of senses (Specia et
al., 2007; Carpuat and Wu, 2007; Chan and Ng,
2007). In such approaches, the global WSD score
is added as a feature to statistical MT systems,
along with additional features, to help the system
on its choice for the best translation of a source
word or phrase.
We exploit contextual information in alternative
ways to standard WSD features and supervised ap-
proaches. Our two systems - USPwlv and WLV
usp - use two main components: (i) a list of pos-
sible translations for the source word regardless of
its context; and (ii) a contextual model that ranks
such translations for each occurrence of the source
word given its context.
While these components constitute the core of
most WSD systems, the way they are created and
integrated in our systems differs from standard ap-
proaches. Our systems do not require a model
to disambiguate / translate each particular source
word, but instead use general models. We experi-
mented with both corpus-based and standard dic-
tionaries, and different learning methodologies to
rank the candidate translations. Our main goal was
to maximize the accuracy of the system in choos-
ing the best translation.
WLVusp is a very simple system based es-
sentially on (i) a Statistical Machine Translation
(SMT) system trained using a large parallel cor-
pus to generate the n-best translations for each
occurrence of the source words and (ii) a stan-
dard English-Spanish dictionary to filter out noisy
translations and provide additional translations in
case the SMT system was not able to produce a
large enough number of legitimate translations,
particularly for the oot subtask.
USPwlv uses a dictionary built from a large par-
allel corpus using inter-language information the-
ory metrics and an online-learning supervised al-
gorithm to rank the options from the dictionary.
The ranking is based on global and local contex-
tual features, such as the mutual information be-
tween the translation and the words in the source
context, which are trained using human annotation
on the trial dataset.
117
2 Resources
2.1 Parallel corpus
The English-Spanish part of Europarl (Koehn,
2005), a parallel corpus from the European Par-
liament proceedings, was used as a source of sen-
tence level aligned data. The nearly 1.7M sentence
pairs of English-Spanish translations, as provided
by the Fourth Workshop on Machine Translation
(WMT09
1
), sum up to approximately 48M tokens
in each language. Europarl was used both to train
the SMT system and to generate dictionaries based
on inter-language mutual information.
2.2 Dictionaries
The dictionary used by WLVusp was extracted us-
ing the free online service Word Reference
2
, which
provides two dictionaries: Espasa Concise and
Pocket Oxford Spanish Dictionary. Regular ex-
pressions were used to extract the content of the
webpages, keeping only the translations of the
words or phrasal expressions, and the outcome
was manually revised. The manual revision was
necessary to remove translations of long idiomatic
expressions which were only defined through ex-
amples, for example, for the verb check: ?we
checked up and found out he was lying ? hicimos
averiguaciones y comprobamos que ment??a?. The
resulting dictionary contains a number of open do-
main (single or multi-word) translations for each
of the 100 source words. This number varies from
3 to 91, with an average of 12.87 translations per
word. For example:
? yet.r = todav??a, a?un, ya, hasta ahora, sin em-
bargo
? paper.n = art??culo, papel, envoltorio, diario,
peri?odico, trabajo, ponencia, examen, parte,
documento, libro
Any other dictionary can in principle be used to
produce the list of translations, possibly without
manual intervention. More comprehensive dictio-
naries could result in better results, particularly
those with explicit information about the frequen-
cies of different translations. Automatic metrics
based on parallel corpus to learn the dictionary can
also be used, but we would expect the accuracy of
the system to drop in that case.
1
http://www.statmt.org/wmt09/
translation-task.html
2
http://www.wordreference.com/
The process to generate the corpus-based dic-
tionary for USPwlv is described in Section 4.
2.3 Pre-processing techniques
The Europarl parallel corpus was tokenized and
lowercased using standard tools provided by the
WMT09 competition. Additionally, the sentences
that were longer than 100 tokens after tokenization
were discarded.
Since the task specifies that translations should
be given in their basic forms, and also in order to
decrease the sparsity due to the rich morphology
of Spanish, the parallel corpus was lemmatized us-
ing TreeTagger (Schmid, 2006), a freely available
part-of-speech (POS) tagger and lemmatizer. Two
different versions of the parallel corpus were built
using both lemmatized words and their POS tags:
Lemma Words are represented by their lemma-
tized form. In case of ambiguity, the original
form was kept, in order to avoid incorrect choices.
Words that could not be lemmatized were also kept
as in their original form.
Lemma.pos Words are represented by their lem-
matized form followed by their POS tags. POS
tags representing content words are generalized
into four groups: verbs, nouns, adjectives and ad-
verbs. When the system could not identify a POS
tag, a dummy tag was used.
The same techniques were used to pre-process
the trial and test data.
2.4 Training samples
The trial data available for this task was used as a
training set for the USPwlv system, which uses a
supervised learning algorithm to learn the weights
of a number of global features. For the 300 oc-
currences of 30 words in the trial data, the ex-
pected lexical substitutions were given by the task
organizers, and therefore the feature weights could
be optimized in a way to make the system result
in good translations. These sentences were pre-
processed in the same way the parallel corpus.
3 WLVusp system
This system is based on a combination of the
Statistical Machine Translation (SMT) frame-
work using the English-Spanish Europarl data
and an English-Spanish dictionary built semi-
automatically (Section 2.2). The parallel corpus
118
was lowercased, tokenized and lemmatized (Sec-
tion 2.3) and then used to train the standard SMT
system Moses (Koehn et al, 2007) and translate
the trial/test sentences, producing the 1000-best
translations for each input sentence.
Moses produces its own dictionary from the
parallel corpus by using a word alignment tool
and heuristics to build parallel phrases of up to
seven source words and their corresponding target
words, to which are assigned translation probabil-
ities using frequency counts in the corpus. This
methodology provides some very localized con-
textual information, which can help guiding the
system towards choosing a correct translation. Ad-
ditional contextual information is used by the lan-
guage model component in Moses, which con-
siders how likely the sentence translation is in
the Spanish language (with a 5-gram language
model).
Using the phrase alignment information, the
translation of each occurrence of a source word
is identified in the output of Moses. Since the
phrase translations are learned using the Europarl
corpus, some translations are very specific to that
domain. Moreover, translations can be very noisy,
given that the process is unsupervised. We there-
fore filter the translations given by Moses to keep
only those also given as possible Spanish trans-
lations according to the semi-automatically built
English-Spanish dictionary (Section 2.2). This is
a general-domain dictionary, but it is less likely to
contain noise.
For best results, only the top translation pro-
duced by Moses is considered. If the actual trans-
lation does not belong to the dictionary, the first
translation in that dictionary is used. Although
there is no information about the order of the
translations in the dictionaries used, by looking at
the translations provided, we believe that the first
translation is in general one of the most frequent.
For oot results, the alternative translations pro-
vided by the 1000-best translations are consid-
ered. In cases where fewer than 10 translations
are found, we extract the remaining ones from the
handcrafted dictionary following their given order
until 10 translations (when available) are found,
without repetition.
WLVusp system therefore combines contextual
information as provided by Moses (via its phrases
and language model) and general translation infor-
mation as provided by a dictionary.
4 USPwlv System
For each source word occurring in the context of
a specific sentence, this system uses a linear com-
bination of features to rank the options from an
automatically built English-Spanish dictionary.
For the best subtask, the translation ranked first
is chosen, while for the oot subtask, the 10 best
ranked translations are used without repetition.
The building of the dictionary, the features used
and the learning scheme are described in what fol-
lows.
Dictionary Building The dictionary building is
based on the concept of inter-language Mutual In-
formation (MI) (Raybaud et al, 2009). It consists
in detecting which words in a source-language
sentence trigger the appearance of other words in
its target-language translation. The inter-language
MI in Equation 3 can be defined for pairs of source
(s) and target (t) words by observing their occur-
rences at the sentence level in a parallel, sentence
aligned corpus. Both simple (Equation 1) and
joint distributions (Equation 2) were built based
on the English-Spanish Europarl corpus using its
Lemma.pos version (Section 2.3).
p
l
(x) =
count
l
(x)
Total
(1)
p
en,es
(s, t) =
f
en,es
(s, t)
Total
(2)
MI(s, t) = p
en,es
(s, t)log
(
p
en,es
(s, t)
p
en
(s)p
es
(t)
)
(3)
Avg
MI
(t
j
) =
?
l
i=1
w(|i? j|)MI(s
i
, t
j
)
?
l
i=1
w(|i? j|)
(4)
In the equations, count
l
(x) is the number of sen-
tences in which the word x appear in a corpus of
l-language texts; count
en,es
(s, t) is the number of
sentences in which source and target words co-
occur in the parallel corpus; and Total is the to-
tal number of sentences in the corpus of the lan-
guage(s) under consideration. The distributions
p
en
and p
es
are monolingual and can been ex-
tracted from any monolingual corpus.
To prevent discontinuities in Equation 3, we
used a smoothing technique to avoid null proba-
bilities. We assume that any monolingual event
occurs at least once and the joint distribution is
smoothed by a Guo?s factor ? = 0.1 (Guo et al,
2004):
p
en,es
(s, t)?
p
en,es
(s, t) + ?p
en
(s)p
es
(t)
1 + ?
119
For each English source word, a list of Span-
ish translations was produced and ranked accord-
ing to inter-language MI. From the resulting list,
the 50-best translations constrained by the POS of
the original English word were selected.
Features The inter-language MI is a feature
which indicates the global suitability of translat-
ing a source token s into a target one t. However,
inter-language MI is not able to provide local con-
textual information, since it does not take into ac-
count the source context sentence c. The following
features were defined to achieve such capability:
Weighted Average MI (aMI) consists in averag-
ing the inter-language MI between the target
word t
j
and every source word s in the con-
text sentence c (Raybaud et al, 2009). The
MI component is scaled in a way that long
range dependencies are considered less im-
portant, as shown in Equation 4. The scaling
factor w(?) is assigned 1 for verbs, nouns, ad-
jectives and adverbs up to five positions from
the source word, and 0 otherwise. This fea-
ture gives an idea of how well the elements in
a window centered in the source word head
(s
j
) align to the target word t
j
, representing
the suitability of t
j
translating s
j
in the given
context.
Modified Weighted Average MI (mMI) takes
the average MI as previously defined, except
that the source word head is not taken into
account. In other words, the scaling function
in Equation 4 equals 0 also when |i? j| = 0.
It gives an idea of how well the source words
align to the target word t
j
without the strong
influence of its source translation s
j
. This
should provide less biased information to the
learning.
Best from WLVusp (B) consists in a flag that in-
dicates whether a candidate t is taken as the
best ranked option according to the WLVusp
system. The goal is to exploit the informa-
tion from the SMT system and handcrafted
dictionary used by that system.
10-best from WLVusp (T) this feature is a flag
which indicates whether a candidate t was
among the 10 best ranked translations pro-
vided by the WLVusp system.
Online Learning In order to train a binary rank-
ing system based on the trial dataset as our train-
ing set, we used the online passive-aggressive al-
gorithm MIRA (Crammer et al, 2006). MIRA is
said to be passive-aggressive because it updates
the parameters only when a misprediction is de-
tected. At training time, for each sentence a set
of pairs of candidate translations is retrieved. For
each of these pairs, the rank given by the system
with the current parameters is compared to the cor-
rect rank
h
(?). A loss function loss(?) controls the
updates attributing non 0 values only for mispre-
dictions. In our implementation, it equals 1 for
any mistake made by the model.
Each element of the kind (c, s, t) = (source
context sentence, source head, translation can-
didate) is assigned a feature vector f(c, s, t) =
?MI, aMI,mMI,B, T ?, which is modeled by a
vector of parameters w ? R
5
.
The binary ranking is defined as the task of find-
ing the best parameters w which maximize the
number of successful predictions. A successful
prediction happens when the system is able to rank
two translation candidates as expected. For do-
ing so, we define an oriented pair x = (a, b) of
candidate translations of s in the context of c and
a feature vector F (x) = f(c, s, a) ? f(c, s, b).
signal(w?F (x)) is the orientation the model gives
to x, that is, whether the system believes a is bet-
ter than b or vice versa. Based on whether or not
that orientation is the same as that of the reference
3
, the algorithm takes the decision between updat-
ing or not the parameters. When an update occurs,
it is the one that results in the minimal changes in
the parameters leading to correct labeling x, that
is, guaranteeing that after the update the system
will rank (a, b) correctly. Algorithm 1 presents
the general method, as proposed in (Crammer et
al., 2006).
In the case of this binary ranking, the minimiza-
tion problem has an analytic solution well defined
as long as f(c, s, a) 6= f(c, s, b) and rank
h
(a) 6=
rank
h
(b), otherwise signal(w ? F (x)) or the hu-
man label would not be defined, respectively.
These conditions have an impact on the content of
Pairs(c), the set of training points built upon the
system outputs for c, which can only contain pairs
of differently ranked translations.
The learning scheme was initialized with a uni-
3
Given s in the context of c and (a, b) a pair of candidate
translations of s, the reference produces 1 if rank
h
(a) >
rank
h
(b) and ?1 if rank
h
(b) > rank
h
(a).
120
Algorithm 1 MIRA
1: for c ? Training Set do
2: for x = (a, b) ? Pairs(c) do
3: y? ? signal(w ? F (x))
4: z ? correct label(x)
5: w = argmax
u
1
2
||w ? u||
2
6: s.t. u ? F (x) ? loss(y?, z)
7: v ? v + w
8: T ? T + 1
9: end for
10: end for
11: return
1
T
v
form vector. The average parameters after N = 5
iterations over the training set was taken.
5 Results
5.1 Official results
Tables 1 and 2 show the main results obtained by
our two systems in the official competition. We
contrast our systems? results against the best base-
line provided by the organizers, DIC, which con-
siders translations from a dictionary and frequency
information from WordNet, and show the relative
position of the system among the 14 participants.
The metrics are defined in (Mihalcea et al, 2010).
Subtask Metric Baseline WLVusp Position
Best
R 24.34 25.27 4
th
P 24.34 25.27 3
rd
Mode R 50.34 52.81 3
rd
Mode P 50.34 52.81 4
th
OOT
R 44.04 48.48 6
th
P 44.04 48.48 6
th
Mode R 73.53 77.91 5
th
Mode P 73.53 77.91 5
th
Table 1: Official results for WLVusp on the test set, com-
pared to the highest baseline, DICT. P = precision, R = recall.
The last column shows the relative position of the system.
Subtask Metric Baseline USPwlv Position
Best
R 24.34 26.81 2
nd
P 24.34 26.81 3
rd
Mode R 50.34 58.85 1
st
Mode P 50.34 58.85 2
nd
OOT
R 44.04 47.60 8
th
P 44.04 47.60 8
th
Mode R 73.53 79.84 3
rd
Mode P 73.53 79.84 3
rd
Table 2: Official results for USPwlv on the test set, com-
pared to the highest baseline, DICT. The last column shows
the relative position of the system.
In the oot subtask, the original systems were
able to output the mode translation approximately
80% of the times. From those translations, nearly
50% were actually considered as best options ac-
cording to human annotators. It is worth noticing
that we focused on the best subtask. Therefore,
for the oot subtask we did not exploit the fact that
translations could be repeated to form the set of 10
best translations. For certain source words, our re-
sulting set of translations is smaller than 10. For
example, in the WLVusp system, whenever the
set of alternative translations identified in Moses?
top 1000-best list did not contain 10 legitimate
translations, that is, 10 translations also found in
the handcrafted dictionary, we simply copied other
translations from that dictionary to amount 10 dif-
ferent translations. If they did not sum to 10 be-
cause the list of translations in the dictionary was
too short, we left the set as it was. As a result, 58%
of the 1000 test cases had fewer than 10 transla-
tions, many of them with as few as two or three
translations. In fact, the list of oot results for the
complete test set resulted in only 1, 950 transla-
tions, when there could be 10, 000 (1, 000 test case
occurrences ? 10 translations). In the next section
we describe some additional experiments to take
this issue into account.
5.2 Additional results
After receiving the gold-standard data, we com-
puted the scores for a number of variations of our
two systems. For example, we checked whether
the performance of USPwlv is too dependent on
the handcrafted dictionary, via the features B and
T. Table 3 presents the performance of two varia-
tions of USPwlv: MI-aMI-mMI was trained with-
out the two contextual flag features which depend
on WLVusp. MI-B-T was trained without the mu-
tual information contextual features. The variation
MI-aMI-mMI of USPwlv performs well even in
the absence of the features coming from WLVusp,
although the scores are lower. These results show
the effectiveness of the learning scheme, since
USPwlv achieves better performance by combin-
ing these feature variations, as compared to their
individual performance.
To provide an intuition on the contribution
of the two different components in the system
WLVusp, we checked the proportion of times a
translation was provided by each of the compo-
nents. In the best subtask, 48% of the translations
came from Moses, while the remaining 52% pro-
121
Subtask Metric Baseline MI-aMI-mMI MI-B-T
Best
R 24.34 22.59 20.50
P 24.34 22.59 20.50
Mode R 50.34 50.21 44.01
Mode P 50.34 50.21 44.01
OOT
R 39.65 47.60 32.75
P 44.04 39.65 32.75
Mode R 73.53 74.19 56.70
Mode P 73.53 74.19 56.70
Table 3: Comparing between variations of the system
USPwlv on the test set and the highest baseline, DICT. The
variations are different sources of contextual knowledge: MI
(MI?aMI?mMI) and the WLVusp (MI?B?T) system.
vided by Moses were not found in the dictionary.
In those cases, the first translation in the dictio-
nary was used. In the oot subtask, only 12% (246)
of the translations came from Moses, while the re-
maining (1, 704) came from the dictionary. This
can be explained by the little variation in the n-
best lists produced by Moses: most of the varia-
tions account for word-order, punctuation, etc.
Finally, we performed additional experiments in
order to exploit the possibility of replicating well
ranked translations for the oot subtask. Table 4
presents the results of some strategies arbitrarily
chosen for such replications. For example, in the
colums labelled ?5? we show the scores for re-
peating (once) the 5 top translations. Notice that
precision and recall increase as we take fewer top
translation and repeat them more times. In terms
of mode metrics, by reducing the number of dis-
tinct translations from 10 to 5, USPwlv still out-
performs (marginally) the baseline. In general, the
new systems outperform the baseline and our pre-
vious results (see Table 1 and 2) in terms of pre-
cision and recall. However, according to the other
mode metrics, they are below our official systems.
System Metric 5 4 3 2
WLVusp
R 69.09 88.36 105.32 122.29
P 69.09 88.36 105.32 122.29
Mode R 68.27 63.05 63.05 52.47
Mode P 68.27 63.05 63.05 52.47
USPwlv
R 73.50 94.78 102.96 129.09
P 73.50 94.78 102.96 129.09
Mode R 73.77 68.27 62.62 57.40
Mode P 73.77 68.27 62.62 57.40
Table 4: Comparison between different strategies for dupli-
cating answers in the task oot. The systems output a number
of distinct guesses and through arbitrarily schemes replicate
them in order to complete a list of 10 translations.
6 Discussion and future work
We have presented two systems combining con-
textual information and a pre-defined set of trans-
lations for cross-lingual lexical substitution. Both
systems performed particularly well in the best
subtask. A handcrafted dictionary has shown to be
essential for the WLVusp system and also helpful
for the USPwlv system, which uses an additional
dictionary automatically build from a parallel cor-
pus. We plan to investigate how such systems can
be improved by enhancing the corpus-based re-
sources to further minimize the dependency on the
handcrafted dictionary.
References
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 61?72.
Yee Seng Chan and Hwee Tou Ng. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics, pages 33?40.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-agressive algorithms. Jornal of Machine
Learning Research, 7:551?585.
Gang Guo, Chao Huang, Hui Jiang, and Ren-Hua
Wang. 2004. A comparative study on various con-
fidence measures in large vocabulary speech recog-
nition. In International Symposium on Chinese Spo-
ken Language Processing, pages 9?12.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval-2010 task 2: Cross-lingual lexical
substitution. In SemEval-2010: 5th International
Workshop on Semantic Evaluations.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Smaili. 2009. Word- and sentence-
level confidence measures for machine translation.
In 13th Annual Conference of the European Associ-
ation for Machine Translation, pages 104?111.
Helmut Schmid. 2006. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Natural Language Pro-
cessing, pages 44?49.
Lucia Specia, Mark Stevenson, and Maria das Grac?as
Volpe Nunes. 2007. Learning expressive models for
word sense disambiguation. In 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 41?148.
122
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 673?678,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UOW: Semantically Informed Text Similarity
Miguel Rios and Wilker Aziz
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton,
WV1 1SB, UK
{M.Rios, W.Aziz}@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello,
Sheffield, S1 4DP, UK
L.Specia@sheffield.ac.uk
Abstract
The UOW submissions to the Semantic Tex-
tual Similarity task at SemEval-2012 use a
supervised machine learning algorithm along
with features based on lexical, syntactic and
semantic similarity metrics to predict the se-
mantic equivalence between a pair of sen-
tences. The lexical metrics are based on word-
overlap. A shallow syntactic metric is based
on the overlap of base-phrase labels. The
semantically informed metrics are based on
the preservation of named entities and on the
alignment of verb predicates and the overlap
of argument roles using inexact matching. Our
submissions outperformed the official base-
line, with our best system ranked above aver-
age, but the contribution of the semantic met-
rics was not conclusive.
1 Introduction
We describe the UOW submissions to the Semantic
Textual Similarity (STS) task at SemEval-2012. Our
systems are based on combining similarity scores as
features using a regression algorithm to predict the
degree of semantic equivalence between a pair of
sentences. We train the regression algorithm with
different classes of similarity metrics: i) lexical,
ii) syntactic and iii) semantic. The lexical similar-
ity metrics are: i) cosine similarity using a bag-of-
words representation, and ii) precision, recall and
F-measure of content words. The syntactic metric
computes BLEU (Papineni et al, 2002), a machine
translation evaluation metric, over a labels of base-
phrases (chunks). Two semantic metrics are used: a
metric based on the preservation of Named Entities
and TINE (Rios et al, 2011). Named entities are
matched by type and content: while the type has to
match exactly, the content is compared with the as-
sistance of a distributional thesaurus. TINE is a met-
ric proposed to measure adequacy in machine trans-
lation and favors similar semantic frames. TINE
attempts to align verb predicates, assuming a one-
to-one correspondence between semantic roles, and
considering ontologies for inexact alignment. The
surface realization of the arguments is compared us-
ing a distributional thesaurus and the cosine similar-
ity metric. Finally, we use METEOR (Denkowski
and Lavie, 2010), also a common metric for ma-
chine translation evaluation, that also computes in-
exact word overlap as at way of measuring the im-
pact of our semantic metrics.
The lexical and syntactic metrics complement the
semantic metrics in dealing with the phenomena ob-
served in the task?s dataset. For instance, from the
MSRvid dataset:
S1 Two men are playing football.
S2 Two men are practicing football.
In this case, as typical of paraphrasing, the situa-
tion and participants are the same while the surface
realization differs, but playing can be considered
similar to practicing. From the SMT-eur dataset:
S3 The Council of Europe, along with the Court of
Human Rights, has a wealth of experience of
such forms of supervision, and we can build on
these.
673
S4 Just as the European Court of Human Rights, the
Council of Europe has also considerable expe-
rience with regard to these forms of control; we
can take as a basis.
Similarly, here although with different realiza-
tions, the Court of Human Rights and the European
Court of Human Rights represent the same entity.
Semantic metrics based on predicate-argument
structure can play a role in cases when different re-
alization have similar semantic roles:
S5 The right of a government arbitrarily to set aside
its own constitution is the defining characteris-
tic of a tyranny.
S6 The right for a government to draw aside its con-
stitution arbitrarily is the definition character-
istic of a tyranny.
In this work we attempt to exploit the fact that su-
perficial variations such the ones in these examples
should still render very similarity scores.
In Section 2 we describe the similarity metrics in
more detail. In Section 3 we show the results of our
three systems. In Section 4 we discuss these results
and in Section 5 we present some conclusions.
2 Similarity Metrics
The metrics used in this work are as follows:
2.1 Lexical metrics
All our lexical metrics use the same surface repre-
sentation: words. However, the cosine metric uses
bag-of-words, while all the other metrics use only
content words. We thus first represent the sentences
as bag-of-words. For example, given the pair of sen-
tences S7 and S8:
S7 A man is riding a bicycle.
S8 A man is riding a bike.
the bag-of-words are S7 = {A, man, is, riding, a,
bicycle,.} and S8 = {A, man, is, riding, a, bike, .},
and the bag-of-content-words are S7 = {man, riding,
bicycle} and S8 = {man, riding, bike}.
We compute similarity scores using the following
metrics between a pair of sentencesA andB: cosine
distance (Equation 1), precision (Equation 2), recall
(Equation 3) and F-measure (Equation 4).
cosine(A,B) =
|A
?
B|
?
|A| ? |B|
(1)
precision(A,B) =
|A
?
B|
|B|
(2)
recall(A,B) =
|A
?
B|
|A|
(3)
F (A,B) = 2 ?
precision(A,B) ? recall(A,B)
precision(A,B) + recall(A,B)
(4)
2.2 BLEU over base-phrases
The BLEU metric is used for the automatic evalua-
tion of Machine Translation. The metric computes
the precision of exact matching of n-grams between
a hypothesis and reference translations. This sim-
ple procedure has limitations such as: the matching
of non-content words mixed with the counts of con-
tent words affects in a perfect matching that can hap-
pen even if the order of sequences of n-grams in the
hypothesis and reference translation are very differ-
ent, changing completely the meaning of the trans-
lation. To account for similarity in word order we
use BLEU over base-phrase labels instead of words,
leaving the lexical matching for other lexical and se-
mantic metrics. We compute the matchings of 1-
4-grams of base-phrase labels. This metric favors
similar syntactic order.
2.3 Named Entities metric
The goal of the metric is to deal with synonym enti-
ties. First, named entities are grouped by class (e.g.
Organization), and then the content of the named en-
tities within the same classes is compared through
cosine similarity. If the surface realization is differ-
ent, we retrieve words that share the same context
with the named entity using Dekang Lin?s distribu-
tional thesaurus (Lin, 1998). Therefore, the cosine
similarity will have more information than just the
named entities themselves. For example, from the
sentence pair S9 and S10:
S9 Companies include IBM Corp. ...
674
S10 Companies include International Business Ma-
chines ...
The entity from S9: IBM Corp. and the entity
from S10: International Business Machines have
the same tag Organization. The metric groups
them and adds words from the thesaurus result-
ing in the following bag-of-words. S9: {IBM
Corp.,... Microsoft, Intel, Sun Microsystems, Mo-
torola/Motorola, Hewlett-Packard/Hewlett-Packard,
Novell, Apple Computer...} and S10: {International
Business Machines,... Apple Computer, Yahoo, Mi-
crosoft, Alcoa...}. The metric then computes the co-
sine similarity between this expanded pair of bag-of-
words.
2.4 METEOR
This metric is also a lexical metric based on uni-
gram matching between two sentences. However,
matches can be exact, using stems, synonyms, or
paraphrases of unigrams. The synonym matching is
computed using WordNet (Fellbaum, 1998) and the
paraphrase matching is computed using paraphrase
tables (Callison-Burch et al, 2010). The structure of
the sentences is not not directly considered, but sim-
ilar word orders are rewarded through higher scores
for the matching of longer fragments.
2.5 Semantic Role Label metric
Rios et al (2011) propose TINE, an automatic met-
ric based on the use semantic roles to align predi-
cates and their respective arguments in a pair of sen-
tences. The metric complements lexical matching
with a shallow semantic component to better address
adequacy in machine translation evaluation. The
main contribution of such a metric is to provide a
more flexible way of measuring the overlap between
shallow semantic representations (semantic role la-
bels) that considers both the semantic structure of
the sentence and the content of the semantic compo-
nents.
This metric allows to match synonym predicates
by using verb ontologies such as VerbNet (Schuler,
2006) and VerbOcean (Chklovski and Pantel, 2004)
and distributional semantics similarity metrics, such
as Dekang Lin?s thesaurus (Lin, 1998), where pre-
vious semantic metrics only perform exact match of
predicate structures and arguments. For example, in
VerbNet the verbs spook and terrify share the same
class amuse-31.1, and in VerbOcean the verb dress
is related to the verb wear, so these are considered
matches in TINE.
The main sources of errors in this metric are the
matching of unrelated verbs and the lack of coverage
of the ontologies. For example, for S11 and S12,
remain and say are (incorrectly) related as given by
VerbOcean.
S11 If snow falls on the slopes this week, Christmas
will sell out too, says Schiefert.
S12 If the roads remain snowfall during the week,
the dates of Christmas will dry up, said
Schiefert.
For this work the matching of unrelated verbs is
a particularly crucial issue, since the sentences to be
compared are not necessarily similar, as it is the gen-
eral case in machine translation. We have thus mod-
ified the metric with a preliminary optimization step
which aligns the verb predicates by measuring two
degrees of similarity: i) how similar their arguments
are, and ii) how related the predicates? realizations
are. Both scores are combined as shown in Equation
5 to score the similarity between the two predicates
(Av, Bv) from a pair of sentences (A,B).
sim(Av,Bv) = (wlex ? lexScore(Av, Bv))
+(warg ? argScore(Aarg, Barg))
(5)
where wlex and warg are the weights for each
component, argScore(Aarg, Barg) is the similarity,
which is computed as in Equation 7, of the argu-
ments between the predicates being compared and
lexScore(Av, Bv) is the similarity score extracted
from the Dekang Lin?s thesaurus between the predi-
cates being compared. The Dekang Lin?s thesaurus
is an automatically built thesaurus, and for each
word it has an entry with the most similar words and
their similarity scores. If the verbs are related in the
thesaurus we use their similarity score as lexScore
otherwise lexScore = 0. The pair of predicates
with the maximum sim score is aligned. The align-
ment is an optimization problem where predicates
are aligned 1-1: we search for all 1-1 alignments that
lead to the maximum average sim for the pair of sen-
tences. For example, S13 and S14 have the follow-
ing list of predicates: S13 = {loaded, rose, ending}
675
and S14 = {laced, climbed}. The metric compares
each pair of predicates and it aligns the predicates
rose and climbed because they are related in the the-
saurus with a similarity score lexScore = 0.796
and a argScore = 0.185 given that the weights are
set to 0.5 and sum up to 1 the predicates reach the
maximum sim = 0.429 score. The output of this
step results in a set of aligned verbs between a pair
of sentences.
S13 The tech - loaded Nasdaq composite rose 0
points to 0 , ending at its highest level for 0
months.
S14 The technology - laced Nasdaq Composite In-
dex IXIC climbed 0 points , or 0 percent , to
0.
The SRL similarity metric semanticRole be-
tween two sentences A and B is then defined as:
semanticRole(A,B) =
?
v?V verbScore(Av, Bv)
|VB |
(6)
The verbScore in Equation 6 is computed over
the set of aligned predicates from the previous opti-
mization step and for each aligned predicate the ar-
gument similarity is computed by Equation 7.
verbScore(Av, Bv) =
?
arg?ArgA?ArgB
argScore(Aarg, Barg)
|ArgB |
(7)
In Equation 6, V is the set of verbs aligned between
the two sentences A and B, and |VB| is the num-
ber of verbs in one of the sentences.1 The similar-
ity between the arguments of a verb pair (Av, Bv)
in V is measured as defined in Equation 7, where
ArgA and ArgB are the sets of labeled arguments
of the first and the second sentences and |ArgB| is
the number of arguments of the verb in B.2 The
argScore(Aarg, Barg) computation is based on the
cosine similarity as in Equation 1. We treat the to-
kens in the argument as a bag-of-words.
1This is inherited from the use of the metric focusing on re-
call in machine translation, where the B is the reference trans-
lation. In this work a better approach could be to compute this
metric twice, in both directions.
2Again, from the analogy of a recall metric for machine
translation.
3 Experiments and Results
We use the following state-of-the-art tools to pre-
process the data for feature extraction: i) Tree-
Tagger3 for lemmas and ii) SENNA (Collobert et
al., 2011)4 for Part-of-Speech tagging, Chunking,
Named Entity Recognition and Semantic Role La-
beling. SENNA has been reported to achieve an F-
measure of 75.79% for tagging semantic roles on the
CoNLL-2005 2 benchmark. The final feature set in-
cludes:
? Lexical metrics
? Cosine metric over bag-of-words
? Precision over content words
? Recall over content words
? F-measure over content words
? BLEU metric over chunks
? METEOR metric over words (with stems, syn-
onyms and paraphrases)
? Named Entity metric
? Semantic Role Labeling metric
The Machine Learning algorithm used for re-
gression is the LIBSVM5 Support Vector Machine
(SVM) implementation using the radial basis kernel
function. We used a simple genetic algorithm (Back
et al, 1999) to tune the parameters of the SVM. The
configuration of the genetic algorithm is as follows:
? Fitness function: minimize the mean squared
error found by cross-validation
? Chromosome: real numbers for SVM parame-
ters ?, cost and 
? Number of individuals: 80
? Number of generations: 100
? Selection method: roulette
? Crossover probability: 0.9
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
5http://www.csie.ntu.edu.tw/ cjlin/libsvm/
676
? Mutation probability: 0.01
We submitted three system runs, each is a varia-
tion of the above feature set. For the official submis-
sion we used the systems with optimized SVM pa-
rameters. We trained SVM models with each of the
following task datasets: MSRpar, MSRvid, SMT-
eur and the combination of MSRpar+MSRvid. For
each test dataset we applied their respective training
models, except for the new test sets, not covered by
any training set: for On-WN we used the combina-
tion MSRpar+MSRvid, and for SMT-news we used
SMT-eur.
Tables 1 to 3 focus on the Pearson correlation
of our three systems/runs for individual datasets of
the predicted scores against human annotation, com-
pared against the official baseline, which uses a sim-
ple word overlap metric. Table 4 shows the aver-
age results over all five datasets, where ALL stands
for the Pearson correlation with the gold standard
for the five dataset, Rank is the absolute rank among
all submissions, ALLnrm is the Pearson correlation
when each dataset is fitted to the gold standard us-
ing least squares, RankNrm is the corresponding
rank and Mean is the weighted mean across the five
datasets, where the weight depends on the number
of sentence pairs in the dataset.
3.1 Run 1: All except SRL features
Our first run uses the lexical, BLEU, METEOR and
Named Entities features, without the SRL feature.
Table 1 shows the results over the test set, where
Run 1-A is the version without SVM parameter op-
timization and Run 1-B are the official results with
optimized parameters for SVM.
Task Run 1-A Run 1-B Baseline
MSRpar 0.455 0.455 0.433
MSRvid 0.706 0.362 0.300
SMT-eur 0.461 0.307 0.454
On-WN 0.514 0.281 0.586
SMT-news 0.386 0.208 0.390
Table 1: Results for Run 1 using lexical, chunking,
named entities and METEOR as features. A is the non-
optimized version, B are the official results
3.2 Run 2: SRL feature
In this run we use only the SRL feature in order to
analyze whether this feature on its own could be suf-
ficient or lexical and other simpler features are im-
portant. Table 2 shows the results over the test set
without parameter optimization (Run 2-A) and the
official results with optimized parameters for SVM
(Run 2-B).
Task Run 2-A Run 2-B Baseline
MSRpar 0.335 0.300 0.433
MSRvid 0.264 0.291 0.300
SMT-eur 0.264 0.161 0.454
On-WN 0.281 0.257 0.586
SMT-news 0.189 0.221 0.390
Table 2: Results for Run 2 using the SRL feature only. A
is the non-optimized version, B are the official results
3.3 Run 3: All features
In the last run we use all features. Table 3 shows
the results over the test set without parameter opti-
mization (Run 3-A) and the official results with op-
timized parameters for SVM (Run 3-B).
Task Run 3-A Run 3-B Baseline
MSRpar 0.472 0.353 0.433
MSRvid 0.705 0.572 0.300
SMT-eur 0.471 0.307 0.454
On-WN 0.511 0.264 0.586
SMT-news 0.410 0.116 0.390
Table 3: Results for Run 3 using all features. A is the
non-optimized version, B are the official results
4 Discussion
Table 4 shows the ranking and normalized offi-
cial scores of our submissions compared against the
baseline. Our submissions outperform the official
baseline but significantly underperform the top sys-
tems in the shared task. The best system (Run 1)
achieved an above average ranking, but disappoint-
ingly the performance of our most complete system
(Run 3) using the semantic metric is poorer. Sur-
prisingly, the results of the non-optimized versions
outperform the optimized versions used in our offi-
cial submission. One possible reason for that is the
overfitting of the optimized models to the training
sets.
Run 1 and Run 3 have very similar results: the
overall correlation between all datasets of these two
systems is 0.98. One of the reasons for these results
is that the SRL metric is compromised by the length
677
System ALL Rank ALLnrm RankNrm Mean RankMean
Run 1 0.640 36 0.719 71 0.382 80
Run 2 0.536 59 0.629 88 0.257 88
Run 3 0.598 49 0.696 82 0.347 84
Baseline 0.311 87 0.673 85 0.436 70
Table 4: Official results and ranking over the test set for Runs 1-3 with SVM parameters optimized
of the sentences. In the MSRvid dataset, where the
sentences are simple such as ?Someone is drawing?,
resulting in a good semantic parsing, a high per-
formance for this metric is achieved. However, in
the SMT datasets, sentences are much longer (and
often ungrammatical, since they are produced by a
machine translation system) and the performance of
the metric drops. In addition, the SRL metric makes
mistakes such as judging as highly similar sentences
such as ?A man is peeling a potato? and ?A man is
slicing a potato?, where the arguments are the same
but the situations are different.
5 Conclusions
We have presented our systems based on similar-
ity scores as features to train a regression algorithm
to predict the semantic similarity between a pair
of sentences. Our official submissions outperform
the baseline method, but have lower performance
than most participants, and a simpler version of the
systems without any parameter optimization proved
more robust. Disappointingly, our main contribu-
tion, the addition of a metric based on Semantic Role
Labels shows no improvement as compared to sim-
pler metrics.
Acknowledgments
This work was supported by the Mexican National
Council for Science and Technology (CONACYT),
scholarship reference 309261.
References
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33?40, Barcelona,
Spain, July.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339?342, July.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. Cambridge, MA ; London,
May.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2011. Tine:
A metric to assess mt adequacy. Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
678
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116?122,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
TINE: A Metric to Assess MT Adequacy
Miguel Rios, Wilker Aziz and Lucia Specia
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton, WV1 1SB, UK
{m.rios, w.aziz, l.specia}@wlv.ac.uk
Abstract
We describe TINE, a new automatic evalua-
tion metric for Machine Translation that aims
at assessing segment-level adequacy. Lexical
similarity and shallow-semantics are used as
indicators of adequacy between machine and
reference translations. The metric is based on
the combination of a lexical matching com-
ponent and an adequacy component. Lexi-
cal matching is performed comparing bags-
of-words without any linguistic annotation.
The adequacy component consists in: i) us-
ing ontologies to align predicates (verbs), ii)
using semantic roles to align predicate argu-
ments (core arguments and modifiers), and
iii) matching predicate arguments using dis-
tributional semantics. TINE?s performance
is comparable to that of previous metrics
at segment level for several language pairs,
with average Kendall?s tau correlation from
0.26 to 0.29. We show that the addition of
the shallow-semantic component improves the
performance of simple lexical matching strate-
gies and metrics such as BLEU.
1 Introduction
The automatic evaluation of Machine Translation
(MT) is a long-standing problem. A number of met-
rics have been proposed in the last two decades,
mostly measuring some form of matching between
the MT output (hypothesis) and one or more human
(reference) translations. However, most of these
metrics focus on fluency aspects, as opposed to ad-
equacy. Therefore, measuring whether the meaning
of the hypothesis and reference translation are the
same or similar is still an understudied problem.
The most commonly used metrics, BLEU (Pap-
ineni et al, 2002) and alike, perform simple exact
matching of n-grams between hypothesis and refer-
ence translations. Such a simple matching proce-
dure has well known limitations, including that the
matching of non-content words counts as much as
the matching of content words, that variations of
words with the same meaning are disregarded, and
that a perfect matching can happen even if the order
of sequences of n-grams in the hypothesis and ref-
erence translation are very different, changing com-
pletely the meaning of the translation.
A number of other metrics have been proposed
to address these limitations, for example, by allow-
ing for the matching of synonyms or paraphrases
of content words, such as in METEOR (Denkowski
and Lavie, 2010). Other attempts have been made
to capture whether the reference translation and hy-
pothesis translations share the same meaning us-
ing shallow semantics, i.e., Semantic Role Labeling
(Gime?nez and Ma?rquez, 2007). However, these are
limited to the exact matching of semantic roles and
their fillers.
We propose TINE, a new metric that comple-
ments lexical matching with a shallow semantic
component to better address adequacy. The main
contribution of such a metric is to provide a more
flexible way of measuring the overlap between shal-
low semantic representations that considers both the
semantic structure of the sentence and the content
of the semantic elements. The metric uses SRLs
such as in (Gime?nez and Ma?rquez, 2007). However,
it analyses the content of predicates and arguments
seeking for either exact or ?similar? matches. The
116
inexact matching is based on the use of ontologies
such as VerbNet (Schuler, 2006) and distributional
semantics similarity metrics, such as Dekang Lin?s
thesaurus (Lin, 1998) .
In the remainder of this paper we describe some
related work (Section 2), present our metric - TINE
- (Section 3) and its performance compared to pre-
vious work (Section 4) as well as some further im-
provements. We then provide an analysis of these
results and discuss the limitations of the metric (Sec-
tion 5) and present conclusions and future work
(Section 6).
2 Related Work
A few metrics have been proposed in recent years
to address the problem of measuring whether a hy-
pothesis and a reference translation share the same
meaning. The most well-know metric is probably
METEOR (Banerjee and Lavie, 2005; Denkowski
and Lavie, 2010). METEOR is based on a general-
ized concept of unigram matching between the hy-
pothesis and the reference translation. Alignments
are based on exact, stem, synonym, and paraphrase
matches between words and phrases. However, the
structure of the sentences is not considered.
Wong and Kit (2010) measure word choice and
word order by the matching of words based on
surface forms, stems, senses and semantic similar-
ity. The informativeness of matched and unmatched
words is also weighted.
Liu et al (2010) propose to match bags of uni-
grams, bigrams and trigrams considering both recall
and precision and F-measure giving more impor-
tance to recall, but also using WordNet synonyms.
Tratz and Hovy (2008) use transformations in or-
der to match short syntactic units defined as Ba-
sic Elements (BE). The BE are minimal-length
syntactically well defined units. For example,
nouns, verbs, adjectives and adverbs can be con-
sidered BE-Unigrams, while a BE-Bigram could be
formed from a syntactic relation (e.g. subject+verb,
verb+object). BEs can be lexically different, but se-
mantically similar.
Pado? et al (2009) uses Textual Entailment fea-
tures extracted from the Standford Entailment Rec-
ognizer (MacCartney et al, 2006). The Textual En-
tailment Recognizer computes matching and mis-
matching features over dependency parses. The met-
ric then predicts the MT quality with a regression
model. The alignment is improved using ontologies.
He et al (2010) measure the similarity between
hypothesis and reference translation in terms of
the Lexical Functional Grammar (LFG) represen-
tation. The representation uses dependency graphs
to generate unordered sets of dependency triples.
Calculating precision, recall, and F-score on the
sets of triples corresponding to the hypothesis and
reference segments allows measuring similarity at
the lexical and syntactic levels. The measure also
matches WordNet synonyms.
The closest related metric to the one proposed in
this paper is that by Gime?nez and Ma?rquez (2007)
and Gime?nez et al (2010), which also uses shallow
semantic representations. Such a metric combines a
number of components, including lexical matching
metrics like BLEU and METEOR, as well as com-
ponents that compute the matching of constituent
and dependency parses, named entities, discourse
representations and semantic roles. However, the se-
mantic role matching is based on exact matching of
roles and role fillers. Moreover, it is not clear what
the contribution of this specific information is for the
overall performance of the metric.
We propose a metric that uses a lexical similar-
ity component and a semantic component in order
to deal with both word choice and semantic struc-
ture. The semantic component is based on seman-
tic roles, but instead of simply matching the surface
forms (i.e. arguments and predicates) it is able to
match similar words.
3 Metric Description
The rationale behind TINE is that an adequacy-
oriented metric should go beyond measuring the
matching of lexical items to incorporate information
about the semantic structure of the sentence, as in
(Gime?nez et al, 2010). However, the metric should
also be flexible to consider inexact matches of se-
mantic components, similar to what is done with lex-
ical metrics like METEOR (Denkowski and Lavie,
2010). We experiment with TINE having English
as target language because of the availability of lin-
guistic processing tools for this language. The met-
ric is particularly dependent on semantic role label-
117
ing systems, which have reached satisfactory perfor-
mance for English (Carreras and Ma?rquez, 2005).
TINE uses semantic role labels (SRL) and lexical se-
mantics to fulfill two requirements by: (i) compare
both the semantic structure and its content across
matching arguments in the hypothesis and refer-
ence translations; and (ii) propose alternative ways
of measuring inexact matches for both predicates
and role fillers. Additionally, it uses an exact lexi-
cal matching component to reward hypotheses that
present the same lexical choices as the reference
translation. The overall score s is defined using the
simple weighted average model in Equation (1):
s(H,R) = max
{
?L(H,R) + ?A(H,R)
?+ ?
}
R?R
(1)
where H represents the hypothesis translation, R
represents a reference translation contained in the set
of available references R; L defines the (exact) lex-
ical match component in Equation (2), A defines the
adequacy component in Equation (3); and ? and ?
are tunable weights for these two components. If
multiple references are provided, the score of the
segment is the maximum score achieved by compar-
ing the segment to each available reference.
L(H,R) =
|H
?
R|
?
|H| ? |R|
(2)
The lexical match component measures the over-
lap between the two representations in terms of the
cosine similarity metric. A segment, either a hypoth-
esis or a reference, is represented as a bag of tokens
extracted from an unstructured representation, that
is, bag of unigrams (words or stems). Cosine sim-
ilarity was chosen, as opposed to simply checking
the percentage of overlapping words (POW) because
cosine does not penalize differences in the length of
the hypothesis and reference translation as much as
POW. Cosine similarity normalizes the cardinality
of the intersection |H?R| using the geometric mean?
|H| ? |R| instead of the union |H?R|. This is par-
ticularly important for the matching of arguments -
which is also based on cosine similarity. If an hy-
pothesized argument has the same meaning as its
reference translation, but differs from it in length,
cosine will penalize less the matching than POW.
That is specially interesting when core arguments
get merged with modifiers due to bad semantic role
labeling (e.g. [A0 I] [T bought] [A1 something to eat
yesterday] instead of [A0 I] [T bought] [A1 some-
thing to eat] [AM-TMP yesterday]).
A(H,R) =
?
v?V verb score(Hv, Rv)
|Vr|
(3)
In the adequacy component, V is the set of verbs
aligned between H and R, and |Vr| is the number of
verbs in R. Hereafter the indexes h and r stand for
hypothesis and reference translations, respectively.
Verbs are aligned using VerbNet (Schuler, 2006) and
VerbOcean (Chklovski and Pantel, 2004). A verb in
the hypothesis vh is aligned to a verb in the refer-
ence vr if they are related according to the follow-
ing heuristics: (i) the pair of verbs share at least one
class in VerbNet; or (ii) the pair of verbs holds a re-
lation in VerbOcean.
For example, in VerbNet the verbs spook and ter-
rify share the same class amuse-31.1, and in VerbO-
cean the verb dress is related to the verb wear.
verb score(Hv, Rv) =
?
a?Ar?At
arg score(Ha, Ra)
|Ar|
(4)
The similarity between the arguments of a verb
pair (vh, vr) in V is measured as defined in Equa-
tion (4), where Ah and At are the sets of labeled
arguments of the hypothesis and the reference re-
spectively and |Ar| is the number of arguments of
the verb in R. In other words, we only measure the
similarity of arguments in a pair of sentences that are
annotated with the same role. This ensures that the
structure of the sentence is taken into account (for
example, an argument in the role of agent would not
be compared against an argument in a role of experi-
encer). Additionally, by restricting the comparison
to arguments of a given verb pair, we avoid argument
confusion in sentences with multiple verbs.
The arg score(Ha, Ra) computation is based on
the cosine similarity as in Equation (2). We treat
the tokens in the argument as a bag-of-words. How-
ever, in this case we change the representation of
the segments. If the two sets do not match exactly,
we expand both of them by adding similar words.
For every mismatch in a segment, we retrieve the
118
20-most similar words from Dekang Lin?s distribu-
tional thesaurus (Lin, 1998), resulting in sets with
richer lexical variety.
The following example shows how the computa-
tion of A(H,R) is performed, considering the fol-
lowing hypothesis and reference translations:
H: The lack of snow discourages people from ordering
ski stays in hotels and boarding houses.
R: The lack of snow is putting people off booking ski
holidays in hotels and guest houses.
1. extract verbs from H: Vh = {discourages, ordering}
2. extract verbs from R: Vr = {putting, booking}
3. similar verbs aligned with VerbNet (shared class
get-13.5.1): V = {(vh = order,vr = book)}
4. compare arguments of (vh = order,vr = book):
Ah = {A0, A1, AM-LOC}
Ar = {A0, A1, AM-LOC}
5. Ah ?Ar = {A0, A1, AM-LOC}
6. exact matches:
HA0 = {people} and RA0 = {people}
argument score = 1
7. different word forms: expand the representation:
HA1 = {ski, stays} and RA1 = {ski, holidays}
expand to:
HA1 = {{ski},{stays, remain... journey...}}
RA1 = {{ski},{holidays, vacations, trips... jour-
ney...}}
argument score = 0.5
8. similarly to HAM?LOC and RAM?LOC
argument score = 0.72
9. verb score (order, book) = 1+0.5+0.723 = 0.74
10. A(H,R) = 0.742 = 0.37
Different from previous work, we have not used
WordNet to measure lexical similarity for two main
reasons: problems with lexical ambiguity and lim-
ited coverage in WordNet (instances of named enti-
ties are not in WordNet, e.g. Barack Obama). For
example, in WordNet the aligned verbs (order/book)
from the previous hypothesis and reference trans-
lations have: 9 senses - order (e.g. give instruc-
tions to or direct somebody to do something with
authority, make a request for something, etc.) - and
4 senses - book (engage for a performance, arrange
for and reserve (something for someone else) in ad-
vance, etc.). Thus, a WordNet-based similarity mea-
sure would require disambiguating segments, an ad-
ditional step and a possible source of errors. Second,
a thresholds would need to be set to determine when
a pair of verbs is aligned. In contrast, the structure of
VerbNet (i.e. clusters of verbs) allows a binary deci-
sion, although the VerbNet heuristic results in some
errors, as we discuss in Section 5.
4 Results
We set the weights ? and ? by experimental test-
ing to ? = 1 and ? = 0.25. The lexical component
weight is prioritized because it has shown a good av-
erage Kendall?s tau correlation (0.23) on a develop-
ment dataset (Callison-Burch et al, 2010). Table 1
shows the correlation of the lexical component with
human judgments for a number of language pairs.
Table 1: Kendall?s tau segment-level correlation of the
lexical component with human judgments
Metric cz-en fr-en de-en es-en avg
Lexical 0.27 0.21 0.26 0.19 0.23
We use the SENNA1 SRL system to tag the
dataset with semantic roles. SENNA has shown to
have achieved an F-measure of 75.79% for tagging
semantic roles over the CoNLL 2005 2 benchmark.
We compare our metric against standard BLEU
(Papineni et al, 2002), METEOR (Denkowski and
Lavie, 2010) and other previous metrics reported in
(Callison-Burch et al, 2010) which also claim to use
some form of semantic information (see Section 2
for their description). The comparison is made in
terms of Kendall?s tau correlation against the human
judgments at a segment-level. For our submission to
the shared evaluation task, system-level scores are
obtained by averaging the segment-level scores.
TINE achieves the same average correlation with
BLUE, but outperforms it for some language pairs.
Additionally, TINE outperforms some of the previ-
ous which use WordNet to deal with synonyms as
part of the lexical matching.
The closest metric to TINE (Gime?nez et al,
2010), which also uses semantic roles as one of its
1http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
119
Table 2: Comparison with previous semantically-
oriented metrics using segment-level Kendall?s tau cor-
relation with human judgments
Metric cz-en fr-en de-en es-en avg
(Liu et al,
2010)
0.34 0.34 0.38 0.34 0.35
(Gime?nez
et al, 2010)
0.34 0.33 0.34 0.33 0.33
(Wong and
Kit, 2010)
0.33 0.27 0.37 0.32 0.32
METEOR 0.33 0.27 0.36 0.33 0.32
TINE 0.28 0.25 0.30 0.22 0.26
BLEU 0.26 0.22 0.27 0.28 0.26
(He et al,
2010)
0.15 0.14 0.17 0.21 0.17
(Tratz
and Hovy,
2008)
0.05 0.0 0.12 0.05 0.05
components, achieves better performance. However,
this metric is a rather complex combination of a
number of other metrics to deal with different lin-
guistic phenomena.
4.1 Further Improvements
As an additional experiment, we use BLEU as the
lexical component L(H,R) in order to test if the
shallow-semantic component can contribute to the
performance of this standard evaluation metric. Ta-
ble 3 shows the results of the combination of BLEU
and the shallow-semantic component using the same
parameter configuration as in Section 4. The addi-
tion of the shallow-semantic component increased
the average correlation of BLEU from 0.26 to 0.28.
Table 3: TINE-B: Combination of BLEU and the
shallow-semantic component
Metric cz-en fr-en de-en es-en avg
TINE-B 0.27 0.25 0.30 0.30 0.28
Finally, we improve the tuning of the weights of
the components (? and ? parameters) by using a
simple genetic algorithm (Back et al, 1999) to se-
lect the weights that maximize the correlation with
human scores on a development set (we use the de-
velopment sets from WMT10 (Callison-Burch et al,
2010)). The configuration of the genetic algorithm
is as follows:
? Fitness function: Kendall?s tau correlation
? Chromosome: two real numbers, ? and ?
? Number of individuals: 80
? Number of generations: 100
? Selection method: roulette
? Crossover probability: 0.9
? Mutation probability: 0.01
Table 4 shows the parameter values obtaining
from tuning for each language pair and the corre-
lation achieved by the metric with such parameters.
With such an optimization step the average correla-
tion of the metric increases to 0.29.
Table 4: Optimized values of the parameters using a ge-
netic algorithm and Kendall?s tau and final correlation of
the metric on the test sets
Language pair Correlation ? ?
cz-en 0.28 0.62 0.02
fr-en 0.25 0.91 0.03
de-en 0.30 0.72 0.1
es-en 0.31 0.57 0.02
avg 0.29 ? ?
5 Discussion
In what follows we discuss with a few examples
some of the common errors made by TINE. Over-
all, we consider the following categories of errors:
1. Lack of coverage of the ontologies.
R: This year, women were awarded the Nobel Prize in all
fields except physics
H: This year the women received the Nobel prizes in all
categories less physical
The lack of coverage in VerbNet prevented the
detection of the similarity between receive and
award.
2. Matching of unrelated verbs.
R: If snow falls on the slopes this week, Christmas will
sell out too, says Schiefert.
H: If the roads remain snowfall during the week, the dates
of Christmas will dry up, said Schiefert.
In VerbOcean remain and say are incorrectly
120
said to be related. VerbOcean was cre-
ated by a semi-automatic extraction algorithm
(Chklovski and Pantel, 2004) with an average
accuracy of 65.5%.
3. Incorrect tagging of the semantic roles by
SENNA.
R: Colder weather is forecast for Thursday, so if anything
falls, it should be snow.
H: On Thursday , must fall temperatures and, if there is
rain, in the mountains should.
The position of the predicates affects the SRL
tagging. The predicate fall has the following
roles (A1, V, and S-A1) in the reference, and
the following roles (AM-ADV, A0, AM-MOD,
and AM-DIS) in the hypothesis. As a con-
sequence, the metric cannot attempt to match
the fillers. Also, SRL systems do not detect
phrasal verbs such as in the example of Section
3, where the action putting people off is similar
to discourages.
6 Conclusions and Future Work
We have presented an MT evaluation metric based
on the alignment of semantic roles and flexible
matching of role fillers between hypothesis and ref-
erence translations. To deal with inexact matches,
the metric uses ontologies and distributional seman-
tics, as opposed to lexical databases like WordNet,
in order to minimize ambiguity and lack of cover-
age. The metric also uses an exact lexical matching
component to reward hypotheses that present lexical
choices similar to those of the reference translation.
Given the simplicity of the metric, it has achieved
competitive results. We have shown that the addition
of the shallow-semantic component into a lexical
component yields absolute improvements in the cor-
relation of 3%-6% on average, depending on the lex-
ical component used (cosine similarity or BLEU).
In future work, in order to improve the perfor-
mance of the metric we plan to add components to
address a few other linguistic phenomena such as
in (Gime?nez and Ma?rquez, 2007; Gime?nez et al,
2010). In order to deal with the coverage problem
of an ontology, we plan to use distributional seman-
tics (i.e. word space models) also to align the pred-
icates. We consider using a backoff model for the
shallow-semantic component to deal with the very
frequent cases where there are no comparable pred-
icates between the reference and hypothesis transla-
tions, which result in a 0 score from the semantic
component. Finally, we plan to improve the lexical
component to better tackle fluency, for example, by
adding information about the word order.
References
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July.
Xavier Carreras and Llu??s Ma?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of the 9th Conference on Natural Lan-
guage Learning, CoNLL-2005, Ann Arbor, MI USA.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33?40, Barcelona,
Spain, July.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339?342, July.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 256?
264, Stroudsburg, PA, USA.
Jesu?s Gime?nez, Llu??s Ma?rquez, Elisabet Comelles, Irene
Castello?n, and Victoria Arranz. 2010. Document-
level automatic mt evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, WMT ?10, pages 333?338, Stroudsburg, PA,
USA.
121
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 349?353, Strouds-
burg, PA, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Tesla: translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, WMT ?10, pages 354?359,
Stroudsburg, PA, USA.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the NAACL, pages 41?48,
New York City, USA, June.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23:181?193, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Stephen Tratz and Eduard Hovy. 2008. Summarisation
evaluation using transformed basic elements. In Pro-
ceedings TAC 2008.
Billy T.-M. Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, WMT ?10, pages 360?
364, Stroudsburg, PA, USA.
122
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 316?322,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Shallow Semantic Trees for SMT
Wilker Aziz, Miguel Rios and Lucia Specia
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton, WV1 1SB, UK
{w.aziz, m.rios, l.specia}@wlv.ac.uk
Abstract
We present a translation model enriched with
shallow syntactic and semantic information
about the source language. Base-phrase la-
bels and semantic role labels are incorporated
into an hierarchical model by creating shal-
low semantic ?trees?. Results show an in-
crease in performance of up to 6% in BLEU
scores for English-Spanish translation over a
standard phrase-based SMT baseline.
1 Introduction
The use of semantic information to improve Statis-
tical Machine Translation (SMT) is a very recent re-
search topic that has been attracting significant at-
tention. In this paper we describe our participation
in the shared translation task of the 6th Workshop on
Statistical Machine Translation (WMT) with a sys-
tem that incorporates shallow syntactic and semantic
information into hierarchical SMT models.
The system is based on the Moses toolkit (Hoang
et al, 2009; Koehn et al, 2007) using hierarchi-
cal models informed with shallow syntactic (chunks)
and semantic (semantic role labels) information for
the source language. The toolkit SENNA (Collobert
et al, 2011) is used to provide base-phrases (chunks)
and semantic role labels.
Experiments with English-Spanish and English-
German news datasets show promising results and
highlight important issues about the use of seman-
tic information in hierarchical models as well as a
number of possible directions for further research.
The remaining of the paper is organized as fol-
lows: Section 2 presents related work; Section 3 de-
scribes the method; Section 4 presents the results ob-
tained for the English-Spanish and English-German
translation tasks; and Section 5 brings some conclu-
sions and directions for further research.
2 Related Work
In hierarchical SMT (Chiang, 2005), a Synchronous
Context Free Grammar (SCFG) is learned from a
parallel corpus.The model capitalizes on the recur-
sive nature of language replacing sub-phrases by
an unlabeled nonterminal. Hierarchical models are
known to produce high coverage rules, once they are
only constrained by the word alignment. Neverthe-
less the lack of specialized vocabulary also leads to
spurious ambiguity (Chiang, 2005).
Syntax-based models are hierarchical models
whose rules are constrained by syntactic informa-
tion.The syntactic constraints have an impact in
the rule extraction process, reducing drastically the
number of rules available to the system. While this
may be helpful to reduce ambiguity, it can lead to
poorer performance (Ambati and Lavie, 2008).
Motivated by the fact that syntactically constrain-
ing a hierarchical model can decrease translation
quality, some attempts to overcome the problems
at rule extraction time have been made. Venugopal
and Zollmann (2006) propose a heuristic method to
relax parse trees known as Syntax Augmented Ma-
chine Translation (SAMT). Significant gains are ob-
tained by grouping nonterminals under categories
when they do not span across syntactic constituents.
Hoang and Koehn (2010) propose a soft syntax-
based model which combines the precision of a
syntax-constrained model with the coverage of an
316
unconstrained hierarchical model. Instead of hav-
ing heuristic strategies to combine nonterminals in a
parse tree, whenever a rule cannot be retrieved be-
cause it does not span a constituent, the extraction
procedure falls back to the hierarchical approach, re-
trieving a rule with unlabeled nonterminals. Perfor-
mance gains are reported over standard hierarchical
models using both full parse trees and shallow syn-
tax.
Moving beyond syntactic information, some at-
tempts have recently been made to add semantic an-
notations to SMT. Wu and Fung (2009) present a
two-pass model to incorporate semantic information
to the phrase-based SMT pipeline. The method per-
forms conventional translation in a first step, fol-
lowed by a constituent reordering step seeking to
maximize the cross-lingual match of the semantic
role labels of the translation and source sentences.
Liu and Gildea (2010) add features extracted from
the source sentences annotated with semantic role
labels in a tree-to-string SMT model. They mod-
ify a syntax-based SMT system in order to penal-
ize/reward role reordering and role deletion. The
input sentence is parsed for semantic roles and the
roles are then projected onto the target side using
word alignment information at decoding time. They
assume that a one-to-one mapping between source
and target roles is desirable.
Baker et al (2010) propose to graft semantic in-
formation, namely named entities and modalities, to
syntactic tags in a syntax-based model. The vocab-
ulary of nonterminals is specialized using the se-
mantic categories, for instance, a noun phrase (NP)
whose head is a geopolitical entity (GPE) will be
tagged as NPGPE, making the rule table less am-
biguous.
Similar to (Baker et al, 2010) we specialize a vo-
cabulary of syntactic nonterminals with semantic in-
formation, however we use shallow syntax (base-
phrases) and semantic role labels instead of con-
stituent parse and named entities. The resulting shal-
low trees are relaxed following SAMT (Venugopal
and Zollmann, 2006). Different from previous work
we add the semantic knowledge at the level of the
corpus annotation. As a consequence, instead of bi-
asing deletion and reordering through additional fea-
tures (Liu and Gildea, 2010), we learn hierarchical
rules that encode those phenomena, taking also into
account the semantic role of base-phrases.
3 Proposed Method
The proposed method is based on an extension of the
hierarchical models in Moses using source language
information. Our submission included systems for
two language pairs: English-Spanish (en-es) and
English-German (en-de) and was constrained to us-
ing data provided by WMT11. Phrase and rule ex-
traction were performed using the entire en-es and
en-de portions of Europarl. Model parameters were
tuned using the news-test2008 dataset. Three 5-
gram Spanish and German language models were
trained using SRILM1 with the News Commentaries
(? 160K sentences), Europarl (? 2M sentences)
and News (? 5M sentences) corpora. These models
were interpolated using scripts provided in Moses
(Koehn and Schroeder, 2007).
At pre-processing stage, sentences longer than 80
tokens were filtered from the training/development
corpus. The parallel corpus was then tokenized and
truecased. Additionally, for en-de, compound split-
ting of the German side of the corpus was performed
using a frequency based method described in (Koehn
and Knight, 2003). This method helps alleviate spar-
sity, reducing the size of the vocabulary by decom-
posing compounds into their base words. Recas-
ing and detokenization, along with compound merg-
ing of the translations into German, were handled
at post-processing stage. Compound merging was
performed by finding the most likely sequences of
words to be merged into previously seen compounds
(Stymne, 2009).
3.1 Source Language Annotation
For rule extraction, training and test, the English side
of the corpus was annotated with Semantic Role La-
bels (SRL) using the toolkit SENNA2, which also
outputs POS and base-phrase (without prepositional
attachment) tags. The resulting source language an-
notation was used to produce trees in order to build
a tree-to-string model in Moses.
1http://www.speech.sri.com/projects/
srilm/
2http://ml.nec-labs.com/senna/
317
S
NP VP NP PP NP O O NP VP NP ADVP
PRP VBZ TO VB DT NN TO NN PUNC CC PRP VBZ RB VBD WDT RB
he intends to donate this money to charity , but he has not decided which yet
Figure 1: Example of POS tags and base-phrase annotation. Base-phrases: noun-phrase (NP), verb-phrase
(VP), prepositional-phrase (PP), adverbial-phrase (ADVP), outside-of-a-phrase (O)
In order to derive trees for the source side of the
corpus from this annotation, a new level is created to
add the POS tags for each word form. Syntactic tags
are then added by grouping words and POS tags into
base phrases using linguistic information as given
by SENNA. Figure 1 shows an example of an input
sentence annotated with POS and base-phrase infor-
mation. Additionally, SRLs are used to enrich the
POS and base-phrase annotation levels. Semantic
roles are assigned to each predicate independently.
As a consequence, the resulting annotation cannot
be considered a tree and there is not an obvious hi-
erarchy of predicates in a sentence. For example,
Figure 2 shows the SRL annotation for the example
in Figure 1.
[A0 He] [T intends] [A1 to donate this money to charity],
but he has not decided which yet
[A0 He] intends to [T donate] [A1 this money] [A2 to
charity], but he has not decided which yet
He intends to donate this money to charity, but [A0 he]
has [AM-NEG not] [T decided] [A1 which] [AM-TMP
yet]
Figure 2: SRL for sentence in Figure 1
Arguments of a single predicate never overlap,
however in longer sentences, the occurrence of mul-
tiple verbs increases the chances that arguments of
different predicates overlap, that is, the argument of
a verb might contain or even coincide with the argu-
ment of another verb and depending on the verb the
argument role might change. For example, in Fig-
ure 2: i) He is both the agent of intend and donate;
ii) this money is the donated thing and also part of
the chunk which express the intention (to donate this
money to charity). In a different example we can see
that arguments might overlap and their roles change
completely depending on their target predicates (e.g
in I gave you something to eat, you is the recipient
of the verb give and the agent of the verb eat). For
this reason, why semantic role labels are usually an-
notated individually in different structures, as shown
in Figure 2, each annotation focusing on a single tar-
get verb. In order to convert the predicates and argu-
ments of a sentence into a single tree, we enrich the
POS-tags and base-phrase annotation as follows:
? Semantic labels are directly grafted to the base-
phrase annotation whenever possible, that is,
if a predicate argument coincides with a sin-
gle base-phrase, the base-phrase type is spe-
cialized with the argument role. In Figure 3,
the noun-phrase (NP) the money is specialized
into NP:A1:donate, since that single NP is the
argument A1 of donate.
? If a predicate argument groups multiple base-
phrases, the semantic label applies to a node in
a new level of the tree subsuming all these base-
phrases. In Figure 3, the base-phrases to (PP)
and charity (NP) are grouped by A2:donate.
? We add the labels sequentially from the short-
est chunks to the largest ones. If two la-
bels spanning the same number of tokens: i)
overlap completely, we merge them so that
no hierarchy is imposed between their targets
(e.g. in Figure 3, the noun-phrase He is spe-
cialized into NP:A0:donate,intend); ii) over-
lap partially, we merge them so that the re-
sulting label will compete against other labels
in a different length category. If a label span-
ning a larger chunk overlaps partially with a
label spanning a shorter chunk, or contains it,
we stack them in a way that the first subsumes
the second (e.g in Figure 3, A1:intend sub-
sumes VP:T:donate, NP:A1:donate,intend and
A2:donate).
? Verb phrases might get split if they contain
multiple target predicates (e.g. in Figure 3,
the VP intends to donate is split into two verb-
318
phrases, each specialized with its own role la-
bel).
? Finally, tags are lexicalized, that is, semantic
labels are composed by their type (e.g. A0) and
target predicate lemma (verb).
Figure 3 shows and example of how semantic la-
bels are combined with shallow syntax in order to
produce the input tree for the sentence in Figure
1. The argument A1 of intend subsumes the target
verb donate and its arguments A1 and A2; A2:donate
groups base-phrases so as to attach the preposition to
the noun phrase.
Finally, following the method for syntactic trees
by Venugopal and Zollmann (2006), the input trees
are relaxed in order to alleviate the impact of the
linguistic constraints on rule extraction. We relax
trees3 by combining any pairs of neighboring nodes.
For example, NP:A0:donate,intend+VP:T:intend
and NP:A1:donate+A2:donate are created for the
tree in Figure 3.
4 Results
As a baseline to compare against our proposed ap-
proach (srl), we took a phrase-based SMT system
(pb) built using the Moses toolkit with the same
datasets and training conditions described in Sec-
tion 3. The results are reported in terms of standard
BLEU (Papineni et al, 2002) (and its case sensitive
version, BLEU-c) and tested for statistical signifi-
cance using an approximate randomization test (Rie-
zler and Maxwell, 2005) with 100 iterations.
In addition, we included an intermediate model
between these two: a hierarchical model in-
formed with source-language base-phrase informa-
tion (chunk). For the English-Spanish task we also
built a purely hierarchical model (hier) using Moses
and the same datasets and training conditions. For
the English-German task, hierarchical models have
not been shown to outperform standard phrase-based
models in previous work (Koehn et al, 2010).
Table 1 shows the performance achieved for the
English-Spanish translation task test set, where (srl)
is our official submission. One can notice a signifi-
cant gain in performance (up to 6% BLEU) in using
tree-based models (with or without source language
3Using the Moses implementation relax-parse for SAMT 2
annotation) as opposed to using standard phrase-
based models.
Model BLEU BLEU-c
pb 0.2429 0.2340
srl 0.2901 0.2805
hier 0.3029 0.2933
chunk 0.3034 0.2935
Table 1: English-Spanish experiments - differences
between all pairs of models are statistically signifi-
cant with 99% confidence, except for the pair (hier,
chunk)
The purely hierarchical approach performs as
well as our linguistically informed tree-based mod-
els (chunk and srl). On the one hand this finding
is somewhat disappointing as we expected that tree-
based models would benefit from linguistic annota-
tion. On the other hand it shows that the linguistic
annotation yields a significant reduction in the num-
ber of unnecessary productions: the linguistically in-
formed models are much smaller than hier (Table
5), but perform just as well. Whether the linguistic
annotation significantly helps make the productions
less ambiguous or not is still a question to be ad-
dressed in further experimentation.
Table 2 shows the performance achieved for the
English-German translation task test set. These re-
sults indicate that the linguistic information did not
lead to any significant gains in terms of automatic
metrics. An in-depth comparative analysis based on
a manual inspection of the translations remains to be
done.
Model BLEU BLEU-c
pb 0.1398 0.1360
srl 0.1381 0.1344
chunk 0.1403 0.1367
Table 2: English-German experiments - differences
between pairs of models are not statistically signifi-
cant
In Table 3 we also show the impact of three com-
pound merging strategies as post-processing for en-
de: i) no compound merging (nm), ii) frequency-
based compound merging (fb), and iii) frequency-
319
SNP:A0:donate,intend
PRP
He
VP:T:intend
VBZ
intends
A1:intend
VP:T:donate
TO
to
VB
donate
NP:A1:donate
DT
this
NN
money
A2:donate
PP
TO
to
NP
NN
charity
...
Figure 3: Tree for example in Figure 1
based compound merging constrained by POS4
(cfb). Applying both frequency-based compound
merging strategies (Stymne, 2009) resulted in sig-
nificant improvements of nearly 0.5% in BLEU.
Model BLEU BLEU-c
nm 0.1334 0.1298
fb 0.1369 0.1332
cfb 0.1381 0.1344
Table 3: English-German compound merging - dif-
ferences between all pairs of models are statistically
significant with 99% confidence
Another somewhat disappoint result is the perfor-
mance of srl when compared to chunk. We believe
the main reason why the chunk models outperform
the srl models is data sparsity. The semantic infor-
mation, and particularly the way it was used in this
paper, with lexicalized roles, led to a very sparse
model. As an attempt to make the srl model less
sparse, we tested a version of this model without
lexicalizing the semantic tags, in other words, us-
ing the semantic role labels only, for example, A1
instead of A1:intend in Figure 3. Table 4 shows that
models with lexicalized semantic roles (lex) consis-
tently outperform the alternative version (non lex),
although the differences were only statistically sig-
nificant for the en-de dataset. One reason for that
may be that non-lexicalized rules do not help mak-
4POS tagging was performed using the TreeTagger toolkit:
http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
ing the chunk rules less ambiguous.
Model BLEU BLEU-c
en-esnon lex 0.2891 0.2795
en-eslex 0.2901 0.2805
en-denon lex 0.1319 0.1284
en-delex 0.1381 0.1344
Table 4: Alternative model with non-lexicalized tags
- differences are statistically significant with 99%
confidence for en-de only
Table 5 shows how the additional annotation con-
strains the rule extraction (for the en-es dataset). The
unconstrained model hier presents the largest rule
table, followed by the chunk model, which is only
constrained by syntactic information. The models
enriched with semantic labels, both the lexicalized
or non-lexicalized versions, contain a comparable
number of rules. They are at least half the size of
the chunk model and about 9 times smaller than the
hier model. However, the number of nonterminals
in the lexicalized models highlights the sparsity of
such models.
Model Rules Nonterminals
hier 962,996,167 1
chunk 235,910,731 3,390
srlnon lex 92,512,493 44,095
srllex 117,563,878 3,350,145
Table 5: Statistics from the rule table
In order to exemplify the importance of having
320
some form of lexicalized information as part of the
semantic models, Figure 4 shows two predicates
which present different semantic roles, even though
they have nearly the same shallow syntactic struc-
ture. In this case, unless lexicalized, rules map-
ping semantic roles into base-phrases become am-
biguous. Besides, the same role might appear sev-
eral times in the same sentence (Figure 2). In this
case, if the semantic roles are not annotated with
their target lemma, they bring additional confusion.
Therefore, the model needs the lexical information
to distinguish role deletion and reordering phenom-
ena across predicates.
Figure 4: Different SRL for similar chunks
[NP:A0 I] [VP:T gave] [NP:A2 you] [NP:A1 a car]
[NP:A0 I] [VP:T dropped] [NP:A1 the glass] [AM-LOC
[PP on] [NP the floor]]
In WMT11?s official manual evaluation, our sys-
tem submissions (srl) were ranked 10th out of 15
systems in the English-Spanish task, and 18th out
of 22 systems participating in the English-German
task. For detailed results refer to the overview paper
of the Shared Translation Task of the Sixth Work-
shop on Machine Translation (WMT11).
5 Conclusions
We have presented an effort towards using shal-
low syntactic and semantic information for SMT.
The model based on shallow syntactic information
(chunk annotation) has significantly outperformed a
baseline phrase-based model and performed as well
as a hierarchical phrase-based model with a signifi-
cantly smaller number of translation rules.
While annotating base-phrases with semantic la-
bels is intuitively a promising research direction, the
current model suffers from sparsity and representa-
tion issues resulting from the fact that multiple pred-
icates share arguments within a given sentence. As
a consequence, shallow semantics has not yet shown
improvements with respect to the chunk-based mod-
els.
In future work, we will address the sparsity is-
sues in the lexicalized semantic models by cluster-
ing predicates in a way that semantic roles can be
specialized with semantic categories, instead of the
verb lemmas.
References
Vamshi Ambati and Alon Lavie. 2008. Improving syntax
driven translation models by re-structuring divergent
and non-isomorphic parse tree structures. In The Eight
Conference of the Association for Machine Translation
in the Americas (AMTA).
Kathryn Baker, Michael Bloodgood, Chris Callison-
burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori
Levin, Scott Miller, and Christine Piatko. 2010.
Semantically-informed syntactic machine translation:
A tree-grafting approach.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceeding of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
arXiv:1103.0398v1.
Hieu Hoang and Philipp Koehn. 2010. Improved trans-
lation with source syntax labels. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 409?417.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A
unified framework for phrase-based, hierarchical, and
syntax-based statistical machine translation. In Pro-
ceedings of International Workshop on Spoken Lan-
guage Translation, pages 152 ? 159.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
tenth conference on European chapter of the Associ-
ation for Computational Linguistics - Volume 1, pages
187?193.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 224?227.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-
lison Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In 45th An-
nual Meeting of the Association for Computational
Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu
Hoang. 2010. More linguistic annotation for statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, pages 115?120.
321
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 716?724.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for mt. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, Work-
shop in Intrinsic and Extrinsic Evaluation Measures
for MT and Summarization.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of german compounds. In Proceed-
ings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics: Stu-
dent Research Workshop, pages 61?69.
Ashish Venugopal and Andreas Zollmann. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: a hybrid two-pass model. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 13?16.
322
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 472?483,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Investigations in Exact Inference for Hierarchical Translation
Wilker Aziz?, Marc Dymetman?, Sriram Venkatapathy?
?University of Wolverhampton, Wolverhampton, UK
?Xerox Research Centre Europe, Grenoble, France
?w.aziz@wlv.ac.uk, ?{first.last}@xrce.xerox.com
Abstract
We present a method for inference in hi-
erarchical phrase-based translation, where
both optimisation and sampling are per-
formed in a common exact inference
framework related to adaptive rejection
sampling. We also present a first imple-
mentation of that method along with ex-
perimental results shedding light on some
fundamental issues. In hierarchical transla-
tion, inference needs to be performed over
a high-complexity distribution defined by
the intersection of a translation hypergraph
and a target language model. We replace
this intractable distribution by a sequence
of tractable upper-bounds for which exact
optimisers and samplers are easy to obtain.
Our experiments show that exact inference
is then feasible using only a fraction of the
time and space that would be required by
the full intersection, without recourse to
pruning techniques that only provide ap-
proximate solutions. While the current im-
plementation is limited in the size of inputs
it can handle in reasonable time, our exper-
iments provide insights towards obtaining
future speedups, while staying in the same
general framework.
1 Introduction
In statistical machine translation (SMT), optimi-
sation ? the task of searching for an optimum
translation ? is performed over a high-complexity
distribution defined by the intersection between a
translation hypergraph and a target language model
(LM). This distribution is too complex to be repre-
sented exactly and one typically resorts to approx-
imation techniques such as beam-search (Koehn et
al., 2003) and cube-pruning (Chiang, 2007), where
maximisation is performed over a pruned represen-
tation of the full distribution.
Often, rather than finding a single optimum, one
is really interested in obtaining a set of proba-
bilistic samples from the distribution. This is the
case for minimum error rate training (Och, 2003;
Watanabe et al, 2007), minimum risk training
(Smith and Eisner, 2006) and minimum risk de-
coding (Kumar and Byrne, 2004). Due to the ad-
ditional computational challenges posed by sam-
pling, n-best lists, a by-product of optimisation, are
typically used as approximation to true probabilis-
tic samples. A known issue with n-best lists is that
they tend to be clustered around only one mode of
the distribution. A more direct procedure is to at-
tempt to directly draw samples from the underlying
distribution rather than rely on n-best list approxi-
mations (Arun et al, 2009; Blunsom and Osborne,
2008).
OS? (Dymetman et al, 2012a) is a recent ap-
proach that stresses a unified view between the two
types of inference, optimisation and sampling. In
this view, rather than resorting to pruning in or-
der to cope with the tractability issues, one upper-
bounds the complex goal distribution with a sim-
pler ?proposal? distribution for which dynamic
programming is feasible. This proposal is incre-
mentally refined to be closer to the goal until the
maximum is found, or until the sampling perfor-
mance exceeds a certain level.
This paper applies the OS? approach to the
problem of inference in hierarchical SMT (Chi-
ang, 2007). In a nutshell, the idea is to replace
the intractable problem of intersecting a context-
free grammar with a full language model by the
tractable problem of intersecting it with a simpli-
fied, optimistic version of this LM which ?forgets?
parts of n-gram contexts, and to incrementally add
more context based on evidence of the need to do
so. Evidence is gathered by optimising or sampling
from the tractable proxy distribution and focussing
on the most serious over-optimistic estimates rela-
tive to the goal distribution.
472
Our main contribution is to provide an exact op-
timiser/sampler for hierarchical SMT that is effi-
cient in exploring only a small fraction of the space
of n-grams involved in a full intersection. Al-
though at this stage our experiments are limited to
short sentences, they provide insights on the be-
havior of the technique and indicate directions to-
wards a more efficient implementation within the
same paradigm.
The paper is organized as follows: ?2 provides
background on OS? and hierarchical translation; ?3
describes our approach to exact inference in SMT;
in ?4 the experimental setup is presented and find-
ings are discussed; ?5 discusses related work, and
?6 concludes.
2 Background
2.1 OS?
The OS? approach (Dymetman et al, 2012a;
Dymetman et al, 2012b) proposes a unified view
of exact inference in sampling and optimisation,
where the two modalities are seen as extremes in a
continuum of inference tasks in Lp spaces (Rudin,
1987), with sampling associated with the L1 norm,
and optimisation with the L? norm.
The objective function p, over which inference
needs to be performed, is a complex non-negative
function over a discrete or continuous space X ,
which defines an unnormalised distribution over
X . The goal is to optimise or sample relative to
p ? where sampling is interpreted in terms of the
normalised distribution p?(.) = p(.)/ ?X p(x)dx.
Directly optimising or sampling from p is unfea-
sible; however, it is possible to define an (unnor-
malized) distribution q of lower complexity than
p, which upper-bounds p everywhere (ie. p(x) ?
q(x), ?x ? X), and from which it is feasible to
optimise or sample directly.
Sampling is performed through rejection sam-
pling: first a sample x is drawn from q, and then x
is accepted or rejected with probability given by the
ratio r = p(x)/q(x), which is less than 1 by con-
struction. Accepted x?s can be shown to produce
an exact sample from p (Robert and Casella, 2004).
When the sample x from q is rejected, it is used as
a basis for ?refining? q into a slightly more com-
plex q?, where p ? q? ? q is still an upper-bound to
p. This ?adaptive rejection sampling? technique in-
crementally improves the rate of acceptance, and is
pursued until some rate above a given threshold is
obtained, at which point one stops refining and uses
the current proposal to obtain further exact samples
from p.
In the case of optimisation, one finds the maxi-
mum x relative to q, and again computes the ratio
r = p(x)/q(x). If this ratio equals 1, then it is
easy to show that x is the actual maximum from
p.1 Otherwise we refine the proposal in a similar
way to the sampling case, continuing until we find
a ratio equal to 1 (or very close to 1 if we are will-
ing to accept an approximation to the maximum).
For finite spaces X , this optimisation technique is
argued to be a generalisation of A?.
An application of the OS? technique to sam-
pling/optimisation with High-Order HMM?s is de-
scribed in Carter et al (2012) and provides back-
ground for this paper. In that work, while the high-
order HMM corresponds to an intractable goal dis-
tribution, it can be upper-bounded by a sequence
of tractable distributions for which optimisers and
samplers can be obtained through standard dy-
namic programming techniques.
2.2 Hierarchical Translation
An abstract formulation of the decoding process
for hierarchical translation models such as that of
Chiang (2007) can be expressed as a sequence of
three steps. In a first step, a translation model
G, represented as a weighted synchronous context-
free grammar (SCFG) (Chiang, 2005), is applied to
(in other words, intersected with) the source sen-
tence f to produce a weighted context-free gram-
mar G(f) over the target language.2 In a second
step, G(f) is intersected with a weighted finite-
state automaton A representing the target language
model, resulting in a weighted context-free gram-
mar G?(f) = G(f) ? A. In a final step, a dynamic
programming procedure (see ?2.4) is applied to
find the maximum derivation x in G?(f), and the
sequence of leaves of yield(x) is the result transla-
tion.
While this formulation gives the general princi-
ple, already mentioned in Chiang (2007), most im-
plementations do not exactly follow these steps or
use this terminology. In practice, the closest ap-
proach to this abstract formulation is that of Dyer
(2010) and the related system cdec (Dyer et al,
2010); we follow a similar approach here.
1This is because if x? was such that p(x?) > p(x), then
q(x?) ? p(x?) > p(x) = q(x), and hence x would not be a
maximum for q, a contradiction.
2G(f) is thus a compact representation of a forest over
target sequences, and is equivalent to a hypergraph, using dif-
ferent terminology.
473
Whatever the actual implementation chosen, all
approaches face a common problem: the complex-
ity of the intersection G?(f) = G(f)?A increases
rapidly with the order of the language model, and
can become unwieldy for moderate-length input
sentences even with a bigram model. In order to
address this problem, most implementations em-
ploy variants of a technique called cube-pruning
(Chiang, 2007; Huang and Chiang, 2007), where
the cells constructed during the intersection pro-
cess retain only a k-best list of promising candi-
dates. This is an approximation technique, related
to beam-search, which performs well in practice,
but is not guaranteed to find the actual optimum.
In the approach presented here ? described in
detail in ?3 ? we do not prune the search space.
While we do construct the full initial grammar
G(f), we proceed by incrementally intersecting
it with simple automata associated with upper-
bounds ofA, for which the intersection is tractable.
2.3 Earley Intersection
In their classical paper Bar-Hillel et al (1961)
showed that the intersection of a CFG with a FSA is
a CFG, and Billot and Lang (1989) were possibly
the first to notice the connection of this construct
with chart-parsing. In general, parsing with a CFG
can be seen as a special case of intersection, with
the input sequence represented as a ?flat? (linear
chain) automaton, and this insight allows to gener-
alise various parsing algorithms to corresponding
intersection algorithms. One such algorithm, for
weighted context-free grammars and automata, in-
spired by the CKY parsing algorithm, is presented
in Nederhof and Satta (2008). The algorithm that
we are using is different; it is inspired by Earley
parsing, and was introduced in chapter 2 of Dyer
(2010). The advantage of Dyer?s ?Earley Intersec-
tion? algorithm is that it combines top-down pre-
dictions with bottom-up completions. The algo-
rithm thus avoids constructing many non-terminals
that may be justified from the bottom-up perspec-
tive, but can never be ?requested? by a top-down
derivation, and would need to be pruned in a sec-
ond pass. Our early experiments showed an impor-
tant gain in intermediary storage and in overall time
by using this Earley-based technique as opposed to
a CKY-based technique.
We do not describe the Earley Intersection algo-
rithm in detail here, but refer to Dyer (2010), which
we follow closely.
2.4 Optimisation and Sampling from a
WCFG
Optimisation in a weighted CFG (WCFG)3, that
is, finding the maximum derivation, is well stud-
ied and involves a dynamic programming proce-
dure that assigns in turn to each nonterminal, ac-
cording to a bottom-up traversal regime, a max-
imum derivation along with its weight, up to the
point where a maximum derivation is found for the
initial nonterminal in the grammar. This can be
seen as working in the max-times semiring, where
the weight of a derivation is obtained through the
product of the weights of its sub-derivations, and
where the weight associated with a nonterminal is
obtained by maximising over the different deriva-
tions rooted in that nonterminal.
The case of sampling can be handled in a very
similar way, by working in the sum-times instead
of the max-times semiring. Here, instead of max-
imising over the weights of the competing deriva-
tions rooted in the same nonterminal, one sums
over these weights. By proceeding in the same
bottom-up way, one ends with an accumulation of
all the weights on the initial nonterminal (this can
also be seen as the partition function associated
with the grammar). An efficient exact sampler is
then obtained by starting at the root nonterminal,
randomly selecting an expansion proportionally to
the weight of this expansion, and iterating in a top-
down way. This process is described in more detail
in section 4 of Johnson et al (2007), for instance.
3 Approach
The complexity of building the full intersection
G(f) ? A, when A represents a language model
of order n, is related to the fact that the number of
states of A grows exponentially with n, and that
each nonterminal N in G(f) tends to generate in
the grammar G?(f) many indexed nonterminals of
the form (i,N, j), where i, j are states of A and
the nonterminal (i,N, j) can be interpreted as an
N connecting an i state to a j state.
In our approach, instead of explicitly construct-
ing the full intersection G(f) ? A, which, using
the notation of ?2.1, is identified with the unnor-
malised goal distribution p(x), we incrementally
produce a sequence of ?proposal? grammars q(t),
which all upper-bound p, where q(0) = G(f) ?
A(0), ..., q(t+1) = q(t) ? A(t), etc. Here A(0) is
3Here the CFG is assumed to be acyclic, which is typically
the case in translation applications.
474
an optimistic, low complexity, ?unigram? version
of the automaton A, and each increment A(t) is a
small automaton that refines q(t) relative to some
specific k-gram context (i.e., sequence of k words)
not yet made explicit in the previous increments,
where k takes some value between 1 and n. This
process produces a sequence of grammars q(t) such
that q(0)(.) ? q(1)(.) ? q(2)(.) ? ... ? p(.).
In the limit ?Mt=0A(t) = A for some largeM , so
that we are in principle able to reconstruct the full
intersection p(.) = q(M) = G(f)?A(0)?...?A(M)
in finite time. In practice our actual process stops
much earlier: in optimisation, when the value of
the maximum derivation x?t relative to q(t) becomes
equal to its value according to the full language
model, in sampling when the acceptance rate of
samples from q(t) exceeds a certain threshold. The
process is detailed in what follows.
3.1 OS? for Hierarchical Translation
Our application of OS? to hierarchical translation is
illustrated in Algorithm 1, with the two modes, op-
timisation and sampling, made explicit and shown
side-by-side to stress the parallelism.
On line 1, we initialise the time step to 0, and
for sampling we also initialise the current accep-
tance rate (AR) to 0. On line 2, we initialise the
initial proposal grammar q(0), where A(0) is de-
tailed in ?3.2. On line 3, we start a loop: in op-
timisation we stop when we have found an x that
is accepted, meaning that the maximum has been
found; in sampling, we stop when the estimated
acceptance rate (AR) of the current proposal q(t)
exceeds a certain threshold (e.g. 20%) ? this AR
can be roughly estimated by observing how many
of the last (say) one hundred samples from the pro-
posal have been accepted, and tends to reflect the
actual acceptance rate obtained by using q(t) with-
out further refinements. On line 4, in optimisation,
we compute the argmax x from the proposal, and in
sampling we draw a sample x from the proposal.4
On line 5, we compute the ratio r = p(x)/q(t)(x);
by construction q(t) is an optimistic version of p,
thus r ? 1.
On line 6, in optimisation we accept x if the
ratio is equal to 1, in which case we have found
the maximum, and in sampling we accept x with
probability r, which is a form of adaptive rejec-
tion sampling and guarantees that accepted sam-
4Following the OS? approach, taking an argmax is actually
assimilated to an extreme form of sampling, with an L? space
taking the place of an L1 space.
ples form exact samples from p; see (Dymetman et
al., 2012a).
If x was rejected (line 7), we then (lines 8, 9)
refine q(t) into a q(t+1) such that p(.) ? q(t+1)(.) ?
q(t)(.) everywhere. This is done by defining the
incremental automatonA(t+1) on the basis of x and
q(t), as will be detailed below, and by intersecting
this automaton with q(t)
Finally, on line 11, in optimisation we return the
x which has been accepted, namely the maximum
of p, and in sampling we return the list of already
accepted x?s, which form an exact sample from p,
along with the current q(t), which can be used as a
sampler to produce further exact samples with an
acceptance rate performance above the predefined
threshold.
3.2 Incremental refinements
Initial automatonA(0) This deterministic au-
tomaton is an ?optimistic? version ofA which only
records unigram information. A(0) has only one
state q0, which is both initial and final. For each
word a of the target language it has a transition
(q0, a, q0) whose weight is denoted by w1(a). This
weight is called the ?max-backoff unigram weight?
(Carter et al, 2012) and it is defined as:
w1(a) ? maxh plm(a|h),
where plm(a|h) is the conditional language model
probability of a relative to the history h, and where
the maximum is taken over all possible histories,
that is, over all possible sequence of target words
that might precede a.
Max-backoffs Following Carter et al (2012),
for any language model of finite order, the unigram
max-backoff weights w1(a) can be precomputed in
a ?Max-ARPA? table, an extension of the ARPA
format (Jurafsky and Martin, 2000) for the target
language model, which can be precomputed on the
basis of the standard ARPA table.
From the Max-ARPA table one can also directly
compute the following ?max-backoff weights?:
w2(a|a?1), w3(a|a?2 a?1), ..., which are defined
by:
w2(a|a?1) ? maxh plm(a|h, a?1)
w3(a|a?2 a?1) ? maxh plm(a|h, a?2 a?1)
...
where the maximum is taken over the part of
the history which is not explicitely indicated.
475
Algorithm 1 OS? for Hierarchical Translation: Optimisation (left) and Sampling (right).
1: t? 0
2: q(0) ? G(f) ?A(0)
3: while not an x has been accepted do
4: Find maximum x in q(t)
5: r ? p(x)/q(t)(x)
6: Accept-or-Reject x according to r
7: if Rejected(x) then
8: define A(t+1) based on x and q(t)
9: q(t+1) ? q(t) ?A(t+1)
10: t? t + 1
11: return x
1: t? 0, AR? 0
2: q(0) ? G(f) ?A(0)
3: while not AR > threshold do
4: Sample x ? q(t)
5: r ? p(x)/q(t)(x)
6: Accept-or-Reject x according to r
7: if Rejected(x) then
8: define A(t+1) based on x and q(t)
9: q(t+1) ? q(t) ?A(t+1)
10: t? t + 1
11: return already accepted x?s along with q(t)
Note that: (i) if the underlying language model
is, say, a trigram model, then w3(a|a?2 a?1)
is simply plm(a|a?2 a?1), and similarly for an
underlying model of order k in general, and
(ii) w2(a|a?1) = maxa?2 w3(a|a?2 a?1) and
w1(a) = maxa?1 w2(a|a?1).
Incremental automata A(t) The weight
assigned to any target sentence by A(0) is larger or
equal to its weight according to A. Therefore, the
initial grammar q(0) = G(f) ? A(0) is optimistic
relative to the actual grammar p = G(f) ? A: for
any derivation x in p, we have p(x) ? q(0)(x).
We can then apply the OS? technique with q(0).
In the case of optimisation, this means that
we find the maximum derivation x from q(0).
By construction, with y = yield(x), we have
A(0)(y) ? A(y). If the two values are equal, we
have found the maximum,5 otherwise there must
be a word yi in the sequence ym1 = y for which
plm(yi|yi?11 ) is strictly smaller than w1(yi). Let us
take among such words the one for which the ratio
? = w2(yi|yi?1)/w1(yi) ? 1 is the smallest, and
for convenience let us rename b = yi?1, a = yi.
We then define the (deterministic) automaton A(1)
as illustrated in the following figure:
b:1 a:? 
else:1 
b:1 else:1 
0 1 
Here the state 0 is both initial and final, and the
state 1 is final; all edges carry a (multiplicative)
weight equal to 1, except edge (1, a, 0), which car-
ries the weight ?. We use the abbreviation ?else?
to refer to any label other than bwhen starting from
0, and other than b or a when starting from 1.
5This case is very unlikely with A(0), but helps introduce
the general case.
It is easy to check that this automaton assigns to
any word sequence y a weight equal to ?k, where k
is the number of occurrences of b a in y. In particu-
lar, if y is such that yi?1 = b, yi = a, then the tran-
sition in (the deterministic automaton) A(0) ?A(1)
that consumes yi carries the weight ? w1(a), in
other words, the weight w2(a|b). Thus the new
proposal grammar q(1) = q(0) ? A(1) has now
?incorporated? knowledge of the bigram a-in-the-
context-b, at the cost of some increase in its com-
plexity.6
The general procedure for choosing A(t+1) fol-
lows the same pattern. We find the max deriva-
tion x in q(t) along with its yield y; if p(x) =
q(t)(x), we stop and output x; otherwise we find
some subsequence yi?m?1, yi?m, ..., yi such that
the knowledge of the n-gram yi?m, ..., yi has al-
ready been registered in q(t), but not that of the
n-gram yi?m?1, yi?m, ..., yi, and we define an
automaton A(t+1) which assign to a sequence a
weight ?k, where
? = wm+1(yi|yi?m?1, yi?m, ..., yi?1)wm(yi|yi?m, ..., yi?1)
,
and where k is the number of occurrences of
yi?m?1, yi?m, ..., yi in the sequence.7
We note that we have p ? q(t+1) ? q(t) ev-
erywhere, and also that the number of possible re-
finement operations is bounded, because at some
point we would have expanded all contexts to their
maximum order, at which point we would have re-
produced p(.) on the whole space X of possible
6Note that without further increasing q(1)?s complexity one
can incorporate knowledge about all bigrams sharing the pre-
fix b. This is because A(1) does not need additional states
to account for different continuations of the context b, all we
need is to update the weights of the transitions leaving state 1
appropriately. More generally, it is not more costly to account
for all n-grams prefixed by the same context of n ? 1 words
than it is to account for only one of them.
7Building A(t+1) is a variant of the standard construction
for a ?substring-searching? automaton (Cormen et al, 2001)
and produces an automaton with n states (the order of the n-
gram). This construction is omitted for the sake of space.
476
derivations exactly. However, we typically stop
much earlier than that, without expanding contexts
in the regions of X which are not promising even
on optimistic assessments based on limited con-
texts.
Following the OS? methodology, the situation
with sampling is completely parallel to that of op-
timisation, the only difference being that, instead
of finding the maximum derivation x from q(t)(.),
we draw a sample x from the distribution asso-
ciated with q(t)(.), then accept it with probabil-
ity given by the ratio r = p(x)/q(t)(x) ? 1. In
the case of a reject, we identify a subsequence
yi?m?1, yi?m, ..., yi in yield(x) as in the optimi-
sation case, and similarly refine q(t) into q(t+1) =
q(t) ? A(t+1). The acceptance rate gradually in-
creases because q(t) comes closer and closer to p.
We stop the process at a point where the current ac-
ceptance rate, estimated on the basis of, say, the last
one hundred trials, exceeds a predefined threshold,
perhaps 20%.
3.3 Illustration
In this section, we present a small running example
of our approach. Consider the lowercased German
source sentence: eine letzte beobachtung .
Table 1 shows the translation associated with the
optimum derivation from each proposal q(i). The
n-gram whose cost, if extended by one word to the
left, would be increased by the largest factor is un-
derlined. The extended context selected for refine-
ment is highlighted in bold.
i Rules Optimum
0 311 <s> one last observation . </s>
1 454 <s> one last observation . </s>
2 628 <s> one last observation . </s>
3 839 <s> one final observation . </s>
4 1212 <s> one final observation . </s>
...
12 3000 <s> a final observation . </s>
13 3128 <s> one final observation . </s>
Table 1: Optimisation steps showing the iteration
(i), the number of rules in the grammar and the
translation associated to the optimum derivation.
Consider the very first iteration (i = 0), at which
point only unigram costs have been incorporated.
The sequence <s> one last observation . </s>
represents the translation associated to the best
derivation x in q(0). We proceed by choosing from
it one sequence to be the base for a refinement that
will lower q(0) bringing it closer to p. Amongst all
possible one-word (to the left) extensions, extend-
ing the unigram ?one? to the bigram ?<s> one? is
the operation that lowers q(0)(x) the most. It might
be helpful to understand it as the bigram ?<s> one?
being associated to the largest LM gap observed
in x. Therefore the context ?<s>? is selected for
refinement, which means that an automaton A(1)
is designed to down-weight derivations compatible
with bigrams prefixed by ?<s>?. The proposal q(0)
is intersected with A(1) producing q(1). We pro-
ceed like this iteratively, always selecting a con-
text not yet accounted for until q(i)(x) = p(x) for
the best derivation (13th iteration in our example),
when the true optimum is found with a certificate
of optimality.
Q Q Q Q Q Q Q Q Q Q Q Q Q Q
0 2 4 6 8 10 12
?
2
?
1
0
1
2
3
Iteration (i)
Scor
e (Q, 
P, B) ;
 Delta
 (C, M
) ; #st
ates (
R)
P P P
P P P P P P P P P P
PB B B B B B B B B B B B B B
C C C C C C C C C C C C C C
M M M M M M M M M M M M M M
R
R R R R
R
R
R R
R R
R
R
R
QPBCMR
QPBestCurrent gapMinimum gapRefinement
Figure 1: Certificate of optimality.
Figure 1 displays the progression of Q (score of
the best derivation) and P (that derivation?s true
score). As guaranteed by construction, Q is always
above P . B represents the score of the best deriva-
tion so far according to the true scoring function,
that is, B is a lower-bound on the true optimum8.
The optimal solution is achieved when P = Q.
Curve B in Figure 1 shows that the best scoring
solution was found quite early in the search (i = 3).
However, optimality could only be proven 10 itera-
tions later. Another way of stating the convergence
criterion Q = P is observing a zero gap (in the log
domain) between Q and P (see curve C ? current
gap), or a zero gap between Q and B (see curve M
? minimum gap). Observe how M drops quickly
from 1 to nearly 0, followed by a long tail whereM
8This observation allows for error-safe pruning in optimi-
sation: if x is a lower-bound on the true optimum, derivations
in q(i) that score lower than p(x) could be safely removed.
We have left that possibility for future work.
477
decreases much slower. Note that if we were will-
ing to accept an approximate solution, we could al-
ready stop the search if B remained unchanged for
a predetermined number of iterations or if changes
in B were smaller than some threshold, at the cost
of giving up on the optimality certificate.
Finally, curve R shows the number of states in
the automaton A(i) that refines the proposal at it-
eration i. Note how lower order n-grams (2-grams
in fact) are responsible for the largest drop in the
first iterations and higher-order n-grams (in fact 3-
grams) are refined later in the long tail.
Figure 2 illustrates the progression of the sam-
pler for the same German sentence. At each iter-
ation a batch of 500 samples is drawn from q(i).
The rejected samples in the batch are used to col-
lect statistics about overoptimistic n-grams and to
heuristically choose one context to be refined for
the next iteration, similar to the optimisation mode.
We start with a low acceptance rate which grows
up to 30% after 15 different contexts were incor-
porated. Note how the L1 norm of q (its partition
function) decreases after each refinement, that is,
q is gradually brought closer to p, resulting in the
increased number of exact samples and better ac-
ceptance rate.
Note that, starting from iteration one, all refine-
ments here correspond to 2-grams (i.e. one-word
contexts). This can be explained by the fact that,
in sampling, lower-order refinements are those that
mostly increase acceptance rate (rationale: high-
order n-grams are compatible with fewer grammar
rules).
Iteration (i)
1.0
1.5
2.0
0 5 10
l
l l l l l l l l l l l l lrefinement
0.1
0.2
0.3
l l
l l l l
l l l l l l l
laccrate
0
100
0
l l l l l
l l l
l l l
l l l
exact
9
10 l
l l l l l l l l l l l l l
L1
Figure 2: L1 norm of q, the number of exact sam-
ples drawn, the acceptance rate and the refinement
type at each iteration.
4 Experiments
We used the Moses toolkit (Koehn et al, 2007)
to extract a SCFG following Chiang (2005) from
the 6th version of the Europarl collection (Koehn,
2005) (German-English portion). We trained lan-
guage models using lmplz (Heafield et al, 2013)
and interpolated the models trained on the En-
glish monolingual data made available by the
WMT (Callison-Burch et al, 2012) (i.e. Eu-
roparl, newscommentaries, news-2012 and com-
moncrawl). Tuning was performed via MERT us-
ing newstest2010 as development set; test sen-
tences were extracted from newstest2011. Finally,
we restricted our SCFGs to having at most 10 tar-
get productions for a given source production.
Figure 3 shows some properties of the initial
grammar G(f) as a function of the input sentence
length (the quantities are averages over 20 sen-
tences for each class of input length). The number
of unigrams grows linearly with the input length,
while the number of unique bigrams compatible
with strings generated by G(f) appears to grow
quadratically9 and the size of the grammar in num-
ber of rules appears to be cubic ? a consequence
of having up to two nonterminals on the right-hand
side of a rule.
Figure 4 shows the number of refinement oper-
ations until convergence in optimisation and sam-
pling, as well as the total duration, as a function of
the input length.10 The plots will be discussed in
detail below.
4.1 Optimisation
In optimisation (Figures 4a and 4b), the number of
refinements up to convergence appears to be lin-
ear with the input length, while the total duration
grows much quicker. These findings are further
discussed in what follows.
Table 2 shows some important quantities regard-
ing optimisation with OS? using a 4-gram LM. The
first column shows how many sentences we are
considering, the second column shows the sentence
length, the third column m is the average num-
ber of refinements up to convergence. Column |A|
refers to the refinement type, which is the number
of states in the automaton A, that is, the order of
9The number of unique bigrams is an estimate obtained by
combining the terminals at the boundary of nonterminals that
may be adjacent in a derivation.
10The current implementation faces timeouts depending on
the length of the input sentence and the order of the language
model, explaining why certain curves are interrupted earlier
than others in Figure 4.
478
2 4 6 8 10
50
100
150
Input length
unigr
ams
l
l
l l
l
l
l l
l
l
(a) Unigrams in G(f)
2 4 6 8 10
0
2000
4000
6000
Input length
bigra
ms
l l
l l
l
l
l
l
l
l
(b) Bigrams compatible with G(f)
2 4 6 8 10
0
1000
2000
3000
4000
5000
Input length
R0
l l l
l
l
l
l
l
l
l
(c) Number of rules in G(f)
Figure 3: Properties of the initial grammar as function of input length
S Length m |A| count |Rf ||R0|9 4 45.0 2 20.3 74.6 ? 53.9
3 19.2
4 5.4
10 5 62.3 2 21.9 145.4 ? 162.6
3 32.9
4 7.5
9 6 102.8 2 34.7 535.8 ? 480.0
3 54.9
4 13.2
Table 2: Optimisation with a 4-gram LM.
the n-grams being re-weighted (e.g. |A| = 2 when
refining bigrams sharing a one-word context). Col-
umn count refers to the average number of refine-
ments that are due to each refinement type. Finally,
the last column compares the number of rules in the
final proposal to that of the initial one.
The first positive result concerns how much con-
text OS? needs to take into account for finding the
optimum derivation. Table 2 (column m) shows
that OS? explores a very reduced space of n-gram
contexts up to convergence. To illustrate that, con-
sider the last row in Table 2 (sentences with 6
words). On average, convergence requires incorpo-
rating only about 103 contexts of variable order, of
which 55 are bigram (2-word) contexts (remember
that |A| = 3 when accounting for a 2-word con-
text). According to Figure 3b, in sentences with
6 words, about 2,000 bigrams are compatible with
strings generated by G(f). This means that only
2.75% of these bigrams (55 out of 2,000) need to
be explicitly accounted for, illustrating how waste-
ful a full intersection would be.
A problem, however, is that the time until con-
vergence grows quickly with the length of the input
(Figure 4b). This can be explained as follows. At
each iteration the grammar is refined to account for
n-grams sharing a context of (n ? 1) words. That
S Input m |A| count |Rf ||R0|10 5 1.0 2 1.0 1.9 ? 1.0
10 6 6.6 2 6.3 17.6 ? 13.6
3 0.3
10 7 14.5 2 12.9 93.8 ? 68.9
3 1.5
4 0.1
Table 3: Sampling with a 4-gram LM and reaching
a 5% acceptance rate.
operation typically results in a larger grammar:
most rules are preserved, some rules are deleted,
but more importantly, some rules are added to ac-
count for the portion of the current grammar that
involves the selected n-grams. Enlarging the gram-
mar at each iteration means that successive refine-
ments become incrementally slower.
The histogram of refinement types of Table 2
highlights how efficient OS? is w.r.t. the space of
n-grams it needs to explore before convergence.
The problem is clearly not the number of refine-
ments, but rather the relation between the growth
of the grammar and the successive intersections.
Controlling for this growth and optimising the in-
tersection as to partially reuse previously computed
charts may be the key for a more generally tractable
solution.
4.2 Sampling
Figure 4c shows that sampling is more economi-
cal than optimisation in that it explicitly incorpo-
rates even fewer contexts. Note how OS? con-
verges to acceptance rates from 1% to 10% in much
fewer iterations than are necessary to find an opti-
mum11. Although the convergence in sampling is
11Currently we use MERT to train the model?s weight vec-
tor ? which is normalised by its L1 norm in the Moses im-
plementation. While optimisation is not sensitive to the scale
of the weights, in sampling the scale determines how flat or
479
2 2 2 2
2 2 2 2 2 2
2 4 6 8 10
0
20
40
60
80
100
Input length
Refi
nem
ents
3 3
3
3
3
3
3
4
4
4
4
4
4
234 2?gram LM3?gram LM4?gram LM
(a) Optimisation: number of refinements.
2 2 2 2 2 2 2 2
2 2
2 4 6 8 10
0
500
0
150
00
250
00
Input length
Tim
e (s)
3 3 3 3
3 3
3
4 4 4 4
4
4
234 2?gram LM3?gram LM4?gram LM
(b) Optimisation: time for convergence.
a a a a a a a a a a a a a a
2 4 6 8 10 12 14
0
5
10
15
20
25
Input length
Refin
emen
ts
b b b b b b b b b
b b b
b b
c c c c c c c
c
c c
c c
1 1 1 1 1 1 1
1
1
2 2 2 2 2 2
2
2
2
3 3 3 3 3
3 3
3
3
4 4 4 4
4
4
4
5 5 5
5
5
5
X X X X
X
X
Y Y Y
Y
Y
Y
abc12345XY
LM2 1%LM2 5%LM2 10%LM3 1%LM3 2%LM3 3%LM3 5%LM3 10%LM4 5%LM4 10%
(c) Sampling: number of refinements.
a a a a a a a a a a a a a
a
2 4 6 8 10 12 14
02
000
6000
1000
0
1400
0
Input length
Time
 (s)
b b b b b b b b b b b
b
b
b
c c c c c c c c c c
c
c
1 1 1 1 1 1 1 1
1
2 2 2 2 2 2 2 2
2
3 3 3 3 3 3 3
3
3
4 4 4 4 4 4
4
5 5 5 5 5 5X X X X X XY Y Y Y Y
Y
abc12345XY
LM2 1%LM2 5%LM2 10%LM3 1%LM3 2%LM3 3%LM3 5%LM3 10%LM4 5%LM4 10%
(d) Sampling: time for convergence.
Figure 4: Convergence for different LM order as function of the input length in optimisation (top) and
sampling (bottom). We show the number of refinements up to convergence on the left, and the convergence
time on the right. In optimisation we stop when the true optimum is found. In sampling we stop at different
acceptance rate levels: (a, b and c) use a 2-gram LM to reach 1, 5 and 10% AR; (1-4) use a 3-gram LM to
reach 2, 3, 5 and 10% AR; and (X, Y) use a 4-gram LM to reach 5 and 10% AR.
faster than in optimisation, the total duration is still
an issue (Figure 4b).
Table 3 shows the same quantities as Table 2, but
now for sampling. It is worth highlighting that even
though we are using an upper-bound over a 4-gram
LM (and aiming at a 5% acceptance rate), very few
contexts are selected for refinement, most of them
lower-order ones (one-word contexts ? rows with
|A| = 2).
Observe that an improved acceptance rate al-
ways leads to faster acquisition of exact samples
after we stop refining our proxy distribution. How-
ever, Figure 4d shows for example that moving
from 5% to 10% acceptance rate using a 4-gram
LM (curves X and Y) is time-consuming. Thus
there is a trade-off between how much time one
spends improving the acceptance rate and how
many exact samples one intends do draw. Figure
5 shows the average time to draw batches between
peaked the distribution is. Arun et al (2010) experiment with
scaling MERT-trained weights as to maximise BLEU on held-
out data, as well as with MBR training. A more adequate
training algorithm along similar lines is reserved for future
work.
1 1 1 1
1
1
1
1e+00 1e+02 1e+04 1e+06
200
500
1000
2000
5000
1000
0
Samples
Time
 (s)
2 2 2 2 2
2
212 5% AR10% AR
Figure 5: Average time to draw 1 to 1 million sam-
ples, for input sentences of length 6, using a 4-gram
LM at 5% (curve 1) and 10% (curve 2) acceptance
rate (including the time to produce the sampler).
one and one million samples from two exact sam-
plers that were refined up to 5% and 10% accep-
tance rate respectively. The sampler at 5% AR
(which is faster to obtain) turns out to be more effi-
cient if we aim at producing less than 10K samples.
Finally, note that samples are independently
480
drawn from the final proposal, making the ap-
proach an appealing candidate to parallelism in or-
der to increase the effective acceptance rate.
5 Related Work
Rush and Collins (2011) do not consider sampling,
but they address exact decoding for hierarchical
translation. They use a Dual Decomposition ap-
proach (a special case of Lagrangian Relaxation),
where the target CFG (hypergraph in their termi-
nology) component and the target language model
component ?trade-off? their weights so as to ensure
agreement on what each component believes to be
the maximum. In many cases, this technique is
able to detect the actual true maximum derivation.
When this is not the case, they use a finite-state-
based intersection mechanism to ?tighten? the first
component so that some constraints not satisfied by
the current solution are enforced, and iterate until
the true maximum is found or a time-out is met,
which results in a high proportion of finding the
true maximum.
Arun et al (2009, 2010) address the question
of sampling in a standard phrase-based transla-
tion model (Koehn et al, 2003). Contrarily to our
use of rejection sampling (a Monte-Carlo method),
they use a Gibbs sampler (a Markov-Chain Monte-
Carlo (MCMC) method). Samples are obtained
by iteratively re-sampling groups of well-designed
variables in such a way that (i) the sampler does not
tend to be trapped locally by high correlations be-
tween conditioning and conditioned variables, and
(ii) the combinatorial space of possibilities for the
next step is small enough so that conditional prob-
abilities can be computed explicitly. By contrast to
our exact approach, the samples obtained by Gibbs
sampling are not independent, but form a Markov
chain that only converges to the target distribution
in the limit, with convergence properties difficult
to assess. Also by contrast to us, these papers do
not address the question of finding the maximum
derivation directly, but only through finding a max-
imum among the derivations sampled so far, which
in principle can be quite different.
Blunsom and Osborne (2008) address proba-
bilistic inference, this time, as we do, in the context
of hierarchical translation, where sampling is used
both for the purposes of decoding and training the
model. When decoding in the presence of a lan-
guage model, an approximate sampling procedure
is performed in two stages. First, cube-pruning is
employed to construct a WCFG which generates
a subset of all the possible derivations that would
correspond to a full intersection with the target lan-
guage model. In a second step this grammar is
sampled through the same dynamic programming
procedure that we have described in ?2.4. By con-
trast to our approach, the paper does not attempt
to perform exact inference. However it does not
only address the question of decoding, but also that
of training the model, which requires, in addition
to sampling, an estimate of the model?s partition
function. In common with Arun et al (2010), the
authors stress the fact that a sampler of derivations
is also a sampler of translations as strings, while a
maximiser over derivations cannot be used to find
the maximum translation string.
6 Conclusions
The approach we have presented is, to our knowl-
edge, the first one to address the problem of ex-
act sampling for hierarchical translation and to do
that in a framework that also handles exact opti-
misation. Our experiments show that only a frac-
tion of the language model n-grams need to be in-
corporated in the target grammar in order to per-
form exact inference in this approach. However,
in the current implementation, we experience time-
outs for sentences of even moderate length. We are
working on improving this situation along three di-
mensions: (i) our implementation of the Earley In-
tersection rebuilds a grammar from scratch at each
intersection, while it could capitalise on the charts
built during the previous steps; (ii) the unigram-
level max-backoffs are not as tight as they could
be if one took into account more precisely the set
of contexts in which each word can appear rela-
tive to the grammar; (iii) most importantly, while
our refinements are ?local? in the sense of address-
ing one n-gram context at a time, they still affect
a large portion of the rules in the current grammar,
even rules that have very low probability of being
ever sampled by this grammar; by preventing re-
finement of such rules during the intersection pro-
cess, we may be able to make the intersection more
local and to produce much smaller grammars, with-
out losing the exactness properties of the approach.
Acknowledgements
The first author wishes to thank the PASCAL-2
Visit to Industry programme for partially funding
his visit to Xerox Research Centre Europe last Fall,
which initiated this collaboration.
481
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning,
CoNLL ?09, pages 102?110, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, Philipp Koehn, Adam
Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte
carlo techniques for phrase-based translation. Ma-
chine Translation, 24(2):103?121, June.
Yehoshua Bar-Hillel, Micha A. Perles, and Eli Shamir.
1961. On formal properties of simple phrase struc-
ture grammars. Zeitschrift fu?r Phonetik, Sprachwis-
senschaft und Kommunikationsforschung, (14):143?
172.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association
for Computational Linguistics, pages 143?151, Van-
couver, British Columbia, Canada, June. Association
for Computational Linguistics.
Phil Blunsom and Miles Osborne. 2008. Probabilis-
tic inference for machine translation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?08, pages 215?
223, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact Sampling and Decoding in
High-Order Hidden Markov Models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125?1134, Jeju
Island, Korea, July. Association for Computational
Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 263?
270, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33:201?228.
Thomas H. Cormen, Clifford Stein, Ronald L. Rivest,
and Charles E. Leiserson. 2001. Introduction to Al-
gorithms. McGraw-Hill Higher Education, 2nd edi-
tion.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: a decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, ACLDemos ?10, pages 7?12, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christopher Dyer. 2010. A Formal Model of Ambiguity
and its Applications in Machine Translation. Ph.D.
thesis, University of Maryland.
M. Dymetman, G. Bouchard, and S. Carter. 2012a. The
OS* Algorithm: a Joint Approach to Exact Optimiza-
tion and Sampling. ArXiv e-prints, July.
Marc Dymetman, Guillaume Bouchard, and Simon
Carter. 2012b. Optimization and sampling for nlp
from a unified viewpoint. In Proceedings of the
First International Workshop on Optimization Tech-
niques for Human Language Technology, pages 79?
94, Mumbai, India, December. The COLING 2012
Organizing Committee.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
Kneser-Ney language model estimation. In Proceed-
ings of the 51st Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria, August.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
139?146, Rochester, New York, April. Association
for Computational Linguistics.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics
and Speech Recognition (Prentice Hall Series in Ar-
tificial Intelligence). Prentice Hall, 1 edition.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
482
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings of
Machine Translation Summit, pages 79?86.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes Risk Decoding for Statistical Machine
Translation. In Joint Conference of Human Lan-
guage Technologies and the North American chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2004).
Mark-Jan Nederhof and Giorgio Satta. 2008. Proba-
bilistic parsing. In M. Dolores Jimnez-Lpez G. Bel-
Enguix and C. Martn-Vide, editors, New Develop-
ments in Formal Languages and Applications, Stud-
ies in Computational Intelligence, volume 113, pages
229?258. Springer.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1 of ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods (Springer Texts in Statis-
tics). Springer-Verlag New York, Inc., Secaucus, NJ,
USA.
Walter Rudin. 1987. Real and Complex Analysis.
McGraw-Hill.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 72?82, Stroudsburg, PA,
USA. Association for Computational Linguistics.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, COLING-ACL ?06, pages 787?794,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764?773, Prague, Czech Republic,
June. Association for Computational Linguistics.
483
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 202?211,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Multilingual WSD-like Constraints for Paraphrase Extraction
Wilker Aziz
Research Group in Computational Linguistics
University of Wolverhampton, UK
W.Aziz@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield, UK
L.Specia@sheffield.ac.uk
Abstract
The use of pivot languages and word-
alignment techniques over bilingual cor-
pora has proved an effective approach for
extracting paraphrases of words and short
phrases. However, inherent ambiguities in
the pivot language(s) can lead to inade-
quate paraphrases. We propose a novel ap-
proach that is able to extract paraphrases
by pivoting through multiple languages
while discriminating word senses in the in-
put language, i.e., the language to be para-
phrased. Text in the input language is an-
notated with ?senses? in the form of for-
eign phrases obtained from bilingual par-
allel data and automatic word-alignment.
This approach shows 62% relative im-
provement over previous work in generat-
ing paraphrases that are judged both more
accurate and more fluent.
1 Introduction
Paraphrases are alternative ways of expressing a
given meaning. Generating paraphrases that go
beyond morphological variants of the original text
is a challenging problem and has been shown to
be useful in many natural language applications.
These include i) expanding the set of reference
translations for Machine Translation (MT) eval-
uation (Denkowski and Lavie, 2010; Liu et al,
2010) and parameter optimisation (Madnani et al,
2007), where multiple reference translations are
important to accommodate for valid variations of
system translations; ii) addressing the problem of
out-of-vocabulary words or phrases in MT, either
by replacing these by paraphrases that are known
to the MT system (Mirkin et al, 2009) or by ex-
panding the phrase table with new translation al-
ternatives (Callison-Burch et al, 2006); and iii)
expanding queries for improved coverage in ques-
tion answering (Riezler et al, 2007).
Bannard and Callison-Burch (2005) introduced
an approach to paraphrasing which has shown par-
ticularly promising results by pivoting through dif-
ferent languages for which bilingual parallel data
is available. The approach consists in aligning
phrases in the bilingual parallel corpus to find
pairs of phrases (e1, e2) in the input language, i.e.,
the language to be paraphrased, which typically
align to the same foreign phrases F = {f : e1 ?
f ? e2}. This intermediate language is called
pivot language and the phrases f ? F that support
the equivalence (e1, e2) are called pivot phrases.
If there exists a non-empty set of pivots connect-
ing e1 to e2, e2 is said to be a paraphrase of e1. The
paraphrase is scored in terms of the conditional
probabilities observed in the parallel corpus1 by
marginalising out the pivot phrases that support
the alignment (e1, e2) as shown in Equation 1.
p(e2|e1) =
?
f?F
p(f |e1)p(e2|f) (1)
Equation 1 allows paraphrases to be extracted
by using multiple pivot languages such that these
languages help discard inadequate paraphrases re-
sulting from ambiguous pivot phrases. However
in this formulation all senses of the input phrase
are mixed together in a single distribution. For ex-
ample, for the Spanish input phrase acabar con,
both paraphrases superar (overcome) and elim-
inar (eliminate) may be adequate depending on
the context, however they are not generally in-
terchangeable. In (Bannard and Callison-Burch,
1The distributions p(f |e) and p(e|f) are extracted from
relative counts in word-aligned parallel corpus.
202
2005), the distributions learnt from different bilin-
gual corpora are combined through a simple av-
erage. This makes the model naturally favour
the most frequent senses of the phrases, assigning
very low probabilities to less frequent senses. Sec-
tion 5 shows evidence of how this limitation makes
paraphrases with certain senses unreachable.
We propose a novel formulation of the problem
of generating paraphrases that is constrained by
sense information in the form of foreign phrases,
which can be thought of as a quasi-sense annota-
tion. Using a bilingual parallel corpus to annotate
phrases with their quasi-senses has proved help-
ful in building word-sense disambiguation (WSD)
models for MT (Carpuat and Wu, 2007; Chan et
al., 2007): instead of monolingual senses, pos-
sible translations of phrases obtained with word-
alignment were used as senses. Our approach per-
forms paraphrase extraction by pivoting through
multiple languages while penalising senses of the
input that are not supported by these pivots.
Our experiments show that the proposed ap-
proach can effectively eliminate inadequate para-
phrases for polysemous phrases, with a significant
improvement over previous approaches. We ob-
serve absolute gains of 15-25% in precision and
recall in generating paraphrases that are judged
fluent and meaning preserving in context.
This paper is structured as follows: Section 2
describes additional previous work on paraphrase
extraction and pivoting. Section 3 presents the
proposed model. Section 4 introduces our experi-
mental settings, while Section 5 shows the results
of a series of experiments.
2 Related work
In addition to the well-known approach by (Ban-
nard and Callison-Burch, 2005), the following
previous approaches using pivot languages for
paraphrasing can be mentioned. For a recent
and comprehensive survey on a number of data-
driven paraphrase generation methods, we refer
the reader to (Madnani and Dorr, 2010).
Cohn and Lapata (2007) make use of multi-
ple parallel corpora to improve Statistical Ma-
chine Translation (SMT) by triangulation for lan-
guages with little or no source-target parallel data
available. Translation tables are learnt by pivot-
ing through languages for which source-pivot and
pivot-target bilingual corpora can be found. Multi-
ple pivot languages were found useful to preserve
the meaning of the source in the triangulated trans-
lation, as different languages are likely to realise
ambiguities differently. Although their findings
apply to generating translation candidates, the in-
put phrases are not constrained to specific senses,
and as a consequence multiple translations, which
are valid in different contexts but not generally
interchangeable, are mixed together in the same
distribution. In SMT the target Language Model
(LM) helps selecting the adequate translation can-
didate in context.
Callison-Burch (2008) extends (Bannard and
Callison-Burch, 2005) by adding syntactic con-
straints to the model. Paraphrase extraction is
done by pivoting using word-alignment informa-
tion, as before, but sentences are syntactically
annotated and paraphrases are restricted to those
with the same syntactic category. This addresses
categorial ambiguity by preventing that words
with a given category (e.g. a noun) are para-
phrased by words with other categories (e.g., a
verb). However, the approach does not solve the
more complex issue of polysemous paraphrases:
words with the same category but different mean-
ings, such as the noun bank as financial institution
and land alongside a river/lake.
Marton et al (2009) derive paraphrases from
monolingual data using distributional similarity
metrics. The approach has the advantage of not re-
quiring bilingual parallel data, but it suffers from
issues typical of distributional similarity metrics.
In particular, it produces paraphrases that share the
same or similar contexts but are related in ways
that do not always characterise paraphrasing, such
as antonymy.
3 Paraphrasing through multilingual
constraints
Our approach to paraphrasing can be applied to
both individual words or sequences of words of
any length, conditioned only on sufficient evi-
dence of these segments in a parallel corpus. We
use segments as provided by the standard phrase
extraction process from phrase-based SMT ap-
proaches (see Section 4), which in most cases
range from individual words to short sequences of
words (up to seven words in our case). Hereafter,
we refer to these segments simply as phrases.
A model for paraphrasing under a constrained
set of senses should take into account both the
input phrase and the sense tag while selecting
203
Paired with en de nl da sv fi fr it pt el
es 1.78 1.56 1.62 1.61 1.51 1.58 1.65 1.51 1.60 5.68
en - 1.73 1.82 1.78 1.67 1.74 1.82 1.73 1.78 1.06
Table 1: Size of the bilingual parallel corpora in millions of sentence pairs
the pivot phrases that will lead to adequate para-
phrases. In our approach a sense tag consists in a
phrase in a foreign language, that is, a valid trans-
lation of the input phrase in a language of interest,
here referred to as target language. Treating the
target language vocabulary as a sense repository is
a good strategy from both theoretical and practi-
cal perspectives: it has been shown that monolin-
gual sense distinctions can be effectively captured
by translations into second languages, especially
as language family distance increases (Resnik and
Yarowsky, 1999; Specia et al, 2006). These trans-
lations can be easily captured given the avail-
ability of bilingual parallel data and robust au-
tomatic word-alignment techniques (Carpuat and
Wu, 2007; Chan et al, 2007).
Figure 1 illustrates the proposed model to pro-
duce sense tagged paraphrases. We start the pro-
cess at e1 and we need to make sure that the pivot
phrases f ? F align back to the input language,
producing the paraphrase e2, and to the target lan-
guage, producing the sense tag q. To avoid com-
puting the distribution p(e2, q|f) ? which would
require a trilingual parallel corpus ? we assume
that e2 and q are conditionally independent on f :
p(e2, q|f)
e2??q|f= p(e2|f)p(q|f)
In other words, we assume that pivot phrases gen-
erate paraphrases and sense tags independently.
Equation 2 shows how paraphrase probabilities are
computed by marginalising out the pivot phrases
under this assumption.
GFED@ABCe1 //GFED@ABCf

// GFED@ABCe2
?>=<89:;q
Figure 1: Pivot phrases must align back to target
phrases (sense annotation).
p(e2|e1, q) =
1
z
?
f?F
p(e2|f)p(q|f)p(f |e1) (2)
In order to constrain the extraction of para-
phrases such that it complies with a sense repos-
itory, in addition to bilingual parallel corpora be-
tween the input language and the pivot languages,
our model requires bilingual parallel corpora be-
tween the pivot languages and the language that is
used for sense annotation.
Callison-Burch (2007) discusses factors affect-
ing paraphrase quality, one of which is word
senses. Paraphrasing through pivoting essentially
relies on the hypothesis that different pivot phrases
can be used to identify synonymy, rather than pol-
ysemy (an assumption made in the WSD liter-
ature). Callison-Burch (2007) also proposes an
extraction procedure that may be conditioned on
specific contexts of the input phrase (Bannard
and Callison-Burch, 2005), where the context is
a given pivot phrase.2 However, that model is un-
able to pivot through multiple languages. As we
show in Section 5, this makes the model extremely
sensitive to ambiguities of the one phrase used as
both sense tag and pivot.
The model we propose attempts to perform
sense-disambiguated paraphrase extraction, that
is, paraphrases are discovered in the context of
translation candidates of the input phrases. In ad-
dition, it allows the use of multiple pivot languages
in the process, capitalising on both the WSD
and the paraphrase assumption. While the target
phrases discriminate different senses of the input
phrases, the pivot phrases coming from multiple
languages bring extra evidence to jointly capture
the ambiguities introduced by the target phrases
themselves.
To illustrate the impact of this contribution, con-
sider the polysemous Spanish word forma, and
some of its translations into English extracted
from our corpus (Section 4): kind, way, means
and form. The English words distinguish three
possible senses of forma: (a) means/way of do-
ing/achieving something, (b) shape, and (c) type
or group sharing common traits. The model pre-
sented in (Bannard and Callison-Burch, 2005)
cannot discriminate these senses. It mixes valid
senses of forma and (correctly) proposes the para-
phrases manera and modo for sense (a), and tipo
2A paraphrase is scored in the context of a given pivot
phrase f : p(e2|e1, f) = p(e2|f)p(f |e1).
204
for sense (c). However, paraphrases for sense (b)
are over penalised and account for very little of the
probability mass of the candidate paraphrases of
forma. Their extension which conditions extrac-
tion on a given pivot phrase is highly sensitive to
the ambiguities of the phrase used as sense anno-
tation. Table 5 shows how this model (CB-wsd in
the Table) makes mistakes for most senses of the
input due to the ambiguities of the English context
kind, way, means and form. Our approach (multi
in the Table) on the other hand successfully sep-
arates paraphrases according to the sense annota-
tion provided.
4 Experimental settings
4.1 Resources
The source of bilingual data used in the experi-
ments is the Europarl collection (Koehn, 2005).
We paraphrase Spanish (es) phrases using their
corresponding English (en) phrases as sense tags
and nine European languages as pivots: Ger-
man (de), Dutch (nl), Danish (da), Swedish (sv),
Finnish (fi), French (fr), Italian (it), Portuguese
(pt) and Greek (el). The tools provided along
with the corpus were used to extract the sentence
aligned parallel data as shown in Table 1.
The sentence aligned parallel data is first word-
aligned using GIZA++ in both source-target and
target-source directions, followed by the applica-
tion of traditional symmetrisation heuristics (Och
and Ney, 2003). These aligned corpora are used
for paraphrase extraction, except for a subset of
them used in the creation of a test set (Section 4.2).
4.2 Test set creation
Since we are interested in showing the ability
of our approach to find adequate paraphrases in
the presence of a foreign phrase (the sense tag),
it is important that our test set contains polyse-
mous phrases. Like in (Bannard and Callison-
Burch, 2005), we use the Spanish WordNet3 to
bias our selection of phrases to paraphrase to con-
tain ambiguous cases. However, rather than bi-
asing selection towards having more multi-word
expressions, we chose to have more polysemous
cases. From the Spanish WordNet, we selected 50
phrases (with at least one content word) to be para-
phrased such that 80% of the samples (40 phrases)
had at least 2 senses (with a given part-of-speech
3http://nlp.lsi.upc.edu/freeling/
Unambiguous Ambiguous
concreto,
pol??tica, fon-
dos, regular,
haber, amor
proprio, sangre
fr??a, dar a luz,
dar con, tomar
el pelo
derecho, comercial, real, particular, le-
gal, justo, comu?n, cerca, esencial, es-
pecial, fuerte, puesto, oficial, figura,
informe, parte, cuenta, forma, claro,
clave, tiempo, seguro, respuesta, traba-
jar, responder, garantizar, volver, au-
mentar, incluir, tratar, ofrecer, estable-
cer, pasar, dejar, realizar, punto de vista,
llevar a cabo, dar vueltas, tener que,
acabar con
Figure 2: Words and phrases selected to be para-
phrased. Ambiguity is determined on the basis of
the number of synsets in the Spanish WordNet. We
note that this information was only used to bias the
selection of the phrases, i.e., WordNet is not used
in the proposed approach.
La idea de conceder a la Unio?n Europea su propia compe-
tencia fiscal - la palabra clave es el ?impuesto por Europa?
- esta? siendo debatida.
The idea of granting the EU its own tax competence - the
keyword is the ?Europe tax? - is being discussed.
Figure 3: Example of context selected for the
phrase clave.
tag to avoid selecting simpler, categorial ambigui-
ties). Figure 2 lists the selected words and phrases
in their base forms.
The bilingual corpus was queried for sentences
containing at least one of the 50 phrases listed in
Figure 2, or any of their morphological variants.
The resulting sentences were then grouped on the
basis of whether or not they shared the same En-
glish translation. To find the English phrase (i.e.,
our sense tag) which constrains the sense of the
Spanish phrase, we followed the heuristics used in
phrase-based SMT to extract the minimal phrase
pair that includes the Spanish phrase and is con-
sistent with the word-alignment4 (Koehn et al,
2003). We discarded groups containing fewer than
five sentence pairs and randomly sampled 2-6 con-
texts per Spanish phrase. The resulting test set is
made of 258 Spanish phrases in context such as
the one exemplified in Figure 3.
4.3 Paraphrasing
Nine pivot languages were used to constrain para-
phrase extraction following the approach pre-
sented in Section 3. The conditional probabil-
ity distributions over phrase pairs in Equation 2
are estimated using relative frequencies. For each
Spanish phrase in the test set, we retrieve their
4Note that we did not use gold-standard word-alignments.
205
paraphrase candidates grouped by sense (English
translation) and rank them based on the evidence
collected from all bilingual corpora. Evidence
from different pivot languages is combined using
their average. English itself was not used as a pivot
language. It was used only to provide sense tags.
The rationale behind this choice is that if the lan-
guage used to provide sense tags is also used as
pivot language, there is no obvious way of esti-
mating p(q|f) in Equation 2. Note that in this case
this probability would represent the likelihood of
the English phrase aligning to itself.
Similar to (Bannard and Callison-Burch, 2005),
we weight our paraphrase probabilities using an
LM to adjust it to the context of the input sentence.
We use a 5-gram LM trained on the Spanish part of
Europarl with the SRILM toolkit (Stolcke, 2002).
Paraphrases are re-ranked in context by multiply-
ing the paraphrase probability and the LM score of
the sentence.5
In order to assess the performance of our model,
we compare it to two variants of the models pro-
posed by Bannard and Callison-Burch (2005).
multi: the paraphrasing model with multilingual
constraints introduced in this paper.
CCB: the model in (Bannard and Callison-
Burch, 2005) which does not explicitly per-
form any sense disambiguation.
CCB-wsd: an extended model in (Bannard and
Callison-Burch, 2005) using English phrases
as sense tags for pivoting.
Using each of these three models, we para-
phrased the 258 samples in our test set, retrieving
the 3-best paraphrases in context for each model.
CCB is used with 10 pivot languages (English is
included as a pivot) to generate paraphrase candi-
dates. Note that CCB relies solely on the LM com-
ponent to fit the paraphrase candidate to the con-
text. On the other hand, CCB-wsd and multi both
have access to sense annotation, but while multi
is able to benefit from multiple pivot languages,
CCB-wsd can only pivot through the one English
phrase provided as sense annotation.
5Given the localised effect of the phrase replacement
within a given context in terms of n-gram language mod-
elling, a neighbourhood of n-1 words on each side of the
selected phrase is sufficient to re-rank paraphrase candidates:
p(w?4 . . . w?1e2w+1 . . . w+4) for our 5-gram LM.
4.4 Evaluation
To assess whether the proposed model effectively
disambiguates senses of candidate paraphrases,
we perform experiments using similar settings
to those in (Bannard and Callison-Burch, 2005).
Paraphrases are evaluated in context (a sentence)
using binary human judgements in terms of the
following components:
Meaning (M): whether or not the candidate con-
veys the meaning of the original phrase; and
Grammar (G): whether or not the candidate pre-
serves the fluency of the sentence.
These two components are assessed separately and
a paraphrase candidate is considered to be cor-
rect only when it is judged to be both meaning
preserving and grammatical. Our evaluators were
presented with one pair of sentences at a time, the
original one and its paraphrased version. For ev-
ery test sample we selected the 3-best paraphrases
of each method and distributed them amongst the
evaluators. We considered two evaluation scenar-
ios:
Gold-standard translations: the English trans-
lation as found in Europarl was taken as
sense tag, using automatic word-alignments
to identify the English phrase that constrains
the sense of the Spanish phrase.
SMT translations: a phrase-based SMT system
built using the Moses toolkit (Koehn et al,
2007) and the whole Spanish-English dataset
(except the sentences in the test set) was
used to translated the Spanish sentences. In-
stead of gold-standard translations as a quasi-
perfect sense annotation (quasi because the
word-alignment is still automatic and thus
prone to errors), the phrase-based SMT sys-
tem plays the role of a sense annotation mod-
ule predicting the ?sense? tags.
Note that models may not be able to produce
a paraphrase for certain input phrases, e.g. when
the input phrase is not found in the bilingual cor-
pora. Therefore, we assess precision (P) and re-
call (R) as the number of paraphrases in context
that are judged correct out of the number of cases
for which a candidate paraphrase was proposed,
and out of the total number of test samples, re-
spectively. To summarise the results, accuracy is
expressed in terms of F1.
206
Method Top M G CorrectF1 F1 P R F1
CCB 1 32 28 25 25 25
CCB-wsd 1 61 38 34 28 30
multi 1 62 55 59 42 49
CCB 2 41 37 33 33 33
CCB-wsd 2 68 44 40 33 36
multi 2 71 64 66 47 55
CCB 3 46 42 37 37 37
CCB-wsd 3 71 47 45 36 40
multi 3 74 67 71 50 59
Table 2: Performance in retrieving paraphrases in
context using gold-standard translations for sense
tags and a 5-gram LM component.
In the following section we present results on
whether the best candidate (Top-1) or at least one
of the two (Top-2) or three (Top-3) best candidates
satisfies the criterion under consideration (mean-
ing/grammar).
5 Results
The evaluation was performed by seven native
speakers of Spanish who judged a total of 5, 110
sentences containing one paraphrased input phrase
each. We used 40 overlapping judgements across
annotators to measure inter-annotator agreement.
The average inter-annotator agreement in terms
of Cohen?s Kappa (Cohen, 1960) is 0.54 ? 0.15
for meaning judgements, 0.63 ? 0.16 for gram-
mar judgements and 0.62 ? 0.20 for correctness
judgements. These figures are similar or superior
to those reported in (Bannard and Callison-Burch,
2005; Callison-Burch, 2008), which we consider
particularly encouraging as in our case we have
seven instead of only two annotators. In Tables
2, 3 and 4 we report the performance of the three
models in terms of precision, recall and F1, with
p-values < 0.01 based on the t-test for statistical
significance.
5.1 Paraphrasing from human translations
We first assess the paraphrasing models us-
ing gold-standard translations, that is, the En-
glish phrases were selected via automatic word-
alignments between the input text and its corre-
sponding human translation from Europarl. Ta-
ble 2 shows the performance in terms of F1 for
our three criteria: meaning preservation, grammat-
icality, and correctness. Our method (multi) out-
performs the best performing alternative (CCB-
wsd) by a large margin. It is 19% more effective
at selecting the 1-best candidate in terms of cor-
Method M G Correct
CCB 33 23 22
CCB-wsd 19 9 8
multi 64 43 37
Table 3: Performance (F1) in correctly retrieving
the best paraphrase in context using gold-standard
translations without the 5-gram LM component.
rectness. A consistent gain is also observed when
more guesses are allowed (top 2?3), showing that
our model is better at ranking the top candidates
as well. CCB-wsd and multi are close in terms of
paraphrases that are meaning preserving, however
their differences become more obvious as more
guesses are allowed, again showing that multi is
better at ranking more adequate paraphrases first.
Moreover, multi consistently chooses more gram-
matical paraphrases.
Table 2 also shows that our model consistently
improves both the precision and recall of the pre-
dictions. Recall improves by 14% w.r.t. CCB-wsd
because multi is able to find more paraphrases,
which we believe are only reachable through the
additional pivots. For example, in our data the
paraphrase forma ? medio in the sense of way
(see Table 5) is only found through the Dutch
pivot middel, which is not accessible to CCB-
wsd. Recall is much lower in CCB because of
the model?s strong bias towards the most frequent
senses: other senses receive very little of the prob-
ability mass and thus rarely feature amongst the
top ranked paraphrases. Our multilingual disam-
biguation model also shows a 25% increase in pre-
cision, which must be due to the stronger contri-
bution of the sense discrimination over the LM
component in getting the senses of the paraphrases
right.
To show the impact of the LM re-ranking com-
ponent, in Table 3 we remove this component from
all models, such that the ranking of paraphrases is
done purely based on the paraphrase probabilities.
All models are harmed by the absence of the LM
component, but to different extents and for differ-
ent reasons. CCB typically ranks at the top para-
phrases that convey the most frequent sense and
the LM is the only component with information
about the input context. CCB-wsd is impacted the
most: typically invalid paraphrases are produced
from unrelated senses of the foreign phrase used
as sense tag, they do not represent any valid sense
of the input but still get ranked at the top. For
207
this model, the LM component is crucial to prune
such unrelated paraphrases. Back to Table 2, the
superior performance of CCB-wsd over CCB in
the presence of the LM component suggest that
CCB-wsd assigns less negligible probabilities to
the paraphrases that convey a valid sense of the
input. Finally, multi?s performance is only truly
harmed in terms of grammaticality: sense discrim-
ination is the main responsible for selecting the
appropriate sense, while the LM component is re-
sponsible for selecting the candidate that makes
the sentence more fluent. Further investigation
showed that in some cases the most meaning pre-
serving option was down-weighted due to low flu-
ency, and a less adequate option was chosen, ex-
plaining the slight improvement under the mean-
ing preservation criterion when no LM re-ranking
is performed.
Table 5 lists the 5-best paraphrases of the Span-
ish phrase forma in its different senses. The para-
phrases are ranked by CCB-wsd and multi out of
context, that is, without LM re-ranking. Note that,
because the sense tags are themselves ambiguous
in English, most of the top-ranked paraphrases
from CCB-wsd are inadequate, that is, they do not
convey any valid sense of forma.
It is also interesting to observe the impact of the
different pivot languages on the performance of
our proposed approach. Figure 4 shows CCB-wsd
and multi, both using LM re-ranking. For multi
we can see the impact of the pivot languages indi-
vidually and in groups.6 Except for Finnish when
used on its own as pivot all other setups are supe-
rior to CCB-wsd. We can also see that putting to-
gether languages of different families has a strong
positive impact, probably due to the fact that am-
biguities are realised differently in languages that
are farther from each other, emphasising the po-
tential of sense discrimination by pivoting through
multiple languages.
5.2 Paraphrasing from machine translations
Finally, we assessed the paraphrasing models us-
ing machine translations instead of gold-standard
translations from Europarl. In order to have an
idea of the quality of the SMT model beforehand,
we evaluated the machine translations in terms of
BLEU scores (Papineni et al, 2002) using a single
reference from Europarl. Our phrase-based SMT
6For a larger version of this figure, we refer the reader
to: http://pers-www.wlv.ac.uk/?in1676/
publications/2013/conll2013pivots.pdf
Method Top M G CorrectF1 F1 P R F1
CCB-wsd 1 71 39 34 32 33
multi 1 69 55 50 45 48
CCB-wsd 2 79 46 40 38 39
multi 2 82 69 63 57 60
CCB-wsd 3 83 50 44 41 42
multi 3 85 74 69 62 65
Table 4: Performance in retrieving paraphrases in
context using machine translations for sense tags
and a 5-gram LM component.
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
en fi el da de fr nl pt sv it sv,fi el,fi sv,el
it,sv it,fi it,el sv,el,fi
it,sv,el
it,sv,fi
it,el,fi
it,sv,el,fi
sv sv,nl
sv,nl,de
sv,nl,de,da
it it,pt it,pt,fr
R,sv R,sv,nl
R,sv,nl,de
R,D R,D,G
R,D,G,F
Co
rre
ctn
ess
Pivot Languages
CCB-wsd
1 pivot
2 families
3 families
4 families
Germanic (1-4)
Romance (1-3)
Romance-All (4-9)
Figure 4: Impact of pivot languages on correct-
ness. Language codes follow the convention pre-
sented in Section 4.1. Additionally R stands for
Romance languages, D for Germanic languages,
G for Greek and F for Finnish.
model achieved 48.9 BLEU, which can be con-
sidered a high score for Europarl data (in-domain
evaluation). Table 4 is analogous to Table 2, but
with paraphrases extracted from machine trans-
lated sentences as opposed to human translations.
We observe that multi still outperforms CCB-
wsd by a large margin. On the one hand there is a
drop in precision of about 9% for correctness with
multi. On the other hand there is an improvement
in recall: multi improves from 3% (top-1 guess)
to 12% (top-3 guesses). Manual inspection re-
vealed that the tags predicted by the SMT model
are more frequent translation options, reducing the
chance of finding rare target phrases as sense an-
notation, for which significant statistics cannot be
computed. However, with respect to correctness,
the differences between this setting and that with
gold-standard translations are not statistically sig-
nificant.
208
multi: English as sense annotation and nine other pivot languages
forma ? way forma ? form forma ? means forma ? kind
forma 0.34 forma 0.64 medio 0.64 tipo 0.37
manera 0.24 tipo 0.10 trave?s 0.23 forma 0.23
modo 0.23 forma de 0.05 instrumento 0.13 especie 0.06
forma de 0.02 formas 0.03 especie de 0.03
medio 0.02 modo 0.02 tipo de 0.03
CCB-wsd: English as sense annotation and sole evidence for pivoting
forma ? way forma ? form forma ? means forma ? kind
?way 0.08 ?formulario 0.18 ?significa contar 0.07 ?amables 0.16
?v??a por 0.08 de sus formas 0.10 medios que tiene 0.07 ?kind 0.12
?camino que hay 0.07 ?formulario de 0.07 ?significa 0.06 especie 0.09
?camino que hay que 0.07 modalidad 0.06 ?significa contar con 0.06 ?amable 0.08
?v??a por la 0.07 aspecto formal 0.05 ?anterior significa 0.06 tipo 0.07
Table 5: Top paraphrases of forma annotated by the English words way, form, means and kind. Starred
phrases denote inadequate candidates.
5.3 Potential applications
In what follows we discuss two applications which
we believe could directly benefit from the para-
phrase extraction approach proposed in this paper.
MT evaluation metrics such as METEOR
(Denkowski and Lavie, 2010) and TESLA (Liu
et al, 2010) already use paraphrases of n-grams
in the machine translated sentence in an attempt
to match more of the reference translation?s n-
grams. TESLA, in particular, uses paraphrases
constrained by a single pivot language as sense tag
as originally proposed in (Bannard and Callison-
Burch, 2005). Metrics like METEOR, which use
paraphrases simply as a repository with extra op-
tions for the n-gram matching, could be extended
to use the word-alignment between the source sen-
tence and the translation to constrain the translated
phrases while paraphrasing them with multilingual
constraints. In this case the model would attempt
to paraphrase the MT, which is not necessarily
fluent, therefore potentially compromising its LM
component. However, even after completely disre-
garding the LM re-ranking (see context-insensitive
model multi in Table 3), we may be able to im-
prove n-gram matching by paraphrasing.
Handling out-of-vocabulary words in SMT by
expanding the bilingual phrase-tables (Callison-
Burch et al, 2006) is a direct application of the
sense constrained paraphrases. We can add trans-
lations for a given unknown phrase f1, whose
paraphrase f2 is present in the phrase-table and
is aligned to the target phrase e (sense tag). We
basically expand the phrase table to translate the
out-of-vocabulary word f1 using the knowledge
associated to its paraphrase f2 in the context of the
known translation e: (f2, e) ? (f1, e). The mul-
tilingual constraints offer more control over ambi-
guities, therefore potentially leading to more accu-
rate phrase pairs added to the phrase-table.
6 Conclusions and future work
We have proposed a new formulation of the prob-
lem of generating ?sense? tagged paraphrases for
words and short phrases using bilingual corpora
and multiple pivot languages to jointly disam-
biguate the input phrase and the sense tag. Sense
tags are phrases in a foreign language of interest,
for instance the target language of a phrase-based
SMT system.
The approach was evaluated against the state of
the art method for paraphrase extraction. Signif-
icant improvements were found in particular with
respect to two aspects: i) the proposed model has
higher recall, since it has access to paraphrases
that would receive a negligible probability mass
and therefore would never be selected in previ-
ous formulations, and ii) the proposed model has
higher precision, since it is able to filter out or rank
down paraphrases with incorrect senses.
In future work we plan to further evaluate the
approach in the two scenarios discussed in Sec-
tion 5.3: i) to expand the phrase table of SMT sys-
tems to address issues such as out-of-vocabulary
words and phrases; and ii) to evaluate and opti-
mise parameters of SMT systems using metrics
that can accommodate sense disambiguated para-
phrases. We also plan to integrate syntactic con-
straints, as proposed in (Callison-Burch, 2008), to
our model to investigate the complementarities be-
tween these two ways of constraining paraphras-
ing.
209
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 597?604, Ann
Arbor, Michigan.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17?24,
New York, New York.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?08, pages
196?205, Honolulu, Hawaii.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense dis-
ambiguation. In The 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?07, pages 61?72, Prague, Czech
Republic.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33?40, Prague, Czech Republic.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46, April.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use of
multi-parallel corpora. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, Prague, Czech Republic.
Michael Denkowski and Alon Lavie. 2010.
METEOR-NEXT and the METEOR Paraphrase Ta-
bles: Improved Evaluation Support For Five Target
Languages. In Proceedings of the ACL 2010 Joint
Workshop on Statistical Machine Translation and
Metrics MATR.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 48?54, Edmonton,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics: Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In The Proceedings
of the Tenth Machine Translation Summit, pages 79?
86, Phuket, Thailand. AAMT, AAMT.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 354?
359, Uppsala, Sweden.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for param-
eter tuning in statistical machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 120?127, Prague, Czech
Republic.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
381?390, Suntec, Singapore.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 791?
799, Suntec, Singapore.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical align-
ment models. Computational Linguistics, 29:19?51,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evalua-
tion methods for word sense disambiguation. Nat.
Lang. Eng., 5(2):113?133.
210
Stefan Riezler, Er Vasserman, Ioannis Tsochantaridis,
Vibhu Mittal, and Yi Liu. 2007. Statistical machine
translation for query expansion in answer retrieval.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 464?
471, Prague, Czech Republic.
Lucia Specia, Mark Stevenson, Maria das Grac?as
Volpe Nunes, and Gabriela C.B. Ribeiro. 2006.
Multilingual versus monolingual WSD. In Pro-
ceedings of the EACL Workshop ?Making Sense of
Sense: Bringing Psycholinguistics and Computa-
tional Linguistics Together?, pages 33?40, Trento,
Italy.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language, vol-
ume 2, pages 901?904, Denver, CO.
211
