Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 667?676, Dublin, Ireland, August 23-29 2014.
Review Topic Discovery with Phrases using the P?lya Urn Model 
 
 
Geli Fei 
Department of Computer 
Science, University of Illi-
nois at Chicago, Chicago, 
USA 
gfei2@uic.edu 
Zhiyuan Chen 
Department of Computer  
Science, University of Illi-
nois at Chicago, Chicago, 
USA 
czyuanacm@gmail.com 
Bing Liu 
Department of Computer  
Science, University of Illi-
nois at Chicago, Chicago, 
USA 
liub@cs.uic.edu 
  
 
Abstract 
Topic modelling has been popularly used to discover latent topics from text documents. Most existing 
models work on individual words. That is, they treat each topic as a distribution over words. However, 
using only individual words has several shortcomings. First, it increases the co-occurrences of words 
which may be incorrect because a phrase with two words is not equivalent to two separate words. These 
extra and often incorrect co-occurrences result in poorer output topics. A multi-word phrase should be 
treated as one term by itself. Second, individual words are often difficult to use in practice because the 
meaning of a word in a phrase and the meaning of a word in isolation can be quite different. Third, 
topics as a list of individual words are also difficult to understand by users who are not domain experts 
and do not have any knowledge of topic models. In this paper, we aim to solve these problems by 
considering phrases in their natural form. One simple way to include phrases in topic modelling is to 
treat each phrase as a single term. However, this method is not ideal because the meaning of a phrase is 
often related to its composite words. That information is lost. This paper proposes to use the generalized 
P?lya Urn (GPU) model to solve the problem, which gives superior results. GPU enables the connection 
of a phrase with its content words naturally. Our experimental results using 32 review datasets show 
that the proposed approach is highly effective. 
1 Introduction 
Topic models such as LDA (Blei et al., 2003) and pSLA (Hofmann 1999) and their extensions have 
been popularly used to find topics in text documents. These models are mostly governed by the phe-
nomenon called ?higher-order co-occurrence? (Heinrich 2009), i.e., how often terms co-occur in differ-
ent contexts. Word w1 co-occurring with word w2 which in turn co-occurs with word w3 denotes a sec-
ond-order co-occurrence between w1 and w3. Almost all these models regard each topic as a distribution 
over words. The words under each topic are often sorted according to their associated probabilities. 
Those top ranked words are used to represent the topic. However, this representation of topics as a list 
of individual words has some 1major shortcomings: 
? Topics are often difficult to understand or interpret by users unless they are domain experts and also 
knowledgeable about topic models. In most real-life situations, these are not the case. In some of our 
applications, we show users several good topics, but they have no idea what they are because many 
domain phrases cannot be split to individual words. For example, ?battery? and ?life? are put under 
the same topic, which is not bad. But the users wondered why ?battery? and ?life? are the same 
because they thought words under a topic should somehow have similar meanings. We had to explain 
that it is due to ?battery life.? As another example, sentences such as ?This hotel has a very nice 
sandy beach? may cause a topic model to put ?hotel? and ?sandy? in a topic, which is not wrong but 
again it is hard to understand by a user who may not be able to connect the two words. Thus in order 
to interpret topics well, the user must know the phrases (they are split into individual words) that may 
                                                 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
 
667
be used in a domain and how words may be associated with each other. To make the matters worse, 
in most cases, the topics generated from a topic model are not perfect. There are some wrong words 
under a topic, which make the interpretation even harder.  
? Individual words are difficult to use in practice because in some cases a word under a topic may not 
have its intended meaning for the topic in a particular sentence context. This can cause many mis-
takes. For example, in sentiment analysis of product reviews, a topic is often regarded as a set of 
words indicating a product feature or attribute. This is not true in many cases. For example, if ?bat-
tery? and ?life? are put in one topic, when the system sees ?life,? it assumes it is related to ?battery.? 
But in the sentence ?The life expectancy of the machine is about 2 years,? this ?life? has nothing to 
do with battery or battery life. This causes an error. If the system can directly use phrases, ?battery 
life? and ?life expectancy,? the error will not occur.   
? Splitting phrases into multiple individual words causes extra co-occurrences that may result in poor 
or wrong topics involving other words. For example, due to sentences like ?Beach staffs are rude? 
and ?The hotel has a nice sandy beach,? a topic model may put ?staff? and ?sandy? under a topic for 
staff and/or put ?beach? and ?rude? together under the topic of beach views.   
Based on our experiences in opinion mining and social media mining, these are major issues with 
topic models. We believe that they must be dealt with before wide spread adaptation of topic models in 
real-life applications. In this paper, we make an attempt to solve this problem. We will use term to 
represent both word and phrase, and use word or phrase when we want to distinguish them.  
One obvious way to consider phrases is to use a natural language parser to find all phrases and then 
treat each phrase as one term, e.g., ?battery life,? ?sandy beach? and ?beach staff.? However, the prob-
lem with this approach is that it may lose the connection of many related words or phrases in a topic. 
For example, under the topic for beach, we may not find ?sandy beach? because there is no co-occur-
rence of ?sandy beach? and ?beach? if we treat ?sandy beach? as a single term. This is clearly not a good 
solution as it may miss a lot of topical terms (words or phrases) for a topic. It can also result in poor 
topics due to the loss of co-occurrences.  
Another obvious solution is to use individual words as they are, but add an extra term representing 
the phrase. For example, we can turn the sentence ?This hotel has a nice sandy beach? to ?This hotel 
has a nice sandy beach <sandy beach>.? This solution helps deal with the problem of losing co-occur-
rences to some extent, but because the words are still treated individually, the three problems discussed 
above still exist, although the phrase ?sandy beach? now can show up in some topics. However, due to 
the fact that phrases are obviously less frequent than individual words, they may be ranked very low, 
which make little difference to solving the three problems. 
In this paper, we propose a novel approach to solve the problem, which is based on the generalized 
P?lya urn (GPU) model (Mahmoud 2008). GPU was first introduced into LDA in (Mimno et al., 2011) 
to concentrate words with high co-document frequency. However, Mimno et al. (2011) and other re-
searchers Chen et al., (2013) still use them in the framework of individual words. In the GPU model, we 
can deal with the problems above by treating phrases as individual terms and allowing their component 
words to have some connections or co-occurrences with them. Furthermore, we can push phrases up in 
a topic as phrases are important for understanding but are usually less frequent than individual words 
and ranked low in a topic. The intuition here is that when we see a phrase, we also see a small fraction 
of their component words; and when we see each individual word, we also see a small fraction of its 
related phrases. Further, in a phrase not all words are equally important. For example, in ?hotel staff?, 
?staff? is more important as it is the head noun, which represents the semantic category of the phrase. 
Our experiments are conducted using online review collections from 32 domains. We will see that 
the proposed method produces significantly better results both quantitatively based on the statistical 
measure of topic coherence and qualitatively based on human labeling of topics and topical terms. 
In summary, this paper makes the following contributions: 
1. It proposes to consider phrases in topic models, which as we have explained above, is important 
for accurate topic generation, the use of the resulting topics and human interpretation. As we will 
see in Section 2, although some prior works exist, they are based on n-grams (Mukherjee and Liu, 
2013). They are different from our approach. N-grams can generate many non-understandable 
phrases. Furthermore, due to infrequency of n-grams (much less frequent than individual words), 
668
typically a huge amount of data is needed in order to produce reasonable topics, which many ap-
plications simply do not have.  
2. It proposes to use the generalized P?lya Urn (GPU) model to deal with the problems arising in 
considering phrases. To the best of our knowledge, the GPU model has not been used in the context 
of phrases. This model not only generates better topics, but also rank phrases relatively high in 
their topics, which greatly helps understanding of the generated topics. 
3. Comprehensive experiments conducted using product and service review collections from 32 do-
mains demonstrate the effectiveness of the proposed model. 
2 Related Work 
GPU was first introduced to topic modelling in (Mimno et al., 2011), in which GPU is used to concen-
trate words with high co-document frequency based on corpus-specific co-occurrence statistics. Chen et 
al. (2013) applied GPU to deal with the adverse effect of using prior domain knowledge in topic 
modeling by increasing the counts of rare words in the knowledge sets. However, these works still use 
only individual words. 
Topics in most topic models like LDA are unigram distributions over words and assume words to be 
exchangeable at the word level. However, there exists some work that tries to take word order into 
consideration by including n-gram language models. Wallach (2006) proposed the Bigram Topic Model 
(BTM) which integrates bigram statistics with topic-based approaches to document modeling. Wang et 
al. (2007) proposed the Topical N-gram Model (TNG), which is a generalization of the BTM. It 
generates words in their textual order by first sampling a topic, then sampling its status as a unigram or 
bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Although the 
?bag-of-words? assumption does not always hold in real-life applications, it offers a great computational 
advantage over more complex models taking word order into account for discovering significant n-
grams. Our approach is different from these works in two ways. First, we still follow the ?bag-of-words? 
or rather ?bag-of-terms? assumption. Second, we find actual phrases rather than just n-grams. Most n-
grams are still hard to understand because they are not natural phrases.   
Blei and Lafferty (2009), Liu et al. (2010) and Zhao et al. (2011) also try to extract keyphrases from 
texts. Their methods, however, are very different because they identify multi-word phrases using 
relevance and likelihood scores in the post-processing step based on the discovered topical unigrams. 
Mukherjee and Liu (2013) and Mukherjee et al. (2013) all try to include n-grams to enhance the 
expressiveness of their models while preserving the advantages of ?bag-of-words? assumption, which 
has a similar idea as our paper. However, as we point out in the introduction, this way of including 
phrases/n-grams suffers from several shortcomings. Solving these problems is the goal of our paper. 
Finally, since we use product reviews as our datasets, our work is also related to opinion mining using 
topic models, e.g. (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008; Zhao et al., 2010; 
Li et al., 2010; Sauper and Barzilay, 2013; Lin and He, 2009; Jo and Oh, 2011). However, none of these 
models uses phrases. 
3 Proposed Model 
We start by briefly reviewing the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003). Then we 
describe the simple P?lya urn (SPU) model, which is embedded in LDA. After that, we present the 
generalized P?lya urn (GPU) model and discuss how it can be applied to our context. The proposed 
model uses GPU for its inference. It shares the same graphical model as LDA. However, the GPU in-
ference mechanism is very different from that of LDA, which cannot be reflected in the graphical model 
or the generative process as it only helps to infer more desirable posterior distributions of topic models. 
3.1 Latent Dirichlet Allocation 
LDA is a generative probabilistic model for a document collection. It assumes that documents are rep-
resented as a mixture of latent topics, and each latent topic is characterized by a distribution over terms. 
In order to generate a term ??
(?)
 in document ?, where ? is its position, we first draw a discrete topic 
assignment ??
(?)
 from a document-specific distribution over ? topics ??, which is drawn from a prior 
Dirichlet distribution with hyperparameter ?. Then we draw a term from the topic-specific distribution 
669
over the vocabulary ?
??
(?), which is drawn from a prior Dirichlet distribution with hyperparameter ?. 
For inference, instead of directly estimating ? and ?, Gibbs sampling is used to approximate them 
based on the posterior estimates of latent topic assignment ?. The Gibbs sampling procedure considers 
each term in the documents in turn, and estimates the probability of assigning the current term to each 
topic, conditioned on the topic assignments to all other terms. Griffiths and Steyvers (2004) showed this 
could be calculated by: 
 
? (??
(?)
= ?|???,?,?, ?, ?) ?
??|? + ?
?? + ??
?
?
??
(?)
|?
+ ?
?? + ??
 (1) 
where  ??
(?)
= ? represents the topic assignment of term ??
(?)
 to topic ?, and  ???,? refers to the topic 
assignments of all other terms. ? denotes all terms in the document collection, ? denotes the size of 
vocabulary of the collection, ? is the number of topics in the corpus, ??|? is the count of term ? under 
topic ?, ?? = ? ???|??? , and ??|? refers the count of topic ? being assigned to some terms in document 
?, ?? = ? ???|??? . All these counts exclude the current term. 
3.2 Simple P?lya Urn Model 
Traditionally, the P?lya urn model is designed in the context of colored balls and urns. In the context of 
topic models, a term can be seen as a ball of a certain color and the urn contains a mixture of balls with 
various colors. The classic topic-word (or topic-term) distribution can be reflected by the color propor-
tion of balls in the urn. LDA follows the simple P?lya urn (SPU) model, which works as follows: when 
a ball of a particular color is drawn from an urn, that ball is put back to the urn along with another ball 
of the same color. This process corresponds to assigning a topic to a term in the Gibbs sampler of LDA. 
Based on the topic-specific ?collapsed? probability of a term ? given topic ?, 
??|?+?
??+??
, which is essen-
tially the second ratio in (1), drawing a term ? will only increase the probability of seeing ? in the 
future sampling process. This self-reinforcing property is known as ?the rich get richer?. In the next 
subsection, we will introduce the generalized P?lya urn (GPU) model, which increases the probability 
of seeing certain other terms when we sample a term. 
3.3 Generalized P?lya Urn Model 
The generalized P?lya urn (GPU) model differs from SPU in that, when a ball of a certain color is 
drawn, two balls of that color is put back along with a certain number of balls of some other colors. 
Unlike SPU, GPU sampling not only allows us to see a ball of the same color again with higher proba-
bility, but also increases the probability of seeing balls with certain other colors. These additional balls 
of certain other colors added to the urn increase their proportions in the urn. We call this the promotion 
of these colored balls. Applying the idea, there are two directions of promotion in our application (Note 
that in each sentence, we need to identify each phrase, but do not need to add any extra information): 
1. Word to phrase: When an individual word is assigned to a topic (analogous to drawing a ball of 
a certain color), each phrase containing the word will be promoted, meaning that the phrase will 
be added to the same topic with a small count. That is, a fraction of the phrase will be assigned to 
the topic. This is justified because it is reasonable to assume that the phrase is related to the word 
to some extent in meaning.  
2. Phrase to word: When a phrase is assigned to a topic, each component word in it is also promoted 
with a certain small count. That is, each word is also assigned the topic by a certain amount. In 
most cases, the head nouns are more important. Thus, we promote the head nouns more. For 
example, in ?hotel staff?, ?staff? is the head noun that determines the category of the noun phrase. 
The rationale of this promotion is similar to that above.  
Let ??
(?)
 be a word and ?_? be the word itself or a phrase containing the word ??
(?)
. ? represents a 
term, and ?_? indicates all the related terms of ?. The new GPU sampling is as follows:  
 
? (??
(?)
= ?|???,?,?, ?, ?, ?) ?
??|? + ?
?? + ??
?
? ??_?|???_?,??
(?) + ??_?
? ? ??_?|???_?,??_?? + ??
 (2) 
670
where ?  is a ? ? ? real-value matrix, each cell of which contains a real value virtualcount, indicating 
the amount of promotion of a term under a topic when assigning this topic to another term. ? is size of 
all terms. The new model retains the document-topic component of standard LDA, which is the first 
ratio in (1), but replaces the usual P?lya urn topic-word (topic-term) component, the second ratio in (1), 
with a generalized P?lya urn framework (Mahmoud 2008; Mimno et al., 2011). The simple P?lya urn 
model is a simplified version of GPU in which matrix ? is an identity matrix. In this paper, ? is an 
asymmetric matrix because the main goal of using GPU is to promote the less frequent phrases in the 
documents. 
4 EXPERIMENTS 
In this section, we evaluate the proposed method of considering phrases in topic discovery, and compare 
it with three baselines. The first baseline discovers topics using LDA in a traditional way without con-
sidering phrases, i.e., using only individual words. We refer to this baseline as LDA(w). The second 
baseline considers phrases by treating each whole phrase as a separate term in the corpus. We refer to 
this baseline as LDA(p). The third baseline considers phrases by keeping individual component words 
in the phrases as they are, but also adding phrases as extra terms. We refer to this baseline as LDA(w_p). 
We refer to our proposed method as LDA(p_GPU). Note that for those words that are not in any phrases, 
they are treated as individual words (or unigrams). 
Data Set: We use product reviews from 30 sub-categories (types of product) in the electronics domain 
from Amazon.com. The sub-categories are ?Camera?, ?Mouse?, ?Cellphone,? etc (see the whole list 
below Figure 1). Each domain contains 1,000 reviews. Besides, we also use a collection of hotel reviews 
and a collection of restaurant reviews from TripAdvisor.com and Yelp.com. The hotel review data con-
tains 101,234 reviews, and the restaurant review data contains 25,459 reviews. We thus have a total of 
32 domains. We ran the Stanford Parser to perform sentence detection, lemmatization and POS tagging. 
Punctuations, stopwords, numbers and words appearing less than 5 times in each dataset are removed. 
Domain names are also removed, e.g., word ?camera? for the domain Camera, since it co-occurs with 
most words in the dataset, leading to high similarity among topics/aspects. 
Sentences as Documents: As noted in (Titov and McDonald, 2008), when standard topic models are 
applied to reviews as documents, they tend to produce topics that correspond to global properties of 
products (e.g., product brand name), but cannot separate different product aspects or features well. The 
reason is that all reviews of the same product type basically evaluate the same aspects of the product 
type. Only the brand names and product names are different. Thus, using individual reviews for model-
ling is ineffective for finding product aspects or features, which are our topics. Although there are ap-
proaches which model sentences (Jo and Oh, 2011; Zhao et al., 2010; Titov and McDonald, 2008), we 
take the approach in (Brody and Elhadad, 2010; Chen et al., 2013), dividing each review into sentences 
and treating each sentence as an independent document. 
Noun Phrase Detection: Although there are different types of phrases, in this first work we focus 
only on noun phrases as they are more representative of topics in online reviews. We will deal with other 
types of phrases in the future. Our first step is thus to obtain all noun phrases from each domain. Due to 
the efficiency issue of full natural language parser with a huge number of reviews, instead of applying 
the Stanford Parser to recognize noun phrases, we design a rule-based approach to recognize noun 
phrases as consecutive nouns based on POS tags of sentences. Although the Stanford Parser may give 
us better noun phrases, our simple method serves the purpose and gives us very good results. In fact, 
based on our initial experiments, the Stanford Parser also gives many wrong phrases. 
Parameter Settings: In all our experiments, the posterior inference was drawn after 2000 Gibbs 
sampling iterations with a burn-in of 400 iterations. Following (Griffiths and Steyvers, 2004), we fix the 
Dirichlet priors as follows: for all document-topic distributions, we set ?=50/?, where ? is the number 
of topics. And for all topic-term distributions, we set ?=0.1. We also experimented with other settings 
of these priors and did not notice much difference. 
Setting the number of topics/aspects in topic models is often tricky as it is difficult to know the exact 
number of topics that a corpus has. While non-parametric Bayesian approaches (Teh et al., 2005) do 
exist for estimating the number of topics, it?s not the focus of this paper. We empirically set the number 
of topics to 15. Although 15 may not be optimum, since all models use the same number, there is no 
bias against any model. 
671
In Section 3.3, we introduced the promotion concept for the GPU model. When we sample a topic for 
a word, we add virtualcount of topic assignment to all its related phrases. However, not all words in a 
phrase are equally important. For example, in phrase ?hotel staff?, ?staff? is more important, and we call 
such words the head nouns. In this work, we apply a simple method used in (Wang et al., 2007), which 
is to always assume that the last word in a noun phrase is the head noun. Although we are aware of the 
potential harm to our model when we promote a wrong word, we will leave it as our future work. Again, 
because we want to connect phrases with their component words and promote the rank of phrases in 
their topics, we add less virtual counts to individual words. Thus, we add 0.5 * virtualcount to the last 
word in a phrase and add 0.25 * virtualcount to all other words. We set virtualcount = 0.1 in our exper-
iments empirically. 
Based on the discovered topics, we conduct statistical evaluation using topic coherence, human eval-
uation and also a case study to quantitatively and qualitatively show the superiority of the proposed 
method in terms of both interpretability and topic wellness. 
4.1 Statistical Evaluation 
Perplexity and KL-divergence are often used to evaluate topic models statistically. However, researchers 
have found that perplexity on held-out documents is not always a good predictor of human judgments 
of topics (Chang et al., 2009). In our application, we are not concerned with the test on future data using 
the hold-out set. KL-divergence measures the difference of distributions, and thus can be used to meas-
ure the distinctiveness of topics. However, distinctiveness of topics does not necessarily mean human 
agreeable topics. Recently, Mimno et al. (2011) proposed a new measure called topic coherence, which 
has been shown to correlate with human judgments of topic quality quite well. Higher topic coherence 
score indicates higher quality of topics, i.e., better topic coherence. Topic coherence is computed as 
below. 
 
??(?; ?(?)) = ? ? ???
? (??
(?)
, ??
(?)
) + 1
? (??
(?)
)
??1
?=1
?
?=2
 (3) 
in which ?(?) is the document frequency of term ? (i.e., the number of documents with at least one 
term ?) and ?(?, ??) is the co-document frequency of term ? and term ?? (i.e., the number of documents 
containing both term ? and term ??). Also, ?(?) = (?1
(?)
, ? , ??
(?)
) is the list of ? most probable terms 
in topic ?. 1 is added as a smoothing count to avoid taking the logarithm of zero. 
We thus use this measure to score all four experiments. Figure 1 and Figure 2 show the topic coher-
ence using top 15 terms and top 30 terms respectively on the 32 different domains. Notice the topic 
coherence is a negative value, and a smaller absolute value is better than a larger one. Firstly, we can 
see from both charts that our proposed model LDA(p_GPU) is better than all other three baselines by a 
large margin. Secondly, the performance of the other three baselines are quite similar. In general, 
LDA(p) is slightly worse than the other two baselines. It is because replacing many words with phrases 
decreases the number of co-occurrences in the corpus. In contrast, LDA(w_p) is slightly better than the 
other two baselines on most domains because some frequent phrases add more reliable co-occurrences 
in the corpus. However, as we point out in the introduction, some problems still exist. Firstly, it does 
not solve the problem of phrases and their component words having different meanings, and thus artifi-
cially creating such wrong co-occurrences may damage the overall performance. Secondly, even if the 
number of co-occurrences increases, most of the phrases are still too infrequent to be ranked high in 
their associated topics to be useful in helping users understand the topic. 
In order to test the significance of the improvement, we conduct paired t-tests on the topic coherence 
results. Using both 15 top terms and 30 top terms, statistical tests show that our proposed method, 
LDA(p_GPU), outperforms all three baselines significantly (p < 0.01). However, there?s no significant 
improvement between any pair of the three baselines. 
4.2 Manual Evaluation 
Although several statistical measures, such as perplexity, KL-divergence and topic coherence, have been 
used to statistically evaluate topic models, since topic models are mostly (including ours) unsupervised, 
672
statistical measures may not always correlate with human interpretations or judgments. Thus, in this 
sub-section, we perform a manual evaluation through manual labeling of topics and topical terms. 
Manual labeling was done by two annotators, who are familiar with reviews and topic models. The 
labeling was carried out in two stages sequentially: (1) labeling of topics and (2) labeling of topical 
terms in each topic. After the first stage, an annotator agreement is computed and then the two annotators 
discuss about the disagreed topics to reach a consensus. Then, they move on to the next stage to label 
the top ranked topical terms in each topic (based on their probabilities in the topic). For the annotator 
 
Figure 1: Topic coherence of the top 15 terms of each model on each of the 32 datasets. Notice that since topic coherence 
is a negative value, a smaller absolute value is better than a larger one.  
Domain/dataset names are listed as follows (1:Amplifier; 2:BluRayPlayer; 3:Camera; 4:CellPhone; 5:Computer; 
6:DVDPlayer; 7:GPS; 8:HardDrive; 9:Headphone; 10:Keyboard; 11:Kindle; 12:MediaPlayer; 13:Microphone; 14:Monitor; 
15:Mouse; 16:MP3Player; 17:NetworkAdapter; 18:Printer; 19:Projector; 20:RadarDetector; 21:RemoteControl; 22:Scan-
ner; 23:Speaker; 24:Subwoofer; 25:Tablet; 26:TV; 27:VideoPlayer; 28:VideoRecorder; 29:Watch; 30:WirelessRouter; 
31:Hotel; 32:Restaurant). 
 
Figure 2: Topic coherence of the top 30 terms of each model on each dataset. Notice again that since topic coherence is a 
negative value, a smaller absolute value is better than a larger one. X-axis indicates the domain id numbers, whose names 
are listed below Figure 1. 
  
Figure 3: Human evaluation on five domains using top 15 and top 30 terms. X-axis indicates the domain id numbers, whose 
corresponding domain names are listed below Figure 1. Y-axis indicates the ratio of correct topic terms. 
  
 
 
 
-440
-390
-340
-290
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
To
p
ic
 C
o
h
er
en
ce
Top 15 Terms Topic Coherence
LDA(w) LDA(p) LDA(w_p) LDA(p_GPU)
-1750
-1550
-1350
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
To
p
ic
 C
o
h
er
en
ce
Top 30 Terms Topic Coherence
LDA(w) LDA(p) LDA(w_p) LDA(p_GPU)
0.4
0.6
0.8
1
31 32 29 25 16
Top 15 Terms
LDA(w) LDA(p)
LDA(w_p) LDA(p_GPU)
0.3
0.5
0.7
0.9
31 32 29 25 16
Top 30 Terms
LDA(w) LDA(p)
LDA(w_p) LDA(p_GPU)
673
agreement, we compute Kappa scores. The Kappa score for topic labeling is 0.838, and the Kappa score 
for topical terms labeling is 0.846. Both scores indicate strong agreement in the labeling. 
Evaluation measure. A commonly used evaluation measure in human evaluation is precision@n (or 
P@n for short), which is the precision at a particular rank position n in a topic. For example, Preci-
sion@5 means the precision of the top ranked 5 terms for a topic. To be consistent with the automatic 
evaluation, we use Precision@15 and 30. Top 15 terms is usually sufficient to represent the topic. How-
ever, since we include phrases in our experiments which may lead to some other terms ranked lower 
than using only words, we labeled up to top 30 terms. The Precision@n measure is also used in (Zhao 
et al., 2010) and some others, e.g., (Chen et al., 2013). 
In our experiments, we labeled four results for each domain, i.e., those of LDA(w), LDA(p), LDA(w_p) 
and LDA(p_GPU). Due to the large amount of human labeling effort, we only labeled 5 domains. We 
find that it is sometimes hard to figure out what some of the topics are about and whether some terms 
are related to a topic or not, so we give the results to our human evaluators together with the phrases in 
each domain extracted by our rules in order to let them be familiar with the domain vocabulary. The 
human evaluation results are shown in Figure 3. 
Results and Discussions. Again, we conduct paired t-tests on the human evaluation results of top 15 
and 30 terms. Statistical tests show that our proposed method, LDA(p_GPU), outperforms all other three 
methods significantly (p < 0.05) using both top 15 and top 30 terms. However, there?s no significant 
improvement between any pair of the three baselines. 
4.3 Case Study 
In order to illustrate the importance of phrases in enhancing human readability, we conduct case study 
using one topic from each of the five manually labeled domains. Due to space limitations, we only 
compare the results of our model LDA(p_GPU) with LDA(w). 
Table 1: Example topics discovered by LDA(w) and LDA(p_GPU) 
Hotel Restaurant Watch 
LDA(w) LDA(p_GPU) LDA(w) LDA(p_GPU) LDA(w) LDA(p_GPU) 
bed clean service service hand big 
comfortable comfortable star friendly minute hand 
small quiet staff server hour minute 
sleep sleep atmosphere staff beautiful cheap 
size large friendly atmosphere casual hour 
large spacious server waiter christmas automatic 
tv size waiter attentive setting seconds 
pillow king size bed attentive star condition line 
king pillow reason service staff worth hour hand 
chair queen size bed decor star service weight durable 
table bed size quick customer service red analog hand 
mattress bed nd pillow customer table service press hand move 
clean bed sheet waitress delivery service gift hand line 
double bed linen tip rush hour service run seconds hand 
big sofa bed pleasant service attitude functionality hand sweep 
Tablet MP3Player 
LDA(w) LDA(p_GPU) LDA(w) LDA(p_GPU) 
screen screen battery battery 
touch size headphone hour 
software easier life battery life 
hard pro media price 
pad touch screen car worth 
option bigger windows charge 
version area hour replacement 
website inch decent free 
angle screen protector reason market 
car screen size xp aaa battery 
charger inch screen program aa battery 
ipod draw aaa purchase 
worth home screen window hour battery 
gb screen look set aaa 
drive line pair life 
 
 
 
 
 
 
 
 
674
In the above table, we notice that with phrases, the topics are much more interpretable than only 
reading individual words given by LDA(w). For example, ?hand? in ?Watch? domain given by LDA(w) 
is quite confusing at first, but in LDA(p_GPU), ?hour hand? makes it more understandable. Another 
example is ?aaa? in ?MP3Player? domain. It is quite confusing at first, but ?aaa battery? should make it 
more interpretable by an application user who is not familiar with topic models or does not have exten-
sive domain knowledge. Also, due to wrong co-occurrences created by individual words in a phrase, the 
LDA(w) results contain much more noise than those of LDA(p_GPU). 
5 CONCLUSION 
This paper proposed a new method to consider phrases in discovering topics using topic models. The 
method is based on the generalized P?lya urn (GPU) model, which allows us to connect phrases with 
their component words during the inference and rank phrases higher in their related topics. Our method 
preserves the advantages of ?bag-of-words? assumption while preventing the side effects that traditional 
methods have when considering phrases. We tested our method against three baselines across 32 differ-
ent domains, and demonstrated the superiority of our method in improving the topic quality and human 
interpretability both quantitatively and qualitatively. 
 
 
References 
 
David M. Blei and John D. Lafferty. 2009. ?Visualizing Topics with Multi-Word Expressions.? Tech. Report. 
(arXiv:0907.1013). 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. ?Latent Dirichlet Allocation.? Journal of Machine 
Learning Research 993-1022. 
Samuel Brody and Noemie Elhadad. 2010. ?An Unsupervised Aspect-Sentiment Model for Online Reviews.? 
NAACL. Los Angeles, California: ACL. 
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. 2009. ?Reading Tea 
Leaves: How Humans Interpret Topic Models.? Neural Information Processing Systems.  
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013. ?Ex-
ploiting Domain Knowledge in Aspect Extraction.? EMNLP. 
Thomas L. Griffiths, and Mark Steyvers. 2004. ?Finding scientific topics.? Proceedings of National Academy of 
Sciences.  
Gregor Heinrich. 2009. ?A Generic Approach to Topic Models.? ECML PKDD. ACM. Pages 517 - 532. 
Thomas Hofmann. 1999. ?Probabilistic latent semantic analysis.? UAI.  
Yohan Jo and Alice Oh. 2011. ?Aspect and Sentiment Unification Model for Online Review Analysis.? WSDM. 
Hong Kong, China: ACM. 
Chenghua Lin and Yulan He. 2009. ?Joint Sentiment/Topic Model for Sentiment Analysis?. CIKM. Hong Kong, 
China. 
Fangtao Li, Minlie Huang, Xiaoyan Zhu. 2010. ?Sentiment Analysis with Global Topics and Local Dependency?. 
AAAI 
Yue Lu and Chengxiang Zhai. 2008. ?Opinion Integration Through Semi-supervised Topic Modeling.? WWW. 
2008, Beijing, China: ACM. 
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. ?Automatic Keyphrase Extraction via Topic 
Decomposition.? EMNLP.  
Arjun Mukherjee and Bing Liu. 2013. ?Discovering User Interactions in Ideological Discussions.? ACL.  
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and Sharon Meraz. 2013. ?Public Dialogue: Analysis of Tol-
erance in Online Discussions.? ACL.  
David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. ?Optimizing 
Semantic Coherence in Topic Models.? EMNLP. Edinburgh, Scotland, UK: ACL. 
675
Hosan Mahmoud. 2008. Polya Urn Models. Chapman & Hall/CRC Texts in Statistical Science.  
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. ?Topic Sentiment Mixture: 
Modeling Facets and Opinions in Weblogs.? WWW. Banff, Alberta, Canada: ACM. 
Christina Sauper and Regina Barzilay. 2013. ?Automatic Aggregation by Joint Modeling of Aspects and Values?. 
Journal of Artificial Intelligence Research 46 (2013) 89-127 
Ivan Titov and Ryan McDonald. 2008. ?Modeling Online Reviews with Multi-grain Topic Models.? WWW. 2008, 
Beijing, China: ACM. 
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2005. ?Hierarchical Dirichlet Processes.? 
Journal of the American Statistical Association.  
Hanna M. Wallach. 2006. ?Topic Modeling: Beyond Bag-of-Words.? ICML. Pittsburgh, PA: ACM. 
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. ?Topical N-grams: Phrase and Topic Discovery, with an 
Application to Information Retrieval.? ICDM.  
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. ?Jointly Modeling Aspects and Opinions with 
a MaxEnt-LDA Hybrid.? EMNLP. Massachusetts, USA: ACL. 
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li. 2011. 
?Topical Keyphrase Extraction from Twitter.? ACL. 
676
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1655?1667,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Domain Knowledge in Aspect Extraction 
 
 
Zhiyuan Chen, Arjun Mukherjee, 
Bing Liu 
Meichun Hsu, Malu Castellanos, 
Riddhiman Ghosh 
University of Illinois at Chicago HP Labs 
Chicago, IL 60607, USA Palo Alto, CA 94304, USA 
{czyuanacm,arjun4787}@gmail.com, 
liub@cs.uic.edu 
{meichun.hsu, malu.castellanos, 
riddhiman.ghosh}@hp.com 
 
 
 
Abstract 
Aspect extraction is one of the key tasks in 
sentiment analysis. In recent years, statistical 
models have been used for the task. However, 
such models without any domain knowledge 
often produce aspects that are not interpreta-
ble in applications. To tackle the issue, some 
knowledge-based topic models have been 
proposed, which allow the user to input some 
prior domain knowledge to generate coherent 
aspects. However, existing knowledge-based 
topic models have several major shortcom-
ings, e.g., little work has been done to incor-
porate the cannot-link type of knowledge or 
to automatically adjust the number of topics 
based on domain knowledge. This paper pro-
poses a more advanced topic model, called 
MC-LDA (LDA with m-set and c-set), to ad-
dress these problems, which is based on an 
Extended generalized P?lya urn (E-GPU) 
model (which is also proposed in this paper).  
Experiments on real-life product reviews 
from a variety of domains show that MC-
LDA outperforms the existing state-of-the-art 
models markedly. 
1 Introduction 
In sentiment analysis and opinion mining, aspect 
extraction aims to extract entity aspects or features 
on which opinions have been expressed (Hu and 
Liu, 2004; Liu, 2012). For example, in a sentence 
?The picture looks great,? the aspect is ?picture.? 
Aspect extraction consists of two sub-tasks: (1) 
extracting all aspect terms (e.g., ?picture?) from 
the corpus, and (2) clustering aspect terms with 
similar meanings (e.g., cluster ?picture? and ?pho-
to? into one aspect category as they mean the 
same in the domain ?Camera?). In this work, we 
adopt the topic modeling approach as it can per-
form both sub-tasks simultaneously (see ? 2). 
Topic models, such as LDA (Blei et al, 2003), 
provide an unsupervised framework for extracting 
latent topics in text documents. Topics are aspect 
categories (or simply aspects) in our context. 
However, in recent years, researchers have found 
that fully unsupervised topic models may not pro-
duce topics that are very coherent for a particular 
application. This is because the objective functions 
of topic models do not always correlate well with 
human judgments and needs (Chang et al, 2009). 
To address the issue, several knowledge-based 
topic models have been proposed. The DF-LDA 
model (Andrzejewski et al, 2009) incorporates 
two forms of prior knowledge, also called two 
types of constraints: must-links and cannot-links. 
A must-link states that two words (or terms) 
should belong to the same topic whereas a cannot-
link indicates that two words should not be in the 
same topic. In (Andrzejewski et al, 2011), more 
general knowledge can be specified using first-
order logic. In (Burns et al, 2012; Jagarlamudi et 
al., 2012; Lu et al, 2011; Mukherjee and Liu, 
2012), seeded models were proposed. They enable 
the user to specify prior knowledge as seed 
words/terms for some topics. Petterson et al (2010) 
also used word similarity as priors for guidance.  
However, none of the existing models is capable 
of incorporating the cannot-link type of knowledge 
except DF-LDA (Andrzejewski et al, 2009). Fur-
thermore, none of the existing models, including 
DF-LDA, is able to automatically adjust the num-
ber of topics based on domain knowledge. The 
domain knowledge, such as cannot-links, may 
change the number of topics. There are two types 
of cannot-links: consistent and inconsistent with 
the domain corpus. For example, in the reviews of 
1655
domain ?Computer?, a topic model may generate 
two topics Battery and Screen that represent two 
different aspects. A cannot-link {battery, screen} 
as the domain knowledge is thus consistent with 
the corpus. However, words Amazon and Price 
may appear in the same topic due to their high co-
occurrences in the Amazon.com review corpus. To 
separate them, a cannot-link {amazon, price} can 
be added as the domain knowledge, which is in-
consistent with the corpus as these two words have 
high co-occurrences in the corpus. In this case, the 
number of topics needs to be increased by 1 since 
the mixed topic has to be separated into two indi-
vidual topics Amazon and Price. Apart from the 
above shortcoming, earlier knowledge-based topic 
models also have some major shortcomings: 
Incapability of handling multiple senses: A 
word typically has multiple meanings or senses. 
For example, light can mean ?of little weight? or 
?something that makes things visible.? DF-LDA 
cannot handle multiple senses because its defini-
tion of must-link is transitive. That is, if A and B 
form a must-link, and B and C form a must-link, it 
implies a must-link between A and C, indicating A, 
B, and C should be in the same topic. This case 
also applies to the models in (Andrzejewski et al, 
2011), (Petterson et al, 2010), and (Mukherjee and 
Liu, 2012). Although the model in (Jagarlamudi et 
al., 2012) allows multiple senses, it requires that 
each topic has at most one set of seed words (seed 
set), which is restrictive as the amount of 
knowledge should not be limited. 
Sensitivity to the adverse effect of knowledge: 
When using must-links or seeds, existing models 
basically try to ensure that the words in a must-
link or a seed set have similar probabilities under a 
topic. This causes a problem: if a must-link com-
prises of a frequent word and an infrequent word, 
due to the redistribution of probability mass, the 
probability of the frequent word will decrease 
while the probability of the infrequent word will 
increase. This can harm the final topics because 
the attenuation of the frequent (often domain im-
portant) words can result in some irrelevant words 
being ranked higher (with higher probabilities). 
To address the above shortcomings, we define 
m-set (for must-set) as a set of words that should 
belong to the same topic and c-set (cannot-set) as a 
set of words that should not be in the same topic. 
They are similar to must-link and cannot-link but 
m-sets do not enforce transitivity. Transitivity is 
the main cause of the inability to handle multiple 
senses. Our m-sets and c-sets are also more con-
cise providing knowledge in the context of a set. 
As in (Andrzejewski et al, 2009), we assume that 
there is no conflict between m-sets and c-sets, i.e., 
if ?1  is a cannot-word of ?2  (i.e., shares a c-set 
with ?2), any word that shares an m-set with ?1 is 
also a cannot-word of ?2. Note that knowledge as 
m-sets has also been used in (Chen et al, 2013a) 
and (Chen et al, 2013b). 
We then propose a new topic model, called MC-
LDA (LDA with m-set and c-set), which is not on-
ly able to deal with c-sets and automatically adjust 
the number of topics, but also deal with the multi-
ple senses and adverse effect of knowledge prob-
lems at the same time. For the issue of multiple 
senses, a new latent variable ? is added to LDA to 
distinguish multiple senses (? 3). Then, we employ 
the generalized P?lya urn (GPU) model 
(Mahmoud, 2008) to address the issue of adverse 
effect of knowledge (? 4). Deviating from the 
standard topic modeling approaches, we propose 
the Extended generalized P?lya urn (E-GPU) 
model (? 5). E-GPU extends the GPU model to 
enable multi-urn interactions. This is necessary for 
handling c-sets and for adjusting the number of 
topics. E-GPU is the heart of MC-LDA. Due to the 
extension, a new inference mechanism is designed 
for MC-LDA (? 6). Note that E-GPU is generic 
and can be used in any appropriate application. 
In summary, this paper makes the following 
three contributions: 
1. It proposed a new knowledge-based topic mod-
el called MC-LDA, which is able to use both 
m-sets and c-sets, as well as automatically ad-
just the number of topics based on domain 
knowledge. At the same time, it can deal with 
some other major shortcomings of early exist-
ing models. To our knowledge, none of the ex-
isting knowledge-based models is as compre-
hensive as MC-LDA in terms of capabilities. 
2. It proposed the E-GPU model to enable multi-
urn interactions, which enables c-sets to be nat-
urally integrated into a topic model. To the best 
of our knowledge, E-GPU has not been pro-
posed and used before.  
3. A comprehensive evaluation has been conduct-
ed to compare MC-LDA with several state-of-
the-art models. Experimental results based on 
both qualitative and quantitative measures 
demonstrate the superiority of MC-LDA. 
1656
2  Related Work 
Sentiment analysis has been studied extensively in 
recent years (Hu and Liu, 2004; Pang and Lee, 
2008; Wiebe and Riloff, 2005; Wiebe et al, 2004). 
According to (Liu, 2012), there are three main ap-
proaches to aspect extraction: 1) Using word fre-
quency and syntactic dependency of aspects and 
sentiment words for extraction (e.g., Blair-
goldensohn et al, 2008; Hu and Liu, 2004; Ku et 
al., 2006; Popescu and Etzioni, 2005; Qiu et al, 
2011; Somasundaran and Wiebe, 2009; Wu et al, 
2009; Yu et al, 2011; Zhang and Liu, 2011; 
Zhuang et al, 2006); 2) Using supervised se-
quence labeling/classification (e.g., Choi and 
Cardie, 2010; Jakob and Gurevych, 2010; 
Kobayashi et al, 2007; Li et al, 2010); 3) Topic 
models (Branavan et al, 2008; Brody and Elhadad, 
2010; Fang and Huang, 2012; Jo and Oh, 2011; 
Kim et al, 2013; Lazaridou et al, 2013; Li et al, 
2011; Lin and He, 2009; Lu et al, 2009, 2012, 
2011; Lu and Zhai, 2008; Mei et al, 2007; 
Moghaddam and Ester, 2011; Mukherjee and Liu, 
2012; Sauper et al, 2011; Titov and McDonald, 
2008; Wang et al, 2010, 2011; Zhao et al, 2010). 
Other approaches include shallow semantic pars-
ing (Li et al, 2012b), bootstrapping (Xia et al, 
2009), Non-English techniques (Abu-Jbara et al, 
2013; Zhou et al, 2012), graph-based representa-
tion (Wu et al, 2011), convolution kernels 
(Wiegand and Klakow, 2010) and domain adap-
tion (Li et al, 2012). Stoyanov and Cardie (2011), 
Wang and Liu (2011), and Meng et al (2012) 
studied opinion summarization outside the reviews. 
Some other works related with sentiment analysis 
include (Agarwal and Sabharwal, 2012; Kennedy 
and Inkpen, 2006; Kim et al, 2009; Mohammad et 
al., 2009). 
In this work, we focus on topic models owing to 
their advantage of performing both aspect extrac-
tion and clustering simultaneously. All other ap-
proaches only perform extraction. Although there 
are several related works on clustering aspect 
terms (e.g., Carenini et al, 2005; Guo et al, 2009; 
Zhai et al, 2011), they all assume that the aspect 
terms have been extracted beforehand. We also 
notice that some aspect extraction models in sen-
timent analysis separately discover aspect words 
and aspect specific sentiment words (e.g., Sauper 
and Barzilay, 2013; Zhao et al, 2010). Our pro-
posed model does not separate them as most sen-
timent words also imply aspects and most adjec-
tives modify specific attributes of objects. For ex-
ample, sentiment words expensive and beautiful 
imply aspects price and appearance respectively. 
Regarding the knowledge-based models, be-
sides those discussed in ? 1, the model (Hu et al, 
2011) enables the user to provide guidance interac-
tively. Blei and McAuliffe (2007) and Ramage et 
al. (2009) used document labels in supervised set-
ting. In (Chen et al, 2013a), we proposed MDK-
LDA to leverage multi-domain knowledge, which 
serves as the basic mechanism to exploit m-sets in 
MC-LDA. In (Chen et al, 2013b), we proposed a 
framework (called GK-LDA) to explicitly deal 
with the wrong knowledge when exploring the 
lexical semantic relations as the general (domain 
independent) knowledge in topic models. But 
these models above did not consider the 
knowledge in the form of c-sets (or cannot-links). 
The generalized P?lya urn (GPU) model 
(Mahmoud, 2008) was first introduced in LDA by 
Mimno et al (2011). However, Mimno et al (2011) 
did not use domain knowledge. Our results in ? 7 
show that using domain knowledge can signifi-
cantly improve aspect extraction. The GPU model 
was also employed in topic models in our work of 
(Chen et al, 2013a, 2013b). In this paper, we pro-
pose the Extended GPU (E-GPU) model. The E-
GPU model is more powerful in handling complex 
situations in dealing with c-sets. 
3 Dealing with M-sets and Multiple Senses 
Since the proposed MC-LDA model is a major 
extension to our earlier work in (Chen et al, 
2013a), which can deal with m-sets, we include 
this earlier work here as the background.     
To incorporate m-sets and deal with multiple 
senses of a word, the MDK-LDA(b) model was 
proposed in (Chen et al, 2013a), which adds a 
new latent variable ? into LDA. The rationale here 
is that this new latent variable ? guides the model 
to choose the right sense represented by an m-set. 
The generative process of MDK-LDA(b) is (the 
notations are explained in Table 1): 
                     ?  ~ ?????????(?) 
                     ??|??  ~ ???????????(??) 
                     ? ~ ?????????(?) 
                     ??|??,? ~ ???????????????? 
                     ? ~ ?????????(?) 
                     ??|?? , ??,? ~ ???????????????,??? 
1657
The corresponding plate is shown in Figure 1. Un-
der MDK-LDA(b), the probability of word ? giv-
en topic ?, i.e., ??(?), is given by: 
 ??(?) = ? ??(?) ? ??,?(?)
?
?=1    (1) 
where ??(?)  denotes the probability of m-set ? 
occurring under topic ? and ??,?(?) is the proba-
bility of word ? appearing in m-set ? under topic ?. 
According to (Chen et al, 2013a), the condi-
tional probability of Gibbs sampler for MDK-
LDA(b) is given by (see notations in Table 1): 
    ???? = ?, ?? = ? ???? , ??? ,?,?,?, ?? ? 
??,?
?? + ?
? ???,??
?? + ?????=1
?
??,?
?? + ?
? ???,??
?? + ?????=1
?
??,?,??
?? + ??
? ???,?,??
?? + ???
?
??=1
 (2) 
The superscript ??  denotes the counts excluding 
the current assignments (?? and ??) for word ??. 
4 Handling Adverse Effect of Knowledge 
4.1 Generalized P?lya urn (GPU) Model 
The P?lya urn model involves an urn containing 
balls of different colors. At discrete time intervals, 
balls are added or removed from the urn according 
to their color distributions. 
In the simple P?lya urn (SPU) model, a ball is 
first drawn randomly from the urn and its color is 
recorded, then that ball is put back along with a 
new ball of the same color. This selection process 
is repeated and the contents of the urn change over 
time, with a self-reinforcing property sometimes 
expressed as ?the rich get richer.? SPU is actually 
exhibited in the Gibbs sampling for LDA.  
The generalized P?lya urn (GPU) model differs 
from the SPU model in the replacement scheme 
during sampling. Specifically, when a ball is ran-
domly drawn, certain numbers of additional balls 
of each color are returned to the urn, rather than 
just two balls of the same color as in SPU. 
4.2 Promoting M-sets using GPU 
To deal with the issue of sensitivity to the adverse 
effect of knowledge, MDK-LDA(b) is extended to 
MDK-LDA which employs the generalized P?lya 
urn (GPU) sampling scheme. 
As discussed in ? 1, due to the problem of the 
adverse effect of knowledge, important words may 
suffer from the presence of rare words in the same 
m-set. This problem can be dealt with the very 
sampling scheme of the GPU model (Chen et al, 
2013a). Specifically, by adding additional ??,??,? 
balls of color ? into ??
?  while keeping the drawn 
ball, we increase the proportion (probability) of 
seeing the m-set ? under topic ? and thus promote 
m-set ? as a whole. Consequently, each word in ? 
is more likely to be emitted. We define ??,??,? as: 
 ??,??,? = ?
1           ? = ??                                   
?           ? ? ?,?? ? ?,? ? ??        
 0           otherwise                             
 (3) 
The corresponding Gibbs sampler for MDK-LDA 
will be introduced in ? 6.  
Hyperparameters 
?, ?, ? Dirichlet priors for ?,  ?,  ? 
Latent & Visible Variables 
? Topic (Aspect) 
? M-set 
? Word 
? Document-Topic distribution 
?? Topic distribution of document ? 
? Topic-M-set distribution 
?? M-set distribution of topic ? 
? Topic-M-set-Word distribution 
??,? Word distribution of topic ?, m-set ? 
Cardinalities 
? Number of documents 
?? Number of words in document ? 
? Number of topics 
? Number of m-sets 
? The vocabulary size 
Sampling & Count Notations 
?? Topic assignment for word ??  
??  M-set assignment for word ??  
??? Topic assignments for all words except ??  
??? M-set assignments for all words except ??  
??,?  
Number of times that topic ? is assigned 
to word tokens in document ? 
??,? 
Number of times that m-set ? occurs un-
der topic ? 
??,?,? 
Number of times that word ? appears in 
m-set ? under topic ? 
Table 1. Meanings of symbols. 
 
 
 
 
 
 
 
 
 
Figure 1. Plate notation for MDK-LDA(b) and MC-LDA. 
T?S Nm 
M 
? 
T 
 ?  z 
 w 
 ? 
? 
 s ? 
 ? 
1658
5 Incorporating C-sets 
5.1 Extended Generalized P?lya urn Model 
To handle the complex situation resulted from in-
corporating c-sets, we propose an Extended gener-
alized P?lya urn (E-GPU) model. Instead of 
involving only one urn as in SPU and GPU, E-
GPU model considers a set of urns in the sampling 
process. The E-GPU model allows a ball to be 
transferred from one urn to another, enabling mul-
ti-urn interactions. Thus, during sampling, the 
populations of several urns will evolve even if on-
ly one ball is drawn from one urn. This capability 
makes the E-GPU model more powerful as it 
models relationships among multiple urns. 
We define three sets of urns which will be used 
in the new sampling scheme in the proposed MC-
LDA model. The first set of urns is the topic urns 
???{1??}
? , where each topic urn contains ? colors 
(topics) and each ball inside has a color ? ?
 {1 ??}. It corresponds to the document-topic dis-
tribution ? in Table 1. The second set of urns (m-
set urn ??? {1??}
? )  corresponds to the topic-m-set 
distribution ? , with balls of colors (m-sets) 
? ?  {1 ? ?}  in each m-set urn. The third set of 
urns is the word urns ??,?
? ,where ? ?  {1 ??} and 
? ?  {1 ? ?} . Each ball inside a word urn has a 
color (word) ? ?  {1 ??}. The distribution ? can 
be reflected in this set of urns. 
5.2 Handling C-sets using E-GPU 
As MDK-LDA can only use m-sets but not c-sets, 
we now extend MDK-LDA to the MC-LDA model 
in order to exploit c-sets. As pointed out in ? 1, c-
sets may be inconsistent with the corpus domain, 
which makes them considerably harder to deal 
with. To tackle the issue, we utilize the proposed 
E-GPU model and incorporate c-sets handling in-
side the E-GPU sampling scheme, which is also 
designed to enable automated adjustment of the 
number of topics based on domain knowledge. 
Based on the definition of c-set, each pair of 
words in a c-set cannot both have large probabili-
ties under the same topic. As the E-GPU model 
allows multi-urn interactions, when sampling a 
ball represents word ? from a word urn ??,?
? , we 
want to transfer the balls representing cannot-
words of ? (sharing a c-set with ?) to other urns 
(see Step 3 a below). That is, decrease the proba-
bilities of those cannot-words under this topic 
while increasing their corresponding probabilities 
under some other topics. In order to correctly 
transfer a ball that represents word ?, it should be 
transferred to an urn which has a higher proportion 
of ? and its related words (i.e., words sharing m-
sets with ?). That is, we randomly sample an urn 
that has a higher proportion of any m-set of ? to 
transfer ? to (Step 3 b below). However, the situa-
tion becomes more involved when a c-set is not 
consistent with the corpus. For example, aspects 
price and amazon may be mixed under one topic 
(say ?) in LDA. The user may want to separate 
them by providing a c-set {price, amazon}. In this 
case, according to LDA, word price has no topic 
with a higher proportion of it (and its related 
words) than topic ?. To transfer it, we need to in-
crement the number of topics by 1 and then trans-
fer the word to this new topic urn (step 3 c below). 
Based on these ideas, we propose the E-GPU sam-
pling scheme for the MC-LDA model below: 
1. Sample a topic ? from ??
? , an m-set ? from ??
?, and 
a word ?  from ??,?
?  sequentially, where ?  is the 
?th document. 
2. Record ?, ? and ?, put back two balls of color ? in-
to urn ??
? , one ball of color ? into urn ??
?, and two 
balls of color ? into urn ??,?
? . Given the matrix ? 
(in Equation 3), for each word ?? ? ?, we put back 
??,?? ,? number of balls of color ? into urn ??
?. 
3.  For each word ?? that shares a c-set with ?: 
a) Sample an m-set ??  from ??
?  which satisfies 
?? ? ?? . Draw a ball ? of color ?? (to be trans-
ferred) from ??,??
?  and remove it from ??,??
? . The 
document of ball ? is denoted by ??. If no ball 
of color ?? can be drawn (i.e., there is no ball 
of color ?? in ??,??
? ), skip steps b) to d). 
b) Produce an urn set {???,??
? } such that each urn in 
it satisfies the following conditions: 
i)   ?? ? ?, ?? ? ?
? 
ii) The proportion of balls of color ?? in ???
?  is    
higher than that of balls of color ??  in ??
?. 
c) If {???,??
? } is not empty, randomly select one urn 
???,??
?  from it. If {???,??
? } is empty, set ? = ? +
1, ?? = ?, draw an m-set ?? from ???
?  which sat-
isfies ?? ? ?
?. Record ?? for step d). 
d) Put the ball ? drawn from Step a) into ???,??
? , as 
well as a ball of color ?? into ???
?  and a ball of 
color ?? into ???
? . 
Note that the E-GPU model cannot be reflected in 
the graphical model in Figure 1 as it is essentially 
1659
sampling scheme, and hence MC-LDA shares the 
same plate as MDK-LDA(b). 
6 Collapsed Gibbs Sampling 
We now describe the collapsed Gibbs sampler 
(Griffiths and Steyvers, 2004) with the detailed 
conditional distributions and algorithms for MC-
LDA. Inference of ? and ? can be computationally 
expensive due to the non-exchangeability of words 
under the E-GPU models. We take the approach of 
(Mimno et al, 2011) which approximates the true 
Gibbs sampling distribution by treating each word 
as if it were the last. 
For each word ?? , we perform hierarchical 
sampling consisting of the following three steps 
(the detailed algorithms are given in Figures 2 and 
3): 
Step 1 (Lines 1-11 in Figure 2): We jointly 
sample a topic ??  and an m-set ??  (containing ??) 
for ?? , which gives us a blocked Gibbs sampler 
(Ishwaran and James, 2001), with the conditional 
probability given by: 
     ?(?? = ?, ?? = ?|?
?? , ??? ,?,?,?, ?,?) ?
     
??,?
?? +?
? ??
?,??
?? +???
??=1
?
? ? ??,??,?? ???,?,??
???
??=1
?
??=1 +?
? ?? ? ???,??,?????,??,??
???
??=1
?
??=1
+???
??=1
?
     
??,?,??
?? +??
? ??
?,?,??
?? +???
?
??=1
  
 (4) 
This step is the same as the Gibbs sampling for the 
MDK-LDA model. 
Step 2 (lines 1-5 in Figure 3): For every cannot-
word (say ??) of ??, randomly pick an urn ???,??
?  
from the urn set {???,??
? } where ??? ? ??. If there ex-
ists at least one ball of color ?? in urn ???,??
? , we 
sample one ball (say ?? ) of color ??  from urn 
???,??
? , based on the following conditional distribu-
tion: 
     ?(? = ??|?, ?,?,?,?, ?,?) ?
???,?+?
? ????,??
+???
??=1
  (5) 
where ??  denotes the document of the ball ??  of 
color ??. 
Step 3 (lines 6-12 in Figure 3): For each drawn 
ball ? from Step 2, resample a topic ? and an m-set 
?  (containing ?? ) based on the following condi-
tional distribution: 
?(?? = ?, ?? = ?|?
?? , ??? ,?,?,?, ?,?, ? = ??)
? ?
?0,???
?????
(????
??)?
????(??)? ?
??,?
?? + ?
? ???,??
?? + ?????=1
?
? ? ??,??,?? ? ??,?,??
???
??=1
?
??=1 + ?
? ?? ? ???,??,?? ? ??,??,??
???
??=1
?
??=1 + ??
?
??=1
?
??,?,??
?? + ??
? ???,?,??
?? + ???
?
??=1
 
(6) 
where ??  (same as ??  in Figure 3) and ??  are the 
original topic and m-set assignments. The super-
script ?? denotes the counts excluding the original 
Algorithm 1. GibbsSampling(?, ?? , ?, ?, ?) 
Input: Document ?, Word ?? , Matrix ?, 
           Transfer cannot-word flag ?, 
           A set of valid topics ? to be assigned to ??  
1:   ??,?? ? ??,?? ? 1; 
2:   for each word ?? in ?? do 
3:       ???,?? ? ???,?? ? ???,?? ,??; 
4:   end for 
5:   ???,??,?? ? ???,??,?? ? 1; 
6:   Jointly sample ?? ? ? and ?? ? ??  using Equation 2; 
7:   ??,?? ? ??,?? + 1; 
8:   for each word ?? in ?? do 
9:       ???,?? ? ???,?? + ???,?? ,??; 
10: end for 
11: ???,??,?? ? ???,??,?? + 1; 
12: if ? is true then 
13:     TransferCannotWords(?? , ??); 
14: end if 
Figure 2. Gibbs sampling for MC-LDA. 
Algorithm 2.TransferCannotWords(?? , ??) 
Input: Word ?? , Topic ??, 
1:   for each cannot-word ?? of ??  do 
2:       Randomly select an m-set ??  from all m-sets of ??; 
3:       Build a set ? containing all the instances of ?? 
from the corpus with topic and m-set assign-
ments being ?? and ??; 
4:       if ? is not empty then 
5:            Draw an instance of ?? from ? (denoting the 
document of this instance by ??) using 
Equation 5; 
6:            Generate a topic set ?? that each topic ?? inside 
satisfies ????????(???(?? )) > ???(??). 
7:            if ?? is not empty then 
8:                GibbsSampling(??, ??, ?, false, ??); 
9:            else 
10:              ????? = ? + 1; // ? is #Topics. 
11:              GibbsSampling(??, ??, ?, false, {?????}); 
12:          end if 
13:     end if 
14: end for 
Figure 3. Transfer cannot-words in Gibbs sampling. 
1660
assignments. ?()  is an indicator function, which 
restricts the ball to be transferred only to an urn 
that contains a higher proportion of its m-set. 
When no topic ? can be successfully sampled and 
the current sweep (iteration) of Gibbs sampling 
has the same number of topic (?) as the previous 
sweep, we increment ? by 1. And then assign ? to 
?? . The counts and parameters are also updated 
accordingly. 
7 Experiments 
We now evaluate the proposed MC-LDA model 
and compare it with state-of-the-art existing mod-
els. Two unsupervised baseline models that we 
compare with are:  
? LDA: LDA is the basic unsupervised topic 
model (Blei et al, 2003). 
? LDA-GPU: LDA with GPU (Mimno et al, 
2011). Specifically, LDA-GPU applies GPU in 
LDA using co-document frequency.  
As for knowledge-based models, we focus on 
comparing with DF-LDA model (Andrzejewski et 
al., 2009), which is perhaps the best known 
knowledge-based model and it allows both must-
links and cannot-links.  
For a comprehensive evaluation, we consider 
the following variations of MC-LDA and DF-LDA:  
? MC-LDA: MC-LDA with both m-sets and c-
sets. This is the newly proposed model.   
? M-LDA: MC-LDA with m-sets only. This is 
the MDK-LDA model in (Chen et al, 2013a). 
? DF-M: DF-LDA with must-links only. 
? DF-MC: DF-LDA with both must-links and 
cannot-links. This is the full DF-LDA model in 
(Andrzejewski et al, 2009).  
We do not compare with seeded models in (Burns 
et al, 2012; Jagarlamudi et al, 2012; Lu et al, 
2011; Mukherjee and Liu, 2012) as seed sets are 
special cases of must-links and they also do not 
allow c-sets (or cannot-links). 
7.1 Datasets and Settings 
Datasets: We use product reviews from four do-
mains (types of products) from Amazon.com for 
evaluation. The corpus statistics are shown in Ta-
ble 2 (columns 2 and 3). The domains are ?Cam-
era,? ?Food,? ?Computer,? and ?Care? (short for 
?Personal Care?). We have made the datasets pub-
lically available at the website of the first author. 
Pre-processing: We ran the Stanford Core NLP 
Tools1 to perform sentence detection and lemmati-
zation. Punctuations, stopwords 2 , numbers and 
words appearing less than 5 times in each corpus 
were removed. The domain name was also re-
moved, e.g., word camera in the domain ?Camera?, 
since it co-occurs with most words in the corpus, 
leading to high similarity among topics/aspects. 
Sentences as documents: As noted in (Titov and 
McDonald, 2008), when standard topic models are 
applied to reviews as documents, they tend to pro-
duce topics that correspond to global properties of 
products (e.g., brand name), which make topics 
overlapping with each other. The reason is that all 
reviews of the same type of products discuss about 
the same aspects of these products. Only the brand 
names and product names are different. Thus, us-
ing individual reviews for modeling is not very 
effective. Although there are approaches which 
model sentences (Jo and Oh, 2011; Titov and 
McDonald, 2008), we take the approach of (Brody 
and Elhadad, 2010), dividing each review into sen-
tences and treating each sentence as an independ-
ent document. Sentences can be used by all three 
baselines without any change to their models. Alt-
hough the relationships between sentences are lost, 
the data is fair to all models. 
Parameter settings: For all models, posterior in-
ference was drawn using 1000 Gibbs iterations 
with an initial burn-in of 100 iterations. For all 
models, we set ? = 1 and ? = 0.1. We found that 
small changes of ? and ? did not affect the results 
much, which was also reported in (Jo and Oh, 
2011) who also used online reviews. For the num-
ber of topics T, we tried different values (see ?7.2) 
as it is hard to know the exact number of topics. 
While non-parametric Bayesian approaches (Teh 
et al, 2006) aim to estimate ?  from the corpus, 
they are often sensitive to the hyper-parameters 
(Heinrich, 2009). 
1 http://nlp.stanford.edu/software/corenlp.shtml 
2 http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list 
Domain #Reviews #Sentences #M-sets #C-sets 
Camera 500 5171 173 18 
Food 500 2416 85 10 
Computer 500 2864 92 6 
Care 500 3008 119 13 
Average 500 3116 103 9 
Table 2. Corpus statistics with #m-sets and #c-sets 
having at least two words. 
                                                          
1661
For DF-LDA, we followed (Andrzejewski et al, 
2009) to generate must-links and cannot-links 
from our domain knowledge. We then ran DF-
LDA3 while keeping its parameters as proposed in 
(Andrzejewski et al, 2009) (we also experimented 
with different parameter settings but they did not 
produce better results). For our proposed model, 
we estimated the thresholds using cross validation 
in our pilot experiments. Estimated value ? = 0.2 
in equation 3 yielded good results. The second 
stage (steps 2 and 3) of the Gibbs sampler for MC-
LDA (for dealing with c-sets) is applied after 
burn-in phrase. 
Domain knowledge: User knowledge about a do-
main can vary a great deal. Different users may 
have very different knowledge. To reduce this var-
iance for a more reliable evaluation, instead of 
asking a human user to provide m-sets, we obtain 
the synonym sets and the antonym sets of each 
word that is a noun or adjective (as words of other 
parts-of-speech usually do not indicate aspects) 
from WordNet (Miller, 1995) and manually verify 
the words in those sets for the domain. Note that if 
a word ?  is not provided with any m-set, it is 
treated as a singleton m-set {?}. For c-sets, we ran 
LDA in each domain and provide c-sets based on 
the wrong results of LDA as in (Andrzejewski et 
al., 2009). Then, the knowledge is provided to 
each model in the format required by each model. 
The numbers of m-sets and c-sets are listed in col-
umns 4 and 5 of Table 2. Duplicate sets have been 
removed. 
7.2 Objective Evaluation 
In this section, we evaluate our proposed MC-
3 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html 
LDA model objectively. Topic models are often 
evaluated using perplexity on held-out test data. 
However, the perplexity metric does not reflect the 
semantic coherence of individual topics learned by 
a topic model (Newman et al, 2010). Recent re-
search has shown potential issues with perplexity 
as a measure: (Chang et al, 2009) suggested that 
the perplexity can sometimes be contrary to human 
judgments. Also, perplexity does not really reflect 
our goal of finding coherent aspects with accurate 
semantic clustering. It only provides a measure of 
how well the model fits the data. 
The Topic Coherence metric (Mimno et al, 
2011) (also called the ?UMass? measure (Stevens 
and Buttler, 2012)) was proposed as a better alter-
native for assessing topic quality. This metric re-
lies upon word co-occurrence statistics within the 
documents, and does not depend on external re-
sources or human labeling. It was shown that topic 
coherence is highly consistent with human expert 
labeling by Mimno et al (2011). Higher topic co-
herence score indicates higher quality of topics, 
i.e., better topic interpretability. 
Effects of Number of Topics 
Since our proposed models and the baseline mod-
els are all parametric models, we first compare 
each model given different numbers of topics. 
Figure 4 shows the average Topic Coherence score 
of each model given different numbers of topics. 
From Figure 4, we note the following: 
1. MC-LDA consistently achieves the highest To-
pic Coherence scores given different numbers 
of topics. M-LDA also works better than the 
other baseline models, but not as well as MC-
LDA. This shows that both m-sets and c-sets 
are beneficial in producing coherent aspects. 
2. DF-LDA variants, DF-M and DF-MC, do not 
perform well due to the shortcomings discussed 
 
Figure 4. Avg. Topic Coherence score of each model 
across different number of topics. 
 
Figure 5. Avg. Topic Coherence score for different 
proportions of knowledge. 
-1600
-1500
-1400
-1300
-1200
3 6 9 12 15
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
-1320
-1300
-1280
-1260
-1240
0% 25% 50% 100%
MC-LDA M-LDA DF-M DF-MC
                                                          
1662
in ? 1. It is slightly better than LDA when ? = 
15, but worse than LDA in other cases. We will 
further analyze the effects of knowledge on 
MC-LDA and DF-LDA shortly. 
3. LDA-GPU does not perform well due to its use 
of co-document frequency. As frequent words 
usually have high co-document frequency with 
many other words, the frequent words are 
ranked top in many topics. This shows that the 
guidance using domain knowledge is more ef-
fective than using co-document frequency. 
In terms of improvements, MC-LDA outperforms 
M-LDA significantly ( ? < 0.03 ) and all other 
baseline models significantly (? < 0.01) based on 
a paired t-test. It is important to note that by no 
means do we say that LDA-GPU and DF-LDA are 
not effective. We only say that for the task of as-
pect extraction and leveraging domain knowledge, 
these models do not generate as coherent aspects 
as ours because of their shortcomings discussed in 
? 1. In general, with more topics, the Topic Coher-
ence scores increase. We found that when ?  is 
larger than 15, aspects found by each model be-
came more and more overlapping, with several 
aspects expressing the same features of products. 
So we fix ? = 15 in the subsequent experiments.  
Effects of Knowledge 
To further analyze the effects of knowledge on 
models, in each domain, we randomly sampled 
different proportions of knowledge (i.e., different 
numbers of m-sets/must-links and c-sets/cannot-
links) as shown in Figure 5, where 0% means no 
knowledge (same as LDA and LDA-GPU, which 
do not incorporate knowledge) and 100% means 
all knowledge. From Figure 5, we see that MC-
LDA and M-LDA both perform consistently better 
than DF-MC and DF-M across different propor-
tions of knowledge. With the increasing number of 
knowledge sets, MC-LDA and M-LDA achieve 
higher Topic Coherence scores (i.e., produce more 
coherent aspects). In general, MC-LDA performs 
the best. For both DF-MC and DF-M, the Topic 
Coherence score increases from 0% to 25% 
knowledge, but decreases with more knowledge 
(50% and 100%). This shows that with limited 
amount of knowledge, the shortcomings of DF-
LDA are not very obvious, but with more 
knowledge, these issues become more serious and 
thus degrade the performance of DF-LDA.  
7.3 Human Evaluation 
Since our aim is to make topics more interpretable 
and conformable to human judgments, we worked 
with two judges who are familiar with Amazon 
products and reviews to evaluate the models sub-
jectively. Since topics from topic models are rank-
ings based on word probability and we do not 
know the number of correct topical words, a natu-
ral way to evaluate these rankings is to use Preci-
sion@n (or p@n) which was also used in 
(Mukherjee and Liu, 2012; Zhao et al, 2010), 
where n is the rank position. We give p@n for n = 
5 and 10. There are two steps in human evaluation: 
topic labeling and word labeling. 
Topic Labeling: We followed the instructions in 
(Mimno et al, 2011) and asked the judges to label 
each topic as good or bad. Each topic was present-
ed as a list of 10 most probable words in descend-
ing order of their probabilities under that topic. 
The models which generated the topics for label-
ing were obscure to the judges. In general, each 
topic was annotated as good if it had more than 
half of its words coherently related to each other 
representing a semantic concept together; other-
wise bad. Agreement of human judges on topic 
 
Figure 6. Avg. p@5 of good topics for each model 
across different domains. 
The models of each bar from left to rights are MC-LDA, M-
LDA, LDA, DF-M, DF-MC, LDA-GPU. (Same for Figure 7) 
 
Figure 7. Avg. p@10 of good topics for each model 
across different domains. 
0.5
0.6
0.7
0.8
0.9
1.0
Camera Food Computer Care
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
0.5
0.6
0.7
0.8
0.9
1.0
Camera Food Computer Care
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
1663
labeling using Cohen?s Kappa yielded a score of 
0.92 indicating almost perfect agreements accord-
ing to the scale in (Landis and Koch, 1977). This 
is reasonable as topic labeling is an easy task and 
semantic coherence can be judged well by humans. 
Word Labeling: After topic labeling, we chose 
the topics, which were labeled as good by both 
judges, as good topics. Then, we asked the two 
judges to label each word of the top 10 words in 
these good topics. Each word was annotated as 
correct if it was coherently related to the concept 
represented by the topic; otherwise incorrect. 
Since judges already had the conception of each 
topic in mind when they were labeling topics, la-
beling each word was not difficult which explains 
the high Kappa score for this labeling task (score = 
0.892). 
Quantitative Results 
Figures 6 and 7 give the average p@5 and p@10 
of all good topics over all four domains. The num-
bers of good topics generated by each model are 
shown in Table 3. We can see that the human 
evaluation results are highly consistent with Topic 
Coherence results in ?7.2. MC-LDA improves 
over M-LDA significantly (? < 0.01) and both 
MC-LDA and M-LDA outperforms the other base-
line models significantly ( ? < 0.005 ) using a 
paired t-test. We also found that when the domain 
knowledge is simple with one word usually ex-
pressing only one meaning/sense (e.g., in the do-
main ?Computer?), DF-LDA performs better than 
LDA. In other domains, it performs similarly or 
worse than LDA. Again, it shows that DF-LDA is 
not effective to handle complex knowledge, which 
is consistent with the results of effects of 
knowledge on DF-LDA in ?7.2. 
Qualitative Results 
We now show some qualitative results to give an 
intuitive feeling of the outputs from different mod-
els. There are a large number of aspects that are 
dramatically improved by MC-LDA. Due to space 
constraints, we only show some examples. To fur-
ther focus, we just show some results of MC-LDA, 
M-LDA and LDA. The results from LDA-GPU 
and DF-LDA were inferior and hard for the human 
judges to match them with aspects found by the 
other models for qualitative comparison. 
Table 4 shows three aspects Amazon, Price, 
Battery generated by each model in the domain 
?Camera?. Both LDA and M-LDA can only dis-
cover two aspects but M-LDA has a higher aver-
age precision. Given the c-set {amazon, price, 
battery}, MC-LDA can discover all three aspects 
with the highest average precision. 
8 Conclusion  
This paper proposed a new model to exploit do-
main knowledge in the form of m-sets and c-sets 
to generate coherent aspects (topics) from online 
reviews. The paper first identified and character-
ized some shortcomings of the existing 
knowledge-based models. A new model called 
MC-LDA was then proposed, whose sampling 
scheme was based on the proposed Extended GPU 
(E-GPU) model enabling multi-urn interactions. A 
comprehensive evaluation using real-life online 
reviews from multiple domains shows that MC-
LDA outperforms the state-of-the-art models sig-
nificantly and discovers aspects with high seman-
tic coherence. In our future work, we plan to 
incorporate aspect specific sentiments in the MC-
LDA model. 
Acknowledgments 
This work was supported in part by a grant from 
National Science Foundation (NSF) under grant no. 
IIS-1111092, and a grant from HP Labs Innova-
tion Research Program. 
#Good 
Topics 
MC-LDA M-LDA LDA DF-M DF-MC LDA-GPU 
Camera 15/18 12 11 9 7 3 
Food 8/16 7 7 5 4 5 
Computer 12/16 10 7 9 6 4 
Care 11/16 10 9 10 9 3 
Average 11.5/16.5 9.75/15 8.5/15 8.25/15 6.5/15 3.75/15 
Table 3. Number of good topics of each model.             
In x/y, x is the number of discovered good topics, and y is the 
total number of topics generated.  
MC-LDA M-LDA LDA 
Amazon Price Battery Price Battery Amazon Battery 
review price battery price battery card battery 
amazon perform life lot review day screen 
software money day money amazon amazon life 
customer expensive extra big life memory lcd 
month cost charger expensive extra product water 
support week water point day sd usb 
warranty cheap time cost power week cable 
package purchase power photo time month case 
product deal hour dot support item charger 
hardware product aa purchase customer class hour 
Table 4. Example aspects in the domain ?Camera?; 
errors are marked in red/italic. 
1664
References  
Amjad Abu-Jbara, Ben King, Mona Diab, and 
Dragomir Radev. 2013. Identifying Opinion 
Subgroups in Arabic Online Discussions. In 
Proceedings of ACL. 
Apoorv Agarwal and Jasneet Sabharwal. 2012. End-to-
End Sentiment Analysis of Twitter Data. In 
Proceedings of the Workshop on Information 
Extraction and Entity Analytics on Social Media 
Data, at the 24th International Conference on 
Computational Linguistics (IEEASMD-COLING 
2012), Vol. 2. 
David Andrzejewski, Xiaojin Zhu, and Mark Craven. 
2009. Incorporating domain knowledge into topic 
modeling via Dirichlet Forest priors. In Proceedings 
of ICML, pages 25?32. 
David Andrzejewski, Xiaojin Zhu, Mark Craven, and 
Benjamin Recht. 2011. A framework for 
incorporating general domain knowledge into latent 
Dirichlet alocation using first-order logic. In 
Proceedings of IJCAI, pages 1171?1177. 
Sasha Blair-goldensohn, Tyler Neylon, Kerry Hannan, 
George A. Reis, Ryan Mcdonald, and Jeff Reynar. 
2008. Building a sentiment summarizer for local 
service reviews. In Proceedings of In NLP in the 
Information Explosion Era. 
David M. Blei and Jon D. McAuliffe. 2007. Supervised 
Topic Models. In Proceedings of NIPS. 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent Dirichlet Allocation. Journal of 
Machine Learning Research, 3, 993?1022. 
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and 
Regina Barzilay. 2008. Learning Document-Level 
Semantic Properties from Free-Text Annotations. In 
Proceedings of ACL, pages 263?271. 
Samuel Brody and Noemie Elhadad. 2010. An 
unsupervised aspect-sentiment model for online 
reviews. In Proceedings of NAACL, pages 804?812. 
Nicola Burns, Yaxin Bi, Hui Wang, and Terry 
Anderson. 2012. Extended Twofold-LDA Model for 
Two Aspects in One Sentence. Advances in 
Computational Intelligence, Vol. 298, pages 265?
275. Springer Berlin Heidelberg. 
Giuseppe Carenini, Raymond T. Ng, and Ed Zwart. 
2005. Extracting knowledge from evaluative text. In 
Proceedings of K-CAP, pages 11?18. 
Jonathan Chang, Jordan Boyd-Graber, Wang Chong, 
Sean Gerrish, and David Blei, M. 2009. Reading Tea 
Leaves: How Humans Interpret Topic Models. In 
Proceedings of NIPS, pages 288?296. 
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun 
Hsu, Malu Castellanos, and Riddhiman Ghosh. 
2013a. Leveraging Multi-Domain Prior Knowledge 
in Topic Models. In Proceedings of IJCAI, pages 
2071?2077. 
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun 
Hsu, Malu Castellanos, and Riddhiman Ghosh. 
2013b. Discovering Coherent Topics Using General 
Knowledge. In Proceedings of CIKM. 
Yejin Choi and Claire Cardie. 2010. Hierarchical 
Sequential Learning for Extracting Opinions and 
their Attributes, pages 269?274. 
Lei Fang and Minlie Huang. 2012. Fine Granular 
Aspect Analysis using Latent Structural Models. In 
Proceedings of ACL, pages 333?337. 
Thomas L. Griffiths and Mark Steyvers. 2004. Finding 
Scientific Topics. PNAS, 101 Suppl, 5228?5235. 
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, 
and Zhong Su. 2009. Product feature categorization 
with multilevel latent semantic association. In 
Proceedings of CIKM, pages 1087?1096. 
Gregor Heinrich. 2009. A Generic Approach to Topic 
Models. In Proceedings of ECML PKDD, pages 517 
? 532. 
Minqing Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. In Proceedings of 
KDD, pages 168?177. 
Yuening Hu, Jordan Boyd-Graber, and Brianna 
Satinoff. 2011. Interactive Topic Modeling. In 
Proceedings of ACL, pages 248?257. 
Hemant Ishwaran and LF James. 2001. Gibbs sampling 
methods for stick-breaking priors. Journal of the 
American Statistical Association, 96(453), 161?173. 
Jagadeesh Jagarlamudi, Hal Daum? III, and 
Raghavendra Udupa. 2012. Incorporating Lexical 
Priors into Topic Models. In Proceedings of EACL, 
pages 204?213. 
Niklas Jakob and Iryna Gurevych. 2010. Extracting 
Opinion Targets in a Single- and Cross-Domain 
Setting with Conditional Random Fields. In 
Proceedings of EMNLP, pages 1035?1045. 
Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment 
unification model for online review analysis. In 
Proceedings of WSDM, pages 815?824. 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence, 22(2), 
110?125. 
Jungi Kim, Jinji Li, and Jong-Hyeok Lee. 2009. 
Discovering the Discriminative Views: Measuring 
Term Weights for Sentiment Analysis. In 
Proceedings of ACL/IJCNLP, pages 253?261. 
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and 
Shixia Liu. 2013. A Hierarchical Aspect-Sentiment 
Model for Online Reviews. In Proceedings of AAAI. 
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 
2007. Extracting Aspect-Evaluation and Aspect-of 
Relations in Opinion Mining. In Proceedings of 
EMNLP, pages 1065?1074. 
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. 
Opinion Extraction, Summarization and Tracking in 
1665
News and Blog Corpora. In Proceedings of AAAI 
Spring Symposium: Computational Approaches to 
Analyzing Weblogs, pages 100?107. 
JR Landis and GG Koch. 1977. The measurement of 
observer agreement for categorical data. biometrics, 
33. 
Angeliki Lazaridou, Ivan Titov, and Caroline Sporleder. 
2013. A Bayesian Model for Joint Unsupervised 
Induction of Sentiment, Aspect and Discourse 
Representations. In Proceedings of ACL. 
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, 
Yingju Xia, Shu Zhang, and Hao Yu. 2010. 
Structure-Aware Review Mining and 
Summarization. In Proceedings of COLING, pages 
653?661. 
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and 
Xiaoyan Zhu. 2012a. Cross-Domain Co-Extraction 
of Sentiment and Topic Lexicons. In Proceedings of 
ACL (1), pages 410?419. 
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011. 
Generating Aspect-oriented Multi-Document 
Summarization with Event-aspect model. In 
Proceedings of EMNLP, pages 1137?1146. 
Shoushan Li, Rongyang Wang, and Guodong Zhou. 
2012b. Opinion Target Extraction Using a Shallow 
Semantic Parsing Framework. In Proceedings of 
AAAI. 
Chenghua Lin and Yulan He. 2009. Joint 
sentiment/topic model for sentiment analysis. In 
Proceedings of CIKM, pages 375?384. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Morgan & Claypool Publishers. 
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K. 
Tsou. 2011. Multi-aspect Sentiment Analysis with 
Topic Models. In Proceedings of ICDM Workshops, 
pages 81?88. 
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan 
Roth. 2012. Unsupervised discovery of opposing 
opinion networks from forum discussions. In 
Proceedings of CIKM, pages 1642?1646. 
Yue Lu and Chengxiang Zhai. 2008. Opinion 
integration through semi-supervised topic modeling. 
In Proceedings of WWW, pages 121?130. 
Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009. 
Rated aspect summarization of short comments. In 
Proceedings of WWW, pages 131?140. 
Hosam Mahmoud. 2008. Polya Urn Models. Chapman 
& Hall/CRC Texts in Statistical Science. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and ChengXiang Zhai. 2007. Topic sentiment 
mixture: modeling facets and opinions in weblogs. In 
Proceedings of WWW, pages 171?180. 
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, 
Sujian Li, and Houfeng Wang. 2012. Entity-centric 
topic-oriented opinion summarization in twitter. In 
Proceedings of KDD, pages 379?387. 
George A. Miller. 1995. WordNet: A Lexical Database 
for English. Commun. ACM, 38(11), 39?41. 
David Mimno, Hanna M. Wallach, Edmund Talley, 
Miriam Leenders, and Andrew McCallum. 2011. 
Optimizing semantic coherence in topic models. In 
Proceedings of EMNLP, pages 262?272. 
Samaneh Moghaddam and Martin Ester. 2011. ILDA: 
interdependent LDA model for learning latent 
aspects and their ratings from online product 
reviews. In Proceedings of SIGIR, pages 665?674. 
Saif Mohammad, Cody Dunne, and Bonnie J. Dorr. 
2009. Generating High-Coverage Semantic 
Orientation Lexicons From Overtly Marked Words 
and a Thesaurus. In Proceedings of EMNLP, pages 
599?608. 
Arjun Mukherjee and Bing Liu. 2012. Aspect 
Extraction through Semi-Supervised Modeling. In 
Proceedings of ACL, pages 339?348. 
David Newman, Youn Noh, Edmund Talley, Sarvnaz 
Karimi, and Timothy Baldwin. 2010. Evaluating 
topic models for digital libraries. In Proceedings of 
JCDL, pages 215?224. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1-2), 1?135. 
James Petterson, Alex Smola, Tib?rio Caetano, Wray 
Buntine, and Shravan Narayanamurthy. 2010. Word 
Features for Latent Dirichlet Allocation. In 
Proceedings of NIPS, pages 1921?1929. 
AM Popescu and Oren Etzioni. 2005. Extracting 
product features and opinions from reviews. In 
Proceedings of HLT, pages 339?346. 
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. 
Opinion Word Expansion and Target Extraction 
through Double Propagation. Computational 
Linguistics, 37(1), 9?27. 
Daniel Ramage, David Hall, Ramesh Nallapati, and 
Christopher D. Manning. 2009. Labeled LDA: a 
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP, pages 
248?256. 
Christina Sauper and Regina Barzilay. 2013. Automatic 
Aggregation by Joint Modeling of Aspects and 
Values. J. Artif. Intell. Res. (JAIR), 46, 89?127. 
Christina Sauper, Aria Haghighi, and Regina Barzilay. 
2011. Content Models with Attitude. In Proceedings 
of ACL, pages 350?358. 
Swapna Somasundaran and J. Wiebe. 2009. 
Recognizing stances in online debates. In 
Proceedings of ACL, pages 226?234. 
Keith Stevens and PKDAD Buttler. 2012. Exploring 
Topic Coherence over many models and many 
topics. In Proceedings of EMNLP-CoNLL, pages 
952?961. 
Veselin Stoyanov and Claire Cardie. 2011. 
Automatically Creating General-Purpose Opinion 
1666
Summaries from Text. In Proceedings of RANLP, 
pages 202?209. 
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, 
and David M. Blei. 2006. Hierarchical dirichlet 
processes. Journal of the American Statistical 
Association, 1?30. 
Ivan Titov and Ryan McDonald. 2008. Modeling online 
reviews with multi-grain topic models. In 
Proceedings of WWW, pages 111?120. 
Dong Wang and Yang Liu. 2011. A Pilot Study of 
Opinion Summarization in Conversations. In 
Proceedings of ACL, pages 331?339. 
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. 
Latent aspect rating analysis on review text data: a 
rating regression approach. In Proceedings of KDD, 
pages 783?792. 
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. 
Latent aspect rating analysis without aspect keyword 
supervision. In Proceedings of KDD, pages 618?626. 
Janyce Wiebe and Ellen Riloff. 2005. Creating 
Subjective and Objective Sentence Classifiers from 
Unannotated Texts. In Proceedings of CICLing, 
pages 486?497. 
Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce, 
Matthew Bell, and Melanie Martin. 2004. Learning 
Subjective Language. Computational Linguistics, 
30(3), 277?308. 
Michael Wiegand and Dietrich Klakow. 2010. 
Convolution Kernels for Opinion Holder Extraction. 
In Proceedings of HLT-NAACL, pages 795?803. 
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide 
Wu. 2009. Phrase dependency parsing for opinion 
mining. In Proceedings of EMNLP, pages 1533?
1541. 
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide 
Wu. 2011. Structural Opinion Mining for Graph-
based Sentiment Representation. In Proceedings of 
EMNLP, pages 1332?1341. 
Yunqing Xia, Boyi Hao, and Kam-Fai Wong. 2009. 
Opinion Target Network and Bootstrapping Method 
for Chinese Opinion Target Extraction. In 
Proceedings of AIRS, pages 339?350. 
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect Ranking: Identifying 
Important Product Aspects from Online Consumer 
Reviews. In Proceedings of ACL, pages 1496?1505. 
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011. 
Constrained LDA for grouping product features in 
opinion mining. In Proceedings of the 15th Pacific-
Asia Conference on Knowledge Discovery and Data 
Mining (PAKDD), pages 448?459. 
Lei Zhang and Bing Liu. 2011. Identifying Noun 
Product Features that Imply Opinions. In 
Proceedings of ACL (Short Papers), pages 575?580. 
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and 
Xiaoming Li. 2010. Jointly Modeling Aspects and 
Opinions with a MaxEnt-LDA Hybrid. In 
Proceedings of EMNLP, pages 56?65. 
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2012. 
Cross-Language Opinion Target Extraction in 
Review Texts. In Proceedings of ICDM, pages 
1200?1205. 
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
CIKM, pages 43?50. ACM Press. 
 
1667
Proceedings of NAACL-HLT 2013, pages 1041?1050,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Identifying Intention Posts in Discussion Forums 
 
 
Zhiyuan Chen, Bing Liu Meichun Hsu, Malu Castellanos,  
Riddhiman Ghosh 
Department of Computer Science HP Labs 
University of Illinois at Chicago Palo Alto, CA 94304, USA 
Chicago, IL 60607, USA {meichun.hsu, malu.castellanos, 
riddhiman.ghosh}@hp.com czyuanacm@gmail.com,liub@cs.uic.edu 
 
 
 
 
Abstract 
This paper proposes to study the problem of 
identifying intention posts in online discus-
sion forums. For example, in a discussion fo-
rum, a user wrote ?I plan to buy a camera,? 
which indicates a buying intention. This in-
tention can be easily exploited by advertisers. 
To the best of our knowledge, there is still no 
reported study of this problem. Our research 
found that this problem is particularly suited 
to transfer learning because in different do-
mains, people express the same intention in 
similar ways. We then propose a new transfer 
learning method which, unlike a general 
transfer learning algorithm, exploits several 
special characteristics of the problem. Exper-
imental results show that the proposed meth-
od outperforms several strong baselines, 
including supervised learning in the target 
domain and a recent transfer learning meth-
od. 
1 Introduction 
Social media content is increasingly regarded as 
an information gold mine. Researchers have stud-
ied many problems in social media, e.g., senti-
ment analysis (Pang & Lee, 2008; Liu, 2010) and 
social network analysis (Easley & Kleinberg, 
2010). In this paper, we study a novel problem 
which is also of great value, namely, intention 
identification, which aims to identify discussion 
posts expressing certain user intentions that can be 
exploited by businesses or other interested parties. 
For example, one user wrote, ?I am looking for a 
brand new car to replace my old Ford Focus?. 
Identifying such intention automatically can help 
social media sites to decide what ads to display so 
that the ads are more likely to be clicked. 
This work focuses on identifying user posts 
with explicit intentions. By explicit we mean that 
the intention is explicitly stated in the text, no 
need to deduce (hidden or implicit intention). For 
example, in the above sentence, the author clearly 
expressed that he/she wanted to buy a car. On the 
other hand, an example of an implicit sentence is 
?Anyone knows the battery life of iPhone?? The 
person may or may not be thinking about buying 
an iPhone. 
To our knowledge, there is no reported study of 
this problem in the context of text documents. The 
main related work is in Web search, where user 
(or query) intent classification is a major issue 
(Hu et al, 2009; Li, 2010; Li, Wang, & Acero, 
2008). Its task is to determine what the user is 
searching for based on his/her keyword queries (2 
to 3 words) and his/her click data. We will discuss 
this and other related work in Section 2. 
We formulate the proposed problem as a two-
class classification problem since an application 
may only be interested in a particular intention. 
We define intention posts (positive class) as the 
posts that explicitly express a particular intention 
of interest, e.g., the intention to buy a product. 
The other posts are non-intention posts (negative 
class). Note that we do not exploit intention spe-
cific knowledge since our aim is to propose a ge-
neric method applicable to different types of 
intentions. 
There is an important feature about this prob-
lem which makes it amenable to transfer learning 
so that we do not need to label data in every do-
main. That is, for a particular kind of intention 
such as buying, the ways to express the intention 
in different domains are often very similar. This 
1041
fact can be exploited to build a classifier based on 
labeled data in some domains and apply it to a 
new/target domain without labeling any training 
data in the target domain. However, this problem 
also has some special difficulties that existing 
general transfer learning methods do not deal 
with. The two special difficulties of the proposed 
problem are as follows: 
1. In an intention post, the intention is typically 
expressed in only one or two sentences while 
most sentences do not express intention, which 
provide very noisy data for classifiers. Fur-
thermore, words/phrases used for expressing 
intention are quite limited compared to other 
types of expressions. These mean that the set 
of shared (or common) features in different 
domains is very small. Most of the existing ad-
vanced transfer learning methods all try to ex-
tract and exploit these shared features. The 
small number of such features in our task 
makes it hard for the existing methods to find 
them accurately, which in turn learn poorer 
classifiers. 
2. As mentioned above, in different domains, the 
ways to express the same intention are often 
similar. This means that only the positive (in-
tention) features are shared among different 
domains, while features indicating the negative 
class in different domains are very diverse. We 
then have an imbalance problem, i.e., the 
shared features are almost exclusively features 
indicating the positive class. To our 
knowledge, none of the existing transfer learn-
ing methods deals with this imbalance problem 
of shared features, which also results in inaccu-
rate classifiers.  
We thus propose a new transfer learning (or do-
main adaptation) method, called Co-Class, which, 
unlike a general transfer learning method, is able 
to deal with these difficulties in solving the prob-
lem. Co-Class works as follows: we first build a 
classifier   using the labeled data from existing 
domains, called the source data, and then apply 
the classifier to classify the target (domain) data 
(which is unlabeled). Based on the target data la-
beled by  , we perform a feature selection on the 
target data. The selected set of features is used to 
build two classifiers, one (  ) from the labeled 
source data and one (  ) from the target data 
which has been labeled by  . The two classifiers 
(   and   ) then work together to perform classi-
fication of the target data. The process then runs 
iteratively until the labels assigned to the target 
data stabilize. Note that in each iteration both 
classifiers are built using the same set of features 
selected from the target domain in order to focus 
on the target domain. The proposed Co-Class ex-
plicitly deals with the difficulties mentioned 
above (see Section 3). Our experiments using four 
real-life data sets extracted from four forum dis-
cussion sites show that Co-Class outperforms sev-
eral strong baselines. What is also interesting is 
that it works even better than fully supervised 
learning in the target domain itself, i.e., using both 
training and test data in the target domain. It also 
outperforms a recent state-of-the-art transfer 
learning method (Tan et al, 2009), which has 
been successfully applied to the NLP task of sen-
timent classification.  
In summary, this paper makes two main contri-
butions: 
1. It proposes to study the novel problem of inten-
tion identification. User intention is an im-
portant type of information in social media 
with many applications. To our knowledge, 
there is still no reported study of this problem.  
2. It proposes a new transfer learning method Co-
Class which is able to exploit the above two 
key issues/characteristics of the problem in 
building cross-domain classifiers. Our experi-
mental results demonstrate its effectiveness. 
2 Related Work 
Although we have not found any paper studying 
intention classification of social media posts, there 
are some related works in the domain of Web 
search, where user or query intent classification is 
a major issue (Hu et al, 2009; Li, 2010; Li et al, 
2008). The task there is to classify a query submit-
ted to a search engine to determine what the user 
is searching for. It is different from our problem 
because they classify based on the user-submitted 
keyword queries (often 2 to 3 words) together 
with the user?s click-through data (which repre-
sent the user?s behavior). Such intents are typical-
ly implicit because people usually do not issue a 
search query like ?I want to buy a digital cam-
era.? Instead, they may just type the keywords 
?digital camera?. Our interest is to identify explic-
it intents expressed in full text documents (forum 
posts). Another related problem is online com-
mercial intention (OCI) identification (Dai et al, 
1042
2006; Hu et al, 2009), which focuses on capturing 
commercial intention based on a user query and 
web browsing history. In this sense, OCI is still a 
user query intent problem. 
In NLP, (Kanayama & Nasukawa, 2008) stud-
ied users? needs and wants from opinions. For 
example, they aimed to identify the user needs 
from sentences such as ?I?d be happy if it is 
equipped with a crisp LCD.? This is clearly dif-
ferent from our explicit intention to buy or to use 
a product/service, e.g., ?I plan to buy a new TV.? 
Our proposed Co-Class technique is related to 
transfer learning or domain adaptation. The pro-
posed method belongs to ?feature representation 
transfer" from source domain to target domain 
(Pan & Yang, 2010). Aue & Gamon (2005) tried 
training on a mixture of labeled reviews from oth-
er domains where such data are available and test 
on the target domain. This is basically one of our 
baseline methods 3TR-1TE in Section 4. Their 
work does not do multiple iterations and does not 
build two separate classifiers as we do. Some re-
lated methods were also proposed in (W. Dai, 
Xue, Yang & Yu, 2007; Tan et al, 2007; Yang, Si 
& Callan, 2006). More sophisticated transfer 
learning methods try to find common features in 
both the source and target domains and then try to 
map the differences of the two domains (Blitzer, 
Dredze, & Pereira, 2007; Pan, et al 2010; Bolle-
gala, Weir & Carroll, 2011; Tan et al, 2009). 
Some researchers also used topic modeling of 
both domains to transfer knowledge (Gao & Li, 
2011; He, Lin & Alani, 2011). However, none of 
these methods deals with the two prob-
lems/difficulties of our task. Co-Class tackles 
them explicitly and effectively (Section 4). 
The proposed Co-Class method is also related 
to Co-Training method in (Blum & Mitchell, 
1998). We will compare them in detail in Section 
3.3. 
3 The Proposed Technique 
We now present the proposed technique. Our ob-
jective is to perform classification in the target 
domain by utilizing labeled data from the source 
domains. We use the term ?source domains? as 
we can combine labeled data from multiple source 
domains. The target domain has no labeled data. 
Only the source domain data are labeled. 
To deal with the first problem in Section 1 (i.e., 
the difficulty of finding common features across 
different domains), Co-Class avoids it by using an 
EM-based method to iteratively transfer from the 
source domains to the target domain while ex-
ploiting feature selection in the target domain to 
focus on important features in the target domain. 
Since our ideas are developed starting from the 
EM (Expectation Maximization) algorithm and its 
shortcomings, we now introduce EM. 
3.1 EM Algorithm 
EM (Dempster, Laird, & Rubin, 1977) is a popu-
lar class of iterative algorithms for maximum like-
lihood estimation in problems with incomplete 
data. It is often used to address missing values in 
the data by computing expected values using ex-
isting values. The EM algorithm consists of two 
steps, the Expectation step (E-step) and the Max-
imization step (M-step). E-step basically fills in 
the missing data, and M-step re-estimates the pa-
rameters. This process iterates until convergence. 
Since our target data have no labels, which can be 
treated as missing values/data, the EM algorithm 
naturally applies. For text classification, each iter-
ation of EM (Nigam, McCallum, Thrun, & Mitch-
ell, 2000) usually uses the na?ve Bayes (NB) 
classifier. Below, we first introduce the NB classi-
fier. 
Given a set of training documents  , each doc-
ument      is an ordered list of words. We use 
      to denote the word in the position   of   , 
where each word is from the vocabulary    
         | | , which is the set of all words con-
sidered in classification. We also have a set of 
classes         representing positive and neg-
ative classes. For classification, we compute the 
posterior probability      |   . Based on the 
Bayes rule and multinomial model, we have: 
      
   (1) 
 
and with Laplacian smoothing: 
    (2) 
where          is the number of times that the 
word    occurs in document   , and   (  |  )  
      is the probability of assigning class    to   . 
Assuming that word probabilities are independent 
given a class, we have the NB classifier: 
? ?
?
? ?
?
?
?? |V|
s
|D|
i ijis
|D|
i ijit
jt d|cd,wN|V|
d|cd,wNc|w
1 1
1
))Pr((
))Pr((1)?r(
||
)|(r)(r
||
1
D
dcc
D
i ij
j
? ? ???
1043
   
  (3) 
The EM algorithm basically builds a classifier 
iteratively using NB and both the labeled source 
data and the unlabeled target data. However, the 
major shortcoming is that the feature set, even 
with feature selection, may fit the labeled source 
data well but not the target data because the target 
data has no labels to be used in feature selection. 
Feature selection is shown to be very important 
for this application as we will see in Section 4. 
3.2 FS-EM 
Based on the discussion above, the key to solve 
the problem of EM is to find a way to reflect the 
features in the target domain during the iterations. 
We propose two alternatives, FS-EM (Feature 
Selection EM) and Co-Class (Co-Classification). 
This sub-section presents FS-EM. 
EM can select features only before iterations 
using the labeled source data and keep using the 
same features in each iteration. However, these 
features only fit the labeled source data but not the 
target data. We then propose to select features 
during iterations, i.e., after each iteration, we re-
do feature selection. For this, we use the predicted 
classes of the target data. In na?ve Bayes, we de-
fine the predicted class for document    as 
 
        
    
     |    (4) 
The detailed algorithm for FS-EM is given in 
Figure 1. First, we select a feature set from the 
labeled source data    and then build an initial 
NB classifier (lines 1 and 2). The feature selection 
is based on Information Gain, which will be intro-
duced in Section 3.4. After that, we classify each 
document in the target data    to obtain its pre-
dicted class (lines 4-6). A new target data set    
is produced in line 7, which is    with added 
classes (predicted in line 5). Line 8 selects a new 
feature set   from the data    (which is discussed 
below), from which a new classifier   is built 
(line 9). The iteration stops when the predicted 
classes of    do not change any more (line 10). 
We now turn to the data set   , which can be 
formed with one of the two methods: 
1.          
2.       
The first method (called FS-EM1) merges the 
labeled source data    and the target data    
(with predicted classes). However, this method 
does not work well because the labeled source 
data can dominate    and the target domain fea-
tures are still not well represented. 
The second method (     ), denoted as FS-
EM2, selects features from the target domain data 
   only based on the predicted classes. The clas-
sifiers are built in iterations (lines 3-10) using on-
ly the target domain data. The weakness of this is 
that it completely ignores the labeled source data 
after initialization, but the source data does con-
tain some valuable information. Our final pro-
posed method Co-Class is able to solve this 
problem. 
3.3 Co-Class 
Co-Class is our final proposed algorithm. It con-
siders both the source labeled data and the target 
data with predicted classes. It uses the idea of FS-
EM, but is also inspired by Co-Training in (Blum 
& Mitchell, 1998). It additionally deals with the 
second issue identified in Section 1 (i.e., the im-
balance of shared positive and negative features). 
Co-Training is originally designed for semi-
supervised learning to learn from a small labeled 
and a large unlabeled set of training examples, 
which assumes the set of features in the data can 
be partitioned into two subsets, and each subset is 
sufficient for building an accurate classifier. The 
proposed Co-Class model is similar to Co-
Training in that it also builds two classifiers. 
However, unlike Co-Training, Co-Class does not 
partition the feature space. Instead, one classifier 
is built based on the target data with predicted 
classes (  ), and the other classifier is built using 
only the source labeled data (  ). Both classifiers 
use the same features (this is an important point) 
that are selected from the target data    only, in 
order to focus on the target domain. The final 
classification is based on both classifiers. Fur-
thermore, Co-Training only uses the data from the 
same domain. 
The detailed Co-Class algorithm is given in 
Figure 2. Lines 1-6 are the same as lines 1, 2 and 
4-7 in FS-EM. Line 8 selects new features   from 
  . Two na?ve Bayes classifiers,    and   , are 
then built using the source data    and predicted 
target data    respectively with the same set of 
? ?
?
? ?
?
??
???? ||
1
||
1 ,
||
1 ,
)|(r)(r
)|(r)(r)|(r C
r
d
k rkdr
d
k jkdj
ij i
i
i
i
cwc
cwcdc
1044
features   (lines 9-10). Lines 11-13 classify each 
target domain document    using the two classifi-
ers.  (             ) is the aggregate function to 
combine the results of two classifiers. It is defined 
as: 
 (             )  {
                          
                                      
  
This aims to deal with the imbalanced feature 
problem. As discussed before, the expressions for 
stating a particular intention (e.g., buying) are 
very similar across domains but the non-intention 
expressions across domains are highly diverse, 
which result in strong positive features and weak 
negative features. We then need to restrict the 
positive class by requiring both classifiers to give 
positive predictions. If we use the method in Co-
Training (multiplying the probabilities of the two 
NB classifiers), the classification results deterio-
rate from iteration to iteration because the positive 
class recall gets higher and higher due to strong 
positive features, but the precision gets lower and 
lower. 
Since we build and use two classifiers for the 
final classification, we call the method Co-Class, 
short for Co-Classification. Co-Class is different 
from EM (Nigam et al, 2000) in two main aspects. 
First, it integrates feature selection into the itera-
tions, which has not been done before. Feature 
selection refines features to enhance the correla-
tion between the features and classes. Second, two 
classifiers are built based on different domains 
and combined to improve the classification. Only 
one classifier is built in existing EM methods, 
which gives poorer results (Section 4). 
3.4 Feature Selection 
As feature selection is important for our task, we 
briefly introduce the Information Gain (IG) meth-
od given in (Yang & Pedersen, 1997), which is a 
popular feature selection algorithm for text classi-
fication. IG is based on entropy reflecting the pu-
rity of the categories or classes by knowing the 
presence or absence of each feature, which is de-
fined as: 
? ??
??
???
ff
m
i
ii
m
i
ii fcPfcPfPcPcPfIG
, 11
)|(log)|()()(log)()(
    
Using the IG value of each feature  , all fea-
tures can be ranked. As in normal classification 
tasks, the common practice is to use a set of top 
ranked features for classification. 
4 Evaluation 
We have conducted a comprehensive set of exper-
iments to compare the proposed Co-Class method 
with several strong baselines, including a state-of-
the-art transfer learning method. 
4.1 Experiment Settings 
Datasets: We created 4 different domain datasets 
crawled from 4 different forum discussion sites: 
Cellphone: http://www.howardforums.com/forums.php 
Electronics: http://www.avsforum.com/avs-vb/ 
Camera: http://forum.digitalcamerareview.com/ 
Algorithm FS-EM 
     Input:  Labeled data    and unlabeled data    
1   Select a feature set   based on IG from   ; 
2   Learn an initial na?ve Bayes classifier   from     
         based on   (using Equations (1) and (2)); 
3   repeat 
4       for each document    in    do 
5                  ;   // predict the class of    using   
6       end 
7       Produce data    based on predicted class of   ; 
8       Select a new feature set   from   ; 
9       Learn a new classifier   on    
based on the new feature set  ; 
10 until the predicted classes of    stabilize 
11 Return the classifier   from the last iteration. 
Figure 1 ? The FS-EM algorithm 
Algorithm Co-Class 
     Input:  Labeled data    and unlabeled data    
1   Select a feature set   based on IG from   ; 
2   Learn an initial na?ve Bayes classifier   from            
         based on   (using Equations (1) and (2)); 
3  for each document    in    do 
4              ;   // predict the class of    using   
5    end 
6   Produce data    based on the predicted class of   ; 
7   repeat 
8       Select a new feature set   from   ; 
9       Build a na?ve Bayes classifier    using   and   ; 
10     Build a na?ve Bayes classifier    using   and   ; 
11     for each document    in    do 
12             (             ); // Aggregate function 
13     end 
14     Produce data    based on predicted class of   ; 
15 until the prediction classes of    stabilize 
16 Return classifiers    and    from the last iteration. 
Figure 2 ? The Co-Class algorithm 
1045
TV: http://www.avforums.com/forums/tvs/  
For our experiments, we are interested in the in-
tention to buy, which is our intention or positive 
class. For each dataset, we manually labeled 1000 
posts. 
Labeling: We initially labeled about one fifth of 
posts by two human annotators. We found their 
labels highly agreed. We then used only one anno-
tator to complete the remaining labeling. The rea-
son for the strong labeling agreement is that we 
are interested in only explicit buying intentions, 
which are clearly expressed in each post, e.g., ?I 
am in the market for a new smartphone.? There is 
little ambiguity or subjectivity in labeling. 
To ensure that the task is realistic, for all da-
tasets we keep their original class distributions as 
they are extracted from their respective websites 
to reflect the real-life situation. The intention class 
is always the minority class, which makes it much 
harder to predict due to the imbalanced class dis-
tribution. Table 1 gives the statistics of each da-
taset. On average, each post contains about 7.5 
sentences and 122 words. We have made the da-
tasets used in this paper publically available at the 
websites of the first two authors.  
Evaluation measures: For all experiments, we 
use precision, recall and F1-score as the evalua-
tion measures. They are suitable because our ob-
jective is to identify intention posts. 
4.2 One Domain Learning 
The objective of our work is to classify the target 
domain instances without labeling any target do-
main data. To set the background, we first give 
the results of one domain learning, i.e., assuming 
that there is labeled training data in the target do-
main (which is the traditional fully supervised 
learning).  We want to see how the results of Co-
Class compare with the fully supervised learning. 
For this set of experiments, we use na?ve Bayes 
and SVM. For na?ve Bayes, we use the Lingpipe 
implementation (http://alias-i.com/lingpipe/). For 
SVM, we use SVMLight (Joachims, 1999) from 
(http://svmlight.joachims.org/) with the linear 
kernel as it has been shown by many researchers 
that linear kernel is sufficient for text classifica-
tion (Joachims, 1998; Yang and Liu, 1999). 
During labeling, we observed that the intention 
in an intention (positive) post is often expressed in 
the first few or the last few sentences. Hence, we 
tried to use the full post (denoted by Full), the first 
5 sentences (denoted by (5, 0)), and first 5 and last 
5 sentences (denoted by (5, 5)). We also experi-
mented with the first 3 sentences, and first 3 and 
last 3 sentences but their results were poorer. 
The experiments were done using 10-fold cross 
validation. For the number of selected features, 
we tried 500, 1000, 1500, 2000, 2500 and all. We 
also tried unigrams, bigrams, trigrams, and 4-
grams. To compare na?ve Bayes with SVM, we 
tried each combination, i.e. number of features 
and n-grams, and found the best model for each 
method. We found that na?ve Bayes works best 
when using trigrams with 1500 selected features. 
Bigrams with 1000 features are the best combina-
tion for SVM. Figure 3 shows the comparison of 
the best results (F1-scores) of na?ve Bayes and 
SVM. 
From Figure 3, we make the following observa-
tions: 
1. SVM does not do well for this task. We tuned 
the parameters of SVM, but the results were 
similar to the default setting, and all were 
worse than na?ve Bayes. We believe the main 
reason is that the data for this application is 
highly noisy because apart from one or two in-
tention sentences, other sentences in an inten-
tion post have little difference from those in a 
non-intention post. SVM does not perform well 
with very noisy data. When there are data 
points far away from their own classes, SVM 
Dataset 
No. of 
Intention 
No. of 
Non-Intention 
Total No. 
of posts 
Cellphone 184 816 1000 
Electronics 280 720 1000 
Camera 282 718 1000 
TV 263 737 1000 
Table 1: Datasets statistics with the buy intention 
 
 
Figure 3 ? Na?ve Bayes vs. SVM 
1046
tends to be strongly affected by such points 
(Wu & Liu, 2007). Na?ve Bayes is more robust 
in the presence of noise due to its probabilistic 
nature. 
2. SVM using only the first few and/or last few 
sentences performs better than using full posts 
because full posts have more noise. However, 
it is still worse than na?ve Bayes. 
3. For na?ve Bayes, using full posts and the first 5 
and last 5 (5, 5) sentences give similar results, 
which is not surprising as (5, 5) has almost all 
the information needed. Without using the last 
5 sentence (5, 0), the results are poorer. 
We also found that without feature selection (us-
ing all features), the results are markedly worse 
for both na?ve Bayes and SVM. This is under-
standable (as we discussed earlier) because most 
words and sentences in both intention and non-
intention posts are very similar. Thus, feature se-
lection is highly desirable for this application. 
Effect of different combinations: Table 2 gives 
the detailed F1-score results of na?ve Bayes with 
best results in different n-grams (with best number 
of features). We can see that using trigrams pro-
duces the best results on average, but bigrams and 
4-grams are quite similar. It turns out that using 
trigrams with 1500 selected features performs the 
best. SVM results are not shown as they are poor-
er. 
In summary, we say that na?ve Bayes is more 
suitable than SVM for our application and feature 
selection is crucial. In our experiments reported 
below, we will only use na?ve Bayes with feature 
selection. 
4.3 Evaluation of Co-Class 
We now compare Co-Class with the baseline 
methods listed below. Note that for this set of ex-
periments, the source data all contain labeled 
posts from three domains and the target data con-
tain unlabeled posts in one domain. That is, for 
each target domain, we merge three other domains 
for training and the target domain for testing. For 
example, for the target of ?Cellphone?, the model 
is built using the data from the other three do-
mains (i.e., ?Electronics?, ?Camera? and ?TV?). 
The results are the classification of the model on 
the target domain ?Cellphone?. Several strong 
baselines are described as follows: 
3TR-1TE: Use labeled data from three do-
mains to train and then classify the target (test) 
domain. There is no iteration. This method was 
used in (Aue & Gamon, 2005). 
EM: This is the algorithm in Section 3.1. The 
combined data from three domains are used as the 
labeled source data. The data of the remaining one 
domain are used as the unlabeled target data, 
which is also used as the test data (since it is unla-
beled). 
ANB: This is a recent transfer learning method 
(Tan et al, 2009). ANB uses frequently co-
occurring entropy (FCE) to pick out generalizable 
(or shared) features that occur frequently in both 
the source and target domains. Then, a weighted 
transfer version of na?ve Bayes classifier is ap-
plied. We chose this method for comparison as it 
is a recent method, also based on na?ve Bayes, and 
has been applied to the NLP task of sentiment 
Na?ve Bayes 
(n-grams, features) 
Cellphone Electronics Camera TV 
Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 
Unigrams, 2000 59.91 55.21 56.76 71.31 70.10 71.24 71.57 71.53 75.78 74.96 74.45 74.13 
Bigrams, 1500 61.97 54.29 59.17 70.71 71.46 72.48 77.02 74.12 77.38 79.76 77.71 79.72 
Trigrams, 1500 61.50 55.78 60.15 71.38 71.07 71.61 77.66 75.71 78.74 80.24 75.66 79.92 
4-grams, 2000 58.94 51.94 57.72 72.03 71.98 73.05 79.84 75.09 79.46 79.12 76.61 79.88 
Table 2: One-domain learning using na?ve Bayes with n-grams (with best no. of features) 
Na?ve Bayes 
(n-grams, features) 
Cellphone Electronics Camera TV 
Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 
Trigrams, 2000 57.98 57.60 58.67 71.85 69.74 71.51 74.45 73.58 74.24 74.07 71.34 73.65 
Trigrams, 2500 58.08 57.48 59.12 72.27 69.65 71.82 76.15 73.64 76.31 74.02 71.25 73.49 
Trigrams, 3000 56.74 56.94 56.74 72.27 70.76 72.43 77.62 74.65 77.62 75.64 71.65 74.73 
Trigrams, 3500 56.60 56.81 57.21 71.86 70.40 72.24 77.17 74.85 76.68 74.25 71.10 73.37 
4-grams, 2000 58.94 51.94 57.72 72.03 71.98 73.05 79.84 75.09 79.46 79.12 76.61 79.88 
Table 3: F1-scores of 3TR-1TE with trigrams and different no. of features 
 
1047
classification, which to some extend is related to 
the proposed task of intention classification. ANB 
was also shown to perform better than EM and 
na?ve Bayes transfer learning method (Dai et al, 
2007). 
We look at the results of 3TR-1TE first, which 
are shown in Table 3. Due to space limitations, we 
only show the trigrams F1-scores as they perform 
the best on average. Table 3 gives the number of 
features with trigrams. We can observe that on 
average using 3000 features gives the best F1-
score results. It has 1000 more features than one 
domain learning because we now combine three 
domains (3000 posts) for training and thus more 
useful features. 
From Table 3, we observe that the F1-score re-
sults of 3TR-1TE are worse than those of one do-
main learning (Table 2), which is intuitive 
because no training data are used from the target 
domain. But the results are not dramatically worse 
which indicate that there are some common fea-
tures in different domains, meaning people ex-
pressing the same intention in similar ways. 
Since we found that trigrams with 3000 features 
perform the best on average, we run EM, FS-
EM1, FS-EM2 and Co-Class based on trigrams 
with 3000 features. For the baseline ANB, we 
tuned the parameters using a development set 
(1/10 of the training data). We found that select-
ing 2000 generalizable/shared features gives the 
best results (the default is 500 in (Tan et al, 
2009)). We kept ANB?s other original parameter 
values. The F1-scores (averages over all 4 da-
tasets) with the number of iterations are shown in 
Figure 4. Iteration 0 is the result of 3TR-1TE. 
From Figure 4, we can make the following obser-
vations: 
1. EM makes a little improvement in iteration 1. 
After that, the results deteriorate. The gain of 
iteration 1 shows that incorporating the target 
domain data (unlabeled) is helpful. However, 
the selected features from source domains can 
only fit the labeled source data but not the tar-
get data, which was explained in Section 3.1. 
2. ANB improves slightly from iteration 1 to iter-
ation 6, but the results are all worse than those 
of Co-Class. We checked the generaliza-
ble/shared features of ANB and found that they 
were not suitable for our problem since they 
were mainly adjectives, nouns and sentiment 
verbs, which do not have strong correlation 
with intentions. This shows that it is hard to 
find the truly shared features indicating inten-
tions. Furthermore, ANB?s results are almost 
the same as those of EM. 
3. FS-EM2 behaves similarly to FS-EM1. After 
two iterations, the results start to deteriorate. 
Selecting features only from the target domain 
makes sense since it can reflect target domain 
data well. However, it also becomes worse 
with the increased number of iterations, due to 
strong positive features. With increased itera-
tions, positive features get stronger due to the 
imbalanced feature problem discussed in Sec-
tion 1. 
4. Co-Class performs much better than all other 
methods. With the increased number of itera-
tions, the results actually improve. Starting 
from iteration 7, the results stabilize. Co-Class 
solves the problem of strong positive features 
by requiring strong conditions for positive 
classification and focusing on features in the 
target domain only. Although the detailed re-
sults of precision and recall are not shown, the 
Co-Class model actually improves the F1-score 
by improving both the precision and recall.  
Significance of improvement: We now discuss 
the significance of improvements by comparing 
the results of Co-Class with other models. Table 4 
summarizes the results among the models. For 
Co-Class, we use the converged models at itera-
tion 7. We also include the One Domain learning 
results which are from fully supervised classifica-
tion in the target domains with trigrams and 1500 
features. The results of 3TR-1TE, EM, ANB, FS-
EM1, and FS-EM2 are obtained based on their 
settings which give the best results in Figure 4. 
 
Figure 4 ? Comparison EM, ANB, FS-EM1, FS-EM2, 
and Co-Class across iterations (0 is 3TR-1TE) 
1048
It is clear from Table 4 that Co-Class is the best 
method in general. It is even better than the fully 
supervised One-Domain learning, although their 
results are not strictly comparable because One-
Domain learning uses training and test data from 
the same domain via 10-fold cross validation, 
while all other methods use one domain as the test 
data (the labeled data are from the other three do-
mains). One possible reason is that the labeled 
data are much bigger than those in One-Domain 
learning, which contain more expressions of buy-
ing intention. Note that FS-EM1 and FS-EM2 
work slightly better than Co-Class in domain 
?Camera? because it is the least noisy domain 
with very short posts while other domains (as 
source data) are quite noisy. With good quality 
data, FS-EM1 and FS-EM2 (also proposed in this 
paper) can do slightly better than Co-Class. Statis-
tical paired t-test shows that Co-Class performs 
significantly better than baseline methods 3TR-
1TE, EM, ANB and FS-EM1 at the confidence 
level of 95%, and better than FS-EM2 at the con-
fidence level of 94%. 
Effect of the number of training domains: In 
our experiments above, we used 3 source domain 
data and tested on one target domain. We now 
show what happens if we use only one or two 
source domain data and test on one target domain. 
We tried all possible combinations of source and 
target data. Figure 5 gives the average results over 
the four target/test domains. We can see that using 
more source domains is better due to more labeled 
data. With more domains, Co-Class also improves 
more over 3TR-1TE. 
5 Conclusion 
This paper studied the problem of identifying in-
tention posts in discussion forums. The problem 
has not been studied in the social media context. 
Due to special characteristics of the problem, we 
found that it is particularly suited to transfer learn-
ing. A new transfer learning method, called Co-
Class, was proposed to solve the problem. Unlike 
a general transfer learning method, Co-Class can 
deal with two specific difficulties of the problem 
to produce more accurate classifiers. Our experi-
mental results show that Co-Class outperforms 
strong baselines including classifiers trained using 
labeled data in the target domains and classifiers 
from a state-of-the-art transfer learning method. 
Acknowledgments 
This work was supported in part by a grant from 
National Science Foundation (NSF) under grant 
no. IIS-1111092, and a grant from HP Labs Inno-
vation Research Program.  
References 
Aue, A., & Gamon, M. (2005). Customizing Sentiment 
Classifiers to New Domains: A Case Study. Pro-
ceedings of Recent Advances in Natural Language 
Processing (RANLP). 
Blitzer, J., Dredze, M., & Pereira, F. (2007). Biog-
raphies, Bollywood, Boom-boxes and Blenders: 
Domain Adaptation for Sentiment Classification. 
Proceedings of Annual Meeting of the Association 
for Computational Linguistics (ACL). 
Model 
Cellphone Electronics Camera TV 
Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 
One-Domain 61.50 55.78 60.15 71.38 71.07 71.61 77.66 75.71 78.74 80.24 75.66 79.92 
3TR-1TE 56.74 56.94 56.74 72.27 70.76 72.43 77.62 74.65 77.62 75.64 71.65 74.73 
EM 60.28 59.59 60.45 70.47 69.90 71.33 79.38 77.01 80.31 74.96 70.76 74.31 
ANB 62.53 59.29 62.41 66.58 68.29 68.36 78.37 77.49 78.83 78.70 75.73 78.26 
FS-EM1 59.01 57.69 59.41 70.75 71.74 72.00 80.58 76.13 80.37 79.29 73.75 77.34 
FS-EM2 59.54 60.09 61.33 71.19 72.09 72.07 80.14 77.93 81.09 78.90 74.21 77.53 
Co-Class 62.69 61.10 62.69 73.38 73.23 73.95 79.69 74.65 78.66 81.12 76.40 81.60 
Table 4: F1-score results of One-Domain, 3TR-1TE, EM, ANB, FS-EM1, FS-EM2, and Co-Class 
 
Figure 5 ? Effect of number of source domains 
using 3TR-1TE and Co-Class. 
 
 
 
 
 
 
1049
Blum, A., & Mitchell, T. (1998). Combining Labeled 
and Unlabeled Data with Co-Training. COLT: Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory. 
Bollegala, D., Weir, D. J., & Carroll, J. (2011). Using 
Multiple Sources to Construct a Sentiment Sensitive 
Thesaurus for Cross-Domain Sentiment Classifica-
tion. Proceedings of Annual Meeting of the Associa-
tion for Computational Linguistics (ACL). 
Dai, H. K., Zhao, L., Nie, Z., Wen, J. R., Wang, L., & 
Li, Y. (2006). Detecting online commercial inten-
tion (OCI). Proceedings of the 15th international 
conference on World Wide Web (WWW). 
Dai, W., Xue, G., Yang, Q., & Yu, Y. (2007). Transfer-
ring naive bayes classifiers for text classification. In 
Proceedings of the 22nd AAAI Conference on Artifi-
cial Intelligence (AAAI). 
Dempster, A., Laird, N., & Rubin, D. (1977). Maxi-
mum likelihood from incomplete data via the EM 
algorithm. Journal of the Royal Statistical Society. 
Series B, 39(1), 1?38. 
Easley, D., & Kleinberg, J. (2010). Networks, Crowds, 
and Markets: Reasoning About a Highly Connected 
World. Cambridge University Press. 
Gao, S., & Li, H. (2011). A cross-domain adaptation 
method for sentiment classification using probabilis-
tic latent analysis. Proceedings of the 20th ACM in-
ternational conference on Information and 
knowledge management (CIKM). 
He, Y., Lin, C., & Alani, H. (2011). Automatically 
Extracting Polarity-Bearing Topics for Cross-
Domain Sentiment Classification. Proceedings of 
the 49th Annual Meeting of the Association for 
Computational Linguistics: Human Language Tech-
nologies (ACL). 
Hu, D. H., Shen, D., Sun, J.-T., Yang, Q., & Chen, Z. 
(2009). Context-Aware Online Commercial Inten-
tion Detection. Proceedings of the 1st Asian Confer-
ence on Machine Learning: Advances in Machine 
Learning (ACML). 
Hu, J., Wang, G., Lochovsky, F., tao Sun, J., & Chen, 
Z. (2009). Understanding user?s query intent with 
wikipedia. Proceedings of the 18th international 
conference on World wide web (WWW). 
Joachims, T. (1998). Text Categorization with Support 
Vector Machines: Learning with Many Relevant 
Features. European Conference on Machine Learn-
ing (ECML). 
Joachims, T. (1999). Making large-Scale SVM Learn-
ing Practical. Advances in Kernel Methods - Support 
Vector Learning. MIT Press. 
Kanayama, H., & Nasukawa, T. (2008). Textual De-
mand Analysis: Detection of Users? Wants and 
Needs from Opinions. Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING). 
Li, X. (2010). Understanding the Semantic Structure of 
Noun Phrase Queries. Proceedings of Annual Meet-
ing of the Association for Computational Linguistics 
(ACL). 
Li, X., Wang, Y.-Y., & Acero, A. (2008). Learning 
query intent from regularized click graphs. Proceed-
ings of the 31st annual international ACM SIGIR 
conference on Research and development in infor-
mation retrieval (SIGIR). 
Liu, B. (2010). Sentiment Analysis and Subjectivity. 
(N. Indurkhya & F. J. Damerau, Eds.) Handbook of 
Natural Language Processing, 2nd ed. 
Nigam, K., McCallum, A. K., Thrun, S., & Mitchell, T. 
(2000). Text Classification from Labeled and Unla-
beled Documents using EM. Mach. Learn., 39(2-3), 
103?134. 
Pan, S. J., Ni, X., Sun, J.-T., Yang, Q., & Chen, Z. 
(2010). Cross-domain sentiment classification via 
spectral feature alignment. Proceedings of the 19th 
international conference on World wide web 
(WWW). 
Pan, S. J., & Yang, Q. (2010). A Survey on Transfer 
Learning. IEEE Trans. Knowl. Data Eng., 22(10), 
1345?1359. 
Pang, B., & Lee, L. (2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2), 1?135. 
Tan, S., Cheng, X., Wang, Y., & Xu, H. (2009). Adapt-
ing Naive Bayes to Domain Adaptation for Senti-
ment Analysis. Proceedings of the 31th European 
Conference on IR Research on Advances in Infor-
mation Retrieval (ECIR). 
Tan, S., Wu, G., Tang, H., & Cheng, X. (2007). A nov-
el scheme for domain-transfer problem in the con-
text of sentiment analysis. Proceedings of the 
sixteenth ACM conference on Conference on infor-
mation and knowledge management (CIKM). 
Wu, Y., & Liu, Y. (2007). Robust truncated-hinge-loss 
support vector machines. Journal of the American 
Statistical Association, 102(479), 974?983. 
Yang, H., Si, L., & Callan, J. (2006). Knowledge 
Transfer and Opinion Detection in the TREC 2006 
Blog Track. Proceedings of TREC. 
Yang, Y., & Liu, X. (1999). A re-examination of text 
categorization methods. Proceedings of the 22nd 
annual international ACM SIGIR conference on Re-
search and development in information retrieval 
(SIGIR). 
Yang, Y., & Pedersen, J. O. (1997). A Comparative 
Study on Feature Selection in Text Categorization. 
Proceedings of the Fourteenth International Con-
ference on Machine Learning (ICML). 
1050
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 347?358,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Aspect Extraction with Automated Prior Knowledge Learning
Zhiyuan Chen Arjun Mukherjee Bing Liu
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607, USA
{czyuanacm,arjun4787}@gmail.com,liub@cs.uic.edu
Abstract
Aspect extraction is an important task in
sentiment analysis. Topic modeling is a
popular method for the task. However,
unsupervised topic models often generate
incoherent aspects. To address the is-
sue, several knowledge-based models have
been proposed to incorporate prior knowl-
edge provided by the user to guide mod-
eling. In this paper, we take a major
step forward and show that in the big data
era, without any user input, it is possi-
ble to learn prior knowledge automatically
from a large amount of review data avail-
able on the Web. Such knowledge can
then be used by a topic model to discover
more coherent aspects. There are two key
challenges: (1) learning quality knowl-
edge from reviews of diverse domains,
and (2) making the model fault-tolerant
to handle possibly wrong knowledge. A
novel approach is proposed to solve these
problems. Experimental results using re-
views from 36 domains show that the pro-
posed approach achieves significant im-
provements over state-of-the-art baselines.
1 Introduction
Aspect extraction aims to extract target entities
and their aspects (or attributes) that people have
expressed opinions upon (Hu and Liu, 2004, Liu,
2012). For example, in ?The voice is not clear,?
the aspect term is ?voice.? Aspect extraction has
two subtasks: aspect term extraction and aspect
term resolution. Aspect term resolution groups ex-
tracted synonymous aspect terms together. For ex-
ample, ?voice? and ?sound? should be grouped to-
gether as they refer to the same aspect of phones.
Recently, topic models have been extensively
applied to aspect extraction because they can per-
form both subtasks at the same time while other
existing methods all need two separate steps (see
Section 2). Traditional topic models such as
LDA (Blei et al, 2003) and pLSA (Hofmann,
1999) are unsupervised methods for extracting la-
tent topics in text documents. Topics are aspects
in our task. Each aspect (or topic) is a distribution
over (aspect) terms. However, researchers have
shown that fully unsupervised models often pro-
duce incoherent topics because the objective func-
tions of topic models do not always correlate well
with human judgments (Chang et al, 2009).
To tackle the problem, several semi-supervised
topic models, also called knowledge-based topic
models, have been proposed. DF-LDA (Andrze-
jewski et al, 2009) can incorporate two forms
of prior knowledge from the user: must-links
and cannot-links. A must-link implies that two
terms (or words) should belong to the same topic
whereas a cannot-link indicates that two terms
should not be in the same topic. In a similar but
more generic vein, must-sets and cannot-sets are
used in MC-LDA (Chen et al, 2013b). Other re-
lated works include (Andrzejewski et al, 2011,
Chen et al, 2013a, Chen et al, 2013c, Mukher-
jee and Liu, 2012, Hu et al, 2011, Jagarlamudi et
al., 2012, Lu et al, 2011, Petterson et al, 2010).
They all allow prior knowledge to be specified by
the user to guide the modeling process.
In this paper, we take a major step further. We
mine the prior knowledge directly from a large
amount of relevant data without any user inter-
vention, and thus make this approach fully au-
tomatic. We hypothesize that it is possible to
learn quality prior knowledge from the big data
(of reviews) available on the Web. The intuition
is that although every domain is different, there
is a decent amount of aspect overlapping across
domains. For example, every product domain
has the aspect/topic of ?price,? most electronic
products share the aspect ?battery? and some also
share ?screen.? Thus, the shared aspect knowl-
347
edge mined from a set of domains can poten-
tially help improve aspect extraction in each of
these domains, as well as in new domains. Our
proposed method aims to achieve this objective.
There are two major challenges: (1) learning qual-
ity knowledge from a large number of domains,
and (2) making the extraction model fault-tolerant,
i.e., capable of handling possibly incorrect learned
knowledge. We briefly introduce the proposed
method below, which consists of two steps.
Learning quality knowledge: Clearly, learned
knowledge from only a single domain can be er-
roneous. However, if the learned knowledge is
shared by multiple domains, the knowledge is
more likely to be of high quality. We thus propose
to first use LDA to learn topics/aspects from each
individual domain and then discover the shared as-
pects (or topics) and aspect terms among a sub-
set of domains. These shared aspects and aspect
terms are more likely to be of good quality. They
can serve as the prior knowledge to guide a model
to extract aspects. A piece of knowledge is a set
of semantically coherent (aspect) terms which are
likely to belong to the same topic or aspect, i.e.,
similar to a must-link, but mined automatically.
Extraction guided by learned knowledge: For
reliable aspect extraction using the learned prior
knowledge, we must account for possible errors
in the knowledge. In particular, a piece of au-
tomatically learned knowledge may be wrong or
domain specific (i.e., the words in the knowledge
are semantically coherent in some domains but
not in others). To leverage such knowledge, the
system must detect those inappropriate pieces of
knowledge. We propose a method to solve this
problem, which also results in a new topic model,
called AKL (Automated Knowledge LDA), whose
inference can exploit the automatically learned
prior knowledge and handle the issues of incorrect
knowledge to produce superior aspects.
In summary, this paper makes the following
contributions:
1. It proposes to exploit the big data to learn prior
knowledge and leverage the knowledge in topic
models to extract more coherent aspects. The
process is fully automatic. To the best of our
knowledge, none of the existing models for as-
pect extraction is able to achieve this.
2. It proposes an effective method to learn qual-
ity knowledge from raw topics produced using
review corpora from many different domains.
3. It proposes a new inference mechanism for
topic modeling, which can handle incorrect
knowledge in aspect extraction.
2 Related Work
Aspect extraction has been studied by many re-
searchers in sentiment analysis (Liu, 2012, Pang
and Lee, 2008), e.g., using supervised sequence
labeling or classification (Choi and Cardie, 2010,
Jakob and Gurevych, 2010, Kobayashi et al, 2007,
Li et al, 2010, Yang and Cardie, 2013) and us-
ing word frequency and syntactic patterns (Hu
and Liu, 2004, Ku et al, 2006, Liu et al, 2013,
Popescu and Etzioni, 2005, Qiu et al, 2011, So-
masundaran and Wiebe, 2009, Wu et al, 2009, Xu
et al, 2013, Yu et al, 2011, Zhao et al, 2012, Zhou
et al, 2013, Zhuang et al, 2006). However,
these works only perform extraction but not as-
pect term grouping or resolution. Separate aspect
term grouping has been done in (Carenini et al,
2005, Guo et al, 2009, Zhai et al, 2011). They
assume that aspect terms have been extracted be-
forehand.
To extract and group aspects simultaneously,
topic models have been applied by researchers
(Branavan et al, 2008, Brody and Elhadad, 2010,
Chen et al, 2013b, Fang and Huang, 2012, He
et al, 2011, Jo and Oh, 2011, Kim et al, 2013,
Lazaridou et al, 2013, Li et al, 2011, Lin and
He, 2009, Lu et al, 2009, Lu et al, 2012, Lu and
Zhai, 2008, Mei et al, 2007, Moghaddam and Es-
ter, 2013, Mukherjee and Liu, 2012, Sauper and
Barzilay, 2013, Titov and McDonald, 2008, Wang
et al, 2010, Zhao et al, 2010). Our proposed AKL
model belongs to the class of knowledge-based
topic models. Besides the knowledge-based topic
models discussed in Section 1, document labels
are incorporated as implicit knowledge in (Blei
and McAuliffe, 2007, Ramage et al, 2009). Ge-
ographical region knowledge has also been con-
sidered in topic models (Eisenstein et al, 2010).
All of these models assume that the prior knowl-
edge is correct. GK-LDA (Chen et al, 2013a) is
the only knowledge-based topic model that deals
with wrong lexical knowledge to some extent. As
we will see in Section 6, AKL outperformed GK-
LDA significantly due to AKL?s more effective er-
ror handling mechanism. Furthermore, GK-LDA
does not learn any prior knowledge.
Our work is also related to transfer learning to
some extent. Topic models have been used to help
348
Input: Corpora DL for knowledge learning
Test corpora DT
1: // STEP 1: Learning prior knowledge.
2: for r = 0 to R do // Iterate R+ 1 times.
3: for each domain corpus D
i
?DL do
4: if r = 0 then
5: A
i
? LDA(D
i
);
6: else
7: A
i
? AKL(D
i
,K);
8: end if
9: end for
10: A? ?
i
A
i
;
11: TC ? Clustering(A);
12: for each cluster T
j
? TC do
13: K
j
? FPM(T
j
);
14: end for
15: K ? ?
j
K
j
;
16: end for
17: // STEP 2: Using the learned knowledge.
18: for each test corpus D
i
?DT do
19: A
i
? AKL(D
i
,K);
20: end for
Figure 1: The proposed overall algorithm.
transfer learning (He et al, 2011, Pan and Yang,
2010, Xue et al, 2008). However, transfer learn-
ing in these papers is for traditional classification
rather than topic/aspect extraction. In (Kang et al,
2012), labeled documents from source domains
are transferred to the target domain to produce
topic models with better fitting. However, we do
not use any labeled data. In (Yang et al, 2011), a
user provided parameter indicating the technical-
ity degree of a domain was used to model the lan-
guage gap between topics. In contrast, our method
is fully automatic without human intervention.
3 Overall Algorithm
This section introduces the proposed overall algo-
rithm. It consists of two main steps: learning qual-
ity knowledge and using the learned knowledge.
Figure 1 gives the algorithm.
Step 1 (learning quality knowledge, Lines 1-
16): The input is the review corpora DL from
multiple domains, from which the knowledge is
automatically learned. Lines 3 and 5 run LDA on
each review domain corpus D
i
? DL to gener-
ate a set of aspects/topics A
i
(lines 2, 4, and 6-
9 will be discussed below). Line 10 unions the
topics from all domains to give A. Lines 11-14
cluster the topics in A into some coherent groups
(or clusters) and then discover knowledgeK
j
from
each group of topics using frequent pattern mining
(FPM) (Han et al, 2007). We will detail these in
Section 4. Each piece of the learned knowledge
is a set of terms which are likely to belong to the
same aspect.
Iterative improvement: The above process can
actually run iteratively because the learned knowl-
edge K can help the topic model learn better top-
ics in each domain D
i
? DL, which results in
better knowledge K in the next iteration. This it-
erative process is reflected in lines 2, 4, 6-9 and 16.
We will examine the performance of the process at
different iterations in Section 6.2. From the sec-
ond iteration, we can use the knowledge learned
from the previous iteration (lines 6-8). The learned
knowledge is leveraged by the new model AKL,
which is discussed below in Step 2.
Step 2 (using the learned knowledge, Lines 17-
20): The proposed model AKL is employed to use
the learned knowledge K to help topic modeling
in test domains DT , which can be DL or other
unseen domains. The key challenge of this step is
how to use the learned prior knowledge K effec-
tively in AKL and deal with possible errors in K.
We will elaborate them in Section 5.
Scalability: the proposed algorithm is naturally
scalable as both LDA and AKL run on each do-
main independently. Thus, for all domains, the
algorithm can run in parallel. Only the resulting
topics need to be brought together for knowledge
learning (Step 1). These resulting topics used in
learning are much smaller than the domain corpus
as only a list of top terms from each topic are uti-
lized due to their high reliability.
4 Learning Quality Knowledge
This section details Step 1 in the overall algorithm,
which has three sub-steps: running LDA (or AKL)
on each domain corpus, clustering the resulting
topics, and mining frequent patterns from the top-
ics in each cluster. Since running LDA is simple,
we will not discuss it further. The proposed AKL
model will be discussed in Section 5. Below we
focus on the other two sub-steps.
4.1 Topic Clustering
After running LDA (or AKL) on each domain cor-
pus, a set of topics is obtained. Each topic is
a distribution over terms (or words), i.e., terms
with their associated probabilities. Here, we use
only the top terms with high probabilities. As dis-
cussed earlier, quality knowledge should be shared
349
by topics across several domains. Thus, it is nat-
ural to exploit a frequency-based approach to dis-
cover frequent set of terms as quality knowledge.
However, we need to deal with two issues.
1. Generic aspects, such as price with aspect
terms like cost and pricy, are shared by many
(even all) product domains. But specific as-
pects such as screen, occur only in domains
with products having them. It means that dif-
ferent aspects may have distinct frequencies.
Thus, using a single frequency threshold in the
frequency-based approach is not sufficient to
extract both generic and specific aspects be-
cause the generic aspects will result in numer-
ous spurious aspects (Han et al, 2007).
2. A term may have multiple senses in different
domains. For example, light can mean ?of little
weight? or ?something that makes things visi-
ble?. A good knowledge base should have the
capacity of handling this ambiguity.
To deal with these two issues, we propose to
discover knowledge in two stages: topic clustering
and frequent pattern mining (FPM).
The purpose of clustering is to group raw topics
from a topic model (LDA or AKL) into clusters.
Each cluster contains semantically related topics
likely to indicate the same real-world aspect. We
then mine knowledge from each cluster using an
FPM technique. Note that the multiple senses of a
term can be distinguished by the semantic mean-
ings represented by the topics in different clusters.
For clustering, we tried k-means and k-
medoids (Kaufman and Rousseeuw, 1990), and
found that k-medoids performs slightly better.
One possible reason is that k-means is more sen-
sitive to outliers. In our topic clustering, each data
point is a topic represented by its top terms (with
their probabilities normalized). The distance be-
tween two data points is measured by symmetrised
KL-Divergence.
4.2 Frequent Pattern Mining
Given topics within each cluster, this step finds
sets of terms that appear together in multiple top-
ics, i.e., shared terms among similar topics across
multiple domains. Terms in such a set are likely
to belong to the same aspect. To find such sets of
terms within each cluster, we use frequent pattern
mining (FPM) (Han et al, 2007), which is suited
for the task. The probability of each term is ig-
nored in FPM.
FPM is stated as follows: Given a set of trans-
actions T, where each transaction t
i
? T is a set
of items from a global item set I , i.e., t
i
? I . In
our context, t
i
is the topic vector comprising the
top terms of a topic (no probability attached). T
is the collection of all topics within a cluster and
I is the set of all terms in T. The goal of FPM is
to find all patterns that satisfy some user-specified
frequency threshold (also called minimum support
count), which is the minimum number of times
that a pattern should appear in T. Such patterns
are called frequent patterns. In our context, a pat-
tern is a set of terms which have appeared multiple
times in the topics within a cluster. Such patterns
compose our knowledge base as shown below.
4.3 Knowledge Representation
As the knowledge is extracted from each cluster
individually, we represent our knowledge base as
a set of clusters, where each cluster consists of a
set of frequent 2-patterns mined using FPM, e.g.,
Cluster 1: {battery, life}, {battery, hour},
{battery, long}, {charge, long}
Cluster 2: {service, support}, {support, cus-
tomer}, {service, customer}
Using two terms in a set is sufficient to cover the
semantic relationship of the terms belonging to the
same aspect. Longer patterns tend to contain more
errors since some terms in a set may not belong to
the same aspect as others. Such partial errors hurt
performance in the downstream model.
5 AKL: Using the Learned Knowledge
We now present the proposed topic model AKL,
which is able to use the automatically learned
knowledge to improve aspect extraction.
5.1 Plate Notation
Differing from most topic models based on topic-
term distribution, AKL incorporates a latent clus-
ter variable c to connect topics and terms. The
plate notation of AKL is shown in Figure 2. The
inputs of the model are M documents, T top-
ics and C clusters. Each document m has N
m
terms. We model distribution P (cluster|topic)
as ? and distribution P (term|topic, cluster) as
? with Dirichlet priors ? and ? respectively.
P (topic|document) is modeled by ? with a
Dirichlet prior ?. The terms in each document are
assumed to be generated by first sampling a topic
z, and then a cluster c given topic z, and finally
350
? ? z c w NmM
? T ? TXC? ?
Figure 2: Plate notation for AKL.
a term w given topic z and cluster c. This plate
notation of AKL and its associated generative pro-
cess are similar to those of MC-LDA (Chen et al,
2013b). However, there are three key differences.
1. Our knowledge is automatically mined which
may have errors (or noises), while the prior
knowledge for MC-LDA is manually provided
and assumed to be correct. As we will see in
Section 6, using our knowledge, MC-LDA does
not generate as coherent aspects as AKL.
2. Our knowledge is represented as clusters. Each
cluster contains a set of frequent 2-patterns
with semantically correlated terms. They are
different from must-sets used in MC-LDA.
3. Most importantly, due to the use of the new
form of knowledge, AKL?s inference mecha-
nism (Gibbs sampler) is entirely different from
that of MC-LDA (Section 5.2), which results in
superior performances (Section 6). Note that
the inference mechanism and the prior knowl-
edge cannot be reflected in the plate notation
for AKL in Figure 2.
In short, our modeling contributions are (1) the
capability of handling more expressive knowledge
in the form of clusters, (2) a novel Gibbs sampler
to deal with inappropriate knowledge.
5.2 The Gibbs Sampler
As the automatically learned prior knowledge may
contain errors for a domain, AKL has to learn
the usefulness of each piece of knowledge dy-
namically during inference. Instead of assigning
weights to each piece of knowledge as a fixed prior
in (Chen et al, 2013a), we propose a new Gibbs
sampler, which can dynamically balance the use
of prior knowledge and the information in the cor-
pus during the Gibbs sampling iterations.
We adopt a Blocked Gibbs sampler (Rosen-Zvi
et al, 2010) as it improves convergence and re-
duces autocorrelation when the variables (topic z
and cluster c in AKL) are highly related. For each
term w
i
in each document, we jointly sample a
topic z
i
and cluster c
i
(containing w
i
) based on the
conditional distribution in Gibbs sampler (will be
detailed in Equation 4). To compute this distribu-
tion, instead of considering how well z
i
matches
with w
i
only (as in LDA), we also consider two
other factors:
1. The extent c
i
corroborates w
i
given the corpus.
By ?corroborate?, we mean whether those fre-
quent 2-patterns in c
i
containing w
i
are also
supported by the actual information in the do-
main corpus to some extent (see the measure in
Equation 1 below). If c
i
corroborates w
i
well,
c
i
is likely to be useful, and thus should also
provide guidance in determining z
i
. Otherwise,
c
i
may not be a suitable piece of knowledge for
w
i
in the domain.
2. Agreement between c
i
and z
i
. By agreement
we mean the degree that the terms (union of all
frequent 2-patterns of c
i
) in cluster c
i
are re-
flected in topic z
i
. Unlike the first factor, this is
a global factor as it concerns all the terms in a
knowledge cluster.
For the first factor, we measure how well c
i
corroborates w
i
given the corpus based on co-
document frequency ratio. As shown in (Mimno
et al, 2011), co-document frequency is a good in-
dicator of term correlation in a domain. Follow-
ing (Mimno et al, 2011), we define a symmetric
co-document frequency ratio as follows:
Co-Doc(w,w
?
) =
D(w,w
?
) + 1
(D(w) +D(w
?
))?
1
2
+ 1
(1)
where (w,w
?
) refers to each frequent 2-pattern in
the knowledge cluster c
i
. D(w,w
?
) is the number
of documents that contain both termsw andw
?
and
D(w) is the number of documents containing w.
A smoothing count of 1 is added to avoid the ratio
being 0.
For the second factor, if cluster c
i
and topic z
i
agree, the intuition is that the terms in c
i
(union of
all frequent 2-patterns of c
i
) should appear as top
terms under z
i
(i.e., ranked top according to the
term probability under z
i
). We define the agree-
ment using symmetrised KL-Divergence between
the two distributions (DIST
c
and DIST
z
) cor-
responding to c
i
and z
i
respectively. As there is
no prior preference on the terms of c
i
, we use
the uniform distribution over all terms in c
i
for
DIST
c
. For DIST
z
, as only top 20 terms un-
der z
i
are usually reliable, we use these top terms
351
with their probabilities (re-normalized) to repre-
sent the topic. Note that a smoothing probability
(i.e., a very small value) is also given to every term
for calculating KL-Divergence. GivenDIST
c
and
DIST
z
, the agreement is computed with:
Agreement(c, z) =
1
KL(DIST
c
, DIST
z
)
(2)
The rationale of Equation 2 is that the lesser di-
vergence between DIST
c
and DIST
z
implies the
more agreement between c
i
and z
i
.
We further employ the Generalized Plya urn
(GPU) model (Mahmoud, 2008) which was shown
to be effective in leveraging semantically related
words (Chen et al, 2013a, Chen et al, 2013b,
Mimno et al, 2011). The GPU model here ba-
sically states that assigning topic z
i
and cluster c
i
to term w
i
will not only increase the probability
of connecting z
i
and c
i
with w
i
, but also make
it more likely to associate z
i
and c
i
with term w
?
where w
?
shares a 2-pattern with w
i
in c
i
. The
amount of probability increase is determined by
matrixA
c,w
?
,w
defined as:
A
c,w
?
,w
=
?
??
??
1, if w = w
?
?, if (w,w
?
) ? c, w 6= w
?
0, otherwise
(3)
where value 1 controls the probability increase of
w by seeingw itself, and ? controls the probability
increase of w
?
by seeing w. Please refer to (Chen
et al, 2013b) for more details.
Putting together Equations 1, 2 and 3 into a
blocked Gibbs Sampler, we can define the follow-
ing sampling distribution in Gibbs sampler so that
it provides helpful guidance in determining the
usefulness of the prior knowledge and in selecting
the semantically coherent topic.
P (z
i
= t, c
i
= c|z?i, c?i,w, ?, ?, ?,A)
?
?
(w,w
?
)?c
Co-Doc(w,w
?
)?Agreement(c, t)
?
n
?i
m,t
+ ?
?
T
t
?
=1
(n
?i
m,t
?
+ ?)
?
?
V
w
?
=1
?
V
v
?
=1
A
c,v
?
,w
?
? n
?i
t,c,v
?
+ ?
?
C
c
?
=1
(
?
V
w
?
=1
?
V
v
?
=1
A
c
?
,v
?
,w
?
? n
?i
t,c
?
,v
?
+ ?)
?
?
V
w
?
=1
A
c,w
?
,w
i
? n
?i
t,c,w
?
+ ?
?
V
v
?
=1
(
?
V
w
?
=1
A
c,w
?
,v
?
? n
?i
t,c,w
?
+ ?)
(4)
where n
?i
denotes the count excluding the current
assignment of z
i
and c
i
, i.e., z
?i
and c
?i
. n
m,t
de-
notes the number of times that topic twas assigned
to terms in document m. n
t,c
denotes the times
that cluster c occurs under topic t. n
t,c,v
refers to
the number of times that term v appears in cluster
c under topic t. ?, ? and ? are predefined Dirichlet
hyperparameters.
Note that although the above Gibbs sampler is
able to distinguish useful knowledge from wrong
knowledge, it is possible that there is no cluster
corroborates for a particular term. For every term
w, apart from its knowledge clusters, we also add
a singleton cluster for w, i.e., a cluster with one
pattern {w,w} only. When no knowledge cluster
is applicable, this singleton cluster is used. As a
singleton cluster does not contain any knowledge
information but only the word itself, Equations 1
and 2 cannot be computed. For the values of sin-
gleton clusters for these two equations, we assign
them as the averages of those values of all non-
singleton knowledge clusters.
6 Experiments
This section evaluates and compares the pro-
posed AKL model with three baseline models
LDA, MC-LDA, and GK-LDA. LDA (Blei et
al., 2003) is the most popular unsupervised topic
model. MC-LDA (Chen et al, 2013b) is a re-
cent knowledge-based model for aspect extrac-
tion. GK-LDA (Chen et al, 2013a) handles wrong
knowledge by setting prior weights using the ratio
of word probabilities. Our automatically extracted
knowledge is provided to these models. Note that
cannot-set of MC-LDA is not used in AKL.
6.1 Experimental Settings
Dataset. We created a large dataset containing
reviews from 36 product domains or types from
Amazon.com. The product domain names are
listed in Table 1. Each domain contains 1, 000 re-
views. This gives us 36 domain corpora. We have
made the dataset publically available at the web-
site of the first author.
Pre-processing. We followed (Chen et al, 2013b)
to employ standard pre-processing like lemmatiza-
tion and stopword removal. To have a fair compar-
ison, we also treat each sentence as a document as
in (Chen et al, 2013a, Chen et al, 2013b).
Parameter Settings. For all models, posterior es-
timates of latent variables were taken with a sam-
pling lag of 20 iterations in the post burn-in phase
(first 200 iterations for burn-in) with 2, 000 itera-
tions in total. The model parameters were tuned
on the development set in our pilot experiments
352
Amplifier DVD Player Kindle MP3 Player Scanner Video Player
Blu-Ray Player GPS Laptop Network Adapter Speaker Video Recorder
Camera Hard Drive Media Player Printer Subwoofer Watch
CD Player Headphone Microphone Projector Tablet Webcam
Cell Phone Home Theater System Monitor Radar Detector Telephone Wireless Router
Computer Keyboard Mouse Remote Control TV Xbox
Table 1: List of 36 domain names.
and set to ? = 1, ? = 0.1, T = 15, and ? = 0.2.
Furthermore, for each cluster, ? is set proportional
to the number of terms in it. The other param-
eters for MC-LDA and GK-LDA were set as in
their original papers. For parameters of AKL, we
used the top 15 terms for each topic in the clus-
tering phrase. The number of clusters is set to
the number of domains. We will test the sensitiv-
ity of these clustering parameters in Section 6.4.
The minimum support count for frequent pattern
mining was set empirically to min(5, 0.4?#T),
where #T is the number of transactions (i.e., the
number of topics from all domains) in a cluster.
Test Settings: We use two test settings as below:
1. (Sections 6.2, 6.3 and 6.4) Test on the same cor-
pora as those used in learning the prior knowl-
edge. This is meaningful as the learning phrase
is automatic and unsupervised (Figure 1). We
call this self-learning-and-improvement.
2. (Section 6.5) Test on new/unseen domain cor-
pora after knowledge learning.
6.2 Topic Coherence
This sub-section evaluates the topics/aspects gen-
erated by each model based on Topic Coher-
ence (Mimno et al, 2011) in test setting 1. Tra-
ditionally, topic models have been evaluated us-
ing perplexity. However, perplexity on the held-
out test set does not reflect the semantic coher-
ence of topics and may be contrary to human judg-
ments (Chang et al, 2009). Instead, the met-
ric Topic Coherence has been shown in (Mimno
-1510
-1490
-1470
-1450
-1430
0 1 2 3 4 5 6
To
pic
 Co
her
enc
e
AKL GK-LDAMC-LDA LDA
Figure 3: Average Topic Coherence of each model
at different learning iterations (Iteration 0 is equiv-
alent to LDA).
et al, 2011) to correlate well with human judg-
ments. Recently, it has become a standard prac-
tice to use Topic Coherence for evaluation of topic
models (Arora et al, 2013). A higher Topic Coher-
ence value indicates a better topic interpretability,
i.e., semantically more coherent topics.
Figure 3 shows the average Topic Coherence of
each model using knowledge learned at different
learning iterations (Figure 1). For MC-LDA or
GK-LDA, this is done by replacing AKL in lines
7 and 19 of Figure 1 with MC-LDA or GK-LDA.
Each value is the average over all 36 domains.
From Figure 3, we can observe the followings:
1. AKL performs the best with the highest Topic
Coherence values at all iterations. It is actu-
ally the best in all 36 domains. These show that
AKL finds more interpretable topics than the
baselines. Its values stabilize after iteration 3.
2. Both GK-LDA and MC-LDA perform slightly
better than LDA in iterations 1 and 2. MC-
LDA does not handle wrong knowledge. This
shows that the mined knowledge is of good
quality. Although GK-LDA uses large word
probability differences under a topic to detect
wrong lexical knowledge, it is not as effective
as AKL. The reason is that as the lexical knowl-
edge is from general dictionaries rather than
mined from relevant domain data, the words
in a wrong piece of knowledge usually have a
very large probability difference under a topic.
However, our knowledge is mined from top
words in related topics including topics from
the current domain. The words in a piece of in-
correct (or correct) knowledge often have sim-
ilar probabilities under a topic. The proposed
dynamic knowledge adjusting mechanism in
AKL is superior.
Paired t-test shows that AKL outperforms all
baselines significantly (p < 0.0001).
6.3 User Evaluation
As our objective is to discover more coherent as-
pects, we recruited two human judges. Here we
also use the test setting 1. Each topic is annotated
as coherent if the judge feels that most of its top
353
0.6
0.7
0.8
0.9
1.0
Camera Computer Headphone GPS
Pre
cisi
on
@ 5
AKL GK-LDA MC-LDA LDA
0.6
0.7
0.8
0.9
1.0
Camera Computer Headphone GPS
Pre
cisi
on
@ 10
AKL GK-LDA MC-LDA LDA
Figure 4: Average Precision@5 (Left) and Precision@10 (Right) of coherent topics from four models
in each domain. (Headphone has a lot of overlapping topics in other domains while GPS has little.)
terms coherently represent a real-world product
aspect; otherwise incoherent. For a coherent topic,
each top term is annotated as correct if it reflects
the aspect represented by the topic; otherwise in-
correct. We labeled the topics of each model
at learning iteration 1 where the same pieces of
knowledge (extracted from LDA topics at learn-
ing iteration 0) are provided to each model. After
learning iteration 1, the gap between AKL and the
baseline models tends to widen. To be consistent,
the results later in Sections 6.4 and 6.5 also show
each model at learning iteration 1. We also notice
that after a few learning iterations, the topics from
AKL model tend to have some resemblance across
domains. We found that AKL with 2 learning it-
erations achieved the best topics. Note that LDA
cannot use any prior knowledge.
We manually labeled results from four domains,
i.e., Camera, Computer, Headphone, and GPS. We
chose Headphone as it has a lot of overlapping
of topics with other domains because many elec-
tronic products use headphone. GPS was cho-
sen because it does not have much topic overlap-
ping with other domains as its aspects are mostly
about Navigation and Maps. Domains Camera and
Computer lay in between. We want to see how
domain overlapping influences the performance of
AKL. Cohen?s Kappa scores for annotator agree-
ment are 0.918 (for topics) and 0.872 (for terms).
To measure the results, we compute
Precision@n (or p@n) based on the anno-
tations, which was also used in (Chen et al,
2013b, Mukherjee and Liu, 2012).
Figure 4 shows the precision@n results for
n = 5 and 10. We can see that AKL makes im-
provements in all 4 domains. The improvement
varies in domains with the most increase in Head-
phone and the least in GPS as Headphone overlaps
more with other domains than GPS. Note that if a
domain shares aspects with many other domains,
its model should benefit more; otherwise, it is rea-
sonable to expect lesser improvements. For the
baselines, GK-LDA and MC-LDA perform simi-
larly to LDA with minor variations, all of which
are inferior to AKL. AKL?s improvements over
other models are statistically significant based on
paired t-test (p < 0.002).
In terms of the number of coherent topics, AKL
discovers one more coherent topic than LDA in
Computer and one more coherent topic than GK-
LDA and MC-LDA in Headphone. For the other
domains, the numbers of coherent topics are the
same for all models.
Table 2 shows an example aspect (battery) and
its top 10 terms produced by AKL and LDA for
each domain to give a flavor of the kind of im-
provements made by AKL. The results for GK-
LDA and MC-LDA are about the same as LDA
(see also Figure 4). Table 2 focuses on the as-
pects generated by AKL and LDA. From Table 2,
we can see that AKL discovers more correct and
meaningful aspect terms at the top. Note that
those terms marked in red and italicized are er-
rors. Apart from Table 2, many aspects are dra-
matically improved by AKL, including some com-
monly shared aspects such as Price, Screen, and
Customer Service.
6.4 Sensitivity to Clustering Parameters
This sub-section investigates the sensitivity of the
clustering parameters of AKL (again in test setting
1). The top sub-figure in Figure 5 shows the aver-
age Topic Coherence values versus the top k terms
per topic used in topic clustering (Section 4.1).
The number of clusters is set to the number of
domains (see below). We can observe that using
k = 15 top terms gives the highest value. This is
intuitive as too few (or too many) top terms may
generate insufficient (or noisy) knowledge.
The bottom sub-figure in Figure 5 shows the
average Topic Coherence given different number
354
Camera Computer Headphone GPS
AKL LDA AKL LDA AKL LDA AKL LDA
battery battery battery battery hour long battery trip
life card hour cable long battery hour battery
hour memory life speaker battery hour long hour
long life long dvi life comfortable model mile
charge usb speaker sound charge easy life long
extra hour sound hour amp uncomfortable charge life
minute minute charge connection uncomfortable headset trip destination
charger sd dvi life comfortable life purchase phone
short extra tv hdmus period money older charge
aa device hdmus tv output hard compass mode
Table 2: Example aspect Battery from AKL and LDA in each domain. Errors are italicized in red.
-1510
-1490
-1470
-1450
-1430
5 10 15 20 25 30T
opi
c C
ohe
ren
ce
#Top Terms for Clustering
-1510
-1490
-1470
-1450
-1430
20 30 40 50 60 70T
opi
c C
ohe
ren
ce
#Clusters
Figure 5: Average topic coherence of AKL versus
#top k terms (Top) and #clusters (Bottom).
-1490
-1480
-1470
-1460
-1450
AKL GK-LDA MC-LDA LDA
To
pic
 Co
her
enc
e
Figure 6: Average topic coherence of each model
tested on new/unseen domain.
of clusters. We fix the number of top terms per
topic to 15 as it yields the best result (see the top
sub-figure in Figure 5). We can see that the per-
formance is not very sensitive to the number of
clusters. The model performs similarly for 30 to
50 clusters, with lower Topic Coherence for less
than 30 or more than 50 clusters. The significance
test indicates that using 30, 40, and 50 clusters,
AKL achieved significant improvements over all
baseline models (p < 0.0001). With more do-
mains, we should expect a larger number of clus-
ters. However, it is difficult to obtain the optimal
number of clusters. Thus, we empirically set the
number of clusters to the number of domains in
our experiments. Note that the number of clus-
ters (C) is expected to be larger than the number
of topics in one domain (T ) because C is for all
domains while T is for one particular domain.
6.5 Test on New Domains
We now evaluate AKL in test setting 2, i.e., the au-
tomatically extracted knowledge K (Figure 1) is
applied in new/unseen domains other than those in
domainsDL used in knowledge learning. The aim
is to see how K can help modeling in an unseen
domain. In this set of experiments, each domain
is tested by using the learned knowledge from the
rest 35 domains. Figure 6 shows the average Topic
Coherence of each model. The values are also av-
eraged over the 36 tested domains. We can see that
AKL achieves the highest Topic Coherence value
while LDA has the lowest. The improvements of
AKL over all baseline models are significant with
p < 0.0001.
7 Conclusions
This paper proposed an advanced aspect extraction
framework which can learn knowledge automati-
cally from a large number of review corpora and
exploit the learned knowledge in extracting more
coherent aspects. It first proposed a technique to
learn knowledge automatically by clustering and
FPM. Then a new topic model with an advanced
inference mechanism was proposed to exploit the
learned knowledge in a fault-tolerant manner. Ex-
perimental results using review corpora from 36
domains showed that the proposed method outper-
forms state-of-the-art methods significantly.
Acknowledgments
This work was supported in part by a grant from
National Science Foundation (NSF) under grant
no. IIS-1111092.
355
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet Forest priors. In Proceedings
of ICML, pages 25?32.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent Dirich-
let alocation using first-order logic. In Proceedings
of IJCAI, pages 1171?1177.
Sanjeev Arora, Rong Ge, Yonatan Halpern, David
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A Practical Algorithm for
Topic Modeling with Provable Guarantees. In Pro-
ceedings of ICML, pages 280?288.
David M. Blei and Jon D McAuliffe. 2007. Supervised
Topic Models. In Proceedings of NIPS, pages 121?
128.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
S R K Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning Document-Level
Semantic Properties from Free-Text Annotations. In
Proceedings of ACL, pages 263?271.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812.
Giuseppe Carenini, Raymond T Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of K-CAP, pages 11?18.
Jonathan Chang, Jordan Boyd-Graber, Wang Chong,
Sean Gerrish, and David Blei, M. 2009. Reading
Tea Leaves: How Humans Interpret Topic Models.
In Proceedings of NIPS, pages 288?296.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013a. Discovering Coherent Topics Using General
Knowledge. In Proceedings of CIKM, pages 209?
218.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013b. Exploiting Domain Knowledge in Aspect
Extraction. In Proceedings of EMNLP, pages 1655?
1667.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013c. Leveraging Multi-Domain Prior Knowledge
in Topic Models. In Proceedings of IJCAI, pages
2071?2077.
Yejin Choi and Claire Cardie. 2010. Hierarchical Se-
quential Learning for Extracting Opinions and their
Attributes. In Proceedings of ACL, pages 269?274.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A Latent Variable Model
for Geographic Lexical Variation. In Proceedings of
EMNLP, pages 1277?1287.
Lei Fang and Minlie Huang. 2012. Fine Granular As-
pect Analysis using Latent Structural Models. In
Proceedings of ACL, pages 333?337.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
and Zhong Su. 2009. Product feature categoriza-
tion with multilevel latent semantic association. In
Proceedings of CIKM, pages 1087?1096.
Jiawei Han, Hong Cheng, Dong Xin, and Xifeng Yan.
2007. Frequent pattern mining: current status and
future directions. Data Mining and Knowledge Dis-
covery, 15(1):55?86.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Au-
tomatically Extracting Polarity-Bearing Topics for
Cross-Domain Sentiment Classification. In Pro-
ceedings of ACL, pages 123?131.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Analysis. In Proceedings of UAI, pages 289?296.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of KDD,
pages 168?177.
Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-
noff. 2011. Interactive Topic Modeling. In Pro-
ceedings of ACL, pages 248?257.
Jagadeesh Jagarlamudi, Hal Daum?e III, and Raghaven-
dra Udupa. 2012. Incorporating Lexical Priors into
Topic Models. In Proceedings of EACL, pages 204?
213.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
Opinion Targets in a Single- and Cross-Domain Set-
ting with Conditional Random Fields. In Proceed-
ings of EMNLP, pages 1035?1045.
Yohan Jo and Alice H. Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM, pages 815?824.
Jeon-hyung Kang, Jun Ma, and Yan Liu. 2012. Trans-
fer Topic Modeling with Ease and Scalability. In
Proceedings of SDM, pages 564?575.
L Kaufman and P J Rousseeuw. 1990. Finding groups
in data: an introduction to cluster analysis. John
Wiley and Sons.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A Hierarchical Aspect-Sentiment
Model for Online Reviews. In Proceedings of AAAI,
pages 526?533.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting Aspect-Evaluation and Aspect-of
Relations in Opinion Mining. In Proceedings of
EMNLP, pages 1065?1074.
356
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion Extraction, Summarization and
Tracking in News and Blog Corpora. In Proceed-
ings of AAAI-CAAW, pages 100?107.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A Bayesian Model for Joint
Unsupervised Induction of Sentiment, Aspect and
Discourse Representations. In Proceedings of ACL,
pages 1630?1639.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-Aware Review Mining and Summariza-
tion. In Proceedings of COLING, pages 653?661.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011.
Generating Aspect-oriented Multi-Document Sum-
marization with Event-aspect model. In Proceed-
ings of EMNLP, pages 1137?1146.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM, pages 375?384.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic
Patterns versus Word Alignment: Extracting Opin-
ion Targets from Online Reviews. In Proceedings of
ACL, pages 1754?1763.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Yue Lu and Chengxiang Zhai. 2008. Opinion inte-
gration through semi-supervised topic modeling. In
Proceedings of WWW, pages 121?130.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In Proceedings of WWW, pages 131?140.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K Tsou.
2011. Multi-aspect Sentiment Analysis with Topic
Models. In Proceedings of ICDM Workshops, pages
81?88.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of CIKM, pages 1642?1646.
Hosam Mahmoud. 2008. Polya Urn Models. Chap-
man & Hall/CRC Texts in Statistical Science.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW, pages 171?180.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of EMNLP, pages 262?272.
Samaneh Moghaddam and Martin Ester. 2013. The
FLDA Model for Aspect-based Opinion Mining:
Addressing the Cold Start Problem. In Proceedings
of WWW, pages 909?918.
Arjun Mukherjee and Bing Liu. 2012. Aspect Extrac-
tion through Semi-Supervised Modeling. In Pro-
ceedings of ACL, pages 339?348.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on
Transfer Learning. IEEE Trans. Knowl. Data Eng.,
22(10):1345?1359.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
James Petterson, Alex Smola, Tib?erio Caetano, Wray
Buntine, and Shravan Narayanamurthy. 2010. Word
Features for Latent Dirichlet Allocation. In Pro-
ceedings of NIPS, pages 1921?1929.
AM Popescu and Oren Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of HLT, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion Word Expansion and Target Extrac-
tion through Double Propagation. Computational
Linguistics, 37(1):9?27.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: a su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP, pages
248?256.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas
Griffiths, Padhraic Smyth, and Mark Steyvers.
2010. Learning author-topic models from text cor-
pora. ACM Transactions on Information Systems,
28(1):1?38.
Christina Sauper and Regina Barzilay. 2013. Auto-
matic Aggregation by Joint Modeling of Aspects and
Values. J. Artif. Intell. Res. (JAIR), 46:89?127.
Swapna Somasundaran and J Wiebe. 2009. Recog-
nizing stances in online debates. In Proceedings of
ACL, pages 226?234.
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL, pages 308?316.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a
rating regression approach. In Proceedings of KDD,
pages 783?792.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of EMNLP, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Mining Opinion Words and Opinion
Targets in a Two-Stage Framework. In Proceedings
of ACL, pages 1764?1773.
GR Xue, Wenyuan Dai, Q Yang, and Y Yu. 2008.
Topic-bridged PLSA for cross-domain text classifi-
cation. In Proceedings of SIGIR, pages 627?634.
357
Bishan Yang and Claire Cardie. 2013. Joint Inference
for Fine-grained Opinion Extraction. In Proceed-
ings of ACL, pages 1640?1649.
Shuang Hong Yang, Steven P. Crain, and Hongyuan
Zha. 2011. Bridging the language gap: Topic adap-
tation for documents with different technicality. In
Proceedings of AISTATS, pages 823?831.
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect Ranking: Identifying
Important Product Aspects from Online Consumer
Reviews. In Proceedings of ACL, pages 1496?1505.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Constrained LDA for grouping product features in
opinion mining. In Proceedings of PAKDD, pages
448?459.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly Modeling Aspects and Opin-
ions with a MaxEnt-LDA Hybrid. In Proceedings of
EMNLP, pages 56?65.
Yanyan Zhao, Bing Qin, and Ting Liu. 2012. Col-
location polarity disambiguation using web-based
pseudo contexts. In Proceedings of EMNLP-
CoNLL, pages 160?170.
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2013.
Collective Opinion Target Extraction in Chinese Mi-
croblogs. In Proceedings of EMNLP, pages 1840?
1850.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of CIKM, pages 43?50.
358
