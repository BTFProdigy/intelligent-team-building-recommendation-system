Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88?95,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
 Clustering Technique in Multi-Document Personal Name Disambigu-
ation 
 
 
Chen Chen 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China 
chenchen@pku.edu.cn 
Hu Junfeng 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China
hujf@pku.edu.cn 
Wang Houfeng 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China
wanghf@pku.edu.cn 
 
  
Abstract 
 
Focusing on multi-document personal name 
disambiguation, this paper develops an agglo-
merative clustering approach to resolving this 
problem. We start from an analysis of point-
wise mutual information between feature and 
the ambiguous name, which brings about a 
novel weight computing method for feature in 
clustering. Then a trade-off measure between 
within-cluster compactness and among-cluster 
separation is proposed for stopping clustering. 
After that, we apply a labeling method to find 
representative feature for each cluster.  Finally, 
experiments are conducted on word-based 
clustering in Chinese dataset and the result 
shows a good effect. 
1 Introduction 
Multi-document named entity co-reference reso-
lution is the process of determining whether an 
identical name occurring in different texts refers 
to the same entity in the real world. With the rap-
id development of multi-document applications 
like multi-document summarization and informa-
tion fusion, there is an increasing need for multi-
document named entity co-reference resolution. 
This paper focuses on multi-document personal 
name disambiguation, which seeks to determine 
if the same name from different documents refers 
to the same person. 
This paper develops an agglomerative cluster-
ing approach to resolving multi-document per-
sonal name disambiguation. In order to represent 
texts better, a novel weight computing method 
for clustering features is presented. It is based on 
the pointwise mutual information between the 
ambiguous name and features. This paper also 
develops a trade-off point based cluster-stopping 
measure and a labeling algorithm for each clus-
ters. Finally, experiments are conducted on 
word-based clustering in Chinese dataset. The 
dataset contains eleven different personal names 
with varying-sized datasets, and has 1669 texts in 
all. 
The rest of this paper is organized as follows: 
in Section 2 we review the related work; Section 
3 describes the framework; section 4 introduces 
our methodologies including feature weight 
computing with pointwise mutual information, 
cluster-stopping measure based on trade-off 
point, and cluster labeling algorithm. These are 
the main contribution of this paper; Section 5 
discusses our experimental result. Finally, the 
conclusion and suggestions for further extension 
of the work are given in Section 6. 
2 Related Work 
Due to the varying ambiguity of personal names 
in a corpus, existing approaches typically cast it 
as an unsupervised clustering problem based on 
vector space model. The main difference among 
these approaches lies in the features, which are 
used to create a similarity space. Bagga & Bald-
win (1998) first performed within-document co-
reference resolution, and then explored features 
in local context. Mann & Yarowsky (2003) ex-
tracted local biographical information as features. 
Al-Kamha and Embley (2004) clustered search 
results with feature set including attributes, links 
and page similarities. Chen and Martin (2007) 
explored the use of a range of syntactic and se-
mantic features in unsupervised clustering of 
documents. Song (2007) learned the PLSA and 
LDA model as feature sets. Ono et al (2008) 
used mixture features including co-occurrences 
88
of named entities, key compound words, and top-
ic information. Previous works usually focus on 
feature identification and feature selection. The 
method to assign appropriate weight to each fea-
ture has not been discussed widely.  
A major challenge in clustering analysis is de-
termining the number of ?clusters?. Therefore, 
clustering based approaches to this problem still 
require estimating the number of clusters. In Hie-
rarchy clustering, it equates to determine the 
stopping step of clustering. The measure to find 
the ?knee? in the criterion function curve is a 
well known cluster-stopping measure. Pedersen 
and Kulkarni had studied this problem (Pedersen 
and Kulkarni, 2006). They developed cluster-
stopping measures named PK1, PK2, PK3, and 
presented the Adapted Gap Statistics.  
After estimating the number of ?clusters?, we 
obtain the clustering result. In order to label the 
?clusters?, the method that finding representative 
features for each ?cluster? is needed. For example, 
the captain John Smith can be labeled as captain. 
Pedersen and Kulkarni (2006) selected the top N 
non-stopping word features from texts grouped 
in a cluster as label. 
3 Framework 
On the assumption of ?one person per document? 
(i.e. all mentions of an ambiguous personal name 
in one document refer to the same personal enti-
ty), the task of disambiguating personal name in 
text set intends to partition the set into subsets, 
where each subset refer to one particular entity. 
Suppose the set of texts containing the ambi-
guous name is denoted by D= {d1,d2,?,dn}, and  
di (0<i<n+1) stands for one text. The entities 
with the ambiguous name are denoted by a set 
E= {e1,e2,?,em}, where the number of entities ?m? 
is unknown. The ambiguous name in each text di 
indicates only one entity ek. The aim of the work 
is to map an ambiguous name appearing in each 
text to an entity. Therefore, those texts indicating 
the same entity need to be clustered together. 
In determining whether a personal name refers 
to a specific entity, the personal information, so-
cial network information and related topics play 
important roles,  all of which are expressed by 
words in texts,. Extracting words as features, this 
paper applies an agglomerative clustering ap-
proach to resolving name co-reference. The 
framework of our approach consists of the fol-
lowing seven main steps: 
 
Step 1: Pre-process each text with Chinese 
word segmentation tool; 
Step 2: Extract words as features from the 
set of texts D;. 
Step 3: Represent texts d1,?,dn by features 
vectors; 
Step 4: Calculate similarity between texts; 
Step 5: Cluster the set D step by step until 
only one cluster exists;  
Step 6: Estimate the number of entities in 
accordance with cluster-stopping 
measure; 
Step 7: Assign each cluster a discriminating 
label. 
 
This paper focuses on the Step 4, Step 6 and 
Step 7, i.e., feature weight computing method, 
clustering stopping measure and cluster labeling 
method. They will be described in the next sec-
tion in detail.  
Step1 and Step3 are simple, and there is no 
further description here. In Step 2, we use co-
occurrence words of the ambiguous name in 
texts as features. In the process of agglomerative 
clustering (see Step 5), each text is viewed as one 
cluster at first, and the most similar two clusters 
are merged together as a new cluster at each 
round. After replacing the former two clusters 
with the new one, we use average linked method 
to update similarity between clusters. 
4 Methodology  
4.1 Feature weight  
Each text is represented as a feature vector, and 
each item of the vector represents the weight 
value for corresponding feature in the text. Since 
our approach is completely unsupervised we 
cannot use supervised methods to select 
significant features. Since the weight of feature 
will be adjusted well instead of feature selection, 
all words in set D are used as feature in our 
approach. 
The problem of computing feature weight is 
involved in both text clustering and text classifi-
cation. By comparing the supervised text classi-
fication and unsupervised text clustering, we find 
that the former one has a better performance ow-
ing to the selection of features and the computing 
method of feature weight. Firstly, in the applica-
tion of supervised text classification, features can 
be selected by many methods, such as, Mutual 
Information (MI) and Expected Cross Entropy 
(ECE) feature selection methods. Secondly, 
model training methods, such as SVM model, are 
generally adopted by programs when to find the 
89
optimal feature weight. There is no training data 
for unsupervised tasks, so above-mentioned me-
thods are unsuitable for text clustering. 
In addition, we find that the text clustering for 
personal name disambiguation is different from 
common text clustering. System can easily judge 
whether a text contains the ambiguous personal 
name or not. Thus the whole collection of texts 
can be easily divided into two classes: texts  with 
or without the name. As a result, we can easily 
calculate the pointwise mutual information 
between feature words and the personal name. 
To a certain extent, it represents the correlative 
degree between feature words and the underlying 
entity corresponding to the personal name. 
For these reasons, our feature weight 
computing method calculates the pointwise 
mutual information between personal name and 
feature word. And the value of pointwise mutual 
information will be used to expresse feature 
word?s weight by combining the feature?s tf (the 
abbreviation for term-frequency) in text and idf 
(the abbreviation for inverse document frequency) 
in dataset. The formula of feature weight compu-
ting proposed in this paper is as below, and it is 
need both texts containing and not containing the 
ambiguous personal name to form dataset D. For 
each tk in di that contains name, its mi_weight is 
computed as follow: 
))(||log()),MI(1log(
))),(log(1(),,_weight(mi
kk
ikik
tdfDnamet
dttfdnamet
?+?
+=
 
(1) 
And 
)()(
||),(
||/)()(
||/),(
)()(
),(
),MI(
2
k
k
k
k
k
k
k
tdfnamedf
Dtnamedf
Dtdfnamedf
Dtnamedf
tpnamep
tnamep
namet
?
?=
?=
?=
     (2) 
 Where tk is a feature; name is the ambiguous 
name; di is the ith text in dataset; tf(tk,di) 
represents term frequency of feature tk in text di; 
df(tk), df(name) is the number of the texts con-
taining tk or name in dataset D respectively; 
df(tk,name) is the number of texts containing both 
tk and name; |D| is the number of all the texts.  
Formula (2) can be comprehended as: if word 
tk occurs much more times in texts containing the 
ambiguous name than in texts not containing the 
name, it must have some information about the 
name. 
 A widely used approach for computing feature 
weight is tf*idf scheme as formula (3) (Salton 
and Buckley. 1998), which only uses the texts 
containing the ambiguous name. We denote it by 
old_weight . For each tk in di containing name, 
the old_weight is computed as follow: 
)),()(log(
))),(log(1(
),,(old_weight
nametdfnamedf
dttf
dnamet
k
ik
ik
?
+=               (3) 
The first term on the right side is tf, and the 
second term is idf. If the idf scheme is computed 
in the whole dataset D for reducing noise, the 
weight computing formula can be expressed as 
follow, and is denoted by imp_weight: 
))(|D|log())),(log(1(
),_weight(imp
kik
ik
tdfdttf
dt
?+= (4) 
Before clustering, the similarity between texts 
is computed by cosine value of the angle 
between vectors (such as dx, dy in formula (5)):     
yx
yx
yx dd
dd
d,d ?
?=)cos(                               (5) 
Each item of the vector (i.e. dx, dy) represents 
the weight value for corresponding feature in the 
text. 
4.2 Cluster-stopping measure 
The process of clustering will produce n cluster 
results, one for each step. Independent of 
clustering algorithm, the cluster stopping meas-
ure should choose the cluster results which can 
represent the structure of data. 
A fundamental and difficult problem in cluster 
analysis is to measure the structure of clustering 
result. The geometric structure is a representative 
method. It defines that a ?good? clustering re-
sults should make data points from one cluster 
?compact?, while data points from different clus-
ter are ?separate? as far as possible. The indica-
tors should quantify the ?compactness? and ?se-
paration? for clusters, and combine both.  In the 
study of cluster stopping measures by Pedersen 
and Kulkarni (2006), the criterion functions de-
fines text similarity based on cosine value of the 
angle between vectors. Their cluster-stopping 
measures focused on finding the ?knee? of crite-
rion function.  
Our cluster-stopping measure is also based on 
the geometric structure of dataset. The measure 
aims to find the trade-off point between within-
cluster compactness and among-cluster 
separation. Both the within-cluster compactness 
(Internal critical function) and among-cluster 
90
separation (External critical function) are defined 
by Euclidean distance. The hybrid critical 
function (Hybrid critical function) combines 
internal and external criterion functions. 
Suppose that the given dataset contains N ref-
erences, which are denoted as: d1,d2,?,dN; the 
data have been repeatedly clustered into k clus-
ters, where k=N,?,1; and clusters are denoted as 
Cr, r=1,?k; and the number of references in 
each cluster is nr, so nr=|Cr|. We introduce Incrf 
(Internal critical function), Excrf (External 
critical function) and Hycrf (Hybrid critical 
function) to measure it as follows. 
 
??
= ?
?=
k
i
k
1
)Incrf(
iyx Cd,d
2
yx dd                  (6) 
? ? ?
= ?= ??
?=
k
i
k
ijj jinn
k
1 ,1
1
)Excrf(
jyix Cd,Cd
2
yx dd
            (7) 
))Excrf()(Incrf(
M
1
)Hycrf( kkk +?=          (8) 
Where M=Incrf(1)=Excrf(N) 
 
 
 
Figure 1 Hycrf vs. t (N-k) 
 
Chen proved the existence of the minimum 
value between (0,1) in Hycrf(k) (see Chen et al 
2008). The Hycrf value in a typical Hycrf(t) 
curve is shown as Figure 1, where t=N-k. 
Function Hycrf based on Incrf and Excrf is 
used as the Hybrid criterion function. The Hycrf 
curve will rise sharply after the minimum, indi-
cating that the cluster of several optimal parti-
tions? subsets will lead to drastic drop in cluster 
quality. Thus cluster partition can be determined. 
Using the attributes of the Hycrf(k) curve, we put 
forward a new cluster-stopping measure named 
trade-off point based cluster-stopping measure 
(TO_CSM). 
)1Hycrf(
)Hycrf(
)1Hycrf(
1
)TO_CSM( +?+= k
k
k
k  
(9) 
 
Trade-off point based cluster-stopping meas-
ure (TO_CSM) selects the k value which max-
imizes TO_CSM(k), and indicates the number of 
cluster. The first term on the right side of formu-
la (9) is used to minimize the value of Hycrf(k), 
and the second one is used to find the ?knee? ris-
ing sharply. 
4.3 Labeling 
Once the clusters are created, we label each 
entity to represent the underlying entity with 
some important information. A label is 
represented as a list of feature words, which 
summarize the information about cluster?s 
underlying entity. 
The algorithm is outlined as follows: after 
clustering N references into m clusters, for each 
cluster Ck in {C1, C2, ?, Cm}, we calculate the 
score of each feature for Ck and choose features 
as the label of Ck whose scores rank top N. In 
particular, the score caculated in this paper is 
different from Pedersen and Kulkarni?s (2006). 
We combine pointwise mutual information 
computing method with term frequency in cluster 
to compute the score.  
The formula of feature scoring for labeling is 
shown as follows: 
 
))),(log(1(
),(MI),MI(),Score( name
ik
ikkik
Cttf
CtnametCt
+?
?=
 
(10) 
The calculation of MI(tk,name) is shown as 
formula (2) in subsection 4.1. tf(tk,Ci) represents 
the total occurrence frequency of feature tk in 
cluster Ci . The MIname(tk,Ci) is computed as for-
mula (11): 
)()(
||),(
||/)()(
||/),(
)()(
),(
)C,(MI
2
ik
ik
ik
ik
ik
ik
ikname
Cdftdf
DCtdf
DCdftdf
DCtdf
Cptp
Ctp
t
?
?=
?=
?=
 
 (11) 
In formula (10), the weight of stopping words 
can be reduced by the first item. The second item 
can increase the weight of words with high dis-
tinguishing ability for a certain ambiguous name. 
The third item of formula (10) gives higher 
scores to features whose frequency are higher.  
0
0.5
1
1.5
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99 10
6
11
3
12
0
Hycrf(t)
91
5 Experiment  
5.1 Data 
The dataset is from WWW, and contains 1,669 
texts with eleven real ambiguous personal names. 
Such raw texts containing ambiguous names are 
collected via search engine1, and most of them 
are news. The eleven person-names are, "??? 
Liu-Yi-si ?Lewis?", "??? Liu-Shu-zhen ", "?
? Li-Qiang", "?? Li-Na", "??? Li-Gui-
ying", "??? Mi-xie-er ?Michelle?", "?? 
Ma-Li ?Mary?", "???  Yue-han-xun ?John-
son?", "?? Wang-Tao", "?? Wang-Gang", "
??? Chen-Zhi-qiang". Names like ?Michelle?, 
?Johnson? are transliterated from English to Chi-
nese, while names like ?Liu ?Shu-zhen?, ?Chen-
Zhi-qiang? are original Chinese personal names. 
Some of these names only have a few persons, 
while others have more persons.  
Table 1 shows our data set. ?#text? presents 
the number of texts with the personal name. 
?#per? presents the number of entities with the 
personal name in text dataset. ?#max? presents 
the maximum of texts for an entity with the per-
sonal name, and ?#min? presents the minimum. 
 
 #text #per #max #min
Lewis 120 6 25 10 
Liu-Shu-zhen 149 15 28 3 
Li-Qiang 122 7 25 9 
Li-Na 149 5 39 21 
Li-Gui-ying 150 7 30 10 
Michelle 144 7 25 12 
Mary 127 7 35 10 
Johnson 279 19 26 1 
Wang-Gang 125 18 26 1 
Wang-Tao 182 10 38 5 
Chen-Zhi-qiang 122 4 52 13 
 
Table 1 Statistics of the test dataset 
 
We first convert all the downloaded docu-
ments into plain text format to facilitate the test 
process, and pre-process them by using the seg-
mentation toolkit ICTCLAS2. 
In testing and evaluating, we adopt B-Cubed 
definition for Precision, Recall and F-Measure 
as indicators (Bagga, Amit and Baldwin. 1998). 
F-Measure is the harmonic mean of Precision 
and Recall. 
The definitions are presented as below: 
                                                 
1 April.2008 
2 http://ictclas.org/ 
? ?= Dd dprecisionNprecision 1              (12) 
? ?= Dd drecallNrecall 1                           (13) 
recallprecision
recallprecision
measureF +
??=? 2      (14) 
where precisiond is the precision for a text d. 
Suppose the text d is in subset A, precisiond is 
the percentage of texts in A which indicates the 
same entity as d. Recalld is the recall ratio for a 
text d. Recalld is the ratio of number of texts 
which indicates the same entity as d in A to that 
in corpus D. n = | D |, D refers to a collection of 
texts containing a particular name (such as Wang 
Tao, e.g. a set of 200 texts, n = 200). Subset A is 
a set formed after clustering (text included in 
class), and d refers to a certain text that contain-
ing "Wang Tao". 
5.2 Result 
All the 1669 texts in the dataset are employed 
during experiment. Each personal name disam-
biguation process only clusters the texts contain-
ing the ambiguous name. After pre-processing, in 
order to verify the mi_weight method for feature 
weight computing, all the words in texts are used 
as features.   
Using formula (1), (3) and (4) as feature 
weight computing formula, we can get the evalu-
ation of cluster result shown as table 2. In this 
step, cluster-stopping measure is not used. In-
stead, the highest F-measure during clustering is 
highlighted to represent the efficiency of the fea-
ture weight computing method.  
Further more, we carry out the experiment on 
the trade-off point based cluster-stopping 
measure, and compare its cluster result with 
highest F-measure and cluster result determined 
by cluster-stopping measure PK3 proposed by 
Pedersen and Kulkarni?s. Based on the 
experiment in Table 2, a structure tree is 
constructed in the clustering process. Cluster-
stopping measures are used to determine where 
to stop cutting the dendrogram. As shown in 
Table 3, the TO-CMS method predicts the 
optimal results of four names in eleven, while 
PK3 method predicts the optimal result of one 
name, which are marked in a bold type. 
 
92
 
 old_weight imp_weight mi_weight 
#pre #rec #F #pre #rec #F #pre #rec #F 
Lewis 0.9488 0.8668. 0.9059 1 1 1 1 1 1 
Liu-Shu-zhen 0.8004 0.7381 0.7680 0.8409 0.8004 0.8201 0.9217 0.7940 0.8531
Li-Qiang 0.8057 0.6886 0.7426 0.9412 0.7968 0.8630 0.8962 0.8208 0.8569
Li-Na 0.9487 0.7719 0.8512 0.9870 0.8865 0.9340 0.9870 0.9870 0.9870
Li-Gui-ying 0.8871 0.9124 0.8996 0.9879 0.8938 0.9385 0.9778 0.8813 0.9271
Michelle 0.9769 0.7205 0.8293 0.9549 0.8146 0.8792 0.9672 0.9498 0.9584
Mary 0.9520 0.6828 0.7953 1 0.9290 0.9632 1 0.9001 0.9474
Johnson 0.9620 0.8120 0.8807 0.9573 0.8083 0.8765 0.9593 0.8595 0.9067
Wang-Gang 0.8130 0.8171 0.8150 0.7804 0.9326 0.8498 0.8143 0.9185 0.8633
Wang-Tao 1 0.9323 0.9650 0.9573 0.9485 0.9529 0.9897 0.9768 0.9832
Chen-Zhi-qiang 0.9732 0.8401 0.9017 0.9891 0.9403 0.9641 0.9891 0.9564 0.9725
Average 0.9153 0.7916 0.8504 0.9451 0.8864 0.9128 0.9548 0.9131 0.9323
 
Table 2 comparison of feature weight computing method (highest F-measure)
 
 Optimal TO-CMS PK3 
#pre #rec #F #pre #rec #F #pre #rec #F 
Lewis 1 1 1 1 1 1 0.8575 1 0.9233
Liu-Shuzhen 0.9217 0.7940 0.8531 0.8466 0.8433 0.8450 0.5451 0.9503 0.6928
Li-Qiang 0.8962 0.8208 0.8569 0.8962 0.8208 0.8569 0.7897 0.9335 0.8556
Li-Na 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9016 0.9424
Li-Gui-ying 0.9778 0.8813 0.9271 0.9778 0.8813 0.9271 0.8750 0.9427 0.9076
Michelle 0.9672 0.9498 0.9584 0.9482 0.9498 0.9490 0.9672 0.9498 0.9584
Mary 1 0.9001 0.9474 0.8545 0.9410 0.8957 0.8698 0.9410 0.9040
Johnson 0.9593 0.8595 0.9067 0.9524 0.8648 0.9066 0.2423 0.9802 0.3885
Wang-Gang 0.8143 0.9185 0.8633 0.9255 0.7102 0.8036 0.5198 0.9550 0.6732
Wang-Tao 0.9897 0.9768 0.9832 0.8594 0.9767 0.9144 0.9700 0.9768 0.9734
Chen-Zhi-qiang 0.9891 0.9564 0.9725 0.8498 1 0.9188 0.8499 1 0.9188
Average 0.9548 0.9131 0.9323 0.9179 0.9068 0.9095 0.7703 0.9574 0.8307
 
Table 3 comparison of cluster-stopping measures? performance
name Entity Created Labels 
Lewis Person-1 ???(Babbitt),???????(Sinclair Lewis),?????(Arrow smith),?
??(Literature Prize),???(Dresser),????(Howells),?????
(Swedish Academy),???????(Sherwood Anderson),???????
(Elmer  Gan Hartley),??(street),??(award),????????(American 
Literature and Arts Association) 
Person-2 ????(Bank of America),????(Bank of America),??(bank),???
(investors),???(credit card),??(Bank of China),??(Citibank),??
(mergers and acquisitions),??(Construction Bank),???(executive officer),
???(banking),??(stock),?????(Ken Lewis) 
Person-3 ??(Single),???(Liana),??(album),???(Liana),???????(Liana 
Lewis),???(Liana),??(airborne),??(sales),???(Music Awards),??
????(Maria Kelly),?(List),??(debut)? 
Person-4 ??????(Carl Lewis),??(long jump),??(Carl),???(Owens),??
(track and field),???(Burrell),?????(the U.S. Olympic Committee),?
?(sprint),???(Taylors),?????(Belgrade),??????(Verde Exxon),
???(Exxon) 
93
Person-5 ??(Tyson),??(King of Boxer),??(knock down),???(heavyweight),?
?(Don King),??(boxing),??(belt),??(Boxing),?(fist),??(bout),??
(Ring),WBC 
Person-6 ???(Daniel),?????(Day Lewis),??(Blood),?????????(Daniel 
Day Lewis),??(There Will Be Blood),??(left crus),??(movie king),??
?????(New York Film Critics Circles),???(the Gold Oscar statues),?
??(Best Actor in a Leading Role),???(Oscar),????(There Will Be 
Blood) 
 
Table 4  Labels for ?Lewis? clusters 
 
On the basis of text clustering result that 
obtained from the Trade-off based cluster-
stopping measure experiment in Table 3, we try 
our labelling method mentioned in subsection 4.3. 
For each cluster, we choose 12 words with 
highest score as its label. The experiment result 
demonstrates that the created label is able to 
represent the category. Take name ???? Liu-
Yi-si ?Lewis?? for example, the labeling result 
shown as Table 4.  
 
5.3 Discussion  
From the test result in table 2, we find that our 
feature weight computing method can improve 
the Chinese personal name clustering disambigu-
ation performance effectively. For each personal 
name in test dataset, the performance is im-
proved obviously. The average value of optimal 
F-measures for eleven names rises from 85.04% 
to 91.28% by using the whole dataset D for cal-
culated idf, and rises from 91.28% to 93.23% by 
using mi_weight. Therefore, in the application of 
Chinese text clustering with constraints, we can 
compute pointwise mutual information between 
constraints and feature, and it can be merged 
with feature weight value to improve the cluster-
ing performance.  
We can see from table 3 that trade-off point 
based cluster-stopping measure (TO_CSM) per-
forms much better than PK3. According to the 
experimental results, PK3 measure is not that 
robust. The optimal number of clusters can be 
determined for certain data. However, we found 
that it did not apply to all cases. For example, it 
obtains the optimal estimation result for data 
?Michelle?, as for ?Liu Shuzhen?, ?Wang Gang? 
and ?Johnson?, the results are extremely bad. 
The better result is achieved by using TO_CSM 
measure, and the selected results are closer to the 
optimal value. The PK3 measure uses the mean 
and the standard deviation to deduce, and its 
processes are more complicated than TO_CSM?s.  
Our cluster labeling method computes the fea-
tures? score with formula (10). From the labeling 
results sample shown in Table 4, we can see that 
all of the labels are representative. Most of them 
are person and organizations? name, and the rest 
are key compound words. Therefore, when the 
clustering performance is good, the quality of 
cluster labels created by our method is also good. 
6 Future Work 
This paper developed a clustering algorithm of 
multi-document personal name disambiguation, 
and put forward a novel feature weight compu-
ting method for vector space model. This method 
computes weight with the pointwise mutual in-
formation between the personal name and feature. 
We also study a hybrid criterion function based 
on trade-off point and put forward the trade-off 
point cluster-stopping measure. At last, we expe-
riment on our score computing method for clus-
ter labeling.  
Unsupervised personal name disambiguation 
techniques can be extended to address the prob-
lem of unsupervised Entity Resolution and unsu-
pervised word sense discrimination. We will at-
tempt to apply the feature weight computing me-
thod to these fields. 
One of the main directions of our future work 
will be how to improve the performance of per-
sonal name disambiguation. Computing weight 
based on a window around names may be helpful. 
Moreover, word-based text features haven?t 
solved two difficult problems of natural language 
problems: Synonym and Polysemy, which se-
riously affect the precision and efficiency of 
clustering algorithms. Text representation based 
on concept and topic may solve the problem.  
 
Acknowledgments 
This research is supported by National Natural 
Science Foundation of Chinese (No.60675035) 
and Beijing Natural Science Foundation 
(No.4072012) 
94
References  
Al-Kamha. R. and D. W. Embley. 2004. Grouping 
search-engine returned citations for person-name 
queries. In Proceedings of WIDM?04, 96-103, 
Washington, DC, USA. 
Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space 
model. In Proceedings of 17th International Con-
ference on Computational Linguistics, 79?85. 
Bagga, Amit and B. Baldwin. 1998. Algorithms for 
scoring co-reference chains. In Proceedings of the 
First International Conference on Language Re-
sources and Evaluation Workshop on Linguistic 
co-reference. 
Chen Ying and James Martin. 2007. Towards Robust 
Unsupervised Personal Name Disambiguation, 
EMNLP 2007. 
Chen Lifei, Jiang Qingshan, and Wang Shengrui. 
2008. A Hierarchical Method for Determining the 
Number of Clusters. Journal of Software, 19(1). [in 
Chinese] 
Chung Heong Gooi and James Allan. 2004. Cross-
document co-reference on a large scale corpus. In S. 
Dumais, D. Marcu, and S. Roukos, editors, HLT-
NAACL 2004: Main Proceedings, 9?16, Boston, 
Massachusetts, USA, May 2 - May 7 2004. Asso-
ciation for Computational Linguistics. 
Gao Huixian. Applied Multivariate Statistical Analy-
sis. Peking Univ. Press. 2004. 
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing and Management, 
Kulkarni Anagha and Ted Pedersen. 2006. How Many 
Different ?John Smiths?, and Who are They? In 
Proceedings of the Student Abstract and Poster 
Session of the 21st National Conference on Artifi-
cial Intelligence, Boston, Massachusetts. 
Mann G. and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In W. Daelemans and 
M. Osborne, editors, Proceedings of CoNLL-2003, 
33?40, Edmonton, Canada. 
Niu Cheng, Wei Li, and Rohini K. Srihari. 2004.  
Weakly Supervised Learning for Cross-document 
Person Name Disambiguation Supported by Infor-
mation Extraction. In Proceedings of ACL 2004. 
Ono. Shingo, Issei Sato, Minoru Yoshida, and Hiroshi 
Nakagawa2. 2008. Person Name Disambiguation 
in Web Pages Using Social Network, Compound 
Words and Latent Topics. T. Washio et al (Eds.): 
PAKDD 2008, LNAI 5012, 260?271. 
Song Yang, Jian Huang, Isaac G. Councill, Jia Li, and 
C. Lee Giles. 2007. Efficient Topic-based Unsu-
pervised Name Disambiguation. JCDL?07, June 
18?23, 2007, Vancouver, British Columbia, Cana-
da. 
Ted Pedersen and Kulkarni Anagha. 2006. Automatic 
Cluster Stopping with Criterion Functions and the 
Gap Statistic. In Proceedings of the Demonstration 
Session of the Human Language Technology Con-
ference and the Sixth Annual Meeting of the North 
American Chapter of the Association for Computa-
tional Linguistic, New York City, NY. 
 
95
 From Text to Exhibitions: A New Approach for E-Learning on Language and 
Literature based on Text Mining 
Qiaozhu Mei 
Department of Electrical Engineering and 
Computer Science 
Vanderbilt University  
Box 1679 Station B  
Nashville, TN 37235 USA 
qiaozhu.mei@vanderbilt.edu 
Junfeng Hu 
Department of Computer Science  
Institute of Computational Linguistics 
Peking University 
100871, Beijing, China 
hujf@pku.edu.cn 
 
Abstract 
Unlike many well established approaches for 
E-Learning on science fields, there isn?t a 
commonly accepted approach of E-Learning 
on humanities fields, especially language and 
literature. Because the knowledge on language 
and literature depends too much on texts, 
advanced text processing has become a 
bottleneck for E-Learning on these domains. 
In traditional learning frameworks learners 
would easily get boring with mass pure texts. 
This article introduces a new approach for E-
Learning on language and literature, by 
intelligently extracting real or virtual objects 
from texts and integrating them as exhibitions 
in a digital museum system. This article also 
discussed how to generate exhibitions from 
texts with computational linguistics methods 
as well as how this E-Learning framework 
pushes the research of computational 
linguistics. The discussion of E-Learning by 
Digital Museum is based on the design of 
Digital Museum of Chinese Ancient Poetry, 
by Peking University. 
1 Introduction 
Computer based Education has become a very 
hot and productive topic in recent years. However, 
most of the existing methodology and models are 
based on science domain. This is because the 
teaching and learning on science domain relies 
much on the ability of reasoning and computation, 
which directly utilizes the advantage of computer. 
The most important carriers of Knowledge on 
humanities domain, especially literature and 
language are textual materials. Therefore, unlike E-
Learning on science and technical fields, a more 
intelligent way of using computer to deal with texts 
is required. Traditional E-Learning models on 
language and literature rely too much on pure text. 
Relevant frameworks include Digital-Archives, 
Digital-Libraries and Digital publications. Most of 
them are just ?gathering mass text materials and 
providing them online?, thus the interface between 
system and learners is onefold, non-interactive and 
lack of guidance. Learners easily get missed in 
excessive bald texts without a ?docent [2]? to 
advise them how to select a well organized 
knowledge structure and a learning pathway. 
Searching and retrieving modules are provided in 
those models to various extents, which provide a 
knowledge retriever. However, it still cannot go 
beyond texts. 
Recently, Digital Museum systems are believed 
to be able to provide a vivid interface which carries 
educational uses to participants. Teaching and 
learning becomes much easier from the special 
circumstance of learning in the presence of real 
objects, which inspires curiosity and creative 
thinking, and gives museums the potential to 
develop distinctive and meaningful educational 
experiences [5].  
There are many good examples that approach E-
learning on humanities fields with a system similar 
to a Digital Museum. The National Palace Museum 
system in Taiwan offers 14 courses on the cultural 
relics of China [3]. Digital Museums on more than 
10 major fields in nature and culture have been 
designed along with Taiwan?s nation wide Digital 
Museum plan. Lo, Feng-ju et? al have designed a 
digital museum of Chinese Ancient Literature, 
which provides some sub-exhibitions of poetry and 
fictions in formats of photocopy of the actual paper 
edition of ancient texts.[7] These works have been 
well exploring the primitive application of Digital 
Museum in E-Learning on Humanities Fields. 
To satisfy the needs of E-Learning on Language 
and Literature fields, a modern digital museum 
should have some specific features. It should 
provide a mechanism to process texts, which 
would be able to integrate some computational 
linguistics methods. It should also provide a way to 
organize knowledge beyond the texts, and be able 
to provide guidance for learning. This can be 
achieved by generating objects out from texts and 
organizing them into interactive exhibitions that 
can be personalized. Moreover, the digital museum 
framework should be reusable to different scope of 
background knowledge. Such a modern digital 
museum associating text processing mechanism is 
believed to be a sound approach of E-Learning on 
Language and Literature.  
This article discussed this approach on the 
Digital Museum framework design, how it is 
associated with Computational Linguistics, and 
how to integrate knowledge to maximize the E-
Learning efficiency. These discussions will be 
based on an example of the Digital Museum of 
Chinese Ancient Poetry Art, by Peking University 
2003. [10] The following section will discuss the 
general framework design of digital museum. We 
will discuss text processing work behind the 
Digital Museum in Section 3, and Knowledge 
Processing and integration in Section 4. Some 
more discussion and future work will be provided 
at the conclusion section.  
2 The Digital Museum Framework 
Instead of digital library and traditional digital 
museum systems, which provide single function of 
exhibition, a modern digital museum provides 
multidimensional functions. Generally, a modern 
digital museum has three key functions, exhibition, 
education and research. In our design of Digital 
Museum for Language and Literature, the three 
dimansion would be: interacting theme based 
exhibitions from texts, E-Learning modules on 
language and literature, and related research on 
Computational Linguistics. 
2.1 Digital Museum and E-Learning on 
Language and Literature 
Digital Museum systems have gone beyond 
exhibitions of digital collections. Instead, they 
would increasingly emphasize educational uses 
rather than traditional exhibitions. It provides users 
with educational and well-motivated exhibitions 
[13]. UK-wide Digital Museum linked exhibitions 
connected by subject and theme with an integrated 
learning environment [6]. By 2000, the National 
Science Plan of Digital Museums of Taiwan has 
defined a specific and integrated program on how 
to utilize scientific technology, especially 
information technology, and how to digitalize the 
archives in both cultural and natural fields, with 
significant humanistic meaning. It has conducted 
further discussions on how to apply these kinds of 
digital projects and productions to education, 
research and industrialization, for the sake of 
conserving culture, promoting education, inspiring 
research and increment of industrialization. [3]. 
Knowledge on a learning topic should be 
organized  as an exhibition theme, which is 
represented by a series of real or virtual objects 
and detailed descriptions. Exhibitions of various 
themes are linked together corresponding to the 
relativity of their themes. Learners can participate 
in the Digital Museum by choosing a pathway of 
linked exhibitions with a typical topic. Special 
modules will also be provided for participants to 
interacting with the system, which will be 
discussed in section 4.  
2.2 General Architecture Design of a Digital 
Museum 
The life cycle of a modern digital museum looks 
like a fountain model  [11]. There are feedbacks 
from each design phase to previous phases. There 
are several milestones in the life cycle, each of 
which acts as a knowledge container and a 
foundation of knowledge processing on upper 
levels. [14]. These knowledge containers are as 
follows: 
Milestones Functionality 
 
Information Origin Pool: 
(Primitive Corpus) 
The mass storage of large-scale 
information from preliminary 
digitalization work. 
 
Refined Knowledge Bases 
(Refined Corpus) 
Database storage of useful and 
relevant knowledge from 
knowledge refining.  
 
Metadata for Exhibitions 
Metadata describing ontology, 
with all detailed metadata for 
knowledge flows, items and 
relations 
 
Integrated Exhibition Base 
 
Database for Exhibiting items, 
individual or integrated, for regular 
accessing by system.  
 
Reusable Tool Base for 
Functional Modules 
Tool pool for reusable module 
functions, individual or integrated 
components for various use. 
 
Multi-functional  Interface 
Web-based interface for 
exhibitions, education and 
research.  
Table 1: Milestones within the Digital Museum 
Architecture 
 
Based on these milestones, the general 
architecture of a Digital Museum on Language and 
Literature can be represented in the following 
figure:
 
 
Figure1: General Architecture of a Digital 
Museum based on language processing 
2.3 Example: Introduction to the Digital 
Museum of Chinese Ancient Poetry 
The Digital Museum of Chinese Ancient Poetry 
Art [10] is a research model by Peking University, 
Beijing, combining E-Learning, computer assisted 
research on Chinese Ancient Poetry and 
computational linguistics. A prototype of this 
Digital Museum was designed in order to meet the 
needs of exhibition, education and research on the 
art of Chinese Ancient Poetry. The analysis, design 
and implementation of this project were on a 
highly abstract level. 
2.3.1 Corpus, Design and Prototype System 
The information origin pool and the refined 
knowledge base of this project were also the 
corpus for related computational linguistics 
research. It involves Chinese Ancient Poetry across 
2,000 years, approximately 100,000 items [10]. 
Other advanced knowledge bases such as Author 
Information base, Image and media base, Location 
information base and Word lists were constructed.  
In the design of this Digital Museum system, 
knowledge mining was divided into two types, 
item entity information mining and relational 
information mining. Item entity information was 
detailed to exhibiting items, characters, images, 
media, locations and words. Relational information 
reflected all aspects of relations among items. 
Metadata for each category of instances was 
defined in the design phase. Particularly, a group 
of items with relating meaning was structured as a 
virtual item class, which was also treated as a 
specific item.  
In the prototype system, items of poetry, 
character, location and others were exhibited along 
with all related formats of knowledge. Users can 
leap from one item to its related items, and learn 
them in the context where they originally belongs. 
Sample exhibitions on specific themes, such as 
clothing, plants, food and spring were also 
designed. 
2.3.2 E-Learning and Related research from 
this Digital Museum 
In the dimension of learning, Digital Museum of 
Chinese Ancient Poetry explored the study of E-
Leaning system for the language and literature 
features of Chinese Ancient Poetry. It enabled a 
way to learn a poem in its background environment, 
with reference to its related poetry and other 
related objects in multiple formats. The system 
also presented statistical research results of the 
corpus to users, such as the words usages of 
authors, the cooccurrence of words, the likelihood 
of the hidden meanings of words, which help users 
to be well-informed and easier to understand in 
learning a poem or a word.  
In the dimension of research, the digital museum 
is closely related to specific research topics on 
computational linguistics, especially statistical  
natural language processing. We refined unknown 
words from the corpus though statistic methods 
and explored to cluster them into concepts. In this 
way, we studied the hidden meanings of words and 
poetry in context and studied the relation discovery 
among poems. We also conducted some research 
of knowledge mining and discovering from corpus, 
which can also inspire extended researches like 
Computer Assisted archaeology on Chinese 
Ancient Poetry. 
3 Language Processing behind the Digital 
Museum Framework 
Knowledge of humanities areas, especially 
language and literature, is commonly carried by 
texts. Therefore, the language processing, 
specifically the text processing will be vital for 
transforming pure texts and domain knowledge 
into abstracted exhibitions. Actually, most digital 
museums today haven't made good use of 
computational linguistics techniques. Most of them 
remain on organizing exhibitions manually and 
providing them online. Those exhibitions are 
relatively isolated from each other.  
However, there are remarkable relations among 
text units and real objects and topics, which are 
hidden in the texts. For example, the word 
?willow? seems having nothing to do with ?getting 
apart? by the semantic definitions, but in the 
context, ?breaking a willow branch? does indicate 
?send-off friends?, or ?seeing a friend leaving? in 
Chinese Ancient Poetry.  
These meaningful entities and relations can be 
learned from the statistical analysis of large scale 
poetry texts. The use of computational linguistics 
methods here is crucial, which distinguishes it with 
traditional Digital Museum models. Statistical 
natural language processing over large scale corpus 
is the most significant approach we have adopted 
in this research.  
3.1 Construction of Corpuses and Integrated 
Knowledge bases 
The first phase of language processing is to build 
corpora and knowledge bases. Primitive corpora 
are constructed by archive digitalization. Refined 
corpora are constructed by applying language 
processors on the primitive corpus. We can use 
Digital Museum of Chinese Ancient Poetry for 
example.  
For the Digital Museum of Chinese Ancient 
Poetry Art,  the primitive corpora include texts of 
poems over 1, 200, 000 lines, descriptions of 4000 
authors, a name dictionary and a location 
dictionary. The refined corpora include a words 
dictionary which is thoroughly discovered from the 
texts, a concept base constructed by supervised 
word clustering and a storage of words 
cooccurances. Other knowledge bases include 
images, music, medias(reading), relics, events, and 
a series of expertise knowledge on Chinese 
Ancoent Poetry.  
The general ontology of domain knowledge was 
carefully studied. Important entities and relations 
from texts and related domains were determined. 
Consequently, we carefully designed the metadata 
and chose a database system to maintain the 
knowledge base. This knowledge base should be 
expandable so that  it can contain texts, entities 
from related domains, and relations.  
The last step of this phase is to design an 
referencing mechanism to query and get the 
answer. The outcome of this phase is an integrated 
knowledge base, the textual part of which is the 
corpus for mining and knowledge discovery. 
3.2 Text Mining: Extracting Objects from 
Texts 
As soon as the corpora and knowledge bases are 
constructed, higher level methods of natural 
language processing are applied to mine in the 
corpus. The goal is to find objects abstracted from 
texts, which are organized by individual topics. 
Statistical natural language processing plays a very 
important role in this procedure, which can be 
described in the following three levels.  
3.2.1 Extracting Direct Relevant Objects from 
Texts. 
Textual knowledge is not ?dead? in the fields of 
language and literature. It is interacting with 
knowledge in other forms, by other carrier or on 
other abstract level.  Taking Chinese ancient poetry 
for example, a poem is associated to its author, its 
era and its writing background. The textual body of 
a poem also refers to certain persons, events, 
locations, plants, scenes, feelings and other entities, 
either real or virtual. In addition, there are various 
sources of objects relevant to the poem, such as 
paintings, calligraphy works, music and cultural 
relics, etc. All these entities above are so important 
to the synopsis of the poem that it is an advisable 
way to learn the poem with the appearance of these 
objects. Furthermore, relying on these directly 
relevant objects makes teaching and learning much 
more open and exciting than barely focusing on 
texts.  
In the early phase of Digital Museum design, an 
integrated exhibition base is built, in which directly 
relevant entities of the texts are refined, stored in 
relational or XML databases and associated with 
the body of texts. 
3.2.2 Discovering Hidden Entities and Relations 
Associated with Language Units. 
As the Computer assisted research develops on 
these fields, we can work on the hidden knowledge 
of texts by means of text mining and retrieval.  As 
language technology evolves, a computational age 
of language has arrived [1].  We can conduct 
computer assisted analytical research on language, 
with both linguistic and statistical approaches. In 
the research on the language of Chinese ancient 
poetry, we studied the statistical concurrences and 
meaningful units in the texts, extracted words from 
collocations and clustered words into meaningful 
concepts. In further research, we explored ways to 
study the hidden meanings of the words and 
collocations, especially those related to emotions 
of human. Consequently, expected to learn 
emotional characteristic of a poem, associating 
words, concepts and other units it refers with the 
similar characteristic.  
On the other hand, language and texts are the 
most important carriers of cultural fragments. 
Many interesting knowledge patterns are hidden in 
the texts.  There is a considerable proportion of 
Chinese ancient history and culture buried in the 
texts of Chinese ancient poetry, which evolutes 
along more than 2,000 years and involves locations 
all over China. By language techniques, fragments 
of culture can be mined from the texts, refined and 
stored, and finally integrated into interacting 
virtual scenes.   
By this we can discover hidden entities and 
relations associated with text and expand it to 
analytical meaningful segments.  
3.2.3 Expanding Indirect Relations. 
In our framework, knowledge entities are not 
living alone but interacting. Both textual entities 
and other objects are associated to its relevant 
entity set. There are two kinds of relations 
identifying that two entities are interacting, direct 
relation, which have already been discussed above, 
and indirect relation.  For instance, a poem refers 
to various knowledge objects, thus poems referring 
to the same objects are indirectly interacting with 
each other. These poems are involved in their 
relevant entity set, with ?identical reference? as an 
indirect relation.  In a more intelligent level, poems 
with the similar hidden meanings or relevant 
emotions are arranged together as a set. This set 
can be associated with a topic, a subject, a scene or 
a specific semantic cluster.  
In these three approaches to expand textual 
knowledge into relevant objects, a former purely 
textual entity has been developed as involving in 
the surrounding of various relevant objects, real or 
virtual. Thus we complete the procedure of 
extracting objects for exhibitions from texts. An 
example from poems to objects is as follows: 
 Figure2: Expanding Objects Set from a Poem 
Text. 
 
3.3 Theme Driven Knowledge Discovery 
From the statistical analysis on character 
concurrences, we applied various methods to 
discover unknown words from the texts. Chinese 
language is different from other language because 
there isn?t natural interval from a word to another. 
We consider all words to be unknown in the 
beginning and generate a word dictionary from the 
filtering by mutual information value, m-test and 
other statistical methods.  
Upon the word dictionary, we conducted words 
clustering by the distance of words concurrence 
vectors. This procedure has abstracted concepts 
from words. After supervised filtering, these 
concepts will indicate some hidden semantic 
meanings.  
The consecutive knowledge discovery work will 
be theme driven. First, a theme, or a learning topic 
is decided, some features and key concepts of this 
theme will be decided with the expert knowledge. 
Using statistical methods, we can find the concepts 
and words which are semantically similar or in 
some way related to this theme. Then, directly and 
indirectly related objects (discussed in section 3.2) 
will be associated with the topic. Then, reluctant 
units are eliminated. We will filter the most 
significant entities and relations, which can be 
represented by combinations of both concepts and 
words, and organize them around the theme. In this 
way, we can put the topic/theme back to its ancient 
living environment.  
Further works includes rebuilding ancient 
scenarios where the topic belongs, and mining for 
relations among topics.  
4 Knowledge Processing and Integration of 
the Digital Museum  
Knowledge processing plays a very significant 
role in the Digital Museum framework. It is 
involved as a clue throughout the life cycle of the 
digital museum. The entire design and 
implementing of the digital museum is focusing on 
language processing, knowledge discovery and 
exhibition integrating. The knowledge processing 
procedures can be represented in the following 
figure: 
 Figure3: Knowledge Processing in this digital 
museum. 
 
4.1 Knowledge Processing Hierarchy  
An intelligent platform of knowledge deals with 
knowledge in five primary hierarchies, namely, 
knowledge citation, knowledge application, 
knowledge transmitting, knowledge learning and 
knowledge developing [8]. This division of 
knowledge hierarchies remarkably adapts the 
needs of an E-Learning program. In the study of 
this article, we make a little modification to this 
division and applied it to the Digital Museum 
system as follows:  
Knowledge Citation 
Knowledge Applying 
Knowledge Learning 
Learning and Teaching  
Knowledge Mining 
Knowledge Representing 
Knowledge Representing to Users  
Information Interacting 
Knowledge Developing 
Table 1: A knowledge processing hierachy in the 
Digital Museum 
 
Poem 
Persons Locations 
Relics 
Events 
Other 
Words Concepts 
Emotions 
Cultural 
 
Fragments 
Scenes 
Relevant Entity Sets? 
Poems, Topics, Scenes, 
Texts, Concepts, Themes, 
Words, Other entities? 
Texts 
Images Medias 
Virtual 
Realities 
 
Actually, this division is somewhat relative and 
not absolute. For instance, in some activities 
defined as knowledge representation and 
knowledge developing, we may also need to do 
knowledge citation and applying. However, this 
division of knowledge hierarchy would help to 
define the functions of Knowledge Platform and 
content the needs for knowledge by systems and 
users. [8] 
The Digital Museum presents multidimensions 
according to the three functions of exhibition, 
education and research. The processing targets, 
procedures and emphases on Knowledge vary 
among dimensions.  
In the dimension of exhibition, system focuses 
on Knowledge citation and Knowledge 
representing in the hierarchy above.  
In the dimension of e-learning, system focuses 
on the hierarchy of Knowledge applying, learning 
and teaching, Knowledge Representing and 
information interaction.  
In the dimension of computational linguistics 
research, system emphasizes the hierarchy of 
Knowledge Mining and Knowledge developing.  
4.2 Two Types of Integration for Knowledge 
Objects 
After discussing the generating of objects from 
the texts, we would be interested in how to 
integrate them for E-Learning.  
Relating and interacting objects are extracted 
from texts and stored in the exhibition base. The 
next phase is to arrange exhibitions by selecting, 
dividing and integrating these objects, and 
construct the digital museum interface.  
There are two key forms of objects integration, 
tutored and theme-oriented exhibitions and virtual 
scenarios.  
In the first form, tutored theme-oriented 
exhibition, objects relevant to a specific subject or 
theme are integrated and represented in multi-
modals. This interface design provides a dynamic 
exhibition module by grouping texts and their 
relevant objects in various formats together, 
providing docent knowledge for this topic and 
links to relevant topic exhibitions. Learners 
participate in one exhibition and go through links 
fitting to their needs or under instructions, thus 
personalized learning paths are formed.   
There are two tips in tutored theme-oriented 
exhibitions. One is ?multi-modal?. Personalized 
exhibitions in our framework enable learning 
through multi channels, in forms of texts, image, 
music and virtual reality, etc. Also taking Chinese 
ancient poetry for example, we first discover the 
relevant scenes and hidden emotions of a poem, 
select objects referring to similar scenes and 
emotions, provide them as background materials 
and then integrate them with the poem.  A more 
detailed instance is the Auto-matching poems and 
paintings. The other is ?interactive?. In our 
framework, a learner can add his remarks or 
discuss in every exhibition topic. These remarks 
are processed and stored as new relevant objects to 
this topic. Users can also provide materials or 
background information to an object or a topic, and 
can provide their own exhibition plans of new 
organizations of objects. The system studies the 
feedbacks and provides users with personalized 
participation paths.  
The second integration form is scenarios. 
Knowledge objects were recorded in texts from 
their original living environments. By collecting 
and extracting relevant objects from texts and 
analytical researching on their relevant 
environmental elements such as emotions, we are 
able to put a textual object back to a scene 
representing its original living environment by 
rebuilding these origin scenes. Teaching and 
learning are made easier and more exciting with 
participating in the original scenes that a topic 
really lived. With the technology of multimedia 
and virtual reality, we are able to integrate objects 
and environmental elements surrounding a specific 
topic and rebuild a virtual scene, which is 
represented in our framework as multimedia 
demonstration, tests and games.  
These two key integrating patterns organize 
various formats of objects and represent these 
integrated exhibitions to users in an interactive and 
personalized way. It maximizes the educational use 
of a digital museum on language and literature 
fields.  
 Figure3:Integrating exhibits in the Digital 
Museum on Chinese Ancient Poetry. 
5 Conclusion 
Computer-based education on language and 
literature has both its advantage and difficulty. On 
one hand it provides learners with abundant 
relating materials, on the other hand it?s tedious 
and difficult for learners to acquire knowledge in 
the sea of information. The approach of extracting 
objects from texts, and integrating them to build an 
interactive and vivid exhibitions enables learners 
both to explore in broad scope of knowledge and to 
enjoy exciting and comprehensible learning. 
Computer techniques are adopted in the framework 
of Digital Museums to maximize its educational 
use. How to make use of the methods from 
computational linguistics, especially statistical 
methods is the bottleneck or the key to success of 
this e-learning approach. On the other hand, the 
needs of e-learning and the abstracting of digital 
exhibitions from texts have very positive effect on 
pushing the research of computational linguistics. 
Significant techniques include unknown word 
discovery, clustering and other issues in text 
mining. Besides the conituous work on text mining, 
future research will focus on how to personalize 
the learning paths of learners, and enable in-time 
processing of user feedbacks. Investigations and 
evaluations will be made both on the e-learning 
system and the efficiency of text mining 
techniques over typical kinds of texts, like Chinese 
ancient poetry. 
6 Acknowledgements 
The authors would thank people in Institute of 
Computational Linguistics, Peking University, who 
gave great help for this research. We will 
especially thanks Miz. Feng-ju Lo, who has given 
us great help ever since the research starts. 
References  
1. Martin A. Nowak, Natalia L. Komarova, Partha 
Niyogi, Computational and Evolutionary 
Aspects of Language, Nature, VOL417, 6 June 
2002 
2. W.Rayward, M. Twidale, From Docent to 
Cyberdocent: education and Guidance in the 
Virtual Museum, Archives and Museum 
Informatics 13, 1999, p23-p53.  
3. Ching-Chun Hsieh, Ying-Chun Hsieh et al 
?Samples of Digital Archive in Taiwan National 
Digital Archive Program?, 2003 
4. Shun-tzu Tsai, Chun-ko Hsieh, Diversity and 
Aesthetic Appeal for a Virtual Reality World of 
Chinese Art, proceeding of the Seventh 
International Conference on Virtual System and 
Multimedia,  2001 
5. ?The Learning Power of Museums?A Vision 
for Museum Education? Published by 
Department for Culture, Media and Sport, 
United Kingdom, 2000  
6. Louise Smith, ?Building the Digital Museum: A 
National Resource for the Learning Age.? joint 
report of The National Museums Directors? 
Conference, Resource and mda, UK, 10 August 
2000 
7. Feng-ju Lo, et al Ancient Literature Museum: 
Design of an E-learning System for non-
Chinese Major, the 4th International Workshop 
on Computer, Multimedia and Education of 
Language, Taiwan, 2000 
8. Chuanzhong Li, Jingzhong Zhang, ?Idea of 
Intelligent Knowledge Platform and a 
Rudimental Prototype?, Research and 
Development on the World Science & 
Technology, Volume 23 Issue 6, 2001 
9. Junfeng Hu, Shiwen Yu, Word meaning 
Similarity analysis in Chinese Ancient Poetry, 
ICL Technical Report, Peking University, 2001 
10.Qiaozhu Mei, ?A Digital Museum of Ancient 
Chinese Poetry Art: It?s Design, Realization and 
Related Researches on Computational 
Linguistics?, Thesis for Bachelor?s Degree in 
Peking University, 2003.6 
11.Krish Pillai, ?The Fountain Model and Its 
Impact on Project Schedule?, ACM SIGSOFT 
Software Engineering Notes, Volume 21 Issue 
2, March 1996 
12.Nikos Kladias, Tassos Pantazidis, Manolis 
Avagianos, A Virtual Reality Learning 
Environment Providing Access to Digital 
Museums, 1998 MultiMedia Modeling October, 
1998, p193 
13. Jen-Shin Hong, Bai-Hsuen Chen, Jieh Hsiang, 
Tien-Yu Hsu, ?Content Management for Digital 
Museum Exhibitions?, Proceeding of JCDL 
2001, pp.450, June 24-28, 2001 
14.Qiaozhu Mei, A Knowledge Processing 
Oriented Life Cycle Study from a Digital 
Museum System., The 42nd ACM Southeast 
Conference, Huntsville, 2004 
 
 
 
 
 
 
