Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 100?103,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Transliteration System using pair HMM with weighted FSTs 
 
 
Peter Nabende 
Alfa Informatica, CLCG, 
University of Groningen, Netherlands 
p.nabende@rug.nl 
 
  
Abstract 
This paper presents a transliteration system 
based on pair Hidden Markov Model (pair 
HMM) training and Weighted Finite State 
Transducer (WFST) techniques. Parameters 
used by WFSTs for transliteration generation 
are learned from a pair HMM. Parameters 
from pair-HMM training on English-Russian 
data sets are found to give better transliteration 
quality than parameters trained for WFSTs for 
corresponding structures. Training a pair 
HMM on English vowel bigrams and standard 
bigrams for Cyrillic Romanization, and using 
a few transformation rules on generated Rus-
sian transliterations to test for context im-
proves the system?s transliteration quality. 
1 Introduction 
Machine transliteration is the automatic trans-
formation of a word in a source language to a 
phonetically equivalent word in a target language 
that uses a different writing system. Translitera-
tion is important for various Natural Language 
Processing (NLP) applications including: Cross 
Lingual Information Retrieval (CLIR), and Ma-
chine Translation (MT). This paper introduces a 
system that utilizes parameters learned for a pair 
Hidden Markov Model (pair HMM) in a shared 
transliteration generation task1. The pair HMM 
has been used before (Mackay and Kondrak, 
2005; Wieling et al, 2007) for string similarity 
estimation, and is based on the notion of string 
Edit Distance (ED). String ED is defined here as 
the total edit cost incurred in transforming a 
source language string (S) to a target language 
string (T) through a sequence of edit operations. 
The edit operations include: (M)atching an ele-
ment in S with an element in T; (I)nserting an 
element into T, and (D)eleting an element in S. 
                                                 
1
 The generation task is part of the NEWS 2009 machine 
transliteration shared task  (Li et al, 2009) 
Based on all representative symbols used for 
each of the two languages, emission costs for 
each of the edit operations and transition parame-
ters can be estimated and used in measuring the 
similarity between two strings. To generate 
transliterations using pair HMM parameters, 
WFST (Graehl, 1997) techniques are adopted. 
Transliteration training is based mainly on the 
initial orthographic representation and no explicit 
phonetic scheme is used. Instead, transliteration 
quality is tested for different bigram combina-
tions including all English vowel bigram combi-
nations and n-gram combinations specified for 
Cyrillic Romanization by the US Board on Geo-
graphic Names and British Permanent Commit-
tee on Geographic Names (BGN/PCGN). How-
ever, transliteration parameters can still be esti-
mated for a pair HMM when a particular phonet-
ic representation scheme is used. 
The quality of transliterations generated using 
pair HMM parameters is evaluated against trans-
literations generated from training WFSTs and 
transliterations generated using a Phrase-based 
Statistical Machine Translation (PBSMT) sys-
tem. Section 2 describes the components of the 
transliteration system that uses pair HMM para-
meters; section 3 gives the experimental set up 
and results associated with the transliterations 
generated; and section 4 concludes the paper. 
2 Machine Transliteration System 
The transliteration system comprises of a training 
and generation components (Figure 1). In the 
training component, the Baum-Welch Expecta-
tion Maximization (EM) algorithm (Baum et al, 
1970) is used to learn the parameters of a pair 
HMM. In the generation component, WFST 
techniques (Graehl, 1997) model the learned pair 
HMM parameters for generating transliterations.  
2.1 Parameter Estimation for a pair-HMM 
A pair HMM has two output observations (Fig-
ure 2) that are aligned through the hidden states,  
100
  
 
 
 
 
 
 
 
 
 
Figure 1: Machine Transliteration system 
 
 
 
 
 
 
 
 
Figure 2: pair-HMM alignment for converting an 
English string ?Peter? to a Russian string ?????? 
 
unlike the classic HMMs that have only one ob-
servation sequence. The pair HMM structure dif-
fers from that of WFSTs in that in WFSTs the 
input and output symbols and associated weights 
occur on a transition arc while for the pair HMM, 
the input and output symbols and associated edit 
costs are encoded in a node. Two main sets of 
parameters are learned for the pair HMM: transi-
tion parameters (?, ?, ?, ?M, ?DI) as shown in Fig-
ure 3 for different state transitions; and emission 
parameters in the (M)atch state and the other two 
gap states (D and I). si in Figure 3 is the ith sym-
bol in the source language string S while tj is the 
jth symbol in T.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Pair Hidden Markov Model [Adapted from 
Mackay and Kondrak, 2005] 
Pair HMM Emission parameters are stored in 
matrix form in three tables associated with the 
edit operations; transition parameters are also 
stored in matrix form in a table. The emission 
parameters are ( )n m n m? + +  in total; n and m 
are the numbers of symbols in the pair HMM 
source language alphabet (VS) and target lan-
guage alphabet (VT) respectively. The parameters 
of starting in a given edit operation state are de-
rived from the parameters of transiting from the 
match state (M) to either D or I or back to M. 
Although pair HMM training is evaluated 
against WFST training, there is no major differ-
ence in the training approach used in both cases; 
a forward-backward EM algorithm is used in 
each case. The main difference is in the struc-
ture; for the pair-HMM, the state transition pa-
rameter is also incorporated into the weight that 
measures the level of relationship between the 
input and output symbol when transformed to a 
WFST arc. 
2.2 Generating Transliterations in WFSTs 
A Weighted Finite State Transducer is a finite 
automaton whose state transitions are labeled 
with input and output elements and weights that 
express the level of relationship between the in-
put and output elements. Although the frame-
work of WFSTs has mostly been applied in 
representing various models for speech recogni-
tion (Mohri et al, 2008) including HMMs, 
WFSTs have as well been used for transliteration  
(Knight and Graehl, 1998), and are the most suit-
able for modeling pair HMM constraints for ge-
nerating transliterations. In the WFST frame-
work, it is possible to specify various configura-
tions associated with constraints inherent in a 
particular model. Figure 4 shows a WFST that 
precisely corresponds to the structure of the pair  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4: Finite State Transducer corresponding to the 
pair HMM. 
M start end 
D 
I 
si:e 
e:tj 
e:tj 
e:tj 
si:e 
si:e 
e:e
 
e:e
 
e:e
 
e:e
 
e:e
 
e:e
 
si:tj 
si:tj si:tj 
P : ? 
 
M 
e : ? 
 
M 
t : ? 
 
M 
e : _ 
D 
r : ? 
M End 
si 
tj 
si 
tj 
? 
M end 
D 
I 
?DI 
?DI 
? 
?M 
? 
? 
? 
? 
1- ?- ?- ?DI 
1- ?- ?- ?DI 
1-2?- ?M 
 
Pairs of correct 
transliterations 
Transliteration parame-
ter estimation for pair 
HMM 
Estimated 
parameters 
Transliteration gen-
eration using 
Weighted Finite State 
Transducers 
Source 
name 
Target 
name 
101
HMM considering the constraints specified for 
the pair HMM. In Figure 4, e is an empty symbol 
while si and sj are as defined for the pair HMM in 
Figure 3. Note that, in Figure 4, a start state is 
needed to model pair HMM parameter con-
straints for starting in any of the three edit states. 
However, it is possible to specify a WFST cor-
responding to the pair HMM with no start state. 
Various WFST configurations that do not con-
form to the bias corresponding to the pair HMM 
constraints had low transliteration quality and for 
space limitations, are not reported in this paper.  
2.3 Transformation Rules 
A look into the transliterations generated using 
pair HMM parameters on English-Russian de-
velopment data showed consistent mistranslitera-
tions mainly due to lack of contextual modeling 
in the generated transliterations. For example in 
all cases where the Russian character ? ?l? pre-
cedes the Russian soft sign ? ? ' ?, the Russian 
soft sign was missing, resulting into a loss of 
transliteration accuracy. Two examples of mi-
stransliterations that do not include the Russian 
soft sign ? are: ??????? instead of ???????? 
?krefeld?, and ?????? instead of ??????? 
?bilbao?. For such cases, simple transformation 
rules, such as ?????? were defined on the out-
put transliterations in a post processing step. 25 
transformation rules were specified for some of 
the mistransliterations to test the effect of model-
ing context.  
2.4 Transliteration using PSMT system  
Transliterations generated using pair HMM pa-
rameters and WFSTs are evaluated against those 
generated from a state of the art Phrase-based 
Statistical Machine Translation system called 
Moses. Moses has been used before for machine 
transliteration (Matthews, 2007) and performed 
way better than a baseline system that was asso-
ciated with finding the most frequent mappings 
between source and target transliteration units in 
the training data. In the PBSMT system, bilin-
gual phrase-tables are used and several compo-
nents are combined in a log-linear model (trans-
lation models, reverse translation model, word 
and phrase penalties, language models, distortion 
parameters, etc.) with weights optimized using 
minimum error rate training. For machine transli-
teration: characters are aligned instead of words, 
phrases refer to character n-grams instead of 
word n-grams, and language models are defined 
over character sequences instead of word se-
quences. A major advantage of the PBSMT sys-
tem over the pair HMM and a WFST models is 
that the phrase tables (character n-grams) cover a 
lot of contextual dependencies found in the data.   
3 Experiments 
3.1 Data Setup 
The data used is divided according to the expe-
rimental runs that were specified for the NEWS 
2009 shared transliteration task (Li et al, 2009): 
a standard run and non-standard runs. The stan-
dard run involved using the transliteration system 
described above that uses pair HMM parameters 
combined with transformation rules. The Eng-
lish-Russian datasets used here were provided for 
the NEWS 2009 shared transliteration task (Ku-
maran and Kellner, 2009): 5977 pairs of names 
for training, 943 pairs for development, and 1000 
for testing.  For the non-standard runs, an addi-
tional English-Russian dataset extracted from the 
Geonames data dump was merged with the 
shared transliteration task data above to form 
10481 pairs for training and development. For a 
second set of experiments (Table 2), a different 
set of test data (1000 pairs) extracted from the 
Geonames data dump was used. For the system 
used in the standard run, the training data was 
preprocessed to include representation of bi-
grams associated with Cyrillic Romanization and 
all English vowel bigram combinations. 
3.2 Results  
Six measures were used for evaluating system 
transliteration quality. These include (Li et al, 
2009): Accuracy (ACC), Fuzziness in Top-1 
(Mean F Score), Mean Reciprocal Rank (MRR), 
Mean Average Precision for reference translite-
rations (MAP_R), Mean Average Precision in 10 
best candidate transliterations (MAP_10), Mean 
Average Precision for the system (MAP_sys). 
Table 1 shows the results obtained using only the 
data sets provided for the shared transliteration 
task. The system used for the standard run is 
?phmm_rules? described in section 2 to sub sec-
tion 2.3. ?phmm_basic? is the system in which 
pair HMM parameters are used for transliteration 
generation but there is no representation for bi-
grams as described for the system used in the 
standard run. Table 2 shows the results obtained 
when additional data from Geonames data dump 
was used for training and development. In Table 
2, ?WFST_basic? and ?WFST_rules? are sys-
tems associated with training WFSTs for the 
?phmm_basic? and ?phmm_rules? systems  
102
     metrics 
models 
ACC Mean F 
Score 
MRR 
phmm_basic 0.293 0.845 0.325 
Moses_PSMT   0.509 0.908 0.619 
phmm_rules 0.354 0.869 0.394 
      metrics 
models 
MAP_R MAP_10 MAP_sys 
phmm_basic 0.293 0.099 0.099 
Moses_PSMT 0.509 0.282 0.282 
phmm_rules 0.354 0.134 0.134 
 
Table 1 Results from data sets for shared transli-
teration task.  
 
     metrics 
models 
ACC Mean F 
Score 
MRR 
phmm_basic 0.341 0.776 0.368 
phmm_rules 0.515 0.821 0.571 
WFST_basic 0.321 0.768 0.403 
WFST_rules 0.466 0.808 0.525 
Moses_PSMT 0.612 0.845 0.660 
      metrics 
models 
MAP_R MAP_10 MAP_sys 
phmm_basic 0.341 0.111 0.111 
phmm_rules 0.515 0.174 0.174 
WFST_basic 0.321 0.128 0.128 
WFST_rules 0.466 0.175 0.175 
Moses_PSMT 0.612 0.364 0.364 
 
Table 2 Results from additional Geonames data 
sets.  
 
respectively. Moses_PSMT is the phrase-based 
statistical machine translation system. The results 
in both tables show that the systems using pair 
HMM parameters perform relatively better than 
the systems trained on WFSTs but not better than 
Moses. The low transliteration quality in the pair 
HMM and WFST systems as compared to Moses 
can be attributed to lack of modeling contextual 
dependencies unlike the case in PBSMT. 
4 Conclusion 
A Transliteration system using pair HMM para-
meters has been presented. Although its perfor-
mance is better than that of systems based on 
only WFSTs, its transliteration quality is lower 
than the PBSMT system. On seeing that the pair 
HMM generated consistent mistransliterations, 
manual specification of a few contextual rules 
resulted in improved performance. As part of 
future work, we expect a technique that automat-
ically identifies the mistransliterations would 
lead to improved transliteration quality.  A more 
general framework, in which we intend to inves-
tigate contextual issues in addition to other fac-
tors such as position in source and target strings 
and edit operation memory in transliteration, is 
that of Dynamic Bayesian Networks (DBNs). 
Acknowledgments 
Funds associated with this work are from a second 
NPT Uganda project. I also thank J?rg Tiedemann for 
helping with experimental runs for the Moses PBSMT 
system. 
References 
A. Kumaran and Tobias Kellner. 2007. A Generic 
Framework for Machine Transliteration. Pro-
ceedings of the 30th SIGIR. 
David Matthews. 2007. Machine Transliteration of 
Proper Names. Master?s Thesis. School of Infor-
matics. University of Edinburgh. 
Jonathan Graehl. 1997. Carmel Finite-state Toolkit. 
http://www.isi.edu/licensed-sw/carmel/. 
Haizhou Li, A. Kumaran, Min Zhang, Vladimir Per-
vouchine. 2009. Whitepaper of NEWS 2009 Ma-
chine Transliteration Shared Task. Proceedings of 
ACL-IJCNLP 2009 Named Entities Workshop 
(NEWS 2009), Singapore. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, 24 (4): 
599-612, MIT Press Cambridge, MA, USA. 
Leonard E. Baum, Ted Petrie, George Soules, and 
Norman Weiss. 1970. A Maximization Technique 
Occurring in the Statistical Analysis of Probabilis-
tic Functions of Markov Chains. The Annals of 
Mathematical Statistics, 41(1):164-171. 
Martijn Wieling, Therese Leinonen and John Ner-
bonne. 2007. Inducing Sound Segment Differences 
using Pair Hidden Markov Models. In John Ner-
bonne, Mark Ellison and Grzegorz Kondrak (eds.) 
Computing Historical Phonology: 9th Meeting of 
the ACL Special Interest Group for Computational 
Morphology and Phonology Workshop, pp. 48-56, 
Prague. 
Mehryar Mohri, Fernando C.N. Pereira, and Michael 
Riley. 2008. Speech Recognition with Weighte Fi-
nite State Transducers. In Larry Rabiner and Fred 
Juang, editors, Handbook on Speech Processing 
and Speech Communication, Part E: Speech Rec-
ognition. Springer-Verlag, Heidelberg, Germany. 
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting Word Similarity and Identifying Cognates 
with Pair Hidden Markov Models. Proceedings of 
the Ninth Conference on Computational Natural 
Language Learning (CoNLL 2005), pp. 40-47, 
Ann-Arbor, Michigan. 
103
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 76?80,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Mining Transliterations from Wikipedia using Pair HMMs
Peter Nabende
Alfa-Informatica, University of Groningen
The Netherlands
p.nabende@rug.nl
Abstract
This paper describes the use of a pair
Hidden Markov Model (pair HMM) sys-
tem in mining transliteration pairs from
noisy Wikipedia data. A pair HMM vari-
ant that uses nine transition parameters,
and emission parameters associated with
single character mappings between source
and target language alphabets is identified
and used in estimating transliteration sim-
ilarity. The system resulted in a precision
of 78% and recall of 83% when evaluated
on a random selection of English-Russian
Wikipedia topics.
1 Introduction
The transliteration mining task as defined in the
NEWS 2010 White paper (Kumaran et al, 2010)
required identifying single word transliteration
pairs from a set of candidate transliteration pairs.
In the case of Wikipedia data, we have a collection
of corresponding source and target language topics
that can be used for extracting candidate translit-
erations. We apply a pair HMM edit-distance
based method to obtain transliteration similarity
estimates. The similarity estimates for a given set
of source and target language words are then com-
pared with the aim of identifying potential translit-
eration pairs. Generally, the pair HMM method
uses the notion of transforming a source string
to a target string through a series of edit opera-
tions. The three edit operations that we consider
for use in transliteration similarity estimation in-
clude: substitution, insertion, and deletion. These
edit operations are represented as hidden states of
a pair HMM. Depending on the source and target
language alphabets, it is possible to design or use a
specific pair HMM algorithm for estimating paired
character emission parameters in the edit opera-
tion states, and transition parameters for a given
design of transitions between the pair HMM?s
states. Before applying the pair HMM method, we
use external datasets to identify a pair HMM vari-
ant that we consider as suitable for application to
transliteration similarity estimation. We then use
the shared task datasets to train the selected pair
HMM variant, and finally apply an algorithm that
is specific to the trained pair HMM for comput-
ing transliteration similarity estimates. In section
2, we discuss transliteration similarity estimation
with regard to applying the pair HMM method;
section 3 describes the experimental setup and re-
sults; section 4 concludes the paper with pointers
to future work.
2 Transliteration Similarity Estimation
using Pair HMMs
To describe the transliteration similarity estima-
tion process, consider examples of corresponding
English (as source language) and Russian (as tar-
get language) Wikipedia topics as shown in Table
1. Across languages, Wikipedia topics are written
in different ways and all words in a topic could be
important for mining transliterations. One main
step in the transliteration mining task is to identify
a set of words in each topic for consideration as
candidate transliterations. As seen in Table 1, it is
very likely that some words will not be selected as
id English topic Russian topic
1 Johnston Atoll ???????? (?????)
2 Oleksandr ????????, ?????????
Palyanytya ??????????
3 Ministers for ?????????:????????
Foreign Affairs of ??????????? ???
Luxembourg ???????????
... ...
Table 1: Example of corresponding English Rus-
sian Wikipedia topics
76
candidate transliterations depending on the criteria
for selection. For example, if a criterion is such
that we consider only words starting with upper-
case characters for English and Russian datasets,
then the Russian word ??????? in the topic pair 1
in Table 1 will not be used as a candidate translit-
eration and that in turn makes the system loose
the likely pair of ?Atoll, ??????. After extracting
candidate transliterations, the approach we use in
this paper takes each candidate word on the source
language side and determines a transliteration es-
timate with each candidate word on the target lan-
guage side. Consider the example for topic id 1 in
Table 1 where we expect to have ?Johnston? and
?Atoll? as candidate source language translitera-
tions, and ?????????? and ??????? as candidate
target language transliterations. The method used
is expected to compare ?Johnston? against ?????-
????? and ???????, and then compare ?Atoll? to
the Russian candidate transliterations. We expect
the output to be ?Johston, ????????? and ?Atoll,
?????? as the most likely single word transliter-
ations from topic pair 1 after sorting out all the
four transliteration similarity estimates in this par-
ticular case. We employ the pair HMM approach
to estimate transliteration similarity for candidate
source-target language words.
A pair HMM has an emission state or states that
generate two observation sequences instead of one
observation sequence as is the case in standard
HMMs. Pair HMMs originate from work in Bi-
ological sequence analysis (Durbin et al, 1998;
Rivas and Eddy, 2001) from which variants were
created and successfully applied in cognate identi-
fication (Mackay and Kondrak, 2005), Dutch di-
alect comparison (Wieling et al, 2007), translit-
eration identification (Nabende et al, 2010),
and transliteration generation (Nabende, 2009).
As mentioned earlier, we have first, tested two
pair HMM variants on manually verified English-
Russian datasets which we obtain from the previ-
ous shared task on machine transliteration (NEWS
2009) (Kumaran and Kellner, 2007). This pre-
liminary test is aimed at determining the effect of
pair HMM parameter changes on the quality of the
transliteration similarity estimates. For the first
pair HMM variant, no transitions are modeled be-
tween edit states; we only use transtion parame-
ters associated with transiting from a start state to
each of the edit operation states, and from each
of the edit operation states to an end state. The
       
 
I 
M 
End 
1-? D-
? I-? M1-
 
? D - ?
D - 
? D 
1 - ? I
 
-
 
? I - ? I
 
? I  ? D  
? I 
? D 
? M 
? D ? I 
? D ? I 
D 
Figure 1: Pair HMM with nine distinct transi-
tion parameters. Emission parameters are speci-
fied with emitting states and their size is dependent
on the characters used in the source and target lan-
guages
second pair HMM variant uses nine distinct tran-
sition parameters between the pair HMM?s states
as shown in Figure 1. The node M in Figure 1 rep-
resents the substitution state in which emission pa-
rameters encode relationships between each of the
source and target language characters. D denotes
the deletion state where emission parameters spec-
ify relationships between source language charac-
ters and a target language gap. I denotes the inser-
tion state where emission parameters encode rela-
tionships between target language characters and
a source language gap. Starting parameters for the
pair HMM in Figure 1 are assoicated with transit-
ing from the M state to one of the edit operation
states including transiting back to M.
The pair HMM parameters are estimated using
the well-known Baum-Welch Expectation Maxi-
mization (EM) algorithm (Baum et al, 1970).
For each pair HMM variant, the training algorithm
starts with a uniform distribution for substitution,
deletion, insertion, and transition parameters, and
iterates through the data until a local maximum.
A method referred to as stratified ten fold cross
validation (Olson and Delen, 2008) is used to eval-
uate the two pair HMM variants. In each fold,
7056 pairs of English-Russian names from the pre-
vious shared task on machine transliteration (Ku-
77
Pair HMM Model CVA CVMRR
phmm00edtrans
Viterbi 0.788 0.809
Forward 0.927 0.954
phmm09edtrans
Viterbi 0.943 0.952
Forward 0.987 0.991
Table 2: CVA and CVMRR results two pair HMM
variants on a preliminary transliteration identifica-
tio experiment. phmm00edtrans is the pair HMM
variant with no transition parameters between the
edit states while phmm09edtrans is the pair HMM
variant with nine distinct transition parameters.
maran and Kellner, 2007) are used for training and
784 name pairs for testing. The Cross Valida-
tion Accuracy (CVA) and Cross Validation Mean
Reciprocal Rank (CVMRR) results obtained from
applying the Forward and Viterbi algorithms of the
two pair HMM variants on this particular dataset
are shown in Table 2.
The CVA and CVMRR values in Table 2 sug-
gest that it is necessary to model for transition pa-
rameters when using pair HMMs for translitera-
tion similarity estimation. Table 2 also suggests
that it is better to use the Forward algorithm for a
given pair HMM variant. Based on the results in
Table 2, the pair HMM variant illustrated in Figure
1 is chosen for application in estimating transliter-
ation similarity for the mining task.
3 Experimental setup and Results
To simplify the analysis of the source and tar-
get strings, the pair HMM system requires unique
whole number representations for each character
in the source and target language data. This is not
suitable for all the different types of writing sys-
tems. In this paper, we look at only the English
and Russian languages where many characters are
associated with a phonemic alphabet and where
numbered representations are hardly expected to
contribute to errors from loss of information in-
herent in the original orthography. A preliminary
run on Chinese-English1 datasets from the previ-
ous shared task on machine transliteration (NEWS
2009) resulted in an accuracy of 0.213 and MRR
of 0.327 using the pair HMM variant in Figure
1. In the following subsection we discuss some
data preprocessing steps on the English-Russian
1In this case Chinese is the source language while English
is the target language
Wikipedia dataset.
3.1 English and Russian candidate
transliteration extraction
The English-Russian Wikipedia dataset that was
provided for the transliteration mining task is very
noisy meaning that it has various types of other en-
tities in addition to words for each language?s or-
thography. A first step in simplifying the translit-
eration mining process was to remove any unnec-
essary entities.
We observed the overlap of writing systems in
both the English and Russian Wikipedia datasets.
We therefore made sure that there is no topic
where the same writing system is used in both the
English and Russian data. Any strings that contain
characters that are not associated with the writ-
ing systems for English and Russian were also re-
moved.
We also observed the presence of many tempo-
ral and numerical expressions that are not neces-
sary on both the English and Russian Wikipedia
datasets. We applied different sets of rules to re-
move such expressions while leaving any neces-
sary words.
Using knowledge about the initial formatting
of strings in both the English and Russian data,
a set of rules was applied to split most of the
strings based on different characters. For ex-
ample almost all strings in the English side had
the underscore ? ? character as a string separa-
tor. We also removed characters such as: colons,
semi-colons, commas, question marks, exclama-
tion marks, dashes, hyphens, forward and back
slashes, mathematical operator symbols, currency
symbols, etc. Some strings were also split based
on string patterns, for example where different
words are joined into one string and it was easy
to identify that the uppercase character for each
word still remained in the combined string just like
when it is alone. We also removed many abbrevia-
tions and titles in the datasets that were not neces-
sary for analysis during the transliteration mining
process.
After selecting candidate words based on most
of the criteria above, we determine all characters
in our extracted candidate transliteration data and
compare against those in the shared task?s seed
data (Kumaran et al, 2010) with the aim of find-
ing all characters that are missing in the seed data.
Matching transliteration pairs with the the miss-
78
ing characters are then hand picked from the can-
didate words dataset and added to the seed data
before training the pair HMM variant that is se-
lected from the previous section. The process for
identifying missing characters and words that have
them is carried out seperately for each language.
However, a matching word in the other language
is identified to constitute a transliteration pair that
can be added to the seed dataset. For the English-
Russian dataset, we use 142 transliteration pairs in
addition to the 1000 transliteration pairs in the ini-
tial seed data. We hence apply the Baum-Welch
algorithm for the selected pair HMM specification
from section 2 on a total of 1142 transliteration
pairs. The algorithm performed 182 iterations be-
fore converging for this particular dataset.
3.2 Results
To obtain transliteration similarity measures, we
apply the Forward algorithm of the trained pair
HMM from section 3.1 to all the remaining
Wikipedia topics. For each word in an English
topic, the algorithm computes transliteration simi-
larity estimates for all words in the Russian topic.
After observing transliteration similarity estimates
for a subset of candidate transliteration words, we
specify a single threshold value (th) and use it
for identifying potential transliteration pairs. A
threshold value of 1 ? 10?13 was chosen after
observing that many of the pairs that had a sim-
ilarity estimate above this threshold were indeed
transliteration pairs. Therefore, a pair of words
was taken as a potential transliteration pair only
when its transliteration estimate (tr sim) was such
that tr sim > th. This resulted in a total of
299389 potential English-Russian transliteration
pairs. This collection of potential transliteration
pairs has been evaluated using a random set of cor-
responding English and Russian Wikipedia topics
as specified in the NEWS 2010 White paper for
the transliteration mining task (Kumaran et al,
2010). Table 3 shows the precision, recall, and
f-score results2 that were obtained after applying
the Forward algorithm for the pair HMM of Fig-
ure 1.
Despite using the pair HMM method with its
basic probabilistic one-to-one mapping for each
2The numbers in Table 3 were obtained from a post eval-
uation after correcting a number of processing errors in the
pair HMM transliteration mining system. The errors initially
led to relatively lower values associated with the measures in
this Table. The values in this Table are therefore not part of
the initial shared task results
Model precision recall f-score
phmm09edtrans 0.780 0.834 0.806
Table 3: Evaluation results for the Pair HMM of
Figure 1 on a random selection of 1000 corre-
sponding English Russian Wikipedia topics.
of the source target character representations, the
result in Table 3 suggests a promising applica-
tion of pair HMMs in mining transliterations from
Wikipedia.
4 Conclusions and Future Work
We have described the application of Pair HMMs
to mining transliterations from Wikipedia. The
transliteration mining evaluation results suggest
a valuable application of Pair HMMs to mining
transliterations. Currently, the pair HMM system
is considered to be best applicable to languages
whose writing system mostly uses a phonemic al-
phabet. Although an experimental test run was
done for Chinese-English data, a conclusion about
the general applicability of the pair HMM neces-
sitates additional tests using other language pairs
such as Hindi and Tamil which were also part of
the shared task.
As future work, we would like to investigate
the performance of Pair HMMs on additional writ-
ing systems. This may require additional modi-
fications to a pair HMM system to minimize on
input formatting errors for other types of writ-
ing systems. It is also necessary to determine the
transliteration mining performance of pair HMMs
when more tolerant criteria are used on the noisy
Wikipedia data. Currently, the pair HMM is ap-
plied in its most basic form, that is, no complex
modifications have been implemented for example
modeling for context in source and target language
words, and other factors that may affect the quality
of a transliteration similarity estimate; it should be
interesting to investigate perfromance of complex
pair HMM variants in transliteration mining.
Acknowledgments
Research in this paper is funded through a second
NPT (Uganda) Project.
References
A Kumaran, Mitesh Khapra, and Haizhou Li. 2010.
Whitepaper on NEWS 2010 Shared Task on
79
Transliteration Mining.
A Kumaran and Tobias Kellner. 2007. A Generic
Framework for Machine Transliteration. Proceed-
ings of the 30th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR 2007), pp 721?722, Ams-
terdam, The Netherlands.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A Maximization Technique Oc-
curring in the Statistical Analysis of Probabilistic
Functions of Markov Chains. Annals of Mathemati-
cal Statistics, 41(1):164?171.
David L. Olson and Dursun Delen. 2008. Advanced
Data Mining Techniques. Springer.
Elena Rivas and Sean R. Eddy. 2001. Noncoding RNA
Gene Detection using Comparative Sequence Anal-
ysis. BMC Bioinformatics 2001, 2:8.
Martijn Wieling, Therese Leinonen, and John Ner-
bonne. 2007. Inducing Sound Segment Differences
using Pair Hidden Markov Models. In John Ner-
bonne, Mark Ellison, and Grzegorz Kondrak (eds.)
Computing Historical Phonology: 9th Meeting of
the ACL Special Interest Group for Computational
Morphology and Phonology Workshop, pp 48?56,
Prague, Czech Republic.
Peter Nabende. 2009. Transliteration System using
Pair HMMs with Weighted FSTs. Proceedings of
the Named Entities Workshop, NEWS?09, pp 100?
103, Suntec, Singapore.
Peter Nabende, Jorg Tiedemann, and John Nerbonne.
2010. Pair Hidden Markov Model for Named Entity
Matching. In Tarek Sobh (ed.) Innovations and Ad-
vances in Computer Sciences and Engineering, pp
497?502, Springer, Heidelberg.
Richard Durbin, Sean R. Eddy, Anders Krogh, Graeme
Mitchison. 1998. Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids.
Cambridge University Press, Cambridge, UK.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting Word Similarity and Identifying Cognates
with Pair Hidden Markov Models. Proceedings
of the ninth Conference on Computational Natural
Language Learning (CoNLL 2005), pp 40?47, Ann
Arbor, Michigan.
80
