Improving Chronological Sentence Ordering
by Precedence Relation
Naoaki OKAZAKI
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656,
Japan
okazaki@miv.t.u-tokyo.ac.jp
Yutaka MATSUO
AIST
2-41-6 Aomi, Koto-ku,
Tokyo 135-0064,
Japan
y.matsuo@carc.aist.go.jp
Mitsuru ISHIZUKA
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656,
Japan
ishizuka@miv.t.u-tokyo.ac.jp
Abstract
It is necessary to find a proper arrangement of sen-
tences in order to generate a well-organized sum-
mary from multiple documents. In this paper we de-
scribe an approach to coherent sentence ordering for
summarizing newspaper articles. Since there is no
guarantee that chronological ordering of extracted
sentences, which is widely used by conventional sum-
marization system, arranges each sentence behind
presupposed information of the sentence, we improve
chronological ordering by resolving antecedent sen-
tences of arranged sentences. Combining the re-
finement algorithm with topical segmentation and
chronological ordering, we address our experiment to
test the effectiveness of the proposed method. The
results reveal that the proposed method improves
chronological sentence ordering.
1 Introduction
The growth of computerized documents enables
us to find relevant information easily owing to
technological advances in Information Retrieval.
Although it is convenient that we can obtain a
great number of documents with a search en-
gine, this situation also presents the information
pollution problem: ?Who is willing to take the
tedious burden of reading all those text docu-
ments?? Automatic text summarization (Mani,
2001), is one solution to the problem, providing
users with a condensed version of the original
text.
Most existing summarization systems make
use of sentence or paragraph extraction, which
finds significant textual segments in source doc-
uments, and compile them in a summary. After
we select significant sentences as a material for
a summary, we must find a proper arrangement
of the sentences and edit each sentence by delet-
ing unnecessary parts or inserting necessary ex-
pressions. Although there has been a great deal
of research on extraction since the early stage
of natural language processing (Luhn, 1958),
research on post-processing of automatic sum-
marization is relatively small in number. It is
essential to pay attention to sentence ordering
in case of multi-document summarization. Sen-
tence position in the original document, which
yields a good clue to sentence arrangement for
single-document summarization, is not enough
for multi-document summarization because we
must consider inter-document order at the same
time.
In this paper we propose an approach to co-
herent text structuring for summarizing news-
paper articles. We improve chronological order-
ing, which is widely used by conventional sum-
marization system, complementing presupposed
information of each sentence. The rest of this
paper is organized as follows. We first review
the sentence ordering problem and present our
approach to generate an acceptable ordering in
the light of coherence relation. The subsequent
section (Section 3) addresses evaluation metrics
and experiment results. In Section 4 we discuss
future work and conclude this paper.
2 Sentence Ordering
2.1 Sentence ordering problem
Our goal is to determine the most probable per-
mutation of given sentences and to generate a
well-structured text. When a human is asked
to make an arrangement of sentences, he or she
may perform this task without difficulty just
as we write out thoughts in a text. However,
we must consider what accomplishes this task
since computers are unaware of order of things
by nature. Discourse coherence as typified by
rhetorical relation (Mann and Thompson, 1988)
and coherence relation (Hobbs, 1990) is of help
to this question. Hume (Hume, 1748) claimed
that qualities from which association arises and
by which the mind is conveyed from one idea
to another are three: resemblance; contiguity
in time or place; and cause and effect. That
is to say we should organize a text from frag-
c) Dolly gave birth to two children in her life.
b) The father is of a different kind and Dolly 
    had been pregnant for about five months.
a) Dolly the clone sheep was born in 1996.
Sentences Preferred ordering
[a-c-b]
Refinement
Chronological order
Figure 1: A chronological ordering is not enough.
mented information on the basis of topical rele-
vancy, chronological sequence, and cause-effect
relation. It is especially true in sentence order-
ing of newspaper articles because we must ar-
range a large number of time-series events con-
cerning several topics.
Barzilay et al (Barzilay et al, 2002) address
the problem of sentence ordering in the context
of multi-document summarization and the im-
pact of sentence ordering on readability of a
summary. They proposed two naive sentence-
ordering techniques such as majority ordering
(examines most frequent orders in the original
documents) and chronological ordering (orders
sentence by the publication date). Showing that
using naive ordering algorithms does not pro-
duce satisfactory orderings, Barzilay et al also
investigates through experiments with humans,
how to identify patterns of orderings that can
improve the algorithm. Based on the exper-
iments, they propose another algorithm that
utilizes chronological ordering with topical seg-
mentation to separate sentences referring to a
topic from ones referring to another.
Lapata (Lapata, 2003) proposes another ap-
proach to information ordering based on a prob-
abilistic model that assumes the probability of
any given sentence is determined by its adja-
cent sentence and learns constraints on sen-
tence order from a corpus of domain specific
texts. Lapata estimates transitional probabil-
ity between sentences by some attributes such
as verbs (precedence relationships of verbs in
the corpus), nouns (entity-based coherence by
keeping track of the nouns) and dependencies
(structure of sentences).
2.2 Improving chronological ordering
Against the background of these studies, we
propose the use of antecedence sentences to ar-
range sentences. Let us consider an example
shown in Figure 1. There are three sentences a,
b, and c from which we get an order [a-b-c]
by chronological ordering. When we read these
sentences in this order, we find sentence b to
be incorrectly positioned. This is because sen-
tence b is written on the presupposition that
the reader may know that Dolly had a child. In
other words, it is more fitting to assume sen-
tence b to be an elaboration of sentence c. As
one may easily imagine, there are some prece-
dent sentences prior to sentence b in the origi-
nal document. Lack of presupposition obscures
what a sentence is saying and confuses the read-
ers. Hence, we should refine the chronological
order and revise the order to [a-c-b], putting
sentence c before sentence b.
We show a block diagram of our ordering al-
gorithm shown in Figure 2. Given nine sen-
tences denoted by [a b ... i], for example,
the algorithm eventually produces an order-
ing, [a-b-f-c-i-g-d-h-e]. We consider top-
ical segmentation and chronological ordering to
be fundamental to sentence ordering as well as
conventional ordering techniques (Barzilay et
al., 2002) and make an attempt to refine the
ordering. We firstly recognize topics in source
documents to separate sentences referring to a
topic from ones referring to another. In Fig-
ure 2 example we obtain two topical segments
(clusters) as an output from the topical cluster-
ing. In the second phase we order sentences of
each segment by the chronological order. If two
sentences have the same chronological order, we
elaborate the order on the basis of sentence po-
sition and resemblance relation. Finally, we re-
fine each ordering by resolving antecedent sen-
tences and output the final ordering. In the rest
of this section we give a detailed description of
each phase.
2.3 Topical clustering
The first task is to categorize sentences by their
topics. We assume a newspaper article to be
written about one topic. Hence, to classify top-
ics in sentences, we have only to classify articles
a ab bc c
d
d
e
e
f
f
g
g
h hi
i
abc
he
ig
d
f
abf
dh
ci
e
g
Topical clustering by documents
Chronological ordering 
with resemblance relation
Ordering refinement
by precedence relation
Cluster #1
Unorderedsentences Orderedsentences
Cluster #2
Figure 2: The outline of the ordering algorithm.
by their topics. Given l articles and we found
m kinds of terms in the articles. Let D be a
document-term matrix (l ?m), whose element
Dij represents frequency of a term #j in doc-
ument #i, We use Di to denote a term vector
(i-component row vector) of document #i. Af-
ter measuring distance or dissimilarity between
two articles #x and #y:
distance(Dx, Dy) = 1? Dx ?Dy|Dx||Dy| , (1)
we apply the nearest neighbor method (Cover
and Hart, 1967) to merge a pair of clusters
when their minimum distance is lower than a
given parameter ? = 0.3 (determined empiri-
cally). At last we classify sentences according
to topical clusters, assuming that a sentence in
a document belonging to a cluster also belongs
to the same cluster.
2.4 Chronological ordering
It is difficult for computers to find a resemblance
or cause-effect relation between two phenom-
ena while we do not have conclusive evidence
whether a pair of sentences gathered arbitrarily
from multiple documents has some relation. A
newspaper usually deals with novel events that
have occurred since the last publication. Hence,
publication date (time) of each article turns out
to be a good estimator of resemblance relation
(i.e., we observe a trend or series of relevant
events in a time period), contiguity in time, and
cause-effect relation (i.e., an event occurs as a
result of previous events). Although resolving
temporal expressions in sentences (e.g., yester-
day, the next year, etc.) (Mani and Wilson,
2000; Mani et al, 2003) may give a more pre-
cise estimation of these relations, it is not an
easy task. For this reason we order sentences
of each segment (cluster) by the chronological
a
.
.
c'
.
b
c
.
Article #1 Article #2 Article #3
chronological order
Figure 3: Background idea of ordering refine-
ment by precedence relation.
order, assigning a time stamp for each sentence
by its publication date (i.e., the date when the
article was written).
When there are sentences having the same
time stamp, we elaborate the order on the ba-
sis of sentence position and sentence connectiv-
ity. We restore an original ordering if two sen-
tences have the same time stamp and belong
to the same article. If sentences have the same
time stamp and are not from the same article,
we arrange a sentence which is more similar to
previously ordered sentences to assure sentence
connectivity.
2.5 Ordering refinement by precedence
relation
After we obtain an ordering of a topical seg-
ment by chronological ordering, we improve it
as shown in Figure 1 based on antecedence sen-
tences. Figure 3 shows the background idea
of ordering refinement by precedence relation.
Just as in the example in Figure 1, we have
three sentences a, b, and c in chronological or-
der. At first we get sentence a out of the sen-
tences and check its antecedent sentences. See-
ing that there are no sentences prior to sentence
a in article #1, we accept to put sentence a
here. Then we get sentence b out of remaining
sentences and check its antecedent sentences.
We find several sentences before sentence b in
article #2 this time. Grasping what the an-
tecedent sentences are saying, we confirm first
of all whether what they are saying is mentioned
by previously arranged sentences (i.e., sentence
a). If it is mentioned, we put sentence b here
and extend the ordering to [a-b]. Otherwise,
we search a substitution for what the precedence
sentences are saying from the remaining sen-
tences (i.e., sentence c in this example). In the
Figure 3 example, we find out that sentence a is
not referring to what sentence c? is saying but
sentence c is approximately referring to that.
Start
End
a
b
ef
e f
c
d
f
f
d
df
d f
e
de
d e
f
No precedent sentences before 
sentence a. Choose a.
Choose the rest, sentence f.
The refined ordering is
a-b-e-c-d-f.
No precedent sentences before 
sentence b. Choose b.
There are precedent sentences 
before sentence c.
Search a shortest path from c 
to b and a. We found sentence 
e to be the closest to the 
precedent sentences of c.
Search a shortest path from e 
to b and a. No precedent 
sentences before e. Choose e.
We find a path from c to b and 
a via e is the shortest.
There are precedent sentences 
before sentence d.
Search a shortest path from d 
to c, e, b and a. We find the 
direct path from d to c is the 
shortest.
0
0
.2
.7
.6 1
0
0
.4.8
(1)(1)
(2)
(3)
(3)
(4)
(5)
(6)
(7)
(8)(8)
(6)(7)
(5)
(4)
(2)
Figure 4: Ordering refinement by precedence relation as a shortest path problem.
Putting sentence c before b, we finally get the
refined ordering [a-c-b].
Supposing that sentence c mentions similar
information as c? but expresses more than c?,
it is nothing unusual that an extraction method
does not choose sentence c? but sentence c.
Because a method for multi-document summa-
rization (e.g., MMR (Carbonell and Goldstein,
1998)) makes effort to acquire information cov-
erage and refuse redundant information at the
same time, it is quite natural that the method
does not choose both sentence c? and c in terms
of redundancy and prefers sentence c as c? in
terms of information coverage.
Figure 4 illustrates how the algorithm refines
a given chronological ordering [a-b-c-d-e-f].
We define distance as a dissimilarity value of
precedent information of a sentence. When
a sentence has antecedent sentences and their
content is not mentioned by previously arranged
sentences, this distance will be high. When a
sentence has no precedent sentences, we define
the distance to be 0. In the example shown
in Figure 4 example we do not change posi-
tion of sentences a and b because they do not
have precedent sentences (i.e., they are lead sen-
tences). On the other hand, sentence c has
some precedent sentences in its original docu-
ment. Preparing a term vector of the precedent
sentences, we calculate how much the precedent
content is covered by other sentences using dis-
tance defined above. In Figure 4 example the
distance from sentence a and b to c is high
(distance = 0.7). We search a shortest path
from sentence c to sentences a and b by best-
first search in order to find suitable sentences
before sentence c. Given that sentence e in Fig-
ure 4 describes similar content as the precedent
sentences of sentence c and is a lead sentence,
we trace the shortest path from sentence c to
sentences a and b via sentence e. We extend
the resultant ordering to [a-b-e-c], inserting
sentence e before sentence c. Then we consider
sentence d, which is not a lead sentence again
(distance = 0.4). Preparing a term vector of the
precedent sentences of sentence d, we search a
shortest path from sentence d to sentences a,
b, c, and e. The search result shows that we
should leave sentence d this time because the
precedent content seems to be described in sen-
tences a, b, c, and e better than f. In this way
we get the final ordering, [a-b-e-c-d-f].
3 Evaluation
In this section we describe our experiment to
test the effectiveness of the proposed method.
3.1 Experiment and evaluation metrics
We conducted an experiment of sentence order-
ing through multi-document summarization to
test the effectiveness of the proposed method.
We utilized the TSC-3 (Hirao et al, to appear in
2004) test collection, which consists of 30 sets of
multi-document summarization tasks. For more
information about TSC-3 task, see the work-
shop proceedings. Performing an important
sentence extraction (Okazaki et al, to appear
in 2004) up to the specified number of sentences
(approximately 10% of summarization rate), we
made a material for a summary (i.e., extracted
sentences) for each task. We order the sentences
by six methods: human-made ordering (HO) as
the highest anchor; random ordering (RO) as
the lowest anchor; chronological ordering (CO)
(i.e., phase 2 only); chronological ordering with
topical segmentation (COT) (i.e., phases 1 and
2); proposed method without topical segmenta-
tion (PO) (i.e., phases 2 and 3); and proposed
method with topical segmentation (POT)). We
asked human judges to evaluate sentence order-
ing of these summaries.
The first evaluation task is a subjective grad-
ing where a human judge marks an ordering of
summary sentences on a scale of 4: 4 (perfect), 3
(acceptable), 2 (poor), and 1 (unacceptable). We
give a clear criterion of scoring to the judges as
follows. A perfect summary is a text that we
cannot improve any further by re-ordering. An
acceptable summary is a one that makes sense
and is unnecessary to be revised even though
there may be some room for improvement in
terms of readability. A poor summary is a one
that loses a thread of the story at some places
and requires minor amendment to bring it up to
the acceptable level. An unacceptable summary
is a one that leaves much to be improved and
requires overall restructuring rather than par-
tial revision. Additionally, we inform the judges
that summaries were made of the same set of
extracted sentences and only sentence ordering
made differences between the summaries in or-
der to avoid any disturbance in rating.
In addition to the rating, it is useful that we
examine how close an ordering is to an accept-
able one when the ordering is regarded as poor.
Considering that several sentence-ordering pat-
terns are acceptable for a given summary, we
An ordering to evaluate: 
The corrected ordering:
s5, s6, s7, s8, s1, s2, s9, s3, s4
s5, s6, s7, s9, s2, s8, s1, s3, s4
( )
)(
Correction by move operation
A judge is supposed to show how to improve an ordering.
The judge's reading is interupted before the points marked with black circles.
Figure 5: Correction of an ordering.
think that it is valuable to measure the degree of
correction because this metric virtually requires
a human corrector to prepare a correct answer
for each ordering in his or her mind. Therefore,
a human judge is supposed to illustrate how to
improve an ordering of a summary when he or
she marks the summary with poor in the rat-
ing task. We restrict applicable operations of
correction to move operation so as to keep the
minimum correction of the ordering. We define
a move operation here as removing a sentence
and inserting the sentence into an appropriate
place (see Figure 5).
Supposing a sentence ordering to be a rank,
we can calculate rank correlation coefficient of a
permutation of an ordering pi and a permutation
of the reference ordering ?. Let {s1, ..., sn} be a
set of summary sentences identified with index
numbers from 1 to n. We define a permutation
pi ? Sn to denote an ordering of sentences where
pi(i) represents an order of sentence si. Simi-
larly, we define a permutation ? ? Sn to denote
the corrected ordering. For example, the pi and
? in Figure 5 will be:
pi =
( 1 2 3 4 5 6 7 8 9
5 6 8 9 1 2 3 4 7
)
, (2)
? =
( 1 2 3 4 5 6 7 8 9
7 5 8 9 1 2 3 6 4
)
. (3)
Spearman?s rank correlation ?s(pi, ?) and
Kendall?s rank correlation ?k(pi, ?) are known
as famous rank correlation metrics.
?s(pi, ?) = 1? 6n(n+ 1)(n? 1)
n?
i=1
(pi(i)? ?(i))2
(4)
?k(pi, ?) = 1n(n? 1)/2 ?
n?1?
i=1
n?
j=i+1
sgn(pi(j)? pi(i)) ? sgn(?(j)? ?(i)), (5)
4 3 2 1
RO 0.0 0.0 6.0 94.0
CO 13.1 22.6 63.1 1.2
COT 10.7 22.6 61.9 4.8
PO 16.7 38.1 45.2 0.0
POT 15.5 36.9 44.0 3.6
HO 52.4 21.4 26.2 0.0
Table 1: Distribution of rating score of order-
ings in percent figures.
where sgn(x) = 1 for x > 0 and ?1 otherwise.
These metrics range from ?1 (an inverse rank)
to 1 (an identical rank) via 0 (a non-correlated
rank). In the example shown in Equations 2 and
3 we obtain ?s(pi, ?) = 0.85 and ?k(pi, ?) = 0.72.
We propose another metric to assess the de-
gree of sentence continuity in reading, ?c(pi, ?):
?c(pi, ?) = 1n
n?
i=1
eq
(
pi??1(i), pi??1(i? 1) + 1
)
,
(6)
where: pi(0) = ?(0) = 0; eq(x, y) = 1 when x
equals y and 0 otherwise. This metric ranges
from 0 (no continuity) to 1 (identical). The
summary in Figure 5 may interrupt judge?s
reading after sentence S7, S1, S2 and S9 as he
or she searches a next sentence to read. Hence,
we observe four discontinuities in the order-
ing and calculate sentence continuity ?c(pi, ?) =
(9? 4)/9 = 0.56.
3.2 Results
Table 1 shows distribution of rating score of
each method in percent figures. Judges marked
about 75% of human-made ordering (HO) as ei-
ther perfect or acceptable while they rejected as
many as 95% of random ordering (RO). Chrono-
logical ordering (CO) did not yield satisfactory
result losing a thread of 63% summaries al-
though CO performed much better than RO.
Topical segmentation could not contribute to
ordering improvement of CO as well: COT is
slightly worse than CO. After taking an in-
depth look at the failure orderings, we found
the topical clustering did not perform well dur-
ing this test. We suppose the topical clustering
could not prove the merits with this test collec-
tion because the collection consists of relevant
articles retrieved by some query and polished
well by a human so as not to include unrelated
articles to a topic.
On the other hand, the proposed method
(PO) improved chronological ordering much
better than topical segmentation. Note that the
sum of perfect and acceptable ratio jumped up
from 36% (CO) to 55% (PO). This shows the
ordering refinement by precedence relation im-
proves chronological ordering by pushing poor
ordering to an acceptable level.
Table 2 reports closeness of orderings to the
corrected ones with average scores (AVG) and
the standard deviations (SD) of the three met-
rics ?s, ?k and ?c. It appears that average figures
shows similar tendency to the rating task with
three measures: HO is the best; PO is better
than CO; and RO is definitely the worst. We
applied one-way analysis of variance (ANOVA)
to test the effect of four different methods (RO,
CO, PO and HO). ANOVA proved the effect of
the different methods (p < 0.01) for three met-
rics. We also applied Tukey test to compare the
difference between these methods. Tukey test
revealed that RO was definitely the worst with
all metrics. However, Spearman?s rank correla-
tion ?S and Kendall?s rank correlation ?k failed
to prove the significant difference between CO,
PO and HO. Only sentence continuity ?c proved
PO is better than CO; and HO is better than
CO (? = 0.05). The Tukey test proved that
sentence continuity has better conformity to the
rating results and higher discrimination to make
a comparison.
Table 3 shows closeness of orderings to ones
made by human (all results of HO should be 1
by necessity). Although we found RO is clearly
the worst as well as other results, we cannot find
the significant difference between CO, PO, and
HO with all metrics. This result presents to the
difficulty of automatic evaluation by preparing
one correct ordering.
4 Conclusions
In this paper we described our approach to co-
herent sentence ordering for summarizing news-
paper articles. We conducted an experiment
of sentence ordering through multi-document
summarization. The proposed method which
utilizes precedence relation of sentence archived
good results, raising poor chronological order-
ings to an acceptable level by 20%. We also pro-
posed an evaluation metric that measures sen-
tence continuity and a amendment-based eval-
uation task. The amendment-based evalua-
tion outperformed the evaluation that compares
an ordering with an answer made by a hu-
man. The sentence continuity metric applied to
the amendment-based task showed more agree-
Spearman Kendall Continuity
Method AVG SD AVG SD AVG SD
RO 0.041 0.170 0.035 0.152 0.018 0.091
CO 0.838 0.185 0.870 0.270 0.775 0.210
COT 0.847 0.164 0.791 0.440 0.741 0.252
PO 0.843 0.180 0.921 0.144 0.856 0.180
POT 0.851 0.158 0.842 0.387 0.820 0.240
HO 0.949 0.157 0.947 0.138 0.922 0.138
Table 2: Comparison with corrected ordering.
Spearman Kendall Continuity
Method AVG SD AVG SD AVG SD
RO -0.117 0.265 -0.073 0.202 0.054 0.064
CO 0.838 0.185 0.778 0.198 0.578 0.218
COT 0.847 0.164 0.782 0.186 0.571 0.229
PO 0.843 0.180 0.792 0.184 0.606 0.225
POT 0.851 0.158 0.797 0.171 0.599 0.237
HO 1.000 0.000 1.000 0.000 1.000 0.000
Table 3: Comparison with human-made ordering.
ments with the rating result.
We plan to do further study on the sentence
ordering problem in future work, exploring how
to apply our algorithm to documents other than
newspaper or integrate ordering problem with
extraction problem to improve each other. We
also recognize the necessity to establish an auto-
matic evaluation method of sentence ordering.
Acknowledgments
We made use of Mainichi Newspaper and Yomi-
uri Newspaper articles and summarization test
collection of TSC-3.
References
R. Barzilay, E. Elhadad, and K. McKeown.
2002. Inferring strategies for sentence order-
ing in multidocument summarization. Jour-
nal of Artifical Intelligence Research (JAIR),
17:35?55.
J. Carbonell and J. Goldstein. 1998. The use of
MMR, diversity-based reranking for reorder-
ing documents and producing summaries.
In Proceedings of the 21st Annual Interna-
tional ACM-SIGIR Conference on Research
and Development in Information Retrieval,
pages 335?336.
T. M. Cover and P. E. Hart. 1967. Nearest
neighbor pattern classification. IEEE Trans-
actions on Information Theory, IT-13:21?27.
T. Hirao, T. Fukusima, M. Okumura, and
H. Nanba. to appear in 2004. Text summa-
rization challenge 3: text summarization eval-
uation at ntcir workshop4. In Working note
of the 4th NTCIR Workshop Meeting.
J. Hobbs. 1990. Literature and Cognition, CSLI
Lecture Notes 21. CSLI.
D. Hume. 1748. Philosophical Essays concern-
ing Human Understanding.
M. Lapata. 2003. Probabilistic text structur-
ing: experiments with sentence ordering. In
Proceedings of the 41st Meeting of the Asso-
ciation of Computational Linguistics, pages
545?552.
H. P. Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
and Development, 2(2):159?165.
I. Mani and G. Wilson. 2000. Robust temporal
processing of news. In Proceedings of the 38th
Annual Meeting of ACL?2000, pages 69?76.
I. Mani, B. Schiffman, and J. Zhang. 2003. In-
ferring temporal ordering of events in news.
Proceedings of the Human Language Technol-
ogy Conference (HLT-NAACL) ?03.
I. Mani. 2001. Audomatic Summarization.
John Benjamins.
W. Mann and S. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory
of text organization. Text, 8:243?281.
N. Okazaki, Y. Matsuo, and M. Ishizuka. to
appear in 2004. TISS: An integrated summa-
rization system for TSC-3. In Working note
of the 4th NTCIR Workshop Meeting.
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 803?812,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
A Relational Model of Semantic Similarity between Words using
Automatically Extracted Lexical Pattern Clusters from the Web
Danushka Bollegala
?
danushka@mi.ci.i.
u-tokyo.ac.jp
Yutaka Matsuo
matsuo@biz-model.
t.u-tokyo.ac.jp
The University of Tokyo
7-3-1, Hongo, Tokyo, 113-8656, Japan
Mitsuru Ishizuka
ishizuka@i.
u-tokyo.ac.jp
Abstract
Semantic similarity is a central concept
that extends across numerous fields such
as artificial intelligence, natural language
processing, cognitive science and psychol-
ogy. Accurate measurement of semantic
similarity between words is essential for
various tasks such as, document cluster-
ing, information retrieval, and synonym
extraction. We propose a novel model
of semantic similarity using the semantic
relations that exist among words. Given
two words, first, we represent the seman-
tic relations that hold between those words
using automatically extracted lexical pat-
tern clusters. Next, the semantic similar-
ity between the two words is computed
using a Mahalanobis distance measure.
We compare the proposed similarity mea-
sure against previously proposed seman-
tic similarity measures on Miller-Charles
benchmark dataset and WordSimilarity-
353 collection. The proposed method out-
performs all existing web-based seman-
tic similarity measures, achieving a Pear-
son correlation coefficient of 0.867 on the
Millet-Charles dataset.
1 Introduction
Similarity is a fundamental concept in theories
of knowledge and behavior. Psychological ex-
periments have shown that similarity acts as an
organizing principle by which individuals clas-
sify objects, and make generalizations (Goldstone,
1994). For example, a biologist would classify
a newly found animal specimen based upon the
properties that it shares with existing categories
of animals. We can then make additional infer-
ences on the new specimen using the properties
?
Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
known for the existing category. As the simi-
larity between two objects X and Y increases,
so does the probability of correctly inferring that
Y has the property T upon knowing that X has
T (Tenenbaum, 1999). Accurate measurement of
semantic similarity between lexical units such as
words or phrases is important for numerous tasks
in natural language processing such as word sense
disambiguation (Resnik, 1995), synonym extrac-
tion (Lin, 1998a), and automatic thesauri gener-
ation (Curran, 2002). In information retrieval,
similar or related words are used to expand user
queries to improve recall (Sahami and Heilman,
2006).
Semantic similarity is a context dependent and
dynamic phenomenon. New words are constantly
being created and existing words are assigned with
new senses on the Web. To decide whether two
words are semantically similar, it is important to
know the semantic relations that hold between the
words. For example, the words horse and cow can
be considered semantically similar because both
horses and cows are useful animals in agriculture.
Similarly, a horse and a car can be considered se-
mantically similar because cars, and historically
horses, are used for transportation. Semantic re-
lations such as X and Y are used in agriculture,
or X and Y are used for transportation, exist be-
tween two words X and Y in these examples. We
use bold-italics, X, to denote the slot of a word X
in a lexical pattern.
We propose a relational model to compute the
semantic similarity between two words. First, us-
ing snippets retrieved from a web search engine,
we present an automatic lexical pattern extraction
algorithm to represent the semantic relations that
exist between two words. For example, given two
words ostrich and bird, we extract X is a Y, X is
a large Y, and X is a flightless Y from the Web.
Using a set of semantically related words as train-
ing data, we evaluate the confidence of a lexical
803
pattern as an indicator of semantic similarity. For
example, the pattern X is a Y is a better indica-
tor of semantic similarity between X and Y than
the pattern X and Y. Consequently, we would like
to emphasize the former pattern by assigning it a
higher confidence score. It is noteworthy that all
lexical patterns are not independent ? multiple lex-
ical patterns can express the same semantic rela-
tion. For example, the pattern X is a large Y sub-
sumes the more general pattern X is a Y and they
both indicate a hypernymic relationship between
X and Y. By clustering the semantically related
patterns into groups, we can both overcome the
data sparseness problem, and reduce the number
of parameters during training. To identify seman-
tically related patterns, we use a sequential pattern
clustering algorithm that is based on the distribu-
tional hypothesis (Harris, 1954). We represent two
words by a feature vector defined over the clus-
ters of patterns. Finally, the semantic similarity
is computed as the Mahalanobis distance between
points corresponding to the feature vectors. By
using Mahalanobis distance instead of Euclidean
distance, we can account for the inter-dependence
between semantic relations.
2 Related Work
Geometric models, such as multi-dimensional
scaling has been used in psychological ex-
periments analyzing the properties of similar-
ity (Krumhansl, 1978). These models represent
objects as points in some coordinate space such
that the observed dissimilarities between objects
correspond to the metric distances between the re-
spective points. Geometric models assume that
objects can be adequately represented as points in
some coordinate space and that dissimilarity be-
haves like a metric distance function satisfying
minimality, symmetry, and triangle inequality as-
sumptions. However, both dimensional and metric
assumptions are open to question.
Tversky (1977) proposed the contrast model of
similarity to overcome the problems in geometric
models. The contrast model relies on featural rep-
resentation of objects, and it is used to compute the
similarity between the representations of two ob-
jects. Similarity is defined as an increasing func-
tion of common features (i.e. features in common
to the two objects), and as a decreasing function of
distinctive features (i.e. features that apply to one
object but not the other). The attributes of objects
are primal to contrast model and it does not ex-
plicitly incorporate the relations between objects
when measuring similarity.
Hahn et al (2003) define similarity between
two representations as the complexity required to
transform one representation into the other. Their
model of similarity is based on the Representa-
tional Distortion theory, which aims to provide
a theoretical framework of similarity judgments.
Their experiments using pattern sequences and ge-
ometric shapes show an inverse correlation be-
tween the number of transformations required to
convert one pattern (or shape) to another, and the
perceived similarity ratings by human subjects.
How to represent an object, which transformations
are allowed on a representation, and how to mea-
sure the complexity of a transformation, are all
important decisions in the transformational model
of similarity. Although distance measures such as
edit distance have been used to find approximate
matches in a dictionary, it is not obvious how to
compute semantic similarity between words using
representational distortion theory.
Given a taxonomy of concepts, a straightfor-
ward method to calculate similarity between two
words (or concepts) is to find the length of the
shortest path connecting the two words in the tax-
onomy (Rada et al, 1989). If a word is polyse-
mous (i.e. has more than one sense) then multi-
ple paths might exist between the two words. In
such cases, only the shortest path between any two
senses of the words is considered for calculating
similarity. A problem that is frequently acknowl-
edged with this approach is that it relies on the
notion that all links in the taxonomy represent a
uniform distance. As a solution to this problem,
Schickel-Zuber and Faltings (2007) propose ontol-
ogy structure based similarity (OSS) between two
concepts in an ontology, which is an asymmetric
distance function.
Resnik (1995) proposed a similarity measure
using information content. He defined the similar-
ity between two concepts C
1
and C
2
in the taxon-
omy as the maximum of the information content of
all concepts C that subsume both C
1
and C
2
. Then
the similarity between two words is defined as the
maximum of the similarity between any concepts
that the words belong to. He used WordNet as the
taxonomy; information content is calculated using
the Brown corpus.
Li et al, (2003) combined structural seman-
804
tic information from a lexical taxonomy, and in-
formation content from a corpus, in a nonlinear
model. They proposed a similarity measure that
uses shortest path length, depth and local density
in a taxonomy. Their experiments reported a Pear-
son correlation coefficient of 0.8914 on the Miller-
Charles benchmark dataset (Miller and Charles,
1998). Lin (1998b) defined the similarity between
two concepts as the information that is in common
to both concepts and the information contained in
each individual concept.
Cilibrasi and Vitanyi (2007) proposed a distance
metric between words using page-counts retrieved
from a web search engine. The proposed metric is
named Normalized Google Distance (NGD) and is
defined as the normalized information distance (Li
et al, 2004) between two strings. They evaluate
NGD in a word classification task. Unfortunately
NGD only uses page-counts of words and ignores
the context in which the words appear. Therefore,
it produces inaccurate similarity scores when one
or both words between which similarity is com-
puted are polysemous.
Sahami and Heilman (2006) measured semantic
similarity between two queries using snippets re-
turned for those queries by a search engine. For
each query, they collect snippets from a search
engine and represent each snippet as a TF-IDF-
weighted term vector. Each vector is L
2
normal-
ized and the centroid of the set of vectors is com-
puted. Semantic similarity between two queries
is then defined as the inner product between the
corresponding centroid vectors. They did not
compare their similarity measure with taxonomy-
based similarity measures.
Chen et al, (2006) propose a web-based double-
checking model to compute the semantic similar-
ity between words. For two words X and Y , they
collect snippets for each word from a web search
engine. Then they count the number of occur-
rences of X in the snippets for Y , and Y in the
snippets for X . The two values are combined non-
linearly to compute the similarity between X and
Y . This method heavily depends on the search en-
gine?s ranking algorithm. Although two words X
and Y may be very similar, there is no reason to
believe that one can find Y in the snippets for X ,
or vice versa. This observation is confirmed by the
experimental results in their paper which reports 0
similarity scores for many pairs of words in the
Miller-Charles dataset.
In our previous work (Bollegala et al, 2007),
we proposed a semantic similarity measure using
page counts and snippets retrieved from a Web
search engine. To compute the similarity between
two words X and Y , we queried a web search en-
gine using the query X AND Y and extract lex-
ical patterns that combine X and Y from snip-
pets. A feature vector is formed using frequen-
cies of 200 lexical patterns in snippets and four
co-occurrence measures: Dice coefficient, overlap
coefficient, Jaccard coefficient and pointwise mu-
tual information. We trained a two-class support
vector machine using automatically selected syn-
onymous and non-synonymous word pairs from
WordNet. This method reports a Pearson corre-
lation coefficient of 0.837 with Miller-Charles rat-
ings. However, it does not consider the relatedness
between patterns.
Gabrilovich and Markovitch (2007) represent
words using weighted vectors of Wikipedia-based
concepts, and define the similarity between words
as the cosine of the angle between the correspond-
ing vectors. Their method can be used to com-
pute similarity between words as well as between
texts. Although Wikipedia is growing in popular-
ity, not all concepts found on the Web have arti-
cles in Wikipedia. Specially, novel or not very
popular concepts are not adequately covered by
Wikipedia. Moreover, their method requires the
concepts to be independent. For non-independent,
hierarchical taxonomies such as open directory
project (ODP)
1
, their method produces suboptimal
results.
3 Relational Model of Similarity
We propose a model to compute the semantic sim-
ilarity between two words a and b using the set
of semantic relations R(a, b) that hold between a
and b. We call the proposed model the relational
model of semantic similarity and it is defined by
the following equation,
sim(a, b) = ?(R(a, b)). (1)
Here, sim(a, b) is the semantic similarity between
the two words a and b, and ? is a weighting
function defined over the set of semantic relations
R(a, b). Given that a particular set of semantic
relations are known to hold between two words,
the function ? expresses our confidence on those
words being semantically similar.
1
http://www.dmoz.org
805
A semantic relation can be expressed in a num-
ber of ways. For example, given a taxonomy of
words such as the WordNet, semantic relations
(i.e. hypernymy, meronymy, synonymy etc.) be-
tween words can be directly looked up in the tax-
onomy. Alternatively, the labels of the edges in
the path connecting two words can be used as
semantic relations. However, in this paper we
do not assume the availability of manually cre-
ated resources such as dictionaries or taxonomies.
We represent semantic relations using automati-
cally extracted lexical patterns. Lexical patterns
have been successfully used to represent various
semantic relations between words such as hyper-
nymy (Hearst, 1992), and meronymy (Berland and
Charniak, 1999). Following these previous ap-
proaches, we represent R(a, b) as a set of lexical
patterns. Moreover, we denote the frequency of a
lexical pattern r for a word pair (a, b) by f(r, a, b).
So far we have not defined the functional form
of ?. A straightforward approach is to use a lin-
early weighted combination of relations as shown
below,
?(R(a, b)) =
?
r
i
?R(a,b)
w
i
? f(r
i
, a, b). (2)
Here, w
i
is the weight associated with the lexical
pattern r
i
and can be determined using training
data. However, this formulation has two funda-
mental drawbacks. First, the number of weight
parameters w
i
is equal to the number of lexical
patterns. Typically two words can co-occur in nu-
merous patterns. Consequently, we end up with a
large number of parameters in the model. Com-
plex models with a large number of parameters
are difficult to train because they tend to overfit to
the training data. Second, the linear combination
given in Equation 2 assumes the lexical patterns
to be mutually independent. However, in practice
this is not true. For example, both patterns X is a
Y and Y such as X indicate a hypernymic relation
between X and Y.
To overcome the above mentioned limitations,
we first cluster the lexical patterns to identify the
semantically related patterns. Our clustering algo-
rithm is detailed in section 3.2. Next, we define ?
using the formed clusters as follows,
?(R(a, b)) = x
T
ab
??. (3)
Here, x
ab
is a feature vector representing the
words a and b. Each formed cluster contributes
a feature in vector x
ab
as described later in Sec-
tion 5. The vector ? is a prototypical vector rep-
resenting synonymous word pairs. We compute
? as the centroid of feature vectors representing
synonymous word pairs. ? is the inter-cluster cor-
relation matrix. The (i, j)-th element of matrix ?
denotes the correlation between the two clusters c
i
and c
j
. Matrix ? is expected to capture the de-
pendence between semantic relations. Intuitively,
if two clusters i and j are highly correlated, then
the (i, j)-th element of ? will be closer to 1. Equa-
tion 3 computes the similarity between a word pair
(a, b) and a set of synonymous word pairs. In-
tuitively, if the relations that exist between a and
b are typical relations that hold between synony-
mous word pairs, then Equation 3 returns a high
similarity score for a and b.
The proposed relational model of semantic sim-
ilarity differs from feature models of similarity,
such as the contrast model (Tversky, 1977), in
that it is defined over the set of semantic relations
that exist between two words instead of the set of
features for each word. Specifically, in contrast
model, the similarity S(a, b) between two objects
a and b is defined in terms of the features common
to a and b, A ? B, the features that are distinctive
to a, A?B, and the features that are distinctive to
b, B ?A. The contrast model is formalized in the
following equation,
S(a, b) = ?f(A ?B)? ?f(A?B)? ?f(B ?A). (4)
Here, the function f measures the salience of a
particular set of features, and non-negative param-
eters ?, ?, and ? determine the relative weights
assigned to the different components. However, in
the relational model of similarity we do not focus
on features of individual words but on relations be-
tween two words.
Modeling similarity as a phenomenon of rela-
tions between objects rather than features of indi-
vidual objects is central to computational models
of analogy-making such as the structure mapping
theory (SMT) (Falkenhainer et al, 1989). SMT
claims that an analogy is a mapping of knowl-
edge from one domain (base) into another (target)
which conveys that a system of relations known
to hold in the base also holds in the target. The
target objects do not have to resemble their corre-
sponding base objects. During the mapping pro-
cess, features of individual objects are dropped
and only relations are mapped. The proposed rela-
tional model of similarity uses this relational view
806
Ostrich, a large, flightless bird that lives in the dry grass-
lands of Africa.
Figure 1: A snippet returned for the query ?ostrich
* * * * * bird?.
of similarity to compute semantic similarity be-
tween words.
3.1 Extracting Lexical Patterns
To compute semantic similarity between two
words using the relational model (Equation 3),
we must first extract the numerous lexical pat-
terns from contexts in which those two words ap-
pear. For this purpose, we propose a pattern ex-
traction algorithm using snippets retrieved from
a web search engine. The proposed method re-
quires no language-dependent preprocessing such
as part-of-speech tagging or dependency parsing,
which can be both time consuming at Web scale,
and likely to produce incorrect results because of
the fragmented and ill-formed snippets.
Given two words a and b, we query a web search
engine using the wildcard query ?a * * * * * b?
and download snippets. The ?*? operator matches
one word or none in a web page. Therefore, our
wildcard query retrieves snippets in which a and
b appear within a window of seven words. We
attempt to approximate the local context of two
words using wildcard queries. For example, Fig-
ure 1 shows a snippet retrieved for the query ?os-
trich * * * * * bird?.
For a snippet S, retrieved for a word pair (a, b),
first, we replace the two words a and b, respec-
tively, with two variables X and Y. We replace all
numeric values by D, a marker for digits. Next,
we generate all subsequences of words from S that
satisfy all of the following conditions.
(i). A subsequence must contain exactly one oc-
currence of each X and Y
(ii). The maximum length of a subsequence is L
words.
(iii). A subsequence is allowed to have gaps. How-
ever, we do not allow gaps of more than g
number of words. Moreover, the total length
of all gaps in a subsequence should not ex-
ceed G words.
(iv). We expand all negation contractions in a con-
text. For example, didn?t is expanded to did
not. We do not skip the word not when gen-
erating subsequences. For example, this con-
dition ensures that from the snippet X is not a
Y, we do not produce the subsequence X is a
Y.
Finally, we count the frequency of all generated
subsequences and only use subsequences that oc-
cur more than N times as lexical patterns.
The parameters L, g, G and N are set exper-
imentally, as explained later in Section 6. It is
noteworthy that the proposed pattern extraction al-
gorithm considers all the words in a snippet, and
is not limited to extracting patterns only from the
mid-fix (i.e., the portion of text in a snippet that
appears between the queried words). Moreover,
the consideration of gaps enables us to capture re-
lations between distant words in a snippet. We use
a modified version of the prefixspan algorithm (Pei
et al, 2004) to generate subsequences from a text
snippet. Specifically, we use the constraints (ii)-
(iv) to prune the search space of candidate sub-
sequences. For example, if a subsequence has
reached the maximum length L, or contains the
maximum number of gaps G, then we will not ex-
tend it further. By pruning the search space, we
can speed up the pattern generation process. How-
ever, none of these modifications affect the accu-
racy of the proposed semantic similarity measure
because the modified version of the prefixspan al-
gorithm still generates the exact set of patterns that
we would obtain if we used the original prefixspan
algorithm (i.e. without pruning) and subsequently
remove patterns that violate the above mentioned
constraints. For example, some patterns extracted
form the snippet shown in Figure 1 are: X, a large
Y, X a flightless Y, and X, large Y lives.
3.2 Clustering Lexical Patterns
A semantic relation can be expressed using more
than one pattern. By grouping the semantically
related patterns, we can both reduce the model
complexity in Equation 2, and consider the depen-
dence among semantic relations in Equation 3. We
use the distributional hypothesis (Harris, 1954) to
find semantically related lexical patterns. The dis-
tributional hypothesis states that words that occur
in the same context have similar meanings. If two
lexical patterns are similarly distributed over a set
of word pairs, then from the distributional hypoth-
esis it follows that the two patterns must be similar.
We represent a pattern p by a vector p in which
807
the i-th element is the frequency f(a
i
, b
i
, p) of p in
a word pair (a
i
, b
i
). Given a set P of patterns and
a similarity threshold ?, Algorithm 1 returns clus-
ters of similar patterns. First, the function SORT
sorts the patterns in the descending order of their
total occurrences in all word pairs. The total oc-
currences of a pattern p is defined as ?(p), and is
given by,
?(p) =
?
(a,b)?W
f(a, b, p). (5)
Here, W is the set of word pairs. Then the outer
for-loop (starting at line 3), repeatedly takes a pat-
tern p
i
from the ordered set P , and in the inner for-
loop (starting at line 6), finds the cluster, c
?
(? C)
that is most similar to p
i
. Similarity between p
i
and the cluster centroid c
j
is computed using co-
sine similarity. The centroid vector c
j
of cluster c
j
is defined as the vector sum of all pattern vectors
for patterns in that cluster (i.e. c
j
=
?
p?c
j
p).
If the maximum similarity exceeds the threshold
?, we append p
i
to c
?
(line 14). Here, the op-
erator ? denotes vector addition. Otherwise, we
form a new cluster {p
i
} and append it to C, the
set of clusters. After all patterns are clustered,
we compute the (i, j) element of the inter-cluster
correlation matrix ? (Equation 3) as the inner-
product between the centroid vectors c
i
and c
j
of
the corresponding clusters i and j. The parame-
ter ? (? [0, 1]) determines the purity of the formed
clusters and is set experimentally in Section 5. Al-
gorithm 1 scales linearly with the number of pat-
terns. Moreover, sorting the patterns by their to-
tal word pair frequency prior to clustering ensures
that the final set of clusters contains the most com-
mon relations in the dataset.
4 Evaluation Procedure
Evaluating a semantic similarity measure is diffi-
cult because the notion of semantic similarity is
subjective. Miller-Charles (1998) dataset has been
frequently used to benchmark semantic similar-
ity measures. Miller-Charles dataset contains 30
word pairs rated by a group of 38 human subjects.
The word pairs are rated on a scale from 0 (no sim-
ilarity) to 4 (perfect synonymy). Because of the
omission of two word pairs in earlier versions of
WordNet, most researchers had used only 28 pairs
for evaluations. The degree of correlation between
the human ratings in the benchmark dataset and
the similarity scores produced by an automatic se-
mantic similarity measure, can be considered as a
Algorithm 1 Sequential pattern clustering algo-
rithm.
Input: patterns P = {p
1
, . . . ,p
n
}, threshold ?
Output: clusters C
1: SORT(P )
2: C ? {}
3: for pattern p
i
? P do
4: max ? ??
5: c
?
? null
6: for cluster c
j
? C do
7: sim ? cosine(p
i
, c
j
)
8: if sim > max then
9: max ? sim
10: c
?
? c
j
11: end if
12: end for
13: if max ? ? then
14: c
?
? c
?
? p
i
15: else
16: C ? C ? {p
i
}
17: end if
18: end for
19: return C
measurement of how well the semantic similarity
measure captures the notion of semantic similar-
ity held by humans. In addition to Miller-Charles
dataset we also evaluate on the WordSimilarity-
353 (Finkelstein et al, 2002) dataset. In con-
trast to Miller-Charles dataset which has only 30
word pairs, WordSimilarity-353 dataset contains
353 word pairs. Each pair has 13-16 human judg-
ments, which were averaged for each pair to pro-
duce a single relatedness score. Following the pre-
vious work, we use both Miller-Charles dataset
and WordSimilarity-353 dataset to evaluate the
proposed semantic similarity measure.
5 Computing Semantic Similarity
To extract lexical patterns that express numer-
ous semantic relations, we first select synonymous
words from WordNet synsets. A synset is a set
of synonymous words assigned for a particular
sense of a word in WordNet. We randomly select
2000 synsets of nouns from WordNet. From each
synset, a pair of synonymous words is selected.
For polysemous nouns, we selected synonyms
from the dominant sense. To perform a fair evalu-
ation, we do not select any words that appear in the
Miller-Charles dataset or the WordSimilarity-353
808
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 1.2
 1.3
 1.4
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
era
ge 
Sim
ilar
ity
Clustering Threshold
Figure 2: Average similarity vs. clustering thresh-
old ?
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Clu
ste
r S
par
sity
Clustering Threshold
Figure 3: Sparsity vs. clustering threshold ?
dataset, which are used later for evaluation pur-
poses. As we describe later, the clustering thresh-
old ? is tuned using this set of 2000 word pairs
selected from the WordNet.
We use the YahooBOSS API
2
and download
1000 snippets for each of those word pairs. Ex-
perimentally, we set the values for the parameters
in the pattern extraction algorithm (Section 3.1):
L = 5, g = 2, G = 4, and extract 5, 238, 637
unique patterns. However, only 1, 680, 914 of
those patterns occur more than twice. Low fre-
quency patterns often contain misspellings and are
not suitable for training. Therefore, we selected
patterns that occur at least 10 times in the snip-
pet collection. Moreover, we remove very long
patterns (ca. over 20 characters). The final set
contains 140, 691 unique lexical patterns. The re-
mainder of the experiments described in the paper
use those patterns.
2
http://developer.yahoo.com/search/boss/
We use the clustering Algorithm 1 to cluster the
extracted patterns. The only parameter in Algo-
rithm 1, the clustering threshold ?, is set as fol-
lows. We vary the value of theta ? from 0 to 1,
and use Algorithm 1 to cluster the extracted set
of patterns. We use the resultant set of clusters to
represent a word pair by a feature vector. We com-
pute a feature from each cluster as follows. First,
we assign a weight w
ij
to a pattern p
i
that is in a
cluster c
j
as follows,
w
ij
=
?(p
i
)
?
q?c
j
?(q)
. (6)
Here, ?(q) is the total frequency of a pattern, and it
is given by Equation 5. Because we perform a hard
clustering on patterns, a pattern can belong to only
one cluster (i.e. w
ij
= 0 for p
i
/? c
j
). Finally, we
compute the value of the j-th feature in the feature
vector for word pair (a, b) as follows,
?
p
i
?c
j
w
ij
f(a, b, p
i
). (7)
For each set of clusters, we compute the element
?
ij
of the corresponding inter-cluster correlation
matrix ? by the cosine similarity between the cen-
troid vectors for clusters c
i
and c
j
. The prototype
vector ? in Equation 3 is computed as the vector
sum of individual feature vectors for the synony-
mous word pairs selected from the WordNet as de-
scribed above. We then use Equation 3 to compute
the average of similarity scores for synonymous
word pairs we selected from WordNet.
We select the ? that maximizes the average
similarity score between those synonymous word
pairs. Formally, the optimal value of ?,
?
? is given
by the following Equation,
?
? = argmax
??[0,1]
(
1
|W |
?
(a,b)?W
sim(a, b)
)
. (8)
Here, W is the set of synonymous word pairs
(a, b), |W | is the total number of synonymous
word pairs (i.e. 2000 in our experiments), and
sim(a, b) is given by Equation 3. Because the av-
erages are taken over 2000 word pairs this proce-
dure gives a reliable estimate for ?. Moreover,
this method does not require negative training
instances such as, non-synonymous word pairs,
which are difficult to create manually. Average
similarity scores for various ? values are shown
in Figure 2. From Figure 2, we see that initially
average similarity increases when ? is increased.
809
This is because clustering of semantically related
patterns reduces the sparseness in feature vectors.
Average similarity is stable within a range of ? val-
ues between 0.5 and 0.7. However, increasing ?
beyond 0.7 results in a rapid drop of average sim-
ilarity. To explain this behavior consider Figure
3 where we plot the sparsity of the set of clusters
(i.e. the ratio between singletons to total clusters)
against threshold ?. As seen from Figure 3, high ?
values result in a high percentage of singletons be-
cause only highly similar patterns will form clus-
ters. Consequently, feature vectors for different
word pairs do not have many features in common.
The maximum average similarity score of 1.303 is
obtained with ? = 0.7, corresponding to 17, 015
total clusters out of which 12, 476 are singletons
with exactly one pattern (sparsity = 0.733). For
the remainder of the experiments in this paper we
set ? to this optimal value and use the correspond-
ing set of clusters to compute semantic similarity
by Equation 3. Similarity scores computed us-
ing Equation 3 can be greater than 1 (see Figure
2) because of the terms corresponding to the non-
diagonal elements in ?. We do not normalize the
similarity scores to [0, 1] range in our experiments
because the evaluation metrics we use are insensi-
tive to linear transformations of similarity scores.
6 Experiments
Table 1 compares the proposed method against
Miller-Charles ratings (MC), and previously pro-
posed web-based semantic similarity measures:
Jaccard, Dice, Overlap, PMI (Bollegala et al,
2007), Normalized Google Distance (NGD) (Cili-
brasi and Vitanyi, 2007), Sahami and Heil-
man (SH) (2006), co-occurrence double checking
model (CODC) (Chen et al, 2006), and support
vector machine-based (SVM) approach (Bollegala
et al, 2007). The bottom row of Table 1 shows the
Pearson correlation coefficient of similarity scores
produced by each algorithm with MC. All similar-
ity scores, except for the human-ratings in Miller-
Charles dataset, are normalized to [0, 1] range for
the ease of comparison. It is noteworthy that the
Pearson correlation coefficient is invariant under a
linear transformation. All similarity scores shown
in Table 1 except for the proposed method are
taken from the original published papers.
The highest correlation is reported by the pro-
posed semantic similarity measure. The improve-
ment of the proposed method is statistically sig-
nificant (confidence interval [0.73, 0.93]) against
all the similarity measures compared in Table 1
except against the SVM approach. From Table 1
we see that measures that use contextual informa-
tion from snippets (e.g. SH, CODC, SVM, and
proposed) outperform the ones that use only co-
occurrence statistics (e.g. Jaccard, overlap, Dice,
PMI, and NGD) such as page-counts. This is be-
cause similarity measures that use contextual in-
formation are better equipped to compute the sim-
ilarity between polysemous words. Although both
SVM and proposed methods use lexical patterns,
unlike the proposed method, the SVM method
does not consider the relatedness between pat-
terns. The superior performance of the proposed
method is attributable to its consideration of relat-
edness of patterns.
Table 2 summarizes the previously proposed
WordNet-based semantic similarity measures. De-
spite the fact that the proposed method does not
use manually compiled resources such as Word-
Net for computing similarity, its performance is
comparable to similarity measures that use Word-
Net. We believe that the proposed method will
be useful to compute the semantic similarity be-
tween named-entities for which manually created
resources are either incomplete or do not exist.
We evaluate the proposed method using the
WordSimilarity-353 dataset. Experimental re-
sults are presented in Table 3. Following pre-
vious work, we use Spearman rank correlation
coefficient, which does not require ratings to be
linearly dependent, for the evaluations on this
dataset. Likewise with the Miller-Charles ratings,
we measure the correlation between the similar-
ity scores produced by the proposed method for
word pairs in the WordSimilarity-353 dataset and
the human ratings. A higher Spearman correla-
tion coefficient (value=0.504, confidence interval
[0.422, 0.578]) indicates a better agreement with
the human notion of semantic similarity. From Ta-
ble 3 we can see that the proposed method outper-
forms a wide variety of semantic similarity mea-
sures developed using numerous resources includ-
ing lexical resources such as WordNet and knowl-
edge sources such as Wikipedia (i.e. WikiRe-
late!). In contrast to the Miller-Charles dataset
which only contains common English words se-
lected from the WordNet, the WordSimilarity-353
dataset contains word pairs where one or both
words are named entities (e.g. (Maradona, foot-
810
Table 1: Semantic similarity scores on Miller-Charles dataset
Word Pair MC Jaccrad Dice Overlap PMI NGD SH CODC SVM Proposed
automobile-car 3.920 0.650 0.664 0.831 0.427 0.466 0.225 0.008 0.980 0.918
journey-voyage 3.840 0.408 0.424 0.164 0.468 0.556 0.121 0.005 0.996 1.000
gem-jewel 3.840 0.287 0.300 0.075 0.688 0.566 0.052 0.012 0.686 0.817
boy-lad 3.760 0.177 0.186 0.593 0.632 0.456 0.109 0.000 0.974 0.958
coast-shore 3.700 0.783 0.794 0.510 0.561 0.603 0.089 0.006 0.945 0.975
asylum-madhouse 3.610 0.013 0.014 0.082 0.813 0.782 0.052 0.000 0.773 0.794
magician-wizard 3.500 0.287 0.301 0.370 0.863 0.572 0.057 0.008 1.000 0.997
midday-noon 3.420 0.096 0.101 0.116 0.586 0.687 0.069 0.010 0.819 0.987
furnace-stove 3.110 0.395 0.410 0.099 1.000 0.638 0.074 0.011 0.889 0.878
food-fruit 3.080 0.751 0.763 1.000 0.449 0.616 0.045 0.004 0.998 0.940
bird-cock 3.050 0.143 0.151 0.144 0.428 0.562 0.018 0.006 0.593 0.867
bird-crane 2.970 0.227 0.238 0.209 0.516 0.563 0.055 0.000 0.879 0.846
implement-tool 2.950 1.000 1.000 0.507 0.297 0.750 0.098 0.005 0.684 0.496
brother-monk 2.820 0.253 0.265 0.326 0.623 0.495 0.064 0.007 0.377 0.265
crane-implement 1.680 0.061 0.065 0.100 0.194 0.559 0.039 0.000 0.133 0.056
brother-lad 1.660 0.179 0.189 0.356 0.645 0.505 0.058 0.005 0.344 0.132
car-journey 1.160 0.438 0.454 0.365 0.205 0.410 0.047 0.004 0.286 0.165
monk-oracle 1.100 0.004 0.005 0.002 0.000 0.579 0.015 0.000 0.328 0.798
food-rooster 0.890 0.001 0.001 0.412 0.207 0.568 0.022 0.000 0.060 0.018
coast-hill 0.870 0.963 0.965 0.263 0.350 0.669 0.070 0.000 0.874 0.356
forest-graveyard 0.840 0.057 0.061 0.230 0.495 0.612 0.006 0.000 0.547 0.442
monk-slave 0.550 0.172 0.181 0.047 0.611 0.698 0.026 0.000 0.375 0.243
coast-forest 0.420 0.861 0.869 0.295 0.417 0.545 0.060 0.000 0.405 0.150
lad-wizard 0.420 0.062 0.065 0.050 0.426 0.657 0.038 0.000 0.220 0.231
cord-smile 0.130 0.092 0.097 0.015 0.208 0.460 0.025 0.000 0 0.006
glass-magician 0.110 0.107 0.113 0.396 0.598 0.488 0.037 0.000 0.180 0.050
rooster-voyage 0.080 0.000 0.000 0.000 0.228 0.487 0.049 0.000 0.017 0.052
noon-string 0.080 0.116 0.123 0.040 0.102 0.488 0.024 0.000 0.018 0.000
Correlation - 0.260 0.267 0.382 0.549 0.205 0.580 0.694 0.834 0.867
Table 2: Comparison with WordNet-based simi-
larity measures.
Method Correlation
Edge-counting 0.664
Jiang & Conrath (1998) 0.848
Lin (1998a) 0.822
Resnik (1995) 0.745
Li et al (2003) 0.891
ball) and (Jerusalem, Israel)). Because the pro-
posed method use snippets retrieved from a web
search engine, it is capable of extracting expres-
sive lexical patterns that can explicitly state the re-
lationship between two entities.
If we must compare n objects using a feature
model of similarity, then we only need to define
features for each of those n objects. However, in
the proposed relational model we must define re-
lations between all pairs of objects. In the case
where all n objects are different, this requires us to
define relations for n(n?1)/2 object pairs. Defin-
ing relations for all pairs can be computationally
costly for large n values. Efficiently comparing n
objects using a relational model is an interesting
future research direction of the current work.
Table 3: Results on WordSimilarity-353 dataset.
Method Correlation
WordNet Edges (Jarmasz, 1993) 0.27
Hirst & St-Onge (1997) 0.34
Jiang & Conrath (1998) 0.34
WikiRelate! (Strube and Ponzetto, 2006) 0.19-0.48
Leacock & Chodrow (1998) 0.36
Lin (1998b) 0.36
Resnik (1995) 0.37
Proposed 0.504
7 Conclusion
We proposed a relational model to measure the
semantic similarity between two words. First, to
represent the numerous semantic relations that ex-
ist between two words, we extract lexical patterns
from snippets retrieved from a web search engine.
Second, we cluster the extracted patterns to iden-
tify the semantically related patterns. Third, us-
ing the pattern clusters we define a feature vector
to represent two words and compute the semantic
similarity by taking into account the inter-cluster
correlation. The proposed method outperformed
all existing web-based semantic similarity mea-
sures on two benchmark datasets.
811
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proc. of ACL?99, pages 57?
64.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proc. of WWW?07, pages 757?
766.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proc. of the COLING/ACL ?06, pages 1009?1016.
R.L. Cilibrasi and P.M.B. Vitanyi. 2007. The google
similarity distance. IEEE Transactions on Knowl-
edge and Data Engineering, 19(3):370?383.
J. Curran. 2002. Ensemble menthods for automatic
thesaurus extraction. In Proc. of EMNLP.
B. Falkenhainer, K.D. Forbus, and D. Gentner. 1989.
Structure mapping engine: Algorithm and examples.
Artificial Intelligence, 41:1?63.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
TOIS, 20:116?131.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proc. of IJCAI?07, pages
1606?1611.
R. L. Goldstone. 1994. The role of similarity in cat-
egorization: providing a groundwork. Cognition,
52:125?157.
U. Hahn, N. Chater, and L. B. Richardson. 2003. Sim-
ilarity as transformation. Cognition, 87:1?32.
Z. Harris. 1954. Distributional structure. Word,
10:146?162.
M.A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of 14th
COLING, pages 539?545.
G. Hirst and D. St-Onge. 1997. Lexical chains as rep-
resentations of context for the detection and correc-
tion of malapropisms.
M. Jarmasz. 1993. Roget?s thesaurus as a lexical re-
source for natural language processing. Master?s
thesis, University of Ottawa.
J.J. Jiang and D.W. Conrath. 1998. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of ROCLING?98.
C. L. Krumhansl. 1978. Concerning the applicability
of geometric models to similarity data: The inter-
relationship between similarity and spatial density.
Psychological Review, 85:445?463.
C. Leacock and M. Chodorow. 1998. Combining Lo-
cal Context and WordNet Similarity for Word Sense
Identification. MIT.
M. Li, X. Chen, X. Li, B. Ma, and P.M.B. Vitanyi.
2004. The similarity metric. IEEE Transactions on
Information Theory, 50(12):3250?3264.
D. Lin. 1998a. Automatic retreival and clustering of
similar words. In Proc. of the 17th COLING, pages
768?774.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proc. of the 15th ICML, pages 296?
304.
G. Miller and W. Charles. 1998. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
J. Pei, J. Han, B. Mortazavi-Asi, J. Wang, H. Pinto,
Q. Chen, U. Dayal, and M. Hsu. 2004. Mining se-
quential patterns by pattern-growth: the prefixspan
approach. IEEE Transactions on Knowledge and
Data Engineering, 16(11):1424?1440.
R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989.
Development and application of a metric on seman-
tic nets. IEEE Transactions on Systems, Man and
Cybernetics, 9(1):17?30.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of
IJCAI?95.
M. Sahami and T. Heilman. 2006. A web-based kernel
function for measuring the similarity of short text
snippets. In Proc. of WWW?06.
V. Schickel-Zuber and B. Faltings. 2007. Oss: A se-
mantic similarity function based on hierarchical on-
tologies. In Proc. of IJCAI?07, pages 551?556.
M. Strube and S. P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using wikipedia. In
Proc. of AAAI? 06.
J. B. Tenenbaum. 1999. Bayesian modeling of human
concept learning. In NIPS?99.
A. Tversky. 1977. Features of similarity. Psychologi-
cal Review, 84:327?652.
D. McLean Y. Li, Zuhair A. Bandar. 2003. An ap-
proch for measuring semantic similarity between
words using multiple information sources. IEEE
Transactions on Knowledge and Data Engineering,
15(4):871?882.
812
A Machine Learning Approach to Sentence
Ordering for Multidocument Summarization
and Its Evaluation
Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka
University of Tokyo, Japan
Abstract. Ordering information is a difficult but a important task for
natural language generation applications. A wrong order of information
not only makes it difficult to understand, but also conveys an entirely
different idea to the reader. This paper proposes an algorithm that learns
orderings from a set of human ordered texts. Our model consists of a set
of ordering experts. Each expert gives its precedence preference between
two sentences. We combine these preferences and order sentences. We
also propose two new metrics for the evaluation of sentence orderings.
Our experimental results show that the proposed algorithm outperforms
the existing methods in all evaluation metrics.
1 Introduction
The task of ordering sentences arises in many fields. Multidocument Summa-
rization (MDS) [5], Question and Answer (QA) systems and concept to text
generation systems are some of them. These systems extract information from
different sources and combine them to produce a coherent text. Proper ordering
of sentences improves readability of a summary [1]. In most cases it is a trivial
task for a human to read a set of sentences and order them coherently. Hu-
mans use their wide background knowledge and experience to decide the order
among sentences. However, it is not an easy task for computers. This paper pro-
poses a sentence ordering algorithm and evaluate its performance with regard
to MDS.
MDS is the task of generating a human readable summary from a given set of
documents. With the increasing amount of texts available in electronic format,
automatic text summarization has become necessary. It can be considered as a
two-stage process. In the first stage the source documments are analyzed and a
set of sentences are extracted. However, the document set may contain repeating
information as well as contradictory information and these challenges should
be considered when extracting sentences for the summary. Researchers have
already investigated this problem and various algorithms exist. The second stage
of MDS creates a coherent summary from this extract. When summarizing a
single document, a naive strategy that arranges extracted sentences according
to the appearance order may yield a coherent summary. However, in MDS the
extracted sentences belong to different source documents. The source documents
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 624?635, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
A Machine Learning Approach to Sentence Ordering 625
may have been written by various authors and on various dates. Therefore we
cannot simply order the sentences according to the position of the sentences in
the original document to get a comprehensible summary.
This second stage of MDS has received lesser attention compared to the
first stage. Chronological ordering; ordering sentences according to the pub-
lished date of the documents they belong to [6], is one solution to this problem.
However, showing that this approach is insufficient, Barzilay [1] proposed an
refined algorithm which integrates chronology ordering with topical relatedness
of documents. Okazaki [7] proposes a improved chronological ordering algorithm
using precedence relations among sentences. His algorithm searches for an order
which satisfies the precedence relations among sentences. In addition to these
studies which make use of chronological ordering, Lapata [3] proposes a prob-
abilistic model of text structuring and its application to the sentence ordering.
Her system calculates the conditional probabilities between sentences from a
corpus and uses a greedy ordering algorithm to arrange sentences according to
the conditional probabilities.
Even though these previous studies proposed different strategies to decide the
sentence ordering, the appropriate way to combine these different methods to
obtain more robust and coherent text remains unknown. In addition to these ex-
isting sentence ordering heuristics, we propose a new method which we shall call
succession in this paper. We then learn the optimum linear combination of these
heuristics that maximises readability of a summary using a set of human-made
orderings. We then propose two new metrics for evaluating sentence orderings;
Weighted Kendall Coefficient and Average Continuity. Comparing with an in-
trinsic evaluation made by human subjects, we perform a quantitative evaluation
using a number of metrics and discuss the possiblity of the automatic evaluation
of sentence orderings.
2 Method
For sentences taken from the same document we keep the order in that docu-
ment as done in single document summarization. However, we have to be careful
when ordering sentences which belong to different documents. To decide the or-
der among such sentences, we implement five ranking experts: Chronological,
Probabilistic, Topical relevance, Precedent and Succedent. These experts return
precedence preference between two sentences. Cohen [2] proposes an elegant
learning model that works with preference functions and we adopt this learn-
ing model to our task. Each expert e generates a pair-wise preference function
defined as following:
PREFe(u, v, Q) ? [0, 1]. (1)
Where, u, v are two sentences that we want to order; Q is the set of sentences
which has been already ordered. The expert returns its preference of u to v. If
the expert prefers u to v then it returns a value greater than 0.5. In the extreme
case where the expert is absolutely sure of preferring u to v it will return 1.0.
On the other hand, if the expert prefers v to u it will return a value lesser than
626 D. Bollegala, N. Okazaki, and M. Ishizuka
0.5. In the extreme case where the expert is absolutely sure of preferring v to u
it will return 0. When the expert is undecided of its preference between u and v
it will return 0.5.
The linear weighted sum of these individual preference functions is taken as
the total preference by the set of experts as follows:
PREFtotal(u, v, Q) =
?
e?E
wePREFe(u, v, Q). (2)
Therein: E is the set of experts and we is the weight associated to expert e ? E.
These weights are normalized so that the sum of them is 1. We use the Hedge
learning algorithm to learn the weights associated with each expert?s preference
function. Then we use the greedy algorithm proposed by Cohen [2] to get an
ordering that approximates the total preference.
2.1 Chronological Expert
Chronological expert emulates conventional chronological ordering [4,6] which
arranges sentences according to the dates on which the documents were published
and preserves the appearance order for sentences in the same document. We
define a preference function for the expert as follows:
PREFchro(u, v, Q) =
?
?
?
?
?
1 T (u) < T (v)
1 [D(u) = D(v)] ? [N(u) < N(v)]
0.5 [T (u) = T (v)] ? [D(u) = D(v)]
0 otherwise
. (3)
Therein: T (u) is the publication date of sentence u; D(u) presents the unique
identifier of the document to which sentence u belongs; N(u) denotes the line
number of sentence u in the original document. Chronological expert gives 1
(preference) to the newly published sentence over the old and to the prior over
the posterior in the same article. Chronological expert returns 0.5 (undecided)
when comparing two sentences which are not in the same article but have the
same publication date.
2.2 Probabilistic Expert
Lapata [3] proposes a probabilistic model to predict sentence order. Her model
assumes that the position of a sentence in the summary depends only upon the
sentences preceding it. For example let us consider a summary T which has
sentences S1, . . . , Sn in that order. The probability P (T ) of getting this order is
given by:
P (T ) =
n
?
i=1
P (Sn|S1, . . . , Sn?i). (4)
She further reduces this probability using bi-gram approximation as follows.
P (T ) =
n
?
i=1
P (Si|Si?1) (5)
A Machine Learning Approach to Sentence Ordering 627
She breaks each sentence into features and takes the vector product of features
as follows:
P (Si|Si?1) =
?
(a<i,j>,a<i?1,k>)?Si?Si?1
P (a<i,j>, a<i?1,k>). (6)
Feature conditional probabilities can be calculated using frequency counts of
features as follows:
P (a<i,j>|a<i?1,k>) =
f(a<i,j>, a<i?1,k>)
?
a<i,j>
f(a<i,j>, a<i?1,k>)
. (7)
Lapata [3] uses nouns,verbs and dependency structures as features. Where as
in our expert we implemented only nouns and verbs as features. We performed
back-off smoothing on the frequency counts in equation 7 as these values were
sparse. Once these conditional probabilities are calculated, for two sentences u,v
we can define the preference function for the probabilistic expert as follows:
PREFprob(u, v, Q) =
{
1+P (u|r)?P (v|r)
2 Q = 
1+P (u)?P (v)
2 Q = 
. (8)
Where, Q is the set of sentences ordered so far and r ? Q is the lastly ordered
sentence in Q. Initially, Q is null and we prefer the sentence with higher absolute
probability. When Q is not null and u is preferred to v, i.e. P (u|r) > P (v|r),
according to definition 8 a preference value greater than 0.5 is returned. If v is
preferred to u, i.e. P (u|r) < P (v|r), we have a preference value smaller than 0.5.
When P (u|r) = P (v|r), the expert is undecided and it gives the value 0.5.
2.3 Topical Relevance Expert
In MDS, the source documents could contain multiple topics. Therefore, the
extracted sentences could be covering different topics. Grouping the extracted
sentences which belong to the same topic, improves readability of the summary.
Motivated by this fact, we designed an expert which groups the sentences which
belong to the same topic. This expert prefers sentences which are more similar
to the ones that have been already ordered. For each sentence l in the extract
we define its topical relevance, topic(l) as follows:
topic(l) = max
q?Q
sim(l, q). (9)
We use cosine similarity to calculate sim(l, q). The preference function of this
expert is defined as follows:
PREFtopic(u, v, Q) =
{
0.5 [Q = ] ? [topic(u) = topic(v)]
1 [Q = ] ? [topic(u) > topic(v)]
0 otherwise
. (10)
Where,  represents the null set, u,v are the two sentences under considera-
tion and Q is the block of sentences that has been already ordered so far in
the summary.
628 D. Bollegala, N. Okazaki, and M. Ishizuka
3
7PQTFGTGF
5GPVGPEGU

.5WOOCT[
Fig. 1. Topical relevance expert
2.4 Precedent Expert
When placing a sentence in the summary it is important to check whether the
preceding sentences convey the necessary background information for this sen-
tence to be clearly understood. Placing a sentence without its context being
stated in advanced, makes an unintelligible summary. As shown in figure 2, for
each extracted sentence l, we can compare the block of text that appears before
it in its source document (P ) with the block of sentences which we have ordered
so far in the summary (Q). If P and Q matches well, then we can safely as-
sume that Q contains the necessary background information required by l. We
can then place l after Q. Such relations among sentences are called precedence
relations. Okazaki [7] proposes precedence relations as a method to improve the
chronological ordering of sentences. He considers the information stated in the
documents preceding the extracted sentences to judge the order. Based on this
idea, we define precedence pre(l) of the extracted sentence l as follows:
pre(l) = max
p?P,q?Q
sim(p, q). (11)
l
2
&QEWOGPV
&5WOOCT[
3
Fig. 2. Precedent expert
Here, P is the set of sentences preceding the extract sentence l in the original
document. We calculate sim(p, q) using cosine similarity. The preference function
for this expert can be written as follows:
PREFpre(u, v, Q) =
{
0.5 [Q = ] ? [pre(u) = pre(v)]
1 [Q = ] ? [pre(u) > pre(v)]
0 otherwise
. (12)
A Machine Learning Approach to Sentence Ordering 629
&QEWOGPV

&
5WOOCT[
3
r
-
7PQTFGTGF
5GPVGPEGU

.
l
Fig. 3. Succedent expert
2.5 Succedent Expert
When extracting sentences from source documents, sentences which are similar to
the ones that are already extracted, are usually ignored to prevent repetition of
information. However, this information is valuable when ordering sentences. For
example, a sentence that was ignored by the sentence extraction algorithm might
turn out to be more suitable when ordering the extracted sentences. However, we
assume that the sentence ordering algorithm is independent from the sentence ex-
traction algorithmand therefore does not possess this knowledge regarding the left
out candidates. This assumption improves the compatibility of our algorithm as it
can be used to order sentences extracted by any sentence extraction algorithm. We
design an expert which uses this information to order sentences.
Let us consider the siuation depicted in Figure 3 where a block Q of text is
orderd in the summary so far. The lastly ordered setence r belongs to document
D in which a block K of sentences follows r. The author of this document assumes
that K is a natural consequence of r. However, the sentence selection algorithm
might not have selected any sentences from K because it already selected some
sentences with this information from some other document. Therefore, we search
the extract L for a sentence that best matches with a sentence in K. We define
succession as a measure of this agreement(13) as follows:
succ(l) = max
k?K
sim(l, k). (13)
Here, we calculate sim(l, k) using cosine similarity. Sentences with higher succes-
sion values are preferred by the expert. The preference function for this expert
can be written as follows:
PREFsucc(u, v, Q) =
{
0.5 [Q = ] ? [succ(u) = succ(v)]
1 [Q = ] ? [succ(u) > succ(v)]
0 otherwise
. (14)
2.6 Ordering Algorithm
Using the five preference functions described in the previous sections, we compute
the total preference function of the set of experts as defined by equation 2. Sec-
tion 2.7 explains the method that we use to calculate the weights assigned to each
expert?s preference. In this section we will consider the problem of finding an order
that satisfies the total preference function. Finding the optimal order for a given
630 D. Bollegala, N. Okazaki, and M. Ishizuka
total preference function is NP-complete [2]. However, Cohen [2] proposes a greedy
algorithm that approximates the optimal ordering. Once the unordered extract X
and total preference (equation 2) are given, this greedy algorithm can be used to
generate an approximately optimal ordering function ??.
let V = X
for each v ? V do
?(v) =
?
u?V
PREF(v, u, Q) ?
?
u?V
PREF(u, v, Q)
while V is non-empty do
let t = arg maxu?V ?(u)
let ??(t) = |V |
V = V ? {t}
for each v ? V do
?(v) = ?(v) + PREF(t, u) ? PREF(v, t)
endwhile
2.7 Learning Algorithm
Cohen [2] proposes a weight allocation algorithm that learns the weights associ-
ated with each expert in equation 2. We shall explain this algorithm in regard
to our model of five experts.
Rate of learning ? ? [0, 1], initial weight vector w1 ? [0, 1]5, s.t.
?
e?E w
1
e = 1.
Do for t = 1, 2, . . . , T where T is the number of training examples.
1. Get Xt; the set of sentences to be ordered.
2. Compute a total order ??t which approximates,
PREFttotal(u, v, Q) =
?
e?E
PREFte(u, v, Q).
We used the greedy ordering algorithm described in section 2.6 to get ??t.
3. Order Xt using ??t.
4. Get the human ordered set F t of Xt. Calculate the loss for each expert.
Loss(PREFte, F
t) = 1 ? 1|F |
?
(u,v)?F
PREFte(u, v, Q) (15)
5. Set the new weight vector,
wt+1e =
wte?
Loss(PREFte,F
t)
Zt
(16)
where, Zt is a normalization constant, chosen so that,
?
e?E w
t+1
e = 1.
A Machine Learning Approach to Sentence Ordering 631
In our experiments we set ? = 0.5 and w1i = 0.2. To explain equation 15 let us
assume that sentence u comes before sentence v in the human ordered summary.
Then the expert must return the value 1 for PREF(u,v,Q). However,if the expert
returns any value less than 1, then the difference is taken as the loss. We do this
for all such sentence pairs in F . For a summary of length N we have N(N ?1)/2
such pairs. Since this loss is taken to the power of ?, a value smaller than 1, the
new weight of the expert gets changed according to the loss as in equation 16.
3 Evaluation
In addition to Kendall?s ? coefficient and Spearman?s rank correlation coefficient
which are widely used for comparing two ranks, we use sentence continuity [7]
as well as two metrics we propose; Weighted Kendall and Average Continuity.
3.1 Weighted Kendall Coefficient
The Kendall?s ? coefficient is defined as following:
? = 1 ? 2Q
nC2
. (17)
Where, Q is the number of discordant pairs and nC2 is the number of combi-
nations that can be generated from a set of n distinct elements by taking two
elements at a time with replacement. However, one major drawback of this met-
ric when evaluating sentence orderings is that, it does not take into consideration
the relative distance d between the discordant pairs. However, when reading a
text a human reader is likely to be more sensitive to a closer discordant pair than
a discordant pair far apart. Therefore, a closer discordant pair is more likely to
harm the readability of the summary compared to a far apart discordant pair. In
order to reflect these differences in our metric, we use an exponentially decreasing
weight function as follows:
h(d) =
{
exp(1 ? d) d ? 1
0 else
. (18)
Here, d is the number of sentences that lie between the two sentences of the
discordant pair. Going by the traditional Kendall?s ? coefficient we defined our
weighted Kendall coefficient as following, so that it becomes a metric in [1, ?1]
range.
?w = 1 ?
2
?
d h(d)
?n
i=1 h(i)
(19)
3.2 Average Continuity
Both Kendall?s ? coefficient and the Weighted Kendall coefficient measure dis-
cordants between ranks. However, in the case of summaries, we need a metric
which expresses the continuity of the sentences. A summary which can be read
632 D. Bollegala, N. Okazaki, and M. Ishizuka
continuously is better compared to a one that cannot. If the ordered extract
contains most of the sentence blocks of the reference summary then we can
safely assume that it is far more readable and coherent to a one that is not.
Sentence n-gram counts of continuous sentences give a rough idea of this kind
of continuity.
For a summary of length N there are N ? n + 1 possible sentence n-grams
of length n. Therefore, we can define a precision Pn of continuity length n as:
Pn =
number of matched n-grams
N ? n + 1 . (20)
Due to sparseness of higher order n-grams Pn decreases in an exponential-like
curve with n. Therefore, we define Average Continuity as the logrithmic average
of Pn as follows:
Average Continuity = exp(
1
3
4
?
n=2
log(Pn)) (21)
We add a small quantity ? to numerator and denominator of Pn in equation
20 so that the logarithm will not diverge when n-grams count is zero. We used
? = 0.01 in our evaluations. Experimental results showed that taking n-grams up
to four gave contrasting results because the n-grams tend to be sparse for larger
n values. BLEU(BiLingual Evaluation Understudy) proposed by Papineni [8]
for the task of evaluating machine translations has an analogical form to our
average continuity. In BLEU, a machine translation is compared against multiple
reference translations and precision values are calculated using word n-grams.
BLEU is then defined as the logarithmic average of these precision values.
4 Results
We used the 3rd Text Summarization Challenge (TSC) corpus for our exper-
iments. TSC1 corpus contains news articles taken from two leading Japanese
newspapers; Mainichi and Yomiuri. TSC-3 corpus contains human selected ex-
tracts for 30 different topics. However, in the TSC corpus the extracted sentences
are not ordered to make a readable summary. Therefore, we first prepared 30
summaries by ordering the extraction data of TSC-3 corpus by hand. We then
compared the orderings by the proposed algorithm against these human ordered
summaries. We used 10-fold cross validation to learn the weights assigned to
each expert in our proposed algorithm. These weights are shown in table 1.
According to table 1, succedent, chronology and precedent experts have the
highest weights among the five experts and therefore almost entirely control the
process of ordering. Whereas probabilistic and topical relevance experts have
almost no influence on their decisions. However, we cannot directly compare La-
pata?s [3] approach with our probabilistic expert as we do not use dependency
1 http://lr-www.pi.titech.ac.jp/tsc/index-en.html
A Machine Learning Approach to Sentence Ordering 633
Table 1. Weights learned
Expert Chronological Probabilistic Topical Relevance Precedent Succedent
Weights 0.327947 0.000039 0.016287 0.196562 0.444102
Table 2. Comparison with Human Ordering
Spearman Kendall Continuity Weighted Kendall Average Continuity
RO -0.267 -0.160 -0.118 -0.003 0.024
PO 0.062 0.040 0.187 0.013 0.029
CO 0.774 0.735 0.629 0.688 0.511
LO 0.783 0.746 0.706 0.717 0.546
HO 1.000 1.000 1.000 1.000 1.000








Identifying Sections in Scientific Abstracts using Conditional Random Fields
Kenji Hirohata?
hirohata@nii.ac.jp
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Sophia Ananiadou?
sophia.ananiadou@manchester.ac.uk
?Graduate School of Information
Science and Technology,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656, Japan
Mitsuru Ishizuka?
ishizuka@i.u-tokyo.ac.jp
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre,
131 Princess Street, Manchester M1 7DN, UK
Abstract
OBJECTIVE: The prior knowledge about
the rhetorical structure of scientific abstracts
is useful for various text-mining tasks such
as information extraction, information re-
trieval, and automatic summarization. This
paper presents a novel approach to cate-
gorize sentences in scientific abstracts into
four sections, objective, methods, results,
and conclusions. METHOD: Formalizing
the categorization task as a sequential label-
ing problem, we employ Conditional Ran-
dom Fields (CRFs) to annotate section la-
bels into abstract sentences. The train-
ing corpus is acquired automatically from
Medline abstracts. RESULTS: The pro-
posed method outperformed the previous
approaches, achieving 95.5% per-sentence
accuracy and 68.8% per-abstract accuracy.
CONCLUSION: The experimental results
showed that CRFs could model the rhetor-
ical structure of abstracts more suitably.
1 Introduction
Scientific abstracts are prone to share a similar
rhetorical structure. For example, an abstract usu-
ally begins with the description of background in-
formation, and is followed by the target problem,
solution to the problem, evaluation of the solution,
and conclusion of the paper. Previous studies ob-
served the typical move of rhetorical roles in sci-
entific abstracts: problem, solution, evaluation, and
conclusion (Graetz, 1985; Salanger-Meyer, 1990;
Swales, 1990; Ora?san, 2001). The American Na-
tional Standard Institute (ANSI) recommends au-
thors and editors of abstracts to state the purpose,
methods, results, and conclusions presented in the
documents (ANSI, 1979).
The prior knowledge about the rhetorical structure
of abstracts is useful to improve the performance of
various text-mining tasks. Marcu (1999) proposed
an extraction method for summarization that cap-
tured the flow of text, based on Rhetorical Struc-
ture Theory (RST). Some extraction methods make
use of cue phrases (e.g., ?in conclusion?, ?our in-
vestigation has shown that ...?), which suggest that
the rhetorical role of sentences is to identify im-
portant sentences (Edmundson, 1969; Paice, 1981).
We can survey the problems, purposes, motivations,
and previous approaches of a research field by read-
ing texts in background sections of scientific papers.
Tbahriti (2006) improved the performance of their
information retrieval engine, giving more weight to
sentences referring to purpose and conclusion.
In this paper, we present a supervised machine-
learning approach that categorizes sentences in sci-
entific abstracts into four sections, objective, meth-
ods, results, and conclusions. Figure 1 illustrates
the task of this study. Given an unstructured ab-
stract without section labels indicated by boldface
type, the proposed method annotates section labels
of each sentence. Assuming that this task is well
formalized as a sequential labeling problem, we use
Conditional Random Fields (CRFs) (Lafferty et al,
2001) to identify rhetorical roles in scientific ab-
stracts.The proposed method outperforms previous
approaches to this problem, achieving 95.5% per-
381
OBJECTIVE: This study assessed the role of adrenergic signal transmission in the control of renal erythropoietin (EPO) pro-
duction in humans. METHODS: Forty-six healthy male volunteers underwent a hemorrhage of 750 ml. After phlebotomy, they
received (intravenously for 6 hours in a parallel, randomized, placebo-controlled and single-blind design) either placebo (0.9%
sodium chloride), or the beta 2-adrenergic receptor agonist fenoterol (1.5 microgram/min), or the beta 1-adrenergic receptor ago-
nist dobutamine (5 micrograms/kg/min), or the nonselective beta-adrenergic receptor antagonist propranolol (loading dose of 0.14
mg/kg over 20 minutes, followed by 0.63 micrograms/kg/min). RESULTS: The AUCEPO(0-48 hr)fenoterol was 37% higher (p ?
0.03) than AUCEPO(0-48 hr)placebo, whereas AUCEPO(0-48 hr)dobutamine and AUCEPO(0-48 hr)propranolol were comparable
with placebo. Creatinine clearance was significantly increased during dobutamine treatment. Urinary cyclic adenosine monophos-
phate excretion was increased only by fenoterol treatment, whereas serum potassium levels were decreased. Plasma renin activity
was significantly increased during dobutamine and fenoterol infusion. CONCLUSIONS: This study shows in a model of con-
trolled, physiologic stimulation of renal erythropoietin production that the beta 2-adrenergic receptor agonist fenoterol but not the
beta 1-adrenergic receptor agonist dobutamine is able to increase erythropoietin levels in humans. The result can be interpreted as
a hint that signals for the control of erythropoietin production may be mediated by beta 2-adrenergic receptors rather than by beta
1-adrenergic receptors. It appears to be unlikely that an increase of renin concentrations or glomerular filtration rate is causally
linked to the control of erythropoietin production in this experimental setting.
Figure 1: An abstract with section labels indicated by boldface type (Gleiter et al, 1997).
sentence accuracy and 68.8% per-abstract accuracy.
This paper is organized as follows. Section 2
describes previous approaches to this task. For-
malizing the task as a sequential-labeling problem,
Section 3 designs a sentence classifier using CRFs.
Training corpora for the classifier are acquired au-
tomatically from the Medline abstracts. Section 4
reports considerable improvements in the proposed
method over the baseline method using Support Vec-
tor Machine (SVM) (Cortes and Vapnik, 1995). We
conclude this paper in Section 5.
2 Related Work
The previous studies regarded the task of identify-
ing section names as a text-classification problem
that determines a label (section name) for each sen-
tence. Various classifiers for text categorization,
Na??ve Bayesian Model (NBM) (Teufel and Moens,
2002; Ruch et al, 2007), Hidden Markov Model
(HMM) (Wu et al, 2006; Lin et al, 2006), and Sup-
port Vector Machines (SVM) (McKnight and Arini-
vasan, 2003; Shimbo et al, 2003; Ito et al, 2004;
Yamamoto and Takagi, 2005) were applied.
Table 1 summarizes these approaches and perfor-
mances. All studies target scientific abstracts except
for Teufel and Moens (2002) who target scientific
full papers. Field classes show the set of section
names that each study assumes: background (B),
objective/aim/purpose (O), method (M), result (R),
conclusion (C), and introduction (I) that combines
the background and objective. Although we should
not compare directly the performances of these stud-
ies, which use a different set of classification labels
and evaluation corpora, SVM classifiers appear to
yield better results for this task. The rest of this sec-
tion elaborates on the previous studies with SVMs.
Shimbo et al (2003) presented an advanced text
retrieval system for Medline that can focus on a
specific section in abstracts specified by a user.
The system classifies sentences in each Medline ab-
stract into four sections, objective, method, results,
and conclusion. Each sentence is represented by
words, word bigrams, and contextual information of
the sentence (e.g., class of the previous sentence,
relative location of the current sentence). They
reported 91.9% accuracy (per-sentence basis) and
51.2% accuracy (per-abstract basis1) for the clas-
sification with the best feature set for quadratic
SVM. Ito et al (2004) extended the work with a
semi-supervised learning technique using transduc-
tive SVM (TSVM).
Yamamoto and Takagi (2005) developed a sys-
tem to classify abstract sentences into five sections,
background, purpose, method, result, and conclu-
sion. They trained a linear-SVM classifier with fea-
tures such as unigram, subject-verb, verb tense, rel-
ative sentence location, and sentence score (average
TF*IDF score of constituent words). Their method
achieved 68.9%, 63.0%, 83.6%, 87.2%, 89.8% F-
scores for classifying background, purpose, method,
result, and conclusion sentences respectively. They
also reported the classification performance of intro-
duction sentences, which combines background and
purpose sentences, with 91.3% F-score.
1An abstract is considered correct if all constituent sentences
are correctly labeled.
382
Methods Model Classes Performance (reported in papers)
Teufel and Moens (2002) NBM (7 classes) 44% precision and 65% recall for aim sentences
Ruch et al (2007) NBM O M R C 85% F-score for conclusion sentences
Wu et al (2006) HMM B O M R C 80.54% precision
Lin et al (2006) HMM I M R C 88.5%, 84.3%, 89.8%, 89.7% F-scores
McKnight and Srinivasan (2003) SVM I M R C 89.2%, 82.0%, 82.1%, 89.5% F-scores
Shimbo et al (2003) SVM B O M R C 91.9% accuracy
Ito et al (2004) TSVM B O M R C 66.0%, 51.0%, 49.3%, 72.9%, 67.7% F-scores
Yamamoto and Takagi (2005) SVM I (B O) M R C 91.3% (68.9%, 63.0%), 83.6%, 87.2%, 89.8% F-scores
Table 1: Approaches and performances of previous studies on section identification
3 Proposed method
3.1 Section identification as a sequence labeling
problem
The previous work saw the task of labeling as a text
categorization that determines the class label yi for
each sentence xi. Even though some work includes
features of the surrounding sentences for xi, e.g.
?class label of xi?1 sentence,? ?class label of xi+1
sentence,? and ?unigram in xi?1 sentence,? the clas-
sifier determines the class label yi for each sentence
xi independently. It has been an assumption for text
classification tasks to decide a class label indepen-
dently of other class labels.
However, as described in Section 1, scientific ab-
stracts have typical moves of rhetorical roles: it
would be very peculiar if result sentences appear-
ing before method sentences were described in an
abstract. Moreover, we would like to model the
structure of abstract sentences rather than model-
ing just the section label for each sentence. Thus,
the task is more suitably formalized as a sequence
labeling problem: given an abstract with sentences
x = (x1, ..., xn), determine the optimal sequence of
section names y = (y1, ..., yn) of all possible se-
quences.
Conditional Random Fields (CRFs) have been
successfully applied to various NLP tasks includ-
ing part-of-speech tagging (Lafferty et al, 2001) and
shallow parsing (Sha and Pereira, 2003). CRFs de-
fine a conditional probability distribution p(y|x) for
output and input sequences, y and x,
p(y|x) =
1
Z?(x)
exp {? ? F (y,x)} . (1)
Therein: function F (y,x) denotes a global feature
vector for input sequence x and output sequence y,
F (y,x) =
?
i
f(y,x, i), (2)
i ranges over the input sequence, function f(y,x, i)
is a feature vector for input sequence x and output
sequence y at position i (based on state features and
transition features), ? is a vector where an element
?k represents the weight of feature Fk(y,x), and
Z?(x) is a normalization factor,
Z?(x) =
?
y
exp {? ? F (y,x)} . (3)
The optimal output sequence y? for an input se-
quence x,
y? = argmax
y
p(y|x), (4)
is obtained efficiently by the Viterbi algorithm. The
optimal set of parameters ? is determined efficiently
by the Generalized Iterative Scaling (GIS) (Darroch
and Ratcliff, 1972) or Limited-memory Broyden-
Fletcher-Goldfarb-Shanno (L-BFGS) (Nocedal and
Wright, 1999) method.
3.2 Features
We design three kinds of features to represent each
abstract sentence for CRFs. The contributions of
these features will be evaluated later in Section 4.
Content (n-gram) This feature examines the exis-
tence of expressions that characterize a specific sec-
tion, e.g. ?to determine ...,? and ?aim at ...? for stat-
ing the objective of a study. We use features for sen-
tence contents represented by: i) words, ii) word bi-
grams, and iii) mixture of words and word bigrams.
Words are normalized into their base forms by the
GENIA tagger (Tsuruoka and Tsujii, 2005), which
is a part-of-speech tagger trained for the biomedical
383
Rank OBJECTIVE METHOD RESULTS CONCLUSIONS
1 # to be measure % ) suggest that
2 be to be perform ( p may be
3 to determine n = p < # these
4 study be be compare ) . should be
5 this study be determine % . these result
Table 2: Bigram features with high ?2 values (?#? stands for a beginning of a sentence).
domain. We measure the co-occurrence strength (?2
value) between each feature and section label. If a
feature appears selectively in a specific section, the
?2 value is expected to be high. Thus, we extract the
top 200,000 features2 that have high ?2 values to re-
duce the total number of features. Table 3.2 shows
examples of the top five bigrams that have high ?2
values.
Relative sentence location An abstract is likely to
state objective of the study at the beginning and its
conclusion at the end. The position of a sentence
may be a good clue for determining its section la-
bel. Thus, we design five binary features to indicate
relative position of sentences in five scales.
Features from previous/next w sentences This
reproduces features from previous and following w
sentences to the current sentence (w = {0, 1, 2}),
so that a classifier can make use of the content of
the surrounding sentences. Duplicated features have
prefixes (e.g. PREV_ and NEXT_) to distinguish
their origins.
3.3 Section labels
It would require much effort and time to prepare a
large amount of abstracts annotated with section la-
bels. Fortunately, some Medline abstracts have sec-
tion labels stated explicitly by its authors. We ex-
amined section labels in 7,811,582 abstracts in the
whole Medline3, using the regular-expression pat-
tern:
?[A-Z]+([ ][A-Z]+){0,3}:[ ]
A sentence is qualified to have a section name if it
begins with up to 4 uppercase token(s) followed by
2We chose the number of features based on exploratory ex-
periments.
3The Medline database was up-to-date on March 2006.
a colon ?:?. This pattern identified 683,207 (ca. 9%)
abstracts with structured sections.
Table 3 shows typical moves of sections in Med-
line abstracts. The majority of sequences in this
table consists of four sections compatible with the
ANSI standard, purpose, methods, results, and con-
clusions. Moreover, the most frequent sequence
is ?OBJECTIVE ? METHOD(S) ? RESULTS
? CONCLUSION(S),? supposing that AIM and
PURPOSE are equivalent to OBJECTIVE. Hence,
this study assumes four sections, OBJECTIVE,
METHOD, RESULTS, and CONCLUSIONS.
Meanwhile, it is common for NP chunking tasks
to represent a chunk (e.g., NP) with two labels,
the begin (e.g., B-NP) and inside (e.g., I-NP) of
a chunk (Ramshaw and Marcus, 1995). Although
none of the previous studies employed this repre-
sentation, attaching B- and I- prefixes to section la-
bels may improve a classifier by associating clue
phrases (e.g., ?to determine?) with the starts of sec-
tions (e.g., B-OBJECTIVE). We will compare clas-
sification performances on two sets of label repre-
sentations: namely, we will compare four section
labels and eight labels with BI prefixes attached to
section names.
4 Evaluation
4.1 Experiment
We constructed two sets of corpora (?pure? and ?ex-
panded?), each of which contains 51,000 abstracts
sampled from the abstracts with structured sections.
The ?pure? corpus consists of abstracts that have the
exact four section labels. In other words, this cor-
pus does not include AIM or PURPOSE sentences
even though they are equivalent to OBJECTIVE sen-
tences. The ?pure? corpus is useful to compare the
performance of this study with the previous work.
384
Rank # abstracts (%) Section sequence
1 111,617 (17.6) OBJECTIVE?METHOD(S)? RESULT(S)? CONCLUSION(S)
2 107,124 (16.9) BACKGROUND(S)?METHOD(S)? RESULT(S)? CONCLUSION(S)
3 40,083 (6.3) PURPOSE?METHOD(S)? RESULT(S)? CONCLUSION(S)
4 20,519 (3.2) PURPOSE?MATERIAL AND METHOD(S)? RESULT(S)? CONCLUSION(S)
5 16,705 (2.6) AIM(S)?METHOD(S)? RESULT(S)? CONCLUSION(S)
6 16,400 (2.6) BACKGROUND? OBJECTIVE?METHOD(S)? RESULT(S)? CONCLUSION(S)
7 12,227 (1.9) OBJECTIVE? STUDY DESIGN? RESULT(S)? CONCLUSION(S)
8 11,483 (1.8) BACKGROUND?METHOD(S) AND RESULT(S)? CONCLUSION(S)
9 8,866 (1.4) OBJECTIVE?MATERIAL AND METHOD(S)? RESULT(S)? CONCLUSION(S)
10 8,537 (1.3) PURPOSE? PATIENT AND METHOD(S)? RESULT(S)? CONCLUSION(S)
.. ... ... ...
Total 683,207 (100.0)
Table 3: Typical sequences of sections in Medline abstracts
Representative Equivalent section labels
OBJECTIVE AIM, AIM OF THE STUDY, AIMS, BACKGROUND/AIMS, BACKGROUND/PURPOSE, BACK-
GROUND, BACKGROUND AND AIMS, BACKGROUND AND OBJECTIVE, BACKGROUND AND
OBJECTIVES, BACKGROUND AND PURPOSE, CONTEXT, INTRODUCTION, OBJECT, OBJEC-
TIVE, OBJECTIVES, PROBLEM, PURPOSE, STUDY OBJECTIVE, STUDY OBJECTIVES, SUM-
MARY OF BACKGROUND DATA
METHOD ANIMALS, DESIGN, DESIGN AND METHODS, DESIGN AND SETTING, EXPERIMENTAL DE-
SIGN,INTERVENTION, INTERVENTION(S), INTERVENTIONS, MATERIAL AND METHODS, MA-
TERIALS AND METHODS, MEASUREMENTS, METHOD, METHODOLOGY, METHODS, METH-
ODS AND MATERIALS, PARTICIPANTS, PATIENT(S), PATIENTS, PATIENTS AND METHODS,
PROCEDURE, RESEARCH DESIGN AND METHODS, SETTING, STUDY DESIGN, STUDY DESIGN
AND METHODS, SUBJECTS, SUBJECTS AND METHODS
RESULTS FINDINGS, MAIN RESULTS, RESULT, RESULT(S), RESULTS
CONCLUSIONS CONCLUSION, CONCLUSION(S), CONCLUSIONS, CONCLUSIONS AND CLINICAL RELE-
VANCE, DISCUSSION, IMPLICATIONS, INTERPRETATION, INTERPRETATION AND CONCLU-
SIONS
Table 4: Representative section names and their expanded sections
In contrast, the ?expanded? corpus includes sen-
tences in equivalent sections: AIM and PURPOSE
sentences are mapped to the OBJECTIVE. Table 4
shows the sets of equivalent sections for representa-
tive sections. We created this mapping table man-
ually by analyzing the top 100 frequent section la-
bels found in the Medline. The ?expanded? corpus
is close to the real situation in which the proposed
method annotates unstructured abstracts.
We utilized FlexCRFs4 implementation to build
a classifier with linear-chain CRFs. As a baseline
method, we also prepared an SVM classifier5 with
the same features.
4Flexible Conditional Random Field Toolkit (FlexCRFs):
http://flexcrfs.sourceforge.net/
5We used SVMlight implementation with the linear kernel,
which achieved the best accuracy through this experiment:
http://svmlight.joachims.org/
 20
 30
 40
 50
 60
 70
 80
 1000  10000  100000
Pe
r-a
bs
tra
ct
 a
cc
ur
ac
y 
(%
)
Number of abstracts for training
CRF
SVM
Figure 2: Training curve
4.2 Results
Given the number of abstracts for training n, we ran-
domly sampled n abstracts from a corpus for train-
ing and 1,000 abtracts for testing. Content (n-gram)
features were generated for each trainig set. We
385
Section labels With B- and I- prefixes Without B- and I- prefixes
Features CRF SVM CRF SVM
n-gram 88.7 (42.4) 81.5 (19.1) 85.7 (33.0) 83.3 (23.4)
n-gram + position 93.4 (59.7) 88.2 (35.5) 92.4 (55.4) 89.6 (39.4)
n-gram + surrounding (w = 1) 93.3 (60.4) 89.9 (42.2) 92.1 (52.8) 90.0 (42.0)
n-gram + surrounding (w = 2) 93.7 (61.1) 91.8 (49.4) 92.8 (54.3) 91.8 (47.0)
Full 94.3 (62.9) 93.3 (55.5) 93.3 (56.1) 92.9 (52.2)
Table 5: Classification performance (accuracy) on ?pure? corpus (n = 10, 000)
Section labels With B- and I- prefixes Without B- and I- prefixes
Features CRF SVM CRF SVM
n-gram 87.7 (35.6) 78.5 (14.5) 81.9 (21.0) 80.0 (16.2)
n-gram + position 92.6 (54.3) 87.1 (31.2) 91.4 (48.7) 88.1 (31.2)
n-gram + surrounding (w = 1) 92.3 (52.0) 88.5 (37.6) 89.9 (44.0) 88.4 (37.1)
n-gram + surrounding (w = 2) 92.4 (52.5) 90.1 (41.1) 91.2 (46.6) 90.4 (41.6)
Full 93.0 (55.0) 92.0 (47.3) 92.5 (50.9) 91.7 (44.0)
Table 6: Classification performance (accuracy) on ?expanded? corpus (n = 10, 000)
measured the classification accuracy of sentences
(per-sentence accuracy) and abstracts (per-abstract
accuracy). In per-abstract accuracy, an abstract is
considered correct if all constituent sentences are
correctly labeled.
Trained with n = 50, 000 abstracts from ?pure?
corpus, the proposed method achieved 95.5% per-
sentence accuracy and 68.8% per-abstract accuracy.
The F-score for each section label was 98.7% (O),
95.8% (M), 95.0% (R), and 94.2% (C). The pro-
posed method performed this task better than the
previous studies by a great margin. Figure 2 shows
the training curve for the ?pure? corpus with all fea-
tures presented in this paper. CRF and SVM meth-
ods performed better with more abstracts used for
training. This training curve demonstrated that, with
less than half the number of training corpus, the pro-
posed method could achieve the same accuracy as
the baseline method.
Tables 5 and 6 report the performance of the
proposed and baseline methods on ?pure? and ?ex-
panded? corpora respectively (n = 10, 000). These
tables show per-sentence accuracy followed by per-
abstract accuracy in parentheses with different con-
figurations of features (row) and label representa-
tions (column). For example, the proposed method
obtained 94.3% per-sentence accuracy and 62.9%
per-abstract accuracy with 10,000 training abstracts
from ?pure? corpus, all features, and BI prefixes for
class labels.
The proposed method outperformed the baseline
method in all experimental configurations. This
suggests that CRFs are more suitable for modeling
moves of rhetorical roles in scientific abstracts. It
is noteworthy that the CRF classifier gained higher
per-abstract accuracy than the SVM. For example,
both the CRF classifier with features from surround-
ing sentences (w = 1), and SVM classifier with full
features, obtained 93.3% per-sentence accuracy in
Table 5. Nevertheless, the per-abstract accuracies of
the former and latter were 60.4% and 55.5% respec-
tively: the CRF classifier had roughly 5% advantage
on per-abstract accuracy over SVM. This analysis
reflects the capability of CRFs to determine the op-
timal sequence of section names.
Additional features such as sentence position and
surrounding sentences improved the performance by
ca. 5?10%. The proposed method achieved the best
results with all features. Another interesting discus-
sion arises with regard to the representations of sec-
tion labels. The BI representation always boosted
the per-abstract accuracy of CRF classifiers by ca.
4?14%. In contrast, the SVM classifier could not
leverage the BI representation, and in some configu-
rations, even degraded the accuracy.
386
5 Conclusion
This paper presented a novel approach to identifying
rhetorical roles in scientific abstracts using CRFs.
The proposed method achieved more successful re-
sults than any other previous reports. The CRF clas-
sifier had roughly 5% advantage on per-abstract ac-
curacy over SVM. The BI representation of section
names also boosted the classification accuracy by
5%. In total, the proposed method gained more than
10% improvement on per-abstract accuracy.
We have evaluated the proposed method only on
medical literatures. In addition to improving the
classification performance, a future direction for this
study would be to examine the adaptability of the
proposed method to include other types of texts. We
are planning to construct a summarization system
using the proposed method.
References
ANSI. 1979. American national standard for writing
abstracts. Z39.14-1979, American National Standards
Institute (ANSI).
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
John N. Darroch and Douglas Ratcliff. 1972. General-
ized iterative scaling for log-linear models. The An-
nals of Mathematical Statistics, 43(5):1470?1480.
Harold P. Edmundson. 1969. New methods in automatic
extracting. Journal of the Association for Computing
Machinery, 16(2):264?285.
Christoph H. Gleiter, Tilmann Becker, Katharina H.
Schreeb, Stefan Freudenthaler, and Ursula Gundert-
Remy. 1997. Fenoterol but not dobutamine increases
erythropoietin production in humans. Clinical Phar-
macology & Therapeutics, 61(6):669?676.
Naomi Graetz. 1985. Teaching EFL students to extract
structural information from abstracts. In Jan M. Ulijn
and Anthony K. Pugh, editors, Reading for Profes-
sional Purposed: Methods and Materials in Teaching
Languages, pages 123?135. Acco, Leuven, Belgium.
Takahiko Ito, Masashi Simbo, Takahiro Yamasaki, and
Yuji Matsumoto. 2004. Semi-supervised sentence
classification for medline documents. In IPSJ SIG
Technical Report, volume 2004-ICS-138, pages 141?
146.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning (ICML-2001), pages 282?289.
Jimmy Lin, Damianos Karakos, Dina Demner-Fushman,
and Sanjeev Khudanpur. 2006. Generative con-
tent models for structural analysis of medical ab-
stracts. In Proceedings of the HLT/NAACL 2006
Workshop on Biomedical Natural Language Process-
ing (BioNLP?06), pages 65?72, New York City, USA.
Daniel Marcu. 1999. Discourse trees are good indicators
of importance in text. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization. MIT Press.
Larry McKnight and Padmini Arinivasan. 2003. Cate-
gorization of sentence types in medical abstracts. In
AMIA 2003 Symposium Proceedings, pages 440?444.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer-Verlag, New York, USA.
Constantin Ora?san. 2001. Patterns in scientific abstracts.
In Proceedings of Corpus Linguistics 2001 Confer-
ence, pages 433 ? 443, Lancaster University, Lan-
caster, UK.
Chris D. Paice. 1981. The automatic generation of litera-
ture abstracts: an approach based on the identification
of self-indicating phrases. In SIGIR ?80: Proceedings
of the 3rd annual ACM conference on Research and
development in information retrieval, pages 172?191,
Kent, UK. Butterworth & Co.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Pro-
ceedings of the ACL 3rd Workshop on Very Large Cor-
pora, pages 82?94.
Patrick Ruch, Celia Boyer, Christine Chichester, Imad
Tbahriti, Antoine Geissbu?hler, Paul Fabry, Julien Gob-
eill, Violaine Pillet, Dietrich Rebholz-Schuhmann,
Christian Lovis, and Anne-Lise Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. International Journal of Medical Infor-
matics, 76(2?3):195?200.
Franc?oise Salanger-Meyer. 1990. Discoursal flaws
in medical english abstracts: A genre analysis per
research- and text-type. Text, 10(4):365?384.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL ?03: Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 134?
141, Edmonton, Canada.
387
Masashi Shimbo, Takahiro Yamasaki, and Yuji Mat-
sumoto. 2003. Using sectioning information for text
retrieval: a case study with the medline abstracts. In
Proceedings of Second International Workshop on Ac-
tive Mining (AM?03), pages 32?41.
John M. Swales, 1990. Genre Analysis: English in aca-
demic and research settings, chapter 6. Cambridge
University Press, UK.
Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,
and Patrick Ruch. 2006. Using argumentation to re-
trieve articles with similar citations: An inquiry into
improving related articles search in the medline digital
library. International Journal OF Medical Informat-
ics, 75(6):488?495.
Simone Teufel and Marc Moens. 2002. Summa-
rizing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 467?474, Vancouver, British Columbia, Canada.
Jien-Chen Wu, Yu-Chia Chang, Hsien-Chin Liou, and
Jason S. Chang. 2006. Computational analysis of
move structures in academic abstracts. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions, pages 41?44, Sydney, Australia.
Yasunori Yamamoto and Toshihisa Takagi. 2005. A sen-
tence classification system for multi-document sum-
marization in the biomedical domain. In Proceedings
of the International Workshop on Biomedical Data En-
gineering (BMDE2005), pages 90?95.
388
A Co-occurrence Graph-based Approach for
Personal Name Alias Extraction from Anchor Texts
Danushka Bollegala ?
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@mi.ci.i.u-
tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and
Technology
1-18-13, Sotokanda, Tokyo,
101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
ishizuka@i.u-
tokyo.ac.jp
Abstract
A person may have multiple name aliases
on the Web. Identifying aliases of a name
is important for various tasks such as in-
formation retrieval, sentiment analysis and
name disambiguation. We introduce the no-
tion of a word co-occurrence graph to rep-
resent the mutual relations between words
that appear in anchor texts. Words in an-
chor texts are represented as nodes in the
co-occurrence graph and an edge is formed
between nodes which link to the same url.
For a given personal name, its neighboring
nodes in the graph are considered as can-
didates of its aliases. We formalize alias
identification as a problem of ranking nodes
in this graph with respect to a given name.
We integrate various ranking scores through
support vector machines to leverage a robust
ranking function and use it to extract aliases
for a given name. Experimental results on a
dataset of Japanese celebrities show that the
proposed method outperforms all baselines,
displaying a MRR score of 0.562.
1 Introduction
Searching for information about people in the Web is
one of the most common activities of Internet users.
Around 30% of search engine queries include person
names (Guha and Garg, 2004). However, an indi-
vidual might have multiple nicknames or aliases on
?Research Fellow of the Japan Society for the Promotion of
Science (JSPS)
the Web. For example, the famous Japanese major
league baseball player Hideki Matsui is often called
as Godzilla in web contents. Identifying aliases of
a name is important in various tasks such as infor-
mation retrieval (Salton and McGill, 1986), senti-
ment analysis (Turney, 2002) and name disambigua-
tion (Bekkerman and McCallum, 2005).
In information retrieval, to improve recall of a
web search on a person name, a search engine can
automatically expand the query using aliases of the
name. In our previous example, a user who searches
for Hideki Matsui might also be interested in re-
trieving documents in which Matsui is referred to
as Godzilla. People use different aliases when ex-
pressing their opinions about an entity. By aggre-
gating texts written on an individual that use various
aliases, a sentiment analysis system can make an in-
formed judgment on the sentiment. Name disam-
biguation focuses on identifying different individu-
als with the same name. For example, for the name
Jim Clark, aside from the two most popular name-
sakes - the formula-one racing champion and the
founder of Netscape - at least ten different people are
listed among the top 100 results returned by Google
for the name. Although namesakes have identical
names, their nicknames usually differ. Therefore, a
name disambiguation algorithm can benefit from the
knowledge related to name aliases.
We propose an alias extraction method that ex-
ploits anchor texts and the links indicated by the
anchor texts. Link structure has been studied
extensively in information retrieval and has been
found to be useful in various tasks such as rank-
ing of web pages, identification of hub-authority
865
sites, text categorization and social network extrac-
tion (Chakrabarti, 2003). Anchor texts pointing to
an url provide useful semantic clues regarding the
resource represented by the url.
If the majority of inbound anchor texts of an
url contain a person name, then it is likely that
the remainder of the anchor texts contain informa-
tion about aliases of the name. For example, an
image of Hideki Matsui on a web page might be
linked using the real name, Hideki Matsui, as well
as aliases Godzilla and Matsu Hide. However, ex-
tracting aliases from anchor texts is a challenging
problem due to the noise in both link structure and
anchor texts. For example, web pages of extremely
diverse topics link to yahoo.com using various an-
chor texts. Moreover, given the scale of the Web,
broken links and incorrectly linked anchor texts are
abundant. Naive heuristics are insufficient to extract
aliases from anchor texts.
Our main contributions are summarized as fol-
lows:
? We introduce word co-occurrence graphs to
represents words that appear in anchor texts
and formalize the problem of alias extraction
as a one of ranking nodes in the graph with re-
spect to a given name.
? We define various ranking scores to evaluate
the appropriateness of a word as an alias of a
name. Moreover, the ranking scores are inte-
grated using support vector machines to lever-
age a robust alias detection method.
2 Related Work
Hokama and Kitagawa (2006) propose an alias ex-
traction method that is specific to Japanese lan-
guage. For a given name p, they search for the query
* koto p 1 and extract the words that match the aster-
isk. However, koto is highly ambiguous and extracts
lots of incorrect aliases. Moreover, the method can-
not extract aliases when a name and its aliases ap-
pear in separate documents.
Anchor texts and link structure have been em-
ployed in synonym extraction (Chen et al, 2003)
and translations extraction (Lu et al, 2004). Chen
et al (2003) propose the use of hyperlink structure
1koto is written in hiragana and and means also known as
Hideki MatsuiGodzilla Matsu Hide
????
Yankees baseball
sportsNew York
Figure 1: Co-occurrence graph for Hideki Matsui
within a particular domain to generate a domain-
specific thesaurus. First, a set of high quality web-
sites from a given domain is selected. Second, sev-
eral link analysis techniques are used to remove
noisy links and the navigational structure of the web-
site is converted into a content structure. Third,
pointwise mutual information is applied to identify
phrases within content structures to create a domain
specific thesaurus. They evaluate the thesaurus in a
query expansion task. Anchor texts written in differ-
ent languages that point the same object have been
used in cross-language information retrieval (CLIR)
to translate user queries. Lu et al (2004) extend this
idea by associating anchor texts written using a piv-
otal third language to find translations of queries.
3 Method
3.1 Outline
We introduce word co-occurrence graph, an undi-
rected graph, to represent words that appear in an-
chor texts. For each word that appears in the vocabu-
lary of words in anchor texts, we create a node in the
graph. Two words are considered as co-occurring if
two anchor texts containing these words link to the
same url. An edge is formed between two nodes if
the words represented by those nodes co-occur. Fig-
ure 1 illustrates a portion of the co-occurrence graph
in the proximity of Hideki Matsui as extracted by
this method from anchor texts.
Representing words that appear in anchor texts
as a graph enables us to capture the complex inter-
relations between the words. Words in inbound an-
chor texts of an url contain important semantic clues
866
regarding the resource represented by the url. Such
words form a clique in the co-occurrence graph,
indicating their close connectivity. Moreover, co-
occurrence graphs represent indirect relationships
between words. For example, in Figure 1 Hideki
Matsui is connected to New York via Yankees.
We model the problem of extracting aliases as
a one of ranking nodes in the co-occurrence graph
with respect to a real name. Usually, an individual
has just one or two aliases. A name alias extraction
algorithm must identify the correct aliases among a
vast number of related terms for an individual.
3.2 Word Co-occurrence Graph
Let V be the vocabulary of words wi that appear
in anchor texts. The boolean function A(ai, wi) re-
turns true if the anchor text ai contains the word wi.
Moreover, let the boolean function L(ai, ui) to be
true if the anchor text ai points to url ui. Then two
words wi, wj are defined to be co-occurring in a url
u, if A(ai, wi) ? A(aj , wj) ? L(ai, u) ? L(aj , u) is
true for at least one pair of anchor texts (ai, aj). In
other words, two words are said to co-occur in an url
if at least one inbound pair of anchor texts contains
the two words. Moreover, we define the number of
co-occurrences of wi and wj to be the number of
different urls they co-occur.
We define word co-occurrence graph, G(V,E)
(V is the set of nodes and E is the set of edges) as an
undirected graph where each word wi in vocabulary
V is represented by a node in the graph. Because
one-to-one mapping pertains between a word and a
node, for simplicity we use wi to represent both the
word and the corresponding node in the graph. An
edge eij ? E is created between two nodes wi, wj if
they co-occur. Given a personal name p, represented
by a node p in the co-occurrence graph, our objec-
tive is to identify the nodes that represent aliases of
p. We rank the nodes in the graph with respect to
p such that more likely a node is an alias of p, the
higher the rank it is assigned. According to our def-
inition, a node that lies n hops away from p has an
n-order co-occurrence with p. Considering the fact
that a single web page might link to many pages with
diverse topics, higher order co-occurrences with p
(i.e. nodes that appear further from p) are unreliable
as aliases of p. Consequently, we limit C(p), the set
of candidate aliases of p, to nodes which are directly
Table 1: Contingency table for a candidate alias x
x C(p)? {x}
p k n? k n
V ? {p} K ? k N ? n?K + k N ? n
V K N ?K N
connected to p in the graph. In Figure 1 candidates
of Hideki Matsui fall inside the dotted ellipse.
3.3 Ranking of Candidates
To evaluate the strength of co-occurrence between
a candidate alias and the real name, for each candi-
date alias x in C(p) we create a contingency table
as shown in Table 1. In Table 1, the first row repre-
sents candidates of p and the first column represents
nodes in the graph. Therein, k is the number of urls
in which p and x co-occur, K is the number of urls
in which at least one inbound anchor text contains
the candidate x, n is the number of urls in which
at least one inbound anchor text contains p and N is
the total number of urls in the crawl. Next, we define
various ranking scores based on Table 1.
Simplest of all ranking scores is the link frequency
(lf ). We define link frequency of an candidate x as
the number of different urls in which x and p co-
occur. This is exactly the value of k in Table 1.
Link frequency is biased towards highly frequent
words. A word that has a high frequency in anchor
texts can also report a high co-occurrence with p.
tfidf measure which is popularly used in information
retrieval can be used to normalize this bias. tfidf is
computed from Table 1 as follows,
tfidf(nj) = k log NK + 1 .
From Table 1 we compute co-occurrence mea-
sures; log likelihood ratio LLR (Dunning, 1993),
chi-squared measure CS, point-wise mutual infor-
mation PMI (Church and Hanks, 1991) and hyper
geometric distribution HG (Hisamitsu and Niwa,
2001). Each of these measures is used to rank candi-
date aliases of a given name. Because of the limited
availability of space, we omit the definitions of these
measures.
Furthermore, we define popular set overlap mea-
sures; cosine measure, overlap coefficient and Dice
coefficient from Table 1 as follows,
867
cosine(p, x) = k?n+?K ,
overlap(p, x) = kmin(n,K) ,
Dice(p, x) = 2kn+K .
3.4 Hub weighting
A frequently observed phenomenon on the Web is
that many web pages with diverse topics link to so
called hubs such as Google, Yahoo or Amazon. Be-
cause two anchor texts might link to a hub for en-
tirely different reasons, co-occurrences coming from
hubs are prone to noise. To overcome the adverse ef-
fects of a hub h when computing the ranking scores
described in section 3.3, we multiply the number
of co-occurrences of words linked to h by a factor
?(h, p) where,
?(h, p) = td? 1 . (1)
Here, t is the number of inbound anchor texts of
h that contain the real name p, d is the total num-
ber of inbound anchor texts of h. If many anchor
texts that link to h contain p (i.e., larger t value)
then the reliability of h as a source of information
about p increases. On the other hand, if h has many
inbound links (i.e., larger d value) then it is likely
to be a noisy hub and gets discounted when mul-
tiplied by ?(<< 1). Intuitively, Formula 1 boosts
hubs that are likely to be containing information re-
garding p, while penalizing those that contain vari-
ous other topics.
3.5 Training
In section 3.3 we introduced 9 ranking scores to
evaluate the appropriateness of a candidate alias for
a given name. Each of the scores is computed
with and without weighting for hubs, resulting in
2? 9 = 18 ranking scores. The ranking scores cap-
ture different statistical properties of candidates; it is
not readily apparent which ranking scores best con-
vey aliases of a name. We use real world name-alias
data to learn the proper combination of the ranking
scores.
We represent each candidate alias as a vector of
the ranking scores. Because we use the 18 rank-
ing scores described above, each candidate is repre-
sented by an 18-dimensional vector. Given a set of
personal names and their aliases, we model the train-
ing process as a preference learning task. For each
name, we impose a binary preference constraint be-
tween the correct alias and each candidate.
For example, let us assume for a name wp we
selected the four candidates a1, a2, a3, a4. With-
out loss of generality, let us further assume that a1
and a2 are the correct aliases of p. Therefore, we
form four partial preferences: a1 ? a3, a1 ? a4,
a2 ? a3 and a2 ? a4. Here, x ? y denotes
the fact that x is preferred to y. We use ranking
SVMs (Joachims, 2002) to learn a ranking function
from preference constraints. Ranking SVMs attempt
to minimize the number of discordant pairs during
training, thereby improving average precision. The
trained SVM model is used to rank a set of candi-
dates extracted for a name. Then the highest ranking
candidate is selected as the alias of the name.
4 Experiments
We crawled Japanese web sites and extracted anchor
texts and urls linked by the anchor texts. A web
site might use links for purely navigational purposes,
which convey no semantic clue. To remove naviga-
tional links in our dataset, we prepare a list of words
that are commonly used in navigational menus, such
as top, last, next, previous, links, etc and remove
anchor texts that contain those words. In addition
we remove any links that point to pages within the
same site. All urls with only one inbound anchor text
are removed from the dataset. After the above men-
tioned processing, the dataset contains 24, 456, 871
anchor texts pointing to 8, 023, 364 urls. The aver-
age number of inbound anchor texts per url is 3.05
and its standard deviation is 54.02. We tokenize
anchor texts using the Japanese morphological an-
alyzer MeCab (Kudo et al, 2004) and select nouns
as nodes in the co-occurrence graph.
For training and evaluation purposes we manually
assigned aliases for 441 Japanese celebrities. The
name-alias dataset covers people from various fields
868
Table 2: Mean Reciprocal Rank
Method MRR Method MRR
SVM (RBF) 0.5625 lf 0.0839
SVM (Linear) 0.5186 cosine 0.0761
SVM (Quad) 0.4898 tfidf 0.0757
SVM (Cubic) 0.4087 Dice 0.0751
tfidf(h) 0.3957 overlap(h) 0.0750
LLR(h) 0.3879 PMI(h) 0.0624
cosine(h) 0.3701 LLR 0.0604
lf(h) 0.3677 HG 0.0399
HG(h) 0.3297 CS 0.0079
Dice(h) 0.2905 PMI 0.0072
CS(h) 0.1186 overlap 0.0056
of cinema, sports, politics and mass-media. The ma-
jority of people in the dataset have only one alias
assigned. For each real name in the dataset we ex-
tract a set of candidates using the proposed method.
We then sort the real names in the dataset accord-
ing to the number of candidates extracted for them.
We select the top 50 real names with the greatest
number of candidate aliases for evaluation purposes
because recognizing the correct alias from numerous
candidates is a more challenging task that enables us
to perform a strict evaluation. On average a name in
our evaluation dataset has 6500 candidates, of which
only one is correct. The rest of the 391 (441 ? 50)
names are used for training.
We compare the proposed method (SVM) against
various baseline ranking scores using mean recip-
rocal rank (MRR) (Baeza-Yates and Ribeiro-Neto,
1999). The MRR is defined as follows;
MRR = 1n
n?
i=1
1
Ri . (2)
Therein, Ri is the rank assigned to a correct alias and
n is the total number of aliases. The MRR is widely
used in information retrieval to evaluate the rank-
ing of search results. Formula 2 gives high MRR to
ranking scores which assign higher ranks to correct
aliases.
Our experimental results are summarized in Ta-
ble 2. The hub weighted versions of ranking scores
are denoted by (h). We trained rank SVMs with
linear SVM (Linear), quadratic SVM (Quad), cubic
SVM (Cubic) and radial basis functions (RBF) SVM
(RBF) kernels. As shown in Table 2, the proposed
SVM-based method has the highest MRR values
among all methods compared. The best results are
obtained with the RBF kernel (SVM RBF). In fact
for 21 out of 50 names in our dataset, SVM (RBF)
correctly ranks their aliases at the first rank. Con-
sidering the fact that each name has more than 6000
candidate aliases, this is a marked improvement over
the baselines. It is noteworthy in Table 2 that the
hub-weighted versions of ranking scores outperform
the corresponding non-weighted version. This jus-
tifies the hub weighting method proposed in sec-
tion 3.4. The hub-weighted tfidf score (tfidf(h)) has
the best MRR among the baseline ranking scores.
For polynomial kernels, we observe a drop of preci-
sion concomitant with the complexity of the kernel,
which occurs as a result of over-fitting.
Table 3 shows the top-three ranked aliases ex-
tracted for Hideki Matsui by various methods. En-
glish translation of words are given within brackets.
The correct alias, Godzilla, is ranked first by SVM
(RBF). Moreover, the correct alias is followed by
the last name Matsui and his team, New York Yan-
kees. In fact, tfidf(h), LLR(h) and lf(h) all have the
exact ranking for the top three candidates. Hide,
which is an abbreviated form of Hideki, is ranked
second by these measures. However, none con-
tains the alias Godzilla among the top three candi-
dates. The non-hub weighted measures tend to in-
clude general terms such as Tokyo, Yomiuri (a pop-
ular Japanese newspaper), Nikkei (a Japanese busi-
ness newspaper), and Tokyo stock exchange. A close
analysis revealed that such general terms frequently
co-occur with a name in hubs. Without adjusting
the co-occurrences coming from hubs, such terms
invariably receive high ranking scores, as shown in
Table 3.
Incorrect tokenization of Japanese names is a
main source of error. Many aliases are out-of-
dictionary (unknown) words, which are known to
produce incorrect tokenizations in Japanese mor-
phological analyzers. Moreover, a name and its
aliases can be written in various scripts: Hiragana,
Katanaka, Kanji, Roman and even combinations of
multiple scripts. Some foreign names such as David
even have orthographic variants in Japanese: da-
bid-do or de-bid-do. Failing to recognize the differ-
ent ways in which a name can be written engenders
wrong preference constraints during training.
869
Table 3: Top ranking candidate aliases for Hideki Matsui
Method First Second Third
SVM (RBF) (Godzilla) (Matsui) (Yankees)
tfidf(h) (Matsui) (Hide) (Yankees)
LLR(h) (Matsui) (Hide) (Yankees)
cosine(h) (Matsui) (Yankees) (Hide)
lf(h) (Matsui) (Hide) (Yankees)
HG(h) (Matsui) (Yankees) (Hide)
Dice(h) (Matsui) (Yankees) (Hide)
CS(h) (Matsui) (Major league) (player)
lf (Tokyo) (Yomiuri) (Nikkei)
cosine (Yomiuri) (Tokyo stock exchange) (Matsui)
tfidf (Yomiuri) (Tokyo) (Tokyo stock exchange)
Dice (Yomiuri) (Tokyo stock exchange) (Matsui)
overlap(h) (play) (Godzilla) (Steinbrenner)
PMI(h) (play) (Godzilla) (Steinbrenner)
LLR (Yomiuri) (Tokyo stock exchange) (jiji.com)
HG (Yomiuri) (Tokyo stock exchange) (Matsui)
CS (jiji.com) (Tokyo stock exchange) (Yomiuri)
PMI (Komdatzien) (picture) (contents)
overlap (Komdatzien) (picture) (contents)
5 Conclusion
We proposed a method to extract aliases of a given
name using anchor texts and link structure. We cre-
ated a co-occurrence graph to represent words in an-
chor texts and modeled the problem of alias extrac-
tion as a one of ranking nodes in this graph with re-
spect to a given name. In future, we intend to apply
the proposed method to extract aliases for other en-
tity types such as products, organizations and loca-
tions.
References
R.A. Baeza-Yates and B.A. Ribeiro-Neto. 1999. Modern
Information Retrieval. ACM Press / Addison-Wesley.
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In
Proc. of the World Wide Web Conference (WWW? 05),
pages 463?470.
S. Chakrabarti. 2003. Mining the Web: Discovering
Knowledge from Hypertext Data. Morgan Kaufmann.
Z. Chen, S. Liu, L. Wenyin, Ge. Pu, and W. Ma. 2003.
Building a web thesaurus from web link structure.
In Proc. of the 26th annual international ACM SI-
GIR conference on Research and development in in-
formaion retrieval, pages 48?55.
K. Church and P. Hanks. 1991. Word association norms,
mutual information and lexicography. Computational
Linguistics, 16:22?29.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19:61?74.
R. Guha and A. Garg. 2004. Disambiguating people in
search. In Stanford University.
T. Hisamitsu and Y. Niwa. 2001. Topic-word selection
based on combinatorial probability. In Proc. of NL-
PRS?01, pages 289?296.
T. Hokama and H. Kitagawa. 2006. Extracting
mnemonic names of people from the web. In Proc.
of 9th International Conference on Asian Digital Li-
braries (ICADL?06), pages 121?130.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of the ACM conference on
Knowledge Discovery and Data Minning (KDD).
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to japanese morpho-
logical analysis. In Proc. of EMNLP?04.
W. Lu, L. Chien, and H. Lee. 2004. Anchor text mining
for translation of web queries: A transitive translation
approach. ACM Transactions on Information Systems,
22(2):242?269.
G. Salton and M.J. McGill. 1986. Introduction to Mod-
ern Information Retreival. McGraw-Hill Inc., New
York, NY.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of the ACL, pages 417?424.
870
A Discriminative Approach to Japanese Abbreviation Extraction
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Mitsuru Ishizuka?
ishizuka@i.u-tokyo.ac.jp
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656, Japan
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre,
131 Princess Street, Manchester M1 7DN, UK
Abstract
This paper addresses the difficulties in rec-
ognizing Japanese abbreviations through the
use of previous approaches, examining ac-
tual usages of parenthetical expressions in
newspaper articles. In order to bridge the
gap between Japanese abbreviations and
their full forms, we present a discrimina-
tive approach to abbreviation recognition.
More specifically, we formalize the abbrevi-
ation recognition task as a binary classifica-
tion problem in which a classifier determines
a positive (abbreviation) or negative (non-
abbreviation) class, given a candidate of ab-
breviation definition. The proposed method
achieved 95.7% accuracy, 90.0% precision,
and 87.6% recall on the evaluation corpus
containing 7,887 (1,430 abbreviations and
6,457 non-abbreviation) instances of paren-
thetical expressions.
1 Introduction
Human languages are rich enough to be able to
express the same meaning through different dic-
tion; we may produce different sentences to convey
the same information by choosing alternative words
or syntactic structures. Lexical resources such as
WordNet (Miller et al, 1990) enhance various NLP
applications by recognizing a set of expressions re-
ferring to the same entity/concept. For example, text
retrieval systems can associate a query with alterna-
tive words to find documents where the query is not
obviously stated.
Abbreviations are among a highly productive type
of term variants, which substitutes fully expanded
terms with shortened term-forms. Most previous
studies aimed at establishing associations between
abbreviations and their full forms in English (Park
and Byrd, 2001; Pakhomov, 2002; Schwartz and
Hearst, 2003; Adar, 2004; Nadeau and Turney,
2005; Chang and Schu?tze, 2006; Okazaki and Ana-
niadou, 2006). Although researchers have proposed
various approaches to solving abbreviation recog-
nition through methods such as deterministic algo-
rithm, scoring function, and machine learning, these
studies rely on the phenomenon specific to English
abbreviations: all letters in an abbreviation appear in
its full form.
However, abbreviation phenomena are heavily de-
pendent on languages. For example, the term one-
segment broadcasting is usually abbreviated as one-
seg in Japanese; English speakers may find this pe-
culiar as the term is likely to be abbreviated as 1SB
or OSB in English. We show that letters do not pro-
vide useful clues for recognizing Japanese abbrevia-
tions in Section 2. Elaborating on the complexity of
the generative processes for Japanese abbreviations,
Section 3 presents a supervised learning approach to
Japanese abbreviations. We then evaluate the pro-
posed method on a test corpus from newspaper arti-
cles in Section 4 and conclude this paper.
2 Japanese Abbreviation Survey
Researchers have proposed several approaches to
abbreviation recognition for non-alphabetical lan-
guages. Hisamitsu and Niwa (2001) compared dif-
ferent statistical measures (e.g., ?2 test, log like-
889
Table 1: Parenthetical expressions used in Japanese newspaper articles
lihood ratio) to assess the co-occurrence strength
between the inner and outer phrases of parenthet-
ical expressions X (Y). Yamamoto (2002) utilized
the similarity of local contexts to measure the para-
phrase likelihood of two expressions based on the
distributional hypothesis (Harris, 1954). Chang and
Teng (2006) formalized the generative processes of
Chinese abbreviations with a noisy channel model.
Sasano et al (2007) designed rules about letter types
and occurrence frequency to collect lexical para-
phrases used for coreference resolution.
How are these approaches effective in recogniz-
ing Japanese abbreviation definitions? As a prelimi-
nary study, we examined abbreviations described in
parenthetical expressions in Japanese newspaper ar-
ticles. We used the 7,887 parenthetical expressions
that occurred more than eight times in Japanese ar-
ticles published by the Mainichi Newspapers and
Yomiuri Shimbun in 1998?1999. Table 1 summa-
rizes the usages of parenthetical expressions in four
groups. The field ?para? indicates whether the inner
and outer elements of parenthetical expressions are
interchangeable.
The first group acronym (I) reduces a full form to
a shorter form by removing letters. In general, the
process of acronym generation is easily interpreted:
the left example in Table 1 consists of two Kanji let-
ters taken from the heads of the two words, while
the right example consists of the letters at the end of
the 1st, 2nd, and 4th words in the full form. Since
all letters in an acronym appear in its full form, pre-
vious approaches to English abbreviations are also
applicable to Japanese acronyms. Unfortunately, in
this survey the number of such ?authentic? acronyms
amount to as few as 90 (1.2%).
The second group acronym with translation (II) is
characteristic of non-English languages. Full forms
are imported from foreign terms (usually in En-
glish), but inherit the foreign abbreviations. The
third group alias (III) presents generic paraphrases
that cannot be interpreted as abbreviations. For ex-
ample, Democratic People?s Republic of Korea is
known as its alias North Korea. Even though the
formal name does not refer to the ?northern? part, the
alias consists of Korea, and the locational modifier
North. Although the second and third groups retain
their interchangeability, computers cannot recognize
abbreviations with their full forms based on letters.
The last group (IV) does not introduce inter-
changeable expressions, but presents additional in-
formation for outer phrases. For example, a location
usage of a parenthetical expression X (Y) describes
an entity X, followed by its location Y. Inner and
outer elements of parenthetical expressions are not
interchangeable. We regret to find that as many as
81.9% of parenthetical expressions were described
for this usage. Thus, this study regards acronyms
(with and without translation) and alias as Japanese
890
Table 2: Top 10 frequent parenthetical expressions
used in Japanese newspapers from 1998?1999
abbreviations in a broad sense, based on their in-
terchangeabilities. In other words, the goal of this
study is to classify parenthetical expressions X (Y)
into true abbreviations (groups I, II, III) and other
usages of parentheses (group IV).
How much potential do statistical approaches
have to identify Japanese abbreviations? Table 2
shows the top 10 most frequently appearing paren-
thetical expressions in this survey. The ?class? field
represents the category1: T: acronym with transla-
tion, A: alias, and O: non-abbreviation. The most
frequently occurring parenthetical expression was
Democratic People?s Republic of Korea (North Ko-
rea) (4,160 occurrences). 7 instances in the table
were acronyms with translation (#2?5, #7?8), and
an alias (#1), but 3 non-abbreviation instances (#6,
#9, and #10) expressed nationalities of information
sources. Even if we designed a simple method
to choose the top 10 parenthetical expressions, the
recognition performance would be no greater than
70% precision.
3 A discriminative approach to
abbreviation recognition
In order to bridge the gap between Japanese abbre-
viations and their full forms, we present a discrim-
inative approach to abbreviation recognition. More
specifically, we formalize the abbreviation recogni-
tion task as a binary classification problem in which
1No acronym was included in the top 10 list.
Figure 1: Paraphrase occurrence with parentheses
a classifier determines a positive (abbreviation) or
negative (non-abbreviation) class, given a parenthet-
ical expression X (Y). We model the classifier by
using Support Vector Machines (SVMs) (Vapnik,
1998). The classifier combines features that char-
acterize various aspects of abbreviation definitions.
Table 3 shows the features and their values for the
abbreviation EU, and its full form: O-shu Rengo
(European Union). A string feature is converted into
a set of boolean features, each of which indicates
?true? or ?false? of the value. Due to the space limita-
tion, the rest of this section elaborates on paraphrase
ratio and SKEW features.
Paraphrase ratio Let us consider the situation in
which an author describes an abbreviation definition
X (Y) to state a paraphrase X ? Y in a document.
The effect of the statement is to define the meaning
of the abbreviation Y as X in case the reader may
be unaware/uncertain of the abbreviation Y. For ex-
ample, if an author wrote a parenthetical expression,
Multi-Document Summarization (MDS), in a docu-
ment, readers would recognize the meaning of the
expression MDS. Even if they were aware of the def-
inition, MDS alone would be ambiguous; it could
stand for Multi Dimensional Scaling, Missile De-
fense System, etc. Therefore, an author rarely uses
the expression Y before describing its definition.
At the same time, the author would use the expres-
sion Y more than X after describing the definition, if
it were to declare the abbreviation Y for X. Figure 1
illustrates this situation with two documents. Doc-
ument (a) introduces the abbreviation EU for Euro-
pean Union because the expression EU occurs more
frequently than European Union after the parentheti-
cal expression. In contrast, the parenthetical expres-
891
Feature Type Description Example
PR(X,Y ) numeric Paraphrase ratio 0.426
SKEW(X,Y ) numeric Similarity of local contexts measured by the skew divergence 1.35
freq(X) numeric Frequency of occurrence of X 2,638
freq(Y ) numeric Frequency of occurrence of Y 8,326
freq(X,Y ) numeric Frequency of co-occurrence of X and Y 3,121
?2(X,Y ) numeric Co-occurrence strength measured by the ?2 test 2,484,521
LLR(X,Y ) numeric Co-occurrence strength measured by the log-likelihood ratio 6.8
match(X,Y ) boolean Predicate to test whether X contains all letters in Y 0
Letter types string Pair of letter types of X and Y Kanji/Alpha
First letter string The first letter in the abbreviation Y E
Last letter string The last letter in the abbreviation Y U
POS tags string Pair of POS tags for X and Y NNP/NNP
POS categories string Pair of POS categories for X and Y NN/NN
NE tags string Pair of NE tags for X and Y ORG/ORG
Table 3: Features for the SVM classifier and their values for the abbreviation EU.
sion in document (b) describes the property (nation-
ality) of a person Beckham.
Suppose that we have a document that has a par-
enthetical expression with expressionsX and Y . We
regard a document introducing an abbreviation Y for
X if the document satisfies both of these conditions:
1. The expression Y appears more frequently than
the expression X does after the definition pat-
tern.
2. The expression Y does not appear before the
definition pattern.
Formula 1 assesses the paraphrase ratio of the ex-
pressions X and Y,
PR(X,Y ) =
dpara(X,Y )
d(X,Y )
. (1)
In this formula, dpara(X,Y ) denotes the number
of documents satisfying the above conditions, and
d(X,Y ) presents the number of documents having
the parenthetical expression X(Y ). The function
PR(X, Y) ranges from 0 (no abbreviation instance)
to 1 (all parenthetical expressions introduce the ab-
breviation).
Similarity of local contexts We regard words that
have dependency relations from/to the target expres-
sion as the local contexts of the expression, apply-
ing all sentences to a dependency parser (Kudo and
Matsumoto, 2002). Collecting the local context of
the target expressions, we compute the skew diver-
gence (Lee, 2001), which is a weighted version of
Kullback-Leibler (KL) divergence, to measure the
resemblance of probability distributions P and Q:
SKEW?(P ||Q) = KL(P ||?Q+ (1? ?)P ), (2)
KL(P ||Q) =
?
i
P (i) log
P (i)
Q(i)
. (3)
In these formulas, P is the probability distribution
function of the words in the local context for the ex-
pression X , Q is for Y , and ? is a skew parameter
set to 0.99. The function SKEW?(P ||Q) becomes
close to zero if the probability distributions of local
contexts for the expressions X and Y are similar.
Other features In addition, we designed twelve
features for abbreviation recognition: five fea-
tures, freq(X), freq(Y ), freq(X,Y ), ?2(X,Y ), and
LLR(X,Y ) to measure the co-occurrence strength
of the expressions X and Y (Hisamitsu and Niwa,
2001), match(X,Y ) feature to test whether or not
all letters in an abbreviation appear in its full form,
three features letter type, first letter, and last let-
ter corresponding to rules about letter types in ab-
breviation definitions, and three features POS tags,
POS categories, and NE tags to utilize information
from a morphological analyzer and named-entity
tagger (Kudo and Matsumoto, 2002).
4 Evaluation
4.1 Results
We built a system for Japanese abbreviation recogni-
tion by using the LIBSVM implementation2 with a
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
892
Group Recall
Acronym 94.4%
Acronym with translation 97.4%
Alias 81.4%
Total 87.6%
Table 4: Recall for each role of parentheses
linear kernel, which obtained the best result through
experiments. The performance was measured under
a ten-fold cross-validation on the corpus built in the
survey, which contains 1,430 abbreviation instances
and 6,457 non-abbreviation instances.
The proposed method achieved 95.7% accuracy,
90.0% precision, and 87.6% recall for recognizing
Japanese abbreviations. We cannot compare this
performance directly with the previous work be-
cause of the differences in the task design and cor-
pus. For reference, Yamamoto (2002) reported 66%
precision (he did not provide the recall value) for
a similar task: the acquisition of lexical paraphrase
from Japanese newspaper articles.
Table 4 reports the recall value for each group
of abbreviations. This analysis shows the distribu-
tion of abbreviations unrecognized by the proposed
method. Japanese acronyms, acronyms with transla-
tion, and aliases were recognized at 94.4%, 97.4%,
and 81.4% recall respectively. It is interesting to see
that the proposed method could extract acronyms
with translation and aliases even though we did not
use any bilingual dictionaries.
4.2 Analyses for individual features
The numerical and boolean features are monotone
increasing functions (decreasing for the SKEW fea-
ture) as two expressions X and Y are more likely
to present an abbreviation definition. For example,
the more authors introduce a paraphrase X ? Y,
the higher the value that PR(X,Y ) feature yields.
Thus, we emulate a simple classifier for each feature
that labels a candidate of abbreviation definition as a
positive instance only if the feature value is higher
than a given threshold ?, e.g., PR(X,Y ) > 0.9.
Figure 2 shows the precision?recall curve for each
feature with variable thresholds.
The paraphrase ratio (PR) feature outperformed
other features with a wide margin: the precision and
recall values for the best F1 score were 66.2% and
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
isi
on
Recall
Co-occurrence frequency
Log likelihood ratio
Skew divergence
Letter match
Paraphrase rate
Chi square
Figure 2: Precision?recall curve of each feature
Feature Accuracy Reduction
All 95.7% ?
- PR(X,Y ) 95.2% 0.5%
- SKEW(X,Y ) 95.4% 0.3%
- freq(X,Y ) 95.6% 0.1%
- ?2(X,Y ) 95.6% 0.1%
- LLR(X,Y ) 95.3% 0.4%
- match(X,Y ) 95.5% 0.2%
- Letter type 94.5% 1.2%
- POS tags 95.6% 0.1%
- NE tags 95.7% 0.0%
Table 5: Contribution of the features
48.1% respectively. Although the performance of
this feature alone was far inferior to the proposed
method, to some extent Formula 1 estimated actual
occurrences of abbreviation definitions.
The performance of the match (letter inclusion)
feature was as low as 58.2% precision and 6.9% re-
call3. It is not surprising that the match feature had
quite a low recall, because of the ratio of ?authentic?
acronyms (about 6%) in the corpus. However, the
match feature did not gain a good precision either.
Examining false cases, we found that this feature
could not discriminate cases where an outer element
contains its inner element accidentally; e.g., Tokyo
Daigaku (Tokyo), which describes a university name
followed by its location (prefecture) name.
Finally, we examined the contribution of each fea-
ture by eliminating a feature one by one. If a feature
was important for recognizing abbreviations, the ab-
sence of the feature would drop the accuracy. Each
row in Table 5 presents an eliminated feature, the
accuracy without the feature, and the reduction of
3This feature drew the precision?recall locus in a stepping
shape because of its discrete values (0 or 1).
893
the accuracy. Unfortunately, the accuracy reductions
were so few that we could not discuss contributions
of features with statistical significance. The letter
type feature had the largest influence (1.2%) on the
recognition task, followed by the paraphrase ratio
(0.5%) and log likelihood ratio (0.4%).
5 Conclusion
In this paper we addressed the difficulties in rec-
ognizing Japanese abbreviations by examining ac-
tual usages of parenthetical expressions in news-
paper articles. We also presented the discrimina-
tive approach to Japanese abbreviation recognition,
which achieved 95.7% accuracy, 90.0% precision,
and 87.6% recall on the evaluation corpus. A future
direction of this study would be to apply the pro-
posed method to other non-alphabetical languages,
which may have similar difficulties in modeling the
generative process of abbreviations. We also plan to
extend this approach to the Web documents.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and Solution-Oriented Research for Science
and Technology (JST, Japan). We used Mainichi
Shinbun and Yomiuri Shinbun newspaper articles for
the evaluation corpus.
References
Eytan Adar. 2004. SaRAD: A simple and robust abbre-
viation dictionary. Bioinformatics, 20(4):527?533.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In S. Ananiadou and
J. McNaught, editors, Text Mining for Biology and
Biomedicine, pages 99?119. Artech House, Inc.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
atomic chinese abbreviation pairs: A probabilistic
model for single character word recovery. In Proceed-
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 17?24, Sydney, Australia,
July. Association for Computational Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10:146?162.
Toru Hisamitsu and Yoshiki Niwa. 2001. Extracting
useful terms from parenthetical expression by combin-
ing simple rules and statistical measures: A compara-
tive evaluation of bigram statistics. In Didier Bouri-
gault, Christian Jacquemin, and Marie-C L?Homme,
editors, Recent Advances in Computational Terminol-
ogy, pages 209?224. John Benjamins.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the CoNLL 2002 (COLING 2002 Post-
Conference Workshops), pages 63?69.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to wordnet: An on-line lexical database.
Journal of Lexicography, 3(4):235?244.
David Nadeau and Peter D. Turney. 2005. A su-
pervised learning approach to acronym identification.
In 8th Canadian Conference on Artificial Intelligence
(AI?2005) (LNAI 3501), pages 319?329.
Naoaki Okazaki and Sophia Ananiadou. 2006. A term
recognition approach to acronym recognition. In Pro-
ceedings of the COLING-ACL 2006 Main Conference
Poster Sessions, pages 643?650, Sydney, Australia.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of 40th
annual meeting of ACL, pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of the EMNLP 2001, pages 126?133.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2007. Improving coreference resolution us-
ing bridging reference resolution and automatically
acquired synonyms. In Anaphora: Analysis, Alo-
gorithms and Applications, 6th Discourse Anaphora
and Anaphor Resolution Colloquium, DAARC2007,
pages 125?136.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Pacific Symposium on Biocom-
puting (PSB 2003), number 8, pages 451?462.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Kazuhide Yamamoto. 2002. Acquisition of lexical para-
phrases from texts. In 2nd International Workshop
on Computational Terminology (Computerm 2002, in
conjunction with COLING 2002), pages 1?7, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
894
Linguistic Interpretation of Emotions for Affect Sensing from Text 
Mostafa Al Masum Shaikh 
Dept. of Information and 
Communication Engineering 
University of Tokyo 
7-3-1 Hongo, Bunkyo-Ku 
113-8656 Tokyo, Japan 
almasum@gmail.com 
Helmut Prendinger 
Digital Contents and Media 
Sciences Research Division 
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda Ku
101-8430 Tokyo, Japan 
helmut@nii.ac.jp 
Mitsuru Ishizuka 
Dept. of Information and 
Communication Engineering
University of Tokyo 
7-3-1 Hongo, Bunkyo-Ku 
113-8656 Tokyo, Japan 
ishizuka@ieee.org 
 
Abstract 
Several approaches have already been em-
ployed to ?sense? affective information from 
text, but none of those ever considered the 
cognitive and appraisal structure of individ-
ual emotions. Hence this paper aims at inter-
preting the cognitive theory of emotions 
known as the OCC emotion model, from a 
linguistic standpoint. The paper provides 
rules for the OCC emotion types for the task 
of sensing affective information from text. 
Since the OCC emotions are associated with 
several cognitive variables, we explain how 
the values could be assigned to those by ana-
lyzing and processing natural language com-
ponents. Empirical results indicate that our 
system outperforms another state-of-the-art 
system.   
1 Introduction and Motivation 
While various conceptual models, computational 
methods, techniques, and tools are reported in 
(Shanahan et. al., 2006), we argue that the current 
work for sensing the affect communicated by text 
is incomplete and often gives inaccurate results. It 
is true that the assessment of affective content is 
inevitably subjective and subject to considerable 
disagreement. Yet the interest in sentiment or af-
fect based text categorization is increasing with the 
large amount of text becoming available on the 
Internet. A brief discussion on available ap-
proaches is given in (Shaikh et. al., 2007a; Liu et. 
al., 2003). For example, keyword spotting, lexical 
affinity, statistical and hand crafted approaches 
target affective lexicons which are not sufficient to 
recognize affective information from text, because 
according to a linguistic survey (Pennebaker et. al., 
2003), only 4% of words used in written texts carry 
affective content. 
In this paper we consider the contextual-
valenced based approached (i.e., SenseNet) as dis-
cussed by Shaikh et. al., (2007a, 2007b) and con-
sider their SenseNet as the basis of our know-
ledgebase. For simplicity, we use the words ?sen-
timent? and ?opinion? synonymously and consider 
sentiment sensing as the prior task of ?affect? or 
?emotion? sensing. The SenseNet can sense either 
positive or negative ?sentiment?, but it cannot clas-
sify different emotions. Therefore, this paper ex-
plains how the SenseNet can be employed to sense 
emotions from text. So the primary focus of this 
paper is to provide a set of rules for emotions char-
acterized by the OCC (Ortony et. al., 1988) emo-
tion model and discuss how the rules are imple-
mented.  
2 Affect Sensing from Text 
2.1 Extending Valence Assignment Approach 
for Emotions Classification 
For the task of affect sensing from text we should 
incorporate both commonsense knowledge and 
cognitive structure of emotions along with the se-
mantic interpretation of the words used in a sen-
tence. We have chosen the OCC model of emo-
tions for this task. The rule-based definition of the 
OCC emotion types characterized by a rich set of 
linguistic tokens makes it appropriate to cope with 
the valence assignment approach for affect sensing 
from text.   
2.2 Characterization of OCC Emotions  
The OCC emotion types can be characterized by 
appropriate rules interplaying with several vari-
ables. There are two kinds of variables, namely, 
895
emotion inducing variables (event, agent and ob-
ject based variables) and emotion intensity vari-
ables. The event-based variables are calculated 
with respect to the event which is usually a verb-
object pair found in the sentence. For example, the 
sentence, John bought Mary an ice-cream, gives 
an event as ?buy, ice-cream?. The variables are 
enlisted in Table 1. In general we call them ?emo-
tion variables?. 
Type Variable Name 
agent_fondness (af) agent based 
cognitive_strength (cs) 
object_fondness (of) object based 
object_appealing (oa) 
self_reaction (sr) 
self_presumption (sp) 
other_presumption (op) 
prospect (pros) 
status (stat) 
unexpectedness (unexp) 
self_appraisal (sa) 
event based 
valenced_reaction (vr) 
event_deservingness (ed) 
effort_of_action (eoa) 
expected_deviation (edev) 
intensity  
event_familiarity (ef) 
        Table 1. OCC emotion variables 
 
The OCC emotion model specifies 22 emotion 
types and 2 cognitive states. For example, OCC 
model literally defines ?Happy-for? as ?Pleased 
about a Desirable event for a Liked agent?, and 
?Fear? as ?Displeased about Negative Prospect of 
an Undesirable Unconfirmed event?. Our goal is to 
represent these literal definitions by rules inter-
playing with the emotion variables so that the sys-
tem can evaluate and get either a ?true? or ?false? 
value. For example, we have an input text txt, that 
has an agent a, associated with an event e, and we 
have a program entity x that detects emotion from 
txt. We can now represent the rule for ?Happy-for? 
emotion as, x senses ?Happy-for? if the following 
condition holds. 
[Linguisitc_Token_found_for_HappyFor(txt) and 
No_Negation_Found (txt)] or [vr =True and sr (e, 
txt) = ?Pleased? and op(e, txt) = ?Desirable? and  
af (x, txt) = ?Liked? and cs (a,x) = ?Other?]    
 
3 Implementation of the Rules 
In this section, we first briefly discuss about the 
SenseNet and its different linguistic resources. 
Then we explain the ?emotion variables?, their 
enumerated values and how the values are assigned 
to the respective variables. 
3.1 SenseNet  
Semantic Parser. The SenseNet has imple-
mented a semantic parser using Machinese Syntax 
(Connexor Oy, 2005) that produces XML-
formatted syntactic output for an input text. For 
example, the sentence, ?My mother presented me a 
nice wrist watch on my birthday and made deli-
cious pancakes.?, the output of the semantic parser 
is shown in Table 2.  
 
Triplet Output of Semantic Parser 
Triplet 1 [['Subject Name:', 'mother', 'Subject 
Type:', 'Person', 'Subject Attrib:', 
['PRON PERS GEN SG1:i']], ['Action 
Name:', 'present', 'Action Status:', 
'Past ', 'Action Attrib:', ['time: my 
birthday', 'Dependency: and']], ['Ob-
ject Name:', 'watch', 'Object Type:', 'N 
NOM SG', 'Object Attrib:', ['Deter-
miner: a', 'A ABS: nice', 'N NOM SG: 
wrist', 'Goal: i']]] 
Triplet 2 [['Subject Name:', 'mother', 'Subject 
Type:', 'Person', 'Subject Attrib:', []], 
['Action Name:', 'make', 'Action 
Status:', 'Past ', 'Action Attrib:', []], 
['Object Name:', 'pancake', 'Object 
Type:', 'N NOM PL', 'Object Attrib:', 
['A ABS: delicious']]] 
Table2.  Semantic Verb-Frames outputted by Se-
mantic Parser 
  
Semantic parser outputs each semantic verb-frame 
of a sentence as a triplet of ?subject, verb, and ob-
ject?. Hence, one obtains multiple triplets if the 
parser encounters multiple verbs in a sentence. In 
our case, we consider each triplet to indicate an 
event encoding the information about ?who is do-
ing what and how?. Therefore, the output given in 
Table 2 has two events, which are dependent to 
each other as indicated by ?dependency? keyword 
in the action attribute of Triplet 1. 
Valenced Output. SenseNet is the implementa-
tion of contextual valence based approach that 
deals with semantic relationships of the words in a 
sentence and assign contextual-valence using a set 
of rules and prior-valence of the words. It outputs a 
numerical value ranging from -15 to +15 flagged 
as the ?sentence-valence? for each input sentence. 
896
For examples, SenseNet outputs -11.158 and 
+10.973 for the inputs, ?The attack killed three 
innocent civilians.? and ?It is difficult to take bad 
photo with this camera.?, respectively. These val-
ues indicate a numerical measure of negative or 
positive sentiments carried by the sentences.  
Scored-list of Action, Adjective, and Adverb. 
SenseNet has initially assigned prior-valence to 
928 verbs, 948 adjectives and 144 adverbs by 
manual investigations of eight judges where the 
inter-agreement among the judges are reported as 
reliable (i.e., the Kappa value is 0.914). The judges 
have manually counted the number of positive and 
negative senses of each word of a selected list ac-
cording to the contextual explanations of each 
sense found in WordNet 2.1. A database of words 
with prior-valence assigned using Equations (1) to 
(3) is developed and scores are stored in the scale 
of -5 to 5. 
Prior-Valence = Average (((Positive-Sense 
Count ? Negative-Sense Count)/Total Sense 
Count) * 5.0) 
(1)
Prospect Polarity = if (Positive-Sense Count > 
Negative-Sense Count) then 1 else -1  
Prospective Valence = Average(max(Positive-
Sense Count, Negative-Sense Count)/Total 
Sense Count) * 5.0*Prospect Polarity) 
(2)
Praiseworthy Valence = Average (Prior-
Valence + Prospective Valence) 
(3)
 
Scored-list of Nouns. SenseNet does an auto-
matic approach to assign prior-valence to nouns by 
employing ConceptNet (Liu and Singh, 2004). A 
value between -5 to 5 is assigned as the valence for 
an unrated noun or concept as follows. To assign a 
prior-valence to a concept, the system collects all 
semantically connected entities that ConceptNet 
returns for the input concept. For example, to get 
the prior-valence for the noun ?rocket?, the system 
failed to find it in the existing knowledgebase, but 
from the action list of the concept the system re-
turned the value 4.112 by averaging the scores of 
the verbs ?carry (4.438)?, ?contain (4.167)?, ?fly 
(3.036)?, ?launch (5.00)? and ?go (3.917)?. 
3.2 Assigning Values to the Emotion Variables 
According to the OCC model, the values for the 
variables self_presumption (sp) and self_reaction 
(sr) are ?Desirable? or ?Undesirable?, and 
?Pleased? or ?Displeased? respectively. For exam-
ple, for the events ?buy ice-cream?, ?present wrist 
watch?, ?kill innocent civilians? referred in the 
example sentences  SenseNet returns contextual 
valence as +7.832, +8.817 and -8.458, respec-
tively. According to SenseNet scoring system the 
valence range for an event (i.e., verb, object pair) 
is ?10. Thereby we decide that for an event if the 
valence is positive (i.e., ?buy ice-cream?), sp and 
sr are set as ?Desirable? and ?Pleased?, and in the 
case of negative valence (i.e., ?Kill innocent civil-
ian?) sp and sr are set to ?Undesirable? and ?Dis-
pleased?, respectively.  
The values for other_presumption (op) could be 
set ?Desirable? or ?Undesirable?. For the sentence 
?A terrorist escaped from the Jail?, the value for 
op (for the event ?escape from jail?) is presumably 
?Desirable? for the agent ?terrorist? but it gets 
?Undesirable? and ?Displeased? for sp and sr be-
cause of negative valence (i.e., -6.715) of the 
event. From SenseNet we get the valence for ter-
rorist as -3.620. Thus in this case we set op as ?De-
sirable? because of having a negative valenced 
event associated with a negative valenced agent. 
Similarly we have the following simple rules to 
assign the values to op. 
? If a positive valenced event is associated with 
a positive valenced agent, op is set ?Desir-
able?. e.g., the Teacher was awarded the best-
teacher award. [(teacher, +4.167) , (award 
best-teacher award, +8.741)]  
? If a negative valenced event is associated with 
a positive valenced agent, op is set ?Undesir-
able?. e.g., the employee was sacked from the 
job.  [(employee, +3.445), (sack from job, -
6.981)]   
? If a positive valenced event is associated with 
a negative valenced agent, op is set ?Undesir-
able?. e.g., the criminal was punished for the 
crime. [(criminal,-3.095), (punish for crime, 
+5.591)] 
In this context and in accordance to the OCC 
model, the value for cognitive_strength (cs) indi-
cates how closely the computer program considers 
selfness. This value is set as ?Self? if the agent de-
scribed in the text is a first person (i.e., I or We); 
otherwise it is set as ?Other?. For the sentence, ?I 
wish I could win the lottery.?, cs is set ?Self?, but 
for the sentence, ?Susan won the million dollar 
lottery.?, cs is set ?Other?. 
According to the OCC model, prospect of an 
event involves a conscious expectation that it will 
897
occur in the future, and the value for the variable 
prospect (pros) can be either ?Positive? or ?Nega-
tive?. In the aforementioned equation (2), Sense-
Net considers either the positive or negative sense-
count (whichever is the maximum for a verb) to 
calculate ?prospective valence? with the notion of 
semantic orientation towards optimistic-pessimistic 
scale. In order to assign pros value to an event we 
also consider the ?prospective valence? of the verb 
instead of ?prior-valence? of that verb. Thus ?posi-
tive? or ?negative? is assigned according to a cer-
tain threshold (i.e., ?3.5) for ?positive? or ?nega-
tive? valence obtained for that event. For example, 
the events ?admit into university?, ?kill innocent 
people?, ?do it?, SenseNet returns  +9.375, -8.728, 
+2.921, respectively and according to this valence, 
pros of the events is set to ?positive?, ?negative? 
and ?null?, respectively.  
The variable status (stat) has the values like: 
?Unconfirmed?, ?Confirmed? and ?Disconfirmed?. 
We decide if the tense of the verb is present or fu-
ture, the value is set to ?Unconfirmed? (e.g., I am 
trying to solve it.); and if it is past or modal with-
out a negation, stat is set ?Confirmed? (e.g., I suc-
ceeded.), but with a negation, stat is set ?Discon-
firmed? (e.g., I did not succeed.). 
If the valence of the agent/object is positive, 
?Liked? is set to the variables agent_fondness (af) 
and object_fondness (of) variables, otherwise 
?Not-liked? is set. For example, for the sentences, 
?The hero appeared to save the girl.?, and ?A ter-
rorist escaped from the Jail?, af for ?hero? and 
?terrorist? is set to ?Liked? and ?Not-Liked? be-
cause of positive and negative valence. Similarly, 
of is set ?Liked? and ?Not-Liked? for ?girl? and 
?Jail? respectively.  
The value for self_appraisal (sa) can be either 
?Praiseworthy? or ?Blameworthy?. In the afore-
mentioned equation (3) SenseNet takes the average 
of ?Prior Valence? and ?Prospective Valence? of a 
verb with the notion of average semantic orienta-
tion of the verb from both good-bad and optimis-
tic-pessimistic perspective. Like assigning pros 
value to an event we consider the ?praiseworthy 
valence? of the verb to assign value to sa. Thereby 
for the same events discussed above to explain 
pros assignment, the value for sa is set ?Praisewor-
thy?, ?Blameworthy? and ?null?, respectively. 
The value of object_appealing (oa) indicates 
whether an object is ?Attractive? or ?Unattractive?. 
In order to assign a value to oa, we deal with two 
scores (i.e., object valence, and familiarity valence) 
having the following heuristic. ?Attractive? is set if 
the object has a positive valence with a familiarity 
valence less than a certain threshold. Reversely 
?Unattractive? is set if the object has a negative 
valence with a familiarity valence above a certain 
threshold. The familiarity valence is obtained from 
the ConceptNet by calculating the percentage of 
nodes (out of 300,000 concept-nodes) linking to 
and from the given object/concept. For example, 
the familiarity valence for ?restaurant?, ?thief? and 
?diamond ring? is 0.242%, 0.120% and 0.013%, 
respectively. Heuristically we kept the threshold 
0.10% to signal familiarity and unfamiliarity of an 
object. Thus ?diamond ring? and ?thief? gets ?At-
tractive? and ?Unattractive? set for oa, but ?restau-
rant? gets ?null? accordingly. 
The value for valenced_reaction (vr) is set either 
?True? or ?False? in order to initiate further analy-
sis to sense emotions or decide the sentence(s) as 
expressing a neutral emotion. We consider vr to be 
?True? if the ?sentence-valence? returned by Sen-
seNet is either above than 3.5 or less than -3.5. For 
example, ?I go.?, doesn?t lead to further process-
ing (i.e., sentence-valence is +3.250) but ?I go to 
gym everyday.?, leads to classify emotion because 
of the sentence-valence +7.351 obtained from Sen-
seNet. The value to the variable unexpectedness 
(unexp) is set ?true? if there is a linguistic token to 
represent suddenness (e.g., abruptly, suddenly, 
swiftly etc.) in the input sentence, otherwise 
?false? is set. We have a list of such tokens to indi-
cate suddenness.  
OCC model has several variables to signify emo-
tional intensity. For example, the value for the in-
tensity variable event_deservingness (ed) is set 
?High? for an event having a higher positive va-
lence (i.e., above +7.0) or ?Low? for higher nega-
tive one (i.e., less than -7.0). If an action is quali-
fied with an adverb (e.g., He worked very hard) or 
target object qualified with an adjective (e.g., I am 
looking for a quiet place) without a negation, the 
value for effort_of_action (eoa) is set ?Obvious?, 
otherwise ?Not-Obvious?. Another variable called 
expected_deviation (edev) indicates the difference 
between the event and its actor. For example, in 
the sentence ?The police caught the criminal fi-
nally.?, the actor ?police? and the event ?catch 
criminal? don?t deviate because the action is pre-
sumably expected by the actor. We set the value 
for edev to ?Low? if ConceptNet can find any se-
898
mantic relationship between the actor and event; 
otherwise ?High? is set. For example, for sentence 
?the student invented the theory.?, edev is set 
?High? because ConceptNet doesn?t return any 
relationship between ?student? and ?invent?. The 
values ?Common? or ?Uncommon? are set for 
event_familiarity (ef) according to the familiarity 
valence obtained from ConceptNet for the input 
event as discussed before. 
4.3 The rules for the OCC Emotion Types 
In section 2.2 we briefly illustrated how a rule 
for the OCC defined emotion (e.g., happy-for) is 
characterized. Now using the same notion we enlist 
the rules for the OCC model defined emotion 
types. Although in txt there might be multiple e 
described and we also deal with such cases to get 
the resultant emotion types from txt, but we don?t 
discuss that in the scope of this paper and describe 
the simple cases. Thus, the rules for emotion types 
are given considering an event e, for example, the 
program x senses ?Joy? for e if following condition 
is true: 
 [Linguisitc_Token_found_for_Joy(txt) and 
No_Negation_Found (txt)] or [vr= true and sr= 
?Pleased? and sp= ?Desirable? and cs= ?Self?] 
(i.e., literally joy means being ?pleased about a de-
sirable event?.) Since we have the token words for 
each emotion types, we omit the first condition in 
the subsequent rules for space limitations. The 
rules for the emotion are listed as following and 
due to space limitations we are not providing the 
rules for all the emotions. 
? if (vr= true and sr= ?Pleased? and pros= ?Posi-
tive? and sp= ?Desirable? and status= ?Uncon-
firmed?), ?hope? is true. 
? if (vr= true and sr = ?Displeased? and pros= 
?Negative? and sp= ?Undesirable? and 
status=?Unconfirmed?), ?fear? is true. 
? if (vr= true and sr = ?Pleased? and pros= 
?Negative? and sp= ?Undesirable? and status= 
?Disconfirmed?), ?relief? is true. 
? if (vr= true and sr = ?Displeased? and pros= 
?Positive? and sp= ?Desirable? and status= 
?Disconfirmed?), ?disappointment? is true. 
? if (vr= true and sr= ?Displeased? and sa= 
?Blameworthy? and sp= ?Undesirable? and 
cs=?Self?), ?shame? is true. 
? if (vr= true and sp= ?Desirable? and sr= 
?Pleased? and of= ?Liked? and oa= ?Attrac-
tive?), ?love? is true. 
? if (vr= true and sp= ?Undesirable? and sr= 
?Displeased? and of= ?Not-Liked? and oa= 
?Unattractive?), ?hate? is true. 
The OCC model has four complex emotions 
namely, ?gratification?, ?remorse?, ?gratitude? and 
?anger?. For example: 
? If both ?joy? and ?pride? are true, ?gratifica-
tion? is also true. 
? If both ?distress? and ?reproach? are true, ?an-
ger? is also true. 
The cognitive states ?Shock? (i.e.; unpleasant sur-
prise) and ?Surprise? (i.e., pleasant surprise) are 
ruled as; If both ?distress? and unexp are true, 
?shock? is true. (e.g., The bad news came unex-
pectedly.). Similarly, if both ?joy? and unexp are 
true, ?surprise? is true. (e.g., I suddenly met my 
school friend in Tokyo.) 
Like Liu et al (2003), we also believe that a 
statement may contain more than one type of emo-
tions. In our case, the 22 emotion types and two 
cognitive states are grouped into seven groups, 
namely, well-being emotion, fortune of other emo-
tion, prospect based emotion, cognitive state, attri-
bution emotion, attraction emotion, and compound 
emotion. Hence an input sentence may contain one 
of the emotion types from each group. For exam-
ple, the sentence ?I suddenly got to know that my 
paper won the best paper award.?, outputs the fol-
lowing emotions: {Joy, Satisfaction, Surprise, 
Pride, Gratification}.The sentence ?She failed to 
pass the entrance examination.?, outputs {Dis-
tress, Sorry-for, Disappointment, Reproach, An-
ger} emotion types. In order to reduce the number 
of emotions, we consider the intensity variables. 
For the first set of emotions, we can reduce it to 
{Satisfaction, Surprise, Pride} because ?Joy? 
doesn?t have any intensity variables and the inten-
sity variables ed and edev are set to ?High? in this 
case. 
4 Test and Evaluation 
The similar system like ours is Liu?s system (Liu 
et. al., 2003). It is a rule based system, and it seems 
to be the best performing system for sentence-level 
affect sensing that senses happy, fearful, sad, an-
gry, disgust, and surprise emotions. On the practi-
cal side, it is freely available on the Internet. Ex-
899
ample input and output are enlisted to given an 
idea about the outputs of the two systems.   
Input: I avoided the accident luckily. 
Liu?s output: fearful(26%),happy (18%), angry 
(12%),sad(8%),surprised(7%),disgusted (0%) 
Ours output: valence: +11.453; [joy, pride, relief, 
surprise, gratification] 
Input: Susan bought a lottery ticket and she was 
lucky to win the million dollar lottery. 
Liu?s output: sad(21%), happy(18%), fearful 
(13%),angry(11%),disgusted(0%),surprised (0%) 
Ours: valence: +12.533; [happy-for, satisfaction, ad-
miration, love] 
We evaluated our system to assess the accuracy 
of sentence-level affect sensing when compared to 
human-ranked scores (as ?gold standard?) for 200 
sentences assessed by two systems. The sentences 
were collected from Internet based sources for re-
views of products, movies, and news. In order to 
conduct system?s performance and acceptance test 
we have two systems X (i.e., Liu?s System) and Y 
(i.e., our system). The judges were not told about 
the characteristics of any of the systems. Each 
judge receives the output from both X and Y for 
each input sentence and can accept either both out-
puts or anyone of the two or reject both. Thus %X 
means the percentage of the number of acceptances 
received by X in terms of accuracy of output. Simi-
larly %Y, %XY, and %!XY indicate the percent-
age of acceptances received by the system Y, both 
the systems and neither of the two systems respec-
tively. For example, for the input sentence ?She is 
extremely generous, but not very tolerant with peo-
ple who don't agree with her.?, among the 5 judges 
3 accepted the output of Y, 2 accepted the output 
of X. Since the majority of the judges accepted Y, 
vote for this sentence was counter for Y. Thus the 
vote for each sentence is counted. Outcome of our 
experiment is reported below while the valence 
range to classify a neutral sentence is considered 
?3.5 for the SenseNet upon which system Y is 
built.  
System Y received 16.069% more acceptances 
than that of X, which indicates that the output of Y 
is more acceptable and accurate than that of X. 
Though the test was conducted with a small group 
of judges with relatively small input size, but the 
experiment result (i.e., 82% accuracy with an aver-
age precision 76.49%, recall 81.04% and F-score 
78% for classifying positive, negative and neutral 
classes using the same dataset) for sentiment sens-
ing reported by SenseNet, provides an optimistic 
believe that the result would not vary even the sur-
vey is conducted with larger group of judges. Ta-
ble 3 summarizes the experimental result for 200 
sentences. 
Data-Set of 200 Sentences 
%X %Y %XY %!XY 
20.344 36.413 24.283 18.96 
 Table 3. Experimental Result 
5  Conclusion 
In order to perform more testing and usability 
study, we plan to implement a web-based user in-
terface where any user can input a chunk of text 
and get outputs from the both systems mentioned 
above. Thereby we can get user?s acceptance test 
in terms of accuracy of output. Next we plan to 
perform the task of affect sensing using online re-
sources (e.g., blogs, reviews, etc.).  
Reference 
Connexor Oy. 2005. Machinese Syntax, web-site: 
http://www.connexor.com/connexor/ 
Hugo Liu and Push Singh. 2004. ConceptNet: A Practi-
cal Commonsense Reasoning Toolkit, BT Technol-
ogy Journal, 22(4):211-226, Kluwer Academic Pub-
lishers. 
Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A 
Model of Textual Affect Sensing using Real-World 
Knowledge, In Proc. IUI 03, pp. 125-132, Miami, 
USA, ACM. 
Opinmind, Discovering Bloggers, (2006), 
http://www.opinmind.com/ 
Andrew Ortony, Gerald L. Clore and Allan Collins. 
1988. The Cognitive Structure of Emotions, Cam-
bridge University Press. 
James W. Pennebaker, Martha E. Francis, and Roger J. 
Booth. 2001. Linguistic inquiry and word count: 
LIWC (2nd ed.) [Computer software]. Mahwah, NJ: 
Erlbaum. 
Mostafa A. M. Shaikh, Helmut Prendinger and Mitsuru 
Ishizuka. 2007a. SenseNet: A Linguistic Tool to 
Visualize Numerical-Valance Based Sentiment of 
Textual Data, In Proc. ICON-2007, pages 147-152. 
Mostafa A. M. Shaikh, Helmut Prendinger and Mitsuru 
Ishizuka. 2007b. Assessing Sentiment of Text by 
Semantic Dependency and Contextual Valence 
Analysis. In Proc. ACII 07, pp. 191-202. 
James G. Shanahan, Yan Qu and Janyce Wiebe (Eds.). 
2006. Computing Attitude and Affect in Text: The-
ory and Applications, Springer. 
900
Proceedings of NAACL HLT 2007, pages 340?347,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Integrated Approach to Measuring Semantic Similarity between Words
Using Information available on the Web
Danushka Bollegala
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@mi.ci.i.u-
tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and
Technology
1-18-13, Sotokanda, Tokyo,
101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
ishizuka@i.u-
tokyo.ac.jp
Abstract
Measuring semantic similarity between
words is vital for various applications
in natural language processing, such as
language modeling, information retrieval,
and document clustering. We propose a
method that utilizes the information avail-
able on the Web to measure semantic sim-
ilarity between a pair of words or entities.
We integrate page counts for each word in
the pair and lexico-syntactic patterns that
occur among the top ranking snippets for
the AND query using support vector ma-
chines. Experimental results on Miller-
Charles? benchmark data set show that the
proposed measure outperforms all the ex-
isting web based semantic similarity mea-
sures by a wide margin, achieving a cor-
relation coefficient of 0.834. Moreover,
the proposed semantic similarity measure
significantly improves the accuracy (F -
measure of 0.78) in a named entity cluster-
ing task, proving the capability of the pro-
posed measure to capture semantic simi-
larity using web content.
1 Introduction
The study of semantic similarity between words has
been an integral part of natural language processing
and information retrieval for many years. Semantic
similarity measures are vital for various applications
in natural language processing such as word sense
disambiguation (Resnik, 1999), language model-
ing (Rosenfield, 1996), synonym extraction (Lin,
1998a) and automatic thesaurus extraction (Curran,
2002).
Pre-compiled taxonomies such as WordNet 1 and
text corpora have been used in previous work on se-
mantic similarity (Lin, 1998a; Resnik, 1995; Jiang
and Conrath, 1998; Lin, 1998b). However, seman-
tic similarity between words change over time as
new senses and associations of words are constantly
created. One major issue behind taxonomies and
corpora oriented approaches is that they might not
necessarily capture similarity between proper names
such as named entities (e.g., personal names, loca-
tion names, product names) and the new uses of ex-
isting words. For example, apple is frequently asso-
ciated with computers on the Web but this sense of
apple is not listed in the WordNet. Maintaining an
up-to-date taxonomy of all the new words and new
usages of existing words is costly if not impossible.
The Web can be regarded as a large-scale, dy-
namic corpus of text. Regarding the Web as a live
corpus has become an active research topic recently.
Simple, unsupervised models have shown to per-
form better when n-gram counts are obtained from
the Web rather than from a large corpus (Keller and
Lapata, 2003; Lapata and Keller, 2005). Resnik and
Smith (2003) extract bilingual sentences from the
Web to create parallel corpora for machine trans-
lation. Turney (2001) defines a point wise mutual
information (PMI-IR) measure using the number of
hits returned by a Web search engine to recognize
synonyms. Matsuo et. al, (2006b) follows a similar
1http://wordnet.princeton.edu/
340
approach to measure the similarity between words
and apply their method in a graph-based word clus-
tering algorithm.
Due to the huge number of documents and the
high growth rate of the Web, it is difficult to di-
rectly analyze each individual document separately.
Search engines provide an efficient interface to this
vast information. Page counts and snippets are two
useful information sources provided by most Web
search engines. Page count of a query is the number
of pages that contain the query words 2. A snippet is
a brief window of text extracted by a search engine
around the query term in a document. Snippets pro-
vide useful information about the immediate context
of the query term.
This paper proposes a Web-based semantic simi-
larity metric which combines page counts and snip-
pets using support vector machines. We extract
lexico-syntactic patterns from snippets. For exam-
ple, X is a Y indicates there is a high semantic sim-
ilarity between X and Y. Automatically extracted
lexico-syntactic patterns have been successfully em-
ployed in various term extraction tasks (Hearst,
1992).
Our contributions are summarized as follows:
? We propose a lexico-syntactic patterns-based
approach to compute semantic similarity using
snippets obtained from a Web search engine.
? We integrate different Web-based similarity
scores using WordNet synsets and support vec-
tor machines to create a robust semantic sim-
ilarity measure. The integrated measure out-
performs all existing Web-based semantic sim-
ilarity measures in a benchmark dataset and a
named entity clustering task. To the best of
our knowledge, this is the first attempt to com-
bine both WordNet synsets and Web content to
leverage a robust semantic similarity measure.
2 Previous Work
Given a taxonomy of concepts, a straightforward
method for calculating similarity between two words
(concepts) is to find the length of the shortest path
2page count may not necessarily be equal to the word fre-
quency because the queried word may appear many times in a
page
connecting the two words in the taxonomy (Rada
et al, 1989). If a word is polysemous (i.e., having
more than one sense) then multiple paths may ex-
ist between the two words. In such cases only the
shortest path between any two senses of the words is
considered for the calculation of similarity. A prob-
lem frequently acknowledged with this approach is
that it relies on the notion that all links in the taxon-
omy represent uniform distances.
Resnik (1995) proposes a similarity measure
based on information content. He defines the sim-
ilarity between two concepts C1 and C2 in the tax-
onomy as the maximum of the information content
of all concepts C that subsume both C1 and C2.
Then the similarity between two words are defined
as the maximum of the similarity between any con-
cepts that the words belong to. He uses WordNet as
the taxonomy and information content is calculated
using the Brown corpus.
Li et al, (2003) combines structural semantic in-
formation from a lexical taxonomy and informa-
tion content from a corpus in a non-linear model.
They propose a similarity measure that uses shortest
path length, depth and local density in a taxonomy.
Their experiments using WordNet and the Brown
corpus reports a Pearson correlation coefficient of
0.8914 on the Miller and Charles? (1998) bench-
mark dataset. They do not evaluate their method on
similarities between named entities. Recently, some
work has been carried out on measuring semantic
similarity using web content. Matsuo et al, (2006a)
propose the use of Web hits for the extraction of
communities on the Web. They measure the associ-
ation between two personal names using the overlap
coefficient, calculated based on the number of Web
hits for each individual name and their conjunction.
Sahami et al, (2006) measure semantic similarity
between two queries using the snippets returned for
those queries by a search engine. For each query,
they collect snippets from a search engine and rep-
resent each snippet as a TF-IDF weighted term vec-
tor. Each vector is L2 normalized and the centroid
of the set of vectors is computed. Semantic similar-
ity between two queries is then defined as the inner
product between the corresponding centroid vectors.
They do not compare their similarity measure with
taxonomy based similarity measures.
Chen et al, (2006) propose a web-based double-
341
checking model to compute semantic similarity be-
tween words. For two words P and Q, they col-
lect snippets for each word from a web search en-
gine. Then they count the number of occurrences of
word P in the snippets for word Q and the number
of occurrences of word Q in the snippets for word
P . These values are combined non-linearly to com-
pute the similarity between P and Q. This method
heavily depends on the search engine?s ranking al-
gorithm. Although two words P and Q may be very
similar, there is no reason to believe that one can find
Q in the snippets for P , or vice versa. This observa-
tion is confirmed by the experimental results in their
paper which reports 0 similarity scores for many
pairs of words in the Miller and Charles (1998) data
set.
3 Method
In this section we will describe the various similarity
features we use in our model. We utilize page counts
and snippets returned by the Google 3 search engine
for simple text queries to define various similarity
scores.
3.1 Page Counts-based Similarity Scores
For the rest of this paper we use the notation H(P )
to denote the page count for the query P in a search
engine. Terra and Clarke (2003) compare various
similarity scores for measuring similarity between
words in a corpus. We modify the traditional Jac-
card, overlap (Simpson), Dice and PMI measures
for the purpose of measuring similarity using page
counts. WebJaccard coefficient between words (or
phrases) P and Q, WebJaccard(P,Q), is defined
by,
WebJaccard(P,Q)
=
{ 0 if H(P ?Q) ? c
H(P?Q)
H(P )+H(Q)?H(P?Q) otherwise
.(1)
Here, P ? Q denotes the conjunction query P AND
Q. Given the scale and noise in the Web, some words
might occur arbitrarily, i.e. by random chance, on
some pages. Given the scale and noise in web data, it
is a possible that two words man order to reduce the
adverse effect due to random co-occurrences, we set
3http://www.google.com
the WebJaccard coefficient to zero if the page counts
for the query P ?Q is less than a threshold c. 4
Likewise, we define WebOverlap coefficient,
WebOverlap(P,Q), as,
WebOverlap(P,Q)
=
{ 0 if H(P ?Q) ? c
H(P?Q)
min(H(P ),H(Q)) otherwise
.(2)
We define WebDice as a variant of Dice coeffi-
cient. WebDice(P,Q) is defined as,
WebDice(P,Q)
=
{ 0 if H(P ?Q) ? c
2H(P?Q)
H(P )+H(Q) otherwise
. (3)
We define WebPMI as a variant form of PMI using
page counts by,
WebPMI(P,Q)
=
?
?
?
0 if H(P ?Q) ? c
log2(
H(P?Q)
N
H(P )
N
H(Q)
N
) otherwise .(4)
Here, N is the number of documents indexed by the
search engine. Probabilities in Formula 4 are esti-
mated according to the maximum likelihood princi-
ple. In order to accurately calculate PMI using For-
mula 4, we must know N , the number of documents
indexed by the search engine. Although estimating
the number of documents indexed by a search en-
gine (Bar-Yossef and Gurevich, 2006) is an interest-
ing task itself, it is beyond the scope of this work. In
this work, we set N = 1010 according to the number
of indexed pages reported by Google.
3.2 Snippets-based Synonymous Word
Patterns
Page counts-based similarity measures do not con-
sider the relative distance between P and Q in a page
or the length of the page. Although P and Q occur
in a page they might not be related at all. Therefore,
page counts-based similarity measures are prone to
noise and are not reliable when H(P ?Q) is low. On
the other hand snippets capture the local context of
query words. We propose lexico-syntactic patterns
extracted from snippets as a solution to the problems
with page counts-based similarity measures.
4we set c = 5 in our experiments
342
To illustrate our pattern extraction algorithm con-
sider the following snippet from Google for the
query jaguar AND cat.
?The Jaguar is the largest cat in Western Hemi-
sphere and can subdue a larger prey than can the
puma?
Here, the phrase is the largest indicates a hy-
pernymic relationship between Jaguar and the cat.
Phrases such as also known as, is a, part of, is an ex-
ample of all indicate various of semantic relations.
Such indicative phrases have been successfully ap-
plied in various tasks such as synonym extraction,
hyponym extraction (Hearst, 1992) and fact extrac-
tion (Pasca et al, 2006).
We describe our pattern extraction algorithm in
three steps.
Step 1
We replace the two query terms in a snippet by two
wildcards X and Y. We extract all word n-grams that
contain both X and Y. In our experiments we ex-
tracted n-grams for n = 2 to 5. For example, from
the previous snippet we extract the pattern, X is the
largest X. In order to leverage the pattern extraction
process, we randomly select 5000 pairs of synony-
mous nouns from WordNet synsets. We ignore the
nouns which do not have synonyms in the WordNet.
For nouns with more than one sense, we select syn-
onyms from its dominant sense. For each pair of
synonyms (P,Q), we query Google for ?P? AND
?Q? and download the snippets. Let us call this col-
lection of snippets as the positive corpus. We apply
the above mentioned n-gram based pattern extrac-
tion procedure and count the frequency of each valid
pattern in the positive corpus.
Step 2
Pattern extraction algorithm described in step 1
yields 4, 562, 471 unique patterns. 80%of these pat-
terns occur less than 10 times in the positive corpus.
It is impossible to learn with such a large number of
sparse patterns. Moreover, some patterns might oc-
cur purely randomly in a snippet and are not good
indicators of semantic similarity. To measure the
reliability of a pattern as an indicator of semantic
similarity we employ the following procedure. We
create a set of non-synonymous word-pairs by ran-
domly shuffling the words in our data set of synony-
Table 1: Contingency table
v other than v All
Freq. in positive corpus pv P ? pv P
Freq. in negative corpus nv N ? nv N
mous word-pairs. We check each pair of words in
this newly created data set against WordNet and con-
firm that they do not belong to any of the synsets
in the WordNet. From this procedure we created
5000 non-synonymous pairs of words. For each
non-synonymous word-pair, we query Google for
the conjunction of its words and download snippets.
Let us call this collection of snippets as the nega-
tive corpus. For each pattern generated in step 1, we
count its frequency in the negative corpus.
Step 3
We create a contingency table as shown in Table 1
for each pattern v extracted in step 1 using its fre-
quency pv in positive corpus and nv in negative cor-
pus. In Table 1, P denotes the total frequency of all
patterns in the positive corpus and N denotes that in
the negative corpus.
Using the information in Table 1, we calculate
?2 (Manning and Schu?tze, 2002) value for each pat-
tern as,
?2 = (P +N)(pv(N ? nv)? nv(P ? pv))
2
PN(pv + nv)(P +N ? pv ? nv) .
(5)
We selected the top ranking 200 patterns experimen-
tally as described in section 4.2 according to their ?2
values. Some of the selected patterns are shown in
Table 2.
3.3 Training
For each pair of synonymous and non-synonymous
words in our datasets, we count the frequency of
occurrence of the patterns selected in Step 3. We
normalize the frequency count of each pattern by
dividing from the total frequency of all patterns.
Moreover, we compute the page counts-based fea-
tures as given by formulae (1-4). Using the 200
pattern features and the 4 page counts-based fea-
tures we create 204 dimensional feature vectors for
each training instance in our synonymous and non-
synonymous datasets. We train a two class support
vector machine (SVM) (Vapnik, 1998), where class
343
+1 represents synonymous word-pairs and class
?1 represents non-synonymous word-pairs. Finally,
SVM outputs are converted to posterior probabilities
(Platt, 2000). We consider the posterior probability
of a given pair of words belonging to class +1 as the
semantic similarity between the two words.
4 Experiments
To evaluate the performance of the proposed se-
mantic similarity measure, we conduct two sets of
experiments. Firstly, we compare the similarity
scores produced by the proposed measure against
the Miller-Charles? benchmark dataset. We analyze
the performance of the proposed measure with the
number of snippets and the size of the training data
set. Secondly, we apply the proposed measure in a
real-world named entity clustering task and measure
its performance.
4.1 The Benchmark Dataset
We evaluated the proposed method against Miller-
Charles (1998) dataset, a dataset of 30 5 word-pairs
rated by a group of 38 human subjects. Word-
pairs are rated on a scale from 0 (no similarity) to
4 (perfect synonymy). Miller-Charles? dataset is
a subset of Rubenstein-Goodenough?s (1965) orig-
inal dataset of 65 word-pairs. Although Miller-
Charles? experiment was carried out 25 years
later than Rubenstein-Goodenough?s, two sets of
ratings are highly correlated (Pearson correlation
coefficient=0.97). Therefore, Miller-Charles ratings
can be considered as a reliable benchmark for eval-
uating semantic similarity measures.
4.2 Pattern Selection
We trained a linear kernel SVM with top N pattern
features (ranked according to their ?2 values) and
calculated the Pearson correlation coefficient against
the Miller-Charles? benchmark dataset. Experimen-
tal results are shown in Figure 1. From Figure 1
we select N = 200, where correlation maximizes.
Features with the highest linear kernel weights are
shown in Table 2 alongside with their ?2 values. The
weight of a feature in the linear kernel can be consid-
ered as a rough estimate of the influence it has on the
5Due to the omission of two word-pairs in earlier versions
of WordNet most researchers had used only 28 pairs for evalu-
ations
0 200 400 600 800 1000120014001600180020000.780
0.782
0.784
0.786
0.788
0.790
0.792
0.794
0.796
0.798
0.800
C
or
re
la
tio
n 
C
oe
ffi
ci
en
t (
r)
Number of pattern features (N)
Figure 1: Correlation vs No of pattern features
Table 2: Features with the highest SVM linear ker-
nel weights
feature ?2 SVM weight
WebDice N/A 8.19
X/Y 33459 7.53
X, Y : 4089 6.00
X or Y 3574 5.83
X Y for 1089 4.49
X . the Y 1784 2.99
with X ( Y 1819 2.85
X=Y 2215 2.74
X and Y are 1343 2.67
X of Y 2472 2.56
final SVM output. WebDice has the highest linear
kernel weight followed by a series of patterns-based
features. WebOverlap (rank=18, weight=2.45), We-
bJaccard (rank=66, weight=0.618) and WebPMI
(rank=138, weight=0.0001) are not shown in Table 2
due to space limitations. It is noteworthy that the
pattern features in Table 2 agree with the intuition.
Lexical patterns (e.g., X or Y, X and Y are, X of Y) as
well as syntactic patterns (e.g., bracketing, comma
usage) are extracted by our method.
4.3 Semantic Similarity
We score the word-pairs in Miller-Charles dataset
using the page counts-based similarity measures,
previous work on web-based semantic similarity
measures (Sahami (2006), Chen (2006)) and the
proposed method (SVM). Results are shown in Ta-
ble 4.3. All figures except for the Miller-Charles
ratings are normalized into [0, 1] range for the ease
of comparison 6. Proposed method (SVM) re-
6Pearson correlation coefficient is invariant against a linear
transformation
344
Table 3: Semantic Similarity of Human Ratings and baselines on Miller-Charles dataset
Word Pair Miller- Web Web Web Web Sahami Chen (CODC) Proposed
Charles Jaccard Dice Overlap PMI (2006) (2006) (SVM)
cord-smile 0.13 0.102 0.108 0.036 0.207 0.090 0 0
rooster-voyage 0.08 0.011 0.012 0.021 0.228 0.197 0 0.017
noon-string 0.08 0.126 0.133 0.060 0.101 0.082 0 0.018
glass-magician 0.11 0.117 0.124 0.408 0.598 0.143 0 0.180
monk-slave 0.55 0.181 0.191 0.067 0.610 0.095 0 0.375
coast-forest 0.42 0.862 0.870 0.310 0.417 0.248 0 0.405
monk-oracle 1.1 0.016 0.017 0.023 0 0.045 0 0.328
lad-wizard 0.42 0.072 0.077 0.070 0.426 0.149 0 0.220
forest-graveyard 0.84 0.068 0.072 0.246 0.494 0 0 0.547
food-rooster 0.89 0.012 0.013 0.425 0.207 0.075 0 0.060
coast-hill 0.87 0.963 0.965 0.279 0.350 0.293 0 0.874
car-journey 1.16 0.444 0.460 0.378 0.204 0.189 0.290 0.286
crane-implement 1.68 0.071 0.076 0.119 0.193 0.152 0 0.133
brother-lad 1.66 0.189 0.199 0.369 0.644 0.236 0.379 0.344
bird-crane 2.97 0.235 0.247 0.226 0.515 0.223 0 0.879
bird-cock 3.05 0.153 0.162 0.162 0.428 0.058 0.502 0.593
food-fruit 3.08 0.753 0.765 1 0.448 0.181 0.338 0.998
brother-monk 2.82 0.261 0.274 0.340 0.622 0.267 0.547 0.377
asylum-madhouse 3.61 0.024 0.025 0.102 0.813 0.212 0 0.773
furnace-stove 3.11 0.401 0.417 0.118 1 0.310 0.928 0.889
magician-wizard 3.5 0.295 0.309 0.383 0.863 0.233 0.671 1
journey-voyage 3.84 0.415 0.431 0.182 0.467 0.524 0.417 0.996
coast-shore 3.7 0.786 0.796 0.521 0.561 0.381 0.518 0.945
implement-tool 2.95 1 1 0.517 0.296 0.419 0.419 0.684
boy-lad 3.76 0.186 0.196 0.601 0.631 0.471 0 0.974
automobile-car 3.92 0.654 0.668 0.834 0.427 1 0.686 0.980
midday-noon 3.42 0.106 0.112 0.135 0.586 0.289 0.856 0.819
gem-jewel 3.84 0.295 0.309 0.094 0.687 0.211 1 0.686
Correlation 1 0.259 0.267 0.382 0.548 0.579 0.693 0.834
ports the highest correlation of 0.8129 in our ex-
periments. Our implementation of Co-occurrence
Double Checking (CODC) measure (Chen et al,
2006) reports the second best correlation of 0.6936.
However, CODC measure reports zero similarity for
many word-pairs. This is because for a word-pair
(P,Q), we might not necessarily find Q among the
top snippets for P (and vice versa). CODC mea-
sure returns zero under these conditions. Sahami
et al (2006) is ranked third with a correlation of
0.5797. Among the four page counts based mea-
sures WebPMI reports the highest correlation (r =
0.5489). Overall, the results in Table 4.3 suggest
that snippet-based measures are more accurate than
page counts-based measures in capturing semantic
similarity. This is evident for word-pairs where at
least one of the words is a polysemous word (e.g.,
pairs that include cock, brother). Page counts-based
measures do not consider the context in which the
words appear in a page, thus cannot disambiguate
Table 4: Comparison with taxonomy based methods
Method correlation
Human replication 0.901
Resnik (1995) 0.745
Lin (1998) 0.822
Li et al(2003) 0.891
Edge-counting 0.664
Information content 0.745
Jiang & Conrath (1998) 0.848
proposed (SVM) 0.834
the multiple senses.
As summarized in Table 4.3, proposed method
is comparable with the WordNet based methods.
In fact, the proposed method outperforms simple
WordNet based approaches such as Edge-Counting
and Information Content measures. However, con-
sidering the high correlation between human sub-
jects (0.9), there is still room for improvement.
Figure 2 illustrates the effect of the number
of snippets on the performance of the proposed
345
0 100 200 300 400 500 600 700 800 900 10000.70
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.80
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Number of snippets
Figure 2: Correlation vs No of snippets
 500  1000  1500  2000  2500  3000  3500  4000negative examples  500
 1000 1500
 2000 2500
 3000 3500
 4000
positive examples
 0.45 0.5
 0.55 0.6
 0.65 0.7
 0.75 0.8
 0.85
correlation
Figure 3: Correlation vs No of positive and negative
training instances
method. Correlation coefficient steadily improves
with the number of snippets used for extracting pat-
terns. When few snippets are processed only a few
patterns are found, thus the feature vector becomes
sparse, resulting in poor performance. Figure 3 de-
picts the correlation with human ratings for various
combinations of positive and negative training in-
stances. Maximum correlation coefficient of 0.834
is achieved with 1900 positive training examples and
2400 negative training examples. Moreover, Fig-
ure 3 reveals that correlation does not improve be-
yond 2500 positive and negative training examples.
Therefore, we can conclude that 2500 examples are
sufficient to leverage the proposed semantic similar-
ity measure.
4.4 Named Entity Clustering
Measuring semantic similarity between named en-
tities is vital in many applications such as query
expansion (Sahami and Heilman, 2006) and com-
munity mining (Matsuo et al, 2006a). Since most
named entities are not covered by WordNet, simi-
larity measures based on WordNet alne cannot be
Table 5: Performance of named entity clustering
Method Precision Recall F Measure
WebJaccard 0.5926 0.712 0.6147
WebOverlap 0.5976 0.68 0.5965
WebDice 0.5895 0.716 0.6179
WebPMI 0.2649 0.428 0.2916
Sahami (2006) 0.6384 0.668 0.6426
Chen (2006) 0.4763 0.624 0.4984
Proposed 0.7958 0.804 0.7897
used in such tasks. Unlike common English words,
named entities are constantly being created. Manu-
ally maintaining an up-to-date taxonomy of named
entities is costly, if not impossible. The proposed
semantic similarity measure is appealing as it does
not require pre-compiled taxonomies. In order to
evaluate the performance of the proposed measure
in capturing the semantic similarity between named
entities, we set up a named entity clustering task.
We selected 50 person names from 5 categories :
tennis players, golfers, actors, politicians and scien-
tists, (10 names from each category) from the dmoz
directory 7. For each pair of names in our dataset,
we measure the association between the two names
using the proposed method and baselines. We use
group-average agglomerative hierarchical clustering
to cluster the names in our dataset into five clusters.
We employed the B-CUBED metric (Bagga and
Baldwin, 1998) to evaluate the clustering results. As
summarized in Table 5 the proposed method outper-
forms all the baselines with a statistically significant
(p ? 0.01 Tukey HSD) F score of 0.7897.
5 Conclusion
We propose an SVM-based approach to combine
page counts and lexico-syntactic patterns extracted
from snippets to leverage a robust web-based seman-
tic similarity measure. The proposed similarity mea-
sure outperforms existing web-based similarity mea-
sures and competes with models trained on Word-
Net. It requires just 2500 synonymous word-pairs,
automatically extracted from WordNet synsets, for
training. Moreover, the proposed method proves
useful in a named entity clustering task. In future,
we intend to apply the proposed method to automat-
ically extract synonyms from the web.
7http://dmoz.org
346
References
A. Bagga and B. Baldwin. 1998. Entity-based cross doc-
ument coreferencing using the vector space model. In
Proc. of 36th COLING-ACL, pages 79?85.
Z. Bar-Yossef and M. Gurevich. 2006. Random sam-
pling from a search engine?s index. In Proceedings of
15th International World Wide Web Conference.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proc. of the COLING/ACL 2006, pages 1009?1016.
J. Curran. 2002. Ensemble menthods for automatic the-
saurus extraction. In Proc. of EMNLP.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of 14th COLING,
pages 539?545.
J.J. Jiang and D.W. Conrath. 1998. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the International Conference on Research in
Computational Linguistics ROCLING X.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
M. Lapata and F. Keller. 2005. Web-based models ofr
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1?31.
D. Lin. 1998a. Automatic retreival and clustering of sim-
ilar words. In Proc. of the 17th COLING, pages 768?
774.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proc. of the 15th ICML, pages 296?304.
C. D. Manning and H. Schu?tze. 2002. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Y. Matsuo, J. Mori, M. Hamasaki, K. Ishida,
T. Nishimura, H. Takeda, K. Hasida, and M. Ishizuka.
2006a. Polyphonet: An advanced social network ex-
traction system. In Proc. of 15th International World
Wide Web Conference.
Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.
2006b. Graph-based word clustering using web search
engine. In Proc. of EMNLP 2006.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts - step one: the one-million fact extraction chal-
lenge. In Proc. of AAAI-2006.
J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. Advances in Large Margin Classifiers, pages
61?74.
R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. IEEE Transactions on Systems, Man and Cyber-
netics, 9(1):17?30.
P. Resnik and N. A. Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of
14th International Joint Conference on Aritificial In-
telligence.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information based measure and its application to prob-
lems of ambiguity in natural language. Journal of Ar-
itificial Intelligence Research, 11:95?130.
R. Rosenfield. 1996. A maximum entropy approach to
adaptive statistical modelling. Computer Speech and
Language, 10:187?228.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8:627?633.
M. Sahami and T. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proc. of 15th International World Wide
Web Conference.
E. Terra and C.L.A. Clarke. 2003. Frequency estimates
for statistical word similarity measures. In Proc. of the
NAACL/HLT, pages 165?172.
P. D. Turney. 2001. Minning the web for synonyms:
Pmi-ir versus lsa on toefl. In Proc. of ECML-2001,
pages 491?502.
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, GB.
D. McLean Y. Li, Zuhair A. Bandar. 2003. An approch
for measuring semantic similarity between words us-
ing multiple information sources. IEEE Transactions
on Knowledge and Data Engineering, 15(4):871?882.
347
Proceedings of NAACL HLT 2007, Companion Volume, pages 125?128,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Subtree Mining for Relation Extraction from Wikipedia
Dat P.T. Nguyen
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
nptdat@mi.ci.i.u-tokyo.ac.jp
Yutaka Matsuo
National Institute of Advanced
Industrial Science and Technology
Sotokanda 1-18-13
Tokyo 101-0021, Japan
y.matsuo@aist.go.jp
Mitsuru Ishizuka
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
ishizuka@i.u-tokyo.ac.jp
Abstract
In this study, we address the problem of extract-
ing relations between entities from Wikipedia?s
English articles. Our proposed method first an-
chors the appearance of entities in Wikipedia?s
articles using neither Named Entity Recognizer
(NER) nor coreference resolution tool. It then
classifies the relationships between entity pairs
using SVM with features extracted from the
web structure and subtrees mined from the
syntactic structure of text. We evaluate our
method on manually annotated data from ac-
tual Wikipedia articles.
1 Introduction
Wikipedia (www.wikipedia.org) has emerged as the
world?s largest online encyclopedia. Because the ency-
clopedia is managed by the Wikipedia Foundation, and
because numerous collaborators in the world continu-
ously develop and edit its articles, its contents are be-
lieved to be quite reliable despite its openness.
This study is intended to deal with the problem of
extracting binary relations between entity pairs from
Wikipedia?s English version. A binary relation is defined
as a triple (ep, rel, es) in which ep and es are entities and
rel indicates a directed relationship of ep and es. Current
experiment limits entities and relations to a reasonable
size in that an entity is classifiable as person, organiza-
tion, location, artifact, year, month or date; and a rela-
tion can be founder, chairman, CEO, COO, president,
director, vice chairman, spouse, birth date, birth place,
foundation, product and location.
To our knowledge, only one recent work has at-
tempted relation extraction on Wikipedia: (Culotta et al,
2006) presents a probabilistic model to integrate extrac-
tion and mining tasks performed on biographical text of
Wikipedia. Some other works (Brin, 1998; Agichtein and
Gravano, 2000; Ravichandran and Hovy, 2002) rely on
the abundance of web data to obtain easy patterns and
learn such patterns based mostly on lexical information.
Rather than analyzing dependency path between entity
pair proposed in (Bunescu and Mooney, 2006; Cui et al,
2005), our method analyzes a subtree derived from the
dependency structure. Such subtree contains more evi-
dence of the entities? inter-relation than the path in some
cases. We propose a new feature obtained from the sub-
tree by using a subtree-mining technique.
In addition, we also make use of the characteristics of
Wikipedia to allocate the mentions of entities and further
identify their types to help the relation extraction process.
2 Wikipedia?s Article Characteristics
Due to the encyclopedic style, each Wikipedia article
mainly provides information for a specific entity and fur-
ther mentions other entities related to it. Culotta et al
(2006) defines the entities as principal entity and sec-
ondary entity respectively. We predict only relationships
between the principal entity and each mentioned sec-
ondary entity that contains a link to its descriptive article.
We put some assumptions in this study: a relation-
ship can be expressed completely in one sentence. Fur-
thermore, a relationship between an entity pair might be
expressed with the implication of the principal entity in
some cases. Thus, for an article, only sentences contain-
ing at least a secondary entity are necessarily analyzed.
An interesting characteristic of Wikipedia is the cate-
gory hierarchy that is used to classify articles according to
their content. Additionally, those articles for famous en-
tities provide summary sections on their right side, which
are created by human editors. Finally, the first sentence
of an article often defines the principal entity.
3 Proposed Method
Figure 1 delineates our framework for relation extrac-
tion. First, Wikipedia articles are processed to remove
HTML tags and to extract hyperlinks that point to other
Wikipedia articles. Raw text is submitted to a pipeline
including a Sentence Splitter, a Tokenizer and a Phrase
Chunker supplied by the OpenNLP 1 tool set. The in-
stances of the principal entity and secondary entities are
then anchored in the articles. The Secondary Entity De-
tector simply labels the appropriate surface texts of the
hyperlinks to other Wikipedia articles, which are proper
1http://opennlp.sourceforge.net/
125
Figure 1: System framework
nouns as secondary entities. The Principal Entity Detec-
tor will be explained in the following subsection.
After the entities are anchored, sentences that include
at least one mention of secondary entities will be selected
by a Sentence Detector. Each mention of the secondary
entities is considered as a relation candidate between the
underlying entity and the principal entity. Secondary en-
tities are always explicit, although the principal entity is
sometimes implicit in sentences containing no mention.
Keywords that provide clues for each relation label will
be identified by a Keyword Extractor. Parallely, an Entity
Classifier module classifies the entities into types. The
Relation Extractor extracts subtree feature from a pair of
the principal entity and a mention of secondary entity. It
then incorporates subtree feature together with entity type
feature into a feature vector and classifies relations of the
entity pair using SVM-based classifiers.
3.1 Principal Entity Detector
This module detects all referring expressions of the prin-
cipal entity in an article. All occurrences of identified
expressions are labeled as mentions of the principal en-
tity. We adopt (Morton, 2000) to classify the expressions
into three types: (1) personal pronoun (2) proper noun
(3) common nouns. Based on chunking information, we
propose a simple technique to identify a set of referring
expressions of the principal entity, denoted as F:
(i) Start with F = {}.
(ii) Select the first two chunks for F: the proper chunk
(nounphase with at least one proper noun) of the article
title and the first proper chunk in the first sentence of the
article, if any. If F is still empty, stop.
(iii) For each remaining proper chunk p in the article, if
p is derived from any expressions selected in (ii), then
F ? p. Proper chunk p1 is derived from proper chunk p2
if all its proper nouns appear in p2.
(iv) In the article, select c as the most frequent subjective
pronouns, find c? as its equivalent objective pronoun and
add them to F.
(v) For each chunk p with the pattern [DT N1 . . . Nk]
where DT is a determiner and Nk?s are a common nouns,
if p appears more frequently than all the selected pro-
nouns in (iv), then F ? p.
Table 1: Sample extracted referring expressions
Article Referring expressions Step
[NP Bill/NNP Gates/NNP ] (ii)
[NP William/NNP H./NNP Gates/NNP ] (ii)
Bill Gates [NP Gates/NNP ] (iii)
[NP The/DT Gates/NNP ] (iii)
[NP he/PRP ] (iv)
[NP him/PRP ] (iv)
[NP Microsoft/NNP ] (ii)
[NP The/DT Microsoft/NNP Corporation/NNP ] (ii)
Microsoft [NP that/DT Microsoft/NNP ] (iii)
[NP It/PRP ] (iv)
[NP the/DT company/NN ] (v)
[NP Microsoft/NNP Windows/NNP ] (ii)
Microsoft [NP Microsoft/NNP ] (iii)
Windows [NP Windows/NNP ] (iii)
[NP the/DT Windows/NNP ] (iii)
[NP it/PRP ] (iv)
Table 2: List of relations and their keywords
Relation Keywords
CEO CEO, chief, executive, officer
Chairmans chairman
COO coo, chief, operating, officer
Director director
Founder found, founder, founded, establish, form, foundation, open
President president
Vice
chairman
vice, chairman
Birth date born, bear, birth, birthday
Birth
place
born, bear
Foundation found, establish, form, founded, open, create, formed, estab-
lished, foundation, founding, cofounder, founder
Location headquartered, based, locate, headquarter, base, location, situate,
located
Product product, include, release, produce, service, operate, provide,
market, manage, development, focus, manufacture, provider,
launch, make, sell, introduce, producer, supplier, possess, re-
tailer, design, involve, production, offering, serve, sale, supply
Spouse marry, wife, married, husband, marriage
Table 1 shows some extracted referring expressions.
The third column indicates in which step the expressions
are selected. Supported by the nature of Wikipedia, our
technique provides better results than those of the coref-
erence tool in LingPipe library 2 and OpenNLP tool set.
3.2 Entity Classifier
Entity type is very useful for relation extraction. For in-
stance, the relation label between a person and an orga-
nization should be founder, chairman, etc., but cannot
be spouse, product, etc. We first identify year, month
and date entities by directly examining their surface text.
Types of other entities are identified by classifying their
corresponding articles. We develop one SVM-based clas-
sifier for each remaining type using the following fea-
tures: category feature (categories collected when trac-
ing from the article upto k level of its category structure),
pronoun feature (the most frequent subjective pronoun
in the article) and singular noun feature (singular nouns
of the first sentence of the article).
3.3 Keyword Extractor
Our hypothesis in this research is that there exist some
keywords that provide clues for the relationship between
2http://www.alias-i.com/lingpipe/index.html
126
Figure 2: Dependency trees in (a) & (b); core trees with respect to CEO relationship in (c) & (d); new representation
of the core trees in (e) & (f); common subtree in (g). The red phrase EP denotes the principal entity; the blue phrase
ES denotes the secondary entity.
a pair. For example, to express the founder relation, a
sentence should contain one keyword such as: found,
founder, founded, co-founders, or establish, etc. We iden-
tify such keywords by using a semi-automatic method.
First, we automatically extract some true relations from
summary sections of Wikipedia articles. Then, we map
entities in such relations to those in sentences to build
sample sentences for each relationship . Tf-idf model is
exploited to measure the relevance of words to each re-
lationship for those on the dependency path between the
entity pair. Finally, we choose the keywords manually
from lists of candidates ranked by relevance score with
respect to each relation. Table 2 shows our result selected
from ranked lists of total 35,820 keyword candidates us-
ing only one hour of human labor.
3.4 Subtree Feature from Dependency Path
In this subsection, we will describe how to obtain effi-
cient features for extracting relation using subtree min-
ing. We extend the idea of Bunescu et al (Bunescu and
Mooney, 2006) suggesting the analysis of dependency
path between the entities for extracting relation, in that
paths between the secondary entity and the keywords of r
will be added to the dependency path between the entities
to create a tree. The expanded tree is defined as core tree
of r because it attempts to capture the clues for r. Steps to
extract the core treeC of a relationship r from a sentence
s are described as follows:
(i)] Initialize the core tree C as blank.
(ii) Derive the dependency tree D from s.
(iii) Label the group of nodes corresponding to words of
secondary entity by an ES node in D.
(iv) If the principal entity appears in s, apply (iii) to re-
place principal entity with EP. Then extract P0 as shortest
path from ES to EP in D and add P0 ?C.
(v) For each keyword w of r, extract Pw as the shortest
path from ES to node of w and add Pw ?C.
Figures 2c & 2d present exemplary core trees of CEO
relationship derived from the dependency trees in Figures
2a & 2b. To analyze both words and relations of a core
tree uniformly, we transform it into a uniform graph for-
mat (Figures 2e & 2f) in which core tree words and rela-
tions are also represented as graph nodes.
We define a basic element of a relationship r as a key
pattern that commonly appears in various core trees of r.
As an example, the core trees in Figures 2e & 2f share
a common pattern in Figure 2g. Intuitively, this subtree
shares the core trees of sentences that express the idea of
?joined the company as CEO? or ?joined the company
and do something as CEO?.
We denote T = (V , E) as a directed tree, in which
V is a set of nodes and E is a set of directed edges.
Node y is an ancestor of node x, denoted by x ? y,
if (x,y) ? E or ?i1, ..., ik (k ? N and k ? 1) such that
(x, i1),(i1, i2), ...,(ik?1, ik),(ik,y) ? E. We define that a
tree S = (VS, ES) is a subtree of T if and only if: (i)VS ?V ,
and (ii) ?(x,y) ? ES, we have x? y in T .
We use a subtree as a feature for relation extraction.
From a set of training sentences with respect to a relation-
ship r, we derive the core trees. A frequent tree-mining
algorithm (Zaki, 2002) is used to generate subtrees from
that set of core trees to form the feature space. Each
mined subtree corresponds to a value of the feature.
4 Experiments and Evaluations
In this experiment, 5,975 articles are selected, in which
45 articles are for testing and 5,930 articles for train-
ing. We apply the framework in Figure 1 on the train-
ing articles to extract keywords and select relation candi-
dates. Subsequently, 3,833 positive instances (each con-
tains at least one relation) and 805 negative instances (the
ones containing no relation) from the candidates are an-
notated to train the Relation Extractor. Among 39,467
127
Table 3: Compare our proposed system and baselines
Precision(%) Recall(%) F1(%)
B0 8.70 22.26 12.51
B1 9.88 25.31 14.21
DepTree 29.07 53.86 37.76
Table 4: Result of Entity Classifier with various levels (k
value) of exploited category structure
Depth k(%) Accuracy(%)
1 64.0
2 69.5
3 81.0
4 81.5
5 79.5
6 77.5
7 77.0
8 78.0
9 75.0
10 74.5
entities collected from all principal and secondary enti-
ties, we randomly select 3,300 entities and manually an-
notate their types for the Entity Classifier. Finally, we use
3,100 entities for training and 200 entities for testing.
We develop two baseline systems to evaluate our
method, which use bag-of-words model. The second sys-
tem (B1 in Table 3) works like the Keyword Extractor
on training instances in that it calculates tf-idf scores for
words on the dependency path between the entities with
respect to each relation. During testing, it accumulates
tf-idf scores of words on the path and chooses the relation
label that gives the highest score for the entity pair. The
only difference between the two baseline systems is that
the first one (B0 in Table 3) focuses on all the words be-
tween the entities in sentence text, not dependency path.
In our experiments, dependency graphs are obtained
by Minipar parser (Lin, 1998), classifiers are trained by
SVM Light (Joachims, 1999) with 2nd- order polynomial
kernel, subtrees are mined by FREQT 3 tree miner.
On the basis of preliminary experiments, we report the
performance of our system compared with those of base-
line systems in Table 3. The result shows that our pro-
posed method gives a substantial improvement over the
baselines. Although the recall is quite adequate, preci-
sion is low. Data analysis reveals that although the mined
subtrees capture key features for relationships, they also
generate many irrelevant features which degrade the per-
formance. It is necessary to carry out feature selection
step for subtree feature. One more reason of the poor
precision is that our system suffers from the error accu-
mulation in a long pipeline of entity detection, entity clas-
sification, dependency parsing and relation classification.
Table 4 shows the effectiveness of different values of k
parameter in Entity Classifier. The classifier works best
when we trace four levels on category system. An inter-
esting fact is that Wikipedia can be used as an external
3http://chasen.org/t?aku/software/freqt/
knowledge source for Named Entity Recognition.
5 Conclusions and Future Works
We have presented a method to extract relations between
entities from Wikipedia articles by incorporating infor-
mation from the Wikipedia structure and by the analysis
of Wikipedia text. The key features of our method in-
clude: (1) an algorithm to build the core syntactic tree
that reflects the relation between a given entity pair more
accurately; (2) the use of a tree-mining algorithm to iden-
tify the basic elements of syntactic structure of sentences
for relationships; (3) method to make use of the nature of
Wikipedia for entity allocation and entity classification.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
the 5th ACM International Conference on Digital Li-
braries.
S. Brin. 1998. Extracting patterns and relations from
the world wide web. In Proceedings of the 1998 Inter-
national Workshop on the Web and Databases, pages
172?183.
R.C. Bunescu and R.J. Mooney. 2006. Extracting rela-
tions from text: From word sequences to dependency
paths. In Anne Kao and Steve Poteet, editors, Text
Mining and Natural Language Processing.
H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua.
2005. Question answering passage retrieval using de-
pendency relations. In Proceedings of SIGIR.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating
probabilistic extraction models and data mining to dis-
cover relations and patterns in text. In Proceedings of
the HLT-NAACL-2006.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
D. Lin. 1998. Dependency-based evaluation of minipar.
In Proceedings of the Workshop on the Evaluation of
Parsing Systems, 1st International Conference on Lan-
guage Resources and Evaluation.
T. Morton. 2000. Coreference for nlp applications. In
Proceedings of the ACL-2000.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the ACL-2002, pages 41?47.
M.J. Zaki. 2002. Efficiently mining frequent trees in a
forest. In Proceedings of 8th ACM SIGKDD.
128
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 385?392,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Bottom-up Approach to Sentence Ordering
for Multi-document Summarization
Danushka Bollegala Naoaki Okazaki ?
Graduate School of Information Science and Technology
The University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
{danushka,okazaki}@mi.ci.i.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Mitsuru Ishizuka
Abstract
Ordering information is a difficult but
important task for applications generat-
ing natural-language text. We present
a bottom-up approach to arranging sen-
tences extracted for multi-document sum-
marization. To capture the association and
order of two textual segments (eg, sen-
tences), we define four criteria, chronol-
ogy, topical-closeness, precedence, and
succession. These criteria are integrated
into a criterion by a supervised learning
approach. We repeatedly concatenate two
textual segments into one segment based
on the criterion until we obtain the overall
segment with all sentences arranged. Our
experimental results show a significant im-
provement over existing sentence ordering
strategies.
1 Introduction
Multi-document summarization (MDS) (Radev
and McKeown, 1999) tackles the information
overload problem by providing a condensed ver-
sion of a set of documents. Among a number
of sub-tasks involved in MDS, eg, sentence ex-
traction, topic detection, sentence ordering, infor-
mation extraction, sentence generation, etc., most
MDS systems have been based on an extraction
method, which identifies important textual seg-
ments (eg, sentences or paragraphs) in source doc-
uments. It is important for such MDS systems
to determine a coherent arrangement of the tex-
tual segments extracted from multi-documents in
order to reconstruct the text structure for summa-
rization. Ordering information is also essential for
?Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
other text-generation applications such as Ques-
tion Answering.
A summary with improperly ordered sen-
tences confuses the reader and degrades the qual-
ity/reliability of the summary itself. Barzi-
lay (2002) has provided empirical evidence that
proper order of extracted sentences improves their
readability significantly. However, ordering a
set of sentences into a coherent text is a non-
trivial task. For example, identifying rhetorical
relations (Mann and Thompson, 1988) in an or-
dered text has been a difficult task for computers,
whereas our task is even more complicated: to
reconstruct such relations from unordered sets of
sentences. Source documents for a summary may
have been written by different authors, by different
writing styles, on different dates, and based on dif-
ferent background knowledge. We cannot expect
that a set of extracted sentences from such diverse
documents will be coherent on their own.
Several strategies to determine sentence order-
ing have been proposed as described in section 2.
However, the appropriate way to combine these
strategies to achieve more coherent summaries re-
mains unsolved. In this paper, we propose four
criteria to capture the association of sentences in
the context of multi-document summarization for
newspaper articles. These criteria are integrated
into one criterion by a supervised learning ap-
proach. We also propose a bottom-up approach
in arranging sentences, which repeatedly concate-
nates textual segments until the overall segment
with all sentences arranged, is achieved.
2 Related Work
Existing methods for sentence ordering are di-
vided into two approaches: making use of chrono-
logical information (McKeown et al, 1999; Lin
385
and Hovy, 2001; Barzilay et al, 2002; Okazaki
et al, 2004); and learning the natural order of sen-
tences from large corpora not necessarily based on
chronological information (Lapata, 2003; Barzi-
lay and Lee, 2004). A newspaper usually dissem-
inates descriptions of novel events that have oc-
curred since the last publication. For this reason,
ordering sentences according to their publication
date is an effective heuristic for multidocument
summarization (Lin and Hovy, 2001; McKeown
et al, 1999). Barzilay et al (2002) have proposed
an improved version of chronological ordering by
first grouping sentences into sub-topics discussed
in the source documents and then arranging the
sentences in each group chronologically.
Okazaki et al (2004) have proposed an algo-
rithm to improve chronological ordering by re-
solving the presuppositional information of ex-
tracted sentences. They assume that each sen-
tence in newspaper articles is written on the basis
that presuppositional information should be trans-
ferred to the reader before the sentence is inter-
preted. The proposed algorithm first arranges sen-
tences in a chronological order and then estimates
the presuppositional information for each sentence
by using the content of the sentences placed before
each sentence in its original article. The evaluation
results show that the proposed algorithm improves
the chronological ordering significantly.
Lapata (2003) has suggested a probabilistic
model of text structuring and its application to the
sentence ordering. Her method calculates the tran-
sition probability from one sentence to the next
from a corpus based on the Cartesian product be-
tween two sentences defined using the following
features: verbs (precedent relationships of verbs
in the corpus); nouns (entity-based coherence by
keeping track of the nouns); and dependencies
(structure of sentences). Although she has not
compared her method with chronological order-
ing, it could be applied to generic domains, not re-
lying on the chronological clue provided by news-
paper articles.
Barzilay and Lee (2004) have proposed con-
tent models to deal with topic transition in do-
main specific text. The content models are formal-
ized by Hidden Markov Models (HMMs) in which
the hidden state corresponds to a topic in the do-
main of interest (eg, earthquake magnitude or pre-
vious earthquake occurrences), and the state tran-
sitions capture possible information-presentation
orderings. The evaluation results showed that
their method outperformed Lapata?s approach by a
wide margin. They did not compare their method
with chronological ordering as an application of
multi-document summarization.
As described above, several good strate-
gies/heuristics to deal with the sentence ordering
problem have been proposed. In order to integrate
multiple strategies/heuristics, we have formalized
them in a machine learning framework and have
considered an algorithm to arrange sentences us-
ing the integrated strategy.
3 Method
We define notation a ? b to represent that sen-
tence a precedes sentence b. We use the term seg-
ment to describe a sequence of ordered sentences.
When segment A consists of sentences a1, a2, ...,
am in this order, we denote as:
A = (a1 ? a2 ? ... ? am). (1)
The two segments A and B can be ordered either
B after A or A after B. We define the notation
A ? B to show that segment A precedes segment
B.
Let us consider a bottom-up approach in arrang-
ing sentences. Starting with a set of segments ini-
tialized with a sentence for each, we concatenate
two segments, with the strongest association (dis-
cussed later) of all possible segment pairs, into
one segment. Repeating the concatenating will
eventually yield a segment with all sentences ar-
ranged. The algorithm is considered as a variation
of agglomerative hierarchical clustering with the
ordering information retained at each concatenat-
ing process.
The underlying idea of the algorithm, a bottom-
up approach to text planning, was proposed by
Marcu (1997). Assuming that the semantic units
(sentences) and their rhetorical relations (eg, sen-
tence a is an elaboration of sentence d) are given,
he transcribed a text structuring task into the prob-
lem of finding the best discourse tree that satisfied
the set of rhetorical relations. He stated that global
coherence could be achieved by satisfying local
coherence constraints in ordering and clustering,
thereby ensuring that the resultant discourse tree
was well-formed.
Unfortunately, identifying the rhetorical rela-
tion between two sentences has been a difficult
386
a
A B C D
b c d
E = (b a)
G = (b a c d)
F = (c d)
Segments
Sentences
f (as
soci
ation
 stre
ngth
)
Figure 1: Arranging four sentences A, B, C, and
D with a bottom-up approach.
task for computers. However, the bottom-up algo-
rithm for arranging sentences can still be applied
only if the direction and strength of the associa-
tion of the two segments (sentences) are defined.
Hence, we introduce a function f(A ? B) to rep-
resent the direction and strength of the association
of two segments A and B,
f(A ? B) =
{ p (if A precedes B)
0 (if B precedes A) , (2)
where p (0 ? p ? 1) denotes the association
strength of the segments A and B. The associa-
tion strengths of the two segments with different
directions, eg, f(A ? B) and f(B ? A), are not
always identical in our definition,
f(A ? B) 6= f(B ? A). (3)
Figure 1 shows the process of arranging four
sentences a, b, c, and d. Firstly, we initialize four
segments with a sentence for each,
A = (a), B = (b), C = (c), D = (d). (4)
Suppose that f(B ? A) has the highest value of
all possible pairs, eg, f(A ? B), f(C ? D), etc,
we concatenate B and A to obtain a new segment,
E = (b ? a). (5)
Then we search for the segment pair with the
strongest association. Supposing that f(C ? D)
has the highest value, we concatenate C and D to
obtain a new segment,
F = (c ? d). (6)
Finally, comparing f(E ? F ) and f(F ? E), we
obtain the global sentence ordering,
G = (b ? a ? c ? d). (7)
In the above description, we have not defined
the association of the two segments. The previ-
ous work described in Section 2 has addressed the
association of textual segments (sentences) to ob-
tain coherent orderings. We define four criteria to
capture the association of two segments: chronol-
ogy; topical-closeness; precedence; and succes-
sion. These criteria are integrated into a function
f(A ? B) by using a machine learning approach.
The rest of this section explains the four criteria
and an integration method with a Support Vector
Machine (SVM) (Vapnik, 1998) classifier.
3.1 Chronology criterion
Chronology criterion reflects the chronological or-
dering (Lin and Hovy, 2001; McKeown et al,
1999), which arranges sentences in a chronologi-
cal order of the publication date. We define the as-
sociation strength of arranging segments B after A
measured by a chronology criterion fchro(A ? B)
in the following formula,
fchro(A ? B)
=
?
???
???
1 T(am) < T(b1)
1 [D(am) = D(b1)] ? [N(am) < N(b1)]
0.5 [T(am) = T(b1)] ? [D(am) 6= D(b1)]
0 otherwise
.
(8)
Here, am represents the last sentence in segment
A; b1 represents the first sentence in segment B;
T (s) is the publication date of the sentence s;
D(s) is the unique identifier of the document to
which sentence s belongs: and N(s) denotes the
line number of sentence s in the original docu-
ment. The chronological order of arranging seg-
ment B after A is determined by the comparison
between the last sentence in the segment A and the
first sentence in the segment B.
The chronology criterion assesses the appropri-
ateness of arranging segment B after A if: sen-
tence am is published earlier than b1; or sentence
am appears before b1 in the same article. If sen-
tence am and b1 are published on the same day but
appear in different articles, the criterion assumes
the order to be undefined. If none of the above
conditions are satisfied, the criterion estimates that
segment B will precede A.
3.2 Topical-closeness criterion
The topical-closeness criterion deals with the as-
sociation, based on the topical similarity, of two
387
a1a2
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....a3a4
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
b1
b2
b3
b3
b2
b1 Pb1 Pb2 Pb3
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
Segment A
?
Segment B
Original articlefor sentence b Original articlefor sentence b2 Original articlefor sentence b31
max
average
maxmax
Figure 2: Precedence criterion
segments. The criterion reflects the ordering strat-
egy proposed by Barzilay et al(2002), which
groups sentences referring to the same topic. To
measure the topical closeness of two sentences, we
represent each sentence with a vector whose ele-
ments correspond to the occurrence1 of the nouns
and verbs in the sentence. We define the topical
closeness of two segments A and B as follows,
ftopic(A ? B) = 1|B|
?
b?B
max
a?A
sim(a, b). (9)
Here, sim(a, b) denotes the similarity of sentences
a and b, which is calculated by the cosine similar-
ity of two vectors corresponding to the sentences.
For sentence b ? B, maxa?A sim(a, b) chooses
the sentence a ? A most similar to sentence b and
yields the similarity. The topical-closeness crite-
rion ftopic(A ? B) assigns a higher value when
the topic referred by segment B is the same as seg-
ment A.
3.3 Precedence criterion
Let us think of the case where we arrange seg-
ment A before B. Each sentence in segment B
has the presuppositional information that should
be conveyed to a reader in advance. Given sen-
tence b ? B, such presuppositional information
may be presented by the sentences appearing be-
fore the sentence b in the original article. How-
ever, we cannot guarantee whether a sentence-
extraction method for multi-document summa-
rization chooses any sentences before b for a sum-
mary because the extraction method usually deter-
1The vector values are represented by boolean values, i.e.,
1 if the sentence contains a word, otherwise 0.
a1a2
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....a3 .... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
b
b2
b3
a3
a2
a1 S a1 S a2 S a3
. ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
Segment A
?
Segment B
Original articlefor sentence a1 Original articlefor sentence a2 Original articlefor sentence a3
max
average
maxmax
.... .. .. .... .. ....... ......1
Figure 3: Succession criterion
mines a set of sentences, within the constraint of
summary length, that maximizes information cov-
erage and excludes redundant information. Prece-
dence criterion measures the substitutability of the
presuppositional information of segment B (eg,
the sentences appearing before sentence b) as seg-
ment A. This criterion is a formalization of the
sentence-ordering algorithm proposed by Okazaki
et al (2004).
We define the precedence criterion in the fol-
lowing formula,
fpre(A ? B) = 1|B|
?
b?B
max
a?A,p?Pb
sim(a, p).
(10)
Here, Pb is a set of sentences appearing before sen-
tence b in the original article; and sim(a, b) de-
notes the cosine similarity of sentences a and b
(defined as in the topical-closeness criterion). Fig-
ure 2 shows an example of calculating the prece-
dence criterion for arranging segment B after A.
We approximate the presuppositional information
for sentence b by sentences Pb, ie, sentences ap-
pearing before the sentence b in the original arti-
cle. Calculating the similarity among sentences in
Pb and A by the maximum similarity of the pos-
sible sentence combinations, Formula 10 is inter-
preted as the average similarity of the precedent
sentences ?Pb(b ? B) to the segment A.
3.4 Succession criterion
The idea of succession criterion is the exact op-
posite of the precedence criterion. The succession
criterion assesses the coverage of the succedent in-
formation for segment A by arranging segment B
388
ab
c
d
Partitioning point
 segment before the
 partitioning point
segment after the
partitioning point
Partitioning 
window
Figure 4: Partitioning a human-ordered extract
into pairs of segments
after A:
fsucc(A ? B) = 1|A|
?
a?A
max
s?Sa,b?B
sim(s, b).
(11)
Here, Sa is a set of sentences appearing after sen-
tence a in the original article; and sim(a, b) de-
notes the cosine similarity of sentences a and b
(defined as in the topical-closeness criterion). Fig-
ure 3 shows an example of calculating the succes-
sion criterion to arrange segments B after A. The
succession criterion measures the substitutability
of the succedent information (eg, the sentences ap-
pearing after the sentence a ? A) as segment B.
3.5 SVM classifier to assess the integrated
criterion
We integrate the four criteria described above
to define the function f(A ? B) to represent
the association direction and strength of the two
segments A and B (Formula 2). More specifi-
cally, given the two segments A and B, function
f(A ? B) is defined to yield the integrated asso-
ciation strength from four values, fchro(A ? B),
ftopic(A ? B), fpre(A ? B), and fsucc(A ? B).
We formalize the integration task as a binary clas-
sification problem and employ a Support Vector
Machine (SVM) as the classifier. We conducted a
supervised learning as follows.
We partition a human-ordered extract into pairs
each of which consists of two non-overlapping
segments. Let us explain the partitioning process
taking four human-ordered sentences, a ? b ?
c ? d shown in Figure 4. Firstly, we place the
partitioning point just after the first sentence a.
Focusing on sentence a arranged just before the
partition point and sentence b arranged just after
we identify the pair {(a), (b)} of two segments
(a) and (b). Enumerating all possible pairs of two
segments facing just before/after the partitioning
point, we obtain the following pairs, {(a), (b ?
c)} and {(a), (b ? c ? d)}. Similarly, segment
+1 : [fchro(A ? B), ftopic(A ? B), fpre(A ? B), fsucc(A ? B)]
?1 : [fchro(B ? A), ftopic(B ? A), fpre(B ? A), fsucc(B ? A)]
Figure 5: Two vectors in a training data generated
from two ordered segments A ? B
pairs, {(b), (c)}, {(a ? b), (c)}, {(b), (c ? d)},
{(a ? b), (c ? d)}, are obtained from the parti-
tioning point between sentence b and c. Collect-
ing the segment pairs from the partitioning point
between sentences c and d (i.e., {(c), (d)}, {(b ?
c), (d)} and {(a ? b ? c), (d)}), we identify ten
pairs in total form the four ordered sentences. In
general, this process yields n(n2?1)/6 pairs from
ordered n sentences. From each pair of segments,
we generate one positive and one negative training
instance as follows.
Given a pair of two segments A and B arranged
in an order A ? B, we calculate four values,
fchro(A ? B), ftopic(A ? B), fpre(A ? B),
and fsucc(A ? B) to obtain the instance with
the four-dimensional vector (Figure 5). We label
the instance (corresponding to A ? B) as a posi-
tive class (ie, +1). Simultaneously, we obtain an-
other instance with a four-dimensional vector cor-
responding to B ? A. We label it as a negative
class (ie, ?1). Accumulating these instances as
training data, we obtain a binary classifier by using
a Support Vector Machine with a quadratic kernel.
The SVM classifier yields the association direc-
tion of two segments (eg, A ? B or B ? A) with
the class information (ie, +1 or ?1). We assign
the association strength of two segments by using
the class probability estimate that the instance be-
longs to a positive (+1) class. When an instance
is classified into a negative (?1) class, we set the
association strength as zero (see the definition of
Formula 2).
4 Evaluation
We evaluated the proposed method by using the
3rd Text Summarization Challenge (TSC-3) cor-
pus2. The TSC-3 corpus contains 30 sets of ex-
tracts, each of which consists of unordered sen-
tences3 extracted from Japanese newspaper arti-
cles relevant to a topic (query). We arrange the
extracts by using different algorithms and evaluate
2http://lr-www.pi.titech.ac.jp/tsc/tsc3-en.html
3Each extract consists of ca. 15 sentences on average.
389
Table 1: Correlation between two sets of human-
ordered extracts
Metric Mean Std. Dev Min Max
Spearman 0.739 0.304 -0.2 1
Kendall 0.694 0.290 0 1
Average Continuity 0.401 0.404 0.001 1
the readability of the ordered extracts by a subjec-
tive grading and several metrics.
In order to construct training data applica-
ble to the proposed method, we asked two hu-
man subjects to arrange the extracts and obtained
30(topics) ? 2(humans) = 60 sets of ordered
extracts. Table 1 shows the agreement of the or-
dered extracts between the two subjects. The cor-
relation is measured by three metrics, Spearman?s
rank correlation, Kendall?s rank correlation, and
average continuity (described later). The mean
correlation values (0.74 for Spearman?s rank cor-
relation and 0.69 for Kendall?s rank correlation)
indicate a certain level of agreement in sentence
orderings made by the two subjects. 8 out of 30
extracts were actually identical.
We applied the leave-one-out method to the pro-
posed method to produce a set of sentence or-
derings. In this experiment, the leave-out-out
method arranges an extract by using an SVM
model trained from the rest of the 29 extracts. Re-
peating this process 30 times with a different topic
for each iteration, we generated a set of 30 ex-
tracts for evaluation. In addition to the proposed
method, we prepared six sets of sentence orderings
produced by different algorithms for comparison.
We describe briefly the seven algorithms (includ-
ing the proposed method):
Agglomerative ordering (AGL) is an ordering
arranged by the proposed method;
Random ordering (RND) is the lowest anchor,
in which sentences are arranged randomly;
Human-made ordering (HUM) is the highest
anchor, in which sentences are arranged by
a human subject;
Chronological ordering (CHR) arranges sen-
tences with the chronology criterion defined
in Formula 8. Sentences are arranged in
chronological order of their publication date;
Topical-closeness ordering (TOP) arranges sen-
tences with the topical-closeness criterion de-
fined in Formula 9;
0 20 40 60 80 100
UnacceptablePoorAcceptablePerfect
HUM
AGL
CHR
RND
%
Figure 6: Subjective grading
Precedence ordering (PRE) arranges sentences
with the precedence criterion defined in For-
mula 10;
Suceedence ordering (SUC) arranges sentences
with the succession criterion defined in For-
mula 11.
The last four algorithms (CHR, TOP, PRE, and
SUC) arrange sentences by the corresponding cri-
terion alone, each of which uses the association
strength directly to arrange sentences without the
integration of other criteria. These orderings are
expected to show the performance of each expert
independently and their contribution to solving the
sentence ordering problem.
4.1 Subjective grading
Evaluating a sentence ordering is a challenging
task. Intrinsic evaluation that involves human
judges to rank a set of sentence orderings is a nec-
essary approach to this task (Barzilay et al, 2002;
Okazaki et al, 2004). We asked two human judges
to rate sentence orderings according to the follow-
ing criteria. A perfect summary is a text that we
cannot improve any further by re-ordering. An ac-
ceptable summary is one that makes sense and is
unnecessary to revise even though there is some
room for improvement in terms of readability. A
poor summary is one that loses a thread of the
story at some places and requires minor amend-
ment to bring it up to an acceptable level. An un-
acceptable summary is one that leaves much to be
improved and requires overall restructuring rather
than partial revision. To avoid any disturbance in
rating, we inform the judges that the summaries
were made from a same set of extracted sentences
and only the ordering of sentences is different.
Figure 6 shows the distribution of the subjective
grading made by two judges to four sets of order-
ings, RND, CHR, AGL and HUM. Each set of or-
390
Teval = (e ? a ? b ? c ? d)
Tref = (a ? b ? c ? d ? e)
Figure 7: An example of an ordering under evalu-
ation Teval and its reference Tref .
derings has 30(topics) ? 2(judges) = 60 ratings.
Most RND orderings are rated as unacceptable.
Although CHR and AGL orderings have roughly
the same number of perfect orderings (ca. 25%),
the AGL algorithm gained more acceptable order-
ings (47%) than the CHR alghrotihm (30%). This
fact shows that integration of CHR experts with
other experts worked well by pushing poor order-
ing to an acceptable level. However, a huge gap
between AGL and HUM orderings was also found.
The judges rated 28% AGL orderings as perfect
while the figure rose as high as 82% for HUM
orderings. Kendall?s coefficient of concordance
(Kendall?s W ), which asses the inter-judge agree-
ment of overall ratings, reported a higher agree-
ment between the two judges (W = 0.939).
4.2 Metrics for semi-automatic evaluation
We also evaluated sentence orderings by reusing
two sets of gold-standard orderings made for the
training data. In general, subjective grading con-
sumes much time and effort, even though we
cannot reproduce the evaluation afterwards. The
previous studies (Barzilay et al, 2002; Lapata,
2003) employ rank correlation coefficients such
as Spearman?s rank correlation and Kendall?s rank
correlation, assuming a sentence ordering to be
a rank. Okazaki et al (2004) propose a metric
that assess continuity of pairwise sentences com-
pared with the gold standard. In addition to Spear-
man?s and Kendall?s rank correlation coefficients,
we propose an average continuity metric, which
extends the idea of the continuity metric to contin-
uous k sentences.
A text with sentences arranged in proper order
does not interrupt a human?s reading while moving
from one sentence to the next. Hence, the qual-
ity of a sentence ordering can be estimated by the
number of continuous sentences that are also re-
produced in the reference sentence ordering. This
is equivalent to measuring a precision of continu-
ous sentences in an ordering against the reference
ordering. We define Pn to measure the precision of
Table 2: Comparison with human-made ordering
Method Spearman Kendall Average
coefficient coefficient Continuity
RND -0.127 -0.069 0.011
TOP 0.414 0.400 0.197
PRE 0.415 0.428 0.293
SUC 0.473 0.476 0.291
CHR 0.583 0.587 0.356
AGL 0.603 0.612 0.459
n continuous sentences in an ordering to be evalu-
ated as,
Pn = mN ? n+ 1 . (12)
Here, N is the number of sentences in the refer-
ence ordering; n is the length of continuous sen-
tences on which we are evaluating; m is the num-
ber of continuous sentences that appear in both the
evaluation and reference orderings. In Figure 7,
the precision of 3 continuous sentences P3 is cal-
culated as:
P3 = 25? 3 + 1 = 0.67. (13)
The Average Continuity (AC) is defined as the
logarithmic average of Pn over 2 to k:
AC = exp
(
1
k ? 1
k?
n=2
log(Pn + ?)
)
. (14)
Here, k is a parameter to control the range of the
logarithmic average; and ? is a small value in case
if Pn is zero. We set k = 4 (ie, more than five
continuous sentences are not included for evalua-
tion) and ? = 0.01. Average Continuity becomes
0 when evaluation and reference orderings share
no continuous sentences and 1 when the two or-
derings are identical. In Figure 7, Average Conti-
nuity is calculated as 0.63. The underlying idea of
Formula 14 was proposed by Papineni et al (2002)
as the BLEU metric for the semi-automatic evalu-
ation of machine-translation systems. The origi-
nal definition of the BLEU metric is to compare a
machine-translated text with its reference transla-
tion by using the word n-grams.
4.3 Results of semi-automatic evaluation
Table 2 reports the resemblance of orderings pro-
duced by six algorithms to the human-made ones
with three metrics, Spearman?s rank correlation,
Kendall?s rank correlation, and Average Continu-
ity. The proposed method (AGL) outperforms the
391
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
AGLCHRSUCPRETOPRND
8765432
P
re
ci s i o
n P
n
Length n
Figure 8: Precision vs unit of measuring continu-
ity.
rest in all evaluation metrics, although the chrono-
logical ordering (CHR) appeared to play the major
role. The one-way analysis of variance (ANOVA)
verified the effects of different algorithms for sen-
tence orderings with all metrics (p < 0.01). We
performed Tukey Honest Significant Differences
(HSD) test to compare differences among these al-
gorithms. The Tukey test revealed that AGL was
significantly better than the rest. Even though we
could not compare our experiment with the prob-
abilistic approach (Lapata, 2003) directly due to
the difference of the text corpora, the Kendall co-
efficient reported higher agreement than Lapata?s
experiment (Kendall=0.48 with lemmatized nouns
and Kendall=0.56 with verb-noun dependencies).
Figure 8 shows precision Pn with different
length values of continuous sentence n for the six
methods compared in Table 2. The number of
continuous sentences becomes sparse for a higher
value of length n. Therefore, the precision values
decrease as the length n increases. Although RND
ordering reported some continuous sentences for
lower n values, no continuous sentences could be
observed for the higher n values. Four criteria de-
scribed in Section 3 (ie, CHR, TOP, PRE, SUC)
produce segments of continuous sentences at all
values of n.
5 Conclusion
We present a bottom-up approach to arrange sen-
tences extracted for multi-document summariza-
tion. Our experimental results showed a signif-
icant improvement over existing sentence order-
ing strategies. However, the results also implied
that chronological ordering played the major role
in arranging sentences. A future direction of this
study would be to explore the application of the
proposed framework to more generic texts, such
as documents without chronological information.
Acknowledgment
We used Mainichi Shinbun and Yomiuri Shinbun
newspaper articles, and the TSC-3 test collection.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen McK-
eown. 2002. Inferring strategies for sentence order-
ing in multidocument news summarization. Journal
of Artificial Intelligence Research, 17:35?55.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. Proceedings of
the annual meeting of ACL, 2003., pages 545?552.
C.Y. Lin and E. Hovy. 2001. Neats:a multidocument
summarizer. Proceedings of the Document Under-
standing Workshop(DUC).
W. Mann and S. Thompson. 1988. Rhetorical structure
theory: Toward a functional theory of text organiza-
tion. Text, 8:243?281.
Daniel Marcu. 1997. From local to global coherence:
A bottom-up approach to text planning. In Proceed-
ings of the 14th National Conference on Artificial
Intelligence, pages 629?635, Providence, Rhode Is-
land.
Kathleen McKeown, Judith Klavans, Vasileios Hatzi-
vassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by
reformulation: Progress and prospects. AAAI/IAAI,
pages 453?460.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru
Ishizuka. 2004. Improving chronological sentence
ordering by precedence relation. In Proceedings
of 20th International Conference on Computational
Linguistics (COLING 04), pages 750?756.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu:a method for automatic eval-
uation of machine translation. Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311?318.
Dragomir R. Radev and Kathy McKeown. 1999.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24:469?500.
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, GB.
392
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1021?1029,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Relation Extraction by Mining Wikipedia Texts Using
Information from the Web
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang and Mitsuru Ishizuka
The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
yulan@mi.ci.i.u-tokyo.ac.jp
okazaki@is.s.u-tokyo.ac.jp
matsuo@biz-model.t.utokyo.ac.jp
yangzl@tkl.iis.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Abstract
This paper presents an unsupervised rela-
tion extraction method for discovering and
enhancing relations in which a specified
concept in Wikipedia participates. Using
respective characteristics of Wikipedia ar-
ticles and Web corpus, we develop a clus-
tering approach based on combinations of
patterns: dependency patterns from depen-
dency analysis of texts in Wikipedia, and
surface patterns generated from highly re-
dundant information related to the Web.
Evaluations of the proposed approach on
two different domains demonstrate the su-
periority of the pattern combination over
existing approaches. Fundamentally, our
method demonstrates how deep linguistic
patterns contribute complementarily with
Web surface patterns to the generation of
various relations.
1 Introduction
Machine learning approaches for relation extrac-
tion tasks require substantial human effort, partic-
ularly when applied to the broad range of docu-
ments, entities, and relations existing on the Web.
Even with semi-supervised approaches, which use
a large unlabeled corpus, manual construction of a
small set of seeds known as true instances of the
target entity or relation is susceptible to arbitrary
human decisions. Consequently, a need exists for
development of semantic information-retrieval al-
gorithms that can operate in a manner that is as
unsupervised as possible.
Currently, the leading methods in unsupervised
information extraction collect redundancy infor-
mation from a local corpus or use the Web as a
corpus (Pantel and Pennacchiotti, 2006); (Banko
et al, 2007); (Bollegala et al, 2007): (Fan et
al., 2008); (Davidov and Rappoport, 2008). The
standard process is to scan or search the cor-
pus to collect co-occurrences of word pairs with
strings between them, and then to calculate term
co-occurrence or generate surface patterns. The
method is used widely. However, even when pat-
terns are generated from well-written texts, fre-
quent pattern mining is non-trivial because the
number of unique patterns is loose, but many pat-
terns are non-discriminative and correlated. A
salient challenge and research interest for frequent
pattern mining is abstraction away from different
surface realizations of semantic relations to dis-
cover discriminative patterns efficiently.
Linguistic analysis is another effective tech-
nology for semantic relation extraction, as de-
scribed in many reports such as (Kambhatla,
2004); (Bunescu and Mooney, 2005); (Harabagiu
et al, 2005); (Nguyen et al, 2007). Currently, lin-
guistic approaches for semantic relation extraction
are mostly supervised, relying on pre-specification
of the desired relation or initial seed words or pat-
terns from hand-coding. The common process is
to generate linguistic features based on analyses of
the syntactic features, dependency, or shallow se-
mantic structure of text. Then the system is trained
to identify entity pairs that assume a relation and
to classify them into pre-defined relations. The ad-
vantage of these methods is that they use linguistic
technologies to learn semantic information from
different surface expressions.
As described herein, we consider integrating
linguistic analysis with Web frequency informa-
tion to improve the performance of unsupervised
relation extraction. As (Banko et al, 2007)
reported, ?deep? linguistic technology presents
problems when applied to heterogeneous text on
the Web. Therefore, we do not parse informa-
tion from the Web corpus, but from well written
texts. Particularly, we specifically examine unsu-
pervised relation extraction from existing texts of
Wikipedia articles. Wikipedia resources of a fun-
1021
damental type are of concepts (e.g., represented
by Wikipedia articles as a special case) and their
mutual relations. We propose our method, which
groups concept pairs into several clusters based on
the similarity of their contexts. Contexts are col-
lected as patterns of two kinds: dependency pat-
terns from dependency analysis of sentences in
Wikipedia, and surface patterns generated from
highly redundant information from the Web.
The main contributions of this paper are as fol-
lows:
? Using characteristics of Wikipedia articles
and the Web corpus respectively, our study
yields an example of bridging the gap sep-
arating ?deep? linguistic technology and re-
dundant Web information for Information
Extraction tasks.
? Our experimental results reveal that relations
are extractable with good precision using
linguistic patterns, whereas surface patterns
from Web frequency information contribute
greatly to the coverage of relation extraction.
? The combination of these patterns produces
a clustering method to achieve high pre-
cision for different Information Extraction
applications, especially for bootstrapping a
high-recall semi-supervised relation extrac-
tion system.
2 Related Work
(Hasegawa et al, 2004) introduced a method for
discovering a relation by clustering pairs of co-
occurring entities represented as vectors of con-
text features. They used a simple representation
of contexts; the features were words in sentences
between the entities of the candidate pairs.
(Turney, 2006) presented an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. Given a word pair, the
output list of lexicon-syntactic patterns was ranked
by pertinence, which showed how well each pat-
tern expresses the relations between word pairs.
(Davidov et al, 2007) proposed a method for
unsupervised discovery of concept specific rela-
tions, requiring initial word seeds. That method
used pattern clusters to define general relations,
specific to a given concept. (Davidov and Rap-
poport, 2008) presented an approach to discover
and represent general relations present in an arbi-
trary corpus. That approach incorporated a fully
unsupervised algorithm for pattern cluster discov-
ery, which searches, clusters, and merges high-
frequency patterns around randomly selected con-
cepts.
The field of Unsupervised Relation Identifica-
tion (URI)?the task of automatically discover-
ing interesting relations between entities in large
text corpora?was introduced by (Hasegawa et
al., 2004). Relations are discovered by cluster-
ing pairs of co-occurring entities represented as
vectors of context features. (Rosenfeld and Feld-
man, 2006) showed that the clusters discovered by
URI are useful for seeding a semi-supervised rela-
tion extraction system. To compare different clus-
tering algorithms, feature extraction and selection
method, (Rosenfeld and Feldman, 2007) presented
a URI system that used surface patterns of two
kinds: patterns that test two entities together and
patterns that test either of two entities.
In this paper, we propose an unsupervised rela-
tion extraction method that combines patterns of
two types: surface patterns and dependency pat-
terns. Surface patterns are generated from the Web
corpus to provide redundancy information for re-
lation extraction. In addition, to obtain seman-
tic information for concept pairs, we generate de-
pendency patterns to abstract away from different
surface realizations of semantic relations. Depen-
dency patterns are expected to be more accurate
and less spam-prone than surface patterns from the
Web corpus. Surface patterns from redundancy
Web information are expected to address the data
sparseness problem. Wikipedia is currently widely
used information extraction as a local corpus; the
Web is used as a global corpus.
3 Characteristics of Wikipedia articles
Wikipedia, unlike the whole Web corpus, has
several characteristics that markedly facilitate in-
formation extraction. First, as an earlier report
(Giles, 2005) explained, Wikipedia articles are
much cleaner than typical Web pages. Because
the quality is not so different from standard writ-
ten English, we can use ?deep? linguistic tech-
nologies, such as syntactic or dependency parsing.
Secondly, Wikipedia articles are heavily cross-
linked, in a manner resembling cross-linking of
the Web pages. (Gabrilovich and Markovitch,
2006) assumed that these links encode numerous
interesting relations among concepts, and that they
provide an important source of information in ad-
1022
dition to the article texts.
To establish the background for this paper, we
start by defining the problem under consideration:
relation extraction from Wikipedia. We use the en-
cyclopedic nature of the corpus by specifically ex-
amining the relation extraction between the enti-
tled concept (ec) and a related concept (rc), which
are described in anchor text in this article. A com-
mon assumption is that, when investigating the se-
mantics in articles such as those in Wikipedia (e.g.
semantic Wikipedia (Volkel et al, 2006)), key in-
formation related to a concept described on a page
p lies within the set of links l(p) on that page; par-
ticularly, it is likely that a salient semantic relation
r exists between p and a related page p? ? l(p).
Given the scenario we described along with
earlier related works, the challenges we face are
these: 1) enumerating all potential relation types
of interest for extraction is highly problematic for
corpora as large and varied as Wikipedia; 2) train-
ing data or seed data are difficult to label. Consid-
ering (Davidov and Rappoport, 2008), which de-
scribes work to get the target word and relation
cluster given a single (?hook?) word, their method
depends mainly on frequency information from
the Web to obtain a target and clusters. Attempt-
ing to improve the performance, our solution for
these challenges is to combine frequency informa-
tion from the Web and the ?high quality? charac-
teristic of Wikipedia text.
4 Pattern Combination Method for
Relation Extraction
With the scene and challenges stated, we propose a
solution in the following way. The intuitive idea is
that we integrate linguistic technologies on high-
quality text in Wikipedia and Web mining tech-
nologies on a large-scale Web corpus. In this sec-
tion, we first provide an overview of our method
along with the function of the main modules. Sub-
sequently, we explain each module in the method
in detail.
4.1 Overview of the Method
Given a set of Wikipedia articles as input, our
method outputs a list of concept pairs for each ar-
ticle with a relation label assigned to each concept
pair. Briefly, the proposed approach has four main
modules, as depicted in Fig. 1.
? Text Preprocessor and Concept Pair Col-
lector preprocesses Wikipedia articles to
Wikipedia articles
Preprocessor
Concept pair collection
Sentence filtering
Web context collector
Web Context
T
i
= t1, t2?tn
P
i
=  p1,p2?pn
Dependency 
pattern Extractor
n1i,?n1j
?
ni2i, ..n2j
ni,?nj
?
surface clustering
depend clustering
Relation list
Output: 
relations for each article
input:
Eric Emerson Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
...
...
?
?
?
?
...
...
?
?
?
?
...
...
?
?
?
?
Tyco becoming
joined
comp:
CEO
obj: cc:
joined
obj:subj:
joined
obj: cc:
Clustering approach
Figure 1: Framework of the proposed approach
split text and filter sentences. It outputs con-
cept pairs, each of which has an accompany-
ing sentence.
? Web Context Collector collects context in-
formation from the Web and generates ranked
relational terms and surface patterns for each
concept pair.
? Dependency Pattern Extractor generates
dependency patterns for each concept pair
from corresponding sentences in Wikipedia
articles.
? Clustering Algorithm clusters concept pairs
based on their context. It consists of the two
sub-modules described below.
? Depend Clustering, which merges con-
cept pairs using dependency patterns
alone, aiming at obtaining clusters of
concept pairs with good precision;
? Surface Clustering, which clusters
concept pairs using surface patterns
based on the resultant clusters of depend
clustering. The aim is to merge more
concept pairs into existing clusters with
surface patterns to improve the coverage
of clusters.
1023
4.2 Text Preprocessor and Concept Pair
Collector
This module pre-processes Wikipedia article texts
to collect concept pairs and corresponding sen-
tences. Given a concept described in a Wikipedia
article, our idea of preprocessing executes initial
consideration of all anchor-text concepts linking
to other Wikipedia articles in the article as related
concepts that might share a semantic relation with
the entitled concept. The link structure, more par-
ticularly, the structure of outgoing links, provides
a simple mechanism for identifying relevant arti-
cles. We split text into sentences and select sen-
tences containing one reference of an entitled con-
cept and one of the linked texts for the dependency
pattern extractor module.
4.3 Web Context Collector
Querying a concept pair using a search engine
(Google), we characterize the semantic relation
between the pair by leveraging the vast size of the
Web. Our hypothesis is that there exist some key
terms and patterns that provide clues to the rela-
tions between pairs. From the snippets retrieved
by the search engine, we extract relational infor-
mation of two kinds: ranked relational terms as
keywords and surface patterns. Here surface pat-
terns are generated with support of ranked rela-
tional terms.
4.3.1 Relational Term Ranking
To collect relational terms as indicators for each
concept pair, we look for verbs and nouns from
qualified sentences in the snippets instead of sim-
ply finding verbs. Using only verbs as relational
terms might engender the loss of various important
relations, e.g. noun relations ?CEO?, ?founder?
between a person and a company. Therefore, for
each concept pair, a list of relational terms is col-
lected. Then all the collected terms of all concept
pairs are combined and ranked using an entropy-
based algorithm which is described in (Chen et al,
2005). With their algorithm, the importance of
terms can be assessed using the entropy criterion,
which is based on the assumption that a term is ir-
relevant if its presence obscures the separability of
the dataset. After the ranking, we obtain a global
ranked list of relational terms Tall for the whole
dataset (all the concept pairs). For each concept
pair, a local list of relational terms Tcp is sorted ac-
cording to the terms? order in Tall. Then from the
relational term list Tcp, a keyword tcp is selected
Table 1: Surface patterns for a concept pair
Pattern Pattern
ec ceo rc rc found ec
ceo rc found ec rc succeed as ceo of ec
rc be ceo of ec ec ceo of rc
ec assign rc as ceo ec found by ceo rc
ceo of ec rc ec found in by rc
for each concept pair cp as the first term appearing
in the term list Tcp. Keyword tcp will be used to
initialize the clustering algorithm in Section 4.5.1.
4.3.2 Surface Pattern Generation
Because simply taking the entire string between
two concept words captures an excess of extra-
neous and incoherent information, we use Tcp of
each concept pair as a key for surface pattern gen-
eration. We classified words into Content Words
(CWs) and Functional Words (FWs). From each
snippet sentence, the entitled concept, related con-
cept, or the keyword kcp is considered to be a Con-
tent Word (CW). Our idea of obtaining FWs is to
look for verbs, nouns, prepositions, and coordinat-
ing conjunctions that can help make explicit the
hidden relations between the target nouns.
Surface patterns have the following general
form.
[CW1] Infix1 [CW2] Infix2 [CW3] (1)
Therein, Infix1 and Infix2 respectively con-
tain only and any number of FWs. A pattern ex-
ample is ?ec assign rc as ceo (keyword)?. All gen-
erated patterns are sorted by their frequency, and
all occurrences of the entitled concept and related
concept are replaced with ?ec? and ?rc?, respec-
tively for pattern matching of different concept
pairs.
Table 1 presents examples of surface patterns
for a sample concept pair. Pattern windows are
bounded by CWs to obtain patterns more precisely
because 1) if we use only the string between two
concepts, it may not contain some important re-
lational information, such as ?ceo ec resign rc?
in Table 1; 2) if we generate patterns by setting
a windows surrounding two concepts, the number
of unique patterns is often exponential.
4.4 Dependency Pattern Extractor
In this section, we describe how to obtain depen-
dency patterns for relation clustering. After pre-
processing, selected sentences that contain at least
1024
one mention of an entitled concept or related con-
cept are parsed into dependency structures. We de-
fine dependency patterns as sub-paths of the short-
est dependency path between a concept pair for
two reasons. One is that the shortest path de-
pendency kernels outperform dependency tree ker-
nels by offering a highly condensed representation
of the information needed to assess their relation
(Bunescu and Mooney, 2005). The other reason
is that embedded structures of the linguistic repre-
sentation are important for obtaining good cover-
age of the pattern acquisition, as explained in (Cu-
lotta and Sorensen, 2005); (Zhang et al, 2006).
The process of inducing dependency patterns has
two steps.
1. Shortest dependency path inducement. From
the original dependency tree structure by parsing
the selected sentence for each concept pair, we
first induce the shortest dependency path with the
entitled concept and related concept.
2. Dependency pattern generation. We use
a frequent tree-mining algorithm (Zaki, 2002) to
generate sub-paths as dependency patterns from
the shortest dependency path for relation cluster-
ing.
4.5 Clustering Algorithm for Relation
Extraction
In this subsection, we present a clustering algo-
rithm that merges concept pairs based on depen-
dency patterns and surface patterns. The algorithm
is based on k-means clustering for relation cluster-
ing.
The dependency pattern has the properties of
being more accurate, but the Web context has the
advantage of containing much more redundant in-
formation than Wikipedia. Our idea of concept
pair clustering is a two-step clustering process:
first it clusters concept pairs into clusters with
good precision using dependency patterns; then it
improves the coverage of the clusters using surface
patterns.
4.5.1 Initial Centroid Selection and Distance
Function Definition
The standard k-means algorithm is affected by
the choice of seeds and the number of clusters
k. However, as we claimed in the Introduc-
tion section, because we aim to extract relations
from Wikipedia articles in an unsupervised man-
ner, cluster number k is unknown and no good
centroids can be predicted. As described in this
paper, we select centroids based on the keyword
tcp of each concept pair.
First of all, all concept pairs are grouped by
their keywords tcp. Let G = {G1, G2, ...Gn}
be the resultant groups, where each Gi =
{cpi1, cpi2, ...} identify a group of concept pairs
sharing the same keyword tcp (such as ?CEO?).
We rank all the groups by their number of concept
pairs and then choose the top k groups. Then a
centroid ci is selected for each group Gi by Eq. 2.
ci = argmaxcp?Gi |{cpij |(dis1(cpij , cp)+
? ? dis2(cpij , cp)) <= Dz, 1 ? j ? |Gi|}| (2)
We assume a centroid for each group to be the
concept pair which has the most other concept
pairs in the same group that have distance less
than Dz with it. Also, Dz is a threshold to avoid
noisy concept pairs: we assign it 1/3. To balance
the contribution between dependency patterns and
surface patterns, ? is used. The distance function
to calculate the distance between dependency pat-
tern sets DPi, DPj of two concept pairs cpi and
cpj is dis1. The distance is decided by the number
of overlapped dependency patterns with Eq. 3.
dis1(cpi, cpj) = 1? |DPi ?DPj |?(|DPi| ? |DPj |)
(3)
Actually, dis2 is the distance function to calcu-
late distance between two surface pattern sets of
two concept pairs. To compute the distance over
surface patterns, we implement the distance func-
tion dis2(cpi, cpj) in Fig. 2.
Algorithm 1: distance function dis2(cpi, cpj)
Input: SP1 = {sp11, ..., sp1m}(surface patterns of
cpi)
SP2 = {sp21, ..., sp2n} (surface patterns of cpj)
Output: dis (distance between SP1 and SP2)
define a m? n distance matrix A:
{Aij = LD(sp1i,sp2j)Max(|sp1i|,|sp2j |) , 1?i?m; 1?j?n};
dis ? 0
for min(m,n) times do
(x, y) ? argmin0<i<m;0<j<nAij ;
dis ? dis + Axy/min(m,n);
Ax? ? 1; A?y ? 1;
return dis
Figure 2: Distance function over surface patterns
As shown in Fig. 2, the distance algorithm per-
forms as: firstly it defines a m?n distance matrix
A, then repeatedly selects two nearest sequences
and sums up their distances. While computing
1025
dis2, we use the Levenshtein distance LD to mea-
sure the difference of two surface patterns. The
Levenshtein distance is a metric for measuring the
amount of difference between two sequences (i.e.,
the so-called edit distance). Each generated sur-
face pattern is a sequence of words. The distance
of two surface patterns is defined as the fraction of
the LD value to the length of the longer sequence.
For estimating the number of clusters k, we ap-
ply the stability-based criteria from (Chen et al,
2005) to decide the number of optimal clusters k
automatically.
4.5.2 Concept Pair Clustering with
Dependency Patterns
Given the initial seed concept pairs and cluster
number k, this stage merges concept pairs over de-
pendency patterns into k clusters. Each concept
pair cpi has a set of dependency patterns DPi. We
calculate distances between two pairs cpi and cpj
using above the function dis1(cpi, cpj). The clus-
tering algorithm is portrayed in Fig. 3. The pro-
cess of depend clustering is to assign each concept
pair to the cluster with the closest centroid and
then recomputing each centroid based on the cur-
rent members of its cluster. As shown in Figure 3,
this is done iteratively by repeating both two steps
until a stopping criterion is met. We apply the ter-
mination condition as: centroids do not change be-
tween iterations.
Algorithm 2: Depend Clustering
Input: I = {cp1, ..., cpn}(all concept pairs)
C = {c1, ..., ck} (k initial centroids)
Output: Md : I ? C (cluster membership)
Ir (rest of concept pairs not clustered)
Cd = {c1, ..., ck} (recomputed centroids)
while stopping criterion has not been met do
for each cpi ? I do
if mins?1..k dis1(cpi, cs) <= Dl then
Md(cpi) ? argmins?1..k dis1(cpi, cs)else
Md(cpi) ? 0
for each j ? {1..k} do
recompute cj as the centroid of
{cpi|mloc(cpi) = j}
Ir ? C0
return C and Cd
Figure 3: Clustering with dependency patterns
Because many concept pairs are scattered and
do not belong to any of the top k clusters, we
filter concept pairs with distance larger than Dl
with the seed concept pairs. Such concept pairs
ST1
ST3 ST4
ST2
Text3: RC was hired as EC?s CEO Text4: EC assign RC as CEO
Text1: the CEO of EC is RC Text2: RC is the CEO of EC
Figure 4: Example showing why surface cluster-
ing is needed
are stored in C0. We named the cluster of concept
pairs Ir which are left to be clustered in the next
step of clustering. After this step, concept pairs
with similar dependency patterns are merged into
same clusters, see Fig. 4 (ST1, ST2).
4.5.3 Concept Pair Clustering with Surface
Patterns
A salient difficulty posed by dependency pattern
clustering is that concept pairs of the same se-
mantic relation cannot be merged if they are ex-
pressed in different dependency structures. Fig-
ure 4 presents an example demonstrating why we
perform surface pattern clustering. As depicted
in Fig. 4, ST1, ST2, ST3, and ST4 are depen-
dency structures for four concept pairs that should
be classified as the same relation ?CEO?. However
ST3 and ST4 can not be merged with ST1 and
ST2 using the dependency patterns because their
dependency structures are too diverse to share suf-
ficient dependency patterns.
In this step, we use surface patterns to merge
more concept pairs for each cluster to improve the
coverage. Figure 5 portrays the algorithm. We
assume that each concept pair has a set of sur-
face patterns from the Web context collector mod-
ule. As shown in Figure 5, surface clustering is
done iteratively by repeating two steps until a stop-
ping criterion is met: using the distance function
dis2 explained in the preceding section, assign
each concept pair to the cluster with the closest
centroid and recomputing each centroid based on
the current members of its cluster. We apply the
same termination condition as depend clustering.
1026
Additionally, we filter concept pairs with distance
greater than Dg with the centroid concept pairs.
Algorithm 3: Surface Clustering
Input: Ir (rest of concept pairs)
Cd = {c1, ..., ck} (initial centroids)
Output: Ms : Ir ? C (cluster membership)
Cs = {c1, ..., ck} (final centroids)
while stopping criterion has not been met do
for each cpi ? Ir do
if mins?1..k dis2(cpi, cs) <= Dg then
Ms(cpi) ? argmins?1..k dis2(cpi, cs)else
Ms(cpi) ? 0
for each j ? 1..k do
recompute cj as the centroid of cluster
{cpi|Md(cpi) = j ?Ms(cpi) = j}
return clusters C
Figure 5: Clustering with surface patterns
Finally we have k clusters of concept pairs, each
of which has a centroid concept pair. To attach
a single relation label to each cluster, we use the
centroid concept pair.
5 Experiments
We apply our algorithm to two categories in
Wikipedia: ?American chief executives? and
?Companies?. Both categories are well defined
and closed. We conduct experiments for extract-
ing various relations and for measuring the quality
of these relations in terms of precision and cover-
age. We use coverage as an evaluation instead of
using recall as a measure. The coverage is used to
evaluate all correctly extracted concept pairs. It is
defined as the fraction of all the correctly extracted
concept pairs to the whole set of concept pairs. To
balance between precision and coverage of clus-
tering, we integrate two parameters: Dl, Dg.
We downloaded the Wikipedia dump as of De-
cember 3, 2008. The performance of the pro-
posed method is evaluated using different pattern
types: dependency patterns, surface patterns, and
their combination. We compare our method with
(Rosenfeld and Feldman, 2007)?s URI method.
Their algorithm outperformed that presented in the
earlier work using surface features of two kinds for
unsupervised relation extraction: features that test
two entities together and features that test only one
entity each. For comparison, we use a k-means
clustering algorithm using the same cluster num-
ber k.
Table 2: Results for the category: ?American chief
executives?
method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
chairman 434 63.52 547 68.37
(x be chairman of y)
ceo 396 73.74 423 77.54
(x be ceo of y)
bear 138 83.33 276 86.96
(x be bear in y)
attend 225 67.11 313 70.28
(x attend y)
member 14 85.71 175 91.43
(x be member of y)
receive 97 67.97 117 73.53
(x receive y)
graduate 18 83.33 92 88.04
(x graduate from y)
degree 5 80.00 78 82.05
(x obtain y degree)
marry 55 41.67 74 61.25
(x marry y)
earn 23 86.96 51 88.24
(x earn y)
award 23 43.47 46 84.78
(x won y award)
hold 5 80.00 37 72.97
(x hold y degree)
become 35 74.29 37 81.08
(x become y)
director 24 67.35 29 79.31
(x be director of y)
die 18 77.78 19 84.21
(x die in y)
all 1510 68.27 2314 75.63
5.1 Wikipedia Category: ?American chief
executives?
We choose appropriate Dl(concept pair filter in
depend clustering) and Dg(concept pair filter in
surface clustering) in a development set. To bal-
ance precision and coverage, we set 1/3 for both
Dl and Dg.
The 526 articles in this category are used for
evaluation. We obtain 7310 concept pairs from
the articles as our dataset. The top 18 groups are
chosen to obtain the centroid concept pairs. Of
these, 15 binary relations are the clearly identifi-
able relations shown in Table 2, where # Ins. rep-
resents the number of concept pairs clustered us-
ing each method, and pre denotes the precision of
each cluster.
The proposed approach shows higher precision
and better coverage than URI in Table 2. This
result demonstrates that adding dependency pat-
terns from linguistic analysis contributes more to
the precision and coverage of the clustering task
than the sole use of surface patterns.
1027
Table 3: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 1127 84.29 13.00%
surface 1510 68.27 14.10%
Combined 2314 75.63 23.94%
Table 4: Results for the category: ?Companies?
Method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
found 82 75.61 163 84.05
(found x in y)
base 82 76.83 122 82.79
(x be base in y)
headquarter 23 86.97 120 89.34
(x be headquarter in y)
service 37 51.35 108 69.44
(x offer y service)
store 113 77.88 88 72.72
(x open store in y)
acquire 59 62.71 70 64.28
(x acquire y)
list 51 64.71 67 70.15
(x list on y)
product 25 76.00 57 77.19
(x produce y)
CEO 37 64.86 39 66.67
(ceo x found y)
buy 53 62.26 37 56.76
(x buy y)
establish 35 82.86 26 80.77
(x be establish in y)
locate 14 50.00 24 75.00
(x be locate in y)
all 685 71.03 1039 76.87
To examine the contribution of dependency pat-
terns, we compare results obtained with patterns
of different kinds. Table 3 shows the precision and
coverage scores. The best precision is achieved by
dependency patterns. The precision is markedly
better than that of surface patterns. However, the
coverage is worse than that by surface patterns. As
we reported, many concept pairs are scattered and
do not belong to any of the top k clusters, the cov-
erage is low.
5.2 Wikipedia Category: ?Companies?
We also evaluate the performance for the ?Com-
panies? category. Instead of using all the arti-
cles, we randomly select 434 articles for evalua-
tion and 4073 concept pairs from the articles form
our dataset for this category. We also set Dl and
Dg to 1/3. Then 28 groups are chosen. For each
group, a centroid concept pair is obtained. Finally,
of 28 clusters, 25 binary relations are clearly iden-
tifiable relations. Table 4 presents some relations.
Table 5: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 551 82.58 11.17%
surface 685 71.03 11.95%
Combined 1039 76.87 19.61%
Our clustering algorithms use two filters Dl and
Dg to filter scattering concept pairs. In Table 4, we
present that concept pairs are clustered with good
precision. As in the first experiments, the combi-
nation of dependency patterns and surface patterns
contribute greatly to the precision and coverage.
Table 5 shows that, using dependency patterns,
the precision is the highest (82.58%), although the
coverage is the lowest.
All experimental results support our idea
mainly in two aspects: 1) Dependency analysis
can abstract away from different surface realiza-
tions of text. In addition, embedded structures of
the dependency representation are important for
obtaining a good coverage of the pattern acqui-
sition. Furthermore, the precision is better than
that of the string surface patterns from Web pages
of various kinds. 2) Surface patterns are used to
merge concept pairs with relations represented in
different dependency structures with redundancy
information from the vast size of Web pages. Us-
ing surface patterns, more concept pairs are clus-
tered, and the coverage is improved.
6 Conclusions
To discover a range of semantic relations from
a large corpus, we present an unsupervised rela-
tion extraction method using deep linguistic in-
formation to alleviate surface and noisy surface
patterns generated from a large corpus, and use
Web frequency information to ease the sparse-
ness of linguistic information. We specifically ex-
amine texts from Wikipedia articles. Relations
are gathered in an unsupervised way over pat-
terns of two types: dependency patterns by parsing
sentences in Wikipedia articles using a linguistic
parser, and surface patterns from redundancy in-
formation from the Web corpus using a search en-
gine. We report our experimental results in com-
parison to those of previous works. The results
show that the best performance arises from a com-
bination of dependency patterns and surface pat-
terns.
1028
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead and Oren Etzioni. 2007.
Open information extraction from the Web. In Pro-
ceedings of IJCAI-2007.
Danushka Bollegala, Yutaka Matsuo and Mitsuru
Ishizuka. 2007. Measuring Semantic Similarity be-
tween Words Using Web Search Engines. In Pro-
ceedings of WWW-2007.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT/EMLNP-2005.
Jinxiu Chen, Donghong Ji, Chew Lim Tan and
Zhengyu Niu. 2005. Unsupervised Feature Se-
lection for Relation Extraction. In Proceedings of
IJCNLP-2005.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL-2004.
Dmitry Davidov, Ari Rappoport and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by Web mining. In Proceed-
ings of ACL-2007.
Dmitry Davidov and Ari Rappoport. 2008. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. In Proceedings of ACL-
2008.
Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng
Yan, Jiawei Han, Philip S. Yu and Olivier Ver-
scheure. 2008. Direct Mining of Discriminative and
Essential Frequent Patterns via Model-based Search
Tree. In Proceedings of KDD-2008.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proceedings of
AAAI-2006.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature 438:900C901.
Sanda Harabagiu, Cosmin Adrian Bejan and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In Proceedings of IJCAI-2005.
Takaaki Hasegawa, Satoshi Sekine and Ralph Grish-
man. 2004. Discovering Relations among Named
Entities from Large Corpora. In Proceedings of
ACL-2004.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els. In Proceedings of ACL-2004.
Dat P.T. Nguyen, Yutaka Matsuo and Mitsuru Ishizuka.
2007. Relation extraction from Wikipedia using sub-
tree mining. In Proceedings of AAAI-2007.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceedings
of ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2006.
URES: an Unsupervised Web Relation Extraction
System. In Proceedings of COLING/ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In
Proceedings of CIKM-2007.
Peter D. Turney. 2006. Expressing implicit seman-
tic relations without supervision. In Proceedings of
ACL-2006.
Max Volkel, Markus Krotzsch, Denny Vrandecic,
Heiko Haller and Rudi Studer. 2006. Semantic
wikipedia. In Proceedings of WWW-2006.
Mohammed J. Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of SIGKDD-2002.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of ACL-2006.
1029
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extracting Key Phrases to Disambiguate
Personal Name Queries in Web Search
Danushka Bollegala Yutaka Matsuo ?
Graduate School of Information Science and Technology
The University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
danushka@mi.ci.i.u-tokyo.ac.jp
y.matsuo@aist.go.jp
ishizuka@i.u-tokyo.ac.jp
Mitsuru Ishizuka
Abstract
Assume that you are looking for informa-
tion about a particular person. A search
engine returns many pages for that per-
son?s name. Some of these pages may
be on other people with the same name.
One method to reduce the ambiguity in the
query and filter out the irrelevant pages, is
by adding a phrase that uniquely identi-
fies the person we are interested in from
his/her namesakes. We propose an un-
supervised algorithm that extracts such
phrases from the Web. We represent each
document by a term-entity model and clus-
ter the documents using a contextual sim-
ilarity metric. We evaluate the algorithm
on a dataset of ambiguous names. Our
method outperforms baselines, achieving
over 80% accuracy and significantly re-
duces the ambiguity in a web search task.
1 Introduction
The Internet has grown into a collection of bil-
lions of web pages. Web search engines are im-
portant interfaces to this vast information. We
send simple text queries to search engines and re-
trieve web pages. However, due to the ambigu-
ities in the queries, a search engine may return
a lot of irrelevant pages. In the case of personal
name queries, we may receive web pages for other
people with the same name (namesakes). For ex-
ample, if we search Google 1 for Jim Clark, even
among the top 100 results we find at least eight
different Jim Clarks. The two popular namesakes;
?National Institute of Advanced Industrial Science and
Technology
1www.google.com
Jim Clark the Formula one world champion (46
pages), and Jim Clark the founder of Netscape (26
pages), cover the majority of the pages. What if
we are interested only in the Formula one world
champion and want to filter out the pages for the
other Jim Clarks? One solution is to modify our
query by including a phrase such as Formula one
or racing driver with the name, Jim Clark.
This paper presents an automatic method to ex-
tract such phrases from the Web. We follow a
three-stage approach. In the first stage we rep-
resent each document containing the ambiguous
name by a term-entity model, as described in sec-
tion 5.2. We define a contextual similarity metric
based on snippets returned by a search engine, to
calculate the similarity between term-entity mod-
els. In the second stage, we cluster the documents
using the similarity metric. In the final stage, we
select key phrases from the clusters that uniquely
identify each namesake.
2 Applications
Two tasks that can readily benefit from automat-
ically extracted key phrases to disambiguate per-
sonal names are query suggestion and social net-
work extraction. In query suggestion (Gauch and
Smith, 1991), the search engine returns a set of
phrases to the user alongside with the search re-
sults. The user can then modify the original query
using these phrases to narrow down the search.
Query suggestion helps the users to easily navigate
through the result set. For personal name queries,
the key phrases extracted by our algorithm can be
used as suggestions to reduce the ambiguity and
narrow down the search on a particular namesake.
Social networking services (SNSs) have been
given much attention on the Web recently. As
a kind of online applications, SNSs can be used
17
to register and share personal information among
friends and communities. There have been recent
attempts to extract social networks using the infor-
mation available on the Web 2(Mika, 2004; Mat-
suo et al, 2006). In both Matsuo?s (2006) and
Mika?s (2004) algorithms, each person is repre-
sented by a node in the social network and the
strength of the relationship between two people
is represented by the length of the edge between
the corresponding two nodes. As a measure of the
strength of the relationship between two people A
and B, these algorithms use the number of hits ob-
tained for the query A AND B. However, this ap-
proach fails when A or B has namesakes because
the number of hits in these cases includes the hits
for the namesakes. To overcome this problem, we
could include phrases in the query that uniquely
identify A and B from their namesakes.
3 Related Work
Person name disambiguation can be seen as
a special case of word sense disambiguation
(WSD) (Schutze, 1998; McCarthy et al, 2004)
problem which has been studied extensively in
Natural Language Understanding. However, there
are several fundamental differences between WSD
and person name disambiguation. WSD typically
concentrates on disambiguating between 2-4 pos-
sible meanings of the word, all of which are a
priori known. However, in person name disam-
biguation in Web, the number of different name-
sakes can be much larger and unknown. From a
resource point of view, WSD utilizes sense tagged
dictionaries such as WordNet, whereas no dictio-
nary can provide information regarding different
namesakes for a particular name.
The problem of person name disambiguation
has been addressed in the domain of research pa-
per citations (Han et al, 2005), with various super-
vised methods proposed for its solution. However,
citations have a fixed format compared to free text
on the Web. Fields such as co-authors, title, jour-
nal name, conference name, year of publication
can be easily extracted from a citation and provide
vital information to the disambiguation process.
Research on multi-document person name res-
olution (Bagga and Baldwin, 1998; Mann and
Yarowsky, 2003; Fleischman and Hovy, 2004) fo-
cuses on the related problem of determining if
2http://flink.sematicweb.org/. The system won the 1st
place at the Semantic Web Challenge in ISWC2004.
two instances with the same name and from dif-
ferent documents refer to the same individual.
Bagga and Baldwin (1998) first perform within-
document coreference resolution to form coref-
erence chains for each entity in each document.
They then use the text surrounding each reference
chain to create summaries about each entity in
each document. These summaries are then con-
verted to a bag of words feature vector and are
clustered using standard vector space model of-
ten employed in IR. The use of simplistic bag of
words clustering is an inherently limiting aspect of
their methodology. On the other hand, Mann and
Yarowsky (2003) proposes a richer document rep-
resentation involving automatically extracted fea-
tures. However, their clustering technique can be
basically used only for separating two people with
the same name. Fleischman and Hovy (2004) con-
structs a maximum entropy classifier to learn dis-
tances between documents that are then clustered.
Their method requires a large training set.
Pedersen et al (2005) propose an unsupervised
approach to resolve name ambiguity by represent-
ing the context of an ambiguous name using sec-
ond order context vectors derived using singular
value decomposition (SVD) on a co-occurrence
matrix. They agglomeratively cluster the vec-
tors using cosine similarity. They evaluate their
method only on a conflated dataset of pseudo-
names, which begs the question of how well such
a technique would fair on a more real-world chal-
lenge. Li et al (2005) propose two approaches to
disambiguate entities in a set of documents: a su-
pervisedly trained pairwise classifier and an unsu-
pervised generative model. However, they do not
evaluate the effectiveness of their method in Web
search.
Bekkerman and McCallum (2005) present two
unsupervised methods for finding web pages re-
ferring to a particular person: one based on
link structure and another using Agglomera-
tive/Conglomerative Double Clustering (A/CDC).
Their scenario focuses on simultaneously disam-
biguating an existing social network of people,
who are closely related. Therefore, their method
cannot be applied to disambiguate an individual
whose social network (for example, friends, col-
leagues) is not known. Guha and Grag (2004)
present a re-ranking algorithm to disambiguate
people. The algorithm requires a user to select one
of the returned pages as a starting point. Then,
18
Table 1: Data set for experiments
Collection No of namesakes
person-X 4
Michael Jackson 3
Jim Clark 8
William Cohen 10
through comparing the person descriptions, the al-
gorithm re-ranks the entire search results in such
a way that pages referring to the same person de-
scribed in the user-selected page are ranked higher.
A user needs to browse the documents in order to
find which matches the user?s intended referent,
which puts an extra burden on the user.
None of the above mentioned works attempt to
extract key phrases to disambiguate person name
queries, a contrasting feature in our work.
4 Data Set
We select three ambiguous names (Micheal Jack-
son, William Cohen and Jim Clark) that appear in
previous work in name resolution. For each name
we query Google with the name and download top
100 pages. We manually classify each page ac-
cording to the namesakes discussed in the page.
We ignore pages which we could not decide the
namesake from the content. We also remove pages
with images that do not contain any text. No pages
were found where more than one namesakes of a
name appear. For automated pseudo-name evalua-
tion purposes, we select four names (Bill Clinton,
Bill Gates, Tom Cruise and Tiger Woods) for con-
flation, who we presumed had one vastly predom-
inant sense. We download 100 pages from Google
for each person. We replace the name of the per-
son by ?person-X? in the collection, thereby intro-
ducing ambiguity. The structure of our dataset is
shown in Table 1.
5 Method
5.1 Problem Statement
Given a collection of documents relevant to an am-
biguous name, we assume that each document in
the collection contains exactly one namesake of
the ambiguous name. This is a fair assumption
considering the fact that although namesakes share
a common name, they specializes in different
fields and have different Web appearances. More-
over, the one-to-one association between docu-
ments and people formed by this assumption, let
us model the person name disambiguation prob-
lem as a one of hard-clustering of documents.
The outline of our method is as following;
Given a set of documents representing a group of
people with the same name, we represent each
document in the collection using a Term-Entity
model (section 5.2). We define a contextual sim-
ilarity metric (section 5.4) and then cluster (sec-
tion 5.5) the term-entity models using the contex-
tual similarity between them. Each cluster is con-
sidered to be representing a different namesake.
Finally, key phrases that uniquely identify each
namesake are selected from the clusters. We per-
form experiments at each step of our method to
evaluate its performance.
5.2 Term-Entity Model
The first step toward disambiguating a personal
name is to identify the discriminating features of
one person from another. In this paper we propose
Term-Entity models to represent a person in a doc-
ument.
Definition. A term-entity model T (A), represent-
ing a person A in a document D, is a boolean
expression of n literals a1, a2, . . . , an. Here, a
boolean literal ai is a multi-word term or a named
entity extracted from the document D.
For simplicity, we only consider boolean ex-
pressions that combine the literals through AND
operator.
The reasons for using terms as well as named
entities in our model are two fold. Firstly, there are
multi-word phrases such as secretary of state, rac-
ing car driver which enable us to describe a person
uniquely but not recognized by named entity tag-
gers. Secondly, automatic term extraction (Frantzi
and Ananiadou, 1999) can be done using statistical
methods and does not require extensive linguistic
resources such as named entity dictionaries, which
may not be available for some domains.
5.3 Creating Term-Entity Models
We extract terms and named entities from each
document to build the term-entity model for that
document. For automatic multi-word term ex-
traction, we use the C-value metric proposed by
Frantzi et al (1999). Firstly, the text from which
we need to extract terms is tagged using a part
of speech tagger. Then a linguistic filter and a
stop words list constrain the word sequences that
19
020
40
60
80
100
120 President of the United States
George Bush
pr
es
id
en
tia
l
ge
or
ge
pr
es
id
en
t
ne
ws
bi
og
ra
ph
y
ga
m
es
bu
sh
bu
sh
s
lib
ra
ry
fa
th
er
vic
e
go
ve
rn
m
en
t
pr
es
id
en
ts
sh
al
l
un
ite
d
st
at
es
ex
ec
ut
ive
Figure 1: Distribution of words in snippets for
?George Bush? and ?President of the United
States?
are allowed as genuine multi-word terms. The
linguistic filter contains a predefined set of pat-
terns of nouns, adjectives and prepositions that are
likely to be terms. The sequences of words that re-
main after this initial filtering process (candidate
terms) are evaluated for their termhood (likeliness
of a candidate to be a term) using C-value. C-
value is built using statistical characteristics of the
candidate string, such as, total frequency of oc-
currence of the candidate string in the document,
the frequency of the candidate string as part of
other longer candidate strings, the number of these
longer candidate terms and the length of candidate
string (in number of words). We select the candi-
dates with higher C-values as terms (see (Frantzi
and Ananiadou, 1999) for more details on C-value
based term extraction).
To extract entities for the term-entity model, the
documents were annotated by a named entity tag-
ger 3. We select personal names, organization
names and location names to be included in the
term-entity model.
5.4 Contextual Similarity
We need to calculate the similarity between term-
entity models derived from different documents,
in order to decide whether they belong to the
same namesake or not. WordNet 4 based similar-
ity metrics have been widely used to compute the
semantic similarity between words in sense dis-
3The named entity tagger was developed by the Cognitive
Computation Group at UIUC. http://L2R.cs.uiuc.edu/ cog-
comp/eoh/ne.html
4http://wordnet.princeton.edu/perl/webwn
0
30
60
90
120
150
President of the United States
Tiger Woods
st
at
es
ge
or
ge
go
lfe
r
bu
sh
w
oo
ds
to
ur
vi
ce
tig
er
ch
ea
ts
sh
al
l
go
ve
rn
m
en
t
pr
es
id
en
ts
pg
a
go
lf
un
ite
d
pr
es
id
en
t
re
vi
ew
s
ex
ec
ut
iv
e
Figure 2: Distribution of words in snippets for
?Tiger Woods? and ?President of the United
States?
ambiguation tasks (Banerjee and Pedersen, 2002;
McCarthy et al, 2004). However, most of the
terms and entities in our term-entity models are
proper names or multi-word expressions which are
not listed in WordNet.
Sahami et al (2005) proposed the use of snip-
pets returned by a Web search engine to calculate
the semantic similarity between words. A snippet
is a brief text extracted from a document around
the query term. Many search engines provide snip-
pets alongside with the link to the original docu-
ment. Since snippets capture the immediate sur-
rounding of the query term in the document, we
can consider a snippet as the context of a query
term. Using snippets is also efficient because we
do not need to download the source documents.
To calculate the contextual similarity between two
terms (or entities), we first collect snippets for
each term (or entity) and pool the snippets into
a combined ?bag of words?. Each collection of
snippets is represented by a word vector, weighted
by the normalized frequency (i.e., frequency of a
word in the collection is divided by the total num-
ber of words in the collection). Then, the contex-
tual similarity between two phrases is defined as
the inner product of their snippet-word vectors.
Figures 1 and 2 show the distribution of most
frequent words in snippets for the queries ?George
Bush?, ?Tiger Woods? and ?President of the
United States?. In Figure 1 we observe the words
?george? and ?bush? appear in snippets for the
query ?President of the United States?, whereas
in Figure 2 none of the high frequent words ap-
pears in snippets for both queries. Contextual
20
similarity calculated as the inner product between
word vectors is 0.2014 for ?George Bush? and
?President of the United States?, whereas the
same is 0.0691 for ?Tiger Woods? and ?Presi-
dent of the United States?. We define the simi-
larity sim(T (A), T (B)), between two term-entity
models T (A) = {a1, . . . , an} and T (B) =
{b1, . . . , bm} of documents A and B as follows,
sim(T (A), T (B)) = 1n
n?
i=1
max
1?j?m
|ai| ? |bj |. (1)
Here, |ai| represents the vector that contains the
frequency of words that appear in the snippets
for term/entity ai. Contextual similarity between
terms/entities ai and bj , is defined as the inner
product |ai| ? |bj |. Without a loss of generality we
assume n ? m in formula 1.
5.5 Clustering
We use Group-average agglomerative clustering
(GAAC) (Cutting et al, 1992), a hybrid of single-
link and complete-link clustering, to group the
documents that belong to a particular namesake.
Initially, we assign a separate cluster for each of
the documents in the collection. Then, GAAC in
each iteration executes the merger that gives rise
to the cluster ? with the largest average correla-
tion C(?) where,
C(?) = 12
1
|?|(|?| ? 1)
X
u??
X
v??
sim(T (u), T (v)) (2)
Here, |?| denotes the number of documents in
the merged cluster ?; u and v are two documents
in ? and sim(T (u), T (v)) is given by equation 1.
Determining the total number of clusters is an im-
portant issue that directly affects the accuracy of
disambiguation. We will discuss an automatic
method to determine the number of clusters in sec-
tion 6.3.
5.6 Key phrases Selection
GAAC process yields a set of clusters representing
each of the different namesakes of the ambiguous
name. To select key phrases that uniquely iden-
tify each namesake, we first pool all the terms and
entities in all term-entity models in each cluster.
For each cluster we select the most discrimina-
tive terms/entities as the key phrases that uniquely
identify the namesake represented by that cluster
from the other namesakes. We achieve this in
two steps. In the first step, we reduce the num-
ber of terms/entities in each cluster by removing
terms/entities that also appear in other clusters.
In the second step, we select the terms/entities
in each cluster according to their relevance to
the ambiguous name. We compute the con-
textual similarity between the ambiguous name
and each term/entity and select the top ranking
terms/entities from each cluster.
6 Experiments and Results
6.1 Evaluating Contextual Similarity
In section 5.4, we defined the similarity between
documents (i.e., term-entity models created from
the documents) using a web snippets based con-
textual similarity (Formula 1). However, how well
such a metric represents the similarity between
documents, remains unknown. Therefore, to eval-
uate the contextual similarity among documents,
we group the documents in ?person-X? dataset
into four classes (each class representing a differ-
ent person) and use Formula 1 to compute within-
class and cross-class similarity histograms, as il-
lustrated in Figure 3.
Ideally, within-class similarity distribution
should have a peak around 1 and cross-class sim-
ilarity distribution around 0, whereas both his-
tograms in Figure 3(a) and 3(b) have their peaks
around 0.2. However, within-class similarity dis-
tribution is heavily biased toward to the right of
this peak and cross-class similarity distribution to
the left. Moreover, there are no document pairs
with more than 0.5 cross-class similarity. The ex-
perimental results guarantees the validity of the
contextual similarity metric.
6.2 Evaluation Metric
We evaluate experimental results based on the
confusion matrix, where A[i.j] represents the
number of documents of ?person i? predicted as
?person j? in matrix A. A[i, i] represents the num-
ber of correctly predicted documents for ?person
i?. We define the disambiguation accuracy as the
sum of diagonal elements divided by the sum of
all elements in the matrix.
6.3 Cluster Quality
Each cluster formed by the GAAC process is sup-
posed to be representing a different namesake.
Ideally, the number of clusters formed should be
equal to the number of different namesakes for
21
0300
600
900
1200
1500
1.11.00.90.80.70.60.50.40.30.20.1
(a) Within-class similarity distribution in
?person-X? dataset
0
1000
2000
3000
4000
5000
1.11.00.90.80.70.60.50.40.30.20.1
(b) Cross-class similarity distribution in
?person-X? dataset
Figure 3: The histogram of within-class and cross-class similarity distributions in ?person-X? dataset. X
axis represents the similarity value. Y axis represents the number of document pairs from the same class
(within-class) or from different classes (cross-class) that have the corresponding similarity value.
the ambiguous name. However, in reality it is
impossible to exactly know the number of name-
sakes that appear on the Web for a particular name.
Moreover, the distribution of pages among name-
sakes is not even. For example, in the ?Jim Clark?
dataset 78% of documents belong to the two fa-
mous namesakes (CEO Nestscape and Formula
one world champion). The rest of the documents
are distributed among the other six namesakes. If
these outliers get attached to the otherwise pure
clusters, both disambiguation accuracy and key
phrase selection deteriorate. Therefore, we moni-
tor the quality of clustering and terminate further
agglomeration when the cluster quality drops be-
low a pre-set threshold. Numerous metrics have
been proposed for evaluating quality of cluster-
ing (Kannan et al, 2000). We use normalized
cuts (Shi and Malik, 2000) as a measure of cluster-
quality.
Let, V denote the set of documents for a name.
Consider, A ? V to be a cluster of documents
taken from V . For two documents x,y in V ,
sim(x, y) represents the contextual similarity be-
tween the documents (Formula 1). Then, the nor-
malized cut Ncut(A) of cluster A is defined as,
Ncut(A) =
?
x?A y?(V?A) sim(x, y)?
x?A y?V sim(x, y)
. (3)
For a set, {A1, . . . , An} of non-overlapping n
clusters Ai, we define the quality of clustering,
Ac
cu
ra
cy
Quality
0
0.2
0.4
0.6
0.8
1
1.2
0.8 0.85 0.9 0.95 1 1.05
Figure 4: Accuracy Vs Cluster Quality for person-
X data set.
Quality({A1, . . . , An}), as follows,
Quality({A1, . . . , An}) = 1n
n?
i=1
Ncut(Ai). (4)
To explore the faithfulness of cluster quality
in approximating accuracy, we compare accuracy
(calculated using human-annotated data) and clus-
ter quality (automatically calculated using For-
mula 4) for person-X data set. Figure 4 shows
cluster quality in x-axis and accuracy in y-axis.
We observe a high correlation (Pearson coefficient
of 0.865) between these two measures, which en-
ables us to guide the clustering process through
cluster quality.
When cluster quality drops below a pre-defined
22
Threshold
Ac
cu
ra
cy
0.69
0.7
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.6 0.7 0.8 0.9 1
Figure 5: Accuracy Vs Threshold value for
person-X data set.
threshold, we terminate further clustering. We
assign the remaining documents to the already
formed clusters based on the correlation (For-
mula 2) between the document and the cluster. To
determine the threshold of cluster quality, we use
person-X collection as training data. Figure 5 il-
lustrates the variation of accuracy with threshold.
We select threshold at 0.935 where accuracy max-
imizes in Figure 5. Threshold was fixed at 0.935
for the rest of the experiments.
6.4 Disambiguation Accuracy
Table 2 summarizes the experimental results. The
baseline, majority sense , assigns all the doc-
uments in a collection to the person that have
most documents in the collection. Proposed
method outperforms the baseline in all data sets.
Moreover, the accuracy values for the proposed
method in Table 2 are statistically significant (t-
test: P(T?t)=0.0087, ? = 0.05) compared to the
baseline. To identify each cluster with a name-
sake, we chose the person that has most num-
ber of documents in the cluster. ?Found? column
shows the number of correctly identified name-
sakes as a fraction of total namesakes. Although
the proposed method correctly identifies the pop-
ular namesakes, it fails to identify the namesakes
who have just one or two documents in the collec-
tion.
6.5 Web Search Task
Key phrases extracted by the proposed method are
listed in Figure 6 (Due to space limitations, we
show only the top ranking key phrases for two col-
lections). To evaluate key phrases in disambiguat-
Table 2: Disambiguation accuracy for each collec-
tion.
Collection Majority Proposed Found
Sense Method Correct
person-X 0.3676 0.7794 4/4
Michael Jackson 0.6470 0.9706 2/3
Jim Clark 0.4407 0.7627 3/8
William Cohen 0.7614 0.8068 3/10
Michael Jackson
Jim Clark
fan club
trial
world network
superstar 
new charity song
neverland ranch
beer hunter
ultimate beer FAQ
christmas beer
great beer
pilsener beer
barvaria
CLUSTER #1 CLUSTER #2
CLUSTER #1 CLUSTER #2
racing driver
rally
scotsman
driving genius
scottish automobile racer
british rally news
entrepreneur
story
silicon valley
CEO
silicon graphics 
SGI/ Netscape
Figure 6: Top ranking key phrases in clusters for
Michael Jackson and Jim Clark datasets.
ing namesakes, we set up a web search experiment
as follows. We search for the ambiguous name and
the key phrase (for example, ?Jim Clark? AND
?racing driver?) and classify the top 100 results
according to their relevance to each namesake. Re-
sults of our experiment on Jim Clark dataset for
the top ranking key phrases are shown in Table 3.
In Table 3 we classified Google search results
into three categories. ?person-1? is the formula
one racing world champion, ?person -2? is the
founder of Netscape and ?other? category contains
rest of the pages that we could not classify to pre-
vious two groups 5. We first searched Google
without adding any key phrases to the name. In-
cluding terms racing diver, rally and scotsman,
Table 3: Effectiveness of key phrases in disam-
biguating namesakes.
Phrase person-1 person-2 others Hits
NONE 41 26 33 1,080,000
racing driver 81 1 18 22,500
rally 42 0 58 82,200
scotsman 67 0 33 16,500
entrepreneur 1 74 25 28,000
story 17 53 30 186,000
silicon valley 0 81 19 46,800
5some of these pages were on other namesakes and some
were not sufficiently detailed to properly classify
23
which were the top ranking terms for Jim Clark
the formula one champion, yields no results for the
other popular namesake. Likewise, the key words
entrepreneur and silicon valley yield results fort
he founder of Netscape. However, the key word
story appears for both namesakes. A close investi-
gation revealed that, the keyword story is extracted
from the title of the book ?The New New Thing:
A Silicon Valley Story?, a book on the founder of
Netscape.
7 Conclusion
We proposed and evaluated a key phrase extraction
algorithm to disambiguate people with the same
name on the Web. We represented each document
with a term-entity model and used a contextual
similarity metric to cluster the documents. We also
proposed a novel approach to determine the num-
ber of namesakes. Our experiments with pseudo
and naturally ambiguous names show a statisti-
cally significant improvement over the baseline
method. We evaluated the key phrases extracted
by the algorithm in a web search task. The web
search task reveals that including the key phrases
in the query considerably reduces ambiguity. In
future, we plan to extend the proposed method
to disambiguate other types of entities such as
location names, product names and organization
names.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proceedings of COLING, pages 79?85.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using word net. In Proceedings of the third in-
ternational conference on computational linguistics
and intelligent text processing, pages 136?145.
Ron Bekkerman and Andrew McCallum. 2005. Dis-
ambiguating web appearances of people in a social
network. In Proceedings of the 14th international
conference on World Wide Web, pages 463?470.
Douglass R. Cutting, Jan O. Pedersen, David Karger,
and John W. Tukey. 1992. Scatter/gather: A cluster-
based approach to browsing large document collec-
tions. In Proceedings SIGIR ?92, pages 318?329.
M.B. Fleischman and E. Hovy. 2004. Multi-document
person name resolution. In Proceedings of 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Reference Resolution Workshop.
K.T. Frantzi and S. Ananiadou. 1999. The c-value/nc-
value domain independent method for multi-word
term extraction. Journal of Natural Language Pro-
cessing, 6(3):145?179.
S. Gauch and J. B. Smith. 1991. Search improvement
via automatic query reformulation. ACM Trans. on
Information Systems, 9(3):249?280.
R. Guha and A. Garg. 2004. Disambiguating people in
search. In Stanford University.
Hui Han, Hongyuan Zha, and C. Lee Giles. 2005.
Name disambiguation in author citations using a k-
way spectral clustering method. In Proceedings of
the International Conference on Digital Libraries.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2000. On clusterings: Good, bad, and spectral. In
Proceedings of the 41st Annual Symposium on the
Foundation of Computer Science, pages 367?380.
Xin Li, Paul Morie, and Dan Roth. 2005. Semantic
integration in text, from ambiguous names to identi-
fiable entities. AI Magazine, American Association
for Artificial Intelligence, Spring:45?58.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceed-
ings of CoNLL-2003, pages 33?40.
Y. Matsuo, J. Mori, and M. Hamasaki. 2006. Poly-
phonet: An advanced social network extraction sys-
tem. In to appear in World Wide Web Conference
(WWW).
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In Proceedings of the 42nd Meeting of the As-
sociation for Computational Linguistics (ACL?04),
pages 279?286.
P. Mika. 2004. Bootstrapping the foaf-web: and ex-
periment in social networking network minning. In
Proceedings of 1st Workshop on Friend of a Friend,
Social Networking and the Semantic Web.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering sim-
ilar contexts. In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Mehran Sahami and Tim Heilman. 2005. A web-based
kernel function for matching short text snippets. In
International Workshop located at the 22nd Inter-
national Conference on Machine Learning (ICML
2005).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
24
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 542?550,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Graph-based Word Clustering using a Web Search Engine
Yutaka Matsuo
National Institute of Advanced
Industrial Science and Technology
1-18-13 Sotokanda, Tokyo 101-0021
y.matsuo@aist.go.jp
Takeshi Sakaki
University of Tokyo
7-3-1 Hongo
Tokyo 113-8656
Ko?ki Uchiyama
Hottolink Inc.
2-11-17 Nishi-gotanda
Tokyo 141-0031
uchi@hottolink.co.jp
Mitsuru Ishizuka
University of Tokyo
7-3-1 Hongo
Tokyo 113-8656
ishizuka@i.u-tokyo.ac.jp
Abstract
Word clustering is important for automatic
thesaurus construction, text classification,
and word sense disambiguation. Recently,
several studies have reported using the
web as a corpus. This paper proposes
an unsupervised algorithm for word clus-
tering based on a word similarity mea-
sure by web counts. Each pair of words
is queried to a search engine, which pro-
duces a co-occurrence matrix. By calcu-
lating the similarity of words, a word co-
occurrence graph is obtained. A new kind
of graph clustering algorithm called New-
man clustering is applied for efficiently
identifying word clusters. Evaluations are
made on two sets of word groups derived
from a web directory and WordNet.
1 Introduction
The web is a good source of linguistic informa-
tion for several natural language techniques such
as question answering, language modeling, and
multilingual lexicon acquisition. Numerous stud-
ies have examined the use of the web as a corpus
(Kilgarriff, 2003).
Web-based models perform especially well
against the sparse data problem: Statistical tech-
niques perform poorly when the words are rarely
used. For example, F. Keller et al (2002) use the
web to obtain frequencies for unseen bigrams in
a given corpus. They count for adjective-noun,
noun-noun, and verb-object bigrams by querying
a search engine, and demonstrate that web fre-
quencies (web counts) correlate with frequencies
from a carefully edited corpus such as the British
National Corpus (BNC). Aside from counting bi-
grams, various tasks are attainable using web-
based models: spelling correction, adjective order-
ing, compound noun bracketing, countability de-
tection, and so on (Lapata and Keller, 2004). For
some tasks, simple unsupervised models perform
better when n-gram frequencies are obtained from
the web rather than from a standard large corpus;
the web yields better counts than the BNC.
The web is an excellent source of information
on new words. Therefore, automatic thesaurus
construction (Curran, 2002) offers great potential
for various useful NLP applications. Several stud-
ies have addressed the extraction of hypernyms
and hyponyms from the web (Miura et al, 2004;
Cimiano et al, 2004). P. Turney (2001) presents a
method to recognize synonyms by obtaining word
counts and calculating pointwise mutual informa-
tion (PMI). For further development of automatic
thesaurus construction, word clustering is benefi-
cial, e.g. for obtaining synsets. It also contributes
to word sense disambiguation (Li and Abe, 1998)
and text classification (Dhillon et al, 2002) be-
cause the dimensionality is reduced efficiently.
This paper presents an unsupervised algorithm
for word clustering based on a word similarity
measure by web counts. Given a set of words, the
algorithm clusters the words into groups so that
the similar words are in the same cluster. Each pair
of words is queried to a search engine, which re-
sults in a co-occurrence matrix. By calculating the
similarity of words, a word co-occurrence graph
is created. Then, a new kind of graph clustering
algorithm, called Newman clustering, is applied.
Newman clustering emphasizes betweenness of an
edge and identifies densely connected subgraphs.
To the best of our knowledge, this is the first
attempt to obtain word groups using web counts.
Our contributions are summarized as follows:
542
? A new algorithm for word clustering is de-
scribed. It has few parameters and thus is
easy to implement as a baseline method.
? We evaluate the algorithm on two sets of
word groups derived from a web directory
and WordNet. The chi-square measure and
Newman clustering are both used in our al-
gorithm, they are revealed to outperform PMI
and hierarchical clustering.
We target Japanese words in this paper. The re-
mainder of this paper is organized as follows: We
overview the related studies in the next section.
Our proposed algorithm is described in Section 3.
Sections 4 and 5 explain evaluations and advance
discussion. Finally, we conclude the paper.
2 Related Works
A number of studies have explained the use of
the web for NLP tasks e.g., creating multilingual
translation lexicons (Cheng et al, 2004), text clas-
sification (Huang et al, 2004), and word sense dis-
ambiguation (Turney, 2004). M. Baroni and M.
Ueyama summarize three approaches to use the
web as a corpus (Baroni and Ueyama, 2005): us-
ing web counts as frequency estimates, building
corpora through search engine queries, and crawl-
ing the web for linguistic purposes. Commercial
search engines are optimized for ordinary users.
Therefore, it is desirable to crawl the web and to
develop specific search engines for NLP applica-
tions (Cafarella and Etzioni, 2005). However, con-
sidering that great efforts are taken in commercial
search engines to maintain quality of crawling and
indexing, especially against spammers, it is still
important to pursue the possibility of using the
current search engines for NLP applications.
P. Turney (Turney, 2001) presents an unsu-
pervised learning algorithm for recognizing syn-
onyms by querying a web search engine. The
task of recognizing synonyms is, given a target
word and a set of alternative words, to choose the
word that is most similar in meaning to the tar-
get word. The algorithm uses pointwise mutual
information (PMI-IR) to measure the similarity of
pairs of words. It is evaluated using 80 synonym
test questions from the Test of English as a Foreign
Language (TOEFL) and 50 from the English as a
Second Language test (ESL). The algorithm ob-
tains a score of 74%, contrasted to that of 64% by
Latent Semantic Analysis (LSA). Terra and Clarke
(Terra and Clarke, 2003) provide a comparative in-
vestigation of co-occurrence frequency estimation
on the performance of synonym tests. They report
that PMI (with a certain window size) performs
best on average. Also, PMI-IR is useful for cal-
culating semantic orientation and rating reviews
(Turney, 2002).
As described, PMI is one of many measures to
calculate the strength of word similarity or word
association (Manning and Schu?tze, 2002). An
important assumption is that similarity between
words is a consequence of word co-occurrence, or
that the proximity of words in text is indicative of
relationship between them, such as synonymy or
antonymy. A commonly used technique to obtain
word groups is distributional clustering (Baker and
McCallum, 1998). Distributional clustering of
words was first proposed by Pereira Tishby & Lee
in (Pereira et al, 1993): They cluster nouns ac-
cording to their conditional verb distributions.
Graphic representations for word similarity
have also been advanced by several researchers.
Kageura et al (2000) propose automatic thesaurus
generation based on a graphic representation. By
applying a minimum edge cut, the corresponding
English terms and Japanese terms are identified
as a cluster. Widdows and Dorow (2002) use a
graph model for unsupervised lexical acquisition.
A graph is produced by linking pairs of words
which participate in particular syntactic relation-
ships. An incremental cluster-building algorithm
achieves 82% accuracy at a lexical acquisition
task, evaluated against WordNet classes. Another
study builds a co-occurrence graph of terms and
decomposes it to identify relevant terms by dupli-
cating nodes and edges (Tanaka-Ishii and Iwasaki,
1996). It focuses on transitivity: if transitivity
does not hold between three nodes (e.g., if edge
a-b and b-c exist but edge a-c does not), the nodes
should be in separate clusters.
A network of words (or named entities) on the
web is investigated also in the context of the Se-
mantic Web (Cimiano et al, 2004; Bekkerman and
McCallum, 2005). Especially, a social network of
persons is mined from the web using a search en-
gine (Kautz et al, 1997; Mika, 2005; Matsuo et
al., 2006). In these studies, the Jaccard coefficient
is often used to measure the co-occurrence of enti-
ties. We compare Jaccard coefficients in our eval-
uations.
In the research field on complex networks,
543
Table 1: Web counts for each word.
printer print InterLaser ink TV Aquos Sharp
17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000
Table 2: Co-occurrence matrix by web counts.
printer print InterLaser ink TV Aquos Sharp
printer ? 4780000 179 4720000 4530000 201000 990000
print 4780000 ? 183 4800000 8390000 86400 1390000
InterLaser 179 183 ? 116 65 0 0
ink 4720000 4800000 116 ? 10600000 144000 656000
TV 4530000 8390000 65 10600000 ? 1660000 42300000
Aquos 201000 86400 0 144000 1660000 ? 1790000
Sharp 990000 1390000 0 656000 42300000 1790000 ?
structures of various networks are investigated in
detail. For example, Motter (2002) targeted a
conceptual network from a thesaurus and demon-
strated its small-world structure. Recently, nu-
merous works have identified communities (or
densely-connected subgraphs) from large net-
works (Newman, 2004; Girvan and Newman,
2002; Palla et al, 2005) as explained in the next
section.
3 Word Clustering using Web Counts
3.1 Co-occurrence by a Search Engine
A typical word clustering task is described as fol-
lows: given a set of words (nouns), cluster words
into groups so that the similar words are in the
same cluster 1. Let us take an example. As-
sume a set of words is given: ???? (printer),
?? (print), ???????? (InterLaser), ?
?? (ink), TV (TV), Aquos (Aquos), and Sharp
(Sharp). Apparently, the first four words are re-
lated to a printer, and the last three words are re-
lated to a TV 2. In this case, we would like to have
two word groups: the first four and the last three.
We query a search engine3 to obtain word
counts. Table 1 shows web counts for each word.
Table 2 shows the web counts for pairs of words.
For example, we submit a query printer AND In-
terLaser to a search engine, and are directed to 179
documents. Thereby, nC2 queries are necessary to
obtain the matrix if we have n words. We call Ta-
ble 2 a co-occurrence matrix.
We can calculate the pointwise mutual informa-
1In this paper, we limit our scope to clustering nouns. We
discuss the extension in Section 4.
2InterLaser is a laser printer made by Epson Corp. Aquos
is a liquid crystal TV made by Sharp Corp.
3Google (www.google.co.jp) is used in our study.
tion between word w1 and w2 as
PMI(w1, w2) = log2
p(w1, w2)
p(w1)p(w2)
.
Probability p(w1) is estimated by fw1/N , where
fw1 represents the web count of w1 and N repre-
sents the number of documents on the web. Prob-
ability of co-occurrence p(w1, w2) is estimated by
fw1,w2/N where fw1,w2 represents the web count
of w1 AND w2.
The PMI values are shown in Table 3. We set
N = 1010 according to the number of indexed
pages on Google. Some values are inconsistent
with our intuition: Aquos is inferred to have high
PMI to TV and Sharp, but also to printer. None
of the words has high PMI with TV. These are be-
cause the range of the word count is broad. Gen-
erally, mutual information tends to provide a large
value if either word is much rarer than the other.
Various statistical measures based on co-
occurrence analysis have been proposed for es-
timating term association: the DICE coefficient,
Jaccard coefficient, chi-square test, and the log-
likelihood ratio (Manning and Schu?tze, 2002). In
our algorithm, we use the chi-square (?2) value in-
stead of PMI. The chi-square value is calculated as
follows: We denote the number of pages contain-
ing both w1 and w2 as a. We also denote b, c, d as
follows4.
w2 ?w2
w1 a b
?w1 c d
Thereby, the expected frequency of (w1, w2) is
(a+ c)(a+ b)/N . Eventually, chi-square is calcu-
lated as follows (Manning and Schu?tze, 2002).
4Note that N = a + b + c + d.
544
Table 3: A matrix of pointwise mutual information.
printer print InterLaser ink TV Aquos Sharp
printer ? 4.771 8.936 7.199 0.598 5.616 1.647
print 4.771 ? 6.369 4.624 -1.111 1.799 -0.463
InterLaser 8.936 6.369 ? 8.157 0.781 ??* ??*
ink 7.199 4.624 8.157 ? 1.672 4.983 0.900
TV 0.598 -1.111 0.781 1.672 ? 1.969 0.370
Aquos 5.616 1.799 ??*. 4.983 1.969 ? 5.319
Sharp 1.647 -0.463 ??* 0.900 0.370 5.319 ?
* represents that the PMI is not available because the co-occurrence web count is zero, in which case we set ??.
Table 4: A matrix of chi-square values.
printer print InterLaser ink TV Aquos Sharp
printer ? 6880482.6 399.2 5689710.7 0.0* 0.0* 0.0*
print 6880482.6 ? 277.8 3321184.6 176855.5 0.0* 0.0*
InterLaser 399.2 277.8 ? 44.8 0.0* 0.0 0.0
ink 5689710.7 3321184.6 44.8 ? 1419485.5 0.0* 0.0*
TV 0.0* 176855.5 0.0* 1419485.5 ? 26803.2 70790877.6
Aquos 0.0* 0.0* 0.0 0.0* 26803.2 ? 729357.7
Sharp 0.0* 0.0* 0.0 0.0* 70790877.6 729357.7 ?
* represents that the observed co-occurrence frequency is below the expected value, in which case we set 0.0.
Figure 1: Examples of Newman clustering.
?2(w1, w2)
= N ? (a? d? b? c)
2
(a + b)? (a + c)? (b + d)? (c + d)
However, N is a huge number on the web and
sometimes it is difficult to know exactly. There-
fore we regard the co-occurrence matrix as a con-
tingency table:
b? =
?
w?W ;w 6=w2
fw1,w , c
? =
?
w?W ;w 6=w1
fw2,w;
d? =
?
w,w??W ;w and w? 6=w1 nor w2
fw,w? , N ? =
?
w,w??W
fw,w? ,
where W represents a given set of words. Then
chi-square (within the word list W ) is defined as
?2W (w1, w2) =
N ? ? (a? d? ? b? ? c?)2
(a + b?)? (a + c?)? (b? + d?)? (c? + d?) .
We should note that ?2W depends on a word
set W . It calculates the relative strength of co-
occurrences. Table 4 shows the ?2W values. Aquos
has high values only with TV and Sharp as ex-
pected.
3.2 Clustering on Co-occurrence Graph
Recently, a series of effective graph clustering
methods has been advanced. Pioneering work that
specifically emphasizes edge betweenness was
done by Girvan and Newman (2002): we call the
method as GN algorithm. Betweenness of an edge
is the number of shortest paths between pairs of
nodes that run along it. Figure 1 (i) shows that
two ?communities? (in Girvan?s term), i.e. {a,b,c}
and {d,e,f,g}, which are connected by edge c-d.
Edge c-d has high betweenness because numerous
shortest paths (e.g., from a to d, from b to e, . . .)
traverse the edge. The graph is likely to be sepa-
rated into densely connected subgraphs if we cut
the high betweenness edge.
The GN algorithm is different from the mini-
mum edge cut. For (i), the results are identical: By
cutting edge c-d, which is a minimum edge cut, we
can obtain two clusters. However in case of (ii),
there are two candidates for the minimum edge
cut, whereas the highest betweenness edge is still
only edge c-d. Girvan et al (2002) shows that this
clustering works well to various networks from
biological to social networks. Numerous studies
have been inspired by that work. One prominent
effort is a faster variant of GN algorithm (New-
man, 2004), which we call Newman clustering in
545
Figure 2: An illustration of graph-based word
clustering.
this paper.
In Newman clustering, instead of explicitly cal-
culating high-betweenness edges (which is com-
putationally demanding), an objective function is
defined as follows:
Q =
?
i
(
eii ?
(
?
j
eij
)2)
(1)
We assume that we have separate clusters, and that
eij is the fraction5 of edges in the network that
connect nodes in cluster i to those in cluster j.
The term eii denotes the fraction of edges within
the clusters. The term
?
j eij represents the ex-
pected fraction of edges within the cluster. If a par-
5We can calculate eij using the number of edges between
cluster i and j divided by the number of all edges.
Figure 3: A word graph for 88 Japanese words.
ticular division gives no more within-community
edges than would be expected by random chance,
then we would obtain Q = 0. In practice, values
greater than about 0.3 appear to indicate signifi-
cant group structure (Newman, 2004).
Newman clustering is agglomerative (although
we can intuitively understand that a graph with-
out high betweenness edges is ultimately ob-
tained). We repeatedly join clusters together in
pairs, choosing at each step the joint that provides
the greatest increase in Q. Currently, Newman
clustering is one of the most efficient methods for
graph-based clustering.
The illustration of our algorithm is shown in
Fig. 2. First, we obtain web counts among a given
set of words using a search engine. Then PMI or
the chi-square values are calculated. If the value is
above a certain threshold6, we invent an edge be-
tween the two nodes. Then, we apply graph clus-
tering and finally identify groups of words. This il-
lustration shows that the chi-square measure yields
the correct clusters.
The algorithm is described in Fig. 4. The pa-
rameters are few: a threshold dthre for a graph and,
optionally, the number of clusters nc. This enables
easy implementation of the algorithm. Figure 3
is a small network of 88 Japanese words obtained
through 3828 search queries. We can see that some
parts in the graph are densely connected.
4 Experimental Results
This section addresses evaluation. Two sets of
word groups are used for the evaluation: one is
derived from documents on a web directory; an-
other is from WordNet. We first evaluate the co-
6In this example, 4.0 for PMI and 200 for ?2.
546
? ?
1. Input A set of words is given. The number of words
is denoted as n.
2. Obtain frequencies Put a query for each pair of
words to a search engine, and obtain a co-
occurrence matrix. Then calculate the chi-square
matrix (alternatively a PMI matrix, or a Jaccard
matrix.)
3. Make a graph Set a node for each word, and an
edge to a pair of nodes whose ?2 value is above a
threshold. The threshold is determined so that the
network density (the number of edges divided by
nC2) is dthre.
4. Apply Newman clustering Initially set each node
as a cluster. Then merge two clusters repeatedly
so that Q is maximized. Terminate if Q does
not increase anymore, or when a given number
of clusters nc is obtained. (Alternatively, apply
average-link hierarchical clustering.)
5. Output Output groups of words.
? ?
Figure 4: Our algorithm for word clustering.
occurrence measures, then we evaluate the cluster-
ing methods.
4.1 Word Groups from an Open Directory
We collected documents from the Japanese Open
Directory (dmoz.org/World/Japanese). The
dmoz japanese category contains about 130,000
documents and more than 10,000 classes. We
chose 9 categories out of the top 12 categories:
art, sports, computer, game, society, family, sci-
ence, and health. We crawled 1000 documents for
each category, i.e., 9000 documents in all.
For each category, a word group is obtained
through the procedure in Fig. 5. We consider
that the specific words to a category are relevant
to some extent, and that they can therefore be re-
garded as a word group. Examples are shown in
Table 5. In all, 90 word sets are obtained and
merged. We call the word set DMOZ-J data.
Our task is, given 90 words, to cluster the words
into the correct nine groups. Here we investigate
whether the correct nine words are selected for
each word using the co-occurrence measure. We
compare pointwise mutual information (PMI), the
Jaccard coefficient (Jaccard), and chi-square (?2).
We chose these methods for comparison because
PMI performs best in (Terra and Clarke, 2003).
The Jaccard coefficient is often used in social net-
work mining from the web. Table 7 shows the pre-
cision of each method. Experiments are repeated
five times. We keep each method that outputs the
? ?
1. For each category, crawl 1000 documents ran-
domlya
2. Apply the Japanese morphological analysis sys-
tem ChaSen (Matsumoto et al, 2000) to the doc-
uments. Calculate the score of each word w in
category c similarly to TF-IDF:
score(w, c) = fc(w)? log(Nall/fall(w))
where fc denotes the document frequency of
word w in category c, Nall denotes the number of
all documents, and fall(w) denotes the frequency
of word w in all documents.
3. For each category, the top 10 words are selected
as the word group.
aWe first get al urls, sort them, and select a sample
randomly.
? ?
Figure 5: Procedure for obtaining word groups for
a category.
Table 7: Precision for DMOZ-J set.
PMI Jaccard ?2
Mean 0.415 0.402 0.537
Min 0.396 0.376 0.493
Max 0.447 0.424 0.569
SD 0.020 0.020 0.032
highest nine words for each word, groups of ten
words. Therefore, recall is the same as the preci-
sion. From the table, the chi-square performs best.
PMI is slightly better than the Jaccard coefficient.
4.2 Word Groups from WordNet
Next, we make a comparison using WordNet 7. By
extracting 10 words that have the same hypernym
(i.e. coordinates), we produce a word group. Ex-
amples are shown in Table 6. Nine word groups
are merged into one, as with DMOZ-J. The exper-
iments are repeated 10 times. Table 8 shows the
result. Again, the chi-square performs best among
the methods that were compared.
Detailed analyses of the results revealed that
word groups such as bacteria and diseases are clus-
tered correctly. However, word groups such as
computers (in which homepage, server and client
are included) are not well clustered: these words
tend to be polysemic, which causes difficulty.
4.3 Evaluation of Clustering
We compare two clustering methods: Newman
clustering and average-link agglomerative cluster-
7We use a partly-translated version of WordNet.
547
Table 5: Examples of word groups from DMOZ-J.
category specific words to a category as a word group
??? (art) ?? (gallery),?? (artwork),?? (theater),???? (saxophone),?? (verse),??? (live con-
cert),??? (guitar),?? (performance),??? (ballet),?? (personal exhibition)
????????
(recreation)
?? (raising), ?? (poult), ????? (hamster), ??? (travel diary), ???? (national park),
?? (brewing),?? (boat race),?? (competition),??? (fishing pond)
?? (health) ?? (illness),?? (patient),?? (myositis),?? (surgery),?? (dialysis),????? (steroid),?
? (test),?? (medical ward),??? (collagen disease),?? (clinic)
Table 6: Examples of word groups from WordNet.
hypernym hyponyms as a word group
?? (gem) ????? (amethyst),?????? (aquamarine),?????? (diamond),????? (emer-
ald),??????? (moonstone),????? (peridot),??? (ruby),????? (sapphire),
???? (topaz),????? (tourmaline)
?? (academic field) ???? (natural science),?? (mathematics),?? (agronomics),??? (architectonics),??
? (geology),??? (psychology),???? (computer science),???? (cognitive science),?
?? (sociology),??? (linguistics)
??? (drink) ?? (milk),????? (alcohol),???? (cooling beverage),???? (carbonated beverage),
???? (soda),??? (cocoa),???????? (fruit juice),???? (coffee),?? (tea),?
???????? (mineral water)
Table 8: Precision of WordNet set.
PMI Jaccard ?2
Mean 0.549 0.484 0.584
Min 0.473 0.415 0.498
Max 0.593 0.503 0.656
SD 0.037 0.027 0.048
Table 9: Precision, recall and the F-measure for
each clustering.
PMI Jaccard ?2
Average precision 0.633 0.603 0.486
-link recall 0.102 0.101 0.100
F-measure 0.179 0.173 0.164
Newman precision 0.751 0.739 0.546
recall 0.103 0.103 0.431
F-measure 0.182 0.181 0.480
ing, which is often used in word clustering.
A word co-occurrence graph is created using
PMI, Jaccard, and chi-square measures. The
threshold is determined so that the network den-
sity dthre is 0.3. Then, we apply clustering to ob-
tain nine clusters; nc = 9. Finally, we compare
the resultant clusters with the correct categories.
Clustering results for DMOZ-J sets are shown
in Table 9. Newman clustering produces higher
precision and recall. Especially, the combination
of chi-square and Newman is the best in our ex-
periments.
5 Discussion
In this paper, the scope of co-occurrence is
document-wide. One reason is that major com-
mercial search engines do not support a type of
query w1 NEAR w2. Another reason is in (Terra
and Clarke, 2003) document-wide co-occurrences
perform comparable to other Windows-based co-
occurrences.
Many types of co-occurrence exist other than
noun-noun. We limit our scope to noun-noun
co-occurrences in this paper. Other types of co-
occurrence such as verb-noun can be investigated
in future studies. Also, co-occurrence for the
second-order similarity can be sought. Because
web documents are sometimes difficult to analyze,
we keep our algorithm as simple as possible. An-
alyzing semantic relations and applying distribu-
tional clustering is another goal for future work.
A salient weak point of our algorithm is the
number of necessary queries allowed to a search
engine. For obtaining a graph of n words, O(n2)
queries are required, which discourages us from
undertaking large experiments. However some de-
vices are possible: if we analyze the texts of the
top retrieved pages by query w, we can guess what
words are likely to co-occur with w. This prepro-
cessing seems promising at least in social network
extraction: we can eliminate 85% of queries in
the 500 nodes case while retaining more than 90%
precision (Asada et al, 2005).
In our evaluation, the chi-square measure per-
formed well. One reason is that the PMI performs
worse when a word group contains rare or frequent
words, as is generally known for mutual informa-
tion measure (Manning and Schu?tze, 2002). An-
other reason is that if we put one word and two
words to a search engine, the result might be in-
consistent. In an extreme case, the web count of
w1 is below the web count of w1ANDw2. This
548
phenomenon depends on how a search engine pro-
cesses AND operator, and results in unstable val-
ues for the PMI. On the other hand, our method
by the chi-square uses a co-occurrence matrix as a
contingency table. For that reason, it suffers less
from the problem. Other statistical measures such
as the likelihood ratio are also applicable.
6 Conclusion
This paper describes a new approach for word
clustering using a search engine. The chi-square
measure is used to overcome the broad range of
word counts for a given set of words. We also ap-
ply recently-developed Newman clustering, which
yields promising results through our evaluations.
Our algorithm has few parameters. Therefore,
it can be used easily as a baseline, as suggested by
(Lapata and Keller, 2004). New words are gener-
ated day by day on the web. We believe that to
automatically identify new words and obtain word
groups potentially enhances many NLP applica-
tions.
References
Yohei Asada, Yutaka Matsuo, and Mitsuru Ishizuka.
2005. Increasing scalability of researcher network
extraction from the web. Journal of Japanese Soci-
ety for Artificial Intelligence, 20(6).
D. Baker and A. McCallum. 1998. Distributional
clustering of words for text classification. In Proc.
SIGIR-98.
M. Baroni and M. Ueyama. 2005. Building general-
and special-purpose corpora by web crawling. In
Proc. NIJL International Workshop on Language
Corpora.
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network.
In Proc. WWW 2005.
M. Cafarella and O. Etzioni. 2005. A search engine for
natural language applications. In Proc. WWW2005.
P. Cheng, W. Lu, J. Teng, and L. Chien. 2004. Cre-
ating multilingual translation lexicons with regional
variations using web corpora. In Proc. ACL 2004,
pages 534?541.
P. Cimiano, S. Handschuh, and S. Staab. 2004. To-
wards the self-annotating web. In Proc. WWW2004,
pages 462?471.
J. Curran. 2002. Ensemble methods for automatic the-
saurus extraction. In Proc. EMNLP 2002.
I. Dhillon, S. Mallela, and R. Kumar. 2002. Enhanced
word clustering for hierarchical text classification.
In Proc. KDD-2002, pages 191?200.
Michelle Girvan and M. E. J. Newman. 2002. Com-
munity structure in social and biological networks.
Proceedings of National Academy of Sciences USA,
99:8271?8276.
C. Huang, S. Chuang, and L. Chien. 2004. Categoriz-
ing unknown text segments for information extrac-
tion using a search result mining approach. In Proc.
IJCNLP 2004, pages 576?586.
K. Kageura, K. Tsuji, and A. Aizawa. 2000. Auto-
matic thesaurus generation through multiple filter-
ing. In Proc. COLING 2000.
H. Kautz, B. Selman, and M. Shah. 1997. The hidden
Web. AI magazine, 18(2):27?35.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using
the web to overcome data sparseness. In EMNLP-
02, pages 230?237.
A. Kilgarriff. 2003. Introduction to the special issue
on the web as corpus. Computer Linguistics, 29(3).
M. Lapata and F. Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised
web-based models for a range of nlp tasks. In Proc.
HLT-NAACL 2004, pages 121?128.
H. Li and N. Abe. 1998. Word clustering and dis-
ambiguation based on co-occurrence data. In Proc.
COLING-ACL98.
C. D. Manning and H. Schu?tze. 2002. Foundations
of statistical natural language processing. The MIT
Press, London.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hi-
rano, H. Matsuda, K. Takaoka, and M. Asahara.
2000. Morphological analysis system ChaSen ver-
sion 2.2.1 manual. Technical report, NIST.
Y. Matsuo, J. Mori, M. Hamasaki, H. Takeda,
T. Nishimura, K. Hasida, and M. Ishizuka. 2006.
POLYPHONET: An advanced social network ex-
traction system. In Proc. WWW 2006.
P. Mika. 2005. Flink: Semantic web technology for the
extraction and analysis of social networks. Journal
of Web Semantics, 3(2).
K. Miura, Y. Tsuruoka, and J. Tsujii. 2004. Auto-
matic acquisition of concept relations from web doc-
uments with sense clustering. In Proc. IJCNLP04.
A. Motter, A. de Moura, Y. Lai, and P. Dasgupta. 2002.
Topology of the conceptual network of language.
Physical Review E, 65.
M. Newman. 2004. Fast algorithm for detecting com-
munity structure in networks. Phys. Rev. E, 69.
549
G. Palla, I. Derenyi, I. Farkas, and T. Vicsek. 2005.
Uncovering the overlapping community structure of
complex networks in nature and society. Nature,
435:814.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proc. ACL93, pages
183?190.
K. Tanaka-Ishii and H. Iwasaki. 1996. Clustering co-
occurrence graph using transitivity. In Proc. 16th In-
ternational Conference on Computational Linguis-
tics, pages 680?585.
E. Terra and C. Clarke. 2003. Frequency estimates
for statistical word similarity measures. In Proc.
HLT/NAACL 2003.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proc. ECML-2001,
pages 491?502.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL?02, pages 417?424.
P. Turney. 2004. Word sense disambiguation by web
mining for word co-occurrence probabilities. In
Proc. SENSEVAL-3.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING
2002.
550
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 806?814,
Beijing, August 2010
Recognition of Affect, Judgment, and Appreciation in Text 
Alena Neviarouskaya 
University of Tokyo 
lena@mi.ci.i.u-
tokyo.ac.jp 
Helmut Prendinger 
Nat. Institute of Informatics 
Tokyo 
helmut@nii.ac.jp 
Mitsuru Ishizuka 
University of Tokyo 
ishizuka@i.u-
tokyo.ac.jp 
 
Abstract 
The main task we address in our research 
is classification of text using fine-grained 
attitude labels. The developed @AM sys-
tem relies on the compositionality prin-
ciple and a novel approach based on the 
rules elaborated for semantically distinct 
verb classes. The evaluation of our me-
thod on 1000 sentences, that describe 
personal experiences, showed promising 
results: average accuracy on the fine-
grained level (14 labels) was 62%, on the 
middle level (7 labels) ? 71%, and on the 
top level (3 labels) ? 88%. 
1 Introduction and Related Work 
With rapidly growing online sources aimed at 
encouraging and stimulating people?s discussions 
concerning personal, public or social issues 
(news, blogs, discussion forums, etc.), there is a 
great need in development of a computational 
tool for the analysis of people?s attitudes. Ac-
cording to the Appraisal Theory (Martin and 
White, 2005), attitude types define the specifics 
of appraisal being expressed: affect (personal 
emotional state), judgment (social or ethical ap-
praisal of other?s behaviour), and appreciation 
(evaluation of phenomena). 
To analyse contextual sentiment of a phrase or 
a sentence, rule-based approaches (Nasukawa 
and Yi, 2003; Moilanen and Pulman, 2007; Sub-
rahmanian and Reforgiato, 2008), a machine-
learning method using not only lexical but also 
syntactic features (Wilson et al, 2005), and a 
model of integration of machine learning ap-
proach with compositional semantics (Choi and 
Cardie, 2008) were proposed. With the aim to 
recognize fine-grained emotions from text on the 
level of distinct sentences, researchers have em-
ployed a keyword spotting technique (Chuang 
and Wu, 2004; Strapparava et al, 2007), a tech-
nique calculating emotion scores using Pointwise 
Mutual Information (PMI) (Kozareva et al, 
2007), an approach inspired by common-sense 
knowledge (Liu et al, 2003), rule-based linguis-
tic approaches (Boucouvalas, 2003; Chaumartin, 
2007), machine-learning methods (Alm, 2008; 
Aman and Szpakowicz, 2008; Strapparava and 
Mihalcea, 2008), and an ensemble based multi-
label classification technique (Bhowmick et al, 
2009). 
Early attempts to focus on distinct attitude 
types in the task of attitude analysis were made 
by Taboada and Grieve (2004), who determined 
a potential value of adjectives for affect, judge-
ment and appreciation by calculating the PMI 
with the pronoun-copular pairs ?I was (affect)?, 
?He was (judgement)?, and ?It was (apprecia-
tion)?, and Whitelaw et al (2005), who used a 
machine learning technique (SVM) with fine-
grained semantic distinctions in features (attitude 
type, orientation) in combination with ?bag of 
words? to classify movie reviews. However, the 
concentration only on adjectives expressing ap-
praisal and their modifiers greatly narrows the 
potential of the Whitelaw et al (2005) approach. 
In this paper we introduce our system @AM 
(ATtitude Analysis Model), which (1) classifies 
sentences according to the fine-grained attitude 
labels (nine affect categories (Izard, 1971): ?an-
ger?, ?disgust?, ?fear?, ?guilt?, ?interest?, ?joy?, 
?sadness?, ?shame?, ?surprise?; four polarity la-
bels for judgment and appreciation: ?POS jud?, 
?NEG jud?, ?POS app?, ?NEG app?; and ?neu-
tral?); (2) assigns the strength of the attitude; and 
(3) determines the level of confidence, with 
which the attitude is expressed. @AM relies on a 
compositionality principle and a novel approach 
806
based on the rules elaborated for semantically 
distinct verb classes. 
2 Lexicon for Attitide Analysis 
We built a lexicon for attitude analysis that in-
cludes: (1) attitude-conveying terms; (2) modifi-
ers; (3) ?functional? words; and (4) modal opera-
tors. 
2.1 The Core of Lexicon 
As a core of lexicon for attitude analysis, we em-
ploy an Affect database and extended version of 
the SentiFul database developed by Neviar-
ouskaya et al (2009). The affective features of 
each emotion-related word are encoded using 
nine emotion labels (?anger?, ?disgust?, ?fear?, 
?guilt?, ?interest?, ?joy?, ?sadness?, ?shame?, and 
?surprise?) and corresponding emotion intensities 
that range from 0.0 to 1.0. The original version 
of SentiFul database, which contains sentiment-
conveying adjectives, adverbs, nouns, and verbs 
annotated by sentiment polarity, polarity scores 
and weights, was manually extended using atti-
tude labels. Some examples of annotated atti-
tude-conveying words are listed in Table 1. It is 
important to note here that some words may ex-
press different attitude types (affect, judgment, 
appreciation) depending on context; such lexical 
entries were annotated by all possible categories. 
POS Word Category Intensity 
adjective honorable 
unfriendly 
POS jud 
NEG aff (sadness) 
NEG jud 
NEG app 
0.3 
0.5 
0.5 
0.5 
adverb gleefully POS aff (joy) 0.9 
noun abnormality NEG app 0.25 
verb frighten 
desire 
NEG aff (fear) 
POS aff (interest) 
POS aff (joy) 
0.8 
1.0 
0.5 
Table 1. Examples of attitude-conveying words 
and their annotations. 
2.2 Modifiers and Functional Words 
We collected 138 modifiers that have an impact 
on contextual attitude features of related words, 
phrases, or clauses. They include: 
1. Adverbs of degree (e.g., ?significantly?, 
?slightly? etc.) and affirmation (e.g., ?absolutely?, 
?seemingly?) that have an influence on the 
strength of the attitude of related words. Two 
annotators gave coefficients for intensity degree 
strengthening or weakening (from 0.0 to 2.0) to 
each adverb, and the result was averaged (e.g., 
coeff(?slightly?) = 0.2). 
2. Negation words (e.g., ?never?, ?nothing? 
etc.) reversing the polarity of related statement. 
3. Adverbs of doubt (e.g., ?scarcely?, 
?hardly? etc.) and falseness (e.g., ?wrongly? etc.) 
reversing the polarity of related statement. 
4. Prepositions (e.g., ?without?, ?despite? etc.) 
neutralizing the attitude of related words. 
5. Condition operators (e.g., ?if?, ?even 
though? etc.) that neutralize the attitude of related 
words. 
We distinguish two types of ?functional? words 
that influence contextual attitude and its strength:  
1. Intensifying adjectives (e.g., ?rising?, ?rap-
idly-growing?), nouns (e.g., ?increase?), and 
verbs (e.g., ?to grow?, ?to rocket?) that increase 
the strength of attitude of related words. 
2. Reversing adjectives (e.g., ?reduced?), 
nouns (e.g., ?termination), and verbs (e.g., ?to 
decrease?, ?to limit?, ?to diminish?), which re-
verse the prior polarity of related words. 
2.3 Modal Operators 
Consideration of the modal operators in the tasks 
of opinion mining and attitude analysis is very 
important, as they indicate a degree of person?s 
belief in the truth of the proposition, which is 
subjective in nature (Hoye, 1997). Modals are 
distinguished by their confidence level. We col-
lected modal operators of two categories: modal 
verbs (13 verbs) and modal adverbs (61 adverbs). 
Three human annotators assigned the confidence 
level ranging from 0.0 to 1.0 to each modal verb 
and adverb; these ratings were averaged (e.g., 
conf(?vaguely?) = 0.17, conf(?arguably?) = 0.63, 
conf(?would?) = 0.8, conf(?veritably?) = 1.0). 
3 Compositionality Principle 
Our algorithm for attitude classification is de-
signed based on the compositionality principle, 
according to which we determine the attitudinal 
meaning of a sentence by composing the pieces 
that correspond to lexical units or other linguistic 
constituent types governed by the rules of polari-
ty reversal, aggregation (fusion), propagation, 
domination, neutralization, and intensification, at 
various grammatical levels. 
Polarity reversal means that a phrase or 
statement containing an attitude-conveying 
807
term/phrase with prior positive polarity becomes 
negative, and vice versa. The rule of polarity re-
versal is applied in three cases: (1) negation 
word-modifier in relation with an attitude-
conveying statement (e.g., ?never? & 
POS(?succeed?) => NEG(?never succeed?)); (2) 
adverb of doubt in relation with attitude-
conveying statement (e.g., ?scarcely? & 
POS(?relax?) => NEG(?scarcely relax?)); (3) 
functional word of reversing type in relation with 
attitude-conveying statement (e.g., adjective ?re-
duced? & POS(?enthusiasm?) => NEG(?reduced 
enthusiasm?)). In the case of judgment and ap-
preciation, the use of the polarity reversal rule is 
straightforward (?POS jud? <=> ?NEG jud?, 
?POS app? <=> ?NEG app?). However, it is not 
trivial to find pairs of opposite emotions in the 
case of a fine-grained classification, except for 
?joy? and ?sadness?. Therefore, we assume that 
(1) the opposite emotion for three positive emo-
tions, i.e. ?interest?, ?joy?, and ?surprise?, is ?sad-
ness? (?POS aff? => ?sadness?); and (2) the oppo-
site emotion for six negative emotions, i.e. ?an-
ger?, ?disgust?, ?fear?, ?guilt?, ?sadness?, and 
?shame?, is ?joy? (?NEG aff? => ?joy?). 
The rules of aggregation (fusion) are as fol-
lows: (1) if polarities of attitude-conveying terms 
in adjective-noun, noun-noun, adverb-adjective, 
adverb-verb phrases have opposite directions, 
mixed polarity with dominant polarity of a pre-
modifier is assigned to the phrase (e.g., 
POS(?beautiful?) & NEG(?fight?) => POS-
neg(?beautiful fight?); NEG(?shamelessly?) & 
POS(?celebrate?) => NEG-pos(?shamelessly 
celebrate?)); otherwise (2) the resulting polarity 
is based on the equal polarities of terms, and the 
strength of attitude is measured as a maximum 
between polarity scores (intensities) of terms 
(max(score1,score2)).  
The rule of propagation is useful, as proposed 
in (Nasukawa and Yi, 2003), for the task of the 
detection of local sentiments for given subjects. 
?Propagation? verbs propagate the sentiment to-
wards the arguments; ?transfer? verbs transmit 
sentiments among the arguments. The rule of 
propagation is applied when a verb of ?propaga-
tion? or ?transfer? type is used in a phrase/clause 
and sentiment of an argument that has prior neu-
tral polarity needs to be investigated (e.g., 
PROP-POS(?to admire?) & ?his behaviour? => 
POS(?his behaviour?); ?Mr. X? & 
TRANS(?supports?) & NEG(?crime business?) 
=> NEG(?Mr. X?)).  
The rules of domination are as follows: (1) if 
polarities of a verb (this rule is applied only for 
certain classes of verbs) and an object in a clause 
have opposite directions, the polarity of verb is 
prevailing (e.g., NEG(?to deceive?) & 
POS(?hopes?) => NEG(?to deceive hopes?)); (2) 
if compound sentence joints clauses using coor-
dinate connector ?but?, the attitude features of a 
clause following after the connector are domi-
nant (e.g., ?NEG(It was hard to climb a mountain 
all night long), but POS(a magnificent view re-
warded the traveler at the morning).? => 
POS(whole sentence)). 
The rule of neutralization is applied when 
preposition-modifier or condition operator relate 
to the attitude-conveying statement (e.g., ?de-
spite? & NEG(?worries?) => NEUT(?despite 
worries?)). 
The rule of intensification means strengthen-
ing or weakening of the polarity score (intensity), 
and is applied when: 
1. adverb of degree or affirmation relates to 
attitude-conveying term (e.g., 
Pos_score(?happy?) < Pos_score(?extremely hap-
py?)); 
2. adjective or adverb is used in a compara-
tive or superlative form (e.g., Neg_score(?sad?) < 
Neg_score(?sadder?) < Neg_score (?saddest?)). 
Our method is capable of processing sentences of 
different complexity, including simple, com-
pound, complex (with complement and relative 
clauses), and complex-compound sentences. We 
employ Connexor Machinese Syntax parser 
(http://www.connexor.eu/) that returns 
lemmas, parts of speech, dependency functions, 
syntactic function tags, and morphological tags. 
When handling the parser output, we represent 
the sentence as a set of primitive clauses. Each 
clause might include Subject formation, Verb 
formation and Object formation, each of which 
may consist of a main element (subject, verb, or 
object) and its attributives and complements. For 
the processing of complex or compound sen-
tences, we build a so-called ?relation matrix?, 
which contains information about dependences 
(e.g., coordination, subordination, condition, 
contingency, etc.) between different clauses in a 
sentence. While applying the compositionality 
principle, we consecutively assign attitude fea-
808
tures to words, phrases, formations, clauses, and 
finally, to the whole sentence. 
4 Consideration of the Semantics of 
Verbs 
All sentences must include a verb, because the 
verb tells us what action the subject is perform-
ing and object is receiving. In order to elaborate 
rules for attitude analysis based on the semantics 
of verbs, we investigated VerbNet (Kipper et al, 
2007), the largest on-line verb lexicon that is or-
ganized into verb classes characterized by syn-
tactic and semantic coherence among members 
of a class. Based on the thorough analysis of 270 
first-level classes of VerbNet and their members, 
73 verb classes (1) were found useful for the task 
of attitude analysis, and (2) were further classi-
fied into 22 classes differentiated by the role that 
members play in attitude analysis and by rules 
applied to them. Our classification is shown in 
Table 2. 
For each of our verb classes, we developed set 
of rules that are applied to attitude analysis on 
the phrase/clause-level. Some verb classes (e.g., 
?Psychological state or emotional reaction?, 
?Judgment?, ?Bodily state and damage to the 
body?, ?Preservation? etc.) include verbs anno-
tated by attitude type, prior polarity orientation, 
and the strength of attitude. The attitude features 
of phrases that involve positively or negatively 
charged verbs from such classes are context-
sensitive and are defined by means of rules de-
signed for each of the class. 
As an example, we provide short description 
and rules elaborated for the subclass ?Object-
centered (oriented) emotional state?. 
Features: subject experiences emotions towards 
some stimulus; verb prior polarity: positive or 
negative; context-sensitive. 
Verb-Object rules (subject is ignored): 
1. ?Interior perspective? (subject?s inner emotion 
state or attitude): 
S & V+(?admires?) & O+(?his brave heart?) 
=> (fusion, max(V_score,O_score)) => ?POS 
aff?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence dominance, V_score) => ?POS 
aff?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, V_score) => ?NEG 
aff?. 
Verb class (verb samples) 
1 Psychological state or emotional reaction 
1.1 Object-centered (oriented) emotional state (adore)
1.2 Subject-driven change in emotional state (trans.)
(charm, inspire, bother) 
1.3 Subject-driven change in emotional state (intrans.)
(appeal to, grate on) 
2 Judgment 
2.1 Positive judgment (bless, honor) 
2.2 Negative judgment (blame, punish) 
3 Favorable attitude (accept, allow, tolerate) 
4 Adverse (unfavorable) attitude (discourage, forbid) 
5 Favorable or adverse calibratable changes of state 
(grow, decline) 
6 Verbs of removing 
6.1 Verbs of removing with neutral charge (delete) 
6.2 Verbs of removing with negative charge (expel) 
6.3 Verbs of removing with positive charge (evacuate)
7 Negatively charged change of state (break, crush) 
8 Bodily state and damage to the body (sicken, injure) 
9 Aspectual verbs 
9.1 Initiation, continuation of activity, and sustaining 
(begin, continue, maintain) 
9.2 Termination of activity (quit, finish) 
10 Preservation (defend, insure) 
11 Verbs of destruction and killing (damage, poison) 
12 Disappearance (disappear, die) 
13 Limitation and subjugation (confine, restrict) 
14 Assistance (succor, help) 
15 Obtaining (win, earn) 
16 Communication indicator/reinforcement of attitude 
(guess, complain, deny) 
17 Verbs of leaving (abandon, desert) 
18 Changes in social status or condition (canonize) 
19 Success and failure 
19.1 Success (succeed, manage) 
19.2 Failure (fail, flub) 
20 Emotional nonverbal expression (smile, weep) 
21 Social interaction (marry, divorce) 
22 Transmitting verbs (supply, provide) 
Table 2. Verb classes for attitude analysis. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (fusion, max(V_score,O_score)) => ?NEG 
aff?. 
2. ?Exterior perspective? (social/ethical judg-
ment): 
S & V+(?admires?) & O+(?his brave heart?) 
=> (fusion, max(V_score,O_score)) => ?POS 
jud?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence reversal, max(V_score,O_score)) 
=> ?NEG jud?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, 
max(V_score,O_score)) => ?NEG jud?. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (verb valence reversal, 
max(V_score,O_score)) => ?POS jud?. 
809
3. In case of neutral object => attitude type and 
prior polarity of verb, verb score (V_score). 
Verb-PP (prepositional phrase) rules: 
1. In case of negatively charged verb and PP 
starting with ?from? => verb dominance:  
S & V-(?suffers?) & PP-(?from illness?) => in-
terior: ?NEG aff?; exterior: ?NEG jud?. 
S & V-(?suffers?) & PP+ (?from love?) => inte-
rior: ?NEG aff?; exterior: ?NEG jud?. 
2. In case of positively charged verb and PP 
starting with ?in?/?for? => treat PP the same way 
as object (see above): 
S & V+(?believes?) & PP-(?in evil?) => inte-
rior: ?POS aff?; exterior: ?NEG jud?. 
S & V+(?believes?) & PP+(?in kindness?) => 
interior: ?POS aff?; exterior: ?POS jud?. 
In the majority of rules the strength of attitude is 
measured as a maximum between attitude scores 
(for example, the attitude conveyed by ?to suffer 
from grave illness? is stronger than that of ?to 
suffer from slight illness?). 
In contrast to the rules of ?Object-centered 
(oriented) emotional state? subclass, which ig-
nore attitude features of a subject in a sentence, 
the rules elaborated for the ?Subject-driven 
change in emotional state (trans.)? disregard the 
attitude features of object, as in sentences involv-
ing members of this subclass object experiences 
emotion, and subject causes the emotional state. 
For example (due to limitation of space, here and 
below we provide only some cases): 
S(?Classical music?) & V+(?calmed?) & O-
(?disobedient child?) => interior: ?POS aff?; exte-
rior: ?POS app?. 
S-(?Fatal consequences of GM food intake?) & 
V-(?frighten?) & O(?me?) => interior: ?NEG aff?; 
exterior: ?NEG app?. 
The Verb-Object rules for the ?Judgment? sub-
classes, namely ?Positive judgment? and ?Nega-
tive judgment?, are very close to those defined 
for the subclass ?Object-centered (oriented) 
emotional state?. However, Verb-PP rules have 
some specifics: for both positive and negative 
judgment verbs, we treat PP starting with 
?for?/?of?/?as? the same way as object in Verb-
Object rules. For example: 
S(?He?) & V-(?blamed?) & O+(?innocent per-
son?) => interior: ?NEG jud?; exterior: ?NEG 
jud?. 
S(?They?) & V-(?punished?) & O(?him?) & PP-
(?for his misdeed?) => interior: ?NEG jud?; exte-
rior: ?POS jud?. 
Verbs from classes ?Favorable attitude? and 
?Adverse (unfavorable) attitude? have prior neu-
tral polarity and positive or negative reinforce-
ment, correspondingly, that means that they only 
impact on the polarity and strength of non-
neutral phrase (object in a sentence written in 
active voice, or subject in a sentence written in 
passive voice, or PP in case of some verbs). The 
rules are: 
1. If verb belongs to the ?Favorable attitude? 
class and the polarity of phrase is not neutral, 
then the attitude score of the phrase is intensified 
(symbol ?^? means intensification): 
S(?They?) & [V pos. reinforcement](?elected?) 
& O+(?fair judge?) => ?POS app?; O_score^. 
S(?They?) & [V pos. reinforcement](?elected?) 
& O-(?corrupt candidate?) => ?NEG app?; 
O_score^. 
2. If verb belongs to the ?Adverse (unfavorable) 
attitude? class and the polarity of phrase is not 
neutral, then the polarity of phrase is reversed 
and score is intensified: 
S(?They?) & [V neg. reinforce-
ment](?prevented?) & O-(?the spread of disease?) 
=> ?POS app?; O_score^. 
S+(?His achievements?) & [V neg. reinforce-
ment](?were overstated?) => ?NEG app?; 
S_score^. 
Below are examples of processing the sentences 
with verbs from ?Verbs of removing? class. 
?Verbs of removing with neutral charge?: 
S(?The tape-recorder?) & [V neutral 
rem.](?automatically ejects?) & O-neutral(?the 
tape?) => neutral. 
S(?The safety invention?) & [V neutral 
rem.](?ejected?) & O(?the pilot?) & PP-(?from 
burning plane?) => ?POS app?; PP_score^. 
?Verbs of removing with negative charge?: 
S(?Manager?) & [V neg. rem.](?fired?) & O-
(?careless employee?) & PP(?from the company?) 
=> ?POS app?; max(V_score,O_score).  
?Verbs of removing with positive charge?: 
S(?They?) & [V pos. rem.](?evacuated?) & 
O(?children?) & PP-(?from dangerous place?) => 
?POS app?; max(V_score,PP_score). 
Along with modal verbs and modal adverbs, 
members of the ?Communication indica-
tor/reinforcement of attitude? verb class also in-
810
dicate the confidence level or degree of certainty 
concerning given opinion. Features are: subject 
(communicator) expresses statement 
with/without attitude; statement is PP starting 
with ?of?, ?on?, ?against?, ?about?, ?concerning?, 
?regarding?, ?that?, ?how? etc.; ground: positive 
or negative; reinforcement: positive or negative. 
The rules are: 
1. If the polarity of expressed statement is neu-
tral, then the attitude is neutral: 
S(?Professor?) & [V pos. ground, pos. rein-
forcement, confidence:0.83](?dwelled?) & PP-
neutral(?on a question?) => neutral. 
2. If the polarity of expressed statement is not 
neutral and the reinforcement is positive, then the 
score of the statement (PP) is intensified: 
S(?Jane?) & [V neg. ground, pos. reinforce-
ment, confidence:0.8](?is complaining?) & PP-
(?of a headache again?) => ?NEG app?; 
PP_score^; confidence:0.8. 
3. If the polarity of expressed statement is not 
neutral and reinforcement is negative, then the 
polarity of the statement (PP) is reversed and 
score is intensified: 
S(?Max?) & [V neg. ground, neg. reinforce-
ment, confidence:0.2](?doubt?) & PP-{?that? 
S+(?his good fortune?) & [V termination](?will 
ever end?)} => ?POS app?; PP_score^; confi-
dence:0.2.  
In the last example, to measure the sentiment of 
PP, we apply rule for the verb ?end? from the 
?Termination of activity? class, which reverses 
the non-neutral polarity of subject (in intransitive 
use of verb) or object (in transitive use of verb). 
For example, the polarity of both sentences ?My 
whole enthusiasm and excitement disappear like 
a bubble touching a hot needle? and ?They dis-
continued helping children? is negative. 
5 Decision on Attitude Label 
The decision on the most appropriate final label 
for the clause, in case @AM annotates it using 
different attitude types according to the words 
with multiple annotations (e.g., see word ?un-
friendly? in Table 1) or based on the availability 
of the words conveying different attitude types, 
is made based on the analysis of: 
1) morphological tags of nominal heads and 
their premodifiers in the clause (e.g., first person 
pronoun, third person pronoun, demonstrative 
pronoun, nominative or genitive noun, etc.); 
2) the sequence of hypernymic semantic re-
lations of a particular noun in WordNet (Miller, 
1990), which allows to determine its conceptual 
domain (e.g., ?person, human being?, ?artifact?, 
?event?, etc.);  
3) the annotations from the Stanford 
Named Entity Recognizer (Finkel et al 2005) 
that labels PERSON, ORGANIZATION, and 
LOCATION entities.  
For ex., ?I feel highly unfriendly attitude towards 
me? conveys emotion (?NEG aff?: ?sadness?), 
while ?The shop assistant?s behavior was really 
unfriendly? and ?Plastic bags are environment 
unfriendly? express judgment (?NEG jud?) and 
appreciation (?NEG app?), correspondingly. 
6 Evaluation 
For the experiments, we used our own data set, 
as, to the best of our knowledge, there is no pub-
licly available data set of sentences annotated by 
the fine-grained labels proposed in our work. In 
order to evaluate the performance of our algo-
rithm, we created the data set of sentences ex-
tracted from personal stories about life expe-
riences that were anonymously published on the 
Experience Project website 
(www.experienceproject.com), where 
people share personal experiences, thoughts, 
opinions, feelings, passions, and confessions 
through the network of personal stories. With 
over 4 million experiences accumulated (as of 
February 2010), Experience Project is a perfect 
source for researchers interested in studying dif-
ferent types of attitude expressed through text. 
6.1 Data Set Description 
For our experiment we extracted 1000 sentences1 
from various stories grouped by topics within 13 
different categories, such as ?Arts and entertain-
ment?, ?Current events?, ?Education?, ?Family 
and friends?, ?Health and wellness?, ?Relation-
ships and romance? and others, on the Expe-
rience Project website. Sentences were collected 
from 358 distinct topic groups, such as ?I still 
remember September 11?, ?I am intelligent but 
airheaded?, ?I think bullfighting is cruel?, ?I quit 
smoking?, ?I am a fashion victim?, ?I was 
adopted? and others. 
                                                 
1 This annotated data set is freely available upon request. 
811
We considered three hierarchical levels of atti-
tude labels in our experiment (see Figure 1). 
Three independent annotators labeled the sen-
tences with one of 14 categories from the ALL 
level and a corresponding score (the strength or 
intensity value). These annotations were further 
interpreted using labels from the MID and the 
TOP levels. Fleiss? Kappa coefficient was used 
as a measure of reliability of human raters? anno-
tations. The agreement coefficient on 1000 sen-
tences was 0.53 on ALL level, 0.57 on MID level, 
and 0.73 on TOP level. 
Only those sentences, on which at least two 
out of three human raters completely agreed, 
were included in the gold standards for our expe-
riment. Three gold standards were created ac-
cording to the hierarchy of attitude labels. Fleiss? 
Kappa coefficients are 0.62, 0.63, and 0.74 on 
ALL, MID, and TOP levels, correspondingly. 
Table 3 shows the distributions of labels in the 
gold standards. 
ALL level MID level 
Label Number Label Number 
anger 45 POS aff 233 
disgust 21 NEG aff 332 
fear 54 POS jud 66 
guilt 22 NEG jud 78 
interest 84 POS app 100 
joy 95 NEG app 29 
sadness 133 neutral 87 
shame 18 total 925 
surprise 36  
POS jud 66 TOP level 
NEG jud 78 Label Number 
POS app 100 POS 437 
NEG app 29 NEG 473 
neutral 87 neutral 87 
total 868 total 997 
Table 3. Label distributions in gold standards. 
6.2 Results 
The results of a simple method selecting the atti-
tude label with the maximum intensity from the 
annotations of sentence tokens found in the data-
base were considered as the baseline. After 
processing each sentence from the data set by the 
baseline method and our @AM system, we 
measured averaged accuracy, precision, recall, 
and F-score for each label in ALL, MID, and 
TOP levels. The results are shown in Table 4. 
As seen from the obtained results, our algo-
rithm performed with high accuracy significantly 
surpassing the baselines in all levels of attitude 
hierarchy, thus demonstrating the contribution of 
the sentence parsing and our hand-crafted rules 
to the reliable recognition of attitude from text. 
Two-tailed t-tests with significance level of 0.05 
showed that the differences in accuracy between 
the baseline method and our @AM system are 
statistically significant (p<0.001) in fine-grained 
as well as coarse-grained classifications. 
In the case of fine-grained attitude recognition 
(ALL level), the highest precision was obtained 
for ?shame? (0.923) and ?NEG jud? (0.889), 
while the highest recall was received for ?sad-
ness? (0.917) and ?joy? (0.905) emotions at the 
cost of low precision (0.528 and 0.439, corre-
spondingly). The algorithm performed with the 
worst results in recognition of ?NEG app? and 
?neutral?. 
The analysis of a confusion matrix for the 
ALL level revealed the following top confusions 
of our system: (1) ?anger?, ?fear?, ?guilt?, ?shame?, 
?NEG jud?, ?NEG app? and ?neutral? were pre-
dominantly incorrectly predicted as ?sadness? 
(for ex., @AM resulted in ?sadness? for the sen-
tence ?I know we have several months left before 
the election, but I am already sick and tired of 
seeing the ads on TV?, while human annotations 
were ?anger?/?anger?/?disgust?); (2) ?interest?, 
?POS jud? and ?POS app? were mostly confused 
with ?joy? by our algorithm (e.g., @AM classi-
fied the sentence ?It?s one of those life changing 
artifacts that we must have in order to have hap-
pier, healthier lives? as ?joy?(-ful), while human 
annotations were ?POS app?/?POS 
app?/?interest?). 
Our system achieved high precision for all 
categories on the MID level (Table 4), with the 
exception of ?NEG app? and ?neutral?, although 
    
TOP POS NEG neutral
    
MID POS aff POS jud 
POS 
app NEG aff 
NEG 
jud 
NEG 
app neutral
        
ALL interest joy surprise POS jud 
POS 
app anger disgust fear guilt sadness shame
NEG 
jud 
NEG 
app neutral
Figure 1. Hierarchy of attitude labels. 
812
high recall was obtained only in the case of cate-
gories related to affect (?POS aff?, ?NEG aff?). 
These results indicate that affect sensing is easier 
than recognition of judgment or appreciation 
from text. TOP level results (Table 4) show that 
our algorithm classifies sentences that convey 
positive or negative sentiment with high accura-
cy (92% and 91%, correspondingly). On the oth-
er hand, ?neutral? sentences still pose a challenge. 
The analysis of errors revealed that system re-
quires common sense or additional context to 
deal with sentences like ?All through my life I?ve 
felt like I?m second fiddle? (gold standard: ?sad-
ness?; @AM: ?neutral?) or ?For me every minute 
on my horse is alike an hour in heaven!? (gold 
standard: ?joy?; @AM: ?neutral?).  
We also evaluated the system performance 
with regard to attitude intensity estimation. The 
percentage of attitude-conveying sentences (not 
considering neutral ones), on which the result of 
our system conformed to the fine-grained gold 
standard (ALL level), according to the measured 
distance between intensities given by human ra-
ters (averaged values) and those obtained by our 
system is shown in Table 5. As seen from the 
table, our system achieved satisfactory results in 
estimation of the strength of attitude expressed 
through text. 
 
Range of intensity 
difference 
Percent of sen-
tences, % 
[0.0 ? 0.2] 55.5 
(0.2 ? 0.4] 29.5 
(0.4 ? 0.6] 12.2 
(0.6 ? 0.8] 2.6 
(0.8 ? 1.0] 0.2 
Table 5. Results on intensity. 
7 Conclusions 
In this paper we introduced @AM, which is so 
far, to the best of our knowledge, the only system 
classifying sentences using fine-grained attitude 
types, and extensively dealing with the semantics 
of verbs in attitude analysis. Our composition 
approach broadens the coverage of sentences 
with complex contextual attitude. The evaluation 
results indicate that @AM achieved reliable re-
sults in the task of textual attitude analysis. The 
limitations include dependency on lexicon and 
on accuracy of the parser. The primary objective 
for the future research is to develop a method for 
the extraction of reasons behind the expressed 
attitude. 
Level Label Baseline method @AM Accuracy Precision Recall F-score Accuracy Precision Recall F-score 
ALL 
anger 
0.437 
0.742 0.511 0.605 
0.621 
0.818 0.600 0.692 
disgust 0.600 0.857 0.706 0.818 0.857 0.837 
fear 0.727 0.741 0.734 0.768 0.796 0.782 
guilt 0.667 0.364 0.471 0.833 0.455 0.588 
interest 0.380 0.357 0.368 0.772 0.524 0.624 
joy 0.266 0.579 0.364 0.439 0.905 0.591 
sadness 0.454 0.632 0.528 0.528 0.917 0.670 
shame 0.818 0.500 0.621 0.923 0.667 0.774 
surprise 0.625 0.694 0.658 0.750 0.833 0.789 
POS jud 0.429 0.227 0.297 0.824 0.424 0.560 
NEG jud 0.524 0.141 0.222 0.889 0.410 0.561 
POS app 0.349 0.150 0.210 0.755 0.400 0.523 
NEG app 0.250 0.138 0.178 0.529 0.310 0.391 
neutral 0.408 0.483 0.442 0.559 0.437 0.490 
MID 
POS aff 
0.524 
0.464 0.695 0.557 
0.709 
0.668 0.888 0.762 
NEG aff 0.692 0.711 0.701 0.765 0.910 0.831 
POS jud 0.405 0.227 0.291 0.800 0.424 0.554 
NEG jud 0.458 0.141 0.216 0.842 0.410 0.552 
POS app 0.333 0.150 0.207 0.741 0.400 0.519 
NEG app 0.222 0.138 0.170 0.474 0.310 0.375 
neutral 0.378 0.483 0.424 0.514 0.437 0.472 
TOP 
POS 
0.732 
0.745 0.796 0.770 
0.879 
0.918 0.920 0.919 
NEG 0.831 0.719 0.771 0.912 0.922 0.917 
neutral 0.347 0.483 0.404 0.469 0.437 0.452 
Table 4. Results of the evaluation of performance of the baseline method and @AM system. 
813
References 
Alm, Cecilia O. 2008. Affect in Text and Speech. PhD 
Dissertation. University of Illinois at Urbana-
Champaign. 
Aman, Saima, and Stan Szpakowicz. 2008. Using 
Roget's Thesaurus for Fine-Grained Emotion Rec-
ognition. Proceedings of the Third International 
Joint Conference on Natural Language Processing, 
Hyderabad, India, pp. 296-302. 
Bhowmick, Plaban K., Anupam Basu, and Pabitra 
Mitra. 2009. Reader Perspective Emotion Analysis 
in Text through Ensemble based Multi-Label Clas-
sification Framework. Computer and Information 
Science, 2 (4): 64-74. 
Boucouvalas, Anthony C. 2003. Real Time Text-to-
Emotion Engine for Expressive Internet Communi-
cations. Being There: Concepts, Effects and Mea-
surement of User Presence in Synthetic Environ-
ments, Ios Press, pp. 306-318. 
Chaumartin, Francois-Regis. 2007. UPAR7: A Know-
ledge-based System for Headline Sentiment Tag-
ging. Proceedings of the SemEval-2007 Interna-
tional Workshop, pp. 422-425. 
Choi, Yejin, and Claire Cardie. 2008. Learning with 
Compositional Semantics as Structural Inference 
for Subsentential Sentiment Analysis. Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing, pp. 793-801. 
Chuang, Ze-Jing, and Chung-Hsien Wu. 2004. Multi-
modal Emotion Recognition from Speech and Text. 
Computational Linguistic and Chinese Language 
Processing, 9(2): 45-62. 
Finkel, Jenny R., Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs 
Sampling. Proceedings of the 43nd Annual Meet-
ing of the ACL, pp. 363-370. 
Hoye, Leo. 1997. Adverbs and Modality in English. 
New York: Addison Wesley Longman Inc. 
Izard, Carroll E. 1971. The Face of Emotion. New 
York: Appleton-Century-Crofts. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and 
Martha Palmer. 2007. A Large-scale Classification 
of English Verbs. Language Resources and Evalu-
ation, 42 (1): 21-40. 
Kozareva, Zornitsa, Borja Navarro, Sonia Vazquez, 
and Andres Montoyo, A. 2007. UA-ZBSA: A 
Headline Emotion Classification through Web In-
formation. Proceedings of the SemEval-2007 In-
ternational Workshop, pp. 334-337. 
Liu, Hugo, Henry Lieberman, and Ted Selker. 2003. 
A Model of Textual Affect Sensing Using Real-
World Knowledge. Proceedings of IUI-2003, pp. 
125-132. 
Martin, James R., and Peter R.R. White. 2005. The 
Language of Evaluation: Appraisal in English. 
Palgrave, London, UK. 
Miller, George A. 1990. WordNet: An On-line Lexi-
cal Database. International Journal of Lexicogra-
phy, Special Issue, 3 (4): 235-312. 
Moilanen, Karo, and Stephen Pulman. 2007. Senti-
ment Composition. Proceedings of the Recent Ad-
vances in Natural Language Processing Interna-
tional Conference, pp. 378-382. 
Nasukawa, Tetsuya, and Jeonghee Yi. 2003. Senti-
ment Analysis: Capturing Favorability using Natu-
ral Language Processing. Proceedings of the 2nd 
International Conference on Knowledge Capture, 
pp. 70-77. 
Neviarouskaya, Alena, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. SentiFul: Generating a Relia-
ble Lexicon for Sentiment Analysis. Proceedings 
of the International Conference on Affective Com-
puting and Intelligent Interaction, IEEE, Amster-
dam, Netherlands, pp. 363-368. 
Strapparava, Carlo, and Rada Mihalcea. 2008. Learn-
ing to Identify Emotions in Text. Proceedings of 
the 2008 ACM Symposium on Applied Computing, 
Fortaleza, Brazil, pp. 1556-1560. 
Strapparava, Carlo, Alessandro Valitutti, and Oliviero 
Stock. 2007. Dances with Words. Proceedings of 
the International Joint Conference on Artificial In-
telligence, pp. 1719-1724. 
Subrahmanian, V.S., and Diego Reforgiato. 2008. 
AVA: Adjective-Verb-Adverb Combinations for 
Sentiment Analysis. Intelligent Systems, IEEE, 23 
(4): 43-50. 
Taboada, Maite, and Jack Grieve. 2004. Analyzing 
Appraisal Automatically. Proceedings of AAAI 
Spring Symposium on Exploring Attitude and Af-
fect in Text, pp.158-161. 
Whitelaw, Casey, Navendu Garg, and Shlomo Arga-
mon. 2005. Using Appraisal Groups for Sentiment 
Analysis. Proceedings of the 14th ACM Interna-
tional Conference on Information and Knowledge 
Management, CIKM, Bremen, Germany, pp. 625-
631. 
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. Proceedings of HLT-
EMNLP-2005, ACL, pp. 347-354. 
814
Coling 2010: Poster Volume, pages 1229?1237,
Beijing, August 2010
Learning Web Query Patterns for Imitating Wikipedia Articles
Shohei Tanaka?
tanaka@mi.ci.i.u-tokyo.ac.jp
Naokaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Mitsuru Ishizuka?
ishizuka@i.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology
University of Tokyo
?Interfaculty Initiative in
Information Studies
University of Tokyo
Abstract
This paper presents a novel method for ac-
quiring a set of query patterns to retrieve
documents containing important informa-
tion about an entity. Given an existing
Wikipedia category that contains the tar-
get entity, we extract and select a small
set of query patterns by presuming that
formulating search queries with these pat-
terns optimizes the overall precision and
coverage of the returned Web informa-
tion. We model this optimization prob-
lem as a weighted maximum satisfiabil-
ity (weighted Max-SAT) problem. The
experimental results demonstrate that the
proposed method outperforms other meth-
ods based on statistical measures such as
frequency and point-wise mutual informa-
tion (PMI), which are widely used in rela-
tion extraction.
1 Introduction
Wikipedia1 is useful for obtaining comprehensive
information of entities and concepts. However,
even with 3.3 million English articles, Wikipedia
does not necessarily include articles about an en-
tity and concept of interest to a user. The ultimate
goal of this study is to generate articles about an
entity of a specified category from the Web by us-
ing Wikipedia articles in the same entity category
as exemplars.
This study follows previous work of the
other authors on query-biased/focused summa-
rization (Tombros and Sanderson, 1998; Berger
1http://en.wikipedia.org/
and Mittal, 2000) for modeling the target article
generation process. In that model, when a user in-
puts an entity of interest, Web pages are retrieved
that describe the entity by issuing queries to an
information retrieval system. Using the retrieved
pages as an information source, an article (sum-
mary) can be produced specialized for the target
entity. From the application point of view, the arti-
cle should include the concepts that best describe
the target entity. In addition, articles concerning
the entities of a category should cover concepts
that are typical of the category. For example, an
article about an actor is expected to mention his
nationality, date of birth, movie credits, awards,
etc.
A great number of researchers have ad-
dressed the problem of query-focused summariza-
tion (Carbonell and Goldstein, 1998; White et
al., 2003; Dang, 2005; Daume? and Marcu, 2006;
Varadarajan and Hristidis, 2006; Fuentes et al,
2007; Gupta et al, 2007; Wang et al, 2007; Ka-
nungo et al, 2009). However, these studies as-
sume that a document collection is provided for
the summarization systems. In other words, col-
lecting source documents that include important
concepts for the target entity is not in the scope
of these studies. For example, queries such as
?(actor) was born in,? ?(actor) born on,? ?(actor)
plays,? and ?(actor) won? may be more suitable
than the simple query ?(actor)? for obtaining con-
cepts concerning the actor.
Source documents can be collected by a simi-
lar idea in relation extraction, which extracts en-
tities having specific relations with the target en-
tity (Hearst, 1992; Brin, 1999; Agichtein and Gra-
vano, 2000; Turney, 2001; Pantel and Pennac-
1229
chiotti, 2006; Blohm et al, 2007). These stud-
ies typically use statistical measures, such as fre-
quency and point-wise mutual information (PMI),
to assess the scores of the query patterns. How-
ever, these studies cannot eliminate the redun-
dancy of concepts retrieved by a query set because
they are designed to extract entities for each rela-
tion independently. For example, the query ?(ac-
tor) born on? would not be necessary if the query
?(actor) was born in? could gather documents re-
ferring to both the actor?s nationality and date of
birth.
In this paper, we propose a novel method for
acquiring a set of high-quality query patterns that
can gather source documents referring to impor-
tant concepts about a specified entity. Given a
category in which the entity is expected to be in-
cluded, we use existing Wikipedia articles in this
category to extract query patterns so that, when
used together with the entity, they can retrieve im-
portant concepts related to the entity. We then
select a small subset of query patterns that max-
imize the coverage and precision of the query re-
sult by modeling the query selection task as a
weighted maximum satisfiability (weighted Max-
SAT) problem.
2 Proposed method
First, let us define the terminology used in this
paper. An entity is a topic for which we need to
obtain an article (summary). Note that this defi-
nition is different from that used in other studies
(e.g., named entity recognition). A concept is a
noun phrase that has a specific relation to an en-
tity. A query pattern is a lexical pattern that con-
tains a slot filled by an entity. Used with an entity,
a query pattern instantiates a query that collects
related concepts. For example, ?X was born in? is
a query pattern in which X is a slot. When replac-
ing X with an entity (e.g., ?Dustin Hoffman?), the
query pattern instantiates a query that may return
the birthplace.
The goal of this study is, for a given entity
category (e.g., American actor), to acquire a set
of query patterns (template) for collecting related
concepts from the Web. We learn the template by
using Wikipedia articles of the category as super-
vision data. The method consists of three steps.
1. Triplet extraction identifies, for each
Wikipedia article, entity mentions, concepts,
and phrases that form a bridge between the
entity mentions and concepts. In the context
of learning query patterns from Wikipedia,
we assume that a Wikipedia article is writ-
ten for an entity. By identifying entity men-
tions and concepts in the article, we ob-
tain bridging phrases between entity men-
tions and concepts as candidates for query
patterns.
2. Pattern assessment verifies whether each
candidate query pattern can actually retrieve
concepts from the Web. This step issues
queries of the form ?(entity) (pattern)? to an
information retrieval system, analyzes the re-
trieved Web pages, and examines whether
each concept is found in the same sentence
as the query expressions.
3. Pattern selection obtains a template by
choosing a small subset of patterns such that
the retrieved Web pages contain as many
kinds of concepts as possible. We also elimi-
nate query patterns that can retrieve descrip-
tions other than concepts. We formalize this
step as a weighted Max-SAT problem.
2.1 Triplet extraction
We first analyze Wikipedia articles to extract
triplets of entities, query patterns, and concepts.
Because a Wikipedia article usually describes a
single entity, we identify the entity from the ti-
tle of the article. We then search for occurrences
of the entity in the body of the article. How-
ever, we might need to resolve coreference expres-
sions because the entity might be described by a
number of surface variations. For example, the
Wikipedia article titled ?Dustin Hoffman? might
refer to the entity using ?he? and ?Hoffman? as
well as ?Dustin Hoffman?; the entity ?Microsoft
Corporation? might be described by ?Microsoft?
and ?the company? in the article.
In general, coreference resolution is a non-
trivial NLP task. Fortunately, Wikipedia articles
are written for target entities. Therefore, we re-
place the occurrences of the following expressions
in the body with the entity name:
1230
 
Hoffman was born in [Los Angeles], [California], the
second and youngest son of Lillian and Harry Hoffman,
a [Russian]-born father who worked as a prop supervi-
sor/set decorator at [Columbia Pictures] before becom-
ing a furniture salesman. Hoffman is from a [Jewish]
family, although he did not have a religious upbring-
ing. He graduated from [Los Angeles High School] in
1955. He enrolled at [Santa Monica College] with the
intention of studying medicine but left after a year to
join the [Pasadena Playhouse]. 
Figure 1: A snippet of a Wikipedia article about
?Dustin Hoffman.?
1. Any token (split by spaces) that appears in
the title of the article.
2. The phrase that appears the most frequently
with the four anaphoric expressions ?he,?
?she,? ?they,? and ?the noun.?
The first rule deals with anaphoric expression
caused by an ellipsis, e.g., ?Dustin Hoffman? is
referred to by ?Dustin? and ?Hoffman.? The
second rule resolves the coreference expressions
caused by pronouns and definite noun phrases.
After detecting the entity mentions, we identify
the concepts concerning the entity in the article.
In this study, we employ WikiLink texts (anchor
texts linked with other Wikipedia articles) that co-
occur with the entity mentions in the same sen-
tences. Finally, we identify a candidate of a query
pattern as a phrase that satisfies the following con-
ditions:
1. It consists of alphanumeric letters and hy-
phens only.
2. Its length is no longer than 6 tokens.
3. It appears between an entity mention and a
concept in a sentence.
Figure 1 shows a snippet of the Wikipedia article
about ?Dustin Hoffman.? The underlined expres-
sions are identified as entity mentions; the text in
square brackets represents a WikiLink text. Be-
cause all WikiLink texts appear in sentences with
the entity mentions, we identify all expressions
with square brackets as concepts. Italic texts are
candidates of the query patterns.
Finally, we extract triplets of the form
?Ek,Pi,C j? from the Wikipedia article, where
Table 1: Triplets extracted from Figure 1.
Entity Query pattern concept
Dustin Hoffman was born in Los Angeles
Dustin Hoffman was born in California
Dustin Hoffman was born in Russian
Dustin Hoffman was born in Columbia Pictures
Dustin Hoffman is from a Jewish
Dustin Hoffman graduated from Los Angeles High School
Dustin Hoffman enrolled at Santa Monica College
Dustin Hoffman enrolled at Pasadena Playhouse
Ek (k ? {1, ...,L}) denotes the entity, Pi (i ?
{1, ...,M}) denotes a query pattern, and C j ( j ?
{1, ...,N}) denotes a concept. For each conceptC j
found in the Wikipedia article, we build a triplet
by setting Ek as the entity of the article and Pj
as the query pattern that precedes the concept C j.
Repeating this process for L Wikipedia articles in
the same category, we obtain triplets withM query
patterns and N concepts.
Table 1 shows the eight triplets obtained from
Figure 1. Here, it might not be clear whether
the indefinite article a is necessary in the pattern
is from a. Although we do not address this is-
sue directly in this paper, we determine the pop-
ularity and usefulness of the pattern by analyzing
Wikipedia articles in the same category. Similarly,
some concepts (e.g., Russian) are not so impor-
tant for the entity. It may be better to filter out the
concept, but we expect that errors in concept iden-
tification are negligible when selecting the query
patterns.
2.2 Pattern assessment
In this step, we verify whether each pattern Pi can
actually retrieve concepts of the entities from the
Web. More specifically, for every combination of
an entity mention Ek and a pattern Pi, we issue a
query ?Ek Pi? (e.g., ?Dustin Hoffman graduated
from?) to Yahoo! Search BOSS2. We download
the top 10 Web pages retrieved by each query and
examine whether any of the concepts, C j, appear
in the same sentence as the query phrase. To de-
scribe the capability of the patterns for retrieving
concepts, we introduce an m?n matrix called R,
Ri j =
{
1 (pattern Pi can retrieve concept C j)
0 (otherwise) .
2http://developer.yahoo.com/search/boss/
1231
DVD
the News
blogs
won the Academy Award
was born in
California
Los Angeles
1 9 3 7
Dustin Hoffman
graduated from
grew up in
in
L.A. High School
Figure 2: Patterns ? collected concepts graph.
Figure 2 illustrates a bipartite graph between
the patterns and concepts with R as the biadja-
cency matrix. Here, nodes with dotted lines are
expressions other than concepts but are retrieved
by the patterns. For example, the pattern ?in? can
retrieve four concepts Academy Award, Califor-
nia, Los Angeles, and 1937, but it also retrieves
non-concepts such as DVD, the News, and blogs.
In other words, this pattern can retrieve sentences
with a number of concepts, but it also gathers un-
necessary sentences. Thus, we define the error
rate of pattern Pi
?i = 1? (# sentences with concepts)(# total sentences retrieved by Pi) .
2.3 Pattern selection
Based on the pattern assessment in Section 2.2,
this step chooses a small set of query patterns as
the template. Let w1, ...,wm denote m Boolean (0?
1 integer) variables, each of which (wi) indicates
whether the corresponding query pattern Pi is se-
lected (1) or unselected (0). Choosing a subset of
query patterns is equivalent to assigning Boolean
values to the variables w1, ...,wm. The number of
selected patterns is ?mi=1wi.
Given an assignment of variables w1, ...,wm for
the query patterns, we can examine whether the
concept C j is retrieved from the patterns by using
the logical sum,
c j = w1R1 j ?w2R2 j ? ...?wmRmj =
m?
i=1
wiRi j.
Here, c j is a Boolean (0?1) variable indicating that
concept C j is retrieved (1) or not retrieved (0) by
the template. In Figure 2, if either the ?in? or ?was
born in? pattern is selected, we can retrieve the
concept ?1937? from the Web search.
To choose a set of query patterns, we maxi-
mize the number of concept coverages ?nj=1 c j as
well as minimize the number of patterns selected
?mi=1wi and the total of the error rates of the se-
lected patterns?mi=1 ?iwi. This is achieved by solv-
ing the following problem.
Problem 1.
Maximize
n?
j=1
c j ??
m?
i=1
wi ??
m?
i=1
?iwi,
subject to: c1 =
m?
i=1
wiRi1
...
cn =
m?
i=1
wiRin,
wi ? {0,1}.
Here, ? and ? are the parameters for control-
ling the preference of a smaller number of patterns
(?) and the preference of accurate patterns (? ).
To solve this problem, we rewrite it as a
weighted maximize satisfiability (weighted Max-
SAT) problem.
Problem 2.
Maximize
n+m?
k=1
?kxk
Subject to: x1 =
m?
i=1
wiRi1 (?1 = 1)
... (...)
xn =
m?
i=1
wiRin (?n = 1)
xn+1 = ?w1 (?n+1 = ?+??1)
... (...)
xn+m = ?wm (?n+m = ?+??m)
wi ? {0,1}
Instead of subtracting the penalty terms from
the objective value, we give bonus weights (? +
??i) if the pattern Pi is not selected. This is
achieved by introducing additional clauses xn+1,
..., xn+m that are satisfied by ?w1, ..., ?wm, respec-
tively. Therefore, the optimization process tries
to find a compromise between selecting patterns
(clauses x1, ..., xn) and rejecting patterns (clauses
xn+1, ..., xn+m). Although the complexity of the
weighted Max-SAT problem is NP-hard, we use
MiniMaxSAT (Heras et al, 2008) to solve the
problem.
1232
3 Evaluation
To verify the performance of the proposed
method, we compare the precision, coverage,
and F?-score of the information retrieval process
by using the template obtained by the proposed
method with that by three other baseline methods.
3.1 Experimental Settings
3.1.1 Data
We use articles of five categories in Wikipedia
as the data for evaluation: American actors, Ge-
netic disorders, American tennis players, Soft-
ware companies, and Operas. Among these cat-
egories, the first two (American actors, Genetic
disorders) have been commonly used as evalua-
tion data in previous research on text summariza-
tion (Sauper and Barzilay, 2009). The other three
(American tennis players, Software companies,
Operas) are categories about three distinct topics
(sport, business, entertainment).Table 2 shows in-
formation about these categories.
We divide the article set of a given category into
six subsets. We use one subset as the development
set for tuning the parameters ? and ? in the pro-
posed method. The remaining five subsets are the
training set and the test set, which are used for the
5-fold cross-validation. We create the template by
using the training set and evaluate it with the test
set. For evaluation of the baseline methods, we
use only the training set and the test set.
3.1.2 Baselines
Random Selection
The baseline ?Random Selection? randomly se-
lects 10 query patterns from the candidate query
patterns as the template for the category.
Frequency
In this baseline method, we sort the query patterns
for each category in the order of frequency of oc-
currences in the category. We then select the top
10 frequent query patterns as the template for the
category.
PMI-Web
The baseline PMI-Web chooses the query patterns
that are the most ?reliable.? Following KnowIt-
Now (Cafarella et al, 2005) and Espresso (Pan-
tel and Pennacchiotti, 2006), the ?reliability? of a
Table 2: The five categories used for evaluation.
Category #Articles #Patterns #Concepts
American actors 1864 2951 10495
American tennis players 444 1039 2826
Software companies 1890 1992 5087
Genetic disorders 657 1087 2400
Operas 1425 2125 6365
Figure 3: Relation between coverage and query
pattern frequency.
pattern is defined by using the strength of the as-
sociation of the pattern with the entities and con-
cepts co-occurring with the pattern. In KnowIt-
Now and Espresso, PMI (point-wise mutual infor-
mation) is used to measure the strength of this as-
sociation. PMI is estimated with the Web search
hit counts as follows:
pmi(Ek,Pi,C j)? hit(Ek,Pi,C j)hit(Ek,Pi) ?hit(C j) ,
where hit(EkPi), hit(C j) are the Web search hit
counts for the query ?Ek, Pi,? ?C j?(Ek,Pi,C j is en-
tity, pattern, concept), and hit(Ek,Pi,C j) is the hit
count for the query ?Ek Pi? and ?C j.? The relia-
bility score of the query pattern is defined as the
following formula:
Score(Pi) = 1|S| ?(Ek,C j)?S pmi(Ek,Pi,C j),
where S is the set of pairs of entity Ek and concept
C j co-occurring with the pattern Pi in a sentence.
The method PMI-Web chooses the top 10 patterns
that have the highest reliability scores.
3.1.3 Experiments
We use each method to generate a template and
retrieve information of the entities by using the
1233
query patterns in the template. We remove the
query patterns occurring only once in each cat-
egory from the candidate patterns because these
patterns may be too entity-specific or noisy.
Figure 3 shows the coverage of the concept re-
trieval process when we use the top N frequently
appearing patterns in the candidate pattern set. We
observe that the coverage does not reach 100%
even if we use all the query patterns. This is be-
cause some concepts cannot be retrieved by any
query pattern. Moreover, we consider a Wikilink
as a concept, even though some Wikilink texts do
not actually represent a concept.
We use query patterns that occur no less than
3 times (for American actors, American ten-
nis players, Software companies, and Operas) or
twice (for Genetic disorders) in the correspond-
ing Wikipedia articles so that the query patterns
reach 95% of the upper bound of the coverage.
This small subset comprises the final candidate
patterns. For the candidate patterns, we use the
proposed method (solving the weighted Max-SAT
problem) and the three baselines described above
to choose N query patterns.
The precision, coverage and quasi F-score (F?-
score) of the information retrieval process by each
template are defined as follows:
precision = freq(Ek,Pi,Cj)freq(Ek,Pi) ,coverage =
Ccollected
Ctotal ,
F ? = 2 ?precision ? coverageprecision+ coverage ,
where freq(Ek,Pi) is the frequency of the phrase
?Ek Pi? in the retrieved documents, freq(Ek,Pi,C j)
is the frequency of co-occurrence of the phrase
?Ek Pi? and ?C j? in the sentences. Ctotal is the to-
tal number of distinct concepts in the data set, and
Ccollected is the number of distinct concepts which
can be collected by the template.
3.2 Result
Table 5 shows the average of the precision, cover-
age and F? scores of the five categories when we
choose 10 query patterns (N=10). The proposed
method obtains the highest score of all methods.
Moreover, the proposed method outperforms the
baselines not only for the average of all categories,
but also for each category. This result indicates
Table 5: Performance of the templates produced
by the proposed method and the three baselines
(N=10).
Method Precision Coverage F? score
Random 16.56 11.40 13.19
Frequency 21.43 29.29 24.40
PMI-Web 22.55 22.08 21.42
Proposed 27.34 30.77 27.95
Figure 4: Number of query patterns (N) in tem-
plate and F? score in American tennis players.
that the proposed method is able to choose query
patterns that precisely and comprehensively col-
lect the target concepts.
Table 3 shows some example templates pro-
duced by the proposed method. In this table, the
number in the parentheses next to a pattern is the
frequency rank of the pattern. We observe that the
proposed method generates templates with two
types of query patterns: generic patterns and spe-
cific patterns. Generic patterns such as ?is a?
and ?is an? are patterns that can appear in every
category. These patterns cover various kinds of
concepts (high coverage), but may retrieve sen-
tences that do not describe any concept (low pre-
cision). Specific patterns, such as ?has a star on
the? and ?was nominated for a,? can retrieve con-
cepts that have specific relations with the entity.
Therefore, queries with specific patterns retrieve
a small number of concepts with high precision.
The proposed method chooses query patterns in
both of these types to achieve both high precision
and high coverage. Therefore, it is able to retrieve
1234
Table 3: Templates generated by the proposed method (N=10): (n) is the frequency rank.
Category Template
American actors ?is an?(1), ?was an?(2), ?was a?(7), ?graduated from?(9), ?died of?(18), ?has a star on the?(24),
?was nominated for a?(28), ?was married to?(47), ?was born on?(56), ?has appeared in?(92)
American tennis players ?defeated?(3), ?beat?(5), ?is a former?(12), ?is an?(16), ?graduated from?(17),
?reached the?(18), ?played?(24), ?of?(30), ?was?(38), ?won?
Software companies ?is a?(1), ?acquired?(3), ?is?(9), ?is headquartered in?(11), ?was founded by?(15),
?has offices in?(22), ?was?(29), ?include?(36), ?introduced?(41), ?is an international?(41)
Genetic disorders ?is a?(1), ?is an?(2), ?has an autosomal recessive pattern of?(3),
?has an autosomal dominant pattern of?(9), ?is named after?(11), ?is a form of?(13),
?is caused by?(16), ?include?(18), ?appears to be inherited in an?(41), ?is considered an?(41)
Operas ?is an?(1), ?is a?(2), ?is an opera by?(17), ?was?(18), ?is a comic?(20), ?premiered at the?(25),
?was first performed at?(31), ?is the second?(38), ?opera?(46), ?libretto by?(62)
Table 4: Templates generated by different methods for the Opera category (N= 10).
Method Template Pre. Cov. F?
Random ?was an,? ?of the complete operas of the,? ?was on,? ?was commissioned by,? 15.79 8.12 10.73
?by,? ?is,? ?popular,? ?for the,? ?New York,? ?the same name by?
Frequency ?is an,? ?is a,? ?the,? ?by,? ?of the,? ?in,? ?and,? ?of,? ?was a,? ?a? 16.83 27.12 20.77
PMI-Web ?was created by,? ?is a three act,? ?premiered at the,? ?was an,? ?is an,? 30.25 18.29 22.80
?is an opera composed by,? ?is a Hindi language,? ?premiered on?
?was commissioned by,? ?was first performed at the,?
Proposed ?is an,? ?is a,? ?is an opera by,? ?was,? ?is a comic,? ?premiered at the,? 31.18 27.28 29.10
?was first performed at,? ?is the second,? ?opera,? ?libretto by?
various types of concepts. This implies that the
method achieves high coverage even for concepts
that cannot be retrieved by generic patterns.
The baseline Frequency obtains the second
highest F?-score. It achieves high coverage but
low precision. This is because this method
chooses high-frequency patterns that can appear
with every concept. Therefore, it is able to retrieve
concepts with high coverage. However, these pat-
terns do not retrieve specific information concern-
ing a concept. Moreover, some high-frequency
patterns, such as ?the ? and ?by,? lead to sentences
that do not describe any concept.
Table 4 shows the patterns generated for the cat-
egoryOpera by each method. We can observe that
the method Frequency chooses very generic pat-
terns, such as ?is a,? ?and,? and ?the,? which are
not specific to Opera.
In contrast, the method PMI-Web achieves high
precision but low coverage. This is because
this method chooses highly reliable patterns (e.g.,
?was commissioned by?), which are strongly as-
sociated with a specific kind of concept. How-
ever, these patterns cannot retrieve a broad range
of concepts related to the target entity. This ex-
plains why the method cannot achieve high cover-
age.
Figure 4 shows the F? scores when we vary the
number of selected query patterns (N) for the cat-
egory American tennis players. We observe that
the templates generated by the proposed method
achieve the highest F?-score at every value of N.
The maximum F?-score is 24.7, which is achieved
when N is 30. Moreover, the proposed method re-
quires only five query patterns to achieve the F?-
score of 21.4. Therefore, the proposed method
achieves a high F?-score by using only a small
number of patterns. This implies that the method
achieves high performance in a short query pro-
cessing time.
4 Related Work
Many studies have addressed the problem of pat-
tern extraction from Wikipedia (or other large cor-
pora). Filatova et al (2006) presented an approach
for automatically extracting important word pat-
terns from a large corpus. They analyzed the BBC
corpus to extract word patterns containing verbs
that are supposed to be important for a specific
domain. Biadsy et al (2008) described a sys-
tem for producing biographies for a given target
name. They used Wikipedia to learn the docu-
ment structures of a biography. Ye et al (2009)
1235
explored a method for generating a series of sum-
maries of various lengths by using information
from Wikipedia.
Sauper and Barzilay (2009) proposed an ap-
proach for creating a summary of many chunks
of text that are related to an entity and retrieved
from the Web. They used Wikipedia not only
for producing the template, but also for improv-
ing the summaries. Although the target of their
work is very close to that of our study, the focus of
each study is different. They address the method
for selecting appropriate sentences for summa-
rization, whereas we consider the method for se-
lecting query patterns that can generate a compre-
hensive summary of an entity.
Various studies have addressed Web page
summarization and query-focused summarization,
from search result summarization (Kanungo et al,
2009) to query biased summarization (Wang et al,
2007). Furthermore, Fujii and Ishikawa (2004)
presented a method to automatically compile en-
cyclopedic knowledge from the Web.
Similar to relation extraction, the proposed
method retrieves information concerning an entity
by using query patterns. This is because query
patterns for relation extraction are also appro-
priate in sentence extraction for multi-document
summarization (Hachey, 2009). However, the re-
lation extraction task primarily obtains query pat-
terns that retrieve instances of a specific relation.
This is different from the goal of this study, which
is obtaining a set of patterns that are able to re-
trieve a large range of topics related to an entity.
5 Conclusion
We present a novel method to acquire a set of
query patterns for retrieving documents that con-
tain important information regarding an entity.
Especially, we concentrate on the method for se-
lecting query patterns that are able to compre-
hensively and precisely retrieve important con-
cepts concerning an entity. The experimental re-
sults demonstrate that the proposed method out-
performs methods based on statistical measures
such as frequency and point-wise mutual infor-
mation (PMI), which are widely used in relation
extraction.
Currently, we use the text between an entity
and a WikiLink as a candidate for a query pat-
tern. In the future, we plan to use the text between
two noun phrases as query patterns to increase the
number of candidates for the pattern selection pro-
cess. Moreover, we intend to build a text summa-
rization application based on the proposed method
to confirm that the selected pattern set is able to
generate a comprehensive summary for an entity.
References
Agichtein, Eugene and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proc. of the fifth ACM conference on
Digital libraries, pages 85?94.
Berger, Adam and Vibhu O. Mittal. 2000. Query-
relevant summarization using FAQs. In Proc. of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 294?301.
Biadsy, Fadi, Julia Hirschberg, and Elena Filatova.
2008. An unsupervised approach to biography pro-
duction using Wikipedia. In Proc. of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies, pages 807?815.
Blohm, Sebastian, Philipp Cimiano, and Egon Stemle.
2007. Harvesting relations from theWeb: quantifiy-
ing the impact of filtering functions. In Proc. of the
22nd national conference on Artificial intelligence,
pages 1316?1321.
Brin, Sergey. 1999. Extracting patterns and relations
from the World Wide Web. Selected papers from
the International Workshop on The World Wide Web
and Databases, pages 172?183.
Cafarella, Michael J., Doug Downey, Stephen Soder-
land, and Oren Etzioni. 2005. KnowItNow: Fast,
scalable information extraction from the Web. In
Proc. of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 563?570.
Carbonell, Jaime and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In Proc. of
the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 335?336.
Dang, Hoa Trang. 2005. Overview of DUC 2005. In
Document Understanding Conference (DUC) 2005.
Daume?, III, Hal and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proc. of the 21st
1236
International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 305?312.
Filatova, Elena, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proc. of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 207?214.
Fuentes, Maria, Enrique Alfonseca, and Horacio
Rodr??guez. 2007. Support vector machines for
query-focused summarization trained and evaluated
on pyramid data. In Proc. of the 45th Annual Meet-
ing of the ACL on Interactive Poster and Demon-
stration Sessions, pages 57?60.
Fujii, Atsushi and Tetsuya Ishikawa. 2004. Summa-
rizing encyclopedic term descriptions on the Web.
In Proc. of the 20th international conference on
Computational Linguistics, pages 645?651.
Gupta, Surabhi, Ani Nenkova, and Dan Jurafsky.
2007. Measuring importance and query relevance
in topic-focused multi-document summarization. In
Proc. of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions, pages
193?196.
Hachey, Ben. 2009. Multi-document summarisation
using generic relation extraction. In Proc. of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 420?429.
Hearst, Marti A. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proc. of
the 14th conference on Computational linguistics,
pages 539?545.
Heras, Federico, Javier Larrosa, and Albert Oliveras.
2008. MiniMaxSat: An efficient weighted Max-
SAT solver. Journal of Artificial Intelligence Re-
search, 31:1?32.
Kanungo, Tapas, Nadia Ghamrawi, Ki Yuen Kim, and
Lawrence Wai. 2009. Web search result summa-
rization: Title selection algorithms and user satis-
faction. In Proceeding of the 18th ACM conference
on Information and knowledge management, pages
1581?1584.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proc. of the
21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 113?
120.
Sauper, Christina and Regina Barzilay. 2009. Auto-
matically generating Wikipedia articles: a structure-
aware approach. In Proc. of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 208?216.
Tombros, Anastasios and Mark Sanderson. 1998. Ad-
vantages of query biased summaries in information
retrieval. In Proc. of the 21st annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 2?10.
Turney, Peter D. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proc. of the 12th
European Conference on Machine Learning, pages
491?502.
Varadarajan, Ramakrishna and Vagelis Hristidis.
2006. A system for query-specific document sum-
marization. In Proc. of the 15th ACM international
conference on Information and knowledge manage-
ment, pages 622?631.
Wang, Changhu, Feng Jing, Lei Zhang, and Hong-
Jiang Zhang. 2007. Learning query-biased Web
page summarization. In Proc. of the sixteenth ACM
conference on information and knowledge manage-
ment, pages 555?562.
White, Ryen W., Joemon M. Jose, and Ian Ruthven.
2003. A task-oriented study on the influencing ef-
fects of query-biased summarisation in Web search-
ing. Information Processing and Management,
39(5):707?733.
Ye, Shiren, Tat-Seng Chua, and Jie Lu. 2009. Sum-
marizing definition from Wikipedia. In Proc. of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1, pages 199?207.
1237
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399?409,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Approach to Improve Classification of
Infrequent Discourse Relations using Feature Vector Extension
Hugo Hernault
hugo@mi.ci.i.
u-tokyo.ac.jp
Danushka Bollegala
danushka@iba.t.
u-tokyo.ac.jp
Graduate School of Information Science & Technology
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
Mitsuru Ishizuka
ishizuka@i.
u-tokyo.ac.jp
Abstract
Several recent discourse parsers have em-
ployed fully-supervised machine learning ap-
proaches. These methods require human an-
notators to beforehand create an extensive
training corpus, which is a time-consuming
and costly process. On the other hand, un-
labeled data is abundant and cheap to col-
lect. In this paper, we propose a novel
semi-supervised method for discourse rela-
tion classification based on the analysis of co-
occurring features in unlabeled data, which is
then taken into account for extending the fea-
ture vectors given to a classifier. Our exper-
imental results on the RST Discourse Tree-
bank corpus and Penn Discourse Treebank in-
dicate that the proposed method brings a sig-
nificant improvement in classification accu-
racy and macro-average F-score when small
training datasets are used. For instance, with
training sets of c.a. 1000 labeled instances, the
proposed method brings improvements in ac-
curacy and macro-average F-score up to 50%
compared to a baseline classifier. We believe
that the proposed method is a first step towards
detecting low-occurrence relations, which is
useful for domains with a lack of annotated
data.
1 Introduction
Automatic detection of discourse relations in natu-
ral language text is important for numerous tasks in
NLP, such as sentiment analysis (Somasundaran et
al., 2009), text summarization (Marcu, 2000) and di-
alogue generation (Piwek et al, 2007). However,
most of the recent work employing discourse re-
lation classifiers are based on fully-supervised ma-
chine learning approaches (duVerle and Prendinger,
2009; Pitler et al, 2009; Lin et al, 2009). Two
of the main corpora with discourse annotations are
the RST Discourse Treebank (RSTDT) (Carlson et
al., 2001) and the Penn Discourse Treebank (PDTB)
(Prasad et al, 2008a), which are both based on the
Wall Street Journal (WSJ) corpus.
In the RSTDT, annotation is done using 78
fine-grained discourse relations, which are usually
grouped into 18 coarser-grained relations. Each of
these relations has furthermore several possible con-
figurations for its arguments?its ?nuclearity? (Mann
and Thompson, 1988). In practice, a classifier
trained on these coarse-grained relations must solve
a 41-class classification problem. Some of the re-
lations corresponding to these classes are relatively
more frequent in the corpus, such as the ELAB-
ORATION[N][S] relation (4441 instances), or the
ATTRIBUTION[S][N] relation (1612 instances).1
However, other relation types occur very rarely,
such as TOPIC-COMMENT[S][N] (2 instances), or
EVALUATION[N][N] (3 instances). A similar phe-
nomenon can be observed in PDTB, in which 15
level-two relations are employed: Some, such as
EXPANSION.CONJUNCTION, occur as often as 8759
times throughout the corpus, whereas the remainder
of the relations, such as EXPANSION.EXCEPTION
and COMPARISON.PRAGMATIC CONCESSION, can
appear as rarely as 17 and 12 times respectively. Al-
though supervised approaches to discourse relation
learning achieve good results on frequent relations,
performance is poor on rare relation types (duVerle
and Prendinger, 2009).
Nonetheless, certain infrequent relation types
might be important for specific tasks. For instance,
1We use the notation [N] and [S] respectively to denote the
nucleus and satellite in a RST discourse relation.
399
capturing the RST TOPIC-COMMENT[S][N] and
EVALUATION[N][N] relations can be useful for
sentiment analysis (Pang and Lee, 2008).
Another situation where detection of low-
occurring relations is desirable is the case where we
have only a small training set at our disposal, for in-
stance when there is not enough annotated data for
all the relation types described in a discourse the-
ory. In this case, all the dataset?s relations can be
considered rare, and being able to build an efficient
classifier depends on the capacity to deal with this
lack of annotated data.
Our contributions in this paper are summarized as
follows.
? We propose a semi-supervised method that
exploits the abundant, freely-available unla-
beled data, which is harvested for feature co-
occurrence information, and used as a basis to
extend feature vectors to help classification for
cases where unknown features are found in test
vectors.
? The proposed method is evaluated on the
RSTDT and PDTB corpus, where it signifi-
cantly improves accuracy and macro-average
F-score when small training sets are used. For
instance, when trained on moderately small
datasets with ca. 1000 instances, the proposed
method increases the macro-average F-score
and accuracy up to 50%, compared to a base-
line classifier.
2 Related Work
Since the release in 2001 of the RSTDT corpus,
several fully-supervised discourse parsers have been
built in the RST framework. In the recent work of
duVerle and Prendinger (2009), a discourse parser
based on Support Vector Machines (SVM) (Vapnik,
1995) is proposed. SVMs are employed to train two
classifiers: One, binary, for determining the pres-
ence of a relation, and another, multi-class, for deter-
mining the relation label between related text spans.
For the discourse relation classifier, shallow lexical,
syntactic and structural features, including ?domi-
nance sets? (Soricut and Marcu, 2003) are used. For
relation classification, they report an accuracy of
0.668, and an F-score of 0.509 for the creation of
the full discourse tree.
The unsupervised method of Marcu and Echihabi
(2002) was the first that tried to detect implicit rela-
tions (i.e. relations not accompanied by a cue phrase,
such as ?however?, ?but?), using word pairs extracted
from two spans of text. Their method attempts to
capture the difference of polarity in words. For ex-
ample, the word pair (sell, hold) indicates a CON-
TRAST relation.
Discourse relation classifiers have also been
trained using PDTB. Pitler et al (2008) performed a
corpus study of the PDTB, and found that ?explicit?
relations can be most of the times distinguished by
their discourse connectives. Their discourse relation
classifier reported an accuracy of 0.93 for explicit
relations and in overall an accuracy of 0.744 for all
relations in PDTB.
Lin et al (2009) studied the problem of detecting
implicit relations in PDTB. Their relational classi-
fier is trained using features extracted from depen-
dency paths, contextual information, word pairs and
production rules in parse trees. They reported for
their classifier an accuracy of 0.402, which is an im-
provement of 14.1% over the previous state-of-the-
art for implicit relation classification in PDTB. For
the same task, Pitler et al (2009) also used word
pairs, as well as several other types of features such
as verb classes, modality, context, and lexical fea-
tures.
In text classification, similarity measures have
been employed in kernel methods, where they have
been shown to improve accuracy over ?bag-of-
words? approaches. In Siolas and d?Alche?-Buc
(2000), a semantic proximity measure based on
WordNet (Fellbaum, 1998) is defined, as a basis to
create a proximity matrix for all terms of the prob-
lem. This matrix is then used to smooth the vectorial
data, and the resulting ?semantic? metric is incorpo-
rated into a SVM kernel, resulting in a significant
increase of accuracy and F-score over a baseline.
Cristianini et al (2002) have used a lexical sim-
ilarity measure derived from Latent Semantic In-
dexing (Deerwester et al, 1990), where the seman-
tic similarity between two terms is inferred from
the analysis of their co-occurrence patterns: Terms
that co-occur often in the same documents are con-
sidered as related. In this work, the statistical co-
occurrence information is extracted by the means of
singular value decomposition. The authors observe
400
substantial improvements in performance for some
datasets, while little effect is obtained for others.
Semantic kernels have also been shown to be effi-
cient for text classification tasks, in the case in of un-
balanced and sparse datasets. In Basili et al (2006),
a ?conceptual density? metric based on WordNet is
introduced, and employed in a SVM kernel. Using
this metric results in improved accuracy of 10% for
text classification in poor training conditions. How-
ever, the authors observe that when the number of
training documents is increased, the improvement
produced by the semantic kernel is lower.
Bloehdorn et al (2006) compare the performance
of different semantic kernels, based on several mea-
sures of semantic relatedness in WordNet. For each
measure, the authors note a performance increase
when little training data is available, or when the
feature representations are very sparse. However,
for our task, classification of discourse relations, we
employ not only words but also other types of fea-
tures such as parse tree production rules, and thus
cannot compute semantic kernels using WordNet.
In this paper, we are not aiming at defining
novel features for improving performance in RST or
PDTB relation classification. Instead we incorporate
numerous features that have been shown to be useful
for discourse relation learning and explore the pos-
sibilities of using unlabeled data for this task. One
of our goals is to improve classification accuracy for
rare discourse relations.
3 Method
Given a set of unlabeled instances U and labeled in-
stances L, our objective is to learn an n-class rela-
tion classifier H such that for a given test instance
x return its correct relation type H(x). In the case
of discourse relation learning we are interested in
the situation where |U | >> |L|. Here, we use the
notation |A| to denote the number of elements in a
set A. A fundamental problem that one encounters
when trying to learn a classifier for a large number
of relations with small training dataset is that most
of the features that appear in the test instances ei-
ther never occur in training instances or appear a
small number of times. Therefore, the classifica-
tion algorithm does not have sufficient information
to correctly predict the relation type of the given test
instance. We propose a method that first computes
the co-occurrence between features using unlabeled
data and use that information to extend the feature
vectors during training and testing, thereby reducing
the sparseness in test feature vectors. In Section 3.1,
we introduce the concept of feature co-occurrence
matrix and describe how it is computed using unla-
beled data. A method to extend feature vectors dur-
ing training and testing is presented in Section 3.2.
We defer the details on exact features used in the
method to Section 3.3. It is noteworthy that the
proposed method does not depend or assume a par-
ticular multi-class classification algorithm. Conse-
quently, it can be used with any multi-class classifi-
cation algorithm to learn a discourse relation classi-
fier.
3.1 Feature Co-occurrence Matrix
We represent an instance using a d dimensional fea-
ture vector f = [f1, . . . , fd]T, where fi ? R. We
define a feature co-occurrence matrix, C such that
the (i, j)-th element of C, C(i,j) ? [0, 1] denotes
the degree of co-occurrence between the two fea-
tures fi and fj . If both fi and fj appear in a fea-
ture vector then we define them to be co-occurring.
The number of different feature vectors in which fi
and fj co-occur is denoted by the function h(fi, fj).
From our definition of co-occurrence it follows that
h(fi, fj) = h(fj , fi). Importantly, feature co-
occurrences can be calculated only using unlabeled
data.
Feature co-occurrence matrices can be computed
using any co-occurrence measure. For the current
task we use the ?2-measure (Plackett, 1983) as the
preferred co-occurrence measure because of its sim-
plicity. ?2-measure between two features fi and fj
is defined as follows,
?2i,j =
2?
k=1
2?
l=1
(Oi,jk,l ? E
i,j
k,l)
2
Ei,jk,l
. (1)
Therein,Oi,j andEi,j are the 2?2 matrices contain-
ing respectively observed frequencies and expected
frequencies, which are respectively computed using
C as,
Oi,j =
(
h(fi, fj) Zi ? h(fi, fj)
Zj ? h(fi, fj) Zs ? Zi ? Zj
)
, (2)
401
and
Ei,j =
(
Zi?Zj
Zs
Zi?(Zs?Zj)
Zs
Zj ?(Zs?Zi)
Zs
(Zs?Zi)?(Zs?Zj)
Zs
)
. (3)
Here, Zi =
?
k 6=i h(fi, fk), and Zs =
?n
i=1 Zi.
Finally, we create the feature co-occurrence ma-
trix C, such that, for all pairs of features (fi, fj),
C(i,j) =
{
??2i,j if ?
2
i,j > c
0 otherwise
. (4)
Here ??2i,j =
?2i,j??
2
min
?2max??
2
min
? [0, 1], and c is the critical
value, which, for a confidence level of 0.05 and one
degree of freedom, can be set to 3.84. KeepingC(i,j)
in the range [0, 1] makes it convenient to filter out
low-relevance co-occurrences at the feature vector
extension step of Section 3.2.
In discourse relation learning, the feature space
can be extremely large. For example, with word
pair features (discussed later in Section 3.3), any
two words that appear in two adjoining discourse
units can form a feature. Because the number of
elements in the feature co-occurrence matrix is pro-
portional to the square of the feature space?s dimen-
sion, computing co-occurrences for all pairs of fea-
tures can be computationally costly. Moreover, stor-
ing a large matrix in memory for further computa-
tions can be problematic. To reduce the dimension-
ality and improve the sparseness in the feature co-
occurrence matrix, we use entropy-based feature se-
lection (Manning and Schu?tze, 1999). The negative
entropy, E(fi), of a feature fi is defined as follows,
E(fi) = ?
?
j 6=i
p(i, j) ? log (p (i, j)) . (5)
Here, p(i, j) is the probability that feature fi co-
occurs with feature fj , and is given by p(i, j) =
h(fi, fj)/Zi.
If a particular feature fi co-occurs with many
other features, then its negative entropy E(fi) de-
creases. Because we are interested in identifying
salient co-occurrences between features, we can ig-
nore the features that tend to co-occur with many
other features. Consequently, we sort the features in
the descending order of their entropy, and select the
top rankedN number of features to build the feature
co-occurrence matrix. This feature selection proce-
dure can efficiently reduce the dimensions of the fea-
ture co-occurrence matrix to N ? N . Because the
feature co-occurrence matrix is symmetric, we must
only store the elements for the upper (or lower) tri-
angular portion of it.
3.2 Feature Vector Extension
Once the feature co-occurrence matrix is computed
using unlabeled data as described in Section 3.1, we
can use it to extend a feature vector during train-
ing and testing. The proposed feature vector exten-
sion method is inspired by query expansion in the
field of Information Retrieval (Salton and Buckley,
1983; Fang, 2008). One of the reasons that a clas-
sifier might perform poorly on a test instance is that
there are features in the test instance that were not
observed during training. We call FU = {fi} the
set of features that were not observed by the clas-
sifier during training (i.e. occurring in test data but
not in training data). For each of those features, we
use the feature co-occurrence matrix to find the set
of co-occurring features, Fc(fi).
Let us denote the feature vector corresponding to
a training or test instance x by fx. We use the su-
perscript notation, f ix to denote the i-th feature in fx.
Moreover, the total number of features of fx is indi-
cated by d(x). For a feature f ix in fx, we define n(i)
number of expansion features, f (i,1)x , . . . , f
(i,n(i))
x as
follows. First, we require that each expansion fea-
ture f (i,j)x belongs to Fc(fi). Second, the value of
f (i,j)x is set to f ix ? C(i,j). The expansion features
for each feature f ix are then appended to the orig-
inal feature vector fx to create an extended feature
vector, f ?x, where,
f ?x = (f
1
x , . . . , f
d(x)
x , (6)
f (i,1)x , . . . , f
(i,n(i))
x , . . . ,
f (d(x),1)x , . . . , f
(d(x),n(d(x))
x ).
In total, doing so augments the original vector?s size
by
?
fi?U
|Fc(fi)|. All training and test instances
are extended in this fashion.
Note that because this process can potentially in-
crease the dimension too much, it is possible to re-
tain only candidate co-occurring features of Fc(fi)
possessing a co-occurrence value C(i,j) above a cer-
tain threshold. In the experiments of Section 4 how-
402
ever, we experienced dimension increase of 10000 at
most, which did not require us to use thresholding.
3.3 Features
We use three types of features: Word pairs, produc-
tion rules from the parse tree, as well as features en-
coding the lexico-syntactic context at the border be-
tween two units of text (Soricut and Marcu, 2003).
Our word pairs are lemmatized using the Wordnet-
based lemmatizer of NLTK (Loper and Bird, 2002).
Figure 1 shows the parse tree for a sentence com-
posed of two discourse units, which serve as argu-
ments of a discourse relation we want to generate a
feature vector from. Lexical heads have been calcu-
lated using the projection rules of Magerman (1995),
and annotated between brackets. Surrounded by
dots is, for each argument, the minimal set of sub-
parse trees containing strictly all the words of the
argument.
We first extract all possible lemmatized word-
pairs from the two arguments, such as (Mr., when),
(decline, ask) or (comment, sale). Next, we extract
from left and right argument separately, all produc-
tion rules from the sub-parse trees, such as NP 7?
NNP NNP, NNP 7? ?Sherry? or TO 7? ?to?.
Finally, we encode in our features three nodes of
the parse tree, which capture the local context at the
connection point between the two arguments: The
first node, which we call Nw, is the highest ances-
tor of the first argument?s last word w, and is such
that Nw?s right-sibling is the ancestor of the second
argument?s first word. Nw?s right-sibling node is
called Nr. Finally, we call Np the parent of Nw and
Nr. For each node, we encode in the feature vec-
tor its part-of-speech (POS) and lexical head. For
instance, in Figure 1, we have Nw = S(comment),
Nr = SBAR(when), and Np = VP(declined). In the
PDTB, certain discourse relations have disjoint ar-
guments. In this case, as well as in the case where
the two arguments belong to different sentences, the
nodes Nw, Nr, Np cannot be defined, and their cor-
responding features are given the value zero.
4 Experiments
The proposed method is independent of any partic-
ular classification algorithm. Because our goal is
strictly to evaluate the relative benefit of employing
the proposed method, and not the absolute perfor-
mance when used with a specific classification algo-
rithm, we select a logistic regression classifier, for its
simplicity. We use the multi-class logistic regression
(maximum entropy model) implemented in the Clas-
sias toolkit (Okazaki, 2009). Regularization param-
eters are set to their default value of one and are fixed
throughout the experiments described in the paper.
To create our unlabeled dataset, we use sentences
extracted from the English Wikipedia2, as they are
freely available and relatively easy to collect. For
further extraction of syntactic features, these sen-
tences are automatically parsed using the Stanford
parser (Klein and Manning, 2003). Then, they are
segmented into elementary discourse units (EDUs)
using our sequential discourse segmenter (Hernault
et al, 2010). The relatively high performance of
this RST segmenter, which has an F-score of 0.95
compared to that of 0.98 between human annota-
tors (Soricut and Marcu, 2003), is acceptable for this
task. We collect and parse 100000 sentences from
random Wikipedia articles. As there is no segmen-
tation tool for the PDTB framework, we assume that
co-occurrence information taken from EDUs created
using a RST segmenter is also useful for extending
feature vectors of PDTB relations. Unless other-
wise noted, the experiments presented in the rest of
this paper are done using those 100000 unlabeled in-
stances.
In the unlabeled data, any two consecutive dis-
course units might not always be connected by a dis-
course relation. Therefore, we introduce an artificial
NONE relation in the training set, in order to facil-
itate this. Instances of the NONE relation are gen-
erated randomly by pairing consecutive discourse
units which are not connected by a discourse relation
in the training data. NONE is also learnt as a separate
discourse relation class by the multi-class classifica-
tion algorithm. This enables us to detect discourse
units between which there exist no discourse rela-
tion, thereby improving the classification accuracy
for other relation types.
We follow the common practice in discourse re-
search for partitioning the discourse corpora into
training and test set. For the RST classifier, the
dedicated training and test sets of the RSTDT are
2http://en.wikipedia.org
403
NP (Sherry)
S (declined)
VP (declined)
NNP NNP
declined
VBD (declined)
Mr. Sherry to
VP (comment)
comment when asked about the sales
TO VP
SBAR (when)
WHADVP (when)
WRB
S (asked)
VP (asked)
VBN
PP (about)
IN NP (sales)
DT NNS
.
. (.)
Argument 1 Argument 2
VB
S (comment)
Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them?lexical heads
are indicated between brackets.
employed. For the PDTB classifier, we conform to
the guidelines of Prasad et al (2008b, 5): The por-
tion of the corpus corresponding to sections 2?21
of the WSJ is used for training the classifier, while
the portion corresponding to WSJ section 23 is used
for testing. In order to extract syntactic features, all
training and test data are furthermore aligned with
their corresponding parse trees in the Penn Treebank
(Marcus et al, 1993).
Because in the PDTB an instance can be
annotated with several discourse relations
simultaneously?called ?senses? in Prasad et
al. (2008b)?for each instance with n senses in
the corpus, we create n identical feature vectors,
each being labeled by one of the instance?s senses.
However, in the RST framework, only one relation
is allowed to hold between two EDUs. Conse-
quently, each instance from the RSTDT is labeled
with a single discourse relation, from which a
single feature vector is created. For RSTDT, we
extract 25078 training vectors and 1633 test vectors.
For PDTB we extract 49748 training vectors and
1688 test vectors. There are 41 classes (relation
types) in the RSTDT relation classification task,
and 29 classes in the PDTB task. For the PDTB,
we selected level-two relations, because they have
better expressivity and are not too fine-grained.
We experimentally set the entropy-based feature
selection parameter to N = 5000. With large N
values, we must store and process large feature
co-occurrence matrices. For example, doubling
the number of selected features, N to 10000 did
not improve the classification accuracy, although
it required 4GB of memory to store the feature
co-occurrence matrix.
Figure 2 shows the number of features that occur
in test data but not in labeled training data, against
the number of training instances. It can be seen from
Figure 2 that, with less training data available to the
classifier, we can potentially obtain more informa-
tion regarding features by looking at unlabeled data.
However, when the training dataset?s size increases,
the number of features that only appear in test data
decreases rapidly. This inverse relation between the
training dataset size and the number of features that
only appear in test data can be observed in both
RSTDT and PDTB datasets. For a training set of
100 instances, there are 23580 unseen features in
the case of RSTDT, and 27757 in the case of PDTB.
The number of unseen features is halved for a train-
ing set of 1800 instances in the case of RSTDT, and
for a training set of 1300 instances in the case of
PDTB. Finally, when selecting all available training
data, we count only 1365 unseen test features in the
case of RSTDT, and 87 in the case of PDTB.
In the following experiments, we use macro-
averaged F-scores to evaluate the performance of the
proposed discourse relation classifier on test data.
Macro-averaged F-score is not influenced by the
number of instances that exist in each relation type.
It equally weights the performance on both frequent
relation types and infrequent relation types. Because
we are interested in measuring the overall perfor-
mance of a discourse relation classifier across all re-
404
0 5000 10000 15000 20000 25000Number of training instances0
5000
10000
15000
20000
25000
30000
Num
ber
 of u
nse
en t
est 
feat
ure
s RSTDTPDTB
Figure 2: Number of features seen only in the test set, as
a function of the number of training instances used.
lation types we use macro-averaged F-score as the
preferred evaluation metric for this task.
We train a multi-class logistic regression model
without extending the feature vectors as a baseline
method. This baseline is expected to show the ef-
fect of using the proposed feature vector extension
approach for the task of discourse relation learn-
ing. Experimental results on RSTDT and PDTB
datasets are depicted in Figures 3 and 4. From
these figures, we see that the proposed feature ex-
tension method outperforms the baseline for both
RSTDT and PDTB datasets for the full range of
training dataset sizes. However, whereas the differ-
ence of scores between the two methods is obvious
for small amounts of training data, this difference
progressively decreases as we increase the amount
of training data. Specifically, with 100 training in-
stances, the difference between baseline and pro-
posed method is the largest: For RSTDT, the base-
line has a macro-averaged F-score of 0.084, whereas
the the proposed method has a macro-averaged F-
score of 0.189 (ca. 119% increase in F-score). For
PDTB, the baseline has an F-score of 0.016, while
the proposed method has an F-score of 0.089 (459%
increase). The difference of scores between the two
methods then progressively diminishes as the num-
ber of training instances is increased, and fades be-
yond 10000 training instances. The reason for this
behavior is given by Figure 2: For a small number
of training instances, the number of unseen features
in training data is large. In this case, the feature vec-
tor extension process is comprehensive, and score
can be increased by the use of unlabeled data. When
more training data is progressively used, the num-
ber of unseen test features sharply diminishes, which
means feature vector extension becomes more lim-
ited, and the performance of the proposed method
gets progressively closer to the baseline. Note that
we plotted PDTB performance up to 25000 train-
ing instances, as the number of unseen test features
becomes so small past this point that the perfor-
mances of the proposed method and baseline are
identical. Using all PDTB training data (49748 in-
stances), both baseline and proposed method reach a
macro-average F-score of 0.308.
0 5000 10000 15000 20000 25000Number of training instances0.05
0.10
0.15
0.20
0.25
0.30
Mac
ro-a
vera
ge F
-sco
re
Baseline RSTDTProposed method
Figure 3: Macro-average F-score (RSTDT) as a function
of the number of training instances used.
0 5000 10000 15000 20000 25000Number of training instances0.00
0.05
0.10
0.15
0.20
0.25
0.30
Mac
ro-a
vera
ge F
-sco
re
Baseline PDTBProposed method
Figure 4: Macro-average F-score (PDTB) as a function
of the number of training instances used.
405
#Tr = 1 #Tr = 2 #Tr = 3 #Tr = 5 #Tr = 7
Relation name B. P.M. B. P.M. B. P.M. B. P.M. B. P.M.
Attribution[N][S] ? 0.127 ? 0.237 ? 0.458 0.038 0.290 0.724 0.773
Attribution[S][N] ? 0.597 ? 0.449 0.009 0.639 0.250 0.721 0.579 0.623
Background[N][S] ? 0.113 ? ? ? 0.036 ? 0.095 ? 0.089
Cause[N][S] ? ? ? 0.128 ? ? ? 0.034 0.057 0.187
Comparison[N][S] ? 0.118 ? 0.037 ? ? 0.133 0.130 0.143 0.031
Condition[N][S] ? 0.041 ? 0.136 ? 0.113 ? 0.154 0.242 0.152
Condition[S][N] ? ? ? 0.122 0.133 0.148 0.214 0.233 0.390 0.308
Contrast[N][N] ? ? ? 0.086 ? 0.073 0.050 0.111 ? 0.109
Contrast[N][S] ? 0.071 ? ? ? 0.188 ? 0.087 ? 0.136
Elaboration[N][S] ? 0.134 ? 0.126 0.004 0.067 0.004 0.340 ? 0.165
Enablement[N][S] ? ? ? 0.462 ? 0.579 0.115 0.423 0.419 0.438
Joint[N][N] ? 0.030 ? 0.015 ? ? 0.016 0.059 0.015 0.155
Manner-Means[N][S] ? ? ? 0.056 ? 0.103 0.345 0.372 0.412 0.383
Summary[N][S] ? 0.429 ? 0.453 0.080 0.358 ? 0.349 0.154 0.471
Temporal[N][S] ? 0.158 ? ? ? 0.091 ? 0.052 0.204 0.101
Accuracy 0.000 0.110 0.000 0.105 0.004 0.146 0.034 0.222 0.122 0.213
Macro-average F-score 0.000 0.060 0.000 0.069 0.008 0.101 0.038 0.118 0.107 0.134
Table 1: F-scores for RSTDT relations, using a training set containing #Tr instances of each relation. B. indicates
F-score for baseline, P.M. for the proposed method. A boldface indicates the best classifier for each relation.
Although the distribution of discourse relations
in RSTDT and PDTB is not uniform, it is possi-
ble to study the performance of the proposed method
when all relations are made equally rare. We evalu-
ate performance on artificially-created training sets
containing an equal amount of each discourse rela-
tion. Table 1 contains the F-score for each RSTDT
relation, using training sets containing respectively
one, two, three, five and seven instances of each
relation. For space considerations, only relations
with significant results are shown. We observe that,
when using respectively one and two instances of
each relation, the baseline classifier is unable to de-
tect any relation, and has a macro-average F-score
of zero. Contrastingly, the classifier built with fea-
ture vector extension reaches in those cases an F-
score of 0.06. Furthermore, when employing the
proposed method, certain relations have relatively
high F-scores even with very little labeled data: With
one training instance, ATTRIBUTION[S][N] has an
F-score of 0.597, while SUMMARY[N][S] has an F-
score of 0.429. With three training instances, EN-
ABLEMENT[N][S] has an F-score of 0.579. When
the amount of each relation is increased, the baseline
classifier starts detecting more relations. In all cases,
the proposed method performs better in terms of ac-
curacy and macro-average F-score. With a train-
ing set containing seven instances of each relation,
the baseline?s macro-average F-score is starting to
get closer to the extended classifier?s, with superior
performances for several relations, such as COM-
PARISON[N][S], CONDITION[N][S], and TEMPO-
RAL[N][S]. Still, in this case, the extended classi-
fier?s accuracy is higher than the baseline (0.213 ver-
sus 0.122). Table 2 summarizes the outcome of the
same experiments performed on the PDTB dataset.
The results exhibit a similar trend, despite the base-
line classifier having a relatively high accuracy for
each case.
Using the data from Figures 2, 3 and 4, it is pos-
sible to calculate the relative score change occur-
ring when using the proposed method, as a func-
tion of the number of unseen features found in test
data. This graph is plotted in Figure 5. Besides
macro-average F-score, we additionally plot accu-
racy change. In the top subfigure, representing the
case of RSTDT, we see that, for the lowest amount
of unseen test features, the proposed method does
406
#Tr = 1 #Tr = 2 #Tr = 3 #Tr = 5 #Tr = 7
Relation name B. P.M. B. P.M. B. P.M. B. P.M. B. P.M.
Comparison.Concession[2][1] ? 0.056 ? ? ? 0.133 ? ? ? 0.154
Comparison.Contrast[2][1] ? ? ? 0.333 ? ? ? 0.190 0.105 0.368
Contingency.Cause[1][2] ? 0.013 ? 0.007 ? ? ? 0.026 ? 0.013
Contingency.Condition[1][2] ? 0.082 ? 0.160 ? 0.127 0.250 0.253 0.214 0.171
Contingency.Condition[2][1] ? ? ? ? ? 0.074 ? 0.143 0.250 0.296
Contingency.Prag. cond.[1][2] ? ? ? 0.133 ? 0.034 ? ? 0.133 0.043
Contingency.Prag. cond.[2][1] ? ? ? ? ? ? 0.133 0.087 0.154 0.087
Expansion.Conjunction[1][2] 0.326 0.352 0.326 0.351 0.326 0.368 0.332 0.371 0.335 0.384
Expansion.Instantiation[1][2] ? ? ? ? ? 0.042 ? 0.057 ? 0.131
Temporal.Asynchronous[1][2] ? 0.204 ? ? ? 0.142 0.039 0.148 ? 0.035
Temporal.Asynchronous[2][1] ? ? ? ? ? 0.316 ? 0.483 0.143 ?
Temporal.Synchrony[1][2] ? ? ? 0.032 ? 0.162 0.032 0.103 0.032 0.157
Temporal.Synchrony[2][1] ? ? ? 0.083 ? 0.143 0.200 0.308 0.211 0.174
Accuracy 0.195 0.201 0.195 0.202 0.195 0.212 0.202 0.214 0.204 0.213
Macro-average F-score 0.015 0.033 0.015 0.054 0.015 0.084 0.045 0.108 0.072 0.100
Table 2: F-scores for PDTB relations.
not bring any change in F-score or accuracy. In-
deed, as the number of unknown features is low,
feature vector extension is very limited, and does
not improve the performance compared to the base-
line. Then, a progressive increase of both accuracy
and macro-average F-score is observed, as the num-
ber of unseen test features is incremented. For in-
stance, for 8500 unseen test features, the macro-
average F-score increase (resp. accuracy increase)
is 25% (resp. 2.5%), while it is 20% (resp. 1%) for
11000 unseen test instances. These values reach a
maximum of 119% macro-average F-score increase,
and 66% accuracy increase, when 23500 features
unseen during training are present in test data. This
situation corresponds in Figures 3 and 4 to the case
of very small training sets. The bottom subfigure
of Figure 2, for the case of PDTB, reveals a sim-
ilar tendency. The macro-average F-score increase
(resp. accuracy increase) is negligible for 1000 un-
seen test features, while this increase is 21% for both
macro-average F-score and accuracy in the case of
9700 unseen test features, and 459% (resp. 630% for
accuracy) when 28000 unseen features are found in
test data. This shows that the proposed method is
useful when large numbers of features are missing
from the training set, which corresponds in practice
to small training sets, with few training instances for
each relation type. For large training sets, most fea-
tures are encountered by the classifier during train-
ing, and feature vector extension does not bring use-
ful information.
We empirically evaluate the effect of using differ-
ent amounts of unlabeled data on the performance of
the proposed method. We use respectively 100 and
10000 labeled training instances, create feature co-
occurrence matrices with different amounts of unla-
beled data, and evaluate the performance in relation
classification. Experimental results for RSTDT are
illustrated in Figure 6 (top). From Figure 6 it appears
clearly that macro-average F-scores improve with
increased number of unlabeled instances. However,
the benefit of using larger amounts of unlabeled data
is more pronounced when only a small number of la-
beled training instances are employed (ca. 100). In
fact, with 100 labeled training instances, the maxi-
mum improvement in F-score is 119% (corresponds
to using all our 100000 unlabeled instances). How-
ever, the maximum improvement in F-score with
10000 labeled training instances is small, only 2.5%
(corresponds to 10000 unlabeled instances).
The effect of using unlabeled data on PDTB rela-
tion classification is illustrated in Figure 6 (bottom).
Similarly, we consecutively set the labeled training
dataset size to 100 and 10000 instances, and plot the
macro-average F-score against the unlabeled dataset
size. As in the RSTDT experiment, the benefit of us-
407
0 5000 10000 15000 20000 25000Number of unseen features in test data
0
20
40
60
80
100
120
Sco
re c
han
ge o
ver 
bas
elin
e (%
) Accuracy change RSTDTMacro-av. F-score change RSTDT
0 5000 10000 15000 20000 25000 30000Number of unseen features in test data0
100
200
300
400
500
600
700
Sco
re c
han
ge o
ver 
bas
elin
e (%
) Accuracy change PDTBMacro-av. F-score change PDTB
Figure 5: Score change as a function of unseen test fea-
tures for RSTDT (top) and PDTB (bottom).
ing unlabeled data is more obvious when the num-
ber of labeled training instances is small. In par-
ticular, with 100 training instances, the maximum
improvement in F-score is 459% (corresponds to
100000 unlabeled instances). However, with 10000
labeled training instances the maximum improve-
ment in F-score is 15% (corresponds to 100 unla-
beled instances). These results confirm that, on the
one hand performance improvement is more promi-
nent for smaller training sets, and that on the other
hand, performance is increased when using larger
amounts of unlabeled data.
5 Conclusion
We presented a semi-supervised method which ex-
ploits the co-occurrence of features in unlabeled
data, to extend feature vectors during training and
testing in a discourse relation classifier. Despite the
0.00
0.05
0.10
0.15
0.20
0.25
Mac
ro-a
vera
ge F
-sco
re
101 102 103 104 105Number of unlabeled instances
RSTDT (100)Baseline RSTDT (100)RSTDT (10000)Baseline RSTDT (10000)
0.10
0.05
0.00
0.05
0.10
0.15
0.20
0.25
Mac
ro-a
vera
ge F
-sco
re
101 102 103 104 105Number of unlabeled instances
PDTB (100)Baseline PDTB (100)PDTB (10000)Baseline PDTB (10000)
Figure 6: Macro-average F-score for RSTDT (top) and
PDTB (bottom), for 100 and 10000 training instances,
against the number of unlabeled instances.
simplicity of the proposed method, it significantly
improved the macro-average F-score in discourse re-
lation classification for small training datasets, con-
taining low-occurrence relations. We performed an
evaluation on two popular datasets, the RSTDT and
PDTB. We empirically evaluated the benefit of using
a variable amount of unlabeled data for the proposed
method. Although the macro-average F-scores of
the classifiers described are too low to be used di-
rectly as discourse analyzers, the gain in F-score and
accuracy for small labeled datasets are a promising
perspective for improving classification accuracy for
infrequent relation types. In particular, the proposed
method can be employed in existing discourse clas-
sifiers that work well on popular relations, and be
expected to improve the overall accuracy.
408
References
R. Basili, M. Cammisa, and A. Moschitti. 2006. A se-
mantic kernel to classify texts with very few training
examples. Informatica (Slovenia), 30(2):163?172.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. In Proc. of
ICDM?06, pages 808?812.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of Rhetorical Structure Theory. Proc. of Second SIG-
dial Workshop on Discourse and Dialogue-Volume 16,
pages 1?10.
N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. La-
tent semantic kernels. Journal of Intelligent Informa-
tion Systems, 18:127?152.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
D. A. duVerle and H. Prendinger. 2009. A novel dis-
course parser based on Support Vector Machine clas-
sification. In Proc. of ACL?09, pages 665?673.
H. Fang. 2008. A re-examination of query expansion us-
ing lexical resources. In Proc. of ACL?08, pages 139?
147.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A
sequential model for discourse segmentation. In Proc.
of CICLing?10, pages 315?326.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in Neural Information Processing Systems,
volume 15. MIT Press.
Z. Lin, M-Y. Kan, and H. T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In Proc. of EMNLP?09, pages 343?351.
E. Loper and S. Bird. 2002. NLTK: The natural lan-
guage toolkit. In Proc. of ACL?02 Workshop on Effec-
tive tools and methodologies for teaching natural lan-
guage processing and computational linguistics, pages
63?70.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. Proc. of ACL?95, pages 276?283.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language processing. MIT Press.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc. of
ACL?02, pages 368?375.
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
N. Okazaki. 2009. Classias: A collection of machine-
learning algorithms for classification. http://
www.chokkan.org/software/classias/.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse rela-
tions. In Proc. of COLING?08 (Posters), pages 87?90.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of ACL?09, pages 683?691.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. Generating dialogues between virtual agents au-
tomatically from text. In Proc. of IVA?07, pages 161?
174.
R. L. Plackett. 1983. Karl Pearson and the chi-squared
test. International Statistical Review / Revue Interna-
tionale de Statistique, 51(1):59?72.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The Penn Discourse
TreeBank 2.0. In Proc. of LREC?08.
R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber. 2008b. The Penn Dis-
course Treebank 2.0 annotation manual. Technical re-
port, University of Pennsylvania Institute for Research
in Cognitive Science.
G. Salton and C. Buckley. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill Book Company.
G. Siolas and F. d?Alche?-Buc. 2000. Support Vector Ma-
chines based on a semantic kernel for text categoriza-
tion. In Proc. of IJCNN?00, volume 5, page 5205.
S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor.
2009. Supervised and unsupervised methods in em-
ploying discourse relations for improving opinion po-
larity classification. In Proc. of EMNLP?09, pages
170?179.
R. Soricut and D. Marcu. 2003. Sentence level discourse
parsing using syntactic and lexical information. Proc.
of NA-ACL?03, 1:149?156.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York, Inc.
409
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 80?88,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
@AM: Textual Attitude Analysis Model 
 
 
Alena Neviarouskaya Helmut Prendinger Mitsuru Ishizuka
University of Tokyo Nat. Institute of Informatics University of Tokyo 
7-3-1 Hongo, Bunkyo-ku 2-1-2 Hitotsubashi Chiyoda 7-3-1 Hongo, Bunkyo-ku 
Tokyo 113-8656, Japan Tokyo 101-8430, Japan Tokyo 113-8656, Japan 
lena@mi.ci.i.u-tokyo.ac.jp helmut@nii.ac.jp ishizuka@i.u-tokyo.ac.jp
 
 
 
 
 
 
Abstract 
The automatic analysis and classification of 
text using fine-grained attitude labels is the 
main task we address in our research. The de-
veloped @AM system relies on compositio-
nality principle and a novel approach based on 
the rules elaborated for semantically distinct 
verb classes. The evaluation of our method on 
1000 sentences, that describe personal expe-
riences, showed promising results: average 
accuracy on fine-grained level was 62%, on 
middle level ? 71%, and on top level ? 88%. 
1 Introduction and Related Work 
With rapidly growing online sources aimed at en-
couraging and stimulating people?s discussions 
concerning personal, public or social issues (news, 
blogs, discussion forums, etc.), there is a great 
need in development of a computational tool for 
the analysis of people?s attitudes. According to the 
Appraisal Theory (Martin and White, 2005), atti-
tude types define the specifics of appraisal being 
expressed: affect (personal emotional state), judg-
ment (social or ethical appraisal of other?s behav-
iour), and appreciation (evaluation of phenomena). 
To analyse contextual sentiment (polarity) of a 
phrase or a sentence, rule-based approaches (Na-
sukawa and Yi, 2003; Mulder et al, 2004; Moila-
nen and Pulman, 2007; Subrahmanian and 
Reforgiato, 2008), a machine-learning method us-
ing not only lexical but also syntactic features 
(Wilson et al, 2005), and a model of integration of 
machine learning approach with compositional 
semantics (Choi and Cardie, 2008) were proposed. 
With the aim to recognize fine-grained emotions 
from text on the level of distinct sentences, re-
searchers have employed a keyword spotting tech-
nique (Olveres et al, 1998; Chuang and Wu, 2004; 
Strapparava et al, 2007), a technique calculating 
emotion scores using Pointwise Mutual Informa-
tion (PMI) (Kozareva et al, 2007), an approach 
inspired by common-sense knowledge (Liu et al, 
2003), rule-based linguistic approaches (Boucou-
valas, 2003; Chaumartin, 2007), machine-learning 
methods (Alm, 2008; Aman and Szpakowicz, 
2008; Strapparava and Mihalcea, 2008), and an 
ensemble based multi-label classification technique 
(Bhowmick et al, 2009). 
Early attempts to focus on distinct attitude types 
in the task of attitude analysis were made by Ta-
boada and Grieve (2004), who determined a poten-
tial value of adjectives for affect, judgement and 
appreciation by calculating the PMI with the pro-
noun-copular pairs ?I was (affect)?, ?He was 
(judgement)?, and ?It was (appreciation)?, and 
Whitelaw et al (2005), who used machine learning 
technique (SVM) with fine-grained semantic dis-
tinctions in features (attitude type, orientation) in 
combination with ?bag of words? to classify movie 
reviews. However, the concentration only on ad-
jectives, that express appraisal, and their modifiers, 
greatly narrows the potential of the Whitelaw et 
al.?s (2005) approach. 
In this paper we introduce our system @AM 
(ATtitude Analysis Model), which (1) classifies 
80
sentences according to the fine-grained attitude 
labels (nine affect categories (Izard, 1971): ?anger?, 
?disgust?, ?fear?, ?guilt?, ?interest?, ?joy?, ?sadness?, 
?shame?, ?surprise?; four polarity labels for judg-
ment and appreciation: ?POS jud?, ?NEG jud?, 
?POS app?, ?NEG app?; and ?neutral?); (2) assigns 
the strength of the attitude; and (3) determines the 
level of confidence, with which the attitude is ex-
pressed. @AM relies on compositionality principle 
and a novel approach based on the rules elaborated 
for semantically distinct verb classes. 
2 Lexicon for Attitide Analysis 
We built the lexicon for attitude analysis that in-
cludes: (1) attitude-conveying terms; (2) modifiers; 
(3) ?functional? words; and (4) modal operators. 
2.1 The Core of Lexicon 
As a core of lexicon for attitude analysis, we em-
ploy Affect database and extended version of Sen-
tiFul database developed by Neviarouskaya et al 
(2009). The affective features of each emotion-
related word are encoded using nine emotion labels 
(?anger?, ?disgust?, ?fear?, ?guilt?, ?interest?, ?joy?, 
?sadness?, ?shame?, and ?surprise?) and correspond-
ing emotion intensities that range from 0.0 to 1.0. 
The original version of SentiFul database, which 
contains sentiment-conveying adjectives, adverbs, 
nouns, and verbs annotated by sentiment polarity, 
polarity scores and weights, was manually ex-
tended using attitude labels. Some examples of 
annotated attitude-conveying words are listed in 
Table 1. It is important to note here that some 
words could express different attitude types (affect, 
judgment, appreciation) depending on context; 
such lexical entries were annotated by all possible 
categories. 
 
POS Word Category Intensity 
adjective honorable 
unfriendly 
POS jud 
NEG aff (sadness) 
NEG jud 
NEG app 
0.3 
0.5 
0.5 
0.5 
adverb gleefully POS aff (joy) 0.9 
noun abnormality NEG app 0.25 
verb frighten 
desire 
NEG aff (fear) 
POS aff (interest) 
POS aff (joy) 
0.8 
1.0 
0.5 
Table 1. Examples of attitude-conveying words and 
their annotations. 
2.2 Modifiers and Functional Words 
The robust attitude analysis method should rely not 
only on attitude-conveying terms, but also on mod-
ifiers and contextual valence shifters (term intro-
duced by Polanyi and Zaenen (2004)), which are 
integral parts of our lexicon. 
We collected 138 modifiers that have an impact 
on contextual attitude features of related words, 
phrases, or clauses. They include: 
1. Adverbs of degree (e.g., ?significantly?, 
?slightly? etc.) and adverbs of affirmation (e.g., 
?absolutely?, ?seemingly?) that have an influence on 
the strength of attitude of the related words. 
2. Negation words (e.g., ?never?, ?nothing? 
etc.) that reverse the polarity of related statement. 
3. Adverbs of doubt (e.g., ?scarcely?, ?hardly? 
etc.) and adverbs of falseness (e.g., ?wrongly? etc.) 
that reverse the polarity of related statement. 
4. Prepositions (e.g., ?without?, ?despite? etc.) 
that neutralize the attitude of related words. 
5. Condition operators (e.g., ?if?, ?even though? 
etc.) that neutralize the attitude of related words. 
Adverbs of degree and adverbs of affirmation af-
fect on related verbs, adjectives, or another adverb. 
Two annotators gave coefficients for intensity de-
gree strengthening or weakening (from 0.0 to 2.0) 
to each of 112 collected adverbs, and the result was 
averaged (e.g., coeff(?perfectly?) = 1.9, 
coeff(?slightly?) = 0.2). 
We distinguish two types of ?functional? words 
that influence contextual attitude and its strength:  
1. Intensifying adjectives (e.g., ?rising?, ?rap-
idly-growing?), nouns (e.g., ?increase?, ?up-tick?), 
and verbs (e.g., ?to grow?, ?to rocket?), which in-
crease the strength of attitude of related words. 
2. Reversing adjectives (e.g., ?reduced?), nouns 
(e.g., ?termination?, ?reduction?), and verbs (e.g., 
?to decrease?, ?to limit?, ?to diminish?), which re-
verse the prior polarity of related words. 
2.3 Modal Operators 
Consideration of the modal operators in the tasks 
of opinion mining and attitude analysis is very im-
portant, as they indicate a degree of person?s belief 
in the truth of the proposition, which is subjective 
in nature. Modal expressions point to likelihood 
and clearly involve the speaker?s judgment (Hoye, 
1997). Modals are distinguished by the confidence 
level. 
81
We collected modal operators of two categories:  
1. Modal verbs (13 verbs). 
2. Modal adverbs (61 adverbs). 
Three human annotators assigned the confidence 
level, which ranges from 0.0 to 1.0, to each modal 
verb and adverb; these ratings were averaged (e.g., 
conf(?vaguely?) = 0.17, conf(?may?) = 0.27, 
conf(?arguably?) = 0.63, conf(?would?) = 0.8, 
conf(?veritably?) = 1.0).  
3 Compositionality Principle 
Words in a sentence are interrelated and, hence, 
each of them can influence the overall meaning 
and attitudinal bias of a statement. The algorithm 
for the attitude classification is designed based on 
the compositionality principle, according to which 
we determine the attitudinal meaning of a sentence 
by composing the pieces that correspond to lexical 
units or other linguistic constituent types governed 
by the rules of polarity reversal, aggregation (fu-
sion), propagation, domination, neutralization, and 
intensification, at various grammatical levels. 
Polarity reversal means that phrase or statement 
containing attitude-conveying term/phrase with 
prior positive polarity becomes negative, and vice 
versa. The rule of polarity reversal is applied in 
three cases: (1) negation word-modifier in relation 
with attitude-conveying statement (e.g., ?never? & 
POS(?succeed?) => NEG(?never succeed?)); (2) 
adverb of doubt in relation with attitude-conveying 
statement (e.g., ?scarcely? & POS(?relax?) => 
NEG(?scarcely relax?)); (3) functional word of 
reversing type in relation with attitude-conveying 
statement (e.g., adjective ?reduced? & 
POS(?enthusiasm?) => NEG(?reduced enthusi-
asm?)). In the case of judgment and appreciation, 
the use of polarity reversal rule is straightforward 
(?POS jud? <=> ?NEG jud?, ?POS app? <=> ?NEG 
app?). However, it is not trivial to find pairs of op-
posite emotions in the case of a fine-grained classi-
fication, except for ?joy? and ?sadness?. Therefore, 
we assume that (1) opposite emotion for three posi-
tive emotions, such as ?interest?, ?joy?, and ?sur-
prise?, is ?sadness? (?POS aff? => ?sadness?); and 
(2) opposite emotion for six negative emotions, 
such as ?anger?, ?disgust?, ?fear?, ?guilt?, ?sadness?, 
and ?shame?, is ?joy? (?NEG aff? => ?joy?). 
The rules of aggregation (fusion) are as follows: 
(1) if polarities of attitude-conveying terms in ad-
jective-noun, noun-noun, adverb-adjective, adverb-
verb phrases have opposite directions, mixed po-
larity with dominant polarity of a descriptive term 
is assigned to the phrase (e.g., POS(?beautiful?) & 
NEG(?fight?) => POS-neg(?beautiful fight?); 
NEG(?shamelessly?) & POS(?celebrate?) => NEG-
pos(?shamelessly celebrate?)); otherwise (2) the 
resulting polarity is based on the equal polarities of 
terms, and the strength of attitude is measured as a 
maximum between polarity scores (intensities) of 
terms (max(score1,score2)).  
The rule of propagation is useful, as proposed in 
(Nasukawa and Yi, 2003), for the task of detection 
of local sentiments for given subjects. ?Propaga-
tion? verbs propagate the sentiment towards the 
arguments; ?transfer? verbs transmit sentiments 
among the arguments. The rule of propagation is 
applied when verb of ?propagation? or ?transfer? 
type is used in a phrase/clause and sentiment of an 
argument that has prior neutral polarity needs to be 
investigated (e.g., PROP-POS(?to admire?) & ?his 
behaviour? => POS(?his behaviour?); ?Mr. X? & 
TRANS(?supports?) & NEG(?crime business?) => 
NEG(?Mr. X?)).  
The rules of domination are as follows: (1) if po-
larities of verb (this rule is applied only for certain 
classes of verbs) and object in a clause have oppo-
site directions, the polarity of verb is prevailing 
(e.g., NEG(?to deceive?) & POS(?hopes?) => 
NEG(?to deceive hopes?)); (2) if compound sen-
tence joints clauses using coordinate connector 
?but?, the attitude features of a clause following 
after the connector are dominant (e.g., ?NEG(It 
was hard to climb a mountain all night long), but 
POS(a magnificent view rewarded the traveler at 
the morning).? => POS(whole sentence)). 
The rule of neutralization is applied when 
preposition-modifier or condition operator relate to 
the attitude-conveying statement (e.g., ?despite? & 
NEG(?worries?) => NEUT(?despite worries?)). 
The rule of intensification means strengthening 
or weakening of the polarity score (intensity), and 
is applied when: 
1. adverb of degree or affirmation relates to at-
titude-conveying term (e.g., Pos_score(?extremely 
happy?) > Pos_score(?happy?)); 
2. adjective or adverb is used in a comparative 
or superlative form (e.g., Neg_score(?sad?) < 
Neg_score(?sadder?) < Neg_score (?saddest?)). 
Our method is capable of processing sentences of 
different complexity, including simple, compound, 
complex (with complement and relative clauses), 
82
and complex-compound sentences. To understand 
how words and concepts relate to each other in a 
sentence, we employ Connexor Machinese Syntax 
parser (http://www.connexor.eu/) that 
returns lemmas, parts of speech, dependency func-
tions, syntactic function tags, and morphological 
tags. When handling the parser output, we repre-
sent the sentence as a set of primitive clauses. Each 
clause might include Subject formation, Verb for-
mation and Object formation, each of which may 
consist of a main element (subject, verb, or object) 
and its attributives and complements. For the 
processing of complex or compound sentences, we 
build a so-called ?relation matrix?, which contains 
information about dependences (e.g., coordination, 
subordination, condition, contingency, etc.) be-
tween different clauses in a sentence. 
The annotations of words are taken from our at-
titude-conveying lexicon. The decision on most 
appropriate label, in case of words with multiple 
annotations (e.g., word ?unfriendly? in Table 1), is 
made based on (1) the analysis of morphological 
tags of nominal heads and their premodifiers in the 
sentence (e.g., first person pronoun, third person 
pronoun, demonstrative pronoun, nominative or 
genitive noun, etc.); (2) the analysis of the se-
quence of hypernymic semantic relations of a par-
ticular noun in WordNet (Miller, 1990), which 
allows to determine its conceptual domain (e.g., 
?person, human being?, ?artifact?, ?event?, etc.). 
For ex., ?I feel highly unfriendly attitude towards 
me? conveys ?NEG aff? (?sadness?), while ?Shop 
assistant?s behavior was really unfriendly? and 
?Plastic bags are environment unfriendly? express 
?NEG jud? and ?NEG app?, correspondingly. 
While applying the compositionality principle, 
we consecutively assign attitude features to words, 
phrases, formations, clauses, and finally, to the 
whole sentence. 
4 Consideration of the Semantics of Verbs 
All sentences must include a verb, because the 
verb tells us what action the subject is performing 
and object is receiving. In order to elaborate rules 
for attitude analysis based on the semantics of 
verbs, we investigated VerbNet (Kipper et al, 
2007), the largest on-line verb lexicon that is orga-
nized into verb classes characterized by syntactic 
and semantic coherence among members of a 
class. Based on the thorough analysis of 270 first-
level classes of VerbNet and their members, 73 
verb classes (1) were found useful for the task of 
attitude analysis, and (2) were further classified 
into 22 classes differentiated by the role that mem-
bers play in attitude analysis and by rules applied 
to them. Our classification is shown in Table 2. 
 
Verb class (verb samples) 
1 Psychological state or emotional reaction 
1.1 Object-centered (oriented) emotional state (adore, re-
gret) 
1.2 Subject-driven change in emotional state (trans.)
(charm, inspire, bother) 
1.3 Subject-driven change in emotional state (intrans.) (ap-
peal to, grate on) 
2 Judgment 
2.1 Positive judgment (bless, honor) 
2.2 Negative judgment (blame, punish) 
3 Favorable attitude (accept, allow, tolerate) 
4 Adverse (unfavorable) attitude (discourage, elude, forbid) 
5 Favorable or adverse calibratable changes of state (grow, 
decline) 
6 Verbs of removing 
6.1 Verbs of removing with neutral charge (delete, remove)
6.2 Verbs of removing with negative charge (deport, expel)
6.3 Verbs of removing with positive charge (evacuate, 
cure) 
7 Negatively charged change of state (break, crush, smash) 
8 Bodily state and damage to the body (sicken, injure) 
9 Aspectual verbs 
9.1 Initiation, continuation of activity, and sustaining (be-
gin, continue, maintain) 
9.2 Termination of activity (quit, finish) 
10 Preservation (defend, insure) 
11 Verbs of destruction and killing (damage, poison) 
12 Disappearance (disappear, die) 
13 Limitation and subjugation (confine, restrict) 
14 Assistance (succor, help) 
15 Obtaining (win, earn) 
16 Communication indicator/reinforcement of attitude (guess, 
complain, deny) 
17 Verbs of leaving (abandon, desert) 
18 Changes in social status or condition (canonize, widow) 
19 Success and failure 
19.1 Success (succeed, manage) 
19.2 Failure (fail, flub) 
20 Emotional nonverbal expression (smile, weep) 
21 Social interaction (marry, divorce) 
22 Transmitting verbs (supply, provide) 
Table 2. Verb classes defined for attitude analysis. 
 
For each of our verb classes, we developed set 
of rules that are applied to attitude analysis on the 
phrase/clause-level. Some verb classes include 
verbs annotated by attitude type, prior polarity 
orientation, and the strength of attitude: ?Psycho-
logical state or emotional reaction?, ?Judgment?, 
?Verbs of removing with negative charge?, ?Verbs 
83
of removing with positive charge?, ?Negatively 
charged change of state?, ?Bodily state and dam-
age to the body?, ?Preservation?, and others. The 
attitude features of phrases, which involve posi-
tively or negatively charged verbs from such 
classes, are context-sensitive, and are defined by 
means of rules designed for each of the class. 
As an example, below we provide short descrip-
tion and rules elaborated for the subclass ?Object-
centered (oriented) emotional state?. 
Features: subject experiences emotions towards 
some stimulus; verb prior polarity: positive or neg-
ative; context-sensitive. 
Verb-Object rules (subject is ignored): 
1. ?Interior perspective? (subject?s inner emotion 
state or attitude): 
S & V+(?admires?) & O+(?his brave heart?) => 
(fusion, max(V_score,O_score)) => ?POS aff?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence dominance, V_score) => ?POS aff?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, V_score) => ?NEG aff?. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (fusion, max(V_score,O_score)) => ?NEG aff?. 
2. ?Exterior perspective? (social/ethical judgment): 
S & V+(?admires?) & O+(?his brave heart?) => 
(fusion, max(V_score,O_score)) => ?POS jud?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence reversal, max(V_score,O_score)) => 
?NEG jud?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, max(V_score,O_score)) 
=> ?NEG jud?. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (verb valence reversal, max(V_score,O_score)) 
=> ?POS jud?. 
3. In case of neutral object => attitude type and 
prior polarity of verb, verb score (V_score). 
Verb-PP (prepositional phrase) rules: 
1. In case of negatively charged verb and PP start-
ing with ?from? => verb valence dominance:  
S & V-(?suffers?) & PP-(?from illness?) => inte-
rior: ?NEG aff?; exterior: ?NEG jud?. 
S & V-(?suffers?) & PP+ (?from love?) => inte-
rior: ?NEG aff?; exterior: ?NEG jud?. 
2. In case of positively charged verb and PP start-
ing with ?in?/?for?, treat PP same as object (see 
above): 
S & V+(?believes?) & PP-(?in evil?) => interior: 
?POS aff?; exterior: ?NEG jud?. 
S & V+(?believes?) & PP+(?in kindness?) => in-
terior: ?POS aff?; exterior: ?POS jud?. 
In the majority of rules the strength of attitude is 
measured as a maximum between attitude scores of 
a verb and an object (max(V_score,O_score)), be-
cause strength of overall attitude depends on both 
scores. For example, attitude conveyed by ?to suf-
fer from grave illness? is stronger than that of ?to 
suffer from slight illness?. 
In contrast to the rules of ?Object-centered 
(oriented) emotional state? subclass, which ignore 
attitude features of a subject in a sentence, the rules 
elaborated for the ?Subject-driven change in emo-
tional state (trans.)? disregard the attitude features 
of object, as in sentences involving members of 
this subclass object experiences emotion, and sub-
ject causes the emotional state. For example (due 
to limitation of space, here and below we provide 
only some cases): 
S(?Classical music?) & V+(?calmed?) & O-
(?disobedient child?) => interior: ?POS aff?; exte-
rior: ?POS app?. 
S-(?Fatal consequences of GM food intake?) & 
V-(?frighten?) & O(?me?) => interior: ?NEG aff?; 
exterior: ?NEG app?. 
The Verb-Object rules for the subclasses ?Positive 
judgment? and ?Negative judgment? (verbs from 
?Judgment? class relate to a judgment or opinion 
that someone may have in reaction to something) 
are very close to those defined for the subclass 
?Object-centered (oriented) emotional state?. 
However, Verb-PP rules have some specifics: for 
both positive and negative judgment verbs, we 
treat PP starting with ?for?/?of?/?as? same as object 
in Verb-Object rules. For example: 
S(?He?) & V-(?blamed?) & O+(?innocent per-
son?) => interior: ?NEG jud?; exterior: ?NEG jud?. 
S(?They?) & V-(?punished?) & O(?him?) & PP-
(?for his misdeed?) => interior: ?NEG jud?; exte-
rior: ?POS jud?. 
Verbs from classes ?Favorable attitude? and ?Ad-
verse (unfavorable) attitude? have prior neutral 
polarity and positive or negative reinforcement, 
correspondingly, that means that they only impact 
on the polarity and strength of non-neutral phrase 
(object in a sentence written in active voice, or 
subject in a sentence written in passive voice, or 
PP in case of some verbs).  
Rules: 
1. If verb belongs to the ?Favorable attitude? class 
and the polarity of phrase is not neutral, then the 
84
attitude score of the phrase is intensified (we use 
symbol ?^? to indicate intensification): 
S(?They?) & [V pos. reinforcement](?elected?) & 
O+(?fair judge?) => ?POS app?; O_score^. 
S(?They?) & [V pos. reinforcement](?elected?) & 
O-(?corrupt candidate?) => ?NEG app?; O_score^. 
2. If verb belongs to the ?Adverse (unfavorable) 
attitude? class and the polarity of phrase is not neu-
tral, then the polarity of phrase is reversed and 
score is intensified: 
S(?They?) & [V neg. reinforcement](?prevented?) 
& O-(?the spread of disease?) => ?POS app?; 
O_score^. 
S+(?His achievements?) & [V neg. reinforce-
ment](?were overstated?) => ?NEG app?; S_score^. 
Below are examples of processing the sentences 
with verbs from ?Verbs of removing? class. 
?Verbs of removing with neutral charge?: 
S(?The tape-recorder?) & [V neutral 
rem.](?automatically ejects?) & O-neutral(?the 
tape?) => neutral. 
S(?The safety invention?) & [V neutral 
rem.](?ejected?) & O(?the pilot?) & PP-(?from 
burning plane?) => ?POS app?; PP_score^. 
?Verbs of removing with negative charge?: 
S(?Manager?) & [V neg. rem.](?fired?) & O-
(?careless employee?) & PP(?from the company?) 
=> ?POS app?; max(V_score,O_score).  
?Verbs of removing with positive charge?: 
S(?They?) & [V pos. rem.](?evacuated?) & 
O(?children?) & PP-(?from dangerous place?) => 
?POS app?; max(V_score,PP_score). 
Along with modal verbs and modal adverbs, mem-
bers of the ?Communication indica-
tor/reinforcement of attitude? verb class also 
indicate the confidence level or degree of certainty 
concerning given opinion.  
Features: subject (communicator) expresses state-
ment with/without attitude; statement is PP starting 
with ?of?, ?on?, ?against?, ?about?, ?concerning?, 
?regarding?, ?that?, ?how? etc.; ground: positive or 
negative; reinforcement: positive or negative. 
Rules: 
1. If the polarity of expressed statement is neutral, 
then the attitude is neutral: 
S(?Professor?) & [V pos. ground, pos. rein-
forcement, confidence:0.83](?dwelled?) & PP-
neutral(?on a question?) => neutral. 
2. If the polarity of expressed statement is not neu-
tral and the reinforcement is positive, then the po-
larity score of the statement (PP) is intensified: 
S(?Jane?) & [V neg. ground, pos. reinforcement, 
confidence:0.8](?is complaining?) & PP-(?of a 
headache again?) => ?NEG app?; PP_score^; con-
fidence:0.8. 
3. If the polarity of expressed statement is not neu-
tral and reinforcement is negative, then the polarity 
of the statement (PP) is reversed and score is inten-
sified: 
S(?Max?) & [V neg. ground, neg. reinforcement, 
confidence:0.2](?doubt?) & PP-{?that? S+(?his 
good fortune?) & [V termination](?will ever end?)} 
=> ?POS app?; PP_score^; confidence:0.2.  
In the last example, to measure the sentiment of 
PP, we apply rule for the verb ?end? from the 
?Termination of activity? class, which reverses the 
non-neutral polarity of subject (in intransitive use 
of verb) or object (in transitive use of verb). For 
example, the polarity of the following sentence 
with positive PP is negative: ?They discontinued 
helping children?. 
5 Evaluation 
In order to evaluate the performance of our algo-
rithm, we conducted experiment on the set of sen-
tences extracted from personal stories about life 
experiences that were anonymously published on 
the social networking website Experience Project 
(www.experienceproject.com). This web-
site represents an interactive platform that allows 
people to share personal experiences, thoughts, 
opinions, feelings, passions, and confessions 
through the network of personal stories. With over 
4 million experiences accumulated (as of February 
2010), Experience Project is a perfect source for 
researchers interested in studying different types of 
attitude expressed through text. 
5.1 Data Set Description 
For our experiment we extracted 1000 sentences 
from various stories grouped by topics within 13 
different categories, such as ?Arts and entertain-
ment?, ?Current events?, ?Education?, ?Family and 
friends?, ?Health and wellness?, ?Relationships 
and romance? and others, on the Experience 
Project. Sentences were collected from 358 dis-
tinct topic groups, such as ?I still remember Sep-
tember 11?, ?I am intelligent but airheaded?, ?I 
think bullfighting is cruel?, ?I quit smoking?, ?I am 
a fashion victim?, ?I was adopted? and others. 
85
We considered three hierarchical levels of atti-
tude labels in our experiment (see Figure 1). Three 
independent annotators labeled the sentences with 
one of 14 categories from ALL level and a corres-
ponding score (the strength or intensity value). 
These annotations were further interpreted using 
labels from MID and TOP levels. Fleiss? Kappa 
coefficient was used as a measure of reliability of 
human raters? annotations. The agreement coeffi-
cient on 1000 sentences was 0.53 on ALL level, 
0.57 on MID level, and 0.73 on TOP level. 
Only those sentences, on which at least two out 
of three human raters completely agreed, were in-
cluded in the ?gold standard? for our experiment. 
Three ?gold standards? were created according to 
the hierarchy of attitude labels. Fleiss? Kappa coef-
ficients are 0.62, 0.63, and 0.74 on ALL, MID, and 
TOP levels, correspondingly. Table 3 shows the 
distributions of labels in the ?gold standards?. 
 
ALL level MID level 
Label Number Label Number 
anger 45 POS aff 233 
disgust 21 NEG aff 332 
fear 54 POS jud 66 
guilt 22 NEG jud 78 
interest 84 POS app 100 
joy 95 NEG app 29 
sadness 133 neutral 87 
shame 18 total 925 
surprise 36  
POS jud 66 TOP level 
NEG jud 78 Label Number 
POS app 100 POS 437 
NEG app 29 NEG 473 
neutral 87 neutral 87 
total 868 total 997 
Table 3. Label distributions in the ?gold standards?. 
5.2 Results 
After processing each sentence from the data set by 
our system, we measured averaged accuracy, pre-
cision, recall, and F-score for each label within 
ALL, MID, and TOP levels. The results are shown 
in Table 4. The ratio of the most frequent attitude 
label in the ?gold standard? was considered as the 
baseline. As seen from the obtained results, our 
algorithm performed with high accuracy signifi-
cantly surpassing the baselines on all levels of atti-
tude hierarchy (except ?neutral? category on the 
TOP level, which is probably due to the unbal-
anced distribution of labels in the ?gold standard?, 
where ?neutral? sentences constitute less than 9%). 
 
ALL level 
Baseline 0.153 
Label Accuracy Precision Recall F-score 
anger 
0.621 
0.818 0.600 0.692 
disgust 0.818 0.857 0.837 
fear 0.768 0.796 0.782 
guilt 0.833 0.455 0.588 
interest 0.772 0.524 0.624 
joy 0.439 0.905 0.591 
sadness 0.528 0.917 0.670 
shame 0.923 0.667 0.774 
surprise 0.750 0.833 0.789 
POS jud 0.824 0.424 0.560 
NEG jud 0.889 0.410 0.561 
POS app 0.755 0.400 0.523 
NEG app 0.529 0.310 0.391 
neutral 0.559 0.437 0.490 
MID level 
Baseline 0.359 
Label Accuracy Precision Recall F-score 
POS aff 
0.709 
0.668 0.888 0.762 
NEG aff 0.765 0.910 0.831 
POS jud 0.800 0.424 0.554 
NEG jud 0.842 0.410 0.552 
POS app 0.741 0.400 0.519 
NEG app 0.474 0.310 0.375 
neutral 0.514 0.437 0.472 
TOP level 
Baseline 0.474 
Label Accuracy Precision Recall F-score 
POS 
0.879 
0.918 0.920 0.919 
NEG 0.912 0.922 0.917 
neutral 0.469 0.437 0.452 
Table 4. Results of the system performance evaluation. 
 
In the case of fine-grained attitude recognition 
(ALL level), the highest precision was obtained for 
?shame? (0.923) and ?NEG jud? (0.889), while the 
highest recall was received for ?sadness? (0.917) 
    
TOP POS NEG neutral
    
MID POS aff POS jud 
POS 
app NEG aff 
NEG 
jud 
NEG 
app neutral
        
ALL interest joy surprise POS jud 
POS 
app anger disgust fear guilt sadness shame 
NEG 
jud 
NEG 
app neutral
 
Figure 1. Hierarchy of attitude labels. 
 
86
and ?joy? (0.905) emotions at the cost of low preci-
sion (0.528 and 0.439, correspondingly). The algo-
rithm performed with the worst results in 
recognition of ?NEG app? and ?neutral?. 
The analysis of a confusion matrix for the ALL 
level revealed the following top confusions of our 
system (see Table 5): (1) ?anger?, ?fear?, ?guilt?, 
?shame?, ?NEG jud?, ?NEG app? and ?neutral? were 
predominantly incorrectly predicted as ?sadness? 
(for ex., @AM resulted in ?sadness? for the sen-
tence ?I know we have several months left before 
the election, but I am already sick and tired of see-
ing the ads on TV?, while human annotations were 
?anger?/?anger?/?disgust?); (2) ?interest?, ?POS jud? 
and ?POS app? were mostly confused with ?joy? by 
our algorithm (e.g., @AM classified the sentence 
?It?s one of those life changing artifacts that we 
must have in order to have happier, healthier lives? 
as ?joy?(-ful), while human annotations were ?POS 
app?/?POS app?/?interest?). 
 
Actual 
label 
Incorrectly predicted labels (%), in descending 
order 
anger sadness (28.9%), joy (4.4%), neutral (4.4%), 
NEG app (2.2%) 
disgust anger (4.8%), sadness (4.8%), NEG jud (4.8%) 
fear sadness (13%), joy (5.6%), POS app (1.9%) 
guilt sadness (50%), anger (4.5%) 
interest joy (33.3%), neutral (7.1%), sadness (3.6%), POS 
app (2.4%), fear (1.2%) 
joy interest (3.2%), POS app (3.2%), sadness (1.1%), 
surprise (1.1%), neutral (1.1%) 
sadness neutral (3.8%), joy (1.5%), anger (0.8%), fear 
(0.8%), guilt (0.8%), NEG app (0.8%) 
shame sadness (16.7%), fear (5.6%), guilt (5.6%), NEG 
jud (5.6%) 
surprise fear (5.6%), neutral (5.6%), joy (2.8%), POS jud 
(2.8%) 
POS jud joy (37.9%), POS app (9.1%), interest (4.5%), 
sadness (1.5%), surprise (1.5%), NEG jud 
(1.5%), neutral (1.5%) 
NEG jud sadness (37.2%), anger (3.8%), disgust (3.8%), 
neutral (3.8%) 
POS app joy (37%), neutral (9%), surprise (7%), interest 
(3%), POS jud (3%), sadness (1%) 
NEG app sadness (44.8%), fear (13.8%), disgust (3.4%), 
surprise (3.4%), neutral (3.4%) 
neutral sadness (29.9%), joy (13.8%), interest (3.4%), 
fear (2.3%), POS jud (2.3%), NEG app (2.3%), 
NEG jud (1.1%), POS app (1.1%) 
Table 5. Data from a confusion matrix for ALL level. 
 
Our system achieved high precision for all cate-
gories on the MID level (Table 4), with the excep-
tion of ?NEG app? and ?neutral?, although high 
recall was obtained only in the case of categories 
related to affect (?POS aff?, ?NEG aff?). These re-
sults indicate that affect sensing is easier than rec-
ognition of judgment or appreciation from text. 
TOP level results (Table 4) show that our algo-
rithm classifies sentences that convey positive or 
negative sentiment with high accuracy (92% and 
91%, correspondingly). On the other hand, ?neu-
tral? sentences still pose a challenge. 
The analysis of errors revealed that system re-
quires common sense or additional context to deal 
with sentences like ?All through my life I?ve felt 
like I?m second fiddle? (?gold standard?: ?sadness?; 
@AM: ?neutral?) or ?For me every minute on my 
horse is alike an hour in heaven!? (?gold stan-
dard?: ?joy?; @AM: ?neutral?).  
We also evaluated the system performance with 
regard to attitude intensity estimation. The percen-
tage of attitude-conveying sentences (not consider-
ing neutral ones), on which the result of our system 
conformed to the fine-grained ?gold standard? 
(ALL level), according to the measured distance 
between intensities given by human raters (aver-
aged values) and those obtained by our system is 
shown in Table 6. As seen from the table, our sys-
tem achieved satisfactory results in estimation of 
the strength of attitude expressed through text. 
 
Range of intensity 
difference 
Percent of 
sentences, % 
[0.0 ? 0.2] 55.5 
(0.2 ? 0.4] 29.5 
(0.4 ? 0.6] 12.2 
(0.6 ? 0.8] 2.6 
(0.8 ? 1.0] 0.2 
Table 6. Results on intensity. 
6 Conclusions 
In this paper we introduced @AM, which is so far, 
to the best of our knowledge, the only system clas-
sifying sentences using fine-grained attitude types, 
and extensively dealing with the semantics of 
verbs in attitude analysis. Our composition ap-
proach broadens the coverage of sentences with 
complex contextual attitude. The evaluation results 
indicate that @AM achieved reliable results in the 
task of textual attitude analysis. The limitations 
include dependency on lexicon and on accuracy of 
the parser. The primary objective for the future 
research is to use the results of named-entity rec-
ognition software in our algorithm. 
 
87
References  
Cecilia O. Alm. 2008. Affect in Text and Speech. PhD 
Dissertation. University of Illinois at Urbana-
Champaign. 
Saima Aman and Stan Szpakowicz. 2008. Using Roget's 
Thesaurus for Fine-Grained Emotion Recognition. 
Proceedings of the Third International Joint Confe-
rence on Natural Language Processing IJCNLP 
2008, Hyderabad, India, pp. 296-302. 
Plaban Kumar Bhowmick, Anupam Basu, and Pabitra 
Mitra. 2009. Reader Perspective Emotion Analysis in 
Text through Ensemble based Multi-Label Classifi-
cation Framework. Computer and Information 
Science, 2 (4): 64-74. 
Anthony C. Boucouvalas. 2003. Real Time Text-to-
Emotion Engine for Expressive Internet Communica-
tions. Being There: Concepts, Effects and Measure-
ment of User Presence in Synthetic Environments, 
Ios Press, pp. 306-318. 
Francois-Regis Chaumartin. 2007. UPAR7: A Know-
ledge-based System for Headline Sentiment Tagging. 
Proceedings of the Fourth International Workshop 
on Semantic Evaluations (SemEval-2007), Prague, 
Czech Republic, pp. 422-425. 
Yejin Choi and Claire Cardie. 2008. Learning with 
Compositional Semantics as Structural Inference for 
Subsentential Sentiment Analysis. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 793-801. 
Ze-Jing Chuang and Chung-Hsien Wu. 2004. Multi-
modal Emotion Recognition from Speech and Text. 
Computational Linguistic and Chinese Language 
Processing, 9(2): 45-62. 
Leo Hoye. 1997. Adverbs and Modality in English. New 
York: Addison Wesley Longman Inc. 
Carroll E. Izard. 1971. The Face of Emotion. New York: 
Appleton-Century-Crofts. 
Karin Kipper, Anna Korhonen, Neville Ryant, and Mar-
tha Palmer. 2007. A Large-scale Classification of 
English Verbs. Language Resources and Evaluation, 
42 (1): 21-40. 
Zornitsa Kozareva, Borja Navarro, Sonia Vazquez, and 
Andres Montoyo, A. 2007. UA-ZBSA: A Headline 
Emotion Classification through Web Information. 
Proceedings of the Fourth International Workshop 
on Semantic Evaluations, pp. 334-337. 
Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A 
Model of Textual Affect Sensing Using Real-World 
Knowledge. Proceedings of the International Confe-
rence on Intelligent User Interfaces, pp. 125-132. 
James R. Martin and Peter R.R. White. 2005. The Lan-
guage of Evaluation: Appraisal in English. Palgrave, 
London, UK. 
George A. Miller. 1990. WordNet: An On-line Lexical 
Database. International Journal of Lexicography, 
Special Issue, 3 (4): 235-312. 
Karo Moilanen and Stephen Pulman. 2007. Sentiment 
Composition. Proceedings of the Recent Advances in 
Natural Language Processing International Confe-
rence, pp. 378-382. 
Matthijs Mulder, Anton Nijholt, Marten den Uyl, and 
Peter Terpstra. 2004. A Lexical Grammatical Imple-
mentation of Affect. Proceedings of the Seventh In-
ternational Conference on Text, Speech and 
Dialogue, pp. 171-178. 
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment 
Analysis: Capturing Favorability using Natural Lan-
guage Processing. Proceedings of the 2nd Interna-
tional Conference on Knowledge Capture, pp. 70-77. 
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru 
Ishizuka. 2009. SentiFul: Generating a Reliable Lex-
icon for Sentiment Analysis. Proceedings of the In-
ternational Conference on Affective Computing and 
Intelligent Interaction, IEEE, Amsterdam, Nether-
lands, pp. 363-368. 
J. Olveres, M. Billinghurst, J. Savage, and A. Holden. 
1998. Intelligent, Expressive Avatars. Proceedings of 
the First Workshop on Embodied Conversational 
Characters, pp. 47-55. 
Livia Polanyi and Annie Zaenen. 2004. Contextual Va-
lence Shifters. Working Notes of the AAAI Spring 
Symposium on Exploring Attitude and Affect in Text: 
Theories and Applications. 
Carlo Strapparava and Rada Mihalcea. 2008. Learning 
to Identify Emotions in Text. Proceedings of the 
2008 ACM Symposium on Applied Computing, Forta-
leza, Brazil, pp. 1556-1560. 
Carlo Strapparava, Alessandro Valitutti, and Oliviero 
Stock. 2007. Dances with Words. Proceedings of the 
International Joint Conference on Artificial Intelli-
gence, Hyderabad, India, pp. 1719-1724. 
V.S. Subrahmanian and Diego Reforgiato. 2008. AVA: 
Adjective-Verb-Adverb Combinations for Sentiment 
Analysis. Intelligent Systems, IEEE, 23 (4): 43-50. 
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal Automatically. Proceedings of American As-
sociation for Artificial Intelligence Spring 
Symposium on Exploring Attitude and Affect in Text, 
pp.158-161. 
Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 
2005. Using Appraisal Groups for Sentiment Analy-
sis. Proceedings of the 14th ACM International Con-
ference on Information and Knowledge Management, 
CIKM, Bremen, Germany, pp. 625-631. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. Proceedings of Human 
Language Technology Conference and Conference 
on Empirical Methods in Natural Language 
Processing, Vancouver: ACL, pp. 347-354. 
88
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 55?58,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards Semi-Supervised Classification of Discourse Relations using
Feature Correlations
Hugo Hernault and Danushka Bollegala and Mitsuru Ishizuka
Graduate School of Information Science & Technology
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
hugo@mi.ci.i.u-tokyo.ac.jp
danushka@iba.t.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Abstract
Two of the main corpora available for
training discourse relation classifiers are
the RST Discourse Treebank (RST-DT)
and the Penn Discourse Treebank (PDTB),
which are both based on the Wall Street
Journal corpus. Most recent work us-
ing discourse relation classifiers have em-
ployed fully-supervised methods on these
corpora. However, certain discourse rela-
tions have little labeled data, causing low
classification performance for their asso-
ciated classes. In this paper, we attempt
to tackle this problem by employing a
semi-supervised method for discourse re-
lation classification. The proposed method
is based on the analysis of feature co-
occurrences in unlabeled data. This in-
formation is then used as a basis to ex-
tend the feature vectors during training.
The proposed method is evaluated on both
RST-DT and PDTB, where it significantly
outperformed baseline classifiers. We be-
lieve that the proposed method is a first
step towards improving classification per-
formance, particularly for discourse rela-
tions lacking annotated data.
1 Introduction
The RST Discourse Treebank (RST-DT) (Carl-
son et al, 2001), based on the Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
framework, and the Penn Discourse Treebank
(PDTB) (Prasad et al, 2008), are two of the most
widely-used corpora for training discourse rela-
tion classifiers. They are both based on the Wall
Street Journal (WSJ) corpus, although there are
substantial differences in the relation taxonomy
used to annotate the corpus. These corpora have
been used in most of the recent work employ-
ing discourse relation classifiers, which are based
on fully-supervised machine learning approaches
(duVerle and Prendinger, 2009; Pitler et al, 2009;
Lin et al, 2009).
Still, when building a discourse relation clas-
sifier on either corpus, one is faced with the
same practical issue: Certain relations are very
prevalent, such as ELABORATION[N][S] (RST-
DT), with more than 4000 instances, whereas
other occur rarely, such as EVALUATION[N][N]1
(RST-DT), with three instances, or COMPARI-
SON.PRAGMATIC CONCESSION (PDTB), with 12
instances. This lack of training data causes poor
classification performance on the classes associ-
ated to these relations.
In this paper, we try to tackle this problem by
using feature co-occurrence information, extracted
from unlabeled data, as a way to inform the classi-
fier when unseen features are found in test vectors.
The advantage of the method is that it relies solely
on unlabeled data, which is abundant, and cheap
to collect.
The contributions of this paper are the follow-
ing: First, we propose a semi-supervised method
that exploits the abundant, freely-available un-
labeled data, which is harvested for feature co-
occurrence information, and used as a basis to ex-
tend feature vectors to help classification for cases
where unknown features are found in test vec-
tors. Second, the proposed method is evaluated
on the RST-DT and PDTB corpus, where it signif-
icantly improves F-score when trained on moder-
ately small datasets. For instance, when trained on
a dataset with around 1000 instances, the proposed
method increases the macro-average F-score up to
30%, compared to a baseline classifier.
2 Related Work
Since the release in 2002 of the RST-DT corpus,
several fully-supervised discourse parsers have
1We use the notation [N] and [S] respectively to denote
the nucleus and satellite in a RST discourse relation.
55
been built in the RST framework. In duVerle and
Prendinger (2009), a discourse parser based on
Support Vector Machines (SVM) (Vapnik, 1995)
is proposed. Shallow lexical, syntactic and struc-
tural features, including ?dominance sets? (Soricut
and Marcu, 2003) are used.
The unsupervised method of Marcu and Echi-
habi (2002) was the first to try to detect ?implicit?
relations (i.e. relations not accompanied by a cue
phrase, such as ?however?, ?but?), using word pairs
extracted from two spans of text. Their method
attempts to capture the difference of polarity in
words.
Discourse relation classifiers have also been
trained using PDTB. Pitler et al (2008) performed
a corpus study of the PDTB, and found that ?ex-
plicit? relations can be most of the times distin-
guished by their discourse connectives.
Lin et al (2009) studied the problem of detect-
ing implicit relations in PDTB. Their relational
classifier is trained using features extracted from
dependency paths, contextual information, word
pairs and production rules in parse trees. For the
same task, Pitler et al (2009) also use word pairs,
as well as several other types of features such as
verb classes, modality, context, and lexical fea-
tures.
In this paper, we are not aiming at defining
novel features for improving performance in RST
or PDTB relation classification. Instead we incor-
porate features that have already shown to be use-
ful for discourse relation learning and explore the
possibilities of using unlabeled data for this task.
3 Method
In this section, we describe a semi-supervised
method for relation classification, based on feature
vector extension. The extension process employs
feature co-occurrence information. Co-occurrence
information is useful in this context as, for in-
stance, we might know that the word pair (for,
when) is a good indicator of a TEMPORAL rela-
tion. Or, after analyzing a large body of unlabeled
data, we might also notice that this word pair co-
occurs often with the word ?run-up? placed at the
end of a span of text. Suppose now that we have to
classify a test instance containing the feature ?run-
up?, but not the word pair (for, when). In this case,
by using the co-occurrence information, we know
that the instance has a chance of being a TEM-
PORAL relation. We first explain how to compute
a feature correlation matrix, using unlabeled data.
In a second section, we show how to extend fea-
ture vectors in order to include co-occurrence in-
formation. Finally, we describe the features used
in the discourse relation classifiers.
3.1 Feature Correlation Matrix
A training/test instance is represented using a d-
dimensional feature vector f = [f1, . . . , fd]T,
where fi ? {0, 1}. We define a feature correla-
tion matrix, C such that the (i, j)-th element of
C, C(i,j) ? {0, 1} denotes the correlation between
the two features fi and fj . If both fi and fj appear
in a feature vector then we define them to be co-
occurring. The number of different feature vectors
in which fi and fj co-occur is used as a basis to
compute C(i,j). Importantly, feature correlations
can be calculated using only unlabeled data.
It is noteworthy that feature correlation matri-
ces can be computed using any correlation mea-
sure. For the current task we use the ?2-measure
(Plackett, 1983) as the preferred correlation mea-
sure because of its simplicity. We create the fea-
ture correlation matrix C, such that, for all pairs of
features (fi, fj),
C(i,j) =
{
1 if ?2i,j > c
0 otherwise
. (1)
Here c is the critical value, which, for a confi-
dence level of 0.05 and one degree of freedom, can
be set to 3.84.
3.2 Feature Vector Extension
Once the feature correlation matrix is computed
using unlabeled data as described in Section 3.1,
we can use it to extend a feature vector during
testing. One of the reasons explaining why a clas-
sifier might perform poorly on a test instance, is
that there are features in the test instance that were
not observed during training. Let us represent the
feature vector corresponding to a test instance x
by fx. Then, we use the feature correlation ma-
trix to find the set of correlated features Fc(fi) of
a particular feature fi that occur in fx.
Specifically, for a feature fi ? fx, F ?(fi) con-
sists of features fj , where C(i,j) = 1. We define
the extended feature vector f ?x of fx as the union of
all the features that appear in fx and Fc(fx). Since
a discourse relation is defined between two spans
of short texts (elementary discourse units), which
are typically two clauses or sentences, a particu-
lar feature does not usually occur more than once
56
in a feature vector. Therefore, we introduced the
proposed method in the context of binary valued
features. However, the above mentioned discus-
sion can be naturally extended to cover real-valued
features.
3.3 Features
Figure 1 shows the parse tree for a sentence com-
posed of two discourse units, which serve as argu-
ments of a discourse relation we want to generate
a feature vector from. Lexical heads have been
calculated using the projection rules of Magerman
(1995), and indicated between brackets. For each
argument, surrounded by dots, is the minimal set
of sub-parse trees containing strictly all the words
of the argument.
We extract all possible lemmatized word pairs
from the two arguments. Next, we extract from
left and right argument separately, all production
rules from the sub-parse trees. Finally, we encode
in our features three nodes of the parse tree, which
capture the local context at the connection point
between the two arguments (Soricut and Marcu,
2003): The first node, which we call Nw, is the
highest ancestor of the first argument?s last word
w, and is such that Nw?s right-sibling is the an-
cestor of the second argument?s first word. Nw?s
right-sibling node is calledNr. Finally, we callNp
the parent of Nw and Nr. For each node, we en-
code in the feature vector its part-of-speech (POS)
and lexical head. For instance, in Figure 1, we
have Nw = S(comment), Nr = SBAR(when), and
Np = VP(declined).
4 Experiments
It is worth noting that the proposed method is inde-
pendent of any particular classification algorithm.
As our goal is strictly to evaluate the relative ben-
efit of employing the proposed method, we se-
lect a logistic regression classifier, for its simplic-
ity. We used the multi-class logistic regression
(maximum entropy model) implemented in Clas-
sias (Okazaki, 2009). Regularization parameters
are set to their default value of one.
Unlabeled instances are created by selecting
texts of the WSJ, and segmenting them into ele-
mentary discourse units (EDUs) using our sequen-
tial discourse segmenter (Hernault et al, 2010).
As there is no segmentation tool for the PDTB
framework, we assumed that feature correlation
information taken from EDUs created using a RST
segmenter is also useful for extending feature vec-
tors of PDTB relations.
Since we are interested in measuring the over-
all performance of a discourse relation classifier
across all relation types, we use macro-averaged
F-score as the preferred evaluation metric for this
task. We train a multi-class logistic regression
model without extending the feature vectors as
a baseline method. This baseline is expected to
show the effect of using the proposed feature ex-
tension approach for the task of discourse relation
learning.
Experimental results on RST-DT and PDTB
datasets are depicted in Figures 2 and 3. We ob-
serve that the proposed feature extension method
outperforms the baseline for both RST-DT and
PDTB datasets for the full range of training dataset
sizes. However, the difference between the two
methods decreases as we increase the amount of
training data. Specifically, with 200 training in-
stances, for RST-DT, the baseline method has a
macro-averaged F-score of 0.079, whereas the the
proposed method has a macro-averaged F-score
of 0.159 (around 101% increase in F-score). For
1000 training instances, the F-score for RST-DT
increases by 29.2%, from 0.143 to 0.185, while
the F-score for PDTB increases by 27.9%, from
0.109 to 0.139. However, the difference between
the two methods diminishes beyond 10000 train-
ing instances.
0 5000 10000 15000 20000Number of training instances0.05
0.10
0.15
0.20
0.25
0.30
Mac
ro-a
vera
ge F
-sco
re
Proposed methodBaseline RST-DT
Figure 2: Macro-average F-score (RST-DT) as a
function of the number of training instances used.
5 Conclusion
We presented a semi-supervised method for im-
proving the performance of discourse relation
classifiers. The proposed method is based on
the analysis of co-occurrence information har-
vested from unlabeled data only. We evaluated
57
NP (Sherry)
S (declined)
VP (declined)
NNP NNP
declined
VBD (declined)
Mr. Sherry to
VP (comment)
comment when asked about the sales
TO VP
SBAR (when)
WHADVP (when)
WRB
S (asked)
VP (asked)
VBN
PP (about)
IN NP (sales)
DT NNS
.
. (.)
Argument 1 Argument 2
VB
S (comment)
Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them?
lexical heads are indicated between brackets.
0 2000 4000 6000 8000 10000Number of training instances0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Mac
ro-a
vera
ge F
-sco
re
Proposed methodBaseline PDTB
Figure 3: Macro-average F-score (PDTB) as a
function of the number of training instances used.
the method on two of the most widely-used dis-
course corpora, RST-DT and PDTB. The method
performs significantly better than a baseline classi-
fier trained on the same features, especially when
the number of labeled instances used for training is
small. For instance, using 1000 training instances,
we observed an increase of nearly 30% in macro-
average F-score. This is an interesting perspective
for improving classification performance of rela-
tions with little training data. In the future, we
plan to improve the method by employing ranked
co-occurrences. This way, only the most relevant
correlated features can be selected during feature
vector extension. Finally, we plan to investigate
using larger amounts of unlabeled training data.
References
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. Proc. of Sec-
ond SIGdial Workshop on Discourse and Dialogue-
Volume 16, pages 1?10.
D. A. duVerle and H. Prendinger. 2009. A novel
discourse parser based on Support Vector Machine
classification. In Proc. of ACL?09, pages 665?673.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010.
A sequential model for discourse segmentation. In
Proc. of CICLing?10, pages 315?326.
Z. Lin, M-Y. Kan, and H. T. Ng. 2009. Recognizing
implicit discourse relations in the Penn Discourse
Treebank. In Proc. of EMNLP?09, pages 343?351.
D. M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. Proc. of ACL?95, pages 276?283.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc.
of ACL?02, pages 368?375.
N. Okazaki. 2009. Classias: A collection of machine-
learning algorithms for classification.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In Proc. of COLING?08 (Posters),
pages 87?90.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In Proc. of ACL?09, pages 683?691.
R. L. Plackett. 1983. Karl Pearson and the chi-squared
test. International Statistical Review, 51(1):59?72.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
Penn Discourse Treebank 2.0. In Proc. of LREC?08.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. Proc. of NA-ACL?03, 1:149?156.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York, Inc.
58
