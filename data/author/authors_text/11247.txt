Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1001?1008
Manchester, August 2008
Exploiting Graph Structure for Accelerating the Calculation of Shortest
Paths in Wordnets
Holger Wunsch
Collaborative Research Center 441 ?Linguistic Data Structures?
University of Tu?bingen, Germany
wunsch@sfs.uni-tuebingen.de
Abstract
This paper presents an approach for sub-
stantially reducing the time needed to cal-
culate the shortest paths between all con-
cepts in a wordnet. The algorithm exploits
the unique ?star-like? topology of word-
nets to cut down on time-expensive calcu-
lations performed by algorithms to solve
the all-pairs shortest path problem in gen-
eral graphs. The algorithm was applied to
two wordnets of two different languages:
Princeton WordNet (Fellbaum, 1998) for
English, and GermaNet (Kunze and Lem-
nitzer, 2002), the German language word-
net. For both wordnets, the time needed
for finding all shortest paths was brought
down from several days to a matter of
minutes.
1 Introduction
Significant effort has been devoted in linguistic re-
search to the problem of determining the semantic
distance1 between two concepts. Many of the ap-
proaches that were developed to provide a solution
for the task use wordnets as their basic knowledge
resource. Budanitsky and Hirst (2006) present an
extensive number of approaches for determining
lexical semantic relatedness based on the Prince-
ton WordNet (Fellbaum, 1998). A large number
of these solutions have in common that at some
point in the calculation, the length of the shortest
path that connects the two concepts in question has
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1see Budanitsky and Hirst (2006) for a critical discussion
of the term ?semantic distance?.
to be taken into account. The length of the short-
est path between two concepts is thus a vital piece
of information in virtually any approach for deter-
mining their semantic distance.
From a computer science perspective, a word-
net is a directed graph. The nodes in the graph
correspond to the concepts that are stored in the
wordnet. Two nodes are connected by an edge if
there exists a semantic relation between the two
concepts that correspond to the nodes. The edges
are directed, reflecting the directedness of seman-
tic relations in wordnets, such as the relation of
hypernymy. The problem of finding the shortest
path between two nodes in a graph has been well
studied in computer science. Sedgewick (1990)
presents two algorithms:
? Dijkstra?s algorithm finds the shortest path
between two concepts in quadratic time, and
is extensible to find the shortest paths be-
tween all concepts in cubic time.
? The Floyd-Warshall algorithm, which is very
easy to implement, solves the all-pairs short-
est path problem in cubic time as well. The
Floyd-Warshall algorithm served as the base-
line algorithm for this paper.
Both algorithms operate on general directed
graphs. The structure of a directed graph is
sketched in figure 1. Given two nodes, there exist
multiple different paths that connect the two nodes.
In figure 1, in order to get from L to D, one could
take the path L?G?E?D, but also L?G?C?A?F?E?
D, and several other paths. An algorithm that looks
for the shortest path must deal with this situation
and basically be able to consider all alternatives.
Computing of the length of all shortest paths
with the Floyd-Warshall algorithm takes about 120
1001
AB
F
G
E
C
J
D
K
L M
H
I
Figure 1: Schematic structure of a general graph.
Example taken from Sedgewick (1990).
hours for GermaNet, using a compiler-optimized
C implementation on a machine equipped with
two AMD Opteron processors. GermaNet, which
contains 53,312 synsets2 in its current release 5,
is of moderate size, compared to the Princeton
WordNet, which contains more than twice as many
synsets. The execution time of 120 hours is accept-
able if the wordnet does not change, and the short-
est paths are calculated once and then kept for later
use. However, when the task involves the modifi-
cation of the wordnet itself and the repeated recal-
culation of all shortest paths, this is unacceptable.
In such cases the execution time will most likely
prohibit research involving such techniques.
It has been shown that there is no faster way for
solving the all-pairs shortest paths problem in gen-
eral graphs. However, the structure of wordnets
is somewhat different to that of a general directed
graph. It can best be described to be similar to that
of a star, as depicted in figure 2. In the middle,
there is a unique top node that dominates all other
nodes3. In the center area around the top node,
the synsets represent general concepts. Closer to
the fringe, the synsets become more and more spe-
cific. A typical configuration in this region would
2A synset is a set of words that are synonymous. Through-
out this paper, we will use the term synset synonymously to
nodes in a graph, since the relevant relations (which are rep-
resented by edges in the graph) hold between synsets.
3In most wordnets such as Princeton WordNet or Ger-
maNet, there is usually no unique top node. Instead, there
is a set of most general concepts, called unique beginners. In
the example in figure 2, the unique beginners would be the
synsets s1, s3, s11, s15, and s23. The algorithm presented
here requires the wordnet to be a connected graph, therefore
we stipulate an explicit artificial top node.
be a biological taxonomy of animals, or specific
kinds of vehicles. The number of such special-
ized synsets in the outer regions of the wordnet
outweighs the number of core concepts by several
orders of magnitude. In other words, the majority
of synsets in a wordnet is part of tree structures that
are arranged around a relatively small core graph.
In the remainder of this paper, we will present
an approach to solving the all-pairs shortest paths
problem in wordnets that is superior in execution
time by consistently exploiting this specific struc-
ture of wordnets.
2 Finding Paths In a Wordnet
We implemented the Floyd-Warshall algorithm as
our baseline. The Floyd-Warshall algorithm uses
a dynamic programming approach. The basic idea
is to find all shortest paths by first computing and
storing the paths lengths between nodes that are
close to each other, and then moving on to longer
paths by combining the results of the shorter paths.
The Floyd-Warshall algorithm uses a matrix that
contains for each pair of nodes the length of the
shortest path that connects the two nodes. The ma-
trix is initialized by setting the length of the short-
est path between any two adjacent nodes to 14. All
other fields are set to ? (which indicates that no
path has been found (yet)). Then the algorithm
checks for any pair of nodes x and y whether there
exists a node z such that the path from x to y that
runs along z is shorter than the path between x and
y that has been found so far.
Floyd-Warshall Algorithm
N : set of nodes
p: matrix that contains the lengths of the short-
est path between any node x and y ? N .
Initialization Step
for all x, y ? N do
5: if neighbors(x, y) then
pxy := 1
else
pxy := ?
end if
10: end for
4The Floyd-Warshall algorithm can also be used to calcu-
late shortest paths in weighted graphs in which case the matrix
would be initialized with the weights of the edges between
adjacent nodes. We will focus on unweighted graphs in this
paper.
1002
Top
s1
s3
s4s5
s6
s2
s9
s10
s11
s12
s13
s14
s22
s18
s17s15s23
s25
s24
s16
s20s19
s21
s7
s8
Figure 2: Schematic star-like structure of a wordnet.
Shortest Path Calculation
for all z ? N do
for all x, y ? N do
pxy := min(pxy, pxz + pzy)
15: end for
end for
Since the shortest of all possible paths must be
found, the Floyd-Warshall algorithm has to check
for any node z if the path x?z?y is the shortest.
Returning to the example in figure 2, the shortest
path between x = s1 and y = s16 is s1?Top?s15?
s16. The Floyd-Warshall algorithm also considers
the other possible paths, such as s1?s3?Top?s15?
s16 or s1?Top?s11?s17?s15?s16, which are even-
tually discarded because they turn out to be not the
shortest connection.
Now consider a path such as the one illustrated
in figure 3, which is the shortest undirected path
between the synsets s8 and s205. In order to find
this shortest path, the Floyd-Warshall algorithm
would check all possible combinations of nodes
x?z?y in order to find a potential shorter alterna-
tive. To the human reader it is obvious that this is
an unnecessary amount of work: there is only one
path that leads from s8 to s1, because the part of
the wordnet rooted in s4 is in fact a tree, and paths
connecting two nodes in trees are always unique.
The same is true for the structure rooted in s18.
5Since the edges in the graph correspond to the hypernymy
relations in the wordnet, they all point towards the top node.
In order to get from s8 to s20, one must first follow the path up
to the top node along the hypernymy axis, and then down to
s20 in the opposite direction. Therefore, paths are undirected.
Moreover, there is a unique link from s4 to s1, and
from s18 to s17. The synsets s1 and s17 are part of
the core structure of the wordnet with higher den-
sity. Here, between s1 and s17 there do exist more
than one paths which must all be considered by the
path finding algorithm.
The complexity of the Floyd-Warshall algorithm
results from the necessity of considering all possi-
ble paths through a general graph in order to find
the shortest one. Wordnets are graphs ? but, as
explained above, they have a very special struc-
ture: There is a core graph consisting of a limited
number of synsets, but the majority of all synsets
is arranged in tree structures that are attached to
the core graph, which gives wordnets the star-like
structure described earlier.
This structure, which is specific to wordnets,
can be exploited such that expensive calculations
are only performed where necessary ? in the core
graph of the wordnet ? while specialized cheaper
calculations are used in the outer parts.
3 Structure-Adapted Shortest Path
Search
The observations about the structure of wordnets
and the nature of general algorithms to find all
shortest paths lead to the conclusion that an al-
gorithm that is adapted to the specific structure
of wordnets should be superior over general algo-
rithms with respect to execution time. In this sec-
tion, we will present such a structure-adapted ap-
proach. It operates in two stages: In the first stage,
1003
Top
s1
s3
s4s5
s6
s2
s9
s10
s11
s12
s13
s14
s22
s18
s17s15s23
s25
s24
s16
s20s19
s21
s7
s8
Figure 3: A path through the wordnet.
all nodes in the wordnet are classified whether they
are part of the core graph or a peripheral tree struc-
ture. The second stage is the shortest path search
proper, which uses the information about the struc-
ture of the wordnet that was acquired in the first
stage.
3.1 Stage 1: Node classification
In the first stage, the algorithm determines whether
a node belongs to the graph proper which consti-
tutes the core network, or to a tree structure in the
periphery. The algorithm classifies nodes in four
classes, which are illustrated in figure 4.
1. Inner nodes: Inner nodes belong to the graph
in the center of the network. A node is an in-
ner node if it has more than one parent node,
or if one of its children is an inner node.
In figure 4, inner nodes have a white back-
ground.
2. Root nodes: A root node, as suggested by its
name, is the root node of a tree. Root nodes
have a unique parent node, which must be an
inner node. In figure 4, root nodes have a dark
gray background, and thick borders.
3. Tree nodes: Tree nodes are part of a tree,
i.e. they have one unique parent node. This
parent node must either be a root node, or a
tree node as well. In figure 4, tree nodes have
a dark gray background (and thin borders).
4. Leaf nodes: Leaf nodes have a unique parent
node, which must be an inner node. They do
not have any child nodes. As such, leaf nodes
are actually a special case of a root node. But
for performance reasons, they will be handled
separately from root nodes. In figure 4, leaf
nodes have a light gray background.
Two remarks are in order. The tree structures
referred to in this classification rely on a well de-
fined parent-child relation. The hypernymy rela-
tion, which is the only relation between synsets we
consider in this paper, is a directed relation that
satisfies this property: if synset x is a hypernym of
synset y, then x is a parent node of y.
The difference between the terms tree node and
leaf node may seem a little arbitrary ? consider-
ing for example s8, which is a tree node, and s9,
which is a leaf node. Both nodes do not have child
nodes. However, from a performance perspective,
it is advantageous to treat leaf nodes and tree nodes
differently in the algorithm, which is why two dif-
ferent nodes types are assumed.
3.2 Stage 2: Shortest Path Search
The second stage is the actual pathfinding step.
The underlying basic idea is to split the calcula-
tion: Consider the sample path in figure 3 between
synsets s8 and s20. This path runs through three
regions of the wordnet. The synsets in the first part
of the path, s8-s5-s4 all belong to the tree that is
rooted in synset s4. Then the path enters the core
1004
Top
s1
s3
s4s5
s6
s2
s9
s10
s11
s12
s13
s14
s22
s18
s17s15s23
s25
s24
s16
s20s19
s21
s7
s8
Figure 4: Node classes. White: inner nodes; dark gray: tree nodes; dark gray with thick border: root
nodes; light gray: leaf nodes.
graph and runs along s1-Top-s15-s176. The third
part of the path is s18-s20. Again, the correspond-
ing synsets are members of a tree whose root is
s18.
The important point is that for any part of a path
that runs though a tree, this is the only possible
path through the tree. A general algorithm for find-
ing the shortest path out of a set of multiple pos-
sible paths is not necessary here. This way, it is
possible to restrict the application of general but
time-expensive algorithms to the area of the word-
net where this is needed ? the core of the network,
while elsewhere it is sufficient to just determine the
length of a path through a tree, a task which can be
solved very efficiently.
Returning to our example, the algorithm splits
the calculation of the shortest path between s8 and
s20 as follows: Both s8 and s20 are part of a tree,
as determined in step 1. The root of the tree that
s8 is a member of is s4. The length of the path
between s2 and s4 is 2. s4?s parent node is s1.
Root nodes have a unique parent node by defini-
tion. Therefore we know that only s1 can be the
next node on the path, and its distance is 1, since
s4 and s1 are neighbors. So, the length of the path
up to s1 is 3. In the same fashion, the length of
the other part of the path that is located in a tree,
the path between s20 and s17, can be computed.
6there are actually two possible paths of the same length
(8) ? the other possible path would be the one that runs along
synset s11.
The length of this part is 2. Now there remains
the part between node s1 and s17. This part of the
path runs through the core graph. Here, a general
algorithm for finding shortest paths, such as the
Floyd-Warshall algorithm, must be used to com-
pute the path?s length. The length of the shortest
possible path within this region turns out to be 3.
Now the lengths of all parts of the path have been
determined ? and the total length of the shortest
path is just the sum of the three parts, which is 8.
4 Implementation
This section will present pseudo-code for the two
stages of structure-adapted shortest path search.
4.1 Stage 1: Node classification
Stage 1 explores a wordnet and classifies every
node whether it is an inner node, or a root, tree, or
leaf node. The procedure starts out by classifying
as leaf nodes all nodes that are childless and that
have one unique parent node. Note however that
the definition of a leaf node requires that its parent
node be an inner node. This constraint can not be
checked at this point, since information about in-
ner nodes is not yet available. Therefore the check
is postponed until later.
Next, any node that has more than one parent
node is classified as an inner node. The transitive
closure of all of this node?s parent nodes is classi-
fied as inner nodes as well.
The remaining nodes that have not been classi-
1005
fied so far are either root or tree nodes. A node
is a root node if its parent node is an inner node,
otherwise it is a tree node.
The last step is to check the constraint on leaf
nodes which has been postponed. As stated in the
definition, leaf nodes are childless and have an in-
ner node as parent. Nodes with no children whose
parent node is either a root node or a tree node are
not leaf nodes, but rather tree nodes. Thus, each
potential leaf node is visited to ensure that its par-
ent node is in fact an inner node in which case the
classification as a leaf node remains unchanged.
Otherwise, the node is reclassified as tree node.
The pseudocode of the node classification algo-
rithm is listed below.
Node Classification Algorithm7
N : set of nodes
Node classes: inner , leaf , root , tree,
undefined
Classify leaf nodes
for all n ? N do
5: if |children(n)| = 0 ?
|parents(n)| = 1 then
assign class(n, leaf )
end if
end for
Classify inner nodes
10: for all n ? N do
if |parents(n)| > 1 then
if class(n) 6= inner then
assign class(n, inner)
{All parent nodes of inner nodes are
inner nodes as well}
15: for all m ? parents?(n) do
assign class(m, inner)
end for
end if
end if
20: end for
Classify root and tree nodes
for all n ? N do
if class(n) = undefined then
if class(parents(n)[0]) = inner then
25: assign class(n, root)
7Notes on the notation: parents(n) and children(n) are
functions that return a list of all parent or child nodes of the
node n. parents(n)[0] returns the first node in the list of
parent nodes of n. parents?(n) is the transitive closure of
all parent nodes of n. |parents(n)| is the number of parent
nodes of n.
else
assign class(n, tree)
end if
end if
30: end for
Reclassify leaf nodes as tree nodes if they
are children of tree nodes
for all n ? N do
if class(n) = leaf then
if class(parents(n)[0]) = root ?
class(parents(n)[0]) = leaf then
35: assign class(n, tree)
end if
end if
end for
4.2 Stage 2: Finding Shortest Paths
The actual calculation of shortest paths takes place
in stage 2. In order to calculate the shortest paths
between two nodes x and y, two main cases are
considered.
4.2.1 x and y do not belong to the same tree
x and y do not belong to the same tree if the root
nodes of the trees that x and y are members of are
different: root(x) 6= root(y).
The path from x to y is then split into three parts:
? lxix : the length of the subpath from x to the
first inner node ix on the path.
? lyiy : the length of the subpath from y to the
first inner node iy on the path.
? lixiy : the length of the subpath from ix to iy,
which runs through the core.
The following cases are considered by the algo-
rithm:
1. x is an inner node: ix = x, and lxix = 0.
2. x is a tree node: lxix is the length of the path
from x to the root node of the tree rx, plus 1
to get from rx to ix: lxix := lxrx + 1.
3. x is a leaf node or a root node: lxix is 1.
4. y is an inner node: iy = y, and lyiy = 0.
5. y is a tree node: lyiy is the length of the path
from y to the root node of the tree ry, plus 1
to get from ry to iy: lyiy := lyry + 1.
6. y is a leaf node or a root node: lyiy is 1.
1006
The length of the path lixiy (the path running
through the core graph) is calculated using the
Floyd-Warshall algorithm, that is lixiy = pixiy
(see the description of the Floyd-Warshall algo-
rithm above).
The total length of the shortest path is then
lxy := lxix + lixiy + lyiy .
4.2.2 x and y belong to the same tree
This is a special case that is treated differently
from all other cases. Let z be the lowest node in
the tree that dominates both x and y (where z may
be equal to x or y). Then lxy = lxz + lyz .
Shortest Paths Algorithm
Input:
Two nodes x, y ? N .
Path matrix p for nodes in the core graph as
calculated by Floyd-Warshall
Output:
5: The length of the shortest path between x and
y, lxy.
if ((class(x) = root ? class(x) = tree) ?
(class(y) = root ? class(y) = tree)) ?
root(x) = root(y) then
First case: x and y belong to the same tree
Let z ? S be the lowest common subsumer
of x and y.
lxy := lxz + lyz
10: return lxy
else
Second case: x and y do not belong to the
same tree
ix := x;
iy := y;
15: if class(x) = tree then
rx := root(x)
lxix := lxrx + 1
else if class(x) = root ? class(x) = leaf
then
lxix := 1
20: else
lxix := 0
end if
if class(y) = tree then
ry := root(y)
25: lyiy := lyry + 1
else if class(y) = root ? class(y) = leaf
then
lyiy := 1
else
lyiy := 0
30: end if
lxy := lxix + lixiy + lyiy
end if
5 Experiments and Results
5.1 The Data
In order to assess the performance of structure
adapted path search, we experimented with two
different wordnets. For English, we looked at
Princeton WordNet (Fellbaum, 1998) in its current
release 3, which contains 117,659 synsets. Only
4,250 of these synsets belong to the core network.
The remaining 113,410 nodes are members of pe-
ripheral tree structures, which amounts to 96 % of
all nodes.
Furthermore, we applied the approach to
GermaNet (Kunze and Lemnitzer, 2002) for Ger-
man. The architecture of GermaNet is modelled
after Princeton WordNet and is largely compati-
ble. GermaNet, in its current release 5.0, contains
53,312 synsets. Out of these, 8,728 synsets are
members of the core of the network (i.e. the part
of the network that is a graph proper). The remain-
ing 44,273 synsets are part of peripheral tree struc-
tures (or leaf nodes). Hence, 83% of the synsets
in GermaNet are part of substructures that do not
require a general algorithm for calculating shortest
paths.
The topology of GermaNet is thus slightly dif-
ferent than that of WordNet. The fact that more
nodes belong to the core graph indicates that
GermaNet?s density with respect to the hypernymy
relation is higher than the density of WordNet on
the level of the more abstract concepts.
5.2 Application of the Algorithm
We conducted our experiments on a machine
equipped with two AMD Opteron 250 processors
running at 2.4 GHz and 8 GB of main memory.
For the path-finding stage, we implemented two
C programs. Both operate on the same input. Both
programs were compiled using gcc?s -O3 option
for maximum optimization.
The first program only used the Floyd-Warshall
algorithm ? the node classes were effectively ig-
nored. For WordNet, the estimated processing
time was at least 35 days. This value was com-
puted by interpolating the time that had passed
to process 15,000 synsets. At this point, the
tests were aborted. Since Floyd-Warshall becomes
1007
Princeton WordNet
Synsets 117,659
Inner nodes 4,250
Root nodes 7,174
Tree nodes 56,532
Leaf nodes 49,704
Node classification time ca. 1 second
Floyd-Warshall path search > 35 days
Structure-adapted path search 9 minutes
GermaNet
Synsets 53,312
Inner nodes 8,728
Root nodes 4,641
Tree nodes 18,949
Leaf nodes 20,683
Node classification time 1.2 seconds
Floyd-Warshall path search 120 hours
Structure-adapted path search 40 minutes
Table 1: Structure-adapted path search ? Summary
of results
slower the more nodes have been processed, this
value is likely to be even higher. GermaNet con-
tains only half of the synsets, and the Floyd-
Warshall algorithm completed in 120 hours8. The
result, a matrix of 53,312?53,312 elements, con-
taining the lengths of the shortest paths between all
nodes, was written to a binary file whose size was
roughly 3 GB.
In the second program, we implemented the
structure-adapted shortest path search approach.
For calculating shortest paths in the core graph
area of the network, we used the same implemen-
tation of the Floyd-Warshall algorithm as in the
first program. With the same input data and the
same machine, we were able to bring the execution
time down to 40 minutes for GermaNet, and only
9 minutes for WordNet. This includes creating the
output file, which had the same binary format as
the one generated by the first program. The dif-
ference in processing time between WordNet and
GermaNet stems from the smaller core graph in
WordNet, which allows for even more nodes to be
excluded from the time consuming Floyd-Warshall
calculation. The results of the experiments are
summarized in table 1.
8The complexity of the Floyd-Warshall algorithm is cubic,
therefore twice as many synsets result in a processing effort
that is eight times higher.
6 Discussion
The experiments show that with a thorough pre-
analysis of the structure of a wordnet and consis-
tent usage of this additional information, the time
it takes to calculate all shortest paths can be re-
duced dramatically. This is because most nodes in
a wordnet are part of substructures that are proper
trees and not general graphs. In trees, it is possible
to calculate the length of a path very much more
efficiently than in an arbitrarily-structured graph.
We successfully applied structure-adapted path
search to two wordnets, the Princeton WordNet
and GermaNet. Since the algorithm does not rely
on concrete properties of a specific wordnet, it can
easily be applied to wordnets for other languages.
The benefits of structure adaptation diminish
with increasing density of the network as more
and more nodes become part of the core graph. In
this case, the execution time will approach that of
the Floyd-Warshall algorithm. It is also obvious
that the approach does not generalize to arbitrary
graphs. As long as the structure of graphs is sim-
ilar to the star-like structure of wordnets, we ex-
pect the approach to be beneficial in applications
involving such graphs as well.
References
Budanitsky, A. and Hirst, G. (2006). Evaluating
WordNet-based Measures of Lexical Semantic
Relatedness. In Computational Linguistics, vol-
ume 32. Association for Computational Lin-
guistics.
Fellbaum, C. (1998). WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Kunze, C. and Lemnitzer, L. (2002). GermaNet
? Representation, Visualization, Application. In
Proceedings of LREC, pages 1485?1491.
Sedgewick, R. (1990). Algorithms in C. Addison
Wesley.
1008
Coling 2010: Poster Volume, pages 267?275,
Beijing, August 2010
Exploring the Data-Driven Prediction of Prepositions in English
Anas Elghafari Detmar Meurers Holger Wunsch
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{aelgafar,dm,wunsch}@sfs.uni-tuebingen.de
Abstract
Prepositions in English are a well-known
challenge for language learners, and the
computational analysis of preposition us-
age has attracted significant attention.
Such research generally starts out by de-
veloping models of preposition usage for
native English based on a range of fea-
tures, from shallow surface evidence to
deep linguistically-informed properties.
While we agree that ultimately a com-
bination of shallow and deep features is
needed to balance the preciseness of ex-
emplars with the usefulness of generaliza-
tions to avoid data sparsity, in this paper
we explore the limits of a purely surface-
based prediction of prepositions.
Using a web-as-corpus approach, we in-
vestigate the classification based solely on
the relative number of occurrences for tar-
get n-grams varying in preposition usage.
We show that such a surface-based ap-
proach is competitive with the published
state-of-the-art results relying on complex
feature sets.
Where enough data is available, in a sur-
prising number of cases it thus is possible
to obtain sufficient information from the
relatively narrow window of context pro-
vided by n-grams which are small enough
to frequently occur but large enough
to contain enough predictive information
about preposition usage.
1 Introduction
The correct use of prepositions is a well-known
difficulty for learners of English, and correspond-
ingly the computational analysis of preposition
usage has attracted significant attention in re-
cent years (De Felice and Pulman, 2007; De Fe-
lice, 2008; Lee and Knutsson, 2008; Gamon et
al., 2008; Chodorow et al, 2007; Tetreault and
Chodorow, 2008a, 2008b).
As a point of reference for the detection of
preposition errors in learner language, most of
the research starts out by developing a model of
preposition usage for native English. For this
purpose, virtually all previous approaches em-
ploy a machine learning setup combining a range
of features, from surface-based evidence to deep
linguistically-informed properties. The overall
task is approached as a classification problem
where the classes are the prepositions and the in-
stances to be classified are the contexts, i.e., the
sentences with the prepositions omitted.
A focus of the previous literature is on the ques-
tion which linguistic and lexical features are the
best predictors for preposition usage. Linguistic
features used include the POS tags of the sur-
rounding words, PP attachment sites, WordNet
classes of PP object and modified item. Lexical
features used include the object of the PP and the
lexical item modified by the PP. Those syntactic,
semantic and lexical features are then extracted
from the training instances and used by the ma-
chine learning tool to predict the missing preposi-
tion in a test instance.
While we agree that ultimately a combination
of shallow and linguistically informed features is
needed to balance the preciseness of exemplars
267
with the usefulness of generalizations to avoid
data sparsity problems, in this paper we want to
explore the limits of a purely surface-based pre-
diction of prepositions. Essentially, our ques-
tion is how much predictive information can be
found in the immediate distributional context of
the preposition. Is it possible to obtain n-gram
contexts for prepositions which are small enough
to occur frequently enough in the available train-
ing data but large enough to contain enough pre-
dictive information about preposition usage?
This perspective is related to that underlying
the variation-n-gram approach for detecting errors
in the linguistic annotation of corpora (Dickin-
son and Meurers, 2003; Dickinson and Meurers,
2005; Boyd et al, 2008). Under that approach, er-
rors in the annotation of linguistic properties (lexi-
cal, constituency, or dependency information) are
detected by identifying units which recur in the
corpus with sufficient identical context so as to
make variation in their annotation unlikely to be
correct. In a sense, the recurring n-gram contexts
are used as exemplar references for the local do-
mains in which the complex linguistic properties
are established. The question now is to what ex-
tent basic1 n-gram contexts can also be success-
fully used to capture the linguistic properties and
relations determining preposition usage, explor-
ing the trade-off expressed in the question ending
the previous paragraph.
To address this question, in this paper we make
use of a web-as-corpus approach in the spirit of
Lapata and Keller (2005). We employ the Yahoo
search engine to investigate a preposition classifi-
cation setup based on the relative number of web
counts obtained for target n-grams varying in the
preposition used. We start the discussion with a
brief review of key previous approaches and the
results they obtain for the preposition classifica-
tion task in native English text. In section 2,
we then describe the experimental setup we used
1While Dickinson and Meurers (2005) also employ dis-
continuous n-grams, we here focus only on contiguous n-
gram contexts. Using discontinuous n-gram contexts for
preposition prediction could be interesting to explore in the
future, once, as a prerequisite for the effective generation
of discontinuous n-grams, heuristics have been identified for
when which kind of discontinuities should be allowed to arise
for preposition classification contexts.
for our exploration and discuss our results in sec-
tion 3.
1.1 Previous work and results
The previous work on the preposition prediction
task varied in i) the features selected, ii) the num-
ber of prepositions tackled, and iii) the training
and testing corpora used.
De Felice (2008) presents a system that (among
other things) is used to predict the correct prepo-
sition for a given context. The system tackles the
nine most frequent prepositions in English: of, to,
in, for, on, with, at, by, from. The approach uses a
wide variety of syntactic and semantic features:
the lexical item modified by the PP, the lexical
item that occurs as the object of the preposition,
the POS tags of three words to the left and three
words to the right of the preposition, the grammat-
ical relation that the preposition is in with its ob-
ject, the grammatical relation the preposition is in
with the word modified by the PP, and the Word-
Net classes of the preposition?s object and the lex-
ical item modified by the PP. De Felice (2008) also
used a named entity recognizer to extract general-
izations about which classes of named entities can
occur with which prepositions. Further, the verbs?
subcategorization frames were taken as features.
For features that used lexical sources (WordNet
classes, verbs subcategorization frames), only par-
tial coverage of the training and testing instances
is available.
The overall accuracy reported by De Felice
(2008) for this approach is 70.06%, testing on sec-
tion J of the British National Corpus (BNC) after
training on the other sections. As the most exten-
sive discussion of the issue, using an explicit set
of prepositions and a precisely specified and pub-
licly accessible test corpus, De Felice (2008) is
well-suited as a reference approach. Correspond-
ingly, our study in this paper is based on the same
set of prepositions and the same test corpus.
Gamon et al (2008) introduce a system for the
detection of a variety of learner errors in non-
native English text, including preposition errors.
For the preposition task, the authors combine the
outputs of a classifier and a language model. The
language model is a 5-gram model trained on the
English Gigaword corpus. The classifier is trained
268
on Encarta encyclopedia and Reuters news text.
It operates in two stages: The presence/absence
classifier predicts first whether a preposition needs
to be inserted at a given location. Then, the choice
classifier determines which preposition is to be in-
serted. The features that are extracted for each
possible insertion site come from a six-token win-
dow around the possible insertion site. Those fea-
tures are the relative positions, POS tags, and sur-
face forms of the tokens in that window. The
choice classifier predicts one of 13 prepositions:
in, for, of, on, to, with, at, by, as, from, since,
about, than, and other. The accuracy of the choice
classifier, the part of the system to which the work
at hand is most similar, is 62.32% when tested on
text from Encarta and Reuters news.
Tetreault and Chodorow (2008a) present a sys-
tem for detecting preposition errors in learner text.
Their approach extracts a total of 25 features from
the local contexts: the adjacent words, the heads
of the nearby phrases, and the POS tags of all
those. They combine word-based features with
POS tag features to better handle cases where a
word from the test instance has not been seen
in training. For each test instance, the system
predicts one of 34 prepositions. In training and
testing performed on the Encarta encyclopedia,
Reuters news text and additional training material
an accuracy figure of 79% is achieved.
Bergsma et al (2009) extract contextual fea-
tures from the Google 5-gram corpus to train an
SVM-based classifier for predicting prepositions.
They evaluate on 10 000 sentences taken from the
New York Times section of the Gigaword corpus,
and achieve an accuracy of 75.4%.
Following De Felice (2008, p. 66), we summa-
rize the main results of the mentioned approaches
to preposition prediction for native text in Fig-
ure 1.2 Since the test sets and the prepositions tar-
geted differ between the approaches, such a com-
parison must be interpreted with caution. In terms
of the big picture, it is useful to situate the results
with respect to the majority baseline reported by
De Felice (2008). It is obtained by always choos-
ing of as the most common preposition in section
J of the BNC. De Felice also reports another inter-
2The Gamon et al (2008) result differs from the one re-
ported in De Felice (2008); we rely on the original paper.
esting figure included in Figure 1, namely the ac-
curacy of the human agreement with the original
text, averaged over two English native-speakers.
Approach Accuracy
Gamon et al (2008) 62.32%
Tetreault and Chodorow (2008a) 79.00%
Bergsma et al (2009) 75.50%
De Felice (2008) system 70.06%
Majority baseline (of) 26.94%
Human agreement 88.60%
Figure 1: Preposition prediction results
2 Experiments
2.1 Data
As our test corpus, we use section J of the BNC,
the same corpus used by De Felice (2008). Based
on the tokenization as given in the corpus, we
join the tokens with a single space, which also
means that punctuation characters end up as sep-
arate, white-space separated tokens. We select all
sentences that contain one or more prepositions,
using the POS annotation in the corpus to iden-
tify the prepositions. The BNC is POS-annotated
with the CLAWS-5 tagset, which distinguishes the
two tags PRF for of and PRP for all other preposi-
tions.3 We mark every occurrence of these prepo-
sition tags in the corpus, yielding one prediction
task for each marked preposition. For example,
the sentence (1) yields four prediction tasks, one
for each of the prepositions for, of, from, and in in
the sentence.
(1) But for the young, it is rather a question
of the scales falling from their eyes, and
having nothing to believe in any more.
In each task, one preposition is masked using
the special marker -*-MASKED-*-. Figure 2
shows the four marked-up prediction tasks result-
ing for example (1).
Following De Felice (2008), we focus our ex-
periments on the top nine prepositions in the
BNC: of, to, in, for, on, with, at, by, from. For
3http://www.natcorp.ox.ac.uk/docs/URG/
posguide.html#guidelines
269
But -*-MASKED-*-for the young , it is
rather a question of the scales falling
from their eyes , and having nothing to
believe in any more .
But for the young , it is rather a
question -*-MASKED-*-of the scales
falling from their eyes , and having
nothing to believe in any more .
But for the young , it is rather
a question of the scales falling
-*-MASKED-*-from their eyes , and having
nothing to believe in any more .
But for the young , it is rather a
question of the scales falling from
their eyes , and having nothing to
believe -*-MASKED-*-in any more .
Figure 2: Four prediction tasks for example (1)
each occurrence of these nine prepositions in sec-
tion J of the BNC, we extract one prediction task,
yielding a test set of 522 313 instances.
Evaluating on this full test set would involve a
prohibitively large number of queries to the Ya-
hoo search engine. We therefore extract a ran-
domly drawn subset of 10 000 prediction tasks.
From this subset, we remove all prediction tasks
which are longer than 4000 characters in length,
as Yahoo only supports queries up to that length.
Finally, in a web-as-corpus setup, the indexing of
the web pages performed by the search engine es-
sentially corresponds to the training step in a typi-
cal machine learning setup. In order to avoid test-
ing on the training data, we thus need to ensure
that the test cases are based on text not indexed by
the search engine. To exclude any such cases, we
query the search engine with each complete sen-
tence that a prediction task is based on and remove
any prediction task for which the search engine re-
turns hits for the complete sentence. The final test
set consists of 8060 prediction tasks.4
2.2 Experimental Setup
Recall that the general issue we are interested in
is whether one can obtain sufficient information
from the relatively narrow distributional window
of context provided by n-grams which are small
enough to occur frequently enough in the training
data but large enough to contain enough predic-
4For a copy of the test set, just send us an email.
tive information about preposition usage for the
instances to be classified. By using a web-as-
corpus approach we essentially try to maximize
the training data size. For the n-gram size, we ex-
plore the use of a maximum order of 7, containing
the preposition in the middle and three words of
context on either side.
For each prediction task, we successively insert
one of the nine most frequent prepositions into
the marked preposition slot of the 8060 n-grams
obtained from the test set. Thus, for each pre-
diction task, we get a cohort consisting of nine
different individual queries, one query for each
potential preposition. For example, the second
prediction task of Figure 2 yields the cohort of
nine queries in Figure 3 below, where the candi-
date prepositions replace the location marked by
-*-MASKED-*-of. The correct preposition of
is stripped off and kept for later use in the evalua-
tion step.
1. rather a question of the scales
falling
2. rather a question to the scales
falling
3. rather a question in the scales
falling
...
9. rather a question from the scales
falling
Figure 3: Cohort of nine queries resulting for the
second prediction task of Figure 2
In cases where a preposition is closer than four
words to the beginning or the end of the corre-
sponding sentence, a lower-order n-gram results.
For example, in the first prediction task in Fig-
ure 2, the preposition occurs already as the sec-
ond word in the sentence, thus not leaving enough
context to the left of the preposition for a sym-
metric 7-gram. Here, the truncated asymmetric 5-
gram ?But <prep> the young ,? includ-
ing only one word of context on the left would
get used.
We issue each query in a cohort to the Ya-
hoo search engine, and determine the number
of hits returned for that query. To that end,
we use Yahoo?s BOSS service, which offers a
270
JSON interface supporting straightforward auto-
mated queries. As part of its response to a query,
the BOSS service includes the deephits field,
which gives an ?approximate count that reflects
duplicate documents and all documents from a
host?.5 In other words, this number is an approx-
imate measure of how many web pages there are
that contain the search pattern.
With the counts for all nine queries in a cohort
retrieved from Yahoo, we select the preposition of
the query with the highest count. For the cases
in which none of the counts in a 7-gram cohort is
greater than zero, we use one of two strategies:
In the baseline condition, for all n-gram cohorts
with zero counts (5160 out of the 8060 cases) we
predict the most frequent preposition of, i.e., the
majority baseline. This results in an overall accu-
racy of 50%.
In the full back-off condition, we explore the
trade-off between the predictive power of the n-
gram as context and the likelihood of having seen
this n-gram in the training material, i.e., finding
it on the web. In this paper we never abstract or
generalize away from the surface string (e.g., by
mapping all proper names to an abstract name tag;
but see the outlook discussion at the end of the pa-
per), so the only option for increasing the number
of occurrences of an n-gram is to approximate it
with multiple shorter n-grams.
Concretely, if no hits could be found for any of
the queries in a cohort, we back off to the sum
of the hits for the two overlapping 6-grams con-
structed in the way illustrated in Figure 4.
[rather a question of the scales falling]
?
[rather a question of the scales]
[a question of the scales falling]
Figure 4: Two overlapping 6-grams approximate
a 7-gram for back-off.
If still no hits can be obtained after backing off
to 6-grams for any of the queries in a cohort, the
system backs off further to overlapping 5-grams,
and so on, down to trigrams.6
5Cited from http://developer.yahoo.com/
search/boss/boss_guide/ch02s02.html
6When backing off, the left-most and the right-most tri-
3 Results
Figure 5 shows the results of the full back-off
approach. Compared to the baseline condition,
accuracy goes up significantly to 76.5%. Thus,
the back-off strategy is effective in increasing the
amount of available data using lower-order n-
grams. This increase of data is also reflected in
the number of cases with zero counts for a cohort,
which goes down to none.
Full back-off
Correct 6166
Incorrect 1894
Total 8060
Accuracy 76.5%
Figure 5: Overall results of our experiments.
Figure 6 provides a detailed analysis of the
back-off experiment. It lists back-off sequences
separately for each maximum n-gram order. The
prediction tasks for which a full 7-gram can be
extracted are displayed in the third column, with
back-off orders of 6 down to 3. Prediction tasks
for which only asymmetric 6-grams can be ex-
tracted follow in column 4, and so on until 4-
grams. There are no predictions tasks that are
shorter than four words. Therefore, n-grams with
a length of less than 4 do not occur.
The ?sum? column shows the combined results
of the full 7-gram prediction tasks and the pre-
diction tasks involving truncated, asymmetric n-
grams of lower orders.
There are 6999 prediction tasks for which full
7-grams can be extracted. The remaining 1061
of the 8060 prediction tasks are the cases where
the system extracts only asymmetric lower-order
n-grams, for the reasons explained in section 2.2.
For 2195 of the 6999 7-gram prediction tasks,
we find full 7-gram contexts on the web, of which
1931 lead to a correct prediction, and 264 to an
incorrect one, leaving 4804 prediction tasks still
to be solved through the back-off approach. Thus,
full 7-gram contexts lead to high-quality predic-
tions at 88% precision, but they are rare and with
a recall of 28,7% cover only a fraction of all cases.
gram do not include the target preposition of the original 7-
gram. However, this only affects 13 cases, cf. Figure 6.
271
sum 7-grams 6-grams 5-grams 4-grams
(3 + prep + 3) (truncated 7-gram) (truncated 7-gram) (truncated 7-gram)
Total 8060 6999 656 182 223
Predictions 2900 2195 379 119 207
correct 2495 1931 326 91 147
incorrect 405 264 53 28 60
Requiring back-off 5160 4804 277 63 16
Precision 86% 88% 86% 76.5% 71%
Recall 32.6% 28.7% 79.6% 59.1% 90.2%
Back-off order 6
Predictions 2028 2028
correct 1620 1620
incorrect 408 408
Still requiring back-off 2776 2776
Predict. orders 7+6 4223 4223
correct 3551 3551
incorrect 672 672
Precision 84.1% 84.1%
Recall 56.1% 56.1%
Back-off order 5
Predictions 2180 2020 160
correct 1542 1411 131
incorrect 638 609 29
Still requiring back-off 873 756 117
Predict. orders 7 ? 5 6782 6243 539
correct 5419 4962 457
incorrect 1363 1281 82
Precision 79.9% 79.5% 84.8%
Recall 86.1% 86.8% 79.6%
Back-off order 4
Predictions 905 743 106 56
correct 488 382 68 38
incorrect 417 361 38 18
Still requiring back-off 31 13 11 7
Predict. orders 7 ? 4 7806 6986 645 175
correct 5998 5344 525 129
incorrect 1808 1642 120 46
Precision 76.8% 76.5% 81.4% 73.7%
Recall 99.5% 99.8% 97.9% 94.9%
Back-off order 3
Predictions 47 13 11 7 16
correct 21 5 7 3 6
incorrect 26 8 4 4 10
Still requiring back-off 0 0 0 0 0
Predict. orders 7 ? 3 8060 6999 656 182 223
correct 6166 5349 532 132 153
incorrect 1894 1650 124 50 70
Precision 76.5% 76.4% 81.1% 72.5% 68.6%
Recall 100% 100% 100% 100% 100%
Figure 6: The results of our experiments
272
Figure 7: Development of precision and recall in
relation to back-off order
Approximating 7-grams with two overlapping
6-grams as the first back-off step provides the
evidence needed to correctly predict 1620 addi-
tional prepositions, with 408 additional false pre-
dictions. The number of correctly solved predic-
tion tasks thus rises to 3551, and the number of
incorrect predictions rises to 672. This back-off
step almost doubles recall (56.1%). At the same
time, precision drops to 84.1%. For 2776 pre-
diction tasks, a further back-off step is necessary
since still no evidence can be found for them. This
pattern repeats with the back-off steps that fol-
low. To summarize, by adding more data using
less restricted contexts, more prediction tasks can
be solved. The better coverage however comes at
the price of reduced precision: Less specific con-
texts are worse predictors of the correct preposi-
tion than more specific contexts.
Figure 7 visualizes the development of preci-
sion and recall with full and truncated 7-grams
counted together as in the ?sum? column in Fig-
ure 6. With each back-off step, more prediction
tasks can be solved (as shown by the rising recall
curve). At the same time, the overall quality of
the predictions drops due to the less specific con-
texts (as shown by the slightly dropping precision
curve). While the curve for recall rises steeply,
the curve for precision remains relatively flat. The
back-off approach thus succeeds in adding data
while preserving prediction quality.
As discussed above, we use the same set of
prepositions and test corpus as De Felice (2008),
but only make use of 8060 test cases. Figure 8
shows that the accuracy stabilizes quickly after
about 1000 predictions, so that the difference in
the size of the test set should have no impact on
the reported results.
Figure 8: The accuracy of the n-gram prediction
stabilizes quickly.
4 Conclusions and Outlook
In this paper, we explored the potential and the
limits of a purely surface-based strategy of pre-
dicting prepositions in English. The use of
surface-based n-grams ensures that fully specific
exemplars of a particular size are stored in train-
ing, but avoiding abstractions in this way leads to
the well-known data sparsity issues. We showed
that using a web-as-corpus approach maximizing
the size of the ?training data?, one can work with
n-grams which are large enough to predict the oc-
currence of prepositions with significant precision
while at the same time ensuring that these specific
n-grams have actually been encountered during
?training?, i.e., evidence for them can be found
on the web.
For the random sample of the BNC section J
we tested on, the surface-based approach results
in an accuracy of 77% for the 7-gram model with
back-off to overlapping shorter n-grams. It thus
outperforms De Felice?s (2008) machine learning
273
approach which uses the same set of prepositions
and the full BNC section J as test set. In broader
terms, the result of our surface-based approach
is competitive with the state-of-the art results for
preposition prediction in English using machine
learning to combine sophisticated sets of lexical
and linguistically motivated features.
In this paper, we focused exclusively on the
impact of n-gram size on preposition prediction.
Limiting ourselves to pure surface-based informa-
tion made it possible to maximize the ?training
data? by using a web-as-corpus approach. Return-
ing from this very specific experiment to the gen-
eral issue, there are two well-known approaches
to remedy the data sparseness problem arising
from storing large, specific surface forms in train-
ing. On the one hand, one can use smaller ex-
emplars, which is the method we used as back-
off in our experiments in this paper. This only
works if the exemplars contain enough context for
the linguistic property or relation that we need to
capture the predictive power. On the other hand,
one can abstract parts of the surface-based train-
ing instances to more general classes. The cru-
cial question this raises is which generalizations
preserve the predictive power of the exemplars
and can reliably be identified. The linguistically-
informed features used in the previous approaches
in the literature naturally provide interesting in-
stances of answers to this question. In the fu-
ture, we intend to compare the results we ob-
tained using the web-as-corpus approach with one
based on the Google-5-gram corpus to study us-
ing controlled, incremental shallow-to-deep fea-
ture development which abstractions or linguistic
generalizations best preserve the predictive con-
text while lowering the demands on the size of the
training data.
Turning to a linguistic issue, it could be use-
ful to distinguish between lexical and functional
prepositions when reporting test results. This is
an important distinction because the information
needed to predict functional prepositions typically
is in the local context, whereas the information
needed to predict lexical prepositions is not nec-
essarily present locally. To illustrate, a competent
human speaker presented with the sentence John
is dependent his brother and asked to fill in
the missing preposition, would correctly pick on.
This is a case of a functional preposition where
the relevant information is locally present: the ad-
jective dependent selects on. On the other hand,
the sentence John put his bag the table is
more problematic, even for a human, since both
on and under are reasonable choices; the infor-
mation needed to predict the omitted preposition
in this case is not locally present. In line with
the previous research, in the work in this paper
we made predictions for all prepositions alike. In
the future, it could be useful to annotate the test
set so that one can distinguish functional and lex-
ical uses and report separate figures for these two
classes in order to empirically confirm their dif-
ferences with respect to locality.
References
Bergsma, Shane, Dekang Lin, and Randy Goebel.
2009. Web-scale n-gram models for lexical disam-
biguation. In IJCAI?09: Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
pages 1507?1512, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Boyd, Adriane, Markus Dickinson, and Detmar Meur-
ers. 2008. On detecting errors in dependency tree-
banks. Research on Language and Computation,
6(2):113?137.
Chodorow, Martin, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25?30,
Prague, Czech Republic, June.
De Felice, Rachele and Stephen Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions, pages 45?50, Prague, Czech Republic,
June. Association for Computational Linguistics.
De Felice, Rachele. 2008. Automatic Error Detection
in Non-native English. Ph.D. thesis, St Catherine?s
College, University of Oxford.
Dickinson, Markus and W. Detmar Meurers. 2003.
Detecting errors in part-of-speech annotation. In
Proceedings of the 10th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-03), pages 107?114, Budapest,
Hungary.
Dickinson, Markus and W. Detmar Meurers. 2005.
Detecting errors in discontinuous structural anno-
274
tation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 322?329.
Gamon, Michael, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William Dolan, Dmitriy Be-
lenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modeling
for esl error correction. In Proceedings of IJCNLP,
Hyderabad, India.
Lapata, Mirella and Frank Keller. 2005. Web-
based models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1):1?30, February.
Lee, John and Ola Knutsson. 2008. The role of pp
attachment in preposition generation. In Gelbukh,
A., editor, Proceedings of CICLing 2008.
Tetreault, Joel and Martin Chodorow. 2008a. Na-
tive judgments of non-native usage: Experiments
in preposition error detection. In Proceedings of
COLING-08, Manchester.
Tetreault, Joel and Martin Chodorow. 2008b. The ups
and downs of preposition error detection in esl writ-
ing. In Proceedings of COLING-08, Manchester.
275
