Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 457?464,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 An Equivalent Pseudoword Solution to Chinese  
Word Sense Disambiguation 
 
Zhimao Lu+    Haifeng Wang++    Jianmin Yao+++    Ting Liu+    Sheng Li+
+ Information Retrieval Laboratory, School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin, 150001, China 
{lzm, tliu, lisheng}@ir-lab.org 
++ Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No. 1, East Chang An Ave., Beijing, 100738, China 
wanghaifeng@rdc.toshiba.com.cn 
+++ School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
jyao@suda.edu.cn 
 
  
 
Abstract 
This paper presents a new approach 
based on Equivalent Pseudowords (EPs) 
to tackle Word Sense Disambiguation 
(WSD) in Chinese language. EPs are par-
ticular artificial ambiguous words, which 
can be used to realize unsupervised WSD. 
A Bayesian classifier is implemented to 
test the efficacy of the EP solution on 
Senseval-3 Chinese test set. The per-
formance is better than state-of-the-art 
results with an average F-measure of 0.80. 
The experiment verifies the value of EP 
for unsupervised WSD. 
1 Introduction 
Word sense disambiguation (WSD) has been a 
hot topic in natural language processing, which is 
to determine the sense of an ambiguous word in 
a specific context. It is an important technique 
for applications such as information retrieval, 
text mining, machine translation, text classifica-
tion, automatic text summarization, and so on. 
Statistical solutions to WSD acquire linguistic 
knowledge from the training corpus using ma-
chine learning technologies, and apply the 
knowledge to disambiguation. The first statistical 
model of WSD was built by Brown et al (1991). 
Since then, most machine learning methods have 
been applied to WSD, including decision tree, 
Bayesian model, neural network, SVM, maxi-
mum entropy, genetic algorithms, and so on. For 
different learning methods, supervised methods 
usually achieve good performance at a cost of 
human tagging of training corpus. The precision 
improves with larger size of training corpus. 
Compared with supervised methods, unsuper-
vised methods do not require tagged corpus, but 
the precision is usually lower than that of the 
supervised methods. Thus, knowledge acquisi-
tion is critical to WSD methods.  
This paper proposes an unsupervised method 
based on equivalent pseudowords, which ac-
quires WSD knowledge from raw corpus. This 
method first determines equivalent pseudowords 
for each ambiguous word, and then uses the 
equivalent pseudowords to replace the ambigu-
ous word in the corpus. The advantage of this 
method is that it does not need parallel corpus or 
seed corpus for training. Thus, it can use a large-
scale monolingual corpus for training to solve 
the data-sparseness problem. Experimental re-
sults show that our unsupervised method per-
forms better than the supervised method. 
The remainder of the paper is organized as fol-
lows. Section 2 summarizes the related work. 
Section 3 describes the conception of Equivalent 
Pseudoword. Section 4 describes EP-based Un-
supervised WSD Method and the evaluation re-
sult. The last section concludes our approach. 
2 Related Work 
For supervised WSD methods,  a knowledge ac-
quisition bottleneck is to prepare the manually 
457
tagged corpus. Unsupervised method is an alter-
native, which often involves automatic genera-
tion of tagged corpus, bilingual corpus alignment, 
etc. The value of unsupervised methods lies in 
the knowledge acquisition solutions they adopt. 
2.1 Automatic Generation of Training Corpus 
Automatic corpus tagging is a solution to WSD, 
which generates large-scale corpus from a small 
seed corpus. This is a weakly supervised learning 
or semi-supervised learning method. This rein-
forcement algorithm dates back to Gale et al 
(1992a). Their investigation was based on a 6-
word test set with 2 senses for each word. 
Yarowsky (1994 and 1995), Mihalcea and 
Moldovan (2000), and Mihalcea (2002) have 
made further research to obtain large corpus of 
higher quality from an initial seed corpus. A 
semi-supervised method proposed by Niu et al 
(2005) clustered untagged instances with tagged 
ones starting from a small seed corpus, which 
assumes that similar instances should have simi-
lar tags. Clustering was used instead of boot-
strapping and was proved more efficient.  
2.2 Method Based on Parallel Corpus 
Parallel corpus is a solution to the bottleneck of 
knowledge acquisition. Ide et al (2001 and 
2002), Ng et al (2003), and Diab (2003, 2004a, 
and 2004b) made research on the use of align-
ment for WSD.  
Diab and Resnik (2002) investigated the feasi-
bility of automatically annotating large amounts 
of data in parallel corpora using an unsupervised 
algorithm, making use of two languages simulta-
neously, only one of which has an available 
sense inventory. The results showed that word-
level translation correspondences are a valuable 
source of information for sense disambiguation. 
The method by Li and Li (2002) does not re-
quire parallel corpus. It avoids the alignment 
work and takes advantage of bilingual corpus. 
In short, technology of automatic corpus tag-
ging is based on the manually labeled corpus. 
That is to say, it still need human intervention 
and is not a completely unsupervised method. 
Large-scale parallel corpus; especially word-
aligned corpus is highly unobtainable, which has 
limited the WSD methods based on parallel cor-
pus.  
3 Equivalent Pseudoword 
This section describes how to obtain equivalent 
pseudowords without a seed corpus. 
Monosemous words are unambiguous priori 
knowledge. According to our statistics, they ac-
count for 86%~89% of the instances in a diction-
ary and 50% of the items in running corpus, they 
are potential knowledge source for WSD.  
A monosemous word is usually synonymous 
to some polysemous words. For example the 
words "?? , ?? , ?? ?? ?? ??, , , , 
??" has similar meaning as one of the senses 
of the ambiguous word "??", while "??, ?
?, ?? ?? ??, , ?? ?? ?? ??, , , , , 
?? ?? ?? ??, , , " are the same for "??". 
This is quite common in Chinese, which can be 
used as a knowledge source for WSD. 
3.1 Definition of Equivalent Pseudoword 
If the ambiguous words in the corpus are re-
placed with its synonymous monosemous word, 
then is it convenient to acquire knowledge from 
raw corpus? For example in table 1, the ambigu-
ous word "??" has three senses, whose syn-
onymous monosemous words are listed on the 
right column. These synonyms contain some in-
formation for disambiguation task. 
An artificial ambiguous word can be coined 
with the monosemous words in table 1. This 
process is similar to the use of general pseu-
dowords (Gale et al, 1992b; Gaustad, 2001; Na-
kov and Hearst, 2003), but has some essential 
differences. This artificial ambiguous word need 
to simulate the function of the real ambiguous 
word, and to acquire semantic knowledge as the 
real ambiguous word does. Thus, we call it an 
equivalent pseudoword (EP) for its equivalence 
with the real ambiguous word. It's apparent that 
the equivalent pseudoword has provided a new 
way to unsupervised WSD. 
S1 ??/??? 
S2 ??/??/??/??/????(ba3 wo4)
S3 ??/??/??/??/??
Table 1. Synonymous Monosemous Words for 
the Ambiguous Word "??" 
The equivalence of the EP with the real am-
biguous word is a kind of semantic synonym or 
similarity, which demands a maximum similarity 
between the two words. An ambiguous word has 
the same number of EPs as of senses. Each EP's 
sense maps to a sense of ambiguous word. 
The semantic equivalence demands further 
equivalence at each sense level. Every corre-
458
sponding sense should have the maximum simi-
larity, which is the strictest limit to the construc-
tion of an EP. 
The starting point of unsupervised WSD based 
on EP is that EP can substitute the original word 
for knowledge acquisition in model training. 
Every instance of each morpheme of the EP can 
be viewed as an instance of the ambiguous word, 
thus the training set can be enlarged easily. EP is 
a solution to data sparseness for lack of human 
tagging in WSD. 
3.2 Basic Assumption for EP-based WSD 
It is based on the following assumptions that EPs 
can substitute the original ambiguous word for 
knowledge acquisition in WSD model training. 
Assumption 1: Words of the same meaning 
play the same role in a language. The sense is an 
important attribute of a word. This plays as the 
basic assumption in this paper. 
Assumption 2: Words of the same meaning 
occur in similar context. This assumption is 
widely used in semantic analysis and plays as a 
basis for much related research. For example, 
some researchers cluster the contexts of ambigu-
ous words for WSD, which shows good perform-
ance (Schutze, 1998). 
Because an EP has a higher similarity with the 
ambiguous word in syntax and semantics, it is a 
useful knowledge source for WSD. 
3.3 Design and Construction of EPs 
Because of the special characteristics of EPs, it's 
more difficult to construct an EP than a general 
pseudo word. To ensure the maximum similarity 
between the EP and the original ambiguous word, 
the following principles should be followed. 
1) Every EP should map to one and only one 
original ambiguous word. 
2) The morphemes of an EP should map one 
by one to those of the original ambiguous word. 
3) The sense of the EP should be the same as 
the corresponding ambiguous word, or has the 
maximum similarity with the word. 
4) The morpheme of a pseudoword stands for 
a sense, while the sense should consist of one or 
more morphemes.  
5) The morpheme should be a monosemous 
word. 
The fourth principle above is the biggest dif-
ference between the EP and a general pseudo 
word. The sense of an EP is composed of one or 
several morphemes. This is a remarkable feature 
of the EP, which originates from its equivalent 
linguistic function with the original word. To 
construct the EP, it must be ensured that the 
sense of the EP maps to that of the original word. 
Usually, a candidate monosemous word for a 
morpheme stands for part of the linguistic func-
tion of the ambiguous word, thus we need to 
choose several morphemes to stand for one sense.  
The relatedness of the senses refers to the 
similarity of the contexts of the original ambigu-
ous word and its EP. The similarity between the 
words means that they serve as synonyms for 
each other. This principle demands that both se-
mantic and pragmatic information should be 
taken into account in choosing a morpheme word. 
3.4 Implementation of the EP-based Solution 
An appropriate machine-readable dictionary is 
needed for construction of the EPs. A Chinese 
thesaurus is adopted and revised to meet this de-
mand. 
Extended Version of TongYiCiCiLin 
To extend the TongYiCiCiLin (Cilin) to hold 
more words, several linguistic resources are 
adopted for manually adding new words. An ex-
tended version of the Cilin is achieved, which 
includes 77,343 items. 
A hierarchy of three levels is organized in the 
extended Cilin for all items. Each node in the 
lowest level, called a minor class, contains sev-
eral words of the same class. The words in one 
minor class are divided into several groups ac-
cording to their sense similarity and relatedness, 
and each group is further divided into several 
lines, which can be viewed as the fifth level of 
the thesaurus. The 5-level hierarchy of the ex-
tended Cilin is shown in figure 1. The lower the 
level is, the more specific the sense is. The fifth 
level often contains a few words or only one 
word, which is called an atom word group, an 
atom class or an atom node. The words in the 
same atom node hold the smallest semantic dis-
tance. 
From the root node to the leaf node, the sense 
is described more and more detailed, and the 
words in the same node are more and more re-
lated. Words in the same fifth level node have 
the same sense and linguistic function, which 
ensures that they can substitute for each other 
without leading to any change in the meaning of 
a sentence. 
 
 
459
 ?  ? 
?
?? ?? 
? ? ? ?
? ? ?
 
? ? ? ?
Level 1 
Level 2 
Level 3 
Level 4 
Level 5 
?  ? 
Figure 1. Organization of Cilin (extended) 
 
The extended version of extended Cilin is 
freely downloadable from the Internet and has 
been used by over 20 organizations in the world1. 
Construction of EPs 
According to the position of the ambiguous word, 
a proper word is selected as the morpheme of the 
EP. Almost every ambiguous word has its corre-
sponding EP constructed in this way. 
The first step is to decide the position of the 
ambiguous word starting from the leaf node of 
the tree structure. Words in the same leaf node 
are identical or similar in the linguistic function 
and word sense. Other words in the leaf node of 
the ambiguous word are called brother words of 
it. If there is a monosemous brother word, it can 
be taken as a candidate morpheme for the EP. If 
there does not exist such a brother word, trace to 
the fourth level. If there is still no monosemous 
brother word in the fourth level, trace to the third 
level. Because every node in the third level con-
tains many words, candidate morpheme for the 
ambiguous can usually be found. 
In most cases, candidate morphemes can be 
found at the fifth level. It is not often necessary 
to search to the fourth level, less to the third. Ac-
cording to our statistics, the extended Cilin con-
tains about monosemous words for 93% of the 
ambiguous words in the fifth level, and 97% in 
the fourth level. There are only 112 ambiguous 
words left, which account for the other 3% and 
mainly are functional words. Some of the 3% 
words are rarely used, which cannot be found in 
even a large corpus. And words that lead to se-
mantic misunderstanding are usually content 
words. In WSD research for English, only nouns, 
verbs, adjectives and adverbs are considered. 
                                                 
1 It is located at http://www.ir-lab.org/. 
From this aspect, the extended version of Cilin 
meets our demand for the construction of EPs. 
If many monosemous brother words are found 
in the fourth or third level, there are many candi-
date morphemes to choose from. A further selec-
tion is made based on calculation of sense simi-
larity. More similar brother words are chosen. 
Computing of EPs 
Generally, several morpheme words are needed 
for better construction of an EP. We assume that 
every morpheme word stands for a specific sense 
and does not influence each other. It is more 
complex to construct an EP than a common 
pseudo word, and the formulation and statistical 
information are also different. 
An EP is described as follows:  
 
iikiiii
k
k
WWWWS
WWWWS
WWWWS
L
MMMMMM
L
L
,,,:
,,,:
,,,:
321
22322212
11312111
2
1
 
WEP?????????? 
Where WEP is the EP word, Si is a sense of the 
ambiguous word, and Wik is a morpheme word of 
the EP. 
The statistical information of the EP is calcu-
lated as follows: 
1? stands for the frequency of the S)( iSC i : 
?=
k
iki WCSC )()(  
2? stands for the co-occurrence fre-
quency of S
),( fi WSC
i and the contextual word Wf : 
?=
k
fikfi WWCWSC ),(),(  
460
 Ambiguous word citation (Qin and Wang, 2005) Ours Ambiguous word 
citation (Qin and 
Wang, 2005) Ours 
??(ba3 wo4) 0.56 0.87 ??(mei2 you3) 0.75 0.68 
?(bao1) 0.59 0.75 ??(qi3 lai2) 0.82 0.54 
??(cai2 liao4) 0.67 0.79 ?(qian2) 0.75 0.62 
??(chong1 ji1) 0.62 0.69 ??(ri4 zi3) 0.75 0.68 
?(chuan1) 0.80 0.57 ?(shao3) 0.69 0.56 
??(di4 fang1) 0.65 0.65 ??(tu1 chu1) 0.82 0.86 
??(fen1 zi3) 0.91 0.81 ??(yan2 jiu1) 0.69 0.63 
??(yun4 dong4) 0.61 0.82 ??(huo2 dong4) 0.79 0.88 
?(lao3) 0.59 0.50 ?(zou3) 0.72 0.60 
?(lu4) 0.74 0.64 ?(zuo4) 0.90 0.73 
Average 0.72 0.69 Note: Average of the 20 words 
Table 2. The F-measure for the Supervised WSD 
 
4 EP-based Unsupervised WSD Method 
EP is a solution to the semantic knowledge ac-
quisition problem, and it does not limit the 
choice of statistical learning methods. All of the 
mathematical modeling methods can be applied 
to EP-based WSD methods. This section focuses 
on the application of the EP concept to WSD, 
and chooses Bayesian method for the classifier 
construction. 
4.1 A Sense Classifier Based on the Bayes-
ian Model 
Because the model acquires knowledge from the 
EPs but not from the original ambiguous word, 
the method introduced here does not need human 
tagging of training corpus. 
In the training stage for WSD, statistics of EPs 
and context words are obtained and stored in a 
database. Senseval-3 data set plus unsupervised 
learning method are adopted to investigate into 
the value of EP in WSD. To ensure the compara-
bility of experiment results, a Bayesian classifier 
is used in the experiments. 
Bayesian Classifier 
Although the Bayesian classifier is simple, it is 
quite efficient, and it shows good performance 
on WSD. 
The Bayesian classifier used in this paper is 
described in (1) 
???
?
???
?
+= ?
? ij
k
cv
kjkSi SvPSPwS )|(log)(logmaxarg)( (1)
Where wi is the ambiguous word,  is the 
occurrence probability of the sense S
)( kSP
k,  
is the conditional probability of the context word 
v
)|( kj SvP
j, and ci is the set of the context words. 
To simplify the experiment process, the Naive 
Bayesian modeling is adopted for the sense clas-
sifier. Feature selection and ensemble classifica-
tion are not applied, which is both to simplify the 
calculation and to prove the effect of EPs in 
WSD. 
Experiment Setup and Results  
The Senseval-3 Chinese ambiguous words are 
taken as the testing set, which includes 20 words, 
each with 2-8 senses. The data for the ambiguous 
words are divided into a training set and a testing 
set by a ratio of 2:1. There are 15-20 training 
instances for each sense of the words, and occurs 
by the same frequency in the training and test set. 
Supervised WSD is first implemented using 
the Bayesian model on the Senseval-3 data set. 
With a context window of (-10, +10), the open 
test results are shown in table 2. 
The F-measure in table 2 is defined in (2). 
RP
RP
F +
??= 2  (2) 
461
Where P and R refer to the precision and recall 
of the sense tagging respectively, which are cal-
culated as shown in (3) and (4) 
)tagged(
)correct(
C
C
P =  (3) 
)all(
)correct(
C
C
R =  (4) 
Where C(tagged) is the number of tagged in-
stances of senses, C(correct) is the number of 
correct tags, and C(all) is the number of tags in 
the gold standard set. Every sense of the am-
biguous word has a P value, a R value and a F 
value. The F value in table 2 is a weighted aver-
age of all the senses. 
In the EP-based unsupervised WSD experi-
ment, a 100M corpus (People's Daily for year 
1998) is used for the EP training instances. The 
Senseval-3 data is used for the test. In our ex-
periments, a context window of (-10, +10) is 
taken. The detailed results are shown in table 3. 
4.2 Experiment Analysis and Discussion 
Experiment Evaluation Method 
Two evaluation criteria are used in the experi-
ments, which are the F-measure and precision. 
Precision is a usual criterion in WSD perform-
ance analysis. Only in recent years, the precision, 
recall, and F-measure are all taken to evaluate 
the WSD performance. 
In this paper, we will only show the f-measure 
score because it is a combined score of precision 
and recall. 
Result Analysis on Bayesian Supervised WSD 
Experiment 
The experiment results in table 2 reveals that the 
results of supervised WSD and those of (Qin and 
Wang, 2005) are different. Although they are all 
based on the Bayesian model, Qin and Wang 
(2005) used an ensemble classifier. However, the 
difference of the average value is not remarkable. 
As introduced above, in the supervised WSD 
experiment, the various senses of the instances 
are evenly distributed. The lower bound as Gale 
et al (1992c) suggested should be very low and 
it is more difficult to disambiguate if there are 
more senses. The experiment verifies this reason-
ing, because the highest F-measure is less than 
90%, and the lowest is less than 60%, averaging 
about 70%. 
With the same number of senses and the same 
scale of training data, there is a big difference 
between the WSD results. This shows that other 
factors exist which influence the performance 
other than the number of senses and training data 
size. For example, the discriminability among the 
senses is an important factor. The WSD task be-
comes more difficult if the senses of the ambigu-
ous word are more similar to each other. 
Experiment Analysis of the EP-based WSD 
The EP-based unsupervised method takes the 
same open test set as the supervised method. The 
unsupervised method shows a better performance, 
with the highest F-measure score at 100%, low-
est at 59% and average at 80%. The results 
shows that EP is useful in unsupervised WSD. 
 
Sequence 
Number Ambiguous word F-measure
Sequence 
Number Ambiguous word 
F-measure 
(%) 
1 ??(ba3 wo4) 0.93 11 ??(mei2 you3) 1.00 
2 ?(bao1) 0.74 12 ??(qi3 lai2) 0.59 
3 ?(cai2 liao4) 0.80 13 ?(qian2) 0.71 
4 ??(chong1 ji1) 0.85 14 ??(ri4 zi3) 0.62 
5 ?(chuan1) 0.79 15 ?(shao3) 0.82 
6 ??(di4 fang1) 0.78 16 ??(tu1 chu1) 0.93 
7 ??(fen1 zi3) 0.94 17 ??(yan2 jiu1) 0.71 
8 ??(yun4 
dong4) 
0.94 18 ??(huo2 dong4) 0.89 
9 ?(lao3) 0.85 19 ?(zou3) 0.68 
10 ?(lu4) 0.81 20 ?(zuo4) 0.67 
Average 0.80 Note: Average of the 20 words 
Table 3. The Results for Unsupervised WSD based on EPs 
462
 
From the results in table 2 and table 3, it can 
be seen that 16 among the 20 ambiguous words 
show better WSD performance in unsupervised 
SWD than in supervised WSD, while only 2 of 
them shows similar results and 2 performs worse . 
The average F-measure of the unsupervised 
method is higher by more than 10%. The reason 
lies in the following aspects: 
1) Because there are several morpheme words 
for every sense of the word in construction of the 
EP, rich semantic information can be acquired in 
the training step and is an advantage for sense 
disambiguation. 
2) Senseval-3 has provided a small-scale train-
ing set, with 15-20 training instances for each 
sense, which is not enough for the WSD model-
ing. The lack of training information leads to a 
low performance of the supervised methods. 
3) With a large-scale training corpus, the un-
supervised WSD method has got plenty of train-
ing instances for a high performance in disam-
biguation. 
4) The discriminability of some ambiguous 
word may be low, but the corresponding EPs 
could be easier to disambiguate. For example, 
the ambiguous word "?" has two senses which 
are difficult to distinguish from each other, but 
its Eps' senses of "??/??/??" and "?/?/
?/?"can be easily disambiguated. It is the same 
for the word "??", whose Eps' senses are "?
?/?? /??" and "??/??". EP-based 
knowledge acquisition of these ambiguous words 
for WSD has helped a lot to achieve high per-
formance. 
5 Conclusion 
As discussed above, the supervised WSD method 
shows a low performance because of its depend-
ency on the size of the training data. This reveals 
its weakness in knowledge acquisition bottleneck. 
EP-based unsupervised method has overcame 
this weakness. It requires no manually tagged 
corpus to achieve a satisfactory performance on 
WSD. Experimental results show that EP-based 
method is a promising solution to the large-scale 
WSD task. In future work, we will examine the 
effectiveness of EP-based method in other WSD 
techniques. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1991. Word-
Sense Disambiguation Using Statistical Methods. 
In Proc. of the 29th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-1991), 
pages 264-270. 
Mona Talat Diab. 2003. Word Sense Disambiguation 
Within a Multilingual Framework. PhD thesis, 
University of Maryland College Park. 
Mona Diab. 2004a. Relieving the Data Acquisition 
Bottleneck in Word Sense Disambiguation. In Proc. 
of the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 303-
310. 
Mona T. Diab. 2004b. An Unsupervised Approach for 
Bootstrapping Arabic Sense Tagging. In Proc. of 
Arabic Script Based Languages Workshop at COL-
ING 2004, pages 43-50. 
Mona Diab and Philip Resnik. 2002. An Unsuper-
vised Method for Word Sense Tagging Using Par-
allel Corpora. In Proc. of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2002), pages 255-262. 
William Gale, Kenneth Church, and David Yarowsky. 
1992a. Using Bilingual Materials to Develop Word 
Sense Disambiguation Methods. In Proc. of the 4th 
International Conference on Theoretical and Meth-
odolgical Issues in Machine Translation(TMI-92), 
pages 101-112. 
William Gale, Kenneth Church, and David Yarowsky. 
1992b. Work on Statistical Methods for Word 
Sense Disambiguation. In Proc. of AAAI Fall Sym-
posium on Probabilistic Approaches to Natural 
Language, pages 54-60. 
William Gale, Kenneth Ward Church, and David 
Yarowsky. 1992c. Estimating Upper and Lower 
Bounds on the Performance of Word Sense Disam-
biguation Programs. In Proc. of the 30th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-1992), pages 249-256. 
Tanja Gaustad. 2001. Statistical Corpus-Based Word 
Sense Disambiguation: Pseudowords vs. Real Am-
biguous Words. In Proc. of the 39th ACL/EACL, 
Student Research Workshop, pages 61-66. 
Nancy Ide, Tomaz Erjavec, and Dan Tufi?. 2001. 
Automatic Sense Tagging Using Parallel Corpora. 
In Proc. of the Sixth Natural Language Processing 
Pacific Rim Symposium, pages 83-89. 
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. 
Sense Discrimination with Parallel Corpora. In 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions, pages 54-60. 
Cong Li and Hang Li. 2002. Word Translation Dis-
ambiguation Using Bilingual Bootstrapping. In 
Proc. of the 40th Annual Meeting of the Association 
463
for Computational Linguistics (ACL-2002), pages 
343-351. 
Rada Mihalcea and Dan Moldovan. 2000. An Iterative 
Approach to Word Sense Disambiguation. In Proc. 
of Florida Artificial Intelligence Research Society 
Conference (FLAIRS 2000), pages 219-223. 
Rada F. Mihalcea. 2002. Bootstrapping Large Sense 
Tagged Corpora. In Proc. of the 3rd International 
Conference on Languages Resources and Evalua-
tions (LREC 2002), pages 1407-1411. 
Preslav I. Nakov and Marti A. Hearst. 2003. Cate-
gory-based Pseudowords. In Companion Volume to 
the Proceedings of HLT-NAACL 2003, Short Pa-
pers, pages 67-69. 
Hwee Tou. Ng, Bin Wang, and Yee Seng Chan. 2003. 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In Proc. of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), pages 455-462. 
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 
2005. Word Sense Disambiguation Using Label 
Propagation Based Semi-Supervised Learning. In 
Proc. of the 43th Annual Meeting of the Association 
for Computational Linguistics (ACL-2005), pages 
395-402. 
Ying Qin and Xiaojie Wang. 2005. A Track-based 
Method on Chinese WSD. In Proc. of Joint Sympo-
sium of Computational Linguistics of China (JSCL-
2005), pages 127-133. 
Hinrich. Schutze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1): 97-
123. 
David Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent Res-
toration in Spanish and French. In Proc. of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics(ACL-1994), pages 88-95. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. of the 33rd Annual Meeting of the Association 
for Computational Linguistics (ACL-1995), pages 
189-196. 
 
464
Combining Neural Networks and Statistics for 
 Chinese Word sense disambiguation 
Zhimao Lu  Ting Liu  Sheng Li  
Information Retrieval Laboratory of Computer Science & Technology School,  
Harbin Institute of Technology 
Harbin, China, 150001 
{lzm, tliu}@ir.hit.edu.cn 
 
Abstract 
The input of network is the key problem for 
Chinese Word sense disambiguation utilizing 
the Neural Network. This paper presents an 
input model of Neural Network that calculates 
the Mutual Information between contextual 
words and ambiguous word by using statistical 
method and taking the contextual words to 
certain number beside the ambiguous word 
according to (-M, +N). The experiment adopts 
triple-layer BP Neural Network model and 
proves how the size of training set and the 
value of M and N affect the performance of 
Neural Network model. The experimental 
objects are six pseudowords owning three 
word-senses constructed according to certain 
principles. Tested accuracy of our approach on 
a close-corpus reaches 90.31%,, and 89.62% on 
a open-corpus. The experiment proves that the 
Neural Network model has good performance 
on Word sense disambiguation. 
1 Introduction 
It is general that one word with many senses in 
natural language. According statistics, there are 
about 42% ambiguous words in Chinese corpus (Lu, 
2001). Word sense disambiguation (WSD) is a 
method to determine the sense of ambiguous word 
given the context circumstance.  
WSD, a long-standing problem in NLP, has been 
a very active research topic,, which can be well 
applied in many NLP systems, such as Information 
Retrieval, Text Mining, Machine Translation, Text 
Categorization, Text Summarization, Speech 
Recognition, Text to Speech, and so on. 
With rising of Corpus linguistics, the machine 
learning methods based on statistics are booming 
(Yarowsky, 1992). These methods draw the support 
from the high-powered computers, get the statistics of 
large real-world corpus, find and acquire knowledge 
of linguistics automatically. They deal with all change 
by invariability, thus it is easy to trace the evaluation 
and development of natural language. So the statistic 
methods of NLP has attracted the attention of 
professional researchers and become the mainstream 
bit by bit. Corpus-based Statistical approaches are  
Decision Tree (Pedersen, 2001), Decision List, 
Genetic Algorithm, Naive-Bayesian Classifier 
(Escudero, 2000)?Maximum Entropy Model (Adam, 
1996; Li, 1999), and so on.  
Corpus-based statistical approaches can be divided 
into supervised and unsupervised according to whether 
training corpus is sense-labeled text. Supervised 
learning methods have the good learning ability and 
can get better accuracy in WSD experiments (Sch?tze, 
1998). Obviously the data sparseness problem is a 
bottleneck for supervised learning algorithm. If you 
want to get better learning and disambiguating effect, 
you can enlarge the size and smooth the data of 
training corpus. According to practical demand, it 
would spend much more time and manpower to 
enlarge the size of training corpus. Smoothing data is 
merely a subsidiary measure. The sufficient large size 
of training corpus is still the foundation to get a 
satisfied effect in WSD experiment. 
Unsupervised WSD never depend on tagged 
corpus and could realize the training of large real 
corpus coming from all kinds of applying field. So 
researchers begin to pay attention to this kind of 
methods (Lu, 2002). The kind of methods can 
overcome the sparseness problem in a degree. 
It is obvious that the two kinds of methods based 
on statistic have their own advantages and 
disadvantages, and cannot supersede each other.  
This paper researches the Chinese WSD using the 
model of artificial neural network and investigates 
the effect on WSD from input model of neural 
network constructed by the context words and the 
size of training corpus.  
2 BP Neural Network 
At the moment, there are about more than 30 kinds of 
artificial neural network (ANN) in the domain of 
research and application. Especially, BP neural 
network is a most popular model of ANN nowadays.  
2.1 The structure of BP Neural Network 
The BP model provides a simple method to 
calculate the variation of network performance 
cased by variation of single weight. This model 
contains not only input nodes and output nodes, but 
also multi-layer or mono-layer hidden nodes. Fig1.1 
is a construction chart of triple-layer BP neural 
network. As it is including the weights modifying 
process from the output layer to the input layer 
resulting from the total errors, the BP neural 
network is called Error Back Propagation network. 
 
 
 
 
 
 
 
 
 
Fig. 1.1 BP Network 
Fig.1.1 The structure of BP neural network 
Except for the nodes of input layer, all nodes of 
other layers are non-linear input and output. So the 
feature function should be differential on every part 
of function. General speaking, we can choose the 
sigmoid, tangent inspired, or linear function as the 
feature function because they are convenient for 
searching and solving by gradient technique. 
Formula (1) is a sigmoid function.  
                         ?1? 
The output of sigmoid function ranges between 0 
and 1, increasing monotonically with its input. 
Because it maps a very large input domain to a 
small range of outputs, it is often referred to as the 
squashing function of the unit. The output layer and 
hidden layer should adopt the sigmoid inspired 
function under the condition of intervention on the 
output, such as confining the output between 0 and 
1.  
2.2 Back Propagation function of BP 
neural network 
The joint weights should be revised many times 
during the progress of the error propagating back in 
BP networks. The variation of joint weights every 
time is solved by the method of gradient descent. 
Because there is no objective output in hidden layer, 
the variation of joint weight in hidden layer is 
solved under the help of error back propagation in 
output layer. If there are many hidden layers, this 
method can reason out the rest to the first layer by 
analogy. 
1) the variation of joint weights in output layer 
To calculate the variation of joint weights from 
input i?th to output k?th is as following: 
?wik = ??      = ??               (2) 
 = ?(tk - Ok )f2? O?i = ? ?ik O?i 
?ik = (tk ? Ok )f2?                    (3) 
?bki = ??       = ??               (4) 
= ?(tk - Ok )f2? = ? ?ik 
 
2) the variation of joint weights in hidden layer 
To calculate the variation of joint weights from 
input j?th to output i?th is as following: 
?w?ij = -?       = -?                   (5) 
1
1 + e-xf (x)  = ???
?E ????wik
?E ??? ?Ok 
?Ok ??? ? wik 
?E ???? w?ij
n?
k=1 
?E ????bki 
?E ??? ?Ok 
?Ok ??? ?bki 
?? ??
?? 
??
x1  x3 xn x2 
y1  y3 ym y2 
Outputs 
Hidden 
Inputs 
?E ??? ?Ok 
?Ok ????O?i 
?O?i???
? w?ij
n?
k=1 
= ?    (tk - Ok )f2? wik f1?pj  
= ? ?ij pj 
where:  ?ij = ei f1??ei =    ?ik wik          (6) 
     ?b?ki = ? ?ij                          (7) 
3. The construction of WSD model 
Under the consideration of fact that only 
numerical data can be accepted by the input and 
output of neural network, if BP neural network is 
used on WSD, the prerequisite is to vector the part of 
semantic meaning (words or phrases) and sense. 
In the event of training BP model, the input vector 
P and objective vector O of WSD should be 
determined firstly. And then we should choose the 
construction of neural network that needs to be 
designed, say, how many layers is network, how 
many neural nodes are in every layer, and the 
inspired function of hidden layer and output layer. 
The training of model still needs the vector added 
weight, output, and error vector. The training is over 
when the sum of square errors is less than the 
objection of error. Or the errors of output very to 
adjust the joint weight back and repeat the training. 
3.1 To vector the vocabulary 
WSD depends on the context to judge the meaning 
of ambiguous words. So the input of model should 
be the ambiguous words and the contextual words 
round them. In order to vector the words in the 
context, the Mutual Information (MI) of ambiguous 
words and context should be calculated. So MI can 
show the opposite distance of ambiguous words and 
contextual words. MI can replace every contextual 
word. That is suitable to as the input model. The 
function of MI is as follow: 
(8) 
  P(w1) and P(w2) are the probability of word w1 
and w2 to appear in the corpus separately. While 
P(w1, w2) is the probability of word w1 and w2 to 
appear together.  
The experimental corpus in this article stems 
from the People Daily of 1998. The extent is 
123,882 lines (10,000,000 words), including 
121,400 words and phrases.  
3.2 The pretreatment of BP network model 
The supervised WSD need artificial mark of 
meaning. But it is time consuming to mark artificially. 
So it is difficult to get the large scope and high quality 
training linguistic corpus. In order to overcome this 
difficulty and get large enough experimental linguistic 
corpuses, we should turn to seek the new way. 
We use pseudoword in place of the real word. That 
can get the arbitrary large experimental corpus 
according to the real demand.  
3.2.1 The construction of Pseudoword 
Pseudoword is the artificial combination of 
several real words on the basis of experimental 
demand to form an unreal word that possesses 
many features of real words and instead of real 
word as the experimental object in natural language 
research.  
In the real world, one word has many meanings 
derives from the variation and flexible application 
of words. That needs a long-term natural evolution. 
Frankly speaking, that evolution never ceases at all 
times. For example, the word ?? ?(da3) extends 
some new uses in recent years. Actually, in the 
endless history river of human beings, the 
development and variation of words meaning are 
rapid so far as to be more rapid than the 
replacement of dictionaries sometimes. Usually that 
makes an awkward position when you use 
dictionary to define the words meanings. Definitely, 
it is inconvenient for the research of natural 
linguistics based on dictionary.  
But the meaning of pseudoword (Sch?tze, 1992) 
need not defined with the aid of dictionary and 
simulates the real ambiguous word to survey the 
effect of various algorithms of classified meanings.  
To form a pseudoword need the single meaning 
word as a morpheme.  
Set:   Wp =  w1 / w2 / ? / wi 
Wp is a pseudoword formed with wi which 
contains i algorithms and meanings for every 
n ?
k=1 
MI(w1, w2) = 
P(w1, w2) ?????P(w1)P(w2) log 
n ?
k=1 
algorithm of pseudoword is single meaning and 
every living example is about equal to a 
pseudoword marked meaning in corpus. That is 
similar to the effect of artificial marked meaning. 
But the effect is more stable and reliable than 
artificial marked meaning. What?s more, the scope 
of corpus can enlarge endless according to the 
demand to avoid the phenomenon of sparse data. 
To define the number of algorithm, we count the 
average number of meanings according to the 
large-sized Chinese dictionaries (Table 3.1). Table 
3.2 show the overall number of ambiguous word 
and percentage of ambiguous word having 2~4 
meanings in all ambiguous word. These two charts 
indicate that verb is most active in Chinese and its 
average number of meanings is most, about 2.56. 
The percentage of ambiguous word having 2~4 
meanings is most in all ambiguous word.  
part of 
speech 
Average 
sense 
?including 
single-sense 
word? 
Average sense
?only 
ambiguous 
word? 
noun 1.136452 2.361200 
verb 1.220816 2.558158 
adjective 1.144717 2.300774 
adverb 1.059524 2.078431 
Table 3.1 the average number of a Chinese 
word?s sense 
3.2.2 Define the input vector  
It should be based on context to determine the 
sense of ambiguous word. The model?s input should 
be the vector of the ambiguous word and context 
words. It is well-known that the number of context  
ambiguous 
word 7955 / 
Bi-senses 
word 5799 72.80% 
Tri-senses 
word 1154 14.51% 
Four-senses 
word 450 5.66% 
Table 3.2 the distributing of ambiguous word 
words showing on the both sides of ambiguous 
word is not fixed in different sentences. But the 
number of vectors needed by BP network is fixed. 
In other words, the number of neural nodes of input 
model is fixed in the training. If the extracting 
method of feature vector is (-M, +N) in context, in 
other words there are M vectors on the left of 
ambiguous word and N vectors on the right, the 
extraction of feature vectors must span the limit of 
sentences. If the number of feature vectors is not 
enough, the ambiguous words on the left and right 
boundaries of whole corpus do not participate in the 
training.  
According to the extracting method of feature 
vector (-M, +N), the vector of model input is as 
following: 
V ?? = {MI11?MI 12???MI1i?MI 11??MI 12????
MI 1j??MI21?MI 22???MI2i?MI 21??MI 22????
MI2j??MI31?MI 32???MI3i?MI 31??MI 32????
MI 3j?}?1?i?M?1?j?N. 
Where, MI1i , MI1j? are the MI of context and the 
first meaning of ambiguous word?MI2i , MI2j? are 
the MI of context and the second meaning of 
ambiguous word?MI3i ?MI3j? are the MI of context 
and the third meaning of ambiguous word. MI1i, 
MI2i and MI3i are the feature words of ambiguous 
word on the left and MI of ambiguous word. MI1j?, 
MI2j? and MI2j? are the feature words of ambiguous 
word on the right and MI of ambiguous word.  
pseudo-
words
word
ID 
sample 
number 
pseudo-
words 
word 
ID 
sample
number
34466 5550 84323 3773
71345 3715 12751 2284
31796 12098 52915 3900W1 
total 21363
W4 
total 9957
71072 9296 53333 1362
78031 6024 29053 6135
48469 1509 75941 1205W2 
total 16829
W5 
total 8702
7464 25925 39945 2346
77375 2478 71335 1640
23077 4704 51491 1012W3 
total 33107
W6 
total 4998
Table 3.3 the total number of the feature -vector 
sample of ambiguous word 
Training corpus are 105,000 lines, and each line 
is a paragraph, totally about 10,000,000 words. 
Table 3.3 shows the number of collected feature 
vector samples (the frequency of ambiguous word).  
3.3 The definition of output model 
Every ambiguous word has three meanings, 
totally eighteen meanings for six ambiguous words. 
Every ambiguous word trains a model and every 
model has three outputs showed by three-bit integer 
of binary system, such as the three meanings of 
ambiguous word W are showed as followed: 
si1 = 100    si2 = 010   si3 = 001 
3.4 The definition of network structure 
According to statistics, when (-M, +N) are (-8, 
+9) using the method of feature extraction, the 
cover percentage of effective information is more 
than 87% (Lu, 2001). However, if the sentence is 
very short, collecting the contextual feature words 
on the basis of (-8, +9) can include much useless 
information to the input model. Undoubtedly, that 
will increase more noisy effect and deduce the 
meaning-distinguish ability of verve network.  
This article makes an on-the-spot investigation of 
experimental corpus, a fairly integrated meaning 
unit (the marks of border including comma, 
semicolon, ellipsis, period, question mark, 
exclamation mark, and the like), which average 
length is between 9~10 words. So this article 
collects the contextual feature words on the basis of 
(-5, +5) in the experiments, 10 feature words 
available that calculate MI with each meaning of 
ambiguous word separately to get 30 vectors. All 
punctuation marks should be filtered while the 
feature words are collected. The input layer of 
neural network model is regarded as 30 neural 
nodes. The triple-layer neural network adopts the 
inspired S function. From that, the number of neural 
nodes in hidden layer is defined as 12 on the basis 
of experimental contrast, and 3 neural nodes in 
output layer. Hence, the structure of model is 30 ? 
12 ? 3, and the precision of differential training is 
defined as 0.3 based on the experimental contrast. 
3.5 The test and training of model 
The experimental corpus appeared in front are 
123,882 lines. It is divided to three parts according 
to the demand of experiment, C1 (15,000 lines), C2 
(60,000 lines), and C3 (105,000 lines). The open 
test corpus is 18,882 lines. 
Table 3.3 tells us that there is a great disparity 
between the sample numbers of different 
ambiguous words in the experimental corpus of the 
same class. And the distribution of different 
meanings is not even for same ambiguous word. 
For the trained neural network has the good ability 
of differentiation for each word, the number of 
training sample should be about equal to each other 
for each meaning. So this experiment selects the 
least training samples. For example, there are 200 
samples of the first meaning in training corpus, the 
second 400, and the third 500. To balance the input, 
each meaning merely has 200 samples to be elected 
for training. 
Three groups of training corpus can train 3 neural 
networks possessing different vectors for every 
ambiguous word and make the unclose and open 
test for these networks separately.  
4 The result of experiment 
In order to analyze the effect that the extent of 
training corpus influences the meaning distinguish 
ability of neural network, this article trains the 
model of neural network using the experimental 
corpus individually, C1, C2 and C3, and makes the 
close and open test for 6 ambiguities separately.  
The close test means the corpus are same in test 
and training.  
The experiment is divided into two groups 
according to the extracting method of contextual 
feature words.  
4.1 The first experiment one 
Table 4.1 shows the result of the first experiment  
which extracts the contextual feature words using 
the method of ??5?+ 5?. 
In addition, the first experiment investigates that 
the extent of training corpus (the number of training 
samples big or small) influences the ability to 
distinguish the models. The result of test for 6  
close-test open-test pseudo- 
words accuracy Training set accuracy 
Training
set 
W1 0.8800 C2 0.8951 C3 
W2 0.8867 C2 0.8775 C2 
W3 0.8652 C3 0.8574 C3 
W4 0.8532 C3 0.8687 C3 
W5 0.8769 C3 0.8745 C3 
W6 0.8868 C2 0.8951 C3 
Table 4.1 The contrast chat of experimental result 
for six ambiguities 
ambiguities is showed in table 4.2 (close test), table 
4.3 (open test), and table 4.4. Considering the 
length of this article, table 4.2 and table 4.3 shows 
the detailed data, and table 4.4 is brief.  
Training set pseudo- 
words C1 C2 C3 
sense 1 0.9226 0.8169 0.8991
sense 2 0.5513 0.8017 0.6872
sense 3 0.8027 0.9564 0.9510W1 
average 0.7589 0.8800 0.8720
sense 1 0.8121 0.8780 0.9377
sense 2 0.8389 0.8968 0.8804
sense 3 0.7248 0.8856 0.8370
W6 
average 0.7919 0.8868 0.8850
Table 4.2 The result of W1 and W6 in close test  
under the different training corpus 
4.2 The second experiment 
The second experiment investigates emphatically 
the effect that the method to collect the feature 
words influences the ability to distinguish BP 
model. 
Training set pseudo- 
words C1 C2 C3 
sense 1 0.9019 0.7827 0.8942
sense 2 0.4607 0.8097 0.7175
sense 3 0.7792 0.9500 0.9515W1 
average 0.7573 0.8798 0.8951
sense 1 0.8233 0.9093 0.9535
sense 2 0.8799 0.8182 0.8604
sense 3 0.7278 0.8544 0.8038W6 
average 0.8259 0.8683 0.8951
Table 4.3 The result of W1 and W6 in open test 
 under the different training corpus 
There are many methods adopted in this 
experiment, including (?10?+ 10),??3?+ 3?,??3?
+ 7?,??7?+ 3?,??4?+ 6?and??6?+ 4?. Merely 
the ambiguous words W1 and W6 are regarded as the 
Training set pseudo- 
words C1 C2 C3 
W2 0.6628 0.8867 0.8772
W3 0.6695 0.8453 0.8652
W4 0.7414 0.8452 0.8532
W5
close
0.8283 0.8537 0.8769
W2 0.7287 0.8613 0.8700
W3 0.8085 0.8384 0.8574
W4 0.7920 0.8655 0.8687
W5
open
0.8288 0.8775 0.8745
Table 4.4 The contrast chart of experimental 
 result for four ambiguities 
Table 4.5 the experimental result under different 
feature collecting method 
experimental objects in this group experiment. See 
table 4.5 for the correct percentage of WSD.  
5. Analysis and discussion  
See table 5.1 for the number of experimental 
corpus samples in experiment.  
According to the table 3.3 and 5.1, the frequency of 
the each meaning (morpheme) of ambiguous word 
showing in corpus is quite different. That accords 
with the distribution of the every meanings of 
ambiguous word. However, there is one different 
point that the frequency of the each meaning of 
ambiguous word is rather high (that is the outcome 
selected by morpheme.). In other words, there are 
many examples showing for the each meaning of 
ambiguous word in training and test corpus. On the 
contrast, the difference of frequency is quite  
accuracy pseudo-
words
feature 
collecting 
method close-test open-test
Training
set 
??10?+10? 0.8897 0.8685 C1 
??3?+3? 0.7917 0.7176 C2 
??4?+6? 0.8600 0.8888 C3 
??6?+4? 0.8797 0.8938 C2 
??3?+7? 0.8514 0.8827 C3 
W1 
??7?+3? 0.8431 0.8825 C3 
??10?+10? 0.9031 0.8962 C2 
??3?+3? 0.8487 0.8460 C2 
??4?+6? 0.8982 0.8873 C3 
??6?+4? 0.8480 0.8772 C2 
??3?+7? 0.8669 0.8359 C3 
W6 
??7?+3? 0.8982 0.8895 C3 
pseudo- 
words 
Morpheme 
ID 
sample 
number 
pseudo- 
words 
Morpheme
ID 
sample
number
34466 1040 84323 591
71345 662 12751 484W1 
31796 2101 
W4 
52915 829
71072 1296 53333 274
78031 1043 29053 1153W2 
48469 315 
W5 
75941 238
7464 4389 39945 430
77375 469 71335 308W3 
23077 865 
W6 
51491 158
Table 5.1 The number of experimental samples 
obvious for the each meaning of real ambiguous 
word, because some meanings are used in oral 
language. But that never or seldom appears in 
experimental corpus.  
The statistics can uncover this linguistic 
phenomenon. We find that the meaning of the most 
percentage of ambiguous word showing in the 
corpus is 83.54% on the whole percentage of each 
meaning. That illustrates the distribution of each 
meaning has a great disparity in real ambiguous 
word. Seeing that condition, to differentiate the 
meaning of ambiguous word is harder than that of 
real ambiguous word absolutely.  
5.1 The analysis and discussion of the first 
experiment 
Table 4.1 records the results of close and open 
tests in detail and the training materials to get these 
results.  
Seeing from the experimental results, the correct 
percentage reaches 89.51% most (ambiguous word 
W1 and W6) in open test of WSD, and 85.74% the 
least (ambiguous word W3). 
The relationship of correct percentage and the 
extent of training corpus can be deduced from the 
experimental results of table 4.2, 4.3 and 4.4. 
The larger the extent of training corpus (the 
number of training sample?, the larger the result of 
close test. It is obvious to see that from C1 to C2. 
From C2 to C3 one or two experimental results 
fluctuate more or less.  
With the growing of training sample, the 
experimental results of open test increase steadily, 
except ambiguous word W2 (a little bit difference).  
The experimental data prove the growing of 
training samples rise the correct percentage. 
However, when the rising reaches to a certain 
degree, more rising is not good for the improvement 
of model. What?s more, the effect of noise is more 
and more remarkable. That decreases the model?s 
ability of differentiation in a certain degree. On the 
other hand, after the growing of training corpus, the 
linguistic phenomenon around ambiguities is richer 
and richer, more and more complex. That makes it 
harder to determine the meaning.  
5.2 The analysis and discussion of the second 
experiment 
This article emphasizes on the collecting method 
of contextual feature words in experiment two, in 
other words, the effect that the different values of M 
and N influence the model of BP network. The 
experimental results (table 4.1 and 4.5) tell us that 
the context windows influence the correct 
percentage heavily. The correct percentage 
increases almost by leaps and bounds from (?3?+ 3) 
to??5?+5?. The discrepancy is obvious despite 
close test or open test. The correct percentage 
increase again to??10?+ 10?, in which the close test 
of ambiguous word W6 is more than 90% and 
89.62% the close test, with the exception of W1 
which open test is slightly special. That illustrates 
the more widely the context windows open, the 
more the effective information is caught to benefit 
the WSD more. 
Comparing the four feature methods of collection, 
including ??3?+ 7?,??7?+ 3?,??4?+ 6?
and??6?+ 4? with??5?+ 5?, the number of feature 
words besides the ambiguous word is various and 
the experimental results (table 4.1 and 4.5) are not 
same, although the windows are same. Among them, 
the correct percentage of ??5?+ 5?is the highest. 
And that of??4?+ 6?and??6?+ 4?is better than 
that of??3?+ 7?and??7?+ 3?a bit. That shows 
the more balanceable the feature words besides 
ambiguous word, the more advantageous to judge 
meaning, and the better the experimental results.  
In addition, some experimental results of open 
test are better than that of close test. The main 
reason is the experimental corpus of open test is 
smaller than training corpus. So the contextual 
meanings of ambiguous word in experimental 
corpus are rather explicit. Thereby, that explains 
why should be this kind of experimental result.  
5.3 Conclusions 
Considering the analysis of experimental data, 
the conclusions are as following: 
First, the artificial model of neural network 
established in this article has good ability of 
differentiation for Chinese meaning.  
Next, higher correct percentage of WSD stems 
from the large enough corpus. 
At last, the larger the windows of contextual 
feature words, the more the effective information. 
At the same time, the more balanceable the number 
of feature words beside the ambiguous word, the 
more beneficial that for WSD.  
6 Concluding remarks 
Although the BP network is a classified model 
applied extensively, the report of research on WSD 
about it is seldom. Especially the report about the 
Chinese WSD is less, and only one report (Zhang, 
2001) is available in internal reports.  
Zhang (2001) uses 96 semantic classes to instead 
the all words in training corpus according to the 
TongyiciCilin. The input model is the codes of 
semantic class of contextual words and ambiguities. 
The experiment of WSD merely makes for one 
phrase ? ?(cai2liao4) in this document and the 
correct percentage of open test is 80.4%. ? ? 
has 3 meanings and that is similar to the 
ambiguities structured in my article.  
Using BP for Chinese WSD, the key point and 
difficulty are on the determination of input model. 
The performance of input model may influence the 
construction of BP network and the output result 
directly.   
We make the experiment on the input of BP 
network many times and finally find the input 
model introduced as above (table 3.1) which test 
result is satisfied.  
Acknowledgements This work was supported by 
the National Natural Science Foundation of China 
(Grant No. 60203020). 
 
Reference 
Lu Song, Bai Shuo, et al 2001. Supervised word 
sense disambiguation bassed on Vector Space 
Model, Journal of Comouter Research & 
Development, 38(6): 662-667. 
Pedersen. 2001. Lexical semantic ambiguous word 
resolution with bigram-based decision trees, In 
Proceedings of the Second International Conference 
on Intelligent Text Processing and Computational 
Linguistics, pages 157-168, Mexico City, February. 
Escudero, G., Marquez,L., et al 2000. Naive Bayes 
and examplar based approaches to word sense    
disambiguation revisited. In Proceedings of the 14th 
Europear Conference on Artificial Intelligence, 
ECAI. 
Adam,L.B. 1996. A maximum entropy approach to 
natural language proceeding. Computational 
Linguistics, 22(1):39-71. 
Li, J. Z. 1999. An improved maximum language 
and its application. Journal of software, 3:257-263. 
Yarowsky, D. Word sense disambiguation using 
statistical models of Roget?s categories trained on 
large corpora. In: Zampolli, A., ed. Computation 
Linguistic?92. Nantas: Association for 
Computational Linguistics, 1992. 454~460.  
Hinrich Sch?tze, 1998. Automatic word sense 
discrimination. Computational Linguistics, 24(1): 
97-124. 
Lu Song., Bai Shuo. 2002. An unsupervised 
approach to word sense disambiguation based on 
sense-word in vector space model. Journal of 
Software. 13(06):1082-08 
Hinrich Sch?tze. 1992. Context space. In AAAI 
Fall Symposium on Probabilistic Approaches to 
Natural Language, pages 113?120, Cambridge, 
MA. 
Lu Song Bai Shuo. 2001. Quantitative Analysis of 
Context Field. In Natural Language Processing, 
CHINESEJ.COMPUTERS, 24(7) , 742-747 
Zhang Guoqing, Zhang Yongkui. 2001. A 
Neural-network Based Word Sense Disambiguation 
Method. Computer Engineering, 27(12). 
