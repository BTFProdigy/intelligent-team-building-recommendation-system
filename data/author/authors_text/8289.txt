Using Maximum Entropy to Extract Biomedical Named Entities
without Dictionaries
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu
Institute of Information Science, Academia Sinica
Nankang, Taipei, Taiwan 115
{thtsai, cwwu, hsu}@iis.sinica.edu.tw
Abstract
Current NER approaches include:
dictionary-based, rule-based, or ma-
chine learning. Since there is no
consolidated nomenclature for most
biomedical NEs, most NER systems
relying on limited dictionaries or rules
do not perform satisfactorily. In this
paper, we apply Maximum Entropy
(ME) to construct our NER framework.
We represent shallow linguistic infor-
mation as linguistic features in our ME
model. On the GENIA 3.02 corpus, our
system achieves satisfactory F-scores
of 74.3% in protein and 70.0% overall
without using any dictionary. Our
system performs significantly better
than dictionary-based systems. Using
partial match criteria, our system
achieves an F-score of 81.3%. Using
appropriate domain knowledge to
modify the boundaries, our system has
the potential to achieve an F-score of
over 80%.
1 Introduction
Biomedical literature available on the web has ex-
perienced unprecedented growth in recent years.
Therefore, demand for efficiently processing
these documents is increasing rapidly. There has
been a surge of interest in mining biomedical
literature. Some possible applications for such
efforts include the reconstruction and prediction
of pathways, establishing connections between
genes and disease, finding the relationships be-
tween genes, and much more.
Critical tasks for biomedical literature min-
ing include named entity recognition (NER), to-
kenization, relation extraction, indexing and cate-
gorization/clustering (Cohen and Hunter, 2005).
Among these technologies, NER is most fun-
damental. It is defined as recognizing objects
of a particular class in plain text. Depending
on required application, NER can extract objects
ranging from protein/gene names to disease/virus
names.
In general, biomedical NEs do not follow any
nomenclature (Shatkay and Feldman, 2003) and
can comprise long compound words and short ab-
breviations (Pakhomov, 2002). Some NEs con-
tain various symbols and other spelling variations.
On average, any NE of interest has five synonyms.
Biomedical NER is a challenging problem. There
are many different aspects to deal with. For ex-
ample, one can have unknown acronyms, abbre-
viations, or words containing hyphens, digits, let-
ters, and Greek letters; Adjectives preceding an
NE may or may not be part of that NE depend-
ing on the context and applications; NEs with the
same orthographical features may fall into differ-
ent categories; An NE may also belong to mul-
tiple categories intrinsically; An NE of one cate-
gory may contain an NE of another category in-
side it.
To tackle these challenges, researchers use
three main approaches: dictionary-based, rule-
based, and machine learning. In biomedical do-
main, there are more and more well-curated re-
sources, including lexical resources such as Lo-
268
cusLink (Maglott, 2002) and ontologies such as
MeSH (NLM, 2003). One might think that
dictionary-based systems relying solely on these
resources could achieve satisfactory performance.
However, according to (Pakhomov, 2002), they
typically perform quite poorly, with average re-
call rates in the range of only 10-30%. Rule-based
approaches, on the other hand, are more accurate,
but less portable across domains. Therefore, we
chose the machine learning approach.
Various machine learning approaches such as
ME (Kazama et al, 2002), SVM (Kazama et al,
2002; Song et al, 2004), HMM (Zhao, 2004) are
applied to NER. In this paper, we chose ME as
our framework since it is much easier to represent
various features in such a framework. In addi-
tion, ME models are flexible enough to capture
many correlated features, including overlapping
and non-independent features. We can thus use
multiple features with more ease than on an HMM
system. ME-based tagger, in particular, excel at
solving sequence tagging problems such as POS
tagging (Ratnaparkhi, 1997), general English
NER (Borthwick, 1999), and Chunking (Koeling,
2000).
In this paper, we describe how to construct a
ME-based framework that can exploit shallow lin-
guistic information in the recognition of biomed-
ical named entities. Hopefully, our experience
in integrating these features may prove useful for
those interested in constructing machine learning
based NER system.
2 Maximum Entropy Based Tagger
2.1 Formulation
In the Biomedical NER problem, we regard each
word in a sentence as a token. Each token is asso-
ciated with a tag that indicates the category of the
NE and the location of the token within the NE,
for example, B c, I c where c is a category, and
the two tags denote respectively the beginning to-
ken and the following token of an NE in category
c. In addition, we use the tag O to indicate that a
token is not part of an NE. The NER problem can
then be phrased as the problem of assigning one
of 2n + 1 tags to each token, where n is the num-
ber of NE categories. For example, one way to
tag the phrase ?IL-2 gene expression, CD28, and
NF-kappa B? in a paper is [B-DNA, I-DNA, O, O,
B-protein, O, O, B-protein, I-protein].
2.2 Maximum Entropy Modeling
ME is a flexible statistical model which assigns
an outcome for each token based on its history
and features. ME computes the probability p(o|h)
for any o from the space of all possible outcomes
O, and for every h from the space of all possi-
ble histories H . A history is all the condition-
ing data that enables one to assign probabilities
to the space of outcomes. In NER, history can
be viewed as all information derivable from the
training corpus relative to the current token. The
computation of p(o|h) in ME depends on a set of
binary-valued features, which are helpful in mak-
ing predictions about the outcome. For instance,
one of our features is: when all alphabets of the
current token are capitalized, it is likely to be part
of a biomedical NE. Formally, we can represent
this feature as follows:
f(h, o) =
?
??
??
1 : if W0-AllCaps(h)=true
and o=B-protein
0 : otherwise
(1)
Here, W0-AllCaps(h) is a binary function that
returns the value true if all alphabets of the cur-
rent token in the history h are capitalized. Given a
set of features and a training corpus, the ME esti-
mation process produces a model in which every
feature fi has a weight ?i. From (Berger et al,
1996), we can compute the conditional probabil-
ity as:
p(o|h) =
1
Z(h)
?
i
?fi(h,o)i (2)
Z(h) =
?
o
?
i
?fi(h,o)i (3)
The probability is given by multiplying the
weights of active features (i.e., those fi(h, o) =
1). The weight ?i is estimated by a procedure
called Generalized Iterative Scaling (GIS) (Dar-
roch and Ratcliff, 1972). This method improves
the estimation of weights iteratively. The ME esti-
mation technique guarantees that, for every fea-
ture fi, the expected value of ?equals the empirical
expectation of ?in the training corpus.
269
As noted in (Borthwick, 1999), ME allows
users to focus on finding features that character-
izes the problem while leaving feature weight as-
signment to the ME estimation routine. When
new features, e.g., syntax features, are added to
ME, users do not need to reformulate the model as
in the HMM model. The ME estimation routine
can automatically calculate new weight assign-
ments. More complete discussions of ME includ-
ing a description of the MEs estimation proce-
dure and references to some of the many success-
ful computational linguistics systems using ME
can be found in the following introduction (Rat-
naparkhi, 1997).
2.3 Decoding
After having trained an ME model and assigned
the proper weights ?to each feature fi, decoding
(i.e., marking up) a new piece of text becomes
simple. First, the ME module tokenizes the text.
Then, for each token, we check which features are
active and combine ?i of the active features ac-
cording to Equation 2. Finally, the probability of
a tag sequence y1...yn given a sentence w1...wn
is approximated as follows:
p(o1...on|w1...wn) ?
n?
j=1
p(oj |hj) (4)
where hj is the context for word wj . The tag-
ger uses beam search to find the most probable
sequence given the sentence. Sequences contain-
ing invalid subsequences are filtered out. For in-
stance, the sequence [B-protein, I-DNA] is in-
valid because it does not contain an ending token
and these two tokens are not in the same cate-
gory. Further details on the beam search can be
found in http://www-jcsu.jesus.cam.
ac.uk/?tdk22/project/beam.html.
3 Linguistic Features
3.1 Orthographical Features
Table 1 lists some orthographical features used
in our system. In our experience, ALLCAPS,
CAPSMIX, and INITCAP are more useful than
others.
Table 1: Orthographical features
Feature name Regular Expression
INITCAP [A-Z].*
CAPITALIZED [A-Z][a-z]+
ALLCAPS [A-Z]+
CAPSMIX .*[A-Z][a-z].* |
.*[a-z][A-Z].*
ALPHANUMERIC .*[A-Za-z].*[0-9].* |
.*[0-9].*[A-Za-z].*
SINGLECHAR [A-Za-z]
SINGLEDIGIT [0-9]
DOUBLEDIGIT [0-9][0-9]
INTEGER -?[0-9]+
REAL -?[0-9][.,]+[0-9]+
ROMAN [IVX]+
HASDASH .*-.*
INITDASH -.*
ENDDASH .*-
PUNCTUATION [,.;:?!-+]
QUOTE [???]
3.2 Context Features
Words preceding or following the target word
may be useful for determining its category. Take
the sentence ?The IL-2 gene localizes to bands
BC on mouse Chromosome 3? for example. If the
target word is ?IL-2,? the following word ?gene?
will help ME to distinguish ?IL-2 gene? from the
protein of the same name. Obviously, the more
context words analyzed the better and more pre-
cise the results. However, widening the context
window quickly leads to an explosion of the num-
ber of possibilities to calculate. In our experience,
a suitable window size is five.
3.3 Part-of-speech Features
Part of speech information is quite useful for iden-
tifying NEs. Verbs and prepositions usually indi-
cate an NEs boundaries, whereas nouns not found
in the dictionary are usually good candidates for
named entities. Our experience indicates that five
is also a suitable window size. The MBT POS
tagger (Daelemans et al, 1996) is used to provide
POS information. We trained it on GENIA 3.02p
and achieves 97.85% accuracy.
3.4 Word Shape Features
NEs in the same category may look similar (e.g.,
IL-2 and IL-4). So we have come up with sim-
ple way to normalize all similar words. Accord-
ing to our method, capitalized characters are all
replaced by ?A?, digits are all replaced by ?0?,
270
Table 2: Basic statistics for the data set
Data # abs # sen # words
GENIA 3.02 2,000 18,546 472,006 (236.00/abs)
non-English characters are replaced by ? ? (un-
derscore), and non-capitalized characters are re-
placed by ?a?. For example, Kappa-B will be nor-
malized as ?Aaaaa A?. To further normalize these
words, we shorten consecutive strings of iden-
tical characters to one character. For example,
?Aaaaa A? is normalized to ?Aa A?.
3.5 Prefix and Suffix Features
Some prefixes and suffixes can provide good
clues for classifying named entities. For example,
words which end in ?ase? are usually proteins. In
our experience, the acceptable length for prefixes
and suffixes is 3-5 characters.
4 Experiment
4.1 Datasets
In our experiment, we use the GENIA version
3.02 corpus (Kim et al, 2003). Its basic statis-
tics is summarized in Table 2. Frequencies for all
NE classes in it are showed in Table 3.
4.2 Results
In Table 4, one can see that F-scores for protein
and cell-type are comparably high. We believe
this is because protein and cell type are among
the top three most frequent categories in the train-
ing set (as shown in Table 3). One notices, how-
ever, that although DNA is the second most fre-
quent category, it does not have a high F-score.
We think this discrepancy is due to the fact that
DNA names are commonly used in proteins, caus-
ing a substantial overlap between these two cate-
gories. RNAs performance is comparably low be-
cause its training set is much smaller than those
of other categories. Cell lines performance is the
lowest since it overlaps heavily with cell type and
its training set is also very small.
In Table 5, one can see that, using the par-
tial matching criterion, the precision rates, recall
rates, and F-scores of protein names are all over
85%. The overall F-Score is 81.3%. The table
also shows that 83.9% of our systems suggestions
Table 4: NER performance of each NE category
on the GENIA 3.02 data (10-fold CV)
NE category Precision Recall F-score
protein 74.1 74.5 74.3
DNA 65.9 54.4 59.6
RNA 75.3 48.0 58.6
cell line 65.4 51.4 57.6
cell type 72.3 69.1 70.7
Overall 72.0 67.9 70.0
Table 5: Partial matching performance on the GE-
NIA 3.02 corpus (10-fold CV)
NE category Precision Recall F-score
protein 85.3 85.5 85.4
DNA 80.3 66.3 72.7
RNA 84.0 53.0 65.0
cell line 80.9 63.3 71.1
cell type 83.1 79.4 81.2
Overall 83.9 78.9 81.3
correctly identify at least one part of an NE, and
that our system tags at least one part of 78.9%
of all NEs in the test corpus. The precision rate in
all categories is over 80%, showing that , by using
appropriate post-processing methods, our system
can achieve high precision in all NE categories.
In Table 6, we compare our system with two
dictionary-based systems. One exploits hand-
crafted rules based on heuristics and protein name
dictionaries (Seki and Mostafa, 2003). We de-
note this system as ?rule + dictionary?. The other
system (Tsuruoka and Tsujii, 2004) has two con-
figurations: the first one exploits patterns to de-
tect protein names and their fragments, which
is denoted as ?dictionary expansion?; the sec-
ond one further applies naive Bayes filters to ex-
clude erroneous detections, which is denoted as
?dictionary expansion + filters?. One can see
that our system performs better than these dic-
tionary/heuristic systems by a wide margin. The
basic ?rule + dictionary? system achieves only
54.4% recall. By expanding the original dic-
tionary (?dictionary expansion?), they improve
the recall rate to 68.1%. After applying post
processing filters (?dictionary expansion + fil-
ters?), the recall rate dropped slightly, but preci-
sion increased by 25.7%. Still, our system per-
forms better than the best dictionary-based system
by 7.6%.
271
Table 3: Frequencies for NEs in each data set
Data protein DNA RNA cell type cell line All
GENIA 3.02 30,269 9,533 951 6,718 3,830 51,301
Table 6: Performance comparison between sys-
tems with and w/o dictionaries in extracting pro-
tein names on the GENIA 3.02 data
System Precision Recall F-score
our system 74.1 74.5 74.3
rule + dictionary 42.6 54.4 47.8
dictionary expansion 46.0 68.1 54.8
dictionary expansion + filters 71.7 62.3 66.6
5 Analysis and discussion
Recognition disagreement between our system
and GENIA is caused by the following two fac-
tors: Annotation problems:
1. Preceding adjective problem
Some descriptive adjectives are annotated as
parts of the following NE, but some are not.
2. Nested NEs
In GENIA, we found that in some instances
only embedded NEs are annotated while in
other instances, only the outside NE is an-
notated. However, according to the GENIA
tagging guidelines, the outside NE should be
tagged. For example, in 59 instances of the
phrase ?IL-2 gene?, ?IL-2? is tagged as a
protein 13 times, while in the other 46 it is
tagged as a DNA. This irregularity can con-
fuse machine learning based systems.
3. Cell-line/cell-type confusion
NEs in the cell line class are from certain cell
types. It is difficult even for an expert to dis-
tinguish them.
System recognition errors:
1. Misclassification
Some protein molecules or regions are mis-
classified as DNA molecules or regions.
These errors may be solved by exploiting
more context information.
2. Coordinated phrases
In GENIA, most conjunction phrases are
tagged as single NEs. However, conjunc-
tion phrases are usually composed of several
NEs, punctuation, and conjunctions such as
?and?, ?or? and ?but not?. Therefore, our
system sometimes only tags one of these NE
components. For example, in the phrase ?c-
Fos and c-Jun family members?, only ?c-
Jun family members? is tagged as a protein
by our system, while in GENIA, the whole
phrase is tagged as a protein.
3. False positives
Some entities appeared without accompany-
ing a specific name, for example, only men-
tion about ?the epitopes? rather than which
kind of epitopes. The GENIA corpus tends
to ignore these entities, but their contexts are
similar to the entities with specific names,
therefore, our system sometimes incorrectly
recognizes them as an NE.
6 Conclusion
Our system successfully integrates linguistic fea-
tures into the ME framework. Without using
any biomedical dictionaries, our system achieves
a satisfactory F-score of 74.3% in protein and
70.0% overall. Our system performs significantly
better than dictionary-based systems. Using par-
tial match criteria, our system achieves an F-score
of 81.3%. That means, with appropriate bound-
ary modification algorithms (with domain knowl-
edge), our system has the potential to achieve an
F-score of over 80%.
It is still difficult to recognize long, compli-
cated NEs and to distinguish between two over-
lapping NE classes, such as cell-line and cell-
type. This is because biomedical texts have com-
plicated syntax and involve more expert knowl-
edge than general domain news articles. An-
other serious problem is annotation inconsistency,
which confuses machine learning models and
makes evaluation difficult. Certain errors, such as
those in boundary identification, are more tolera-
ble if the main purpose is to discover relationships
272
between NEs.
In the future, we will exploit more linguistic
features such as composite features and external
features. Finally, to reduce human annotation ef-
fort and to alleviate the scarcity of available anno-
tated corpora, we will develop machine learning
techniques to learn from Web corpora in different
biomedical domains.
Acknowledgements
We are grateful for the support of National Sci-
ence Council under GRANT NSC94-2752-E-
001-001.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Computer Linguistics, 22:39?
71.
A. Borthwick. 1999. A Maximum Entropy Approach
to Named Entity Recognition. Phd thesis, New York
University.
K. Bretonnel Cohen and Lawrence Hunter. 2005. Nat-
ural language processing and systems biology. In
W. Dubitzky and F. Azuaje, editors, Artificial In-
telligence and Systems Biology, Springer Series on
Computational Biology. Springer.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part of
speech tagger-generator. In E. Ejerhed and I. Da-
gan, editors, Fourth Workshop on Very Large Cor-
pora, pages 14?27.
J. N. Darroch and D. Ratcliff. 1972. Generalized iter-
ative scaling for log-linear models. Annals of Math-
ematicl Statistics, 43:1470?1480.
J. Kazama, T. Makino, Y. Ohta, and J. Tsujii. 2002.
Tuning support vector machines for biomedical
named entity recognition. In ACL-02 Workshop on
Natural Language Processing in Biomedical Appli-
cations.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. Genia corpus - a semanti-
cally annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1).
Rob Koeling. 2000. Chunking with maximum en-
tropy models. In CoNLL-2000.
D. Maglott. 2002. Locuslink: a directory of genes. In
NCBI Handbook, pages 19?1 to 19?16.
NLM. 2003. Mesh: Medical subject headings.
S. Pakhomov. 2002. Semi-supervised maximum en-
tropy based approach to acronym and abbreviation
normalization in medical text. In the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
A. Ratnaparkhi. 1997. A simple introduction to maxi-
mum entropy models for natural language process-
ing. Technical Report Techical Report 97-08, Insti-
tute for Research in Cognitive Science University
of Pennsylvania.
Kazuhiro Seki and Javed Mostafa. 2003. An approach
to protein name extraction using heuristics and a
dictionary. In ASIST 2003.
Hagit Shatkay and Ronen Feldman. 2003. Min-
ing the biomedical literature in the genomic era:
an overview. Journal of Computational Biology,
10(6):821?855.
Yu Song, Eunju Kim, Gary Geunbae Lee, and
Byoung-kee Yi. 2004. Posbiotm-ner in the shared
task of bionlp/nlpba 2004. In the Joint Workshop on
Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), Geneva, Switzer-
land.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. Im-
proving the performance of dictionary-based ap-
proaches in protein name recognition. Journal of
Biomedical Informatics, 37(6):461?470.
Shaojun Zhao. 2004. Named entity recognition in
biomedical texts using an hmm model. In COL-
ING 2004 International Joint Workshop on Natural
Language Processing in Biomedicine and its Appli-
cations (NLPBA).
273
 
Text Categorization Using Automatically Acquired Domain Ontology
 
 
Shih-Hung Wu, Tzong-Han Tsai, Wen-Lian Hsu 
Institute of Information Science 
Academia Sinica 
Nankang, Taipei, Taiwan, R.O.C. 
shwu@iis.sinica.edu.tw, thtsai@iis.sinica.edu.tw, hsu@iis.sinica.edu.tw 
 
 
 
Abstract 
 
In this paper, we describe ontology-based 
text categorization in which the domain 
ontologies are automatically acquired 
through morphological rules and statistical 
methods. The ontology-based approach is 
a promising way for general information 
retrieval applications such as knowledge 
management or knowledge discovery. As 
a way to evaluate the quality of domain 
ontologies, we test our method through 
several experiments.  Automatically 
acquired domain ontologies, with or 
without manual editing, have been used 
for text categorization. The results are 
quite satisfactory. Furthermore, we have 
developed an automatic method to 
evaluate the quality of our domain 
ontology.  
1. Introduction 
Domain ontology, consisting of important 
concepts and relationships of the concepts in the 
domain, is useful in a variety of applications 
(Gruber, 1993). However, evaluating the quality of 
domain ontologies is not straightforward. Reusing 
an ontology for several applications can be a 
practical method for evaluating domain ontology. 
Since text categorization is a general tool for 
information retrieval, knowledge management and 
knowledge discovery, we test the ability of 
domain ontology to categorize news clips in this 
paper. 
Traditional IR methods use keyword 
distribution form a training corpus to assign 
testing document. However, using only keywords 
in a training set cannot guarantee satisfactory 
results since authors may use different  keywords. 
We believe that, news clip events are categorized 
by concepts, not just keywords. Previous works 
shows that the latent semantic index (LSI) method 
and the n-gram method give good results for 
Chinese news categorization (Wu et al, 1998). 
However, the indices of LSI and n-grams are less 
meaningful semantically. The implicit rules 
acquired by these methods can be understood by 
computers, not humans. Thus, manual editing for 
exceptions and personalization are not possible 
and it is difficult to further reuse these indices for 
knowledge management. 
With good domain ontology we can identify 
the concept structure of sentences in a document. 
Our idea is to compile the concepts within 
documents in a training set and use these concepts 
to understand documents in a testing set. However, 
building rigorous domain ontology is laborious 
and time-consuming. Previous works suggest that 
ontology acquisition is an iterative process, which 
includes keyword collection and structure 
reorganization. The ontology is revised, refined, 
and accumulated by a human editor at each 
iteration (Noy and McGuinness, 2001). For 
example, in order to find a hyponym of a keyword, 
the human editor must observe sentences 
containing this keyword and its related hyponyms 
(Hearst, 1992). The editor then deduces rules for 
finding more hyponyms of this keyword. At each 
iteration the editor refines the rules to obtain better 
quality pairs of keyword-hyponyms. To speed up 
the above labor-intensive approach, semi-
automatic approaches have been designed in 
which a human editor only has to verify the results 
of the acquisition (Maedche and Staab, 2000).  
A knowledge representation framework, 
Information Map (InfoMap) in our previous work 
(Hsu et al, 2001), has been designed to integrate 
various linguistic, common-sense and domain 
knowledge. InfoMap is designed to perform 
natural language understanding, and applied to 
many application domains, such as question 
answering (QA), knowledge management and 
organization memory (Wu et al, 2002), and shows 
good results. An important characteristic of 
InfoMap is that it extracts events from a sentence 
by capturing the topic words, usually subject-verb 
pairs or hypernym-hyponym pairs, which are 
defined in the domain ontology.  
 
We shall review the InfoMap ontology 
framework in Section 2. The ontology acquisition 
process and extraction rules will be introduced in 
Section 3. We describe ontology-based text 
categorization in Section 4. Experimental results 
are reported in Section 5. We conclude our work 
in Section 6.  
2. Information Map 
InfoMap can serve as domain ontology as well as 
an inference engine. InfoMap is designed for NLP 
applications; its basic function is to identify the 
event structure of a sentence. We shall briefly 
describe InfoMap in this section. Figure 1 gives 
example ontology of the Central News Agency 
(CNA), the target in our experiment.  
2.1 InfoMap Structure Format 
As a domain ontology, InfoMap consists of 
domain concepts and their related sub-concepts 
such as categories, attributes, activities. The 
relationships of a concept and its associated sub-
concepts form a tree-like taxonomy. InfoMap also 
defines references to connect nodes from different 
branches which serves to integrate these 
hierarchical concepts into a network. InfoMap not 
only classifies concepts, but also connects the 
concepts by defining the relationships among them. 
 
Concept A
Category
Attribute
       Concept A'        
(Sub-concept of 
concept A)
   Concept B   
(relavant but not 
belong to concept A)
Action
      Concept  C      
(An activity of 
concept A)
Function node
Concept node
Legend
 
Figure 2. Skeleton of the Ontology Structure of 
InfoMap  
Figure 1. Ontology Structure for CNA News 
In InfoMap, concept nodes represent concepts 
and function nodes represent the relationships 
between concepts. The root node of a domain is 
the name of the domain. Following the root node, 
important topics are stored in a hierarchical order. 
These topics have sub-categories that list related 
sub-topics in a recursive fashion. Figure 1 is a 
partial view of the domain ontology of the CNA. 
Under each domain there are several topics and 
each topic might have sub-concepts and associated 
attributes. In this example, note that, the domain 
ontology is automatically acquired from a domain 
corpus, hence the quality is poor. Figure 2 shows 
the skeleton order of a concept using InfoMap.  
2.2 Event Structure 
Since concepts that are semantically related are 
often clustered together, one can use InfoMap to 
discern the main event structure in a natural 
language sentence. The process of identifying the 
event structure, we call a firing mechanism, which 
matches words in a sentence to both concepts and 
relationships in InfoMap. 
Suppose keywords of concept A and its sub-
concept B (or its hyponyms) appear in a sentence. 
It is likely that the author is describing an event ?B 
of A?. For example, when the words ?tire? and 
?car? appear in a sentence, normally this sentence 
would be about the tire of a car (not tire in the 
sense of fatigue). Therefore, a word-pair with a 
semantic relationship can give more concrete 
information than two words without a semantic 
relationship. Of course, certain syntactic 
constraints also need to be satisfied. This can be 
extended to a noun-verb pair or a combination of 
noun, verb and adjective. We call such words in a 
sentence an event structure. This mechanism 
seems to be especially effective for Chinese 
sentences. 
2.3 Domain Speculation 
With the help of domain ontologies, one can 
categorize a piece of text into a specific domain by 
categorizing each individual sentence within the 
text. There are many different ways to use domain 
ontology to categorize text. It can be used as a 
dictionary, as a keyword lists and as a structure to 
identify NL events. Take a single sentence for 
example. We first use InfoMap as a dictionary to 
do word segmentation (necessary for Chinese 
sentences) in which the ambiguity can be resolved 
by checking the domain topic in the ontology. 
After words are segmented, we can examine the 
distribution of these words in the ontology and 
effectively identify the densest cluster. Thus, we 
can use InfoMap to identify the domains of the 
sentences and their associated keywords. Section 
4.1 will further elaborate on this. 
3. Automatic Ontology Acquisition 
The automatically domain ontology acquisition 
from a domain corpus has three steps: 
1. Identify the domain keywords. 
2. Find the relative concepts. 
3. Merge the correlated activities. 
3.1 Domain Keyword Identification 
The first step of automatic domain ontology 
acquisition is to identify domain keywords. 
Identifying Chinese unknown words is difficult 
since the word boundary is not marked in Chinese 
corpus. According to an inspection of a 5 million 
word Chinese corpus (Chen et al, 1996), 3.51% of 
words are not listed in the CKIP lexicon (a 
Chinese lexicon with more than 80,000 entries). 
We use reoccurrence frequency and fan-out 
numbers to characterize words and their 
boundaries according to PAT-tree (Chien, 1999). 
We then adopt the TF/IDF classifier to choose 
domain keywords. The domain keywords serve as 
the seed topics in the ontology. We then apply 
SOAT to automatically obtain related concepts. 
3.2 SOAT 
To build the domain ontology for a new domain, 
we need to collect domain keywords and concepts 
by finding relationships among keywords. We 
adopt a semi-automatic domain ontology 
acquisition tool (SOAT, Wu et al, 2002), to 
construct a new ontology from a domain corpus. 
With a given domain corpus, SOAT can build a 
prototype of the domain ontology. 
InfoMap uses two major relationships among 
concepts: taxonomic relationships (category and 
synonym) and non-taxonomic relationships 
(attribute and action). SOAT defines rules, which 
consist of patterns of keywords and variables, to 
capture these relationships. The extraction rules in 
SOAT are morphological rules constructed from 
part-of-speech (POS) tagged phrase structure. 
Here we briefly introduce the SOAT process: 
Input: domain corpus with the POS tag 
Output: domain ontology prototype 
Steps: 
1 Select a keyword (usually the name of 
the domain) in the corpus as the seed to 
form a potential root set R 
2 Begin the following recursive process:  
2.1 Pick a keyword A as the root from R 
2.2 Find a new related keyword B of the 
root A by extraction rules and add it 
into the domain ontology according to 
the rules   
2.3 If there is no more related keywords, 
remove A from R 
2.4 Put B into the potential root set 
Repeat step 2 until either R becomes 
empty or the total number of nodes reach 
a threshold 
3.3 Morphological Rules 
To find the relative words of a keyword, we check 
the context in the sentence from which the 
keyword appears. We can then find attributes or 
hyponyms of the keyword. For example, in a 
sentence, we find a noun in front of a keyword 
(say, computer) may form a specific kind of 
concept (say, quantum computer). A noun (say, 
connector) followed by ?of? and a keyword may 
be an attribute of the keyword, (say, connector of 
computer). See (Wu et al, 2002) for details. 
3.4 Ontology Merging 
Ontologies can be created by merging different 
resources.  One NLP resource that we will merge 
into our domain ontology is the noun-verb event 
frame (NVEF) database (Tsai and Hsu, 2002). 
NVEF is a collection of permissible noun-verb 
sense-pairs that appear in general domain corpora. 
The noun will be the subject or object of the verb. 
This noun-verb sense-pair collection is domain 
independent. We can use nouns as domain 
keywords and find their correlated verbs. Adding 
these verbs into the domain ontology makes the 
ontology more suitable for NLP. The correlated 
verbs are added under the action function node. 
4. Ontology-Based Text Categorization 
To incorporate the domain ontology into a text 
categorization, we have to adjust both the training 
process and testing process. Section 4.1 describes 
how to make use of the ontology and the event 
structure during the training process. Section 4.2 
describes how to use ontology to perform domain 
speculation. Section 4.3 describes how to 
categorize news clippings. 
4.1 Feature and Threshold Selection 
With the event structure matched (fired) in the 
domain ontology, we have more features with 
which to index a text. To select useful features and 
a proper threshold, we apply Microsoft Decision 
Tree Algorithm to determine a path?s relevance as 
this algorithm can extract human interpretable 
rules (Soni et al, 2000). 
Features of the event structure include event 
structure score, node score, fired node level, and 
node type. During the training process, we record 
all features of the event structure fired by the news 
clippings in the domain-categorized training 
corpus. The decision tree shows that a threshold of 
0.85 is sufficient to evaluate event structure scores. 
We use event structure score to determine if the 
path is relevant. According to Figure 3, if the 
threshold of true probability is 85%, then the event 
structure score (Pathscore in the figure) should be 
65.75. And the relevance of a path p is true if p 
falls in a node on the decision tree whose ratio of true 
instance is greater than ? .  
 
 4.2 Domain Speculation  
The goal of domain speculation is to categorize a 
sentence S into a domain Dj according to the 
combined score of the keywords and the event 
structure in sentence S. We first calculate the 
similarity score of S and Dj. The keyword score 
and the event structure score are calculated 
independently.  
 
),(_*
),(_),(
SDScoretureEventStruc
SDScoreKeywordSDSimScore
j
jj
?
+=  
We use the TF/IDF classifier (Salton, 1989) to 
calculate the Keyword_Score of a sentence  as 
follows. First, we use a segmentation module to 
split a Chinese sentence into words. The TF/IDF 
classifier represents a domain as a weighted vector, 
Dj =( wj1, wj2,?, wjn), where n is the number of 
words in this domain and wk is the weight of word 
k. wk is defined as nfjk * idfjk, where nfjk is the term 
frequency (i.e., the number of times the word wk 
occurs in the domain j). Let DFk be the number of 
domains in which word k appears and |D| the total 
number of domains. idfk, the inverse document 
frequency, is given by:  
)||log(
k
k DF
Didf = . 
This weighting function assigns high values to 
domain-specific words, i.e. words which appear 
frequently in one domain and infrequently in 
others. Conversely, it will assign low weights to 
words appearing in many domains. The similarity 
between a domain j and a sentence represented by 
a vector Di is measured by the following cosine: 
??
?
==
==
=
n
k ik
n
k jk
n
k ikjk
ij
j
ww
ww
DDSim
SDScoreKeyword
1
2
1
2
1
)()(
),(
),(_  
The event structure score is calculated by 
InfoMap Engine. First, find all the nodes in 
ontology that match the words in the sentence. 
Then determine if there is any concept-attribute 
pair, or hypernym-hyponym pair. Finally, assign a 
score to each fired event structure according to the 
string length of words that match the nodes in the 
ontology. The selected event structure is the one 
with the highest score. 
))((
),(_
max SDkeywordsthStringLeng
SDScoretureEventStruc
j
Event
j
?= ?  
4.3 News Categorization 
Upon receiving a news clipping C, we split it into 
sentences Si. The sentences are scored and 
categorized according to domains.  Thus, every 
sentence has an individual score for each domain 
Score(D, Si). We add up these scores of every 
sentence in the text according to domain, giving us 
total domain scores for the entire text.  The 
domain which has the highest score is the domain 
into which the text is categorized. 
)),(()( maxarg ?
?
=
CS
i
D
SDScoreCDomain  
5. Refining Ontology through the Text 
Categorization Application 
The advantage of ontology compared to other 
implicit knowledge representation mechanism is 
that it can be read, interpreted and edited by 
human. Noise and errors can be detected and 
refined, especially for the automatically acquired 
ontology, in order to obtain a better ontology. 
Another advantage of allowing human editing is 
that the ontology produced can be shared by 
various applications, such as from a QA system to 
a knowledge management system. In contrast, the 
implicit knowledge represented in LSI or other 
representations is difficult to port from one 
application to another. 
Figure 3. Threshold selection using decision 
tree 
In this section, we show how the human 
editing feature improves news categorization. First, 
we can identify a common error type: ambiguity; 
then, depending on the degree of categorization 
ambiguity, the system can report to a human editor 
the possible errors of certain concepts in the 
domain ontology as clues. 
Consider the following common error type: 
event structure ambiguity. Some event structures 
are located in several domains due to the noise of 
training data. We define two formulas to find such 
event structures. The ambiguity of an event 
structure E(Si) is proportional to the number of 
domains in which it appears, and inversely 
proportional to its event score, where Si are the 
sentences that fire event E. 
GlobalCategorizationAmiguityFactor(E(Si) ) 
= number of domains fired by 
Si/average( EventScore(Si) ) 
We also measure the similarity between every 
two event structures by calculating the co-
occurrence multiplied by the global categorization 
ambiguity factor. 
GlobalCategorizationAmbiguityij (E i, E j) 
=Co-occurrence(E i, E j) * 
GlobalCategorizationAmbiguityFactor(E j) 
When the GlobalCategorizationAmbiguity of an 
event structure E i exceeds a threshold, the system 
will suggest that the human editor refine the 
ontology. 
6. Experiments 
To assess the power of domain identification of 
ontology, we test the text categorization ability on 
two different corpora. The ontology of the first 
experiment is edited manually; the ontology of the 
second experiment is automatically acquired. And 
we also conduct an experiment on the effect of 
human editing of the automatically acquired 
ontology. 
6.1 Single Sentence Test 
We test 9,143 sentences, edited manually for a QA 
system. The accuracy is 94%. These sentences are 
questions in the financial domain. Because the 
sentence topics are quite focused, the accuracy is 
very high. See Table 1. 
Table 1. Sentence Categorization Accuracy 
Domain # Sentence # Accuracy 
24 9143 94.01% 
6.2 News Clippings Collection 
The second experiment that we conduct is news 
categorization. We collect daily news from China 
News Agency (CNA) ranging from 1991 to 1999. 
Each news clipping is short with 352 Chinese 
characters (about 150 words) on the average. 
There are more than thirty domains and we choose 
10 major categories for the experiment. 
6.3 10 Categories News Categorization 
Our ten categories are: domestic arts and education 
(DD), foreign affairs (FA), finance report (FX), 
domestic health (HD), Taiwan local news (LD), 
Taiwan sports (LD), domestic military (MD), 
domestic politics (PD), Taiwan stock markets (SD), 
and weather report (WE). From each category, we 
choose the first 100 news clippings as the training 
set and the following 100 news clippings as the 
testing set. After data cleansing, the total training 
set has 979 news clippings, with 27,951 nodes and 
less than 10,000 distinct words. The training set 
for which domain ontologies are automatically 
acquired is shown in Table 2. A partial view of 
this ontology is in Figure 1. 
The result of text categorization based on this 
automatically acquired domain ontology is shown 
in Table 5, which contains the recall and precision 
for each domain. Note that, without the help of the 
event structure, the macro average f-score is 
85.16%. Even the total number of domain key 
concepts is less than 10,000 words (instead of 
100,000 words in standard dictionary), we can still 
obtain a good categorization result. With the help 
of event structure, the macro average f-score is 
85.55%.  
6.4 Human Editing 
To verify the refinement method, we conduct 
an experiment to compare the result of using 
automatically acquired domain ontology and that 
of limited human editing (on only one domain 
ontology). After the training process, we use 
domain ontologies to classify the training data, 
and to calculate the global categorization 
ambiguity factor formula in order to obtain 
ambiguous event structure pairs as candidates for 
human editing. For simplicity, we restrict the 
action of refinement to deletion. It takes a human 
editor one half day to finish the task and delete 
0.62% nodes (172 out of 27,951 nodes). In the 
testing phase, we select 928 new news clippings as 
the testing set. Table 3 shows the results from 
before and after human editing. Due to time 
constraints, we only edit the part of the ontology 
that might affect domain DD. The recall and 
precision of domain DD increase as well as both 
the average recall and average precision. In 
addition, the recall of domains having higher 
correlation with DD, such as PD and FA, 
decreases. Apparently, the event structures that 
mislead the categorization system to theses 
domain have mostly been deleted. The experiment 
result is very consistent with our intuition. 
Table 2. Ten Category training set CNA news 
Training set size 
Domain 
Doc# Char# 
DD 98 41870 
FA 97 38143 
FX 100 30771 
HD 96 39818 
JD 107 35381 
LD 96 36957 
MD 89 32903 
PD 100 43152 
SD 109 33030 
WE 87 30457 
total 979 362,482 
7. Discussions and Conclusions 
Compared to an ordinary n-gram dictionary, our 
ontology dictionary is quite small (roughly 10%) 
but records certain important relations between 
keywords.  
Our goal is to generate rules that are human 
readable via ontology. The experiment result 
shows that event structure enhances text 
categorization, even when the domain ontology is 
automatically acquired without human verification. 
To improve our ontological approach, our future 
work are: 1. human editing in more domains; 2. 
enlarge our dictionary by merging existing 
ontologies, e.g., the names of countries, capitals 
and important persons, which are absent from the 
training corpus; 3. incorporate more sense pairs 
such as N-A (noun-adjective), Adv-V (adverb-
verb); 4. use machine learning model on the 
weighting of the ontological features. 
Previous research shows that some NLP 
techniques can improve information retrieval. 
Ontology-based IR is one of them. However, the 
construction of domain ontology is too costly. 
Thus, automatic acquisition of domain ontology is 
becoming an interesting research topic. Previous 
research shows that implicit rules (such as LSI, N-
gram dictionaries) learned from a training corpus 
give better results than explicit rules generated by 
humans. However, it is hard to use these implicit 
rules or to combine them with other resources for 
further refinement. With the help of domain 
ontology, we can automatically generate rules that 
humans can understand. Since humans and 
machines can maintain ontology independently, 
the ontological approach can be applied more 
easily to other IR applications. Ontologies from 
different sources can be merged into the domain 
ontology. The system should include an editing 
interface that human thoughts can be incorporated 
to complement statistical rules. With semi-
automatically acquired domain ontology, text 
categorization can be adapted to personal 
preferences.  
8. References 
Chen, K.J., C.R. Huang, L.P. Chang & H.L. Hsu, 
SINICA CORPUS: Design Methodology for 
Balanced Corpora, in Proceedings of PACLIC 
11th Conference, pp.167-176, 1996. 
Chien, L.F., PAT-tree-based Adaptive keyphrase 
extraction for Intelligent Chinese Information 
Retrieval, Information Processing and 
Management, Vol. 35, pp. 501-521, 1999. 
Gruber, T.R. (1993), A translation approach to 
portable ontologies. Knowledge Acquisition, 
5(2), pp. 199-220, 1993. 
Hearst, M.A. (1992), Automatic acquisition of 
hyponyms from large text corpora. In 
COLING-92, pp. 539-545. 
Hsu, W.L., Wu, S.H. and Chen, Y.S., Event 
Identification Based On The Information Map - 
INFOMAP, in Natural Language Processing 
and Knowledge Engineering Symposium of the 
IEEE Systems, Man, and Cybernetics 
Conference, Tucson, Arizona, USA, 2001. 
Maedche, A. and Staab, S. (2000), Discovering 
Conceptual Relationships from Text. In: Horn, 
W. (ed.): ECAI 2000. Proceedings of the 14th 
European Conference on Artificial Intelligence, 
IOS Press, Amsterdam. 
Noy, N.F. and McGuinness D.L. (2001), Ontology 
Development 101: A Guide to Creating Your 
First Ontology, SMI technical report SMI-
2001-0880, Stanford Medical Informatics. 
Salton, G., Automatic Text Processing, Addison-
Wesley, Massachusetts, 1989. 
Soni, S, Tang, Z. and Yang, J., ?Microsoft 
Performance Study of Microsoft Data Mining 
Algorithms?, UniSys, 2000/12. 
Tsai, J.L. and Hsu, W.L., ?Applying an NVEF 
Word-Pair Identifier to the Chinese Syllable-to-
Word Conversion Problem,? COLING-02, 
Taipei, ACM press, 2002. 
Wu, S.H. and Hsu, W.L., SOAT: A Semi-
Automatic Domain Ontology Acquisition Tool 
from Chinese Corpus, COLING-02, Taipei, 
ACM press, 2002. 
Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu, W.L., 
FAQ-centered Organizational Memory, in 
Nada Matta and Rose Dieng-Kuntz (ed.), 
Knowledge Management and Organizational 
Memories, Kluwer Academic Publishers, 
Boston, 2002. 
Wu, S.H., Yang, P.C. and Soo, V.W., An 
Assessment on Character-based Chinese News 
Filtering Using Latent Semantic Indexing, 
Computational Linguistics & Chinese 
Language Processing, Vol. 3, no.2, August 
1998. 
Table 3. Experiment result of CNA news categorization 
# of nodes 
automatically 
acquired 
#of nodes  
deleted in 
human editing 
TF/IDF(baseline)
TF/IDF+Event 
Structure(first 
improvement) 
TF/IDF+Event Structure 
with Human Editing 
(second improvement) 
The different between 
(second improvement) and 
(first improvement) Domain 
Before  After   #  % P% R% F% P% R% F% P% R% F% P+% R+% F+% 
DD 4616 4574 42 0.91 72.90 
82.9
8 
77.6
1 74.04 81.91 77.78 74.29 82.98 78.39 0.25 1.07 0.61
FA 8352 8348 4 0.05 75.83 
94.7
9 
84.2
6 71.32 95.83 81.78 76.67 95.83 85.19 5.35 0.00 3.41
FX 44 44 0 0.00 100 100 100 100 100 100 100 100 100 0.00 0.00 0.00
HD 3357 3348 9 0.27 78.79 
88.6
4 
83.4
2 80.21 87.50 83.70 78.79 88.64 83.42 -1.42 1.14 -0.28
JD 1854 1846 8 0.43 88 71.74 
79.0
4 87.18 73.91 80 87.84 70.65 78.31 0.66 -3.26 -1.69
LD 2925 2831 94 3.21 87.64 
80.4
1 
83.8
7 90.36 77.32 83.33 88.51 79.38 83.70 -1.85 2.06 0.37
MD 2010 1999 11 0.55 95.59 
66.3
3 
78.3
1 95.71 68.37 79.76 97.26 72.45 83.04 1.55 4.08 3.28
PD 3199 3195 4 0.13 65.81 
68.7
5 
67.2
5 70.43 72.32 71.37 66.67 69.64 68.12 -3.76 -2.68 -3.25
SD 585 585 0 0.00 100 100 100 100 100 100 100 100 100 0.00 0.00 0.00
WE 1009 1009 0 0.00 95.74 100 
97.8
3 95.74 100 97.83 95.74 100 97.83 0.00 0.00 0.00
Total 27951 27779 172 0.62                   
Macro  
Average         
86.0
3 
85.3
6 
85.1
6 86.50 85.72 85.55 86.58 85.96 85.80 0.08 0.24 0.25
 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 233?236, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploiting Full Parsing Information to Label Semantic Roles Using an  
Ensemble of ME and SVM via Integer Linear Programming 
 
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, Wen-Lian Hsu 
Institute of Information Science 
Academia Sinica  
Taipei 115, Taiwan 
{thtsai, cwwu, sbb, hsu}@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we propose a method that 
exploits full parsing information by repre-
senting it as features of argument classifi-
cation models and as constraints in integer 
linear learning programs. In addition, to 
take advantage of SVM-based and Maxi-
mum Entropy-based argument classifica-
tion models, we incorporate their scoring 
matrices, and use the combined matrix in 
the above-mentioned integer linear pro-
grams. The experimental results show that 
full parsing information not only in-
creases the F-score of argument classifi-
cation models by 0.7%, but also 
effectively removes all labeling inconsis-
tencies, which increases the F-score by 
0.64%. The ensemble of SVM and ME 
also boosts the F-score by 0.77%. Our 
system achieves an F-score of 76.53% in 
the development set and 76.38% in Test 
WSJ. 
1 Introduction 
The Semantic Role Labeling problem can be for-
mulated as a sentence tagging problem. A sentence 
can be represented as a sequence of words, as 
phrases (chunks), or as a parsing tree. The basic 
units of a sentence are words, phrases, and con-
stituents in these representations, respectively.. 
Pradhan et al (2004) established that Constituent-
by-Constituent (C-by-C) is better than Phrase-by-
Phrase (P-by-P), which is better than Word-by-
Word (W-by-W).  This is probably because the 
boundaries of the constituents coincide with the 
arguments; therefore, C-by-C has the highest ar-
gument identification F-score among the three ap-
proaches.  
In addition, a full parsing tree also provides 
richer syntactic information than a sequence of 
chunks or words. Pradhan et al (2004) compared 
the seven most common features as well as several 
features related to the target constituent?s parent 
and sibling constituents. Their experimental results 
show that using other constituents? information 
increases the F-score by 6%. Punyakanok et al 
(2004) represent full parsing information as con-
straints in integer linear programs. Their experi-
mental results show that using such information 
increases the argument classification accuracy by 
1%. 
In this paper, we not only add more full parsing 
features to argument classification models, but also 
represent full parsing information as constraints in 
integer linear programs (ILP) to resolve label in-
consistencies. We also build an ensemble of two 
argument classification models: Maximum Entropy 
and SVM by combining their argument classifica-
tion results and applying them to the above-
mentioned ILPs. 
2 System Architecture 
Our SRL system is comprised of four stages: prun-
ing, argument classification, classification model 
incorporation, and integer linear programming. 
This section describes how we build these stages, 
including the features used in training the argu-
ment classification models. 
2.1 Pruning 
233
When the full parsing tree of a sentence is avail-
able, only the constituents in the tree are consid-
ered as argument candidates. In CoNLL-2005, full 
parsing trees are provided by two full parsers: the 
Collins parser (Collins, 1999)  and the Charniak 
parser (Charniak, 2000). According to Punyakanok 
et al (2005), the boundary agreement of Charniak 
is higher than that of Collins; therefore, we choose 
the Charniak parser?s results. However, there are 
two million nodes on the full parsing trees in the 
training corpus, which makes the training time of 
machine learning algorithms extremely long. Be-
sides, noisy information from unrelated parts of a 
sentence could also affect the training of machine 
learning models. Therefore, our system exploits the 
heuristic rules introduced by Xue and Palmer 
(2004) to filter out simple constituents that are 
unlikely to be arguments. Applying pruning heuris-
tics to the output of Charniak?s parser effectively 
eliminates 61% of the training data and 61.3% of 
the development data, while still achieves 93% and 
85.5% coverage of the correct arguments in the 
training and development sets, respectively. 
2.2 Argument Classification 
This stage assigns the final labels to the candidates 
derived in Section 2.1. A multi-class classifier is 
trained to classify the types of the arguments sup-
plied by the pruning stage. In addition, to reduce 
the number of excess candidates mistakenly output 
by the previous stage, these candidates can be la-
beled as null (meaning ?not an argument?). The 
features used in this stage are as follows. 
Basic Features 
? Predicate ? The predicate lemma. 
? Path ? The syntactic path through the 
parsing tree from the parse constituent be-
ing classified to the predicate. 
? Constituent Type 
? Position ? Whether the phrase is located 
before or after the predicate. 
? Voice ? passive: if the predicate has a POS 
tag VBN, and its chunk is not a VP, or it is 
preceded by a form of ?to be? or ?to get? 
within its chunk; otherwise, it is active. 
? Head Word ? calculated using the head 
word table described by Collins (1999). 
? Head POS ? The POS of the Head Word. 
? Sub-categorization ? The phrase structure 
rule that expands the predicate?s parent 
node in the parsing tree. 
? First and Last Word/POS 
? Named Entities ? LOC, ORG, PER, and 
MISC. 
? Level ? The level in the parsing tree. 
Combination Features 
? Predicate Distance Combination 
? Predicate Phrase Type Combination 
? Head Word and Predicate Combination 
? Voice Position Combination 
Context Features 
? Context Word/POS ? The two words pre-
ceding and the two words following the 
target phrase, as well as their correspond-
ing POSs.  
? Context Chunk Type ? The two chunks 
preceding and the two chunks following 
the target phrase. 
Full Parsing Features 
We believe that information from related constitu-
ents in the full parsing tree helps in labeling the 
target constituent. Denote the target constituent by 
t. The following features are the most common 
baseline features of t?s parent and sibling constitu-
ents. For example, Parent/ Left Sibling/ Right Sib-
ling Path denotes t?s parents?, left sibling?s, and 
right sibling?s Path features.  
? Parent / Left Sibling / Right Sibling 
Path 
? Parent / Left Sibling / Right Sibling 
Constituent Type 
? Parent / Left Sibling / Right Sibling Po-
sition 
? Parent / Left Sibling / Right Sibling 
Head Word 
? Parent / Left Sibling / Right Sibling 
Head POS 
? Head of PP parent ? If the parent is a PP, 
then the head of this PP is also used as a 
feature. 
Argument Classification Models 
234
We use all the features of the SVM-based and ME-
based argument classification models. All SVM 
classifiers are realized using SVM-Light with a 
polynomial kernel of degree 2. The ME-based 
model is implemented based on Zhang?s MaxEnt 
toolkit1 and L-BFGS (Nocedal and Wright, 1999) 
method to perform parameter estimation. 
2.3 Classification Model Incorporation  
We now explain how we incorporate the SVM-
based and ME-based argument classification mod-
els. After argument classification, we acquire two 
scoring matrices, PME and PSVM, respectively. In-
corporation of these two models is realized by 
weighted summation of PME and PSVM as follows: 
P? = wMEPME + wSVMPSVM
We use P? for the objective coefficients of the 
ILP described in Section 2.4. 
2.4 Integer Linear Programming (ILP) 
To represent full parsing information as features, 
there are still several syntactic constraints on a 
parsing tree in the SRL problem. For example, on a 
path of the parsing tree, there can be only one con-
stituent annotated as a non-null argument. How-
ever, it is difficult to encode this constraint in the 
argument classification models. Therefore, we ap-
ply integer linear programming to resolve inconsis-
tencies produced in the argument classification 
stage.  
According to Punyakanok et al (2004), given a 
set of constituents, S, and a set of semantic role 
labels, A, the SRL problem can be formulated as 
an ILP as follows: 
Let zia be the indicator variable that represents 
whether or not an argument,  a, is assigned to any 
Si ? S; and let pia = score(Si = a). The scoring ma-
trix P composed of all pia is calculated by the ar-
gument classification models. The goal of this ILP 
is to find a set of assignments for all zia that maxi-
mizes the following function: 
??
? ?S AiS a
iaia zp . 
Each Si?  S should have one of these argument 
types, or no type (null). Therefore, we have  
?
?
=
Aa
iaz 1 . 
Next, we show how to transform the constraints in 
                                                          
1 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html 
the filter function into linear equalities or inequali-
ties, and use them in this ILP. 
Constraint I: No overlapping or embedding  
For arguments Sj1 , . . . , Sjk  on the same path in a 
full parsing tree, only one argument can be as-
signed to an argument type. Thus, at least k ? 1 
arguments will be null, which is represented by ?  
in the following linear equality: 
?
=
??
k
i
j ki
1
1z ? .                                                     
Constraint II: No duplicate argument classes 
Within the same sentence, A0-A5 cannot appear 
more than once. The inequality for A0 is therefore: 
?
=
?
k
i
iz
1
A0 1. 
Constraint III: R-XXX arguments  
The linear inequalities that represent A0 and its 
reference type R-A0 are: 
?
=
????
k
i
mi zzMm
1
A0RA0:},...,1{ . 
Constraint IV: C-XXX arguments  
The continued argument XXX has to occur before 
C-XXX. The linear inequalities for A0 are: 
??
=
????
1
1
A0CA0:},...,2{
m
i
mj zzMm i . 
Constraint V: Illegal arguments  
For each verb, we look up its allowed roles. This 
constraint is represented by summing all the corre-
sponding indicator variables to 0. 
3 Experiment Results  
3.1 Data and Evaluation Metrics 
The data, which is part of the PropBank corpus, 
consists of sections from the Wall Street Journal 
part of the Penn Treebank. All experiments were 
carried out using Section 2 to Section 21 for train-
ing, Section 24 for development, and Section 23 
for testing. Unlike CoNLL-2004, part of the Brown 
corpus is also included in the test set.  
3.2 Results 
Table 1 shows that our system makes little differ-
ence to the development set and Test WSJ. How-
ever, due to the intrinsic difference between the 
WSJ and Brown corpora, our system performs bet-
ter on Test WSJ than on Test Brown. 
235
Precision Recall F
?=1
Development 81.13% 72.42% 76.53
Test WSJ 82.77% 70.90% 76.38
Test Brown 73.21% 59.49% 65.64
Test WSJ+Brown 81.55% 69.37% 74.97
Test WSJ Precision Recall F
?=1
Overall 82.77% 70.90% 76.38
A0 88.25% 84.93% 86.56
A1 82.21% 72.21% 76.89
A2 74.68% 52.34% 61.55
A3 78.30% 47.98% 59.50
A4 84.29% 57.84% 68.60
A5 100.00% 60.00% 75.00
AM-ADV 64.19% 47.83% 54.81
AM-CAU 70.00% 38.36% 49.56
AM-DIR 38.20% 40.00% 39.08
AM-DIS 83.33% 71.88% 77.18
AM-EXT 86.67% 40.62% 55.32
AM-LOC 63.71% 41.60% 50.33
AM-MNR 63.36% 48.26% 54.79
AM-MOD 98.00% 97.64% 97.82
AM-NEG 99.53% 92.61% 95.95
AM-PNC 44.44% 17.39% 25.00
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.21% 61.09% 70.45
R-A0 91.08% 86.61% 88.79
R-A1 79.49% 79.49% 79.49
R-A2 87.50% 43.75% 58.33
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 25.00% 16.67% 20.00
R-AM-TMP 72.73% 61.54% 66.67
V 97.32% 97.32% 97.32  
Table 1. Overall results (top) and detailed results 
on the WSJ test (bottom). 
Precision Recall F
?=1
ME w/o parsing 77.28% 70.55% 73.76%
ME 78.19% 71.08% 74.46%
ME with ILP 79.57% 71.11% 75.10%
SVM 79.88% 72.03% 75.76%
Hybrid 81.13% 72.42% 76.53%
 
Table 2. Results of all configurations on the devel-
opment set. 
From Table 2, we can see that the model with 
full parsing features outperforms the model with-
out the features in all three performance matrices. 
After applying ILP, the performance is improved 
further. We also observe that SVM slightly outper-
forms ME. However, the hybrid argument classifi-
cation model achieves the best results in all three 
metrics. 
4 Conclusion  
In this paper, we add more full parsing features to 
argument classification models, and represent full 
parsing information as constraints in ILPs to re-
solve labeling inconsistencies. We also integrate 
two argument classification models, ME and SVM, 
by combining their argument classification results 
and applying them to the above-mentioned ILPs. 
The results show full parsing information increases 
the total F-score by 1.34%. The ensemble of SVM 
and ME also boosts the F-score by 0.77%. Finally, 
our system achieves an F-score of 76.53% in the 
development set and 76.38% in Test WSJ. 
Acknowledgement 
We are indebted to Wen Shong Lin and Prof. Fu 
Chang for their invaluable advice in data pruning, 
which greatly speeds up the training of our ma-
chine learning models. 
References  
X. Carreras and L. M?rquez. 2005. Introduction to the 
CoNLL-2005 Shared Task: Semantic Role Labeling. 
In Proceedings of the CoNLL-2005. 
E. Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. Proceedings of the NAACL-2000. 
M. J. Collins. 1999. Head-driven Statistical Models for 
Natural Language Parsing. Ph.D. thesis, University 
of Pennsylvania. 
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion, Springer. 
S. Pradhan, K. Hacioglu, V. Kruglery, W. Ward,J. H. 
Martin, and D. Jurafsky. 2004. Support Vector 
Learning for Semantic Argument Classification. 
Journal of Machine Learning. 
V. Punyakanok, D. Roth, and W. Yih. 2005. The Neces-
sity of Syntactic Parsing for Semantic Role Labeling. 
In Proceedings of the 19th International Joint Con-
ference on Artificial Intelligence (IJCAI-05). 
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. 
Semantic Role Labeling via Integer Linear Pro-
gramming Inference. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics 
(COLING-04). 
N. Xue and M. Palmer. 2004. Calibrating Features for 
Semantic Role Labeling. In Proceedings of the 
EMNLP 2004. 
236
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 134?137,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Closed Task of Chinese Word Segmentation: An Improved CRF 
Model Coupled with Character Clustering and  
Automatically Generated Template Matching 
Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-Lung Sung,  
Hong-Jie Dai, and Wen-Lian Hsu 
Intelligent Agent Systems Lab 
Institute of Information Science, Academia Sinica 
No. 128, Sec. 2, Academia Rd., 115 Nankang, Taipei, Taiwan, R.O.C. 
{thtsai,yabt,clsung,hongjie,hsu}@iis.sinica.edu.tw 
 
  
 
Abstract 
This paper addresses two major prob-
lems in closed task of Chinese word 
segmentation (CWS): tagging sentences 
interspersed with non-Chinese words, 
and long named entity (NE) identifica-
tion. To resolve the former, we apply K-
means clustering to identify non-Chinese 
characters, and then adopt a two-tagger 
architecture: one for Chinese text and the 
other for non-Chinese text. For the latter 
problem, we apply postprocessing to our 
CWS output using automatically gener-
ated templates. The experiment results 
show that, when non-Chinese characters 
are sparse in the training corpus, our 
two-tagger method significantly im-
proves the segmentation of sentences 
containing non-Chinese words. Identifi-
cation of long NEs and long words is 
also enhanced by template-based post-
processing. In the closed task of 
SIGHAN 2006 CWS, our system 
achieved F-scores of 0.957, 0.972, and 
0.955 on the CKIP, CTU, and MSR cor-
pora respectively.  
1 Introduction 
Unlike Western languages, Chinese does not 
have explicit word delimiters. Therefore, word 
segmentation (CWS) is essential for Chinese 
text processing or indexing. There are two main 
problems in the closed CWS task. The first is to 
identify and segment non-Chinese word se-
quences in Chinese documents, especially in a 
closed task (described later). A good CWS sys-
tem should be able to handle Chinese texts pep-
pered with non-Chinese words or phrases. Since 
non-Chinese language morphologies are quite 
different from that of Chinese, our approach 
must depend on how many non-Chinese words 
appear, whether they are connected with each 
other, and whether they are interleaved with 
Chinese words. If we can distinguish non-
Chinese characters automatically and apply dif-
ferent strategies, the segmentation performance 
can be improved. The second problem in closed 
CWS is to correctly identify longer NEs. Most 
ML-based CWS systems use a five-character 
context window to determine the current charac-
ter?s tag. In the majority of cases, given the con-
straints of computational resources, this com-
promise is acceptable. However, limited by the 
window size, these systems often handle long 
words poorly. 
In this paper, our goal is to construct a general 
CWS system that can deal with the above prob-
lems. We adopt CRF as our ML model. 
2 Chinese Word Segmentation System 
2.1 Conditional Random Fields 
Conditional random fields (CRFs) are undirected 
graphical models trained to maximize a condi-
tional probability (Lafferty et al, 2001). A lin-
ear-chain CRF with parameters ?={?1, ?2, ?} 
defines a conditional probability for a state se-
quence y = y1 ?yT , given that an input sequence 
x = x1 ?xT  is 
???
????
?= ??
=
??
T
t k
ttkk tyyfZ
P
1
1 ),,,(exp
1
)|( xxy
x
?  ,(1)                          
where Zx is the normalization factor that makes 
the probability of all state sequences sum to one; 
fk(yt-1, yt, x, t) is often a binary-valued feature 
function and ?k is its weight. The feature 
134
functions can measure any aspect of a state 
transition, yt-1?yt, and the entire observation 
sequence, x, centered at the current time step, t. 
For example, one feature function might have 
the value 1 when yt-1 is the state B, yt is the state 
I, and tx  is the character ???. 
2.2 Character Clustering 
In many cases, Chinese sentences may be inter-
spersed with non-Chinese words. In a closed 
task, there is no way of knowing how many lan-
guages there are in a given text. Our solution is 
to apply a clustering algorithm to find homoge-
neous characters belonging to the same character 
clusters. One general rule we adopted is that a 
language?s characters tend to appear together in 
tokens. In addition, character clusters exhibit 
certain distinct properties. The first property is 
that the order of characters in some pairs can be 
interchanged. This is referred to as exchange-
ability. The second property is that some charac-
ters, such as lowercase characters, can appear in 
any position of a word; while others, such as 
uppercase characters, cannot. This is referred to 
as location independence. According to the gen-
eral rule, we can calculate the pairing frequency 
of characters in tokens by checking all tokens in 
the corpus. Assuming the alphabet is ?, we first 
need to represent each character as a |?|-
dimensional vector. For each character ci, we use 
vj to represent its j-dimension value, which is 
calculated as follows: 
r
jiijj ffv )],)[min(1( ?? ?+= ?             (2), 
where fij denotes the frequency with which ci and 
cj appear in the same word when ci?s position 
precedes that of cj. We take the minimum value 
of fij and fji because even when ci and cj have a 
high co-occurrence frequency, if either fij or fji is 
low, then one order does not occur often, so vj?s 
value will be low. We use two parameters to 
normalize vj within the range 0 to 1; ? is used to 
enlarge the gap between non-zero and zero fre-
quencies, and ? is used to weaken the influence 
of very high frequencies. 
Next, we apply the K-means algorithm to 
generate candidate cluster sets composed of K 
clusters (Hartigan et al, 1979). Different K?s, 
??s, and ??s are used to generate possible charac-
ter cluster sets. Our K-means algorithm uses the 
cosine distance. 
After obtaining the K clusters, we need to se-
lect the N1 best character clusters among them. 
Assuming the angle between the cluster centroid 
vector and (1, 1, ... , 1) is ?, the cluster with the 
largest cosine ? will be removed. This is because 
characters whose co-occurrence frequencies are 
nearly all zero will be transformed into vectors 
very close to (?, ?, ... , ?); thus, their centroids 
will also be very close to (?, ?, ... , ?), leading to 
unreasonable clustering results. 
After removing these two types of clusters, 
for each character c in a cluster M, we calculate 
the inverse relative distance (IRDist) of c using 
(3): 
??
?
?
?
??
?
?
?
=
?
),cos(
),(cos
log)IRDist(
mc
mc
c
i
i   ,            (3) 
where mi stands for the centroid of cluster Mi, 
and m stands for the centroid of M.  
We then calculate the average inverse dis-
tance for each cluster M. The N1 best clusters are 
selected from the original K clusters.   
The above K-means clustering and character 
cluster selection steps are executed iteratively 
for each cluster set generated from K-means 
clustering with different K?s, ??s, and ??s.  
After selecting the N1 best clusters for each 
cluster set, we pool and rank them according to 
their inner ratios. Each cluster?s inner ratio is 
calculated by the following formula: 
?
?
?
?
= ?
ji
ji
cc
ji
Mcc
ji
cc
cc
M
,
,
),occurence(co
 ),occurence(co
)inner(
 ,   (4) 
where co-occurrence(ci, cj) denotes the fre-
quency with which characters  ci and cj co-occur 
in the same word. 
To ensure that we select a balanced mix of 
clusters, for each character in an incoming clus-
ter M, we use Algorithm 1 to check if the fre-
quency of each character in C?M is greater 
than a threshold ?. 
 
Algorithm 1 Balanced Cluster Selection 
Input: A set of character clusters P={M1 ,  . . .  , MK} 
          Number of selections N2, 
Output: A set of clusters Q={ '1M  ,  . . .  , 
'
2N
M }. 
 
1: C={} 
2: sort the clusters in P by their inner ratios; 
3: while |C|<=N2 do 
4:     pick the cluster M that has highest inner ratio; 
5:     for each character c in M do 
6:          if the frequency of c in C?M is over thresh-
old ? 
7:                 P?P?M; 
8:                 continue; 
9 :        else 
135
10:               C?C?M; 
11:               P?P?M; 
12:        end; 
13:   end; 
14: end 
 
The above algorithm yields the best N1 clus-
ters in terms of exchangeability. Next, we exe-
cute the above procedures again to select the 
best  N2 clusters based on their location inde-
pendence and exchangeability. However, for 
each character ci, we use vj to denote the value of 
its j-th dimension. We calculate vj as follows: 
r
jijiijijj ffffv )]',,',)[min(1(
' ?? ?+= ,      (5) 
where ijf  stands for the frequency with which ci 
and cj appear in the same word when ci is the 
first character; and f?ij stands for the frequency 
with which ci and cj co-occur in the same word 
when ci precedes cj  but not in the first position. 
We choose the minimum value from ijf , f?ij, jif , 
and f?ji  because if ci and cj both appear in the 
first position of a word and their order is ex-
changeable, the four frequency values, including 
the minimum value, will all be large enough. 
Type Cluster Inner (K, ?, ?) 
,.0123456789 0.94 (10, 0.60, 0.16)
EX 
 
-/ABCDEFGHIKLMNOPR 
STUVWabcdefghiklmnoprst 
uvwxy 
0.93 (10, 0.70, 0.16)
??ABCDEFGHIKLMNO 
PRSTUVWabcdefghiklmno 
prstvwxy 
0.84 (10, 0.50, 0.25)EL 
?????????? 0.76 (10, 0.50, 0.26)
Table 1. Clustering Results of the CTU corpus 
Our next goal is to create the best hybrid of 
the above two cluster sets. The set selected for 
exchangeability is referred to as the EX set, 
while the set selected for both exchangeability 
and location independence is referred to as the 
EL set. We create a development set and use the 
best first strategy to build the optimal cluster set 
from EX?EL. The EX and EL for the CTU 
corpus are shown in Table 1. 
2.3 Handling Non-Chinese Words 
Non-Chinese characters suffer from a serious 
data sparseness problem, since their frequencies 
are much lower than those of Chinese characters. 
In bigrams containing at least one non-Chinese 
character (referred as non-Chinese bigrams), the 
problem is more serious. Take the phrase ???  
20?? (about 20 years old) for example. ?2? is 
usually predicted as I, (i.e., ???? is connected 
with ?2?) resulting in incorrect segmentation, 
because the frequency of ?2? in the I class is 
much higher than that of ?2? in the B class, even 
though the feature C-2C-1=???? has a high 
weight for assigning ?2? to the B class. 
Traditional approaches to CWS only use one 
general tagger (referred as the G tagger) for 
segmentation. In our system, we use two CWS 
taggers. One is a general tagger, similar to the 
traditional approaches; the other is a specialized 
tagger designed to deal with non-Chinese words. 
We refer to the composite tagger (the general 
tagger plus the specialized tagger) as the GS 
tagger. 
Here, we refer to all characters in the selected 
clusters as non-Chinese characters. In the devel-
opment stage, the best-first feature selector de-
termines which clusters will be used. Then, we 
convert each sentence in the training data and 
test data into a normalized sentence. Each non-
Chinese character c is replaced by a cluster rep-
resentative symbol ?M, where c is in the cluster 
M. We refer to the string composed of all ?M as 
F. If the length of F is more than that of W, it 
will be shortened to W. The normalized sentence 
is then placed in one file, and the non-Chinese 
character sequence is placed in another. Next, 
we use the normalized training and test file for 
the general tagger, and the non-Chinese se-
quence training and test file for the specialized 
tagger. Finally, the results of these two taggers 
are combined. 
The advantage of this approach is that it re-
solves the data sparseness problem in non-
Chinese bigrams. Consider the previous example 
in which ? stands for the numeral cluster. Since 
there is a phrase ??? 8??  in the training data, 
C-1C0= ?? 8? is still an unknown bigram using 
the G tagger. By using the GS tagger, however, 
??? 20?? and ??? 8?? will be converted 
as ??? ???? and ??? ???, respectively. 
Therefore, the bigram feature C-1C0=?? ?? is no 
longer unknown. Also, since ? in ?? ?? is 
tagged as B, (i.e., ??? and ??? are separated), 
??? and ??? will be separated in  ??? ????. 
2.4 Generating and Applying Templates 
Template Generation 
We first extract all possible word candidates 
from the training set. Given a minimum word 
length L, we extract all words whose length is 
greater than or equal to L, after which we align 
all word pairs. For each pair, if more than fifty 
136
percent of the characters are identical, a template 
will be generated to match both words in the pair. 
Template Filtering 
We have two criteria for filtering the extracted 
templates. First, we test the matching accuracy 
of each template t on the development set. This 
is calculated by the following formula: 
strings matched all of #
separators no with strings matched of #
)( =tA . 
In our system, templates whose accuracy is 
lower than the threshold ?1 are discarded. For the 
remaining templates, we apply two different 
strategies. According to our observations of the 
development set, most templates whose accu-
racy is less than ?2 are ineffective. To refine such 
templates, we employ the character class infor-
mation generated by character clustering to im-
pose a class limitation on certain template slots. 
This regulates the potential input and improves 
the precision. Consider a template with one or 
more wildcard slots. If any string matched with 
these wildcard slots contains characters in dif-
ferent clusters, this template is also discarded.  
Template-Based Post-Processing (TBPP) 
After the generated templates have been filtered, 
they are used to match our CWS output and 
check if the matched tokens can be combined 
into complete words. If a template?s accuracy is 
greater than ?2, then all separators within the 
matched strings will be eliminated; otherwise, 
for a template t with accuracy between ?1 and ?2, 
we eliminate all separators in its matched string 
if no substring matched with t?s wildcard slots 
contains characters in different clusters. Resul-
tant words of less than three characters in length 
are discarded because CRF performs well with 
such words. 
3 Experiment 
3.1 Dataset 
We use the three larger corpora in SIGHAN 
Bakeoff 2006: a Simplified Chinese corpus pro-
vided by Microsoft Research Beijing, and two 
Traditional Chinese corpora provided by Aca-
demia Sinica in Taiwan and the City University 
of Hong Kong respectively. Details of each cor-
pus are listed in Table 2. 
Training Size Test SizeCorpus 
Types Words Types Words
CKIP 141 K 5.45 M 19 K 122 K
City University (CTU) 69 K 1.46 M 9 K 41 K
Microsoft Research (MSR) 88 K 2.37 M 13 K 107 K
Table 2. Corpora Information 
3.2 Results 
Table 3 lists the best combination of n-gram fea-
tures used in the G tagger. 
Uni-gram Bigram  
C-2, C-1, C0, C1 C-2C-1, C-1C0, C0C1, C-3C-1, C-2C0, C-1C1
Table 3. Best Combination of N-gram Features 
Table 4 compares the baseline G tagger and the 
enhanced GST tagger. We observe that the GST 
tagger outperforms the G tagger on all three cor-
pora. 
Conf R P F ROOV RIV 
CKIP-g 0.958 0.949 0.954 0.690 0.969 
CKIP-gst 0.961 0.953 0.957 0.658 0.974 
CTU-g 0.966 0.967 0.966 0.786 0.973 
CTU-gst 0.973 0.972 0.972 0.787 0.981 
MSR-g 0.949 0.957 0.953 0.673 0.959 
MSR-gst 0.953 0.956 0.955 0.574 0.966 
Table 4 Performance Comparison of the G Tag-
ger and the GST Tagger  
4 Conclusion  
The contribution of this paper is two fold. First, 
we successfully apply the K-means algorithm to 
character clustering and develop several cluster 
set selection algorithms for our GS tagger. This 
significantly improves the handling of sentences 
containing non-Chinese words as well as the 
overall performance. Second, we develop a post-
processing method that compensates for the 
weakness of ML-based CWS on longer words. 
References 
Hartigan, J. A., & Wong, M. A. (1979). A K-means 
Clustering Algorithm. Applied Statistics, 28, 100-
108. 
Lafferty, J., McCallum, A., & Pereira, F. (2001). 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. Pa-
per presented at the ICML-01. 
 
 
137
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 142?145,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Using Ensemble Methods for Chinese Named Entity Recognition  
Chia-Wei Wu Shyh-Yi Jan Richard Tzong-Han 
Tsai 
Wen-Lian Hsu 
 
Institute of Information Science, Academia Sinica, Nankang, Taipei,115, Taiwan 
{cwwu,shihyi,thtsai,Hsu}@iis.sinica.edu.tw 
 
  
Abstract 
In sequence labeling tasks, applying dif-
ferent machine learning models and fea-
ture sets usually leads to different results. 
In this paper, we exploit two ensemble 
methods in order to integrate multiple 
results generated under different condi-
tions. One method is based on majority 
vote, while the other is a memory-based 
approach that integrates maximum en-
tropy and conditional random field clas-
sifiers. Our results indicate that the 
memory-based method can outperform 
the individual classifiers, but the major-
ity vote method cannot. 
1 Introduction 
Sequence labeling and segmentation tasks have 
been studied extensively in the fields of computa-
tional linguistics and information extraction. Sev-
eral tasks, including, word segmentation, and 
semantic role labeling, provide rich information 
for various applications, such as segmentation in 
Chinese information retrieval and named entity 
recognition in biomedical literature mining.  
Probabilistic state automata models, such as the 
Hidden Markov model  (HMM) [6] and condi-
tional random fields (CRF) [5] are some of best, 
and therefore most popular, approaches for se-
quence labeling tasks. Both HMM and CRF con-
sider that the state transition and the state 
prediction are conditional on the observation of 
data. The advantage of the CRF model is that 
richer feature sets can be considered, because, 
unlike HMM, it does not make a dependence as-
sumption. However, the obvious drawback of the 
CRF model is that it needs more computing re-
sources, so we can not apply all the features of 
the model. One possible way to resolve this prob-
lem is to effectively combine the results of vari-
ous individual classifiers trained with different 
feature sets. In this paper, we use two ensemble 
methods to combine the results of the classifiers. 
We also combine the results generated by two 
machine learning models: maximum entropy 
(ME) [1] and CRF. One ensemble method is 
based on the majority vote [3], and the other is 
the memory based learner [7]. Although the en-
semble methods have been applied in some se-
quence labeling tasks [2],[3], similar work in 
Chinese named entity recognition is scarce. 
Our Chinese named entity tagger uses a charac-
ter-based model. For English named entity tasks, 
a character-based NER model proposed by Dan 
Klein [4] proves the usefulness of substrings 
within words. In Chinese NER, the character-
based model is more straightforward, since there 
are no spaces between Chinese words and each 
Chinese character is actually meaningful.  An-
other reason for using a character-based model is 
that it can avoid the errors sometimes made by a 
Chinese word segmentor.  
The remainder of this paper is organized as fol-
lows. In the Section 2, we introduce the machine 
learning models, the features we apply in the ma-
chine learning models, and the ensemble methods. 
In Section 3, we briefly describe the experimental 
data and the experiment results. Then, in Section 
4, we present our conclusions.. 
2 Method 
2.1 Machine Learning Models 
In this section, we introduce ME and CRF.
Maximum Entropy 
ME[1] is a statistical modeling technique used 
for estimating the conditional probability of a 
target label based on given information. The 
technique computes the probability p(y|x), where 
y denotes all possible outcomes of the space, and 
x denotes all possible features of the space. The 
computation of p(y|x) depends on a set of fea-
142
tures in x; the features are helpful for making 
predictions about the outcomes, y. 
Given a set of features and a training set, the ME 
estimation process produces a model, in which 
every feature fi has a weight ?i. The ME model 
can be represented by the following formula: 
( ) ( ) ( )???
?
???
?= ?
i
ii yxfxz
xyp ,exp| ?1
, 
( ) ( )? ? ???
?
???
?=
y i
ii yxfxz ,exp ?
.  
The probability is derived by multiplying the 
weights of the active features (i.e., those fi (y,x) = 
1). 
Conditional Random Field 
A conditional random field (CRF)[5] can be seen 
as an undirected graph model in which the nodes 
corresponding to the label sequence y are condi-
tional on the observed sequence x. The goal of 
CRF is to find the label sequence y that has the 
maximized probability, given an observation se-
quence x. The formula for the CRF model can be 
written as: 
( ) ( ) ( )( )xyxxy ,exp1| jj j FZP ?= ? ,  
where ?j is the parameter of a corresponding fea-
ture Fj , Z(x) is an normalizing factor, and Fj can 
be written as:   
( ) (? = ?= ni iiij iyyfF 0 1 ,,,, xxy ), 
where i means the relative position in the se-
quence, and yi-1 and yi denote the label at position 
i-1 and i respectively. In this paper, we only con-
sider linear chain and first-order Markov assump-
tion CRFs. In NER applications, a feature 
function fj (yi-1, yi, x, i) can be set to check 
whether x is a specific character, and whether yi-1 
is a label (such as Location) and yi is a label (such 
as Others).   
2.2 Chinese Named Entity Recognition 
In this section, we present the features applied in 
our CRF and ME models, namely, characters, 
words, and chuck information. 
Character Features 
The character features we apply in the CRF 
model and the ME model are presented in Tables 
1 and 2 respectively. The numbers listed in the 
feature type column indicate the relative position 
of a character in the sliding window. For example, 
-1 means the previous character of the target 
character. Therefore, the characters in those posi-
tions are applied in the model. The numbers in 
parentheses mean that the feature includes a 
combination of the characters in those positions. 
The unigrams in Tables 1 and 2 indicate that the 
listed features only consider to their own labels, 
whereas the bigram model considers the combi-
nation of the current label and the previous label. 
Since ME does not consider multiple states in a 
single feature, there are only unigrams in Table 2. 
In addition, as ME can handle more features than 
CRF, we apply extra features in the ME model  
 
Table 1 Character features for CRF 
 Feature Types 
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1), 
(1,2), (-1,0,1) 
bigram -2 -1 0 +1 +2, (0,1) 
 
Table 2 Character features for ME 
 Feature Types 
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1), 
(1,2), (-1,0,1) (-1,1) 
Word Information  
Because of the limitations of the closed task, we 
use the NER corpus to train the segmentors based 
on the CRF model. To simulate noisy word in-
formation in the test corpus, we use a ten-fold 
method for training segmentors to tag the training 
corpus. The word features we apply in our NER 
systems are presented in Tables 3 and 4. 
In addition to the word itself, chuck information, 
i.e., the relative position of a character in a word, 
is also valuable information. Hence, we also add 
chuck information to our models. As the diversity 
of Chinese words is greater than that of Chinese 
characters, the number of features that can be 
used in CRF is much lower than the number that 
can be used in ME.   
Table 3 Word features for CRF 
 Feature Types 
unigram 0 
bigram 0  
 
Table 4 Word features for ME 
 Feature Types 
unigram -1, 0, 1, (-2,-1), (-1,0), (0,1), (1,2) 
2.3 Ensemble Methods 
Majority vote 
We can not put all the features into the CRF 
model because of its limited resources. Therefore, 
we train several CRF classifiers with different 
feature sets so that we can use as many features 
143
as possible. Then, we use the following simple, 
equally weighted linear equation, called majority 
vote, to combine the results of the CRF classifi-
ers.   
( ) ( )? == Ti i xyCxyS 0 ,, , 
where S(y,x) is the score of a label y and a char-
acter x respectively; T denotes the total number 
of CRF models; and the value of Ci(y,x) is 1 if 
the decision of the result of the ith CRF model is 
y, otherwise it is zero. The highest score of y is 
chosen as the label of x. The results are incorpo-
rated into the Viterbi algorithm to search for the 
path with the maximum scores. 
In this paper, the first step in the majority vote 
experiment is to train three CRF classifiers with 
different feature sets. Then, in the second step, 
we use the results obtained in the first step to 
generate the voting scores for the Viterbi algo-
rithm. 
Memory Based learner 
The memory-based learning method memorizes 
all examples in a training corpus. If a word is 
unknown, the memory-based classifier uses the 
k-nearest neighbors to find the most similar ex-
ample as the answer. Instead of using the com-
plete algorithm of the memory-based learner, we 
do not handle unseen data. In our memory- based 
combination method, the learner remembers all 
named entities from the results of the various 
classifiers and then tags the characters that were 
originally tagged as ?Other?. For example, if a 
character x is tagged by one classifier as ?0? 
(?Others? tag) and if the memory-based classifier 
learns from another classifier that this character 
is tagged as PER, then x will be tagged as ?B-
PER? by the memory-based classifier. 
The obvious drawback of this method is that the 
precision rate might decrease as the recall rate 
increases. Therefore, we set the following three 
rules to filter out samples that are likely to have a 
high error rate.  
1. Named entities can not be tagged as differ-
ent named entity tags by different classifiers. 
2. We set an absolute frequency threshold to 
filter out examples that occur less than the 
threshold. 
3. We set a relative frequency threshold to 
filter out examples that occur less than the 
threshold. For example, if a word x appears 
10 times in the corpus, then half of the in-
stances of x have to be tagged as named en-
tities; otherwise, x will be filtered out of the 
memory classifier. 
In our experiment, we used the memory-based 
learner to memorize the named entities from the 
tagging results of an ME classifier and a CRF 
classifier, and then tagged the tagging results of 
the CRF classifier.   
3 Experiments 
3.1 Data 
We selected the corpora of City University of 
Hong Kong (CityU) and Microsoft Research 
(MSRA) corpora to evaluate our methods. CityU 
is a Traditional Chinese corpus, and MSRA is 
Simplified Chinese corpus. 
3.2 Results 
Table 5 shows the results of several methods ap-
plied to the MSRA corpus. The memory-based 
ensemble method, which combines the results of 
a maximum entropy model and those of a CRF 
classifier, achieves the best performance. The 
majority vote combined with the results of three 
CRF models based on different feature sets has 
the worst performance. 
 
Table 5 msra  
 Precision Recall FB1 
Memory based 86.21 78.14 81.98 
Majority Vote 85.83 76.06 80.65 
Only-Character 86.70 75.54 80.74 
CRF 86.23 77.40 81.58 
 
The results obtained on Cityu, presented in Table 
6, show that the single CRF classifier achieved 
the best performance. None of the ensemble 
methods can outperform the non-ensemble meth-
ods. 
 
Table 6 cityu 
 Precision Recall FB1 
Memory based  90.79 86.26 88.47 
Majority Vote 90.52 84.15 87.22 
Only-Character 91.32 84.55 87.80 
CRF 92.01 85.45 88.61 
 
Tables 7 and 8 show the results of the memory-
based ensemble methods under different rules. 
We set the frequency threshold as 2 and the rela-
tive frequency threshold as 0.5. The results show 
that the relative frequencies rule effectively re-
duces the loss of precision caused by more enti-
ties being tagged by the memory-based classifier. 
The memory-based ensemble method works well 
on the MSRA corpus, but not on the CityU cor-
pus. In the MSRA corpus, the memory-based 
144
ensemble method outperforms the individual 
CRF model by approximately 0.4 % in FB1. We 
found that the memory-based classifier can not 
achieve a better performance than the CRF model 
because it misclassifies many organizations? 
names. Therefore, we chose another strategy that 
restricts the memory-based classifier to tagging 
person names only. Under this restriction, the 
performance of the memory-based classifier im-
proves FB1 by approximately 0.2%. 
 
Table 7 msra- The performances of memory 
based ensemble methods under different rules. 
 Precision Recall FB1 
Frequency Threshold 86.18 78.16 81.97 
Relative Frequency 
Threshold 
86.21 78.14 81.98 
Only Person 86.27 77.58 81.69 
 
Table 8 cityu- The performances of memory 
based ensemble methods under different rules. 
 Precision Recall FB1 
Frequency Threshold 90.69 86.55 88.57 
Relative Frequency 
Threshold 
90.87 86.29 88.52 
Only Person 92.00 85.66 88.72 
4 Conclusion  
In this paper, we use ME and CRF models to 
train a Chinese named entity tagger. Like previ-
ous researchers, we found that CRF models out-
perform ME models. We also apply two 
ensemble methods, namely, majority vote and 
memory-based approaches, to the closed NER 
shared task. Our results show that integrating 
individual classifiers as the majority vote ap-
proach does not outperform the individual classi-
fiers. Furthermore, a memory-based combination 
only seems to work when we restrict the mem-
ory-based classifier to handling person names. 
Acknowledgement 
We are grateful for the support of National Sci-
ence Council under Grant NSC 95-2752-E-001-
001-PAE. 
References  
1. Berger, A., Pietra, S.A.D. and Pietra, V.J.D. A 
Maximum Entropy Approach to Natural Language 
Processing. Computer Linguistic, 22. 1996 39-71. 
2. Florian, R., Ittycheriah, A., Jing, H. and Zhang, T., 
Named Entity Recognition through Classifier 
Combination. in Proceedings of Conference on 
Computational Natural Language Learning, 2003, 
168-171. 
3. Halteren, H.v., Zavrel, J. and Daelemans, W. Im-
proving accuracy in word class tagging through 
combination of machine learning systems. Compu-
tational Linguistics, 27 (2). 2001 199-230. 
4. Klein, D., Smarr, J., Nguyen, H. and Manning, 
C.D., Named Entity Recognition with Character-
Level Models. in Conference on Computational 
Natural Language Learning, 2003, 180-183. 
5. Lafferty, J., McCallum, A. and Pereira, F. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. International 
Conference on Machine Learning. 2001 282-289. 
6. Rabiner, L. A tutorial on hidden Markov models 
and selected applications in speech recognition. 
Proceedings of the IEEE, 77 (2). 1989 257-286. 
7. Sutton, C., Rohanimanesh, K. and McCallum, A., 
Dynamic Conditional Random Fields: Factorized 
Probabilistic Models for Labeling and Segmenting 
Sequence Data. in Proceedings of the Twenty-First 
International Conference on Machine Learning, 
2004, 99-107. 
8. Zavrel, J. and Daelemans, W. Memory-based learn-
ing: using similarity for smoothing. Proceedings of 
the eighth conference on European chapter of the 
Association for Computational Linguistics. 1997 
436 - 443. 
145
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 243?246,
New York, June 2006. c?2006 Association for Computational Linguistics
A Hybrid Approach to Biomedical Named Entity Recognition and  
Semantic Role Labeling 
 
Richard Tzong-Han Tsai 
Department of Computer Science and Information Engineering 
National Taiwan University 
Nankang, Taipei, Taiwan, 115 
thtsai@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we describe our hybrid ap-
proach to two key NLP technologies: 
biomedical named entity recognition 
(Bio-NER) and (Bio-SRL). In Bio-NER, 
our system successfully integrates linguis-
tic features into the CRF framework. In 
addition, we employ web lexicons and 
template-based post-processing to further 
boost its performance. Through these 
broad linguistic features and the nature of 
CRF, our system outperforms state-of-
the-art machine-learning-based systems, 
especially in the recognition of protein 
names (F=78.5%). In Bio-SRL, first, we 
construct a proposition bank on top of the 
popular biomedical GENIA treebank fol-
lowing the PropBank annotation scheme. 
We only annotate the predicate-argument 
structures (PAS?s) of thirty frequently 
used biomedical verbs (predicates) and 
their corresponding arguments. Second, 
we use our proposition bank to train a 
biomedical SRL system, which uses a 
maximum entropy (ME) machine-
learning model. Thirdly, we automatically 
generate argument-type templates, which 
can be used to improve classification of 
biomedical argument roles. Our experi-
mental results show that a newswire Eng-
lish SRL system that achieves an F-score 
of 86.29% in the newswire English do-
main can maintain an F-score of 64.64% 
when ported to the biomedical domain. 
By using our annotated biomedical corpus, 
we can increase that F-score by 22.9%. 
Adding automatically generated template 
features further increases overall F-score 
by 0.47% and adjunct (AM) F-score by 
1.57%, respectively. 
1 Introduction 
The volume of biomedical literature available on 
the Web has experienced unprecedented growth in 
recent years, and demand for efficient methods to 
process this material has increased accordingly. 
Lately, there has been a surge of interest in mining 
biomedical literature. To this end, more and more 
information extraction (IE) systems using natural 
language processing (NLP) technologies have been 
developed for use in the biomedical field. Key 
biomedical IE tasks include named entity (NE) 
recognition (NER), such as the recognition of pro-
tein and gene names; and relation extraction, such 
as the extraction of protein-protein and gene-gene 
interactions. 
NER identifies named entities from natural lan-
guage texts and classifies them into specific classes 
according to a defined ontology or classification. 
In general, biomedical NEs do not follow any no-
menclature and may comprise long compound 
words and short abbreviations. Some NEs contain 
various symbols and other spelling variations. On 
average, an NE has five synonyms (Tsai et al, 
2006a), and it may belong to multiple categories 
intrinsically. Since biomedical language and vo-
243
cabulary are highly complex and evolving rapidly, 
Bio-NER is a very challenging problem, which 
raises a number of difficulties. 
The other main focus of Bio-IE is relation ex-
traction. Most systems only extract the relation 
targets (e.g., proteins, genes) and the verbs repre-
senting those relations, overlooking the many ad-
verbial and prepositional phrases and words that 
describe location, manner, timing, condition, and 
extent. However, the information in such phrases 
may be important for precise definition and clarifi-
cation of complex biological relations. 
This problem can be tackled by using semantic 
role labeling (SRL) because it not only recognizes 
main roles, such as agents and objects, but also 
extracts adjunct roles such as location, manner, 
timing, condition, and extent. (Morarescu et al, 
2005) has demonstrated that full-parsing and SRL 
can improve the performance of relation extraction, 
resulting in an F-score increase of 15% (from 67% 
to 82%). This significant result leads us to surmise 
that SRL may also have potential for relation ex-
traction in the biomedical domain. Unfortunately, 
no SRL system for the biomedical domain exists.  
In this paper, we tackle the problems of both 
biomedical SRL and NER. Our contributions are (1) 
employing web lexicons and template-based post-
processing to boost the performance of Bio-NER; 
(2) constructing a proposition bank on top of the 
popular biomedical GENIA treebank following the 
PropBank annotation scheme and developing a 
Biomedical SRL system. We adapt an SRL system 
trained the World Street Journal (WSJ) corpus to 
the biomedical domain. On adjunct arguments, 
especially those relevant to the biomedical domain, 
the performance is unsatisfactory. We, therefore, 
develop automatically generated templates for 
identifying these arguments. 
2 Biomedical Named Entity Recognition  
Our Bio-NER system uses the CRF model 
(Lafferty et al, 2001), which has proven its effec-
tiveness in several sequence tagging tasks.  
2.1 Features and Post-Processing 
Orthographical Features 
In our experience, ALLCAPS, CAPSMIX, and 
INITCAP are more useful than others. The details 
are listed in (Tsai et al, 2006a). 
Context Features 
Words preceding or following the target word may 
be useful for determining its category. In our ex-
perience, a suitable window size is five. 
Part-of-speech Features 
Part-of-speech information is quite useful for iden-
tifying NEs. Verbs and prepositions usually indi-
cate an NE?s boundaries, whereas nouns not found 
in the dictionary are usually good candidates for 
named entities. Our experience indicates that five 
is also a suitable window size. The MBT POS tag-
ger is used to provide POS information. We trained 
it on GENIA 3.02p and achieved 97.85% accuracy. 
Word Shape Features 
As NEs in the same category may look similar 
(e.g., IL-2 and IL-4), we have to find a simple way 
to normalize all similar words. According to our 
method, capitalized characters are all replaced by 
?A?, digits are all replaced by ?0?, non-English 
characters are replaced by ?_? (underscore), and 
non-capitalized characters are replaced by ?a?. To 
further normalize these words, we reduce consecu-
tive strings of identical characters to one character. 
Affix Features 
Some affixes can provide good clues for classify-
ing named entities (e.g., ?ase?). In our experience, 
an acceptable affix length is 3-5 characters. 
Lexicon Features 
Depending on the quality of a given dictionary, our 
system uses one of two different lexicon features to 
estimate the possibility of a token in a biomedical 
named entity. The first feature determines whether 
a token is part of a multi-word NE in the dictionary, 
while the second feature calculates the minimum 
distance between the given token and a dictionary. 
In our experience, the first feature is effective for a 
dictionary containing high-quality items, for ex-
ample, human-curated protein dictionaries. The 
second feature is effective for a dictionary that has 
a large number of items that are not very accurate, 
for example, web or database lexicons. Details can 
be found in (Tsai et al, 2006a). 
Post-Processing 
We count the number of occurrences of a word x 
appearing in the rightmost position of all NEs in 
each category. Let the maximum occurrence be n, 
244
and the corresponding category be c. The total 
number of occurrences of x in the rightmost posi-
tion of an NE is T; c/T is the consistency rate of x. 
According to our analysis of the training set of the 
JNLPBA 2004 data, 75% of words have a consis-
tency rate of over 95%. We record this 75% of 
words and their associated categories in a table. 
After testing, we crosscheck all the rightmost 
words of NEs found by our system against this ta-
ble. If they match, we overwrite the NE categories 
with those from the table.  
2.2 Experiments and Summary 
We perform 10-fold cross validation on the 
GENIA V3.02 corpus (Kim et al, 2003) to com-
pare our CRF-based system with other biomedical 
NER systems. The experimental results are re-
ported in Table 1. Our system  outperforms other 
systems in protein names by an F-score of at least 
2.6%. For DNA names, our performance is very 
close to that of the best system. 
BioNER System Protein DNA 
Our System (Tsai et al, 2006a) 78.4 66.3 
HMM (Zhou et al, 2004) 75.8 63.3 
Two Phase SVM (Lee et al, 2003) 70.6 66.4 
Table 1. Performance of protein and DNA name 
recognition on the GENIA V3.02 corpus 
We have made every effort to implement a vari-
ety of linguistic features in our system?s CRF 
framework. Thanks to these features and the nature 
of CRF, our system outperforms state-of-the-art 
machine-learning-based systems, especially in the 
recognition of protein names. 
Our system still has difficulty recognizing long, 
complicated NEs and coordinated NEs and distin-
guishing between overlapping NE classes, e.g., 
cell-line and cell-type. This is because biomedical 
texts have complicated sentence structures and in-
volve more expert knowledge than texts from the 
general newswire domain. Since pure machine 
learning approaches cannot model long contextual 
phenomena well due to context window size limi-
tations and data sparseness, we believe that tem-
plate-based methods, which exploit long templates 
containing different levels of linguistic information, 
may be of help. Certain errors, such as incorrect 
boundary identification, are more tolerable if the 
main purpose is to discover relations between NEs 
(Tsai et al, 2006c). We shall exploit more linguis-
tic features, such as composite features and exter-
nal features, in the future. However, machine 
leaning approaches suffer from a serious problem 
of annotation inconsistency, which confuses ma-
chine learning models and makes evaluation diffi-
cult. In order to reduce human annotation effort 
and alleviate the scarcity of available annotated 
corpora, we shall learn from web corpora to de-
velop machine learning techniques in different 
biomedical domains. 
3 Biomedical Semantic Role Labeling 
In this section, we describe the main steps in build-
ing a biomedical SRL system: (1) create semantic 
roles for each biomedical verb; (2) construct a 
biomedical corpus, annotated with verbs and their 
corresponding semantic roles; (3) build an auto-
matic semantic interpretation model, using the an-
notated text as a training corpus for machine 
learning. However, on adjunct arguments, espe-
cially on those highly relevant to the biomedical 
domain, such as AM-LOC (location), the perform-
ance is not satisfactory. We therefore develop a 
template generation method to create templates 
that are used as features for identifying these ar-
gument types. 
3.1 Biomedical Proposition Bank -- BioProp 
Our biomedical proposition bank, BioProp, is 
based on the GENIA Treebank (Yuka et al, 2005), 
which is a 491-abstract corpus annotated with syn-
tactic structures. The semantic annotation in Bio-
Prop is added to the proper constituents in a 
syntactic tree. 
Basically, we adopt the definitions in PropBank 
(Palmer et al, 2005). For the verbs not in Prop-
Bank, such as ?phosphorylate?, we define their 
framesets. Since the annotation is time-consuming, 
we adopt a semi-automatic approach. We adapt an 
SRL system trained on PropBank (Wall Street 
Journal corpus) to the biomedical domain. We first 
use this SRL system to automatically annotate our 
corpus, and then human annotators to double check 
the system?s results. Therefore, human effort is 
greatly reduced. 
3.2 Biomedical SRL System -- SEROW 
245
Following (Punyakanok et al, 2004), we formulate 
SRL as a constituent-by-constituent (C-by-C) tag-
ging problem. We use BioProp to train our bio-
medical SRL system, SEROW (Tsai et al, 2006b), 
which uses a maximum entropy (ME) machine-
learning model. We use the basic features de-
scribed in (Xue & Palmer, 2004). In addition, we 
automatically generate templates which can be 
used to improve classification of biomedical argu-
ment types. The details of SEROW system are de-
scribed in (Tsai et al, 2005) and (Tsai et al, 
2006b). 
3.3 Experiment and Summary 
Our experimental results show that a newswire 
English SRL system that achieves an F-score of 
86.29% can maintain an F-score of 64.64% when 
ported to the biomedical domain. By using SE-
ROW, we can increase that F-score by 22.9%. 
Adding automatically generated template features 
further increases overall F-score by 0.47% and ad-
junct (AM) F-score by 1.57%, respectively.  
4 Conclusion 
NER and SRL are two key topics in biomedical 
NLP. For NER, we find broad linguistic features 
and integrate them into our CRF framework. Our 
system outperforms most machine learning-based 
systems, especially in the recognition of protein 
names (78.4% of F-score). In the future, templates 
that can match long contextual relations and coor-
dinated NEs may be applied to NER post-
processing. Web corpora may also be used to en-
hance unknown NE detection. In Bio-SRL, our 
contribution is threefold. First, we construct a bio-
medical proposition bank, BioProp, on top of the 
popular biomedical GENIA treebank following the 
PropBank annotation scheme. We employ semi-
automatic annotation using an SRL system trained 
on PropBank thereby significantly reducing anno-
tation effort. Second, we construct SEROW, which 
uses BioProp as its training corpus. Thirdly, we 
develop a method to automatically generate tem-
plates that can boost overall performance, espe-
cially on location, manner, adverb, and temporal 
arguments. In the future, we will expand BioProp 
to include more biomedical verbs and will also 
integrate a parser into SEROW. 
References  
Kim, J.-D., Ohta, T., Teteisi, Y., & Tsujii, J. i. (2003). 
Genia corpus - a semantically annotated corpus for 
bio-textmining. Bioinformatics, 19(suppl. 1). 
Lafferty, J., McCallum, A., & Pereira, F. (2001). Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. Paper presented 
at the ICML-01. 
Lee, K.-J., Hwang, Y.-S., & Rim, H.-C. (2003). Two 
phase biomedical ne recognition based on svms. Pa-
per presented at the ACL-03 Workshop on Natural 
Language Processing in Biomedicine. 
Morarescu, P., Bejan, C., & Harabagiu, S. (2005). Shal-
low semantics for relation extraction. Paper presented 
at the IJCAI-05. 
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The 
proposition bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1). 
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). 
Semantic role labeling via integer linear program-
ming inference. Paper presented at the 20th Interna-
tional Conference on Computational Linguistics 
(COLING-04). 
Tsai, R. T.-H., Chou, W.-C., Wu, S.-H., Sung, T.-Y., 
Hsiang, J., & Hsu, W.-L. (2006a). Integrating lin-
guistic knowledge into a conditional random field 
framework to identify biomedical named entities. 
Expert Systems with Applications, 30(1), 117-128. 
Tsai, R. T.-H., Lin, W.-C. C. Y.-C., Ku, W., Su, Y.-S., 
Sung, T.-Y., & Hsu, W.-L. (2006b). Serow: Adapting 
semantic role labeling for biomedical verbs: An ex-
ponential model coupled with adapting semantic role 
labeling for biomedical verbs: An exponential model 
coupled with automatically generated template fea-
tures. To appear in BioNLP-2006. 
Tsai, R. T.-H., Wu, C.-W., Lin, Y.-C., & Hsu, W.-L. 
(2005). Exploiting full parsing information to label 
semantic roles using an ensemble of me and svm via 
integer linear programming. Paper presented at the 
CoNLL-2005. 
Tsai, R. T.-H., Wu, S.-H., Chou, W.-C., Lin, Y.-C., He, 
D., Hsiang, J., et al (2006c). Various criteria in the 
evaluation of biomedical named entity recognition. 
BMC Bioinformatics, 7(92). 
Xue, N., & Palmer, M. (2004). Calibrating features for 
semantic role labeling. Paper presented at the 
EMNLP 2004. 
Yuka, T., Yakushiji, A., Ohta, T., & Tsujii, J. (2005). 
Syntax annotation for the genia corpus. 
Zhou, G., Zhang, J., Su, J., Shen, D., & Tan, C. (2004). 
Recognizing names in biomedical texts: A machine 
learning approach. Bioinformatics, 20, 1178-1190. 
 
 
246
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 5?12,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Semi-Automatic Method for  
Annotating a Biomedical Proposition Bank 
 
Wen-Chi Chou1, Richard Tzong-Han Tsai1,2, Ying-Shan Su1, 
Wei Ku1,3, Ting-Yi Sung1 and Wen-Lian Hsu1 
1Institute of Information Science, Academia Sinica, Taiwan, ROC. 
2Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan, ROC. 
3Institute of Molecular Medicine, National Taiwan University, Taiwan, ROC. 
{jacky957,thtsai,qnn,wilmaku,tsung,hsu}@iis.sinica.edu.tw 
 
  
 
Abstract 
In this paper, we present a semi-
automatic approach for annotating se-
mantic information in biomedical texts. 
The information is used to construct  
a biomedical proposition bank called 
BioProp. Like PropBank in the newswire 
domain, BioProp contains annotations of 
predicate argument structures and seman-
tic roles in a treebank schema. To con-
struct BioProp, a semantic role labeling 
(SRL) system trained on PropBank is 
used to annotate BioProp. Incorrect tag-
ging results are then corrected by human 
annotators. To suit the needs in the bio-
medical domain, we modify the Prop-
Bank annotation guidelines and charac-
terize semantic roles as components of 
biological events. The method can sub-
stantially reduce annotation efforts, and 
we introduce a measure of an upper 
bound for the saving of annotation efforts. 
Thus far, the method has been applied 
experimentally to a 4,389-sentence tree-
bank corpus for the construction of Bio-
Prop. Inter-annotator agreement meas-
ured by kappa statistic reaches .95 for 
combined decision of role identification 
and classification when all argument la-
bels are considered. In addition, we show 
that, when trained on BioProp, our bio-
medical SRL system called BIOSMILE 
achieves an F-score of 87%. 
1 Introduction 
The volume of biomedical literature available on 
the Web has grown enormously in recent years, a 
trend that will probably continue indefinitely. 
Thus, the ability to process literature automati-
cally would be invaluable for both the design and 
interpretation of large-scale experiments. To this 
end, several information extraction (IE) systems 
using natural language processing techniques 
have been developed for use in the biomedical 
field. Currently, the focus of IE is shifting from 
the extraction of nominal information, such as 
named entities (NEs) to verbal information that 
represents the relations between NEs, e.g., events 
and function (Tateisi et al, 2004; Wattarujeekrit 
et al, 2004). In the IE of relations, the roles of 
NEs participating in a relation must be identified 
along with a verb of interest. This task involves 
identifying main roles, such as agents and objects, 
and adjunct roles (ArgM), such as location, man-
ner, timing, condition, and extent. This identifi-
cation task is called semantic role labeling (SRL). 
The corresponding roles of the verb (predicate) 
are called predicate arguments, and the whole 
proposition is known as a predicate argument 
structure (PAS). 
To develop an automatic SRL system for the 
biomedical domain, it is necessary to train the 
system with an annotated corpus, called proposi-
tion bank (Palmer et al, 2005). This corpus con-
tains annotations of semantic PAS?s superim-
posed on the Penn Treebank (PTB) (Marcus et 
al., 1993; Marcus et al, 1994). However, the 
process of manually annotating the PAS?s to 
construct a proposition bank is quite time-
consuming. In addition, due to the complexity of 
proposition bank annotation, inconsistent annota-
tion may occur frequently and further complicate 
5
the annotation task. In spite of the above difficul-
ties, there are proposition banks in the newswire 
domain that are adequate for training SRL sys-
tems (Xue and Palmer, 2004; Palmer et al, 2005). 
In addition, according to the CoNLL-2005 
shared task (Carreras and M?rquez, 2005), the 
performance of SRL systems in general does not 
decline significantly when tagging out-of-domain 
corpora. For example, when SRL systems trained 
on the Wall Street Journal (WSJ) corpus were 
used to tag the Brown corpus, the performance 
only dropped by 15%, on average. In comparison 
to annotating from scratch, annotation efforts 
based on the results of an available SRL system 
are much reduced. Thus, we plan to use a news-
wire SRL system to tag a biomedical corpus and 
then manually revise the tagging results. This 
semi-automatic procedure could expedite the 
construction of a biomedical proposition bank for 
use in training a biomedical SRL system in the 
future. 
2 The Biomedical Proposition Bank - 
BioProp 
As proposition banks are semantically annotated 
versions of a Penn-style treebank, they provide 
consistent semantic role labels across different 
syntactic realizations of the same verb. The an-
notation captures predicate-argument structures 
based on the sense tags of polysemous verbs 
(called framesets) and semantic role labels for 
each argument of the verb. Figure 1 shows the 
annotation of semantic roles, exemplified by the 
following sentence: ?IL4 and IL13 receptors ac-
tivate STAT6, STAT3 and STAT5 proteins in 
normal human B cells.? The chosen predicate is 
the word ?activate?; its arguments and their as-
sociated word groups are illustrated in the figure. 
 
IL4 and IL 13 
 receptors 
activate STAT6, STAT3 
and  
STAT5 proteins 
the human 
B cells 
in 
NP 
Arg0 predicate AM-LOC Arg1 
NP 
NP-SBJ VP 
VP PP 
Figure 1. A treebank annotated with semantic 
role labels 
Since proposition banks are annotated on top 
of a Penn-style treebank, we selected a biomedi-
cal corpus that has a Penn-style treebank as our 
corpus. We chose the GENIA corpus (Kim et al, 
2003), a collection of MEDLINE abstracts se-
lected from the search results with the following 
keywords: human, blood cells, and transcription 
factors. In the GENIA corpus, the abstracts are 
encoded in XML format, where each abstract 
also contains a MEDLINE UID, and the title and 
content of the abstract. The text of the title and 
content is segmented into sentences, in which 
biological terms are annotated with their seman-
tic classes. The GENIA corpus is also annotated 
with part-of-speech (POS) tags (Tateisi and Tsu-
jii, 2004), and co-references are added to part of 
the GENIA corpus by the MedCo project at the 
Institute for Infocomm Research, Singapore 
(Yang et al, 2004). 
The Penn-style treebank for GENIA, created 
by Tateisi et al (2005), currently contains 500 
abstracts. The annotation scheme of the GENIA 
Treebank (GTB), which basically follows the 
Penn Treebank II (PTB) scheme (Bies et al, 
1995), is encoded in XML. However, in contrast 
to the WSJ corpus, GENIA lacks a proposition 
bank. We therefore use its 500 abstracts with 
GTB as our corpus. To develop our biomedical 
proposition bank, BioProp, we add the proposi-
tion bank annotation on top of the GTB annota-
tion. 
In the following, we report on the selection of 
biomedical verbs, and explain the difference be-
tween their meaning in PropBank (Palmer et al, 
2005), developed by the University of Pennsyl-
vania, and their meaning in BioProp (a biomedi-
cal proposition bank). We then introduce Bio-
Prop?s annotation scheme, including how we 
modify a verb?s framesets and how we define 
framesets for biomedical verbs not defined in 
VerbNet (Kipper et al, 2000; Kipper et al, 2002). 
2.1 Selection of Biomedical Verbs 
We selected 30 verbs according to their fre-
quency of use or importance in biomedical texts. 
Since our targets in IE are the relations of NEs, 
only sentences containing protein or gene names 
are used to count each verb?s frequency. Verbs 
that have general usage are filtered out in order 
to ensure the focus is on biomedical verbs. Some 
verbs that do not have a high frequency, but play 
important roles in describing biomedical rela-
tions, such as ?phosphorylate? and ?transacti-
vate?, are also selected. The selected verbs are 
listed in Table 1. 
6
 Predicate Frameset Example 
express  
(VerbNet) 
Arg0: agent  
Arg1: theme 
Arg2: recipient or destina-
tion 
[Some legislatorsArg0][expressedpredicate] [concern that a gas-tax 
increase would take too long and possibly damage chances of a 
major gas-tax-increasing ballot initiative that voters will consider 
next JuneArg1 ]. 
translate 
(VerbNet) 
Arg0: causer of transfor-
mation  
Arg1: thing changing   
Arg2: end state 
Arg3: start state 
But some cosmetics-industry executives wonder whether [tech-
niques honed in packaged goodsArg1] [willAM-MOD] [translatepredicate] 
[to the cosmetics businessArg2]. 
express  
(BioProp) 
Arg0: causer of expression 
Arg1: thing expressing 
[B lymphocytes and macrophagesArg0] [expresspredicate] [closely 
related immunoglobulin G ( IgG ) Fc receptors ( Fc gamma RII ) 
that differ only in the structures of their cytoplasmic domainsArg1]. 
Table 2. Framesets and examples of ?express? and ?translate? 
 
Type Verb list 
1 encode, interact, phosphorylate,  transactivate 
2 express, modulate 
3 bind 
4 
activate, affect, alter, associate, block, 
decrease differentiate, encode, enhance, 
increase, induce, inhibit, mediate, mu-
tate, prevent, promote, reduce, regulate, 
repress, signal, stimulate, suppress, 
transform, trigger 
Table 1. Selected biomedical verbs and their 
types 
2.2 Framesets of Biomedical Verbs 
Annotation of BioProp is mainly based on 
Levin?s verb classes, as defined in the VerbNet 
lexicon (Kipper et al, 2000). In VerbNet, the 
arguments of each verb are represented at the 
semantic level, and thus have associated seman-
tic roles. However, since some verbs may have 
different usages in biomedical and newswire 
texts, it is necessary to customize the framesets 
of biomedical verbs. The 30 verbs in Table 1 are 
categorized into four types according to the de-
gree of difference in usage: (1) verbs that do not 
appear in VerbNet due to their low frequency in 
the newswire domain; (2) verbs that do appear in 
VerbNet, but whose biomedical meanings and 
framesets are undefined; (3) verbs that do appear 
in VerbNet, but whose primary newswire and 
biomedical usage differ; (4) verbs that have the 
same usage in both domains. 
Verbs of the first type play important roles in 
biomedical texts, but rarely appear in newswire 
texts and thus are not defined in VerbNet. For 
example, ?phosphorylate? increasingly appears 
in the fast-growing PubMed abstracts that report 
experimental results on phosphorylated events; 
therefore, it is included in our verb list. However, 
since VerbNet does not define the frameset for 
?phosphorylate?, we must define it after analyz-
ing all the sentences in our corpus that contain 
the verb. Other type 1 verbs may correspond to 
verbs in VerbNet; in such cases, we can borrow 
the VerbNet definitions and framesets. For ex-
ample, ?transactivate? is not found in VerbNet, 
but we can adopt the frameset of ?activate? for 
this verb. 
Verbs of the second type appear in VerbNet, 
but have unique biomedical meanings that are 
undefined. Therefore, the framesets correspond-
ing to their biomedical meanings must be added. 
In most cases, we can adopt framesets from 
VerbNet synonyms. For example, ?express? is 
defined as ?say? and ?send very quickly? in 
VerbNet. However, in the biomedical domain, its 
usage is very similar to ?translate?. Thus, we can 
use the frameset of ?translate? for ?express?. Ta-
ble 2 shows the framesets and corresponding ex-
amples of ?express? in the newswire domain and 
biomedical domain, as well as that of ?translate? 
in VerbNet.  
Verbs of the third type also appear in VerbNet. 
Although the newswire and biological senses are 
defined therein, their primary newswire sense is 
not the same as their primary biomedical sense. 
?Bind,? for example, is common in the newswire 
domain, and it usually means ?to tie? or ?restrain 
with bonds.? However, in the biomedical domain, 
its intransitive use- ?attach or stick to?- is far 
more common. For example, a Google search for 
the phrase ?glue binds to? only returned 21 re-
sults, while the same search replacing ?glue? 
with ?protein? yields 197,000 hits. For such 
verbs, we only need select the appropriate alter-
native meanings and corresponding framesets. 
Lastly, for verbs of the fourth type, we can di-
7
rectly adopt the newswire definitions and frame-
sets, since they are identical.  
2.3 Distribution of Selected Verbs 
There is a significant difference between the oc-
currence of the 30 selected verbs in biomedical 
texts and their occurrence in newswire texts. The 
verbs appearing in verb phrases constitute only 
1,297 PAS?s, i.e., 1% of all PAS?s, in PropBank 
(shown in Figure 2), compared to 2,382 PAS?s, 
i.e., 16% of all PAS?s, in BioProp (shown in 
Figure 3). Furthermore, some biomedical verbs 
have very few PAS?s in PropBank, as shown in 
Table 3. The above observations indicate that it 
is necessary to annotate a biomedical proposition 
bank for training a biomedical SRL system. 
 
Figure 2. The percentage of the 30 biomedical 
verbs and other verbs in PropBank 
 
Figure 3. The percentage of the 30 biomedical 
verbs and other verbs in BioProp 
3  Annotation of BioProp 
3.1 Annotation Process 
After choosing 30 verbs as predicates, we 
adopted a semi-automatic method to annotate 
BioProp. The annotation process consists of the 
following steps: (1) identifying predicate candi-
dates; (2) automatically annotating the biomedi-
cal semantic roles with our WSJ SRL system; (3) 
transforming the automatic tagging results into 
WordFreak (Morton and LaCivita, 2003) format; 
and (4) manually correcting the annotation re-
sults with the WordFreak annotation tool. We 
now describe these steps in detail: 
  
Verbs # in BioProp Ratio(%) 
# in 
PropBank Ratio(%) 
induce 290 1.89 16 0.01 
bind 252 1.64 0 0 
activate 235 1.53 2 0 
express 194 1.26 53 0.03 
inhibit 184 1.20 6 0 
increase 166 1.08 396 0.24 
regulate 122 0.79 23 0.01 
mediate 104 0.68 1 0 
stimulate 93 0.61 11 0.01 
associate 82 0.53 51 0.03 
encode 79 0.51 0 0 
affect 60 0.39 119 0.07 
enhance 60 0.39 28 0.02 
block 58 0.38 71 0.04 
reduce 55 0.36 241 0.14 
decrease 54 0.35 16 0.01 
suppress 38 0.25 4 0 
interact 36 0.23 0 0 
alter 27 0.18 17 0.01 
transactivate 24 0.16 0 0 
modulate 22 0.14 1 0 
phosphorylate 21 0.14 0 0 
transform 21 0.14 22 0.01 
differentiate 21 0.14 2 0 
repress 17 0.11 1 0 
prevent 15 0.10 92 0.05 
promote 14 0.09 52 0.03 
trigger 14 0.09 40 0.02 
mutate 14 0.09 1 0 
signal 10 0.07 31 0.02 
Table 3. The number and percentage of PAS?s 
for each verb in BioProp and PropBank 
1. Each word with a VB POS tag in a verb 
phrase that matches any lexical variant of 
the 30 verbs is treated as a predicate candi-
date. The automatically selected targets are 
then double-checked by human annotators. 
As a result, 2,382 predicates were identified 
in BioProp.  
2. Sentences containing the above 2,382 
predicates were extracted and labeled 
automatically by our WSJ SRL system. In 
total, 7,764 arguments were identified. 
3. In this step, sentences with PAS annota-
tions are transformed into WordFreak for-
mat (an XML format), which allows anno-
tators to view a sentence in a tree-like fash-
ion. In addition, users can customize the tag 
set of arguments. Other linguistic informa-
tion can also be integrated and displayed in 
8
WordFreak, which is a convenient annota-
tion tool. 
4. In the last step, annotators check the pre-
dicted semantic roles using WordFreak and 
then correct or add semantic roles if the 
predicted arguments are incorrect or miss-
ing, respectively. Three biologists with suf-
ficient biological knowledge in our labora-
tory performed the annotation task after re-
ceiving computational linguistic training 
for approximately three months.   
Figure 4 illustrates an example of BioProp an-
notation displayed in WordFreak format, using 
the frameset of ?phophorylate? listed in Table 4.  
This annotation process can be used to con-
struct a domain-specific corpus when a general-
purpose tagging system is available.  In our ex-
perience, this semi-automatic annotation scheme 
saves annotation efforts and improves the anno-
tation consistency. 
 
Predicate Frameset 
phosphorylate  
 
Arg0: causer of phosphorylation 
Arg1: thing being phosphorylated 
Arg2: end state  
Arg3: start state 
Table 4. The frameset of ?phosphorylate? 
3.2 Inter-annotation Agreement 
We conducted preliminary consistency tests on 
2,382 instances of biomedical propositions. The 
inter-annotation agreement was measured by the 
kappa statistic (Siegel and Castellan, 1988), the 
definition of which is based on the probability of 
inter-annotation agreement, denoted by P(A), and 
the agreement expected by chance, denoted by 
P(E). The kappa statistics for inter-annotation 
agreement were .94 for semantic role identifica-
tion and .95 for semantic role classification when 
ArgM labels were included for evaluation. When 
ArgM labels were omitted, kappa statistics 
were .94 and .98 for identification and classifica-
tion, respectively. We also calculated the results 
of combined decisions, i.e., identification and 
classification. (See Table 5.) 
3.3 Annotation Efforts 
Since we employ a WSJ SRL system that labels 
semantic roles automatically, human annotators 
can quickly browse and determine correct tag-
ging results; thus, they do not have to examine  
 
Figure 4. An example of BioProp displayed with 
WordFreak 
 
  P(A) P(E) Kappa 
score 
role identification .97 .52 .94 
role classification .96 .18 .95 including ArgM 
combined decision .96 .18 .95 
role identification .97 .26 .94 
role classification .99 .28 .98 excluding ArgM 
combined decision .99 .28 .98 
Table 5. Inter-annotator agreement 
all tags during the annotation process, as in the 
full manual annotation approach. Only incor-
rectly predicted tags need to be modified, and 
missed tags need to be added. Therefore, annota-
tion efforts can be substantially reduced. To 
quantify the reduction in annotation efforts, we 
define the saving of annotation effort, ?, as: 
)1(
nodes missed of# incorrect  of # correct  of #
nodes  labeled correctly  of #
nodes all of#
nodes  labeled correctly  of #
++
<
=?
 
In Equation (1), since the number of nodes 
that need to be examined is usually unknown, we 
9
use an easy approximation to obtain an upper 
bound for ?. This is based on the extremely op-
timistic assumption that annotators should be 
able to recover a missed or incorrect label by 
only checking one node. However, in reality, this 
would be impossible. In our annotation process, 
the upper bound of ? for BioProp is given by: 
%46
40975
18932
15316668218932
18932
==
++
<? , 
which means that, at most, the annotation effort 
could be reduced by 46%. 
A more accurate tagging system is preferred 
because the more accurate the tagging system, 
the higher the upper bound ? will be.  
4 Disambiguation of Argument Annota-
tion 
During the annotation process, we encountered a 
number of problems resulting from different us-
age of vocabulary and writing styles in general 
English and the biomedical domain. In this sec-
tion, we describe three major problems and pro-
pose our solutions. 
4.1 Cue Words for Role Classification 
PropBank annotation guidelines provide a list of 
words that can help annotators decide an argu-
ment?s type. Similarly, we add some rules to our 
BioProp annotation guideline. For example, ?in 
vivo? and ?in vitro? are used frequently in bio-
medical literature; however, they seldom appear 
in general English articles. According to their 
meanings, we classify them as location argument 
(AM-LOC).  
In addition, some words occur frequently in 
both general English and in biomedical domains 
but have different meanings/usages. For instance, 
?development? is often tagged as Arg0 or Arg1 
in general English, as shown by the following 
sentence: 
 
Despite the strong case for stocks, however, most 
pros warn that [individualsArg0] shouldn't try to 
[profitpredicate] [from short-term developmentsArg1].  
 
However, in the biomedical domain, ?devel-
opment? always means the stage of a disease, 
cell, etc. Therefore, we tag it as temporal argu-
ment (AM-TMP), as shown in the following sen-
tence: 
 
[Rhom-2 mRNAArg1] is [expressedpredicate] [in 
early mouse developmentAM-TMP] [in central 
nervous system, lung, kidney, liver, and spleen 
but only very low levels occur in thymusAM-LOC]. 
4.2 Additional Argument Types 
In PropBank, the negative argument (AM-NEG) 
usually contains explicit negative words such as 
?not?. However, in the biomedical domain, re-
searchers usually express negative meaning im-
plicitly by using ?fail?, ?unable?, ?inability?, 
?neither?, ?nor?, ?failure?, etc. Take ?fail? as an 
example. It is tagged as a verb in general English, 
as shown in the following sentence: 
 
But [the new pactArg1] will force huge debt on the 
new firm and [couldAM-MOD] [stillAM-TMP] [failpredi-
cate] [to thwart rival suitor McCaw CellularArg2]. 
 
Negative results are important in the biomedi-
cal domain. Thus, for annotation purposes, we 
create additional negation tag (AM-NEG1) that 
does not exist in PropBank. The following sen-
tence is an example showing the use of AM-
NEG1: 
  
[TheyArg0] [failAM-NEG1] to [inducepredicate] [mRNA 
of TNF-alphaArg1] [after 3 h of culture AM-TMP]. 
  
In this example, if we do not introduce the 
AM-NEG1, ?fail? is considered as a verb like in 
PropBank, not as a negative argument, and it will 
not be included in the proposition for the predi-
cate ?induce?. Thus, BioProp requires the ?AM-
NEG1? tag to precisely express the correspond-
ing proposition. 
4.3 Essentiality of Biomedical Knowledge 
Since PAS?s contain more semantic information, 
proposition bank annotators require more domain 
knowledge than annotators of other corpora. In 
BioProp, many ambiguous expressions require 
biomedical knowledge to correctly annotate them, 
as exemplified by the following sentence in Bio-
Prop: 
 
In the cell types tested, the LS mutations indi-
cated an apparent requirement not only for the 
intact NF-kappa B and SP1-binding sites but also 
for [several regions between -201 and -130Arg1] 
[notAM-NEG] [previouslyAM-MNR] [associatedpredi-
cate][with viral infectivityArg2]. 
 
Annotators without biomedical knowledge 
may consider [between -201 and -130] as extent 
argument (AM-EXT), because the PropBank 
guidelines define numerical adjuncts as AM-
10
EXT. However, it means a segment of DNA. It is 
an appositive of [several regions]; therefore, it 
should be annotated as part of Arg1 in this case. 
5 Effect of Training Corpora on SRL 
Systems 
To examine the possibility that BioProp can im-
prove the training of SRL systems used for 
automatic tagging of biomedical texts, we com-
pare the performance of systems trained on Bio-
Prop and PropBank in different domains. We 
construct a new SRL system (called a BIOmedi-
cal SeMantIc roLe labEler, BIOSMILE) that is 
trained on BioProp and employs all the features 
used in our WSJ SRL system (Tsai et al, 2006).  
As with POS tagging, chunking, and named 
entity recognition, SRL can also be formulated as 
a sentence tagging problem. A sentence can be 
represented by a sequence of words, a sequence 
of phrases, or a parsing tree; the basic units of a 
sentence in these representations are words, 
phrases, and constituents, respectively. Hacioglu 
et al (2004) showed that tagging phrase-by-
phrase (P-by-P) is better than word-by-word (W-
by-W). However, Punyakanok et al (2004) 
showed that constituent-by-constituent (C-by-C) 
tagging is better than P-by-P. Therefore, we use 
C-by-C tagging for SRL in our BIOSMILE. 
SRL can be divided into two steps. First, we 
identify all the predicates. This can be easily ac-
complished by finding all instances of verbs of 
interest and checking their part-of-speech (POS) 
tags. Second, we label all arguments correspond-
ing to each predicate. This is a difficult problem, 
since the number of arguments and their posi-
tions vary according to a verb?s voice (ac-
tive/passive) and sense, along with many other 
factors.  
In BIOSMILE, we employ the maximum en-
tropy (ME) model for argument classification. 
We use Zhang?s MaxEnt toolkit 
(http://www.nlplab.cn/zhangle/maxent_toolkit.ht
ml) and the L-BFGS (Nocedal and Wright, 1999) 
method of parameter estimation for our ME 
model. Table 6 shows the features we employ in 
BIOSMILE and our WSJ SRL system. 
To compare the effects of using biomedical 
training data versus using general English data, 
we train BIOSMILE on 30 randomly selected 
training sets from BioProp (g1,.., g30), and WSJ 
SRL system on 30 from PropBank (w1,.., w30), 
each of which has 1,200 training PAS?s. 
BASIC FEATURES 
z Predicate ? The predicate lemma 
z Path ? The syntactic path through the parsing tree 
from the parse constituent being classified to the 
predicate 
z Constituent type 
z Position ? Whether the phrase is located before or af-
ter the predicate 
z Voice ? passive: If the predicate has a POS tag VBN, 
and its chunk is not a VP, or it is preceded by a form 
of ?to be? or ?to get? within its chunk; otherwise, it is 
active 
z Head word ? Calculated using the head word table 
described by Collins (1999) 
z Head POS ? The POS of the Head Word 
z Sub-categorization ? The phrase structure rule that 
expands the predicate?s parent node in the parsing 
tree 
z First and last Word and their POS tags 
z Level ? The level in the parsing tree 
PREDICATE FEATURES 
z Predicate?s verb class 
z Predicate POS tag 
z Predicate frequency 
z Predicate?s context POS 
z Number of predicates 
FULL PARSING FEATURES 
z Parent?s, left sibling?s, and right sibling?s paths, 
constituent types, positions, head words and head 
POS tags 
z Head of PP parent ? If the parent is a PP, then the 
head of this PP is also used as a feature 
COMBINATION FEATURES 
z Predicate distance combination 
z Predicate phrase type combination 
z Head word and predicate combination 
z Voice position combination 
OTHERS 
z Syntactic frame of predicate/NP 
z Headword suffixes of lengths 2, 3, and 4 
z Number of words in the phrase 
z Context words & POS tags 
Table 6. The features used in our argument clas-
sification model 
 We then test both systems on 30 400-PAS test 
sets from BioProp, with g1 and w1 being tested on 
test set 1, g2 and w2 on set 2, and so on. Then we 
generate the scores for g1-g30 and w1-w30, and 
compare their averages. 
Table 7 shows the experimental results. When 
tested on the biomedical corpus, BIOSMILE out-
performs the WSJ SRL system by 22.9%. This 
result is statistically significant as expected. 
 
Training Test Precision Recall F-score 
PropBank BioProp 74.78 56.25 64.20 
BioProp BioProp 88.65 85.61 87.10 
Table 7. Performance comparison of SRL sys-
tems trained on BioProp and PropBank 
11
6 Conclusion & Future Work 
The primary contribution of this study is the an-
notation of a biomedical proposition bank that 
incorporates the following features. First, the 
choice of 30 representative biomedical verbs is 
made according to their frequency and impor-
tance in the biomedical domain. Second, since 
some of the verbs have different usages and oth-
ers do not appear in the WSJ proposition bank, 
we redefine their framesets and add some new 
argument types. Third, the annotation guidelines 
in PropBank are slightly modified to suit the 
needs of the biomedical domain. Fourth, using 
appropriate argument types, framesets and anno-
tation guidelines, we construct a biomedical 
proposition bank, BioProp, on top of the popular 
biomedical GENIA Treebank. Finally, we em-
ploy a semi-automatic annotation approach that 
uses an SRL system trained on the WSJ Prop-
Bank. Incorrect tagging results are then corrected 
by human annotators. This approach reduces an-
notation efforts significantly. For example, in 
BioProp, the annotation efforts can be reduced 
by, at most, 46%. In addition, trained on BioProp, 
BIOSMILE?s F-score increases by 22.9% com-
pared to the SRL system trained on the PropBank. 
In our future work, we will investigate more 
biomedical verbs. Besides, since there are few 
biomedical treebanks, we plan to integrate full 
parsers in order to annotate syntactic and seman-
tic information simultaneously. It will then be 
possible to apply the SRL techniques more ex-
tensively to biomedical relation extraction. 
References 
Ann Bies, Mark Ferguson, Karen Katz, and Robert 
MacIntyre. 1995. Bracketing Guidelines for Tree-
bank II Style Penn Treebank Project. Technical re-
port, University of Pennsylvania. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL-2005. 
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, 
James H. Martin, and Daniel Jurafsky. 2004. Se-
mantic Role Labeling by Tagging Syntactic 
Chunks. In Proceedings of CoNLL-2004. 
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun'-
ichi Tsujii. 2003. GENIA corpus?a semantically 
annotated corpus for bio-textmining. Bioinformat-
ics, 19(Suppl. 1): i180-i182. 
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 
2000. Class-based construction of a verb lexicon. 
In Proceedings of AAAI-2000. 
Karin Kipper, Martha Palmer, and Owen Rambow. 
2002. Extending PropBank with VerbNet semantic 
predicates. In Proceedings of AMTA-2002. 
Mitchell Marcus, Grace Kim, Mary Ann 
Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark 
Ferguson, Karen Katz, and Britta Schasberger. 
1994. The Penn Treebank: Annotating predicate 
argument structure. In Proceedings of ARPA Hu-
man Language Technology Workshop. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2): 313-330. 
Thomas Morton and Jeremy LaCivita. 2003. Word-
Freak: an open tool for linguistic annotation. In 
Proceedings of HLT/NAACL-2003. 
Jorge Nocedal and Stephen J Wright. 1999. Numeri-
cal Optimization, Springer. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1). 
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav 
Zimak. 2004. Semantic Role Labeling via Integer 
Linear Programming Inference. In Proceedings of 
COLING-2004. 
Sidney Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences. 
New York, McGraw-Hill. 
Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun 
Lin, Cheng-Lung Sung, Wei Ku, Ying-Shan Su, 
Ting-Yi Sung, and Wen-Lian Hsu. 2006. BIOS-
MILE: Adapting Semantic Role Labeling for Bio-
medical Verbs: An Exponential Model Coupled 
with Automatically Generated Template Features. 
In Proceedings of BioNLP'06. 
Yuka Tateisi, Tomoko Ohta, and Jun-ichi Tsujii. 2004. 
Annotation of Predicate-argument Structure of Mo-
lecular Biology Text. In Proceedings of the 
IJCNLP-04 workshop on Beyond Shallow Analyses. 
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-
Speech Annotation of Biology Research Abstracts. 
In Proceedings of the 4th International Conference 
on Language Resource and Evaluation. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun-ichi Tsujii. 2005. Syntax Annotation for the 
GENIA corpus. In Proceedings of IJCNLP-2005. 
Tuangthong Wattarujeekrit, Parantu K Shah, and 
Nigel Collier1. 2004. PASBio: predicate-argument 
structures for event extraction in molecular biology. 
BMC Bioinformatics, 5(155). 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. In Proceed-
ings of the EMNLP-2004. 
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew 
Lim Tan. 2004. Improving Noun Phrase Corefer-
ence Resolution by Matching Strings. In Proceed-
ings of 1st International Joint Conference on Natu-
ral Language Processing: 226-233. 
  
12
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 57?64,
New York City, June 2006. c?2006 Association for Computational Linguistics
BIOSMILE: Adapting Semantic Role Labeling for Biomedical Verbs: 
An Exponential Model Coupled with 
Automatically Generated Template Features 
 
 
Richard Tzong-Han Tsai1,2, Wen-Chi Chou1, Yu-Chun Lin1,2, Cheng-Lung Sung1,  
Wei Ku1,3, Ying-Shan Su1,4, Ting-Yi Sung1 and Wen-Lian Hsu1 
1Institute of Information Science, Academia Sinica 
2Dept. of Computer Science and Information Engineering, National Taiwan University 
3Institute of Molecular Medicine, National Taiwan University  
4Dept. of Biochemical Science and Technology, National Taiwan University 
{thtsai,jacky957,sbb,clsung,wilmaku,qnn,tsung,hsu}@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we construct a biomedical 
semantic role labeling (SRL) system that 
can be used to facilitate relation extraction. 
First, we construct a proposition bank on 
top of the popular biomedical GENIA 
treebank following the PropBank annota-
tion scheme. We only annotate the predi-
cate-argument structures (PAS?s) of thirty 
frequently used biomedical predicates and 
their corresponding arguments. Second, 
we use our proposition bank to train a 
biomedical SRL system, which uses a 
maximum entropy (ME) model. Thirdly, 
we automatically generate argument-type 
templates which can be used to improve 
classification of biomedical argument 
types. Our experimental results show that 
a newswire SRL system that achieves an 
F-score of 86.29% in the newswire do-
main can maintain an F-score of 64.64% 
when ported to the biomedical domain. 
By using our annotated biomedical corpus, 
we can increase that F-score by 22.9%. 
Adding automatically generated template 
features further increases overall F-score 
by 0.47% and adjunct arguments (AM) F-
score by 1.57%, respectively. 
1 Introduction 
The volume of biomedical literature available has 
experienced unprecedented growth in recent years. 
The ability to automatically process this literature 
would be an invaluable tool for both the design and 
interpretation of large-scale experiments. To this 
end, more and more information extraction (IE) 
systems using natural language processing (NLP) 
have been developed for use in the biomedical 
field. A key IE task in the biomedical field is ex-
traction of relations, such as protein-protein and 
gene-gene interactions. 
Currently, most biomedical relation-extraction 
systems fall under one of the following three ap-
proaches: cooccurence-based (Leroy et al, 2005), 
pattern-based (Huang et al, 2004), and machine-
learning-based. All three, however, share the same 
limitation when extracting relations from complex 
natural language. They only extract the relation 
targets (e.g., proteins, genes) and the verbs repre-
senting those relations, overlooking the many ad-
verbial and prepositional phrases and words that 
describe location, manner, timing, condition, and 
extent. The information in such phrases may be 
important for precise definition and clarification of 
complex biological relations. 
The above problem can be tackled by using se-
mantic role labeling (SRL) because it not only rec-
ognizes main roles, such as agents and objects, but 
also extracts adjunct roles such as location, manner, 
57
timing, condition, and extent. The goal of SRL is 
to group sequences of words together and classify 
them with semantic labels. In the newswire domain, 
Morarescu et al (2005) have demonstrated that 
full-parsing and SRL can improve the performance 
of relation extraction, resulting in an F-score in-
crease of 15% (from 67% to 82%). This significant 
result leads us to surmise that SRL may also have 
potential for relation extraction in the biomedical 
domain. Unfortunately, no SRL system for the 
biomedical domain exists. 
In this paper, we aim to build such a biomedical 
SRL system. To achieve this goal we roughly im-
plement the following three steps as proposed by 
Wattarujeekrit et al, (2004): (1) create semantic 
roles for each biomedical verb; (2) construct a 
biomedical corpus annotated with verbs and their 
corresponding semantic roles (following defini-
tions created in (1) as a reference resource;) (3) 
build an automatic semantic interpretation model 
using the annotated text as a training corpus for 
machine learning. In the first step, we adopt the 
definitions found in PropBank (Palmer et al, 2005), 
defining our own framesets for verbs not in Prop-
Bank, such as ?phosphorylate?. In the second step, 
we first use an SRL system (Tsai et al, 2005) 
trained on the Wall Street Journal (WSJ) to auto-
matically tag our corpus. We then have the results 
double-checked by human annotators. Finally, we 
add automatically-generated template features to 
our SRL system to identify adjunct (modifier) ar-
guments, especially those highly relevant to the 
biomedical domain. 
2 Biomedical Proposition Bank  
As proposition banks are semantically annotated 
versions of a Penn-style treebank, they provide 
consistent semantic role labels across different syn-
tactic realizations of the same verb (Palmer et al, 
2005). The annotation captures predicate-argument 
structures based on the sense tags of polysemous 
verbs (called framesets) and semantic role labels 
for each argument of the verb. Figure 1 shows the 
annotation of semantic roles, exemplified by the 
following sentence: ?IL4 and IL13 receptors acti-
vate STAT6, STAT3 and STAT5 proteins in the 
human B cells.? The chosen predicate is the word 
?activate?; its arguments and their associated word 
groups are illustrated in the figure. 
 
 
Figure 1. A Treebank Annotated with Semantic 
Role Labels 
Since proposition banks are annotated on top of 
a Penn-style treebank, we selected a biomedical 
corpus that has a Penn-style treebank as our corpus. 
We chose the GENIA corpus (Kim et al, 2003), a 
collection of MEDLINE abstracts selected from 
the search results with the following keywords: 
human, blood cells, and transcription factors. In the 
GENIA corpus, the abstracts are encoded in XML 
format, where each abstract also contains a 
MEDLINE UID, and the title and content of the 
abstract. The text of the title and content is seg-
mented into sentences, in which biological terms 
are annotated with their semantic classes. The 
GENIA corpus is also annotated with part-of-
speech (POS) tags (Tateisi et al, 2004), and co-
references (Yang et al, 2004). 
The Penn-style treebank for GENIA, created by 
Tateisi et al (2005), currently contains 500 ab-
stracts. The annotation scheme of the GENIA 
Treebank (GTB), which basically follows the Penn 
Treebank II (PTB) scheme (Bies et al, 1995), is 
encoded in XML. However, in contrast to the WSJ 
corpus, GENIA lacks a proposition bank. We 
therefore use its 500 abstracts with GTB as our 
corpus. To develop our biomedical proposition 
bank, BioProp, we add the proposition bank anno-
tation on top of the GTB annotation. 
2.1 Important Argument Types 
In the biomedical domain, relations are often de-
pendent upon locative and temporal factors 
(Kholodenko, 2006). Therefore, locative (AM-
LOC) and temporal modifiers (AM-TMP) are par-
ticularly important as they tell us where and when 
biomedical events take place. Additionally, nega-
58
tive modifiers (AM-NEG) are also vital to cor-
rectly extracting relations. Without AM-NEG, we 
may interpret a negative relation as a positive one 
or vice versa. In total, we use thirteen modifiers in 
our biomedical proposition bank. 
2.2 Verb Selection 
We select 30 frequently used verbs from the mo-
lecular biology domain given in Table 1. 
express trigger encode 
associate repress enhance 
interact signal increase 
suppress activate induce 
prevent alter Inhibit 
modulate affect Mediate 
phosphorylate bind Mutated 
transactivate block Reduce 
transform decrease Regulate 
differentiated promote Stimulate 
Table 1. 30 Frequently Biomedical Verbs 
Let us examine a representative verb, ?activate?. 
Its most frequent usage in molecular biology is the 
same as that in newswire. Generally speaking, ?ac-
tivate? means, ?to start a process? or ?to turn on.? 
Many instances of this verb express the action of 
waking genes, proteins, or cells up. The following 
sentence shows a typical usage of the verb ?acti-
vate.?  
[NF-kappaB
 Arg1
] is [not
 AM-NEG
] [activated
predicate
] [upon tetra-
cycline removal
AM-TMP
] [in the NIH3T3 cell line
AM-LOC
]. 
3 Semantic Role Labeling on BioProp 
In this section, we introduce our BIOmedical Se-
MantIc roLe labEler, BIOSMILE. Like POS tag-
ging, chunking, and named entity recognition, SRL 
can be formulated as a sentence tagging problem. 
A sentence can be represented by a sequence of 
words, a sequence of phrases, or a parsing tree; the 
basic units of a sentence are words, phrases, and 
constituents arranged in the above representations, 
respectively. Hacioglu et al (2004) showed that 
tagging phrase by phrase (P-by-P) is better than 
word by word (W-by-W). Punyakanok et al, (2004) 
further showed that constituent-by-constituent (C-
by-C) tagging is better than P-by-P. Therefore, we 
choose C-by-C tagging for SRL. The gold standard 
SRL corpus, PropBank, was designed as an addi-
tional layer of annotation on top of the syntactic 
structures of the Penn Treebank. 
SRL can be broken into two steps. First, we 
must identify all the predicates. This can be easily 
accomplished by finding all instances of verbs of 
interest and checking their POS?s. 
Second, for each predicate, we need to label all 
arguments corresponding to the predicate. It is a 
complicated problem since the number of argu-
ments and their positions vary depending on a 
verb?s voice (active/passive) and sense, along with 
many other factors.  
In this section, we first describe the maximum 
entropy model used for argument classification. 
Then, we illustrate basic features as well as spe-
cialized features such as biomedical named entities 
and argument templates.  
3.1 Maximum Entropy Model 
The maximum entropy model (ME) is a flexible 
statistical model that assigns an outcome for each 
instance based on the instance?s history, which is 
all the conditioning data that enables one to assign 
probabilities to the space of all outcomes. In SRL, 
a history can be viewed as all the information re-
lated to the current token that is derivable from the 
training corpus. ME computes the probability, 
p(o|h), for any o from the space of all possible out-
comes, O, and for every h from the space of all 
possible histories, H. 
The computation of p(o|h) in ME depends on a 
set of binary features, which are helpful in making 
predictions about the outcome. For instance, the 
node in question ends in ?cell?, it is likely to be 
AM-LOC. Formally, we can represent this feature 
as follows: 
??
??
?
=
=
=
otherwise :0
LOC-AMo and    
 true)(s_in_cellde_endcurrent_no if :1
),(
h
ohf
Here, current_node_ends_in_cell(h) is a binary 
function that returns a true value if the current 
node in the history, h, ends in ?cell?. Given a set of 
features and a training corpus, the ME estimation 
process produces a model in which every feature f i 
has a weight ?i. Following Bies et al (1995), we 
can compute the conditional probability as: 
?=
i
ohf
i
i
hZ
hop ),(
)(
1
)|( ?  
??=
o i
ohf
i
ihZ ),()( ?  
59
The probability is calculated by multiplying the 
weights of the active features (i.e., those of f i (h,o) 
= 1).  ?i is estimated by a procedure called Gener-
alized Iterative Scaling (GIS) (Darroch et al, 
1972). The ME estimation technique guarantees 
that, for every feature, f i, the expected value of ?i 
equals the empirical expectation of ?i in the train-
ing corpus. We use Zhang?s MaxEnt toolkit and 
the L-BFGS (Nocedal et al, 1999) method of pa-
rameter estimation for our ME model. 
BASIC FEATURES 
z Predicate ? The predicate lemma 
z Path ? The syntactic path through the parsing tree from 
the parse constituent be-ing classified to the predicate 
z Constituent type 
z Position ? Whether the phrase is located before or after 
the predicate 
z Voice ? passive: if the predicate has a POS tag VBN, 
and its chunk is not a VP, or it is preceded by a form of 
?to be? or ?to get? within its chunk; otherwise, it is ac-
tive 
z Head word ? calculated using the head word table de-
scribed by (Collins, 1999) 
z Head POS ? The POS of the Head Word 
z Sub-categorization ? The phrase structure rule that ex-
pands the predicate?s parent node in the parsing tree 
z First and last Word and their POS tags 
z Level ? The level in the parsing tree 
PREDICATE FEATURES 
z Predicate?s verb class 
z Predicate POS tag 
z Predicate frequency 
z Predicate?s context POS 
z Number of predicates 
FULL PARSING FEATURES 
z Parent?s, left sibling?s, and right sibling?s paths, con-
stituent types, positions, head words and head POS 
tags 
z Head of PP parent ? If the parent is a PP, then the head 
of this PP is also used as a feature 
COMBINATION FEATURES 
z Predicate distance combination 
z Predicate phrase type combination 
z Head word and predicate combination 
z Voice position combination 
OTHERS 
z Syntactic frame of predicate/NP 
z Headword suffixes of lengths 2, 3, and 4 
z Number of words in the phrase 
z Context words & POS tags 
Table 2. The Features Used in the Baseline Argu-
ment Classification Model 
3.2 Basic Features 
Table 2 shows the features that are used in our 
baseline argument classification model. Their ef-
fectiveness has been previously shown by (Pradhan 
et al, 2004; Surdeanu et al, 2003; Xue et al, 
2004). Detailed descriptions of these features can 
be found in (Tsai et al, 2005). 
3.3 Named Entity Features 
In the newswire domain, Surdeanu et al (2003) 
used named entity (NE) features that indicate 
whether a constituent contains NEs, such as per-
sonal names, organization names, location names, 
time expressions, and quantities of money. Using 
these NE features, they increased their system?s F-
score by 2.12%. However, because NEs in the 
biomedical domain are quite different from news-
wire NEs, we create bio-specific NE features using 
the five primary NE categories found in the 
GENIA ontology1: protein, nucleotide, other or-
ganic compounds, source and others. Table 3 illus-
trates the definitions of these five categories. When 
a constituent exactly matches an NE, the corre-
sponding NE feature is enabled.  
 NE Definition 
Protein Proteins include protein groups, families, molecules, complexes, and substructures.  
Nucleotide A nucleic acid molecule or the compounds that consist of nucleic acids. 
Other organic 
compounds 
Organic compounds exclude protein and 
nucleotide. 
Source 
Sources are biological locations where 
substances are found and their reactions 
take place.  
Others 
The terms that are not categorized as 
sources or substances may be marked up, 
with 
Table 3. Five GENIA Ontology NE Categories 
3.4 Biomedical Template Features 
Although a few NEs tend to belong almost exclu-
sively to certain argument types (such as ??cell? 
being mainly AM-LOC), this information alone is 
not sufficient for argument-type classification. For 
one, most NEs appear in a variety of argument 
types. For another, many appear in more than one 
constituent (node in a parsing tree) in the same 
sentence. Take the sentence ?IL4 and IL13 recep-
tors activate STAT6, STAT3 and STAT5 proteins 
in the human B cells,? for example. The NE ?the 
human B cells? is found in two constituents (?the 
                                                          
1 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
genia-ontology.html  
60
human B cells? and ?in the human B cells?) as 
shown in figure 1. Yet only ?in the human B cells? 
is an AM-LOC because here ?human B cells? is 
preceded by the preposition ?in? and the deter-
miner ?the?. Another way to express this would be 
as a template?<prep> the <cell>.? We believe 
such templates composed of NEs, real words, and 
POS tags may be helpful in identifying constitu-
ents? argument types. In this section, we first de-
scribe our template generation algorithm, and then 
explain how we use the generated templates to im-
prove SRL performance. 
Template Generation (TG) 
Our template generation (TG) algorithm extracts 
general patterns for all argument types using the 
local alignment algorithm. We begin by pairing all 
arguments belonging to the same type according to 
their similarity. Closely matching pairs are then 
aligned word by word and a template that fits both 
is created. Each slot in the template is given con-
straint information in the form of either a word, NE 
type, or POS. The hierarchy of this constraint in-
formation is word > NE type > POS. If the argu-
ments share nothing in common for a given slot, 
the TG algorithm will put a wildcard in that posi-
tion. Figure 2 shows an aligned pair arguments. 
For this pair, the TG algorithm generated the tem-
plate ?AP-1 CC PTN? (PTN: protein name) be-
cause in the first position, both arguments have 
?AP-1;? in the second position, they have the same 
POS ?CC;? and in the third position, they share a 
common NE type, ?PTN.? The complete TG algo-
rithm is described in Algorithm 1. 
AP-1/PTN/NN and/O/CC NF-AT/PTN/NN 
AP-1/PTN/NN or/O/CC NFIL-2A/PTN/NN 
Figure 2. Aligned Argument Pair 
Applying Generated Templates 
The generated templates may match exactly or par-
tially with constituents. According to our observa-
tions, the former is more useful for argument 
classification. For example, constituents that per-
fectly match the template ?IN a * <cell>? are 
overwhelmingly AM-LOCs. Therefore, we only 
accept exact template matches. That is, if a con-
stituent exactly matches a template t, then the fea-
ture corresponding to t will be enabled. 
Algorithm 1 Template Generation 
Input: Sentences set S = {s1, . . . , sn}, 
Output: A set of template T = {t1, . . . , tk}. 
 
1: T = {}; 
2: for each sentence si from s1 to sn-1 do 
3:    for each sentence sj from si to sn do 
4:        perform alignment on si and sj, then 
5:          pair arguments according to similarity; 
6:        generate common template t from argument pairs; 
7:        T?T?t; 
8:    end; 
9: end; 
10: return T; 
4 Experiments 
4.1 Datasets 
In this paper, we extracted all our datasets from 
two corpora, the Wall Street Journal (WSJ) corpus 
and the BioProp, which respectively represent the 
newswire and biomedical domains. The Wall 
Street Journal corpus has 39,892 sentences, and 
950,028 words. It contains full-parsing information, 
first annotated by Marcus et al (1997), and is the 
most famous treebank (WSJ treebank). In addition 
to these syntactic structures, it was also annotated 
with predicate-argument structures (WSJ proposi-
tion bank) by Palmer et al (2005).  
In biomedical domain, there is one available 
treebank for GENIA, created by Yuka Tateshi et al 
(2005), who has so far added full-parsing informa-
tion to 500 abstracts. In contrast to WSJ, however, 
GENIA lacks any proposition bank. 
Since predicate-argument annotation is essential 
for training and evaluating statistical SRL systems, 
to make up for GENIA?s lack of a proposition 
bank, we constructed BioProp. Two biologists with 
masters degrees in our laboratory undertook the 
annotation task after receiving computational lin-
guistic training for approximately three months.  
We adopted a semi-automatic strategy to anno-
tate BioProp. First, we used the PropBank to train 
a statistical SRL system which achieves an F-score 
of over 86% on section 24 of the PropBank. Next, 
we used this SRL system to annotate the GENIA 
treebank automatically. Table 4 shows the amounts 
of all adjunct argument types (AMs) in BioProp. 
The detail description of can be found in (Babko-
Malaya, 2005).  
 
61
Type Description # Type Description # 
NEG negation 
marker 
103 ADV general  
purpose 
307
LOC location 389 PNC purpose 3
TMP time 145 CAU cause 15
MNR manner 489 DIR direction 22
EXT extent 23 DIS discourse 
connectives 
179
   MOD modal verb 121
Table 4. Subtypes of the AM Modifier Tag 
4.2 Experiment Design 
Experiment 1: Portability 
Ideally, an SRL system should be adaptable to the 
task of information extraction in various domains 
with minimal effort. That is, we should be able to 
port it from one domain to another. In this experi-
ment, we evaluate the cross-domain portability of 
our SRL system. We use Sections 2 to 21 of the 
PropBank to train our SRL system. Then, we use 
our system to annotate Section 24 of the PropBank 
(denoted by Exp 1a) and all of BioProp (denoted 
by Exp 1b). 
Experiment 2: The Necessity of BioProp 
To compare the effects of using biomedical train-
ing data vs. using newswire data, we train our SRL 
system on 30 randomly selected training sets from 
BioProp (g1,.., g30) and 30 from PropBank (w1,.., 
w30), each having 1200 training PAS?s. We then 
test our system on 30 400-PAS test sets from Bio-
Prop, with g1 and w1 being tested on test set 1, g2 
and w2 on set 2, and so on. Then we add up the 
scores for w1-w30 and g1-g30, and compare their 
averages. 
Experiment 3: The Effect of Using Biomedical-
Specific Features 
In order to improve SRL performance, we add do-
main specific features. In Experiment 3, we inves-
tigate the effects of adding biomedical NE features 
and argument template features composed of 
words, NEs, and POSs. The dataset selection pro-
cedure is the same as in Experiment 2. 
5 Results and Discussion 
All experimental results are summarized in Table 5. 
For argument classification, we report the preci-
sion (P), recall (R) and F-scores (F). The details 
are illustrated in the following paragraphs. 
Configuration Training Test P R F 
Exp 1a PropBank PropBank 90.47 82.48 86.29
Exp 1b PropBank BioProp 75.28 56.64 64.64
Exp 2a PropBank BioProp 74.78 56.25 64.20
Exp 2b BioProp BioProp 88.65 85.61 87.10
Exp 3a BioProp BioProp 88.67 85.59 87.11
Exp 3b BioProp BioProp 89.13 86.07 87.57
Table 5. Summary of All Experiments 
Exp 1a Exp 1b Role 
P R F P R F 
+/-(%)
Overall 90.47 82.48 86.29 75.28 56.64 64.64 -21.65
ArgX 91.46 86.39 88.85 78.92 67.82 72.95 -15.90
Arg0 86.36 78.01 81.97 85.56 64.41 73.49   -8.48
Arg1 95.52 92.11 93.78 82.56 75.75 79.01 -14.77
Arg2 87.19 84.53 85.84 32.76 31.59 32.16 -53.68
AM 86.76 70.02 77.50 62.70 32.98 43.22 -34.28
-ADV 73.44 52.32 61.11 39.27 26.34 31.53 -29.58
-DIS 81.71 48.18 60.62 67.12 48.18 56.09 -4.53
-LOC 89.19 57.02 69.57 68.54 2.67 5.14 -64.43
-MNR 67.93 57.86 62.49 46.55 22.97 30.76 -31.73
-MOD 99.42 92.5 95.84 99.05 88.01 93.2 -2.64
-NEG 100 91.21 95.40 99.61 80.13 88.81 -6.59
-TMP 88.15 72.83 79.76 70.97 60.36 65.24 -14.52
Table 6. Performance of Exp 1a and Exp 1b 
Experiment 1 
Table 6 shows the results of Experiment 1. The 
SRL system trained on the WSJ corpus obtains an 
F-score of 64.64% when used in the biomedical 
domain. Compared to traditional rule-based or 
template-based approaches, our approach suffers 
acceptable decrease in overall performance when 
recognizing ArgX arguments. However, Table 6 
also shows significant decreases in F-scores from 
other argument types. AM-LOC drops 64.43% and 
AM-MNR falls 31.73%. This may be due to the 
fact that the head words in PropBank are quite dif-
ferent from those in BioProp. Therefore, to achieve 
better performance, we believe it will be necessary 
to annotate biomedical corpora for training bio-
medical SRL systems. 
Experiment 2 
Table 7 shows the results of Experiment 2. When 
tested on BioProp, BIOSMILE (Exp 2b) outper-
forms the newswire SRL system (Exp 2a) by 
22.9% since the two systems are trained on differ-
ent domains. This result is statistically significant. 
Furthermore, Table 7 shows that BIOSMILE 
outperforms the newswire SRL system in most 
62
argument types, especially Arg0, Arg2, AM-ADV, 
AM-LOC, AM-MNR.  
Exp 2a Exp 2b Role 
P R F P R F 
+/-(%)
Overall 74.78 56.25 64.20 88.65 85.61 87.10 22.90
ArgX 78.40 67.32 72.44 91.96 89.73 90.83 18.39
Arg0 85.55 64.40 73.48 92.24 90.59 91.41 17.93
Arg1 81.41 75.11 78.13 92.54 90.49 91.50 13.37
Arg2 34.42 31.56 32.93 86.89 81.35 84.03 51.10
AM 61.96 32.38 42.53 81.27 76.72 78.93 36.40
-ADV 36.00 23.26 28.26 64.02 52.12 57.46 29.20
-DIS 69.55 51.29 59.04 82.71 75.60 79.00 19.96
-LOC 75.51 3.23 6.20 80.05 85.00 82.45 76.25
-MNR 44.67 21.66 29.17 83.44 82.23 82.83 53.66
-MOD 99.38 88.89 93.84 98.00 95.28 96.62 2.78
-NEG 99.80 79.55 88.53 97.82 94.81 96.29 7.76
-TMP 67.95 60.40 63.95 80.96 61.82 70.11 6.16
Table 7. Performance of Exp 2a and Exp 2b 
The performance of Arg0 and Arg2 in our sys-
tem increases considerably because biomedical 
verbs can be successfully identified by BIOSMILE 
but not by the newswire SRL system. For AM-
LOC, the newswire SRL system scored as low as 
76.25% lower than BIOSMILE. This is likely due 
to the reason that in the biomedical domain, many 
biomedical nouns, e.g., organisms and cells, func-
tion as locations, while in the newswire domain, 
they do not. In newswire, the word ?cell? seldom 
appears. However, in biomedical texts, cells repre-
sent the location of many biological reactions, and, 
therefore, if a constituent node on a parsing tree 
contains ?cell?, this node is very likely an AM-
LOC. If we use only newswire texts, the SRL sys-
tem will not learn to recognize this pattern. In the 
biomedical domain, arguments of manner (AM-
MNR) usually describe how to conduct an experi-
ment or how an interaction arises or occurs, while 
in newswire they are extremely broad in scope. 
Without adequate biomedical domain training cor-
pora, systems will easily confuse adverbs of man-
ner (AM-MNR), which are differentiated from 
general adverbials in semantic role labeling, with 
general adverbials (AM-ADV). In addition, the 
performance of the referential arguments of Arg0, 
Arg1, and Arg2 increases significantly. 
Experiment 3 
Table 8 shows the results of Experiment 3. The 
performance does not significantly improve after 
adding NE features. We originally expected that 
NE features would improve recognition of AM 
arguments such as AM-LOC. However, they failed 
to ameliorate the results since in the biomedical 
domain most NEs are just matched parts of a con-
stituent. This results in fewer exact matches. Fur-
thermore, in matched cases, NE information alone 
is insufficient to distinguish argument types. For 
example, even if a constituent exactly matches a 
protein name, we still cannot be sure whether it 
belongs to the subject (Arg0) or object (Arg1). 
Therefore, NE features were not as effective as we 
had expected. 
NE (Exp 3a) Template (Exp 3b) Role 
P R F P R F 
+/-(%)
Overall 88.67 85.59 87.11 89.13 86.07 87.57 0.46
ArgX 91.99 89.70 90.83 91.89 89.73 90.80 -0.03
Arg0 92.41 90.57 91.48 92.19 90.59 91.38 -0.1
Arg1 92.47 90.45 91.45 92.42 90.44 91.42 -0.03
Arg2 86.93 81.3 84.02 87.08 81.66 84.28 0.26
AM 81.30 76.75 78.96 82.96 78.18 80.50 1.54
-ADV 64.11 52.23 57.56 65.66 55.60 60.21 2.65
-DIS 82.51 75.42 78.81 83.00 75.79 79.23 0.42
-LOC 80.07 85.09 82.50 84.24 85.48 84.86 2.36
-MNR 83.50 82.19 82.84 84.56 84.14 84.35 1.51
-MOD 98.14 95.28 96.69 98.00 95.28 96.62 -0.07
-NEG 97.66 94.81 96.21 97.82 94.81 96.29 0.08
-TMP 81.14 62.06 70.33 83.10 63.95 72.28 1.95
Table 8. Performance of Exp 3a and Exp 3b 
6 Conclusions and Future Work 
In Experiment 3b, we used the argument templates 
as features. Since ArgX?s F-score is close to 90%, 
adding the template features does not improve its 
score. However, AM?s F-score increases by 1.54%. 
For AM-ADV, AM-LOC, and AM-TMP, the in-
crease is greater because the automatically gener-
ated templates effectively extract these AMs.  
In Figure 3, we compare the performance of ar-
gument classification models with and without ar-
gument template features. The overall F-score 
improves only slightly. However, the F-scores of 
main adjunct arguments increase significantly. 
The contribution of this paper is threefold. First, 
we construct a biomedical proposition bank, Bio-
Prop, on top of the popular biomedical GENIA 
treebank following the PropBank annotation 
scheme. We employ semi-automatic annotation 
using an SRL system trained on PropBank, thereby 
significantly reducing annotation effort. Second, 
we create BIOSMILE, a biomedical SRL system, 
which uses BioProp as its training corpus. Thirdly, 
we develop a method to automatically generate 
templates that can boost overall performance, es-
63
pecially on location, manner, adverb, and temporal 
arguments. In the future, we will expand BioProp 
to include more verbs and will also integrate an 
automatic parser into BIOSMILE. 
 
Figure 3. Improvement of Template Features 
Overall and on Several Adjunct Types 
Acknowledgement 
We would like to thank Dr. Nianwen Xue for his 
instruction of using the WordFreak annotation tool. 
This research was supported in part by the National 
Science Council under grant NSC94-2752-E-001-
001 and the thematic program of Academia Sinica 
under grant AS94B003. Editing services were pro-
vided by Dorion Berg. 
References  
Babko-Malaya, O. (2005). Propbank Annotation 
Guidelines. 
Bies, A., Ferguson, M., Katz, K., MacIntyre, R., 
Tredinnick, V., Kim, G., et al (1995). Bracketing 
Guidelines for Treebank II Style Penn Treebank 
Project  
Collins, M. J. (1999). Head-driven Statistical Models 
for Natural Language Parsing. Unpublished Ph.D. 
thesis, University of Pennsylvania. 
Darroch, J. N., & Ratcliff, D. (1972). Generalized 
Iterative Scaling for Log-Linear Models. The Annals 
of Mathematical Statistics. 
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & 
Jurafsky, D. (2004). Semantic Role Labeling by 
Tagging Syntactic Chunks. Paper presented at the 
CONLL-04. 
Huang, M., Zhu, X., Hao, Y., Payan, D. G., Qu, K., & 
Li, M. (2004). Discovering patterns to extract 
protein-protein interactions from full texts. 
Bioinformatics, 20(18), 3604-3612. 
Kholodenko, B. N. (2006). Cell-signalling dynamics in 
time and space. Nat Rev Mol Cell Biol, 7(3), 165-176. 
Kim, J. D., Ohta, T., Tateisi, Y., & Tsujii, J. (2003). 
GENIA corpus--semantically annotated corpus for 
bio-textmining. Bioinformatics, 19 Suppl 1, i180-182. 
Leroy, G., Chen, H., & Genescene. (2005). An 
ontology-enhanced integration of linguistic and co-
occurrence based relations in biomedical texts. 
Journal of the American Society for Information 
Science and Technology, 56(5), 457-468. 
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. 
(1997). Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics, 19. 
Morarescu, P., Bejan, C., & Harabagiu, S. (2005). 
Shallow Semantics for Relation Extraction. Paper 
presented at the IJCAI-05. 
Nocedal, J., & Wright, S. J. (1999). Numerical 
Optimization: Springer. 
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The 
proposition bank: an annotated corpus of semantic 
roles. Computational Linguistics, 31(1). 
Pradhan, S., Hacioglu, K., Kruglery, V., Ward, W., 
Martin, J. H., & Jurafsky, D. (2004). Support vector 
learning for semantic argument classification. 
Journal of Machine Learning  
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). 
Semantic Role Labeling via Integer Linear 
Programming Inference. Paper presented at the 
COLING-04. 
Surdeanu, M., Harabagiu, S. M., Williams, J., & 
Aarseth, P. (2003). Using Predicate-Argument 
Structures for Information Extraction. Paper 
presented at the ACL-03. 
Tateisi, Y., & Tsujii, J. (2004). Part-of-Speech 
Annotation of Biology Research Abstracts. Paper 
presented at the LREC-04. 
Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J. (2005). 
Syntax Annotation for the GENIA corpus. 
Tsai, T.-H., Wu, C.-W., Lin, Y.-C., & Hsu, W.-L. 
(2005). Exploiting Full Parsing Information to Label 
Semantic Roles Using an Ensemble of ME and SVM 
via Integer Linear Programming. . Paper presented at 
the CoNLL-05. 
Wattarujeekrit, T., Shah, P. K., & Collier, N. (2004). 
PASBio: predicate-argument structures for event 
extraction in molecular biology. BMC Bioinformatics, 
5, 155. 
Xue, N., & Palmer, M. (2004). Calibrating Features for 
Semantic Role Labeling. Paper presented at the 
EMNLP-04. 
Yang, X., Zhou, G., Su, J., & Tan., C. (2004). 
Improving Noun Phrase Coreference Resolution by 
Matching Strings. Paper presented at the IJCNLP-04. 
64
Learning Patterns from the Web to Translate Named Entities for
Cross Language Information Retrieval
Yu-Chun Wang?? Richard Tzong-Han Tsai?? Wen-Lian Hsu?
?Institute of Information Science, Academia Sinica, Taiwan
?Department of Eletrical Engineering, National Taiwan University, Taiwan
?Department of Computer Science and Engineering, Yuan Ze University, Taiwan
albyu@iis.sinica.edu.tw
thtsai@saturn.yzu.edu.tw
hsu@iis.sinica.edu.tw
?corresponding author
Abstract
Named entity (NE) translation plays
an important role in many applications.
In this paper, we focus on translating
NEs from Korean to Chinese to improve
Korean-Chinese cross-language informa-
tion retrieval (KCIR). The ideographic
nature of Chinese makes NE translation
difficult because one syllable may map to
several Chinese characters. We propose
a hybrid NE translation system. First,
we integrate two online databases to ex-
tend the coverage of our bilingual dic-
tionaries. We use Wikipedia as a trans-
lation tool based on the inter-language
links between the Korean edition and
the Chinese or English editions. We
also use Naver.com?s people search en-
gine to find a query name?s Chinese or
English translation. The second compo-
nent is able to learn Korean-Chinese (K-
C), Korean-English (K-E), and English-
Chinese (E-C) translation patterns from
the web. These patterns can be used to
extract K-C, K-E and E-C pairs from
Google snippets. We found KCIR per-
formance using this hybrid configura-
tion over five times better than that
a dictionary-based configuration using
only Naver people search. Mean average
precision was as high as 0.3385 and recall
reached 0.7578. Our method can han-
dle Chinese, Japanese, Korean, and non-
CJK NE translation and improve perfor-
mance of KCIR substantially.
1 Introduction
Named entity (NE) translation plays an impor-
tant role in machine translation, information re-
trieval, and question answering. It is a chal-
lenging task because, although there are many
online bilingual dictionaries, they usually lack
domain specific words or NEs. Furthermore,
new NEs are generated everyday, but bilingual
dictionaries cannot update their contents fre-
quently. Therefore, it is necessary to construct
a named entity translation (NET) system.
Economic ties between China and Korea have
become closer as China has opened its mar-
kets further, and demand for the latest news
and information from China continues to grow
rapidly in Korea. One key way to meet this
demand is to retrieve information written in
Chinese by using Korean queries, referred to
as Korean-Chinese cross-language information
retrieval (KCIR). The main challenge involves
translating NEs because they are usually the
main concepts of queries. In (Chen et al, 1998),
the authors romanized Chinese NEs and selected
their English transliterations from English NEs
extracted from the Web by comparing their
phonetic similarities with Chinese NEs. Yaser
Al-Onaizan (Al-Onaizan and Knight, 2002)
281
transliterated an NE in Arabic into several can-
didates in English and ranked the candidates by
comparing their counts in several English cor-
pora. Unlike the above works, whose target lan-
guages are alphabetic, in K-C translation, the
target language is Chinese, which uses an ideo-
graphic writing system. Korean-Chinese NET
is much more difficult than NET considered in
previous works because, in Chinese, one sylla-
ble may map to tens or hundreds of characters.
For example, if an NE written in Korean com-
prises three syllables, there may be thousands of
possible translation candidates in Chinese.
In this paper, we propose an effective hybrid
NET method which can help improve perfor-
mance of cross-language information retrieval
systems. We also describe the construction of
a Korean-Chinese CLIR system able to evaluate
the effectiveness of our NE translation method.
2 Difficulties in Korean-Chinese
Named Entity Translation for IR
2.1 Korean NET
Most Korean NEs originate from Hanja. There-
fore, the most straightforward way to translate
a Korean name into Chinese is to use its Hanja
equivalent. Take the name of Korea?s president,
?x4? (No Mu-hyeon), as an example. We
can directly convert it to its Hanja equivalent:
??fI? (Lu Wu-Xuan). Or in the case of the
city name ???? (Pusan/?q/Fu-shan) and
the company name ??1? (Samsung/	/San-
xing), Chinese also presents Hanja equivalents.
If the Hanja name is unknown, the name is
translated character by character. Each Hangul
character is basically translated into a corre-
sponding Hanja character. For example, the
name of the Korean actor ?px1? (Cho In-
seong) is usually translated as ???? (Zhao
Ren-cheng) because ?p? is mapped to ???, ?x?
mapped to ???, and ?1? mapped to ??. How-
ever, that translation may differ from the per-
son?s given Hanja name.
For native Korean NEs which have no cor-
responding Hanja characters, we must turn to
transliteration or convention. Take the name of
South Korea?s capital ??? (Seoul) as an ex-
ample. Before 2005, Chinese media and govern-
ment used the old Hanja name of the city ?"??
(Han-cheng), which was used during Joseon dy-
nasty (A.D. 1392?1910). However, after 2005,
Chinese switched to using the transliteration
??>? (Shou-er) instead of ?"?? at the re-
quest of the Seoul Metropolitan Government.
This example illustrate how more than one Chi-
nese translation for a Korean name is possible,
a phenomenon which, at times, makes Korean-
Chinese information retrieval more difficult.
2.2 Chinese NET
To translate a Chinese NE written in Hangul,
we begin by considering the two C-K NET ap-
proaches. The older is based on the Sino-Korean
pronunciation and the newer on the Mandarin.
For example, ??c? (Taiwan) used to be
transliterated solely as ? ?? (Dae-man). How-
ever, during the 1990s, transliteration based on
Mandarin pronunciation became more popular.
Presently, the most common transliteration for
??c? is ??tD? (Ta-i-wan), though the Sino-
Korean-based ? ?? is still widely used. For
Chinese personal names, both ways are used.
For example, the name of Chinese actor Jackie
Chan (??? Cheng-long) is variously translit-
erated as ?1?? Seong-ryong (Sino-Korean)
and ???? Cheong-rung (Mandarin).
Translating Chinese NEs by either method is
a major challenge because each Hangul charac-
ter may correspond to several different Chinese
characters that have similar pronunciations in
Korean. This results in thousands of possible
combinations of Chinese characters, making it
very difficult to choose the most widely used one
one.
2.3 Japanese NET
Japanese NEs may contain Hiraganas,
Katakanas, or Kanjis. For each character
type, J-C translation rules may be similar to
or very different from K-C translation rules.
Some of these rules are based on Japanese
pronunciation, while some are not. For NEs
composed of all Kanjis, their Chinese transla-
tions are generally exactly the same as their
Kanji written forms. In contrast, Japanese NEs
282
are transliterated into Hangul characters. Take
??K? (Nagoya) for example. Its Chinese
translation ??K? is exactly the same as
its Kanji written form, while its pronuncia-
tion (Ming Gu Wu) is very different from its
Japanese pronunciation. This is different from
its Korean translation, ???|? (Na go ya).
In this example, we can see that, because the
translation rules in Chinese and Korean are
different, it is ineffective to utilize phonetic sim-
ilarity to find the Chinese translation equivalent
to the Korean translation.
2.4 Non-CJK NET
In both Korean and Chinese, transliteration
methods are mostly used to translate non-CJK
NEs. Korean uses the Hangul alphabet for
transliteration. Because of the phonology of
Korean, some phonemes are changed during
translation because the language lacks these
phonemes. (Oh, 2003; Lee, 2003) In contrast,
Chinese transliterates each syllable in a NE into
Chinese characters with similar pronunciation.
Although there are some conventions for select-
ing the transliteration characters, there are still
many possible transliterations since so many
Chinese characters have the same pronunciation.
For instance, the name ?Greenspan? has sev-
eral Chinese transliterations, such as ?[????
(Ge-lin-si-ban) and ?[??X? (Ge-lin-si-pan).
In summary, it is difficult to match a non-CJK
NE transliterated from Korean with its Chinese
transliteration due to the latter?s variations.
3 Our Method
In this section, we describe our Korean-Chinese
NE translation method for dealing with the
problems described in Section 2. We either
translate NE candidates from Korean into Chi-
nese directly, or translate them into English first
and then into Chinese. Our method is a hybrid
of two components: extended bilingual dictio-
naries and web-based NET.
3.1 Named Entity Candidate Selection
The first step is to identify which words in a
query are NEs. In general, Korean queries are
composed of several eojeols, each of which is
composed of a noun followed by the noun?s post-
position, or a verb stem followed by the verb?s
ending. We remove the postposition or the end-
ing to extract the key terms, and then select per-
son name candidates from the key terms. Next,
the maximum matching algorithm is applied to
further segment each term into words in the
Daum Korean-Chinese bilingual dictionary1. If
the length of any token segmented from a term
is 1, the term is regarded as an NE to be trans-
lated.
3.2 Extension of Bilingual Dictionaries
Most NEs are not included in general bilingual
dictionaries. We adopt two online databases
to translate NEs: Wikipedia and Naver people
search.
3.2.1 Wikipedia
In Wikipedia, each article has an inter-
language link to other language editions, which
we exploit to translate NEs. Each NE candidate
is first sent to the Korean Wikipedia, and the
title of the matched article?s Chinese version is
treated as the NE?s translation in Chinese. How-
ever, if the article lacks a Chinese version, we use
the English edition to acquire the NE?s transla-
tion in English. The English translation is then
transliterated into Chinese by the method de-
scribed in Section 3.3.3.
3.2.2 Naver People Search Engine
Most NEs are person names that cannot all
be covered by the encyclopedia. We use Naver
people search engine to extend the coverage of
person names. Naver people search is a transla-
tion tool that maintains a database of famous
people?s basic profiles. If the person is from
CJK, the search engine returns his/her name in
Chinese; otherwise, it returns the name in En-
glish. In the former case, we can adopt the re-
turned name directly, but in the latter, we need
to translate the name into Chinese. The trans-
lation method is described in Section 3.3.3.
1http://cndic.daum.net
283
3.3 Translation Pattern from the Web
Obviously, the above methods cannot cover all
possible translations of NEs. Therefore, we pro-
pose a pattern-based method to find the trans-
lation from the Web. Since the Chinese transla-
tions of some NEs cannot be found by patterns,
we find their Chinese translations indirectly by
first finding their English translations and then
finding the Chinese translations. Therefore,
we must generate K-C patterns to extract K-C
translation pairs, as well as K-E and E-C pat-
terns to extract K-E and E-C pairs, respectively.
3.3.1 Translation Pattern Learning
Our motivation is to learn patterns for ex-
tracting NEs written in the source language and
their equivalents in the target language from
the Web. First, we need to prepare the train-
ing set. To generate K-C and K-E patterns,
we collect thousands of NEs that originated in
Korean, Chinese, Japanese, or non-CJK lan-
guages from Dong-A Ilbo (a South Korean news-
paper). Then, all the Korean NEs are translated
into Chinese manually. NEs from non-CJK lan-
guages are also translated into English. To gen-
erate E-C patterns, we collect English NEs from
the MUC-6 and MUC-7 datasets and translate
them into Chinese manually.
We submit each NE in the source language
(source NE) and its translation in the target lan-
guage as a query to Google search engine. For
instance, the Korean NE ?Tt ??? and its
translation ?Major League? are first composed
as a query ?+Bjs$o?? + Major League?,
which is then sent to Google. The search en-
gine will return the relevant web documents with
their snippets. We collect the snippets in the
top 20 pages and we break them into sentences.
Only the sentences that contain at least one
source NE and its translation are retained.
For each pair of retained sentences, we apply
the Smith-Waterman local alignment algorithm
to find the longest common string, which is then
added to the candidate pattern pool. During the
alignment process, positions where the two in-
put sequences share the same word are counted
as a match. The following is an example of a pair
of sentences that contains ?Tt ??? and its
English translation, ?Major League?:
? ?Tt ??(Major League)??@?
? ]? ??\ ?<????
? ??m Tt ??(Major League)?,?
After alignment, the pattern is generated as:
<Korean NE>(<English Translation>)?
This pattern generation process is repeated for
each NE-translation pair.
3.3.2 Translation Pattern Filtering
After learning the patterns, we have to filter
out some ineffective patterns. First, we send
a Korean NE, such as ?Tt ???, to re-
trieve the snippets in the top 50 pages. Then,
we apply all the patterns to extract the trans-
lations from the snippets. The correct rate of
each translation pattern is calculated as follows:
CorrectRate = Ccorrect/Call, where Ccorrect is
the total number of correct translations ex-
tracted by the pattern and Call is the total num-
ber of translations extracted by the pattern.
If the correct rate of the pattern is below the
threshold ? , the pattern will be dropped.
3.3.3 Pattern-Based NET
The translations of some NEs, especially from
CJK, can be found comparatively easily from
the Web. However, for other NEs, especially
from non-CJK, this is not the case. There-
fore, we split the translation process into two
stages: the first translates the NE into its En-
glish equivalent, and the second translates the
English equivalent into Chinese.
To find an NE?s Chinese translation, we first
apply the translation patterns to extract possi-
ble Chinese translations. If its Chinese transla-
tion cannot be found, the K-E patterns are used
to find its English translation instead. If its En-
glish translation can be found, the E-C patterns
are then used to find its Chinese translation.
4 System Description
We construct a Korean-Chinese cross language
information retrieval (KCIR) system to deter-
mine how our person name translation methods
affect KCIR?s performance. A Korean query is
284
translated into Chinese and then used to retrieve
Chinese documents. The following sections de-
scribe the four stages of our KCIR system. We
use an example query, ?T??X??,??, 
?? (Kosovo?s situation, NATO, UN), to demon-
strate the work flow of our system.
4.1 Query Processing
Unlike English, Korean written texts do not
have word delimiters. Spaces in Korean sen-
tences separate eojeols. First, the postposition
or verb ending in each eojeol is removed. In our
example query, we remove the possessive post-
position ?X? at the end of the first eojeol. Then,
NE candidates are selected using the method de-
scribed in Section 3.1. ?T??? (Kosovo) is
recognized as an NE, and other terms ????
(situation), ???? (NATO), and ? ?? (UN)
are general terms because they can be found in
the bilingual dictionary.
4.1.1 Query Translation
Terms not selected as NE candidates are sent
to the online Daum Korean-Chinese dictionary
and Naver Korean-Chinese dictionary2 to get
their Chinese translations. In our example, the
terms ???? (situation), ???? (NATO), and
? ?? (UN) can be correctly translated into
Chinese by the bilingual dictionaries as ??K?
(situation), ?'lDT? (NATO), and
?o? (UN), respectively.
We employ Wikipedia, Naver people search,
and the pattern-based method simultaneously to
translate the NE candidate ?T??? (Kosovo).
Up to now, there is no article about Kosovo in
Korean Wikipedia. Naver people search does
not contain an article either because it is not a
person name. Meanwhile, since the K-C transla-
tion patterns cannot extract any Chinese trans-
lations, the K-E patterns are used to get the En-
glish translations, such as ?Kosovo?, ?Cosbo?,
and ?Kosobo?. The E-C patterns are then em-
ployed to get the Chinese translation from the
three English translations. Among them, only
Chinese translations for ?Kosovo? can be found
because the other two are either wrong or rarely
2http://cndic.naver.com
used translations. The Chinese translations ex-
tracted by our patterns are ??"+? (Ke-suo-
fu), ??"? (Ke-suo-fu), and ??"?? (Ke-
suo-wuo). They are all correct transliterations.
4.2 Term Disambiguation
A Hangul word might have many meanings. Be-
sides, sometimes the translation patterns might
extract wrong translations of the NE. This phe-
nomenon causes ambiguities during information
retrieval and influence the performance of IR sig-
nificantly. To solve this problem, we adopt the
mutual information score (MI score) to evaluate
the co-relation between a translation candidate
tcij for a term qti and all translation candidates
for all the other terms in Q; tcij ?s MI score given
Q is calculated as follows:
MI score(tcij |Q) =
|Q|
?
x=1,x6=i
Z(qtx)
?
y=1
Pr(tcij , tcxy)
Pr(tcij)Pr(tcxy)
where Z(qtx) is the number of translation can-
didates of the x-th query term qtx; tcxy is y-
th translation candidate for qtx; Pr(tcij , tcxy) is
the probability that tcij and tcxy co-occur in
the same sentence; and Pr(tcij) is the proba-
bility of tcij . Next, we compute the ratio of
the each candidate?s score over the highest can-
didate?s score as follows: ScoreRatio(tcij) =
MI score(tcij |Q)/MI score(tcih|Q), where tcih is
the candidate with highest MI score from the
qti. If the candidate?s score ratio is below the
threshold ?MI, the candidate will be discarded.
Here, we use the above example to illustrate
the term disambiguation mechanism. For the
given English term ?Kosovo?, the MI scores of
??"+?, ??"?, and ??"?? are computed;
??"? achieves the highest score, while the
score ratio of the other two candidates are much
lower than the threshold. Thus, only ??"?
is treated as Kosovo?s translation and used to
build the final Chinese query to perform the IR.
4.3 Indexing and Retrieval Model
We use the Lucene information retrieval engine
to index all documents and the bigram index
based on Chinese characters. The Okapi BM25
function (Robertson et al, 1996) is used to score
285
a retrieved document?s relevance. In addition,
we employ the following document re-ranking
function (Yang et al, 2007):
?
(
?K
i=1 df(t, di)? f(i))/K
DF (t, C)/R
?
?
|t|
df(t, di) =
{
1 t ? di
0 t /? di
,
where di is the ith document; R is the total num-
ber of documents in the collection C; DF (t, C)
is the number of documents containing a term t
in C; and |t| is t?s length, f(i) = 1sqrt(i) .
5 Evaluation and Analysis
To evaluate our KCIR system, we use the topic
and document collections of the NTCIR-5 CLIR
tasks (Kishida et al, 2005). The document
collection is the Chinese Information Retrieval
Benchmark (CIRB) 4.0, which contains news
articles published in four Taiwanese newspa-
pers from 2000 to 2001. The topics have four
fields: title, description, narration, and con-
centrate words. We use 50 topics provided by
NTCIR-5 and use the title field as the input
query because it is similar to queries input to
search engines.
We construct five runs as follows:
? Baseline: using a Korean-Chinese
dictionary-based translation.
? Baseline+Extended Dictionaries only:
the baseline system plus the extended dic-
tionaries translation.
? Baseline+NET Methods: the baseline
system plus our NET methods, namely,
Wikipedia, Naver people search, and the
pattern-based method.
? Google Translation: using the Google
translation tool.
? Chinese monolingual: using the Chinese
versions of the topics given by NTCIR.
We use the Mean Average Precision (MAP)
and Recall (Saracevic et al, 1988) to evaluate
the performance of IR. NTCIR provides two
Table 1: Evaluation Results
Run MAP RecallRigid Relax Rigid Relax
Baseline 0.0553 0.0611 0.2202 0.2141
Baseline+extended
dictionaries
0.1573 0.1751 0.5706 0.5489
Baseline+NET 0.2576 0.2946 0.7255 0.7103
Google translation 0.1340 0.1521 0.5254 0.5149
Chinese mono 0.2622 0.3019 0.7705 0.7452
kinds of relevance judgments: Rigid and Re-
lax. A document is rigid-relevant if it is highly
relevant to the topic; and relax-relevant if it is
highly relevant or partially relevant to the topic.
Table 1 shows that our method improves
KCIR substantially. Our method?s performance
is about five times better than that of the base-
line system and very close to that of Chinese
monolingual IR. Wikipedia translation improves
the performance, but not markedly because
Wikipedia cannot cover some NEs. Google
translation is not very satisfactory either, since
many NEs cannot be translated correctly.
To evaluate our NE translation method, we
create two additional datasets. The first dataset
contains all the 30 topics with NEs in NTCIR-
5. To further investigate the effectiveness of
our method for queries containing person names,
which are the most frequent NEs, we construct
a second dataset containing 16 topics with per-
son names in NTCIR-5. We compare the per-
formance of our method on KCIR with that of
Chinese monolingual IR on these two datasets.
The results are shown in Tables 2 and 3.
5.1 Effectiveness of Extended Dict
We adopt two online dictionaries to extend
our bilingual dictionaries: Wikipedia and Naver
people search engine. Wikipedia is an effective
tool for translating well-known NEs. In the test
topics, NEs like ?@|?(Kim Jong-il, North
Korea?s leader), ????(Taliban), ?t??
0?(Harry Potter) and ?\?|??(Great Na-
tional Party in South Korea) are all translated
correctly by Wikipedia.
We observe that the most difficult cases in
Korean-Chinese person name translation, espe-
cially Japanese and non-CJK person names, can
286
be successfully translated by the Naver people
search engine. For example, ?T??(William
Cohen, the ex-Secretary of Defense of the U.S.)
and ?tX\?(Ichiro Suzuki, a Japanese base-
ball player). The major advantage of the Naver
people search engine is it can can provide the
original names written in Chinese characters.
According to our evaluation, the extended
dictionaries improve the IR performance of the
baseline system about threefold. It shows that
the extended dictionaries can translate part of
Korean NEs into Chinese. However, there are
still many NEs that the extended dictionaries
cannot cover.
5.2 Effectiveness of Patterns
In our method, we employ automatically learned
patterns to extract translations for the remain-
ing NEs not covered by the offline or online dic-
tionaries. For example, we can extract Chinese
translations for ?$??@?(Okinawa, in Japan)
by using K-C translation patterns. Most non-
CJK NEs can be translated correctly by us-
ing the K-E translation patterns. For exam-
ple, ??| t?D??(Jennifer Capriati),
?? ?(anthrax), and ????(mad cow dis-
ease) can be extracted from Google snippets ef-
fectively by our translation patterns.
Although our method translates some NEs
into English first and then into Chinese in an
indirect manner, it is very effective because the
non-CJK NEs in Korean are mainly from En-
glish. In fact, 16 of the 17 NEs can be suc-
cessfully translated by the two stage translation
method that employs two types of translation
patterns: K-E and E-C.
5.3 Effectiveness Analysis of NET
As shown in Table 2, for topics with NEs, the
rigid MAP of our method is very close to that
of Chinese monolingual IR, while the relax MAP
of our method is even better than that of Chi-
nese monolingual IR. We observe that 26 of the
31 NEs in the topics are successfully translated
into Chinese. These results demonstrate that
our hybrid method comprising the extended dic-
tionaries and translation patterns can deal with
Korean-Chinese NE translation effectively and
Table 2: Results on Topics with NEs
Run MAP RecallRigid Relax Rigid Relax
NET 0.2700 0.3385 0.7565 0.7578
Chinese 0.2746 0.3273 0.7922 0.7846
improve the performance of IR substantially.
Note that, our method can extract more pos-
sible Chinese translations, which is similar to
query expansion. For non-CJK NEs, there may
exist several Chinese transliterations that are ac-
tually used in Chinese, especially for the per-
son names. Take ?Tito?for example; its six
common Chinese transliterations, namely, ??
X?(di-tuo), ??X?(di-tuo), ?X?(di-tuo), ??
X?(ti-tuo), and ??X?(di-tuo) can be extracted.
With our method, the rigid MAP of this topic
achieves 0.8361, which is much better than that
of the same topic in the Chinese monolingual run
(0.4459) because the Chinese topic has only one
transliteration ?X?(di-tuo). This is the rea-
son that our method outperforms the Chinese
monolingual run in topics with NEs.
5.4 Error Analysis
NEs that cannot be translated correctly can
be divided into two categories. The first con-
tains names not selected as NE candidates. The
Japanese person name ?????? (Alberto Fu-
jimori, Peru?s ex-president) is in this category.
For the name ?????? (Fujimori), the first
two characters ???? (hind legs) and the last
two characters ???? (profiting) are all Sino-
Korean words, so it is regarded as a compound
word, not an NE. The other category contains
names with few relevant web pages, like the non-
CJK names ?H??$ ?? (Antonio Toddy).
The other problem is that our method can
translate the Korean NEs into correct Chinese
translations, but not the translation used in the
CIRB 4.0 news collection. For example, ??t?
l? (Kursk) is translated into ??>?K? (Ku-
er-si-ke) correctly, but only the transliteration
???K? (Ke-si-ke) is used in CIRB 4.0. In this
situation, the extracted translation cannot im-
prove the performance of the KCIR.
287
Table 3: Results on Topics with Person Names
Run MAP RecallRigid Relax Rigid Relax
NET 0.2730 0.3274 0.7146 0.7299
Chinese 0.2575 0.3169 0.7513 0.7708
6 Conclusion
In this paper, we have considered the difficul-
ties that arise in translating NEs from Korean
to Chinese for IR. We propose a hybrid method
for K-C NET that exploits an extended dictio-
narie containing Wikipedia and the Naver peo-
ple search engine, combined with the translation
patterns automatically learned from the search
results of the Google search engine. To eval-
uate our method, we use the topics and doc-
ument collection of the NTCIR-5 CLIR task.
Our method?s performance on KCIR is over five
times better than that of the baseline configura-
tion with only an offline dictionary-based trans-
lation module. Moreover, its overall MAP score
is up to 0.2986, and its MAP on the NE topics
is up to 0.3385 which is even better than that
of the Chinese monolingual IR system. The pro-
posed method can translate NEs that originated
in the Chinese, Japanese, Korean, and non-
CJK languages and improve the performance of
KCIR substantially. Our NET method is not
language-specific; therefore, it can be applied to
the other CLIR systems beside K-C IR.
References
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating named entities using monolingual and bilin-
gual resources. Proceedings of the 40th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 400?408.
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding,
and Shih-Cbung Tsai. 1998. Proper name transla-
tion in cross-language information retrieval. Pro-
ceedings of 17th COLING and 36th ACL, pages
232?236.
Kazuaki Kishida, Kuang hua Chen, Sukhoon Lee,
Kazuko Kuriyama, Noriko Kando, Hsin-Hsi Chen,
and Sung Hyon Myaeng. 2005. Overview of clir
task at the fifth ntcir workshop. Proceedings of the
Fifth NTCIR Workshop.
Juhee Lee. 2003. Loadword phonology revisted: Im-
plications of richness of the base for the analysis
of loanwords input. Explorations in Korean Lan-
guage and Linguistics, pages 361?375.
Mira Oh. 2003. English fricatives in loanword adap-
tion. Explorations in Korean Language and Lin-
guistics, pages 471?487.
S.E. Robertson, S. Walker, MM Beaulieu, M. Gat-
ford, and A. Payne. 1996. Okapi at trec-4. Pro-
ceedings of the Fourth Text Retrieval Conference,
pages 73?97.
Tefko Saracevic, Paul Kantor, Alice Y. Chamis, and
Donna Trivison. 1988. A study of information
seeking and retrieving. Journal of the American
Society for Information Science, 39(3):161?176.
L. Yang, D. Ji, and M. Leong. 2007. Docu-
ment reranking by term distribution and maxi-
mal marginal relevance for chinese information re-
trieval. Information Processing and Management:
an International Journal, 43(2):315?326.
288
Exploiting Unlabeled Text to Extract New Words of Different
Semantic Transparency for Chinese Word Segmentation
Richard Tzong-Han Tsai?? and Hsi-Chuan Hung?
?Department of Computer Science and Engineering,
Yuan Ze University, Chung-Li, Taoyuan, Taiwan
?Department of Computer Science and Information Engineering,
National Taiwan University, Taipei, Taiwan
thtsai@saturn.yzu.edu.tw yabthung@gmail.com
?corresponding author
Abstract
This paper exploits unlabeled text data
to improve new word identification and
Chinese word segmentation performance.
Our contributions are twofold. First,
for new words that lack semantic trans-
parency, such as person, location, or
transliteration names, we calculate as-
sociation metrics of adjacent character
segments on unlabeled data and encode
this information as features. Second, we
construct an internal dictionary by using
an initial model to extract words from
both the unlabeled training and test set
to maintain balanced coverage on the
training and test set. In comparison
to the baseline model which only uses
n-gram features, our approach increases
new word recall up to 6.0%. Addition-
ally, our approaches reduce segmenta-
tion errors up to 32.3%. Our system
achieves state-of-the-art performance for
both the closed and open tasks of the
2006 SIGHAN bakeoff.
1 Introduction
Many Asian languages do not delimit words by
spaces. Word segmentation is therefore a key
step for language processing tasks in these lan-
guages. Chinese word segmentation (CWS) sys-
tems can be built by supervised learning from a
labeled data set. However, labeled data sets are
expensive to prepare as it involves manual an-
notation efforts. Therefore, exploiting unlabeled
data to improve CWS performance becomes an
important research goal. In addition, new word
identification (NWI) is also very important be-
cause they represent the latest information, such
as new product names.
This paper explores methods of extracting
information from both internal and external
unlabeled data to augment NWI and CWS.
According to (Tseng and Chen, 2002), new
words can be divided into two major cate-
gories: Words with high or low semantic trans-
parency (ST), which describes the correlation of
semantic meanings between a word and its mor-
phemes. We designed effective strategies toward
the identification of these two new word types.
One is based on transductive learning and the
other is based on association metrics.
2 The Model
2.1 Formulation
We convert the manually segmented words into
tagged character sequences. We tag each char-
acter with either B, if it begins a word, or I, if
it is inside or at the end of a word.
2.2 Conditional Random Fields
CRFs are undirected graphical models trained
to maximize a conditional probability (Lafferty
et al, 2001). A linear-chain CRF with parame-
ters ? = ?1, ?2, . . . defines a conditional proba-
bility for a state sequence y = y1 . . . yT given an
input sequence x = x1. . . xT to be
P?(y|x) =
1
fx
exp
( T
?
t=1
?
k
?kfk(yt?1, yt, x, t)
)
where Zx is the normalization that makes the
probability of all state sequences sum to one;
fk(yt?1, yt, x, t) is often a binary-valued feature
function and ?k is its weight. The feature func-
tions can measure any aspect of a state transi-
tion, yt?1 ? yt, and the entire observation se-
quence, x, centered at the current position, t.
For example, one feature function might have
value 1 when yt?1 is the state B, yt is the state
I, and is the character ??. Large positive val-
ues for ?k indicate a preference for such an event;
large negative values make the event unlikely.
In our CRF model, each binary feature is mul-
tiplied with all states (yt) or all state transitions
(yt?1yt). For simplicity, we omit them in the fol-
lowing discussion. In addition, we use C0 rather
than xt to denote the current character.
931
3 Baseline n-gram Features
Character n-gram features have proven their ef-
fectiveness in ML-based CWS (Xue and Shen,
2003). We use 4 types of unigram feature func-
tions: C0, C1 (next character), C?1 (previous
character), C?2 (character preceding C?1). Fur-
thermore, 6 types of bigram features are used,
and are designated here as conjunctions of the
previously specified unigram features, C?2C?1,
C?1C0, C0C1, C?3C?1, C?2C0, and C?1C1.
4 New Word Identification
We mainly focus on improving new word iden-
tification (NWI) using unlabeled text. Words
with high and low ST are discussed separately
due to the disparity in their morphological char-
acteristics. However, it is unnecessary for our
system to classify words as high- or low-ST be-
cause our strategies for dealing with these two
classes are employed synchronously.
4.1 High-ST words
For a high-ST word, its meaning can be easily
derived from those of its morphemes. A word?s
semantic meaning correlates to its tendency of
being affixed to longer words. This behavior
can be recorded by the baseline n-gram model.
When the baseline model is used to segment a
sentence containing a high-ST word, since this
tendency is consistent with that is recorded in
the baseline model, this word tends to be suc-
cessfully segmented. For example, supposeW
? zhi-nan-che (compass chariot) is in the train-
ing set. The baseline n-gram model will record
the tendency of W zhi-nan (guide) that it
tends to be a prefix of a longer word. When
tagging a sentence that contains another high
ST word also containing W, such as W?
zhi-nan-zhen (compass), this word can be cor-
rectly identified.
Using only n-gram features may prevent some
occurrences of high-ST words from being iden-
tified due to the ambiguity of neighboring n-
grams. To rectify this problem, we introduce
the transductive dictionary (TD) feature, which
is similar to the traditional dictionary feature
that indicates if a sequence of characters in a
sentence matches a word w in an existing dic-
tionary. The difference is that the TD not only
comprises words in the training set, but con-
tains words extracted from the unlabeled test
set. The transductive dictionary is so named be-
cause it is generated following general concepts
of transductive learning. We believe adding TD
features can boost recall of high-ST words. More
details on the TD are found in Section 5. The
TD features that identify high-ST words are de-
tailed in Section 6.1.
4.2 Low-ST words
On the contrary, new words lack of ST, such
as transliteration names, are more likely to
be missed by the baseline n-gram model, be-
cause their morphemes? morphological tenden-
cies are not guaranteed to be consistent with
those recorded by n-gram features. For instance,
suppose )s tian-ping (libra) only appears as
individual words in the training set. The base-
line model cannot identify ?)s xiong-tian-
ping (a singer?s name) because ?)s is a low-
ST word and the morphological tendency of )
s is not consistent with the recorded one.
In English, there is a similar phenomenon
called multi-word expressions (MWEs).
(Choueka, 1988) regarded MWE as connected
collocations: a sequence of neighboring words
?whose exact meaning cannot be derived from
the meaning or connotation of its components?,
which means that MWEs also have low ST.
As some pioneers provide MWE identification
methods which are based on association metrics
(AM), such as likelihood ratio (Dunning, 1993).
The methods of identifying low-ST words can
be divided into two: filtering and merging. The
former uses AM to measure the likelihood that a
candidate is actually a whole word that cannot
be divided. Candidates with AMs lower than
the threshold are filtered out. The latter strat-
egy merges character segments in a bottom-up
fashion. AMs are employed to suggest the next
candidates for merging. Both methods suffer
from two main drawbacks of AM: dependency
on segment length and inability to use relational
information between context and tags. In the
first case, applying AMs to ranking character
segment pairs, it is difficult to normalize the val-
ues calculated from pairs of character segments
of various lengths. Secondly, AMs ignore the re-
lationships among n-grams (or other contextual
information) and labels, which are abundant in
annotated corpora, and they only use annota-
tion data to determine thresholds. In Section
6.2, we illustrate how encoding AMs as features
can avoid the above weaknesses.
5 Balanced Transductive Dictionary
The simplest TD is composed of words in the
training set and words extracted from the unla-
beled test set. The main problem of such TD
is the disparity in training and test set cov-
erage. During training, since its coverage is
100%, the enabled dictionary features will be as-
signed very high weights while n-gram features
932
will be assigned low weights. During testing,
when coverage is approximately 80-90%, most
tags are decided by dictionary features enabled
by IV words, while n-gram features have lit-
tle influence. As a result, it is likely that only
IV words are correctly segmented, while OOV
words are over-segmented. Loosely speaking, a
dictionary?s coverage of the training set is linked
to the degree of reliance placed by the CRF
model on the corresponding dictionary features.
Therefore, the dictionary should be made more
balanced in order to avoid the potential problem
of overfitting. Here a dictionary is said to be
more balanced if its coverage of the training set
approximates its coverage of the test set while
maximizing the latter. Afterward we name the
TD composed of words from gold training set
and tagged test set and as Na??ve TD (NTD) for
its unbalanced coverage in training and test set.
Our TD is constructed as follows. Given ini-
tial features, we use the model trained on the
whole training set with these features to label
the test set and add all words into our TD.
The next step is to balance our TD?s cover-
age of the training and test sets. Since coverage
of the test set cannot reach 100%, the only way
to achieve this goal is by slightly lowering the
dictionary?s coverage on the training set. We
apply n-fold cross-tagging to label the training
set data: Each fold that has 1/n of the train-
ing set is tagged by the model trained on the
other n ? 1 folds with initial features. All the
words identified by this cross-tagging process are
then added to our TD. The difference between
the NTD and our TD is that the NTD extracts
words from the gold training set, but our TD ex-
tracts words from the cross-tagged training set.
Finally, our TD is used to generate dictionary
features to train the final model. Since the TD
constructed from cross-tagging training set and
tagged test set exists more balanced coverage
of the training and test set, we call such a TD
?balanced TD?, shorted as BTD.
6 Our NWI Features
6.1 Transductive Dictionary Features
If a sequence of characters in a sentence matches
a word w in an existing dictionary, it may indi-
cate that the sequence of characters should be
segmented as one word. The traditional way
is to encode this information as binary word
match features. To distinguish the matches with
the same position and length, we propose a new
word match feature that contains frequency in-
formation to replace the original binary word
match feature. Since over 90% of words are four
or fewer characters in length, we only consider
words of one to four characters. In the following
sections, we use D to denote the dictionary.
6.1.1 Word Match Features (WM)
This feature indicates if there is a sequence of
neighboring characters around C0 that match a
word inD. Features of this type are identified by
their positions relative to C0 and their lengths.
Word match features are defined as:
WM(w = C?pos . . . C?pos+len?1)
=
{
1 if w ? D
0 otherwise
where len ? [1..4] is w?s length and pos ? [0..len]
is C0?s zero-based relative position in w (when
pos = len, the previous len characters form a
word found in D). If C0 is ?? and ???
is found in D, WM(C?2 . . . C0) is enabled.
6.1.2 Word Match with Word
Frequency (WMWF)
Given two different words that have the same
position and length, WM features cannot dif-
ferentiate which should have the greater weight.
This could cause problems when two matched
words of same length overlap. (Chen and Bai,
1998) solved this conflict by selecting the word
with higher (frequency ? length). We utilize
this idea to reform the WM features into our
WMWF features:
WMWFq(w = C?pos . . . C?pos+len?1)
=
{
1 if w ? D and log feq(w) = q
0 otherwise
where the word frequency is discretized into
10 bins in a logarithmic scale:log feq(w) =
min(dlog2 w?s frequency + 1e, 10)
thus q[0..10] is the discretized log frequency
of w. In this formulation, matching words with
higher log frequencies are more likely to be the
correct segmentation. Following the above ex-
ample, if the frequency of ??? is 15, then
the feature WMWF4(C?2 . . . C0) is enabled.
6.1.3 Discretization v.s. Zipf?s Law
Since current implementations of CRF models
only allow discrete features, the word frequency
must be discretized. There are two commonly
used discretization methods: equal-width inter-
val and equal-frequency interval, where the lat-
ter is shown to be more suitable for data fol-
lowing highly skewed distribution (Ismail and
Ciesielski, 2003). The word frequency distribu-
tion is the case: Zipf?s law (Zipf, 1949) states
that the word frequency is inversely proportional
to its rank (Adamic and Huberman, 2002):
f(x) ? z??
933
where f(x) is x?s frequency, z is its rank in the
frequency table, and ? is empirically found to be
close to unity. Obviously this distribution is far
from flat uniform. Hence the equal-frequency
binning turns out to be our choice.
Ideally, we would like each bin to have equal
expected number of values rather than following
empirical distribution. Therefore, we attempt
to discretize according to their underlying Zip-
fian distribution.
Adamic & Huberman (2002) shows that Zipf?s
law is equivalent to the power law, which de-
scribes Zipf?s law in a unranked form:
fX(x) ? x?(1+(1/?)),
where X is the random variable denoting the
word frequency and fX(x) is its probability den-
sity function. Approximated by integration, the
expected number of values in the bin [a, b] can
be calculated as
?
a?x?b
x ? Pr [X = x] ?
? b
a
x ? fX(d)dx
?
? b
a
x ? x?(1+(1/?))dx ? lnx|ba = ln(b/a)
(? ? ? 1)
Thus each bin has equal number of values within
it if and only if b/a is a constant, which is in a log
scale. This shows that our strategy to discretize
the WMWF and WMNF features in a log scale
is not only a conventional heuristic but also has
theoretical support.
6.2 Association Metric Features (AM)
In this section, we describe how to formulate
the association metrics as features to avoid the
weakness stated in Section 4.2. Our idea is to
enumerate all possible character segment pairs
before and after the segmentation point and
treat their association metrics as feature values.
Each possible pair corresponds to an individual
feature. For computational feasibility, only pairs
with total length shorter than five characters are
selected. All the enumerated segment pairs are
listed in the following table:
Feature x,y Feature x,y
AM1+1 c?1, c0 AM2+1 c?2c?1, c0
AM1+2 c?1, c0c1 AM2+2 c?2c?1, c0c1
AM1+3 c?1, c0c1c2 AM3+1 c?3c?2c?1, c0
We use Dunning?s method (Dunning, 1993)
because it does not depend on the assumption of
normality and it allows comparisons to be made
between the significance of the occurrences of
both rare and common phenomenon. The like-
lihood ratio test is applied as follows:
LR(x, y) =2? (logl(p1, k1, n1) + logl(p2, k2, n2)
? logl(p, k1, n1)? logl(p, k2, n2))
where logl(P,K,M) = K? lnP +(M ?K)? ln(1?
P ), k1 = freq(x, y); k2 = f(x,?y) = freq(x)?k1;
n1 = freq(y); n2 = N ? n1; p1 = p(x|y) = k1/n1;
p2 = p(x|y) = k2/n2; p = p(x) = (k1 + k2)/N ;
N is the number of words in corpus.
An important property of likelihood ratio is
that ?2LR is asymptotically x21 distributed.
Hence we can directly compute its p-value. We
then discretize the p-value into several bins, each
bin is defined by two significance levels 2?(q+1)
and 2?q. Thus, our AM feature is defined as:
AMq(x, y) =
?
?
?
1 if the p-value of
LR(x, y) ? [2?(q+1), 2?q]
0 otherwise
Since we have a constraint 0 ? q ? 10, thus,
the last interval is [0, 2?10]. We can think that
larger q implies higher tendency of current char-
acter to be labeled as ?I ?.
7 External Dictionary Features
7.1 Word Match with Ngram
Frequency (WMNF)
In addition to internal dictionaries extracted
from the training and test data, external dic-
tionaries can also be used. Unlike with internal
dictionaries, the true frequency of words in ex-
ternal dictionaries cannot be acquired. We must
treat each external dictionary word as an n-gram
and calculate its frequency in the entire unseg-
mented (training plus test) set as follows:
WMNFq(w = C?pos . . . C?pos+len?1)
=
{
1 if w ? D log ngram freq(w) = q
0 otherwise
where the frequencies are discretized into 10 bins
by the same way describing in previous section.
In this formulation, matching n-grams with
higher log frequencies are more likely to repre-
sent correct segmentations.
8 Experiments and Results
8.1 Data and Evaluation Metrics
We use two datasets in SIGHAN Bakeoff 2006:
one Simplified Chinese provided by Univ. of
Pennsylvania (UPUC) and one Traditional Chi-
nese provided by the City Univ. of HK
(CITYU), as shown in Table 1.
Two unlabeled text data used in our exper-
iments. For the CITYU dataset, we use part
of the CIRB40 corpus1 (134M). For the UPUC
dataset, we use the Contemporary Chinese Cor-
pus at PKU2 (73M).
1http://clqa.jpn.org/2006/04/corpus.html
2http://icl.pku.edu.cn/icl_res/
934
UPUC CITYU
F +/- Roov +/- Riv NC NCRR F +/- Roov +/- Riv NC NCRR
cl
os
ed
1 N-grams 93.0 n/a 71.1 n/a 95.7 14094 n/a 96.6 n/a 78.8 n/a 97.3 9642 n/a
2 (1) + AM (int raw) 94.3 +1.3 76.4 +5.3 96.5 11655 +17.3 97.3 +0.7 80.3 +1.5 97.9 7890 +18.2
3 (1) + WM, NTD(1) 93.4 +0.4 74.8 +3.7 95.4 13182 +6.5 97.0 +0.4 81.6 +2.8 97.3 8597 +10.8
4 (1) + WMWF, NTD(1) 93.7 +0.7 75.0 +3.9 95.8 12719 +9.7 97.2 +0.6 82.0 +3.2 97.6 8029 +16.7
5 (1) + WMWF, BTD(1) 94.0 +1.0 73.4 +2.3 96.7 12218 +13.3 97.4 +0.8 79.2 +0.4 98.3 7429 +23.0
6 (1) + WMWF, BTD(2)
+ AM (int raw)
94.5 +1.5 76.6 +5.5 96.7 11173 +20.7 97.5 +0.9 80.3 +1.5 98.2 7377 +23.5
7 Rank 1 in Closed 93.3 n/a 70.7 n/a 96.3 n/a n/a 97.2 n/a 78.7 n/a 98.1 n/a n/a
op
en
8 (1) + AM (ext raw) 94.3 +1.3 75.9 +4.8 96.6 11695 +17.0 97.3 +0.7 81.9 +3.1 97.9 7747 +19.7
9 (1) + WMWF, BTD(8)
+ AM (ext raw)
94.7 +1.7 77.1 +6.0 96.9 10844 +23.1 97.8 +1.2 82.2 +3.4 98.5 6531 +32.3
10 (9) + WMNF 95.0 +2.0 78.7 +7.6 97.1 10326 +26.7 97.9 +1.3 84.0 +5.2 98.5 6117 +36.6
11 Rank 1 in Open 94.4 n/a 76.8 n/a 96.6 n/a n/a 97.7 n/a 84.0 n/a 98.4 n/a n/a
Table 2: Comparison scores for UPUC and CITYU
Source Training
(Wds/Types)
Test
(Wds/Types)
UPUC 509K/37K 155K/17K
CITYU 1.6M/76K 220K/23K
Table 1: An overview of corpus statistics
We use SIGHAN?s evaluation script to score
all segmentation results. This script pro-
vides three basic metrics: Precision (P), Re-
call (R), and F-Measure (F). In addition, it
also provides three detailed metrics: ROOV
stands for the recall rate of the OOV words.
RIV stands for the recall rate of the IV
words, and NC stands for NChanges (inser-
tion+deletion+substitution) (Sproat and Emer-
son, 2003). In addition, we also compare the
NChange reduction rate (NCRR) because the
CWS?s state-of-the art F-measure is over 90%.
Here, the NCRR of any system s is calculated:
NCRR(s) = NChangebaseline ?NChanges
NChangebaseline
8.2 Results
Our system uses the n-gram features described
in Section 3 as our baseline features, denoted
as n-grams. We then sequentially add other fea-
tures and show the results in Table 2. Each con-
figuration is labeled with the features and the
resources used in it. For instance, AM(int raw)
means AM features computed from the inter-
nal raw data, including the unlabeled training
and test set, and WM, NTD(1) stands for WM
features based on the NTD employing config.1?s
feature as its initial features.
Our experiments are conducted in the follow-
ing order: starting from baseline model, we then
gradually add AM features (config.2) and TD
features (config.4 & 5) and combined them as
our final setting (config.6) for the closed task. In
the open task, we sequentially add AM features
(config.8), TD features (config.9), which only ex-
ploit internal and unlabeled data. Finally, the
last setting (config.10) employs external dictio-
naries besides all above features.
Association Metric At first, we compare
the effects after adding AM which is computed
based on the internal raw data (config.2). We
can see that adding AM can significantly im-
prove the performance on both datasets. Also,
the OOV-recall is improved 5.3% and 1.5% on
UPUC and CITYU respectively.
Transductive Dictionary Without lost of
generality, we firstly use the WM features in-
troduced in Section 6.1.1 to represent dictio-
nary features which is denoted as config. 3 in
Tables 3. We can see that the configuration
with WM features outperforms that with N-
grams (config.1). It is worth mentioning that
even though N-grams achieve satisfactory OOV
recall (0.788 and 0.711) in CITYU and UPUC,
config. 3 achieves higher OOV recall.
Frequency Information and BTD To show
the effectiveness of frequency, we compare WM
with WMWF features. In Table 2, we can
see that WMWF features (config.4) outperform
WM features (config.3) on both datasets in
terms of F-Measure and RIV. In addition,
switching the NTD (config.4) with BTD (con-
fig.5) can further improve RIV and F-score while
ROOV slightly decreases. This is not surpris-
ing. In a BTD, most incorrectly segmented
words appear infrequently. Unfortunately, the
new words detected by the baseline model also
have comparatively low frequencies. Therefore,
these words will be assigned into the same sev-
eral bins corresponding to infrequent words as
the incorrectly segmented words and share low
weights with them.
Combined Effects In config.6, we use the
model with N-gram plus AM features as initial
features to construct the BTD. In Table 2, we
can see that the increase of ROOV?s can recover
the loss brought by using BTD and further raise
935
the F-measure to the level of the state-of-the-art
open task performance.
In comparison of the baseline n-gram model,
our approach reduces the errors by an significant
number of 20.7% and 23.5% in the UPUC and
CITYU datasets, respectively. The OOV recall
of our approach increases 5.5% and 1.5% on the
UPUC and CITYU datasets, respectively. As-
tonishingly, in the UPUC dataset, with limited
information provided by training corpus and un-
labeled test data, our system still outperforms
the best SIGHAN open CWS system that are
allowed to use unlimited external resources.
8.2.1 Using External Unlabeled Data
In config.9, we also use the ngrams plus AM as
initial features to generate the BTD, but exter-
nal unlabeled data are used along with internal
data to calculate values of AM features. Com-
paring with config.6, we can see that ROOV,
RIV, and F-score are further improved, espe-
cially ROOV. Notably, this configuration can
reduce NChanges by 2.4% in comparison of the
best closed configuration.
8.2.2 Using External Dictionaries
To demonstrate that our approach can be
expandable by installing external dictionaries,
we add WMNF features based on the external
dictionaries into the config.9, and denote this
to be our config.10. We use the Grammati-
cal Knowledge-Base of Contemporary Chinese
(GKBCC) (Yu et al, 2003) and Chinese Elec-
tronic Dictionary for the UPUC and CITYU
dataset, respectively.
As shown in Table 2, all metrics of config.10
are better than config.9, especially ROOV. This
is because most of the new words do not exist in
external dictionaries; therefore, using external
dictionaries can complement our results.
9 Conclusion
This paper presents how to exploit unlabeled
data to improve both NWI and CWS perfor-
mance. For new high-ST words, since they
can be decomposed into semantically relevant
atomic parts, they could be identified by the n-
gram models. Using the property, we construct
an internal dictionary by using this model to
extract words from both the unlabeled training
and test set to maintain balanced coverage on
them, which makes the weights of the internal
dictionary features more accurate. Also, fre-
quency is initiatively considered in dictionary
features and shows its effectiveness.
For low-ST words, we employ AMs, which
is frequently used in English MWE extraction
to enhance the baseline n-gram model. We
show that this idea effectively extract much
more unknown person, location, and transliter-
ation names which are not found by the baseline
model.
The experiment results demonstrate that
adopting our two strategies generally benefi-
cial to NWI and CWS on both traditional
and simplified Chinese datasets. Our sys-
tem achieves state-of-the-art closed task perfor-
mance on SIGHAN bakeoff 2006 datasets. Un-
der such most stringent constraints defined in
the closed task, our performances are even com-
parable to open task performance. Moreover,
with only external unlabeled data, our system
also achieves state-of-the-art open task perfor-
mance on SIGHAN bakeoff 2006 datasets.
References
L.A. Adamic and B.A. Huberman. 2002. Zipf?s law
and the internet. Glottometrics, 3:143?150.
K. J. Chen and M. H. Bai. 1998. Unknown word
detection for chinese by a corpus-based learning
method. Computational Linguistics and Chinese
Language Processing, 3(1):27?44.
Y. Choueka. 1988. Looking for needles in a haystack
or locating interesting collocation expressions in
large textual databases. In RIAO.
T. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):65?74.
Michael K. Ismail and Vic Ciesielski. 2003. An em-
pirical investigation of the impact of discretization
on common data distributions. In Design and Ap-
plication of Hybrid Intelligent Systems. IOS Press,
Amsterdam, Netherlands.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. In ICML-
01, pages 282?289.
Richard Sproat and Thomas Emerson. 2003.
The first international chinese word segmentation
bakeoff. In SIGHAN-03.
Huihsin Tseng and Keh-Jiann Chen. 2002. Design of
chinese morphological analyzer. In SIGHAN-02.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In SIGHAN-03.
G.K. Zipf. 1949. Human Behavior and the Principle
of Least Effort. Addison-Wesley, Cambridge, MA.
936
Coling 2010: Poster Volume, pages 223?231,
Beijing, August 2010
Abstract 
Global ranking, a new information re-
trieval (IR) technology, uses a ranking 
model for cases in which there exist re-
lationships between the objects to be 
ranked. In the ranking task, the ranking 
model is defined as a function of the 
properties of the objects as well as the 
relations between the objects. Existing 
global ranking approaches address the 
problem by ?learning to rank?. In this 
paper, we propose a global ranking 
framework that solves the problem via 
data fusion. The idea is to take each re-
trieved document as a pseudo-IR sys-
tem. Each document generates a pseu-
do-ranked list by a global function. The 
data fusion algorithm is then adapted to 
generate the final ranked list. Taking a 
biomedical information extraction task, 
namely, interactor normalization task 
(INT), as an example, we explain how 
the problem can be formulated as a 
global ranking problem, and demon-
strate how the proposed fusion-based 
framework outperforms baseline me-
thods. By using the proposed frame-
work, we improve the performance of 
the top 1 INT system by 3.2% using 
the official evaluation metric of the 
BioCreAtIvE challenge. In addition, by 
employing the standard ranking quality 
measure, NDCG, we demonstrate that 
                                                 
* Corresponding author 
the proposed framework can be cas-
caded with different local ranking 
models and improve their ranking re-
sults. 
1 Introduction 
Information Retrieval (IR) involves finding 
documents that are relevant to a given query in 
a large corpus. The task is usually formulated 
as a ranking problem. When a user submits a 
query, the IR system retrieves all documents 
that contain at least one query term, calculates 
a ranking score for each of the documents us-
ing a ranking model, and sorts the documents 
according to the ranking scores. The scores 
represent the relevance, importance, and/or 
diversity of the retrieved documents. Thus, the 
quality of a search engine can be determined 
by the accuracy of the ranking results.  
Recently, a machine learning technology 
called learning to rank has been applied exten-
sively to the task. Several state-of-the-art ma-
chine learning-based ranking algorithms have 
been proposed, e.g., RankSVM and RankNet. 
These algorithms differ substantially in terms 
of the ranking models and optimization tech-
niques employed, but most of them can be re-
garded as ?local ranking? approaches in the 
sense that each model is defined on a single 
document without considering the possible 
relations to other documents to be ranked. In 
many applications, this is only a loose approx-
imation as there is always relational informa-
tion among documents. For example, in some 
cases, users may prefer that two similar docu-
ments have similar relevance scores; even 
Global Ranking via Data Fusion 
Hong-Jie Dai1,2 Po-Ting Lai3 Richard Tzong-Han Tsai3* Wen-Lian Hsu1,2* 
1Department of Computer Science, National Tsing-Hua University,  
2Institute of Information Science, Academia Sinica,  
3Department of Computer Science & Engineering, Yuan Ze University 
hongjie@iis.sinica.edu.tw 
s951416@mail.yzu.edu.tw  
thtsai@saturn.yzu.edu.tw 
hsu@iis.sinica.edu.tw 
223
though one of the documents is not as relevant 
to the given query as the other; this problem is 
similar to Pseudo Relevance Feedback (Kwok, 
1984). In other cases, web pages from the 
same site form a sitemap hierarchy in which a 
parent document should be ranked higher than 
its child documents (referred to as Topic Dis-
tillation at TREC (Chowdhury, 2007)). To util-
ize all available information, more advanced 
ranking algorithms define a ranking model as a 
function of all the documents to be ranked, i.e., 
a global ranking model (Qin et al, 2008a; Qin 
et al, 2008b). 
Unlike conventional ranking and learning to 
rank models, such as BM25 and RankSVM, 
whose ranking functions are defined on a 
query and document pair, global ranking mod-
els utilize both content information and rela-
tion information. Qin et al (2008) proposed 
the first supervised learning framework for the 
global ranking problem. They formulated the 
problem as an optimization problem that in-
volves finding an objective function to minim-
ize the trade-off between local consistence and 
global consistence and implemented it on 
SVM. Subsequently, they defined the global 
ranking problem formally in (Qin et al, 2008) 
and solved it by employing continuous condi-
tional random fields (CRF). 
In this paper, we propose a new framework 
for the global ranking problem. The major dif-
ference between our work and that of Qin et al 
(2008a; 2008b) is that we do not compile a 
feature vector of relational information directly 
to construct a new machine-learned ranking 
model for global ranking. Instead, we use the 
ranking results generated by the original rank-
ing model and then employ an algorithm with 
the relational information to transform the 
global ranking problem into a data fusion prob-
lem; that is also known as a rank aggregate 
problem. The proposed framework is flexible 
and can be cascaded with conventional ranking 
models or learning to rank models. 
The remainder of this paper is organized as 
follows. In Section 2, we present a formal de-
finition of global ranking. In Section 3, we de-
scribe the proposed framework and consider 
three fusion algorithms that can be used with 
our framework. We also explain how the algo-
rithms can be adapted to solve the global rank-
ing problem. In Section 4, we introduce a bio-
medical text mining task called the interactor 
normalization task (INT) (Krallinger et al, 
2009) and show why it should be formulated 
as a global ranking problem. In Section 5, we 
report extensive experiments conducted on the 
INT dataset released by BioCreAtIvE 
(Krallinger et al, 2009). Section 6 contains 
some concluding remarks. 
2 Global Ranking Problem 
The global ranking problem was first defined 
formally by Qin et al (2008). In this paper, we 
propose a new global ranking framework 
based on their definition. Although we devel-
oped the framework independently, we adopt 
Qin et al?s terminology. 
Let   denote a query. In addition, let 
        
      
       
    
   
  denote the docu-
ments retrieved by  , and let      
   
      
       
    
   
  denote the ranking scores 
assigned to the documents. Here,      
represents the number of documents retrieved 
by  . Note that the numbers of documents va-
ries according to different queries. We assume 
that      is determined by a ranking model. 
If a ranking model is defined on a single 
document, i.e., in the form of 
  
        
               , 
it is called a ?local ranking? model.  
Let       
      
          
   
 
 be a set of 
real-value functions defined on   
   
,   
   
, and  
     (                ). The functions 
           
represents the relations between documents. 
Equation 2 is defined according to the re-
quirements of different tasks. For example, for 
the Pseudo Relevance Feedback problem, Qin 
et al (2008) defined Equation 2 as the similari-
ties between any two documents in their CRF-
based model. 
If a ranking model takes all the documents 
as its input and exploits both local and global 
information (Equation 2) in the documents, i.e., 
in the form of 
            , 
it is called a ?global ranking? approach. 
(1) 
(2) 
224
 3 Fusion-based Global Ranking 
Framework 
It is usually difficult to develop a global rank-
ing algorithm that can fully utilize all the local 
and global information in documents to pro-
duce a document rank and also consider the 
score ranks. One example of a global ranking 
algorithm that satisfied these criteria is the one 
proposed in (Qin et al, 2008) in which the 
modified CRF algorithm handles context (local) 
features and relational (global) features in the 
documents. Without solving a ranking problem 
directly, however, the modified CRF algorithm 
is more like a regression algorithm since it op-
timizes the CRF parameters in a maximum 
likelihood estimate without considering the 
score ranks. With respect to the ranking feature, 
in this section, we describe our framework 
based on the idea of data fusion for solving the 
global ranking problem. 
3.1 Framework Description 
The flow chart of the proposed framework is 
illustrated in Figure 1. The first step is the 
same as that of the traditional local ranking 
model. Given a query, the local ranking model 
  
   
 defined in Equation 1 is used to calculate 
the ranking score for each document, and re-
turn a document list sorted according to the 
local scores.  
The second step transforms the global rank-
ing problem into a data fusion problem. Our 
idea is to take each retrieved document as a 
pseudo-IR system, and the pseudo-ranking 
model,    
   
, used by each system is the func-
tion defined in Equation 2. For each pseudo-IR 
system,   
   
, the pseudo-ranking model for a 
document   
   
 is defined as follows:  
 
   
        
         
      
           
          . 
There are totally      pseudo-IR systems, 
which generate      pseudo-ranked lists. As a 
result, the global ranking problem is trans-
formed into a data fusion problem, that is to 
aggregate the pseudo-ranked lists. Figure 2 
shows the steps of the transformation algo-
rithm. 
The final step is to adapt fusion algorithms 
to aggregate the pseudo-ranked lists. A canoni-
cal data fusion task is called meta-search 
(Aslam and Montague, 2001; Fox and Shaw, 
1994; Lee, 1997; Nuray and Can, 2006), which 
aggregates Web search query results from sev-
eral engines into a more accurate ranking. The 
origin of research on data fusion can be traced 
back to (Borda, 1781). In recent years, the 
process has been used in many new applica-
tions, including aggregating data from micro-
array experiments to discover cancer-related 
genes (Pihura et al, 2008), integration of re-
sults from multiple mRNA studies (Lin and 
Ding, 2008), and similarity searches across 
datasets and information merging (Adler et al, 
2009; Zhao et al, 2010).  
Liu et al (2007) classified data fusion tech-
nologies into two categories: order-based fu-
sion and score-based fusion. In the first catego-
ry, the orders of the entities in individual rank-
ing lists are used by the fusion algorithm. In 
the second category, the entities in individual 
ranking lists are assigned scores and the fusion 
algorithm uses the scores. In the following 
sub-sections, we adapt three fusion algorithms 
Step 3
Step 2
Step 1
Local Ranking Model
Document Set
Query
Local Ranked 
Document List
Transformation 
Algorithm
Pseudo-IR 
System
1
Pseudo-IR 
System
2
Pseudo-IR 
System
n
(q)
...
Fusion 
Algorithm
Pseudo-
Ranked List
1
Pseudo-
Ranked List
2
Pseudo-
Ranked List
n
(q)
...
Global Ranked 
Document List
Pseudo-
Ranking Model
1
Pseudo-Ranking 
Model
2
Pseudo-Ranking 
Model
n
(q)
 
Figure 1. The Proposed Framework for 
Global Ranking. 
(3) 
225
for the proposed framework. The first is the 
Borda-fuse model (Aslam and Montague, 
2001), an order-based fusion approach based 
on an optimal voting procedure. The second is 
a linear combination (LC) model (Vogt and 
Cottrell, 1999), which is a score-based fusion 
approach. 
3.2 Borda-fuse 
The Borda-fuse model (Aslam and Montague, 
2001) is based on a political election strategy 
called the Borda Count. For our framework, 
the rationale behind the strategy is as follows. 
Each pseudo-IR system   
   
 is an analogy for 
a voter; and each voter ranks a fixed set of      
documents in order of preference (Equation 3). 
For each voter, the top ranked document is 
given      points, the second ranked document 
is given     -  points, and so on. If some doc-
uments left unranked by the voter, the remain-
ing points are divided equally among the un-
ranked documents. The documents are ranked 
in descending order of the total points. 
In our framework, we implement two Bor-
da-fuse-based models. The first is the modified 
Borda-fuse (MBF) model. In MBF, the number 
of points given for a voter's first and subse-
quent preferences is determined by the number 
of documents they have actually ranked, rather 
than the total number of ranked. Because the 
ranking model,    
   
, used by the pseudo-IR 
system may only retrieve  documents where 
  is smaller than     , we penalize systems 
that do not rank a full document set by reduc-
ing the number of points their vote distributes 
among the documents. In other words, if there 
are ten documents, but the pseudo-IR system 
only retrieves five, then the first document will 
only receive 5 points; the second will receive 4 
points, and so on. 
The second is the weighted Borda-fuse 
(WBF) model. The original Borda-fuse model 
reflects a democratic election in which each 
voter has equal weight. However, in many cas-
es, we prefer some voters because they are 
more reliable. We employ a simple weighting 
scheme that multiplies the points assigned to a 
document determined by system   
   
 by a 
weight 
  
   . 
3.3 LC Model 
The LC model has been used by many IR re-
searchers with varying degrees of success 
(Bartell et al, 1994; Knaus et al, 1995; Vogt 
and Cottrell, 1999; Vogt and Cottrell, 1998). In 
our framework, it is defined as follows. Given 
a query  , a document   
   
, the weights 
                     for  
    individual 
pseudo-IR systems, and jth pseudo-IR sys-
tem?s ranking score     
   
, the LC model cal-
culates the ranking score   of   
   
 against all 
pseudo-IR systems as follows: 
      
            
        
    
This score is then used to rank the documents. 
For example, for two pseudo-IR systems, this 
reduces to: 
          
           
          
   
 
Compared with MBF, Equation 4 requires 
both relevance scores and training data to de-
 function transform (    : the documents retrieved 
with query  ) 
{generate pseudo-ranked lists for     } 
 # a dictionary that maps the pseudo-IR systems to 
# their corresponding pseudo-ranked lists 
1. pseudoRankedLists = {} 
2. for   
   
 in     : 
     # a dictionary that maps the relation score (real 
    # value) to a list of documents. 
3.     relation = {} 
     for   
   
 in     : 
4.         relation[    
      
         ].append(  
   
) 
     # relation.keys() returns all keys stored in the  
 # dictionary relation. The key of relation is the 
 # relation score. 
5.     Sort relation.keys() in decreasing order 
     # a dictionary that maps a new rank to a list of 
    # documents. 
6.     pseudoRankedList = {} 
7.     newRank = 0 
     for score in sorted relation.keys(): 
         # relation[score] returns the document list  
         # corresponding to the given score 
         for doc in relation[score]: 
8.             pseudoRankedList[1+newRank] 
                                          .append(doc) 
9.         newRank = newRank + 1 
10.     pseudoRankedLists [  
   
] = pseudoRankedList 
 return pseudoRankedLists 
Figure 2. The Dependent Ranked List Gen-
eration Algorithm (represented using python 
syntax). 
(4) 
226
termine the weight   given to each pseudo-IR 
system. 
4 Case Study 
In this section, we describe the task examined 
in our study. We also explain how we formu-
late the task as a global ranking problem. The 
experiments results are detailed in Section 5. 
4.1 Interactor Normalization Task 
The interactor normalization task (INT) is a 
complicated text mining task that involves the 
following steps: (1) It recognizes gene men-
tions in a full text article. (2) It maps the rec-
ognized gene mentions to corresponding 
unique database identifiers which is similar to 
the word sense disambiguation task in compu-
tational linguistics. (3) It generates a ranked 
list of the identifiers according to their impor-
tance in the article and their probability of 
playing the interactor role in protein-protein 
interactions (PPIs). Such ranked lists are useful 
for human curators and can speed up PPI data-
base curation. 
Dai et al (2010) won first place in the Bio-
CreAtIvE II.5 INT challenge (Mardis et al, 
2009) by using a SVM-based local ranking 
model in which they treat gene mentions? iden-
tifiers in an article as the document set, and the 
query is a constant string ?interactor?. Based 
on their feature sets and evaluation results, we 
can find that their local ranking model tends to 
rank focus genes higher (Dai et al, 2010). 
However, the primary objective of INT is to 
generate a ranked list of interaction gene iden-
tifiers. According to (Jenssen et al, 2001), co-
mentioned genes are usually related in some 
way. For example, if two gene mentions fre-
quently occur alongside each other in the same 
sentence in an article, they probably have an 
association and influence each other?s rank. 
Take a low-ranked interactor that is only men-
tioned twice in an article as an example. If 
both mentions are next to the highest-ranked 
interactor in the article, then the low-ranked 
interactor?s rank should be boosted significant-
ly. Therefore, the ranking task for each article 
can be formulated as a global ranking problem; 
the global ranking algorithm should consider 
both the local information from Dai et al?s 
model and the global information from the as-
sociations among identifiers. 
4.2 Global Ranking in INT 
Let   be a constant ?interactor.? The identifier 
set generated by an INT system for a full-text 
article is analogous to the document set 
        
      
       
    
   
 . Here      denotes 
the number of identifiers. Note that the number 
of identifiers varies for different articles. Let 
        
      
       
    
   
  denote the ranking 
scores assigned to the identifiers given by a 
local ranking model. In this study, we used the 
INT system and SVM-based local ranking 
model released by (Dai et al, 2010) to gener-
ate the identifier set and ranking scores. 
To obtain the global information, we con-
sider the co-occurrence of identifiers and em-
ploy mutual information (MI) to measure the 
association between two identifiers as follows: 
                     
                      . 
In the above formula, the identifier probabili-
ties       and       are estimated by counting 
the number of occurrences in an article norma-
lized by  , i.e., the number of sentences con-
taining identifiers. The joint probability, 
        , is estimated by the number of times 
   co-occurs with    in a window of   words 
normalized by  . Note that, in practice, other 
advanced approaches can be used to calculate 
the association score. 
For the proposed framework, each identifier 
  
   
 is a pseudo-IR system with MI as its 
pseudo-ranking model    
   
. The identifiers 
that co-occur with   
   
 become candidates on 
  
   
?s pseudo-ranked list. 
5 Experiments 
In the following sub-sections, we introduce the 
dataset used in the experiments, describe the 
evaluation methods, report the results of the 
experiments conducted to compare the perfor-
mance of different methods, and discuss the 
efficiency of the proposed global ranking 
framework. 
227
5.1 Dataset 
We used the BioCreAtIvE II.5 Elsevier corpus 
released by BioCreAtIvE II.5 challenge in the 
experiments. The corpus contains 1,190 full-
text journal articles selected mainly from 
FEBS Letters. Following the same format as 
the BioCreAtIvE II.5 INT challenge, we used 
articles published in 2008 (61 articles) as our 
training set and articles published in 2007 or 
earlier (61 articles) as our test set. 
5.2 A Fusion-based Global Ranking 
Framework for INT 
Before applying the proposed framework, we 
preprocess the articles in the dataset to identify 
all gene mentions, and map them to their cor-
responding identifiers. After preprocessing, 
each full-text article is associated with a list of 
identifiers (Step 1 in Figure 1). The transform 
and fusion algorithm is then applied on each 
article (Steps 2 and 3 in Figure 1). 
To apply the WBF and LC models, we need 
to determine the weight assigned to each pseu-
do-IR system. To obtain the weight, we calcu-
late the precision of each rank of the ranked 
lists generated by Dai et al?s INT system. Fig-
ure 3 shows the precision of ranks 1 to 15 cal-
culated by applying three-fold cross validation 
on the INT training set. We observe that the 
precision declines as the rank increases, which 
implies that the higher ranks predicted by their 
SVM-based local ranking model are more reli-
able than the lower ranks. 
5.3 Evaluation Metrics 
Our evaluations focus on two comparisons: the 
first compares the ranking of the proposed 
framework with the original local ranking 
model by using the area under the curve of the 
interpolated precision/recall (iP/R) curve. This 
is the evaluation metric used in the BioCreA-
tIvE II.5 challenge and is a common way to 
depict the degradation of precision as one tra-
verses the retrieved results by plotting interpo-
lated precision numbers against percentage 
recall. The area under the iP/R function     is 
defined as follows: 
                              
 
   
                  
, 
where   is the total number of correct identifi-
ers and    is the highest interpolated precision 
for the correct identifier   at   , the recall for 
that hit. The interpolated precision    is calcu-
lated for each recall   by taking the highest 
precision at   or any     . 
In the second comparison, we use a standard 
quality measure in IR to estimate the ranking 
performance of local ranking models and the 
proposed framework. We adopt Normalized 
Discounted Cumulative Gain (NDCG) to 
measure the performance. The NDCG score of 
a ranking is computed based on DCG (Dis-
counted Cumulative Gain) as follows: 
             
    
       
 
   , 
where   is the rank position, and            
is the relevance grade of the  th identifier in 
the ranked result set. In our experiment, 
       corresponds to an interaction iden-
tifier, and        corresponds to other iden-
tifiers. NDCG is then computed as follows: 
        
      
       
, 
where      denotes the results of a perfect 
ranking. The NDCG values for all articles are 
averaged to obtain the average performance of 
the proposed framework. 
5.4 INT Test Set Performance 
Figure 4 shows the Area_iPR scores of four 
configurations. In the baseline configuration 
(Local/Rank1), the SVM-based local ranking 
model released by Dai et al is employed. In 
the configuration Global+LC, Global+MBF, 
and Global+WBF, the proposed global ranking 
framework is cascaded with the local ranking 
model and with three data fusion models: the 
LC model, the modified Borda-fuse (MBF) 
model, and the weighted Borda-fuse model. 
The figure also shows the Area_iPR scores of 
 
Figure 3. Precision of Different Ranks. 
0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
P
re
c
is
io
n
Rank
228
the top three teams and the average Area_iPR 
score of all BioCreAtIvE II.5 INT participants 
(Average).  
The results show that under the global rank-
ing framework, Area_iPR performance is im-
proved in addition to Global+MBF. The high-
est Area_iPR (Global+LC: 46.7%) is 3.2% 
higher than the Rank 1 score in the BioCreA-
tIvE II.5 INT challenge. According to our 
analysis, before global ranking, identifiers 
whose feature values rarely appear in the train-
ing set are often ranked incorrectly because 
their feature values are under-estimated by the 
ranking model. However, if the identifiers co-
occur with higher-ranked identifiers whose 
feature values appear frequently, the proposed 
framework is very likely to increase their ranks. 
This results in an improved Area_iPR score. 
5.5 Global Ranking Performance 
 
To illustrate the effectiveness of the proposed 
global ranking framework and assess its per-
formance when it is cascaded with other con-
ventional ranking models, we implement a 
simple term frequency-based ranking function, 
which is based on the identifier frequency in 
an article as another local ranking model. If 
two or more identifiers have the same frequen-
cy, two heuristic rules are employed sequen-
tially to rank them: (1) the identifier with the 
highest frequency in the Results section of the 
article, and (2) the identifier mentioned first in 
the article.  
Table 1 shows the NDCG percentage gain 
of different ranking models. It compares the 
ranked list generated by our global ranking 
framework and by the local ranking models. 
We observe that (1) irrespective of whether the 
local ranking model is a conventional model or 
a learning to rank model, Global+LC and 
Global+WBF models achieve NDCG gains 
over the original rankings of the local ranking 
models; (2) the results show that our global 
ranking framework can improve the perfor-
mance by only exploiting MI analysis. Howev-
er, it is expected that employing more ad-
vanced relation extraction methods to deter-
mine the global information (Equation 3) 
would yield more reliable pseudo-ranked lists 
and lead to a further improvement in the final 
ranking; and (3) similar to the results in Sec-
tion 5.4, the performance of Global+MBF does 
not improve. Global+MBF has a negative 
NDCG gain and the Area_iPR decreases by 
2.61%. We believe this is due to MBF gives 
equal weight to each pseudo-IR system. As 
mentioned in Section 4.1, the document set in 
INT is comprised of the identifiers of the gene 
mentions derived by Dai et al?s system. Un-
fortunately, there must be incorrect identifiers 
(the errors may be due to their gene mention 
recognition or identifier mapping processes). 
As in the meta-search, the best performance is 
often achieved by weighting the input systems 
unequally. Reasonable weights allow the algo-
rithm to concentrate on good feedback from 
pseudo-IR systems and ignore poor feedback. 
As shown by the average precision results in 
Figure 3, the identifiers (corresponding to the 
pseudo-IR systems in our framework) in the 
higher ranks are more reliable; however, MBF 
cannot use this information, which leads to a 
negative NDCG gain and a lower Area_iPR 
score. 
6 Conclusion 
We have presented a new global ranking 
framework based on data fusion technology. 
Our approach solves the global ranking prob-
lem in three stages: the first stage ranks the 
document set by the original local ranking 
model; the second stage transforms the prob-
Based on Global Ranking NDCG1 NDCG3 NDCG5 
Local Ranking 
/Rank1 
Global+LC +0.908 +1.323 -0.003 
Global+MBF -3.279 -1.034 -0.020 
Global+WBF -0.016 +3.630 +2.071 
Freq Global+LCf +1.639 +3.152 +2.817 
Global+MBFf -6.860 -4.275 -4.839 
Global+WBFf +2.549 +2.390 +3.043 
Table 1. The NDCG Gain (%) of Different 
Ranking Models. 
 
Figure 4. The Area_iPR Results of Different 
Ranking Models 
0.22 0.27 0.32 0.37 0.42 0.47 0.52
Global+WBF
Global+MBF
Global+LC
Local/Rank1
Hakenberg/Rank 2
S? tre/Rank 3
Average
229
lem into a data fusion task by using global in-
formation, and the final stage adapts fusion 
algorithms to solve the ranking problem. The 
framework is flexible and it can be combined 
with other mature ranking models and fusion 
algorithms. We also show how the BioCreA-
tIvE INT can be formulated as a global ranking 
problem and solved by the proposed frame-
work. Experiments on the INT dataset demon-
strate the effectiveness of the proposed frame-
work and its superior performance over other 
ranking models. 
In our future work, we will address the fol-
lowing issues: (1) the use of advanced data 
fusion algorithms in the proposed framework; 
(2) assessing the performance of the proposed 
framework on other tasks, such as Pseudo Re-
levance Feedback and Topic Distillation; and 
(3) design an advanced supervised learning 
relation extraction algorithm to replace MI in 
INT to evaluate the system performance. 
References 
Adler, P., R. Kolde, M. Kull, A. Tkachenko, H. 
Peterson, J. Reimand and J. Vilo (2009). Mining 
for coexpression across hundreds of datasets 
using novel rank aggregation and visualization 
methods. Genome Biology 10(R139). 
Aslam, J. A. and M. Montague (2001). Models for 
metasearch. Proceedings of the 24th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval, New Orleans, Louisiana, United States, 
ACM. 
Bartell, B. T., G. W. Cottrell and R. K. Belew 
(1994). Automatic combination of multiple 
ranked retrieval systems. Proceedings of the 
17th annual international ACM SIGIR 
conference on Research and development in 
information retrieval, Dublin, Ireland Springer-
Verlag New York, Inc. 
Borda, J. (1781). M?moire sur les ?lections au 
scrutin. Histoire del'Acad e?mie Royale des 
Sciences 2: 13. 
Chowdhury, G. (2007). TREC: Experiment and 
Evaluation in Information Retrieval. Online 
Information Review 31(5): 462. 
Dai, H.-J., P.-T. Lai and R. T.-H. Tsai (2010). 
Multi-stage gene normalization and SVM-based 
ranking for protein interactor extraction in full-
text articles. IEEE TRANSACTIONS ON 
COMPUTATIONAL BIOLOGY AND 
BIOINFORMATICS 14 May. 2010. IEEE 
computer Society Digital Library. IEEE 
Computer Society. 
Fox, E. A. and J. A. Shaw (1994). Combination of 
Multiple Searches. 1994, Proceedings of the 
Second Text REtrieval Conference (TREC 2)  
Jenssen, T.-K., A. Lagreid, J. Komorowski and E. 
Hovig (2001). A literature network of human 
genes for high-throughput analysis of gene 
expression. Nature Genetics 28(1): 21-28. 
Knaus, D., E. Mittendorf and P. Sch?uble (1995). 
Improving a basic retrieval method by links and 
passage level evidence. NIST Special 
Publication 500-225: Overview of the Third Text 
REtrieval Conference (TREC-3). 
Krallinger, M., F. Leitner and A. Valencia (2009). 
The BioCreative II.5 challenge overview. 
Proceedings of the BioCreative II.5 Workshop 
2009 on Digital Annotations, Madrid, Spain. 
Kwok, K. L. (1984). A document-document 
similarity measure based on cited titles and 
probability theory, and its application to 
relevance feedback retrieval. SIGIR'84. 
Lee, J. H. (1997). Analyses of multiple evidence 
combination. Proceedings of the 20th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval, Philadelphia, Pennsylvania, United 
States, ACM. 
Lin, S. and J. Ding (2008). Integration of Ranked 
Lists via Cross Entropy Monte Carlo with 
Applications to mRNA and microRNA Studies. 
Biometrics 65(1): 9-18. 
Liu, Y.-T., T.-Y. Liu, T. Qin, Z.-M. Ma and H. Li 
(2007). Supervised rank aggregation. 
Proceedings of the 16th international conference 
on World Wide Web, Banff, Alberta, Canada, 
ACM. 
Mardis, S., F. Leitner and L. Hirschman (2009). 
BioCreative II.5: Evaluation and ensemble 
system performance. Proceedings of the 
BioCreative II.5 Workshop 2009 on Digital 
Annotations, Madrid, Spain. 
Nuray, R. and F. Can (2006). Automatic ranking of 
information retrieval systems using data fusion. 
Inf. Process. Manage. 42(3): 595-614. 
Pihura, V., S. Dattaa and S. Datta (2008). Finding 
common genes in multiple cancer types through 
meta?analysis of microarray experiments: A 
230
rank aggregation approach Genomics 92(6): 
400-403  
Qin, T., T.-Y. Liu, X.-D. Zhang, D.-S. Wang and H. 
Li (2008). Global Ranking Using Continuous 
Conditional Random Fields. Proceedings of the 
Twenty-Second Annual Conference on Neural 
Information Processing Systems  (NIPS 2008), 
Vancouver, Canada. 
Qin, T., T. Liu, X. Zhang, D. Wang, W. Xiong and 
H. Li (2008). Learning to rank relational objects 
and its application to web search, ACM. 
Vogt, C. and G. Cottrell (1999). Fusion via a linear 
combination of scores. Information Retrieval 
1(3): 151-173. 
Vogt, C. C. and G. W. Cottrell (1998). Predicting 
the performance of linearly combined IR systems. 
Proceedings of the 21st annual international 
ACM SIGIR conference on Research and 
development in information retrieval, Melbourne, 
Australia ACM. 
Zhao, Z., J. Wang, S. Sharma, N. Agarwal, H. Liu 
and Y. Chang (2010). An Integrative Approach 
to Identifying Biologically Relevant Genes. 
Proceedings of SIAM International Conference 
on Data Mining (SDM). 
 
 
231
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 586?591,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cross-language and Cross-encyclopedia Article Linking Using
Mixed-language Topic Model and Hypernym Translation
Yu-Chun Wang
Department of CSIE
National Taiwan University
Taipei, Taiwan
d97023@csie.ntu.edu.tw
Chun-Kai Wu
Department of CSIE
National Tsinghua University
Hsinchu, Taiwan
s102065512@m102.
nthu.edu.tw
Richard Tzong-Han Tsai
?
Department of CSIE
National Central University
Chungli, Taiwan
thtsai@csie.ncu.edu.tw
Abstract
Creating cross-language article links
among different online encyclopedias is
now an important task in the unification
of multilingual knowledge bases. In this
paper, we propose a cross-language article
linking method using a mixed-language
topic model and hypernym translation
features based on an SVM model to link
English Wikipedia and Chinese Baidu
Baike, the most widely used Wiki-like
encyclopedia in China. To evaluate our
approach, we compile a data set from the
top 500 Baidu Baike articles and their
corresponding English Wiki articles. The
evaluation results show that our approach
achieves 80.95% in MRR and 87.46%
in recall. Our method does not heavily
depend on linguistic characteristics and
can be easily extended to generate cross-
language article links among different
online encyclopedias in other languages.
1 Introduction
Online encyclopedias are among the most fre-
quently used Internet services today. One of
the largest and best known online encyclopedias
is Wikipedia. Wikipedia has many language ver-
sions, and articles in one language contain hyper-
links to corresponding pages in other languages.
However, the coverage of different language ver-
sions of Wikipedia is very inconsistent. Table 1
shows the statistics of inter-language link pages
in the English and Chinese editions in February
2014. The total number of Chinese articles is
about one-quarter of English ones, and only 2.3%
of English articles have inter-language links to
their Chinese versions.
?
corresponding author
Articles Inter-language Links Ratio
zh 755,628 zh2en 486,086 64.3%
en 4,470,246 en2zh 106,729 2.3%
Table 1: Inter-Language Links in Wikipedia
However, there are alternatives to Wikipedia for
some languages. In China, for example Baidu
Baike and Hudong are the largest encyclopedia
sites, containing more than 6.2 and 7 million Chi-
nese articles respectively. Similarly, in Korea,
Naver Knowledge Encyclopedia has a large pres-
ence.
Since alternative encyclopedias like Baidu
Baike are larger (by article count) and growing
faster than the Chinese Wikipedia, it is worth-
while to investigate creating cross-language links
among different online encyclopedias. Several
works have focused on creating cross-language
links between Wikipedia language versions (Oh
et al, 2008; Sorg and Cimiano, 2008) or find-
ing a cross-language link for each entity mention
in a Wikipedia article, namely Cross-Language
Link Discovery (CLLD) (Tang et al, 2013; Mc-
Namee et al, 2011). These works were able to
exploit the link structure and metadata common
to all Wikipedia language versions. However,
when linking between different online encyclope-
dia platforms this is more difficult as many of these
structural features are different or not shared. To
date, little research has been done into linking be-
tween encyclopedias on different platforms.
Title translation is an effective and widely used
method of creating cross-language links between
encyclopedia articles. (Wang et al, 2012; Adafre
and de Rijke, 2005) However, title translation
alone is not always sufficient. In some cases, for
example, the titles of corresponding articles in dif-
ferent languages do not even match. Other meth-
ods must be used along with title translation to cre-
ate a more robust linking tool.
586
In this paper, we propose a method compris-
ing title and hypernym translation and mixed-
language topic model methods to select and link
related articles between the English Wikipedia and
Baidu Baike online encyclopedias. We also com-
pile a suitable dataset from the above two ency-
clopedias to evaluate the linking accuracy of our
method.
2 Method
Cross-language article linking between different
encyclopedias can be formulated as follows: For
each encyclopedia K, a collection of human-
written articles, can be defined as K = {a
i
}
n
i=1
,
where a
i
is an article in K and n is the size of
K. Article linking can then be defined as fol-
lows: Given two encyclopedia K
1
and K
2
, cross-
language article linking is the task of finding the
corresponding equivalent article a
j
from encyclo-
pedia K
2
for each article a
i
from encyclopedia
K
1
. Equivalent articles are articles that describe
the same topic in different languages.
Our approach to cross-language article linking
comprises two stages: candidate selection, which
produces a list of candidate articles, and candidate
ranking, which ranks that list.
2.1 Candidate Selection
Since knowledge bases (KB) may contain millions
of articles, comparison between all possible pairs
in two knowledge bases is time-consuming and
sometimes impractical. To avoid brute-force com-
parison, we first select plausible candidate articles
on which to focus our efforts. To extract possible
candidates, two similarity calculation methods are
carried out: title matching and title similarity.
2.1.1 Title Matching
In our title matching method, we formulate can-
didate selection as an English-Chinese cross-
language information retrieval (CLIR) problem
(Sch?onhofen et al, 2008), in which every English
article?s title is treated as a query and all the arti-
cles in the Chinese encyclopedia are treated as the
documents. We employ the two main CLIR meth-
ods: query translation and document translation.
In query translation, we translate the title of ev-
ery English article into Chinese and then use these
translated titles as queries to retrieve articles from
the Chinese encyclopedia. In document transla-
tion, we translate the contents of the entire Chinese
encyclopedia into English and then search them
using the original English titles. The top 100 re-
sults for the query-translation and the top 100 re-
sults for document-translation steps are unionized.
The resulting list contains our title-matching can-
didates.
For the query- and document-translation steps,
we use the Lucene search engine with similar-
ity scores calculated by the Okapi BM25 ranking
function (Beaulieu et al, 1997). We separate all
words in the translated and original English article
titles with the ?OR? operator before submission to
the search engine. For all E-C and C-E translation
tasks, we use Google Translate.
2.1.2 Title Similarity
In the title similarity method, every Chinese arti-
cle title is represented as a vector, and each dis-
tinct character in all these titles is a dimension of
all vectors. The title of each English article is
translated into Chinese and represented as a vec-
tor. Then, cosine similarity between this vector
and the vector of each Chinese title is measured as
title similarity.
2.2 Candidate Ranking
The second stage of our approach is to score
each viable candidate using a supervised learning
method, and then sort all candidates in order of
score from high to low as final output.
Each article x
i
in KB K
1
can be
represented by a feature vector x
i
=
(f
1
(x
i
), f
2
(x
i
), . . . , f
n
(x
i
)). Also, we have
y
j
= (f
1
(y
j
), f
2
(y
j
), . . . , f
n
(y
j
)) for a candidate
article y
j
in KB K
2
. Then, individual feature
functions F
k
(x
i
, y
j
) are based on the feature
properties of both article a
i
and a
j
. The top pre-
dicted corresponding article y
j
in the knowledge
base K
2
for an input article x
i
in K
1
should
receive a higher score than any other entity in
K
2
, a
m
? K
2
,m 6= j. We use the support
vector machine (SVM) approach to determine the
probability of each pair (x
i
,y
j
) being equivalent.
Our SVM model?s features are described below.
Title Matching and Title Similarity Feature
(Baseline)
We use the results of title matching and title sim-
ilarity from the candidate selection stage as two
features for the candidate ranking stage. The sim-
ilarity values generated by title matching and title
similarity are used directly as real value features
in the SVM model.
587
Mixed-language Topic Model Feature (MTM)
For a linked English-Chinese article pair, the dis-
tribution of words used in each usually shows
some convergence. The two semantically corre-
sponding articles often have many related terms,
which results in clusters of specific words. If two
articles do not describe the same topic, the distri-
bution of terms is often scattered. (Misra et al,
2008) Thus, the distribution of terms is good mea-
surement of article similarity.
Because the number of all possible words is too
large, we adopt a topic model to gather the words
into some latent topics. For this feature, we use
the Latent Dirichlet Allocation (LDA) (Blei et al,
2003). LDA can be seen as a typical probabilistic
approach to latent topic computation. Each topic
is represented by a distribution of words, and each
word has a probability score used to measure its
contribution to the topic. To train the LDA model,
the pair English and Chinese articles are concate-
nated into a single document. English and Chinese
terms are all regarded as terms of the same lan-
guage and the LDA topic model, namely mixed-
language topic model, generates both English and
Chinese terms for each latent topic. Then, for each
English article and Chinese candidate pair in test-
ing, the LDA model provides the distribution of
the latent topics. Next, we can use entropy to mea-
sure the distribution of topics. The entropy of the
estimated topic distribution of a related article is
expected to be lower than that of an unrelated ar-
ticle. We can calculate the entropy of the distribu-
tion as a value for SVM. The entropy is defined as
follows:
H = ?
T
?
j=1
~
?
dj
log
~
?
dj
where T is the number of latent topics, ?
dj
is the
topic distribution of a given topic j.
Hypernym Translation Feature (HT)
The first sentence of an encyclopedia article usu-
ally contains the title of the article. It may also
contain a hypernym that defines the category of
the article. For example, the first sentence of the
?iPad? article in the English Wikipedia begins,
?iPad is a line of tablet computers designed and
marketed by Apple Inc. . .? In this sentence, the
term ?tablet computers? is the hypernym of iPad.
These extracted hypernyms can be treated as arti-
cle categories. Therefore, articles containing the
same hypernym are likely to belong to the same
category.
In this study, we only carry out title hypernym
extraction on the first sentences of English articles
due to the looser syntactic structure of Chinese. To
generate dependency parse trees for the sentences,
we adopt the Stanford Dependency Parser. Then,
we manually designed seven patterns to extract hy-
pernyms from the parse tree structures. To demon-
strate this idea, let us take the English article ?The
Hunger Games? for example. The first sentence of
this article is ?The Hunger Games is a 2008 young
adult novel by American writer Suzanne Collins.?
Since article titles may be named entities or com-
pound nouns, the dependency parser may mislabel
them and thus output an incorrect parse tree. To
avoid this problem, we first replace all instances of
an article?s title in the first sentence with pronouns.
For example, the previous sentence is rewritten as
?It is a 2008 young adult novel by American writer
Suzanne Collins.? Then, the dependency parser
generates the following parse tree:
novel
It is a 2008 young adult collinsnsubj cop det num amod nn prep_by
suzanne writer Americannn nnamod
Next, we apply our predefined syntactic patterns
to extract the hypernym. (Hearst, 1992) If any pat-
tern matches the structure of the dependency parse
tree, the hypernym can be extracted. In the above
example, the following pattern is matched:
NN
It is NNnsubj cop nn
[target]
In this pattern, the rightmost leaf is the hyper-
nym target. Thus, we can extract the hypernym
?novel? from the previous example. The term
?novel? is the extracted hypernym of the English
article ?The Hunger Games?.
After extracting the hypernym of the English ar-
ticle, the hypernym is translated into Chinese. The
value of this feature in the SVM model is calcu-
lated as follows:
F
hypernym
(h) = log count(translated(h))
where h is the hypernym, translated(h) is the
Chinese translation of the term h.
English Title Occurrence Feature (ETO)
In a Baidu Baike article, the first sentence may
contain a parenthetical translation of the main ti-
tle. For example, the first sentence of the Chinese
588
article on San Francisco is ?????San Fran-
cisco????????????????????.
We regard the appearance of the English title in
the first sentence of a Baidu Baike article as a bi-
nary feature: If the English title appears in the first
sentence, the value of this feature is 1; otherwise,
the value is 0.
3 Evalutaion
3.1 Evaluation Dataset
In order to evaluate the performance of cross-
language article linking between English Wikiep-
dia and Chinese Baidu Baike, we compile
an English-Chinese evaluation dataset from
Wikipedia and Baidu Baike online encyclopedias.
First, our spider crawls the entire contents of En-
glish Wikipedia and Chinese Baidu Baike. Since
the two encyclopedias? article formats differ, we
copy the information in each article (title, content,
category, etc.) into a standardized XML structure.
In order to generate the gold standard evalua-
tion sets of correct English and Chinese article
pairs, we automatically collect English-Chinese
inter-language links from Wikipedia. For pairs
that have both English and Chinese articles, the
Chinese article title is regarded as the translation
of the English one. Next, we check if there is a
Chinese article in Baidu Baike with exactly the
same title as the one in Chinese Wikipedia. If
so, the corresponding English Wikipedia article
and the Baidu Baike article are paired in the gold
standard.
To evaluate the performance of our method on
linking different types of encyclopedia articles, we
compile a set containing the most popular articles.
We select the top 500 English-Chinese article pairs
with the highest page view counts in Baidu Baike.
This set represents the articles people in China are
most interested in.
Because our approach uses an SVM model, the
data set should be split into training and test sets.
For statistical generality, each data set is randomly
split 4:1 (training:test) 30 times. The final evalua-
tion results are calculated as the mean of the aver-
age of these 30 evaluation sets.
3.2 Evaluation Metrics
To measure the quality of cross-language entity
linking, we use the following three metrics. For
each English article queries, ten output Baidu
Baike candidates are generated in a ranked list. To
define the metrics, we use following notations: N
is the number of English query; r
i,j
is j-th correct
Chinese article for i-th English query; c
i,k
is k-th
candiate the system output for i-th English query.
Top-k Accuracy (ACC)
ACC measures the correctness of the first candi-
date in the candidate list. ACC = 1 means that all
top candidates are correctly linked (i.e. they match
one of the references), and ACC = 0 means that
none of the top candidates is correct.
ACC =
1
N
N
?
i=1
{
1 if ?r
i,j
: r
i,j
= c
i,k
0 otherwise
}
Mean Reciprocal Rank (MRR)
Traditional MRR measures any correct answer
produced by the system from among the candi-
dates. 1/MRR approximates the average rank of
the correct transliteration. An MRR closer to 1 im-
plies that the correct answer usually appears close
to the top of the n-best lists.
RR
i
=
{
min
j
1
j
if ?r
i,j
, c
i,k
: r
i,j
= c
i,k
0 otherwise
}
MRR =
1
N
?
N
i=1
RR
i
Recall
Recall is the fraction of the retrieved articles that
are relevant to the given query. Recall is used to
measure the performance of the candidate selec-
tion method. If the candidate selection method can
actually select the correct Chinese candidate, the
recall will be high.
Recall =
|relevant articles| ? |retrieved articles|
|relevant articles|
3.3 Evaluation Results
The overall results of our method achieves 80.95%
in MRR and 87.46% in recall. Figure 1 shows the
top-k ACC from the top 1 to 5. These results show
that our method is very effective in linking articles
in English Wikipedia to those in Baidu Baike.
In order to show the benefits of each feature
used in the SVM model, we conduct a experiment
to test the performance of different feature combi-
nations. Because title similarity of the articles is a
widely used method, we choose English and Chi-
nese title similarity as the baseline. Then, another
feature is added to each configuration until all the
features have been added. Table 2 shows the final
results of different feature combinations.
589
0.76	 ?
0.839	 ?
0.858	 ?
0.869	 ? 0.87	 ?
0.7	 ?
0.72	 ?
0.74	 ?
0.76	 ?
0.78	 ?
0.8	 ?
0.82	 ?
0.84	 ?
0.86	 ?
0.88	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
TopK	 ?
Figure 1: Top-k Accuracy
Level Configuration MRR
0 Baseline (BL) 0.6559
1
BL + MTM
?1
0.6967
?
BL + HT
?2
0.6975
?
BL + ETO
?3
0.6981
?
2
BL + MTM + HT 0.7703
?
BL + MTM + ETO 0.7558
?
BL + HT + ETO 0.7682
?
3 BL + MTM + HT + ETO 0.8095
?
?1
MTM: mix-language topic model
?2
HT: hypernym translation
?3
ETO: English title occurrence
?
This config. outperforms the best config. in last level with
statistically significant difference.
Table 2: MRRs of Feature Combinations
In the results, we can observe that mix-language
topic model, hypernym, and English title oc-
curence features all noticeably improve the perfor-
mance. Combining two of these three feature has
more improvement and the combination of all the
features achieves the best.
4 Discussion
Although our method can effectively generate
cross-language links with high accuracy, some
correct candidates are not ranked number one. Af-
ter examining the results, we can divide errors into
several categories:
The first kind of error is due to large literal dif-
ferences between the English and Chinese titles.
For example, for the English article ?Nero?, our
approach ranks the Chinese candidate ?????
(?King Nero?) as number one, instead of the cor-
rect answer ?????????????????
????? (the number two candidate). The title
of the correct Chinese article is the full name of
the Roman Emperor Nero (Nero Claudius Drusus
Germanicus). The false positive ????? is a his-
torical novel about the life of the Emperor Nero.
Because of the large difference in title lengths, the
value of the title similarity feature between the En-
glish article ?Nero? and the corresponding Chi-
nese article is low. Such length differences may
cause the SVM model to rank the correct answer
lower when the difference of other features are not
so significant because the contents of the Chinese
candidates are similar.
The second error type is caused by articles that
have duplicates in Baidu Baike. For example, for
the English article ?Jensen Ackles?, our approach
generates a link to the Chinese article ?Jensen?
in Baidu Baike. However, there is another Baidu
article ???????? (?Jensen Ackles?). These
two articles both describe the actor Jensen Ackles.
In this case, our approach still generates a correct
link, although it is not the one in the gold standard.
The third error type is translation errors. For ex-
ample, the English article ?Raccoon? is linked to
the Baidu article ??? (raccoon dog), though the
correct one is ???? (raccoon). The reason is that
Google Translate provides the translation ??? in-
stead of ????.
5 Conclusion
Cross-language article linking is the task of creat-
ing links between online encyclopedia articles in
different languages that describe the same content.
We propose a method based on article hypernym
and topic model to link English Wikipedia articles
to corresponding Chinese Baidu Baike articles.
Our method comprises two stages: candidate se-
lection and candidate ranking. We formulate can-
didate selection as a cross-language information
retrieval task based on the title similarity between
English and Chinese articles. In candidate rank-
ing, we employ several features of the articles in
our SVM model. To evaluate our method, we com-
pile a dataset from English Wikipedia and Baidu
Baike, containing the 500 most popular Baidu ar-
ticles. Evaluation results of our method show an
MRR of up to 80.95% and a recall of 87.46%. This
shows that our method is effective in generating
cross-language links between English Wikipedia
and Baidu Baike with high accuracy. Our method
does not heavily depend on linguistic characteris-
tics and can be easily extended to generate cross-
language article links among different encyclope-
dias in other languages.
590
References
Sisay Fissaha Adafre and Maarten de Rijke. 2005.
Discovering missing links in wikipedia. In Proceed-
ings of the 3rd international workshop on Link dis-
covery (LinkKDD ?05).
M. Beaulieu, M. Gatford, X. Huang, S. Robertson,
S. Walker, and P. Williams. 1997. Okapi at TREC-
5. In Proceedings of the fifth Text REtrieval Confer-
ence (TREC-5), pages 143?166.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3(4-5):993?1022.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics,
volume 2.
Paul McNamee, James Mayfield, Dawn Lawrie, Dou-
glas W Oard, and David S Doermann. 2011. Cross-
language entity linking. In Proceedings of Interna-
tional Joint Con-ference on Natural Language Pro-
cessing (IJCNLP), pages 255?263.
Hemant Misra, Olivier Cappe, and Franc?ois Yvon.
2008. Using lda to detect semantically incoherent
documents. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning
(CoNLL ?08).
Jong-Hoon Oh, Daisuke Kawahara, Kiyotaka Uchi-
moto, Jun?ichi Kazama, and Kentaro Torisawa.
2008. Enriching multilingual language re-
sources by discovering missing cross-language
links in wikipedia. In Proceedings of the 2008
IEEE/WIC/ACM International Conference on Web
Intelligence and Intelligent Agent Technology, vol-
ume 1, pages 322?328.
P`eter Sch?onhofen, Andr`as Bencz`ur, Istv`an B`?r`o, and
K`aroly Csalog`any. 2008. Cross-language retrieval
with wikipedia. Advances in Multilingual and
Multimodal Information Retrieval, Lecture Notes in
Computer Science, 5152:72?79.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of wikipedia-a
classification-based approach. In Proceedings of the
AAAI 2008 Workshop on Wikipedia and Artifical In-
telligence, pages 49?54.
Ling-Xiang Tang, In-Su Kang, Fuminori Kimura, Yi-
Hsun Lee, Andrew Trotman, Shlomo Geva, and Yue
Xu. 2013. Overview of the ntcir-10 cross-lingual
link discovery task. In Proceedings of the Tenth NT-
CIR Workshop Meeting.
Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie Tang.
2012. Cross-lingual knowledge linking across wiki
knowledge bases. In Proceedings of the 21st in-
ternational conference on World Wide Web (WWW
?12).
591
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 57?60,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
English-Korean Named Entity Transliteration Using Substring
Alignment and Re-ranking Methods
Chun-Kai Wu? Yu-Chun Wang? Richard Tzong-Han Tsai?
?Department of Computer Science and Engineering,
Yuan Ze University, Taiwan
?Department of Computer Science and Information Engineering,
National Taiwan University, Taiwan
s983301@mail.yzu.edu.tw d97023@csie.ntu.edu.tw
thtsai@saturn.yzu.edu.tw
Abstract
In this paper, we describe our approach
to English-to-Korean transliteration task in
NEWS 2012. Our system mainly consists
of two components: an letter-to-phoneme
alignment with m2m-aligner,and translitera-
tion training model DirecTL-p. We construct
different parameter settings to train several
transliteration models. Then, we use two re-
ranking methods to select the best transliter-
ation among the prediction results from the
different models. One re-ranking method is
based on the co-occurrence of the translitera-
tion pair in the web corpora. The other one is
the JLIS-Reranking method which is based on
the features from the alignment results. Our
standard and non-standard runs achieves 0.398
and 0.458 in top-1 accuracy in the generation
task.
1 Introduction
Named entity translation is a key problem in many
NLP research fields such as machine translation,
cross-language information retrieval, and question
answering. Most name entity translation is based on
transliteration, which is a method to map phonemes
or graphemes from source language into target lan-
guage. Therefore, named entity transliteration sys-
tem is important for translation.
In the shared task, we focus on English-Korean
transliteration. We consider to transform the translit-
eration task into a sequential labeling problem. We
adopt m2m-aligner and DirecTL-p (Jiampojamarn et
al., 2010) to do substring mapping and translitera-
tion predicting, respectively. With this approach (Ji-
ampojamarn et al, 2010) achieved promising results
on NEWS 2010 transliteration tasks. In order to im-
prove the transliteration performance, we also apply
several ranking techniques to select the best Korean
transliteration.
This paper is organized as following. In section
2 we describe the main approach we use including
how we deal with the data, the alignment and train-
ing methods and our re-ranking techniques. In sec-
tion 3, we show and discuss our results on English-
Korean transliteration task. And finally the conclu-
sion is in section 4.
2 Our Approach
In this section, we describe our approach for
English-Korean transliteration which comprises the
following steps:
1. Pre-processing
2. Letter-to-phoneme alignment
3. DirecTL-p training
4. Re-ranking results
2.1 Pre-processing
Korean writing system, namely Hangul, is alphabet-
ical. However, unlike western writing system with
Latin alphabets, Korean alphabet is composed into
syllabic blocks. Each Korean syllabic block repre-
sent a syllable which has three components: initial
consonant, medial vowel and optionally final con-
sonant. Korean has 14 initial consonants, 10 medial
vowels, and 7 final consonants. For instance, the syl-
labic block ??? (sin) is composed with three letters:
57
a initial consonant ??? (s), a medial vowel ??? (i),
and a final consonant ??? (n).
For transliteration from English to Korean , we
have to break each Korean syllabic blocks into two
or three Korean letters. Then, we convert these Ko-
rean letters into Roman letters according to Revised
Romanization of Korean for convenient processing.
2.2 Letter-to-phoneme Alignment
After obtaining English and Romanized Korean
name entity pair, we generate the alignment between
each pair by using m2m-aligner.
Since English orthography might not reflect its ac-
tual phonological forms, it makes one-to-one char-
acter alignment between English and Korean not
practical.
Compared with traditional one-to-one alignment,
the m2m-aligner overcomes two problems: One is
double letters where two letters are mapped to one
phoneme. English may use several characters for
one phoneme which is presented in one letter in Ko-
rean, such as ?ch? to ??? and ?oo? to ???. How-
ever, one-to-one alignment only allows one letter to
be mapped to one phoneme, so it must have to add
an null phoneme to achieve one-to-one alignment.
It may interfere with the transliteration prediction
model.
The other problem is double phonemes problem
where one letter is mapped to two phonemes. For
example, the letter ?x? in the English name entity
?Texas? corresponds to two letters ??? and ???
in Korean. Besides, some English letters in the
word might not be pronounced, like ?k? in the En-
glish word ?knight?. We can eliminate this by pre-
processing the data to find out double phonemes and
merge them into single phoneme. Or we can add
an null letter to it, but this may also disturb the pre-
diction model. While performing alignments, m2m
aligner allows us to set up the maximum length sub-
string in source language (with the parameter x) and
in target language (with the parameter y). Thus,
when aligning, we set both parameter x and y to two
because we think there are at most 2 English letters
mapped to 2 Korean letters. To capture more double
phonemes, we also have another parameter set with
x = 1 and y = 2.
As mentioned in previous section, Korean syl-
labic block is composed of three or two letters. In
order to cover more possible alignments, we con-
struct another alignment configurations to take null
consonant into consideration. Consequently, for any
Korean syllabic block containing two Korean letters
will be converted into three Roman letters with the
third one being a predefined Roman letter represent-
ing null consonant. We also have two set of param-
eters for this change, that is x = 2, y = 3 and x = 1
,y = 3. The reason we increase both y by one is that
there are three Korean letters for each word.
2.3 DirecTL-p Training
With aligned English-Korean pairs, we can train
our transliteration model. We apply DirecTL-p (Ji-
ampojamarn et al, 2008) for our training and testing
task. We train the transliteration models with differ-
ent alignment parameter settings individually men-
tioned in section 2.2.
2.4 Re-ranking Results
Because we train several transliteration models with
different alignment parameters, we have to combine
the results from different models. Therefore, the
re-ranking method is necessary to select the best
transliteration result. For re-ranking, we propose
two approaches.
1. Web-based re-ranking
2. JLIS-Reranking
2.4.1 Web-based re-ranking
The first re-ranking method is based on the oc-
currence of transliterations in the web corpora. We
send each English-Korean transliteration pair gen-
erated by our transliteration models to Google web
search engine to get the co-occurrence count of the
pair in the retrieval results. But the result number
may vary a lot, most of them will get millions of
results while some will only get a few hundred.
2.4.2 JLIS-Reranking
In addition to web-based re-ranking approach, we
also adopt JLIS-Reranking (Chang et al, 2010) to
re-rank our results for the standard run. For an
English-Korean transliteration pair, we can mea-
sure if they are actual transliteration of each other
by observing the alignment between them. Since
58
Table 1: Results on development data.
Run Accuracy Mean F-score MRR MAPref
1 (x = 2, y = 2) 0.488 0.727 0.488 0.488
2 (x = 1, y = 2) 0.494 0.730 0.494 0.494
3 (x = 1, y = 3, with null consonant) 0.452 0.713 0.452 0.452
4 (x = 2, y = 3, with null consonant) 0.474 0.720 0.474 0.473
Web-based Reranking 0.536 0.754 0.563 0.536
JLIS-Reranking 0.500 0.737 0.500 0.500
Table 2: Results on test data
Run Accuracy Mean F-score MRR MAPref
Standard (JLIS-Reranking) 0.398 0.731 0.398 0.397
Non-standard (Web-based reranking) 0.458 0.757 0.484 0.458
DirecTL-p model outputs a file containing the align-
ment of each result, there are some features in the
results that we can use for re-ranking. In our re-
ranking approach, there are three features used in
the process: source grapheme chain feature, target
grapheme chain feature and syllable consistent fea-
ture. These three feature are proposed in (Song et
al., 2010).
Source grapheme chain feature: This feature
can tell us that how the source characters are aligned.
Take ?A|D|A|M? for example, we will get three
chains which are A|D, D|A and A|M. With this fea-
ture we may know the alignment in the source lan-
guage.
Target grapheme chain feature: Similar to the
above feature, it tell us how the target characters are
aligned. Take ?NG:A:n|D|A|M? for example, which
is the Korean transliteration of ADAM, we will get
three chains which are n|D, D|A and A|M. With this
feature we may know the alignment in the target lan-
guage. ?n? is the predefined null consonant.
Syllable consistent feature: We use this feature
to measure syllable counts in both English and Ko-
rean. For English, we apply an Perl module1 to mea-
sure the syllable counts. And for Korean, we simply
count the number of syllabic blocks. This feature
may guard our results, since a wrong prediction may
not have the same number of syllable.
1http://search.cpan.org/?gregfast/
Lingua-EN-Syllable-0.251/Syllable.pm
Other than the feature vectors created by above
features, there is one important field when training
the re-ranker, performance measure. For this field,
we give it 1 when we predict a correct result other-
wise we give it 0 since we think it is useless to get a
partially correct result.
3 Result
To measure the transliteration models with different
alignment parameters and the re-ranking methods,
we construct several runs for experiments as follows.
? Run 1: m2m-aligner with parameters x = 2
and y = 2.
? Run 2: m2m-aligner with parameters x = 1
and y = 2.
? Run 3: m2m-aligner with parameters x = 1
and y = 3 and add null consonants in the Ko-
rean romanized representation.
? Run 4: m2m-aligner with parameters x = 2
and y = 3 and add null consonants in the Ko-
rean romanized representation.
? Web-based reranking: re-rank the results from
run 1 to 4 based on Google search results.
? JLIS-Reranking: re-rank the results from run 1
to 4 based on JLIS-rerakning features.
Table 1 shows our results on the development
data. As we can see in this table, Run 2 is better than
Run 1 by 6 NEs. It may be that the data in develop
59
set are double phonemes. And we also observe that
both Run 1 and Run 2 is better than Run 3 and Run
4, the reason may be that the extra null consonant
distract the performance of the prediction model.
From the results, it shows that our re-ranking
methods can actually improve transliteration.
Reranking based on web corpora can achieve better
accuracy compared with web-based reranking.
The JLIS-Reranking method slightly improve the
accuracy. It could be that the features we use
are not enough to capture the alignment between
English-Korean NE pair.
Because the runs with re-ranking achieving bet-
ter results, we submit the result on the test data with
JLIS-Reranking as the standard run, and the result
with the web-based re-ranking as the non-standard
run for our final results. The results on the test data
set are shown in table 2. The results also shows that
the web-based re-ranking can achieve the best accu-
racy up to 0.458.
4 Conclusion
In this paper, we describe our approach to English-
Korean named entity transliteration task for NEWS
2012. First, we decompose Korean word into Ko-
rean letters and then romanize them into sequential
Roman letters. Since a Korean word may not contain
the final consonant, we also create some alignment
results with the null consonant in romanized Korean
representations. After preprocessing the training
data, we use m2m-aligner to get the alignments from
English to Korean. Next, we train several translitera-
tion models based on DirecTL-p with the alignments
from the m2m-aligner. Finally, we propose two
re-ranking methods. One is web-based re-ranking
with Google search engine. We send the English
NE and its Korean transliteration pair our model
generates to Google to get the co-occurrence count
to re-rank the results. The other method is JLIS-
reranking based on three features from the alignment
results, including source grapheme chain feature,
target grapheme chain feature, and syllable consis-
tent feature. In the experiment results, our method
achieves the good accuracy up to 0.398 in the stan-
dard run and 0.458 in non-standard run. Our results
show that the transliteration model with a web-based
re-ranking method can achieve better accuracy in
English-Korean transliteration.
References
Ming-Wei Chang, Vivek Srikumar, Dan Goldwas-ser, and
Dan Roth. 2010. Structured output learning with indi-
rect supervision. Proceeding of the International Con-
ference on Machine Learning (ICML).
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. Association for Computational Linguistics,
pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. Association
for Computational Linguistics, pages 905?912.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and
Grzegorz Kondrak. 2010. Transliteration generation
and mining with limited training resources. Proceed-
ings of the 2010 Named Entities Workshop, ACL 2010,
pages 39?47.
Yan Song, Chunyu Kit, and Hai Zhao. 2010. Reranking
with multiple features for better transliteration. Pro-
ceedings of the 2010 Named Entities Work-shop, ACL
2010, pages 62?65.
60
