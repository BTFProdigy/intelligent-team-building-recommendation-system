Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 76?84,
Dublin, Ireland, August 23 2014.
Part-of-Speech Tag Disambiguation by Cross-Linguistic Majority Vote
No
?
emi Aepli
?
URPP Language and Space
University of Zurich
Ruprecht von Waldenfels
?
Institute of Computer Science
Polish Academy of Sciences
Tanja Samard?zi
?
c
?
URPP Language and Space
University of Zurich
Abstract
In this paper, we present an approach to developing resources for a low-resource language, taking
advantage of the fact that it is closely related to languages with more resources. In particular, we
test our approach on Macedonian, which lacks tools for natural language processing as well as
data in order to build such tools. We improve the Macedonian training set for supervised part-of-
speech tagging by transferring available manual annotations from a number of similar languages.
Our approach is based on multilingual parallel corpora, automatic word alignment, and a set
of rules (majority vote). The performance of a tagger trained on the improved data set of 88%
accuracy is significantly better than the baseline of 76%. It can serve as a stepping stone for
further improvement of resources for Macedonian. The proposed approach is entirely automatic
and it can be easily adapted to other language in similar circumstances.
1 Introduction
Developing natural language processing tools for various languages proves to be of great interest for both,
practical applications and linguistic research. Speakers of various languages and varieties increasingly
use social media to interact in their own varieties. To make use of these interactions as a relatively easily
accessible source of data, we need to be able to process different varieties automatically. However, a
great majority of languages of the world lack resources for natural language processing.
With a relatively small number of speakers and weak research infrastructure, Macedonian is one of
the languages lacking basic tools for natural language processing. On the other hand, this language is
in a convenient position in the sense that it is very similar to other Slavic languages for which more re-
sources are available. We can take advantage of this fact to automatise and facilitate creation of linguistic
resources necessary for building tools for automatic processing of Macedonian.
In this paper, we build a part-of-speech tagger for Macedonian. Part-of-speech tagging is a crucial
component in a natural language processing pipeline and it is a logical starting point in developing
resources for a new language. To obtain a good performance on this task, one needs a sufficiently large
corpus with manually annotated tags which can then be used to train a tagger. This is exactly the kind
of resource which is often missing (or not easily available) because its development is long, costly and
language specific. The current state of language technology allows us to automatise this process to a
large degree.
We improve a training set for Macedonian part-of-speech tagging by automatic projection of manual
annotation available in other languages. The basis of our method is automatic word alignment, which is
widely used in applications for machine translation.
Automatic word alignment has already been used for improving language resources and tools for part-
of-speech tagging in the context of supervised (Yarowsky et al., 2001) and unsupervised (Snyder et al.,
2008) learning. The success of these techniques strongly depends on the amount of available parallel
?
{noemi.aepli|tanja.samardzic}@uzh.ch
?
ruprecht.waldenfels@issl.unibe.ch
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
76
corpora for training models for both word alignment and part-of-speech tagging. It is also strongly influ-
enced by the limitations of automatic word alignment which often produces alignment errors, even if it
is trained on a large parallel corpus. Our approach to obtaining robust word alignment in a small corpus
available for Macedonian is to use a multiple parallel corpus of similar languages. Lexical similarity be-
tween the languages is expected to make word alignment easier than for unrelated languages. Combining
the information from different languages is expected to cancel out wrong alignments.
2 The Challenge of Developing Resources for Macedonian
Macedonian is an Indo-European language of the Slavic branch. It has around 1.7 million speakers.
1
It is one of the youngest Slavic standard languages, with most of its codification done after the formal
declaration of Macedonian as the official language of the Yugoslav Republic of Macedonia in 1944
(Friedman, 2001). Its closest relative is Bulgarian, with whose dialects the Macedonian dialects form a
continuum.
2.1 Linguistic Properties
Macedonian belongs to the ?Balkan Sprachbund?, a famous group of Balkan languages consisting of
three Slavic languages (Bulgarian, Macedonian, and some dialects of Serbian), one Romance language
(Romanian) and two Indo-European isolates (Greek, Albanian). The members of this group share impor-
tant structural features developed as a result of areal linguistic contact. The ?Sprachbund? features can
distinguish the languages belonging to the group from the other languages of the same genetical branch.
For example, the Slavic languages belonging to the group differ from all the other Slavic languages in
that they do not distinguish cases. To express grammatical relations expressed by case in other Slavic
languages, Macedonian and Bulgarian use prepositions (Tomi?c, 2006). This is an important property in
the context of our project because it influences the choice of the direction of automatic word alignment
across languages, as it will be shortly described in section 3.2. This property also influences our decision
to include in our data set English as the only non-Slavic language (as described in section 2.3).
2.2 Sparse Resources
As far as we know, there is no publicly available part-of-speech tagger for Macedonian at the moment
of writing. There are references to morphological resources developed using the NOOJ environment
(Ivanovska-Naskova, 2006; Silberstein, 2003). Also, some work on automatic morphological analy-
sis of Macedonian was done in the context of developing an open-source machine translation system
(Rangelov, 2011; Peradin and Tyers, 2012).
Most importantly for the current project, a morphologically annotated Macedonian translation of Or-
well?s 1984 was made available as part of the MULTEXT-East resources (Erjavec, 2012). The annotation
in this corpus, however, is incomplete. The main problem is that tokens are assigned all potential part-of-
speech tags without disambiguation. Multiple potential tags are assigned to 44,387 tokens, which makes
39% of the whole corpus. Another important problem is missing annotation. There are 4,810 tokens
(around 4%) for which there is no annotation at all. The proportion of 43% tokens which lack the crucial
information makes this corpus inadequate for training processing tools. To obtain an adequate training
set for Macedonian from this corpus, we add the missing information from other languages available in
the MULTEXT-East resources with more complete annotation.
2.3 The Overview of our Approach
We take parallel texts for Macedonian (MK), Bulgarian (BG), Czech (CZ), Slovene (SL), Serbian (SR)
and English (EN) from the MULTEXT-East corpus (see section 3.1). We select Bulgarian, Czech,
Slovene and Serbian as languages closely related to Macedonian. Since these languages are related,
they have similar lexicon, grammar and word order. As a result, it can be expected that many words
in a parallel text can be aligned as a one-to-one relation, with less cross-linguistic transformations and
reordering than in the case of distant languages. In addition to the Slavic languages we also include
1
https://www.ethnologue.com/language/mkd, 17.04.2014
77
English because of the fact that Macedonian differs from other Slavic languages (except Bulgarian) in
the use of cases. As mentioned above, Macedonian uses analytic prepositional phrases instead of Slavic
cases, which makes it closer to languages such as English in this respect.
For each of the five selected languages, manually disambiguated part-of-speech tags are available as
part of the MULTEXT-East resources. Moreover, the annotation in different languages can be automati-
cally aligned since the MULTEXT-East corpus consists of translations of the novel ?1984? into different
languages. All the texts are manually aligned at the level of sentence. Given the sentence alignment, we
automatically align Macedonian with the selected languages. We then use word alignments to transfer
automatically the annotation found in the other languages to Macedonian. As a next step, we put together
all the tags from all the languages, including the available Macedonian tags. This results in a set of part-
of-speech candidates for each Macedonian token. We choose the best candidate by a majority vote: the
most frequent tag in the set of candidates is chosen as the correct tag. This step relies on the intuition that
tags which end up in the candidate set by mistake will not be frequent because their distribution does not
depend on the token for which they are candidates. On the other hand, the tags which are truly related to
the token in question should be frequent in the set.
The five languages included in the study are not equally close to Macedonian. In addition to the most
related languages (Bulgarian and Serbian), we include the data from other Slavic languages (Czech and
Slovene) and English to deal with the noise caused by potentially wrong word alignments. We expect that
a correct word alignment is more likely to be found in an increased data set. On the other hand, including
more languages is not expected to introduce more noise. If word alignments with other languages are
wrong, they are not expected to result in repeated tags in the tag candidate set.
Although the general idea is rather intuitive and straightforward, actual realisation of the plan proved
technically not trivial. The main difficulty lies in combining word alignment with the original annotation
and in cross-linguistic mapping of the manual annotation.
To evaluate the results of the cross-linguistic disambiguation, we provide manual disambiguation for
a small section of the Macedonian corpus, which serves as the gold standard. To evaluate how useful
our cross-linguistic tag disambiguation is for automatic tagging, we train a tagger on the automatically
disambiguated corpus and test it on the portion for which we have provided the gold standard annotation.
In the following section, we describe in more detail the decisions taken at each step of our approach.
3 Materials and Methods
As shortly mentioned before, we work with the corpus of the MULTEXT-East resources (Erjavec, 2012),
?Multilingual Text Tools and Corpora for Central and Eastern European Languages?. The corpus con-
tains the novel ?1984? by George Orwell, annotated with part-of-speech tags and further morphosyntac-
tic specifications. It is a parallel corpus available in Macedonian, Bulgarian, Czech, English, Slovene,
Serbian and many more. Furthermore, the parallel texts are manually sentence-aligned. The Macedo-
nian corpus was only added in version 4 in 2010. It consists of 113,158 tokens corresponding to 6,790
sentences.
3.1 Multilingual Morphosyntactic Specifications
Morphosyntactic specifications are assigned manually to each token in the corpus. They are similar and
largely equivalent across the languages included in the resource, but they are not fully consistent.
Each morphosyntactic definition specifies a value for a number of categories. Each definition consists
of a string of characters, where each character specifies the value for one category. These strings can be
rather long for words for which many categories need to be encoded. For example, the tag #Vmia2s------
--e specifies a Macedonian verb form with 15 categories: 1) V for Verb, 2) main as type, 3) indicative as
the verb form, 4) aorist as tense, 5) 2nd person, 6) singular, and 7) perfective (e) as aspect. In between,
there are no specifications (-) for the subcategories 8) gender, 9) voice, and 10) negative, which could be
specified in Macedonian, but have no value in this specific case. Furthermore, there are five subcategories
which are not specified for Macedonian but only for other languages, they are marked with a dash too.
78
Detailed descriptions can be found on the web page of the MULTEXT-East resources.
2
We notice that the cross-linguistic mapping of the morphosyntactic definitions is more straightforward
towards the left-hand side of the definition than towards the right-hand side. For our purpose we only
consider the first two letters: the main category and its type (in this example Vm). We ignore the infor-
mation concerning the grammatical categories and reduce the morphosyntactic definitions to relatively
coarse part-of-speech tags.
There are 14 main categories (e.g. noun, verb, etc.). Each of these categories can be further specified
for the type, but not necessarily. All the combinations of the first two letters in the corpus give a tag set
which consists of 58 tags.
Even though morphosyntactic definitions are more consistent across languages for the first two than
for the subsequent characters, some variation is found in our tags too. The variations in the subcategories
are due to differences in the languages as well as different annotation strategies.
Table 1 shows the categories with the corresponding subcategory type across the languages we use.
The first and second column of table 1 specify the PoS category to which the types for the six languages
are specified. The possible values for the type of the category in one language are separated by a slash
(/). The dash (-) means that the type is not specified for that language. A missing entry shows that the
whole category is not specified for the language. We can see, for example, that there are three kinds
of adjectives in Macedonian: Af, As, and Ao. There are no types in Bulgarian, while the types in other
languages overlap with Macedonian only partially. The types which are found in other languages, but
not in Macedonian (e.g. Ag and Ap in Slovenian) cannot be transferred to Macedonian.
MK BG CS SL SR EN
N Noun c/p c/p c/p c/p c/p c/p
V Verb m/a/o m/a m/a/o/c m/a m/a/o/c m/a/o/b
A Adjective f/s/o - f/s g/s/p f/s/o f
P Pronoun p/d/i/s/q
r/x/z/g
p/d/i/s/q
r/x/z/g
p/d/i/s q/r/x p/s/d/r/x
g/q/i/z
p/d/i/s/q
r/x/z/g
p/s/q/r x/g/t
R Adverb g/a/v g/a g g/r g/z/a/v m/s
S Adposition p p p - p p/t
C Conjunction c/s c/s c/s c/s c/s c/s
M Number c/o/l/s c/o c/o/m/s c/o/p/s c/o/m/l/s c/o
I Interjection - - - - - -
Y Abbreviation - - n/r - n/r -
X Residual - - - f/t/p - -
Q Particle s/c z/g/c/v/q/o z/q/o/r - c/a/o/r
D Determiner d/i/s/g
T Article
Table 1: Cross-linguistic mapping of part-of-speech tags in our data set.
3.2 Automatic Word Alignment
The MULTEXT-East corpus contains manual sentence alignment for each language pair. We extract the
information about sentence alignment between Macedonian and the five languages included in our study.
Given the sentence alignment, we word align each of the parallel texts using GIZA++ (Och and Ney,
2003). As it is required by the input format for GIZA++, we remove sentence boundaries in the cases
where sentence alignment is not one-to-one. For example, if two English sentences are aligned with one
Macedonian sentence, we remove the boundary between the two English sentences. We then restore
the sentence boundaries in the alignment output so that we can identify the sentences in the original
annotated corpus and retrieve the annotation.
For each pair of languages, word alignment can be performed in two directions. One language is
considered as the source and the other as target. The choice of the alignment direction can have an
important influence on the resulting alignment (Och and Ney, 2003; Samard?zi?c and Merlo, 2010). The
influence of the alignment direction on the results follows from the formal definition of word alignment
2
http://nl.ijs.si/ME/, 24.06.2014
79
in the practical implementation. Since alignment is a single-valued function which assigns to each target
language word exactly one source language word, many-to-one alignments are only possible in one
direction: multiple target language words can be aligned with one source language word, but not the
other way around.
The performance of the programs for automatic word alignment is not perfect. To obtain more reliable
alignment, researchers usually take the intersection of both directions as the resulting alignment. This
technique yields very reliable alignments reaching a precision of 98.6%. However, since it allows only
one-to-one alignment, it necessarily leaves a good proportion of words unaligned (recall as low as 52.9%)
(Pad?o, 2007).
Since our corpus is small, we need to obtain as many word alignments as possible. Thus we do not use
the intersection of both alignments, but we use the full output of one alignment direction. It follows from
the formal definition of alignment that all target words need to be aligned, which necessarily increases
the recall, but potentially at the cost of precision.
To obtain a better precision, we choose the more suitable direction of alignment. Since the many-to-
one mappings are possible only from the target language to the source language, we choose the alignment
direction for each pair of languages so that the target language is the more analytic one. In all Slavic pairs,
Macedonian is the target, due to the fact that it uses analytic prepositional expressions where other Slavic
languages use single words in a particular case. In the pair English-Macedonian, the target language is
English, because its forms are more analytic than in Macedonian.
3.3 Combining Information from All Languages
Given the word alignment, we replace each word of the other languages (OL) which is aligned to a
Macedonian word with its corresponding part-of-speech tag retrieved from the original manually anno-
tated corpus. Table 2 illustrates the resulting data structure. The first column in the table is the sentence
ID, the second the Macedonian word. In the next columns the part-of-speech information is stored: first
the Macedonian tags and then the tags projected from other languages. Language code is given before
?#? and the full morphosyntactic definition found in the language in question after ?#?.
As it can be seen in Table 2, none, one, or several tags can be specified for each language. In the
first example, there is exactly one tag for every language. In the second example, the part-of-speech
information in English is missing because there was no alignment between the Macedonian word ????
and any English word. This is the case for all five other languages in the last example, where the tags are
specified only for Macedonian.
3
The third example shows the opposite, with one PoS tag for each other
language, but none for Macedonian.
ID Word MK PoS OL PoS
1.1.1.1 j???? ?clear? mk#Af bg#AM cs#Af en#Af sl#Ag sr#Af
1.1.1.2 ?? ?with? mk#Sp bg#SP cs#Rg en sl#Si sr#Sp
1.1.1.2 ??????? ?Winston? bg#Np cs#Np en#Np sl#Np sr#Np
2.7.2.3 ???? ?one? mk#C- mk#Rg mk#Mc bg#VM cs#Mc en#Di sl#Ap sr#Vm
1.1.11.2 ??? ?what? mk#Pq mk#Pr mk#C- mk#Q- mk#Rg mk#I bg cs en sl sr
Table 2: Macedonian text with PoS tags of aligned words of other languages
3.4 Choosing the Best Candidate
Having collected sets of possible tags for each Macedonian word, the next step is to choose the best tag.
The general idea is to take into consideration all the tags of all languages that are given for one word
and choose the most frequent of them as the correct tag for Macedonian. As the tags do not match
3
Note that alignments are not missing in the technical sense in the case of Slavic languages. According to the formal
definition of alignment discussed above, all Macedonian words need to be aligned in the direction that we chose. The fact that
there is no alignment in our data means that the Macedonian word is aligned with the special ?NULL? word in other Slavic
languages in this case. This special word is added to each sentence of each source language in the process of alignment, so that
the target language words for which there are no corresponding words in the source language can be aligned too.
80
completely (see section 3.1), the chosen tag has to be checked for validity. In other words, we check if
the most frequent tag is a valid tag for Macedonian according to the MULTEXT-East specifications.
For the task of choosing the best tag, we define a set of if-then rules. We apply an outer structure
of three if/else statements checking how many tags are given for Macedonian: one, zero or several. If
exactly one tag is given, we choose it as the best candidate. The latter two cases include further checks
taking into account the number of specified tags of the other languages (zero or several) as well as the
number of most frequent tags (the maximum). The former check is necessary because of the cases where
there are zero tags in Macedonian. If there are no tags in other languages either, we have to assign
a ?dummy tag?. The dummy tag is the most frequently occurring tag in the original annotation for
Macedonian. This is the Nc (common noun) tag in our case. The latter check, the number of maxima, is
done because more than one tag could have the same frequency. In cases where the competition between
the tags remains unresolved because of no matchings and/or sparse data, we reduce the tag to make it
less specific. We ignore the type, that is, the second letter of the tag, which leaves us with only the
category. Even this approach does not solve all the decision problems. If this is the case we have two
procedures: if there is no tag information coming from any language, we assign a ?dummy tag?. In the
second case, where we can not decide but we do have some information in Macedonian, we randomly
choose one of the given Macedonian tags. The cases in which we had to apply some additional heuristics
(comparing reduced tags, random choice and dummy tag) because there was not one single most frequent
tag constitute around 10%. The decision process for choosing the best candidate is given in more detail
in the pseudocode ?Algorithm 1?.
Consider, for example, the fourth entry in Table 2, ??????. There are three tags for Macedonian,
which means it satisfies the third condition of the outer if/else structure (more than 1 MK PoS tag). Next,
the most frequent tag considering all the given PoS tags of all the languages is searched. As described in
Section 3.1, we only take into account the first two letters (category and type) of a given morphosyntactic
definition. In this case, we have the following tags with the corresponding frequencies: (MC : 2), (VM
: 2), (C : 1), (AP : 1), (DI : 1), (RG : 1). Looking for the maximum, we find two tags with the same
frequency (2): MC and VM. Because there is more than one maximum, we check for each of the two
tags if they are identical to one of the Macedonian tags. In this case, the test is true for MC (cardinal
numeral). This is one of the maxima and one of the Macedonian tags, therefore the winner.
3.5 Training a Tagger
To asses whether disambiguating part-of-speech tags as described in the previous sections is useful for
training a statistical part-of-speech tagger, we divide our data set into a training and test portion. We train
a tagger on the training portion of the disambiguated corpus and we measure its performance on the test
set. We use the BTagger (Gesmundo and Samardzic, 2012), since it has good generalisation capacities,
which makes it suitable for small data sets. Furthermore, it does not need any manually constructed
morphological dictionaries and it can be used for any language.
4 Evaluation
To evaluate both our disambiguation method and the performance of the tagger on the disambiguated
corpus, we chose an arbitrary sample section of the corpus as the test set. The sample included 9,954
tokens (around 10% of the whole corpus), out of which 616 were missing annotation, and 3,231 were not
disambiguated. We manually add the missing tags and disambiguate the ambiguous ones. In this way,
we obtain the gold standard for the evaluation.
4.1 The baseline
We compare both, the success of our cross-linguistic disambiguation and the performance of the tagger
with a baseline. To define the baseline, we use a simple heuristic which allows us to disambiguate
Macedonian tags without cross-linguistic information: we take the first tag in the list as the correct one.
In the case of missing tags, we add NC (common noun), which is the most frequent tag in the corpus.
We run the tagger on the corpus disambiguated in this way, which gives us the baseline performance.
81
Algorithm 1 Find the best PoS-tag for an MK word given MK, BG, CS, EN, SL and SR tags
1: if number of MK-PoS-tags = 1 then
2: result? this MK-PoS-tag
3: else if number of MK-PoS-tags = 0 then
4:
5: if number of OL-PoS-tags = 0 then
6: result? dummy-tag
7: else if number of OL-PoS-tags > 0 then
8:
9: if 1 maximum then
10: result? maximum (? to be checked whether it is a valid MK-tag)
11: else if >1 maximum then
12: result? dummy-tag
13: end if
14: end if
15: else if number of MK-PoS-tags > 1 then
16:
17: if 1 maximum then
18:
19: if maximum = one of MK-PoS-tags then
20: result? maximum
21: else if reduced PoS-tag = one of MK-PoS-tags then
22: result?MK-PoS-tag with the same category like the maximum
23: else if maximum not in MK-PoS-tags then
24: result? random choice of available MK-PoS-tags
25: end if
26: else if > 1 maximum then
27:
28: for candidate in maxima do
29:
30: if candidate = one of MK-PoS-tags then
31: result? candidate
32: else if candidate not one of MK-PoS-tags then
33: reduce candidate to 1 letter
34: if reduced candidate = one of reduced MK-PoS-tags then
35: result? not-reduced MK-PoS-tags
36: else
37: result? random choice of available MK-PoS-tags
38: end if
39: end if
40: end for
41: else if number of OL-PoS-tags = 0 then
42: result? random choice of available MK-PoS-tags
43: end if
44: end if
82
4.2 Results and Discussion
Table 3 shows the accuracy of cross-linguistic disambiguation and tagging in comparison with the base-
line. The second column shows the agreement between manual disambiguation (the gold standard) and
automatic disambiguation in the two settings.
We can see that our simple heuristics alone provide some correct disambiguation. Roughly half of
the 43% of tags which are potentially wrong in the original corpus (because they are not disambiguated
or because they miss annotation) are correctly disambiguated by the baseline heuristics. This gives the
baseline disambiguation accuracy of 78%. Adding the information from other languages improves the
accuracy of automatic disambiguation to 87%.
Accuracy (%) Disambiguation BTagger
All 77
Baseline 78 Known 76
Unknown 77
Cross-linguistic All 88
Majority Vote 87 Known 88
Unknown 91
Table 3: The accuracy of disambiguation and tagging compared with the gold standard.
When trained on the corpus disambiguated in the baseline setting, the tagger?s accuracy is 77%, while
its accuracy is improved to 88% when it is trained on the corpus disambiguated using our cross-linguistic
majority vote.
It is important to note that the tagger?s performance improves more than the disambiguation accuracy
compared to the baseline (77% to 88% vs. 78% to 87%). The tagger outperforms the direct disambigua-
tion in the cross-linguistic setting. This means that eliminating wrong tags from the training set allows
the tagger not only to learn better correct tags, but also to come up with generalisations and provide a
more robust output. Although it assigns learned wrong tags to the words seen in the training set (accu-
racy on known words 88%), it uses the learned generalisations to predict more correct tags on the words
unseen in the training set (accuracy on unknown words 91%).
5 Conclusion
We have presented a method for improving resources in a new language using the existing resources
in similar languages and state-of-the art language technology. We evaluated our method as applied
to Macedonian, a low-resource Slavic language, closely related to other Slavic languages with more
available resources.
By cross-linguistic annotation projection, we improved the existing annotation, assigning the correct
tag to two thirds of potentially wrong part-of-speech tags in the original corpus. The performance of a
tagger trained on the disambiguated corpus reaches 88% accuracy. This is not a satisfying performance in
itself, but this tagger is the first trained and evaluated tool for Macedonian. Another important outcome of
our experiments is the fact that an improved training set allows a tagger to develop crucial generalisations
and to provide a more robust output. This finding can be useful for further improvement of the resources
not only in Macedonian, but in other low-resource languages too.
The presented approach to improving annotated language resources across languages is entirely
automatic. It can be applied to any other language in similar circumstances. Instead of repeating the
same kind of costly, time-consuming manual work in each new language, our approach makes use of
available annotations by transferring them automatically from one language to another.
Acknowledgements
The work presented in this paper is supported by the URPP Language and Space, University of Zurich
and the Swiss National Science Foundation. Training data annotation was co-financed by the Slavic
Institute of Bern University. Many thanks to Andrea Gesmundo for valuable comments and suggestions.
83
References
Toma?z Erjavec. 2012. MULTEXT-East: Morphosyntactic Resources for Central and Eastern European Lan-
guages. In Language Resources and Evaluation, volume 46, pages 131?142.
Victor A. Friedman, 2001. Facts About TheWorld?s Languages: An Encyclopedia of the World?s Major Languages,
Past and Present, chapter Macedonian, pages 435 ? 439. The H. W. Wilson Company New York and Dublin.
Andrea Gesmundo and Tanja Samardzic. 2012. Lemmatisation as a tagging task. In Proceedings of the 50th
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 368?372,
Jeju Island, Korea, July. Association for Computational Linguistics.
Ruska Ivanovska-Naskova. 2006. Development of the First LRs for Macedonian: Current Projects. In Proceed-
ings of the Fifth International Conference on Language Resources and Evaluation (LREC?06), pages 1837?
1841. European Language Resources Association (ELRA).
Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. In
Computational Linguistics, volume 29, pages 19?51.
Sebastian Pad?o. 2007. Cross-Lingual Annotation Projection Models for Role-Semantic Information. Ph.D. thesis,
Saarland University.
Hrvoje Peradin and Francis Tyers. 2012. A rule-based machine translation system from Serbo-Croatian to Mace-
donian. In Proceedings of the Workshop Free/Open-Source Rule-Based Machine Translation, pages 55 ? 62,
Gothenburg, Sweden.
Tihomir Rangelov. 2011. Rule-based machine translation between Bulgarian and Macedonian. Universitat Oberta
de Catalunya.
Tanja Samard?zi?c and Paola Merlo. 2010. Cross-lingual variation of light verb constructions: Using parallel corpora
and automatic alignment for linguistic research. In Proceedings of the 2010 Workshop on NLP and Linguistics:
Finding the Common Ground, pages 52?60, Uppsala, Sweden. Association for Computational Linguistics.
Max Silberstein. 2003. NooJ Manual. Available at www.nooj4nlp.net.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay. 2008. Unsupervised Multilingual
Learning for POS Tagging. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 1041?1050, Honolulu. Association for Computational Linguistics.
Olga Mi?seska Tomi?c. 2006. Balkan Sprachbund Morpho-syntactic Features. Springer, Dordrecht, The Nether-
lands.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via
robust projection across aligned corpora. In Proceedings of the 1st international conference Human Language
Technology, pages 161?168, San Diego, CA. Association for Computational Linguistics.
84
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 85?94,
Dublin, Ireland, August 23 2014.
Compilation of a Swiss German Dialect Corpus
and its Application to PoS Tagging
Nora Hollenstein
University of Zurich
hollenstein@cl.uzh.ch
No
?
emi Aepli
University of Zurich
noemi.aepli@uzh.ch
Abstract
Swiss German is a dialect continuum whose dialects are very different from Standard German,
the official language of the German part of Switzerland. However, dealing with Swiss German in
natural language processing, usually the detour through Standard German is taken. As writing in
Swiss German has become more and more popular in recent years, we would like to provide data
to serve as a stepping stone to automatically process the dialects. We compiled NOAH?s Corpus
of Swiss German Dialects consisting of various text genres, manually annotated with Part-of-
Speech tags. Furthermore, we applied this corpus as training set to a statistical Part-of-Speech
tagger and achieved an accuracy of 90.62%.
1 Introduction
Swiss German is not an official language of Switzerland, rather it includes dialects of Standard German,
which is one of the four official languages. However, it is different from Standard German in terms of
phonetics, lexicon, morphology and syntax. Swiss German is not dividable into a few dialects, in fact it is
a dialect continuum with a huge variety. Swiss German is not only a spoken dialect but increasingly used
in written form, especially in less formal text types. Often, Swiss German speakers write text messages,
emails and blogs in Swiss German. However, in recent years it has become more and more popular and
authors are publishing in their own dialect. Nonetheless, there is neither a writing standard nor an official
orthography, which increases the variations dramatically due to the fact that people write as they please
with their own style.
So far, there are almost no natural language processing (NLP) tools for Swiss German (Scherrer and
Owen, 2010). Considering the fact that the major part of communication between Swiss people of the
German part is in dialect, we would like to start building NLP tools for Swiss German dialects.
Furthermore, it is an attempt to deal with dialect varieties directly instead of taking the detour through
the standard of a language. Speakers of various dialects increasingly communicate through social media
in their own varieties. These interactions are relatively easily accessible and could be used as a source
of data. However, there is a lack of natural language processing tools for dialects, which need to be
developed first in order to process these data automatically.
We start with training a model for a Swiss German Part-of-Speech tagger, which is one of the first steps
dealing with the automatic processing of natural language. Based on a part-of-speech tagged corpus, fur-
ther processes like semantical analysis, syntactical parsing or even applications like machine translation
can be conducted.
In order to train a PoS tagger we need a corpus annotated with parts-of-speech. As such data does not
exist yet, we compiled NOAH?s Corpus of Swiss German Dialects containing Swiss German texts of dif-
ferent genres, and annotated it manually. This is an iterative process alternating between running/training
a PoS tagger and manually annotating/correcting the output. The corpus we present in this paper consists
of 73,616 manually annotated tokens covering many dialect variations of the German-speaking part of
Switzerland.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
85
In the next section, we will mention some related work before we will have a closer look at the Swiss
German dialects and its differences to Standard German in section 3. In section 4 we introduce our
corpus including the adapted tagset before we present the application of our corpus to the Part-of-Speech
tagging task in section 5.
2 Related Work
Most natural language processing applications focus on standardised, written language varieties, but
from a methodological as well as a practical point of view, it is interesting to develop NLP methods for
variational linguistics. Even though there are no other resources of this size and no studies on PoS tagging
for written Swiss German, there have been a few approaches which share some common aspects with our
work. While there are some corpora of spoken texts, such as the Archimob project (Dejung et al., 1999)
which comprises transcribed interviews, it is difficult to find resources to build a written Swiss German
corpus. One of the rare written resources is the sms4science project (D?urscheid and Stark, 2011), a
collection of text messages in all official languages of Switzerland as well as Swiss German dialects.
Concerning Part-of-Speech tagging for non-standard dialects, there are some approaches addressing
linguistic varieties in historical texts, Hinrichs and Zastrow (2012) and Rayson et al. (2007) for German
and English respectively. Furthermore, Diab (2009), Habash and Rambow (2009) and Duh and Kirchhoff
(2005) worked on PoS tagging for Arabic dialects. The latter developed a minimally supervised PoS
tagger for an Egyptian Arabic dialect, which does not have a standard orthography either, without using
any dialect-specific tools.
As far as Swiss German NLP goes, there are approaches to dialect identification (Scherrer and Owen,
2010), dialect machine translation (Scherrer, 2012) and morphology generation (Scherrer, 2013).
3 Swiss German
Swiss German belongs to the Alemannic group of dialects, a branch of the Germanic language family.
This group can be split into three linguistic divisions; Low, High and Highest Alemannic, each of which
contains a few regions of Switzerland. There is no strict border between the Swiss German dialects and
the other Alemannic dialects, rather it is referred to as a dialect continuum. Unlike the continuum among
Swiss German dialects, there is a strict separation between Swiss German and Standard German. When
it comes to the dialects of Swiss German, one can find the concept of diglossia. Diglossia is defined
as a situation in which two languages (or two varieties of the same language) are used under different
conditions within a language community. In the case of the German language, Standard German is used
in Switzerland nearly exclusively in written context while Swiss German is in daily use, mostly in spoken
form but also in informal written contexts (Siebenhaar and Wyler, 1997). However, this distinction is
becoming more and more blurred. Schools are one of a few environments where Standard German
is expected to be used in spoken language. Unlike the situation in other languages, it is standard in
Switzerland to use dialect even in formal situations. In Swiss media, both TV and radio, Swiss German
is well represented and commonly used.
With the introduction of emails, text messages, blogs and chats, Swiss German is taking over more
and more space in written contexts. Nowadays, especially for the younger generations, it is completely
normal to write in Swiss German. However, it is not limited to the private communication. In fact, it
is even becoming a cult status to write and publish in Swiss German. Many authors, among them for
example Lenz (2013), Schobinger (2014) and Kaiser (2012) write books in their dialect, and newspaper
agencies publish newspapers in Swiss German, e.g. Blick am Abend (Ringier AG, 2013, 2014). Even the
Swiss company Swatch has published their annual report 2012 in addition to Standard German, French
and English also Swiss German (The Swatch Group AG, 2012). This hype does not seem to cease, in
the contrary. Speaking a certain dialect is part of the identification. Swiss are proud of their dialect,
which makes it possible to identify their home region if they move to another canton. Despite the big
differences, speakers of various dialects usually understand each other, except a few German varieties of
the canton Valais which others usually have troubles understanding (Keller, 1961).
86
3.1 Differences to Standard German
Swiss German differs from Standard German in many aspects such as phonetics, lexicon, morphology
and syntax. One of the most significant differences is the vocabulary, which even introduces a new word
class not in use in Standard German (see section 4.2). In Swiss German, the Standard German words
are sometimes used in a different manner. For instance, in some cases the genus may change: the word
Radio (radio) as a masculine word (in Swiss German) instead of neutral (in Standard German). However,
there are not merely differences between Swiss and Standard German, but also between the different
dialectal regions. Scherrer (2011) differs between variations which apply for the whole Swiss German
speaking area and differences which appear only in certain dialects and not outside of Allemanic dialects.
The differences between the dialects are partly due to the influence from other languages. For instance
dialects closer to the French speaking part of Switzerland use different grammatical constructions than
Eastern Swiss dialects. In this section we describe some examples of disparities between the Swiss
German dialects and Standard German.
In Swiss German there is no preterite tense (?Pr?ateritum?) and the pluperfect (?Plusquamperfekt?)
is used extremely rarely. Both of them are expressed using the present perfect (?Perfekt?) or rather a
duplication of it (for an example see table 1). Another difference exists with regards to verb tenses
and the use of the auxiliary verbs sein (to be) and haben (to have). For instance, if you are cold, in
Switzerland you would say Ich ha chalt., where ha is the first person singular of ?to have?. However, to
express yourself in this situation in Standard German, the auxilary verb ?to be? is used: Mir ist kalt.
Furthermore, there is more freedom in the order of words of a sentence, especially concerning verbs
(for an example see table 1) as well as more possibilities to correctly arrange phrases. The overt specifi-
cation of the subject is another difference. In Swiss German the subject can be dropped in many cases,
the information about the person is then usually given in the conjugation of the verb. In the question
Chunnsch au? (Swiss German) vs. Kommst du auch? (Standard German) (Are you coming too?), the
subject du is not explicitly expressed in the Swiss German version but only in the second person singular
conjugation of the verb.
Regarding nouns, the four cases of Standard German (nominative, accusative, dative and genitive) are
not all in use in the dialects (Siebenhaar and Voegeli, 1997). Swiss German speakers generally neither
speak nor write in the genitive case, apart from a few exceptions e.g. in the dialect of the canton Valais.
The genitive is replaced by a possessive dative or a phrase using prepositions. This means, in order to
express the German phrase die Ohren des Hasen (the bunny?s ears), either the possessive dative am Haas
sini Ohr?a or a preposition d Ohr?a vom Haas (where vom is a fusion of an preposition von and an article
dem) is used. Moreover, nominative and accusative forms only differ in personal pronouns, whereas the
dative case, if used, is marked with its own determiner and endings for adjectives and nouns.
There are many phenomena, which are treated differently not only in regards to Standard German
but also in different dialects. First of all, the lexicon varies a lot. The variations do not only include
different pronunciation but also completely different words. For instance in some regions of Switzerland,
the Standard German word Butter (butter) is used (even though with a masculine article instead of the
feminine one, which is correct in Standard German). In other regions, however, different words such as
Anke are used instead. Another variation concerns the order of verbs if there is more than one of them in
a sentence. It is often inverted compared to Standard German, but this varies according to the dialect. To
express a final clause with um . . . zu (in order to) for instance, people in eastern Switzerland would use
the concatenation zum. Closer to the French speaking part though, the construction f?ur . . . z is commonly
used, which marks the similarity to the French pour . . . .
The following sentences in table 1 contain examples of both kinds of differences. On the one hand,
there are the Standard German preterite forms liess and hatte, which are expressed in the perfect tense
across dialects: hat . . . (gehen) lassen and hat gehabt. On the other hand, the order of the verbs in the
perfect construction (het gha vs. gha h?at) as well as the final clause with um . . . zu differs from dialect to
dialect.
Considering the way people write in Swiss German reveals another characteristic. The aforementioned
lack of a spelling standard causes variations not only between different authors but also within texts of
87
Dialect around Bern Si het ne la ga, w?u er ne gnue G?aud het gha, f ?ur es Billet z?l?ose.
Dialect around Zurich Si h
?
at ihn gah lah, wil er n?od gnueg G?ald gha h
?
at, zum es Billet l?ose.
Standard German Sie liess ihn gehen, weil er nicht genug Geld hatte, um ein Billet zu kaufen.
English She let him go because he did not have enough money to buy a ticket.
Table 1: Differences between dialects and Standard German
the same author. As people write how they speak, they are not consistent and may spell the same word
differently in the same sentence. They are also free to merge any words, which is quite common. Joining
words into compounds is not an unseen phenomena in Standard German either. However, a compound
is a word consisting of more than one stem, which can act as one word with one corresponding part-of-
speech (usually the one of the last part), e.g. Skilift (ski lift). In Swiss German, the process of merging
words rather resembles the phenomena of clitics, i.e. phonologically bound to another word (Loos et al.,
2004). For example g?ommer is Swiss German for gehen wir (we go). G?ommer can not be split into
verb and pronoun, as the separate occurrences would be g?ond (first person plural of to go) and mir (we).
Thus, such merged words are grammatically different words which, however, are phonologically bound
and can not stand alone. One phonological word (realised as one alphabetic string limited by white
spaces) can even contain the subject, an object and the finite verb of the sentence (see section 4.2 for an
example). This means it can not be assigned to one part-of-speech. In section 4.2 we present how we
deal with them in the part-of-speech tagging task.
To strengthen our argumentation for the necessity of a Swiss German PoS tagger we compare our
results of the training with our corpus with the performance of a Standard German tagger. We run the
German model of the most common tagger for Standard German, the TreeTagger (Schmid, 1995), on our
Swiss German test set. The tagger reaches an accuracy of 50.8%, which is significantly lower than the
result after the training with our corpus.
As we have shown in this section, the dialects of Swiss German differ in many aspects from Standard
German. It is not only a different pronounciation or spelling with some variations in the vocabulary.
It also involves syntactic differences and constructions which are ungrammatical when transferred to
German. Therefore we argue against a normalisation of Swiss German as a mapping to Standard German,
a frequently proposed approach dealing with varieties.
4 Corpus Creation
We compiled a Swiss German dialect corpus in order to provide resources to work with Swiss German.
Furthermore, we applied the corpus to the basic natural language processing task of Part-of-Speech
tagging as a first application. Therefore, we specified a tagset for Swiss German and annotated the
corpus according to this tagset.
4.1 NOAH?s Corpus of Swiss German Dialects
We present NOAH?s Corpus of Swiss German Dialects, a unique resource for Swiss German. We com-
piled a Swiss German corpus containing manually annotated part-of-speech tags of 73,616 tokens. As
the first annotated resource for written texts in Swiss German dialects, the goal is to cover various text
genres as well as different dialects from all regions of Switzerland. NOAH?s Corpus is freely available
for research.
1
In NOAH?s Corpus, we include articles from the Alemannic Wikipedia (Wikipedia, The Free Encyclo-
pedia, 2011) in five major dialects (Aarau, Basel, Bern, Zurich and the Eastern part of Switzerland) and a
Swiss German special edition of the newspaper ?Blick am Abend? (Ringier AG, 2013), which was pub-
lished in 2013. In addition, we added sections of the Swiss German dialect version of the official annual
report of the Swatch company from 2012 (The Swatch Group AG, 2012). Furthermore, we incorporated
extracts of novels from the Swiss author Viktor Schobinger (Viktor Schobinger, 2013) which are written
exclusively in the Zurich dialect. Finally, we also included three blogs from BlogSpot in various dialects
as a web resource. The detailed token quantities for each text source are shown in table 2.
1
http://www.cl.uzh.ch/research/downloads.html
88
Text source No. of tokens
Alemannic Wikipedia 20,135
Swatch Annual Report 2012 13,386
Novels from Viktor Schobinger 11,165
Newspaper articles 11,259
Blogs 17,671
Total 73,616
Table 2: Corpus composition
Manning (2011) suggests that the largest opportunities for improvement in part-of-speech tagging lies
in improving the tagset and the accuracy of annotation, even though a perfect annotation of words into
discrete lexical categories is not possible because some words do not fall clearly into one category. Thus,
since the consistency of annotations in natural language corpora is of great importance for PoS tagging
performance, we put great emphasis on the manual annotations. After the annotation of the corpus by
native speakers, various consistency checks were conducted. For instance, we checked words with low
probabilities in the tagging model and we also conducted random checks for cases of difficult tags.
4.2 Tagset
As the basic tagset we use the Stuttgart-T?ubingen-TagSet (STTS), which is the standard for German
(Schiller et al., 1999). Because of the differences between German and the Swiss German dialects we
additionally introduced the tag PTKINF as well as the adding of a ?+?-sign to any PoS tag.
The newly introduced tag PTKINF represents an infinitive particle suggested by Glaser (2003). It
is a commonly used and therefore widely analysed phenomenon for Swiss German dialects with no
corresponding word or construction in German. In Swiss German people say Ich go go poschte. (I?m
going shopping.). The second go corresponds to the finite verb gehen (to go) in the according Standard
German sentence Ich gehe einkaufen. The first go, however, does not exist in the Standard German
version. This particle is probably originally derived from gehen. However, as a particle it exceeds the
use in gehen (Glaser, 2003). This infinitive particle go (derived from gehen; to go) also comes in other
forms like for instance cho (derived from kommen; to come) and afa (probably derived from anfangen;
to begin). In our corpus we found 37 occurrences of this tag.
Furthermore, we introduce special tags for merged words. Since Swiss German does not have official
spelling rules, words can be freely joined. Splitting these words in a pre-processing step would be one
approach to deal with them. However, it is not always clear where to split them and would result in
strange words as the words phonologically assimilate when merged with others (see section 3.1). Also
Manning (2011) suggests that splitting tags seems to be largely a waste of time for the goal of improving
PoS tagging numbers.
Instead of splitting, we identify these merged words by using the corresponding STTS-tag for the first
part and add a plus sign to show that a given word consists of more than one simple word. There are
sequences of words that are commonly joined, but also less common combinations can appear as it de-
pends on the preferences of the writer. A commonly joined sequence is, for instance, VAFIN+PPER,
a personal pronoun attached to a finite auxiliary verb, e.g. hets for German hat es (there is). An ex-
ample for a less commonly joined sequence would be a concatenation of three different parts of speech
VVFIN+PIS+PPER such as bruchtmese for the German words braucht man sie (one uses/needs it). Fig-
ure 3 shows some more examples of the most frequent combinations (e.g. a verb, a conjunction or a
particle followed by a pronoun). We found 1008 occurrences of merged words, which represent 1.37%
of all tokens in the corpus.
The STTS-tagset already contains one tag that is a combination of two, namely the APPRART, con-
sisting of a preposition APPR and an article ART. This is used for words like beim, which is composed of
bei and dem. However, these are ?normal? Standard German prepositions. This is not the case with the
word combinations in Swiss German writing habits, where any words of completely different parts-of-
speech can be merged together. Using the approach of simply joining the corresponding part-of-speech
tags of the words like the APPRART-case, we would end up with an infinite tagset. Thus, the approach
89
PoS tag Swiss German Standard German English
VAFIN+ isches ist es is it
KOUS+ dasme dass man that one
VMFIN+ chame kann man can one
PTKZU+ zfl?ug?a zu fliegen to fly
ADV+ deetobe dort oben up there
Table 3: PoS tags for compound words
of adding a plus sign allows us to have a clearly defined tagset. Another advantage is that it is possible
to identify all the concatenated words easily, looking for PoS tags with a ?+?-sign attached. Once the list
of all occurrences is given, the corresponding tags can still be modified according to one?s requirements
for further processing in a text or corpus. Moreover, there is not a huge loss of information due to the
omitted part-of-speech information for the other word part(s). For many combinations it is very clear
which part of speech follows. Coming across a PTKZU+ for example, the only possibility for the second
part is a verb in the infinitive, a fact that can be inferred from the grammar.
5 Evaluation of PoS Tagging
In order to achieve the best results we trained different statistical, open source PoS taggers: TreeTagger
(Schmid, 1995), hunpos tagger (Hal?acsy et al., 2007), RFTagger (Schmid and Laws, 2008), Wapiti CRF
Tagger (Lavergne et al., 2010), TnT (Trigrams?n?Tags) tagger (Brants, 2000) and BTagger (Gesmundo
and Samard?zi?c, 2012). The BTagger and the TnT tagger reach the best results for our corpus, therefore
we did a more detailed evaluation of the tagging results based on these two taggers.
5.1 Results
We evaluated the performance of the BTagger and the TnT tagger over our corpus with 10-fold cross
validation. The folds we created are non-stratified, i.e. not contiguous sentences. This is because our
corpus consists of diverse kinds of text. If we train the tagger on the whole corpus with diverse kinds
of text and then evaluate only on blogs for instance, we will not get a fair result. Thus, in order to get
balanced test sets, we chose the sentence for the 10 folds randomly. With the whole corpus as training
set, we reach an accuracy of 90.62% with the BTagger and 90.14% with the TnT tagger (see table 4).
Considering the 26.36% unknown tokens in average over all test sets, the accuracy for the unknown
tokens is surprisingly high.
Accuracy BTagger TnT tagger
Unknown tokens 77.99% 72.39%
Known tokens 93.34% 93.26%
Overall 90.62% 90.14%
Table 4: Accuracy of taggers over the whole corpus
As stated in section 4.1, our corpus contains texts from different genres. Therefore we additionally
evaluated the different text genres individually. The results are shown in table 5. The Wikipedia articles
score best with 90.92% accuracy. This is due to the fact that it is the biggest part of the corpus with
20,135 tokens (one third). In addition, the amount of unknown words is not as high as in other texts
because the variety of different words is limited to one topic per article. The literary texts are on the
second place. This corpus part is only half of the size of the Wikipedia articles. However, the texts are all
extracted from the criminal novels of Viktor Schobinger. This means, they are written in one dialect by
one person, which reduces the number of orthographic varieties and thus the number of unknown tokens.
As table 5 shows, the novels have only 16% of unknown tokens, less than all the other parts.
Furthermore, we analysed the relation between the size of the corpus and the accuracy we achieved
(see figure 1). In the case of Swiss German we found that the accuracy increases significantly until
approximately 40,000 tokens. Increasing the size of the corpus beyond this amount of tokens is helpful
90
Accuracy Accuracy Accuracy Number of
Text type overall unknown tokens known tokens unknown tokens
Wikipedia articles 90.92% 75.64% 94.60% 22.7%
Literary texts (novels) 89.37% 70.41% 92.89% 16.0%
Annual report 88.82% 76.95% 92.72% 24.7%
Blogs 88.10% 71.69% 91.73% 18.2%
Newspaper articles 87.17% 71.19% 93.15% 27.4%
Table 5: Results for the different text genres with the BTagger
to cover a larger amount of orthographic varieties and reducing the number of unknown words, but does
not considerably improve the accuracy of known tokens.
Another fact that stands out in figure 1 is the difference of the tagger performances for a training set
of 10,000 tokens. This is due to the fact that that the BTagger makes use of context information and
thus emphasises the transition probability by learning sequences of tags. Therefore, not a huge amount
of data is needed to get a comparably good performance (Gesmundo and Samard?zi?c, 2012). The TnT
tagger, on the other hand, emphasises the emission probability and does not generalise as well.
Figure 1: Relation between PoS tagging accuracy and corpus size for the TnT tagger (grey line) and the
slightly better results from the BTagger (black line).
In section 3.1, discussing the differences between Standard German and Swiss German, we argue
that Standard German tools are not capable of dealing with Swiss German dialects. As an additional
experiment we extend our Swiss German corpus with a Standard German corpus to see if the addition of
information of Standard German data improves the result. We combined our Swiss German corpus with
the T?uBa-D/Z German Treebank (Telljohann et al., 2006), which contains more than 1,300,000 tokens.
The results on a 10-fold cross validation reached an accuracy of 87.6% which is lower than the results
for the Swiss German corpus by itself. This implies that the addition of Standard German training data
to our Swiss German corpus is not helpful for the training of a Swiss German PoS tagger.
91
5.2 Error Analysis
The most frequent errors were the confusion of nouns (NN) and proper names (NE), which represent
ca. 15% of all errors. This is also a common problem for Standard German due to the capitalisation of
nouns. The different kinds of adjectives and the adverbs as well as various types of verbs are also often
mistaken, but these are confusions inside one part-of-speech category. Furthermore, there are many
mistakes between articles and some types of pronouns, especially personal and demonstrative. However,
this is not surprising as they often have the same form. For example the German indefinite article ein is
often realised as es in Swiss German, the definite article das as s. The Swiss German es also stands for
the German neutral personal pronoun es if it is not abbreviated to s. This issue is exemplified in table 6.
PoS tag Swiss German example Standard German English
ART (definite) es Buech ein Buch a book
ART (indefinite) s Buech das Buch the book
PPER Es isch rot. Es ist rot. It is red.
PPER S r?agnet. Es regnet. It is raining.
Table 6: Example of the same types with different PoS tags and meanings
5.3 Discussion & Future Work
We achieved reasonable PoS tagging results for the Swiss German dialects considering the low amount
of available resources. As stated in section 3, we are dealing with a dialect continuum missing an orthog-
raphy standard. We neither select one specific dialect (or region) of Switzerland nor do we normalise the
data in any way. Thus, our data contains a high amount of hapax legomena, i.e. words which only appear
once. This fact explains the considerably lower accuracy for unknown tokens compared to taggers for
standardised languages. Furthermore, we include different sources and different text genres in one cor-
pus, which does not simplify the work for a statistical PoS tagger. Thus, it is conceivable that accuracy
improvements may be achieved by concentrating on one particular dialect.
In future work we will enlarge NOAH?s Corpus of Swiss German Dialects by including more texts
per dialect in order to reduce the number of unknown tokens. Another approach we are pursuing is
to develop a procedure based on lexical distance measures and syntactical patterns in order to map the
different orthographic version of a token, so that the tagger can benefit from these mappings. This
procedure may also serve as a starting point towards the lemmatisation of Swiss German texts.
The goal of improving Part-of-Speech tagging for Swiss German as well as extending the corpus is to
enable and facilitate the development of further NLP tasks, such as dependency parsing, opinion mining
or deeper dialectology studies.
6 Conclusion
We have presented our work on compiling a corpus of Swiss German dialects and its application to the
training of a Part-of-Speech tagger. As a first resource, our corpus is a stepping stone for natural language
processing for the Swiss German dialect area. Training the BTagger on our corpus results in an accuracy
of 90.62%. With little post processing effort on the tagger output, a PoS-annotated corpus for Swiss
German can be obtained and thus resources extended.
NOAH?s Corpus of Swiss German Dialects contains 73,616 tokens from texts of different genres in
different dialects, manually annotated with PoS tags. We are happy to share it with interested parties.
The corpus including the PoS tags can be downloaded in XML format.
Acknowledgements
We are grateful to the Institute of Computational Linguistics of the University of Zurich for their support.
We would like to thank Martin Volk and Simon Clematide for valuable comments and suggestions.
Furthermore, many thanks to Tanja Samard?zi?c for inputs concerning the PoS taggers and David Klaper
for providing some of the raw data for the corpus.
92
References
Thorsten Brants. TnT: a statistical part-of-speech tagger. In Proceedings of the sixth conference on Ap-
plied natural language processing, pages 224?231. Association for Computational Linguistics, 2000.
Christof Dejung, Thomas Gull, and Tanja Wirz. Landigeist und Judenstempel: Erinnerungen einer
Generation 19301945. Limmat Verlag, 1999.
Mona Diab. Second generation AMIRA tools for Arabic processing: Fast and robust tokenization, POS
tagging, and base phrase chunking. In 2nd International Conference on Arabic Language Resources
and Tools, 2009.
Kevin Duh and Katrin Kirchhoff. POS tagging of dialectal Arabic: a minimally supervised approach. In
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 55?62.
Association for Computational Linguistics, 2005.
Christa D?urscheid and Elisabeth Stark. SMS4science: An international corpus-based texting project and
the specific challenges for multilingual Switzerland. Digital Discourse: Language in the New Media,
pages 299?320, 2011.
Andrea Gesmundo and Tanja Samard?zi?c. Lemmatisation as a tagging task. In Proceedings of the 50th
Annual Meeting of the Association for Computational Linguistics, pages 368?372. ACL, 2012.
Elvira Glaser. Schweizerdeutsche Syntax: Ph?anomene und Entwicklungen. In Beat Dittli, An-
nelies H?acki Buhofe, and Walter Haas, editors, G?ommer MiGro?, pages 39?66, Freiburg, Schweiz,
2003.
Nizar Habash and Owen Rambow. Arabic tokenization, part-of-speech tagging and morphological dis-
ambiguation in one fell swoop. In Proceedings of the 43rd Annual Meeting on Association for Com-
putational Linguistics, 2009.
P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz. HunPos - an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions, pages 209?212, Prague, Czech Republic, 2007.
Erhard Hinrichs and Thomas Zastrow. Linguistic annotations for a diachronic corpus of German. In
Proceedings of the 10th Workshop on Treebanks and Linguistic Theories, Heidelberg, 2012.
Renato Kaiser. UUFPASS
?
A, N
?
OD AAPASS
?
A! Der gesunde Menschenversand, 2012.
R.E. Keller. German dialects: phonology and morphology, with selected texts. Manchester University
Press, 1961.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. Practical Very Large Scale CRFs. In Proceedings
the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513,
Uppsala, Sweden, July 2010. Association for Computational Linguistics.
Pedro Lenz. I bi meh aus eine. Cosmos Verlag AG, 2013.
Eugene Loos, Susan Anderson, Day Dwight, Paul Jordan, and Douglas Wingate. Glossary of linguis-
tic terms. http://www-01.sil.org/linguistics/GlossaryOfLinguisticTerms/WhatIsACliticGrammar.htm,
2004.
Christopher D. Manning. Part-of-speech tagging from 97% to 100%: is it time for some linguistics?
Computational Linguistics and Intelligent Text Processing, pages 171?189, 2011.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan Culpeper, and Nicholas Smith. Tagging the Bard:
Evaluating the accuracy of a modern POS tagger on Early Modern English corpora. 2007.
Ringier AG. Blick am Abig. http://epaper.blick.ch/webreader/baa/download/?doc=BAA280513ZH, May
2013.
Ringier AG. Blick am Abig. http://epaper.blick.ch/webreader/baa/download/?doc=BAA020614ZH, June
2014.
93
Yves Scherrer. Syntactic transformations for Swiss German dialects. In First Workshop on Algorithms
and Resources for Modelling of DIalects and Language Vareities, Edinburgh, 2011. EMNLP.
Yves Scherrer. Machine translation into multiple dialects: The example of Swiss German. 7th SIDG
Congress - Dialect 2.0, 2012.
Yves Scherrer. Continuous variation in computational morphology - the example of Swiss German. In
TheoreticAl and Computational MOrphology: New Trends and Synergies (TACMO), Gen`eve, Suisse,
2013. 19th International Congress of Linguists. URL http://hal.inria.fr/hal-00851251.
Yves Scherrer and Rambow Owen. Natural Language Processing for the Swiss German Dialect Area.
In Proceedings of the Conference on Natural Language Processing (KONVENS), pages 93?102,
Saarbr?ucken, Germany, 2010.
Anne Schiller, Simone Teufel, Christine St?ockert, and Christine Thielen. Guidelines f?ur das Taging
deutscher Textkorpora mit STTS, August 1999.
Helmut Schmid. Improvements in Part-of-Speech Tagging with an Application to German. In Proceed-
ings of the ACL SIGDAT-Workshop, Dublin, 1995.
Helmut Schmid and Florian Laws. Estimation of Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. COLING, 2008.
Viktor Schobinger. Der
?
A?aschmen und de scht`u`urzmord. Schobinger-Verlaag, 2014.
Beat Siebenhaar and Walter Voegeli. 6 Mundart und Hochdeutsch im Vergleich. In Mundart und
Hochdeutsch im Unterricht. Orientierungshilfen f?ur Lehrer, 1997.
Beat Siebenhaar and Alfred Wyler. Dialekt und Hochsprache in der deutschsprachigen Schweiz. 1997.
Heike Telljohann, Erhard W. Hinrichs, Sandra K?ubler, Heike Zinsmeister, and Kathrin Beck. Stylebook
for the T?ubingen Treebank of Written German (T?uBa-D/Z). Technical report, Universit?at T?ubingen,
2006.
The Swatch Group AG. Swatch Group Gesch?aftsbericht 2012. http://www.swatchgroup.com/de/investor
relations/jahres und halbjahresberichte/fruehere jahres und halbjahresberichte, 2012.
Viktor Schobinger. Viktor?s z?urit?u(?u)tsch. http://www.zuerituetsch.ch/index.html, 2013.
Wikipedia, The Free Encyclopedia. Alemannic Wikipedia. http://als.wikipedia.org/wiki/Wikipedia:
Houptsyte, 2011.
94
