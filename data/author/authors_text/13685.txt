Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 958?967,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bootstrapping Semantic Analyzers from Non-Contradictory Texts
Ivan Titov Mikhail Kozhevnikov
Saarland University
Saarbru?cken, Germany
{titov|m.kozhevnikov}@mmci.uni-saarland.de
Abstract
We argue that groups of unannotated texts
with overlapping and non-contradictory
semantics represent a valuable source of
information for learning semantic repre-
sentations. A simple and efficient infer-
ence method recursively induces joint se-
mantic representations for each group and
discovers correspondence between lexical
entries and latent semantic concepts. We
consider the generative semantics-text cor-
respondence model (Liang et al, 2009)
and demonstrate that exploiting the non-
contradiction relation between texts leads
to substantial improvements over natu-
ral baselines on a problem of analyzing
human-written weather forecasts.
1 Introduction
In recent years, there has been increasing inter-
est in statistical approaches to semantic parsing.
However, most of this research has focused on su-
pervised methods requiring large amounts of la-
beled data. The supervision was either given in
the form of meaning representations aligned with
sentences (Zettlemoyer and Collins, 2005; Ge and
Mooney, 2005; Mooney, 2007) or in a some-
what more relaxed form, such as lists of candidate
meanings for each sentence (Kate and Mooney,
2007; Chen and Mooney, 2008) or formal repre-
sentations of the described world state for each
text (Liang et al, 2009). Such annotated resources
are scarce and expensive to create, motivating the
need for unsupervised or semi-supervised tech-
niques (Poon and Domingos, 2009). However,
unsupervised methods have their own challenges:
they are not always able to discover semantic
equivalences of lexical entries or logical forms or,
on the contrary, cluster semantically different or
even opposite expressions (Poon and Domingos,
2009). Unsupervised approaches can only rely on
distributional similarity of contexts (Harris, 1968)
to decide on semantic relatedness of terms, but this
information may be sparse and not reliable (Weeds
and Weir, 2005). For example, when analyzing
weather forecasts it is very hard to discover in an
unsupervised way which of the expressions among
?south wind?, ?wind from west? and ?southerly?
denote the same wind direction and which are not,
as they all have a very similar distribution of their
contexts. The same challenges affect the problem
of identification of argument roles and predicates.
In this paper, we show that groups of unanno-
tated texts with overlapping and non-contradictory
semantics provide a valuable source of informa-
tion. This form of weak supervision helps to
discover implicit clustering of lexical entries and
predicates, which presents a challenge for purely
unsupervised techniques. We assume that each
text in a group is independently generated from
a full latent semantic state corresponding to the
group. Importantly, the texts in each group do
not have to be paraphrases of each other, as they
can verbalize only specific parts (aspects) of the
full semantic state, yet statements about the same
aspects must not contradict each other. Simulta-
neous inference of the semantic state for the non-
contradictory and semantically overlapping docu-
ments would restrict the space of compatible hy-
potheses, and, intuitively, ?easier? texts in a group
will help to analyze the ?harder? ones.1
As an illustration of why this weak supervi-
sion may be valuable, consider a group of two
non-contradictory texts, where one text mentions
?2.2 bn GBP decrease in profit?, whereas another
one includes a passage ?profit fell by 2.2 billion
pounds?. Even if the model has not observed
1This view on this form of supervision is evocative of co-
training (Blum and Mitchell, 1998) which, roughly, exploits
the fact that the same example can be ?easy? for one model
but ?hard? for another one.
958
Current?temperature?is?about?70F,
Cith?high?of?around?75F?amd?low
of?around?64u
Overcast,
Rain?is?quite?possible?tonight,
as?t-storms?arru
South?wind?of?around?19?mph.
Cu
u r euA?slight?chance?of?showers?
Mostly?cloudy,??
with?a?high?near?75.
South?wind?between?15?and?20?mph,
Chance?of?precipitation?is?30%.
with?gusts?as?high?as?30?mph.
and?thunderstorms?after?noon.
Thunderstorms?and?pouring?are?possiblr
throughout?the?day,
with?precipitation?chance?of?about?25%.
possibly?growing?up?to?75?F?during?the?day,
as?south?wind?blows?at?about?20?mphu
The?sky?is?heavyu
It?is?70?F?now,
erntr?mep?raiesnrabao70F,anshabaogfanmdaba5lfanrmhaba5w6
Csh4Os?iesnrbo70Ffnv4r bc6
Rpqeiesnrbo70Ffanshbwfanmdb0-fanrmhb0l6
t?rSst1verhesm9iesnrbo70Ffnshb0wfnmdb.0fnrmhb 0o6
eAph4r?MAmhSriesnrbo70Ffnv4rbSAmhSr6
y?rr2shR3mshMAmhSriesnrbF57.wfnv4rb776
q9rreMAmhSriesnrb%o70F%fnv4rb776
qTkSvIr?iesnrbo70FfpST reb5l7Fww6
Csh4ctrr4iesnrbo70F,an shbFgfnmdb00fnrmhbF-fapSTrebFw70w6
?mshMAmhSriesnrbo70Ffnv4rbSAmhSr6
Csh4MAs99iesnrbo70Ffnshbwfnmdbwfnrmhbw6
aaaaaaaaaaaaaaaauuuuuu
Figure 1: An example of three non-contradictory weather forecasts and their alignment to the semantic
representation. Note that the semantic representation (the block in the middle) is not observable in
training.
the word ?fell? before, it is likely to align these
phrases to the same semantic form because of sim-
ilarity of their arguments. And this alignment
would suggest that ?fell? and ?decrease? refer to
the same process, and should be clustered together.
This would not happen for the pair ?fell? and ?in-
crease? as similarity of their arguments would nor-
mally entail contradiction. Similarly, in the exam-
ple mentioned earlier, when describing a forecast
for a day with expected south winds, texts in the
group can use either ?south wind? or ?southerly?
to indicate this fact but no texts would verbalize
it as ?wind from west?, and therefore these ex-
pressions will be assigned to different semantic
clusters. However, it is important to note that the
phrase ?wind from west? may still appear in the
texts, but in reference to other time periods, un-
derlying the need for modeling alignment between
grouped texts and their latent meaning representa-
tion.
As much of the human knowledge is re-
described multiple times, we believe that non-
contradictory and semantically overlapping texts
are often easy to obtain. For example, consider
semantic analysis of news articles or biographies.
In both cases we can find groups of documents re-
ferring to the same events or persons, and though
they will probably focus on different aspects and
have different subjective passages, they are likely
to agree on the core information (Shinyama and
Sekine, 2003). Alternatively, if such groupings are
not available, it may still be easier to give each se-
mantic representation (or a state) to multiple an-
notators and ask each of them to provide a tex-
tual description, instead of annotating texts with
semantic expressions. The state can be communi-
cated to them in a visual or audio form (e.g., as
a picture or a short video clip) ensuring that their
interpretations are consistent.
Unsupervised learning with shared latent se-
mantic representations presents its own chal-
lenges, as exact inference requires marginalization
over possible assignments of the latent semantic
state, consequently, introducing non-local statisti-
cal dependencies between the decisions about the
semantic structure of each text. We propose a sim-
ple and fairly general approximate inference algo-
rithm for probabilistic models of semantics which
is efficient for the considered model, and achieves
favorable results in our experiments.
In this paper, we do not consider models
which aim to produce complete formal meaning
of text (Zettlemoyer and Collins, 2005; Mooney,
2007; Poon and Domingos, 2009), instead focus-
ing on a simpler problem studied in (Liang et al,
2009). They investigate grounded language ac-
quisition set-up and assume that semantics (world
state) can be represented as a set of records each
consisting of a set of fields. Their model seg-
ments text into utterances and identifies records,
fields and field values discussed in each utter-
ance. Therefore, one can think of this problem as
an extension of the semantic role labeling prob-
lem (Carreras and Marquez, 2005), where predi-
cates (i.e. records in our notation) and their ar-
guments should be identified in text, but here ar-
guments are not only assigned to a specific role
(field) but also mapped to an underlying equiv-
alence class (field value). For example, in the
weather forecast domain field sky cover should get
the same value given expressions ?overcast? and
?very cloudy? but a different one if the expres-
959
sions are ?clear? or ?sunny?. This model is hard
to evaluate directly as text does not provide in-
formation about all the fields and does not neces-
sarily provide it at the sufficient granularity level.
Therefore, it is natural to evaluate their model
on the database-text alignment problem (Snyder
and Barzilay, 2007), i.e. measuring how well the
model predicts the alignment between the text and
the observable records describing the entire world
state. We follow their set-up, but assume that in-
stead of having access to the full semantic state
for every training example, we have a very small
amount of data annotated with semantic states and
a larger number of unannotated texts with non-
contradictory semantics.
We study our set-up on the weather forecast
data (Liang et al, 2009) where the original textual
weather forecasts were complemented by addi-
tional forecasts describing the same weather states
(see figure 1 for an example). The average overlap
between the verbalized fields in each group of non-
contradictory forecasts was below 35%, and more
than 60% of fields are mentioned only in a single
forecast from a group. Our model, learned from
100 labeled forecasts and 259 groups of unanno-
tated non-contradictory forecasts (750 texts in to-
tal), achieved 73.9% F1. This compares favorably
with 69.1% shown by a semi-supervised learning
approach, though, as expected, does not reach the
score of the model which, in training, observed se-
mantics states for all the 750 documents (77.7%
F1).
The rest of the paper is structured as follows.
In section 2 we describe our inference algorithm
for groups of non-contradictory documents. Sec-
tion 3 redescribes the semantics-text correspon-
dence model (Liang et al, 2009) in the context of
our learning scenario. In section 4 we provide an
empirical evaluation of the proposed method. We
conclude in section 5 with an examination of ad-
ditional related work.
2 Inference with Non-Contradictory
Documents
In this section we will describe our inference
method on a higher conceptual level, not speci-
fying the underlying meaning representation and
the probabilistic model. An instantiation of the
algorithm for the semantics-text correspondence
model is given in section 3.2.
Statistical models of parsing can often be re-
garded as defining the probability distribution of
meaning m and its alignment a with the given
text w, P (m,a,w) = P (a,w|m)P (m). The
semantics m can be represented either as a logical
formula (see, e.g., (Poon and Domingos, 2009)) or
as a set of field values if database records are used
as a meaning representation (Liang et al, 2009).
The alignment a defines how semantics is verbal-
ized in the text w, and it can be represented by
a meaning derivation tree in case of full semantic
parsing (Poon and Domingos, 2009) or, e.g., by
a hierarchical segmentation into utterances along
with an utterance-field alignment in a more shal-
low variation of the problem. In semantic parsing,
we aim to find the most likely underlying seman-
tics and alignment given the text:
(m?, a?) = argmax
m,a
P (a,w|m)P (m). (1)
In the supervised case, where a and m are observ-
able, estimation of the generative model parame-
ters is generally straightforward. However, in a
semi-supervised or unsupervised case variational
techniques, such as the EM algorithm (Demp-
ster et al, 1977), are often used to estimate the
model. As common for complex generative mod-
els, the most challenging part is the computation
of the posterior distributions P (a,m|w) on the
E-step which, depending on the underlying model
P (m,a,w), may require approximate inference.
As discussed in the introduction, our goal is to
integrate groups of non-contradictory documents
into the learning procedure. Let us denote by
w1,...,wK a group of non-contradictory docu-
ments. As before, the estimation of the poste-
rior probabilities P (mi,ai|w1 . . .wK) presents
the main challenge. Note that the decision about
mi is now conditioned on all the texts wj rather
than only on wi. This conditioning is exactly what
drives learning, as the information about likely se-
mantics mj of text j affects the decision about
choice of mi:
P (mi|w1,...,wK) ?
?
ai
P (ai,wi|mi)?
?
?
m?i,a?i
P (mi|m?i)P (m?i,a?i,w?i), (2)
where x?i denotes {xj : j 6= i}. P (mi|m?i)
is the probability of the semantics mi given all
the meanings m?i. This probability assigns zero
weight to inconsistent meanings, i.e. such mean-
960
ings (m1,...,mK) that ?Ki=1mi is not satisfiable,
2
and models dependencies between components in
the composite meaning representation (e.g., argu-
ments values of predicates). As an illustration, in
the forecast domain it may express that clouds, and
not sunshine, are likely when it is raining. Note,
that this probability is different from the probabil-
ity that mi is actually verbalized in the text.
Unfortunately, these dependencies between mi
and wj are non-local. Even though the dependen-
cies are only conveyed via {mj : j 6= i} the space
of possible meaningsm is very large even for rela-
tively simple semantic representations, and, there-
fore, we need to resort to efficient approximations.
One natural approach would be to use a form
of belief propagation (Pearl, 1982; Murphy et al,
1999), where messages pass information about
likely semantics between the texts. However, this
approach is still expensive even for simple mod-
els, both because of the need to represent distribu-
tions over m and also because of the large number
of iterations of message exchange needed to reach
convergence (if it converges).
An even simpler technique would be to parse
texts in a random order conditioning each mean-
ing m?k for k ? {1,...,K} on all the previous se-
mantics m?<k = m
?
1,...,m
?
k?1:
m?k = argmax
mk
P (wk|mk)P (mk|m
?
<k).
Here, and in further discussion, we assume that
the above search problem can be efficiently solved,
exactly or approximately. However, a major weak-
ness of this algorithm is that decisions about com-
ponents of the composite semantic representation
(e.g., argument values) are made only on the ba-
sis of a single text, which first mentions the cor-
responding aspects, without consulting any future
texts k? > k, and these decisions cannot be revised
later.
We propose a simple algorithm which aims to
find an appropriate order of the greedy inference
by estimating how well each candidate semantics
m?k would explain other texts and at each step se-
lecting k (and m?k) which explains them best.
The algorithm, presented in figure 23, con-
structs an ordering of texts n = (n1,..., nK)
2Note that checking for satisfiability may be expensive or
intractable depending on the formalism.
3We slightly abuse notation by using set operations with
the lists n and m? as arguments. Also, for all the document
indices j we use j /? S to denote j ? {1,...,K}\S.
1: n := (), m? := ()
2: for i := 1 : K ? 1 do
3: for j /? n do
4: m?j := argmaxmj P (mj ,wj |m
?)
5: end for
6: ni := argmaxj /?n P (m?j ,wj |m
?)?
?
?
k/?n?{j}maxmk P (mk,wk|m
?, m?j)
7: m?i := m?ni
8: end for
9: nK := {1,...,K}\n
10: m?K := argmaxmnK P (mnK ,wnK |m
?)
Figure 2: The approximate inference algorithm.
and corresponding meaning representations m? =
(m?1,...,m
?
K), where m
?
k is the predicted mean-
ing representation of text wnk . It starts with an
empty ordering n = () and an empty list of mean-
ings m? = () (line 1). Then it iteratively pre-
dicts meaning representations m?j conditioned on
the list of semantics m? = (m?1,...,m
?
i?1) fixed
on the previous stages and does it for all the re-
maining texts wj (lines 3-5). The algorithm se-
lects a single meaning m?j which maximizes the
probability of all the remaining texts and excludes
the text j from future consideration (lines 6-7).
Though the semantics mk (k /? n?{j}) used in
the estimates (line 6) can be inconsistent with each
other, the final list of meanings m? is guaranteed
to be consistent. It holds because on each iteration
we add a single meaning m?ni to m
? (line 7), and
m?ni is guaranteed to be consistent with m
?, as the
semantics m?ni was conditioned on the meaning
m? during inference (line 4).
An important aspect of this algorithm is that un-
like usual greedy inference, the remaining (?fu-
ture?) texts do affect the choice of meaning rep-
resentations made on the earlier stages. As soon
as semantics m?k are inferred for every k, we find
ourselves in the set-up of learning with unaligned
semantic states considered in (Liang et al, 2009).
The induced alignments a1,...,aK of semantics
m? to texts w1,...,wK at the same time induce
alignments between the texts. The problem of pro-
ducing multiple sequence alignment, especially in
the context of sentence alignments, has been ex-
tensively studied in NLP (Barzilay and Lee, 2003).
In this paper, we use semantic structures as a pivot
for finding the best alignment in the hope that pres-
ence of meaningful text alignments will improve
the quality of the resulting semantic structures by
enforcing a form of agreement between them.
961
3 A Model of Semantics
In this section we redescribe the semantics-text
correspondence model (Liang et al, 2009) with an
extension needed to model examples with latent
states, and also explain how the inference algo-
rithm defined in section 2 can be applied to this
model.
3.1 Model definition
Liang et al (2009) considered a scenario where
each text was annotated with a world state, even
though alignment between the text and the state
was not observable. This is a weaker form of
supervision than the one traditionally considered
in supervised semantic parsing, where the align-
ment is also usually provided in training (Chen and
Mooney, 2008; Zettlemoyer and Collins, 2005).
Nevertheless, both in training and testing the
world state is observable, and the alignment and
the text are conditioned on the state during infer-
ence. Consequently, there was no need to model
the distribution of the world state. This is differ-
ent for us, and we augment the generative story by
adding a simplistic world state generation step.
As explained in the introduction, the world
states s are represented by sets of records (see the
block in the middle of figure 1 for an example of
a world state). Each record is characterized by a
record type t ? {1,..., T}, which defines the set of
fields F (t). There are n(t) records of type t and
this number may change from document to docu-
ment. For example, there may be more than a sin-
gle record of type wind speed, as they may refer
to different time periods but all these records have
the same set of fields, such as minimal, maximal
and average wind speeds. Each field has an asso-
ciated type: in our experiments we consider only
categorical and integer fields. We write s(t)n,f = v
to denote that n-th record of type t has field f set
to value v.
Each document k verbalizes a subset of the en-
tire world state, and therefore semantics mk of
the document is an assignment to |mk| verbalized
fields: ?|mk|q=1 (s
(tq)
nq ,fq
= vq), where tq, nq, fq are
the verbalized record types, records and fields, re-
spectively, and vq is the assigned field value. The
probability of meaning mk then equals the prob-
ability of this assignment with other state vari-
ables left non-observable (and therefore marginal-
ized out). In this formalism checking for con-
tradiction is trivial: two meaning representations
Figure 3: The semantics-text correspondence
model with K documents sharing the same latent
semantic state.
contradict each other if they assign different val-
ues to the same field of the same record.
The semantics-text correspondence model de-
fines a hierarchical segmentation of text: first, it
segments the text into fragments discussing differ-
ent records, then the utterances corresponding to
each record are further segmented into fragments
verbalizing specific fields of that record. An exam-
ple of a segmented fragment is presented in fig-
ure 4. The model has a designated null-record
which is aligned to words not assigned to any
record. Additionally there is a null-field in each
record to handle words not specific to any field.
In figure 3 the corresponding graphical model is
presented. The formal definition of the model for
documents w1,...,wK sharing a semantic state is
as follows:
? Generation of world state s:
? For each type ? ? {1,..., T} choose a number of
records of that type n(?) ? Unif(1,..., nmax).
? For each record s(?)n , n ? {1, .., n(?)} choose
field values s(?)nf for all fields f ? F
(?) from the
type-specific distribution.
? Generation of the verbalizations, for each document
wk, k ? {1,...,K}:4
? Record Types: Choose a sequence of verbalized
record types t = (t1,..., t|t|) from the first-order
Markov chain.
? Records: For each type ti choose a verbalized
record ri from all the records of that type: l ?
Unif(1,..., n(?)), ri := s
(ti)
l .
? Fields: For each record ri choose a sequence of
verbalized fields f i = (fi1,..., fi|fi|) from the
first-order Markov chain (fij ? F (ti)).
? Length: For each field fij , choose length cij ?
Unif(1,..., cmax).
? Words: Independently generate cij words from
the field-specific distribution P (w|fij , rifij ).
4We omit index k in the generative story and figure 3 to
simplify the notation.
962
Figure 4: A segmentation of a text fragment into records and fields.
Note that, when generating fields, the Markov
chain is defined over fields and the transition pa-
rameters are independent of the field values rifij .
On the contrary, when drawing a word, the distri-
bution of words is conditioned on the value of the
corresponding field.
The form of word generation distributions
P (w|fij , rifij ) depends on the type of the field
fi,j . For categorical fields, the distribution of
words is modeled as a distinct multinomial for
each field value. Verbalizations of numerical fields
are generated via a perturbation on the field value
rifij : the value rifij can be perturbed by either
rounding it (up or down) or distorting (up or down,
modeled by a geometric distribution). The param-
eters corresponding to each form of generation are
estimated during learning. For details on these
emission models, as well as for details on model-
ing record and field transitions, we refer the reader
to the original publication (Liang et al, 2009).
In our experiments, when choosing a world
state s, we generate the field values independently.
This is clearly a suboptimal regime as often there
are very strong dependencies between field val-
ues: e.g., in the weather domain many record
types contain groups of related fields defining min-
imal, maximal and average values of some param-
eter. Extending the method to model, e.g., pair-
wise dependencies between field values is rela-
tively straightforward.
As explained above, semantics of a textm is de-
fined by the assignment of state variables s. Anal-
ogously, an alignment a between semantics m
and a text w is represented by all the remaining
latent variables: by the sequence of record types
t = (t1,..., t|t|), choice of records ri for each ti,
the field sequence f i and the segment length cij
for every field fij .
3.2 Learning and inference
We select the model parameters ? by maximiz-
ing the marginal likelihood of the data, where
the data D is given in the form of groups w =
{w1,...,wK} sharing the same latent state:5
max
?
?
w?D
?
s
P (s)
?
k
?
r,f ,c
P (r,f , c,wk|s, ?).
To estimate the parameters, we use the
Expectation-Maximization algorithm (Dempster
et al, 1977). When the world state is observ-
able, learning does not require any approxima-
tions, as dynamic programming (a form of the
forward-backward algorithm) can be used to in-
fer the posterior distribution on the E-step (Liang
et al, 2009). However, when the state is latent,
dependencies are not local anymore, and approxi-
mate inference is required.
We use the algorithm described in section 2 (fig-
ure 2) to infer the state. In the context of the
semantics-text correspondence model, as we dis-
cussed above, semantics m defines the subset of
admissible world states. In order to use the algo-
rithm, we need to understand how the conditional
probabilities of the form P (m?|m) are computed,
as they play the key role in the inference proce-
dure (see equation (2)). If there is a contradiction
(m??m) then P (m?|m) = 0, conversely, if m?
is subsumed by m (m ? m?) then this proba-
bility is 1. Otherwise, P (m?|m) equals the prob-
ability of new assignments ?|m
?\m|
q=1 (s
(t?q)
n?q ,f ?q
= v?q)
(defined by m?\m) conditioned on the previously
fixed values of s (given by m). Summarizing,
when predicting the most likely semantics m?j
(line 4), for each span the decoder weighs alter-
natives of either (1) aligning this span to the pre-
viously induced meaning m?, or (2) aligning it to
a new field and paying the cost of generation of its
value.
The exact computation of the most probable se-
mantics (line 4 of the algorithm) is intractable, and
we have to resort to an approximation. Instead
of predicting the most probable semantics m?j we
search for the most probable pair (a?j , m?j), thus
assuming that the probability mass is mostly con-
centrated on a single alignment. The alignment aj
5For simplicity, we assume here that all the examples are
unlabeled.
963
is then discarded and not used in any other compu-
tations. Though the most likely alignment a?j for
a fixed semantic representation m?j can be found
efficiently using a Viterbi algorithm, computing
the most probable pair (a?j , m?j) is still intractable.
We use a modification of the beam search algo-
rithm, where we keep a set of candidate meanings
(partial semantic representations) and compute an
alignment for each of them using a form of the
Viterbi algorithm.
As soon as the meaning representations m? are
inferred, we find ourselves in the set-up studied
in (Liang et al, 2009): the state s is no longer
latent and we can run efficient inference on the
E-step. Though some fields of the state s may
still not be specified by m?, we prohibit utterances
from aligning to these non-specified fields.
On the M-step of EM the parameters are es-
timated as proportional to the expected marginal
counts computed on the E-step. We smooth the
distributions of values for numerical fields with
convolution smoothing equivalent to the assump-
tion that the fields are affected by distortion in the
form of a two-sided geometric distribution with
the success rate parameter equal to 0.67. We use
add-0.1 smoothing for all the remaining multino-
mial distributions.
4 Empirical Evaluation
In this section, we consider the semi-supervised
set-up, and present evaluation of our approach on
on the problem of aligning weather forecast re-
ports to the formal representation of weather.
4.1 Experiments
To perform the experiments we used a subset
of the weather dataset introduced in (Liang et
al., 2009). The original dataset contains 22,146
texts of 28.7 words on average, there are 12
types of records (predicates) and 36.0 records per
forecast on average. We randomly chose 100
texts along with their world states to be used as
the labeled data.6 To produce groups of non-
contradictory texts we have randomly selected a
subset of weather states, represented them in a vi-
sual form (icons accompanied by numerical and
6In order to distinguish from completely unlabeled exam-
ples, we refer to examples labeled with world states as la-
beled examples. Note though that the alignments are not ob-
servable even for these labeled examples. Similarly, we call
the models trained from this data supervised though full su-
pervision was not available.
symbolic parameters) and then manually anno-
tated these illustrations. These newly-produced
forecasts, when combined with the original texts,
resulted in 259 groups of non-contradictory texts
(650 texts, 2.5 texts per group). An example of
such a group is given in figure 1.
The dataset is relatively noisy: there are incon-
sistencies due to annotation mistakes (e.g., number
distortions), or due to different perception of the
weather by the annotators (e.g., expressions such
as ?warm? or ?cold? are subjective). The overlap
between the verbalized fields in each group was
estimated to be below 35%. Around 60% of fields
are mentioned only in a single forecast from a
group, consequently, the texts cannot be regarded
as paraphrases of each other.
The test set consists of 150 texts, each corre-
sponding to a different weather state. Note that
during testing we no longer assume that docu-
ments share the state, we treat each document in
isolation. We aimed to preserve approximately the
same proportion of new and original examples as
we had in the training set, therefore, we combined
50 texts originally present in the weather dataset
with additional 100 newly-produced texts. We an-
notated these 100 texts by aligning each line to one
or more records,7 whereas for the original texts the
alignments were already present. Following Liang
et al (2009) we evaluate the models on how well
they predict these alignments.
When estimating the model parameters, we fol-
lowed the training regime prescribed in (Liang et
al., 2009). Namely, 5 iterations of EM with a basic
model (with no segmentation or coherence mod-
eling), followed by 5 iterations of EM with the
model which generates fields independently and,
at last, 5 iterations with the full model. Only
then, in the semi-supervised learning scenarios,
we added unlabeled data and ran 5 additional it-
erations of EM.
Instead of prohibiting records from crossing
punctuation, as suggested by Liang et al (2009),
in our implementation we disregard the words not
attached to specific fields (attached to the null-
field, see section 3.1) when computing spans of
records. To speed-up training, only a single record
of each type is allowed to be generated when run-
ning inference for unlabeled examples on the E-
7The text was automatically tokenized and segmented into
lines, with line breaks at punctuation characters. Information
about the line breaks is not used during learning and infer-
ence.
964
P R F1
Supervised BL 63.3 52.9 57.6
Semi-superv BL 68.8 69.4 69.1
Semi-superv, non-contr 78.8 69.5 73.9
Supervised UB 69.4 88.6 77.9
Table 1: Results (precision, recall and F1) on the
weather forecast dataset.
step of the EM algorithm, as it significantly re-
duces the search space. Similarly, though we pre-
served all records which refer to the first time pe-
riod, for other time periods we removed all the
records which declare that the corresponding event
(e.g., rain or snowfall) is not expected to happen.
This preprocessing results in the oracle recall of
93%.
We compare our approach (Semi-superv, non-
contr) with two baselines: the basic supervised
training on 100 labeled forecasts (Supervised BL)
and with the semi-supervised training which disre-
gards the non-contradiction relations (Semi-superv
BL). The learning regime, the inference proce-
dure and the texts for the semi-supervised baseline
were identical to the ones used for our approach,
the only difference is that all the documents were
modeled as independent. Additionally, we report
the results of the model trained with all the 750
texts labeled (Supervised UB), its scores can be
regarded as an upper bound on the results of the
semi-supervised models. The results are reported
in table 1.
4.2 Discussion
Our training strategy results in a substantially
more accurate model, outperforming both the su-
pervised and semi-supervised baselines. Surpris-
ingly, its precision is higher than that of the model
trained on 750 labeled examples, though admit-
tedly it is achieved at a very different recall level.
The estimation of the model with our approach
takes around one hour on a standard desktop PC,
which is comparable to 40 minutes required to
train the semi-supervised baseline.
In these experiments, we consider the problem
of predicting alignment between text and the cor-
responding observable world state. The direct
evaluation of the meaning recognition (i.e. se-
mantic parsing) accuracy is not possible on this
dataset, as the data does not contain information
which fields are discussed. Even if it would pro-
value top words
0-25 clear, small, cloudy, gaps, sun
25-50 clouds, increasing, heavy, produce, could
50-75 cloudy, mostly, high, cloudiness, breezy
75-100 amounts, rainfall, inch, new, possibly
Table 2: Top 5 words in the word distribution for
field mode of record sky cover, function words and
punctuation are omitted.
vide this information, the documents do not ver-
balize the state at the necessary granularity level
to predict the field values. For example, it is not
possible to decide to which bucket of the field sky
cover the expression ?cloudy? refers to, as it has a
relatively uniform distribution across 3 (out of 4)
buckets. The problem of predicting text-meaning
alignments is interesting in itself, as the extracted
alignments can be used in training of a statisti-
cal generation system or information extractors,
but we also believe that evaluation on this prob-
lem is an appropriate test for the relative compar-
ison of the semantic analyzers? performance. Ad-
ditionally, note that the success of our weakly-
supervised scenario indirectly suggests that the
model is sufficiently accurate in predicting seman-
tics of an unlabeled text, as otherwise there would
be no useful information passed in between se-
mantically overlapping documents during learning
and, consequently, no improvement from sharing
the state.8
To confirm that the model trained by our ap-
proach indeed assigns new words to correct fields
and records, we visualize top words for the field
characterizing sky cover (table 2). Note that the
words ?sun?, ?cloudiness? or ?gaps? were not ap-
pearing in the labeled part of the data, but seem to
be assigned to correct categories. However, cor-
relation between rain and overcast, as also noted
in (Liang et al, 2009), results in the wrong assign-
ment of the rain-related words to the field value
corresponding to very cloudy weather.
5 Related Work
Probably the most relevant prior work is an ap-
proach to bootstrapping lexical choice of a gen-
eration system using a corpus of alternative pas-
8We conducted preliminary experiments on synthetic data
generated from a random semantic-correspondence model.
Our approach outperformed the baselines both in predicting
?text?-state correspondence and in the F1 score on the pre-
dicted set of field assignments (?text meanings?).
965
sages (Barzilay and Lee, 2002), however, in their
work all the passages were annotated with un-
aligned semantic expressions. Also, they as-
sumed that the passages are paraphrases of each
other, which is stronger than our non-contradiction
assumption. Sentence and text alignment has
also been considered in the related context of
paraphrase extraction (see, e.g., (Dolan et al,
2004; Barzilay and Lee, 2003)) but this prior
work did not focus on inducing or learning se-
mantic representations. Similarly, in information
extraction, there have been approaches for pat-
tern discovery using comparable monolingual cor-
pora (Shinyama and Sekine, 2003) but they gener-
ally focused only on discovery of a single pattern
from a pair of sentences or texts.
Radev (2000) considered types of potential rela-
tions between documents, including contradiction,
and studied how this information can be exploited
in NLP. However, this work considered primarily
multi-document summarization and question an-
swering problems.
Another related line of research in machine
learning is clustering or classification with con-
straints (Basu et al, 2004), where supervision is
given in the form of constraints. Constraints de-
clare which pairs of instances are required to be
assigned to the same class (or required to be as-
signed to different classes). However, we are not
aware of any previous work that generalized these
methods to structured prediction problems, as triv-
ial equality/inequality constraints are probably too
restrictive, and a notion of consistency is required
instead.
6 Summary and Future Work
In this work we studied the use of weak supervi-
sion in the form of non-contradictory relations be-
tween documents in learning semantic represen-
tations. We argued that this type of supervision
encodes information which is hard to discover in
an unsupervised way. However, exact inference
for groups of documents with overlapping seman-
tic representation is generally prohibitively expen-
sive, as the shared latent semantics introduces non-
local dependences between semantic representa-
tions of individual documents. To combat it, we
proposed a simple iterative inference algorithm.
We showed how it can be instantiated for the
semantics-text correspondence model (Liang et
al., 2009) and evaluated it on a dataset of weather
forecasts. Our approach resulted in an improve-
ment over the scores of both the supervised base-
line and of the traditional semi-supervised learn-
ing.
There are many directions we plan on inves-
tigating in the future for the problem of learn-
ing semantics with non-contradictory relations. A
promising and challenging possibility is to con-
sider models which induce full semantic represen-
tations of meaning. Another direction would be
to investigate purely unsupervised set-up, though
it would make evaluation of the resulting method
much more complex. One potential alternative
would be to replace the initial supervision with a
set of posterior constraints (Graca et al, 2008) or
generalized expectation criteria (McCallum et al,
2007).
Acknowledgements
The authors acknowledge the support of the Excel-
lence Cluster on Multimodal Computing and Inter-
action (MMCI). Thanks to Alexandre Klementiev,
Alexander Koller, Manfred Pinkal, Dan Roth, Car-
oline Sporleder and the anonymous reviewers for
their suggestions, and to Percy Liang for answer-
ing questions about his model.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence align-
ment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of the
Conference on Human Language Technology and
North American chapter of the Association for Com-
putational Linguistics (HLT-NAACL).
Sugatu Basu, Arindam Banjeree, and Raymond
Mooney. 2004. Active semi-supervision for pair-
wise constrained clustering. In Proc. of the SIAM
International Conference on Data Mining (SDM),
pages 333?344.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT: Pro-
ceedings of the Workshop on Computational Learn-
ing Theory, Morgan Kaufmann Publishers, pages
209?214.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Arbor,
MI USA.
966
David L. Chen and Raymond L. Mooney. 2008. Learn-
ing to sportcast: A test of grounded language acqui-
sition. In Proc. of International Conference on Ma-
chine Learning, pages 128?135.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithms. Journal of the Royal Statistical So-
ciety. Series B (Methodological), 39(1):1?38.
P. Diaconis and B. Efron. 1983. Computer-intensive
methods in statistics. Scientific American, pages
116?130.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the Conference on Computational
Linguistics (COLING), pages 350?356.
Ruifang Ge and Raymond J. Mooney. 2005. A sta-
tistical semantic parser that integrates syntax and
semantics. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CONLL-05), Ann Arbor, Michigan.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
Advances in Neural Information Processing Systems
20 (NIPS).
Zellig Harris. 1968. Mathematical structures of lan-
guage. Wiley.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambigous supervision.
In Association for the Advancement of Artificial In-
telligence (AAAI), pages 895?900.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
Andrew McCallum, Gideon Mann, and Gregory
Druck. 2007. Generalized expectation criteria.
Technical Report TR 2007-60, University of Mas-
sachusetts, Amherst, MA.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Proceedings of the 8th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 982?991.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proc. of Uncertainty
in Artificial Intelligence (UAI), pages 467?475.
Judea Pearl. 1982. Reverend bayes on inference en-
gines: A distributed hierarchical approach. In Proc.
of the National Conference on Artificial Intelligence
(AAAI), pages 133?136.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, (EMNLP-09).
Dragomir Radev. 2000. A common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In 1st SIGdial Workshop
on Discourse and Dialogue, pages 74?83.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In
Proceedings of Second International Workshop on
Paraphrasing (IWP2003), pages 65?71.
Benjamin Snyder and Regina Barzilay. 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of International Joint
Conference on Artificial Intelligence (IJCAI-05),
pages 1713?1718.
J. Weeds and W. Weir. 2005. Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):439?475.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial grammar.
In Proceedings of the Twenty-first Conference on
Uncertainty in Artificial Intelligence, Edinburgh,
UK, August.
967
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1190?1200,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Cross-lingual Transfer of Semantic Role Labeling Models
Mikhail Kozhevnikov and Ivan Titov
Saarland University, Postfach 15 11 50
66041 Saarbru?cken, Germany
{mkozhevn|titov}@mmci.uni-saarland.de
Abstract
Semantic Role Labeling (SRL) has be-
come one of the standard tasks of natural
language processing and proven useful as
a source of information for a number of
other applications. We address the prob-
lem of transferring an SRL model from
one language to another using a shared
feature representation. This approach is
then evaluated on three language pairs,
demonstrating competitive performance as
compared to a state-of-the-art unsuper-
vised SRL system and a cross-lingual an-
notation projection baseline. We also con-
sider the contribution of different aspects
of the feature representation to the perfor-
mance of the model and discuss practical
applicability of this method.
1 Background and Motivation
Semantic role labeling has proven useful in many
natural language processing tasks, such as ques-
tion answering (Shen and Lapata, 2007; Kaisser
and Webber, 2007), textual entailment (Sammons
et al, 2009), machine translation (Wu and Fung,
2009; Liu and Gildea, 2010; Gao and Vogel, 2011)
and dialogue systems (Basili et al, 2009; van der
Plas et al, 2009).
Multiple models have been designed to auto-
matically predict semantic roles, and a consider-
able amount of data has been annotated to train
these models, if only for a few more popular lan-
guages. As the annotation is costly, one would like
to leverage existing resources to minimize the hu-
man effort required to construct a model for a new
language.
A number of approaches to the construction of
semantic role labeling models for new languages
have been proposed. On one end of the scale is
unsupervised SRL, such as Grenager and Manning
(2006), which requires some expert knowledge,
but no labeled data. It clusters together arguments
that should bear the same semantic role, but does
not assign a particular role to each cluster. On the
other end is annotating a new dataset from scratch.
There are also intermediate options, which often
make use of similarities between languages. This
way, if an accurate model exists for one language,
it should help simplify the construction of a model
for another, related language.
The approaches in this third group often use par-
allel data to bridge the gap between languages.
Cross-lingual annotation projection systems (Pado?
and Lapata, 2009), for example, propagate infor-
mation directly via word alignment links. How-
ever, they are very sensitive to the quality of par-
allel data, as well as the accuracy of a source-
language model on it.
An alternative approach, known as cross-lingual
model transfer, or cross-lingual model adaptation,
consists of modifying a source-language model to
make it directly applicable to a new language. This
usually involves constructing a shared feature rep-
resentation across the two languages. McDon-
ald et al (2011) successfully apply this idea to
the transfer of dependency parsers, using part-of-
speech tags as the shared representation of words.
A later extension of Ta?ckstro?m et al (2012) en-
riches this representation with cross-lingual word
clusters, considerably improving the performance.
In the case of SRL, a shared representation that
is purely syntactic is likely to be insufficient, since
structures with different semantics may be realized
by the same syntactic construct, for example ?in
August? vs ?in Britain?. However with the help of
recently introduced cross-lingual word represen-
1190
tations, such as the cross-lingual clustering men-
tioned above or cross-lingual distributed word rep-
resentations of Klementiev et al (2012), we may
be able to transfer models of shallow semantics in
a similar fashion.
In this work we construct a shared feature repre-
sentation for a pair of languages, employing cross-
lingual representations of syntactic and lexical in-
formation, train a semantic role labeling model on
one language and apply it to the other one. This
approach yields an SRL model for a new language
at a very low cost, effectively requiring only a
source language model and parallel data.
We evaluate on five (directed) language pairs ?
EN-ZH, ZH-EN, EN-CZ, CZ-EN and EN-FR, where
EN, FR, CZ and ZH denote English, French, Czech
and Chinese, respectively. The transferred model
is compared against two baselines: an unsuper-
vised SRL system and a model trained on the out-
put of a cross-lingual annotation projection sys-
tem.
In the next section we will describe our setup,
then in section 3 present the shared feature repre-
sentation we use, discuss the evaluation data and
other technical aspects in section 4, present the
results and conclude with an overview of related
work.
2 Setup
The purpose of the study is not to develop a yet
another semantic role labeling system ? any exist-
ing SRL system can (after some modification) be
used in this setup ? but to assess the practical ap-
plicability of cross-lingual model transfer to this
problem, compare it against the alternatives and
identify its strong/weak points depending on a par-
ticular setup.
2.1 Semantic Role Labeling Model
We consider the dependency-based version of se-
mantic role labeling as described in Hajic? et al
(2009) and transfer an SRL model from one lan-
guage to another. We only consider verbal pred-
icates and ignore the predicate disambiguation
stage. We also assume that the predicate identifi-
cation information is available ? in most languages
it can be obtained using a relatively simple heuris-
tic based on part-of-speech tags.
The model performs argument identification
and classification (Johansson and Nugues, 2008)
separately in a pipeline ? first each candidate is
classified as being or not being a head of an argu-
ment phrase with respect to the predicate in ques-
tion and then each of the arguments is assigned a
role from a given inventory. The model is factor-
ized over arguments ? the decisions regarding the
classification of different arguments are made in-
dependently of each other.
With respect to the use of syntactic annotation
we consider two options: using an existing depen-
dency parser for the target language and obtaining
one by means of cross-lingual transfer (see sec-
tion 4.2).
Following McDonald et al (2011), we assume
that a part-of-speech tagger is available for the tar-
get language.
2.2 SRL in the Low-resource Setting
Several approaches have been proposed to obtain
an SRL model for a new language with little or
no manual annotation. Unsupervised SRL mod-
els (Lang and Lapata, 2010) cluster the arguments
of predicates in a given corpus according to their
semantic roles. The performance of such models
can be impressive, especially for those languages
where semantic roles correlate strongly with syn-
tactic relation of the argument to its predicate.
However, assigning meaningful role labels to the
resulting clusters requires additional effort and the
model?s parameters generally need some adjust-
ment for every language.
If the necessary resources are already available
for a closely related language, they can be uti-
lized to facilitate the construction of a model for
the target language. This can be achieved ei-
ther by means of cross-lingual annotation projec-
tion (Yarowsky et al, 2001) or by cross-lingual
model transfer (Zeman and Resnik, 2008).
This last approach is the one we are considering
in this work, and the other two options are treated
as baselines. The unsupervised model will be fur-
ther referred to as UNSUP and the projection base-
line as PROJ.
2.3 Evaluation Measures
We use the F1 measure as a metric for the argu-
ment identification stage and accuracy as an ag-
gregate measure of argument classification perfor-
mance. When comparing to the unsupervised SRL
system the clustering evaluation measures are used
instead. These are purity and collocation
1191
PU = 1N
?
i
max
j
|Gj ? Ci|
CO = 1N
?
j
max
i
|Gj ? Ci|,
where Ci is the set of arguments in the i-th induced
cluster, Gj is the set of arguments in the jth gold
cluster and N is the total number of arguments.
We report the harmonic mean of the two (Lang and
Lapata, 2011) and denote it F c1 to avoid confusing
it with the supervised metric.
3 Model Transfer
The idea of this work is to abstract the model away
from the particular source language and apply it
to a new one. This setup requires that we use the
same feature representation for both languages, for
example part-of-speech tags and dependency rela-
tion labels should be from the same inventory.
Some features are not applicable to certain lan-
guages because the corresponding phenomena are
absent in them. For example, consider a strongly
inflected language and an analytic one. While the
latter can usually convey the information encoded
in the word form in the former one (number, gen-
der, etc.), finding a shared feature representation
for such information is non-trivial. In this study
we will confine ourselves to those features that are
applicable to all languages in question, namely:
part-of-speech tags, syntactic dependency struc-
tures and representations of the word?s identity.
3.1 Lexical Information
We train a model on one language and apply it to a
different one. In order for this to work, the words
of the two languages have to be mapped into a
common feature space. It is also desirable that
closely related words from both languages have
similar representations in this space.
Word mapping. The first option is simply to use
the source language words as the shared represen-
tation. Here every source language word would
have itself as its representation and every target
word would map into a source word that corre-
sponds to it. In other words, we supply the model
with a gloss of the target sentence.
The mapping (bilingual dictionary) we use is
derived from a word-aligned parallel corpus, by
identifying, for each word in the target language,
the word in the source language it is most often
aligned to.
Cross-lingual clusters. There is no guarantee
that each of the words in the evaluation data is
present in our dictionary, nor that the correspond-
ing source-language word is present in the training
data, so the model would benefit from the ability
to generalize over closely related words. This can,
for example, be achieved by using cross-lingual
word clusters induced in Ta?ckstro?m et al (2012).
We incorporate these clusters as features into our
model.
3.2 Syntactic Information
Part-of-speech Tags. We map part-of-speech tags
into the universal tagset following Petrov et al
(2012). This may have a negative effect on the
performance of a monolingual model, since most
part-of-speech tagsets are more fine-grained than
the universal POS tags considered here. For exam-
ple Penn Treebank inventory contains 36 tags and
the universal POS tagset ? only 12. Since the finer-
grained POS tags often reflect more language-
specific phenomena, however, they would only be
useful for very closely related languages in the
cross-lingual setting.
The universal part-of-speech tags used in eval-
uation are derived from gold-standard annotation
for all languages except French, where predicted
ones had to be used instead.
Dependency Structure. Another important aspect
of syntactic information is the dependency struc-
ture. Most dependency relation inventories are
language-specific, and finding a shared representa-
tion for them is a challenging problem. One could
map dependency relations into a simplified form
that would be shared between languages, as it is
done for part-of-speech tags in Petrov et al (2012).
The extent to which this would be useful, however,
depends on the similarity of syntactic-semantic in-
terfaces of the languages in question.
In this work we discard the dependency rela-
tion labels where the inventories do not match and
only consider the unlabeled syntactic dependency
graph. Some discrepancies, such as variations in
attachment order, may be present even there, but
this does not appear to be the case with the datasets
we use for evaluation. If a target language is poor
in resources, one can obtain a dependency parser
for the target language by means of cross-lingual
model transfer (Zeman and Resnik, 2008). We
1192
take this into account and evaluate both using the
original dependency structures and the ones ob-
tained by means of cross-lingual model transfer.
3.3 The Model
The model we use is based on that of Bjo?rkelund
et al (2009). It is comprised of a set of linear clas-
sifiers trained using Liblinear (Fan et al, 2008).
The feature model was modified to accommodate
the cross-lingual cluster features and the reranker
component was not used.
We do not model the interaction between differ-
ent argument roles in the same predicate. While
this has been found useful, in the cross-lingual
setup one has to be careful with the assumptions
made. For example, modeling the sequence of
roles using a Markov chain (Thompson et al,
2003) may not work well in the present setting,
especially between distant languages, as the order
or arguments is not necessarily preserved. Most
constraints that prove useful for SRL (Chang et
al., 2007) also require customization when applied
to a new language, and some rely on language-
specific resources, such as a valency lexicon. Tak-
ing into account the interaction between different
arguments of a predicate is likely to improve the
performance of the transferred model, but this is
outside the scope of this work.
3.4 Feature Selection
Compatibility of feature representations is neces-
sary but not sufficient for successful model trans-
fer. We have to make sure that the features we use
are predictive of similar outcomes in the two lan-
guages as well.
Depending on the pair of languages in ques-
tion, different aspects of the feature representation
will retain or lose their predictive power. We can
be reasonably certain that the identity of an ar-
gument word is predictive of its semantic role in
any language, but it might or might not be true
of, for example, the word directly preceding the
argument word. It is therefore important to pre-
POS part-of-speech tags
Synt unlabeled dependency graph
Cls cross-lingual word clusters
Gloss glossed word forms
Deprel dependency relations
Table 1: Feature groups.
vent the model from capturing overly specific as-
pects of the source language, which we do by con-
fining the model to first-order features. We also
avoid feature selection, which, performed on the
source language, is unlikely to help the model to
better generalize to the target one. The experi-
ments confirm that feature selection and the use
of second-order features degrade the performance
of the transferred model.
3.5 Feature Groups
For each word, we use its part-of-speech tag,
cross-lingual cluster id, word identity (glossed,
when evaluating on the target language) and its
dependency relation to its parent. Features associ-
ated with an argument word include the attributes
of the predicate word, the argument word, its par-
ent, siblings and children, and the words directly
preceding and following it. Also included are the
sequences of part-of-speech tags and dependency
relations on the path between the predicate and the
argument.
Since we are also interested in the impact of dif-
ferent aspects of the feature representation, we di-
vide the features into groups as summarized in ta-
ble 1 and evaluate their respective contributions to
the performance of the model. If a feature group
is enabled ? the model has access to the corre-
sponding source of information. For example, if
only POS group is enabled, the model relies on
the part-of-speech tags of the argument, the pred-
icate and the words to the right and left of the ar-
gument word. If Synt is enabled too, it also uses
the POS tags of the argument?s parent, children
and siblings.
Word order information constitutes an implicit
group that is always available. It includes the
Position feature, which indicates whether the
argument is located to the left or to the right of
the predicate, and allows the model to look up the
attributes of the words directly preceding and fol-
lowing the argument word. The model we com-
pare against the baselines uses all applicable fea-
ture groups (Deprel is only used in EN-CZ and
CZ-EN experiments with original syntax).
4 Evaluation
4.1 Datasets and Preprocessing
Evaluation of the cross-lingual model transfer re-
quires a rather specific kind of dataset. Namely,
the data in both languages has to be annotated
1193
with the same set of semantic roles following the
same (or compatible) guidelines, which is seldom
the case. We have identified three language pairs
for which such resources are available: English-
Chinese, English-Czech and English-French.
The evaluation datasets for English and Chi-
nese are those from the CoNLL Shared Task
2009 (Hajic? et al, 2009) (henceforth CoNLL-ST).
Their annotation in the CoNLL-ST is not identi-
cal, but the guidelines for ?core? semantic roles
are similar (Kingsbury et al, 2004), so we eval-
uate only on core roles here. The data for the
second language pair is drawn from the Prague
Czech-English Dependency Treebank 2.0 (Hajic?
et al, 2012), which we converted to a format simi-
lar to that of CoNLL-ST1. The original annotation
uses the tectogrammatical representation (Hajic?,
2002) and an inventory of semantic roles (or func-
tors), most of which are interpretable across vari-
ous predicates. Also note that the syntactic anno-
tation of English and Czech in PCEDT 2.0 is quite
similar (to the extent permitted by the difference
in the structure of the two languages) and we can
use the dependency relations in our experiments.
For English-French, the English CoNLL-ST
dataset was used as a source and the model was
evaluated on the manually annotated dataset from
van der Plas et al (2011). The latter contains one
thousand sentences from the French part of the Eu-
roparl (Koehn, 2005) corpus, annotated with se-
mantic roles following an adapted version of Prop-
Bank (Palmer et al, 2005) guidelines. The au-
thors perform annotation projection from English
to French, using a joint model of syntax and se-
mantics and employing heuristics for filtering. We
use a model trained on the output of this projec-
tion system as one of the baselines. The evalua-
tion dataset is relatively small in this case, so we
perform the transfer only one-way, from English
to French.
The part-of-speech tags in all datasets were re-
placed with the universal POS tags of Petrov et al
(2012). For Czech, we have augmented the map-
pings to account for the tags that were not present
in the datasets from which the original mappings
were derived. Namely, tag ?t? is mapped to
?VERB? and ?Y? ? to ?PRON?.
We use parallel data to construct a bilingual
dictionary used in word mapping, as well as
in the projection baseline. For English-Czech
1see http://www.ml4nlp.de/code-and-data/treex2conll
and English-French, the data is drawn from Eu-
roparl (Koehn, 2005), for English-Chinese ? from
MultiUN (Eisele and Chen, 2010). The word
alignments were obtained using GIZA++ (Och
and Ney, 2003) and the intersection heuristic.
4.2 Syntactic Transfer
In the low-resource setting, we cannot always
rely on the availability of an accurate dependency
parser for the target language. If one is not avail-
able, the natural solution would be to use cross-
lingual model transfer to obtain it.
Unfortunately, the models presented in the pre-
vious work, such as Zeman and Resnik (2008),
McDonald et al (2011) and Ta?ckstro?m et al
(2012), were not made available, so we repro-
duced the direct transfer algorithm of McDonald
et al (2011), using Malt parser (Nivre, 2008) and
the same set of features. We did not reimple-
ment the projected transfer algorithm, however,
and used the default training procedure instead of
perceptron-based learning. The dependency struc-
ture thus obtained is, of course, only a rough ap-
proximation ? even a much more sophisticated al-
gorithm may not perform well when transferring
syntax between such languages as Czech and En-
glish, given the inherent difference in their struc-
ture. The scores are shown in table 2.
We will henceforth refer to the syntactic annota-
tions that were provided with the datasets as orig-
inal, as opposed to the annotations obtained by
means of syntactic transfer.
4.3 Baselines
Unsupervised Baseline: We are using a version
of the unsupervised semantic role induction sys-
tem of Titov and Klementiev (2012a) adapted to
Setup UAS, %
EN-ZH 35
ZH-EN 42
EN-CZ 36
CZ-EN 39
EN-FR 67
Table 2: Syntactic transfer accuracy, unlabeled at-
tachment score (percent). Note that in case of
French we evaluate against the output of a super-
vised system, since manual annotation is not avail-
able for this dataset. This score does not reflect the
true performance of syntactic transfer.
1194
the shared feature representation considered in or-
der to make the scores comparable with those
of the transfer model and, more importantly, to
enable evaluation on transferred syntax. Note
that the original system, tailored to a more ex-
pressive language-specific syntactic representa-
tion and equipped with heuristics to identify ac-
tive/passive voice and other phenomena, achieves
higher scores than those we report here.
Projection Baseline: The projection baseline we
use for English-Czech and English-Chinese is a
straightforward one: we label the source side of a
parallel corpus using the source-language model,
then identify those verbs on the target side that are
aligned to a predicate, mark them as predicates and
propagate the argument roles in the same fashion.
A model is then trained on the resulting training
data and applied to the test set.
For English-French we instead use the output of
a fully featured projection model of van der Plas et
al. (2011), published in the CLASSiC project.
5 Results
In order to ensure that the results are consistent,
the test sets, except for the French one, were par-
titioned into five equal parts (of 5 to 10 thousand
sentences each, depending on the dataset) and the
evaluation performed separately on each one. All
evaluation figures for English, Czech or Chinese
below are the average values over the five sub-
sets. In case of French, the evaluation dataset is
too small to split it further, so instead we ran the
evaluation five times on a randomly selected 80%
sample of the evaluation data and averaged over
those. In both cases the results are consistent over
the subsets, the standard deviation does not exceed
0.5% for the transfer system and projection base-
line and 1% for the unsupervised system.
5.1 Argument Identification
We summarize the results in table 3. Argument
identification is known to rely heavily on syntac-
tic information, so it is unsurprising that it proves
inaccurate when transferred syntax is used. Our
simple projection baseline suffers from the same
problem. Even with original syntactic information
available, the performance of argument identifica-
tion is moderate. Note that the model of (van der
Plas et al, 2011), though relying on more expres-
sive syntax, only outperforms the transferred sys-
tem by 3% (F1) on this task.
Setup Syntax TRANS PROJ
EN-ZH trans 34.5 13.9
ZH-EN trans 32.6 15.6
EN-CZ trans 46.3 12.4
CZ-EN trans 42.3 22.2
EN-FR trans 61.6 43.5
EN-ZH orig 51.7 19.6
ZH-EN orig 53.2 29.7
EN-CZ orig 63.9 59.3
CZ-EN orig 67.3 60.9
EN-FR orig 71.0 51.3
Table 3: Argument identification, transferred
model vs. projection baseline, F1.
Most unsupervised SRL approaches assume
that the argument identification is performed
by some external means, for example heuristi-
cally (Lang and Lapata, 2011). Such heuristics
or unsupervised approaches to argument identifi-
cation (Abend et al, 2009) can also be used in the
present setup.
5.2 Argument Classification
In the following tables, TRANS column contains
the results for the transferred system, UNSUP ?
for the unsupervised baseline and PROJ ? for pro-
jection baseline. We highlight in bold the higher
score where the difference exceeds twice the max-
imum of the standard deviation estimates of the
two results.
Table 4 presents the unsupervised evaluation re-
sults. Note that the unsupervised model performs
as well as the transferred one or better where the
Setup Syntax TRANS UNSUP
EN-ZH trans 83.3 73.9
ZH-EN trans 79.2 67.6
EN-CZ trans 66.4 66.1
CZ-EN trans 68.2 68.7
EN-FR trans 74.6 65.1
EN-ZH orig 84.5 89.7
ZH-EN orig 79.2 83.0
EN-CZ orig 74.1 74.0
CZ-EN orig 74.6 76.7
EN-FR orig 73.3 72.3
Table 4: Argument classification, transferred
model vs. unsupervised baseline in terms of the
clustering metric F c1 (see section 2.3).
1195
Setup Syntax TRANS PROJ
EN-ZH trans 70.1 69.2
ZH-EN trans 65.6 61.3
EN-CZ trans 50.1 46.3
CZ-EN trans 53.3 54.7
EN-FR trans 65.1 66.1
EN-ZH orig 71.7 69.7
ZH-EN orig 66.1 64.4
EN-CZ orig 59.0 53.2
CZ-EN orig 61.0 60.8
EN-FR orig 63.0 68.0
Table 5: Argument classification, transferred
model vs. projection baseline, accuracy.
original syntactic dependencies are available. In
the more realistic scenario with transferred syn-
tax, however, the transferred model proves more
accurate.
In table 5 we compare the transferred system
with the projection baseline. It is easy to see
that the scores vary strongly depending on the lan-
guage pair, due to both the difference in the anno-
tation scheme used and the degree of relatedness
between the languages. The drop in performance
when transferring the model to another language
is large in every case, though, see table 6.
Setup Target Source
EN-ZH 71.7 87.1
ZH-EN 66.1 86.2
EN-CZ 59.0 80.1
CZ-EN 61.0 75.4
EN-FR 63.0 82.5
Table 6: Model accuracy on the source and target
language using original syntax. The source lan-
guage scores for English vary between language
pairs because of the difference in syntactic anno-
tation and role subset used.
We also include the individual F1 scores for
the top-10 most frequent labels for EN-CZ trans-
fer with original syntax in table 7. The model
provides meaningful predictions here, despite low
overall accuracy.
Most of the labels2 are self-explanatory: Pa-
tient (PAT), Actor (ACT), Time (TWHEN), Effect
(EFF), Location (LOC), Manner (MANN), Ad-
dressee (ADDR), Extent (EXT). CPHR marks the
2http://ufal.mff.cuni.cz/?toman/pcedt/en/functors.html
Label Freq. F1 Re. Pr.
PAT 14707 69.4 70.0 68.7
ACT 14303 81.1 81.7 80.4
TWHEN 3631 70.6 65.1 77.0
EFF 2601 45.4 67.2 34.3
LOC 1990 41.8 35.3 51.3
MANN 1208 54.0 63.8 46.9
ADDR 1045 30.2 34.4 26.8
CPHR 791 20.4 13.1 45.0
EXT 708 42.2 40.5 44.1
DIR3 695 20.1 17.3 23.9
Table 7: EN-CZ transfer (with original syntax), F1,
recall and precision for the top-10 most frequent
roles.
nominal part of a complex predicate, as in ?to have
[a plan]CPHR?, and DIR3 indicates destination.
5.3 Additional Experiments
We now evaluate the contribution of different as-
pects of the feature representation to the perfor-
mance of the model. Table 8 contains the results
for English-French.
Features Orig Trans
POS 47.5 47.5
POS, Synt 53.0 53.1
POS, Cls 53.7 53.7
POS, Gloss 63.7 63.7
POS, Synt, Cls 55.9 56.4
POS, Synt, Gloss 65.2 66.3
POS, Cls, Gloss 61.5 61.5
POS, Synt, Cls, Gloss 63.0 65.1
Table 8: EN-FR model transfer accuracy with dif-
ferent feature subsets, using original and trans-
ferred syntactic information.
The fact that the model performs slightly bet-
ter with transferred syntax may be explained by
two factors. Firstly, as we already mentioned, the
original syntactic annotation is also produced au-
tomatically. Secondly, in the model transfer setup
it is more important how closely the syntactic-
semantic interface on the target side resembles that
on the source side than how well it matches the
?true? structure of the target language, and in this
respect a transferred dependency parser may have
an advantage over one trained on target-language
data.
The high impact of the Gloss features here
1196
may be partly attributed to the fact that the map-
ping is derived from the same corpus as the eval-
uation data ? Europarl (Koehn, 2005) ? and partly
by the similarity between English and French in
terms of word order, usage of articles and prepo-
sitions. The moderate contribution of the cross-
lingual cluster features are likely due to the insuf-
ficient granularity of the clustering for this task.
For more distant language pairs, the contribu-
tions of individual feature groups are less inter-
pretable, so we only highlight a few observations.
First of all, both EN-CZ and CZ-EN benefit notice-
ably from the use of the original syntactic annota-
tion, including dependency relations, but not from
the transferred syntax, most likely due to the low
syntactic transfer performance. Both perform bet-
ter when lexical information is available, although
the improvement is not as significant as in the case
of French ? only up to 5%.
The situation with Chinese is somewhat compli-
cated in that adding lexical information here fails
to yield an improvement in terms of the metric
considered. This is likely due to the fact that we
consider only the core roles, which can usually be
predicted with high accuracy based on syntactic
information alone.
6 Related Work
Development of robust statistical models for core
NLP tasks is a challenging problem, and adapta-
tion of existing models to new languages presents
a viable alternative to exhaustive annotation for
each language. Although the models thus obtained
are generally imperfect, they can be further refined
for a particular language and domain using tech-
niques such as active learning (Settles, 2010; Chen
et al, 2011).
Cross-lingual annotation projection (Yarowsky
et al, 2001) approaches have been applied ex-
tensively to a variety of tasks, including POS
tagging (Xi and Hwa, 2005; Das and Petrov,
2011), morphology segmentation (Snyder and
Barzilay, 2008), verb classification (Merlo et al,
2002), mention detection (Zitouni and Florian,
2008), LFG parsing (Wro?blewska and Frank,
2009), information extraction (Kim et al, 2010),
SRL (Pado? and Lapata, 2009; van der Plas et al,
2011; Annesi and Basili, 2010; Tonelli and Pi-
anta, 2008), dependency parsing (Naseem et al,
2012; Ganchev et al, 2009; Smith and Eisner,
2009; Hwa et al, 2005) or temporal relation pre-
diction (Spreyer and Frank, 2008). Interestingly,
it has also been used to propagate morphosyntac-
tic information between old and modern versions
of the same language (Meyer, 2011).
Cross-lingual model transfer methods (McDon-
ald et al, 2011; Zeman and Resnik, 2008; Durrett
et al, 2012; S?gaard, 2011; Lopez et al, 2008)
have also been receiving much attention recently.
The basic idea behind model transfer is similar to
that of cross-lingual annotation projection, as we
can see from the way parallel data is used in, for
example, McDonald et al (2011).
A crucial component of direct transfer ap-
proaches is the unified feature representation.
There are at least two such representations of
lexical information (Klementiev et al, 2012;
Ta?ckstro?m et al, 2012), but both work on word
level. This makes it hard to account for phenom-
ena that are expressed differently in the languages
considered, for example the syntactic function of
a certain word may be indicated by a preposi-
tion, inflection or word order, depending on the
language. Accurate representation of such infor-
mation would require an extra level of abstrac-
tion (Hajic?, 2002).
A side-effect of using adaptation methods is that
we are forced to use the same annotation scheme
for the task in question (SRL, in our case), which
in turn simplifies the development of cross-lingual
tools for downstream tasks. Such representations
are also likely to be useful in machine translation.
Unsupervised semantic role labeling meth-
ods (Lang and Lapata, 2010; Lang and Lapata,
2011; Titov and Klementiev, 2012a; Lorenzo and
Cerisara, 2012) also constitute an alternative to
cross-lingual model transfer.
For an overview of of semi-supervised ap-
proaches we refer the reader to Titov and Klemen-
tiev (2012b).
7 Conclusion
We have considered the cross-lingual model trans-
fer approach as applied to the task of semantic role
labeling and observed that for closely related lan-
guages it performs comparably to annotation pro-
jection approaches. It allows one to quickly con-
struct an SRL model for a new language without
manual annotation or language-specific heuristics,
provided an accurate model is available for one of
the related languages along with a certain amount
of parallel data for the two languages. While an-
1197
notation projection approaches require sentence-
and word-aligned parallel data and crucially de-
pend on the accuracy of the syntactic parsing and
SRL on the source side of the parallel corpus,
cross-lingual model transfer can be performed us-
ing only a bilingual dictionary.
Unsupervised SRL approaches have their ad-
vantages, in particular when no annotated data is
available for any of the related languages and there
is a syntactic parser available for the target one,
but the annotation they produce is not always suf-
ficient. In applications such as Information Re-
trieval it is preferable to have precise labels, rather
than just clusters of arguments, for example.
Also note that when applying cross-lingual
model transfer in practice, one can improve upon
the performance of the simplistic model we use
for evaluation, for example by picking the features
manually, taking into account the properties of the
target language. Domain adaptation techniques
can also be employed to adjust the model to the
target language.
Acknowledgments
The authors would like to thank Alexandre Kle-
mentiev and Ryan McDonald for useful sugges-
tions and Ta?ckstro?m et al (2012) for sharing the
cross-lingual word representations. This research
is supported by the MMCI Cluster of Excellence.
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, ACL ?09,
pages 28?36, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of FrameNet annotations through hidden
Markov models. In Proceedings of the 11th interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing?10, pages 12?
25, Berlin, Heidelberg. Springer-Verlag.
Roberto Basili, Diego De Cao, Danilo Croce, Bonaven-
tura Coppola, and Alessandro Moschitti. 2009.
Cross-language frame semantics transfer in bilin-
gual corpora. In Alexander F. Gelbukh, editor, Pro-
ceedings of the 10th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, pages 332?345.
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43?48, Boulder, Colorado, June.
Association for Computational Linguistics.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL.
Chenhua Chen, Alexis Palmer, and Caroline Sporleder.
2011. Enhancing active learning for semantic role
labeling via compressed dependency trees. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 183?191, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. Proceedings of the Association for
Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1?11,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Andreas Eisele and Yu Chen. 2010. MultiUN:
A multilingual corpus from United Nation docu-
ments. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10). European Language Resources Associ-
ation (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th Annual Meeting of the ACL, pages 369?377,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Qin Gao and Stephan Vogel. 2011. Corpus expan-
sion for statistical machine translation with seman-
tic role label substitution rules. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 294?298, Portland, Oregon, USA.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of EMNLP.
Jan Hajic?. 2002. Tectogrammatical representation:
Towards a minimal transfer in machine translation.
In Robert Frank, editor, Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
1198
and Related Frameworks (TAG+6), pages 216?
226, Venezia. Universita di Venezia.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr
Sgall, Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?,
Marie Mikulova?, Petr Pajas, Jan Popelka, Jir???
Semecky?, Jana S?indlerova?, Jan S?te?pa?nek, Josef
Toman, Zden?ka Ures?ova?, and Zdene?k Z?abokrtsky?.
2012. Announcing Prague Czech-English depen-
dency treebank 2.0. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel text.
Natural Language Engineering, 11(3):311?325.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 69?78, Honolulu, Hawaii.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL Work-
shop on Deep Linguistic Processing.
Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and
Gary Geunbae Lee. 2010. A cross-lingual an-
notation projection approach for relation detection.
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
564?571, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Paul Kingsbury, Nianwen Xue, and Martha Palmer.
2004. Propbanking in parallel. In In Proceedings
of the Workshop on the Amazing Utility of Paral-
lel and Comparable Corpora, in conjunction with
LREC?04.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), Bombay, India.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 939?947, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Joel Lang and Mirella Lapata. 2011. Unsupervised
semantic role induction via split-merge clustering.
In Proc. of Annual Meeting of the Association for
Computational Linguistics (ACL).
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), Beijing, China.
Adam Lopez, Daniel Zeman, Michael Nossal, Philip
Resnik, and Rebecca Hwa. 2008. Cross-language
parser adaptation between related languages. In
IJCNLP-08 Workshop on NLP for Less Privileged
Languages, pages 35?42, Hyderabad, India, Jan-
uary.
Alejandra Lorenzo and Christophe Cerisara. 2012.
Unsupervised frame based semantic role induction:
application to French and English. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically
Rich Languages, pages 30?35, Jeju, Republic of Ko-
rea, July. Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 62?72, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multi-lingual paradigm
for automatic verb classification. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02), pages 207?
214, Philadelphia, PA.
Roland Meyer. 2011. New wine in old wineskins??
Tagging old Russian via annotation projection
from modern translations. Russian Linguistics,
35(2):267(15).
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 629?637, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513?553, December.
1199
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1).
Sebastian Pado? and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31:71?105.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Mark Sammons, Vinod Vydiswaran, Tim Vieira,
Nikhil Johri, Ming wei Chang, Dan Goldwasser,
Vivek Srikumar, Gourab Kundu, Yuancheng Tu,
Kevin Small, Joshua Rule, Quang Do, and Dan
Roth. 2009. Relation alignment for textual en-
tailment recognition. In Text Analysis Conference
(TAC).
Burr Settles. 2010. Active learning literature survey.
Computer Sciences Technical Report, 1648.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP.
David A Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 822?831. Association for Com-
putational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
Proceedings of the 23rd national conference on Ar-
tificial intelligence.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 2 of HLT ?11, pages
682?686, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kathrin Spreyer and Anette Frank. 2008. Projection-
based acquisition of a temporal labeller. Proceed-
ings of IJCNLP 2008.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. of the An-
nual Meeting of the North American Association
of Computational Linguistics (NAACL), pages 477?
487, Montre?al, Canada.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for seman-
tic role labeling. In Proceedings of the 14th Eu-
ropean Conference on Machine Learning, ECML
2003, pages 397?408, Dubrovnik, Croatia.
Ivan Titov and Alexandre Klementiev. 2012a. A
Bayesian approach to unsupervised semantic role in-
duction. In Proc. of European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
Ivan Titov and Alexandre Klementiev. 2012b. Semi-
supervised semantic role labeling: Approaching
from an unsupervised perspective. In Proceedings
of the International Conference on Computational
Linguistics (COLING), Bombay, India, December.
Sara Tonelli and Emanuele Pianta. 2008. Frame infor-
mation transfer from English to Italian. In Proceed-
ings of LREC 2008.
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2009. Domain adaptation with artificial
data for semantic parsing of speech. In Proc. 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 125?128, Boulder, Colorado.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
HLT ?11, pages 299?304, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Alina Wro?blewska and Anette Frank. 2009. Cross-
lingual projection of LFG F-structures: Building
an F-structure bank for Polish. In Eighth Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries, page 209.
Dekai Wu and Pascale Fung. 2009. Can semantic
role labeling improve SMT? In Proceedings of 13th
Annual Conference of the European Association for
Machine Translation (EAMT 2009), Barcelona.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-English
languages. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 851?858,
Stroudsburg, PA, USA.
David Yarowsky, Grace Ngai, and Ricahrd Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of Human Language Technology Con-
ference.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages, pages 35?
42, Hyderabad, India, January. Asian Federation of
Natural Language Processing.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
1200
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579?585,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cross-lingual Model Transfer Using Feature Representation Projection
Mikhail Kozhevnikov
MMCI, University of Saarland
Saarbr?ucken, Germany
mkozhevn@mmci.uni-saarland.de
Ivan Titov
ILLC, University of Amsterdam
Amsterdam, Netherlands
titov@uva.nl
Abstract
We propose a novel approach to cross-
lingual model transfer based on feature
representation projection. First, a com-
pact feature representation relevant for the
task in question is constructed for either
language independently and then the map-
ping between the two representations is
determined using parallel data. The tar-
get instance can then be mapped into
the source-side feature representation us-
ing the derived mapping and handled di-
rectly by the source-side model. This ap-
proach displays competitive performance
on model transfer for semantic role label-
ing when compared to direct model trans-
fer and annotation projection and suggests
interesting directions for further research.
1 Introduction
Cross-lingual model transfer approaches are con-
cerned with creating statistical models for var-
ious tasks for languages poor in annotated re-
sources, utilising resources or models available
for these tasks in other languages. That includes
approaches such as direct model transfer (Ze-
man and Resnik, 2008) and annotation projec-
tion (Yarowsky et al, 2001). Such methods have
been successfully applied to a variety of tasks,
including POS tagging (Xi and Hwa, 2005; Das
and Petrov, 2011; T?ackstr?om et al, 2013), syntac-
tic parsing (Ganchev et al, 2009; Smith and Eis-
ner, 2009; Hwa et al, 2005; Durrett et al, 2012;
S?gaard, 2011), semantic role labeling (Pad?o and
Lapata, 2009; Annesi and Basili, 2010; Tonelli
and Pianta, 2008; Kozhevnikov and Titov, 2013)
and others.
Direct model transfer attempts to find a shared
feature representation for samples from the two
languages, usually generalizing and abstract-
ing away from language-specific representations.
Once this is achieved, instances from both lan-
guages can be mapped into this space and a model
trained on the source-language data directly ap-
plied to the target language. If parallel data is
available, it can be further used to enforce model
agreement on this data to adjust for discrepancies
between the two languages, for example by means
of projected transfer (McDonald et al, 2011).
The shared feature representation depends on
the task in question, but most often each aspect
of the original feature representation is handled
separately. Word types, for example, may be re-
placed by cross-lingual word clusters (T?ackstr?om
et al, 2012) or cross-lingual distributed word rep-
resentations (Klementiev et al, 2012). Part-of-
speech tags, which are often language-specific,
can be converted into universal part-of-speech
tags (Petrov et al, 2012) and morpho-syntactic
information can also be represented in a unified
way (Zeman et al, 2012; McDonald et al, 2013;
Tsarfaty, 2013). Unfortunately, the design of such
representations and corresponding conversion pro-
cedures is by no means trivial.
Annotation projection, on the other hand, does
not require any changes to the feature represen-
tation. Instead, it operates on translation pairs,
usually on sentence level, applying the available
source-side model to the source sentence and
transferring the resulting annotations through the
word alignment links to the target one. The quality
of predictions on source sentences depends heav-
ily on the quality of parallel data and the domain
it belongs to (or, rather, the similarity between this
domain and that of the corpus the source-language
model was trained on). The transfer itself also
introduces errors due to translation shifts (Cyrus,
2006) and word alignment errors, which may lead
to inaccurate predictions. These issues are gen-
erally handled using heuristics (Pad?o and Lapata,
2006) and filtering, for example based on align-
ment coverage (van der Plas et al, 2011).
579
Figure 1: Dependency-based semantic role labeling example. The top arcs depict dependency relations,
the bottom ones ? semantic role structure. Rendered with https://code.google.com/p/whatswrong/.
1.1 Motivation
The approach proposed here, which we will refer
to as feature representation projection (FRP), con-
stitutes an alternative to direct model transfer and
annotation projection and can be seen as a com-
promise between the two.
It is similar to direct transfer in that we also
use a shared feature representation. Instead of
designing this representation manually, however,
we create compact monolingual feature represen-
tations for source and target languages separately
and automatically estimate the mapping between
the two from parallel data. This allows us to make
use of language-specific annotations and account
for the interplay between different types of infor-
mation. For example, a certain preposition at-
tached to a token in the source language might
map into a morphological tag in the target lan-
guage, which would be hard to handle for tradi-
tional direct model transfer other than using some
kind of refinement procedure involving parallel
data. Note also that any such refinement procedure
applicable to direct transfer would likely work for
FRP as well.
Compared to annotation projection, our ap-
proach may be expected to be less sensitive to par-
allel data quality, since we do not have to com-
mit to a particular prediction on a given instance
from parallel data. We also believe that FRP
may profit from using other sources of informa-
tion about the correspondence between source and
target feature representations, such as dictionary
entries, and thus have an edge over annotation pro-
jection in those cases where the amount of parallel
data available is limited.
2 Evaluation
We evaluate feature representation projection on
the task of dependency-based semantic role label-
ing (SRL) (Haji?c et al, 2009).
This task consists in identifying predicates and
their arguments in sentences and assigning each
argument a semantic role with respect to its pred-
icate (see figure 1). Note that only a single word
? the syntactic head of the argument phrase ? is
marked as an argument in this case, as opposed
to constituent- or span-based SRL (Carreras and
M`arquez, 2005). We focus on the assignment of
semantic roles to identified arguments.
For the sake of simplicity we cast it as a multi-
class classification problem, ignoring the interac-
tion between different arguments in a predicate. It
is well known that such interaction plays an impor-
tant part in SRL (Punyakanok et al, 2008), but it
is not well understood which kinds of interactions
are preserved across languages and which are not.
Also, should one like to apply constraints on the
set of semantic roles in a given predicate, or, for
example, use a reranker (Bj?orkelund et al, 2009),
this can be done using a factorized model obtained
by cross-lingual transfer.
In our setting, each instance includes the word
type and part-of-speech and morphological tags (if
any) of argument token, its parent and correspond-
ing predicate token, as well as their dependency
relations to their respective parents. This repre-
sentation is further denoted ?
0
.
2.1 Approach
We consider a pair of languages (L
s
, L
t
) and
assume that an annotated training set D
s
T
=
{(x
s
, y
s
)} is available in the source language as
well as a parallel corpus of instance pairs D
st
=
{(
x
s
, x
t
)}
and a target dataset D
t
E
=
{
x
t
}
that
needs to be labeled.
We design a pair of intermediate compact
monolingual feature representations ?
s
1
and ?
t
1
and models M
s
and M
t
to map source and target
samples x
s
and x
t
from their original representa-
tions, ?
s
0
and ?
t
0
, to the new ones. We use the par-
580
allel instances in the new feature representation
?
D
st
=
{(
x
s
1
, x
t
1
)}
=
{(
M
s
(x
s
),M
t
(x
t
)
)}
to determine the mapping M
ts
(usually, linear) be-
tween the two spaces:
M
ts
= argmax
M
?
(x
s
1
,x
t
1
?
?
D
st
)
?
?
?
x
s
1
?M(x
t
1
)
?
?
?
2
Then a classification model M
y
is trained on the
source training data
?
D
s
T
= {(x
s
1
, y
s
)} = {(M
s
(x
s
), y
s
)}
and the labels are assigned to the target samples
x
t
? D
t
E
using a composition of the models:
y
t
= M
y
(M
ts
(M
t
(x
t
)))
2.2 Feature Representation
Our objective is to make the feature represen-
tation sufficiently compact that the mapping be-
tween source and target feature spaces could be
reliably estimated from a limited amount of paral-
lel data, while preserving, insofar as possible, the
information relevant for classification.
Estimating the mapping directly from raw cat-
egorical features (?
0
) is both computationally ex-
pensive and likely inaccurate ? using one-hot en-
coding the feature vectors in our experiments
would have tens of thousands of components.
There is a number of ways to make this repre-
sentation more compact. To start with, we re-
place word types with corresponding neural lan-
guage model representations estimated using the
skip-gram model (Mikolov et al, 2013a). This
corresponds to M
s
and M
t
above and reduces the
dimension of the feature space, making direct es-
timation of the mapping practical. We will refer to
this representation as ?
1
.
To go further, one can, for example, apply
dimensionality reduction techniques to obtain a
more compact representation of ?
1
by eliminating
redundancy or define auxiliary tasks and produce
a vector representation useful for those tasks. In
source language, one can even directly tune an in-
termediate representation for the target problem.
2.3 Baselines
As mentioned above we compare the performance
of this approach to that of direct transfer and an-
notation projection. Both baselines are using the
same set of features as the proposed model, as de-
scribed earlier.
The shared feature representation for di-
rect transfer is derived from ?
0
by replacing
language-specific part-of-speech tags with univer-
sal ones (Petrov et al, 2012) and adding cross-
lingual word clusters (T?ackstr?om et al, 2012) to
word types. The word types themselves are left as
they are in the source language and replaced with
their gloss translations in the target one (Zeman
and Resnik, 2008). In English-Czech and Czech-
English we also use the dependency relation infor-
mation, since the annotations are partly compati-
ble.
The annotation projection baseline implementa-
tion is straightforward. The source-side instances
from a parallel corpus are labeled using a classi-
fier trained on source-language training data and
transferred to the target side. The resulting anno-
tations are then used to train a target-side classifier
for evaluation. Note that predicate and argument
identification in both languages is performed us-
ing monolingual classifiers and only aligned pairs
are used in projection. A more common approach
would be to project the whole structure from the
source language, but in our case this may give
unfair advantage to feature representation projec-
tion, which relies on target-side argument identifi-
cation.
2.4 Tools
We use the same type of log-linear classifiers
in the model itself and the two baselines to
avoid any discrepancy due to learning proce-
dure. These classifiers are implemented using
PYLEARN2 (Goodfellow et al, 2013), based on
THEANO (Bergstra et al, 2010). We also use this
framework to estimate the linear mapping M
ts
be-
tween source and target feature spaces in FRP.
The 250-dimensional word representations for
?
1
are obtained using WORD2VEC tool. Both
monolingual data and that from the parallel cor-
pus are included in the training. In Mikolov et al
(2013b) the authors consider embeddings of up to
800 dimensions, but we would not expect to bene-
fit as much from larger vectors since we are using
a much smaller corpus to train them. We did not
tune the size of the word representation to our task,
as this would not be appropriate in a cross-lingual
transfer setup, but we observe that the classifier
is relatively robust to their dimension when evalu-
581
ated on source language ? in our experiments the
performance of the monolingual classifier does not
improve significantly if the dimension is increased
past 300 and decreases only by a small margin
(less than one absolute point) if it is reduced to
100. It should be noted, however, that the dimen-
sion that is optimal in this sense is not necessarily
the best choice for FRP, especially if the amount
of available parallel data is limited.
2.5 Data
We use two language pairs for evaluation:
English-Czech and English-French. In the first
case, the data is converted from Prague Czech-
English Dependency Treebank 2.0 (Haji?c et al,
2012) using the script from Kozhevnikov and
Titov (2013). In the second, we use CoNLL 2009
shared task (Haji?c et al, 2009) corpus for English
and the manually corrected dataset from van der
Plas et al (2011) for French. Since the size of
the latter dataset is relatively small ? one thou-
sand sentences ? we reserve the whole dataset for
testing and only evaluate transfer from English to
French, but not the other way around. Datasets for
other languages are sufficiently large, so we take
30 thousand samples for testing and use the rest
as training data. The validation set in each exper-
iment is withheld from the corresponding training
corpus and contains 10 thousand samples.
Parallel data for both language pairs is de-
rived from Europarl (Koehn, 2005), which we pre-
process using MATE-TOOLS (Bj?orkelund et al,
2009; Bohnet, 2010).
3 Results
The classification error of FRP and the baselines
given varying amount of parallel data is reported
in figures 2, 3 and 4. The training set for each
language is fixed. We denote the two baselines AP
(annotation projection) and DT (direct transfer).
The number of parallel instances in these exper-
iments is shown on a logarithmic scale, the values
considered are 2, 5, 10, 20 and 50 thousand pairs.
Please note that we report only a single value
for direct transfer, since this approach does not ex-
plicitly rely on parallel data. Although some of
the features ? namely, gloss translations and cross-
lingual clusters ? used in direct transfer are, in fact,
derived from parallel data, we consider the effect
of this on the performance of direct transfer to be
indirect and outside the scope of this work.
2 5 10 20 50
0.34
0.36
0.38
0.40
0.42
Number of parallel instances, ?10
3
Error
FRP
AP
DT
Figure 2: English-Czech transfer results
2 5 10 20 50
0.32
0.34
0.36
0.38
0.40
Number of parallel instances, ?10
3
Error
FRP
AP
DT
Figure 3: Czech-English transfer results
The rather inferior performance of direct trans-
fer baseline on English-French may be partially
attributed to the fact that it cannot rely on depen-
dency relation features, as the corpora we consider
make use of different dependency relation inven-
tories. Replacing language-specific dependency
annotations with the universal ones (McDonald
et al, 2013) may help somewhat, but we would
still expect the methods directly relying on paral-
lel data to achieve better results given a sufficiently
large parallel corpus.
Overall, we observe that the proposed method
with ?
1
representation demonstrates performance
competitive to direct transfer and annotation pro-
jection baselines.
582
2 5 10 20 50
0.34
0.36
0.38
0.40
Number of parallel instances, ?10
3
Error
FRP
AP
DT
Figure 4: English-French transfer results
4 Additional Related Work
Apart from the work on direct/projected transfer
and annotation projection mentioned above, the
proposed method can be seen as a more explicit
kind of domain adaptation, similar to Titov (2011)
or Blitzer et al (2006).
It is also somewhat similar in spirit to Mikolov
et al (2013b), where a small number of word
translation pairs are used to estimate a mapping
between distributed representations of words in
two different languages and build a word transla-
tion model.
5 Conclusions
In this paper we propose a new method of cross-
lingual model transfer, report initial evaluation re-
sults and highlight directions for its further devel-
opment.
We observe that the performance of this method
is competitive with that of established cross-
lingual transfer approaches and its application re-
quires very little manual adjustment ? no heuris-
tics or filtering and no explicit shared feature rep-
resentation design. It also retains compatibility
with any refinement procedures similar to pro-
jected transfer (McDonald et al, 2011) that may
have been designed to work in conjunction with
direct model transfer.
6 Future Work
This paper reports work in progress and there is
a number of directions we would like to pursue
further.
Better Monolingual Representations The rep-
resentation we used in the initial evaluation does
not discriminate between aspects that are relevant
for the assignment of semantic roles and those that
are not. Since we are using a relatively small set of
features to start with, this does not present much of
a problem. In general, however, retaining only rel-
evant aspects of intermediate monolingual repre-
sentations would simplify the estimation of map-
ping between them and make FRP more robust.
For source language, this is relatively straight-
forward, as the intermediate representation can be
directly tuned for the problem in question using
labeled training data. For target language, how-
ever, we assume that no labeled data is available
and auxiliary tasks have to be used to achieve this.
Alternative Sources of Information The
amount of parallel data available for many
language pairs is growing steadily. However,
cross-lingual transfer methods are often applied
in cases where parallel resources are scarce or of
poor quality and must be used with care. In such
situations an ability to use alternative sources of
information may be crucial. Potential sources
of such information include dictionary entries or
information about the mapping between certain
elements of syntactic structure, for example a
known part-of-speech tag mapping.
The available parallel data itself may also be
used more comprehensively ? aligned arguments
of aligned predicates, for example, constitute only
a small part of it, while the mapping of vector rep-
resentations of individual tokens is likely to be the
same for all aligned words.
Multi-source Transfer One of the strong points
of direct model transfer is that it naturally fits the
multi-source transfer setting. There are several
possible ways of adapting FRP to such a setting.
It remains to be seen which one would produce
the best results and how multi-source feature rep-
resentation projection would compare to, for ex-
ample, multi-source projected transfer (McDonald
et al, 2011).
Acknowledgements
The authors would like to acknowledge the
support of MMCI Cluster of Excellence and
Saarbr?ucken Graduate School of Computer Sci-
ence and thank the anonymous reviewers for their
suggestions.
583
References
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of FrameNet annotations through hidden
Markov models. In Proceedings of the 11
th
interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing?10, pages 12?
25. Springer-Verlag.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), Austin, TX.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43?48, Boulder, Colorado, June.
Association for Computational Linguistics.
John Blitzer, Ryan McDonal, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. Conference on Empirical
Methods in Natural Language Processing, Sydney,
Australia.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23
rd
International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August.
Xavier Carreras and Llu??s M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
Lea Cyrus. 2006. Building a resource for studying
translation shifts. CoRR, abs/cs/0606096.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1?11,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
369?377, Suntec, Singapore, August. Association
for Computational Linguistics.
Ian J. Goodfellow, David Warde-Farley, Pascal Lam-
blin, Vincent Dumoulin, Mehdi Mirza, Razvan Pas-
canu, James Bergstra, Fr?ed?eric Bastien, and Yoshua
Bengio. 2013. Pylearn2: a machine learning re-
search library. CoRR, abs/1308.4214.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado.
Jan Haji?c, Eva Haji?cov?a, Jarmila Panevov?a, Petr
Sgall, Ond?rej Bojar, Silvie Cinkov?a, Eva Fu?c??kov?a,
Marie Mikulov?a, Petr Pajas, Jan Popelka, Ji?r??
Semeck?y, Jana
?
Sindlerov?a, Jan
?
St?ep?anek, Josef
Toman, Zde?nka Ure?sov?a, and Zden?ek
?
Zabokrtsk?y.
2012. Announcing Prague Czech-English depen-
dency treebank 2.0. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel text.
Natural Language Engineering, 11(3):311?325.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), Bombay, India.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT.
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190?1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 62?72, Edinburgh, United King-
dom. Association for Computational Linguistics.
584
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. CoRR, abs/1309.4168.
Sebastian Pad?o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for semantic
projection. In Proc. 44
th
Annual Meeting of Associ-
ation for Computational Linguistics and 21
st
Inter-
national Conf. on Computational Linguistics, ACL-
COLING 2006, pages 1161?1168, Sydney, Aus-
tralia.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
David A Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 822?831. Association for Com-
putational Linguistics.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49
th
Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 2 of HLT ?11, pages
682?686, Portland, Oregon. Association for Com-
putational Linguistics.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. of the An-
nual Meeting of the North American Association
of Computational Linguistics (NAACL), pages 477?
487, Montr?eal, Canada.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62?71, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Sara Tonelli and Emanuele Pianta. 2008. Frame infor-
mation transfer from English to Italian. In Proceed-
ings of LREC 2008.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 578?584, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49
th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
HLT ?11, pages 299?304, Portland, Oregon, USA.
Association for Computational Linguistics.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-english
languages. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
851?858, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1?
8. Association for Computational Linguistics.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages, pages 35?
42, Hyderabad, India, January. Asian Federation of
Natural Language Processing.
Daniel Zeman, David Mare?cek, Martin Popel,
Loganathan Ramasamy, Jan
?
St?ep?anek, Zden?ek
?
Zabokrtsk?y, and Jan Haji?c. 2012. Hamledt: To
parse or not to parse? In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
585
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 317?327, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Bootstrapping Semantic Role Labelers from Parallel Data
Mikhail Kozhevnikov Ivan Titov
Saarland University, Postfach 15 11 50
66041 Saarbru?cken, Germany
{mkozhevn|titov}@mmci.uni-saarland.de
Abstract
We present an approach which uses the sim-
ilarity in semantic structure of bilingual par-
allel sentences to bootstrap a pair of seman-
tic role labeling (SRL) models. The setting
is similar to co-training, except for the inter-
mediate model required to convert the SRL
structure between the two annotation schemes
used for different languages. Our approach
can facilitate the construction of SRL models
for resource-poor languages, while preserving
the annotation schemes designed for the tar-
get language and making use of the limited re-
sources available for it. We evaluate the model
on four language pairs, English vs German,
Spanish, Czech and Chinese. Consistent im-
provements are observed over the self-training
baseline.
1 Introduction
The success of statistical modeling methods in a va-
riety of natural language processing (NLP) tasks in
the last decade depended crucially on the availability
of annotated resources for their training. And while
sizable resources for most standard tasks are only
available for a few languages, the human effort re-
quired to achieve reasonable performance on such
tasks for other languages may be significantly re-
duced by leveraging existing resources and the sim-
ilarities between languages.
This idea has lead to the development of cross-
lingual annotation projection approaches, which
make use of parallel corpora (Pado? and Lapata,
2009), as well as attempts to adapt models directly
to other languages (McDonald et al, 2011). In this
paper we consider correspondences between SRL
structures in translated sentences from a different
perspective. Most cross-lingual annotation projec-
tion approaches transfer the source language anno-
tation scheme to the target language without modifi-
cation, which makes it hard to combine their output
with existing target language resources, as annota-
tion schemes may vary significantly. We instead ad-
dress the problem of information transfer between
two existing annotation schemes (figure 1) for a pair
of languages using an intermediate model of role
correspondence (RCM). An RCM models the prob-
ability of a pair of corresponding arguments being
assigned a certain pair of roles. We then use it to
guide a pair of monolingual models toward compat-
ible predictions on parallel data in order to extend
the coverage and/or accuracy of one or both models.
Romanian is not taught in their schools .
Ve ?kol?ch se neu?? rumunsky .
A1
PAT
AM-LOC
LOC
AM-NEG
Figure 1: Role correspondence in parallel sentences, an
example.
The notion of compatibility here is highly non-
trivial, even for sentences translated as close to the
original as possible. Zhuang and Zong (2010), for
example, observe that in the English-Chinese paral-
lel PropBank (Palmer et al, 2005b) corresponding
arguments often bear different labels, even though
the same inventory of semantic roles is used for both
317
languages and the annotation guidelines are similar.
When different annotation schemes are considered,
the problem is further complicated by the difference
in the granularity of semantic roles used and varying
notions of what is an argument and what is not.
Manually annotated training data for such a model
is hard to come by. Instead, we propose an itera-
tive procedure similar to bootstrapping, where the
parameters of the RCM are initially estimated from
a parallel corpus automatically annotated with se-
mantic roles using the monolingual models indepen-
dently, and then the RCM is used to refine these an-
notations via a joint inference procedure, serving to
enforce consistency on the predictions of monolin-
gual models on parallel sentences. The obtained an-
notations on the parallel corpus are expected to be
of higher quality than the independent predictions of
the models, so they can be used to improve the SRL
models? performance and/or coverage. We evalu-
ate this approach by augmenting the original train-
ing data with the annotations obtained on parallel
data and observing the change in the model?s perfor-
mance. This is especially useful if one of the lan-
guages is relatively poor in resources, in which case
the proposed procedure will help propagate infor-
mation from the stronger model to the weaker one.
Even if the two models are comparable in their pre-
dictive power, we may be able to benefit from the
fact that certain semantic roles are realized less am-
biguously in one language than in another. We will
henceforth refer to these two alternatives as the pro-
jection and symmetric setups.
The paper is structured as follows. In the next sec-
tion we present our approach and discuss the issues
of role correspondence modeling, then describe the
implementation and datasets used in evaluation in
section 3, present the evaluation and results in sec-
tion 4 and conclude with the discussion of related
work in section 5.
2 Approach
We consider bootstrapping a pair of SRL models on
a parallel corpus, using the correspondence between
their predictions on parallel sentences to guide the
learning. The models are forced toward compatible
predictions, where the notion of compatibility is de-
fined by a (statistical) role correspondence model.
Let us consider a pair of languages, ? and ?,
and their corresponding datasets T 0? and T
0
? , anno-
tated with semantic roles (the upper indices here de-
note the iteration number). We will refer to these
as the initial training sets. We also assume that a
word-aligned parallel corpus is available for the pair
of languages, which we denote P , with the pred-
icates and their respective arguments identified on
both sides.
The procedure is then as follows: we train mono-
lingual models M0? and M
0
? on T
0
? and T
0
? , respec-
tively, apply them to the two sides of the parallel
corpus, resulting in a labeling P 0. We collect the se-
mantic role co-occurrence information and train the
role correspondence model C0 on it, then proceed to
the joint inference step involving M0?, M
0
? and C
0,
resulting in a refined labeling P 1 of the parallel cor-
pus. The two sides of the P 1 are then used to aug-
ment the initial training sets, yielding T 1? and T
1
? ,
and new models M1? and M
1
? are trained on these.
The process can then be repeated using M1? and M
1
?
instead of the initial models.
We report the model?s performance on a held-out
test set, drawn from the same corpus as the corre-
sponding initial training set.
The procedure can be seen as a form of co-
training (Blum and Mitchell, 1998) of a pair of
monolingual SRL models. In our case, however, the
question of the models? agreement is not as trivial as
in most applications of co-training, requiring a sta-
tistical model of its own (Ci).
In the low-resource (projection) setup our ap-
proach is also similar to self-training with weak su-
pervision coming from the stronger model.
Note that although the approach is iterative, we
have observed no significant improvements from re-
peating the procedure, possibly owing to the noise
introduced by the errors in preprocessing. In the
evaluation we run only one iteration. In the notation
introduced above, the self-training baseline model
(SELF) is trained on P 0? , the joint model (JOINT) ?
on P 1? and the combined model (COMB) ? on T
1
? .
2.1 Modeling Role Correspondence
It is necessary to distinguish between semantic
roles and their interpretation in a particular con-
text. The former can be defined in a variety of
318
ways, depending on the formalism used. In case of
FrameNet (Baker et al, 1998), for example, the in-
terpretation of a semantic role (frame element) is ex-
plicitly provided for each separate frame, so a frame
and a frame element label together describe the se-
mantics of an argument. PropBank (Palmer et al,
2005a) follows a mixed strategy ? the labels for a
relatively small set of core roles are numbered and
their interpretations are provided separately for each
predicate (although those of the first two roles, A0
and A1, consistently denote what is known as Proto-
Agent and Proto-Patient), while modifiers (Merlo
and Leybold, 2001) bear labels that are interpreted
consistently across all predicates. Other resources,
such as Prague Dependency Treebank (Hajic? et al,
2006), use a single set of semantic roles (functors),
which are interpretable across different predicates.
From the standpoint of defining the semantic sim-
ilarity of parallel sentences, the important implica-
tion is that we cannot assume that the corresponding
arguments should bear the same label, even if the
annotation schemes used are compatible (Zhuang
and Zong, 2010). Nor can we write down a single
mapping between the roles that will be valid across
different predicates (figure 2), which motivates the
need for a statistical model of semantic role corre-
spondence.
I do not have these concerns
Yo no tengo tales preocupaciones
A0
arg1-tem
A1
arg2-atr
Parliament adopted the resolution
El Parlamento aprueba la resoluci?n
A0
arg0-agt
A1
arg1-pat
We would like to know their names
Nos gustar?a conocer sus nombres
A0
arg2-ben
A1
arg1-tem
Figure 2: Predicate-specific role mapping. Note that A0
corresponds to art0-agt, art1-tem or art2-ben,
depending on the predicate.
We assume the existence of a one-to-one map-
ping between semantic roles for a given predicate
pair. As the mappings are not completely indepen-
dent ? at least some roles have the same interpre-
tation across different predicate pairs, ? we choose
to build a single model, which relies on features de-
rived from the pair of predicates in question, rather
than create a separate model for each predicate pair.
The model can then make decisions specific to par-
ticular predicates or predicate pairs, where sufficient
data has been observed or back off to a generic map-
ping where there is not enough data.
For the purpose of this study, we choose to sep-
arately model the probability of a target role, given
the source one and the necessary contextual infor-
mation and vice versa. These two components are
referred to as projection models and realized as a
pair of linear classifiers.
Training such a model in a conventional fash-
ion would require a rather specific kind of dataset,
namely a parallel corpus annotated with semantic
roles, and assuming the availability of such data
would severely limit the applicability of the ap-
proach proposed, as, to our knowledge, it is cur-
rently only available for two language pairs, namely
English-Chinese (Palmer et al, 2005b) and English-
Czech (Hajic? et al, 2012). We instead use the auto-
matically produced annotations on a parallel corpus,
effectively enforcing consistency on the role corre-
spondence in the monolingual models? predictions.
2.2 Joint Inference
The joint inference would have been simplest if the
arguments were classified independently. This as-
sumption is too restrictive, though, since the inter-
dependencies between the arguments can be used
to improve the accuracy of semantic role label-
ing (Roth and Yih, 2005).
2.2.1 Projection Setup
In the projection setup we assume that the model
for one of the languages, which we will henceforth
refer to as source, is much better informed than
the one for the other language, referred to as tar-
get, so we only have to propagate the information
one way. The scoring functions of these two mod-
els will be denoted fs and ft, respectively, and that
of the projection model from source to target ? fst.
Source and target sentences are denoted Ss and St,
319
and aligned predicates in these sentences ? ps and
pt. The task is then to identify the target language
role assignment rt that would maximize the objec-
tive L(rt) = ?tft(rt, St, pt) + ?stfst(rt, rs, ps, pt),
where rs = argmaxrfs(rs, Ss, ps) is the role as-
signment of the source-side arguments as predicted
by the monolingual model and ? are the weights as-
sociated with the models.
The exact maximization of this objective is com-
putationally expensive, so we resort to an approx-
imation. We chose to use the dual decomposition
method primarily because it fits the structure of the
objective particularly well (in that it is a sum of the
objectives of two independent models) and since it
allows a wide range of monolingual models to be
used in this setup. The only requirement here is that
the monolingual model must be able to incorporate
a bias toward or away from a certain prediction.
To apply this approximation, we decouple the
rt variables into rt and rst and get L1(rt, rst) =
?tft(rt, St, pt) + ?stfst(rst, rs, ps, pt) under the
condition that rt = rst. Applying the Lagrangian
relaxation, we replace the hard equality constraint
on rt and rst with a soft one, using slack variables ?,
which results in the following objective:
min?maxrt,rstL
?
1(rt, rst, ?) =
?tft(rt, St, pt) + ?stfst(rst, rs, ps, pt)+ (1)
?
i
?
r?Rt
?i,r
(
I(rit = r)? I(r
i
st = r)
)
,
where i indexes aligned argument pairs and I is an
indicator function. This is equivalent to
min?maxrt,rstL
?
1(rt, rst, ?) =
min?
(
maxrtgt(rt, St, pt, ?)+ (2)
maxrstgst(rst, rs, ps, pt, ?)
)
,
where
gt(rt, St, pt, ?) =
?tft(rt, St, pt) +
?
i
?
r?Rt
?i,rI(rit = r)
gstp(rst, rs, ps, pt, ?) = (3)
?stfst(rst, rs, ps, pt)?
?
i
?
r?Rt
?i,rI(rist = r)
are the augmented objectives of the two component
models, incorporating bias factors on various possi-
ble predictions.
The minimization with respect to ? is per-
formed using a subgradient descent algorithm fol-
lowing Sontag et al (2011). Whenever the method
converges, it converges to the global maximum of
the sum of the objectives. We found that in our case
it reaches a solution within the first 1000 iterations
over 99% of the time.
2.2.2 Symmetric Setup
If the models have comparable accuracy, the
above inference procedure can be extended to per-
form projection both ways. Formulating this as a
dual decomposition problem would require using
three separate components, two for the monolingual
models and one for the RCM, which would have to
make its own predictions for the semantic roles on
both sides without conditioning on the predictions
of the monolingual models. This calls for a different
kind of model than the one we use ? a model that
will rely on a (possibly simplified) feature represen-
tation of the source and target arguments to jointly
predict their labels. Instead, we perform the pro-
jection setup inference procedure in both directions
simultaneously, interleaving gradient descent steps
and allowing the projection models to access the up-
dated predictions of the monolingual models. This
results in a block gradient descent algorithm with the
following updates:
rn+1t = argmaxrtgt(rt, St, pt, ?
n
t )
rn+1s = argmaxrtgs(rs, Ss, ps, ?
n
s )
rn+1st = argmaxrstgst(rst, r
n
s , ps, pt, ?
n
t )
rn+1ts = argmaxrtsgts(rts, r
n
t , pt, ps, ?
n
s ) (4)
?i?r?Rs?
n+1,i,r
s = ?
n,i,r
s +
?s(n)(I(r
n,i
ts = r)? I(r
n,i
s = r))
?i?r?Rt?
n+1,i,r
t = ?
n,i,r
t +
?t(n)(I(r
n,i
st = r)? I(r
n,i
t = r)),
where ?s(n) = ?t(n) =
?0
n+1 is the update rate func-
tion for step n, and gs and gts are defined as in (3),
but with the direction reversed.
This procedure allows us to use the same RCM
implementation as in the projection setup. More-
over, the inference procedure for projection setup is
320
a special case of this one with ?s(n) set to 0. The
algorithm also demonstrates convergence similar to
that of the projection version, although it lacks the
optimality guarantees.
3 Experimental Setup
We evaluate our approach on four language pairs,
namely English vs German, Spanish, Czech and
Chinese, which we will denote en-de, en-es,
en-cz and en-zh respectively.
3.1 Parallel Data
The parallel data for the first three language pairs
is drawn from Europarl v6 (Koehn, 2005) and
from MultiUN (Eisele and Chen, 2010) for English-
Chinese. We applied Stanford Tokenizer for En-
glish, tokenizer scripts (Koehn, 2005) provided
with the Europarl corpus to German, Spanish and
Czech, and Stanford Chinese Segmenter (Chang et
al., 2008) to Chinese, then performed POS-tagging,
morphology tagging (where applicable) and depen-
dency parsing using MATE-tools (Bohnet, 2010).
Word alignments were acquired using
GIZA++ (Och and Ney, 2003) with its stan-
dard settings. Predicate identification on the parallel
data was done using the supervised classifiers of
the monolingual SRL systems, except for German,
where a simple heuristic had to be used instead,
as only some of the predicates are marked in
the training data, which makes it hard to train a
supervised classifier. Following van der Plas et al
(2011), we then retain only those sentences where
all identified predicates were aligned.
In the experiments we used 50 thousand predicate
pairs in each case, as increasing the amount further
did not yield noticeable benefits, while increasing
the running time.
3.2 Annotated Data
The CoNLL?09 (Hajic? et al, 2009) datasets were
used as a source of annotated data for all languages.
Only verbal predicates were considered and pre-
dicted syntax was used in evaluation.
We consider subsets of the training data in order
to emulate the scenario with a resource-poor lan-
guage. Due to the different sources the datasets
were derived from, sentences contain different pro-
portions of annotated predicates depending on the
language. The German corpus, for example, con-
tains about 6 times fewer argument labels per sen-
tence than the English one. We will therefore in-
dicate the sizes of the datasets used in the number
of argument labels they contain, referred to as in-
stances, rather than the number of predicates or sen-
tences. The corpus for English, for example, con-
tains 6.2 such instances per sentence on average.
We use the 20 thousand instances of the available
data as the training corpus for each language and
split the rest equally between the development and
the test set. The secondary (?out-of-domain?) test
sets are preserved as they are.
In dependency-based SRL, only heads of syntac-
tic constituents are marked with semantic roles. The
heads of corresponding arguments may or may not
align, however, even if the arguments are lexically
very similar, because their syntactic structure may
differ. In general, one would have to identify the
whole phrase for each argument and take into ac-
count the links between constituents, rather than sin-
gle words (Pado? and Lapata, 2005). As reconstruct-
ing the constituents from the dependency tree is non-
trivial (Hwang et al, 2010), we are using a heuristic
to address the most common version of this problem,
i.e. a preposition or an auxiliary verb being an argu-
ment head. In such a case we also take into account
any alignment links involving the head?s immediate
descendants.
3.3 Implementation
Our system is based on that of Bjo?rkelund et al
(2009). It is a pipeline system comprised of a set of
binary or multiclass linear classifiers. Both here and
in the projection model, the classifiers are trained
using Liblinear (Fan et al, 2008).
We employed a uniqueness constraint on role la-
bels (Chang et al, 2007), preventing some of them
from being assigned to more than one argument in
the same predicate, which appears to be more reli-
able in a low-resource setting we consider than the
reranker the original system employed. The con-
straint is enforced in the monolingual model infer-
ence using a beam-search approximation with the
beam size of 10. The label uniqueness information
was derived from the training sets.
321
3.4 The Projection Model
Each projection model is realized by a single lin-
ear classifier applied to each argument pair indepen-
dently. It relies on features derived from the source
semantic role and source and target predicates, and
predicts the semantic role for the argument in the
target sentence.
The features include the source semantic role and
its conjunctions with (lowercased) forms and lem-
mata of the source and target predicates. For ex-
ample, assuming the source semantic role is A3 and
the source and target predicates are went and ging
(past tense of ?gehen?, German), the features would
be as shown in figure 3.
FORMPAIR=A3-went-ging
LEMMAPAIR=A3-go-gehen
FORMSRC=A3-went
FORMTGT=A3-ging
LEMMASRC=A3-go
LEMMATGT=A3-gehen
LABEL=A3
Figure 3: Projection model features example.
3.5 Parameters
In case of projection there are two parameters, ?st
and ?t, ? the weights of the component models in the
objective. Only their relative values matter (except
in the choice of ?0), so we set ?t to 1 and only tune
the weight of the role correspondence model.
In the symmetric setup, the objective takes
the form L(rt, rs) = ?tft(rt, St, pt) +
?stfst(rt, rs, ps, pt) + ?sfs(rs, Ss, ps) +
?tsfts(rs, rt, pt, ps). Since we assume that the
two monolingual models here have comparable
performance, we do not tune their relative weights,
setting both ?s and ?t to 1.
We also use the same weight for both projection
models, ?st = ?ts, and this value plays an important
role ? it basically indicates how strongly we insist
on the role correspondence models? correctness. If
this weight is set to 0, the RCM will accept the ini-
tial predictions the monolingual models make, and if
it is set to a sufficiently large value, the predictions
of the monolingual models will be biased until they
match the mapping suggested by the RCM. The op-
timal weight will therefore depend on the language
pair, the sizes of the initial training sets and the RCM
used. We use the value of 0.7 in all projection ex-
periments and 0.5 in the symmetric setup, however,
as excessive tuning may be undesirable in the low-
resource setting.
3.6 Domains
One important factor in the understanding of the
evaluation figures presented is the fact that sources
of annotated and parallel data belong to different do-
mains. The former usually contains some sort of
newswire text ? Wall Street Journal in case of En-
glish, Xinhua newswire, Hong Kong news and Sino-
rama news magazine for Chinese, etc. Parallel data,
on the other hand, comes from the proceedings of
European Parliament and United Nations, which are
quite different. For example, the sentences in the
latter domain often start with someone being ad-
dressed, either by name or by title, which can hardly
be expected to occur as often in a newspaper or a
magazine article.
As is well-known, the performance of many sta-
tistical tools drops significantly outside the domain
they were trained on (Pradhan et al, 2008), and the
preprocessing and SRL models used here are no ex-
ception, which results in relatively low quality of
the initial predictions on the parallel text. The low
argument identification performance, in particular,
is presumably due to inaccurate dependency parses,
on which it heavily relies. Several approached have
been proposed to improve the accuracy of depen-
dency parsers and other tools on out-of-domain data,
but this is beyond the scope of this paper. In some
cases (though seldom), sources of parallel data be-
longing to the same domain as the annotated training
data can be obtained.
Another concern is that the performance of a
model trained on automatically labeled parallel data
as measured on a test set we use may not reflect the
quality of these annotations. To assess the resulting
model?s coverage, it would be interesting to evaluate
it on data outside the original domain, so we con-
sider the out-of-domain (OOD) test sets as provided
for the CoNLL Shared Task 2009 where available.
Perhaps the most interesting one of these is the
German OOD test set, which is drawn from Europarl
(as is the parallel data we use). It was originally
annotated with syntactic dependency trees and se-
322
mantic structure in the SALSA format (Burchardt
et al, 2006) for Pado? and Lapata (2005), and then
converted into a PropBank-like form for the CoNLL
Shared Task 2009 (Hajic? et al, 2009). The OOD
test set for English is drawn from the Brown cor-
pus (Francis and Kucera, 1967) and the one for
Czech ? from a Czech translation of Wall Street
Journal articles (Hajic? et al, 2012).
4 Evaluation
The first question we are interested in is how the
joint inference affects the quality of the automati-
cally obtained annotations on the parallel data. To
answer this, we will run the monolingual models in-
dependently and jointly, then train models on the
output of these two procedures and compare their
performance on a test set. Note that we do not add
the initial training data at this point, so the initial
model scores are provided for reference, rather than
as a baseline.
4.1 Projection Setup
A small initial training set of 600 instances was used
here for the target language here and the full training
set (20000 instances) for the source one. ?st was set
to 0.7 in all experiments in this section.
INIT SELF JOINT ?SELF
en-cz* 61.11 60.68 63.01 2.33
en-cz 62.45 62.15 63.11 0.96
en-de* 66.81 63.96 67.64 3.69
en-de 70.40 68.34 70.13 1.79
en-es 64.20 64.51 66.01 1.50
en-zh 75.80 73.52 74.87 1.35
cz-en* 66.82 63.95 64.97 1.02
cz-en 74.92 71.60 71.90 0.29
de-en* 66.82 63.58 63.21 -0.37
de-en 74.93 71.31 70.72 -0.59
es-en* 66.82 63.95 64.18 0.23
es-en 74.93 71.47 72.09 0.62
zh-en* 66.82 64.51 63.67 -0.83
zh-en 74.93 72.26 71.24 -1.01
Table 1: Projection setup results: self-training baseline,
refined model and the difference in their performance.
Asterisk indicates out-of-domain test set, statistically sig-
nificant improvements are highlighted in bold.
In table 1, we present the accuracy of the model
trained on the output of the joint inference (JOINT)
against that of the self-training baseline (SELF). The
?SELF column contains the difference between the
two. Note that the SELF model is trained on the
parallel data automatically annotated using mono-
lingual SRL models (not mixed with the initial train-
ing set), since we are interested in the effect of joint
inference on the quality of the annotation obtained.
Where the improvement is positive and statistically
significant with p < 0.005 according to the permuta-
tion test (Good, 2000), they are highlighted in bold.
We can see that the refined model (JOINT) outper-
forms the self-training baseline in most cases by a
moderate, but statistically significant margin, which
indicates that the joint inference does improve the
quality of annotations on the parallel corpus.
The slightly higher improvement on the German
OOD test set supports our hypothesis that the proce-
dure enhances the performance of the model on par-
allel data, as the data for this test set is also drawn
from the Europarl corpus. The improvement over
the initial model (?INIT) in this case is statistically
significant with p < 0.05. Higher p-value may be
attributed to the smaller test set size.
Figure 4 shows how the performance of the JOINT
model changes with the size of the initial training
set. The improvements are smaller for en-cz, en-
de and en-zh, but they are also statistically signifi-
cant for initial training sets of up to 2000 instances.
Projection to English from other languages performs
worse.
Figure 4: Projection setup, English-Spanish, model per-
formance as a function of the size of the initial training
set.
323
4.2 Combining
In practice, automatically obtained annotations are
usually combined with the existing labeled data. For
this purpose, the initial training set is replicated so
as to constitute 0.3 (an empirically chosen value that
appears to work well in most experiments) of the
size of the automatically labeled dataset. We com-
pare the performance of the model trained on the re-
sulting dataset (COMB) with that of the JOINT model
and the initial models. The results are presented in
table 2. We omit projection from other languages to
English, since the JOINT model there fails to outper-
form the initial model and we do not expect to ben-
efit from adding the automatically annotated data to
the initial training set in this case.
INIT JOINT COMB ?JOINT ?INIT
en-cz* 61.11 63.01 62.98 -0.03 1.87
en-cz 62.45 63.11 63.30 0.19 0.85
en-de* 66.81 67.64 67.64 0.00 0.84
en-de 70.39 70.19 70.53 0.34 0.15
en-es 64.20 66.01 66.01 0.00 1.81
en-zh 75.80 74.87 75.03 0.16 -0.77
Table 2: The effect of adding automatically obtained an-
notation to the initial training set. Asterisk indicates out-
of-domain test set, statistically significant improvements
are highlighted in bold.
4.3 Symmetric Setup
In the symmetric setup evaluation, we use a slightly
larger initial training set of 1400 instances for both
source and target language. The projection model
weight is set to 0.5. Table 3 shows the accuracy of
the JOINT model and the SELF baseline.
Note that here, unlike section 4.1, the joint in-
ference is run once and then a model is trained for
each language and evaluated on the corresponding
test set(s).
The results support our intuition that joint infer-
ence helps improve the quality of the resulting an-
notations, at least in some cases.
4.4 Oracle RCM
It would be useful to know to what extent the per-
formance of the role correspondence model affects
the quality of the output (and thus the performance
of the resulting model). The RCM we use is rather
INIT SELF JOINT ?SELF
en-cz* 67.07 66.15 68.18 2.02
en-cz 67.56 66.42 66.72 0.30
en-de* 67.64 66.72 68.57 1.84
en-de 75.13 71.97 73.57 1.60
en-es 68.14 67.80 69.04 1.24
en-zh 76.28 72.96 75.22 2.26
cz-en* 69.37 66.45 66.22 -0.23
cz-en 77.32 74.72 75.02 0.31
de-en* 69.37 66.45 66.68 0.23
de-en 77.32 73.56 73.72 0.17
es-en* 69.37 66.64 66.40 -0.23
es-en 77.32 74.05 74.89 0.84
zh-en* 69.37 66.08 65.53 -0.56
zh-en 77.32 74.48 74.25 -0.24
Table 3: Comparing JOINT model against the self-
training baseline in symmetric setup. Asterisk indicates
out-of-domain test set, statistically significant improve-
ments are highlighted in bold.
simplistic, and we believe it can be substantially im-
proved for any given language pair by incorporat-
ing prior knowledge and/or using external sources
of information. In order to estimate the potential
impact of such improvements, we simulate a better
informed projection model, giving it access to the
predictions of more accurate monolingual models on
the parallel data ? those trained on the full training
set, rather than the initial training set used in this par-
ticular experiment. We refer to the resulting RCM as
oracle and assess the difference it makes, compared
to a regular one (table 4).
5 Related Work
There is a number of approaches to semi-supervised
semantic role labeling, and most suggest that some
external supervision is required for such approaches
to work (He and Gildea, 2006), such as measures of
syntactic and semantic similarity (Fu?rstenau and La-
pata, 2009) or external confidence measures (Gold-
wasser et al, 2011). The alternative we propose is
primarily motivated by the research on annotation
projection (Pado? and Lapata, 2009; van der Plas
et al, 2011; Annesi and Basili, 2010; Naseem et
al., 2012) and direct transfer (Durrett et al, 2012;
S?gaard, 2011; Lopez et al, 2008; McDonald et al,
2011). The key difference of the present approach
compared to annotation projection is that we assume
324
INIT SELF JOINT ?SELF ?INIT
en-cz* 61.11 60.68 72.49 11.81 11.38
en-cz 62.45 62.15 70.19 8.04 7.74
en-de* 66.81 63.96 76.78 12.82 9.97
en-de 70.39 68.34 79.22 10.88 8.84
en-es 64.20 64.51 75.43 10.92 11.23
en-zh 75.80 73.52 76.75 3.22 0.94
cz-en* 66.82 63.95 70.75 6.80 3.93
cz-en 74.93 71.60 79.70 8.10 4.76
de-en* 66.82 63.58 69.46 5.88 2.64
de-en 74.93 71.31 77.34 6.03 2.41
es-en* 66.82 63.95 69.92 5.97 3.10
es-en 74.93 71.47 79.55 8.08 4.62
zh-en* 66.82 64.51 67.19 2.68 0.37
zh-en 74.93 72.26 76.51 4.26 1.58
Table 4: Oracle RCM performance, projection setup: ini-
tial model, self-training baseline, refined model and its
improvement over the other two. Asterisk indicates out-
of-domain test set, statistically significant improvements
are highlighted in bold.
the availability of some amount of training data for
the target language, possibly using a different inven-
tory of semantic roles.
As mentioned previously, from the training point
of view this approach can be seen as similar to co-
training (Blum and Mitchell, 1998), other applica-
tions of which to NLP are too numerous to list here.
Most closely related is the joint inference in
Zhuang and Zong (2010), the main difference being
that it relies on a manually annotated parallel corpus,
aligned on the argument level, and evaluates only the
inference procedure and only on in-domain data.
Other related approaches include Kim et al
(2010), where a cross-lingual transfer of relations
is performed (which basically represent parts of
the predicate-argument structure considered by SRL
methods), and Frermann and Bond (2012), where
semantic structure matching is used to rank HPSG
parses for parallel sentences.
Unsupervised semantic role labeling meth-
ods (Lang and Lapata, 2010; Lang and Lapata,
2011; Titov and Klementiev, 2012a; Lorenzo and
Cerisara, 2012) present an alternative to the cross-
lingual information propagation approaches such as
ours, and at least one the methods in this area also
makes use of parallel data (Titov and Klementiev,
2012b).
Conclusions
We have presented an approach to information trans-
fer between SRL systems for different language
pairs using parallel data. The task proves challeng-
ing due to non-trivial mapping between the role la-
bels used in different SRL annotation schemes and
the nature of parallel data ? the difference in do-
mains and the limited accuracy of the preprocess-
ing tools. We observe consistent improvements over
self-training baseline from using joint inference and
the experiments suggest that improving the role cor-
respondence model, for example using language-
specific prior knowledge or external data sources,
may dramatically increase the performance of the re-
sulting system.
Acknowledgments
The authors acknowledge the support of the MMCI
Cluster of Excellence and thank Alexandre Klemen-
tiev and Manfred Pinkal for valuable suggestions.
References
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of framenet annotations through hidden
markov models. In Proceedings of the 11th interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing?10, pages 12?25,
Berlin, Heidelberg. Springer-Verlag.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Pro-
ceedings of the Thirty-Sixth Annual Meeting of the
Association for Computational Linguistics and Sev-
enteenth International Conference on Computational
Linguistics (ACL-COLING?98), pages 86?90, Mon-
treal, Canada.
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43?48, Boulder, Colorado, June.
Association for Computational Linguistics.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Workshop on Computational Learning The-
ory (COLT 98).
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
325
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC 2006,
Genoa, Italy.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. Ur-
bana, 51:61801.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, StatMT ?08, pages 224?232, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1?11, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from united nation documents. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, May. European Language Resources
Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
S. Francis and H. Kucera. 1967. Computing Analysis
of Present-day American English. Brown University
Press, Providence, RI.
Lea Frermann and Francis Bond. 2012. Cross-lingual
parse disambiguation based on semantic correspon-
dence. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 125?129, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role labeling.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 11?
20, Singapore.
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In ACL.
P. Good. 2000. Permutation Tests: A Practical
Guide to Resampling Methods for Testing Hypotheses.
Springer.
J. Hajic?, J. Panevova?, E. Hajic?ova?, P. Sgall, P. Pajas,
J. S?te?pa?nek, J. Havelka, M. Mikulova?, Z. Z?abokrtsky`,
and M. S?evc???kova?-Raz??mova?. 2006. Prague depen-
dency treebank 2.0. LDC.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, Colorado.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing prague czech-english dependency treebank 2.0.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an,
Bente Maegaard, Joseph Mariani, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC?12), Istanbul, Turkey, May. Euro-
pean Language Resources Association (ELRA).
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical report, University of Rochester.
Jena D. Hwang, Rodney D. Nielsen, and Martha Palmer.
2010. Towards a domain independent semantics:
Enhancing semantic representation with construction
grammar. In Proceedings of the NAACL HLT Work-
shop on Extracting and Using Constructions in Com-
putational Linguistics, pages 1?8, Los Angeles, Cali-
fornia, June. Association for Computational Linguis-
tics.
Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and
Gary Geunbae Lee. 2010. A cross-lingual annota-
tion projection approach for relation detection. In Pro-
ceedings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 564?571,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Human Language Tech-
326
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 939?947, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proc. of Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Adam Lopez, Daniel Zeman, Michael Nossal, Philip
Resnik, and Rebecca Hwa. 2008. Cross-Language
Parser Adaptation between Related Languages. In
IJCNLP-08 Workshop on NLP for Less Privileged
Languages, pages 35?42, Hyderabad, India, January.
Alejandra Lorenzo and Christophe Cerisara. 2012. Un-
supervised frame based semantic role induction: ap-
plication to french and english. In Proceedings of the
ACL 2012 Joint Workshop on Statistical Parsing and
Semantic Processing of Morphologically Rich Lan-
guages, pages 30?35, Jeju, Republic of Korea, July 12.
Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 62?72, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Paola Merlo and Matthias Leybold. 2001. Automatic
distinction of arguments and modifiers: the case of
prepositional phrases. In Proceedings of the Fifth
Computational Natural Language Learning Workshop
(CoNLL-2001), pages 121?128, Toulouse, France.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 629?637, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Sebastian Pado? and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 859?866, Vancouver,
British Columbia, Canada.
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection for semantic roles. Journal of
Artificial Intelligence Research, 36:307?340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005a. The Proposition Bank: An annotated corpus
of semantic roles. Computational Linguistics, 31:71?
105.
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-
ing Chen, and Benjamin Snyder. 2005b. A parallel
Proposition Bank II for Chinese and English. In Pro-
ceedings of the Workshop on Frontiers in Corpus An-
notations II: Pie in the Sky, CorpusAnno ?05, pages
61?67, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Compu-
tational Linguistics, 34(2):289?310.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
ICML, pages 736?743.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, HLT ?11,
pages 682?686, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
Ivan Titov and Alexandre Klementiev. 2012a. A
Bayesian approach to unsupervised semantic role in-
duction. In Proc. of European Chapter of the Associa-
tion for Computational Linguistics (EACL).
Ivan Titov and Alexandre Klementiev. 2012b. Crosslin-
gual induction of semantic roles. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, Jeju Island, South Korea, July.
Association for Computational Linguistics.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual seman-
tic role annotation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 299?304, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 304?314,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
327
