Detecting Article Errors Based on
the Mass Count Distinction
Ryo Nagata1, Takahiro Wakana2, Fumito Masui2,
Atsuo Kawai2, and Naoki Isu2
1 Hyogo University of Teacher Education,
942-1 Shimokume, Yashiro, 673-1494 Japan
rnagata@info.hyogo-u.ac.jp
2 Mie University, 1577, Kurimamachiya, Tsu, 514-8507, Japan
{wakana, masui, kawai, isu}@ai.info.mie-u.ac.jp
Abstract. This paper proposes a method for detecting errors concern-
ing article usage and singular/plural usage based on the mass count
distinction. Although the mass count distinction is particularly impor-
tant in detecting these errors, it has been pointed out that it is hard
to make heuristic rules for distinguishing mass and count nouns. To
solve the problem, first, instances of mass and count nouns are auto-
matically collected from a corpus exploiting surface information in the
proposed method. Then, words surrounding the mass (count) instances
are weighted based on their frequencies. Finally, the weighted words are
used for distinguishing mass and count nouns. After distinguishing mass
and count nouns, the above errors can be detected by some heuristic
rules. Experiments show that the proposed method distinguishes mass
and count nouns in the writing of Japanese learners of English with
an accuracy of 93% and that 65% of article errors are detected with a
precision of 70%.
1 Introduction
Although several researchers [1,2,3] have shown that heuristic rules are effective
to detecting grammatical errors in the English writing of second language learn-
ers, it has been pointed out that it is hard to write heuristic rules for detecting
article errors [1]. To be precise, it is hard to write heuristic rules for distinguish-
ing mass and count nouns which are particularly important in detecting article
errors. The major reason for this is that whether a noun is a mass noun or a
count noun greatly depends on its meaning or its surrounding context (Refer to
Pelletier and Schubert [4] for detailed discussion on the mass count distinction).
Article errors are very common among Japanese learners of English [1,5].
This is perhaps because the Japanese language does not have an article system
similar to that of English. Thus, it is favorable for error detecting systems aiming
at Japanese learners of English to be capable of detecting article errors. In other
words, such systems need to somehow distinguish mass and count nouns in the
writing of Japanese learners of English.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 815?826, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
816 R. Nagata et al
In view of this background, we propose a method for automatically dis-
tinguishing mass and count nouns in context to complement the conventional
heuristic rules for detecting grammatical errors. In this method, mass and count
nouns are distinguished by words surrounding the target noun. Words surround-
ing the target noun are collected from a corpus and weighted based on their
occurrences. The weighted words are used for distinguishing mass and count
nouns in detecting article errors.
Given the mass count distinction, errors concerning singular/plural usage,
which are also common in the writing of Japanese learners of English, can be
detected as well as article errors. For example, given that the noun information
is a mass noun, informations can be detected as an error. Considering this, we
include errors concerning singular/plural usage in the target errors of this paper.
Hereafter, to keep the notation simple, the target errors1 will be referred to as
article errors.
The next section describes related work on distinguishing mass and count
nouns. Section 3 proposes the method for automatically distinguishing mass and
count nouns. Section 4 describes heuristic rules for detecting article errors based
on the mass count distinction given by the proposed method. Section 5 discusses
results of experiments conducted to evaluate the proposed method.
2 Related Work
Several researchers have proposed methods for distinguishing mass and count
nouns in the past. Allan [6] has presented an approach to distinguishing nouns
that are used only as either mass or count based on countability environments.
This distinction is called countability preferences. Baldwin and Bond [7,8] have
proposed several methods for learning the countability preferences from cor-
pora2. Bond and Vatikitis-Bateson [9] have shown that nouns? countability can
be predicted using an ontology3. O?Hara et al [10] have proposed a method for
classifying mass and count nouns based on semantic information (Cyc ontological
types [11]).
Unfortunately, it is difficult to apply the above methods to complement
the conventional heuristic rules for detecting grammatical errors. The meth-
ods [6,7,8,9] are not enough for the purpose, because the majority of nouns can
be used as both mass and count depending on the surrounding context [12].
The methods [9,10] cannot be readily applicable to the purpose because they
work only when semantic information on nouns is given. It would be difficult to
extract semantic information from nouns in the writing of learners of English.
1 The details of the target errors are shown in Sect. 4.
2 They define four way countability preferences: fully countable, uncountable, bipar-
tite, and plural only.
3 They define five way countability preferences: fully countable, strongly countable,
weakly countable, uncountable, and plural only.
Detecting Article Errors Based on the Mass Count Distinction 817
3 Distinguishing Mass and Count Nouns
In the proposed method, decision lists [13] are used to distinguish mass and count
nouns. Generally, decision lists are learned from a set of manually tagged training
data. In the proposed method, however, training data can be automatically
generated from a raw corpus.
Section 3.1 describes how to generate training data. Section 3.2 describes how
to learn decision lists from the training data. Section 3.3 explains the method
for distinguishing mass and count nouns using the decision lists.
3.1 Generating Training Data
To generate training data, first, instances of the target noun that head their
noun phrase (NP) are collected from a corpus with their surrounding words.
This can be simply done by an existing chunker or parser.
Then, the collected instances are tagged with mass or count by tagging rules.
For example, the underlined chicken:
Example 1. ... are a lot of chickens in the roost ...
is tagged as
Example 2. ... are a lot of chickens/count in the roost ...
because it is in plural form.
We have made tagging rules based on linguistic knowledge [6,14,12]. Figure 1
and Table 1 represent the tagging rules. Figure 1 shows the framework of the tag-
ging rules. Eachnode in Fig. 1 represents a question applied to the instance in ques-
tion. For example, the root node reads ?Is the instance in question plural??. Each
leaf represents a result of the classification. For example, if the answer is ?yes? at the
root node, the instance in question is tagged with count. Otherwise, the question
at the lower node is applied and so on. The tagging rules do not classify instances
as mass or count in some cases. These unclassified instances are tagged with the
symbol ???. Unfortunately, they cannot readily be included in training data. For
simplicity of implementation, they are excluded from training data.
Table 1. Words used in the tagging rules
(a) (b) (c)
the indefinite article much the definite article
another less demonstrative adjectives
one enough possessive adjectives
each all interrogative adjectives
? sufficient quantifiers
? ? ?s genitives
818 R. Nagata et al




























yes
yes
yes
yes
no
no
no
no










yes no
COUNT
modified by a little?
?
COUNT
MASS
? MASS
plural?
modified by one of the words
in Table 1(a)?
modified by one of the words
in Table 1(b)?
modified by one of the words
in Table 1(c)?
Fig. 1. Framework of the tagging rules
Note that the tagging rules can be used only for distinguishing mass and
count nouns in texts containing no errors. They cannot be used in the writing
of Japanese learners of English that may contain errors including article errors;
they are based on article and the distinction between singular and plural.
Finally, the tagged instances are stored in a file with their surrounding words.
Each line in the file consists of one of the tagged instances and its surrounding
words as shown in Example 2. The file is used as training data for learning a
decision list.
3.2 Learning Decision Lists
A decision list consists of a set of rules that are learned from training data. Each
rule matches with the template as follows:
If a condition is true, then a decision . (1)
To define the template in the proposed method, let us have a look at the
following two examples:
Example 3. I read the paper .
Example 4. The paper is made of hemp pulp.
The underlined papers in both sentences cannot simply be classified as mass or
count by the tagging rules presented in Sect. 3.1 because both are singular and
modified by the definite article. Nevertheless, we can tell that the former is a
count noun and that the latter is a mass noun from the contexts. This suggests
that the mass count distinction is often determined by words surrounding the
target noun. In Example 3, we can tell that the paper refers to something that
can be read from read , and therefore it is a count noun. Likewise, in Example 4,
the paper refers to a certain substance from made and pulp, and therefore it is
a mass noun.
Detecting Article Errors Based on the Mass Count Distinction 819
Taking this observation into account, we define the template based on words
surrounding the target noun. To formalize the template, we will use a random
variable MC that takes either mass or count to denote that the target noun is
a mass noun or a count noun, respectively. We will also use w and C to denote
a word and a certain context around the target noun, respectively. We define
three types of C: np, ?k, and +k that denote the contexts consisting of the noun
phrase that the target noun heads, k words to the left of the noun phrase, and
k words to its right, respectively. Then the template is formalized by
If a word w appears in the context C of the target noun,
then the target noun is distinguished as MC.
Hereafter, to keep the notation simple, the template is abbreviated to
wC ? MC. (2)
Now rules that match with the template can be learned from the training
data generated in Sect. 3.1. All we need to do is to collect words in C from the
training data. Here, the words in Table 1 are excluded. Also, function words
such as pronouns and auxiliary verbs, cardinal and quasi-cardinal numerals, and
the target noun are excluded. All words are reduced to their morphological stem
and converted entirely to lower case when collected. For example, the following
tagged instance:
Example 5. She ate a piece of fried chicken/mass for dinner.
would give a set of rules that match with the template:
Example 6.
piece
?3 ? mass, frynp ? mass, dinner+3 ? mass
for the target noun chicken being mass when k = 3.
In addition to the above rules, a default rule is defined. It is based on the
target noun itself and used when no other confident rules4 are found in the
decision list for the target noun. It is defined by
t ? MCmajor (3)
where t and MCmajor denote the target noun and the major case of MC in the
training data, respectively. Equation (3) reads ?If the target noun appears, then
it is distinguished as the major case?.
The log-likelihood ratio [15] decides in which order rules in a decision list are
applied to the target noun in novel context. It is defined by
log
p(MC|wC)
p(MC|wC)
(4)
4 Confidence is given by the log-likelihood ratio, which will be defined by (4).
820 R. Nagata et al
where MC is the exclusive event of MC and p(MC|wC) is the probability that
the target noun is used as MC when w appears in the context C. For the default
rule, the log-likelihood ratio is defined by
log
p(MCmajor|t)
p(MCmajor|t)
(5)
It is important to exercise some care in estimating p(MC|wC). In principle,
we could simply count the number of times that w appears in the context C of
the target noun used as MC in the training data. However, this estimate can be
unreliable, when w does not appear often in the context. To solve this problem,
using a smoothing parameter ? [16], p(MC|wC) is estimated by
p(MC|wC) =
f(wC , MC) + ?
f(wC) + m?
(6)
where f(wC) and f(wC , MC) are occurrences of w appearing in C and those in
C of the target noun used as MC, respectively. The constant m is the number
of possible classes, that is, m = 2 (mass or count) in our case, and introduced
to satisfy p(MC|wC) + p(MC|wC) = 1. In this paper, ? is set to 0.5. Likewise,
p(MCmajor|t) is estimated by
p(MCmajor|t) =
f(t, MCmajor) + ?
f(t) + m?
(7)
Rules in a decision list are sorted in descending order by (4) and (5). They are
tested on the target noun in novel context in this order. Rules sorted below the
default rule are discarded because they are never used as we will see in Sect. 3.3.
Table 2 shows part of a decision list for the target noun chicken that was
learned from a subset of the BNC (British National Corpus) [17]. Note that the
rules are divided into two columns for the purpose of illustration in Table 2; in
practice, they are merged into one just as shown in Table 3.
On one hand, we associate the words in the left half with food or cooking.
On the other hand, we associate those in the right half with animals. From
this observation, we can say that chicken is a count noun in the sense of an
animal but a mass noun when referring to food or cooking, which agrees with
the knowledge presented in previous work [18].
Table 2. Rules in a decision list (target noun: chicken, k = 3)
Mass Count
wC Log-likelihood ratio wC Log-likelihood ratio
piece?3 1.49 count?3 1.49
fish?3 1.28 peck+3 1.32
dish?3 1.23 pignp 1.23
skin+3 1.23 run?3 1.23
serve+3 1.18 eggnp 1.18
Detecting Article Errors Based on the Mass Count Distinction 821
Table 3. An example of a decision list (target noun: chicken, k = 3)
Rules Log-likelihood ratio
piece?3 ? mass, count?3 ? count 1.49
peck+3 ? count 1.32
fish?3 ? mass 1.28
dish?3 ? mass, pignp ? count, ? ? ? 1.23
: :
3.3 Distinguishing the Target Noun in Novel Context
To distinguish the target noun in novel context, each rule in the decision list
is tested on it in the sorted order until the first applicable one is found. It is
distinguished by the first applicable one. If two or more applicable rules (e.g.,
?piece
?3 ? mass? and ?count?3 ? count? in Table 3) are found, it is distin-
guished by the major decisions of the two or more applicable rules. For example,
suppose there are three applicable rules and two of them are for mass nouns
(one of them is for count nouns). In this case, the target noun is distinguished as
mass. Ties are broken by rules sorted below the ties. If ties include the default
rule, it is distinguished by the default rule.
The following is an example of distinguishing the target noun chicken. Sup-
pose that the decision list shown in Table 3 and the following sentence are given:
Example 7. I ate a piece of chicken with salad.
It turns out that the first rule ?piece3 ? mass? in Table 3 is applicable to the
instance. Thus, it is distinguished as a mass noun.
It should be noted that rules sorted below the default rule are never used
because the default rule is always applicable to the target noun. This is the
reason why rules sorted below the default rule are discarded as mentioned in
Sect. 3.2.
4 Heuristic Rules for Detecting Article Errors
So far, a method for distinguishing mass and count nouns has been described.
This section describes heuristic rules for detecting article errors based on the
mass count distinction given by the method.
Article errors are detected by the following three steps. Rules in each step
are examined on each target noun in the target text.
In the first step, any mass noun in plural form is detected as an article error.
If an article error is detected in the first step, the rest of the steps are not applied.
In the second step, article errors are detected by the rules described in Ta-
ble 4. The symbol ?? in Table 4 denotes that the combination of the corre-
sponding row and column is erroneous. For example, the third row denotes that
822 R. Nagata et al
Table 4. Detection rules used in the second step
Count Mass
Pattern Singular Plural Singular Plural
{another, each, one} ?   
{a lot of, all, enough, lots of, sufficient}  ? ? 
{much}   ? 
{kind of, sort of, that, this} ?  ? 
{few, many, these,those}  ?  
{countless, numerous, several, various}  ?  
cardinal number except one  ?  
{any, some, no, ?s genitives} ? ? ? 
{interrogative adjectives, possessive adjectives} ? ? ? 
Table 5. Detection rules used in the third step
Singular Plural
a the ? a the ?
Mass  ? ?   
Count ? ?   ? ?
plural count nouns, singular mass nouns, and plural mass nouns that are mod-
ified by another , each, or one are erroneous. The symbol ??? denotes that no
error can be detected by the table. If one of the rules in Table 4 is applied to
the target noun, the third step is not applied.
In the third step, article errors are detected by the rules described in Table 5.
The symbols ?a?, ?the?, and ??? in Table 5 denote the indefinite article, the
definite article, and no article, respectively. The symbols ?? and ??? are the
same as in Table 4. For example, ?? in the third row and second column denotes
that the singular mass nouns modified by the indefinite article is erroneous.
In addition to the three steps, article errors are detected by exceptional rules.
The indefinite article that modifies other than the head noun is judged to be
erroneous (e.g., *an expensive). Likewise, the definite article that modifies other
than the head noun and adjectives is judged to be erroneous (e.g., *the them).
5 Experiments
5.1 Experimental Conditions
A subset of essays5 written by Japanese learners of English were used as the
target texts in the experiments. The subset contained 30 essays (1747 words). A
native speaker of English who was a professional rewriter of English recognized
62 article errors in the subset.
5 http://www.lb.u-tokai.ac.jp/lcorpus/index-j.html
Detecting Article Errors Based on the Mass Count Distinction 823
The British National Corpus (BNC) [17] was used to learn decision lists.
Spoken data were excluded from the corpus. Also, sentences the OAK system6,
which was used to extract NPs from the corpus, failed to analyze were excluded.
After these operations, the size of the corpus approximately amounted to 80
million words (the size of the original BNC is approximately 100 million words).
Hereafter, unless otherwise specified, the corpus will be referred to as the BNC.
Performance of the proposed method was evaluated by accuracy, recall, and
precision. Accuracy is defined by
No. of mass and count nouns distinguished correctly
No. of distinguished target nouns
. (8)
Namely, accuracy measures how accurately the proposed method distinguishes
mass and count nouns. Recall is defined by
No. of article errors detected correctly
No. of article errors in the target essays
. (9)
Recall measures how well the proposed method detects all the article errors in
the target essays. Precision is defined by
No. of article errors detected correctly
No. of detected article errors
. (10)
Precision measures how well the proposed method detects only the article errors
in the target essays.
5.2 Experimental Procedures
First, decision lists for each target noun in the target essays were learned from
the BNC. To extract noun phrases and their head nouns, the OAK system was
used7. An optimal value for k (window size of context) was estimated as follows.
For 23 nouns8 shown in [12] as examples of nouns used as both mass and count
nouns, accuracy was calculated using the BNC and ten-fold cross validation. As
a result of setting k = 3, 10, 50, it turned out that k = 3 maximized the average
accuracy. Following this result, k = 3 was selected in the experiments.
Second, the target nouns were distinguished whether they were mass or count
by the proposed method, and then article errors were detected by the mass
6 OAK System Homepage: http://nlp.cs.nyu.edu/oak/
7 We evaluated how accurately training data can be generated by the tagging rules
using the OAK system. It turned out that the accuracy was 0.997 against 2903
instances of 23 nouns shown in [12] which were randomly selected from the BNC;
1694 of those were tagged with mass or count by the tagging rules and 1689 were
tagged correctly. The five errors were due to the OAK system.
8 In [12], 25 nouns are shown. Of those, two nouns (hate and spelling) were excluded
because they only appeared 12.1 and 15.6 times on average in the ten-fold cross
validation, respectively.
824 R. Nagata et al
count distinction and the heuristic rules described in Sect. 4. As a preprocessing,
spelling errors in the target essays were corrected using a spell checker.
Finally, the results of the detection were compared to those done by the
native-speaker of English. From the comparison, accuracy, recall, and precision
were calculated.
Comparison of performance of the proposed method to that of other meth-
ods is difficult because there is no generally accepted test set or performance
baseline [19]. Given this limitation, we compared performance of the proposed
method to that of Grammarian9, a commercial grammar checker. We also com-
pared it to that of a method that used only the default rules in the decision lists.
We tested them on the same target essays to measure their performances.
5.3 Experimental Results and Discussion
In the experiments, the proposed method distinguished mass and count nouns in
the target essays with accuracy of 0.93. This means that the proposed method
is effective to distinguishing mass and count nouns in the writing of Japanese
learners of English. From this result, we can say that the proposed method can
complement the conventional heuristic rules for detecting grammatical errors.
Because of the high accuracy of the proposed method, it detected more than
half of the article errors in the target essays (Table 6). Of the undetected article
errors (22 out of 62), only four were due to the misclassification of mass and
count nouns by the proposed method. The rest were article errors that were not
detected even if the mass count distinction was given. For example, extra definite
articles such as ?I like *the gardening.? cannot be detected even if whether the
noun ?gardening? is a mass noun or a count noun is given. Therefore, it is
necessary to exploit other sources of information than the mass count distinction
to detect these kinds of article error. For instance, exploiting the relation between
sentences could be used to detect these kinds of article error.
The proposed method outperformed the method using only the default rules
in both recall and precision. This means that words surrounding the target nouns
are good indicators of the mass count distinction. For example, the proposed
method correctly distinguished the target noun place in the phrase beautiful
place as a count noun by ?beautifulnp ? count? and detected an article error
from it whereas the method using only the default rules did not.
Table 6. Experimental results
Method Recall Precision
Proposed 0.65 0.70
Default only 0.60 0.69
Grammarian 0.13 1.00
9 Grammarian Pro X ver. 1.5: http://www.mercury-soft.com/
Detecting Article Errors Based on the Mass Count Distinction 825
In precision, the proposed method was outperformed by Grammarian; since
Grammarian is a commercial grammar checker, it seems to be precision-oriented.
The proposed method made 17 false-positives. Of the 17 false-positives, 13
were due to the misclassification of mass and count nouns by the proposed
method. Especially, the proposed method often made false-positives in idiomatic
phrases (e.g., by plane). This result implies that some methods for handling
idiomatic phrases may improve the performance. Four were due to the chun-
ker used to analyze the target essays. Since the chunker is designed for ana-
lyzing texts that contain no errors, it is possible that a chunker designed for
analyzing texts written by Japanese learners of English reduces this kind of
false-positive.
6 Conclusions
This paper has proposed a method for distinguishing mass and count nouns to
complement the conventional heuristic rules for detecting grammatical errors.
The experiments have shown that the proposed method distinguishes mass and
count nouns with a high accuracy (0.93) and that the recall and precision are
0.65 and 0.70, respectively. From the results, it follows that the proposed method
can complement the conventional heuristic rules for detecting grammatical errors
in the writing of Japanese learners of English.
The experiments have also shown that approximately 35% of article errors
in the target essays are not detected by the mass count distinction. For future
work, we will study methods for detecting the undetected article errors.
Acknowledgments
The authors would like to thank Sekine Satoshi who has developed the OAK
System. The authors also would like to thank three anonymous reviewers for
their advice on this paper.
References
1. Kawai, A., Sugihara, K., Sugie, N.: ASPEC-I: An error detection system for English
composition. IPSJ Journal (in Japanese) 25 (1984) 1072?1079
2. McCoy, K., Pennington, C., Suri, L.: English error correction: A syntactic user
model based on principled ?mal-rule? scoring. In: Proc. 5th International Confer-
ence on User Modeling. (1996) 69?66
3. Schneider, D., McCoy, K.: Recognizing syntactic errors in the writing of second
language learners. In: Proc. 17th International Conference on Computational Lin-
guistics. (1998) 1198?1204
4. Pelletier, F., Schubert, L.: Two theories for computing the logical form of mass
expressions. In: Proc.10th International Conference on Computational Linguistics.
(1984) 108?111
826 R. Nagata et al
5. Izumi, E., Uchimoto, K., Saiga, T., Supnithi, T., Isahara, H.: Automatic error
detection in the Japanese learners? English spoken data. In: Proc. 41st Annual
Meeting of the Association for Computational Linguistics. (2003) 145?148
6. Allan, K.: Nouns and countability. J. Linguistic Society of America 56 (1980)
541?567
7. Baldwin, T., Bond, F.: A plethora of methods for learning English countability.
In: Proc. 2003 Conference on Empirical Methods in Natural Language Processing.
(2003) 73?80
8. Baldwin, T., Bond, F.: Learning the countability of English nouns from corpus
data. In: Proc. 41st Annual Meeting of the Association for Computational Lin-
guistics. (2003) 463?470
9. Bond, F., Vatikiotis-Bateson, C.: Using an ontology to determine English count-
ability. In: Proc. 19th International Conference on Computational Linguistics.
(2002) 99?105
10. O?Hara, T., Salay, N., Witbrock, M., Schneider, D., Aldag, B., Bertolo, S., Panton,
K., Lehmann, F., Curtis, J., Smith, M., Baxter, D., Wagner, P.: Inducing criteria
for mass noun lexical mappings using the Cyc KB, and its extension to WordNet.
In: Proc. 5th International Workshop on Computational Semantics. (2003) 425?441
11. Lenat, D.: CYC: A large-scale investment in knowledge infrastructure. Communi-
cations of the ACM 38 (1995) 33?38
12. Huddleston, R., Pullum, G.: The Cambridge Grammar of the English Language.
Cambridge University Press, Cambridge (2002)
13. Rivest, R.: Learning decision lists. Machine Learning 2 (1987) 229?246
14. Gillon, B.: The lexical semantics of English count and mass nouns. In: Proc. Special
Interest Group on the Lexicon of the Association for Computational Linguistics.
(1996) 51?61
15. Yarowsky, D.: Unsupervised word sense disambiguation rivaling supervised meth-
ods. In: Proc. 33rd Annual Meeting of the Association for Computational Linguis-
tics. (1995) 189?196
16. Yarowsky, D.: Homograph Disambiguation in Speech Synthesis. Springer-Verlag
(1996)
17. Burnard, L.: Users Reference Guide for the British National Corpus. version 1.0.
Oxford University Computing Services, Oxford (1995)
18. Ostler, N., Atkins, B.: Predictable meaning shift: Some linguistic properties of
lexical implication rules. In: Proc. of 1st SIGLEX Workshop on Lexical Semantics
and Knowledge Representation. (1991) 87?100
19. Chodorow, M., Leacock, C.: An unsupervised method for detecting grammatical
errors. In: Proc. 1st Meeting of the North America Chapter of the Association for
Computational Linguistics. (2000) 140?147
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 241?248,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Feedback-Augmented Method for Detecting Errors in the Writing of
Learners of English
Ryo Nagata
Hyogo University of Teacher Education
6731494, Japan
rnagata@hyogo-u.ac.jp
Atsuo Kawai
Mie University
5148507, Japan
kawai@ai.info.mie-u.ac.jp
Koichiro Morihiro
Hyogo University of Teacher Education
6731494, Japan
mori@hyogo-u.ac.jp
Naoki Isu
Mie University
5148507, Japan
isu@ai.info.mie-u.ac.jp
Abstract
This paper proposes a method for detect-
ing errors in article usage and singular plu-
ral usage based on the mass count distinc-
tion. First, it learns decision lists from
training data generated automatically to
distinguish mass and count nouns. Then,
in order to improve its performance, it is
augmented by feedback that is obtained
from the writing of learners. Finally, it de-
tects errors by applying rules to the mass
count distinction. Experiments show that
it achieves a recall of 0.71 and a preci-
sion of 0.72 and outperforms other meth-
ods used for comparison when augmented
by feedback.
1 Introduction
Although several researchers (Kawai et al, 1984;
McCoy et al, 1996; Schneider and McCoy, 1998;
Tschichold et al, 1997) have shown that rule-
based methods are effective to detecting gram-
matical errors in the writing of learners of En-
glish, it has been pointed out that it is hard to
write rules for detecting errors concerning the ar-
ticles and singular plural usage. To be precise, it
is hard to write rules for distinguishing mass and
count nouns which are particularly important in
detecting these errors (Kawai et al, 1984). The
major reason for this is that whether a noun is a
mass noun or a count noun greatly depends on its
meaning or its surrounding context (refer to Al-
lan (1980) and Bond (2005) for details of the mass
count distinction).
The above errors are very common among
Japanese learners of English (Kawai et al, 1984;
Izumi et al, 2003). This is perhaps because the
Japanese language does not have a mass count dis-
tinction system similar to that of English. Thus, it
is favorable for error detection systems aiming at
Japanese learners to be capable of detecting these
errors. In other words, such systems need to some-
how distinguish mass and count nouns.
This paper proposes a method for distinguishing
mass and count nouns in context to complement
the conventional rules for detecting grammatical
errors. In this method, first, training data, which
consist of instances of mass and count nouns, are
automatically generated from a corpus. Then,
decision lists for distinguishing mass and count
nouns are learned from the training data. Finally,
the decision lists are used with the conventional
rules to detect the target errors.
The proposed method requires a corpus to learn
decision lists for distinguishing mass and count
nouns. General corpora such as newspaper ar-
ticles can be used for the purpose. However,
a drawback to it is that there are differences in
character between general corpora and the writ-
ing of non-native learners of English (Granger,
1998; Chodorow and Leacock, 2000). For in-
stance, Chodorow and Leacock (2000) point out
that the word concentrate is usually used as a noun
in a general corpus whereas it is a verb 91% of
the time in essays written by non-native learners
of English. Consequently, the differences affect
the performance of the proposed method.
In order to reduce the drawback, the proposed
method is augmented by feedback; it takes as feed-
back learners? essays whose errors are corrected
by a teacher of English (hereafter, referred to as
the feedback corpus). In essence, the feedback
corpus could be added to a general corpus to gen-
erate training data. Or, ideally training data could
be generated only from the feedback corpus just as
241
from a general corpus. However, this causes a se-
rious problem in practice since the size of the feed-
back corpus is normally far smaller than that of a
general corpus. To make it practical, this paper
discusses the problem and explores its solution.
The rest of this paper is structured as follows.
Section 2 describes the method for detecting the
target errors based on the mass count distinction.
Section 3 explains how the method is augmented
by feedback. Section 4 discusses experiments con-
ducted to evaluate the proposed method.
2 Method for detecting the target errors
2.1 Generating training data
First, instances of the target noun that head their
noun phrase (NP) are collected from a corpus with
their surrounding words. This can be simply done
by an existing chunker or parser.
Then, the collected instances are tagged with
mass or count by the following tagging rules. For
example, the underlined chicken:
... are a lot of chickens in the roost ...
is tagged as
... are a lot of chickens/count in the roost ...
because it is in plural form.
We have made tagging rules based on linguistic
knowledge (Huddleston and Pullum, 2002). Fig-
ure 1 and Table 1 represent the tagging rules. Fig-
ure 1 shows the framework of the tagging rules.
Each node in Figure 1 represents a question ap-
plied to the instance in question. For example, the
root node reads ?Is the instance in question plu-
ral??. Each leaf represents a result of the classi-
fication. For example, if the answer is yes at the
root node, the instance in question is tagged with
count. Otherwise, the question at the lower node
is applied and so on. The tagging rules do not
classify instances as mass or count in some cases.
These unclassified instances are tagged with the
symbol ???. Unfortunately, they cannot readily be
included in training data. For simplicity of imple-
mentation, they are excluded from training data1.
Note that the tagging rules can be used only for
generating training data. They cannot be used to
distinguish mass and count nouns in the writing
of learners of English for the purpose of detecting
1According to experiments we have conducted, approxi-
mately 30% of instances are tagged with ??? on average. It is
highly possible that performance of the proposed method will
improve if these instances are included in the training data.
the target errors since they are based on the articles
and the distinction between singular and plural.
Finally, the tagged instances are stored in a file
with their surrounding words. Each line of it con-
sists of one of the tagged instances and its sur-
rounding words as in the above chicken example.
2.2 Learning Decision Lists
In the proposed method, decision lists are used for
distinguishing mass and count nouns. One of the
reasons for the use of decision lists is that they
have been shown to be effective to the word sense
disambiguation task and the mass count distinc-
tion is highly related to word sense as we will see
in this section. Another reason is that rules for dis-
tinguishing mass and count nouns are observable
in decision lists, which helps understand and im-
prove the proposed method.
A decision list consists of a set of rules. Each
rule matches the template as follows:
If a condition is true, then a decision   (1)
To define the template in the proposed method,
let us have a look at the following two examples:
1. I read the paper.
2. The paper is made of hemp pulp.
The underlined papers in both sentences cannot
simply be classified as mass or count by the tag-
ging rules presented in Section 2.1 because both
are singular and modified by the definite article.
Nevertheless, we can tell that the former is a count
noun and the latter is a mass noun from the con-
texts. This suggests that the mass count distinc-
tion is often determined by words surrounding the
target noun. In example 1, we can tell that the pa-
per refers to something that can be read such as
a newspaper or a scientific paper from read, and
therefore it is a count noun. Likewise, in exam-
ple 2, we can tell that the paper refers to a certain
substance from made and pulp, and therefore it is
a mass noun.
Taking this observation into account, we define
the template based on words surrounding the tar-
get noun. To formalize the template, we will use
a random variable  that takes either 	 or

	 to denote that the target noun is a mass noun
or a count noun, respectively. We will also use
 and  to denote a word and a certain context
around the target noun, respectively. We define
242



























yes
yes
yes
yes
no
no
no
no










yes no
COUNT
modified by a little?
?
COUNT
MASS
? MASS
plural?
modified by one of the wordsin Table 1(a)?
modified by one of the wordsin Table 1(b)?
modified by one of the wordsin Table 1(c)?
Figure 1: Framework of the tagging rules
Table 1: Words used in the tagging rules
(a) (b) (c)
the indenite article much the denite article
another less demonstrative adjectives
one enough possessive adjectives
each sufficient interrogative adjectives
? ? quantiers
? ? ?s genitives
three types of  :  ,  , and Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 595?602,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Reinforcing English Countability Prediction with One Countability per
Discourse Property
Ryo Nagata
Hyogo University of Teacher Education
6731494, Japan
rnagata@hyogo-u.ac.jp
Atsuo Kawai
Mie University
5148507, Japan
kawai@ai.info.mie-u.ac.jp
Koichiro Morihiro
Hyogo University of Teacher Education
6731494, Japan
mori@hyogo-u.ac.jp
Naoki Isu
Mie University
5148507, Japan
isu@ai.info.mie-u.ac.jp
Abstract
Countability of English nouns is impor-
tant in various natural language process-
ing tasks. It especially plays an important
role in machine translation since it deter-
mines the range of possible determiners.
This paper proposes a method for reinforc-
ing countability prediction by introducing
a novel concept called one countability per
discourse. It claims that when a noun
appears more than once in a discourse,
they will all share the same countability in
the discourse. The basic idea of the pro-
posed method is that mispredictions can
be correctly overridden using efficiently
the one countability per discourse prop-
erty. Experiments show that the proposed
method successfully reinforces countabil-
ity prediction and outperforms other meth-
ods used for comparison.
1 Introduction
Countability of English nouns is important in var-
ious natural language processing tasks. It is par-
ticularly important in machine translation from
a source language that does not have an article
system similar to that of English, such as Chi-
nese and Japanese, into English since it determines
the range of possible determiners including arti-
cles. It also plays an important role in determining
whether a noun can take singular and plural forms.
Another useful application is to detect errors in ar-
ticle usage and singular/plural usage in the writing
of second language learners. Given countability,
these errors can be detected in many cases. For
example, an error can be detected from ?We have
a furniture.? given that the noun furniture is un-
countable since uncountable nouns do not tolerate
the indefinite article.
Because of the wide range of applications, re-
searchers have done a lot of work related to
countability. Baldwin and Bond (2003a; 2003b)
have proposed a method for automatically learn-
ing countability from corpus data. Lapata and
Keller (2005) and Peng and Araki (2005) have
proposed web-based models for learning count-
ability. Others including Bond and Vatikiotis-
Bateson (2002) and O?Hara et al (2003) use on-
tology to determine countability.
In the application to error detection, re-
searchers have explored alternative approaches
since sources of evidence for determining count-
ability are limited compared to other applications.
Articles and the singular/plural distinction, which
are informative for countability, cannot be used in
countability prediction aiming at detecting errors
in article usage and singular/plural usage. Return-
ing to the previous example, the countability of the
noun furniture cannot be determined as uncount-
able by the indefinite article; first, its countabil-
ity has to be predicted without the indefinite arti-
cle, and only then whether or not it tolerates the
indefinite article is examined using the predicted
countability. Also, unlike in machine translation,
the source language is not given in the writing of
second language learners such as essays, which
means that information available is limited.
To overcome these limitations, Nagata
et al (2005a) have proposed a method for
predicting countability that relies solely on words
(except articles and other determiners) surround-
ing the target noun. Nagata et al (2005b) have
shown that the method is effective to detecting
errors in article usage and singular/plural usage in
the writing of Japanese learners of English. They
595
also have shown that it is likely that performance
of the error detection will improve as accuracy of
the countability prediction increases since most of
false positives are due to mispredictions.
In this paper, we propose a method for reinforc-
ing countability prediction by introducing a novel
concept called one countability per discourse that
is an extension of one sense per discourse pro-
posed by Gale et al (1992). It claims that when
a noun appears more than once in a discourse,
they will all share the same countability in the dis-
course. The basic idea of the proposed method
is that initially mispredicted countability can be
corrected using efficiently the one countability per
discourse property.
The next section introduces the one countability
per discourse concept and shows that it can be a
good source of evidence for predicting countabil-
ity. Section 3 discusses how it can be efficiently
exploited to predict countability. Section 4 de-
scribes the proposed method. Section 5 describes
experiments conducted to evaluate the proposed
method and discusses the results.
2 One Countability per Discourse
One countability per discourse is an extension
of one sense per discourse proposed by Gale
et al (1992). One sense per discourse claims that
when a polysemous word appears more than once
in a discourse it is likely that they will all share
the same sense. Yarowsky (1995) tested the claim
on about 37,000 examples and found that when a
polysemous word appeared more than once in a
discourse, they took on the majority sense for the
discourse 99.8% of the time on average.
Based on one sense per discourse, we hypothe-
size that when a noun appears more than once in a
discourse, they will all share the same countability
in the discourse, that is, one countability per dis-
course. The motivation for this hypothesis is that
if one sense per discourse is satisfied, so is one
countability per discourse because countability is
often determined by word sense. For example, if
the noun paper appears in a discourse and it has
the sense of newspaper, which is countable, the
rest of papers in the discourse also have the same
sense according to one sense per discourse, and
thus they are also countable.
We tested this hypothesis on a set of nouns1
1The conditions of this test are shown in Section 5. Note
that although the source of the data is the same as in Section 5,
as Yarowsky (1995) did. We calculated how ac-
curately the majority countability for each dis-
course predicted countability of the nouns in the
discourse when they appeared more than once. If
the one countability per discourse property is al-
ways satisfied, the majority countability for each
discourse should predict countability with the ac-
curacy of 100%. In other others, the obtained ac-
curacy represents how often the one countability
per discourse property is satisfied.
Table 1 shows the results. ?MCD? in Table 1
stands for Majority Countability for Discourse and
its corresponding column denotes accuracy where
countability of individual nouns was predicted
by the majority countability for the discourse in
which they appeared. Also, ?Baseline? denotes
accuracy where it was predicted by the majority
countability for the whole corpus used in this test.
Table 1: Accuracy obtained by Majority Count-
ability for Discourse
Target noun MCD Baseline
advantage 0.772 0.618
aid 0.943 0.671
authority 0.864 0.771
building 0.850 0.811
cover 0.926 0.537
detail 0.829 0.763
discipline 0.877 0.652
duty 0.839 0.714
football 0.938 0.930
gold 0.929 0.929
hair 0.914 0.902
improvement 0.735 0.685
necessity 0.769 0.590
paper 0.807 0.647
reason 0.858 0.822
sausage 0.821 0.750
sleep 0.901 0.765
stomach 0.778 0.778
study 0.824 0.781
truth 0.783 0.724
use 0.877 0.871
work 0.861 0.777
worry 0.871 0.843
Average 0.851 0.754
Table 1 reveals that the one countability per dis-
discourses in which the target noun appears only once are
excluded from this test unlike in Section 5.
596
course property is a good source of evidence for
predicting countability compared to the baseline
while it is not as strong as the one sense per dis-
course property is. It also reveals that the tendency
of one countability per discourse varies from noun
to noun. For instance, nouns such as aid and
cover show a strong tendency while others such
as advantage and improvement do not. On aver-
age, ?MCD? achieves an improvement of approx-
imately 10% in accuracy over the baseline.
Having observed the results, it is reasonable to
exploit the one countability per discourse prop-
erty for predicting countability. In order to do
it, however, the following two questions should
be addressed. First, how can the majority count-
ability be obtained from a novel discourse? Since
our intention is to predict values of countability of
instances in a novel discourse, none of them are
known. Second, even if the majority countability
is known, how can it be efficiently exploited for
predicting countability? Although we could sim-
ply predict countability of individual instances of
a target noun in a discourse by the majority count-
ability for the discourse, it is highly possible that
this simple method will cause side effects consid-
ering the results in Table 1. These two questions
are addressed in the next section.
3 Basic Idea
3.1 How Can the Majority Countability be
Obtained from a Novel Discourse?
Although we do not know the true value of the ma-
jority countability for a novel discourse, we can
at least estimate it because we have a method for
predicting countability to be reinforced by the pro-
posed method. That is, we can predict countability
of the target noun in a novel discourse using the
method. Simply counting the results would give
the majority countability for it.
Here, we should note that countability of each
instance is not the true value but a predicted one.
Considering this fact, it is sensible to set a cer-
tain criterion in order to filter out spurious predic-
tions. Fortunately, most methods based on ma-
chine learning algorithms give predictions with
their confidences. We use the confidences as the
criterion. Namely, we only take account of predic-
tions whose confidences are greater than a certain
threshold when we estimate the majority count-
ability for a novel discourse.
3.2 How Can the Majority Countability be
Efficiently Exploited?
In order to efficiently exploit the one countabil-
ity per discourse property, we treat the majority
countability for each discourse as a feature in ad-
dition to other features extracted from instances of
the target noun. Doing so, we let a machine learn-
ing algorithm decide which features are relevant to
the prediction. If the majority countability feature
is relevant, the machine learning algorithm should
give a high weight to it compared to others.
To see this, let us suppose that we have a set
of discourses in which instances of the target noun
are tagged with their countability (either countable
or uncountable2) for the moment; we will describe
how to obtain it in Subsection 4.1. For each dis-
course, we can know its majority countability by
counting the numbers of countables and uncount-
ables. We can also generate a model for predicting
countability from the set of discourses using a ma-
chine learning algorithm. All we have to do is to
extract a set of training data from the tagged in-
stances and to apply a machine learning algorithm
to it. This is where the majority countability fea-
ture comes in. The majority countability for each
instance is added to its corresponding training data
as a feature to create a new set of training data be-
fore applying a machine learning algorithm; then
a machine learning algorithm is applied to the new
set. The resulting model takes the majority count-
ability feature into account as well as the other fea-
tures when making predictions.
It is important to exercise some care in count-
ing the majority countability for each discourse.
Note that one countability per discourse is always
satisfied in discourses where the target noun ap-
pears only once. This suggests that it is highly
possible that the resulting model too strongly fa-
vors the majority countability feature. To avoid
this, we could split the discourses into two sets,
one for where the target noun appears only once
and one for where it appears more than once, and
train a model on each set. However, we do not
take this strategy because we want to use as much
data as possible for training. As a compromise,
we approximate the majority countability for dis-
courses where the target noun appears only once
to the value unknown.
2This paper concentrates solely on countable and un-
countable nouns, since they account for the vast majority of
nouns (Lapata and Keller, 2005).
597
  
 
 
























yes
yes
yes
yes
no
no
no
no
 









yes no
COUNTABLE
modified by a little?
?
COUNTABLE
UNCOUNTABLE
? UNCOUNTABLE
plural?
modified by one of the wordsin Table 2(a)?
modified by one of the wordsin Table 2(b)?
modified by one of the wordsin Table 2(c)?
Figure 1: Framework of the tagging rules
Table 2: Words used in the tagging rules
(a) (b) (c)
the indenite article much the denite article
another less demonstrative adjectives
one enough possessive adjectives
each sufficient interrogative adjectives
? ? quantiers
? ? ?s genitives
4 Proposed Method
4.1 Generating Training Data
As discussed in Subsection 3.2, training data are
needed to exploit the one countability per dis-
course property. In other words, the proposed
method requires a set of discourses in which in-
stances of the target noun are tagged with their
countability. Fortunately, Nagata et al (2005b)
have proposed a method for tagging nouns with
their countability. This paper follows it to gener-
ate training data.
To generate training data, first, instances of the
target noun used as a head noun are collected from
a corpus with their surrounding words. This can be
simply done by an existing chunker or parser.
Second, the collected instances are tagged with
either countable or uncountable by tagging rules.
For example, the underlined paper:
... read a paper in the morning ...
is tagged as
... read a paper/countable in the morning ...
because it is modified by the indefinite article.
Figure 1 and Table 2 represent the tagging rules
based on Nagata et al (2005b)?s method. Fig-
ure 1 shows the framework of the tagging rules.
Each node in Figure 1 represents a question ap-
plied to the instance in question. For instance, the
root node reads ?Is the instance in question plu-
ral??. Each leaf represents a result of the classifi-
cation. For instance, if the answer is ?yes? at the
root node, the instance in question is tagged with
countable. Otherwise, the question at the lower
node is applied and so on. The tagging rules do
not classify instances in some cases. These unclas-
sified instances are tagged with the symbol ???.
Unfortunately, they cannot readily be included in
training data. For simplicity of implementation,
they are excluded from training data (we will dis-
cuss the use of these excluded data in Section 6).
Note that the tagging rules cannot be used for
countability prediction aiming at detecting errors
in article usage and singular/plural usage. The
reason is that they are useless in error detection
where whether determiners and the singular/plural
distinction are correct or not is unknown. Obvi-
ously, the tagging rules assume that the target text
contains no error.
Third, features are extracted from each instance.
As the features, the following three types of con-
textual cues are used: (i) words in the noun phrase
that the instance heads, (ii) three words to the left
of the noun phrase, and (iii) three words to its
right. Here, the words in Table 2 are excluded.
Also, function words (except prepositions) such
as pronouns, cardinal and quasi-cardinal numer-
598
als, and the target noun are excluded. All words
are reduced to their morphological stem and con-
verted entirely to lower case when collected. In
addition to the features, the majority countability
is used as a feature. For each discourse, the num-
bers of countables and uncountables are counted
to obtain its majority countability. In case of ties,
it is set to unknown. Also, it is set to unknown
when only one instance appears in the discourse
as explained in Subsection 3.2.
To illustrate feature extraction, let us consider
the following discourse (target noun: paper):
... writing a new paper/countable in his room ...
... read papers/countable with ...
The discourse would give a set of features:
-3=write, NP=new, +3=in, +3=room, MC=c
-3=read, +3=with, MC=c
where ?MC=c? denotes that the majority count-
ability for the discourse is countable. In this exam-
ple (and in the following examples), the features
are represented in a somewhat simplified manner
for the purpose of illustration. In practice, features
are represented as a vector.
Finally, the features are stored in a file with their
corresponding countability as training data. Each
piece of training data would be as follows:
-3=read, +3=with, MC=c, LABEL=c
where ?LABEL=c? denotes that the countability
for the instance is countable.
4.2 Model Generation
The model used in the proposed method can be re-
garded as a function. It takes as its input a feature
vector extracted from the instance in question and
predicts countability (either countable or uncount-
able). Formally, 	 
 where  ,  , and 

denote the model, the feature vector, and 
 ,
respectively; here, 0 and 1 correspond to count-
able and uncountable, respectively.
Given the specification, almost any kind of ma-
chine learning algorithm cab be used to generate
the model used in the proposed method. In this
paper, the Maximum Entropy (ME) algorithm is
used which has been shown to be effective in a
wide variety of natural language processing tasks.
Model generation is done by applying the ME
algorithm to the training data. The resulting model
takes account of the features including the major-
ity countability feature and is used for reinforcing
countability prediction.
4.3 Reinforcing Countability Prediction
Before explaining the reinforcement procedure, let
us introduce the following discourse for illustra-
tion (target noun: paper):
... writing paper in room ... wrote paper in ...
... submitted paper to ...
Note that articles and the singular/plural distinc-
tion are deliberately removed from the discourse.
This kind of situation can happen in machine
translation from a source language that does not
have articles and the singular/plural distinction3.
The situation is similar in the writing of second
language learners of English since they often omit
articles and the singular/plural distinction or use
improper ones. Here, suppose that the true values
of the countability for all instances are countable.
A method to be reinforced by the proposed
method would predict countability as follows:
... writing paper/countable (0.97) in room ...
... wrote paper/countable (0.98) in ...
... submitted paper/uncountable (0.57) to ...
where the numbers in brackets denote the confi-
dences given by the method. The third instance is
mistakenly predicted as uncountable4.
Now let us move on to the reinforcement pro-
cedure. It is divided into three steps. First, the
majority countability for the discourse in question
is estimated by counting the numbers of the pre-
dicted countables and uncountables whose confi-
dences are greater than a certain threshold. In case
of ties, the values of the majority countability is
set to unknown. In the above example, the major-
ity countability for the discourse is estimated to be
countable when the threshold is set to  (two
countables). Second, features explained in Sub-
section 4.1 are extracted from each instance. As
for the majority countability feature, the estimated
one is used. Returning to the above example, the
three instances would give a set of features:
-3=write, +3=in, +3=room, MC=c,
-3=write, +3=in, MC=c,
-3=submit, +3=to, MC=c.
Finally, the model generated in Subsection 4.2
is applied to the features to predict countability.
Because of the majority countability feature, it
3For instance, the Japanese language does not have an ar-
ticle system similar to that of English, neither does it mark
the singular/plural distinction.
4The reason would be that the contextual cues did not ap-
pear in the training data used in the method.
599
is likely that previous mispredictions are overrid-
den by correct ones. In the above example, the
third one would be correctly overridden by count-
able because of the majority countability feature
(MC=c) that is informative for the instance being
countable.
5 Experiments
5.1 Experimental Conditions
In the experiments, we chose Nagata
et al (2005a)?s method as the one to be re-
inforced by the proposed method. In this
method, the decision list (DL) learning algo-
rithm (Yarowsky, 1995) is used. However, we
used the ME algorithm because we found that the
method with the ME algorithm instead of the DL
learning algorithm performed better when trained
on the same training data.
As the target noun, we selected 23 nouns that
were also used in Nagata et al (2005a)?s experi-
ments. They are exemplified as nouns that are used
as both countable and uncountable by Huddleston
and Pullum (2002).
Training data were generated from the writ-
ten part of the British National Corpus (Burnard,
1995). A text tagged with the text tags was used
as a discourse unit. From the corpus, 314 texts,
which amounted to about 10% of all texts, were
randomly taken to obtain test data. The rest of
texts were used to generate training data.
We evaluated performance of prediction by ac-
curacy. We defined accuracy by the ratio of the
number of correct predictions to that of instances
of the target noun in the test data.
5.2 Experimental Procedures
First, we generated training data for each target
noun from the texts using the tagging rules ex-
plained in Subsection 4.1. We used the OAK sys-
tem5 to extract noun phrases and their heads. Of
the extracted instances, we excluded those that had
no contextual cues from the training data (and also
the test data). We also generated another set of
training data by removing the majority countabil-
ity features from them. This set of training data
was used for comparison.
Second, we obtained test data by applying the
tagging rules described in Subsection 4.1 to each
instance of the target noun in the 314 texts. Na-
gata et al (2005b) showed that the tagging rules
5http://www.cs.nyu.edu/ sekine/PROJECT/OAK/
achieved an accuracy of 0.997 in the texts that
contained no errors. Considering these results, we
used the tagging rules to obtain test data. Instances
tagged with ??? were excluded in the experiments.
Third, we applied the ME algorithm6 to the
training data without the majority countability fea-
ture. Using the resulting model, countability of
the target nouns in the test data was predicted.
Then, the predictions were reinforced by the pro-
posed method. The threshold to filter out spu-
rious predictions was set to  . For compar-
ison, the predictions obtained by the ME model
were simply replaced with the estimated majority
countability for each discourse. In this method, the
original predictions were used when the estimated
majority countability was unknown. Also, Nagata
et al (2005a)?s method that was based on the DL
learning algorithm was implemented for compari-
son.
Finally, we calculated accuracy of each method.
In addition to the results, we evaluated the baseline
on the same test data where all predictions were
done by the majority countability for the whole
corpus (training data).
5.3 Experimental Results and Discussion
Table 3 shows the accuracies7. ?ME? and ?Pro-
posed? in Table 3 refer to accuracies of the ME
model and the ME model reinforced by the pro-
posed method, respectively. ?ME+MCD? refers
to accuracy obtained by replacing predictions of
the ME model with the estimated majority count-
ability for each discourse. Also, ?DL? refers to
accuracy of the DL-based method.
Table 3 shows that the three ME-based meth-
ods (?Proposed?, ?ME?, and ?ME+MCD?) per-
form better than ?DL? and the baseline. Espe-
cially, ?Proposed? outperforms the other methods
in most of the target nouns.
Figure 2 summarizes the comparison between
the three ME-based methods. Each plot in Fig-
ure 2 represents each target noun. The horizon-
tal and vertical axises correspond to accuracy of
?ME? and that of ?Proposed? (or ?ME+MCD?),
respectively. The diagonal line corresponds to the
line ffProceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 27?35,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recognizing Noisy Romanized Japanese Words in Learner English
Ryo Nagata
Konan University
Kobe 658-8501, Japan
rnagata[at]konan-u.ac.jp
Jun-ichi Kakegawa
Hyogo University of Teacher Education
Kato 673-1421, Japan
kakegawa[at]hyogo-u.ac.jp
Hiromi Sugimoto
The Japan Institute for
Educational Measurement, Inc.
Tokyo 162-0831, Japan
sugimoto[at]jiem.co.jp
Yukiko Yabuta
The Japan Institute for
Educational Measurement, Inc.
Tokyo 162-0831, Japan
yabuta[at]jiem.co.jp
Abstract
This paper describes a method for recognizing
romanized Japanese words in learner English.
They become noise and problematic in a vari-
ety of tasks including Part-Of-Speech tagging,
spell checking, and error detection because
they are mostly unknown words. A problem
one encounters when recognizing romanized
Japanese words in learner English is that the
spelling rules of romanized Japanese words
are often violated by spelling errors. To ad-
dress the problem, the described method uses
a clustering algorithm reinforced by a small
set of rules. Experiments show that it achieves
an   -measure of 0.879 and outperforms other
methods. They also show that it only requires
the target text and a fair size of English word
list.
1 Introduction
Japanese learners of English frequently use roman-
ized Japanese words in English writing, which will
be referred to as Roman words hereafter; examples
of Roman words are: SUKIYAKI1, IPPAI (many),
and GANBARU (work hard). Approximately 20%
of different words are Roman words in a corpus con-
sisting of texts written by Japanese second and third
year junior high students. Part of the reason is that
they are lacking in English vocabulary, which leads
them to using Roman words in English writing.
Roman words become noise in a variety of tasks.
In the field of second language acquisition, re-
searchers often use a Part-Of-Speech (POS) tagger
1For consistency, we print Roman words in all capitals.
to analyze learner corpora (Aarts and Granger, 1998;
Granger, 1998; Granger, 1993; Tono, 2000). Since
Roman words are romanized Japanese words and
thus are unknown to POS taggers, they degrades the
performance of POS taggers. In spell checking, they
are a major source of false positives because they
are unknown words as just mentioned. In error de-
tection, most methods such as Chodorow and Lea-
cock (2000), Izumi et al (2003), Nagata et al (2005;
2006), and Han et al (2004; 2006) use a POS tag-
ger and/or a chunker to detect errors. Again, Roman
words degrades their performances.
When viewed from another perspective, Roman
words play an interesting role in second language ac-
quisition. It would be interesting to see what Roman
words are used in the writing of Japanese learners of
English. A frequency list of Roman words should
be useful in vocabulary learning and teaching. En-
glish words corresponding to frequent Roman words
should be taught because learners do not know the
English words despite the fact that they frequently
use the Roman words.
To the best knowledge, there has been no method
for recognizing Roman words in the writing of learn-
ers of English as Sect. 2 will discuss. Therefore, this
paper explores a novel method for the purpose. At
first sight, it might appear to be trivial to recognize
Roman words in English writing since the spelling
system of Roman words is very different from that
of English words. On the contrary, it is not because
spelling errors occur so frequently that the rules in
both spelling systems are violated in many cases. To
address spelling errors, the described method uses a
clustering algorithm reinforced with a small set of
27
rules. One of the features of the described method
is that it only requires the target text and a fair size
of an English word list. In other words, it does not
require sources of knowledge such as manually an-
notated training data that are costly to obtain.
The rest of this paper is structured as follows.
Section 2 discusses related work. Section 3 intro-
duces some knowledge of Roman words which is
needed to understand the rest of this paper. Section 4
discusses our initial idea. Section 5 describes the
method. Section 6 describes experiments conducted
to evaluate the method and discusses the results.
2 Related Work
Basically, no methods for recognizing Roman words
have been proposed in the past. However, there have
been a great deal of work related to Roman words.
Transliteration and back-transliteration often in-
volve romanization from Japanese Katakana words
into their equivalents spelled in Roman alphabets as
in Knight and Graehl (1998) and Brill et al (2001).
For example, Knight and Graehl (1998) back-
transliterate Japanese Katakana words into English
via Japanese romanized equivalents.
Transliteration and back-transliteration, however,
are different tasks from ours. Transliteration and
back-transliteration are a task where given English
and Japanese Katakana words are put into their cor-
responding Japanese Katakana and English words,
respectively, whereas our task is to recognize Roman
words in English text written by learners of English.
More related to our task is loanword identifica-
tion; our task can be viewed as loanword identifica-
tion where loanwords are Roman words in English
text. Jeong et al (1999) describe a method for distin-
guishing between foreign and pure Korean words in
Korean text. Nwesri et al(2006) propose a method
for identifying foreign words in Arabic text. Khal-
tar et al (2006) extract loanwords from Mongolian
corpora using a Japanese loanword dictionary.
These methods are fundamentally different from
ours in the following two points. First, the target text
in our task is full of spelling errors both in Roman
and English words. Second, the above methods re-
quire annotated training data and/or other sources of
knowledge such as a Japanese loanword dictionary
that are hard to obtain in our task.
3 Roman Words
This section briefly introduces the spelling sys-
tem of Roman words which is needed to under-
stand the rest of this paper. For detailed discussion
of Japanese-English romanization, see Knight and
Graehl (1998).
The spelling system has five vowels:  a, i, u, e,
o  . It has 18 consonants :  b, c, d, f, g, h, j, k, l, m,
n, p, r, s, t, w, y, z  . Note that some alphabets such
as q and x are not used in Roman words.
Roman words basically satisfy the following two
rules:
1. Roman words end with either a vowel or n
2. A consonant is always followed by a vowel
The first rule implies that one can tell that a word
ending with a consonant except n is not a Roman
word without looking at the whole word. There are
two exceptions to the second rule. The first is that
the consonant n sometimes behaves like a vowel and
is followed by other consonants such as nb as in
GANBARU. The second is that some combinations
of two consonants such as ky and tt are used to ex-
press gemination and contracted sounds. However,
the second rule is satisfied if these combinations are
regarded to function as a consonant to express gem-
ination and contracted sounds. An implication from
the second rule is that alternate occurrences of a
consonant-vowel are very common to Roman words
as in SAMURAI2 and SUKIYAKI. Another is that
a sequence of three consonants, such as tch and btl
as in watch and subtle, respectively, never appear in
Roman words excluding the exceptional consecutive
consonants for gemination and contracted sounds.
In the writing of Japanese learners of English,
the two rules are often violated because of spelling
errors. For example, SHSHI, GZUUNOTOU, and
MATHYA appear in corpora used in the experi-
ments where the underline indicates where the vio-
lations of the rules exist; we believe that even na-
tive speakers of the Japanese language have diffi-
culty guessing the right spellings (The answers are
shown in Sect. 6.2).
2Well-known Japanese words such as SAMURAI and
SUKIYAKI are used as examples for illustration purpose. In
the writing of Japanese learners of English, however, a wide
variety of Japanese words appear as exemplified in Sect. 1.
28
Also, English words are mis-spelled in the writing
of Japanese learners of English. Mis-spelled English
words often satisfy the two rules. For example, the
word because is mis-spelled with variations in error
such as becaus, becose, becoue, becouese, becuse,
becaes, becase, and becaues where the underlines
indicate words that satisfy the two rules.
In summary, the spelling system of Roman words
is quite different from that of English. However, in
the writing of Japanese learners of English, the two
rules are often violated because of spelling errors.
4 Initial (but Failed) Idea
This section discusses our initial idea for the task,
which turned out to be a failure. Nevertheless, this
section discusses it because it will play an important
role later on.
Our initial idea was as follows. As shown in
Sect. 3, Roman words are based on a spelling sys-
tem that is very different from that of English. The
spelling system is so different that a clustering al-
gorithm such as  -means clustering (Abney, 2007)
is able to distinguish Roman words from English
words if the differences are represented well in the
feature vector.
A trigram-based feature vector is well-suited for
capturing the differences. Each attribute in the vec-
tor corresponds to a certain trigram such as sam. The
value corresponds to the number of occurrences of
the trigram in a given word. For example, the value
of the attribute corresponding to the trigram sam is
1 in the Roman word SAMURAI. The dummy sym-
bols ? and $ are appended to denote the beginning
and end of a word, respectively. All words are con-
verted entirely to lowercase when transformed into
feature vectors. For example, the Roman word:
SAMURAI
would give the trigrams:
??s ?sa sam amu mur ura rai ai$ i$$,
and be transformed into a feature vector where the
values corresponding to the above trigrams are 1,
otherwise 0.
The algorithm for recognizing Roman words
based on this initial idea is as follows:
Input: target corpus and English word list
Output: lists of Roman words and English words
Step 1. make a word list from the target corpus
Step 2. remove all words from the list that are in
the English word list
Step 3. transform each word in the resulting list
into the feature vector
Step 4. run  -means clustering on the feature vec-
tors with 
Step 5. output the result
In Step 1., the target corpus is turned into a word
list. In Step 2., words that are in the English word
list are recognized as English words and removed
from the word list. Note that at this point, there will
be still English words on the list because an English
word list is never comprehensive. More importantly,
the list includes mis-spelled English words. In Step
3., each word in the resulting list is transformed into
the feature vector as just explained above. In Step
4.,  -means clustering is used to find two clusters
for the feature vectors; 	 because there are two
classes of words ? one for Roman words and one
for English words. In Step 5., each word is outputted
with the result of the clustering. This was our initial
idea. It was unsupervised and easy to implement.
Contrary to our expectation, however, the results
were far from satisfactory as Sect. 6 will show. The
resulting clusters were meaningless in terms of Ro-
man word recognition. For instance, one of the
obtained two clusters was for gerunds and present
participles (namely, words ending with ing) and the
other was for the rest (including Roman words and
other English words). The results reveal that it is
impossible to represent all English words by one
cluster obtained from a centroid that is initially ran-
domly chosen. The algorithm was tested with dif-
ferent settings (different  and different numbers of
instances to compute the initial centroids). It some-
times performed slightly better, but it was too ad hoc
to be a reliable method.
This is why we had to take another approach. At
the same time, this initial idea will play an important
role soon as already mentioned.
5 Proposed Method
So far, we have seen that a clustering algorithm does
not work well on the task. However, there is no
29
doubt that the spelling system of Roman words is
very different from that of English words. Because
of the differences, the two rules described in Sect. 3
should almost perfectly recognize Roman words if
there were no spelling errors.
To make the task simple, let us assume that there
were no spelling errors in the target corpus for the
time being. Under this assumption, the task is
greatly simplified. As with the initial idea, known
English words can easily be removed from the word
list. Then, all Roman words will be retrieved from
the list with few English words by pattern matching
based on the two rules.
For pattern matching, words are first put into a
Consonant Vowel (CV) pattern. It is simply done
by replacing consonants and vowels as defined in
Sect. 3 with dummy characters denoting consonants
and vowels (C and V in this paper), respectively. For
example, the Roman word:
SAMURAI
would be transformed into the CV pattern:
CVCVCVV
while the English word:
ghter
into the CV pattern:
CVCCCVC.
There are some notable differences between the two.
An exception to the transformation is that the conso-
nant n is replaced with C only when it follows one
of the consonants since it sometimes behaves like
a vowel (see Sect. 3 for details) and requires a spe-
cial care. Before the transformation, the exceptional
consecutive consonants for gemination and contract
sounds are normalized by the following simple re-
placement rules:
double consonants 
 single consonant
(e.g, tt 
 t),
([bdfghjklmnstprz])y([auo]) 
 $1$2
(e.g., bya 
 ba),
([sc])h([aiueo]) 
 $1$2
(e.g., sha 
 sa),
tsu 
 tu
For example, the double consonant tt is replaced
with the single consonant t using the first rule. Then,
a word is recognized as a Roman word if its CV pat-
tern matches:
?[Vn]*(C[Vn]+)*$
where the matcher is written in Perl or Java-like reg-
ular expression. Roughly, words that comprise se-
quences of a consonant-vowel, and end with a vowel
or the consonant n are recognized as Roman words.
This method should work perfectly if we disre-
gard spelling errors. We will refer to this method as
the rule-based method, hereafter. Actually, it works
surprisingly well even with spelling errors as the ex-
periments in Sect. 6 will show. However, there is
still room for improvement in handling mis-spelled
words.
Now back to the real world. The sources of false
positives and negatives in the rule-based method are
spelling errors both in Roman and English words.
For instance, the rule-based method recognizes mis-
spelled English words such as becose, becoue, and
becouese, which are correctly the word because, as
Roman words. Likewise, mis-spelled Roman words
are recognized as English words.
Here, the initial idea comes to play an important
role. Like in the initial idea, each word can be trans-
formed into a point in vector space as exemplified
in a somewhat simplified manner in Fig. 1; R and E
in Fig. 1 denote words recognized by the rule-based
method as Roman and English words, respectively.
Pale R and E correspond to false positives and nega-
tives, (which of course is unknown to the rule-based
method). Unlike in the initial idea, we now know
plausible centroids for Roman and English words.
We can compute the centroid for Roman words from
the words recognized as Roman words by the rule-
based method. Also, we can compute the centroid
for English words from the words in the English
word dictionary. This situation is shown in Fig. 2
where the centroids are denoted by +. False pos-
itives and negatives are expected to be nearer to
the centroids for their true class, because even with
spelling errors they share a structural similarly with
their correctly-spelled counterparts. Taking this into
account, all predictions obtained by the rule-based
method are overridden by the class of their nearest
centroid as shown in Fig. 3. The procedures for com-
puting the centroids and overriding the predictions
can be repeated until convergence. Then, this part is
30
the same as the initial idea based on  -means clus-
tering.


R 
R R 
R 
R 
E 
E 
E 
E 
E 
E 
Figure 1: Roman and English words in vector space


R 
R R 
R 
R 
+ 
E 
E 
E 
E 
E 
E 
+ 
Decision boundary
Figure 2: Plausible centroids
The algorithm of the proposed method is:
Input: target corpus and English word list
Output: list of Roman words
Step A. make a word list from the target corpus
Step B. remove all words from the list that are in
the English word list
Step C. transform each word in the resulting list
into the feature vector
Step D. obtain a tentative list of Roman words using
the rule-based method
Step E. compute centroids for Roman and English
words from the tentative list and the English
word list, respectively

R 
R R 
R 
E
+ 
R
R
E 
E 
E 
E 
+ 
Decision boundary
Figure 3: Overridden false positives and negatives
Step F. override the previous class of each word by
the class of its nearest centroid
Step G. repeat Step E and F until convergence
Step H. output the result
Steps A to C are the same as in the algorithm of the
initial idea. Step D then uses the rule-based method
to obtain a tentative list of Roman words. Step E
computes centroids for Roman and English words
by taking averages of each value of the feature vec-
tors. Step F overrides previous classes obtained by
the rule-based method or previous iteration. The
distances between each feature vector and the cen-
troids are measured by the Euclidean distance. Step
G computes centroids and overrides previous predic-
tions until convergence. This step may be omitted
to give a variation of the proposed method. Step H
outputs words belonging to the centroid for Roman
words.
6 Experiments
6.1 Experimental Conditions
Three sets of corpora were used for evaluation. The
first consisted of essays on the topic winter holiday
written by second year junior high students. It was
used to develop the rule-based method. The second
consisted of essays on the topic school trip written
by third year junior high students. The third was
the combination of the two. Table 1 shows the tar-
get corpora statistics3. Evaluation was done on only
unknown words in the target corpora since known
31
Table 1: Target corpora statistics
Corpus # sentences # words # diff. words # diff. unknown words # diff. Roman words
Jr. high 2 9928 56724 1675 1040 275
Jr. high 3 10441 60546 2163 1334 500
Jr. high 2&3 20369 117270 3299 2237 727
words can be easily recognized as English words by
referring to an English word list.
As an English word list, the 7,726 words (Leech
et al, 2001) that occur at least 10 times per mil-
lion words in the British National Corpus (Burnard,
1995) were combined with the English word list in
Ispell, the spell checker. The whole list consisted of
19816 words.
As already mentioned in Sect. 2, there has been no
method for recognizing Roman words. Therefore,
we set three baselines for comparison. In the first,
all words that were not listed in the English word list
were recognized as Roman words. In the second,
 -means clustering was used to recognize Roman
words in the target corpora as described in Sect. 4
(i.e., the initial idea). The  -means clustering-based
method was tested on each target corpora five times
and the results were averaged to calculate the overall
performances. Five instances were randomly chosen
to compute the initial centroids for each class. In the
third, the rule-based method described in Sect. 5 was
used as a baseline.
The performance was evaluated by recall, preci-
sion, and  -measure. Recall and precision were de-
fined by


# Roman words correctly recognized
# diff. Roman words (1)
and


# Roman words correctly recognized
# words recognized as Roman words  (2)
respectively.  -measure was defined by
	


 (3)
3From the Jr. high 2&3 corpus, we randomly took 200 sen-
tences (1645 words) to estimate the spelling error rate. It was an
error rate of 2.8% (46/1645). We also investigated if there was
ambiguity between Roman and English words in the target cor-
pora (for example, the word sake can be a Roman word (a kind
of alcohol) and an English word (as in God?s sake). It turned
out that there were no such cases in the target corpora.
6.2 Experimental Results and Discussion
Table 2, Table 3, and Table 4 show the experimen-
tal results for the target corpora. In the tables, List-
based, K-means, and Rule-based denote the English
word list-based,  -means clustering-based, and rule-
based baselines, respectively. Also, Proposed (itera-
tion) and Proposed denote the proposed method with
and without iteration, respectively.
Table 2: Experimental results for Jr. high 2
Method   
List-based 1.00 0.268 0.423
 -means 0.737 0.298 0.419
Rule-based 0.898 0.737 0.810
Proposed (iteration) 0.855 0.799 0.826
Proposed 0.938 0.761 0.840
Table 3: Experimental results for Jr. high 3
Method   
List-based 1.00 0.382 0.553
 -means 0.736 0.368 0.490
Rule-based 0.824 0.831 0.827
Proposed (iteration) 0.852 0.916 0.883
Proposed 0.914 0.882 0.898
Table 4: Experimental results for Jr. high 2&3
Method   
List-based 1.00 0.331 0.497
 -means 0.653 0.491 0.500
Rule-based 0.849 0.794 0.820
Proposed (iteration) 0.851 0.867 0.859
Proposed 0.922 0.840 0.879
The results show that the English word list-based
baseline does not work well. The reason is that mis-
32
spelled words occur so frequently in the writing of
Japanese learners of English that simply recogniz-
ing unknown words as Roman words causes a lot of
false positives.
The  -means clustering-based baseline performs
similarly or even worse in terms of  -measure. Sec-
tion 4 has already discussed the reason. Namely, it
is impossible to represent all English words by one
cluster obtained by simple  -means clustering.
Unlike the other two, the rule-based baseline per-
forms surprisingly well considering the fact that it is
based on a simple (pattern matching ) rule. This in-
dicates that the spelling system of Roman words is
quite different from that of English words. Thus, it
would almost perfectly perform for English writing
without spelling errors.
The proposed methods further improve the per-
formance of the rule-based method in all target cor-
pora. Especially, the proposed method without it-
eration performs well. Indeed, it performs signif-
icantly better than the rule-based method does in
both recall (99% confidence level, difference of pro-
portion test) and precision (95% confidence level,
difference of proportion test) in the whole corpus.
They reinforce the rule-based method by overriding
false positives and negatives via centroid identifica-
tion as initially estimated from the results of the rule-
based method as Fig. 1, Fig.2, and Fig. 3 illustrate
in Sect. 5. This implies that the estimated centroids
represent Roman and English words well. Because
of this property, the proposed methods can distin-
guish mis-spelled Roman words from (often mis-
spelled) English words. Interestingly, the proposed
methods recognized mis-spelled Roman words that
we would prove are difficult for even native speakers
of the Japanese language to recognize as words; e.g.,
SHSHI, GZUUNOTOU, and MATHYA; correctly,
SUSHI, GOZYUNOTOU (five-story pagoda), and
MATTYA (strong green tea).
To see the property, we extracted characteristic
trigrams of the Roman and English centroids. We
sorted each trigram in descending and ascending or-
ders by Coling 2010: Poster Volume, pages 894?900,
Beijing, August 2010
Evaluating performance of grammatical error detection to maximize
learning effect
Ryo Nagata
Konan University
rnagata @ konan-u.ac.jp.
Kazuhide Nakatani
Konan University
Abstract
This paper proposes a method for eval-
uating grammatical error detection meth-
ods to maximize the learning effect ob-
tained by grammatical error detection.
To achieve this, this paper sets out the
following two hypotheses ? imperfect,
rather than perfect, error detection max-
imizes learning effect; and precision-
oriented error detection is better than a
recall-oriented one in terms of learning ef-
fect. Experiments reveal that (i) precision-
oriented error detection has a learning ef-
fect comparable to that of feedback by a
human tutor, although the first hypothesis
is not supported; (ii) precision-oriented er-
ror detection is better than recall-oriented
in terms of learning effect; (iii)   -measure
is not always the best way of evaluating
error detection methods.
1 Introduction
To reduce the efforts taken to correct grammat-
ical errors in English writing, there has been a
great deal of work on grammatical error detec-
tion (Brockett et al, 2006; Chodorow and Lea-
cock, 2000; Chodorow and Leacock, 2002; Han
et al, 2004; Han et al, 2006; Izumi et al, 2003;
Nagata et al, 2004; Nagata et al, 2005; Nagata
et al, 2006). One of its promising applications
is writing learning assistance by detecting errors
and showing the results to the learner as feedback
that he or she can use to rewrite his or her essay.
Grammatical error detection has greatly improved
in detection performance as well as in the types of
the errors it is able to detect, including errors in
articles, number, prepositions, and agreement.
In view of writing learning assistance, how-
ever, one important factor has been missing in
the previous work. In the application to writ-
ing learning assistance, error detection methods
should be evaluated by learning effect obtained
by error detection. Nevertheless, they have been
evaluated only by detection performance such as
  -measure.
This brings up a further research question ?
are any of the previous methods effective as writ-
ing learning assistance? It is very important to an-
swer this question because it is almost impossible
to develop a perfect method. In other words, one
has to use an imperfect method to assist learners
no matter how much improvement is achieved. In
practice, it is crucial to reveal the lower bound of
detection performance that has a learning effect.
Related to this, one should discuss the follow-
ing question. Most error detection methods are
adjustable to be recall-oriented/precision-oriented
by tuning their parameters. Despite this fact,
no one has examined which is better in terms
of learning effect ? recall-oriented or precision-
oriented? (hereafter, this problem will be referred
to as the recall-precision problem). Chodorow
and Leacock (2000) and Chodorow et al (2007)
argue that precision-oriented is better, but they do
not give any concrete reason. This means that the
recall-precision problem has not yet been solved.
Accordingly, this paper explores the relation
between detection performance and learning ef-
fect. To do this, this paper sets out two hypothe-
ses:
Hypothesis I : imperfect, rather than perfect, er-
ror detection maximizes learning effect
Hypothesis II : precision-oriented is better than
recall-oriented in terms of learning effect
Hypothesis I contradicts the intuition that the
better the detection performance is, the higher the
learning effect is. To see the motivation for this,
894
suppose that we had a perfect method. It would
detect all errors in a given essay with no false-
positives. In that case, the learner would not have
to find any errors by himself or herself. Neither
would he or she have to examine the causes of
the errors. In the worst case, they just copy the
detection results. By contrast, with an imperfect
method, he or she has to do these activities, which
is expected to result in better learning effect. Be-
sides, researchers, including Robb et al (1986),
Bitchener et al (2005), and Ferris and Roberts
(2001), report that the amount of feedback that
learners receive does not necessarily correspond
to the amount of learning effect. For instance,
Robb et al (1986) compared four types of feed-
back ((1) error detection and correction, (2) error
detection and error type, (3) error detection, and
(4) number of errors per line) and reported that
(1), the most-detailed feedback, did not necessar-
ily have the highest learning effect.
Hypothesis II concerns the recall-precision
problem. If a limited number of errors are
detected with high precision (i.e., precision-
oriented), learners have to carefully read their own
essay to find the rest of the errors by examining
whether their writing is correct or not, using sev-
eral sources of information including (i) the in-
formation that can be obtained from the detected
errors, which is useful for finding undetected er-
rors similar to the detected ones; (ii) their knowl-
edge on English grammar and writing, and (iii)
dictionaries and textbooks. We believe that learn-
ing activities, especially learning from similar in-
stances, have a favorable learning effect. By con-
trast, in a recall-oriented setting, these activities
relatively decrease. Instead, learners focus on
judging whether given detection results are correct
or not. Besides, learning from similar instances is
likely not to work well because a recall-oriented
setting frequently makes false-positives.
This paper proposes a method for testing the
two hypotheses in Sect. 2. It conducts experiments
based on the method in Sect. 3. It discusses the ex-
perimental results in Sect. 4.
2 Method
We conducted a pre-experiment where ten sub-
jects participated and wrote 5.6 essays on average.
We used the obtained data to design the method.
2.1 Target Errors
To obtain general conclusions, one has to test Hy-
pothesis I and Hypothesis II against a variety of
errors and also a variety of error detection meth-
ods. However, it would not be reasonable or fea-
sible to do this from the beginning.
Considering this, this paper targets errors in ar-
ticles and number. The reasons for selecting these
are that (a) articles and number are difficult for
learners of English (Izumi et al, 2003; Nagata et
al., 2005), and (b) there has been a great deal of
work on the detection of these errors.
2.2 Error detection method
Among the previous methods for detecting errors
in articles and number, this paper selects Nagata et
al. (2006)?s method that detects errors in articles
and number based on countability prediction. It
has been shown to be effective in the detection of
errors in articles and number (Nagata et al, 2005;
Nagata et al, 2006). It also has the favorable prop-
erty that it can be adjusted to be recall-oriented or
precision-oriented by setting a threshold for the
probability used in countability prediction. This
subsection briefly describes Nagata et al (2006)?s
method (See Nagata et al (2006) for the details).
The method, first, automatically generates
training instances for countability prediction. In-
stances of each noun that head their noun phrase
(NP) are collected from a corpus with their sur-
rounding words. Then, the collected instances are
tagged with their countability by a set of hand-
coded rules. The resulting tagged instances are
used as training data for countability prediction.
Decision lists (Yarowsky, 1995) are used to pre-
dict countability. Tree types of contextual cue are
used as features: (i) words in the NP that the target
noun heads; (ii) three words to the left of the NP;
(iii) three words to its right. The log-likelihood ra-
tio (Yarowsky, 1995) decides in which order rules
in a decision list are applied to the target noun in
countability prediction. It is the log ratio of the
probabilities of the target noun being count and
non-count when one of the features appears in its
context. To predict countability in error detection,
each rule in the decision list is tested on the target
895
noun in the sorted order until the first applicable
one is found. The prediction is made by the first
applicable one.
After countability prediction, errors in articles
and number are detected by using a set of rules.
For example, if the noun in question is plural and
predicted to be non-count, then it is an error. Sim-
ilarly, the noun in question has no article and is
singular and is predicted to be count, then it is an
error.
The balance of recall and precision in error de-
tection can be adjusted by setting a certain thresh-
old to the probabilities used to calculate the log-
likelihood ratio1. If the probability of the applied
rule in countability prediction is lower than a cer-
tain threshed, error detection is blocked. Namely,
the higher the threshed is, the more precision-
oriented the detection is.
2.3 Learning Activity
The proposed method is based on a learning ac-
tivity consisting of essay writing, error detection,
and rewriting. Table 1 shows the flow of the learn-
ing activity. In Step 1, an essay topic is assigned
to learners. In Step 2, they have time to think
about what to write with a piece of white paper for
preparation (e.g., to summarize his or her ideas).
In Step 3, they write an essay on a blog system in
which the error detection method (Nagata et al,
2005) is implemented. This system allows them
to write, submit, and rewrite their essays (though
it does not allow them to access the others? es-
says or their own previous essays). They are not
allowed to use any dictionary or textbook in this
step. They are required to write ten sentences or
more. In Step 4, the system detects errors in each
essay. It displays each essay of which errors are
indicated in red to the corresponding learner. Al-
though the detection itself takes only a few sec-
onds, five minutes are assigned to this step for two
purposes: to take a short break for learners and
to remove time differences between learners. Fi-
nally, in Step 5, learners rewrite their essay using
the given feedback. Here, they are allowed to use
1Setting a threshold to the probability is equivalent to set-
ting a threshold to the log-likelihood and both has the same
effect on the balance of recall and precision. However, we
use the former because it is intuitive and easy to set a thresh-
old
Table 1: Flow of learning activity
Procedure Min
1. Learner is assigned an essay topic ?
2. Learner prepares for writing 5
3. Learner writes an essay 35
4. System detects errors in the essay 5
5. Learner rewrites the essay 15
a dictionary (Konishi and Minamide, 2007) and
an A4 paper that briefly explains article and num-
ber usage, which was made based on grammar
books (Hirota, 1992; Iizuka and Hagino, 1997).
They are informed that the feedback may contain
false-positives and false-negatives.
2.4 How to Measure Learning Effect
Before discussing how to measure learning effect,
one has to define the ability to write English. Con-
sidering that this paper aims at the evaluation of
error detection, it is reasonable to define the abil-
ity as the degree of error occurrence (that is, the
fewer errors, the better). To measure this, this pa-
per uses error rate, which is defined by

Number of target errors in Step 3 
Number of NPs in Step 3   (1)
Ones (?  ?) are added to the numerator and de-
nominator for a mathematical reason that will be
clear shortly. The addition also has the advan-
tage that it can evaluate a longer essay to be better
when no errors occur.
Having defined ability, it is natural to measure
learning effect by a decrease in the error rate. Sim-
ply, it is estimated by applying the linear regres-
sion to the number of instances of learning and the
corresponding error rates.
Having said this, this paper applies an expo-
nential regression instead of the linear regression.
There are two reasons for this. The first is that
it becomes more difficult to decrease the error
rate as it decreases (in other words, it becomes
more difficulty to improve one?s ability as one im-
proves). The other is that the error rate is expected
to asymptotically decrease to zero as learning pro-
ceeds. The exponential regression is defined by
	

ffProceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1940?1949, Dublin, Ireland, August 23-29 2014.
Language Family Relationship Preserved in Non-native English
Ryo Nagata
Konan University
8-9-1 Okamoto, Higashinada, Kobe, Hyogo 658-8501, Japan
nagata-coling@hyogo-u.ac.jp
Abstract
Mother tongue interference is the phenomenon where linguistic systems of a mother tongue
are transferred to another language. Recently, Nagata and Whittaker (2013) have shown that
language family relationship among mother tongues is preserved in English written by Indo-
European language speakers because of mother tongue interference. At the same time, their
findings further introduce the following two research questions: (1) Does the preservation uni-
versally hold in non-native English other than in English of Indo-European language speakers?
(2) Is the preservation independent of proficiency in English? In this paper, we address these
research questions. We first explore the two research questions empirically by reconstructing
language family trees from English texts written by speakers of Asian languages. We then dis-
cuss theoretical reasons for the empirical results. We finally introduce another hypothesis called
the existence of a probabilistic module to explain why the preservation does or does not hold in
particular situations.
1 Introduction
Transfer of linguistic systems of a mother tongue to another language, namely mother tongue interfer-
ence, is often observable in the writing of non-native speakers. The reader may be able to determine
the mother tongue of the writer of the following sentence from the underlined article error: The alien
wouldn?t use my spaceship but the hers. The answer would probably be French or Spanish; the defi-
nite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes
negatively transferred to English writing.
Researchers in corpus linguistics including Swan and Smith (2001), Aarts and Granger (1998),
and Altenberg and Tapper (1998) have been working on mother tongue interference to reveal
overused/underused words, part of speech (POS), or grammatical items. Recently, Nagata and Whittaker
(2013) have shown that language family relationship between mother tongues is preserved in English
written by Indo-European language speakers; because of the preservation, one can reconstruct a lan-
guage family tree similar to the canonical Indo-European family tree (Beekes, 2011; Ramat and Ramat,
2006) from their English writings. They have further revealed factors contributing to the preservation
of the language family relationship, which they show is useful for related natural language processing
(NLP) tasks such as grammatical error detection/correction and native language identification (Wong and
Dras, 2009).
At the same time, their findings further introduce the following two research questions: (1) Does the
preservation universally hold in non-native English? (2) Is the preservation independent of proficiency
in English? The results (Nagata and Whittaker, 2013) for English written by Indo-European language
speakers suggest that the answer to question (1) is yes. Based on this, we hypothesize that:
Hypothesis I: The preservation of language family relationship universally holds in non-native English.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1940
However, one can counter Hypothesis I, arguing that the preservation holds only in English written
by Indo-European language speakers because Indo-European languages share large part of linguistic
properties with English which is a member of the Indo-European languages, which contributes to the
preservation. Apparently, this is not the case in languages in other language families. In these languages,
other properties than language family relationship may be more dominant. Furthermore, Kachru?s Three
Circles of English (Kachru, 1992) raises a question. In Kachru?s model, world Englishes are classified
into the inner, outer, and expanding circles. The inner circle roughly corresponds to the traditional native
speakers of English. The outer circle refers to the non-native varieties in the regions where English
serves as a useful lingua franca. The expanding circle roughly corresponds to the other non-native
speakers of English. Then, it would be difficult to answer question (1) for the outer circle of English
(e.g., English in Hong Kong). For example, on one hand, English in Hong Kong is expected to have
mother tongue interference from Chinese language. From this point of view, it is expected to have the
family relationship with the Sino-Tibetan language family. On the other hand, one can point out that
the outer circle of English should be closer to native English than the expanding circle of English (e.g.,
English in China) is. This implies that English in Hong Kong might have some other relationship with the
members in the outer circle. For question (2), the answer is likely no considering that theoretically, the
higher one?s proficiency is, the closer to native English his or her English becomes; it would be difficult
to distinguish between native English and English of non-native speakers whose proficiency is very high.
With this reason, we hypothesize that:
Hypothesis II: The preservation of language family relationship is dependent on proficiency in English.
In view of this background, we address these research questions in this paper. We first examine the two
hypotheses empirically by reconstructing language family trees from English texts written by speakers
of Asian languages, including the outer and expanding circles of English. If we can reconstruct language
family trees similar to their canonical family trees from these English texts, it will be a good piece of
evidence for Hypothesis I. Similarly, to examine Hypothesis II, we reconstruct a language family tree
from the English texts using the information about their proficiency levels. If we cannot reconstruct
language family trees similar to the canonical trees, Hypothesis II will be accepted. We then explore
theoretical reasons for the empirical results. We finally introduce another hypothesis called the existence
of a probabilistic module to explain why the preservation does or does not hold in particular situations.
The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work.
Sect. 3 and Sect. 4 examine Hypothesis I and Hypothesis II, respectively. Sect. 5 describes theoretical
reasons for the experimental results.
2 Approach
2.1 Data Set
Through this paper, we use the International Corpus Network of Asian Learners of English (IC-
NALE) (Ishikawa, 2011) as the target data to examine the two hypotheses. ICNALE consists of English
essays of the outer and expanding circles of English in Asia together with those of native speakers of
English. Table 1 (a) shows the statistics on ICNALE.
In ICNALE, each essay, except native essays, is annotated with a proficiency level of the writer,
ranging from A 2 (lowest) B1 1, B1 2, to B2+ (highest); Table 1 (b) shows the correspondence between
these four proficiency levels and TOEIC scores. We use this information to examine Hypothesis II.
2.2 Method for Reconstructing Language Family Trees
We employ the method proposed by Nagata and Whittaker (2013) for reconstructing language family
trees, which in turn is based on the method proposed by Kita (1999). In this method, each group of
the essays in ICNALE is modeled by an n-gram language model. Then, agglomerative hierarchical
clustering (Han and Kamber, 2006) is applied to the language models to reconstruct a language family
tree. The distance used for clustering is based on a divergence-like distance between two language
models that was originally proposed by Juang and Rabiner (1985).
1941
Category # of essays # of tokens
Native 400 88,792
Outer Circle
Hong Kong 200 46,111
Pakistan 400 93,100
Philippines 400 96,586
Singapore 400 96,733
Expanding Circle
China 800 194,613
Indonesia 400 92,316
Japan 800 176,537
Korea 600 130,626
Thailand 800 176,936
Taiwan 400 89,736
(a) Statistics on ICNALE
Level A2 B1 1 B1 2 B2+
Score 225-549 550-669 670-784 785+
(b) Correspondence between the Proficiency Levels and
TOEIC Score
Table 1: Summary of ICNALE.
To explain the method in more detail, let us define the following symbols used in the method. Let D
i
be a set of English texts where i denotes a mother tongue i. Similarly, letM
i
be a language model trained
using D
i
.
To reduce the influences from the topics of the data set, we use an n-gram language model based on
a mixture of word and POS tokens. In this language model, content words in n-grams are replaced with
their corresponding POS tags. This greatly decreases the influence of the topics of texts. It also decreases
the number of parameters in the language model.
To build the language model, the following three preprocessing steps are applied to D
i
. First, texts
in D
i
are split into sentences. Second, each sentence is tokenized, POS-tagged, and mapped entirely to
lowercase. For instance, the example sentence in Sect. 1 would give:
the/DT alien/NN would/MD not/RB use/VB my/PRP$ spaceship/NN but/CC the/DT hers/PRP
./.
Finally, words are replaced with their corresponding POS tags; for the following words, word tokens are
used as their corresponding POS tags: coordinating conjunctions, determiners, prepositions, modals, pre-
determiners, possessives, pronouns, question adverbs. Also, proper nouns are treated as common nouns.
At this point, the special POS tags BOS and EOS are added at the beginning and end of each sentence,
respectively. For instance, the above example would result in the following word/POS sequence:
BOS the NN would RB VB my NN but the hers . EOS.
Note that the content of the original sentence is far from clear while reflecting mother tongue interference,
especially in the hers.
Now, the language model M
i
can be built from D
i
. We set n = 3 (i.e., trigram language model) and
use Kneser-Ney (KN) smoothing (Kneser and Ney, 1995) to estimate its conditional probabilities.
The clustering algorithm used is agglomerative hierarchical clustering with the average linkage
method. The distance
1
between two language models is measured as follows. The probability that
M
i
generates D
i
is calculated by Pr(D
i
|M
i
). Note that
Pr(D
i
|M
i
) ? Pr(w
1,i
) Pr(w
2,i
|w
1,i
)
|D
i
|
?
t=3
Pr(w
t,i
|w
t?2,i
, w
t?1,i
) (1)
1
It is not a distance in a mathematical sense. However, we will use the term distance following the convention in the
literature.
1942
where w
t,i
and |D
i
| denote the tth token in D
i
and the number of tokens in D
i
, respectively, since we
use the trigram language model. Then, the distance from M
i
to M
j
is defined by
d(M
i
? M
j
) ?
1
|D
j
|
log
Pr(D
j
|M
j
)
Pr(D
j
|M
i
)
. (2)
In other words, the distance is determined based on the ratio of the probabilities that each language model
generates the language data. Because d(M
i
? M
j
) and d(M
j
? M
i
) are not symmetrical, we define
the distance between M
i
and M
j
to be their average:
d(M
i
,M
j
) ?
d(M
i
? M
j
) + d(M
j
? M
i
)
2
. (3)
Equation (3) is used to calculate the distance between two language models for clustering.
To sum up, the procedure of the language family tree construction method is as follows: (i) Preprocess
each D
i
; (ii) Build M
i
from D
i
; (iii) Calculate the distances between the language models; (iv) Cluster
the language data using the distances; (v) Output the result as a language family tree.
3 Reconstructing Language Family Trees from Asian English
We used the whole ICNALE as the target data. We used a POS-tagger with the Penn Treebank Tag-
set (Santorini, 1990), which we had specially developed for analyzing non-native English; we trained it
on native and non-native corpora we had manually annotated with POS tags, part of which is available to
the public as the Konan-JIEM (KJ) learner corpus (Nagata et al., 2011). Then, we generated a cluster tree
from the corpus data using the method described in Subsect. 2.2. We used the Kyoto Language Modeling
toolkit
2
to build language models from the corpus data. We removed n-grams that appeared less than
five times
3
in each subcorpus in the language models.
Fig. 1 shows the resulting cluster tree. The number at each branching node denotes in which step the
two clusters were merged.
The cluster tree supportsHypothesis I that the preservation of language family relationship universally
holds in non-native English. Although the detailed language family relationship is less well-known in
these Asian languages than in the Indo-European languages, still the cluster tree shown in Fig. 1 reflects
a rational interpretation of their language family relationship. In the cluster tree, Taiwanese and Chinese
Englishes are first merged into a cluster. This perfectly agrees with the fact that their mother tongues are
primarily Chinese and thus both should belong to the Sino-Tibetan language family. In turn, Japanese
and Korean Englishes are merged into a cluster. Their mother tongues are said to be a member of the
Altaic language family. Admittedly, it is still controversial whether the two languages belong to the
Altaic language family or not. However, the current research often treats them as a member of the
Altaic language family (Crystal, 1997). After Japanese and Korean Englishes, Thai and Indonesian
Englishes are merged in to a cluster of which mother tongues belong to different language families; the
former belong to the Thai language family while the latter mostly belong to the Austronesian language
family. Having said that, it has been pointed out that Thai has some language family relationship with
the Austronesian language family (Crystal, 1997). All these observations support Hypothesis I.
Interestingly, the cluster tree shown in Fig. 1 preserves, together with language family relationship,
the three circles of English, namely, the inner (native), outer, and expanding circles of English with an
exception of Pakistani English. This can be interpreted as that some other properties are more dominant
than language family relationship in the outer circle of English. An implication from this is that we should
not treat the outer and expanding circles as a group of non-native speakers of English but separately as
different groups in the related NLP tasks such as grammatical error correction. For example, a method
performing well on the outer circle of English (e.g., the NUS corpus (Dahlmeier et al., 2013)) does not
necessarily perform equally well on the expanding circle of English (e.g., the CLC corpus) and vice
versa. Similarly, a model trained on English written by Indo-European language speakers may perform
2
The Kyoto Language Modeling toolkit: http://www.phontron.com/kylm/
3
We found that the results were not sensitive to the value of frequency cutoff so long as we set it to a small number.
1943
7
11
PhilippineEnglish
5
Hon KongEnglish SingaporeanEnglishThai English Indonesian English
3
Chinese English TaiwaneseEnglish
1
6 Inner circle  
Expanding circle
Outer circle
JapaneseEnglish KoreanEnglish
2
Native English1 Native English2
4
8
10
9
Pakistani English
Figure 1: Cluster Tree Reconstructed from Asian Englishes (ICNALE).
better on Chinese English than a model trained on Hon Kong English does. Above all, the subtree for
the outer circle of English is a piece of evidence that partly denies Hypothesis I.
We further reconstructed a clustering tree from the same data set using 5-gram language models so that
the resulting clustering reflects longer-distance syntactic relations. Fig. 2 shows the resulting cluster tree,
which reveals that the tree is almost the same as in Fig. 1 with an exception of the Philippine English.
After having observed all these, it would be rational to partly accept Hypothesis I and to modify it as
follows:
Hypothesis I
?
: The preservation of language family relationship universally holds in
the expanding circle of English.
4 Exploring Correlation between the Preservation and Proficiency
The simplest way to examine Hypothesis II would be clustering that uses only either high-proficiency or
low-proficiency essays. However, it is not so straightforward because the distribution of each proficiency
level varies depending on the English groups. Particularly, some of the 10 non-native Englishes contains
no or very few low-proficiency essays
4
.
As a simple solution, we first generated a clustering tree from only the high-proficiency essays (B1 2
and B2+) with the same conditions as in Sect. 3. As a more sophisticated solution, we created a new
data set from ICNALE so that one of the two Englishes merged into a cluster in Fig. 1 consists of only
low-proficiency essays and the other of only high-proficiency essays. For instance, we used only low-
proficiency essays (A 2 and B1 1) for Chinese English and only high-proficiency essays (B1 2 and B2+)
for Taiwanese English. Then, we generated another cluster tree from the new data set again with the
same conditions as in Sect. 3. In addition, as a reference, we generated a cluster tree only using the
information about the proficiency levels. In this clustering, we created a vector for each English whose
elements and values corresponded to the four proficiency levels and the relative frequencies of the essays
falling into the corresponding proficiency level
5
. In this method, we defined the distance for clustering
by the Euclidean distance between two vectors.
The idea behind this experiment is as follows. If the preservation is completely independent of profi-
ciency, we will obtain the exact same tree as in Fig.1 both from the only-high-proficiency data set and the
high-low proficiency-paired data set. Otherwise, the cluster tree will result in a different form, similar to
the one obtained by the vector-based method solely relying on the information about proficiency.
Fig. 1 and Fig. 3 show the cluster trees obtained from the only-high-proficiency data set and the high-
low proficiency-paired data set, respectively. In the case of the only-high-proficiency data set, the result-
ing tree is the exact same as in the one generated from the original data set. Fig. 3 also shows that the
cluster tree is very similar to that in Fig. 1. Besides, both tree are far from the cluster tree obtained by the
4
For instance, Singapore English contains no low-proficiency essays (A2 and B1 1), and Philippine English 26 essays out
of 400. See http://language.sakura.ne.jp/icnale/ for the complete list of the distribution.
5
We create vectors for the native English essays by setting 1.0 to the element corresponding to B2+ and 0.0 to the others
because proficiency levels are not available for the native English essays in ICNALE.
1944
8
11
PhilippineEnglish
4
Hon KongEnglish SingaporeanEnglishThai English Indonesian English
3
6 Inner circle  
Expanding circle
Outer circle
JapaneseEnglish KoreanEnglish
5
Native English1 Native English2
2
7
10 9
Pakistani EnglishChinese English TaiwaneseEnglish
1
Figure 2: Cluster Tree Reconstructed from Asian Englishes (ICNALE) using 5-gram language models.
vector-based method solely relying on the information about proficiency as shown in Fig. 4. In summary,
Fig. 1 to 4 show that the preservation of language family relationship holds in the expanding circle of
English regardless of proficiency in English.
These results deny Hypothesis II that the preservation of language family relationship is dependent
on proficiency in English. Contrary to our expectation, they support
6
:
Hypothesis II
?
: The preservation of language family relationship is independent of proficiency in En-
glish.
5 Discussion
The experiments show that the tree generation method relying on the distributions of word/POS se-
quences reconstructs from Asian Englishes cluster trees reflecting the family relationship in the Asian
languages. These empirical findings, together with those about English written by Indo-European lan-
guage speakers (Nagata and Whittaker, 2013), support Hypothesis I
?
.
In order to explain theoretically Hypothesis I
?
, we introduce another hypothesis called the existence
of a probabilistic module, that is, that a probabilistic module that stores the distributional information
exists in the human brain. We hypothesize that the probabilistic module consists of sets of probabilities
where each set corresponds to a linguistic item which has arbitrariness in its use; the arbitrariness is
expressed by means of the probabilities that one of the candidates allowed in the linguistic item is chosen
in one?s mother tongue. An example of such a linguistic item would be the position of adverb in English
where the probabilities in this case represent how likely adverbs appear in certain positions (e.g., the
beginning, middle, and end of a sentence). The probabilistic module is equipped with the values of
the probabilities which are set according to one?s mother tongue. To be precise, in our hypothesis, the
probabilities are adapted as follows: (1) proto-languages had developed their values of the probabilities
and handed them down to their descendants; (2) over the time, some of the values changed and the
others remained unchanged; (3) in turn, the decedent languages handed their values of the probabilities
to their descendants with the changes. An example of this would be as follows. The proto-Indo-European
language handed down its values of the probabilities to, for example, the Proto-Germanic language and
the Proto-Italic language with some changes in the values. Then the Proto-German language handed
them down to the Germanic languages such as German and Dutch, again with some changes. So did the
Proto-Italic language to the romance languages such as French and Italian. Therefore, the values of the
probabilities in German should be more similar to those in Dutch than to those in French or Italian.
With this probabilistic module in the human brain, we can naturally explain the preservation of lan-
guage family relationship. When non-native speakers use English, the candidates of the arbitrary lin-
guistic items in English are chosen according to the probabilistic module adapted to their mother tongue.
6
It would be worth while to see if Hypothesis II holds in the case of Indo-European Englishes. The difficult part is that
there are only a few data annotated with proficiency levels.
1945
8
11
PhilippineHIGH
6
Hon KongLOW SingaporeanHIGHThai     LOW Indonesian HIGH
4
Chinese LOW TaiwaneseHIGH
2
5
JapaneseLOW KoreanHIGH
1
Native English1 Native English2
3
7
10 9
Pakistani LOW
Figure 3: Cluster Tree Reconstructed from the High-low Proficiency-paired ICNALE Data Set (HIGH:
high proficiency; LOW: low proficiency).
7
11
PhilippineEnglish
9
SingaporeanEnglishThai English IndonesianEnglish
2
Chinese EnglishTaiwaneseEnglish
3
KoreanEnglish Native English1 Native English2
1
4
10
5
Pakistani EnglishJapaneseEnglish
6
Hon KongEnglish
8
Figure 4: Cluster Tree Generated Based on Only Proficiency Levels.
For example, speakers of languages which have a preference for sentence-beginning adverbs would also
prefer sentence-beginning adverbs in English writing. Accordingly, the values of the probabilities are
implicitly encoded in word/POS sequences such as BOS RB , and NN RB .
7
in their English writings, and
thus the tree generation method can recognize language family relationship as language family trees via
the trigram language model. Provided that the probabilistic module exists in the human brain, this argu-
ment can be made about any mother tongues and the target language (not only English) as long as they
have arbitrary linguistic items in their language systems, which should be the case in most languages.
This is of course another hypothesis and we need more data and evidence to examine the hypothesis.
Nagata and Whittaker (2013) show some evidence that implies the existence of a probabilistic module.
They reveal that Englishes written by Indo-European language speakers exhibit certain probabilistic pat-
terns at least in the way of constructing noun phrases (NPs), adverb positions, and article use, reflecting
the Italic, Germanic, and Slavic branches of the Indo-European family. Take as an example Fig. 5 (i)
which shows frequencies of the trigram NN of NN in English written by Indo-European language speak-
ers
8
. Here, note that English language has arbitrariness between the noun-noun compound and the NN
of NN construction to form an NP (e.g., education system vs. system of education). Fig. 5 (i) reveals
that speakers of the Italic languages (French, Italian, and Spanish) which have a preference for the NN
of NN construction over the noun-noun compound exhibit relatively high frequencies of the trigram NN
of NN in English writing. Conversely, speakers of the Germanic languages (Dutch, Swedish, German,
and Norwegian) have a preference for the noun-noun compound over the NN of NN construction ac-
cordingly exhibit lower frequencies of the trigram NN of NN. In total, the frequencies roughly classify
the 11 Englishes into three groups corresponding to the Italic, Slavic, and Germanic branches of the
7
These two trigrams roughly correspond to adverbs at the beginning and end of a sentence, respectively.
8
The ICLE corpus (Granger et al., 2009) was used to calculate the frequencies. The three letters such as FRA in Fig. 5 and
Fig. 6 denote the ISO 31661 alpha-3 codes except NS1 (Native Speaker 1) and NS2 (Native Speaker 2).
1946
Indo-European language family.
(i) Indo-European language speakers (ICLE)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
FRA ITA ESP POL RUS BGR CZE NLD SWE DEU NOR
Re
lati
ve 
fre
que
ncy
 of 
NN
 of 
NN
 (%
)
Italic Slavic Germanic
(ii) Asian language speakers (ICNALE)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
CHN TWN JPN KOR IDN THA SIN HKG PHL NS1 NS2 PAK
Re
lati
ve 
fre
que
ncy
 of 
NN
 of 
NN
 (%
)
Figure 5: Relative Frequency of NN of NN in English Texts Written by Non-native Speakers of English.
The data of Asian Englishes we used in the experiments exhibit similar tendencies. Fig. 5 (ii) shows
frequencies of the trigram NN of NN for the Asian Englishes together with the native Englishes (denoted
as NS1 and NS2). Fig. 5 (ii) reveals that the pairs of Englishes which share language family relationship
each other exhibit similar frequencies of the trigram NN of NN as in Fig. 5. Furthermore, Fig. 6 (i)
shows a similar tendency in the distribution of adverb positions. The horizontal and vertical axes of
Fig. 6 correspond to the ratios of adverbs at the beginning and the end of sentences, respectively, in the
Asian and native Englishes. It turns out that the pairs again tend to be located in near positions in the
distribution. All of these imply the existence of the probabilistic module.
The probabilistic module also explains why the preservation is independent of proficiency. It is be-
cause the values of the probabilities in the probabilistic module will change quite slowly as one improves
his or her proficiency. First of all, unlike grammatical errors, explicit feedback such as correction by
teachers is not normally given to language learners in the case of the use of the arbitrary linguistic items
since any choice among the candidates allowed in a linguistic item is normally correct, as in the adverb
positions in English: Already, I have done it., I have already done it., and I have done it already, al-
though each of which might have a slightly difference in meaning. Therefore, language learners have
little opportunity to adapt the values of the probabilities in their probabilistic module to those in the target
language in the first place. Even if feedback is given, it would still be difficult to do so considering that
learners scarcely observe the values of the probabilities directly. This is why the values of the probabili-
ties in the probabilistic module tend to be similar within a mother tongue regardless of one?s proficiency
in English. We can actually see this in Fig. 6 (ii). Fig. 6 (ii) shows the distribution of the ratios of adverbs
at the beginning and the end of sentences in the high/low-proficiency essays in ICNALE where X-H and
X-L denote high-proficiency and low-proficiency essays of X English, respectively (e.g., THA-H de-
notes the high-proficiency essays of Thai English). Fig. 6 (ii) reveals that Englishes of the same language
speakers tend to remain in near positions regardless of the difference in proficiency.
All these observations would be a good place to start to explore the existence of the probabilistic
module. The next step would be to name other arbitrary linguistic items concerning the probabilistic
module, one of which for example might be the order of the main and subordinate clauses (e.g., Because
I did it, I did it. vs I did it because I did it.), and then one can reveal their values (probabilities) depending
on mother tongues.
6 Conclusions
In this paper, we examined the following two hypotheses: Hypothesis I: The preservation of language
family relationship universally holds in non-native English; Hypothesis II: The preservation of language
family relationship is dependent on proficiency in English. The experimental results partly accepted Hy-
1947
(i) All levels
 3
 4
 5
 6
 7
 8
 9
 10
 15  20  25  30  35  40  45
Ra
tio
 o
f a
dv
er
bs
 a
t t
he
 e
nd
 (%
)
Ratio of adverbs at the beginning (%)
CHN
NS1
NS2 HKGIDN
JPN
KOR
PAK
PHL
SIN
THA
TWN
(ii) High and low levels
 4
 5
 6
 7
 8
 9
 10
 15  20  25  30  35  40  45
Ra
tio
 o
f a
dv
er
bs
 a
t t
he
 e
nd
 (%
)
Ratio of adverbs at the beginning (%)
CHN-H
CHN-L
HKG-H
HKG-L
IDN-HIDN-L
JPN-H
JPN-L
KOR-H
KOR-L
PAK-H
PAK-L
PHL-H
PHL-L
SIN-H
THA-H
THA-L
TWN-H
TWN-L
Figure 6: Distribution of Adverb Position in Asian Englishes (ICNALE).
pothesis I and revealed that the following hypothesis fitted the data better: Hypothesis I
?
: The preser-
vation of language family relationship universally holds in the expanding circle of English. By contrast,
the experimental results denied Hypothesis II, supporting the counter hypothesis: Hypothesis II
?
: The
preservation of language family relationship is independent of proficiency in English. We then proposed
another hypothesis that a probabilistic module exists in the human brain to explain why Hypothesis I
?
andHypothesis II
?
hold. We further introduced empirical data implying the existence of the probabilistic
module.
For future work, we will examine Hypothesis I
?
and II
?
using English texts written by speakers of
languages in other families to see if the preservation really universally holds. Also, we will explore the
existence of the probabilistic module.
Acknowledgments
The author would like to thank the anonymous reviewers for their thoughtful comments and suggestions
on this paper.
References
Jan Aarts and Sylviane Granger, 1998. Tag sequences in learner corpora: a key to interlanguage grammar and
discourse, pages 132?141. Longman, New York.
Bengt Altenberg and Marie Tapper, 1998. The use of adverbial connectors in advanced Swedish learners? written
English, pages 80?93. Longman, New York.
Robert S.P. Beekes. 2011. Comparative Indo-European Linguistics: An Introduction (2nd ed.). John Benjamins
Publishing Company, Amsterdam.
David Crystal. 1997. The Cambridge Encyclopedia of Language (2nd ed.). Cambridge University Press, Cam-
bridge.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner En-
glish: The NUS corpus of learner English. In Proc. of 8th Workshop on Innovative Use of NLP for Building
Educational Applications, pages 22?31.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. International Corpus of Learner
English v2. Presses universitaires de Louvain, Louvain.
Jiawei Han and Micheline Kamber. 2006. Data Mining: Concepts and Techniques (2nd Ed.). Morgan Kaufmann
Publishers, San Francisco.
Shinichiro Ishikawa, 2011. A new horizon in learner corpus studies: The aim of the ICNALE project, pages 3?11.
University of Strathclyde Publishing, Glasgow.
1948
Bing-Hwang Juang and Lawrence R. Rabiner. 1985. A probabilistic distance measure for hidden Markov models.
AT&T Technical Journal, 64(2):391?408.
Braj B. Kachru, 1992. Teaching World Englishes, pages 355?365. University of Illinois Press, Urbana and
Chicago.
Kenji Kita. 1999. Automatic clustering of languages based on probabilistic models. Journal of Quantitative
Linguistics, 6(2):167?171.
Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. of
International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 181?184.
Ryo Nagata and Edward Whittaker. 2013. Reconstructing an Indo-European family tree from non-native English
texts. In Proc. of 51st Annual Meeting of the Association for Computational Linguistics, pages 1137?1147.
Ryo Nagata, Edward Whittaker, and Vera Sheinman. 2011. Creating a manually error-tagged and shallow-parsed
learner corpus. In Proc. of 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies, pages 1210?1219.
Anna Giacalone Ramat and Paolo Ramat. 2006. The Indo-European Languages. Routledge, New York.
Beatrice Santorini. 1990. Part-of-speech tagging guidelines for the Penn Treebank Project. University of Penn-
sylvania.
Michael Swan and Bernard Smith. 2001. Learner English (2nd Ed.). Cambridge University Press, Cambridge.
Sze-Meng J. Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proc.
Australasian Language Technology Workshop, pages 53?61.
1949
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1210?1219,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Creating a manually error-tagged and shallow-parsed learner corpus
Ryo Nagata
Konan University
8-9-1 Okamoto,
Kobe 658-0072 Japan
rnagata @ konan-u.ac.jp.
Edward Whittaker Vera Sheinman
The Japan Institute for
Educational Measurement Inc.
3-2-4 Kita-Aoyama, Tokyo, 107-0061 Japan
 
whittaker,sheinman  @jiem.co.jp
Abstract
The availability of learner corpora, especially
those which have been manually error-tagged
or shallow-parsed, is still limited. This means
that researchers do not have a common devel-
opment and test set for natural language pro-
cessing of learner English such as for gram-
matical error detection. Given this back-
ground, we created a novel learner corpus
that was manually error-tagged and shallow-
parsed. This corpus is available for research
and educational purposes on the web. In
this paper, we describe it in detail together
with its data-collection method and annota-
tion schemes. Another contribution of this
paper is that we take the first step toward
evaluating the performance of existing POS-
tagging/chunking techniques on learner cor-
pora using the created corpus. These contribu-
tions will facilitate further research in related
areas such as grammatical error detection and
automated essay scoring.
1 Introduction
The availability of learner corpora is still somewhat
limited despite the obvious usefulness of such data
in conducting research on natural language process-
ing of learner English in recent years. In particular,
learner corpora tagged with grammatical errors are
rare because of the difficulties inherent in learner
corpus creation as will be described in Sect. 2. As
shown in Table 1, error-tagged learner corpora are
very few among existing learner corpora (see Lea-
cock et al (2010) for a more detailed discussion
of learner corpora). Even if data is error-tagged,
it is often not available to the public or its access
is severely restricted. For example, the Cambridge
Learner Corpus, which is one of the largest error-
tagged learner corpora, can only be used by authors
and writers working for Cambridge University Press
and by members of staff at Cambridge ESOL.
Error-tagged learner corpora are crucial for devel-
oping and evaluating error detection/correction al-
gorithms such as those described in (Rozovskaya
and Roth, 2010b; Chodorow and Leacock, 2000;
Chodorow et al, 2007; Felice and Pulman, 2008;
Han et al, 2004; Han et al, 2006; Izumi et al,
2003b; Lee and Seneff, 2008; Nagata et al, 2004;
Nagata et al, 2005; Nagata et al, 2006; Tetreault et
al., 2010b). This is one of the most active research
areas in natural language processing of learner En-
glish. Because of the restrictions on their availabil-
ity, researchers have used their own learner corpora
to develop and evaluate error detection/correction
methods, which are often not commonly available
to other researchers. This means that the detec-
tion/correction performance of each existing method
is not directly comparable as Rozovskaya and Roth
(2010a) and Tetreault et al (2010a) point out. In
other words, we are not sure which methods achieve
the best performance. Commonly available error-
tagged learner corpora are therefore essential to fur-
ther research in this area.
For similar reasons, to the best of our knowledge,
there exists no such learner corpus that is manually
shallow-parsed and which is also publicly available,
unlike, say, native-speaker corpora such as the Penn
Treebank. Such a comparison brings up another cru-
cial question: ?Do existing POS taggers and chun-
1210
Name Error-tagged Parsed Size (words) Availability
Cambridge Learner Corpus Yes No 30 million No
CLEC Corpus Yes No 1 million Partially
ETLC Corpus Partially No 2 million Not Known
HKUST Corpus Yes No 30 million No
ICLE Corpus (Granger et al, 2009) No No 3.7 million+ Yes
JEFLL Corpus (Tono, 2000) No No 1 million Partially
Longman Learners? Corpus No No 10 million Not Known
NICT JLE Corpus (Izumi et al, 2003a) Partially No 2 million Partially
Polish Learner English Corpus No No 0.5 million No
Janus Pannoius University Learner Corpus No No 0.4 million Not Known
In Availability, Yes denotes that the full texts of the corpus is available to the public. Partially denotes that it is acces-
sible through specially-made interfaces such as a concordancer. The information in this table may not be consistent
because many of the URLs of the corpora give only sparse information about them.
Table 1: Learner corpus list.
kers work on learner English as well as on edited text
such as newspaper articles?? Nobody really knows
the answer to the question. The only exception in the
literature is the work by Tetreault et al (2010b) who
evaluated parsing performance in relation to prepo-
sitions. Nevertheless, a great number of researchers
have used existing POS taggers and chunkers to ana-
lyze the writing of learners of English. For instance,
error detection methods normally use a POS tagger
and/or a chunker in the error detection process. It is
therefore possible that a major cause of false pos-
itives and negatives in error detection may be at-
tributed to errors in POS-tagging and chunking. In
corpus linguistics, researchers (Aarts and Granger,
1998; Granger, 1998; Tono, 2000) use such tools to
extract interesting patterns from learner corpora and
to reveal learners? tendencies. However, poor per-
formance of the tools may result in misleading con-
clusions.
Given this background, we describe in this paper
a manually error-tagged and shallow-parsed learner
corpus that we created. In Sect. 2, we discuss the
difficulties inherent in learner corpus creation. Con-
sidering the difficulties, in Sect. 3, we describe our
method for learner corpus creation, including its
data collection method and annotation schemes. In
Sect. 4, we describe our learner corpus in detail. The
learner corpus is called the Konan-JIEM learner cor-
pus (KJ corpus) and is freely available for research
and educational purposes on the web1. Another
contribution of this paper is that we take the first
step toward answering the question about the per-
formance of existing POS-tagging/chunking tech-
niques on learner data. We report and discuss the
results in Sect. 5.
2 Difficulties in Learner Corpus Creation
In addition to the common difficulties in creating
any corpus, learner corpus creation has its own dif-
ficulties. We classify them into the following four
categories of the difficulty in:
1. collecting texts written by learners;
2. transforming collected texts into a corpus;
3. copyright transfer; and
4. error and POS/parsing annotation.
The first difficulty concerns the problem in col-
lecting texts written by learners. As in the case
of other corpora, it is preferable that the size of a
learner corpus be as large as possible where the size
can be measured in several ways including the total
number of texts, words, sentences, writers, topics,
and texts per writer. However, it is much more diffi-
cult to create a large learner corpus than to create a
1http://www.gsk.or.jp/index_e.html
1211
large native-speaker corpus. In the case of native-
speaker corpora, published texts such as newspa-
per articles or novels can be used as a corpus. By
contrast, in the case of learner corpora, we must
find learners and then let them write since there
are no such published texts written by learners of
English (unless they are part of a learner corpus).
Here, it should be emphasized that learners often
do not spontaneously write but are typically obliged
to write, for example, in class, or during an exam.
Because of this, learners may soon become tired of
writing. This in itself can affect learner corpus cre-
ation much more than one would expect especially
when creating a longitudinal learner corpus. Thus, it
is crucial to keep learners motivated and focused on
the writing assignments.
The second difficulty arises when the collected
texts are transformed into a learner corpus. This
involves several time-consuming and troublesome
tasks. The texts must be archived in electronic
form, which requires typing every single collected
text since learners normally write on paper. Be-
sides, each text must be archived and maintained
with accompanying information such as who wrote
what text when and on what topic. Optionally, a
learner corpus could include other pieces of infor-
mation such as proficiency, first language, and age.
Once the texts have been electronically archived, it
is relatively easy to maintain and access them. How-
ever, this is not the case when the texts are first col-
lected. Thus, it is better to have an efficient method
for managing such information as well as the texts
themselves.
The third difficulty concerning copyright is a
daunting problem. The copyright for each text
must be transferred to the corpus creator so that the
learner corpus can be made available to the public.
Consider the case when a number of learners par-
ticipate in a learner corpus creation project and ev-
eryone has to sign a copyright transfer form. This is-
sue becomes even more complicated when the writer
does not actually have such a right to transfer copy-
right. For instance, under the Japanese law, those
younger than 20 years of age do not have the right;
instead their parents do. Thus, corpus creators have
to ask learners? parents to sign copyright transfer
forms. This is often the case since the writers in
learner corpus creation projects are normally junior
high school, high school, or college students.
The final difficulty is in error and POS/parsing
annotation. For error annotation, several annota-
tion schemes exist (for example, the NICT JLE
scheme (Izumi et al, 2005)). While designing an an-
notation scheme is one issue, annotating errors is yet
another. No matter how well an annotation scheme
is designed, there will always be exceptions. Every
time an exception appears, it becomes necessary to
revise the annotation scheme. Another issue we have
to remember is that there is a trade-off between the
granularity of an annotation scheme and the level of
the difficulty in error annotation. The more detailed
an annotation scheme is, the more information it can
contain and the more difficult identifying errors is,
and vice versa.
For POS/parsing annotation, there are also a num-
ber of annotation schemes including the Brown tag
set, the Claws tag set, and the Penn Treebank tag
set. However, none of them are designed to be used
for learner corpora. In other words, a variety of lin-
guistic phenomena occur in learner corpora which
the existing annotation schemes do not cover. For
instance, spelling errors often appear in texts writ-
ten by learners of English as in sard year, which
should be third year. Grammatical errors prevent us
applying existing annotation schemes, too. For in-
stance, there are at least three possibilities for POS-
tagging the word sing in the sentence everyone sing
together. using the Penn Treebank tag set: sing/VB,
sing/VBP, or sing/VBZ. The following example is
more complicated: I don?t success cooking. Nor-
mally, the word success is not used as a verb but
as a noun. The instance, however, appears in a po-
sition where a verb appears. As a result, there are
at least two possibilities for tagging: success/NN
and success/VB. Errors in mechanics are also prob-
lematic as in Tonight,we and beautifulhouse (miss-
ing spaces)2. One solution is to split them to obtain
the correct strings and then tag them with a normal
scheme. However, this would remove the informa-
tion that spaces were originally missing which we
want to preserve. To handle these and other phe-
nomena which are peculiar to learner corpora, we
need to develop a novel annotation scheme.
2Note that the KJ corpus consists of typed essays.
1212
3 Method
3.1 How to Collect and Maintain Texts Written
by Learners
Our text-collection method is based on writing exer-
cises. In the writing exercises, learners write essays
on a blog system. This very simple idea of using a
blog system naturally solves the problem of archiv-
ing texts in electronic form. In addition, the use of a
blog system enables us to easily register and main-
tain accompanying information including who (user
ID) writes when (uploaded time) and on what topic
(title of blog item). Besides, once registered in the
user profile, the optional pieces of information such
as proficiency, first language, and age are also easy
to maintain and access.
To design the writing exercises, we consulted
with several teachers of English and conducted pre-
experiments. Ten learners participated in the pre-
experiments and were assigned five essay topics on
average. Based on the experimental results, we
designed the procedure of the writing exercise as
shown in Table 2. In the first step, learners are as-
signed an essay topic. In the second step, they are
given time to prepare during which they think about
what to write on the given topic before they start
writing. We found that this enables the students to
write more. In the third step, they actually write an
essay on the blog system. After they have finished
writing, they submit their essay to the blog system
to be registered.
The following steps were considered optional. We
implemented an article error detection method (Na-
gata et al, 2006) in the blog system as a trial at-
tempt to keep the learners motivated since learners
are likely to become tired of doing the same exercise
repeatedly. To reduce this, the blog system high-
lights where article errors exist after the essay has
been submitted. The hope is that this might prompt
the learners to write more accurately and to continue
the exercises. In the pre-experiments, the detection
did indeed seem to interest the learners and to pro-
vide them with additional motivation. Considering
these results, we decided to include the fourth and
fifth steps in the writing exercises when we created
our learner corpus. At the same time, we should of
course be aware that the use of error detection affects
learners? writing. For example, it may change the
Step Min.
1. Learner is assigned an essay topic ?
2. Learner prepares for writing 5
3. Learner writes an essay 35
4. System detects errors in the essay 5
5. Learner rewrites the essay 15
Table 2: Procedure of writing exercise.
distribution of errors. Nagata and Nakatani (2010)
reported the effects in detail.
To solve the problem of copyright transfer, we
took legal professional advice but were informed
that, in Japan at least, the only way to be sure is
to have a copyright transfer form signed every time.
We considered having it signed on the blog system,
but it soon turned out that this did not work since
participating learners may still be too young to have
the legal right to sign the transfer. It is left for our
long-term future work to devise a better solution to
this legal issue.
3.2 Annotation Scheme
This subsection describes the error and
POS/chunking annotation schemes. Note that
errors and POS/chunking are annotated separately,
meaning that there are two files for any given text.
Due to space restrictions we limit ourselves to only
summarizing our annotation schemes in this section.
The full descriptions are available together with the
annotated corpus on the web.
3.2.1 Error Annotation
We based our error annotation scheme on that used
in the NICT JLE corpus (Izumi et al, 2003a), whose
detailed description is readily available, for exam-
ple, in Izumi et al (2005). In that annotation
scheme and accordingly in ours, errors are tagged
using an XML syntax; an error is annotated by tag-
ging a word or phrase that contains it. For in-
stance, a tense error is annotated as follows: I  v tns
crr=?made?  make  /v tns  pies last year.
where v tns denotes a tense error in a verb. It
should be emphasized that the error tags contain the
information on correction together with error anno-
tation. For instance, crr=?made? in the above ex-
ample denotes the correct form of the verb is made.
For missing word errors, error tags are placed where
1213
a word or phrase is missing (e.g., My friends live
 prp crr=?in?  /prp  these places.).
As a pilot study, we applied the NICT JLE annota-
tion scheme to a learner corpus to reveal what mod-
ifications we needed to make. The learner corpus
consisted of 455 essays (39,716 words) written by
junior high and high school students3. The follow-
ing describes the major modifications deemed nec-
essary as a result of the pilot study.
The biggest difference between the NICT JLE
corpus and our targeted corpus is that the former is
spoken data and the latter is written data. This differ-
ence inevitably requires several modifications to the
annotation scheme. In speech data, there are no er-
rors in spelling and mechanics such as punctuation
and capitalization. However, since such errors are
not usually regarded as grammatical errors, we de-
cided simply not to annotate them in our annotation
schemes.
Another major difference is fragment errors.
Fragments that do not form a complete sentence of-
ten appear in the writing of learners (e.g., I have
many books. Because I like reading.). In written
language, fragments can be regarded as a grammat-
ical error. To annotate fragment errors, we added a
new tag  f  (e.g., I have many books.  f  Because
I like reading.  /f  ).
As discussed in Sect. 2, there is a trade-off be-
tween the granularity of an annotation scheme and
the level of the difficulty in annotating errors. In our
annotation scheme, we narrowed down the number
of tags to 22 from 46 in the original NICT JLE tag
set to facilitate the annotation; the 22 tags are shown
in Appendix A. The removed tags are merged into
the tag for other. For instance, there are only three
tags for errors in nouns (number, lexis, and other) in
our tag set whereas there are six in the NICT JLE
corpus (inflection, number, case, countability, com-
plement, and lexis); the other tag (  n o  ) covers
the four removed tags.
3.2.2 POS/Chunking Annotation
We selected the Penn Treebank tag set, which is
one of the most widely used tag sets, for our
3The learner corpus had been created before this reported
work started. Learners wrote their essays on paper. Unfortu-
nately, this learner corpus cannot be made available to the pub-
lic since the copyrights were not transferred to us.
POS/chunking annotation scheme. Similar to the er-
ror annotation scheme, we conducted a pilot study
to determine what modifications we needed to make
to the Penn Treebank scheme. In the pilot study, we
used the same learner corpus as in the pilot study for
the error annotation scheme.
As a result of the pilot study, we found that the
Penn Treebank tag set sufficed in most cases except
for errors which learners made. Considering this, we
determined a basic rule as follows: ?Use the Penn
Treebank tag set and preserve the original texts as
much as possible.? To handle such errors, we made
several modifications and added two new POS tags
(CE and UK) and another two for chunking (XP and
PH), which are described below.
A major modification concerns errors in mechan-
ics such as Tonight,we and beautifulhouse as already
explained in Sect. 2. We use the symbol ?-? to an-
notate such cases. For instance, the above two ex-
amples are annotated as follows: Tonight,we/NN-
,-PRP and beautifulhouse/JJ-NN. Note that each
POS tag is hyphenated. It can also be used
for annotating chunks in the same manner. For
instance, Tonight,we is annotated as [NP-PH-NP
Tonight,we/NN-,-PRP ]. Here, the tag PH stands for
 chunk label and denotes tokens which are not
normally chunked (cf., [NP Tonight/NN ] ,/, [NP
we/PRP ]).
Another major modification was required to han-
dle grammatical errors. Essentially, POS/chunking
tags are assigned according to the surface informa-
tion of the word in question regardless of the ex-
istence of any errors. For example, There is ap-
ples. is annotated as [NP There/EX ] [VP is/VBZ
] [NP apples/NNS ] ./. Additionally, we define the
CE4 tag to annotate errors in which learners use a
word with a POS which is not allowed such as in I
don?t success cooking. The CE tag encodes a POS
which is obtained from the surface information to-
gether with the POS which would have been as-
signed to the word if it were not for the error. For
instance, the above example is tagged as I don?t
success/CE:NN:VB cooking. In this format, the sec-
ond and third POSs are separated by ?:? which de-
notes the POS which is obtained from the surface
information and the POS which would be assigned
4CE stands for cognitive error.
1214
to the word without an error. The user can select
either POS depending on his or her purposes. Note
that the CE tag is compatible with the basic anno-
tation scheme because we can retrieve the basic an-
notation by extracting only the second element (i.e.,
success/NN). If the tag is unknown because of gram-
matical errors or other phenomena, UK and XP5 are
used for POS and chunking, respectively.
For spelling errors, the corresponding POS and
chunking tag are assigned to mistakenly spelled
words if the correct forms can be guessed (e.g., [NP
sird/JJ year/NN ]); otherwise UK and XP are used.
4 The Corpus
We carried out a learner corpus creation project us-
ing the described method. Twenty six Japanese col-
lege students participated in the project. At the be-
ginning, we had the students or their parents sign
a conventional paper-based copyright transfer form.
After that, they did the writing exercise described in
Sect. 3 once or twice a week over three months. Dur-
ing that time, they were assigned ten topics, which
were determined based on a writing textbook (Ok-
ihara, 1985). As described in Sect. 3, they used a
blog system to write, submit, and rewrite their es-
says. Through out the exercises, they did not have
access to the others? essays and their own previous
essays.
As a result, 233 essays were collected; Table 3
shows the statistics on the collected essays. It turned
out that the learners had no difficulties in using the
blog system and seemed to focus on writing. Out of
the 26 participants, 22 completed the 10 assignments
while one student quit before the exercises started.
We annotated the grammatical errors of all 233
essays. Two persons were involved in the annota-
tion. After the annotation, another person checked
the annotation results; differences in error annota-
Number of essays 233
Number of writers 25
Number of sentences 3,199
Number of words 25,537
Table 3: Statistics on the learner corpus.
5UK and XP stand for unknown and X phrase, respectively.
tion were resolved by consulting the first two. The
error annotation scheme was found to work well on
them. The error-annotated essays can be used for
evaluating error detection/correction methods.
For POS/chunking annotation, we chose 170 es-
says out of 233. We annotated them using our
POS/chunking scheme; hereafter, the 170 essays
will be referred to as the shallow-parsed corpus.
5 Using the Corpus and Discussion
5.1 POS Tagging
The 170 essays in the shallow-parsed corpus was
used for evaluating existing POS-tagging techniques
on texts written by learners. It consisted of 2,411
sentences and 22,452 tokens.
HMM-based and CRF-based POS taggers were
tested on the shallow-parsed corpus. The former was
implemented using tri-grams by the author. It was
trained on a corpus consisting of English learning
materials (213,017 tokens). The latter was CRFTag-
ger6, which was trained on the WSJ corpus. Both
use the Penn Treebank POS tag set.
The performance was evaluated using accuracy
defined by
number of tokens correctly POS-tagged
number of tokens  (1)
If the number of tokens in a sentence was differ-
ent in the human annotation and the system out-
put, the sentence was excluded from the calcula-
tion. This discrepancy sometimes occurred because
the tokenization of the system sometimes differed
from that of the human annotators. As a result, 19
and 126 sentences (215 and 1,352 tokens) were ex-
cluded from the evaluation in the HMM-based and
CRF-based POS taggers, respectively.
Table 4 shows the results. The second column
corresponds to accuracies on a native-speaker cor-
pus (sect. 00 of the WSJ corpus). The third column
corresponds to accuracies on the learner corpus.
As shown in Table 4, the CRF-based POS tagger
suffers a decrease in accuracy as expected. Interest-
ingly, the HMM-based POS tagger performed bet-
ter on the learner corpus. This is perhaps because it
6?CRFTagger: CRF English POS Tagger,? Xuan-Hieu Phan,
http://crftagger.sourceforge.net/, 2006.
1215
was trained on a corpus consisting of English learn-
ing materials whose distribution of vocabulary was
expected to be relatively similar to that of the learner
corpus. By contrast, it did not perform well on the
native-speaker corpus because the size of the train-
ing corpus was relatively small and the distribution
of vocabulary was not similar, and thus unknown
words often appeared. This implies that selecting
appropriate texts as a training corpus may improve
the performance.
Table 5 shows the top five POSs mistakenly
tagged as other POSs. An obvious cause of mis-
takes in both taggers is that they inevitably make
errors in the POSs that are not defined in the Penn
Treebank tag set, that is, UK and CE. A closer
look at the tagging results revealed that phenom-
ena which were common to the writing of learners
were major causes of other mistakes. Errors in cap-
italization partly explain why the taggers made so
many mistakes in NN (singular nouns). They often
identified erroneously capitalized common nouns
as proper nouns as in This Summer/NNP Vaca-
tion/NNP. Spelling errors affected the taggers in the
same way. Grammatical errors also caused confu-
sion between POSs. For instance, omission of a cer-
tain word often caused confusion between a verb and
an adjective as in I frightened/VBD. which should
be I (was) frightened/JJ. Another interesting case
is expressions that learners overuse (e.g., and/CC
so/RB on/RB and so/JJ so/JJ). Such phrases are not
erroneous but are relatively infrequent in native-
speaker corpora. Therefore, the taggers tended to
identify their POSs according to the surface infor-
mation on the tokens themselves when such phrases
appeared in the learner corpus (e.g., and/CC so/RB
on/IN and so/RB so/RB). We should be aware that
tokenization is also problematic although failures in
tokenization were excluded from the accuracies.
The influence of the decrease in accuracy on other
NLP tasks is expected to be task and/or method de-
pendent. Methods that directly use or handle se-
Method Native Corpus Learner Corpus
CRF 0.970 0.932
HMM 0.887 0.926
Table 4: POS-tagging accuracy.
HMM CRF
POS Freq. POS Freq.
NN 259 NN 215
VBP 247 RB 166
RB 163 CE 144
CE 150 JJ 140
JJ 108 FW 86
Table 5: Top five POSs mistakenly tagged.
quences of POSs are likely to suffer from it. An
example is the error detection method (Chodorow
and Leacock, 2000), which identifies unnatural se-
quences of POSs as grammatical errors in the writ-
ing of learners. As just discussed above, existing
techniques often fail in sequences of POSs that have
a grammatical error. For instance, an existing POS
tagger likely tags the sentence I frightened. as I/PRP
frightened/VBD ./. as we have just seen, and in turn
the error detection method cannot identify it as an
error because the sequence PRP VBD is not unnatu-
ral; it would correctly detect it if the sentence were
correctly tagged as I/PRP frightened/JJ ./. For the
same reason, the decrease in accuracy may affect the
methods (Aarts and Granger, 1998; Granger, 1998;
Tono, 2000) for extracting interesting sequences of
POSs from learner corpora; for example, BOS7 PRP
JJ is an interesting sequence but is never extracted
unless the phrase is correctly POS-tagged. It re-
quires further investigation to reveal how much im-
pact the decrease has on these methods. By contrast,
error detection/correction methods based on the bag-
of-word features (or feature vectors) are expected to
suffer less from it since mistakenly POS-tagged to-
kens are only one of the features. At the same time,
we should notice that if the target errors are in the
tokens that are mistakenly POS-tagged, the detec-
tion will likely fail (e.g., verbs should be correctly
identified in tense error detection).
In addition to the above evaluation, we at-
tempted to improve the POS taggers using the
transformation-based POS-tagging technique (Brill,
1994). In the technique, transformation rules are
obtained by comparing the output of a POS tagger
and the human annotation so that the differences be-
tween the two are reduced. We used the shallow-
7BOS denotes a beginning of a sentence.
1216
Method Original Improved
CRF 0.932 0.934
HMM 0.926 0.933
Table 6: Improvement obtained by transformation.
parsed corpus as a test corpus and the other man-
ually POS-tagged corpus created in the pilot study
described in Subsect. 3.2.1 as a training corpus. We
used POS-based and word-based transformations as
Brill (1994) described.
Table 6 shows the improvements together with the
original accuracies. Table 6 reveals that even the
simple application of Brill?s technique achieves a
slight improvement in both taggers. Designing the
templates of the transformation for learner corpora
may achieve further improvement.
5.2 Head Noun Identification
In the evaluation of chunking, we focus on head
noun identification. Head noun identification often
plays an important role in error detection/correction.
For example, it is crucial to identify head nouns to
detect errors in article and number.
We again used the shallow-parsed corpus as a test
corpus. The essays contained 3,589 head nouns.
We implemented an HMM-based chunker using 5-
grams whose input is a sequence of POSs, which
was obtained by the HMM-based POS tagger de-
scribed in the previous subsection. The chunker was
trained on the same corpus as the HMM-based POS
tagger. The performance was evaluated by recall and
precision defined by
number of head nouns correctly identified
number of head nouns (2)
and
number of head nouns correctly identified
number of tokens identified as head noun  (3)
respectively.
Table 7 shows the results. To our surprise, the
chunker performed better than we had expected. A
possible reason for this is that sentences written by
learners of English tend to be shorter and simpler in
terms of their structure.
The results in Table 7 also enable us to quanti-
tatively estimate expected improvement in error de-
tection/correction which is achieved by improving
chunking. To see this, let us define the following
symbols:  : Recall of head noun identification, 	 :
recall of error detection without chunking error, 
	
recall of error detection with chunking error. 	 and

	 are interpreted as the true recall of error detection
and its observed value when chunking error exists,
respectively. Here, note that 
	 can be expressed
as 
		 . For instance, according to Han et al
(2006), their method achieves a recall of 0.40 (i.e.,

	

), and thus 	

assuming that chunk-
ing errors exist and recall of head noun identification
is 

 just as in this evaluation. Improving  to

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1137?1147,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reconstructing an Indo-European Family Tree
from Non-native English texts
Ryo Nagata1,2 Edward Whittaker3
1Konan University / Kobe, Japan
2LIMSI-CNRS / Orsay, France
3Inferret Limited / Northampton, England
nagata-acl@hyogo-u.ac.jp, ed@inferret.co.uk
Abstract
Mother tongue interference is the phe-
nomenon where linguistic systems of a
mother tongue are transferred to another
language. Although there has been plenty
of work on mother tongue interference,
very little is known about how strongly
it is transferred to another language and
about what relation there is across mother
tongues. To address these questions,
this paper explores and visualizes mother
tongue interference preserved in English
texts written by Indo-European language
speakers. This paper further explores lin-
guistic features that explain why certain
relations are preserved in English writing,
and which contribute to related tasks such
as native language identification.
1 Introduction
Transfer of linguistic systems of a mother tongue
to another language, namely mother tongue inter-
ference, is often observable in the writing of non-
native speakers. The reader may be able to deter-
mine the mother tongue of the writer of the fol-
lowing sentence from the underlined article error:
The alien wouldn?t use my spaceship but
the hers.
The answer would probably be French or Span-
ish; the definite article is allowed to modify pos-
sessive pronouns in these languages, and the us-
age is sometimes negatively transferred to English
writing. Researchers such as Swan and Smith
(2001), Aarts and Granger (1998), Davidsen-
Nielsen and Harder (2001), and Altenberg and
Tapper (1998) work on mother tongue interfer-
ence to reveal overused/underused words, part of
speech (POS), or grammatical items.
In contrast, very little is known about how
strongly mother tongue interference is transferred
to another language and about what relation there
is across mother tongues. At one extreme, one
could argue that it is so strongly transferred to
texts in another language that the linguistic rela-
tions between mother tongues are perfectly pre-
served in the texts. At the other extreme, one
can counter it, arguing that other features such as
non-nativeness are more influential than mother
tongue interference. One possible reason for this
is that a large part of the distinctive language sys-
tems of a mother tongue may be eliminated when
transferred to another language from a speaker?s
mother tongue. For example, Slavic languages
have a rich inflectional case system (e.g., Czech
has seven inflectional cases) whereas French does
not. However, the difference in the richness cannot
be transferred into English because English has al-
most no inflectional case system. Thus, one can-
not determine the mother tongue of a given non-
native text from the inflectional case. A similar
argument can be made about some parts of gen-
der, tense, and aspect systems. Besides, Wong and
Dras (2009) show that there are no significant dif-
ferences, between mother tongues, in the misuse
of certain syntactic features such as subject-verb
agreement that have different tendencies depend-
ing on their mother tongues. Considering these,
one could not be so sure which argument is cor-
rect. In any case, to the best of our knowledge, no
one has yet answered this question.
In view of this background, we take the first step
in addressing this question. We hypothesize that:
Hypothesis: Mother tongue interference is so
strong that the relations in a language fam-
ily are preserved in texts written in another
language.
In other words, mother tongue interference is so
strong that one can reconstruct a language fam-
1137
ily tree from non-native texts. One of the major
contributions of this work is to reveal and visual-
ize a language family tree preserved in non-native
texts, by examining the hypothesis. This becomes
important in native language identification1 which
is useful for improving grammatical error correc-
tion systems (Chodorow et al, 2010) or for pro-
viding more targeted feedback to language learn-
ers. As we will see in Sect. 6, this paper reveals
several crucial findings that contribute to improv-
ing native language identification. In addition, this
paper shows that the findings could contribute to
reconstruction of language family trees (Enright
and Kondrak, 2011; Gray and Atkinson, 2003;
Barbanc?on et al, 2007; Batagelj et al, 1992;
Nakhleh et al, 2005), which is one of the central
tasks in historical linguistics.
The rest of this paper is structured as follows.
Sect. 2 introduces the basic approach of this work.
Sect. 3 discusses the methods in detail. Sect. 4 de-
scribes experiments conducted to investigate the
hypothesis. Sect. 5 discusses the experimental re-
sults. Sect. 6 discusses implications for work in
related domains.
2 Approach
To examine the hypothesis, we reconstruct a
language family tree from English texts writ-
ten by non-native speakers of English whose
mother tongue is one of the Indo-European lan-
guages (Beekes, 2011; Ramat and Ramat, 2006).
If the reconstructed tree is sufficiently similar to
the original Indo-European family tree, it will sup-
port the hypothesis. If not, it suggests that some
features other than mother tongue interference are
more influential.
The approach we use for reconstructing a lan-
guage family tree is to apply agglomerative hi-
erarchical clustering (Han and Kamber, 2006) to
English texts written by non-native speakers. Re-
searchers have already performed related work
on reconstructing language family trees. For in-
stance, Kroeber and Chrie?tien (1937) and Ellega?rd
(1959) proposed statistical methods for measuring
the similarity metric between languages. More re-
cently, Batagelj et al (1992) and Kita (1999) pro-
posed methods for reconstructing language fam-
ily trees using clustering. Among them, the
1Recently, native language identification has drawn the at-
tention of NLP researchers. For instance, a shared task on
native language identification took place at an NAACL-HLT
2013 workshop.
most related method is that of Kita (1999). In
his method, a variety of languages are modeled
by their spelling systems (i.e., character-based
n-gram language models). Then, agglomera-
tive hierarchical clustering is applied to the lan-
guage models to reconstruct a language family
tree. The similarity used for clustering is based on
a divergence-like distance between two language
models that was originally proposed by Juang and
Rabiner (1985). This method is purely data-driven
and does not require human expert knowledge for
the selection of linguistic features.
Our work closely follows Kita?s work. How-
ever, it should be emphasized that there is a signif-
icant difference between the two. Kita?s work (and
other previous work) targets clustering of a variety
of languages whereas our work tries to reconstruct
a language family tree preserved in non-native En-
glish. This significant difference prevents us from
directly applying techniques in the literature to our
task. For instance, Batagelj et al (1992) use basic
vocabularies such as belly in English and ventre in
French to measure similarity between languages.
Obviously, this does not work on our task; belly is
belly in English writing whoever writes it. Kita?s
method is also likely not to work well because all
texts in our task share the same spelling system
(i.e., English spelling). Although spelling is some-
times influenced by mother tongues, it involves a
lot more including overuse, underuse, and misuse
of lexical, grammatical, and syntactic systems.
To solve the problem, this work adopts a word-
based language model in the expectation that word
sequences reflect mother tongue interference. At
the same time, its simple application would cause
a serious side effect. It would reflect the topics
of given texts rather than mother tongue interfer-
ence. Unfortunately, there exists no such English
corpus that covers a variety of language speakers
with uniform topics; moreover the availability of
non-native corpora is still somewhat limited. This
also means that available non-native corpora may
be too small to train reliable word-based language
models. The next section describes two methods
(language model-based and vector-based), which
address these problems.
3 Methods
3.1 Language Model-based Method
To begin with, let us define the following symbols
used in the methods. Let Di be a set of English
1138
texts where i denotes a mother tongue i. Similarly,
let Mi be a language model trained using Di.
To solve the problems pointed out in Sect. 2, we
use an n-gram language model based on a mixture
of word and POS tokens instead of a simple word-
based language model. In this language model,
content words in n-grams are replaced with their
corresponding POS tags. This greatly decreases
the influence of the topics of texts, as desired. It
also decreases the number of parameters in the
language model.
To build the language model, the following
three preprocessing steps are applied to Di. First,
texts in Di are split into sentences. Second, each
sentence is tokenized, POS-tagged, and mapped
entirely to lowercase. For instance, the first ex-
ample sentence in Sect. 1 would give:
the/DT alien/NN would/MD not/RB
use/VB my/PRP$ spaceship/NN but/CC
the/DT hers/PRP ./.
Finally, words are replaced with their correspond-
ing POS tags; for the following words, word to-
kens are used as their corresponding POS tags:
coordinating conjunctions, determiners, preposi-
tions, modals, predeterminers, possessives, pro-
nouns, question adverbs. Also, proper nouns are
treated as common nouns. At this point, the spe-
cial POS tags BOS and EOS are added at the begin-
ning and end of each sentence, respectively. For
instance, the above example would result in the
following word/POS sequence:
BOS the NN would RB VB my NN but
the hers . EOS
Note that the content of the original sentence is far
from clear while reflecting mother tongue interfer-
ence, especially in the hers.
Now, the language model Mi can be built from
Di. We set n = 3 (i.e., trigram language model)
following Kita?s work and use Kneser-Ney (KN)
smoothing (Kneser and Ney, 1995) to estimate its
conditional probabilities.
With Mi and Di, we can naturally apply Kita?s
method to our task. The clustering algorithm used
is agglomerative hierarchical clustering with the
average linkage method. The distance2 between
two language models is measured as follows. The
2It is not a distance in a mathematical sense. However,
we will use the term distance following the convention in the
literature.
probability that Mi generates Di is calculated by
Pr(Di|Mi). Note that
Pr(Di|Mi) ?
Pr(w1,i) Pr(w2,i|w1,i)
?
|Di|?
t=3
Pr(wt,i|wt?2,i, wt?1,i) (1)
where wt,i and |Di| denote the tth token in Di and
the number of tokens in Di, respectively, since we
use the trigram language model. Then, the dis-
tance from Mi to Mj is defined by
d(Mi ? Mj) =
1
|Dj |
log Pr(Dj |Mj)Pr(Dj |Mi)
. (2)
In other words, the distance is determined based
on the ratio of the probabilities that each lan-
guage model generates the language data. Because
d(Mi ? Mj) and d(Mj ? Mi) are not symmet-
rical, we define the distance between Mi and Mj
to be their average:
d(Mi,Mj)=
d(Mi ? Mj)+d(Mj ? Mi)
2 . (3)
Equation (3) is used to calculate the distance be-
tween two language models for clustering.
To sum up, the procedure of the language fam-
ily tree construction method is as follows: (i) Pre-
process each Di; (ii) Build Mi from Di; (iii) Cal-
culate the distances between the language models;
(iv) Cluster the language data using the distances;
(v) Output the result as a language family tree.
3.2 Vector-based Method
We also examine a vector-based method for lan-
guage family tree reconstruction. As we will see
in Sect. 5, this method allows us to interpret clus-
tering results more easily than with the language
model-based method while both result in similar
language family trees.
In this method, Di is modeled by a vector. The
vector is constructed based on the relative frequen-
cies of trigrams. As a consequence, the distance
is naturally defined by the Euclidean distance be-
tween two vectors. The clustering procedure is the
same as for the language model-based method ex-
cept that Mi is vector-based and that the distance
metric is Euclidean.
1139
4 Experiments
We selected the ICLE corpus v.2 (Granger et al,
2009) as the target language data. It consists of
English essays written by a wide variety of non-
native speakers of English. Among them, the 11
shown in Table 1 are of Indo-European languages.
Accordingly, we selected the subcorpora of the 11
languages in the experiments. Before the exper-
iments, we preprocessed the corpus data to con-
trol the experimental conditions. Because some of
the writers had more than one native language, we
excluded essays that did not meet the following
three conditions: (i) the writer has only one na-
tive language; (ii) the writer has only one language
at home; (iii) the two languages in (i) and (ii) are
the same as the native language of the subcorpus
to which the essay belongs3. After the selection,
markup tags such as essay IDs were removed from
the corpus data. Also, the symbols ? and ? were
unified into ?4. For reference, we also used na-
tive English (British and American university stu-
dents? essays in the LOCNESS corpus5) and two
sets of Japanese English (ICLE and the NICE cor-
pus (Sugiura et al, 2007)). Table 1 shows the
statistics on the corpus data.
Performance of POS tagging is an important
factor in our methods because they are based on
word/POS sequences. Existing POS taggers might
not perform well on non-native English texts be-
cause they are normally developed to analyze na-
tive English texts. Considering this, we tested
CRFTagger6 on non-native English texts contain-
ing various grammatical errors before the exper-
iments (Nagata et al, 2011). It turned out that
CRFTagger achieved an accuracy of 0.932 (com-
pared to 0.970 on native texts). Although it did not
perform as well as on native texts, it still achieved
a fair accuracy. Accordingly, we decided to use it
in our experiments.
Then, we generated cluster trees from the cor-
pus data using the methods described in Sect. 3.
3For example, because of (iii), essays written by native
speakers of Swedish in the Finnish subcorpus were excluded
from the experiments. This is because they were collected in
Finland and might be influenced by Finnish.
4The symbol ? is sometimes used for ? (e.g., I?m).
5The LOCNESS corpus is a corpus of native En-
glish essays made up of British pupils? essays, British
university students? essays, and American university
students? essays: https://www.uclouvain.be/
en-cecl-locness.html
6Xuan-Hieu Phan, ?CRFTagger: CRF English POS
Tagger,? http://crftagger.sourceforge.net/,
2006.
Native language # of essays # of tokens
Bulgarian 294 219,551
Czech 220 205,264
Dutch 244 240,861
French 273 202,439
German 395 236,841
Italian 346 219,581
Norwegian 290 218,056
Polish 354 251,074
Russian 255 236,748
Spanish 237 211,343
Swedish 301 268,361
English 298 294,357
Japanese1 (ICLE) 171 224,534
Japanese2 (NICE) 340 130,156
Total 4,018 3,159,166
Table 1: Statistics on target corpora.
We used the Kyoto Language Modeling toolkit7
to build language models from the corpus data.
We removed n-grams that appeared less than five
times8 in each subcorpus in the language mod-
els. Similarly, we implemented the vector-based
method with trigrams using the same frequency
cutoff (but without smoothing).
Fig. 1 shows the experimental results. The
tree at the top is the Indo-European family tree
drawn based on the figure shown in Crystal
(1997). It shows that the 11 languages are divided
into three groups: Italic, Germanic, and Slavic
branches. The second and third trees are the clus-
ter trees generated by the language model-based
and vector-based methods, respectively. The num-
ber at each branching node denotes in which step
the two clusters were merged.
The experimental results strongly support the
hypothesis we made in Sect. 1. Fig. 1 reveals
that the language model-based method correctly
groups the 11 Englishes into the Italic, Ger-
manic, and Slavic branches. It first merges
Norwegian-English and Swedish-English into a
cluster. The two languages belong to the North
Germanic branch of the Germanic branch and
thus are closely related. Subsequently, the lan-
guage model-based method correctly merges the
other languages into the three branches. A dif-
7The Kyoto Language Modeling toolkit: http://www.
phontron.com/kylm/
8We found that the results were not sensitive to the value
of frequency cutoff so long as we set it to a small number.
1140
Polish
Italic Germanic Slavic
13
6
7
8
9
10
BulgarianSwedishFrench Spanish Norwegian CzechItalian RussianDutchGerman
French 
English
Spanish 
English
Italian 
English
Swedish 
English
Norwegian 
English
Dutch 
English
German
English
Polish 
English
Bulgarian 
English
Czech
English
Russian 
English
2 45
Indo-European family tree
Cluster tree generated by  LM-based method
13
4
7
6
8
10
French English Spanish English Italian English Swedish EnglishNorwegian EnglishDutch English German English Polish EnglishBulgarian EnglishCzech English Russian English
2 5
9 Cluster tree generated by vector-based clustering
Figure 1: Experimental results.
ference between its cluster tree and the Indo-
European family tree is that there are some mis-
matches within the Germanic and Slavic branches.
While the difference exists, the method strongly
distinguishes the three branches from one an-
other. The third tree shows that the vector-based
method behaves similarly while it mistakenly at-
taches Polish-English into an independent branch.
From these results, we can say that mother tongue
interference is transferred into the 11 Englishes,
strongly enough for reconstructing its language
family tree, which we propose calling the inter-
language Indo-European family tree in English.
Fig. 2 shows the experimental results with na-
tive and Japanese Englishes. It shows that the
same interlanguage Indo-European family tree
was reconstructed as before. More interestingly,
native English was detached from the interlan-
guage Indo-European family tree contrary to the
expectation that it would be attached to the Ger-
manic branch because English is of course a mem-
ber of the Germanic branch. This implies that
non-nativeness common to the 11 Englishes is
more influential than the intrafamily distance is9;
9Admittedly, we need further investigation to confirm this
argument especially because we applied CRFTagger, which is
developed to analyze native English, to both non-native and
native Englishes, which might affect the results.
Interlanguage Indo-European family tree Other family
JapaneseEnglish1 JapaneseEnglish2
3
Native English
12 13
ACL 2013
Figure 2: Experimental results with native and
Japanese Englishes.
otherwise, native English would be included in
the German branch. Fig. 2 also shows that the
two sets of Japanese English were merged into
a cluster and that it was the most distant in the
whole tree. This shows that the interfamily dis-
tance is the most influential factor. Based on
these results, we can further hypothesize as fol-
lows: interfamily distance > non-nativeness >
intrafamily distance.
5 Discussion
To get a better understanding of the interlanguage
Indo-European family tree, we further explore lin-
guistic features that explain well the above phe-
nomena. When we analyze the experimental re-
sults, however, some problems arise. It is al-
most impossible to find someone who has a good
knowledge of the 11 languages and their mother
language interference in English writing. Besides,
there are a large number of language pairs to com-
pare. Thus, we need an efficient and effective way
to analyze the experimental results.
To address these problems, we did the follow-
ing. First, we focused on only a few Englishes
out of the 11. Because one of the authors had
some knowledge of French, we selected French-
English as the main target. This naturally made
us select the other Italic Englishes as its counter-
parts. Also, because we had access to a native
speaker of Russian who had a good knowledge of
English, we included Russian-English in our fo-
cus. We analyzed these Englishes and then ex-
amined whether the findings obtained apply to the
other Englishes or not. Second, we used a method
for extracting interesting trigrams from the cor-
pus data. The method compares three out of the
11 corpora (for example, French-, Spanish-, and
Russian-Englishes). If we remove instances of a
trigram from each set, the clustering tree involving
1141
the three may change. For example, the removal
of but the hers may result in a cluster tree merg-
ing French- and Russian-Englishes before French-
and Spanish-Englishes. Even if it does not change,
the distances may change in that direction. We an-
alyzed what trigrams had contributed to the clus-
tering results with this approach.
To formalize this approach, we will denote a tri-
gram by t. We will also denote its relative fre-
quency in the language data Di by rti. Then, the
change in the distances caused by the removal of t
from Di, Dj , and Dk is quantified by
s = (rtk ? rti)2 ? (rtj ? rti)2 (4)
in the vector-based method. The quantity (rtk ?
rti)2 is directly related to the decrease in the dis-
tance between Di and Dk and similarly, (rtj ?
rti)2 to that between Di and Dj in the vector-
based method. Thus, the greater s is, the higher the
chance that the cluster tree changes. Therefore, we
can obtain a list of interesting trigrams by sorting
them according to s. We could do a similar calcu-
lation in the language model-based method using
the conditional probabilities. However, it requires
a more complicated calculation. Accordingly, we
limit ourselves to the vector-based method in this
analysis, noting that both methods generated sim-
ilar cluster trees.
Table 2 shows the top 15 interesting trigrams
where Di, Dj , and Dk are French-, Spanish-, and
Russian-Englishes, respectively. Note that s is
multiplied by 106 and r is in % for readability. The
list reveals that many of the trigrams contain the
article a or the. Interestingly, their frequencies are
similar in French-English and Spanish-English,
and both are higher than in Russian-English. This
corresponds to the fact that French and Spanish
have articles whereas Russian does not. Actu-
ally, the same argument can be made about the
other Italic and Slavic Englishes (e.g., the JJ NN:
Italian-English 0.82; Polish-English 0.72)10. An
exception is that of trigrams containing the definite
article in Bulgarian-English; it tends to be higher
in Bulgarian-English than in the other Slavic En-
glishes. Surprisingly and interestingly, however, it
reflects the fact that Bulgarian does have the def-
inite article but not the indefinite article (e.g., the
JJ NN: 0.82; a JJ NN: 0.60 in Bulgarian-English).
10Due to the space limitation, other lists were not included
in this paper but are available at http://web.hyogo-u.
ac.jp/nagata/acl/.
Table 3 shows that the differences in article
use exist even between the Italic and Germanic
branches despite the fact that both have the in-
definite and definite articles. The list still con-
tains a number of trigrams containing articles. For
a better understanding of this, we looked further
into the distribution of articles in the corpus data.
It turns out that the distribution almost perfectly
groups the 11 Englishes into the corresponding
branches as shown in Fig. 3. The overall use of
articles is less frequent in the Slavic-Englishes.
The definite article is used more frequently in the
Italic-Englishes than in the Germanic Englishes
(except for Dutch-English). We speculate that
this is perhaps because the Italic languages have a
wider usage of the definite article such as its modi-
fication of possessive pronouns and proper nouns.
The Japanese Englishes form another group (this
is also true for the following findings). This corre-
sponds to the fact that the Japanese language does
not have an article system similar to that of En-
glish.
s Trigram t rti rtj rtk
5.14 the NN of 1.01 0.98 0.78
4.38 a JJ NN 0.85 0.77 0.62
2.74 the JJ NN 0.87 0.86 0.71
2.30 NN of the 0.49 0.52 0.33
1.64 . . . 0.22 0.12 0.05
1.56 NNS . EOS 0.77 0.70 0.92
1.31 NNS and NNS 0.09 0.13 0.21
1.25 BOS RB , 0.25 0.22 0.14
1.22 of the NN 0.42 0.44 0.30
1.17 VBZ to VB 0.26 0.22 0.14
1.09 BOS i VBP 0.07 0.05 0.17
1.03 NN of NN 0.74 0.70 0.63
0.88 NN of JJ 0.15 0.15 0.25
0.67 the JJ NNS 0.28 0.28 0.20
0.65 NN to VB 0.40 0.38 0.31
Table 2: Interesting trigrams (French- (Di),
Spanish- (Dj), and Russian- (Dk) Englishes).
Another interesting trigram, though not as ob-
vious as article use, is NN of NN, which ranks
12th and 2nd in Table 2 and 3, respectively. In the
Italic Englishes, the trigram is more frequent than
the other non-native Englishes as shown in Fig. 4.
This corresponds to the fact that noun-noun com-
pounds are less common in the Italic languages
than in English and that instead, the of -phrase (NN
of NN) is preferred (Swan and Smith, 2001). For
1142
s Trigram t rti rtj rtk
21.49 the NN of 1.01 0.98 0.54
5.70 NN of NN 0.74 0.70 0.50
3.26 NN of the 0.49 0.52 0.30
3.10 the JJ NN 0.87 0.86 0.70
2.62 . . . 0.22 0.12 0.03
1.53 of the NN 0.42 0.44 0.29
1.50 NN , NN 0.30 0.30 0.18
1.50 BOS i VBP 0.07 0.05 0.19
0.85 NNS and NNS 0.09 0.13 0.19
0.81 JJ NN of 0.40 0.39 0.31
0.68 . . EOS 0.13 0.06 0.02
0.63 a JJ NN 0.85 0.77 0.73
0.63 RB . EOS 0.21 0.16 0.31
0.56 NN , the 0.16 0.16 0.08
0.50 NN of a 0.17 0.09 0.06
Table 3: Interesting trigrams (French- (Di),
Spanish- (Dj), and Swedish- (Dk) Englishes).
instance, orange juice is expressed as juice of or-
ange in the Italic languages (e.g., jus d?orange in
French). In contrast, noun-noun compounds or
similar constructions are more common in Russian
and Swedish. As a result, NN of NN becomes rel-
atively frequent in the Italic Englishes. Fig. 4 also
shows that its distribution roughly groups the 11
Englishes into the three branches. Therefore, the
way noun phrases (NPs) are constructed is a clue
to how the three branches were clustered.
This finding in turn reveals that the consecu-
tive repetitions of nouns occur less in the Italic
Englishes. In other words, the length tends to
be shorter than in the others where we define
the length as the number of consecutive repeti-
tions of common nouns (for example, the length
of orange juice is one because a noun is con-
secutively repeated once). To see if this is true,
we calculated the average length for each English.
Fig. 5 shows that the average length roughly dis-
tinguishes the Italic Englishes from the other non-
native Englishes; French-English is the shortest,
which is explained by the discussion above, while
Dutch- and German-Englishes are longest, which
may correspond to the fact that they have a prefer-
ence for noun-noun compounds as Snyder (1996)
argues. For instance, German allows the concate-
nated form as in Orangensaft (equivalently or-
angejuice). This tendency in the length of noun-
noun compounds provides us with a crucial insight
for native language identification, which we will
 2
 3
 4
 5
 6
 1  1.5  2  2.5  3
R
el
at
iv
e 
fre
qu
en
cy
 o
f d
ef
in
ite
 a
rti
cle
 (%
)
Relative frequency of indefinite article (%)
Bulgarian
Czech
Dutch
French
German
Italian
Norwegian
Polish
Russian
Spanish
Swedish
English
Japanese1
Japanese2
Italic
Germanic
Slavic
Japanese
Figure 3: Distribution of articles.
 0
 0.5  1
Relative frequency of NN of NN (%)
French
Italian
Spanish
Italic
Polish
Russian
Bulgarian
Czech
Slavic
English
Dutch
Swedish
German
Norwegian
Germanic
Japanese1
Japanese2 Japanese
Figure 4: Relative frequency of NN of NN in each
corpus (%).
come back to in Sect. 6.
The trigrams BOS RB , in Table 2 and RB . EOS
in Table 3 imply that there might also be a certain
pattern in adverb position in the 11 Englishes (they
roughly correspond to adverbs at the beginning
and end of sentences). Fig. 6 shows an insight into
this. The horizontal and vertical axes correspond
to the ratio of adverbs at the beginning and the end
of sentences, respectively. It turns out that the Ger-
man Englishes form a group. So do the Italic En-
glishes although it is less dense. In contrast, the
Slavic Englishes are scattered. However, the ra-
tios give a clue to how to distinguish Slavic En-
glishes from the others when combined with other
1143
 0
 0.1
Average length of noun-noun compounds
French
Italian
Spanish
Italic
Bulgarian
Czech
Russian
Polish
Slavic
Swedish
Norwegian
German
Dutch
English
Germanic
Japanese1
Japanese2 Japanese
Figure 5: Average length of noun-noun com-
pounds in each corpus.
 5
 10
 15  20  25  30
R
at
io
 o
f a
dv
er
bs
 a
t t
he
 e
nd
 (%
)
Ratio of adverbs at the beginning (%) 
Bulgarian
Czech
Polish
Russian
Dutch
German
Norwegian
Swedish
French
ItalianSpanish
English
Japanese1
Japanese2
Italic
Germanic
Slavic
Japanese
Figure 6: Distribution of adverb position.
trigrams. For instance, although Polish-English
is located in the middle of Swedish-English and
Bulgarian-English in the distribution of articles
(in Fig. 3), the ratios tell us that Polish-English is
much nearer to Bulgarian-English.
6 Implications for Work in Related
Domains
Researchers including Wong and Dras (2009),
Wong et al (2011; 2012), and Koppel et al (2005)
work on native language identification and show
that machine learning-based methods are effec-
tive. Wong and Dras (2009) propose using infor-
mation about grammatical errors such as errors in
determiners to achieve better performance while
they show that its use does not improve the perfor-
mance, contrary to the expectation. Related to this,
other researchers (Koppel and Ordan, 2011; van
Halteren, 2008) show that machine learning-based
methods can also predict the source language of
a given translated text although it should be em-
phasized that it is a different task from native lan-
guage identification because translation is not typ-
ically performed by non-native speakers but rather
native speakers of the target language11.
The experimental results show that n-grams
containing articles are predictive for identify-
ing native languages. This indicates that they
should be used in the native language identifi-
cation task. Importantly, all n-grams contain-
ing articles should be used in the classifier unlike
the previous methods that are based only on n-
grams containing article errors. Besides, no ar-
ticles should be explicitly coded in n-grams for
taking the overuse/underuse of articles into con-
sideration. We can achieve this by adding a spe-
cial symbol such as ? to the beginning of each NP
whose head noun is a common noun and that has
no determiner in it as in ?I like ? orange juice.?
In addition, the length of noun-noun com-
pounds and the position of adverbs should also
be considered in native language identification. In
particular, the former can be modeled by the Pois-
son distribution as follows. The Poisson distribu-
tion gives the probability of the number of events
occurring in a fixed time. In our case, the number
of events in a fixed time corresponds to the num-
ber of consecutive repetitions of common nouns in
NPs, which in turn corresponds to the length. To
be precise, the probability of a noun-noun com-
pound with length l is given by
Pr(l) = ?
l
l! e
??, (5)
where ? corresponds to the average length. Fig. 7
shows that the observed values in the French-
English data very closely fit the theoretical proba-
11For comparison, we conducted a pilot study where we
reconstructed a language family tree from English texts
in European Parliament Proceedings Parallel Corpus (Eu-
roparl) (Koehn, 2011). It turned out that the reconstructed
tree was different from the canonical tree (available at http:
//web.hyogo-u.ac.jp/nagata/acl/). However,
we need further investigation to confirm it because each sub-
corpus in Europarl is variable in many dimensions includ-
ing its size and style (e.g., overuse of certain phrases such as
ladies and gentlemen).
1144
 0
 0.5
 1
 0  1  2  3
Pr
ob
ab
ilit
y
Length of noun-noun compound
Theoretical
Observed
Figure 7: Distribution of noun-noun compound
length for French-English.
bilities given by Equation (5)12. This holds for the
other Englishes although we cannot show them be-
cause of the space limitation. Consequently, Equa-
tion (5) should be useful in native language identi-
fication. Fortunately, it can be naturally integrated
into existing classifiers.
In the domain of historical linguistics, re-
searchers have used computational and corpus-
based methods for reconstructing language fam-
ily trees. Some (Enright and Kondrak, 2011;
Gray and Atkinson, 2003; Barbanc?on et al, 2007;
Batagelj et al, 1992; Nakhleh et al, 2005) ap-
ply clustering techniques to the task of language
family tree reconstruction. Others (Kita, 1999;
Rama and Singh, 2009) use corpus statistics for
the same purpose. These methods reconstruct lan-
guage family trees based on linguistic features that
exist within words including lexical, phonological,
and morphological features.
The experimental results in this paper suggest
the possibility of the use of non-native texts for re-
constructing language family trees. It allows us to
use linguistic features that exist between words, as
seen in our methods, which has been difficult with
previous methods. Language involves the features
between words such as phrase construction and
syntax as well as the features within words and
thus they should both be considered in reconstruc-
12The theoretical and observed values are so close that it
is difficult to distinguish between the two lines in Fig. 7. For
example, Pr(l = 1) = 0.0303 while the corresponding ob-
served value is 0.0299.
tion of language family trees.
7 Conclusions
In this paper, we have shown that mother tongue
interference is so strong that the relations be-
tween members of the Indo-European language
family are preserved in English texts written by
Indo-European language speakers. To show this,
we have used clustering to reconstruct a lan-
guage family tree from 11 sets of non-native
English texts. It turned out that the recon-
structed tree correctly groups them into the Italic,
Germanic, and Slavic branches of the Indo-
European family tree. Based on the resulting
trees, we have then hypothesized that the fol-
lowing relation holds in mother tongue interfer-
ence: interfamily distance > non-nativeness >
intrafamily distance. We have further explored
several intriguing linguistic features that play an
important role in mother tongue interference: (i)
article use, (ii) NP construction, and (iii) adverb
position, which provide several insights for im-
proving the tasks of native language identification
and language family tree reconstruction.
Acknowledgments
This work was partly supported by the Digiteo for-
eign guest project. We would like to thank the
three anonymous reviewers and the following per-
sons for their useful comments on this paper: Ko-
taro Funakoshi, Mitsuaki Hayase, Atsuo Kawai,
Robert Ladig, Graham Neubig, Vera Sheinman,
Hiroya Takamura, David Valmorin, Mikko Vile-
nius.
References
Jan Aarts and Sylviane Granger, 1998. Tag sequences
in learner corpora: a key to interlanguage gram-
mar and discourse, pages 132?141. Longman, New
York.
Bengt Altenberg and Marie Tapper, 1998. The use of
adverbial connectors in advanced Swedish learners?
written English, pages 80?93. Longman, New York.
Franc?ois Barbanc?on, Tandy Warnow, Steven N. Evans,
Donald Ringe, and Luay Nakhleh. 2007. An exper-
imental study comparing linguistic phylogenetic re-
construction methods. Statistics Technical Reports,
page 732.
Vladimir Batagelj, Tomaz? Pisanski, and Dami-
jana Kerz?ic?. 1992. Automatic clustering of lan-
guages. Computational Linguistics, 18(3):339?352.
1145
Robert S.P. Beekes. 2011. Comparative Indo-
European Linguistics: An Introduction (2nd ed.).
John Benjamins Publishing Company, Amsterdam.
Martin Chodorow, Michael Gamon, and Joel R.
Tetreault. 2010. The utility of article and prepo-
sition error correction systems for English language
learners: feedback and assessment. Language Test-
ing, 27(3):419?436.
David Crystal. 1997. The Cambridge Encyclopedia of
Language (2nd ed.). Cambridge University Press,
Cambridge.
Niels Davidsen-Nielsen and Peter Harder, 2001.
Speakers of Scandinavian languages: Danish, Nor-
wegian, Swedish, pages 21?36. Cambridge Univer-
sity Press, Cambridge.
Alvar Ellega?rd. 1959. Statistical measurement of lin-
guistic relationship. Language, 35(2):131?156.
Jessica Enright and Grzegorz Kondrak. 2011. The ap-
plication of chordal graphs to inferring phylogenetic
trees of languages. In Proc. of 5th International
Joint Conference on Natural Language Processing,
pages 8?13.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English v2. Presses universitaires de Lou-
vain, Louvain.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435?438.
Jiawei Han and Micheline Kamber. 2006. Data Min-
ing: Concepts and Techniques (2nd Ed.). Morgan
Kaufmann Publishers, San Francisco.
Bing-Hwang Juang and Lawrence R. Rabiner. 1985.
A probabilistic distance measure for hidden Markov
models. AT&T Technical Journal, 64(2):391?408.
Kenji Kita. 1999. Automatic clustering of languages
based on probabilistic models. Journal of Quantita-
tive Linguistics, 6(2):167?171.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proc. of International Conference on Acoustics,
Speech, and Signal Processing, volume 1, pages
181?184.
Philipp Koehn. 2011. Europarl: A parallel corpus for
statistical machine translation. In Proc. of 10th Ma-
chine Translation Summit, pages 79?86.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of 49th Annual Meeting
of the Association for Computational Linguistics,
pages 1318?1326.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proc. of 11th ACM
SIGKDD International Conference on Knowledge
Discovery in Data Mining, pages 624?628.
Alfred L. Kroeber and Charles D. Chrie?tien. 1937.
Quantitative classification of Indo-European lan-
guages. Language, 13(2):83?103.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1210?1219.
Luay Nakhleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A comparison of phyloge-
netic reconstruction methods on an Indo-European
dataset. Transactions of the Philological Society,
103(2):171?192.
Taraka Rama and Anil Kumar Singh. 2009. From bag
of languages to family trees from noisy corpus. In
Proc. of Recent Advances in Natural Language Pro-
cessing, pages 355?359.
Anna Giacalone Ramat and Paolo Ramat, 2006. The
Indo-European Languages. Routledge, New York.
William Snyder. 1996. The acquisitional role of the
syntax-morphology interface: Morphological com-
pounds and syntactic complex predicates. In Proc.
of Annual Boston University Conference on Lan-
guage Development, volume 2, pages 728?735.
Masatoshi Sugiura, Masumi Narita, Tomomi Ishida,
Tatsuya Sakaue, Remi Murao, and Kyoko Muraki.
2007. A discriminant analysis of non-native speak-
ers and native speakers of English. In Proc. of Cor-
pus Linguistics Conference CL2007, pages 84?89.
Michael Swan and Bernard Smith. 2001. Learner En-
glish (2nd Ed.). Cambridge University Press, Cam-
bridge.
Hans van Halteren. 2008. Source language markers
in EUROPARL translations. In Proc. of 22nd Inter-
national Conference on Computational Linguistics,
pages 937?944.
Sze-Meng J. Wong and Mark Dras. 2009. Con-
trastive analysis and native language identification.
In Proc. Australasian Language Technology Work-
shop, pages 53?61.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2011. Exploiting parse structures for native lan-
guage identification. In Proc. Conference on Em-
pirical Methods in Natural Language Processing,
pages 1600?1611.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2012. Exploring adaptor grammars for native lan-
guage identification. In Proc. Joint Conference on
1146
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 699?709.
1147
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 754?764,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Correcting Preposition Errors in Learner English Using Error Case
Frames and Feedback Messages
Ryo Nagata
1?
Mikko Vilenius
2
Edward Whittaker
3
1
Konan University / Kobe, Japan
2
The Japan Institute for Educational Measurement, Inc. / Tokyo, Japan
3
Inferret Limited / Northampton, England
nagata-acl@hyogo-u.ac.jp.
Abstract
This paper presents a novel framework
called error case frames for correcting
preposition errors. They are case frames
specially designed for describing and cor-
recting preposition errors. Their most dis-
tinct advantage is that they can correct er-
rors with feedback messages explaining
why the preposition is erroneous. This pa-
per proposes a method for automatically
generating them by comparing learner and
native corpora. Experiments show (i) au-
tomatically generated error case frames
achieve a performance comparable to con-
ventional methods; (ii) error case frames
are intuitively interpretable and manually
modifiable to improve them; (iii) feedback
messages provided by error case frames
are effective in language learning assis-
tance. Considering these advantages and
the fact that it has been difficult to provide
feedback messages by automatically gen-
erated rules, error case frames will likely
be one of the major approaches for prepo-
sition error correction.
1 Introduction
This paper presents a novel framework for correct-
ing preposition errors. Its most significant advan-
tage over previous methods is that it can provide
learners with feedback messages, that is, explana-
tory notes describing why the detected preposi-
tion is erroneous and should be corrected as in-
dicated, as shown in Fig. 1. Despite the fact that
appropriate feedback messages are essential in
language learning assistance (Ferris and Roberts,
2001; Robb et al, 1986), which is one of the im-
mediate applications of grammatical error correc-
?
Part of this work was performed while the author was a
visiting researcher at LIMSI, Orsay (France).
Target sentence:  In the univerysity, I studied English in the morning.Error: correct preposition atFeedback message:Though both at and in are prepositions of place, at is used to denote the place (university) to which the person belongs and where  the learning activities  take place. 
Target sentence:  When the day is holiday, I go to shopping, singing in Karaoke and talking in cafe. Error: remove toFeedback message:Go directly takes the activity  without a preposition when it means traveling to a place in order to take part in an activity  by go and ?ing,        e.g.,  I went shopping. Similar expressions: go swimming, go fishing, go sightseeing
Figure 1: Error correction and feedback messages
provided by the proposed method.
tion, almost all previous methods are incapable of
providing feedback messages.
Grammatical error correction has been inten-
sively studied in recent years. Current methods
mostly exploit machine learning-based classifiers
to correct target errors; examples are errors in ar-
ticle (Han et al, 2006; Nagata et al, 2006; Ro-
zovskaya and Roth, 2011), preposition (Chodorow
et al, 2007; Felice and Pulman, 2008; Rozovskaya
and Roth, 2011; Tetreault et al, 2010), and
tense (Nagata and Kawai, 2011; Tajiri et al, 2012),
to name a few. Recently, Wu and Ng (2013) and
Rozovskaya and Roth (2013) proposed methods
for simultaneously correcting multiple types of er-
rors using integer linear programming. Another
major approach is to use a language model (LM)
for predicting correct words or phrases for a given
context. Some researchers (Brockett et al, 2006;
Yoshimoto et al, 2013) use statistical machine
translation (SMT) for the same purpose, which
can be regarded as the mixture of a classifier and
an LM. With these diverse techniques, correction
performance has dramatically improved against a
wide variety of target errors.
As noted above, however, one of the crucial
limitations of these previous methods is that they
754
are not capable of providing feedback messages.
They are not suitable for generating open-class
text such as feedback messages by their nature.
Some researchers (Kakegawa et al, 2000; McCoy
et al, 1996) made an attempt to develop hand-
crafted rules for correcting errors with feedback
messages. However, this approach encounters the
tremendous difficulty of covering a wide variety of
errors using hand-crafted rules.
In view of this background, this paper presents a
novel error correction framework called error case
frames an example of which is shown in Fig. 2.
They are case frames specially designed for de-
scribing and correcting errors in preposition at-
tached to a verb; the reader may be able to see that
it describes preposition errors such as *John often
goes shopping to the market with his family. and
that the preposition to should be replaced with at.
This paper proposes a method for automatically
generating them by comparing learner and native
corpora. Achieving a comparable correction per-
formance, they have the following two advantages
over the conventional approaches: (i) they are in-
tuitively interpretable and manually modifiable to
enrich them; (ii) they are capable of providing
feedback messages.
The rest of this paper is structured as fol-
lows. Sect. 2 introduces the definition of error case
frames. Sect. 3 discusses the method for generat-
ing error case frames. Sect. 4 describes how to cor-
rect preposition errors with feedback messages by
error case frames. Sect. 5 describes experiments
conducted to evaluate error case frames. Sect. 6
discusses the experimental results.
2 Error Case Frame
An error case frame consists of a verb, cases, and
a feedback message as shown in Fig. 2
1
. The fol-
lowing explains error case frames in detail based
on this example; occasionally consulting it may
help understanding the following sections.
An error case frame always has a verb. In Fig. 2,
the verb is go.
Cases are arguments the verb takes in an error
case frame. A case consists of a case tag and case
elements. A case tag and case elements describe,
respectively, the role that the case plays in the er-
ror case frame and a set of words that are allowed
1
Fig. 2 shows an example of error case frames for illus-
tration purposes. They are formally expressed in a machine-
readable format such as XML.
go
Feedback messageTo mean traveling to a place in order to take part in an activity, go takes at, in, or on depending on the activity. For example, the activity shoppingtakes place at a store (not  shopping  to a store),    and thus go shopping at a store.cf. We went  sightseeing in Baltimore.
Preposition casePrep_dobj: {shopping} *Prep_to: {store,market}  ?  Prep_at(Prep_with: {family})
Basic caseSubj: {PERSON}
verbcase elementcase tagcases
Figure 2: Example of an error case frame.
to appear as the argument. For instance, in Fig. 2,
?Subj: {PERSON}? is a case where its case tag
and element are ?Subj:? and ?{PERSON},? re-
spectively, denoting that a person such as John
plays a role of the subject of the verb. Note that
tokens in all upper case such as ?PERSON? refer
to a group of words such as {john,he,? ? ? } in this
paper.
Cases are classified into two categories: basic
and preposition cases. Basic cases are either a sub-
ject or a particle, whose case tags are ?Subj:? and
?Ptr:?, respectively. The ?Subj:? case is obliga-
tory while the ?Ptr:? is optional. Preposition cases
correspond to the prepositions the verb takes as its
arguments. Its case tag has the form of ?Prep x?
where x ranges over the target prepositions. It
should be emphasized that direct and indirect ob-
jects are included in the preposition cases for effi-
ciency; their case tags are denoted as ?Prep dobj?
and ?Prep iobj?, respectively. Preposition cases
are classified into those obligatory and optional.
Optional here means that the verb can constitute
a sentence with or without the preposition. Op-
tional prepositions are written in parentheses as in
?(Prep with:{family})?.
Preposition cases describe the information
about an error. An error case frame is constrained
to contain only one erroneous preposition case. It
is marked with the symbol ?*?. So, the preposi-
tion case ?*Prep to:{store,market}? is erroneous
in Fig. 2. The correct preposition is described af-
ter the symbol ??? as in ?? Prep at?.
Error case frames are furnished with feedback
messages. Unlike verbs and cases, which are au-
tomatically filled based on corpus data, they are
manually edited. A human annotator interprets
error case frames and adds explanatory notes to
them. This may seem time-consuming. How-
755
ever, the editing is far more efficient than manually
creating correction rules with feedback messages
from scratch because error case frames are highly
abstracted as explained in Sect. 3. Above all, it is a
significant advantage over the previous classifier-
/LM-based methods considering that there exists
no effective technique for augmenting these meth-
ods with feedback messages.
3 Generating Error Case Frames
The method proposed here exploits two sources
of corpus data: native and learner corpora. Case
frames (error case frames without the information
about an error and a feedback message) can be
automatically extracted from parsed sentences as
Kawahara and Uchimoto (2008) show. The pro-
posed method generates error case frames by com-
paring case frames generated from the learner cor-
pus with those from the native corpus. The basic
approach is to extract, as error case frames, case
frames which appear in the learner corpus but not
in the native corpus. However, this approach is so
simple that it extracts undesirable false error case
frames which do not actually correspond to prepo-
sition errors. To overcome the problem, the fol-
lowing procedures are applied:
(1) Filtering input sentences
(2) Extracting case frames
(3) Recognizing optional cases
(4) Grouping case frames
(5) Selecting candidate error case frames
(6) Determining correct prepositions
(7) Enriching error case frames
(8) Manually editing error case frames
(1) Filtering input sentences: This is a pre-
process to filter out unsuitable input sentences for
case frame generation. Accurate parsing is es-
sential for accurate case frame generation. Pars-
ing errors tend to occur in longer sentences. To
reduce parsing errors, Kawahara and Uchimoto
(2008) propose filtering out sentences which are
longer than 20 words. We adopt this filtering in
our method. We also filter out sentences contain-
ing commas, which often introduce complex struc-
tures. We apply the filtering pre-process only to
the native corpus; the availability of learner cor-
pora is still somewhat limited and therefore we use
all the sentences available in the learner corpus for
better coverage of preposition errors.
(2) Extracting case frames: This procedure
can be viewed as a slot filling task where the
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_to: {market}  
Basic caseSubj: {PERSON}
Input: John went shopping to the market.
wentJohn shoppingsubj dobjprep_to
Dependency parse
PERSONmapping to sense 
marketthedet
Case framegostemming
Figure 3: Example of case frame extraction.
slots are the verb and the cases in a case frame.
To achieve this, the corpus data are first parsed
by a parser. Then, for each verb, the predicate-
argument structures are extracted from the parses
as shown in Fig. 3. Here, only head words are ex-
tracted as arguments. They are reduced to their
base form when extracted. Certain classes of
words are replaced with their corresponding sense
(e.g., John to PERSON); the mapping between
words and their senses is shown in Appendix A.
In the case of the learner corpus, mis-spelt words
are automatically corrected using a spell-checker.
Finally, a case frame is created by filling its slots
with the extracted predicate-argument structures.
Hereafter, case frames generated from the native
and learner corpora will be referred to as the na-
tive and learner case frames, respectively.
(3) Recognizing optional cases: it is crucial
for generating flexible error case frames to recog-
nize optional preposition cases. Optional preposi-
tion cases are determined by the following heuris-
tic rules: (a) Objects are always obligatory; (b)
The number of obligatory preposition cases (ex-
cept objects) is at most one; (c) Prepositions ap-
pearing left of the verb are optional; (d) Preposi-
tions appearing right of the verb are optional ex-
cept the one which is nearest to the verb. Rule (a)
states that objects are always recognized as oblig-
atory
2
. Rule (b) constrains an error case frame to
have at most one obligatory preposition. Certain
verbs sometimes have more than one obligatory
preposition as in range from A to B. However, the
large majority of verbs satisfy rule (b). Rule (c)
states that prepositions appearing left of the verb
2
A sentence can be constituted without objects as in We
sing. Rule (a) always mistakenly recognizes such objects as
obligatory. However, preposition errors never appear in sen-
tences consisting of no object nor prepositions, and thus, the
objects mistakenly recognized as obligatory never cause any
problems in preposition error correction in practice.
756
in the input sentence are optional preposition cases
as in In the morning, he went shopping. Rule (c)
is based on the assumption that obligatory cases
are tied to the verb more strongly than optional
cases. In other words, obligatory cases cannot
easily change their position. Conversely, optional
cases have more freedom of their position, which
enables them to appear left of a verb. Admittedly,
obligatory prepositions can appear left of a verb
as in To school, he went in certain circumstances
such as in poetry. However, this usage is not so
frequent in corpora normally used as training data
such as newspaper articles. Rule (d), together with
rule (b), states that if more than one preposition
appears right of the verb, the one nearest to the
verb is obligatory and the rest are optional. Rule
(d) is based on the same reasoning as in rule (c).
Optional preposition cases are sometimes deter-
mined naturally by comparing two case frames.
In this case, one of them must consist of only
the object(s) as its preposition case(s) as in ?[go
Subj:{PERSON} Prep dobj:{shopping} ].? Then,
the other case frame must consist of the same verb,
the same basic cases, and the same object(s). The
only difference between them is preposition cases
(except the object(s)) (e.g., [go Subj:{PERSON}
Prep dobj:{shopping} Prep at:{market} ]). The
case frame only with the object(s) proves the other
to be valid without the preposition case(s). Thus,
these preposition cases are recognized as optional
(e.g., [go Subj:{PERSON} Prep dobj:{shopping}
(Prep at:{market}) ]).
(4) Grouping case frames: Similar case frames
in the native case frames are grouped into one,
which will play an important role in (7) Enriching
error case frames. Case frames comprising sim-
ilar cases tend to denote similar usage of a verb.
Considering this, case frames are merged into one
if they consist of the same verb, the same basic
cases, and the same case tags of the obligatory
preposition cases. The grouping procedure is illus-
trated in Fig. 4. When preposition cases are oblig-
atory in one case frame and optional in the other,
the discrepancy is resolved by setting the prepo-
sition case to optional in the merged case frame.
Note that this grouping procedure is not applied to
the learner case frames so that erroneous usages in
the learner case frames do not propagate to other
(correct) learner case frames.
(5) Selecting candidate error case frames:
Candidates for error case frames are selected from
the learner case frames. If a learner case frame
does not match, ignoring optional preposition
cases, any native case frame, it is selected as a
candidate for an error case frame on the assump-
tion that case frames corresponding to erroneous
usages do not appear in the native corpus.
Alternatively, an error-annotated learner cor-
pus can be used to select error case frames; sim-
ply extracting case frames of which preposition is
marked as an error gives error case frames. In this
case
3
, procedure (6) may be omitted and proce-
dure (7) is directly applied after procedure (5).
(6) Determining correct prepositions: Now,
correct prepositions for the candidate error case
frames are explored. Each case tag of the prepo-
sition cases in a candidate is replaced, one at a
time, with one of the other target prepositions.
This replacement can be interpreted as error cor-
rection. Take as an example the following can-
didate error case frame: [go Subj:{PERSON}
Prep dobj:{shopping} Prep to:{market} ]. Re-
placing the case tag ?Prep to? with ?Prep at? cor-
responds to correct expressions such as John of-
ten goes shopping at the market. Note that re-
placing a direct object with one of the preposi-
tions corresponds to correcting an omission er-
ror as in ?Prep dobj? with ?Prep to? in ?[go
Subj:{PERSON} Prep dobj:{market} ]?. Simi-
larly, replacing a preposition with an object cor-
responds to correcting an extra-preposition er-
ror (e.g., ?Prep to? with ?Prep dobj? in ?[go
Subj:{PERSON} Prep to:{shopping} ])?.
To examine whether each correction is valid
or not, the native case frames are again used; if
the replaced case frame matches one of the na-
tive case frames, the correction is determined to
be valid. Here, we define the match as the two
case frames consisting of the same verb, the same
basic cases, the same obligatory preposition cases,
and the same preposition case to which the cor-
rection is applied (if it is an optional one). If the
condition is satisfied, the information on the error
and correction is added to the candidate error case
frame. If a valid correction is found, the candi-
date is determined to be a valid error case frame.
In total, their validity is double-checked, once in
(5) and once in (6), by comparing them with the
3
We do not make use of error-annotated learner corpora in
this paper in order to reveal how well the proposed methods
perform without such corpora. In practice, one can use error-
annotated learner corpora together with raw learner corpora
to achieve better performance.
757
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {store}(Prep_with:   {family} ) 
Basic caseSubj: {PERSON}
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {store,market}(Prep_with:   {family} ) 
Basic caseSubj: {PERSON}
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {market}
Basic caseSubj: {PERSON}
Figure 4: Example of grouping case frames.
native case frames.
(7) Enriching error case frames: The gener-
ated error cases are limited in error coverage be-
cause the procedures so far solely rely on prepo-
sition errors appearing in the learner corpus. In
other words, it is impossible to generate error case
frames corresponding to preposition errors which
do not appear in the learner corpus. To overcome
this limitation, the generated error case frames are
enriched using the native case frames. For each er-
ror case frame, we already know the correspond-
ing native (thus, correct) case frame, which is ob-
tained in (6). The corresponding native case frame
is normally much richer in preposition cases be-
cause of the optional cases and grouping given
by procedures (3) and (4), as shown at the top of
Fig. 5. These additional cases are useful to enrich
error case frames.
For the preposition case which is determined
to be erroneous, its correct preposition is found
in the error case frame (e.g., ?? Prep at? at the
top-left of Fig. 5). Also, its correct preposition
case is found in the corresponding native case
frame (e.g., ?Prep at:{market,store}? at the
top-right). Replacing the case element of the
erroneous case by one of the case elements of
the correct preposition case gives a new can-
didate for an error case frame (e.g., replacing
market of ?*Prep to:{market}? by store gives
?[go Subj:{PERSON} Prep dobj:{shopping}
*Prep to:{store} ].? It should be emphasized that
this new error case frame is still a candidate at this
point and the usage might be correct. To verify
if it really describes an erroneous preposition
use, the native case frames are searched for; if
it matches one of them, that means that the use
of the preposition actually appears in the native
go
Feedback message    
Preposition casePrep_dobj: {shopping} *Prep_to:  {market} ?  Prep_at
Basic caseSubj: {PERSON}
Error case frame go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {market,store}(Prep_with:   {family} ) 
Basic caseSubj: {PERSON}
Native case frame
Verificationgo
Feedback message    
Preposition casePrep_dobj: {shopping} *Prep_to:       {market,store} ?  Prep_at(Prep_with:   {family})  
Basic caseSubj: {PERSON}
Expanded error case frame
Figure 5: Enriching an error case frame.
corpus. Therefore, it should be discarded. Only
if a match is not found, is the case element added
to the erroneous preposition case in the original
error case frame. This process is illustrated in the
box denoted as Verification in Fig. 5.
For the other preposition cases which are not er-
roneous, the enriching procedure is much simpler.
They are simply added to the error case frame as
shown in Fig. 5. One thing we should take care
of is that there might be a discrepancy in obliga-
tory/optional between the cases of the error case
frame and the native case frame. This discrepancy
is solved by setting the preposition case in the er-
ror case frame to optional. The resulting expanded
error case frame after procedure (7) is shown at
the bottom of Fig. 5 where the enriched cases are
shown in red.
(8) Manually Editing Error Case Frames:
The most important editing is the addition of feed-
back messages. A human annotator interprets the
generated error case frames and adds explanatory
notes to them. Although this basically requires
manual editing, part of feedback messages can be
automatically created to facilitate the procedure.
For example, example sentences corresponding to
an error case frame can be automatically added
to it, whether correct or error examples, because
the original sentences from which the (error) case
frames extracted are available in the native and
learner corpora. Besides, setting a variable to
the feedback message allows it to be adaptable to
correction results as shown in Fig. 6. In Fig. 6,
X
Prep to
is a variable. It is replaced with one
of the case elements of ?Prep to:? depending on
correction results. Also, it will be beneficial to
link similar error case frames each other, which
allows the user to obtain additional information.
758
go
Feedback messageTo mean traveling to a place in order to take part in an activity, go takes at, in, or on depending on the activity. For example, the activity shoppingtakes place at a _ (not  shopping  to a	_ ), and thus go shopping at 		_ .cf. We went  sightseeing in Baltimore.
Preposition casePrep_dobj: {shopping} *Prep_to: {store,market}  ?  Prep_at(Prep_with: {family})
Basic caseSubj: {PERSON}
verbcase elementcase tag
cases
Figure 6: Error case frame with a variable.
For example, the example error case frame in
Fig. 6 may be linked to similar case frames such as
?[ go Subj:{PERSON} Prep dobj:{sightseeing}
*Prep to:{Baltimore} ? Prep in ].? One can re-
trieve similar error case frames from the generated
error case frames where the similarity between
two error case frames are defined by the overlap
in the verb, the basic cases, and the case tags of
the preposition cases.
The generated error case frames may be further
edited to enrich them. As we can see in Fig. 5, the
generated error case frames are easy to interpret.
This property enables us to manually edit them to
enrich their preposition cases. For example, one
might add a case element such as supermarket to
the preposition case ?Prep to:{market,store}? in
the example error case frame. Conversely, one
might discard unnecessary case elements, cases,
or even error case frames.
4 Correcting Preposition Errors
Preposition errors are corrected by applying the
generated error case frames to the target text. Case
frames are first extracted from the target text by the
same procedures (2) and (3) in Sect. 3. Then, each
extracted case frame is examined if it matches one
of the error case frames. If a match is found, the
preposition is detected as an error and the correct
preposition is suggested with the feedback mes-
sage according to the matched error case frame.
The match between a case frame and an error case
frame is defined in the exact same manner as in
procedure (4) in Sect. 3. Sometimes, a case frame
matches more than one error case frame suggest-
ing different corrections. In this case, the most
frequent correction among the candidates is cho-
sen to correct the error, which was applied in the
evaluation described in Sect. 5
4
.
One of the advantages of error case frames is
that they do not require an error-annotated corpus
as explained in the previous section. This means
that the target text itself can be used as part of a
learner corpus for generating error case frames at
the time of error correction. Applying procedures
(2) to (7) to the target text generates additional er-
ror case frames
5
. Although feedback messages are
not available in these additional error case frames,
they are still useful for improving correction per-
formance, especially in recall. Hereafter, this way
of error case frame generation will be referred to
as active generation.
A pre-experiment using a development data set
revealed that there were some preposition errors
for which error case frames were not generated
even though the corresponding erroneous and cor-
rect preposition usages appeared in the learner and
native corpora, respectively. They are preposition
errors where the preposition is incorrectly used
with an adverb as in *John went to there. To be
precise, they are either an adverb denoting a place
(e.g., there) with a preposition concerning a place
(at, in, on, and to) or a noun denoting time, fre-
quency, and duration with a preposition concern-
ing time, frequency, and duration (at, for, in, and
on). In the native corpus, these adverbs or nouns
are correctly used without a preposition and thus
they are not recognized as a prepositional phrase
by a parser. Therefore, corresponding native case
frames are never found for these types of errors
in procedure (6), and in turn error case frames are
never generated for them.
Considering that they are limited in number
because they are independent of verbs and ba-
sic cases, we decided to manually create er-
ror case frames describing these types of er-
rors. In these error case frames, the verb
and the basic cases are filled with ANY denot-
ing any word. The preposition cases are man-
ually filled based on the linguistic knowledge
known as absence of preposition (Quirk et al,
1985). For example, an error case frame for
the above error would be ?[ANY Subj:{ANY}
*Prep to:{here,somewhere,there}? Prep dobj ].?
Certain errors involve a phrase such as *John goes
shopping in every morning. To handle these cases,
4
Ties are broken by random selection.
5
Recall that procedure (1) is only applied to the native
corpus.
759
these manually created error case frames are al-
lowed to have phrases as their case elements (e.g.,
[ANY Subj:{ANY} *Prep in:{every morning} ?
Prep dobj ]).
5 Evaluation
We evaluated the proposed method from two
points of view: correction performance and use-
fulness of feedback messages. We measured cor-
rection performance by recall, precision, and F -
measure. In the evaluation on usefulness of feed-
back messages, three human raters (a teacher of
English at college and two who have a master
degree in TESOL) separately examined whether
each feedback message was useful for learning the
correct usage of the preposition. We defined use-
fulness by the ratio of feedback messages evalu-
ated as useful to the total number of feedback mes-
sages.
We used the following data sets in the evalua-
tion. We selected the Konan-JIEM (KJ) learner
corpus (Nagata et al, 2011) as the target texts. The
KJ learner corpus is fully annotated with grammat-
ical errors. In addition, it includes error correc-
tion results of several benchmark systems. This
means that one can directly compare correction
results of a new method with those of the bench-
mark systems, which reveals where the method is
strong and weak compared to the benchmark sys-
tems. The KJ corpus consists of training and test
sets. We used the training set to generate error case
frames and evaluated correction performance on
the test set. In addition to these data sets, we cre-
ated a development set, which we had collected
to develop the proposed method. We did not use
it in the final evaluation. As a native corpus, we
used the EDR corpus (Japan electronic dictionary
research institute Ltd, 1993), the Reuters-21578
corpus
6
, and the LOCNESS corpus
7
. We used
the lexicalized dependency parser in the Stanford
Statistical Natural Language Parser (ver.2.0.3) (de
Marneffe et al, 2006) to obtain parses for the data
sets. Table 1 shows the statistics on the data sets.
Using these data sets, we implemented three
versions of the proposed method. The first one
was based on error case frames generated from the
training set of the KJ corpus. The second one was
the first one with active generation. To implement
6
Reuters-21578, Distribution 1.0, http://www.
research.att.com/
?
lewis
7
http://www.uclouvain.be/en-cecl.html
Name # of tokens # of errors
KJ training 22,701 327
KJ test 8,065 131
Dev. set 47,217 774
EDR 1,745,863 ?
Reuters 28,431,228 ?
LOCNESS 294,325 ?
Table 1: Statistics on the data sets for evaluation.
the third one, we manually edited the error case
frames of the first version to remove unnecessary
error case frames and case elements (but no addi-
tion) and to add feedback messages to them. Af-
ter this, active generation was applied to augment
the edited error case frames. In implementing the
proposed methods, we selected as target preposi-
tions the ten most frequent prepositions, the same
as in previous work (Rozovskaya and Roth, 2011):
about, at, by, for, from, in, of , on, to, with.
For comparison, we selected two conventional
methods. One was the best-performing sys-
tem among the benchmark systems, which is the
classifier-based method (Sakaguchi et al, 2012)
which had participated in the HOO 2012 shared
task (Dale et al, 2012). The other was the SMT-
based method (Yoshimoto et al, 2013) which was
the best-performing system in preposition error
correction in the CoNLL 2013 shared task (Ng et
al., 2013). In addition, we evaluated performance
of hybrid methods combining the correction re-
sults of the third version of the proposed method
with those of the classifier-/SMT-based method;
we simply took the union of the two.
Table 2 shows the evaluation results. The sim-
ple error case frame-based method achieves an F -
measure of 0.189. It improves recall when com-
bined with active generation, which shows the
effectiveness of active generation for augment-
ing error case frames. It further improves pre-
cision without decreasing recall by manual edit-
ing; note that manual editing was only applied
to the error case frames generated from the train-
ing data but not to those generated by active gen-
eration. The performance is comparable to both
classifier-/SMT-based methods. The hybrid meth-
ods achieve the best performances in F -measure.
In the usefulness evaluation, the third version of
the proposed method was able to provide 20 feed-
back messages for the target texts. The three hu-
man raters evaluated 80%, 80%, and 85% of the
760
Method R P F
ECF 0.107 0.823 0.189
ECF with AG 0.130 0.680 0.218
ME-ECF with AG 0.130 0.708 0.219
Classifier-based 0.167 0.310 0.217
SMT-based 0.115 0.385 0.176
Classifier hybrid 0.235 0.369 0.287
SMT hybrid 0.191 0.446 0.267
ECF: Error Case Frame, ME-ECF: Manually
Edited Error Case Frame, AG: Active Generation
Table 2: Correction performance in recall (R),
precision (P ), and F -measure (F ).
20 feedback messages as useful (82% on average).
The agreement among the raters was ? = 0.67 in
Fleiss?s ?.
6 Discussion
As the experimental results show, the proposed
method achieves a comparable correction perfor-
mance with the classifier-/SMT-based methods. A
closer look at the correction results reveals the dif-
ferences in correction tendencies between these
methods, which explains well why the hybrid
methods achieve better performance.
One of the tendencies is that the proposed
method performs better on preposition errors
where relatively wider contexts are required
to correct them. Error case frames naturally
exploit wider contexts based on the cases
which are extracted by parsing. In contrast,
classifier-/SMT-based methods rely on narrower
contexts such as a few words surrounding the
preposition in question. Take as an example the
following sentence which appeared in the test
set: *In the univerysity, I studied English in the
morning
8
. To confirm that the preposition In
is erroneous requires the verb studied and the
object English. The proposed method successfully
corrected this error by the error case frame ?[study
Subj:{PERSON} Prep dobj:{english,math,? ? ? }
*Prep in:{university} ? at ]? in the evaluation.
This would be difficult for methods relying on
only a few words surrounding the preposition In.
It is also difficult for classifier-/SMT-based
methods to correct missing preposition errors.
Classifier-based methods need to be informed of
8
The word univerysity is a mis-spelt word of university.
Note that mis-spelt words are automatically corrected by a
spell-checker when case frames are extracted.
the position of the preposition to predict a cor-
rect preposition. Because the position of a miss-
ing preposition is implicit, classifier-based meth-
ods would have to make a prediction at every
single position between words, which would be
inefficient. Because of this, the classifier-based
method used in the evaluation (and often other
classifier-based methods) excludes missing prepo-
sition errors from its target. SMT-based methods
do not perform well either on missing preposition
errors because of the fact that they implicitly, but
not directly, handle missing preposition errors. In
contrast, error case frames directly model miss-
ing prepositions by treating objects as one of the
preposition cases (i.e., Prep dobj).
Grammatical errors other than preposition er-
rors influence both the proposed and classifier-
/SMT-based methods, but differently. Grammat-
ical errors appearing around the preposition in
question seem to influence the previous methods
more significantly than the proposed method be-
cause they rely on words surrounding the prepo-
sition. On the other hand, structural errors such
as errors in voice tend to degrade performance of
the proposed method. For instance, if an error in
voice occurs as in *I excited this, correctly, I was
excited by this, error case frames are not properly
applied.
The precisions of the proposed methods are
high compared to those of the previous methods.
To be precise, the number of false positives is only
seven in the third version of the proposed method.
Out of seven, four false positives are due to prob-
lems with the used error case frames themselves.
Two are the influence of other grammatical errors
(e.g., *I like to look beautiful view. was corrected
as look at beautiful view by the proposed method
but as see beautiful view in the error annotation).
Unlike false positives, it is difficult to precisely
point out causes for false negatives, which often
involve several factors. One cause which is theo-
retically clear is errors in preposition attached to
a noun phrase (NP), which amounts to 11 % of
all false negatives. Since error case frames de-
scribe errors in preposition attached to a verb, they
do not target these types of errors. Extending er-
ror case frames to general frames might overcome
this limitation, which will require further investi-
gation. Similarly, error case frames are not gener-
ated for preposition errors where prepositions are
incorrectly used with words other than a noun as
761
in *make me to happy (5 % of all). Although er-
ror case frames can describe these types of errors,
case frames are not extracted for their correspond-
ing correct usages from the native corpus. This
is because the word in question (e.g., happy) cor-
rectly appears without the erroneous preposition in
the native corpus, and thus it is not recognized as
a preposition case. This means that a correspond-
ing correct case frame is never found for any er-
ror of these types in the generation procedure (6).
Accordingly, error case frames are never gener-
ated for these types of errors. The most influen-
tial cause of false negatives, which is also a major
cause of false negatives in the previous methods,
is other grammatical errors (at least 22 % of all).
One of such errors is errors in voice as already ex-
plained (4%). Another is the omission of the ob-
ject of a verb (4%). In these cases, even if an ap-
propriate error case frame exists, it is not applied
because of the grammatical error.
In addition to correction performance, error
case frames are effective in providing feedback
messages; Fig. 1 (on the first page) shows excerpts
of the feedback messages provided in the evalua-
tion. The evaluation shows that 82% of the pro-
vided feedback messages were actually rated as
useful for language learning on average (the rest
were mostly evaluated as not-useful due to false
positive corrections). With the feedback messages
of error case frames, we now have the follow-
ing three choices as the way of error correction:
(a) just indicating the correct preposition (as in
previous methods); (b) indicating the correction
preposition with a feedback message; (c) display-
ing only a feedback message. In (a), the learner
might just copy the correct preposition to correct
his or her writing, which would result in little or
no learning effect. This suggests that the ultimate
goal of grammatical error correction for language
learning assistance is not to correct all errors in the
given text but to maximize learning effect for the
learner. (b) might give a similar result because the
learner can copy the correct preposition without
reading the feedback message. In (c), the learner
has to actually read and understand the feedback
message to select the correct preposition. Taking
these into consideration, (c) will likely give the
learner better learning effect than the other two.
Therefore, we propose applying the feedback (c)
to language learning assistance. To the best of our
knowledge, it is only the error case frame-based
method that is capable of this manner of error cor-
rection.
7 Conclusions
This paper presented a novel framework called
error case frames for correcting preposition er-
rors with feedback messages. The evaluation
showed that (i) automatically generated error case
frames achieve a performance comparable to con-
ventional methods; (ii) they are intuitively in-
terpretable and manually modifiable to improve
them; (iii) feedback messages provided by error
case frames are effective in language learning as-
sistance. Considering these advantages and the
fact that it has been difficult to provide feedback
messages by automatically generated rules, error
case frames will likely be one of the major ap-
proaches for preposition error correction.
Appendix A. Sense mapping
The following list shows the mapping between
words and senses developed based on the Word-
Net (Miller, 1995) and GSK dictionary of places
and facilities (2nd Ed.)
9
. Each line consists of a
token for a sense, its definition, examples of its
member. DRINK (drink): tea, coffee
FOOD (food): cake, sandwich
MONTH (names of months): January, February
MINST (musical instruments): guitar, piano
PERSON (persons): John, he
PLACE (place names): Canada, Paris
SPORT (sports): football, tennis
SPORTING (sporting activities): swimming
WEEK (the days of the week): Monday
VEHICLE (vehicles): train, bus
Acknowledgments
We would like to thank Daisuke Kawahara for his
advice on case frame generation. We also would
like to thank Keisuke Sakaguchi and Mamoru Ko-
machi for providing the authors with their system
outputs. Finally, we would acknowledge the help
from the members of the ILES group at LIMSI,
Orsay (France) where the first author performed
part of this work. This work was partly supported
by Kaken Grant-in-Aid for Young Scientists (B)
(26750091).
9
GSK dictionary of places and facilities second edi-
tion: http://www.gsk.or.jp/en/catalog/
gsk2012-c/
762
References
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
SMT techniques. In Proc. of 21th International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 249?256, Sydney, Aus-
tralia, July.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proc. of 4th ACL-SIGSEM Work-
shop on Prepositions, pages 25?30.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proc. 7th
Workshop on Building Educational Applications Us-
ing NLP, pages 54?62.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of 5th International Conference on Language
Resources and Evaluation, pages 449?445.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and de-
terminer error correction in L2 English. In Proc.
of 22nd International Conference on Computational
Linguistics, pages 169?176.
Dana Ferris and Barrie Roberts. 2001. Error feed-
back in L2 writing classes: How explicit does it
need to be? Journal of Second Language Writing,
10(3):161?184.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Japan electronic dictionary research institute Ltd.
1993. EDR electronic dictionary specifications
guide. Japan electronic dictionary research institute
ltd.
Jun?ichi Kakegawa, Hisayuki Kanda, Eitaro Fujioka,
Makoto Itami, and Kohji Itoh. 2000. Diagnostic
processing of Japanese for computer-assisted second
language learning. In Proc. of 38th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 537?546.
Daisuke Kawahara and Kiyotaka Uchimoto. 2008. A
method for automatically constructing case frames
for English. In Proc. of 6th International Confer-
ence on Language Resources and Evaluation.
Kathleen F. McCoy, Christopher A. Pennington, and
Linda Z. Suri. 1996. English error correction: A
syntactic user model based on principled ?mal-rule?
scoring. In Proc. of 5th International Conference on
User Modeling, pages 69?66.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41.
Ryo Nagata and Atsuo Kawai. 2011. Exploiting learn-
ers? tendencies for detecting English determiner er-
rors. In Lecture Notes in Computer Science, volume
6882/2011, pages 144?153.
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and
Naoki Isu. 2006. A feedback-augmented method
for detecting errors in the writing of learners of En-
glish. In Proc. of 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 241?
248.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proc. of 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1210?1219.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proc. 17th Conference on Computational Natural
Language Learning: Shared Task, pages 1?12.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New York.
Thomas Robb, Steven Ross, and Ian Shortreed. 1986.
Salience of feedback on error and its effect on EFL
writing quality. TESOL QUARTERY, 20(1):83?93.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proc. of 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 924?
933.
Alla Rozovskaya and Dan Roth. 2013. Joint learning
and inference for grammatical error correction. In
Proc. of Conference on Empirical Methods in Natu-
ral Language Processing, pages 791?802.
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo,
Lis Kanashiro, Tomoya Mizumoto, Mamoru Ko-
machi, and Yuji Matsumoto. 2012. NAIST at the
HOO 2012 shared task. In Proc. of 7th Workshop on
the Innovative Use of NLP for Building Educational
Applications, pages 281?288.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction
for ESL learners using global context. In Proc. of
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
198?202.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proc. of 48nd Annual Meet-
ing of the Association for Computational Linguistics
Short Papers, pages 353?358.
763
Yuanbin Wu and Hwee Tou Ng. 2013. Grammatical
error correction using integer linear programming.
In Proc. of 51st Annual Meeting of the Association
for Computational Linguistics, pages 1456?1465.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL grammatical error
correction shared task. In Proc. of 17th Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 26?33.
764
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 260?265,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
LIMSI?s Participation in the 2013 Shared Task
on Native Language Identification
Thomas Lavergne, Gabriel Illouz, Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
Orsay, France
{firstname.lastname}@limsi.fr
Ryo Nagata
LIMSI-CNRS & Konan University
8-9-1 Okamoto
Kobe 658-0072 Japan
rnagata@konan-u.ac.jp
Abstract
This paper describes LIMSI?s participation to
the first shared task on Native Language Iden-
tification. Our submission uses a Maximum
Entropy classifier, using as features character
and chunk n-grams, spelling and grammati-
cal mistakes, and lexical preferences. Perfor-
mance was slightly improved by using a two-
step classifier to better distinguish otherwise
easily confused native languages.
1 Introduction
This paper describes the submission from LIMSI to
the 2013 shared task on Native Language Identifica-
tion (Tetreault et al, 2013). The creation of this new
challenge provided us with a dataset (12,100 TOEFL
essays by learners of English of eleven native lan-
guages (Blanchard et al, 2013)) that was necessary
to us to develop an initial framework for studying
Native Language Identification in text. We expect
that this challenge will draw conclusions that will
provide the community with new insights into the
impact of native language in foreign language writ-
ing. We believe that such a research domain is
crucial, not only for improving our understanding
of language learning and language production pro-
cesses, but also for developing Natural Language
Processing applications to support text improve-
ment.
This article is organized as follows. We first de-
scribe in Section 2 our maximum entropy system
used for the classification of a given text in English
into the native languages of the shared task. We then
introduce the various sets of features that we have in-
cluded in our submission, comprising basic n-gram
features (3.1) and features to capture spelling mis-
takes (3.2), grammatical mistakes (3.3), and lexical
preference (3.4). We next report the performance of
each of our sets of features (4.1) and our attempt to
perform a two-step classification to reduce frequent
misclassifications (4.2). We finally conclude with a
short discussion (section 5).
2 A Maximum Entropy model
Our system is based on a classical maximum entropy
model (Berger et al, 1996):
p?(y|x) =
1
Z?(x)
exp(?>F (x, y))
whereF is a vector of feature functions, ? a vector of
associated parameter values, and Z?(x) the partition
function.
Given N independent samples (xi, yi), the model
is trained by minimizing, with respect to ?, the neg-
ative conditional log-likelihood of the observations:
L(?) = ?
N?
i=1
log p(yi|xi).
This term is complemented with an additional regu-
larization term so as to avoid overfitting. In our case,
an `1 regularization is used, with the additional ef-
fect to produce a sparse model.
The model is trained with a gradient descent algo-
rithm (L-BFGS) using the Wapiti toolkit (Lavergne
et al, 2010). Convergence is determined either by
error rate stability on an held-out dataset or when
limits of numerical precision are reached.
260
3 Features
Our submission makes use of basic features, includ-
ing n-grams of characters and part-of-speech tags.
We further experimented with several sets of fea-
tures that will be described and compared in the fol-
lowing sections.
3.1 Basic features
We used n-grams of characters up to length 4 as fea-
tures. In order to reduce the size of the feature space
and the sparsity of these features, we used a hash
kernel (Shi et al, 2009) of size 216 with a hash fam-
ily of size 4. This allowed us to significantly reduce
the training time with no noticeable impact on the
model?s performance.
Our set of basic features also includes n-grams of
part-of-speech (POS) tags and chunks up to length 3.
Both were computed using an in-house CRF-based
tagger trained on PennTreeBank (Marcus et al,
1993). The POS tags sequences were post-processed
so that word tokens were used in lieu of their cor-
responding POS tags for the following: coordinat-
ing conjunctions, determiners, prepositions, modals,
predeterminers, possessives, pronouns, and question
adverbs (Nagata, 2013).
For instance, from this sentence excerpt:
[NP Some/DT people/NNS] [VP
might/MD think/VB] [SBAR that/IN]
[VP traveling/VBG] [PP in/IN]. . .
we extract n-grams from the pseudo POS-tag se-
quence:
Some NNS MD VB that VBG in. . .
and n-grams from the chunk sequence:
NP VP SBAR VP PP. . .
The length of chunks is encoded as separate fea-
tures that correspond to mean length of each type of
chunks. As shown in (Nagata, 2013), length of noun
sequences is also informative and thus was encoded
as a feature.
3.2 Capturing spelling mistakes
We added a set of features to capture information
about spelling mistakes in the model, following the
intuition that some spelling mistakes may be at-
tributed to the influence of the writer?s native lan-
guage.
To extract these features, each document is pro-
cessed using the ispell1 spell checker. This re-
sults in a list of incorrectly written word forms and
a set of potential corrections. For each word, the
best correction is next selected using a set of rules,
which were built manually after a careful study of
the training dataset.
When a corrected word is found, the incorrect
fragment of the word is isolated by striping from
the original and corrected words common prefix and
suffix, keeping only the inner-most substring differ-
ence. For example, given the following mistake and
correction:
appartment? apartment
this procedure generates the following feature:
pp? p
Such a feature may for instance help to identify na-
tive languages (using latin scripts) where doubling
of letters is frequent.
3.3 Capturing grammatical mistakes
Errors at the grammatical level are captured using
the ?language tool? toolkit (Milkowski, 2010), a
rule-based grammar and style checker. Each rule fir-
ing in a document is mapped to an individual feature.
This triggers features such as
BEEN PART AGREEMENT, corresponding to
cases where the auxiliary be is not followed by a
past participle, or EN A VS AN, corresponding to
confusions between the correct form the articles a
and an.
3.4 Capturing lexical preferences
Learners of a foreign language may have some pref-
erence for lexical choice given some semantic con-
tent that they want to convey2. We made the follow-
ing assumption: the lexical variant chosen for each
word may correspond to the less ambiguous choice
if mapping from the native language to English3.
1http://www.gnu.org/software/ispell/
2We assumed that we should not expect thematic differences
in the contents of the essays across original languages, as the
prompts for the essays were evenly distributed.
3This assumption of course could not hold for advanced
learners of English, who should make their lexical choices in-
dependently of their native language.
261
Thus, for each word in an English essay, if we
knew a corresponding word (or sense) that a writer
may have thought of in her native language, we
would like to consider the most likely translation
into English, according to some reliable probabilis-
tic model of lexical translation into English, as the
lexical choice most likely to be made by a learner of
this native language.
As we obviously do not have access to the word
in the native language of the writer, we approximate
this information by searching for the word that max-
imizes the translation probability of translating back
from the native language after translating from the
original English word. This in fact corresponds to a
widely used way of computing paraphrase probabili-
ties from bilingual translation distributions (Bannard
and Callison-Burch, 2005):
e?l ? argmax
e
?
f
pl(f |e).pl(e|f)
where f ranges over all possible translations of En-
glish word e in a given native language l.
Preferably, we would like to obtain candidate
translations into the native language in context,
that is, by translating complete sentences and us-
ing a posteriori translation probabilities. We could
not do this for a number of reasons, the main one
being that we did not have the possibility of using
or building Statistical Machine Translation systems
for all the language pairs involving English and the
native languages of the shared task. We therefore
resorted to simply finding, for each English word,
the most likely back-translation into English via a
given native language. Using the Google Transla-
tion online Statistical Machine Translation service4,
which proposed translations from and to English and
all the native languages of the shared task, a further
approximation had to be made as, in practice, we
were only able to access the most likely translations
for words in isolation: we considered only the best
translation of the original English word in the native
language, and then kept its best back-translation into
English. We here note some common intuitions with
the use of roundtrip translation as a Machine Trans-
lation evaluation metrics (Rapp, 2009).
4http://translate.google.com
Table 1 provides various examples of back-
translations for English adjectives obtained via each
native language. The samples from the Table show
that our procedure produces a significant number of
non identical back-translations. They also illustrate
some types of undesirable results obtained, which
led us to only consider as features for our classi-
fier the proportion of words in essays for which
the above-defined back-translation yielded the same
word, considering all possible native languages. We
only considered content words, as out-of-context
back-translation for function words would be too un-
reliable. Table 2 shows values for some documents
of the training set. As can be seen, there are impor-
tant differences across languages, some languages
obtaining high scores on average (e.g. French and
Japanese) and others obtaining low scores on aver-
age (e.g. Korean, Turkish). Furthermore, the high-
est score is only rarely obtained for the actual native
language of each document, showing that keeping
the most probable language according to this value
alone would not allow to obtain a good classification
performance.
4 Experiments
4.1 Results per set of features
For all our experiments reported here, we used the
full training data provided using cross-validation to
tune the regularization parameter. Our results are
presented in the top part of Table 3. Using our com-
plete set of features yields our best performance on
accuracy, corresponding to a 0.75% absolute im-
provement over using our basic n-gram features
only. No type of features allows a significant im-
provement over the n-gram features when added in-
dividually.
4.2 Two-step classification
Table 4 contains the confusion matrix for our system
across languages. It clearly stands out that two lan-
guage pairs were harder to distinguish: Hindi (hin)
and Telugu (tel) on the one hand, and Korean (kor)
and Japanese (jpn) on the other.
In order to improve the performance of our model,
we performed a two-step classification focused on
these difficult pairs. For this, we built additional
classifiers for each difficult pairs. Both are built
262
eng abrupt affirmative amazing ambiguous anarchic atrocious attentive awkward
ara sudden positive amazing mysterious messy terrible heedful inappropriate
chi sudden sure amazing ambiguous anarchic atrocious careful awkward
fre sudden affirmative amazing ambiguous anarchic atrocious careful awkward
ger abrupt affirmative incredible ambiguous anarchical gruesome attentively awkward
hin suddenly positive amazing vague chaotic brutal observant clumsy
ita abrupt affirmative amazing ambiguous anarchist atrocious careful uncomfortable
jap sudden positive surprising ambiguous anarchy heinous cautious awkward
kor fortuitous positive amazing ambiguous anarchic severe kind awkward
spa abrupt affirmative surprising ambiguous anarchic atrocious attentive clumsy
tel abrupt affirmative amazing ambiguous anarchic formidable attentive awkward
tur sudden positive amazing uncertain anarchic brutal attentive strange
Table 1: Examples of back translations for English adjectives from the training set via each of the eleven native
languages of the shared task. Back-translations that differ from the original word are indicated using a bold face.
Doc id. Native l. ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
976 ARA 0.80 0.88 0.91 0.95 0.75 0.91 0.87 0.73 0.89 0.79 0.71
29905 CHI 0.84 0.81 0.93 0.87 0.79 0.89 0.89 0.56 0.93 0.62 0.75
61765 FRE 0.73 0.84 0.90 0.71 0.73 0.83 0.86 0.50 0.91 0.58 0.66
100416 GER 0.78 0.80 0.86 0.83 0.72 0.89 0.86 0.70 0.90 0.67 0.67
26649 HIN 0.68 0.75 0.88 0.89 0.67 0.85 0.86 0.69 0.86 0.75 0.77
39189 ITA 0.68 0.85 0.92 0.94 0.74 0.93 0.89 0.69 0.92 0.72 0.72
3044 JPN 0.83 0.81 0.89 0.83 0.68 0.94 0.91 0.71 0.94 0.83 0.70
3150 KOR 0.75 0.86 0.91 0.84 0.76 0.88 0.87 0.55 0.88 0.67 0.73
6614 SPA 0.79 0.90 0.86 0.85 0.78 0.85 0.92 0.67 0.90 0.70 0.68
12600 TEL 0.65 0.74 0.84 0.73 0.71 0.92 0.90 0.76 0.95 0.82 0.58
5565 TUR 0.70 0.77 0.88 0.78 0.70 0.84 0.86 0.72 0.84 0.74 0.71
Table 2: Values corresponding to the proportion of content words in a random essay for each native language for which
back-translation yielded the same word.
FRE GER ITA SPA TUR ARA HIN TEL KOR JPN CHI
FRE 79 4 4 3 2 3 0 0 2 2 1
GER 0 89 2 4 1 0 1 0 2 1 0
ITA 6 1 83 6 1 1 0 0 0 1 1
SPA 4 4 5 72 2 3 3 2 1 1 3
TUR 3 2 1 3 81 1 3 2 0 3 1
ARA 3 0 1 3 3 81 5 2 1 0 1
HIN 1 1 1 3 2 1 64 26 1 0 0
TEL 0 0 1 0 0 1 17 81 0 0 0
KOR 1 1 0 0 3 1 0 0 80 12 2
JPN 1 0 2 2 0 3 0 1 13 73 5
CHI 0 1 0 0 2 2 0 2 3 3 87
Table 4: Confusion matrix on the Test set.
263
Features X-Val Test
ngm 74.83% 75.27%
ngm+ort 74.98% 75.29%
ngm+grm 75.18% 75.63%
ngm+lex 74.85% 75.47%
all 75.57% 75.81%
2-step (a) 75.46% 75.69%
2-step (b) 75.89% 75.98%
Table 3: Accuracy results obtained by cross-validation
and using the provided Test set for various combina-
tions of features and our two 2-step strategies. The fea-
ture sets are: character and part-of-speech n-grams fea-
tures (ngm), spelling features (ort), grammatical features
(grm), and lexical preference features (lex).
from the same feature sets as for the first-step model
but with only three labels: one for each language of
the pair and one for any other language.
The training data used for these new models in-
clude all documents from both languages as well as
document misclassified as one of them by the first-
step classifier (using cross-validation to label the full
training set). The formers keep their original labels
while the later are relabeled as other.
Document classified in one of the difficult pairs
by the first-step classifier were post-processed with
these new models. When the new label predicted is
other, the second best choice of the first step is used.
We investigated two setups for the first classifier:
(a) using the original 11 native languages classi-
fier, and (b) using a new classifier with languages
of the difficult pairs merged, resulting in 9 native
?languages?.
Our results, shown in Figure 3 for easy com-
parison, improve over our system using all fea-
tures only when the first-pass classifier uses the set
of 9 merged pseudo-languages (b). We obtain a
moderate 0.32% absolute improvement in accuracy
over one-step classification on cross-validation, and
0.17% improvement on the Test set.
5 Discussion and conclusion
We have submitted on maximum entropy system to
the shared task on Native Language Identification,
for which our basic set of n-gram features already
obtained a level of performance, around 75% in ac-
curacy, close to the best performance reported in our
submission. The additional feature sets that we have
included in our system, while improving the model,
did not allow us to capture a deeper influence of the
native language.
A first analysis reveals that the model fails to fully
use the additional feature sets due to lack of context.
Future experiments will need to link more closely
these features to the documents for which they pro-
vide useful information.
Due to time constraints and engineering issues,
the two-pass system was not ready by the time of
submission. The results that we have included in
this report show that it is a promising approach that
we should continue to explore. We also plan to con-
duct experiments that exploit the information about
the level of English available in the essays, some-
thing that we did not consider for this submission.
While this information is not directly available, it
may be infered from the data as a first-step classifi-
cation. We believe that studying its influence on the
mistakes make learners of different native language
is a promising direction.
The approach that we have described in this sub-
mission, as most of previously published approaches
for this task, attempts to find mistakes in the text of
the documents. The most typical mistakes are then
used by the classifier to detect the native language.
This does not take into consideration the fact that na-
tive English writers also make errors. It would be in-
teresting to explore the divergence between various
sets of writers/learners, not from the mean of non-
native writers, but from the mean of native writers.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 597?604,
Ann Arbor, Michigan.
Adam Berger, Stephen Della Pietra, and Vincent
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1), March.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
264
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics, July.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
Marcin Milkowski. 2010. Developing an open-source,
rule-based proofreading tool. Software - Practice and
Experience, 40(7):543?566.
Ryo Nagata. 2013. Generating a language family tree
from indo-european non-native english texts (to ap-
pear). In Proceedings the 51th Annual Meeting of the
Association for Computational Linguistics (ACL). As-
sociation for Computational Linguistics.
Reinhard Rapp. 2009. The backtranslation score: Auto-
matic mt evalution at the sentence level without refer-
ence translations. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 133?136, Sun-
tec, Singapore.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. Journal of Machine
Learning Research, 10:2615?2637, December.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A Report on the First Native Language Identification
Shared Task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
265
