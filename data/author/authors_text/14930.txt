Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 107?116,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Temporally Anchored Relation Extraction
Guillermo Garrido, Anselmo Pen?as, Bernardo Cabaleiro, and A?lvaro Rodrigo
NLP & IR Group at UNED
Madrid, Spain
{ggarrido,anselmo,bcabaleiro,alvarory}@lsi.uned.es
Abstract
Although much work on relation extraction
has aimed at obtaining static facts, many of
the target relations are actually fluents, as their
validity is naturally anchored to a certain time
period. This paper proposes a methodologi-
cal approach to temporally anchored relation
extraction. Our proposal performs distant su-
pervised learning to extract a set of relations
from a natural language corpus, and anchors
each of them to an interval of temporal va-
lidity, aggregating evidence from documents
supporting the relation. We use a rich graph-
based document-level representation to gener-
ate novel features for this task. Results show
that our implementation for temporal anchor-
ing is able to achieve a 69% of the upper
bound performance imposed by the relation
extraction step. Compared to the state of the
art, the overall system achieves the highest
precision reported.
1 Introduction
A question that arises when extracting a relation is
how to capture its temporal validity: Can we assign a
period of time when the obtained relation held? As
pointed out in (Ling and Weld, 2010), while much
research in automatic relation extraction has focused
on distilling static facts from text, many of the tar-
get relations are in fact fluents, dynamic relations
whose truth value is dependent on time (Russell and
Norvig, 2010).
The Temporally anchored relation extraction
problem consists in, given a natural language text
document corpus, C, a target entity, e, and a target
relation, r, extracting from the corpus the value of
that relation for the entity, and a temporal interval
for which the relation was valid.
In this paper, we introduce a methodological ap-
proach to temporal anchoring of relations automat-
ically extracted from unrestricted text. Our system
(see Figure 1) extracts relational facts from text us-
ing distant supervision (Mintz et al, 2009) and then
anchors the relation to an interval of temporal va-
lidity. The intuition is that a distant supervised sys-
tem can effectively extract relations from the source
text collection, and a straightforward date aggrega-
tion can then be applied to anchor them. We pro-
pose a four step process for temporal anchoring:
(1) represent temporal evidence; (2) select tempo-
ral information relevant to the relation; (3) decide
how a relational fact and its relevant temporal in-
formation are themselves related; and (4) aggregate
imprecise temporal intervals across multiple docu-
ments. In contrast with previous approaches that
aim at intra-document temporal information extrac-
tion (Ling and Weld, 2010), we focus on mining
a corpus aggregating temporal evidences across the
supporting documents.
We address the following research questions:
(1) Validate whether distant supervised learning is
suitable for the task, and evaluate its shortcomings.
(2) Explore whether the use of features extracted
from a document-level rich representation could im-
prove distant supervised learning. (3) Compare the
use of document metadata against temporal expres-
sions within the document for relation temporal an-
choring. (4) Analyze how, in a pipeline architecture,
the propagation of errors limits the overall system?s
107
Training
(1) IR candidate document 
retrieval
(3) Distant supervised
      learning
(5) Relation Extraction
(6) Temporal Anchoring
Document
Collection
Document 
Index
(2) Document 
Representation
(4) Classifiers
Knowledge
Base
Training seeds
< entity, relation name, value >
Training
examples
+ / -
relation 
instances
unlabelled
candidate
Training
Application
Date Extraction
output:
temporally 
anchored 
relations
Date 
Aggregation
Input: Query 
entity
Figure 1: System overview diagram.
performance.
The representation we use for temporal informa-
tion is detailed in section 2; the rich document-level
representation we exploit is described in section 3.
For a query entity and target relation, the system first
performs relation extraction (section 4); then, we
find and aggregate time constraint evidence for the
same relation across different documents, to estab-
lish a temporal validity anchor interval (section 5).
Empirical comparative evaluation of our approach is
introduced in section 6; while some related work is
shown in section 7 and conclusions in section 8.
2 Temporal Anchors
We will denominate relation instance a triple
?entity, relation name, value?. We aim at anchor-
ing relation instances to their temporal validity. We
need a representation flexible enough to capture the
imprecise temporal information available in text,
but expressed in a structured style. Allen?s (1983)
interval-based algebra for temporal representation
and reasoning, underlies much research, such as the
Tempeval challenges (Verhagen et al, 2007; Puste-
jovsky and Verhagen, 2009). Our task is different,
as we focus on obtaining the temporal interval as-
sociated to a fact, rather than reasoning about the
temporal relations among the events appearing in a
single text.
Let us assume that each relation instance is valid
during a certain temporal interval, I = [t0, tf ]. This
sharp temporal interval fails to capture the impreci-
sion of temporal boundaries conveyed in natural lan-
guage text. The Temporal Slot Filling task at TAC-
KBP 2011 (Ji et al, 2011) proposed a 4-tuple rep-
resentation that we will refer to as imprecise anchor
intervals. An imprecise temporal interval is defined
as an ordered 4-tuple of time points: (t1, t2, t3, t4),
with the following semantics: the relation is true for
a period which starts at some point between t1 and
t2 and ends between t3 and t4. It should hold that:
t1 ? t2, t3 ? t4, and t1 ? t4. Any of the four
endpoints can be left unconstrained (t1 or t3 would
be ??, and t2 or t4 would be +?). This represen-
tation is flexible and expressive, although it cannot
capture certain types of information (Ji et al, 2011).
3 Document Representation
We use a rich document representation that employs
a graph structure obtained by augmenting the syn-
tactic dependency analysis of the document with se-
mantic information.
A document D is represented as a document
graph GD; with node set VD and edge set, ED. Each
node v ? VD represents a chunk of text, which is a
sequence of words1. Each node is labeled with a
dictionary of attributes, some of which are common
for every node: the words it contains, their part-of-
speech annotations (POS) and lemmas. Also, a rep-
resentative descriptor, which is a normalized string
value, is generated from the chunks in the node. Cer-
tain nodes are also annotated with one or more types.
There are three families of types: Events (verbs
that describe an action, annotated with tense, polar-
ity and aspect); standardized Time Expressions; and
Named Entities, with additional annotations such as
gender or age.
Edges in the document graph, e ? ED, represent
four kinds of relations between the nodes:
? Syntactic: a dependency relation.
? Coreference: indicates that two chunks refer to
1Most chunks consist in one word; we join words into a
chunk (and a node) in two cases: a multi-word named entity
and a verb and its auxiliaries.
108
David[NNP,David]
NER: PERSON
DESCRIPTOR: 
David
POS: N 
Julia[NNP,Julia]
CLASS:WIFE
NER: PERSON
DESCRIPTOR: 
Julia
POS: N
GENDER:FEMALE 
September[NNP,September] 1979[CD,1979]
NER:DATE
TIMEVALUE:197909
DESCRIPTOR: September 1979
POS: NNP 
wife[NN,wife]
DESCRIPTOR: 
wife
POS: NN 
is[VBZ,be] celebrating[VBG,celebrate]
ASPECT:PROGRESSIVE
TENSE:PRESENT
POLARITY:POS
DESCRIPTOR: celebrate
POS: V 
birthday[NN,birthday]
DESCRIPTOR: 
birthday
POS: NN 
was[VBD,be] born[VBN,bear]
ASPECT:NONE
TENSE:PAST
POLARITY:POS
DESCRIPTOR: bear
POS: V 
arg0
hasClass
prep_in
arg1
arg1
has
INCLUDES
has_wife
Figure 2: Collapsed document graph representation, GC ,
for the sample text document ?David?s wife, Julia, is cel-
ebrating her birthday. She was born in September 1979?.
the same discourse referent.
? Semantic relations between two nodes, such as
hasClass, hasProperty and hasAge.
? Temporal relations between events and time ex-
pressions.
The processing includes dependency parsing,
named entity recognition and coreference reso-
lution, done with the Stanford CoreNLP soft-
ware (Klein and Manning, 2003); and events and
temporal information extraction, via the TARSQI
Toolkit (Verhagen et al, 2005).
The document graph GD is then further trans-
formed into a collapsed document graph, GC . Each
node of GC clusters together coreferent nodes, rep-
resenting a discourse referent. Thus, a node u in GC
is a cluster of nodes u1, . . . , uk of GD. There is an
edge (u, v) in GC if there was an edge between any
of the nodes clustered into u and any of the nodes
v1, . . . , vk? . The coreference edges do not appear in
this representation. Additional semantic information
is also blended into this representation: normaliza-
tion of genitives, semantic class indicators inferred
from appositions and genitives, and gender annota-
tion inferred from pronouns. A final graph example
can be seen in Figure 2.
4 Distant Supervised Relation Extraction
To perform relation extraction, our proposal fol-
lows a distant supervision approach (Mintz et al,
2009), which has also inspired other slot filling sys-
tems (Agirre et al, 2009; Surdeanu et al, 2010).
We capture long distance relations by introducing
a document-level representation and deriving novel
features from deep syntactic and semantic analysis.
Seed harvesting. From a reference Knowledge
Base (KB), we extract a set of relation triples
or seeds: ?entity, relation, value?, where the
relation is one of the target relations. Our
document-level distant supervision assumption is
that if entity and value are found in a document
graph (see section 3), and there is a path connect-
ing them, then the document expresses the relation.
Relation candidates gathering. From a seed triple,
we retrieve candidate documents that contain both
the entity and value, within a span of 20 tokens,
using a standard IR approach. Then, entity and
value are matched to the document graph represen-
tation. We first use approximate string comparison
to find nodes matching the seed entity. After an en-
tity node has been found we use local breadth-first-
search (BFS) to find a matching value and the short-
est connecting path between them. We enforce the
Named Entity type of entity and value to match a
expected type, predefined for the relation.
Our procedure traverses the document graph look-
ing for entity and value nodes meeting those condi-
tions; when found, we generate features for a pos-
itive example for the relation2. If we encounter a
node that matches the expected NE type of the rela-
tion, but does not match the seed value, we generate
a negative example for that relation.
Training. From positive and negative examples, we
generate binary features; some of them are inspired
by previous work (Surdeanu and Ciaramita, 2007;
Mintz et al, 2009; Riedel et al, 2010; Surdeanu et
al., 2010), and others are novel, taking advantage of
our graph representation. Table 1 summarizes our
choice of features. Features appearing in less than 5
training examples were discarded.
Relation instance extraction. Given an input entity
and a target relation, we aim at finding a filler value
for a relation instance. This task is known as Slot
Filling. From the set of retrieved documents relevant
to the query entity, represented as document graphs,
2From the collapsed document graph representation we ob-
tained an average of 9213 positive training examples per slot;
from the uncollapsed document graph, a slightly lower average
of 8178.5 positive examples per slot.
109
Feature name Description
path dependency path between ENTITY and
VALUE in the sentence
X-annotation NE annotations for X
X-pos Part-of-speech annotations for X
X-gov Governor of X in the dependency path
X-mod Modifiers of X in the dependency path
X-has age X is a NE, with an age attribute
X-has class-C X is a NE, with a class C
X-property-P X is a NE, and it has a property P
X-has-Y X is a NE, with a possessive relation with
another NE, Y
X-is-Y X is a NE, in a copula with another NE, Y
X-gender-G X is a NE, and it has gender G
V -tense Tense of the verb V in the path
V -aspect Aspect of the verb V in the path
V -polarity Polarity (positive or negative) of the verb V
Table 1: Features included in the model. X stands for
ENTITY and VALUE. Verb features are generated from
the verbs, V , identified in the path between ENTITY and
VALUE.
we locate matching entities and start a local BFS of
candidate values, generating for them an unlabelled
example. For each of the relations to extract, a bi-
nary classifier (extractor) decides whether the exam-
ple is a valid relation instance. For each particular
relation classifier, only candidates with the expected
entity and value types for the relation were used in
the application phase. Each extractor was a SVM
classifier with linear kernel (Joachims, 2002). All
learning parameters were set to their default values.
The classification process yields a predicted class
label, plus a real number indicating the margin. We
performed an aggregation phase to sum the mar-
gins over distinct occurrences of the same extracted
value. The rationale is that when the same value is
extracted from more than one document, we should
accumulate that evidence.
The output of this phase is the set of extracted re-
lations (positive for each of the classifiers), plus the
documents where the same fact was detected (sup-
porting documents).
5 Temporal Anchoring of Relations
In this section, we propose and discuss a unified
methodological approach for temporal anchoring of
relations. We assume the input is a relation instance
and a set of supporting documents. The task is es-
tablishing a imprecise temporal anchor interval for
the relation.
We present a four-step methodological approach:
(1) representation of intra-document temporal infor-
mation; (2) selection of relevant temporal informa-
tion for the relation; (3) mapping of the link between
relational fact and temporal information into an in-
terval; and (4) aggregation of imprecise intervals.
Temporal representation. The first methodologi-
cal step is to obtain and represent the available intra-
document temporal information; the input is a doc-
ument, and the task is to identify temporal signals
and possible links among them. We use the term link
for a relation between a temporal expression (a date)
and an event; we want to avoid confusion with the
term relation (a relational fact extracted from text).
In our particular implementation:
? We use TARSQI to extract temporal expressions
and link them to events. In particular, TARSQI
uses the following temporal links: included, si-
multaneous, after, before, begun by or ended.
? We focus also on the syntactic pattern [Event-
preposition-Time] within the lexical context of the
candidate entity and value.
? Both are normalized into one from a set of prede-
fined temporal links: within, throughout, begin-
ning, ending, after and before.
Selection of temporal evidence. For each docu-
ment and relational instance, we have to select those
temporal expressions that are relevant.
a. Document-level metadata. The default value
we use is the document creation time (DCT),
if available. The underlying assumption is that
there is a within link from each fact expressed in
the text and the document creation time.
b. Temporal expressions. Temporal evidence
comes also from the temporal expressions
present in the context of a relation. In our par-
ticular implementation, we followed a straight-
forward approach, looking for the time expres-
sion closest in the document graph to the short-
est path between the entity and value nodes. This
search is performed via a limited depth BFS,
starting from the nodes in the path, in order from
value to entity.
Mapping of temporal links into intervals. The
third step is deciding how a relational fact and its rel-
evant temporal information are themselves related.
We have to map this information, expressed in text,
110
Temporal link Constraints mapping
Before t4 = first
After t1 = last
Within and Throughout t2 = first and t3 = last
Beginning t1 = first and t2 = last
Ending t3 = first and t4 = last
Table 2: Mapping from time expression and temporal re-
lation to temporal constraints.
to a temporal representation. We will use the impre-
cise anchor intervals described is section 2.
Let T be a temporal expression identified in the
document or its metadata. Now, the mapping of tem-
poral constraints depends on the temporal link to the
time expression identified; also, the semantics of the
event have to be considered in order to decide the
time period associated to a relation instance. This
step is important because the event could refer just to
the beginning of the relation, its ending, or both. For
instance, it is obvious that having the event marry
is different to having the event divorce, when decid-
ing the temporal constraints associated to the spouse
relation.
Table 2 shows our particular mapping between
temporal links and constraints. In particular, for the
default document creation time, we suppose that a
relation which appears in a document with creation
time d held true at least in that date; that is, we are
assuming a within link, and we map t2 = d, t3 = d.
Inter-document temporal evidence aggregation.
The last step is aggregating all the time constraints
found for the same relation and value across differ-
ent documents. If we found that a relation started af-
ter two dates d and d?, where d? > d, the closest con-
straint to the real start of the relation is d?. Mapped to
temporal constraints, it means that we would choose
the biggest t1 possible. Following the same reason-
ing, we would want to maximize t3. On the other
side, when a relation started before two dates d2 and
d?2, where d
?
2 > d2, the closest constraint is d2 and
we would choose the smallest t2. In summary, we
will maximize t1 and t3 and minimize t2 and t4, so
we will narrow the margins.
6 Evaluation
We have used for our evaluation the dataset com-
piled within the TAC-KBP 2011 Temporal Slot Fill-
ing Task (Ji et al, 2011). We employed as initial
KB the one distributed to participants in the task,
which has been compiled from Wikipedia infoboxes.
It contains 898 triples ?entity, slot type, value? for
100 different entities and up to 8 different slots (re-
lations) per entity3. This gold standard contains the
correct responses pooled from the participant sys-
tems plus a set of responses manually found by
annotators. Each triple has associated a temporal
anchor. The relations had to be extracted from a
domain-general collection of 1.7 million documents.
Our system was one of the five that took part in
the task.We have evaluated the overall system and
the two main components of the architecture: Rela-
tion Extraction, and Temporal Anchoring of the re-
lations. Due to space limitations, the description of
our implementation is very concise; refer to (Garrido
et al, 2011) for further details.
6.1 Evaluation of Relation Extraction
System response in the relation extraction step con-
sists in a set of triples ?entity, slot type, value?.
Performance is measured using precision, recall and
F-measure (harmonic mean) with respect to the 898
triples in the key. Target relations (slots) are poten-
tially list-valued, that is, more than one value can
be valid for a relation (possibly at different points
in time). Only correct values yield any score, and
redundant triples are ignored.
Experiments. We run two different system settings
for the relation extraction step. They differ in the
document representation used (detailed in section3),
in order to empirically assess whether clustering of
discourse referents into single nodes benefits the ex-
traction. In SETTING 1, each document is repre-
sented as a document graph, GD, while in SETTING
2 collapsed document graph representation, GC , is
employed.
Results. Results are shown in Table 3 in the col-
umn Relation Extraction. Both settings have a sim-
ilar performance with a slight increase in the case
of graphs with clustered referents. Although preci-
sion is close to 0.5, recall is lower than 0.1. We have
studied the limits of the assumptions our approach
3There are 7 person relations: cities of residence, state-
orprovinces of residence, countries of residence, employee of,
member of, title, spouse, and an organization relation:
top members/employees.
111
is based on. First, our standard retrieval component
performance limits the overall system?s. As a matter
of example, if we retrieve the first 100 documents
per entity, we find relevant documents only for 62%
of the triples in the key. This number means that no
matter how good relation extraction method is, 38%
of relations will not be found.
Second, the distant supervision assumption un-
derlying our approach is that for a seed relation in-
stance ?entity, relation, value?, any textual men-
tion of entity and value expresses the relation. It
has been shown that this assumption is more often
violated when training knowledge base and docu-
ment collection are of different type, e.g. Wikipedia
and news-wire (Riedel et al, 2010). We have real-
ized that a more determinant factor is the relation
itself and the type of arguments it takes. We ran-
domly sampled 100 training examples per relation,
and manually inspected them to assess if they were
indeed mentions of the relation. While for the re-
lation cities of residence only 30% of the training
examples are expressing the relation, for spouse the
number goes up to 59%. For title, up to 90% of the
examples are correct. This fact explains, at least par-
tially, the zeros we obtain for some relations.
6.2 Evaluation of Temporal Anchoring
Under the evaluation metrics proposed by TAC-KBP
2011, if the value of the relation instance is judged
as correct, the score for temporal anchoring depends
on how well the returned interval matches the one
provided in the key. More precisely, let the correct
imprecise anchor interval in the gold standard key
be Sk = (k1, k2, k3, k4) and the system response be
S = (r1, r2, r3, r4). The absence of a constraint in
t1 or t3 is treated as a value of ??; the absence of
a constraint in t2 or t4 is treated as a value of +?.
Then, let di = |ki ? ri|, for i ? 1, . . . , 4, be the
difference, a real number measured in years. The
score for the system response is:
Q(S) =
1
4
4?
i=1
1
1 + di
The score for a target relation Q(r) is computed
by summing Q(S) over all unique instances of the
relation whose value is correct. If the gold standard
contains N responses, and the system output M re-
sponses, then precision is: P = Q(r)/M , and recall:
R = Q(r)/N ; F1 is the harmonic mean of P and R.
Experiments. We evaluated two different set-
tings for the temporal anchoring step; both use
the collapsed document graph representation, GC
(SETTING 2). The goal of the experiment is two-
fold. First, test the strength of the document creation
time as evidence for temporal anchoring. Second,
test how hard this metadata-level baseline is to beat
using contextual temporal expressions.
The SETTING 2-I assumes a within temporal link
between the document creation time and any relation
expressed inside the document, and aggregates this
information across the documents that we have iden-
tified as supporting the relation. The SETTING 2-II
considers documents content in order to extract tem-
poral links from the context of the text that expresses
the relation. If no temporal expression is found, the
date of the document is used as default. Temporal
links from all supporting documents are mapped into
intervals and aggregated as detailed in section 5.
The performance on relation extraction is an up-
per bound for temporal anchoring, attainable if tem-
poral anchoring is perfect. Thus, we also evaluate
the temporal anchoring performance as the percent-
age the final system achieves with respect to the re-
lation extraction upper bound.
Results. Results are shown in Table 3 under column
Temporal Anchoring. They are low, due to the upper
bound that error propagation in candidate retrieval
and relation extraction imposes upon this step: tem-
porally anchoring alone achives 69% of its upper
bound. This value corresponds to the baseline SET-
TING 2-I, showing its strength. The difference with
SETTING 2-II shows that this baseline is difficult
to beat by considering temporal evidence inside the
document content. There is a reason for this. The
temporal link mapping into time intervals does not
depend only on the type of link, but also on the se-
mantics of the text that expresses the relation as we
pointed out above. We have to decide how to trans-
form the link between relation and temporal expres-
sion into a temporal interval. Learning a model for
this is a hard open research problem that has a strong
adversary in the baseline proposed.
112
Relation Extraction Temporal Anchoring
SETTING 1 SETTING 2 SETTING 2-I SETTING 2-II
P R F P R F P R F % P R F %
(1) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(2) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(3) 0.33 0.02 0.03 0 0 0 0 0 0 0 0 0 0 0
(4) 0.22 0.09 0.13 0.29 0.11 0.16 0.23 0.09 0.13 79 0.21 0.08 0.11 72
(5) 0.53 0.13 0.20 0.54 0.12 0.19 0.34 0.07 0.12 63 0.30 0.06 0.11 56
(6) 0.70 0.12 0.20 0.75 0.13 0.22 0.57 0.10 0.16 76 0.50 0.08 0.14 67
(7) 0.50 0.06 0.10 0.50 0.07 0.12 0.29 0.04 0.07 58 0.25 0.04 0.06 50
(8) 0.25 0.04 0.07 0.20 0.04 0.07 0.15 0.03 0.05 75 0.06 0.01 0.02 30
(9) 0.42 0.08 0.14 0.45 0.08 0.14 0.31 0.06 0.10 69 0.27 0.05 0.09 60
Table 3: Results of experiments for each relation: (1) per:stateorprovinces of residence; (2) per:employee of; (3)
per:countries of residence; (4) per:member of; (5) per:title; (6) org:top members/employees; (7) per:spouse; (8)
per:cities of residence; (9) overall results (calculated as a micro-average).
System # Filled Precision Recall F1
BLENDER2 1206 0.1789 0.3030 0.2250
BLENDER1 1116 0.1796 0.2942 0.2231
BLENDER3 1215 0.1744 0.2976 0.2199
IIRG1 346 0.2457 0.1194 0.1607
Setting 2-1 167 0.2996 0.0703 0.1139
Setting 2-2 167 0.2596 0.0609 0.0986
Stanford 12 5140 0.0233 0.1680 0.0409
Stanford 11 4353 0.0238 0.1453 0.0408
USFD20112 328 0.0152 0.0070 0.0096
USFD20113 127 0.0079 0.0014 0.0024
Table 4: System ID, number of filled responses of the
system, precision, recall and F measure.
6.3 Comparative Evaluation
Our approach was compared with the other four
participants at the KBP Temporal Slot Filling Task
2011. Table 4 shows results sorted by F-measure in
comparison to our two settings (described above).
These official results correspond to a previous
dataset containing 712 triples4.
As shown in column Filled our approach returns
less triples than other systems, explaining low recall.
However, our system achieves the highest precision
for the complete task of temporally anchored rela-
tion extraction. Despite low recall, our system ob-
tains the third best F1 value. This is a very promis-
ing result, since several directions can be explored
to consider more candidates and increase recall.
7 Related Work
Compiling a Knowledge Base of temporally an-
chored facts is an open research challenge (Weikum
et al, 2011). Despite the vast amount of research fo-
cusing on understanding temporal expressions and
4Slot-fillers from human assessors were not considered
their relation to events in natural language, the com-
plete problem of temporally anchored relation ex-
traction remains relatively unexplored. Also, while
much research has focused on single-document ex-
traction, it seems clear that extracting temporally an-
chored relations needs the aggregation of evidences
across multiple documents.
There have been attempts to extend an existing
knowledge base. Wang et al (2010) use regular
expressions to mine Wikipedia infoboxes and cat-
egories and it is not suited for unrestricted text. An
earlier attempt (Zhang et al, 2008), is specific for
business and difficult to generalize to other relations.
Two recent promising works are more related to our
research. Wang et al (2011) uses manually defined
patterns to collect candidate facts and explicit dates,
and re-rank them using a graph label propagation al-
gorithm; their approach is complementary to ours,
as our aim is not to harvest temporal facts but to
extract the relations in which a query entity takes
part; unlike us, they require entity, value, and a ex-
plicit date to appear in the same sentence. Talukdar
et al (2012) focus on the partial task of temporally
anchoring already known facts, showing the useful-
ness of the document creation time as temporal sig-
nal, aggregated across documents.
Earlier work has dealt mainly with partial aspects
of the problem. The TempEval community focused
on the classification of the temporal links between
pairs of events, or an event and a temporal expres-
sion; using shallow features (Mani et al, 2003; La-
pata and Lascarides, 2004; Chambers et al, 2007),
or syntactic-based structured features (Bethard and
Martin, 2007; Pus?cas?u, 2007; Cheng et al, 2007).
Aggregating evidence across different documents
113
to temporally anchor facts has been explored in set-
tings different to Information Extraction, such as
answering of definition questions (Pas?ca, 2008) or
extracting possible dates of well-known historical
events (Schockaert et al, 2010).
Temporal inference or reasoning to solve con-
flicting temporal expressions and induce temporal
order of events has been used in TempEval (Tatu
and Srikanth, 2008; Yoshikawa et al, 2009) and
ACE (Gupta and Ji, 2009) tasks, but focused on
single-document extraction. Ling et al (2010), use
cross-event joint inference to extract temporal facts,
but only inside a single document.
Evaluation campaigns, such as ACE and TAC-
KBP 2011 have had an important role in promoting
this research. While ACE required only to identify
time expressions and classify their relation to events,
KBP requires to infer explicitly the start/end time of
relations, which is a realistic approach in the context
of building time-aware knowledge bases. KBP rep-
resents an important step for the evaluation of tem-
poral information extraction systems. In general, the
participant systems adapted existing slot filling sys-
tems, adding a temporal classification component:
distant supervised (Chen et al, 2010; Surdeanu et
al., 2010) on manually-defined patterns (Byrne and
Dunnion, 2010).
8 Conclusions
This paper introduces the problem of extracting,
from unrestricted natural language text, relational
knowledge anchored to a temporal span, aggregat-
ing temporal evidence from a collection of docu-
ments. Although compiling time-aware knowledge
bases is an important open challenge (Weikum et
al., 2011), it has remained unexplored until very re-
cently (Wang et al, 2011; Talukdar et al, 2012).
We have elucidated the two challenges of the task,
namely relation extraction and temporal anchoring
of the extracted relations.
We have studied how, in a pipeline architecture,
the propagation of errors limits the overall system?s
performance. The performance attainable in the full
task is limited by the quality of the output of the
three main phases: retrieval of candidate passages/
documents, extraction of relations and temporal an-
choring of those.
We have also studied the limits of the distant su-
pervision approach to relation extraction, showing
empirically that its performance depends not only
on the nature of reference knowledge base and doc-
ument corpus (Riedel et al, 2010), but also on the
relation to be extracted. Given a relation between
two arguments, if it is not dominant among textual
expressions of those arguments, the distant supervi-
sion assumption will be more often violated.
We have introduced a novel graph-based docu-
ment level representation, that has allowed us to gen-
erate new features for the task of relation extraction,
capturing long distance structured contexts. Our re-
sults show how, in a document level syntactic repre-
sentation, it yields better results to collapse corefer-
ent nodes.
We have presented a methodological approach
to temporal anchoring composed of: (1) intra-
document temporal information representation; (2)
selection of relation-dependent relevant temporal in-
formation; (3) mapping of temporal links to an inter-
val representation; and (4) aggregation of imprecise
intervals.
Our proposal has been evaluated within a frame-
work that allows for comparability. It has been able
to extract temporally anchored relational informa-
tion with the highest precision among the partici-
pant systems taking part in the competitive evalu-
ation TAC-KBP 2011.
For the temporal anchoring sub-problem, we have
demonstrated the strength of the document creation
time as a temporal signal. It is possible to achieve
a performance of 69% of the upper-bound imposed
by relation extraction by assuming that any relation
mentioned in a document held at the document cre-
ation time (there is a within link between the rela-
tional fact and the document creation time). This
baseline has proved stronger than extracting and an-
alyzing the temporal expressions present in the doc-
ument content.
Acknowledgments
This work has been partially supported by the Span-
ish Ministry of Science and Innovation, through
the project Holopedia (TIN2010-21128-C02), and
the Regional Government of Madrid, through the
project MA2VICMR (S2009/TIC1542).
114
References
Eneko Agirre, Angel X. Chang, Daniel S. Jurafsky,
Christopher D. Manning, Valentin I. Spitkovsky, and
Eric Yeh. 2009. Stanford-UBC at TAC-KBP. In TAC
2009, November.
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26:832?843,
November.
Steven Bethard and James H. Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and se-
mantic features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 129?132, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Lorna Byrne and John Dunnion. 2010. UCD IIRG at
TAC 2010 KBP Slot Filling Task. In Proceedings of
the Third Text Analysis Conference (TAC 2010). NIST,
November.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 173?176, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Wen-Pin Lin, Matthew Snover, Javier Artiles, Marissa
Passantino, and Heng Ji. 2010. CUNY-BLENDER
TAC-KBP2010: Entity linking and slot filling system
description. In Proceedings of the Third Text Analysis
Conference (TAC 2010). NIST, November.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist.japan: temporal relation identifi-
cation using dependency parsed tree. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 245?248, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Guillermo Garrido, Bernardo Cabaleiro, Anselmo Peas,
varo Rodrigo, and Damiano Spina. 2011. A distant
supervised learning system for the TAC-KBP Slot Fill-
ing and Temporal Slot Filling Tasks. In Text Analysis
Conference, TAC 2011 Proceedings Papers.
Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event propaga-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, ACLShort ?09, pages 369?372,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In Text Analysis Conference, TAC 2011 Work-
shop, Notebook Papers.
T. Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines ? Methods, Theory, and
Algorithms. Kluwer/Springer. We used Joachim?s
SVMLight implementation available at http://
svmlight.joachims.org/.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 2003, pages 423?430.
Mirella Lapata and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT 2004.
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence (AAAI-10).
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in news.
In NAACL-Short?03.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL 2009, pages 1003?1011,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M Pas?ca. 2008. Answering Definition Questions via
Temporally-Anchored Text Snippets. Proc. of IJC-
NLP2008.
Georgiana Pus?cas?u. 2007. Wvali: temporal relation
identification by syntactico-semantic analysis. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ?07, pages 484?487,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Pustejovsky and Marc Verhagen. 2009. SemEval-
2010 task 13: evaluating events, time expressions,
and temporal relations (TempEval-2). In Proceed-
ings of the Workshop on Semantic Evaluations: Re-
cent Achievements and Future Directions, DEW ?09,
pages 112?116, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Jose? Balca?zar, Francesco Bonchi,
Aristides Gionis, and Miche`le Sebag, editors, Machine
Learning and Knowledge Discovery in Databases,
volume 6323 of LNCS, pages 148?163. Springer
Berlin / Heidelberg.
Stuart J. Russell and Peter Norvig. 2010. Artificial Intel-
ligence - A Modern Approach (3. internat. ed.). Pear-
son Education.
Steven Schockaert, Martine De Cock, and Etienne Kerre.
2010. Reasoning about fuzzy temporal information
from the web: towards retrieval of historical events.
Soft Computing - A Fusion of Foundations, Method-
ologies and Applications, 14:869?886.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
ACE07, March.
115
Mihai Surdeanu, David McClosky, Julie Tibshirani, John
Bauer, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2010. A simple distant
supervision approach for the tac-kbp slot filling task.
In Proceedings of the Third Text Analysis Conference
(TAC 2010), Gaithersburg, Maryland, USA, Novem-
ber. NIST.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
Proceedings of the Fifth ACM International Confer-
ence on Web Search and Data Mining (WSDM), Seat-
tle, Washington, USA, February. Association for Com-
puting Machinery.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING?08.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Seok Bae Jang, Jessica Littman, Anna
Rumshisky, John Phillips, and James Pustejovsky.
2005. Automating temporal annotation with TARSQI.
In ACLdemo?05.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal re-
lation identification. In SemEval?07.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely YAGO: har-
vesting, querying, and visualizing temporal knowledge
from Wikipedia. In Proceedings of the 13th Inter-
national Conference on Extending Database Technol-
ogy, EDBT ?10, pages 697?700, New York, NY, USA.
ACM.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from textual
web sources by constrained label propagation. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM ?11,
pages 837?846, New York, NY, USA. ACM.
Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.
2011. Temporal knowledge for timely intelligence.
In Malu Castellanos, Umeshwar Dayal, Volker Markl,
Wil Aalst, John Mylopoulos, Michael Rosemann,
Michael J. Shaw, and Clemens Szyperski, editors, En-
abling Real-Time Business Intelligence, volume 84
of Lecture Notes in Business Information Processing,
pages 1?6. Springer Berlin Heidelberg.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ?09, pages 405?413, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Qi Zhang, Fabian M. Suchanek, Lihua Yue, and Gerhard
Weikum. 2008. TOB: Timely ontologies for business
relations. In 11th International Workshop on the Web
and Databases, WebDB.
116
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54?59,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Pattern Learning for Relation Extraction with a Hierarchical Topic Model
Enrique Alfonseca Katja Filippova Jean-Yves Delort
Google Research
Brandschenkestrasse 110
8002 Zurich, Switzerland
{ealfonseca,katjaf,jydelort}@google.com
Guillermo Garrido?
NLP & IR Group, UNED
Juan del Rosal, 16.
28040 Madrid, Spain
ggarrido@lsi.uned.es
Abstract
We describe the use of a hierarchical topic
model for automatically identifying syntactic
and lexical patterns that explicitly state on-
tological relations. We leverage distant su-
pervision using relations from the knowledge
base FreeBase, but do not require any man-
ual heuristic nor manual seed list selections.
Results show that the learned patterns can be
used to extract new relations with good preci-
sion.
1 Introduction
The detection of relations between entities for the
automatic population of knowledge bases is very
useful for solving tasks such as Entity Disambigua-
tion, Information Retrieval and Question Answer-
ing. The availability of high-coverage, general-
purpose knowledge bases enable the automatic iden-
tification and disambiguation of entities in text
and its applications (Bunescu and Pasca, 2006;
Cucerzan, 2007; McNamee and Dang, 2009; Kwok
et al, 2001; Pasca et al, 2006; Weld et al, 2008;
Pereira et al, 2009; Kasneci et al, 2009).
Most early works in this area were designed
for supervised Information Extraction competitions
such as MUC (Sundheim and Chinchor, 1993) and
ACE (ACE, 2004; Doddington et al, 2004; Li et
al., 2011), which rely on the availability of anno-
tated data. Open Information Extraction (Sekine,
2006; Banko et al, 2007; Bollegala et al, 2010)
started as an effort to approach relation extraction in
?Work done during an internship at Google Zurich.
a completely unsupervised way, by learning regular-
ities and patterns from the web. Two example sys-
tems implementing this paradigm are TEXTRUN-
NER (Yates et al, 2007) and REVERB (Fader et al,
2011). These systems do not need any manual data
or rules, but the relational facts they extract are not
immediately disambiguated to entities and relations
from a knowledge base.
A different family of unsupervised methods for
relation extraction is unsupervised semantic pars-
ing, which aims at clustering entity mentions and
relation surface forms, thus generating a semantic
representation of the texts on which inference may
be used. Some techniques that have been used are
Markov Random Fields (Poon and Domingos, 2009)
and Bayesian generative models (Titov and Klemen-
tiev, 2011). These are quite powerful approaches
but have very high computational requirements (cf.
(Yao et al, 2011)).
A good trade-off between fully supervised and
fully unsupervised approaches is distant supervi-
sion, a semi-supervised procedure consisting of find-
ing sentences that contain two entities whose rela-
tion we know, and using those sentences as train-
ing examples for a supervised classifier (Hoffmann
et al, 2010; Wu and Weld, 2010; Hoffmann et al,
2011; Wang et al, 2011; Yao et al, 2011). A usual
problem is that two related entities may co-occur in
one sentence for many unrelated reasons. For ex-
ample, Barack Obama is the president of the United
States, but not every sentence including the two en-
tities supports and states this relation. Much of the
previous work uses heuristics, e.g. extracting sen-
tences only from encyclopedic entries (Mintz et al,
54
2009; Hoffmann et al, 2011; Wang et al, 2011), or
syntactic restrictions on the sentences and the entity
mentions (Wu and Weld, 2010). These are usually
defined manually and may need to be adapted to dif-
ferent languages and domains. Manually selected
seeds can also be used (Ravichandran and Hovy,
2002; Kozareva and Hovy, 2010).
The main contribution of this work is presenting
a variant of distance supervision for relation extrac-
tion where we do not use heuristics in the selection
of the training data. Instead, we use topic models to
discriminate between the patterns that are expressing
the relation and those that are ambiguous and can be
applied across relations. In this way, high-precision
extraction patterns can be learned without the need
of any manual intervention.
2 Unsupervised relational pattern learning
Similar to other distant supervision methods, our ap-
proach takes as input an existing knowledge base
containing entities and relations, and a textual cor-
pus. In this work it is not necessary for the corpus
to be related to the knowledge base. In what follows
we assume that all the relations studied are binary
and hold between exactly two entities in the knowl-
edge base. We also assume a dependency parser is
available, and that the entities have been automat-
ically disambiguated using the knowledge base as
sense inventory.
One of the most important problems to solve in
distant supervision approaches is to be able to dis-
tinguish which of the textual examples that include
two related entities, ei and ej , are supporting the re-
lation. This section describes a fully unsupervised
solution to this problem, computing the probability
that a pattern supports a given relation, which will
allow us to determine the most likely relation ex-
pressed in any sentence. Specifically, if a sentence
contains two entities, ei and ej , connected through a
pattern w, our model computes the probability that
the pattern is expressing any relation ?P (r|w)? for
any relation r defined in the knowledge base. Note
that we refer to patterns with the symbol w, as they
are the words in our topic models.
Preprocessing As a first step, the textual corpus
is processed and the data is transformed in the fol-
lowing way: (a) the input corpus is parsed and en-
Author-book
(Mark Twain, Adventures of Huckleberry
Finn)
ARG1
poss
,,
ARG2
ARG1
nn
,,
novels
nn
,,
ARG2
ARG1
nsubj
--
released ARG2dobj
qq
ARG2 ARG1
conj
rr
ARG1
nsubj
,,
wrote ARG2dobj
rr
ARG1
poss
,,
ARG2
...
(Jhumpa Lahiri, The Namesake)
ARG1
nn
--
ARG2
ARG2 by
prep
qq
ARG1
nn
uu
ARG1
nn
,,
novel
appos
--
ARG2
ARG2 by
prep
qq
ARG1
nn
uu
ARG2 by
prep
qq
ARG1
nn
uu
ARG1
poss
--
ARG1
...
(...)
Person-parent
(Liza Minneli, Judy Garland)
...
(Achilles, Peleus)
...
(...)
Person-death place
(Napoleon Bonaparte, Saint
Helena)
...
(Johann Christian Bach, Lon-
don)
...
(...)
Person-birth place
(Charles Darwin, Shrewsbury)
...
(Anthony Daniels, Salisbury)
...
(...)
Figure 1: Example of a generated set of document collec-
tions from a news corpus for relation extraction. Larger
boxes are document collections (relations), and inner
boxes are documents (entity pairs). Document contain
dependency patterns, which are words in the topic model.
tities are disambiguated; (b) for each relation r in
the knowledge base, a new (initially empty) docu-
ment collection Cr is created; (c) for each entity pair
(ei, ej) which are related in the knowledge base, a
new (initially empty) document Dij is created; (d)
for each sentence in the input corpus containing one
mention of ei and one mention of ej , a new term is
added to Dij consisting of the context in which the
two entities were seen in the document. This context
may be a complex structure, such as the dependency
path joining the two entities, but it is considered for
our purposes as a single term; (e) for each relation r
relating ei with ej , document Dij is added to collec-
tion Cr. Note that if the two entities are related in
different ways at the same time, an identical copy of
the document Dij will be added to the collection for
all those relations.
Figure 1 shows a set of document collections gen-
55
Figure 2: Plate diagram of the generative model used.
erated for three relations using this procedure. Each
relation r has associated a different document col-
lection, which contains one document associated to
each entity pair from the knowledge base which is
in relation r. The words in each document can be,
for example, all the dependency paths that have been
observed in the input textual corpus between the two
related entities. Each document will contain some
very generic paths (e.g. the two entities consecutive
in the text) and some more specific paths.
Generative model Once these collections are
built, we use the generative model from Figure 2
to learn the probability that a dependency path is
conveying some relation between the entities it con-
nects. This model is very similar to the one used
by Haghighi and Vanderwende (2009) in the con-
text of text summarization. w (the observed vari-
able) represents a pattern between two entities. The
topic model ?G captures general patterns that appear
for all relations. ?D captures patterns that are spe-
cific about a certain entity pair, but which are not
generalizable across all pairs with the same relation.
Finally ?A contains the patterns that are observed
across most pairs related with the same relation. ?A
is the topic model of interest for us.
We use Gibbs sampling to estimate the different
models from the source data. The topic assignments
(for each pattern) that are the output of this process
are used to estimate P (r|w): when we observe pat-
tern w, the probability that it conveys relation r.
3 Experiments and results
Settings We use Freebase as our knowledge base.
It can be freely downloaded1. text corpus used con-
tains 33 million English news articles that we down-
loaded between January 2004 and December 2011.
A random sample of 3M of them is used for building
the document collections on which to train the topic
models, and the remaining 30M is used for testing.
The corpus is preprocessed by identifying Freebase
entity mentions, using an approach similar to (Milne
and Witten, 2008), and parsing it with an inductive
dependency parser (Nivre, 2006).
From the three million training documents, a set
of document collections (one per relation) has been
generated, by considering the sentences that contain
two entities which are related in FreeBase through
any binary relation and restricting to high-frequency
200 relations. Two ways of extracting patterns have
been used: (a) Syntactic, taking the dependency
path between the two entities, and (b) Intertext,
taking the text between the two. In both cases, a
topic model has been trained to learn the probabil-
ity of a relation given a pattern w: p(r|w). For ?
we use symmetric Dirichlet priors ?G = 0.1 and
?D = ?A = 0.001, following the intuition that for
the background the probability mass across patterns
should be more evenly distributed. ? is set as (15,
15, 1), indicating in the prior that we expect more
patterns to belong to the background and entity-pair-
specific distributions due to the very noisy nature of
the input data. These values have not been tuned.
As a baseline, using the same training corpus, we
have calculated p(r|w) using the maximum likeli-
hood estimate: the number of times that a pattern w
has been seen connecting two entities for which r
holds divided by the total frequency of the pattern.
Extractions evaluation The patterns have been
applied to the 30 million documents left for testing.
For each pair of entities disambiguated as FreeBase
entities, if they are connected through a known pat-
tern, they are assigned argmaxr p(r|w). We have
randomly sampled 4,000 such extractions and sent
them to raters. An extraction is to be judged cor-
rect if both it is correct in real life and the sentence
from which it was extracted really supports it. We
1http://wiki.freebase.com/wiki/Data dumps
56
Figure 3: Evaluation of the extractions. X-axis has the threshold for p(r|w), and Y-axis has the precision of the extractions as a percentage.
have collected three ratings per example and taken
the majority decision. There was disagreement for
9.4% of the items on whether the sentence supports
the relation, and for 20% of the items on whether the
relation holds in the real world.
The results for different thresholds of p(r|w) are
shown in Figure 3. As can be seen, the MLE base-
lines (in red with syntactic patterns and green with
intertext) perform consistently worse than the mod-
els learned using the topic models (in pink and blue).
The difference in precision, aggregated across all re-
lations, is statistically significant at 95% confidence
for most of the thresholds.
Extractions aggregation We can take advantage
of redundancy on the web to calculate a support met-
ric for the extractions. In this experiment, for every
extracted relation (r, e1, e2), for every occurrence
of a pattern wi connecting e1 and e2, we add up
p(r|wi). Extractions that are obtained many times
and from high-precision patterns will rank higher.
Table 1 describes the results of this aggregation.
We have considered the top four highest-frequency
relations for people. After aggregating all the ex-
tracted relations and ranking them by support, we
have divided the evaluation set into two parts: (a)
for relations that were not already in FreeBase, we
evaluate the precision; (b) for extractions that were
already in FreeBase, we take the top-confidence sen-
tence identified and evaluate whether the sentence
is providing support to the relation. For each of
these, both syntactic patterns and intermediate-text
patterns have been evaluated.
The results are very interesting: using syntax,
Death place appears easy to extract new relations
and to find support. The patterns obtained are quite
unambiguous, e.g.
ARG1
subj
**
died at
prep
vv
home
pobj
ww
in
prep
uu
ARG2
pobj
ww
Relation Unknown relations Known relations
Correct relation P@50 Sentence support P@50
Syntax Intertext Syntax Intertext
Parent 0.58 0.38 1.00 1.00
Death place 0.90 0.68 0.98 0.94
Birth place 0.38 0.56 0.54 0.98
Nationality 0.86 0.78 0.34 0.40
Table 1: Evaluation on aggregated extractions.
On the other hand, birth place and nationality have
very different results for new relation acquisition
vs. finding sentence support for new relations. The
reason is that these relations are very correlated to
other relations that we did not have in our training
set. In the case of birth place, many relations re-
fer to having an official position in the city, such as
mayor; and for nationality, many of the patterns ex-
tract presidents or ministers. Not having mayor or
president in our initial collection (see Figure 1), the
support for these patterns is incorrectly learned. In
the case of nationality, however, even though the ex-
tracted sentences do not support the relation (P@50
= 0.34 for intertext), the new relations extracted are
mostly correct (P@50 = 0.86) as most presidents and
ministers in the real world have the nationality of the
country where they govern.
4 Conclusions
We have described a new distant supervision model
with which to learn patterns for relation extraction
with no manual intervention. Results are promising,
we could obtain new relations that are not in Free-
Base with a high precision for some relation types. It
is also useful to extract support sentences for known
relations. More work is needed in understanding
which relations are compatible or overlapping and
which ones can partially imply each other (such as
president-country or born in-mayor).
57
References
ACE. 2004. The automatic content extraction projects.
http://projects.ldc.upenn.edu/ace.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI?07.
D.T. Bollegala, Y. Matsuo, and M. Ishizuka. 2010. Rela-
tional duality: Unsupervised extraction of semantic re-
lations between entities on the web. In Proceedings of
the 19th international conference on World wide web,
pages 151?160. ACM.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 2007, pages 708?716.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The automatic
content extraction (ace) program?tasks, data, and eval-
uation. In Proceedings of LREC, volume 4, pages
837?840. Citeseer.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identify-
ing relations for open information extraction. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370. Association for Computational Lin-
guistics.
R. Hoffmann, C. Zhang, and D.S. Weld. 2010. Learning
5000 relational extractors. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 286?295. Association for Computa-
tional Linguistics.
R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and
D.S. Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 541?550. Asso-
ciation for Computational Linguistics.
G. Kasneci, M. Ramanath, F. Suchanek, and G. Weikum.
2009. The yago-naga approach to knowledge discov-
ery. ACM SIGMOD Record, 37(4):41?47.
Z. Kozareva and E. Hovy. 2010. Learning arguments
and supertypes of semantic relations using recursive
patterns. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1482?1491. Association for Computational Lin-
guistics.
C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling
question answering to the web. ACM Transactions on
Information Systems (TOIS), 19(3):242?262.
D. Li, S. Somasundaran, and A. Chakraborty. 2011. A
combination of topic models with max-margin learn-
ing for relation detection.
P. McNamee and H.T. Dang. 2009. Overview of the tac
2009 knowledge base population track. In Text Analy-
sis Conference (TAC).
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM conference
on Information and knowledge management, pages
509?518. ACM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2-Volume 2, pages 1003?
1011. Association for Computational Linguistics.
J. Nivre. 2006. Inductive dependency parsing. In
Text, Speech and Language Technology, volume 34.
Springer Verlag.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts-step one: the one-million fact extraction chal-
lenge. In Proceedings of the National Conference on
Artificial Intelligence, page 1400. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
F. Pereira, A. Rajaraman, S. Sarawagi, W. Tunstall-
Pedoe, G. Weikum, and A. Halevy. 2009. An-
swering web questions using structured data: dream
or reality? Proceedings of the VLDB Endowment,
2(2):1646?1646.
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 1-Volume 1, pages 1?10. Association for
Computational Linguistics.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 41?47. Associa-
tion for Computational Linguistics.
S. Sekine. 2006. On-demand information extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731?738. Association for Com-
putational Linguistics.
Beth M. Sundheim and Nancy A. Chinchor. 1993. Sur-
vey of the message understanding conferences. In
HLT?93.
58
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics.
C. Wang, J. Fan, A. Kalyanpur, and D. Gondek. 2011.
Relation extraction with relation topics. In Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing.
Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amershi,
James Fogarty, Raphael Hoffmann, Kayur Patel, and
Michael Skinner. 2008. Intelligence in wikipedia. In
Proceedings of the 23rd national conference on Artifi-
cial intelligence, pages 1609?1614. AAAI Press.
F. Wu and D.S. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127. Association for Computational
Linguistics.
L. Yao, A. Haghighi, S. Riedel, and A. McCallum. 2011.
Structured relation discovery using generative models.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-
head, and S. Soderland. 2007. Textrunner: Open in-
formation extraction on the web. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
59
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1243?1253,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
HEADY: News headline abstraction through event pattern clustering
Enrique Alfonseca
Google Inc.
ealfonseca@google.com
Daniele Pighin
Google Inc.
biondo@google.com
Guillermo Garrido?
NLP & IR Group at UNED
ggarrido@lsi.uned.es
Abstract
This paper presents HEADY: a novel, ab-
stractive approach for headline generation
from news collections. From a web-scale
corpus of English news, we mine syntac-
tic patterns that a Noisy-OR model gener-
alizes into event descriptions. At inference
time, we query the model with the patterns
observed in an unseen news collection,
identify the event that better captures the
gist of the collection and retrieve the most
appropriate pattern to generate a head-
line. HEADY improves over a state-of-the-
art open-domain title abstraction method,
bridging half of the gap that separates
it from extractive methods using human-
generated titles in manual evaluations, and
performs comparably to human-generated
headlines as evaluated with ROUGE.
1 Introduction
Motivation. News events are rarely reported
only in one way, from a single point of view. Dif-
ferent news agencies will interpret the event in dif-
ferent ways; various countries or locations may
highlight different aspects of it depending on how
they are affected; and opinions and in-depth anal-
yses will be written after the fact.
The variety of contents and styles is both an op-
portunity and a challenge. On the positive side, we
have the same events described in different ways;
this redundancy is useful for summarization, as
the information content reported by the majority
of news sources most likely represents the central
part of the event. On the other hand, variability
and subjectivity can be difficult to isolate. For
some applications it is important to understand,
given a collection of related news articles and re-
?Work done during an internship at Google Zurich.
? Carmelo and La La Party It Up with Kim and Ciara
? La La Vazquez and Carmelo Anthony: Wedding
Day Bliss
? Carmelo Anthony, actress LaLa Vazquez wed in
NYC
? Stylist to the Stars
? LaLa, Carmelo Set Off Celebrity Wedding Weekend
? Ciara rocks a sexy Versace Spring 2010 mini to
LaLa Vasquez and Carmelo Anthony?s wedding
(photos)
? Lala Vasquez on her wedding dress, cake, reality tv
show and fiance?, Carmelo Anthony (video)
? VAZQUEZ MARRIES SPORTS STAR AN-
THONY
? Lebron Returns To NYC For Carmelo?s Wedding
? Carmelo Anthony?s stylist dishes on the wedding
? Paul pitching another Big Three with ?Melo in
NYC?
? Carmelo Anthony and La La Vazquez Get Married
at Star-Studded Wedding Ceremony
Table 1: Headlines observed for a news collection
reporting the same wedding event.
ports, how to formulate in an objective way what
has happened.
As a motivating example, Table 1 shows the dif-
ferent headlines observed in news reporting the
wedding between basketball player Carmelo An-
thony and actress LaLa Vazquez. As can be seen,
there is a wide variety of ways to report the same
event, including different points of view, high-
lighted aspects, and opinionated statements on the
part of the reporter. When presenting this event to
a user in a news-based information retrieval or rec-
ommendation system, different event descriptions
may be more appropriate. For example, a user may
only be interested in objective, informative sum-
maries without any interpretation on the part of
the reporter. In this case, Carmelo Anthony, ac-
1243
tress LaLa Vazquez wed in NYC would be a good
choice.
Goal. Our final goal in this research is to build a
headline generation system that, given a news col-
lection, is able to describe it with the most com-
pact, objective and informative headline. In par-
ticular, we want the system to be able to:
? Generate headlines in an open-domain, unsu-
pervised way, so that it does not need to rely
on training data which is expensive to pro-
duce.
? Generalize across synonymous expressions
that refer to the same event.
? Do so in an abstractive fashion, to enforce
novelty, objectivity and generality.
In order to advance towards this goal, this paper
explores the following questions:
? What is a good way of using syntactic pat-
terns to represent events for generating head-
lines?
? Can we have satisfactory readability with an
open-domain abstractive approach, not rely-
ing on training data nor on manually pre-
defined generation templates?
? How far can we get in terms of informative-
ness, compared to the human-produced head-
lines, i.e., extractive approaches?
Contributions. In this paper we present
HEADY, which is at the same time a novel system
for abstractive headline generation, and a smooth
clustering of patterns describing the same events.
HEADY is fully open-domain and can scale to
web-sized data. By learning to generalize events
across the boundaries of a single news story or
news collection, HEADY produces compact and
effective headlines that objectively convey the
relevant information.
When compared to a state-of-the-art open-
domain headline abstraction system (Filippova,
2010), the new headlines are statistically signifi-
cantly better both in terms of readability and in-
formativeness. Also, automatic evaluations using
ROUGE, having objective headlines for the news
as references, show that the abstractive headlines
are on par with human-produced headlines.
2 Related work
Headline generation and summarization.
Most headline generation work in the past has
focused on the problem of single-document sum-
marization: given the main passage of a single
news article, generate a very short summary of
the article. From early in the field, it was pointed
out that a purely extractive approach is not good
enough to generate headlines from the body
text (Banko et al, 2000). Sometimes the most
important information is distributed across several
sentences in the document. More importantly,
quite often, the single sentence selected as the
most informative for the news collection is already
longer than the desired headline size. For this
reason, most early headline generation work fo-
cused on either extracting and reordering n-grams
from the document to be summarized (Banko et
al., 2000), or extracting one or two informative
sentences from the document and performing
linguistically-motivated transformations to them
in order to reduce the summary length (Dorr et
al., 2003). The first approach is not guaranteed
to produce grammatical headlines, whereas the
second approach is tightly tied to the actual
wording found in the document. Single-document
headline generation was also explored at the
Document Understanding Conferences between
2002 and 20041.
In later years, there has been more interest in
problems such as sentence compression (Galley
and McKeown, 2007; Clarke and Lapata, 2008;
Cohn and Lapata, 2009; Napoles et al, 2011;
Berg-Kirkpatrick et al, 2011), text simplification
(Zhu et al, 2010; Coster and Kauchak, 2011;
Woodsend and Lapata, 2011) and sentence fu-
sion (Barzilay and McKeown, 2005; Wan et al,
2007; Filippova and Strube, 2008; Elsner and San-
thanam, 2011). All of them have direct applica-
tions for headline generation, as it can be con-
strued as selecting one or a few sentences from
the original document(s), and then reducing them
to the target title size. For example, Wan et al
(2007) generate novel utterances by combining
Prim?s maximum-spanning-tree algorithm with an
n-gram language model to enforce fluency. Un-
like HEADY, the method by Wan and colleagues
is an extractive method that can summarize single
documents into a sentence, as opposed to generat-
ing a sentence that can stand for a whole collec-
1http://duc.nist.gov/
1244
tion of news. Filippova (2010) reports a system
that is very close to our settings: the input is a
collection of related news articles, and the system
generates a headline that describes the main event.
This system uses sentence compression techniques
and benefits from the redundancy in the collection.
One difference with respect to HEADY is that it
does not use any syntactic information aside from
part-of-speech tags, and it does not require a train-
ing step. We have used this approach as a baseline
for comparison.
There are not many fully abstractive systems for
news summarization. The few that exist, such as
the work by Genest and Lapalme (2012), rely on
manually written generation templates. In con-
trast, HEADY automatically learns the templates
or headline patterns automatically, which allows it
to work in open-domain settings without relying
on supervision or manual annotations.
Open-domain pattern learning. Pattern learn-
ing for relation extraction is an active area of re-
search that is very related to our problem of event
pattern learning for headline generation. TextRun-
ner (Yates et al, 2007), ReVerb (Fader et al, 2011)
and NELL (Carlson et al, 2010; Mohamed et al,
2011) are some examples of open-domain systems
that learn surface patterns that express relations
between pairs of entities. PATTY (Nakashole et
al., 2012) generalizes the patterns to also include
syntactic information and ontological (class mem-
bership) constraints. Our patterns are more similar
to the ones used by PATTY, which also produces
clusters of synonymous patterns. The main differ-
ences are that (a) HEADY is not limited to con-
sider patterns expressing relations between pairs
of entities; (b) we identify synonym patterns us-
ing a probabilistic, Bayesian approach that takes
advantage of the multiplicity of news sources re-
porting the same events. Chambers and Jurafsky
(2009) present an unsupervised method for learn-
ing narrative schemas from news, i.e., coherent
sets of events that involve specific entity types (se-
mantic roles). Similarly to them, we move from
the assumptions that 1) utterances involving the
same entity types within the same document (in
our case, a collection of related documents) are
likely describing aspects of the same event, and
2) meaningful representations of the underlying
events can be learned by clustering these utter-
ances in a principled way.
Noisy-OR networks. Noisy-OR Bayesian net-
works (Pearl, 1988) have been applied in the
past to a wide class of large-scale probabilis-
tic inference problems, from the medical do-
main (Middleton et al, 1991; Jaakkola and Jor-
dan, 1999; Onisko et al, 2001), to synthetic
image-decomposition and co-citation data analy-
sis (S?ingliar and Hauskrecht, 2006). By assum-
ing independence between the causes of the hid-
den variables, noisy-OR models tend to be reli-
able (Friedman and Goldszmidt, 1996) as they re-
quire a relatively small number of parameters to
be estimated (linear with the size of the network).
3 Headline generation
In this section, we describe the HEADY system for
news headline abstraction. Our approach takes as
input, for training, a corpus of news articles or-
ganized in news collections. Once the model is
trained, it can generate headlines for new collec-
tions. An outline of HEADY?s main components
follows (details of each component are provided
in Sections 3.1, 3.2 and 3.3):
Pattern extraction. Identify, in each of the news
collections, syntactic patterns connecting k enti-
ties, for k ? 1. These will be the candidate pat-
terns expressing events.
Training. Train a Noisy-OR Bayesian network
on the co-occurrence of syntactic patterns. Each
pattern extracted in the previous step is added as
an observed variable, and latent variables are used
to represent the hidden events that generate pat-
terns. An additional noise variable links to every
terminal node, allowing every terminal to be gen-
erated by language background (noise) instead of
by an actual event.
Inference. Generate a headline from an unseen
news collection. First, patterns are extracted using
the pattern extraction procedure mentioned above.
Given the patterns, the posterior probability of the
hidden event variables is estimated. Then, from
the activated hidden events, the likelihood of ev-
ery pattern can be estimated, even if they do not
appear in the collection. The single pattern with
the maximum probability is selected to generate a
new headline from it. Being the product of extra-
news collection generalization, the retrieved pat-
tern is more likely to be objective and informative
than patterns directly observed in the news collec-
tion.
1245
Algorithm 1 COLLECTIONTOPATTERNS?(N ):
N is a repository of news collections, ? is a set
of parameters controlling the extraction process.
R ? {}
for all N ? N do
PREPROCESSDATA(N)
E ? GETRELEVANTENTITIES(N ?)
for all Ei ? COMBINATIONS?(E) do
for all n ? N do
P ? EXTRACTPATTERNS?(n, Ei)
R{N,Ei} ? R{N,Ei} ? P
returnR
3.1 Pattern extraction
In this section we detail the process for obtain-
ing the event patterns that constitute the building
blocks of learning and inference.
Patterns are extracted from a large repository
N of news collections N1, . . . , N|N |. Each news
collection N = {ni} is an unordered collec-
tion of related news, each of which can be seen
as an ordered sequence of sentences, i.e.: n =
[s0, . . . s|n|].
Algorithm 1 presents a high-level view of the
pattern extraction process. The different steps are
described below:
PREPROCESSDATA: We start by preprocess-
ing all the news in the news collections with a
standard NLP pipeline: tokenization and sentence
boundary detection (Gillick, 2009), part-of-speech
tagging, dependency parsing (Nivre, 2006), co-
reference resolution (Haghighi and Klein, 2009)
and entity linking based on Wikipedia and Free-
base. Using the Freebase dataset, each entity is
annotated with all its Freebase types (class labels).
In the end, for each entity mentioned in the docu-
ment we have a unique identifier, a list with all its
mentions in the document and a list of class labels
from Freebase.
As a result of this process, we obtain for each
sentence in the corpus a representation as exem-
plified in Figure 1 (1). In this example, the men-
tions of three distinct entities have been identified,
i.e., e1, . . . , e3. In the Freebase list of types (class
labels), e1 is a person and a celebrity, and e3 is a
state and a location.
GETRELEVANTENTITIES: For each news col-
lection N we collect the set E of the entities men-
tioned most often within the collection. Next, we
generate the set COMBINATIONS?(E) consisting
NNP CC NNP TO VB IN NNP
Portia and Helen to marry in California
e1 e2 e3
person actress statecelebrity location
root
cc conj
nsubj
aux prep pobj
1
NNP NNP
e1 e2
person actresscelebrity
conj 2
NNP CC NNP TO VB
e1 and e2 to marry
person actresscelebrity
cc conj
nsubj
aux
3
NNP CC NNP TO VBperson and actress to marry
cc conj
nsubj
aux
4
NNP CC NNP TO VBcelebrity and actress to marry
cc conj
nsubj
aux
Figure 1: Pattern extraction process from an anno-
tated dependency parse. (1): an MST is extracted
from the entity pair e1, e2 (2); nodes are heuristi-
cally added to the MST to enforce grammaticality
(3); entity types are recombined to generate the fi-
nal patterns (4).
of non-empty subsets of E, without repeated en-
tities. The number of entities to consider in each
collection, and the maximum size for the subsets
of entities to consider are meta-parameters embed-
ded in ?.2
EXTRACTPATTERNS: For each subset of rel-
evant entities Ei, event patterns are mined from
the articles in the news collection. The process
by which patterns are extracted from a news is
explained in Algorithm 2 and exemplified graphi-
cally in Figure 1 (2?4).
GETMENTIONNODES: Using the dependency
parse T for a sentence s, we first identify the set
of nodes Mi that mention the entities in Ei. If
T does not contain exactly one mention of each
target entity in Ei, then the sentence is ignored.
Otherwise, we obtain the minimum spanning tree
for the nodeset Pi, i.e., the shortest path in the de-
pendency tree connecting all the nodes inMi (Fig-
ure 1, 2). Pi is the set of nodes around which the
patterns will be constructed.
APPLYHEURISTICS: With very high probabil-
ity, the MST Pi that we obtain does not constitute
a grammatical or useful extrapolation of the origi-
nal sentence s. For example, the MST for the en-
2As our objective is to generate very short titles (under
10 words), we only consider combinations of up to three ele-
ments of E.
1246
Algorithm 2 EXTRACTPATTERNS?(n, Ei): n is
the list of sentences in a news article. Sentences
are POS-tagged, dependency parsed and annotated
with respect to a set of entities E ? Ei
P ? ?
for all s ? n[0 : 2) do
T ? DEPPARSE(s)
Mi ? GETMENTIONNODES(t, Ei)
if ?e ? Ei, count(e,Mi) 6= 1 then continue
Pi ? GETMINIMUMSPANNINGTREE?(Mi)
APPLYHEURISTICS?(Pi) or continue
P ? P ? COMBINEENTITYTYPES?(Pi)
return P
tity pair ?e1, e2? in the example does not provide a
good description of the event as it is neither ade-
quate nor fluent. For this reason, we apply a set of
post-processing heuristic transformations that aim
at including a minimal set of meaningful nodes.
These include making sure that both the root of the
clause and its subject appear in the extracted pat-
tern, and that conjunctions between entities should
not be dropped (Figure 1, 3).
COMBINEENTITYTYPES: Finally, a distinct
pattern is generated from each possible combina-
tion of entity type assignments for the participat-
ing entities. (Figure 1, 4).
It is important to note that both at training and
test time, for pattern extraction we only consider
the title and the first sentence of the article body.
The reason is that we want to limit ourselves, in
each news collection, to the most relevant event
reported in the collection, which appears most of
the times in these two sentences. Unlike titles, first
sentences do not extensively use puns or rhetorics
as they tend to be grammatical and informative
rather than catchy.
The patterns mined from the same news collec-
tion and for the same set of entities are grouped
together, and constitute the building blocks of the
clustering algorithm which is described below.
3.2 Training
The extracted patterns are used to learn a Noisy-
OR (Pearl, 1988) model by estimating the prob-
ability that each (observed) pattern activates one
or many (hidden) events. Figure 2 represents the
two levels: the hidden event variables at the top,
and the observed pattern variables at the bottom.
An additional noise variable links to every termi-
e1 ... en noise
p3p2p1 ... pm
Figure 2: Probabilistic model. The associations
between latent event variables and observed pat-
tern variables are modeled by noisy-OR gates.
Events are assumed to be marginally independent,
and patterns conditionally independent given the
events.
nal node, allowing all terminals to be generated by
language background (noise) instead of by an ac-
tual event. The associations between latent events
and observed patterns are modeled by noisy-OR
gates.
In this model, the conditional probability of a
hidden event ei given a configuration of observed
patterns p ? {0, 1}|P| is calculated as:
P (ei = 0 | p) = (1? qi0)
?
j?pij
(1? qij)pj
= exp
?
???i0 ?
?
j?pii
?ijpj
?
? ,
where pii is the set of active events (i.e., pii =
?j{pj} | pj = 1), and qij = P (ei = 1 | pj = 1)
is the estimated probability that the observed pat-
tern pi can, in isolation, activate the event e. The
term qi0 is the so-called ?noise? term of the model,
and it accounts for the fact that an observed event
ei might be activated by some pattern that has
never been observed (Jaakkola and Jordan, 1999).
In Algorithm 1, at the end of the process we
group in R[N,Ei] all the patterns extracted from
the same news collection N and entity sub-set Ei.
These groups represent rough clusters of patterns,
that we can use to bootstrap the optimization of
the model parameters ?ij = ? log(1 ? qij). We
initiate the training process by randomly selecting
100,000 of these groups, and optimize the weights
of the model through 40 EM (Dempster et al,
1977) iterations.
3.3 Inference (generation of new headlines)
Given an unseen news collection N , the inference
component of HEADY generates a single headline
that captures the main event reported by the news
in N . In order to do so, we first need to select a
1247
single event-pattern p? that is especially relevant
for N . Having selected p?, in order to generate a
headline it is sufficient to replace the entity place-
holders in p? with the surface forms observed in
N .
To identify p?, we start from the assumption that
the most descriptive event encoded by N must de-
scribe an important situation in which some subset
of the relevant entities E in N are involved.
The basic inference algorithm is a two-
step random walk in the Bayesian network.
Given a set of entities E and sentences n,
EXTRACTPATTERNS?(n, E) collects patterns in-
volving those entities. By normalizing the fre-
quency of the extracted patterns, we get a prob-
ability distribution over the observed variables in
the network. A two-step random walk traversing
to the latent event nodes and back to the pattern
nodes allows us to generalize across events. We
call this algorithm INFERENCE(n, E).
In order to decide which is the most relevant set
of events that should appear in the headline, we
use the following procedure:
1. Given the set of entities E mentioned in the
news collection, we consider each entity sub-
set Ei ? E including up to three entities3.
For each Ei, we run INFERENCE(n, Ei),
which computes a distribution wi over pat-
terns involving the entities in Ei.
2. We invoke again INFERENCE, now using at
the same time all the patterns extracted for
every subset of Ei ? E. This computes a
probability distribution w over all patterns in-
volving any admissible subset of the entities
mentioned in the collection.
3. Third, we select the entity-specific distribu-
tion that approximates better the overall dis-
tribution
w? = arg max
i
cos(w,wi)
We assume that the corresponding set of en-
tities Ei are the most central entities in the
collection and therefore any headline should
make sure to mention them all.
3As we noted before, we impose this limitation to keep the
generated headlines relatively short and to limit data sparsity
issues.
4. Finally, we select the pattern with the highest
weight in w? as the pattern that better cap-
tures the main event reported in the news col-
lection:
p? = pj | wj = arg maxj w
?
j
The headline is then produced from p?, replac-
ing placeholders with the entities in the document
from which the pattern was extracted.
While in many cases information about entity
types would be sufficient to decide about the or-
der of the entities in the generated sentences (e.g.,
?[person] married in [location]? for the entity
set {ea = ?Mr. Brown?, eb = ?Los Angeles?}),
in other cases class assignment can be ambigu-
ous (e.g., ?[person] killed [person]? for {ea =
?Mr. A?, eb = ?Mr. B?}). To handle these cases,
when extracting patterns for an entity set {ea, eb},
we keep track of the alphabetical ordering of
the entities, e.g., from a news collection about
?Mr. B? killing ?Mr. A? we would produce
patterns such as ?[person:2] killed [person:1]? or
?[person:1] was killed by [person:2]? since ea =
?Mr. A? < eb = ?Mr. B?. At inference time,
when we query the model with such patterns we
can only activate events whose assignments are
compatible with the entities observed in the text,
making the replacement straightforward and un-
ambiguous.
4 Experiment settings
In our method we use patterns that are fully lex-
icalized (with the exception of entity placehold-
ers) and enriched with syntactic data. Under these
circumstances, the Noisy-OR can effectively gen-
eralize and learn meaningful clusters only if pro-
vided with large amounts of data. To our best
knowledge, available data sets for headline gen-
eration are not large enough to support this kind
of inference.
For this reason, we rely on a corpus of news
crawled from the web between 2008 and 2012
which have been clustered based on closeness in
time and cosine similarity, using the vector-space
model and tf.idf weights. News collections with
less than 5 documents are discarded4, and those
4There is a very long tail of singleton articles, which do
not offer useful examples of lexical or syntactic variation, and
many very small collections that tend to be especially noisy,
hence the decision to consider only collections with at least 5
documents.
1248
larger than 50 documents are capped, by randomly
picking 50 documents from the collection5. The
total number of news collections after clustering
is 1.7 million. From this set, we have set aside
a few hundred collections that will remain unseen
until the final evaluation.
As we have no development set, we have done
no tuning of the parameters for pattern extraction
nor for the Bayesian network training (100,000 la-
tent variables to represent the different events, 40
EM iterations, as mentioned in Section 3.2). The
EM iterations on the noisy-OR were distributed
across 30 machines with 16 GB of memory each.
4.1 Systems used
One of the questions we wanted to answer in
this research was whether it was possible to ob-
tain the same quality with automatically abstracted
headlines as with human-generated headlines. For
every news collection we have as many human-
generated headlines as documents. To decide
which human-generated headline should be used
in this comparison, we used three different meth-
ods that pick one of the collection headlines:
? Latest headline: selects the headline from
the latest document in the collection. Intu-
itively this should be the most relevant one
for news about sport matches and competi-
tions, where the earlier headlines offer pre-
views and predictions, and the later headlines
report who won and the final scores.
? Most frequent headline: some headlines
are repeated across the collection, and this
method chooses the most frequent one. If
there are several with the same frequency,
one is taken at random6.
? TopicSum: we use TopicSum (Haghighi and
Vanderwende, 2009), a 3-layer hierarchical
topic model, to infer the language model that
is most central for the collection. The news
title that has the smallest Kullback-Leibler
5Even though we did not run any experiment to find an
optimal value for this parameter, 50 documents seems like
a reasonable choice to avoid redundancy while allowing for
considerable lexical and syntactic variation.
6The most frequent headline only has a tie in 6 collections
in the whole test set. In 5 cases two headlines are tied at fre-
quencies around 4, and in one case three headlines are tied at
frequency 2. All six are large collections with 50 news arti-
cles, so this baseline is significantly different from a random
baseline.
R-1 R-2 R-SU4
HEADY 0.3565 0.1903 0.1966
Most frequent pattern 0.3560 0.1864 0.1959
TopicSum 0.3594 0.1821 0.1935
MSC 0.3470 0.1765 0.1855
Most frequent headline 0.3177 0.1401 0.1668
Latest headline 0.2814 0.1191 0.1425
Table 2: Results from the automatic evaluation,
sorted according to the ROUGE-2 and ROUGE-
SU4 scores.
divergence with respect the collection lan-
guage model is the one chosen.
A headline generation system that addresses
the same application as ours is (Filippova, 2010),
which generates a graph from the collection sen-
tences and selects the shortest path between the
begin and the end node traversing words in the
same order in which they were found in the orig-
inal documents. We have used this system, called
Multi-Sentence Compression (MSC), for compar-
isons.
Finally, in order to understand whether the
noisy-OR Bayesian network is useful for general-
izing across patterns into latent events, we added a
baseline that extracts all patterns from the test col-
lection following the same COLLECTIONTOPAT-
TERNS algorithm (including the application of the
linguistically motivated heuristics), and then pro-
duces a headline straightaway from the most fre-
quent pattern extracted. In other words, the only
difference with respect to HEADY is that in this
case no generalization through the Noisy-OR net-
work is carried out, and that headlines are gen-
erated from patterns directly observed in the test
news collections. We call this system Most fre-
quent pattern.
4.2 Annotation activities
In order to evaluate HEADY?s performance, we
carried out two annotation activities.
First, from the set of collections that we had
set aside at the beginning, we randomly chose 50
collections for which all the systems could gen-
erate an output, and we asked raters to manually
write titles for them. As this is not a simple task
to be crowdsourced, for this evaluation we relied
on eight trained raters. We collected between four
and five reference titles for each of the fifty news
collections, to be used to compare the headline
1249
Readability Informativeness
TopicSum 4.86 4.63
Most freq. headline ??4.61 ??34.43
Latest headline ??4.55 ? 4.00
HEADY ? 4.28 ? 3.75
Most freq. pattern ? 3.95 ? 3.82
MSC 3.00 3.05
Table 3: Results from the manual evaluation. At
95% confidence, TopicSum is significantly better
than all others for readability, and only indistin-
guishable from the most frequent pattern for in-
formativeness. For the rest, 3 means being signifi-
cantly better than HEADY, ? than the most frequent
pattern, and ? than MSC.
generation methods using automatic summariza-
tion metrics.
Then, we took the output of the systems for the
50 test collections and asked human raters to eval-
uate the headlines:
1. Raters were shown one headline and asked to
rate it in terms of readability on a 5-point
Likert scale. In the instructions, the raters
were provided with examples of ungrammat-
ical and grammatical titles to guide them in
this annotation.
2. After the previous rating is done, raters were
shown a selection of five documents from the
collection, and they were asked to judge the
informativeness of the previous headline for
the news in the collection, again on a 5-point
Likert scale.
This second annotation was carried out by inde-
pendent raters in a crowd-sourcing setting. The
raters did not have any involvement with the in-
ception of the model or the writing of the pa-
per. They did not know that the headlines they
were rating were generated according to differ-
ent methods. We measured inter-judge agreement
on the Likert-scale annotations using their Intra-
Class Correlation (ICC) (Cicchetti, 1994). The
ICC for readability is 0.76 (0.95 confidence in-
terval [0.71, 0.80]), and for informativeness it is
0.67 (0.95 confidence interval [0.60, 0.73]). This
means strong agreement for readability, and mod-
erate agreement for informativeness.
5 Results
The COLLECTIONTOPATTERNS algorithm was
run on the training set, producing a 230 million
event patterns. Patterns that were obtained from
the same collection and involving the same entities
were grouped together, for a total of 1.7 million
pattern collections. The pattern groups are used to
bootstrap the Noisy-OR model training. Training
the HEADY model that we used for the evaluation
took around six hours on 30 cores.
Table 2 shows the results of the comparison
of the headline generation systems using ROUGE
(R-1, R-2 and R-SU4) (Lin, 2004) with the col-
lected references. According to Owczarzak et
al. (2012), ROUGE is still a competitive met-
ric that correlates well with human judgements
for ranking summarizers. The significance tests
for ROUGE are performed using bootstrap resam-
pling and a graphical significance test (Minka,
2002). The human annotators that created the
references for this evaluation were explicitly in-
structed to write objective titles, which is the kind
of headlines that the abstractive systems aim at
generating. It is common to see real headlines
that are catchy, joking, or with a double mean-
ing, and therefore they use a different vocabulary
than objective titles that simply mention what hap-
pened. TopicSum sometimes selects objective ti-
tles amongst the human-made titles and that is
why it also scores very well with the ROUGE
scores. But the other two criteria for choosing
human-made headlines select non-objective titles
much more often, and this lowers their perfor-
mance when measured with ROUGE with respect
to the objective references.
Table 3 lists the results of the manual evaluation
of readability and informativeness of the generated
headlines. The first result that we can see is the
difference in the rankings between the two evalu-
ations. Part of this difference might be due to the
fact that ROUGE is not as good for discriminating
between human-made and automatic summaries.
In fact, in the DUC competitions, the gap between
human summaries and automatic summaries was
also more apparent in the manual evaluations than
using ROUGE. Another part of the observed dif-
ference may be due to the design of the evalua-
tion. The manual evaluation is asking raters to
judge whether real, human-written titles that were
actually used for those news are grammatical and
informative. As could be expected, as these are
published titles, the real titles score very good on
the manual evaluation.
Some other interesting results are:
1250
Model Generated title
TopicSum Modern Family?s Eric Stonestreet laughs off
Charlize Theron rumours
MSC Modern Family star Eric Stonestreet is dating
Charlize Theron.
Latest headline Eric laughs off Theron dating rumours
Frequent pattern Eric Stonestreet jokes about Charlize relationship
Frequent headline Charlize Theron dating Modern Family star
HEADY Eric Stonestreet not dating Charlize Theron
TopicSum McFadzean rescues point for Crawley Town
MSC Crawley side challenging for a point against Old-
ham Athletic.
Latest headline Reds midfielder victim of racist tweet
Frequent pattern Kyle McFadzean fired a equaliser Crawley were
made
Frequent headline Latics halt Crawley charge
HEADY Kyle McFadzean rescues point for Crawley Town
F.C.
TopicSum UCI to strip Lance Armstrong of his 7 Tour titles
MSC The international cycling union said today.
Latest headline Letters: elderly drivers and Lance Armstrong
Frequent pattern Lance Armstrong stripped of Tour de France ti-
tles
Frequent headline Today in the news: third debate is tonight
HEADY Lance Armstrong was stripped of Tour de France
titles
Table 4: A comparison of the titles generated by
the different models for three news collections.
? Amongst the automatic systems, HEADY per-
formed better than MSC, with statistical sig-
nificance at 95% for all the metrics. Head-
lines based on the most frequent patterns
were better than MSC for all metrics but
ROUGE-2.
? The most frequent pattern baseline and
HEADY have comparable performance across
all the metrics (not statistically significantly
different), although HEADY has slightly bet-
ter scores for all metrics except for informa-
tiveness.
While we do not take any step to explicitly
model stylistic variation, estimating the weights
of the Noisy-OR network turns out to be a very
effective way of filtering out sensational wording
to the advantage of plainer, more objective style.
This may not clearly emerge from the evaluation,
as we did not explicitly ask the raters to annotate
the items based on their objectivity, but a manual
inspection of the clusters suggests that the gener-
alization is working in the right direction.
Table 4 presents a selection of outputs produced
by the six models for three different news collec-
tions. The first example shows a news collection
containing news about a rumour that was imme-
diately denied. In the second example, HEADY
generalization improves over the most frequent
pattern. In the third case, HEADY generates a
good title from a noisy collection (containing dif-
ferent but related events). The examples also
show that TopicSum is very effective in selecting
a good human-generated headline for each collec-
tion. This opens the possibility of using TopicSum
to automatically generate ROUGE references for
future evaluations of abstractive methods.
6 Conclusions
We have presented HEADY, an abstractive head-
line generation system based on the generaliza-
tion of syntactic patterns by means of a Noisy-OR
Bayesian network. We evaluated the model both
automatically and through human annotations.
HEADY performs significantly better than a state-
of-the-art open domain abstractive model (Filip-
pova, 2010) in all evaluations, and is in par with
human-generated headlines in terms of ROUGE
scores. We have shown that it is possible to
achieve high quality generation of news headlines
in an open-domain, unsupervised setting by suc-
cessfully exploiting syntactic and ontological in-
formation. The system relies on a standard NLP
pipeline, requires no manual data annotation and
can effectively scale to web-sized corpora.
For feature work, we plan to improve all compo-
nents of HEADY in order to fill in the gap with the
human-generated titles in terms of readability and
informativeness. One of the directions in which
we plan to move is the removal of the syntac-
tic heuristics that currently enforce pattern well-
formedness and to automatically learn the neces-
sary transformations from the data.
Two other lines of work that we plan to explore
are the possibility of personalizing the headlines
to user interests (as stored in user profiles or ex-
pressed as user queries), and to investigate further
applications of the Bayesian network of event pat-
terns, such as its use for relation extraction and
knowledge base population.
Acknowledgments
The research leading to these results has received
funding from: the EU?s 7th Framework Pro-
gramme (FP7/2007-2013) under grant agreement
number 257790; the Spanish Ministry of Science
and Innovation?s project Holopedia (TIN2010-
21128-C02); and the Regional Government of
Madrid?s MA2VICMR (S2009/TIC1542). We
would like to thank Katja Filippova and the anony-
mous reviewers for their insightful comments.
1251
References
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?00, pages 318?325. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?
328.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481?490. Association for
Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010), pages 3?3.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and Their
Participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2 - Volume
2, pages 602?610.
Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical Assessment, 6(4):284.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31(1):399?429.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637?674.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using Wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9. Association for Computational
Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B.
Rubi. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1?38.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1?8. Association for Computational Lin-
guistics.
Micha Elsner and Deepak Santhanam. 2011. Learn-
ing to fuse disparate sentences. In Proceedings of
the Workshop on Monolingual Text-To-Text Gener-
ation, pages 54?63. Association for Computational
Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535?1545. Association for Computational
Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 177?185. As-
sociation for Computational Linguistics.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 322?330. Association
for Computational Linguistics.
Nir Friedman and Moises Goldszmidt. 1996. Learning
Bayesian networks with local structure. In Proceed-
ings of the Twelfth Conference Annual Conference
on Uncertainty in Artificial Intelligence (UAI-96),
pages 252?262, San Francisco, CA. Morgan Kauf-
mann.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. Proceedings of the North American Chap-
ter of the Association for Computational Linguistics,
pages 180?187.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics, short papers.
Association for Computational Linguistics.
Dan Gillick. 2009. Sentence boundary detection and
the problem with the us. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 241?244. Association for
Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1152?1161. Asso-
ciation for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362?370. Association for
Computational Linguistics.
1252
Tommi S. Jaakkola and Michael I. Jordan. 1999.
Variational probabilistic inference and the QMR-
DT Network. Journal of Artificial Intelligence Re-
search, 10:291?322.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81.
Blackford Middleton, Michael Shwe, David Hecker-
man, Max Henrion, Eric Horvitz, Harold Lehmann,
and Gregory Cooper. 1991. Probabilistic diag-
nosis using a reformulation of the INTERNIST-
1/QMR knowledge base. I. The probabilistic model
and inference algorithms. Methods of information in
medicine, 30(4):241?255, October.
Tom Minka. 2002. Judging Significance from Error
Bars. CM U Tech R eport.
Thahir P Mohamed, Estevam R Hruschka Jr, and
Tom M Mitchell. 2011. Discovering relations be-
tween noun categories. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1447?1455. Association for Com-
putational Linguistics.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84?90. Association for Computa-
tional Linguistics.
Joakim Nivre. 2006. Inductive Dependency Parsing,
volume 34 of Text, Speech and Language Technol-
ogy. Springer.
Agnieszka Onisko, Marek J. Druzdzel, and Hanna Wa-
syluk. 2001. Learning Bayesian network parame-
ters from small data sets: application of Noisy-OR
gates. International Journal of Approximated Rea-
soning, 27(2):165?182.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of the NAACL-HLT 2012 Work-
shop on Evaluation Metrics and System Comparison
for Automatic Summarization, pages 1?9. Associa-
tion for Computational Linguistics.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Mor-
gan Kaufmann.
Toma?s? S?ingliar and Milos? Hauskrecht. 2006. Noisy-or
component analysis and its application to link analy-
sis. J. Mach. Learn. Res., 7:2189?2213, December.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile
Paris. 2007. Global Revision in Summarisation:
Generating Novel Sentences with Prim?s Algorithm.
In Proceedings of PACLING 2007 - 10th Conference
of the Pacific Association for Computational Lin-
guistics.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 409?420. Association
for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: Open information
extraction on the web. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics, pages 1353?1361.
1253
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 43?47,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Detecting compositionality using semantic
vector space models based on syntactic context.
Shared task system description?
Guillermo Garrido
NLP & IR Group at UNED
Madrid, Spain
ggarrido@lsi.uned.es
Anselmo Pen?as
NLP & IR Group at UNED
Madrid, Spain
anselmo@lsi.uned.es
Abstract
This paper reports on the participation of the
NLP GROUP at UNED in the DiSCo?2011
compositionality evaluation task. The aim of
the task is to predict compositionality judge-
ments assigned by human raters to candidate
phrases, in English and German, from three
common grammatical relations: adjective-
noun, subject-verb and subject-object.
Our participation is restricted to adjective-
noun relations in English. We explore the
use of syntactic-based contexts obtained from
large corpora to build classifiers that model
the compositionality of the semantics of such
pairs.
1 Introduction
This paper reports on the NLP GROUP at UNED ?s
participation in DiSCo?2011 Shared Task. We at-
tempt to model the notion of compositionality from
analyzing language use in large corpora. In doing
this, we are assuming the distributional hypothesis:
words that occur in similar contexts tend to have
similar meanings (Harris, 1954). For a review of
the field, see (Turney and Pantel, 2010).
1.1 Approach
In previous approaches to compositionality detec-
tion, different kinds of information have been used:
morphological, lexical, syntactic, and distributional.
? This work has been partially supported by the Spanish
Ministry of Science and Innovation, through the project Holo-
pedia (TIN2010-21128-C02), and the Regional Government of
Madrid, through the project MA2VICMR (S2009/TIC1542).
For our participation, we are interested in exploring,
exclusively, the reach of pure syntactic information
to explain semantics.
Our approach draws from the Background
Knowledge Base representation of texts introduced
in (Pen?as and Hovy, 2010). We hypothesize that
behind syntactic dependencies in natural language
there are semantic relations; and that syntactic con-
texts can be leveraged to represent meaning, particu-
larly of nouns. A system could learn these semantic
relations from large quantities of natural language
text, to build an independent semantic resource, a
Background Knowledge Base (BKB) (Pen?as and
Hovy, 2010).
From a dependency-parsed corpus, we automat-
ically harvest meaning-bearing patterns, matching
the dependency trees to a set of pre-specified syn-
tactic patterns, similarly to (Pado and Lapata, 2007).
Patterns are matched to dependency trees to produce
propositions, carriers of minimal semantic units.
Their frequency in the collection is the fundamen-
tal source of our representation.
Our participation, due to time constraints, is re-
stricted to adjective-noun pairs in English.
2 System Description
Our hypothesis can be spelled out as: words (or
word compounds) with similar syntactic contexts are
semantically similar.
The intuition behind our approach is that non-
compositional compounds are units of meaning.
Then, the meaning of an adjective-noun combina-
tion that is not compositional should be different
from the meaning of the noun alone; for similar
43
approaches, see (Baldwin et al, 2003; Katz and
Giesbrecht, 2006; Mitchell and Lapata, 2010). We
propose studying the distributional semantics of a
adjective-noun compound; in particular, we will rep-
resent it via its syntactic contexts.
2.1 Adjective-noun compounds
Given a particular adjective-noun compound, de-
noted ?a, n?, we want to measure its composition-
ality by comparing its syntactic contexts to those of
the noun: ?n?. After exploring the dataset we real-
ized that considering nouns alone introduced noise,
as contexts of the target and different meanings of
the noun might be hard to separate; in order to soften
this problem we decided to compare the occurrences
of the ?a, n? pair to those of the noun with a different
adjective.
Given a dependency-parsed corpus C, we denote
N the set of all nouns occurring in C and A the set of
all adjectives. An adjective-noun pair, ?a, n?, is an
occurrence in the dependency parse of the sentence
of an arc (a, n), where n is the governor of an adjec-
tival relation, with a as modifier. We define the com-
plementary of ?a, n? as the set of all adjective-noun
pairs with the same noun but a different adjective:
?ac, n? = {?b, n? such that b ? A, b 6= a}
In order to detect compositionality, we compare
the semantics of ?a, n? to those of its complemen-
tary ?ac, n?. We use syntactic context as the repre-
sentation of these compounds? semantics.
We call target pairs those ?a, n? in which we are
interested, as they appear in the training, validation,
or test sets for the task. For each of them, its com-
plementary target is: ?ac, n?.
We model the syntactic contexts of any ?a, n? pair
as a set of vectors in a set of vector spaces defined as
follows. After inspection of the corpus, and its de-
pendency parse annotation layer, we manually spec-
ified a few syntactic relations, which we consider
codify the relevant syntactic relations in which an
?a, n? takes part. For each of these syntactic rela-
tions, we built a vector space model, and we repre-
sented as a vector in it each of the target patterns,
and each of their respective complementary targets.
To compute compositionality of a target, we calcu-
lated the cosine similarity between the target vec-
tor and the target?s complementary vector. So, for
each syntactic relation, and for each target, we have
a value of its similarity to the complementary tar-
get. These similarity values are considered features,
from which to learn the compositionality of targets.
For results comparability, we used the PukWaC
corpus1 as dataset. PukWaC adds to UkWaC a layer
of syntactic dependency annotation. The corpus has
been POS-tagged and lemmatized with the TreeTag-
ger2. The dependency parse was done with Malt-
Parser (Nivre and Scholz, 2004).
2.2 Implementation details
We defined a set of 19 syntactic patterns that define
interesting relations in which an ?a, n? pair might
take part, trying to exploit the dependencies pro-
duced by the MaltParser (Nivre and Scholz, 2004),
including:
? Relations to a verb, other than the auxiliary to
be and to have: subject; object; indirect object;
subject of a passive construction; logical sub-
ject of a passive construction.
? The relations defined in the previous point, en-
riched with a noun that acts as the other element
of a [subject-verb-object] or [subject-passive
verb-logical subject] construction.
? Collapsed prepositional complexes.
? Noun complexes.
? As subject or object of the verb to be.
? Modified by a second adjective.
? As modifier of a possessive.
The paths were defined manually to match our in-
tuitions of which are the paths that best describe the
context of an ?a, n?pair, similarly to (Pado and Lap-
ata, 2007). For each of the patterns, the set of words
that are related through it to the target ?a, n? define
the target?s context.
For most of our processing, we used simple pro-
grams implemented in Prolog and Python. We im-
plemented Prolog programs to model the depen-
dency parsed sentences of the full PUkWaC corpus,
and to match and extract these patterns from them.
After an aggregating step, where proper nouns, num-
bers and dates are substituted by place-holder vari-
1Available at http://wacky.sslmit.unibo.
it
2http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
44
ables, they amount to over 16 million instances,
representing the syntactic relations in which every
?a, n? pair in the corpus takes part. In further pro-
cessing, only those that affect the target pairs, or the
nouns in them, have to be taken into account.
As described above, each pattern we have defined
yields a vector space, where each target and its com-
plementary are represented as a vector. The base
vectors of the vector space model for a pattern are
the words that are syntactic contexts, with that syn-
tactic pattern, of any target in the target set3.
The value of the coordinate for a target and a base
vector is the frequency of the context word as related
to the target by the pattern. All frequencies were
locally scaled using logarithms4.
For each syntactic pattern, and for each target
and complementary, we have two vectors, represent-
ing their meanings in the vector space distributional
model. The complementary vector, in particular,
represents the centroid (average) of the meanings of
all ?b, n? pairs, that share the noun with the target
but have a different adjective, b
We propose that a target will be more composi-
tional if its meaning is more similar to the meaning
of the centroid of its complementary, that codifies
the general meaning of that noun (whenever it ap-
pears with a different adjective).
For each syntactic pattern and target, we can com-
pute the cosine similarity to the complementary tar-
get, and obtain a value to use as a feature of the com-
positionality of the target. Those features will be
used to train a classifier, being the compositionality
score of each sample the label to be learnt.
We used RapidMiner5 (Mierswa et al, 2006) as
our Machine Learning framework. The classifiers
we have used, that are described below, are the im-
plementations available in RapidMiner.
3It would have been possible to consider a common vector
space, using all patterns as base vectors. We decided not to do
so after realising that a single similarity value for a target and
its complementary was not by itself a signal strong enough to
predict the compositionality score. A second objective was to
assess the relative importance of different syntactic contexts for
the task.
4We did not attempt any global weighting. We leave this for
future work.
5http://rapid-i.com
2.3 Feature selection
From the 19 original features, inspection of the cor-
relation to the compositionality score label showed
that some of them were not to be expected to have
much predictive power, while some of them were
too sparse in the collection.
We decided to perform feature selection previ-
ous to all subsequent learning steps. We used
RapidMiner genetic algorithm for feature selection6.
Among the patterns which features were not selected
were those where the ?a, n? pair appears in prepo-
sitional complexes, in noun complexes, as indirect
object, as subject or object of the verb to be, and as
subject of a possessive. Among those selected were
subject and objects of both active and passive con-
structions, and the object of possessives.
2.4 Runs description
Numeric scores For the numeric evaluation task,
we built a regression model by means of a SVM
classifier. We used RapidMiner?s implementation
of mySVMLearner (Ru?ping, 2000), that is based on
the optimization algorithm of SVM-light (Joachims,
1998). We used the default parameters for the clas-
sifier. A simple dot product kernel seemed to ob-
tain the best results in 10-fold cross validation over
the union of the provided train and validation re-
sults. For the three runs, we used identical settings,
optimizing different quality measures in each run:
absolute error (RUN SCORE-1), Pearson?s correla-
tion coefficient (RUN SCORE-2), and Spearman?s
rho (RUN SCORE-3). The choice of a SVM classifier
was motivated by the objective of learning a good
parametric classifier model. In initial experiments,
SVM showed to perform better than other possible
choices, like logistic regression. In hindsight, the
relatively small size of the dataset might be a reason
for the relatively poor results. Experimenting with
other approaches is left for future work.
Coarse scores For the coarse scoring, we decided
to build a different set of classifiers, that would learn
the nominal 3-valued compositionality label. The
classifiers built in our initial experiments turned out
6The mutation step switches features on and off, while the
crossover step interchanges used features. Selection is done
randomly. The algorithm used to evaluate each of the feature
subsets was a SVM identical as the one described below.
45
Run avg4 r ?
RUN-SCORE-1 16.395 0.483 0.487
RUN-SCORE-2 15.874 0.475 0.463
RUN-SCORE-3 16.318 0.494 0.486
baseline 17.857 ? ?
Table 1: TRAINING. Numeric score runs results on 10-fold
cross-validation for the training set. avg4: average absolute
error; r: Pearson?s correlation;?: Spearman?s rho.
Run avg4 r ?
RUN-SCORE-1 17.016 0.237 0.267
RUN-SCORE-2 17.180 0.217 0.219
RUN-SCORE-3 17.289 0.180 0.189
baseline 17.370 ? ?
Table 2: TEST. Numeric score runs for the test set. Only
for the en-ADJ-NN samples. avg4: average absolute error; r:
Pearson?s correlation;?: Spearman?s rho.
to lazily choose the most frequent class (?high?) for
most of the test samples. In an attempt to overcome
this situation and possibly learn non linearly separa-
ble classes, we tried neural network classifiers7. In
hindsight, from seeing the very poor performance of
this classifiers on the test set, it is clear that any per-
formance gains were due to over-fitting on the train-
ing set.
For RUN COARSE-2, we binned the numeric
scores obtained in RUN-SCORE-1, dividing the score
space in three equal sized parts; we decided not to
assume the same distribution of the three labels for
the training and test sets. The results were worse
than the numeric scores, due to the fact that the 3
classes are not equally sized.
2.5 Results
Results in the training phase For all our training,
we performed 10-fold cross validation. For refer-
ence, we report the results as evaluated by averag-
ing over the 10 splits of the union of the provided
training and validation set in Table 1. We compared
against a dummy baseline: return as constant score
the average of the scores in the training and valida-
7For RUN COARSE-1, we used AutoMLP (Breuel and
Shafait, 2010), an algorithm that learns a neural network, op-
timizing both the learning rate and number of hidden nodes of
the network. For RUN COARSE-3, we learnt a simple neural net-
work model, by means of a feed-forward neural network trained
by a backpropagation algorithm (multi-layer perceptron), with
a hidden layer with sigmoid type and size 8.
tion sample sets.
Disappointingly, the resulting classifiers seemed
to be quite lazy, yielding values significatively close
to the average of the compositionality label in the
training and validation set.
The AutoMNLP and neural network seemed to
perform reasonably, and better than other classifiers
we tried (e.g., SVM based). We were wary, though,
of the risk of having learnt an over-fitted model; un-
fortunately, the results on the test set confirmed that:
for instance, the accuracy of RUN-SCORE-3 for the
training set was 0.548, but for the test set it was only
0.327.
Results in the test phase After the task results
were distributed, we verified that our numeric score
runs, for the subtask en-ADJ-NN performed quite
well: fifth among the 17 valid submissions for the
subtask, using the average point difference as quality
measure. Nevertheless, in terms of ranking correla-
tion scores, our system performs presumably worse,
although separate correlation results for the en-ADJ-
NN subtask were not available to us at the time of
writing this report.
Our naive baseline turns out to be strong in terms
of average point score. Of course, the ranking corre-
lation of such a baseline is none; using ranking cor-
relation as quality measure would be more sensible,
given that it discards such a baseline.
3 Conclusions
We obtained modest results in the task. Our three
numeric runs obtained results very similar to each
other. Only taking part in the en-ADJ-NN subtask,
we obtained the 5th best of a total of 17 valid sys-
tems in average point difference. Nevertheless, in
terms ranking correlation scores, our systems seem
to perform worse. The modifications we tried to spe-
cialize for coarse scoring were unsuccessful, yield-
ing poor results.
A few conclusions we can draw at this moment
are: our system could benefit from global frequency
weighting schemes that we did not try but that have
shown to be successful in the past; the relatively
small size of the dataset has not allowed us to learn a
better classifier; finally, we believe the ranking cor-
relation quality measures are more sensible than the
point difference for this particular task.
46
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions: analysis, acquisition and treatment - Volume 18,
MWE ?03, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thomas Breuel and Faisal Shafait. 2010. Automlp: Sim-
ple, effective, fully automated learning rate and size
adjustment. In The Learning Workshop. Online, 4.
Zellig S. Harris. 1954. Distributional structure. Word,
pages 146?162.
Thorsten Joachims. 1998. Making large-scale svm learn-
ing practical. LS8-Report 24, Universita?t Dortmund,
LS VIII-Report.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin
Scholz, and Timm Euler. 2006. Yale: rapid prototyp-
ing for complex data mining tasks. In KDD?06, pages
935?940.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. COLING ?04.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models. Com-
putational Linguistics, 33(2):161?199, jun.
Anselmo Pen?as and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. pages
15?23, jun.
Stefan Ru?ping. 2000. mySVM-Manual.
http://www-ai.cs.uni-dortmund.de
/SOFTWARE/MYSVM/.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141?188.
47
