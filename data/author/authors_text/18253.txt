Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233?237,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling of term-distance and term-occurrence information for im-
proving n-gram language model performance 
 
Tze Yuang Chong1,2, Rafael E. Banchs3, Eng Siong Chng1,2, Haizhou Li1,2,3 
1Temasek Laboratory, Nanyang Technological University, Singapore 639798 
2School of Computer Engineering, Nanyang Technological University, Singapore 639798 
3Institute for Infocomm Research, Singapore 138632 
tychong@ntu.edu.sg, rembanchs@i2r.a-star.edu.sg, 
aseschng@ntu.edu.sg, hli@i2r.a-star.edu.sg 
  
 
Abstract 
In this paper, we explore the use of distance 
and co-occurrence information of word-pairs 
for language modeling. We attempt to extract 
this information from history-contexts of up to 
ten words in size, and found it complements 
well the n-gram model, which inherently suf-
fers from data scarcity in learning long histo-
ry-contexts. Evaluated on the WSJ corpus, bi-
gram and trigram model perplexity were re-
duced up to 23.5% and 14.0%, respectively. 
Compared to the distant bigram, we show that 
word-pairs can be more effectively modeled in 
terms of both distance and occurrence. 
1 Introduction 
Language models have been extensively studied 
in natural language processing. The role of a lan-
guage model is to measure how probably a (tar-
get) word would occur based on some given evi-
dence extracted from the history-context. The 
commonly used n-gram model (Bahl et al 1983) 
takes the immediately preceding history-word 
sequence, of length   1 , as the evidence for 
prediction. Although n-gram models are simple 
and effective, modeling long history-contexts 
lead to severe data scarcity problems. Hence, the 
context length is commonly limited to as short as 
three, i.e. the trigram model, and any useful in-
formation beyond this window is neglected. 
In this work, we explore the possibility of 
modeling the presence of a history-word in terms 
of: (1) the distance and (2) the co-occurrence, 
with a target-word. These two attributes will be 
exploited and modeled independently from each 
other, i.e. the distance is described regardless the 
actual frequency of the history-word, while the 
co-occurrence is described regardless the actual 
position of the history-word. We refer to these 
two attributes as the term-distance (TD) and the 
term-occurrence (TO) components, respectively. 
The rest of this paper is structured as follows. 
The following section presents the most relevant 
related works. Section 3 introduces and moti-
vates our proposed approach. Section 4 presents 
in detail the derivation of both TD and TO model 
components. Section 5 presents some perplexity 
evaluation results. Finally, section 6 presents our 
conclusions and proposed future work. 
2 Related Work 
The distant bigram model (Huang et.al 1993, 
Simon et al 1997, Brun et al 2007) disassembles 
the n-gram into (n?1) word-pairs, such that each 
pair is modeled by a distance-k bigram model, 
where 1      1 . Each distance-k bigram 
model predicts the target-word based on the oc-
currence of a history-word located k positions 
behind.  
Zhou & Lua (1998) enhanced the effective-
ness of the model by filtering out those word-
pairs exhibiting low correlation, so that only the 
well associated distant bigrams are retained. This 
approach is referred to as the distance-dependent 
trigger model, and is similar to the earlier pro-
posed trigger model (Lau et al 1993, Rosenfeld 
1996) that relies on the bigrams of arbitrary dis-
tance, i.e. distance-independent. 
Latent-semantic language model approaches 
(Bellegarda 1998, Coccaro 2005) weight word 
counts with TFIDF to highlight their semantic 
importance towards the prediction. In this type of 
approach, count statistics are accumulated from 
long contexts, typically beyond ten to twenty 
words. In order to confine the complexity intro-
duced by such long contexts, word ordering is 
ignored (i.e. bag-of-words paradigm). 
Other approaches such as the class-based lan-
guage model (Brown 1992, Kneser & Ney 1993) 
233
use POS or POS-like classes of the history-words 
for prediction. The structured language model 
(Chelba & Jelinek 2000) determines the ?heads? 
in the history-context by using a parsing tree. 
There are also works on skipping irrelevant his-
tory-words in order to reveal more informative n-
grams (Siu & Ostendorf 2000, Guthrie et al 
2006). Cache language models exploit temporal 
word frequencies in the history (Kuhn & Mori 
1990, Clarkson & Robinson 1997). 
3 Motivation of the Proposed Approach 
The attributes of distance and co-occurrence are 
exploited and modeled differently in each lan-
guage modeling approach. In the n-gram model, 
for example, these two attributes are jointly taken 
into account in the ordered word-sequence. Con-
sequently, the n-gram model can only be effec-
tively implemented within a short history-context 
(e.g. of size of three or four). 
Both, the conventional trigger model and the 
latent-semantic model capture the co-occurrence 
information while ignoring the distance informa-
tion. It is reasonable to assume that distance in-
formation at far contexts is less likely to be in-
formative and, hence, can be discarded. Howev-
er, intermediate distances beyond the n-gram 
model limits can be very useful and should not 
be discarded. 
On the other hand, distant-bigram models and 
distance-dependent trigger models make use of 
both, distance and co-occurrence, information up 
to window sizes of ten to twenty. They achieve 
this by compromising inter-dependencies among 
history-words (i.e. the context is represented as 
separated word-pairs). However, similarly to n-
gram models, distance and co-occurrence infor-
mation are implicitly tied within the word-pairs. 
In our proposed approach, we attempt to ex-
ploit the TD and TO attributes, separately, to in-
corporate distant context information into the n-
gram, as a remedy to the data scarcity problem 
when learning the far context. 
4 Language Modeling with TD and TO 
A language model estimates word probabilities 
given their history, i.e.  	 
| 	 
 , 
where  denotes the target word and  denotes its 
corresponding history. Let the word located at ith 
position, 
 , be the target-word and its preceding 
word-sequence 
 	 
?

  of 
length   1, be its history-context. Also, in or-
der to alleviate the data scarcity problem, we as-
sume the occurrences of the history-words to be 
independent from each other, conditioned to the 
occurrence of the target-word 
 , i.e.  
 
