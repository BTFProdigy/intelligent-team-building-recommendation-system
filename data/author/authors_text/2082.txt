Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 334?343,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Bayesian Unsupervised Topic Segmentation
Jacob Eisenstein and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge MA 02139
{jacobe,regina}@csail.mit.edu
Abstract
This paper describes a novel Bayesian ap-
proach to unsupervised topic segmentation.
Unsupervised systems for this task are driven
by lexical cohesion: the tendency of well-
formed segments to induce a compact and
consistent lexical distribution. We show that
lexical cohesion can be placed in a Bayesian
context by modeling the words in each topic
segment as draws from a multinomial lan-
guage model associated with the segment;
maximizing the observation likelihood in such
a model yields a lexically-cohesive segmenta-
tion. This contrasts with previous approaches,
which relied on hand-crafted cohesion met-
rics. The Bayesian framework provides a prin-
cipled way to incorporate additional features
such as cue phrases, a powerful indicator of
discourse structure that has not been previ-
ously used in unsupervised segmentation sys-
tems. Our model yields consistent improve-
ments over an array of state-of-the-art systems
on both text and speech datasets. We also
show that both an entropy-based analysis and
a well-known previous technique can be de-
rived as special cases of the Bayesian frame-
work.1
1 Introduction
Topic segmentation is one of the fundamental prob-
lems in discourse analysis, where the task is to
divide a text into a linear sequence of topically-
coherent segments. Hearst?s TEXTTILING (1994)
introduced the idea that unsupervised segmentation
1Code and materials for this work are available at
http://groups.csail.mit.edu/rbg/code/
bayesseg/.
can be driven by lexical cohesion, as high-quality
segmentations feature homogeneous lexical distri-
butions within each topic segment. Lexical cohesion
has provided the inspiration for several successful
systems (e.g., Utiyama and Isahara, 2001; Galley et
al.2003; Malioutov and Barzilay, 2006), and is cur-
rently the dominant approach to unsupervised topic
segmentation.
But despite the effectiveness of lexical cohesion
for unsupervised topic segmentation, it is clear that
there are other important indicators that are ignored
by the current generation of unsupervised systems.
For example, consider cue phrases, which are ex-
plicit discourse markers such as ?now? or ?how-
ever? (Grosz and Sidner, 1986; Hirschberg and Lit-
man, 1993; Knott, 1996). Cue phrases have been
shown to be a useful feature for supervised topic
segmentation (Passonneau and Litman, 1993; Gal-
ley et al, 2003), but cannot be incorporated by
current unsupervised models. One reason for this
is that existing unsupervised methods use arbitrary,
hand-crafted metrics for quantifying lexical cohe-
sion, such as weighted cosine similarity (Hearst,
1994; Malioutov and Barzilay, 2006). Without su-
pervision, it is not possible to combine such met-
rics with additional sources of information. More-
over, such hand-crafted metrics may not general-
ize well across multiple datasets, and often include
parameters which must be tuned on development
sets (Malioutov and Barzilay, 2006; Galley et al,
2003).
In this paper, we situate lexical cohesion in a
Bayesian framework, allowing other sources of in-
formation to be incorporated without the need for
labeled data. We formalize lexical cohesion in a
generative model in which the text for each seg-
334
ment is produced by a distinct lexical distribution.
Lexically-consistent segments are favored by this
model because probability mass is conserved for
a narrow subset of words. Thus, lexical cohesion
arises naturally through the generative process, and
other sources of information ? such as cue words
? can easily be incorporated as emissions from the
segment boundaries.
More formally, we treat the words in each sen-
tence as draws from a language model associated
with the topic segment. This is related to topic-
modeling methods such as latent Dirichlet alocation
(LDA; Blei et al 2003), but here the induced topics
are tied to a linear discourse structure. This property
enables a dynamic programming solution to find the
exact maximum-likelihood segmentation. We con-
sider two approaches to handling the language mod-
els: estimating them explicitly, and integrating them
out, using the Dirichlet Compound Multinomial dis-
tribution (also known as the multivariate Polya dis-
tribution).
We model cue phrases as generated from a sep-
arate multinomial that is shared across all topics
and documents in the dataset; a high-likelihood
model will obtain a compact set of cue phrases.
The addition of cue phrases renders our dynamic
programming-based inference inapplicable, so we
design a sampling-based inference technique. This
algorithm can learn in a completely unsupervised
fashion, but it also provides a principled mechanism
to improve search through the addition of declara-
tive linguistic knowledge. This is achieved by bias-
ing the selection of samples towards boundaries with
known cue phrases; this does not change the under-
lying probabilistic model, but guides search in the
direction of linguistically-plausible segmentations.
We evaluate our algorithm on corpora of spoken
and written language, including the benchmark ICSI
meeting dataset (Janin et al, 2003) and a new tex-
tual corpus constructed from the contents of a med-
ical textbook. In both cases our model achieves per-
formance surpassing multiple state-of-the-art base-
lines. Moreover, we demonstrate that the addition of
cue phrases can further improve segmentation per-
formance over cohesion-based methods.
In addition to the practical advantages demon-
strated by these experimental results, our model re-
veals interesting theoretical properties. Other re-
searchers have observed relationships between dis-
course structure and entropy (e.g., Genzel and Char-
niak, 2002). We show that in a special case of
our model, the segmentation objective is equal to
a weighted sum of the negative entropies for each
topic segment. This finding demonstrates that a re-
lationship between discourse segmentation and en-
tropy is a natural consequence of modeling topic
structure in a generative Bayesian framework. In
addition, we show that the benchmark segmentation
system of Utiyama and Isahara (2001) can be viewed
as another special case of our Bayesian model.
2 Related Work
Existing unsupervised cohesion-based approaches
can be characterized in terms of the metric used to
quantify cohesion and the search technique. Galley
et al (2003) characterize cohesion in terms of lexical
chains ? repetitions of a given lexical item over some
fixed-length window of sentences. In their unsu-
pervised model, inference is performed by selecting
segmentation points at the local maxima of the cohe-
sion function. Malioutov and Barzilay (2006) opti-
mize a normalized minimum-cut criteria based on a
variation of the cosine similarity between sentences.
Most similar to our work is the approach of Utiyama
and Isahara (2001), who search for segmentations
with compact language models; as shown in Sec-
tion 3.1.1, this can be viewed as a special case of our
model. Both of these last two systems use dynamic
programming to search the space of segmentations.
An alternative Bayesian approach to segmentation
was proposed by Purver et al (2006). They assume a
set of documents that is characterized by some num-
ber of hidden topics that are shared across multiple
documents. They then build a linear segmentation
by adding a switching variable to indicate whether
the topic distribution for each sentence is identical
to that of its predecessor. Unlike Purver et al, we
do not assume a dataset in which topics are shared
across multiple documents; indeed, our model can
be applied to single documents individually. Addi-
tionally, the inference procedure of Purver et al re-
quires sampling multiple layers of hidden variables.
In contrast, our inference procedure leverages the
nature of linear segmentation to search only in the
space of segmentation points.
335
The relationship between discourse structure and
cue phrases has been studied extensively; for an
early example of computational work on this topic,
see (Grosz, 1977). Passonneau and Litman (1993)
were the first to investigate the relationship between
cue phrases and linear segmentation. More recently,
cue phrases have been applied to topic segmentation
in the supervised setting. In a supervised system that
is distinct from the unsupervised model described
above, Galley et al (2003) automatically identify
candidate cue phrases by mining labeled data for
words that are especially likely to appear at segment
boundaries; the presence of cue phrases is then used
as a feature in a rule-based classifier for linear topic
segmentation. Elsner and Charniak (2008) specify
a list of cue phrases by hand; the cue phrases are
used as a feature in a maximum-entropy classifier
for conversation disentanglement. Unlike these ap-
proaches, we identify candidate cue phrases auto-
matically from unlabeled data and incorporate them
in the topic segmentation task without supervision.
3 Lexical Cohesion in a Bayesian
Framework
The core idea of lexical cohesion is that topically-
coherent segments demonstrate compact and con-
sistent lexical distributions (Halliday and Hasan,
1976). Lexical cohesion can be placed in a prob-
abilistic context by modeling the words in each
topic segment as draws from a multinomial language
model associated with the segment. Formally, if sen-
tence t is in segment j, then the bag of words xt
is drawn from the multinomial language model ?j .
This is similar in spirit to hidden topic models such
as latent Dirichlet alocation (Blei et al, 2003), but
rather than assigning a hidden topic to each word,
we constrain the topics to yield a linear segmenta-
tion of the document.
We will assume that topic breaks occur at sen-
tence boundaries, and write zt to indicate the topic
assignment for sentence t. The observation likeli-
hood is,
p(X|z,?) =
T?
t
p(xt|?zt), (1)
where X is the set of all T sentences, z is the vector
of segment assignments for each sentence, and ? is
the set of all K language models.2 A linear segmen-
tation is ensured by the additional constraint that zt
must be equal to either zt?1 (the previous sentence?s
segment) or zt?1 + 1 (the next segment).
To obtain a high likelihood, the language mod-
els associated with each segment should concentrate
their probability mass on a compact subset of words.
Language models that spread their probability mass
over a broad set of words will induce a lower likeli-
hood. This is consistent with the principle of lexical
cohesion.
Thus far, we have described a segmentation in
terms of two parameters: the segment indices z, and
the set of language models ?. For the task of seg-
menting documents, we are interested only in the
segment indices, and would prefer not to have to
search in the space of language models as well. We
consider two alternatives: taking point estimates of
the language models (Section 3.1), and analytically
marginalizing them out (Section 3.2).
3.1 Setting the language model to the posterior
expectation
One way to handle the language models is to choose
a single point estimate for each set of segmenta-
tion points z. Suppose that each language model
is drawn from a symmetric Dirichlet prior: ?j ?
Dir(?0). Let nj be a vector in which each element is
the sum of the lexical counts over all the sentences
in segment j: nj,i =
?
{t:zt=j}mt,i, where mt,i is
the count of word i in sentence t. Assuming that
each xt ? ?j , then the posterior distribution for ?j
is Dirichlet with vector parameter nj+?0 (Bernardo
and Smith, 2000). The expected value of this distri-
bution is the multinomial distribution ??j , where,
??j,i =
nj,i + ?0
?W
i nj,i +W?0
. (2)
In this equation,W indicates the number of words
in the vocabulary. Having obtained an estimate for
the language model ??j , the observed data likelihood
for segment j is a product over each sentence in the
segment,
2Our experiments will assume that the number of topics K
is known. This is common practice for this task, as the desired
number of segments may be determined by the user (Malioutov
and Barzilay, 2006).
336
p({xt : zt = j}|??j) =
?
{t:zt=j}
?
i?xt
??j,i (3)
=
?
{t:zt=j}
W?
i
??
mt,i
j,i (4)
=
W?
i
??
nj,i
j,i . (5)
By viewing the likelihood as a product over all
terms in the vocabulary, we observe interesting con-
nections with prior work on segmentation and infor-
mation theory.
3.1.1 Connection to previous work
In this section, we explain how our model gen-
eralizes the well-known method of Utiyama and
Isahara (2001; hereafter U&I). As in our work,
Utiyama and Isahara propose a probabilistic frame-
work based on maximizing the compactness of the
language models induced for each segment. Their
likelihood equation is identical to our equations 3-5.
They then define the language models for each seg-
ment as ??j,i =
nj,i+1
W+
PW
i nj,i
, without rigorous justifi-
cation. This form is equivalent to Laplacian smooth-
ing (Manning and Schu?tze, 1999), and is a special
case of our equation 2, with ?0 = 1. Thus, the lan-
guage models in U&I can be viewed as the expec-
tation of the posterior distribution p(?j |{xt : zt =
j}, ?0), in the special case that ?0 = 1. Our ap-
proach generalizes U&I and provides a Bayesian
justification for the language models that they ap-
ply. The remainder of the paper further extends this
work by marginalizing out the language model, and
by adding cue phrases. We empirically demonstrate
that these extensions substantially improve perfor-
mance.
3.1.2 Connection to entropy
Our model also has a connection to entropy,
and situates entropy-based segmentation within a
Bayesian framework. Equation 1 defines the objec-
tive function as a product across sentences; using
equations 3-5 we can decompose this across seg-
ments instead. Working in logarithms,
log p(X|z, ??) =
T?
t
log p(xt|??zt)
=
K?
j
?
{t:zt=j}
log p(xt|??j)
=
K?
j
W?
i
nj,i log ??j,i (6)
The last line substitutes in the logarithm of equa-
tion 5. Setting ?0 = 0 and rearranging equation 2,
we obtain nj,i = Nj ??j,i, with Nj =
?W
i nj,i, the
total number of words in segment j. Substituting
this into equation 6, we obtain
log p(X|z, ??) =
K?
j
Nj
?
i
??j,i log ??j,i
=
K?
j
NjH(??j),
where H(??j) is the negative entropy of the multino-
mial ??j . Thus, with ?0 = 0, the log conditional prob-
ability in equation 6 is optimized by a segmentation
that minimizes the weighted sum of entropies per
segment, where the weights are equal to the segment
lengths. This result suggests intriguing connections
with prior work on the relationship between entropy
and discourse structure (e.g., Genzel and Charniak,
2002; Sporleder and Lapata, 2006).
3.2 Marginalizing the language model
The previous subsection uses point estimates of
the language models to reveal connections to en-
tropy and prior work on segmentation. However,
point estimates are theoretically unsatisfying from
a Bayesian perspective, and better performance may
be obtained by marginalizing over all possible lan-
337
guage models:
p(X|z, ?0) =
K?
j
?
{t:zt=j}
p(xt|?0)
=
K?
j
?
d?j
?
{t:zt=j}
p(xt|?j)p(?j |?0)
=
K?
j
pdcm({xt : zt = j}|?0), (7)
where pdcm refers to the Dirichlet compound multi-
nomial distribution (DCM), also known as the multi-
variate Polya distribution (Johnson et al, 1997). The
DCM distribution expresses the expectation over all
multinomial language models, when conditioning
on the Dirichlet prior ?0. When ?0 is a symmetric
Dirichlet prior,
pdcm({xt : zt = j}|?0)
=
?(W?0)
?(Nj +W?0)
W?
i
?(nj,i +W?0)
?(?0)
,
where nj,i is the count of word i in segment j, and
Nj =
?W
i nj,i, the total number of words in the
segment. The symbol ? refers to the Gamma func-
tion, an extension of the factorial function to real
numbers. Using the DCM distribution, we can com-
pute the data likelihood for each segment from the
lexical counts over the entire segment. The overall
observation likelihood is a product across the likeli-
hoods for each segment.
3.3 Objective function and inference
The optimal segmentation maximizes the joint prob-
ability,
p(X, z|?0) = p(X|z, ?0)p(z).
We assume that p(z) is a uniform distribution over
valid segmentations, and assigns no probability
mass to invalid segmentations. The data likelihood
is defined for point estimate language models in
equation 5 and for marginalized language models
in equation 7. Note that equation 7 is written as a
product over segments. The point estimates for the
language models depend only on the counts within
each segment, so the overall likelihood for the point-
estimate version also decomposes across segments.
Any objective function that can be decomposed
into a product across segments can be maximized
using dynamic programming. We define B(t) as the
value of the objective function for the optimal seg-
mentation up to sentence t. The contribution to the
objective function from a single segment between
sentences t? and t is written,
b(t?, t) = p({xt? . . .xt}|zt?...t = j)
The maximum value of the objective function
is then given by the recurrence relation, B(t) =
maxt?<tB(t?)b(t?+1, t), with the base caseB(0) =
1. These values can be stored in a table of size T
(equal to the number of sentences); this admits a dy-
namic program that performs inference in polyno-
mial time.3 If the number of segments is specified
in advance, the dynamic program is slightly more
complex, with a table of size TK.
3.4 Priors
The Dirichlet compound multinomial integrates
over language models, but we must still set the
prior ?0. We can re-estimate this prior based on
the observed data by interleaving gradient-based
search in a Viterbi expectation-maximization frame-
work (Gauvain and Lee, 1994). In the E-step, we
estimate a segmentation z? of the dataset, as de-
scribed in Section 3.3. In the M-step, we maxi-
mize p(?0|X, z?) ? p(X|?0, z?)p(?0). Assuming a
non-informative hyperprior p(?0), we maximize the
likelihood in Equation 7 across all documents. The
maximization is performed using a gradient-based
search; the gradients are dervied by Minka (2003).
This procedure is iterated until convergence or a
maximum of twenty iterations.
4 Cue Phrases
One of the key advantages of a Bayesian framework
for topic segmentation is that it permits the prin-
cipled combination of multiple data sources, even
3This assumes that the objective function for individual seg-
ments can also be computed efficiently. In our case, we need
only keep vectors of counts for each segment, and evaluate
probability density functions over the counts.
338
without labeled data. We are especially interested
in cue phrases, which are explicit markers for dis-
course structure, such as ?now? or ?first? (Grosz
and Sidner, 1986; Hirschberg and Litman, 1993;
Knott, 1996). Cue phrases have previously been
used in supervised topic segmentation (e.g., Gal-
ley et al 2003); we show how they can be used in
an unsupervised setting.
The previous section modeled lexical cohesion by
treating the bag of words in each sentence as a se-
ries of draws from a multinomial language model
indexed by the topic segment. To incorporate cue
phrases, this generative model is modified to reflect
the idea that some of the text will be topic-specific,
but other terms will be topic-neutral cue phrases
that express discourse structure. This idea is imple-
mented by drawing the text at each topic boundary
from a special language model ?, which is shared
across all topics and all documents in the dataset.
For sentences that are not at segment bound-
aries, the likelihood is as before: p(xt|z,?, ?) =?
i?xt ?zt,i. For sentences that immediately follow
segment boundaries, we draw the first ` words from
? instead. Writing x(`)t for the ` cue words in xt,
and x?t for the remaining words, the likelihood for a
segment-initial sentence is,
p(xt|zt 6= zt?1,?, ?) =
?
i?x(`)t
?i
?
i?x?t
?zt,i.
We draw ? from a symmetric Dirichlet prior ?0. Fol-
lowing prior work (Galley et al, 2003; Litman and
Passonneau, 1995), we consider only the first word
of each sentence as a potential cue phrase; thus, we
set ` = 1 in all experiments.
4.1 Inference
To estimate or marginalize the language models ?
and ?, it is necessary to maintain lexical counts for
each segment and for the segment boundaries. The
counts for ? are summed across every segment in
the entire dataset, so shifting a boundary will af-
fect the probability of every segment, not only the
adjacent segments as before. Thus, the factoriza-
tion that enabled dynamic programming inference
in Section 3.3 is no longer applicable. Instead, we
must resort to approximate inference.
Sampling-based inference is frequently used in
related Bayesian models. Such approaches build
a stationary Markov chain by repeatedly sampling
among the hidden variables in the model. The most
commonly-used sampling-based technique is Gibbs
sampling, which iteratively samples from the condi-
tional distribution of each hidden variable (Bishop,
2006). However, Gibbs sampling is slow to con-
verge to a stationary distribution when the hidden
variables are tightly coupled. This is the case in
linear topic segmentation, due to the constraint that
zt ? {zt?1, zt?1 + 1} (see Section 3).
For this reason, we apply the more general
Metropolis-Hastings algorithm, which permits sam-
pling arbitrary transformations of the latent vari-
ables. In our framework, such transformations cor-
respond to moves through the space of possible seg-
mentations. A new segmentation z? is drawn from
the previous hypothesized segmentation z based on
a proposal distribution q(z?|z).4 The probability of
accepting a proposed transformation depends on the
ratio of the joint probabilities and a correction term
for asymmetries in the proposal distribution:
paccept(z? z?) = min
{
1,
p(X, z?|?0, ?0)
p(X, z|?0, ?0)
q(z|z?)
q(z?|z)
}
.
The Metropolis-Hastings algorithm guarantees
that by accepting samples at this ratio, our sampling
procedure will converge to the stationary distribu-
tion for the hidden variables z. When cue phrases
are included, the observation likelihood is written:
p(X|z,?, ?) =
?
{t:zt 6=zt?1}
?
i?x(`)t
?i
?
i?x?t
?zt,i
?
?
{t:zt=zt?1}
?
i?xt
?zt,i.
As in Section 3.2, we can marginalize over the
language models. We obtain a product of DCM dis-
tributions: one for each segment, and one for all cue
phrases in the dataset.
4.2 Proposal distribution
Metropolis-Hastings requires a proposal distribution
to sample new configurations. The proposal distri-
4Because the cue phrase language model ? is used across
the entire dataset, transformations affect the likelihood of all
documents in the corpus. For clarity, our exposition will focus
on the single-document case.
339
bution does not affect the underlying probabilistic
model ? Metropolis-Hastings will converge to the
same underlying distribution for any non-degenerate
proposal. However, a well-chosen proposal distribu-
tion can substantially speed convergence.
Our basic proposal distribution selects an existing
segmentation point with uniform probability, and
considers a set of local moves. The proposal is con-
structed so that no probability mass is allocated to
moves that change the order of segment boundaries,
or merge two segments; one consequence of this re-
striction is that moves cannot add or remove seg-
ments.5 We set the proposal distribution to decrease
exponentially with the move distance, thus favoring
incremental transformations to the segmentation.
More formally, let d(z ? z?) > 0 equal the dis-
tance that the selected segmentation point is moved
when we transform the segmentation from z to z?.
We can write the proposal distribution q(z? | z) ?
c(z ? z?)d(z ? z?)?, where ? < 0 sets the rate
of exponential decay and c is an indicator function
enforcing the constraint that the moves do not reach
or cross existing segmentation points.6
We can also incorporate declarative linguistic
knowledge by biasing the proposal distribution in
favor of moves that place boundaries near known
cue phrase markers. We multiply the unnormalized
chance of proposing a move to location z? z? by a
term equal to one plus the number of candidate cue
phrases in the segment-initial sentences in the new
configuration z?, written num-cue(z?). Formally,
qling(z? | z?) ? (1 + num-cue(z?))q(z? | z). We
use a list of cue phrases identified by Hirschberg and
Litman (1993). We evaluate our model with both the
basic and linguistically-enhanced proposal distribu-
tions.
4.3 Priors
As in section 3.4, we set the priors ?0 and ?0 us-
ing gradient-based search. In this case, we perform
gradient-based optimization after epochs of 1000
5Permitting moves to change the number of segments would
substantially complicate inference.
6We set ? = ? 1max-move , where max-move is the maximum
move-length, set to 5 in our experiments. These parameters af-
fect the rate of convergence but are unrelated to the underly-
ing probability model. In the limit of enough samples, all non-
pathological settings will yield the same segmentation results.
Metropolis-Hasting steps. Interleaving sampling-
based inference with direct optimization of param-
eters can be considered a form of Monte Carlo
Expectation-Maximization (MCEM; Wei and Tan-
ner, 1990).
5 Experimental Setup
Corpora We evaluate our approach on corpora
from two different domains: transcribed meetings
and written text.
For multi-speaker meetings, we use the ICSI cor-
pus of meeting transcripts (Janin et al, 2003), which
is becoming a standard for speech segmentation
(e.g., Galley et al 2003; Purver et al 2006). This
dataset includes transcripts of 75 multi-party meet-
ings, of which 25 are annotated for segment bound-
aries.
For text, we introduce a dataset in which each
document is a chapter selected from a medical text-
book (Walker et al, 1990).7 The task is to divide
each chapter into the sections indicated by the au-
thor. This dataset contains 227 chapters, with 1136
sections (an average of 5.00 per chapter). Each
chapter contains an average of 140 sentences, giv-
ing an average of 28 sentences per segment.
Metrics All experiments are evaluated in terms
of the commonly-used Pk (Beeferman et al, 1999)
and WindowDiff (WD) (Pevzner and Hearst, 2002)
scores. Both metrics pass a window through the
document, and assess whether the sentences on the
edges of the window are properly segmented with
respect to each other. WindowDiff is stricter in
that it requires that the number of intervening seg-
ments between the two sentences be identical in
the hypothesized and the reference segmentations,
while Pk only asks whether the two sentences are in
the same segment or not. Pk and WindowDiff are
penalties, so lower values indicate better segmenta-
tions. We use the evaluation source code provided
by Malioutov and Barzilay (2006).
System configuration We evaluate our Bayesian
approach both with and without cue phrases. With-
out cue phrases, we use the dynamic programming
inference described in section 3.3. This system is
referred to as BAYESSEG in Table 1. When adding
7The full text of this book is available for free download at
http://onlinebooks.library.upenn.edu.
340
cue phrases, we use the Metropolis-Hastings model
described in 4.1. Both basic and linguistically-
motivated proposal distributions are evaluated (see
Section 4.2); these are referred to as BAYESSEG-
CUE and BAYESSEG-CUE-PROP in the table.
For the sampling-based systems, results are av-
eraged over five runs. The initial configuration is
obtained from the dynamic programming inference,
and then 100,000 sampling iterations are performed.
The final segmentation is obtained by annealing the
last 25,000 iterations to a temperature of zero. The
use of annealing to obtain a maximum a posteri-
ori (MAP) configuration from sampling-based in-
ference is common (e.g., Finkel 2005; Goldwater
2007). The total running time of our system is on the
order of three minutes per document. Due to mem-
ory constraints, we divide the textbook dataset into
ten parts, and perform inference in each part sepa-
rately. We may achieve better results by performing
inference over the entire dataset simultaneously, due
to pooling counts for cue phrases across all docu-
ments.
Baselines We compare against three com-
petitive alternative systems from the literature:
U&I (Utiyama and Isahara, 2001); LCSEG (Galley
et al, 2003); MCS (Malioutov and Barzilay, 2006).
All three systems are described in the related work
(Section 2). In all cases, we use the publicly avail-
able executables provided by the authors.
Parameter settings For LCSEG, we use the pa-
rameter values specified in the paper (Galley et al,
2003). MCS requires parameter settings to be tuned
on a development set. Our corpora do not include
development sets, so tuning was performed using the
lecture transcript corpus described by Malioutov and
Barzilay (2006). Our system does not require pa-
rameter tuning; priors are re-estimated as described
in Sections 3.4 and 4.3. U&I requires no parameter
tuning, and is used ?out of the box.? In all exper-
iments, we assume that the number of desired seg-
ments is provided.
Preprocessing Standard preprocessing techniques
are applied to the text for all comparisons. The
Porter (1980) stemming algorithm is applied to
group equivalent lexical items. A set of stop-words
is also removed, using the same list originally em-
ployed by several competitive systems (Choi, 2000;
Textbook Pk WD
U&I .370 .376
MCS .368 .382
LCSEG .370 .385
BAYESSEG .339 .353
BAYESSEG-CUE .339 .353
BAYESSEG-CUE-PROP .343 .355
Meetings Pk WD
U&I .297 .347
MCS .370 .411
LCSEG .309 .322
BAYESSEG .264 .319
BAYESSEG-CUE .261 .316
BAYESSEG-CUE-PROP .258 .312
Table 1: Comparison of segmentation algorithms. Both
metrics are penalties, so lower scores indicate bet-
ter performance. BAYESSEG is the cohesion-only
Bayesian system with marginalized language mod-
els. BAYESSEG-CUE is the Bayesian system with cue
phrases. BAYESSEG-CUE-PROP adds the linguistically-
motivated proposal distribution.
Utiyama and Isahara, 2001; Malioutov and Barzilay,
2006).
6 Results
Table 1 presents the performance results for three
instantiations of our Bayesian framework and three
competitive alternative systems. As shown in the ta-
ble, the Bayesian models achieve the best results on
both metrics for both corpora. On the medical text-
book corpus, the Bayesian systems achieve a raw
performance gain of 2-3% with respect to all base-
lines on both metrics. On the ICSI meeting corpus,
the Bayesian systems perform 4-5% better than the
best baseline on the Pk metric, and achieve smaller
improvement on the WindowDiff metric. The results
on the meeting corpus also compare favorably with
the topic-modeling method of Purver et al (2006),
who report a Pk of .289 and a WindowDiff of .329.
Another observation from Table 1 is that the con-
tribution of cue phrases depends on the dataset. Cue
phrases improve performance on the meeting cor-
pus, but not on the textbook corpus. The effective-
ness of cue phrases as a feature depends on whether
the writer or speaker uses them consistently. At the
341
Meetings Textbook
okay* 234.4 the 1345.9
I 212.6 this 14.3
so* 113.4 it 4.1
um 91.7 these 4.1
and* 67.3 a 2.9
yeah 10.5 on 2.1
but* 9.4 most 2.0
uh 4.8 heart 1.8
right 2.4 creating 1.8
agenda 1.3 hundred 1.8
Table 2: Cue phrases selected by our unsupervised
model, sorted by chi-squared. Boldface indicates that the
chi-squared value is significant at the level of p < .01.
Asterisks indicate cue phrases that were extracted by the
supervised procedure of Galley et al (2003).
same time, the addition of cue phrases prevents the
use of exact inference techniques, which may ex-
plain the decline in results for the meetings dataset.
To investigate the quality of the cue phrases that
our model extracts, we list its top ten cue phrases
for each dataset in Table 2. Cue phrases are ranked
by their chi-squared value, which is computed based
on the number of occurrences for each word at the
beginning of a hypothesized segment, as compared
to the expectation. For cue phrases listed in bold,
the chi-squared value is statistically significant at
the level of p < .01, indicating that the frequency
with which the cue phrase appears at the beginning
of segments is unlikely to be a chance phenomenon.
As shown in the left column of the table, our
model has identified several strong cue phrases from
the meeting dataset which appear to be linguistically
plausible. Galley et al (2003) performed a simi-
lar chi-squared analysis, but used the true segment
boundaries in the labeled data; this can be thought
of as a sort of ground truth. Four of the ten cue
phrases identified by our system overlap with their
analysis; these are indicated with asterisks. In con-
trast to our model?s success at extracting cue phrases
from the meeting dataset, only very common words
are selected for the textbook dataset. This may help
to explain why cue phrases improve performance for
meeting transcripts, but not for the textbook.
7 Conclusions
This paper presents a novel Bayesian approach to
unsupervised topic segmentation. Our algorithm is
capable of incorporating both lexical cohesion and
cue phrase features in a principled manner, and out-
performs state-of-the-art baselines on text and tran-
scribed speech corpora. We have developed exact
and sampling-based inference techniques, both of
which search only over the space of segmentations
and marginalize out the associated language mod-
els. Finally, we have shown that our model provides
a theoretical framework with connections to infor-
mation theory, while also generalizing and justify-
ing prior work. In the future, we hope to explore the
use of similar Bayesian techniques for hierarchical
segmentation, and to incorporate additional features
such as prosody and speaker change information.
Acknowledgments
The authors acknowledge the support of the Na-
tional Science Foundation (CAREER grant IIS-
0448168) and the Microsoft Research Faculty Fel-
lowship. Thanks to Aaron Adler, S. R. K. Branavan,
Harr Chen, Michael Collins, Randall Davis, Dan
Roy, David Sontag and the anonymous reviewers for
helpful comments and suggestions. We also thank
Michel Galley, Igor Malioutov, and Masao Utiyama
for making their topic segmentation code publically
available. Any opinions, findings, and conclusions
or recommendations expressed above are those of
the authors and do not necessarily reflect the views
of the NSF.
References
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Jose? M. Bernardo and Adrian F. M. Smith. 2000.
Bayesian Theory. Wiley.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
NAACL, pages 26?33.
342
Micha Elsner and Eugene Charniak. 2008. You Talk-
ing to Me? A Corpus and Algorithm for Conversation
Disentanglement. In Proceedings of ACL.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL, pages 363?370.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. Proceedings of
ACL, pages 562?569.
Jean-Luc Gauvain and Chin-Hui Lee. 1994. Maximum
a posteriori estimation for multivariate Gaussian mix-
ture observations of Markov chains. IEEE Transac-
tions on Speech and Audio Processing, 2(2):291?298.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In Proceedings of ACL, pages
199?206.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL, pages 744?751.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Barbara Grosz. 1977. The representation and use of fo-
cus in dialogue understanding. Technical Report 151,
Artificial Intelligence Center, SRI International.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9?16.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics, 19(3):501?530.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, et al 2003. The ICSI Meeting Corpus. Acous-
tics, Speech, and Signal Processing, 2003. Proceed-
ings.(ICASSP?03). 2003 IEEE International Confer-
ence on, 1.
Norman L. Johnson, Samuel Kotz, and N. Balakrishnan.
1997. Discrete Multivariate Distributions. Wiley.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
Diane J. Litman and Rebecca J. Passonneau. 1995. Com-
bining multiple knowledge sources for discourse seg-
mentation. In Proceedings of the ACL, pages 108?115.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25?32.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press.
Thomas P. Minka. 2003. Estimating a dirichlet distri-
bution. Technical report, Massachusetts Institute of
Technology.
Rebecca Passonneau and Diane Litman. 1993. Intention-
based segmentation: Human reliability and correlation
with linguistic cues. In Proceedings of ACL, pages
148?155.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14:130?137.
M. Purver, T.L. Griffiths, K.P. Ko?rding, and J.B. Tenen-
baum. 2006. Unsupervised topic modelling for multi-
party spoken discourse. In Proceedings of ACL, pages
17?24.
Caroline Sporleder and Mirella Lapata. 2006. Broad
coverage paragraph segmentation across languages
and domains. ACM Transactions on Speech and Lan-
guage Processing, 3(2):1?35.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL, pages 491?498.
H. Kenneth Walker, W. Dallas Hall, and J. Willis Hurst,
editors. 1990. Clinical Methods : The History, Physi-
cal, and Laboratory Examinations. Butterworths.
Greg C. G. Wei and Martin A. Tanner. 1990. A
monte carlo implementation of the EM algorithm and
the poor man?s data augmentation algorithms. Jour-
nal of the American Statistical Association, 85(411),
September.
343
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1041?1050,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Unsupervised Multilingual Learning for POS Tagging
Benjamin Snyder and Tahira Naseem and Jacob Eisenstein and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge MA 02139
{bsnyder, tahira, jacobe, regina}@csail.mit.edu
Abstract
We demonstrate the effectiveness of multilin-
gual learning for unsupervised part-of-speech
tagging. The key hypothesis of multilin-
gual learning is that by combining cues from
multiple languages, the structure of each be-
comes more apparent. We formulate a hier-
archical Bayesian model for jointly predicting
bilingual streams of part-of-speech tags. The
model learns language-specific features while
capturing cross-lingual patterns in tag distri-
bution for aligned words. Once the parame-
ters of our model have been learned on bilin-
gual parallel data, we evaluate its performance
on a held-out monolingual test set. Our evalu-
ation on six pairs of languages shows consis-
tent and significant performance gains over a
state-of-the-art monolingual baseline. For one
language pair, we observe a relative reduction
in error of 53%.
1 Introduction
In this paper, we explore the application of multilin-
gual learning to part-of-speech tagging when no an-
notation is available. This core task has been studied
in an unsupervised monolingual framework for over
a decade and is still an active area of research. In this
paper, we demonstrate the effectiveness of multilin-
gual learning when applied to both closely related
and distantly related language pairs. We further ana-
lyze the language features which lead to robust bilin-
gual performance.
The fundamental idea upon which our work is
based is that the patterns of ambiguity inherent in
part-of-speech tag assignments differ across lan-
guages. At the lexical level, a word with part-of-
speech tag ambiguity in one language may corre-
spond to an unambiguous word in the other lan-
guage. For example, the word ?can? in English may
function as an auxiliary verb, a noun, or a regular
verb. However, each of the corresponding functions
in Serbian is expressed with a distinct lexical item.
Languages also differ in their patterns of structural
ambiguity. For example, the presence of an article
in English greatly reduces the ambiguity of the suc-
ceeding tag. In Serbian, a language without articles,
this constraint is obviously absent. The key idea of
multilingual learning is that by combining cues from
multiple languages, the structure of each becomes
more apparent.
While multilingual learning can address ambigu-
ities in each language, it must be flexible enough
to accommodate cross-lingual variations such as tag
inventory and syntactic structure. As a result of
such variations, two languages often select and order
their tags differently even when expressing the same
meaning. A key challenge of multilingual learning
is to model language-specific structure while allow-
ing information to flow between languages.
We jointly model bilingual part-of-speech tag se-
quences in a hierarchical Bayesian framework. For
each word, we posit a hidden tag state which gen-
erates the word as well as the succeeding tag. In
addition, the tags of words with common seman-
tic or syntactic function in parallel sentences are
combined into bilingual nodes representing the tag
pair. These joined nodes serve as anchors that cre-
ate probabilistic dependencies between the tag se-
1041
quences in each language. We use standard tools
from machine translation to discover aligned word-
pairs, and thereafter our model treats the alignments
as observed data.
Our model structure allows language-specific tag
inventories. Additionally, it assumes only that the
tags at joined nodes are correlated; they need not be
identical. We factor the conditional probabilities of
joined nodes into two individual transition probabil-
ities as well as a coupling probability. We define
priors over the transition, emission, and coupling
parameters and perform Bayesian inference using
Gibbs sampling and the Metropolis-Hastings algo-
rithm.
We evaluate our model on a parallel corpus of
four languages: English, Bulgarian, Serbian, and
Slovene. For each of the six language pairs, we
train a bilingual model on this corpus, and evaluate it
on held-out monolingual test sets. Our results show
consistent improvement over a monolingual baseline
for all languages and all pairings. In fact, for one
language pair ? Serbian and Slovene ? the error is
reduced by over 53%. Moreover, the multilingual
model significantly reduces the gap between unsu-
pervised and supervised performance. For instance,
in the case of Slovene this gap is reduced by 71%.
We also observe significant variation in the level of
improvement across language pairs. We show that a
cross-lingual entropy measure corresponds with the
observed differentials in performance.
2 Related Work
Multilingual Learning A number of approaches
for multilingual learning have focused on induc-
ing cross-lingual structures, with applications to
machine translation. Examples of such efforts
include work on the induction of synchronous
grammars (Wu and Wong, 1998; Chiang, 2005)
and learning multilingual lexical resources (Genzel,
2005).
Another thread of work using cross-lingual links
has been in word-sense disambiguation, where
senses of words can be defined based on their trans-
lations (Brown et al, 1991; Dagan et al, 1991;
Resnik and Yarowsky, 1997; Ng et al, 2003).
When annotations for a task of interest are avail-
able in a source language but are missing in the
target language, the annotations can be projected
across a parallel corpus (Yarowsky et al, 2000;
Diab and Resnik, 2002; Pado? and Lapata, 2006; Xi
and Hwa, 2005). In fact, projection methods have
been used to train highly accurate part-of-speech
taggers (Yarowsky and Ngai, 2001; Feldman et al,
2006). In contrast, our own work assumes that an-
notations exist for neither language.
Finally, there has been recent work on applying
unsupervised multilingual learning to morphologi-
cal segmentation (Snyder and Barzilay, 2008). In
this paper, we demonstrate that unsupervised mul-
tilingual learning can be successfully applied to the
sentence-level task of part-of-speech tagging.
Unsupervised Part-of-Speech Tagging Since
the work of Merialdo (1994), the HMM has been the
model of choice for unsupervised tagging (Banko
and Moore, 2004). Recent advances in these
approaches include the use of a fully Bayesian
HMM (Johnson, 2007; Goldwater and Griffiths,
2007). In very recent work, Toutanova and John-
son (2008) depart from this framework and propose
an LDA-based generative model that groups words
through a latent layer of ambiguity classes thereby
leveraging morphological features. In addition, a
number of approaches have focused on develop-
ing discriminative approaches for unsupervised and
semi-supervised tagging (Smith and Eisner, 2005;
Haghighi and Klein, 2006).
Our focus is on developing a simple model that
effectively incorporates multilingual evidence. We
view this direction as orthogonal to refining mono-
lingual tagging models for any particular language.
3 Model
We propose a bilingual model for unsupervised part-
of-speech tagging that jointly tags parallel streams
of text in two languages. Once the parameters have
been learned using an untagged bilingual parallel
text, the model is applied to a held-out monolingual
test set.
Our key hypothesis is that the patterns of ambigu-
ity found in each language at the part-of-speech level
will differ in systematic ways; by considering multi-
ple language simultaneously, the total inherent am-
biguity can be reduced in each language. The model
is designed to permit information to flow across the
1042
I love fish
J' adore les poissons
x1
y1
x2
y2 y3 y4
x3
I love fish
J' adore les poissons
x1/y1 x1/y1 x1/y1
y3
(a) (b)
Figure 1: (a) Graphical structure of two standard monolingual HMM?s. (b) Graphical structure of our bilingual model
based on word alignments.
language barrier, while respecting language-specific
idiosyncrasies such as tag inventory, selection, and
order. We assume that for pairs of words that share
similar semantic or syntactic function, the associ-
ated tags will be statistically correlated, though not
necessarily identical. We use such word pairs as
the bilingual anchors of our model, allowing cross-
lingual information to be shared via joint tagging de-
cisions. We use standard tools from machine trans-
lation to identify these aligned words, and thereafter
our model treats them as fixed and observed data.
To avoid cycles, we remove crossing edges from the
alignments.
For unaligned parts of the sentence, the tag and
word selections are identical to standard monolin-
gual HMM?s. Figure 1 shows an example of the
bilingual graphical structure we use, in comparison
to two independent monolingual HMM?s.
We formulate a hierarchical Bayesian model that
exploits both language-specific and cross-lingual
patterns to explain the observed bilingual sentences.
We present a generative story in which the observed
words are produced by the hidden tags and model
parameters. In Section 4, we describe how to in-
fer the posterior distribution over these hidden vari-
ables, given the observations.
3.1 Generative Model
Our generative model assumes the existence of two
tagsets, T and T ?, and two vocabularies W and W ?,
one of each for each language. For ease of exposi-
tion, we formulate our model with bigram tag de-
pendencies. However, in our experiments we used
a trigram model, which is a trivial extension of the
model discussed here and in the next section.
1. For each tag t ? T , draw a transition distri-
bution ?t over tags T , and an emission distri-
bution ?t over words W , both from symmetric
Dirichlet priors.1
2. For each tag t ? T ?, draw a transition distri-
bution ??t over tags T ?, and an emission distri-
bution ??t over words W ?, both from symmetric
Dirichlet priors.
3. Draw a bilingual coupling distribution ? over
tag pairs T ? T ? from a symmetric Dirichlet
prior.
4. For each bilingual parallel sentence:
(a) Draw an alignment a from an alignment
distribution A (see the following para-
graph for formal definitions of a and A),
(b) Draw a bilingual sequence of part-of-
speech tags (x1, ..., xm), (y1, ..., yn) ac-
cording to:
P (x1, ..., xm, y1, ..., yn|a, ?, ??, ?). 2
This joint distribution is given in equa-
tion 1.
1The Dirichlet is a probability distribution over the simplex,
and is conjugate to the multinomial (Gelman et al, 2004).
2Note that we use a special end state rather than explicitly
modeling sentence length. Thus the values of m and n depend
on the draw.
1043
(c) For each part-of-speech tag xi in the first
language, emit a word from W : ei ? ?xi ,
(d) For each part-of-speech tag yj in the sec-
ond language, emit a word from W ?: fj ?
??yj .
We define an alignment a to be a set of one-to-
one integer pairs with no crossing edges. Intuitively,
each pair (i, j) ? a indicates that the words ei and
fj share some common role in the bilingual paral-
lel sentences. In our experiments, we assume that
alignments are directly observed and we hold them
fixed. From the perspective of our generative model,
we treat alignments as drawn from a distribution A,
about which we remain largely agnostic. We only
require that A assign zero probability to alignments
which either: (i) align a single index in one language
to multiple indices in the other language or (ii) con-
tain crossing edges. The resulting alignments are
thus one-to-one, contain no crossing edges, and may
be sparse or even possibly empty. Our technique for
obtaining alignments that display these properties is
described in Section 5.
Given an alignment a and sets of transition param-
eters ? and ??, we factor the conditional probability
of a bilingual tag sequence (x1, ...xm), (y1, ..., yn)
into transition probabilities for unaligned tags, and
joint probabilities over aligned tag pairs:
P (x1, ..., xm, y1, ..., yn|a, ?, ??, ?) =
?
unaligned i
?xi?1(xi) ?
?
unaligned j
??yj?1(yj) ?
?
(i,j)?a
P (xi, yj |xi?1, yj?1, ?, ??, ?)
(1)
Because the alignment contains no crossing
edges, we can model the tags as generated sequen-
tially by a stochastic process. We define the dis-
tribution over aligned tag pairs to be a product of
each language?s transition probability and the cou-
pling probability:
P (xi, yj |xi?1, yj?1, ?, ??, ?) =
?xi?1(xi) ??yj?1(yj) ?(xi, yj)
Z (2)
The normalization constant here is defined as:
Z =
?
x,y
?xi?1(x) ??yj?1(y) ?(x, y)
This factorization allows the language-specific tran-
sition probabilities to be shared across aligned and
unaligned tags. In the latter case, the addition of
the coupling parameter ? gives the tag pair an addi-
tional role: that of multilingual anchor. In essence,
the probability of the aligned tag pair is a product
of three experts: the two transition parameters and
the coupling parameter. Thus, the combination of
a high probability transition in one language and a
high probability coupling can resolve cases of inher-
ent transition uncertainty in the other language. In
addition, any one of the three parameters can ?veto?
a tag pair to which it assigns low probability.
To perform inference in this model, we predict
the bilingual tag sequences with maximal probabil-
ity given the observed words and alignments, while
integrating over the transition, emission, and cou-
pling parameters. To do so, we use a combination of
sampling-based techniques.
4 Inference
The core element of our inference procedure is
Gibbs sampling (Geman and Geman, 1984). Gibbs
sampling begins by randomly initializing all unob-
served random variables; at each iteration, each ran-
dom variable zi is sampled from the conditional dis-
tribution P (zi|z?i), where z?i refers to all variables
other than zi. Eventually, the distribution over sam-
ples drawn from this process will converge to the
unconditional joint distribution P (z) of the unob-
served variables. When possible, we avoid explic-
itly sampling variables which are not of direct inter-
est, but rather integrate over them?this technique
is known as ?collapsed sampling,? and can reduce
variance (Liu, 1994).
We sample: (i) the bilingual tag sequences (x,y),
(ii) the two sets of transition parameters ? and ??,
and (iii) the coupling parameter ?. We integrate over
the emission parameters ? and ??, whose priors are
Dirichlet distributions with hyperparameters ?0 and
??0. The resulting emission distribution over words
ei, given the other words e?i, the tag sequences x
1044
and the emission prior ?0, can easily be derived as:
P (ei|x, e?i, ?0) =
?
?xi
?xi(ei)P (?xi |?0) d?xi
= n(xi, ei) + ?0n(xi) + Wxi?0
(3)
Here, n(xi) is the number of occurrences of the
tag xi in x?i, n(xi, ei) is the number of occurrences
of the tag-word pair (xi, ei) in (x?i, e?i), and Wxi
is the number of word types in the vocabulary W
that can take tag xi. The integral is tractable due
to Dirichlet-multinomial conjugacy (Gelman et al,
2004).
We will now discuss, in turn, each of the variables
that we sample. Note that in all cases we condi-
tion on the other sampled variables as well as the
observed words and alignments, e, f and a, which
are kept fixed throughout.
4.1 Sampling Part-of-speech Tags
This section presents the conditional distributions
that we sample from to obtain the part-of-speech
tags. Depending on the alignment, there are several
scenarios. In the simplest case, both the tag to be
sampled and its succeeding tag are not aligned to
any tag in the other language. If so, the sampling
distribution is identical to the monolingual case, in-
cluding only terms for the emission (defined in equa-
tion 3), and the preceding and succeeding transi-
tions:
P (xi|x?i, y, e, f, a, ?, ??, ?, ?0, ??0) ?
P (ei|x, e?i, ?0) ?xi?1(xi) ?xi(xi+1).
For an aligned tag pair (xi, yj), we sample the
identity of the tags jointly. By applying the chain
rule we obtain terms for the emissions in both lan-
guages and a joint term for the transition probabili-
ties:
P (xi, yj |x?i, y?j , e, f, a, ?, ??, ?, ?0, ??0) ?
P (ei|x, e?i, ?0)P (fj |y, f?j , ??0)
P (xi, yj |x?i, y?j , a, ?, ??, ?)
The expansion of the joint term depends on the
alignment of the succeeding tags. In the case that
the successors are not aligned, we have a product of
the bilingual coupling probability and four transition
probabilities (preceding and succeeding transitions
in each language):
P (xi, yj |x?i, y?j , a, ?, ??, ?) ?
?(xi, yj)?xi?1(xi) ??yj?1(yj) ?xi(xi+1) ?
?
yj (yj+1)
Whenever one or more of the succeeding tags is
aligned, the sampling formulas must account for the
effect of the sampled tag on the joint probability
of the succeeding tags, which is no longer a sim-
ple multinomial transition probability. We give the
formula for one such case?when we are sampling
an aligned tag pair (xi, yj), whose succeeding tags
(xi+1, yj+1) are also aligned to one another:
P (xi, yj |x?i, y?j , a, ?, ??, ?) ? ?(xi, yj)
? ?xi?1(xi)??yj?1(yj)
[
?xi(xi+1)??yj (yj+1)
?
x,y ?xi(x)??yj (y)?(x, y)
]
Similar equations can be derived for cases where
the succeeding tags are not aligned to each other, but
to other tags.
4.2 Sampling Transition Parameters and the
Coupling Parameter
When computing the joint probability of an aligned
tag pair (Equation 2), we employ the transition pa-
rameters ?, ?? and the coupling parameter ? in a nor-
malized product. Because of this, we can no longer
regard these parameters as simple multinomials, and
thus can no longer sample them using the standard
closed formulas.
Instead, to resample these parameters, we re-
sort to the Metropolis-Hastings algorithm as a sub-
routine within Gibbs sampling (Hastings, 1970).
Metropolis-Hastings is a Markov chain sampling
technique that can be used when it is impossible to
directly sample from the posterior. Instead, sam-
ples are drawn from a proposal distribution and then
stochastically accepted or rejected on the basis of:
their likelihood, their probability under the proposal
distribution, and the likelihood and proposal proba-
bility of the previous sample.
We use a form of Metropolis-Hastings known as
an independent sampler. In this setup, the proposal
distribution does not depend on the value of the
previous sample, although the accept/reject decision
1045
does depend on the previous model likelihood. More
formally, if we denote the proposal distribution as
Q(z), the target distribution as P (z), and the previ-
ous sample as z, then the probability of accepting a
new sample z? ? Q is set at:
min
{
1, P (z
?) Q(z)
P (z) Q(z?)
}
Theoretically any non-degenerate proposal distri-
bution may be used. However, a higher acceptance
rate and faster convergence is achieved when the
proposal Q is a close approximation of P . For a par-
ticular transition parameter ?x, we define our pro-
posal distribution Q to be Dirichlet with parameters
set to the bigram counts of the tags following x in
the sampled tag data. Thus, the proposal distribu-
tion for ?x has a mean proportional to these counts,
and is thus likely to be a good approximation to the
target distribution.
Likewise for the coupling parameter ?, we de-
fine a Dirichlet proposal distribution. This Dirichlet
is parameterized by the counts of aligned tag pairs
(x, y) in the current set of tag samples. Since this
sets the mean of the proposal to be proportional to
these counts, this too is likely to be a good approxi-
mation to the target distribution.
4.3 Hyperparameter Re-estimation
After every iteration of Gibbs sampling the hyper-
parameters ?0 and ??0 are re-estimated using a single
Metropolis-Hastings move. The proposal distribu-
tion is set to a Gaussian with mean at the current
value and variance equal to one tenth of the mean.
5 Experimental Set-Up
Our evaluation framework follows the standard pro-
cedures established for unsupervised part-of-speech
tagging. Given a tag dictionary (i.e., a set of possi-
ble tags for each word type), the model has to select
the appropriate tag for each token occurring in a text.
We also evaluate tagger performance when only in-
complete dictionaries are available (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007). In both
scenarios, the model is trained only using untagged
text.
In this section, we first describe the parallel data
and part-of-speech annotations used for system eval-
uation. Next we describe a monolingual base-
line and our procedures for initialization and hyper-
parameter setting.
Data As a source of parallel data, we use Orwell?s
novel ?Nineteen Eighty Four? in the original English
as well as translations to three Slavic languages ?
Bulgarian, Serbian and Slovene. This data is dis-
tributed as part of the Multext-East corpus which
is publicly available. The corpus provides detailed
morphological annotation at the world level, includ-
ing part-of-speech tags. In addition a lexicon for
each language is provided.
We obtain six parallel corpora by considering
all pairings of the four languages. We compute
word level alignments for each language pair using
Giza++. To generate one-to-one alignments at the
word level, we intersect the one-to-many alignments
going in each direction and automatically remove
crossing edges in the order in which they appear left
to right. This process results in alignment of about
half the tokens in each bilingual parallel corpus. We
treat the alignments as fixed and observed variables
throughout the training procedure.
The corpus consists of 94,725 English words (see
Table 2). For every language, a random three quar-
ters of the data are used for learning the model while
the remaining quarter is used for testing. In the test
set, only monolingual information is made available
to the model, in order to simulate future performance
on non-parallel data.
Tokens Tags/Token
SR 89,051 1.41
SL 91,724 1.40
BG 80,757 1.34
EN 94,725 2.58
Table 2: Corpus statistics: SR=Serbian, SL=Slovene,
EN=English, BG=Bulgarian
Tagset The Multext-East corpus is manually an-
notated with detailed morphosyntactic information.
In our experiments, we focus on the main syntac-
tic category encoded as a first letter of the labels.
The annotation distinguishes between 13 parts-of-
speech, of which 11 are common for all languages
1046
Random Monolingual Unsupervised Monolingual Supervised Trigram Entropy
EN 56.24 90.71 96.97 1.558
BG 82.68 88.88 96.96 1.708
SL 84.70 87.41 97.31 1.703
SR 83.41 85.05 96.72 1.789
Table 1: Monolingual tagging accuracy for English, Bulgarian, Slovene, and Serbian for two unsupervised baselines
(random tag selection and a Bayesian HMM (Goldwater and Griffiths, 2007)) as well as a supervised HMM. In
addition, the trigram part-of-speech tag entropy is given for each language.
in our experiments.3
In the Multext-East corpus, punctuation marks are
not annotated. We expand the tag repository by
defining a separate tag for all punctuation marks.
This allows the model to make use of any transition
or coupling patterns involving punctuation marks.
We do not consider punctuation tokens when com-
puting model accuracy.
Table 2 shows the tag/token ratio for these lan-
guages. For Slavic languages, we use the tag dic-
tionaries provided with the corpus. For English,
we use a different process for dictionary construc-
tion. Using the original dictionary would result in
the tag/token ratio of 1.5, in comparison to the ra-
tio of 2.3 observed in the Wall Street Journal (WSJ)
corpus. To make our results on English tagging more
comparable to previous benchmarks, we expand the
original dictionary of English tags by merging it
with the tags from the WSJ dictionary. This process
results in a tag/token ratio of 2.58, yielding a slightly
more ambiguous dictionary than the one used in pre-
vious tagging work. 4
Monolingual Baseline As our monolingual base-
line we use the unsupervised Bayesian HMM model
of Goldwater and Griffiths (2007) (BHMM1). This
model modifies the standard HMM by adding pri-
ors and by performing Bayesian inference. Its is in
line with state-of-the-art unsupervised models. This
model is a particulary informative baseline, since
our model reduces to this baseline model when there
are no alignments in the data. This implies that any
performance gain over the baseline can only be at-
3The remaining two tags are Particle and Determiner; The
English tagset does not include Particle while the other three
languages Serbian, Slovene and Bulgarian do not have Deter-
miner in their tagset.
4We couldn?t perform the same dictionary expansion for the
Slavic languages due to a lack of additional annotated resources.
tributed to the multilingual aspect of our model. We
used our own implementation after verifying that its
performance on WSJ was identical to that reported
in (Goldwater and Griffiths, 2007).
Supervised Performance In order to provide a
point of comparison, we also provide supervised re-
sults when an annotated corpus is provided. We use
the standard supervised HMM with Viterbi decod-
ing.
Training and Testing Framework Initially, all
words are assigned tags randomly from their tag
dictionaries. During each iteration of the sam-
pler, aligned tag pairs and unaligned tags are sam-
pled from their respective distributions given in Sec-
tion 4.1 above. The hyperparameters ?0 and ??0 are
initialized with the values learned during monolin-
gual training. They are re-estimated after every iter-
ation of the sampler using the Metropolis Hastings
algorithm. The parameters ? and ?? are initially
set to trigram counts and the ? parameter is set to
tag pair counts of aligned pairs. After every 40 it-
erations of the sampler, a Metropolis Hastings sub-
routine is invoked that re-estimates these parameters
based on the current counts. Overall, the algorithm
is run for 1000 iterations of tag sampling, by which
time the resulting log-likelihood converges to stable
values. Each Metropolis Hastings subroutine sam-
ples 20 values, with an acceptance ratio of around
1/6, in line with the standard recommended values.
After training, trigram and word emission prob-
abilities are computed based on the counts of tags
assigned in the final iteration. For smoothing, the
final sampled values of the hyperparameters are
used. The highest probability tag sequences for each
monolingual test set are then predicted using trigram
Viterbi decoding. We report results averaged over
five complete runs of all experiments.
1047
6 Results
Complete Tag Dictionary In our first experiment,
we assume that a complete dictionary listing the pos-
sible tags for every word is provided in each lan-
guage. Table 1 shows the monolingual results of a
random baseline, an unsupervised Bayesian HMM
and a supervised HMM. Table 3 show the results
of our bilingual models for different language pair-
ings while repeating the monolingual unsupervised
results from Table 1 for easy comparison. The final
column indicates the absolute gain in performance
over this monolingual baseline.
Across all language pairs, the bilingual model
consistently outperforms the monolingual baseline.
All the improvements are statistically significant by
a Fisher sign test at p < 0.05. For some lan-
guage pairs, the gains are quite high. For instance,
the pairing of Serbian and Slovene (two closely re-
lated languages) yields absolute improvements of
6.7 and 7.7 percentage points, corresponding to rel-
ative reductions in error of 51.4% and 53.2%. Pair-
ing Bulgarian and English (two distantly related lan-
guages) also yields large gains: 5.6 and 1.3 percent-
age points, corresponding to relative reductions in
error of 50% and 14%, respectively.5
When we compare the best bilingual result for
each language (Table 3, in bold) to the monolin-
gual supervised results (Table 1), we find that for
all languages the gap between supervised and un-
supervised learning is reduced significantly. For En-
glish, this gap is reduced by 21%. For the Slavic lan-
guages, the supervised-unsupervised gap is reduced
by even larger amounts: 57%, 69%, and 78% for
Serbian, Bulgarian, and Slovene respectively.
While all the languages benefit from the bilin-
gual learning framework, some language combina-
tions are more effective than others. Slovene, for in-
stance, achieves a large improvement when paired
with Serbian (+7.7), a closely related Slavic lan-
guage, but only a minor improvement when coupled
5The accuracy of the monolingual English tagger is rela-
tively high compared to the 87% reported by (Goldwater and
Griffiths, 2007) on the WSJ corpus. We attribute this discrep-
ancy to the slight differences in tag inventory used in our data-
set. For example, when Particles and Prepositions are merged
in the WSJ corpus (as they happen to be in our tag inventory
and corpus), the performance of Goldwater?s model on WSJ is
similar to what we report here.
Entropy Mono- Bilingual Absolute
lingual Gain
EN 0.566 90.71 91.01 +0.30
SR 0.554 85.05 90.06 +5.03
EN 0.578 90.71 92.00 +1.29
BG 0.543 88.88 94.48 +5.61
EN 0.571 90.71 92.01 +1.30
SL 0.568 87.41 88.54 +1.13
SL 0.494 87.41 95.10 +7.69
SR 0.478 85.05 91.75 +6.70
BG 0.568 88.88 91.95 +3.08
SR 0.588 85.05 86.58 +1.53
BG 0.579 88.88 90.91 +2.04
SL 0.609 87.41 88.20 +0.79
Table 3: The tagging accuracy of our bilingual models
on different language pairs, when a full tag dictionary is
provided. The Monolingual Unsupervised results from
Table 1 are repeated for easy comparison. The first col-
umn shows the cross-lingual entropy of a tag when the
tag of the aligned word in the other language is known.
The final column shows the absolute improvement over
the monolingual Bayesian HMM. The best result for each
language is shown in boldface.
with English (+1.3). On the other hand, for Bulgar-
ian, the best performance is achieved when coupling
with English (+5.6) rather than with closely related
Slavic languages (+3.1 and +2.4). As these results
show, an optimal pairing cannot be predicted based
solely on the family connection of paired languages.
To gain a better understanding of this variation
in performance, we measured the internal tag en-
tropy of each language as well as the cross-lingual
tag entropy of language pairs. For the first measure,
we computed the conditional entropy of a tag de-
cision given the previous two tags. Intuitively, this
should correspond to the inherent structural uncer-
tainty of part-of-speech decisions in a language. In
fact, as Table 1 shows, the trigram entropy is a good
indicator of the relative performance of the mono-
lingual baseline. To measure the cross-lingual tag
entropies of language pairs, we considered all bilin-
gual aligned tag pairs, and computed the conditional
entropy of the tags in one language given the tags
in the other language. This measure should indi-
cate the amount of information that one language in
a pair can provide the other. The results of this anal-
1048
Mono- Bilingual Absolute
lingual Gain
EN 63.57 68.22 +4.66
SR 41.14 54.73 +13.59
EN 63.57 71.34 +7.78
BG 53.19 62.55 +9.37
EN 63.57 66.48 +2.91
SL 49.90 53.77 +3.88
SL 49.90 59.68 +9.78
SR 41.14 54.08 +12.94
BG 53.19 54.22 +1.04
SR 41.14 56.91 +15.77
BG 53.19 55.88 +2.70
SL 49.90 58.50 +8.60
Table 4: Tagging accuracy for Bilingual models with re-
duced dictionary: Lexicon entries are available for only
the 100 most frequent words, while all other words be-
come fully ambiguous. The improvement over the mono-
lingual Bayesian HMM trained under similar circum-
stances is shown. The best result for each language is
shown in boldface.
ysis are given in the first column of Table 3. We ob-
serve that the cross-lingual entropy is lowest for the
Serbian and Slovene pair, corresponding with their
large gain in performance. Bulgarian, on the other
hand, has lowest cross-lingual entropy when paired
with English. This corresponds with the fact that
English provides Bulgarian with its largest perfor-
mance gain. In general, we find that the largest per-
formance gain for any language is achieved when
minimizing its cross-lingual entropy.
Reduced Tag Dictionary We also conducted ex-
periments to investigate the impact of the dictio-
nary size on the performance of the bilingual model.
Here, we provide results for the realistic scenario
where only a very small dictionary is present. Ta-
ble 4 shows the performance when a tag dictionary
for the 100 most frequent words is present in each
language. The bilingual model?s results are consis-
tently and significantly better than the monolingual
baseline for all language pairs.
7 Conclusion
We have demonstrated the effectiveness of multilin-
gual learning for unsupervised part-of-speech tag-
ging. The key hypothesis of multilingual learn-
ing is that by combining cues from multiple lan-
guages, the structure of each becomes more appar-
ent. We formulated a hierarchical Bayesian model
for jointly predicting bilingual streams of tags. The
model learns language-specific features while cap-
turing cross-lingual patterns in tag distribution. Our
evaluation shows significant performance gains over
a state-of-the-art monolingual baseline.
Acknowledgments
The authors acknowledge the support of the National
Science Foundation (CAREER grant IIS-0448168 and
grant IIS-0835445) and the Microsoft Research Faculty
Fellowship. Thanks to Michael Collins, Amir Glober-
son, Lillian Lee, Yoong Keok Lee, Maria Polinsky and
the anonymous reviewers for helpful comments and sug-
gestions. Any opinions, findings, and conclusions or rec-
ommendations expressed above are those of the authors
and do not necessarily reflect the views of the NSF.
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of the COL-
ING, pages 556?561.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-sense dis-
ambiguation using statistical methods. In Proceedings
of the ACL, pages 264?270.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the ACL, pages 263?270.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the ACL, pages 130?137.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the ACL, pages 255?262.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of new
morpho-syntactically annotated resources. In Pro-
ceedings of LREC, pages 549?554.
Andrew Gelman, John B. Carlin, Hal .S. Stern, and Don-
ald .B. Rubin. 2004. Bayesian data analysis. Chap-
man and Hall/CRC.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
1049
Dmitriy Genzel. 2005. Inducing a multilingual dictio-
nary from a parallel multitext in related languages. In
Proceedings of HLT/EMNLP, pages 875?882.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. Proceedings of HLT-
NAACL, pages 320?327.
W. K. Hastings. 1970. Monte carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57:97?109.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP/CoNLL,
pages 296?305.
Jun S. Liu. 1994. The collapsed Gibbs sampler in
Bayesian computations with applications to a gene
regulation problem. Journal of the American Statis-
tical Association, 89(427):958?966.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the ACL, pages
455?462.
Sebastian Pado? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL, pages 1161 ? 1168.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 79?86.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the ACL, pages 354?362.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the ACL/HLT, pages 737?
745.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian lda-based model for semi-supervised part-
of-speech tagging. In Advances in Neural Information
Processing Systems 20, pages 1521?1528. MIT Press.
Dekai Wu and Hongsing Wong. 1998. Machine trans-
lation with a stochastic grammatical channel. In Pro-
ceedings of the ACL/COLING, pages 1408?1415.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851 ? 858.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1?8.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT, pages 161?168.
1050
A Salience-Based Approach to Gesture-Speech Alignment
Jacob Eisenstein and C. Mario Christoudias
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street
Cambridge, MA 02139
{jacobe+cmch}@csail.mit.edu
Abstract
One of the first steps towards understanding
natural multimodal language is aligning ges-
ture and speech, so that the appropriate ges-
tures ground referential pronouns in the speech.
This paper presents a novel technique for
gesture-speech alignment, inspired by salience-
based approaches to anaphoric pronoun reso-
lution. We use a hybrid between data-driven
and knowledge-based mtehods: the basic struc-
ture is derived from a set of rules about gesture
salience, but the salience weights themselves
are learned from a corpus. Our system achieves
95% recall and precision on a corpus of tran-
scriptions of unconstrained multimodal mono-
logues, significantly outperforming a competi-
tive baseline.
1 Introduction
In face to face communication, speakers frequently use
gesture to supplement speech (Chovil, 1992), using the
additional modality to provide unique, non-redundant in-
formation (McNeill, 1992). In the context of pen/speech
user interfaces, Oviatt finds that ?multimodal ... language
is briefer, syntactically simpler, and less disfluent than
users? unimodal speech.? (Oviatt, 1999)
One of the simplest and most direct ways in which ges-
ture can supplement verbal communication is by ground-
ing references, usually through deixis. For example, it is
impossible to extract the semantic content of the verbal
utterance ?I?ll take this one? without an accompanying
pointing gesture indicating the thing that is desired. The
problem of gesture-speech alignment involves choosing
the appropriate gesture to ground each verbal utterance.
This paper describes a novel technique for this problem.
We evaluate our system on a corpus of multimodal mono-
logues with no fixed grammar or vocabulary.
1.1 Example
[This]_1 thing goes over [here]_2 so
that it goes back ...
--------------------
1. Deictic: Hand rests on latch mechanism
2. Iconic: Hand draws trajectory from
right to left
In this example, there are three verbal references. The
word ?this? refers to the latch mechanism, which is indi-
cated by the rest position of the hand. ?Here? refers to the
endpoint of the trajectory indicated by the iconic gesture.
?It? is an anaphoric reference to a noun phrase defined
earlier in the sentence; there is no accompanying gesture.
The word ?that? does not act as a reference, although it
could in other cases. Not every pronoun keyword (e.g.,
this, here, it, that, etc.) will act as a reference in all cases.
In addition, there will be many gestures that do not re-
solve any keyword.
2 Related Work
This research draws mainly from two streams of re-
lated work. Researchers in human-computer interaction
have worked towards developing multimodal user inter-
faces, which allow spoken and gestural input. These sys-
tems often feature powerful algorithms for fusing modal-
ities; however, they also restrict communication to short
grammatically-constrained commands over a very lim-
ited vocabulary. Since our goal is to handle more com-
plex linguistic phenomena, these systems were of little
help in the design of our algorithm. Conversely, we found
that the problem of anaphora resolution faces a very sim-
ilar set of challenges as gesture-speech alignment. We
were able to apply techniques from anaphora resolution
to gesture-speech alignment.
2.1 Multimodal User Interfaces
Discussion of multimodal user interfaces begins with the
seminal ?Put-That-There? system (Bolt, 1980), which al-
lowed users to issue natural language commands and use
deictic hand gestures to resolve references from speech.
Commands were subject to a strict grammar and align-
ment was straightforward: keywords created holes in the
semantic frame, and temporally-aligned gestures filled
the holes.
More recent systems have extended this approach
somewhat. Johnston and Bangalore describe a multi-
modal parsing algorithm that is built using a 3-tape, finite
state transducer (FST) (Johnston and Bangalore, 2000).
The speech and gestures of each multimodal utterance
are provided as input to an FST whose output is a se-
mantic representation conveying the combined meaning.
A similar system, based on a graph-matching algorithm,
is described in (Chai et al, 2004). These systems per-
form mutual disambiguation, where each modality helps
to correct errors in the others. However, both approaches
restrict users to a predefined grammar and lexicon, and
rely heavily on having a complete, formal ontology of
the domain.
In (Kettebekov et al, 2002), a co-occurrence model
relates the salient prosodic features of the speech (pitch
variation and pause) to characteristic features of gestic-
ulation (velocity and acceleration). The goal was to im-
prove performance of gesture recognition, rather than to
address the problem of alignment directly. Their ap-
proach also differs from ours in that they operate at the
level of speech signals, rather than recognized words.
Potentially, the two approaches could compliment each
other in a unified system.
2.2 Anaphora Resolution
Anaphora resolution involves linking an anaphor to its
corresponding antecedent in the same or previous sen-
tence. In many cases, speech/gesture multimodal fusion
works in a very similar way, with gestures grounding
some of the same anaphoric pronouns (e.g., ?this?, ?that?,
?here?).
One approach to anaphora resolution is to assign a
salience value to each noun phrase that is a candidate
for acting as a grounding referent, and then to choose
the noun phrase with the greatest salience (Lappin and
Leass, 1994). Mitkov showed that a salience-based ap-
proach can be applied across genres and without com-
plex syntactic, semantic, and discourse analysis (Mitkov,
1998). Salience values are typically computed by apply-
ing linguistic knowledge; e.g., recent noun phrases are
more salient, gender and number should agree, etc. This
knowledge is applied to derive a salience value through
the application of a set of predefined salience weights
on each feature. Salience weights may be defined by
hand, as in (Lappin and Leass, 1994), or learned from
data (Mitkov et al, 2002).
Anaphora resolution and gesture-speech alignment are
very similar problems. Both involve resolving ambigu-
ous words which reference other parts of the utterance.
In the case of anaphora resolution, pronomial references
resolve to previously uttered noun phrases; in gesture-
speech alignment, keywords are resolved by gestures,
which usually precede the keyword. The salience-based
approach works for anaphora resolution because the fac-
tors that contribute to noun-phrase salience are well un-
derstood. We define a parallel set of factors for evaluating
the salience of gestures.
3 Our Approach
The most important goal of our system is the ability to
handle natural, human-to-human language usage. This
includes disfluencies and grammatically incorrect utter-
ances, which become even more problematic when con-
sidering that the output of speech recognizers is far from
perfect. Any approach that requires significant parsing
or other grammatical analysis may be ill-suited to meet
these goals.
Instead, we identify keywords that are likely to require
gestural referents for resolution. Our goal is to produce
an alignment ? a set of bindings ? that match at least some
of the identified keywords with one or more gestures.
There are several things that are known to contribute to
the salience of candidate gesture-speech bindings:
? The relevant gesture is usually close in time to the
keyword (Oviatt et al, 1997; Cohen et al, 2002)
? The gesture usually precedes the keyword (Oviatt et
al., 1997).
? A one-to-one mapping is preferred. Multiple key-
words rarely align with a single gesture, and mul-
tiple gestures almost never align with a single key-
word (Eisenstein and Davis, 2003).
? Some types of gestures, such as deictic pointing ges-
tures, are more likely to take part in keyword bind-
ings. Other gestures (i.e., beats) do not carry this
type of semantic content, and instead act to moder-
ate turn taking or indicate emphasis. These gestures
are unlikely to take part in keyword bindings (Cas-
sell, 1998).
? Some keyword/gesture combinations may be partic-
ularly likely; for example, the keyword ?this? and a
deictic pointing gesture.
These rules mirror the salience weighting features em-
ployed by the anaphora resolution methods described in
the previous section. We define a parameterizable penalty
function that prefers alignments that adhere to as many of
these rules as possible. Given a set of verbal utterances
and gestures, we then try to find the set of bindings with
the minimal penalty. This is essentially an optimization
approach, and we use the simplest possible optimization
technique: greedy hill-climbing. Of course, given a set
of penalties and the appropriate representation, any op-
timization technique could be applied. In the evaluation
section, we discuss whether and how much our system
would benefit from using a more sophisticated optimiza-
tion technique. Later in this section, we formalize the
problem and our proposed solution.
3.1 Leveraging Empirical Data
One of the advantages of the salience-based approach is
that it enables the creation of a hybrid system that ben-
efits both from our intuitions about multimodal commu-
nication and from a corpus of annotated data. The form
of the salience metric, and the choice of features that fac-
tor into it, is governed by our knowledge about the way
speech and gesture work. However, the penalty func-
tion also requires parameters that weigh the importance
of each factor. These parameters can be crafted by hand
if no corpus is available, but they can also be learned from
data. By using knowledge about multimodal language to
derive the form and features of the salience metric, and
using a corpus to fine-tune the parameters of this metric,
we can leverage the strengths of both knowledge-based
and data-driven approaches.
4 Formalization
We define a multimodal transcript M to consist of a set
of spoken utterances S and gestures G. S contains a set
of references R that must be ground by a gestural ref-
erent. We define a binding, b ? B, as a tuple relating
a gesture, g ? G, to a corresponding speech reference,
r ? R. Provided G and R, the set B enumerates all
possible bindings between them. Formally, each gesture,
reference, and binding are defined as
g = ?tgs , t
g
e , Gtype?
r = ?trs, t
r
e, w?
b = ?g, r?
(1)
where ts, te describe the start and ending time of a gesture
or reference, w ? S is the word corresponding to r, and
Gtype is the type of gesture (e.g. deictic or trajectory).
An alternative, useful description of the set B is as the
function b(g) which returns for each gesture a set of cor-
responding references. This function is defined as
b(g) = {r|?g, r? ? B} (2)
4.1 Rules
In this section we provide the analytical form for the
penalty functions of Section 3. We have designed these
functions to penalize bindings that violate the preferences
that model our intuitions about the relationship between
speech and gesture. We begin by presenting the analytical
form for the binding penalty function, ?b.
It is most often the case that verbal references closely
follow the gestures that they refer to; the verbal reference
rarely precedes the gesture. To reflect this knowledge,
we parameterize ?b using a time gap penalty, ?tg, and a
wrong order penalty, ?wo as follows,
?b(b) = ?tgwtg(b) + ?wowwo(b) (3)
where,
wwo(b) =
{
0 trs ? t
g
s
1 trs < t
g
s
and wtg = |trs ? tgs |
In addition to temporal agreement, specific words or
parts-of-speech have varying affinities for different types
of gestures. We incorporate these penalties into ?b by in-
troducing a binding agreement penalty, ?(b), as follows:
?b(b) = ?tgwwo(b) + ?(b) (4)
The remaining penalty functions model binding fertil-
ity. Specifically, we assign a penalty for each unassigned
gesture and reference, ?g(g) and ?r(r) respectively, that
reflect our desire for the algorithm to produce bindings.
Certain gesture types (e.g., deictics) are much more likely
to participate in bindings than others (e.g., beats). An
unassigned gesture penalty is associated with each ges-
ture type, given by ?g(g). Similarly, we expect refer-
ences to have a likelihood of being bound that is condi-
tioned on their word or part-of-speech tag. However, we
currently handle all keywords in the same way, with a
constant penalty ?r(r) for unassigned keywords.
4.2 Minimization Algorithm
GivenG andR we wish to find aB? ? B that minimizes
the penalty function ?(B,G,R):
B? = arg min
B?
?(B?,G,R) (5)
Using the penalty functions of Section 4.1 ?(B?,G,R) is
defined as,
?(B?,G,R) =
?
b?B?
?b(b) + ?g(Ga) + ?r(Ra) (6)
where
Ga = {g|b(g) = ?}
Ra = {r|b(r) = ?}
.
Although there are numerous optimization techniques
that may be applied to minimize Equation 5, we have
chosen to implement a naive gradient decent algorithm
presented below as Algorithm 1. Observing the prob-
lem, note we could have initialized B? = B; in other
Algorithm 1 Gradient Descent
Initialize B? = ? and B? = B
repeat
Let b0 be the first element in B?
?max = ?(B?, G,R)? ?({B?, b0}, G,R)
bmax = b0
for all b ? B?, b 6= b0 do
? = ?(B?, G,R)? ?({B?, b}, G,R)
if ? > ?max then
bmax = b
?max = ?
end if
end for
if ?max > 0 then
B? = {B?, bmax}
B? = B? ? bmax
end if
Convergence test: is ?max < limit?
until convergence
words, start off with all possible bindings, and gradu-
ally prune away the bad ones. But it seems likely that
|B?| ? min(|R|, |G|); thus, starting from the empty set
will converge faster. The time complexity of this algo-
rithm is given by O(|B?||B|). Since |B| = |G||R|, and
assuming |B?| ? |G| ? |R|, this simplifies to O(|B?|3),
cubic in the number of bindings returned.
4.3 Learning Parameters
We explored a number of different techniques for find-
ing the parameters of the penalty function: setting them
by hand, gradient descent, simulated annealing, and a ge-
netic algorithm. A detailed comparison of the results with
each approach is beyond the scope of this paper, but the
genetic algorithm outperformed the other approaches in
both accuracy and rate of convergence.
The genome representation consisted of a thirteen bit
string for each penalty parameter; three bits were used
for the exponent, and the remaining ten were used for
the base. Parameters were allowed to vary from 10?4
to 103. Since there were eleven parameters, the overall
string length was 143. A population size of 200 was used,
and training proceeded for 50 generations. Single-point
crossover was applied at a rate of 90%, and the mutation
rate was set to 3% per bit. Tournament selection was used
rather than straightforward fitness-based selection (Gold-
berg, 1989).
5 Evaluation
We evaluated our system by testing its performance
on a set of 26 transcriptions of unconstrained human-
to-human communication, from nine different speak-
Baseline Training Test
Recall 84.2% 94.6% 95.1%
? n/a 1.2% 5.1%
Precision 82.8% 94.5% 94.5%
? n/a 1.2% 5.0%
Table 1: Performance of our system versus a baseline
ers (Eisenstein and Davis, 2003). Of the four women and
five men who participated, eight were right-handed, and
one was a non-native English speaker. The participants
ranged in age from 22 to 28. All had extensive computer
experience, but none had any experience in the task do-
main, which required explaining the behavior of simple
mechanical devices.
The participants were presented with three conditions,
each of which involved describing the operation of a me-
chanical device based on a computer simulation. The
conditions were shown in order of increasing complexity,
as measured by the number of moving parts: a latchbox, a
piston, and a pinball machine. Monologues ranged in du-
ration from 15 to 90 seconds; the number of gestures used
ranged from six to 58. In total, 574 gesture phrases were
transcribed, of which 239 participated in gesture-speech
bindings.
In explaining the devices, the participants were al-
lowed ? but not instructed ? to refer to a predrawn di-
agram that corresponded to the simulation. Vocabulary,
grammar, and gesture were not constrained in any way.
The monologues were videotaped, transcribed, and an-
notated by hand. No gesture or speech recognition was
performed. The decision to use transcriptions rather than
speech and gesture recognizers will be discussed in detail
below.
5.1 Empirical Results
We averaged results over ten experiments, in which 20%
of the data was selected randomly and held out as a test
set. Entire transcripts were held out, rather than parts of
each transcript. This was necessary because the system
considers the entire transcript holistically when choosing
an alignment.
For a baseline, we evaluated the performance of choos-
ing the temporally closest gesture to each keyword.
While simplistic, this approach is used in several imple-
mented multimodal user interfaces (Bolt, 1980; Koons
et al, 1993). Kettebekov and Sharma even reported that
93.7% of gesture phrases were ?temporally aligned? with
the semantically associated keyword in their corpus (Ket-
tebekov and Sharma, 2001). Our results with this base-
line were somewhat lower, for reasons discussed below.
Table 1 shows the results of our system and the base-
line on our corpus. Our system significantly outperforms
the baseline on both recall and precision on this corpus
(p < 0.05, two-tailed). Precision and recall differ slightly
because there are keywords that do not bind to any ges-
ture. Our system does not assume a one-to-one mapping
between keywords and gestures, and will refuse to bind
some keywords if there is no gesture with a high enough
salience. One benefit of our penalty-based approach is
that it allows us to easily trade off between recall and
precision. Reducing the penalties for unassigned ges-
tures and keywords will cause the system to create fewer
alignments, increasing precision and decreasing recall.
This could be useful in a system where mistaken ges-
ture/speech alignments are particularly undesirable. By
increasing these same penalties, the opposite effect can
also be achieved.
Both systems perform worse on longer monologues.
On the top quartile of monologues by length (measured
in number of keywords), the recall of the baseline system
falls to 75%, and the recall of our system falls to 90%.
For the baseline system, we found a correlation of -0.55
(df = 23, p < 0.01) between F-measure and monologue
length.
This may help to explain why Kettebekov and Sharma
found such success with the baseline algorithm. The mul-
timodal utterances in their corpus consisted of relatively
short commands. The longer monologues in our corpus
tended to be more grammatically complex and included
more disfluency. Consequently, alignment was more dif-
ficult, and a relatively na??ve strategy, such as the baseline
algorithm, was less effective.
6 Discussion
To our knowledge, very few multimodal understanding
systems have been evaluated using natural, unconstrained
speech and gesture. One exception is (Quek et al, 2002),
which describes a system that extracts discourse struc-
ture from gesture on a corpus of unconstrained human-
to-human communication; however, no quantitative anal-
ysis is provided. Of the systems that are more relevant
to the specific problem of gesture-speech alignment (Co-
hen et al, 1997; Johnston and Bangalore, 2000; Kette-
bekov and Sharma, 2001), evaluation is always conducted
from an HCI perspective, in which participants act as
users of a computer system and communicate in short,
grammatically-constrained multimodal commands. As
shown in Section 5.1, such commands are significantly
easier to align than the natural multimodal communica-
tion found in our corpus.
6.1 The Corpus
A number of considerations went into gathering this cor-
pus.1 One of our goals was to minimize the use of
discourse-related ?beat? gestures, so as to better focus on
the deictic and iconic gestures that are more closely re-
lated to the content of the speech; that is why we focused
on monologues rather than dialogues. We also wanted the
corpus to be relevant to the HCI community. That is why
we provided a diagram to gesture at, which we believe
serves a similar function to a computer display, providing
reference points for deictic gestures. We used a predrawn
diagram ? rather than letting participants draw the dia-
gram themselves ? because interleaved speech, gesture,
and sketching is a much more complicated problem, to
be addressed only after bimodal speech-gesture commu-
nication is better understood.
For a number of reasons, we decided to focus on tran-
scriptions of speech and gesture, rather than using speech
and gesture recognition systems. Foremost is that we
wanted the language in our corpus to be as natural as pos-
sible; in particular, we wanted to avoid restricting speak-
ers to a finite list of gestures. Building a recognizer that
could handle such unconstrained gesture would be a sub-
stantial undertaking and an important research contribu-
tion in its own right. However, we are sensitive to the con-
cern that our system should scale to handle possibly erro-
neous recognition data. There are three relevant classes of
errors that our system may need to handle: speech recog-
nition, gesture recognition, and gesture segmentation.
? Speech Recognition Errors
The speech recognizer could fail to recognize a key-
word; in this case, a binding would simply not be
created. If the speech recognizer misrecognized
a non-keyword as a keyword, a spurious binding
might be created. However, since our system does
not require that all keywords have bindings, we feel
that our approach is likely to degrade gracefully in
the face of this type of error.
? Gesture Recognition Errors
This type of error would imply a gestural misclas-
sification, e.g., classifying a deictic pointing gesture
as an iconic. Again, we feel that a salience-based
system will degrade gracefully with this type of er-
ror, since there are no hard requirements on the type
of gesture for forming a binding. In contrast, a sys-
tem that required, say, a deictic gesture to accom-
pany a certain type of command would be very sen-
sitive to a gesture misclassification.
1We also considered using the recently released FORM2
corpus from the Linguistic Data Consortium. However, this
corpus is presently more focused on the kinematics of hand and
upper body movement, rather than on higher-level linguistic in-
formation relating to gestures and speech.
? Gesture Segmentation Errors
Gesture segmentation errors are probably the most
dangerous, since this could involve incorrectly
grouping two separate gestures into a single gesture,
or vice versa. It seems that this type of error would
be problematic for any approach, and we have no
reason to believe that our salience-based approach
would fare differently from any other approach.
6.2 Success Cases
Our system outperformed the baseline by more than 10%.
There were several types of phenomena that the base-
line failed to handle. In this corpus, each gesture pre-
cedes the semantically-associated keyword 85% of the
time. Guided by this fact, we first created a baseline sys-
tem that selected the nearest preceding gesture for each
keyword; clearly, the maximum performance for such a
baseline is 85%. Slightly better results were achieved by
simply choosing the nearest gesture regardless of whether
it precedes the keyword; this is the baseline shown in Ta-
ble 1. However, this baseline incorrectly bound several
cataphoric gestures. The best strategy is to accept just a
few cataphoric gestures in unusual circumstances, but a
na??ve baseline approach is unable to do this.
Most of the other baseline errors came about when
the mapping from gesture to speech was not one-to-one.
For example, in the utterance ?this piece here,? the two
keywords actually refer to a single deictic gesture. In
the salience-based approach, the two keywords were cor-
rectly bound to a single gesture, but the baseline insisted
on finding two gestures. The baseline similarly mishan-
dled situations where a keyword was used without refer-
ring to any gesture.
6.3 Failure Cases
Although the recall and precision of our system neared
95%, investigating the causes of error could suggest
potential improvements. We were particularly interested
in errors on the training set, where overfitting could not
be blamed. This section describes two sources of error,
and suggests some potential improvements.
6.3.1 Disfluencies
We adopted a keyword-based approach so that our sys-
tem would be more robust to disfluency than alternative
approaches that depended on parsing. While we were
able to handle many instances of disfluent speech, we
found that disfluencies occasionally disturbed the usual
relationship between gesture and speech. For example,
consider the following utterance:
It has this... this spinning thing...
Our system attempted to bind gestures to each occur-
rence of ?this?, and ended up binding each reference to a
different gesture. Moreover, both references were bound
incorrectly. The relevant gesture in this case occurs after
both references. This is an uncommon phenomenon, and
as such, was penalized highly. However, anecdotally
it appears that the presence of a disfluency makes this
phenomenon more likely. A disfluency is frequently
accompanied by an abortive gesture, followed by the
full gesture occurring somewhat later than the spoken
reference. It is possible that a system that could detect
disfluency in the speech transcript could account for this
phenomenon.
6.3.2 Greedy Search
Our system applies a greedy hill-climbing opti-
mization to minimize the penalty. While this greedy
optimization performs surprisingly well, we were able
to identify a few cases of errors that were caused by the
greedy nature of our optimization, e.g.
...once it hits this, this thing is blocked.
In this example, the two references are right next to
each other. The relevant gestures are also very near each
other. The ideal bindings are shown in Figure 1a. The
earlier ?this? is considered first, but from the system?s
perspective, the best possible binding is the second ges-
ture, since it overlaps almost completely with the spoken
utterance (Figure 1b). However, once the second gesture
is bound to the first reference, it is removed from the list
of unassigned gestures. Thus, if the second gesture were
also bound to the second utterance, the penalty would
still be relatively high. Even though the earlier gesture
is farther away from the second reference, it is still on the
list of unassigned gestures, and the system can reduce the
overall penalty considerably by binding it. The system
ends up crisscrossing, and binding the earlier gesture to
the later reference, and vice versa (Figure 1c).
7 Future Work
The errors discussed in the previous section suggest some
potential improvements to our system. In this section, we
describe four possible avenues of future work: dynamic
programming, deeper syntactic analysis, other anaphora
resolution techniques, and user adaptation.
7.1 Dynamic Programming
Algorithm 1 provides only an approximate solution to
Equation 5. As demonstrated in Section 6.3.2, the greedy
choice is not always optimal. Using dynamic program-
ming, an exhaustive search of the space of bindings can
be performed within polynomial time.
(a) (b) (c)
Figure 1: The greedy binding problem. (a) The correct binding, (b) the greedy binding, (c) the result.
We define m[i, j] to be the penalty of the optimal sub-
set B? ? {bi, ..., bj} ? B, i ? j. m[i, j] is implemented
as a k ? k lookup table, where k = |B| = |G||R|. Each
entry of this table is recursively defined by preceding ta-
ble entries. Specifically, m[i, j] is computed by perform-
ing exhaustive search on its subsets of bindings. Using
this lookup table, an optimal solution to Equation 5 is
therefore found as ?(B?, G,R) = m[1, k]. Again as-
suming |B?| ? |G| ? |R|, the size of the lookup table is
given by O(|B?|4). Thus, it is possible to find the glob-
ally optimal set of bindings, by moving from an O(n3)
algorithm to O(n4). The precise definition of a recur-
rence relation for m[i, j] and a proof of correctness will
be described in a future publication.
7.2 Syntactic Analysis
One obvious possibility for improvement would be to in-
clude more sophisticated syntactic information beyond
keyword spotting. However, we require that our system
remain robust to disfluency and recognition errors. Part
of speech tagging is a robust method of syntactic anal-
ysis which could allow us to refine the penalty function
depending on the usage case. Consider that there at least
three relevant uses of the keyword ?this.?
1. This movie is better than A.I.
2. This is the bicycle ridden by E.T.
3. The wheel moves like this.
When ?this? is followed by a noun (case 1), a deic-
tic gesture is likely, although not strictly necessary. But
when ?this? is followed by a verb (case 2), a deictic
gesture is usually crucial for understanding the sentence.
Thus, the penalty for not assigning this keyword should
be very high. Finally, in the third case, when the keyword
follows a preposition, a trajectory gesture is more likely,
and the penalty for any such binding should be lowered.
7.3 Other Anaphora Resolution Techniques
We have based this research on salience values, which
is just one of several possible alternative approaches to
anaphora resolution. One such alternative is the use of
constraints: rules that eliminate candidates from the list
of possible antecedents (Rich and Luperfoy, 1988). An
example of a constraint in anaphora resolution is a rule
requiring the elimination of all candidates that disagree
in gender or number with the referential pronoun. Con-
straints may be used in combination with a salience met-
ric, to prune away unlikely choices before searching.
The advantage is that enforcing constraints could be sub-
stantially less computationally expensive than searching
through the space of all possible bindings for the one with
the highest salience. One possible future project would be
to develop a set of constraints for speech-gesture align-
ment, and investigate the effect of these constraints on
both accuracy and speed.
Ge, Hale, and Charniak propose a data-driven ap-
proach to anaphora resolution (Ge et al, 1998). For a
given pronoun, their system can compute a probability
for each candidate antecedent. Their approach of seek-
ing to maximize this probability is similar to the salience-
maximizing approach that we have described. However,
instead of using a parametric salience function, they learn
a set of conditional probability distributions directly from
the data. If this approach could be applied to gesture-
speech alignment, it would be advantageous because the
binding probabilities could be combined with the output
of probabilistic recognizers to produce a pipeline archi-
tecture, similar to that proposed in (Wu et al, 1999). Such
an architecture would provide multimodal disambigua-
tion, where the errors of each component are corrected
by other components.
7.4 Multimodal Adaptation
Speakers have remarkably entrenched multimodal com-
munication patterns, with some users overlapping ges-
ture and speech, and others using each modality sequen-
tially (Oviatt et al, 1997). Moreover, these multimodal
integration patterns do not seem to be malleable, sug-
gesting that multimodal user interfaces should adapt to
the user?s tendencies. We have already shown how the
weights of the salience metric can adapt for optimal per-
formance against a corpus of user data; this approach
could also be extended to adapt over time to an individual
user.
8 Conclusions
This work represents one of the first efforts at aligning
gesture and speech on a corpus of natural multimodal
communication. Using greedy optimization and only a
minimum of linguistic processing, we significantly out-
perform a competitive baseline, which has actually been
implemented in existing multimodal user interfaces. Our
approach is shown to be robust to spoken English, even
with a high level of disfluency. By blending some of the
benefits of empirical and knowledge-based approaches,
our system can learn from a large corpus of data, but de-
grades gracefully when limited data is available.
Obviously, alignment is only one small component of
a comprehensive system for recognizing and understand-
ing multimodal communication. Putting aside the issue
of gesture recognition, there is still the problem of de-
riving semantic information from aligned speech-gesture
units. The solutions to this problem will likely have to be
specially tailored to the application domain. While our
evaluation indicates that our approach achieves what ap-
pears to be a high level of accuracy, the true test will be
whether our system can actually support semantic infor-
mation extraction from multimodal data. Only the con-
struction of such a comprehensive end-to-end system will
reveal whether the algorithm and features that we have
chosen are sufficient, or whether a more sophisticated ap-
proach is required.
Acknowledgements
We thank Robert Berwick, Michael Collins, Trevor Darrell,
Randall Davis, Tracy Hammond, Sanshzar Kettebekov, ?Ozlem
Uzuner, and the anonymous reviewers for their helpful com-
ments on this paper.
References
Richard A. Bolt. 1980. Put-That-There: Voice and gesture at
the graphics interface. Computer Graphics, 14(3):262?270.
Justine Cassell. 1998. A framework for gesture generation and
interpretation. In Computer Vision in Human-Machine Inter-
action, pages 191?215. Cambridge University Press.
Joyce Y. Chai, Pengyu Hong, , and Michelle X. Zhou. 2004. A
probabilistic approach to reference resolution in multimodal
user interfaces. In Proceedings of 2004 International Con-
ference on Intelligent User Intefaces (IUI?04), pages 70?77.
Nicole Chovil. 1992. Discourse-oriented facial displays in con-
versation. Research on Language and Social Interaction,
25:163?194.
Philip R. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman,
I. Smith, L. Chen, and J. Clow. 1997. Quickset: Multimodal
interaction for distributed applications. In ACM Multime-
dia?97, pages 31?40. ACM Press.
Philip R. Cohen, Rachel Coulston, and Kelly Krout. 2002.
Multimodal interaction during multiparty dialogues: Initial
results. In IEEE Conference on Multimodal Interfaces.
Jacob Eisenstein and Randall Davis. 2003. Natural gesture in
descriptive monologues. In UIST?03 Supplemental Proceed-
ings, pages 69?70.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. In Proceedings of the Sixth
Workshop on Very Large Corpora, pages 161?171.
David E. Goldberg. 1989. Genetic Algorithms in Search, Opti-
mization, and Machine Learning. Addison-Wesley.
Michael Johnston and Srinivas Bangalore. 2000. Finite-state
multimodal parsing and understanding. In Proceedings of
COLING-2000. ICCL.
Sanshzar Kettebekov and Rajeev Sharma. 2001. Toward natu-
ral gesture/speech control of a large display. In Engineering
for Human-Computer Interaction (EHCI?01). Lecture Notes
in Computer Science. Springer Verlag.
Sanshzar Kettebekov, Mohammed Yeasin, and Rajeev Sharma.
2002. Prosody based co-analysis for continuous recognition
of coverbal gestures. In International Conference on Mul-
timodal Interfaces (ICMI?02), pages 161?166, Pittsburgh,
USA.
David B. Koons, Carlton J. Sparrell, and Kristinn R. Thorisson.
1993. Integrating simultaneous input from speech, gaze, and
hand gestures. In Intelligent Multimedia Interfaces, pages
257?276. AAAI Press.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Linguistics,
20(4):535?561.
David McNeill. 1992. Hand and Mind. The University of
Chicago Press.
Ruslan Mitkov, Richard Evans, and Constantin Ora?san. 2002.
A new, fully automatic version of mitkov?s knowledge-poor
pronoun resolution method. In Intelligent Text Processing
and Computational Linguistics (CICLing?02), Mexico City,
Mexico, February, 17 ? 23.
Ruslan Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In COLING-ACL, pages 869?875.
Sharon L. Oviatt, Antonella DeAngeli, and Karen Kuhn. 1997.
Integration and synchronization of input modes during mul-
timodal human-computer interaction. In Human Factors in
Computing Systems (CHI?97), pages 415?422. ACM Press.
Sharon L. Oviatt. 1999. Ten myths of multimodal interaction.
Communications of the ACM, 42(11):74?81.
Francis Quek, David McNeill, Robert Bryll, Susan Duncan,
Xin-Feng Ma, Cemil Kirbas, Karl E. McCullough, and
Rashid Ansari. 2002. Multimodal human discourse: gesture
and speech. Transactions on Computer-Human Interaction,
9(3):171?193.
Elaine Rich and Susann Luperfoy. 1988. An architecture for
anaphora resolution. In Proceedings of the Second Confer-
ence on Applied Natural Language Processing (ANLP-2),
pages 18?24, Texas, USA.
Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen. 1999.
Multimodal integration - a statistical view. IEEE Transac-
tions on Multimedia, 1(4):334?341.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 37?40,
New York, June 2006. c?2006 Association for Computational Linguistics
Gesture Improves Coreference Resolution
Jacob Eisenstein and Randall Davis
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139 USA
{jacobe+davis}@csail.mit.edu
Abstract
Coreference resolution, like many problems
in natural language processing, has most of-
ten been explored using datasets of written
text. While spontaneous spoken language
poses well-known challenges, it also offers ad-
ditional modalities that may help disambiguate
some of the inherent disfluency. We explore
features of hand gesture that are correlated
with coreference. Combining these features
with a traditional textual model yields a statis-
tically significant improvement in overall per-
formance.
1 Introduction
Although the natural language processing community has
traditionally focused largely on text, face-to-face spoken
language is ubiquitous, and offers the potential for break-
through applications in domains such as meetings, lec-
tures, and presentations. We believe that in face-to-face
discourse, it is important to consider the possibility that
non-verbal communication may offer features that are
critical to language understanding. However, due to the
long-standing emphasis on text datasets, there has been
relatively little work on non-textual features in uncon-
strained natural language (prosody being the most no-
table exception).
Multimodal research in NLP has typically focused
on dialogue systems for human-computer interaction
(e.g., (Oviatt, 1999)); in contrast, we are interested in
the applicability of multimodal features to unconstrained
human-human dialogues. We believe that such features
will play an essential role in bringing NLP applications
such as automatic summarization and segmentation to
multimedia documents, such as lectures and meetings.
More specifically, in this paper we explore the possi-
bility of applying hand gesture features to the problem
of coreference resolution, which is thought to be fun-
damental to these more ambitious applications (Baldwin
and Morton, 1998). To motivate the need for multimodal
features in coreference resolution, consider the following
transcript:
?[This circle (1)] is rotating clockwise and [this
piece of wood (2)] is attached at [this point (3)]
and [this point (4)] but [it (5)] can rotate. So as
[the circle (6)] rotates, [this (7)] moves in and
out. So [this whole thing (8)] is just going back
and forth.?
Even given a high degree of domain knowledge (e.g.,
that ?circles? often ?rotate? but ?points? rarely do), de-
termining the coreference in this excerpt seems difficult.
The word ?this? accompanied by a gesture is frequently
used to introduce a new entity, so it is difficult to deter-
mine from the text alone whether ?[this (7)]? refers to
?[this piece of wood (2)],? or to an entirely different part
of the diagram. In addition, ?[this whole thing (8)]? could
be anaphoric, or it might refer to a new entity, perhaps
some superset of predefined parts.
The example text was drawn from a small corpus of di-
alogues, which has been annotated for coreference. Par-
ticipants in the study had little difficulty understanding
what was communicated. While this does not prove that
human listeners are using gesture or other multimodal
features, it suggests that these features merit further in-
vestigation. We extracted hand positions from the videos
in the corpus, using computer vision. From the raw hand
positions, we derived gesture features that were used to
supplement traditional textual features for coreference
resolution. For a description of the study?s protocol, auto-
matic hand tracking, and a fuller examination of the ges-
ture features, see (Eisenstein and Davis, 2006). In this pa-
per, we present results showing that these features yield a
significant improvement in performance.
37
2 Implementation
A set of commonly-used linguistic features were selected
for this problem (Table 1). The first five features apply
to pairs of NPs; the next set of features are applied indi-
vidually to both of the NPs that are candidates for coref-
erence. Thus, we include two features each, e.g., J is
PRONOUN and I is PRONOUN, indicating respectively
whether the candidate anaphor and candidate antecedent
are pronouns. We include separate features for each of
the four most common pronouns: ?this?, ?it?, ?that?, and
?they,? yielding features such as J=?this?.
2.1 Gesture Features
The gesture features shown in Table 1 are derived from
the raw hand positions using a simple, deterministic sys-
tem. Temporally, all features are computed at the mid-
point of each candidate NP; for a further examination
of the sensitivity to temporal offset, see (Eisenstein and
Davis, 2006).
At most one hand is determined to be the ?focus hand,?
according to the following heuristic: select the hand far-
thest from the body in the x-dimension, as long as the
hand is not occluded and its y-position is not below the
speaker?s waist. If neither hand meets these criteria, than
no hand is said to be in focus. Occluded hands are also
not permitted to be in focus; the listener?s perspective was
very similar to that of the camera, so it seemed unlikely
that the speaker would occlude a meaningful gesture. In
addition, our system?s estimates of the position of an oc-
cluded hand are unlikely to be accurate.
If focus hands can be identified during both mentions,
the Euclidean distance between focus points is computed.
The distance is binned, using the supervised method de-
scribed in (Fayyad and Irani, 1993). An advantage of
binning the continuous features is that we can create a
special bin for missing data, which occurs whenever a fo-
cus hand cannot be identified.
If the same hand is in focus during both NPs, then the
value of WHICH HAND is set to ?same?; if a different
hand is in focus then the value is set to ?different?; if a
focus hand cannot be identified in one or both NPs, then
the value is set to ?missing.? This multi-valued feature is
automatically converted into a set of boolean features, so
that all features can be represented as binary variables.
2.2 Coreference Resolution Algorithm
(McCallum and Wellner, 2004) formulates coreference
resolution as a Conditional Random Field, where men-
tions are nodes, and their similarities are represented as
weighted edges. Edge weights range from ?? to ?,
with larger values indicating greater similarity. The op-
timal solution is obtained by partitioning the graph into
cliques such that the sum of the weights on edges within
cliques is maximized, and the sum of the weights on
edges between cliques is minimized:
y? = argmaxy
?
i,j,i6=j
yi,js(xi, xj) (1)
In equation 1, x is a set of mentions and y is a corefer-
ence partitioning, such that yi,j = 1 if mentions xi and xj
corefer, and yi,j = ?1 otherwise. s(xi, xj) is a similarity
score computed on mentions xi and xj .
Computing the optimal partitioning y? is equivalent to
the problem of correlation clustering, which is known to
be NP-hard (Demaine and Immorlica, to appear). De-
maine and Immorlica (to appear) propose an approxima-
tion using integer programming, which we are currently
investigating. However, in this research we use average-
link clustering, which hierarchically groups the mentions
x, and then forms clusters using a cutoff chosen to maxi-
mize the f-measure on the training set.
We experiment with both pipeline and joint models for
computing s(xi, xj). In the pipeline model, s(xi, xj) is
the posterior of a classifier trained on pairs of mentions.
The advantage of this approach is that any arbitrary clas-
sifier can be used; the downside is that minimizing the er-
ror on all pairs of mentions may not be equivalent to min-
imizing the overall error of the induced clustering. For
experiments with the pipeline model, we found best re-
sults by boosting shallow decision trees, using the Weka
implementation (Witten and Frank, 1999).
Our joint model is based on McCallum and Well-
ner?s (2004) adaptation of the voted perceptron to corefer-
ence resolution. Here, s is given by the product of a vec-
tor of weights ? with a set of boolean features ?(xi, xj)
induced from the pair of noun phrases: s(xi, xj) =
??(xi, xj). The maximum likelihood weights can be ap-
proximated by a voted perceptron, where, in the iteration
t of the perceptron training:
?t = ?t?1 +
?
i,j,i6=j
?(xi, xj)(y
?
i,j ? y?i,j) (2)
In equation 2, y? is the ground truth partitioning from
the labeled data. y? is the partitioning that maximizes
equation 1 given the set of weights ?t?1. As before,
average-link clustering with an adaptive cutoff is used to
partition the graph. The weights are then averaged across
all iterations of the perceptron, as in (Collins, 2002).
3 Evaluation
The results of our experiments are computed using
mention-based CEAF scoring (Luo, 2005), and are re-
ported in Table 2. Leave-one-out evaluation was used to
form 16 cross-validation folds, one for each document in
the corpus. Using a planned, one-tailed pairwise t-test,
the gesture features improved performance significantly
38
MARKABLE DIST The number of markables between the candidate NPs
EXACT MATCH True if the candidate NPs have identical surface forms
STR MATCH True if the candidate NPs match after removing articles
NONPRO MATCH True if the candidate NPs are not pronouns and have identical surface forms
NUMBER MATCH True if the candidate NPs agree in number
PRONOUN True if the NP is a pronoun
DEF NP True if the NP begins with a definite article, e.g. ?the box?
DEM NP True if the NP is not a pronoun and begins with the word ?this?
INDEF NP True if the NP begins an indefinite article, e.g. ?a box?
pronouns Individual features for each of the four most common pronouns: ?this?, ?it?, ?that?, and
?they?
FOCUS DIST Distance between the position of the in-focus hand during j and i (see text)
WHICH HAND Whether the hand in focus during j is the same as in i (see text)
Table 1: The feature set
System Feature set F1
AdaBoost Gesture + Speech 54.9
AdaBoost Speech only 52.8
Voted Perceptron Gesture + Speech 53.7
Voted Perceptron Speech only 52.9
Baseline EXACT MATCH only 50.2
Baseline None corefer 41.5
Baseline All corefer 18.8
Table 2: Results
for the boosted decision trees (t(15) = 2.48, p < .02),
though not for the voted perceptron (t(15) = 1.07, p =
.15).
In the ?all corefer? baseline, all NPs are grouped into
a single cluster; in the ?none corefer?, each NP gets its
own cluster. In the ?EXACT MATCH? baseline, two NPs
corefer when their surface forms are identical. All ex-
perimental systems outperform all baselines by a statis-
tically significant amount. There are few other reported
results for coreference resolution on spontaneous, uncon-
strained speech; (Strube and Mu?ller, 2003) similarly finds
low overall scores for pronoun resolution on the Switch-
board Corpus, albeit by a different scoring metric. Unfor-
tunately, they do not compare performance to equivalent
baselines.
For the AdaBoost method, 50 iterations of boosting are
performed on shallow decision trees, with a maximum
tree depth of three. For the voted perceptron, 50 training
iterations were performed. The performance of the voted
perceptron on this task was somewhat unstable, varying
depending on the order in which the documents were
presented. This may be because a small change in the
weights can lead to a very different partitioning, which
in turn affects the setting of the weights in the next per-
ceptron iteration. For these results, the order of presenta-
tion of the documents was randomized, and the scores for
the voted perceptron are the average of 10 different runs
(? = 0.32% with gestures, 0.40% without).
Although the AdaBoost method minimizes pairwise
error rather than the overall error of the partitioning, its
performance was superior to the voted perceptron. One
possible explanation is that by boosting small decision
trees, AdaBoost was able to take advantage of non-linear
combinations of features. We tested the voted perceptron
using all pairwise combinations of features, but this did
not improve performance.
4 Discussion
If gesture features play a role in coreference resolu-
tion, then one might expect the probability of corefer-
ence to vary significantly when conditioned on features
describing the gesture. As shown in Table 3, the pre-
diction holds: the binned FOCUS DIST gesture feature
has the fifth highest ?2 value, and the relationship be-
tween coreference and all gesture features was significant
(?2 = 727.8, dof = 4, p < .01). Note also that although
FOCUS DIST ranks fifth, three of the features above it
are variants of a string-match feature, and so are highly
redundant.
The WHICH HAND feature is less strongly corre-
lated with coreference, but the conditional probabilities
do correspond with intuition. If the NPs corefer, then
the probability of using the same hand to gesture during
both NPs is 59.9%; if not, then the likelihood is 52.8%.
The probability of not observing a focus hand is 20.3%
when the NPs corefer, 25.1% when they do not; in other
words, gesture is more likely for both NPs of a corefer-
ent pair than for the NPs of a non-coreferent pair. The
relation between the WHICH HAND feature and coref-
erence is also significantly different from the null hypoth-
esis (?2 = 57.2, dof = 2, p < .01).
39
Rank Feature ?2
1. EXACT MATCH 1777.9
2. NONPRO MATCH 1357.5
3. STR MATCH 1201.8
4. J = ?it? 732.8
5. FOCUS DIST 727.8
6. MARKABLE DIST 619.6
7. J is PRONOUN 457.5
8. NUMBER 367.9
9. I = ?it? 238.6
10. I is PRONOUN 132.6
11. J is INDEF NP 79.3
12. SAME FOCUS HAND 57.2
Table 3: Top 12 Features By Chi-Squared
5 Related Work
Research on multimodality in the NLP community
has usually focused on multimodal dialogue systems
(e.g., (Oviatt, 1999)). These systems differ fundamen-
tally from ours in that they address human-computer in-
teraction, whereas we address human-human interaction.
Multimodal dialogue systems tackle interesting and dif-
ficult challenges, but the grammar, vocabulary, and rec-
ognized gestures are often pre-specified, and dialogue is
controlled at least in part by the computer. In our data, all
of these things are unconstrained.
Prosody has been shown to improve performance on
several NLP problems, such as topic and sentence seg-
mentation (e.g., (Shriberg et al, 2000)). We are aware of
no equivalent work showing statistically significant im-
provement on unconstrained speech using hand gesture
features. (Nakano et al, 2003) shows that body posture
predicts turn boundaries, but does not show that these
features improve performance beyond a text-only system.
(Chen et al, 2004) shows that gesture may improve sen-
tence segmentation; however, in this study, the improve-
ment afforded by gesture is not statistically significant,
and evaluation was performed on a subset of their original
corpus that was chosen to include only the three speakers
who gestured most frequently. Still, this work provides a
valuable starting point for the integration of gesture fea-
ture into NLP systems.
6 Conclusion
We have described how gesture features can be used to
improve coreference resolution on a corpus of uncon-
strained speech. Hand position and hand choice corre-
late significantly with coreference, explaining this gain in
performance. We believe this is the first example of hand
gesture features improving performance by a statistically
significant margin on unconstrained speech.
References
Breck Baldwin and Thomas Morton. 1998. Dy-
namic coreference-based summarization. In Proc. of
EMNLP.
Lei Chen, Yang Liu, Mary P. Harper, and Eliza-
beth Shriberg. 2004. Multimodal model integra-
tion for sentence unit detection. In Proceedings of
International Conference on Multimodal Interfaces
(ICMI?04). ACM Press.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Erik D. Demaine and Nicole Immorlica. to appear. Cor-
relation clustering in general weighted graphs. Theo-
retical Computer Science.
Jacob Eisenstein and Randall Davis. 2006. Gesture fea-
tures for coreference resolution. In Workshop on Mul-
timodal Interaction and Related Machine Learning Al-
gorithms.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-
interval discretization of continuousvalued attributes
for classification learning. In Proceedings of IJCAI-
93.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT-EMNLP, pages 25?32.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems.
Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-
tine Cassell. 2003. Towards a model of face-to-face
grounding. In Proceedings of ACL?03.
Sharon L. Oviatt. 1999. Mutual disambiguation of
recognition errors in a multimodel architecture. In Hu-
man Factors in Computing Systems (CHI?99), pages
576?583.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur,
and Gokhan Tur. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of ACL ?03, pages 168?175.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
40
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 215?218,
New York, June 2006. c?2006 Association for Computational Linguistics
Semantic Back-Pointers from Gesture
Jacob Eisenstein
MIT Computer Science and Artificial Intelligence Laboratory
77 Massachusetts Ave, MA 02139
jacobe@csail.mit.edu
1 Introduction
Although the natural-language processing commu-
nity has dedicated much of its focus to text, face-
to-face spoken language is ubiquitous, and offers
the potential for breakthrough applications in do-
mains such as meetings, lectures, and presentations.
Because spontaneous spoken language is typically
more disfluent and less structured than written text,
it may be critical to identify features from additional
modalities that can aid in language understanding.
However, due to the long-standing emphasis on text
datasets, there has been relatively little work on non-
textual features in unconstrained natural language
(prosody being the most studied non-textual modal-
ity, e.g. (Shriberg et al, 2000)).
There are many non-verbal modalities that may
contribute to face-to-face communication, includ-
ing body posture, hand gesture, facial expression,
prosody, and free-hand drawing. Hand gesture may
be more expressive than any non-verbal modality
besides drawing, since it serves as the foundation
for sign languages in hearing-disabled communi-
ties. While non-deaf speakers rarely use any such
systematized language as American Sign Language
(ASL) while gesturing, the existence of ASL speaks
to the potential of gesture for communicative expres-
sivity.
Hand gesture relates to spoken language in several
ways:
? Hand gesture communicates meaning. For ex-
ample, (Kopp et al, 2006) describe a model
of how hand gesture is used to convey spatial
properties of its referents when speakers give
navigational directions. This model both ex-
plains observed behavior of human speakers,
and serves as the basis for an implemented em-
bodied agent.
? Hand gesture communicates discourse struc-
ture. (Quek et al, 2002) and (McNeill, 1992)
describe how the structure of discourse is mir-
rored by the the structure of the gestures, when
speakers describe sequences of events in car-
toon narratives.
? Hand gesture segments in unison with speech,
suggesting possible applications to speech
recognition and syntactic processing. (Morrel-
Samuels and Krauss, 1992) show a strong cor-
relation between the onset and duration of ges-
tures, and their ?lexical affiliates? ? the phrase
that is thought to relate semantically to the ges-
ture. Also, (Chen et al, 2004) show that gesture
features may improve sentence segmentation.
These examples are a subset of a broad litera-
ture on gesture that suggests that this modality could
play an important role in improving the performance
of NLP systems on spontaneous spoken language.
However, the existence of significant relationships
between gesture and speech does not prove that
gesture will improve NLP; gesture features could
be redundant with existing textual features, or they
may be simply too noisy or speaker-dependant to be
useful. To test this, my thesis research will iden-
tify specific, objective NLP tasks, and attempt to
show that automatically-detected gestural features
improve performance beyond what is attainable us-
ing textual features.
The relationship between gesture and meaning is
particularly intriguing, since gesture seems to offer
a unique, spatial representation of meaning to sup-
215
plement verbal expression. However, the expression
of meaning through gesture is likely to be highly
variable and speaker dependent, as the set of pos-
sible mappings between meaning and gestural form
is large, if not infinite. For this reason, I take the
point of view that it is too difficult to attempt to de-
code individual gestures. A more feasible approach
is to identify similarities between pairs or groups
of gestures. If gestures do communicate semantics,
then similar gestures should predict semantic sim-
ilarity. Thus, gestures can help computers under-
stand speech by providing a set of ?back pointers?
between moments that are semantically related. Us-
ing this model, my dissertation will explore mea-
sures of gesture similarity and applications of ges-
ture similarity to NLP.
A set of semantic ?back pointers? decoded from
gestural features could be relevant to a number of
NLP benchmark problems. I will investigate two:
coreference resolution and disfluency detection. In
coreference resolution, we seek to identify whether
two noun phrases refer to the same semantic entity.
A similarity in the gestural features observed during
two different noun phrases might suggest a similar-
ity in meaning. This problem has the advantage of
permitting a quantitative evaluation of the relation-
ship between gesture and semantics, without requir-
ing the construction of a domain ontology.
Restarts are disfluencies that occur when a
speaker begins an utterance, and then stops and
starts over again. It is thought that the gesture
may return to its state at the beginning of the utter-
ance, providing a back-pointer to the restart inser-
tion point (Esposito et al, 2001). If so, then a similar
training procedure and set of gestural features can
be used for both coreference resolution and restart
correction. Both of these problems have objective,
quantifiable success measures, and both may play
an important role in bringing to spontaneous spoken
language useful NLP applications such as summa-
rization, segmentation, and question answering.
2 Current Status
My initial work involved hand annotation of ges-
ture, using the system proposed in (McNeill, 1992).
It was thought that hand annotation would identify
relevant features to be detected by computer vision
systems. However, in (Eisenstein and Davis, 2004),
we found that the gesture phrase type (e.g., deic-
tic, iconic, beat) could be predicted accurately by
lexical information alone, without regard to hand
movement. This suggests that this level of annota-
tion inherently captures a synthesis of gesture and
speech, rather than gesture alone. This conclusion
was strengthened by (Eisenstein and Davis, 2005),
where we found that hand-annotated gesture fea-
tures correlate well with sentence boundaries, but
that the gesture features were almost completely re-
dundant with information in the lexical features, and
did not improve overall performance.
The corpus used in my initial research was not
suitable for automatic extraction of gesture features
by computer vision, so a new corpus was gath-
ered, using a better-defined experimental protocol
and higher quality video and audio recording (Adler
et al, 2004). An articulated upper body tracker,
largely based on the work of (Deutscher et al, 2000),
was used to identify hand and arm positions, using
color and motion cues. All future work will be based
on this new corpus, which contains six videos each
from nine pairs of speakers. Each video is roughly
two to three minutes in length.
Each speaker was presented with three different
experimental conditions regarding how information
in the corpus was to be presented: a) a pre-printed
diagram was provided, b) the speaker was allowed
to draw a diagram using a tracked marker, c) no pre-
sentational aids were allowed. The first condition
was designed to be relevant to presentations involv-
ing pre-created presentation materials, such as Pow-
erpoint slides. The second condition was intended to
be similar to classroom lectures or design presenta-
tions. The third condition was aimed more at direct
one-on-one interaction.
My preliminary work has involved data from the
first condition, in which speakers gestured at pre-
printed diagrams. An empirical study on this part
of the corpus has identified several gesture features
that are relevant to coreference resolution (Eisen-
stein and Davis, 2006a). In particular, gesture sim-
ilarity can be measured by hand position and the
choice of the hand which makes the gesture; these
similarities correlate with the likelihood of coref-
erence. In addition, the likelihood of a gestural
hold ? where the hand rests in place for a period of
216
time ? acts as a meta-feature, indicating that gestural
cues are likely to be particularly important to disam-
biguate the meaning of the associated noun phrase.
In (Eisenstein and Davis, 2006b), these features are
combined with traditional textual features for coref-
erence resolution, with encouraging results. The
hand position gesture feature was found to be the
fifth most informative feature by Chi-squared anal-
ysis, and the inclusion of gesture features yielded a
statistically significant increase in performance over
the textual features.
3 Future Directions
The work on coreference can be considered prelimi-
nary, because it is focused on a subset of our corpus
in which speakers use pre-printed diagrams as an ex-
planatory aide. This changes their gestures (Eisen-
stein and Davis, 2003), increasing the proportion of
deictic gestures, in which hand position is the most
important feature (McNeill, 1992). Hand position
is assumed to be less useful in characterizing the
similarity of iconic gestures, which express meaning
through motion or handshape. Using the subsection
of the corpus in which no explanatory aids were pro-
vided, I will investigate how to assess the similarity
of such dynamic gestures, in the hope that corefer-
ence resolution can still benefit from gestural cues in
this more general case.
Disfluency repair is another plausible domain in
which gesture might improve performance. There
are at least two ways in which gesture could be rel-
evant to disfluency repair. Using the semantic back-
pointer model, restart repairs could be identified if
there is a strong gestural similarity between the orig-
inal start point and the restart. Alternatively, gesture
could play a pragmatic function, if there are char-
acteristic gestures that indicate restarts or other re-
pairs. In one case, we are looking for a similarity
between the disfluency and the repair point; in the
other case, we are looking for similarities across all
disfluencies, or across all repair points. It is hoped
that this research will not only improve processing
of spoken natural language, but also enhance our un-
derstanding of how speakers use gesture to structure
their discourse.
4 Related Work
The bulk of research on multimodality in the NLP
community relates to multimodal dialogue systems
(e.g., (Johnston and Bangalore, 2000)). This re-
search differs fundamentally from mine in that it ad-
dresses human-computer interaction, whereas I am
studying human-human interaction. Multimodal di-
alogue systems tackle many interesting challenges,
but the grammar, vocabulary, and recognized ges-
tures are often pre-specified, and dialogue is con-
trolled at least in part by the computer. In my data,
all of these things are unconstrained.
Another important area of research is the gen-
eration of multimodal communication in animated
agents (e.g., (Cassell et al, 2001; Kopp et al, 2006;
Nakano et al, 2003)). While the models devel-
oped in these papers are interesting and often well-
motivated by the psychological literature, it remains
to be seen whether they are both broad and precise
enough to apply to gesture recognition.
There is a substantial body of empirical work de-
scribing relationships between non-verbal and lin-
guistic phenomena, much of which suggests that
gesture could be used to improve the detection of
such phenomena. (Quek et al, 2002) describe ex-
amples in which gesture correlates with topic shifts
in the discourse structure, raising the possibility
that topic segmentation and summarization could be
aided by gesture features; Cassell et al (2001) make
a similar argument using body posture. (Nakano et
al., 2003) describes how head gestures and eye gaze
relate to turn taking and dialogue grounding. All
of the studies listed in this paragraph identify rel-
evant correlations between non-verbal communica-
tion and linguistic phenomena, but none construct a
predictive system that uses the non-verbal modali-
ties to improve performance beyond a text-only sys-
tem.
Prosody has been shown to improve performance
on several NLP problems, such as topic and sentence
segmentation (e.g., (Shriberg et al, 2000; Kim et
al., 2004)). The prosody literature demonstrates that
non-verbal features can improve performance on a
wide variety of NLP tasks. However, it also warns
that performance is often quite sensitive, both to the
representation of prosodic features, and how they are
integrated with other linguistic features.
217
The literature on prosody would suggest paral-
lels for gesture features, but little such work has
been reported. (Chen et al, 2004) shows that ges-
ture may improve sentence segmentation; however,
in this study, the improvement afforded by gesture is
not statistically significant, and evaluation was per-
formed on a subset of their original corpus that was
chosen to include only the three speakers who ges-
tured most frequently. Still, this work provides a
valuable starting point for the integration of gesture
feature into NLP systems.
5 Summary
Spontaneous spoken language poses difficult prob-
lems for natural language processing, but these diffi-
culties may be offset by the availability of additional
communicative modalities. Using a model of hand
gesture as providing a set of semantic back-pointers
to previous utterances, I am exploring whether ges-
ture can improve performance on quantitative NLP
benchmark tasks. Preliminary results on coreference
resolution are encouraging.
References
Aaron Adler, Jacob Eisenstein, Michael Oltmans, Lisa
Guttentag, and Randall Davis. 2004. Building the de-
sign studio of the future. In Making Pen-Based Inter-
action Intelligent and Natural, pages 1?7, Menlo Park,
California, October 21-24. AAAI Press.
Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-
more, Candace L. Sidner, and Charles Rich. 2001.
Non-verbal cues for discourse structure. In Proc. of
ACL, pages 106?115.
Lei Chen, Yang Liu, Mary P. Harper, and Eliza-
beth Shriberg. 2004. Multimodal model integra-
tion for sentence unit detection. In Proceedings of
International Conference on Multimodal Interfaces
(ICMI?04). ACM Press.
Jonathan Deutscher, Andrew Blake, and Ian Reid. 2000.
Articulated body motion capture by annealed particle
filtering. In IEEE Conference on Computer Vision and
Pattern Recognition, volume 2, pages 126?133.
Jacob Eisenstein and Randall Davis. 2003. Natural ges-
ture in descriptive monologues. In UIST?03 Supple-
mental Proceedings, pages 69?70. ACM Press.
Jacob Eisenstein and Randall Davis. 2004. Visual and
linguistic information in gesture classification. In Pro-
ceedings of International Conference on Multimodal
Interfaces(ICMI?04). ACM Press.
Jacob Eisenstein and Randall Davis. 2005. Gestural cues
for sentence segmentation. Technical Report AIM-
2005-014, MIT AI Memo.
Jacob Eisenstein and Randall Davis. 2006a. Gesture fea-
tures for coreference resolution. In Workshop on Mul-
timodal Interaction and Related Machine Learning Al-
gorithms.
Jacob Eisenstein and Randall Davis. 2006b. Gesture
improves coreference resolution. In Proceedings of
NAACL.
Anna Esposito, Karl E. McCullough, and Francis Quek.
2001. Disfluencies in gesture: Gestural correlates to
filled and unfilled speech pauses. In Proceedings of
IEEE Workshop on Cues in Communication.
Michael Johnston and Srinivas Bangalore. 2000. Finite-
state multimodal parsing and understanding,. In Pro-
ceedings of COLING-2000, pages 369?375.
Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf.
2004. Detecting structural metadata with decision
trees and transformation-based learning. In Proceed-
ings of HLT-NAACL?04. ACL Press.
Stefan Kopp, Paul Tepper, Kim Ferriman, and Justine
Cassell. 2006. Trading spaces: How humans and
humanoids use speech and gesture to give directions.
Spatial Cognition and Computation, In preparation.
David McNeill. 1992. Hand and Mind. The University
of Chicago Press.
P. Morrel-Samuels and R. M. Krauss. 1992. Word fa-
miliarity predicts temporal asynchrony of hand ges-
tures and speech. Journal of Experimental Psychol-
ogy: Learning, Memory and Cognition, 18:615?623.
Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-
tine Cassell. 2003. Towards a model of face-to-face
grounding. In Proceedings of ACL?03.
Francis Quek, David McNeill, Robert Bryll, Susan Dun-
can, Xin-Feng Ma, Cemil Kirbas, Karl E. McCul-
lough, and Rashid Ansari. 2002. Multimodal human
discourse: gesture and speech. ACM Transactions on
Computer-Human Interaction (TOCHI), pages 171?
193.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur,
and Gokhan Tur. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32.
218
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 83?91,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Adding More Languages Improves Unsupervised Multilingual
Part-of-Speech Tagging: A Bayesian Non-Parametric Approach
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{bsnyder, tahira, jacobe, regina}@csail.mit.edu
Abstract
We investigate the problem of unsupervised
part-of-speech tagging when raw parallel data
is available in a large number of languages.
Patterns of ambiguity vary greatly across lan-
guages and therefore even unannotated multi-
lingual data can serve as a learning signal. We
propose a non-parametric Bayesian model that
connects related tagging decisions across lan-
guages through the use of multilingual latent
variables. Our experiments show that perfor-
mance improves steadily as the number of lan-
guages increases.
1 Introduction
In this paper we investigate the problem of unsu-
pervised part-of-speech tagging when unannotated
parallel data is available in a large number of lan-
guages. Our goal is to develop a fully joint multilin-
gual model that scales well and shows improved per-
formance for individual languages as the total num-
ber of languages increases.
Languages exhibit ambiguity at multiple levels,
making unsupervised induction of their underlying
structure a difficult task. However, sources of lin-
guistic ambiguity vary across languages. For exam-
ple, the word fish in English can be used as either a
verb or a noun. In French, however, the noun pois-
son (fish) is entirely distinct from the verbal form
pe?cher (to fish). Previous work has leveraged this
idea by building models for unsupervised learning
from aligned bilingual data (Snyder et al, 2008).
However, aligned data is often available for many
languages. The benefits of bilingual learning vary
markedly depending on which pair of languages is
selected, and without labeled data it is unclear how
to determine which supplementary language is most
helpful. In this paper, we show that it is possi-
ble to leverage all aligned languages simultaneously,
achieving accuracy that in most cases outperforms
even optimally chosen bilingual pairings.
Even in expressing the same meaning, languages
take different syntactic routes, leading to variation
in part-of-speech sequences. Therefore, an effec-
tive multilingual model must accurately model com-
mon linguistic structure, yet remain flexible to the
idiosyncrasies of each language. This tension only
becomes stronger as additional languages are added
to the mix. From a computational standpoint, the
main challenge is to ensure that the model scales
well as the number of languages increases. Care
must be taken to avoid an exponential increase in
the parameter space as well as the time complexity
of inference procedure.
We propose a non-parametric Bayesian model for
joint multilingual tagging. The topology of our
model connects tagging decisions within a language
as well as across languages. The model scales lin-
early with the number of languages, allowing us to
incorporate as many as are available. For each lan-
guage, the model contains an HMM-like substruc-
ture and connects these substructures to one another
by means of cross-lingual latent variables. These
variables, which we refer to as superlingual tags,
capture repeated multilingual patterns and thus re-
duce the overall uncertainty in tagging decisions.
We evaluate our model on a parallel corpus of
eight languages. The model is trained once using all
83
languages, and its performance is tested separately
for each on a held-out monolingual test set. When a
complete tag lexicon is provided, our unsupervised
model achieves an average accuracy of 95%, in com-
parison to 91% for an unsupervised monolingual
Bayesian HMM and 97.4% for its supervised coun-
terpart. Thus, on average, the gap between unsu-
pervised and supervised monolingual performance
is cut by nearly two thirds. We also examined sce-
narios where the tag lexicon is reduced in size. In
all cases, the multilingual model yielded substantial
performance gains. Finally, we examined the per-
formance of our model when trained on all possible
subsets of the eight languages. We found that perfor-
mance improves steadily as the number of available
languages increases.
2 Related Work
Bilingual Part-of-Speech Tagging Early work on
multilingual tagging focused on projecting annota-
tions from an annotated source language to a target
language (Yarowsky and Ngai, 2001; Feldman et al,
2006). In contrast, we assume no labeled data at
all; our unsupervised model instead symmetrically
improves performance for all languages by learning
cross-lingual patterns in raw parallel data. An addi-
tional distinction is that projection-based work uti-
lizes pairs of languages, while our approach allows
for continuous improvement as languages are added
to the mix.
In recent work, Snyder et al (2008) presented
a model for unsupervised part-of-speech tagging
trained from a bilingual parallel corpus. This bilin-
gual model and the model presented here share a
number of similarities: both are Bayesian graphi-
cal models building upon hidden Markov models.
However, the bilingual model explicitly joins each
aligned word-pair into a single coupled state. Thus,
the state-space of these joined nodes grows exponen-
tially in the number of languages. In addition, cross-
ing alignments must be removed so that the result-
ing graph structure remains acyclic. In contrast, our
multilingual model posits latent cross-lingual tags
without explicitly joining or directly connecting the
part-of-speech tags across languages. Besides per-
mitting crossing alignments, this structure allows the
model to scale gracefully with the number of lan-
guages.
Beyond Bilingual Learning While most work on
multilingual learning focuses on bilingual analysis,
some models operate on more than one pair of lan-
guages. For instance, Genzel (2005) describes a
method for inducing a multilingual lexicon from
a group of related languages. His model first in-
duces bilingual models for each pair of languages
and then combines them. Our work takes a different
approach by simultaneously learning from all lan-
guages, rather than combining bilingual results.
A related thread of research is multi-source ma-
chine translation (Och and Ney, 2001; Utiyama and
Isahara, 2006; Cohn and Lapata, 2007) where the
goal is to translate from multiple source languages to
a single target language. Rather than jointly training
all the languages together, these models train bilin-
gual models separately, and then use their output to
select a final translation. The selection criterion can
be learned at training time since these models have
access to the correct translation. In unsupervised set-
tings, however, we do not have a principled means
for selecting among outputs of different bilingual
models. By developing a joint multilingual model
we can automatically achieve performance that ri-
vals that of the best bilingual pairings.
3 Model
We propose a non-parametric directed Bayesian
graphical model for multilingual part-of-speech tag-
ging using a parallel corpus. We perform a joint
training pass over the corpus, and then apply the
parameters learned for each language to a held-out
monolingual test set.
The core idea of our model is that patterns of
ambiguity vary across languages and therefore even
unannotated multilingual data can serve as a learn-
ing signal. Our model is able to simultaneously har-
ness this signal from all languages present in the
corpus. This goal is achieved by designing a sin-
gle graphical model that connects tagging decisions
within a language as well as across languages.
The model contains language-specific HMM sub-
structures connected to one another by cross-lingual
latent variables spanning two or more languages.
These variables, which we refer to as superlingual
tags, capture repeated cross-lingual patterns and
84
I l o v e f i s h J ? a d o r e l e s p o i s s o n
a n i o h e v d a g i m M u j h e m a c h c h l i p a s a n d h a i
Figure 1: Model structure for parallel sentences in English, French, Hebrew, and Urdu. In this example, there are three
superlingual tags, each connected to the part-of-speech tag of a word in each of the four languages.
thus reduce the overall uncertainty in tagging deci-
sions. To encourage the discovery of a compact set
of such cross-lingual patterns, we place a Dirichlet
process prior on the superlingual tag values.
3.1 Model Structure
For each language, our model includes an HMM-
like substructure with observed word nodes, hid-
den part-of-speech nodes, and directed transition
and emission edges. For each set of aligned words
in parallel sentences, we add a latent superlingual
variable to capture the cross-lingual context. A set
of directed edges connect this variable to the part-
of-speech nodes of the aligned words. Our model
assumes that the superlingual tags for parallel sen-
tences are unordered and are drawn independently
of one another.
Edges radiate outward from superlingual tags to
language-specific part-of-speech nodes. Thus, our
model implicitly assumes that superlingual tags are
drawn prior to the part-of-speech tags of all lan-
guages and probabilistically influence their selec-
tion. See Figure 1 for an example structure.
The particular model structure for each set of par-
allel sentences (i.e. the configuration of superlingual
tags and their edges) is determined by bilingual lexi-
cal alignments and ? like the text itself ? is consid-
ered an observed variable. In practice, these lexical
alignments are obtained using standard techniques
from machine translation.
Our model design has several benefits. Crossing
and many-to-many alignments may be used with-
out creating cycles in the graph, as all cross-lingual
information emanates from the hidden superlingual
tags. Furthermore, the model scales gracefully with
the number of languages, as the number of new
edges and nodes will be proportional to the number
of words for each additional language.
3.2 Superlingual Tags
Each superlingual tag value specifies a set of dis-
tributions ? one for each language?s part-of-speech
tagset. In order to learn repeated cross-lingual pat-
terns, we need to constrain the number of superlin-
gual tag values and thus the number of distributions
they provide. For example, we might allow the su-
perlingual tags to take on integer values from 1 to
K, with each integer value indexing a separate set
of distributions. Each set of distributions should cor-
respond to a discovered cross-lingual pattern in the
data. For example, one set of distributions might fa-
vor nouns in each language and another might favor
verbs.
Rather than fixing the number of superlingual
tag values to an arbitrary and predetermined size
1, . . . ,K, we allow them to range over the entire set
of integers. In order to encourage the desired multi-
lingual clustering behavior, we use a Dirichlet pro-
cess prior for the superlingual tags. This prior allows
high posterior probability only when a small number
85
of values are used repeatedly. The actual number of
sampled values will be dictated by the data and the
number of languages.
More formally, suppose we have n lan-
guages, ?1, . . . , ?n. According to our genera-
tive model, a countably infinite sequence of sets
???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ?, . . . is drawn from
some base distribution. Each ??i is a distribution
over the parts-of-speech in language ?.
In parallel, an infinite sequence of mixing compo-
nents ?1, ?2, . . . is drawn from a stick-breaking pro-
cess (Sethuraman, 1994). These components define
a distribution over the integers with most probabil-
ity mass placed on some initial set of values. The
two sequences ???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ? . . .
and ?1, ?2 . . . now define the distribution over su-
perlingual tags and their associated distributions on
parts-of-speech. That is, each superlingual tag z ?
N is drawn with probability ?z , and indexes the set
of distributions ???1z , . . . , ??nz ?.
3.3 Part-of-Speech Tags
Finally, we need to define the generative probabili-
ties of the part-of-speech nodes. For each such node
there may be multiple incoming edges. There will
always be an incoming transition edge from the pre-
vious tag (in the same language). In addition, there
may be incoming edges from zero or more superlin-
gual tags. Each edge carries with it a distribution
over parts-of-speech and these distributions must be
combined into the single distribution from which the
tag is ultimately drawn.
We choose to combine these distributions as a
product of experts. More formally: for language ?
and tag position i, the part-of-speech tag yi is drawn
according to
yi ?
?yi?1(yi)
?
z ??z(yi)
Z (1)
Where ?yi?1 indicates the transition distribution,
and the z?s range over the values of the incoming
superlingual tags. The normalization term Z is ob-
tained by summing the numerator over all part-of-
speech tags yi in the tagset.
This parameterization allows for a relatively sim-
ple and small parameter space. It also leads to a
desirable property: for a tag to have high probabil-
ity each of the incoming distributions must allow it.
That is, any expert can ?veto? a potential tag by as-
signing it low probability, generally leading to con-
sensus decisions.
We now formalize this description by giving the
stochastic process by which the observed data (raw
parallel text) is generated, according to our model.
3.4 Generative Process
For n languages, we assume the existence of n
tagsets T 1, . . . , Tn and vocabularies, W 1, . . . ,Wn,
one for each language. For clarity, the generative
process is described using only bigram transition
dependencies, but our experiments use a trigram
model.
1. Transition and Emission Parameters: For
each language ? and for each tag t ? T ?, draw
a transition distribution ??t over tags T? and
an emission distribution ??t over words W ?, all
from symmetric Dirichlet priors of appropriate
dimension.
2. Superlingual Tag Parameters:
Draw an infinite sequence of sets
???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ?, . . . from
base distribution G0. Each ??i is a distribution
over the tagset T ?. The base distribution G0 is
a product of n symmetric Dirichlets, where the
dimension of the ith such Dirichlet is the size
of the corresponding tagset T ?i .
At the same time, draw an infinite sequence
of mixture weights ? ? GEM(?), where
GEM(?) indicates the stick-breaking distribu-
tion (Sethuraman, 1994), and ? = 1. These
parameters together define a prior distribution
over superlingual tags,
p(z) =
??
k
?k?k=z, (2)
or equivalently over the part-of-speech distri-
butions ???1 , . . . , ??n? that they index:
??
k
?k????1k ,...,??nk ?=???1 ,...,??n ?. (3)
In both cases, ?v=v? is defined as one when
v = v? and zero otherwise. Distribution 3 is
said to be drawn from a Dirichlet process, con-
ventionally written as DP (?,G0).
86
3. Data: For each multilingual parallel sentence,
(a) Draw an alignment a specifying sets of
aligned indices across languages. Each
such set may consist of indices in any sub-
set of the languages. We leave the distri-
bution over alignments undefined, as we
consider alignments observed variables.
(b) For each set of indices in a, draw a super-
lingual tag value z according to Distribu-
tion 2.
(c) For each language ?, for i = 1, . . . (until
end-tag reached):
i. Draw a part-of-speech tag yi ? T ? ac-
cording to Distribution 1
ii. Draw a word wi ? W ? according to
the emission distribution ?yi .
To perform Bayesian inference under this model
we use a combination of sampling techniques, which
we describe in detail in the next section.
3.5 Inference
Ideally we would like to predict the part-of-speech
tags which have highest marginal probability given
the observed words x and alignments a. More
specifically, since we are evaluating our accuracy per
tag-position, we would like to predict, for language
index ? and word index i, the single part-of-speech
tag:
argmax
t?T ?
P
(
y?i = t
??x,a)
which we can rewrite as the argmaxt?T ? of the inte-
gral,
? [
P
(
y?i = t
???y?(?,i),?,?, z,?,x,a
)
?
P
(
y?(?,i),?,?, z,pi,?,
???x,a
)]
dy?(?,i) d? d? dz dpi d?,
in which we marginalize over the settings of all
tags other than y?i (written as y?(?,i)), the tran-
sition distributions ? = {??t?
}
, emission distri-
butions ? = {??t?
}
, superlingual tags z, and su-
perlingual tag parameters pi = {?1, ?2, . . .} and
? = {???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ? . . .} (where t?
ranges over all part-of-speech tags).
As these integrals are intractable to compute ex-
actly, we resort to the standard Monte Carlo approx-
imation. We collect N samples of the variables over
which we wish to marginalize but for which we can-
not compute closed-form integrals, where each sam-
ple samplek is drawn from P (samplek|x,a). We
then approximate the tag marginals as:
P
(
y?i = t
??x,a) ?
?
k P
(
y?i = t
??samplek,x,a
)
N (4)
We employ closed forms for integrating out the
emission parameters ?, transition parameters ?, and
superlingual tag parameters pi and ?. We explic-
itly sample only part-of-speech tags y, superlingual
tags z, and the hyperparameters of the transition and
emission Dirichlet priors. To do so, we apply stan-
dard Markov chain sampling techniques: a Gibbs
sampler for the tags and a within-Gibbs Metropolis-
Hastings subroutine for the hyperparameters (Hast-
ings, 1970).
Our Gibbs sampler samples each part-of-speech
and superlingual tag separately, conditioned on the
current value of all other tags. In each case, we use
standard closed forms to integrate over all parameter
values, using currently sampled counts and hyperpa-
rameter pseudo-counts. We note that conjugacy is
technically broken by our use of a product form in
Distribution 1. Nevertheless, we consider the sam-
pled tags to have been generated separately by each
of the factors involved in the numerator. Thus our
method of using count-based closed forms should be
viewed as an approximation.
3.6 Sampling Part-of-Speech Tags
To sample the part-of-speech tag for language ? at
position i we draw from
P (y?i |y?(?,i),x,a, z) ?
P (y?i+1|y?i ,y?(?,i),a, z) P (y?i |y?(?,i),a, z)?
P (x?i |x??i,y?) ,
where the first two terms are the generative proba-
bilities of (i) the current tag given the previous tag
and superlingual tags, and (ii) the next tag given the
current tag and superlingual tags. These two quan-
tities are similar to Distribution 1, except here we
integrate over the transition parameter ?yi?1 and the
superlingual tag parameters ??z . We end up with a
product of integrals. Each integral can be computed
in closed form using multinomial-Dirichlet conju-
gacy (and by making the above-mentioned simpli-
fying assumption that all other tags were gener-
ated separately by their transition and superlingual
87
parameters), just as in the monolingual Bayesian
HMM of (Goldwater and Griffiths, 2007).
For example, the closed form for integrating over
the parameter of a superlingual tag with value z is
given by:
?
??z(yi)P (??z|?0)d??z = count(z, yi, ?) + ?0count(z, ?) + T ??0
where count(z, yi, ?) is the number of times that tag
yi is observed together with superlingual tag z in
language ?, count(z, ?) is the total number of times
that superlingual tag z appears with an edge into lan-
guage ?, and ?0 is a hyperparameter.
The third term in the sampling formula is the
emission probability of the current word x?i given
the current tag and all other words and sampled tags,
as well as a hyperparameter which is suppressed for
the sake of clarity. This quantity can be computed
exactly in closed form in a similar way.
3.7 Sampling Superlingual Tags
For each set of aligned words in the observed align-
ment a we need to sample a superlingual tag z.
Recall that z is an index into an infinite sequence
???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ? . . ., where each ??z is
a distribution over the tagset T ?. The generative dis-
tribution over z is given by equation 2. In our sam-
pling scheme, however, we integrate over all possi-
ble settings of the mixing components pi using the
standard Chinese Restaurant Process (CRP) closed
form (Antoniak, 1974):
P
(
zi
??z?i,y
)
?
?
?
P
(
y?i
??z,y?(?,i)
)
?
{
1
k+?count(zi) if zi ? z?i
?
k+? otherwise
The first term is the product of closed form tag prob-
abilities of the aligned words, given z. The final term
is the standard CRP closed form for posterior sam-
pling from a Dirichlet process prior. In this term,
k is the total number of sampled superlingual tags,
count(zi) is the total number of times the value zi
occurs in the sampled tags, and ? is the Dirichlet
process concentration parameter (see Step 2 in Sec-
tion 3.4).
Finally, we perform standard hyperparameter re-
estimation for the parameters of the Dirichlet distri-
bution priors on ? and ? (the transition and emis-
sion distributions) using Metropolis-Hastings. We
assume an improper uniform prior and use a Gaus-
sian proposal distribution with mean set to the pre-
vious value, and variance to one-tenth of the mean.
4 Experimental Setup
We test our model in an unsupervised framework
where only raw parallel text is available for each
of the languages. In addition, we assume that for
each language a tag dictionary is available that cov-
ers some subset of words in the text. The task is to
learn an independent tagger for each language that
can annotate non-parallel raw text using the learned
parameters. All reported results are on non-parallel
monolingual test data.
Data For our experiments we use the Multext-
East parallel corpus (Erjavec, 2004) which has been
used before for multilingual learning (Feldman et
al., 2006; Snyder et al, 2008). The tagged portion of
the corpus includes a 100,000 word English text, Or-
well?s novel ?Nineteen Eighty Four?, and its trans-
lation into seven languages: Bulgarian, Czech, Es-
tonian, Hungarian, Romanian, Slovene and Serbian.
The corpus also includes a tag lexicon for each of
these languages. We use the first 3/4 of the text for
learning and the last 1/4 as held-out non-parallel test
data.
The corpus provides sentence level alignments.
To obtain word level alignments, we run GIZA++
(Och and Ney, 2003) on all 28 pairings of the 8 lan-
guages. Since we want each latent superlingual vari-
able to span as many languages as possible, we ag-
gregate the pairwise lexical alignments into larger
sets of aligned words. These sets of aligned words
are generated as a preprocessing step. During sam-
pling they remain fixed and are treated as observed
data.
We use the set of 14 basic part-of-speech tags pro-
vided by the corpus. In our first experiment, we
assume that a complete tag lexicon is available, so
that for each word, its set of possible parts-of-speech
is known ahead of time. In this setting, the aver-
age number of possible tags per token is 1.39. We
also experimented with incomplete tag dictionaries,
where entries are only available for words appearing
more than five or ten times in the corpus. For other
words, the entire tagset of 14 tags is considered. In
these two scenarios, the average per-token tag ambi-
88
Lexicon: Full Lexicon: Frequency > 5 Lexicon: Frequency > 10
MONO
BI
MULTI MONO
BI
MULTI MONO
BI
MULTI
AVG BEST AVG BEST AVG BEST
BG 88.8 91.3 94.7 92.6 73.5 80.2 82.7 81.3 71.9 77.8 80.2 78.8
CS 93.7 97.0 97.7 98.2 72.2 79.0 79.7 83.0 66.7 75.3 76.7 79.4
EN 95.8 95.9 96.1 95.0 87.3 90.4 90.7 88.1 84.4 88.8 89.4 86.1
ET 92.5 93.4 94.3 94.6 72.5 76.5 77.5 80.6 68.3 72.9 74.9 77.9
HU 95.3 96.8 96.9 96.7 73.5 77.3 78.0 80.8 69.0 73.8 75.2 76.4
RO 90.1 91.8 94.0 95.1 77.1 82.7 84.4 86.1 73.0 80.5 82.1 83.1
SL 87.4 89.3 94.8 95.8 75.7 78.7 80.9 83.6 70.4 76.1 77.6 80.0
SR 84.5 90.2 94.5 92.3 66.3 75.9 79.4 78.8 63.7 72.4 76.1 75.9
Avg. 91.0 93.2 95.4 95.0 74.7 80.1 81.7 82.8 70.9 77.2 79.0 79.7
Table 1: Tagging accuracy for Bulgarian, Czech, English, Estonian, Hungarian, Romanian, Slovene, and Serbian. In
the first scenario, a complete tag lexicon is available for all the words. In the other two scenarios the tag lexicon
only includes words that appear more than five or ten times. Results are given for a monolingual Bayesian HMM
(Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here.
In the case of the bilingual model, we present both the average accuracy over all pairings as well as the result from the
best performing pairing for each language. The best results for each language in each scenario are given in boldface.
guity is 4.65 and 5.58, respectively.
Training and testing In the full lexicon ex-
periment, each word is initialized with a random
part-of-speech tag from its dictionary entry. In the
two reduced lexicon experiments, we initialize the
tags with the result of our monolingual baseline (see
below) to reduce sampling time. In both cases,
we begin with 14 superlingual tag values ? corre-
sponding to the parts-of-speech ? and initially as-
sign them based on the most common initial part-of-
speech of words in each alignment.
We run our Gibbs sampler for 1,000 iterations,
and store the conditional tag probabilities for the last
100 iterations. We then approximate marginal tag
probabilities on the training data using Equation 4
and predict the highest probability tags. Finally, we
compute maximum likelihood transition and emis-
sion probabilities using these tag counts, and then
apply smoothed viterbi decoding to each held-out
monolingual test set. All reported results are aver-
aged over five runs of the sampler.
Monolingual and bilingual baselines We
reimplemented the Bayesian HMM model of Gold-
water and Griffiths (2007) (BHMM1) as our mono-
lingual baseline. It has a standard HMM structure
with conjugate Bayesian priors over transitions and
emissions. We note that our model, in the absence
of any superlingual tags, reduces to this Bayesian
HMM. As an additional baseline we use a bilingual
model (Snyder et al, 2008). It is a directed graphical
model that jointly tags two parallel streams of text
aligned at the word level. The structure of the model
consists of two parallel HMMs, one for each lan-
guage. The aligned words form joint nodes that are
shared by both HMMs. These joint nodes are sam-
pled from a probability distribution that is a prod-
uct of the transition and emission distributions in the
two languages and a coupling distribution.
We note that the numbers reported here for
the bilingual model differ slightly from those re-
ported by Snyder et al (2008) for two reasons: we
use a slightly larger set of sentences, and an im-
proved sampling scheme. The new sampling scheme
marginalizes over the transition and coupling param-
eters by using the same count-based approximation
that we utilize for our multilingual model. This leads
to higher performance, and thus a stronger baseline.1
5 Results
Table 1 shows the tagging accuracy of our multilin-
gual model on the test data, when training is per-
formed on all eight languages together. Results from
both baselines are also reported. In the case of the
bilingual baseline, seven pairings are possible for
each language, and the results vary by pair. There-
1Another difference is that we use the English lexicon pro-
vided with the Multext-East corpus, whereas (Snyder et al,
2008) augment this lexicon with tags found in WSJ.
89
fore, for each language, we present the average accu-
racy over all seven pairings, as well as the accuracy
of its highest performing pairing.
We provide results for three scenarios. In the first
case, a tag dictionary is provided for all words, lim-
iting them to a restricted set of possible tags. In the
other two scenarios, dictionary entries are limited to
words that appear more than five or ten times in the
corpus. All other words can be assigned any tag,
increasing the overall difficulty of the task. In the
full lexicon scenario, our model achieves an average
tagging accuracy of 95%, compared to 91% for the
monolingual baseline and 93.2% for the bilingual
baseline when averaged over all pairings. This ac-
curacy (95%) comes close to the performance of the
bilingual model when the best pairing for each lan-
guage is chosen by an oracle (95.4%). This demon-
strates that our multilingual model is able to effec-
tively learn from all languages. In the two reduced
lexicon scenarios, the gains are even more striking.
In both cases the average multilingual performance
outpaces even the best performing pairs.
Looking at individual languages, we see that in
all three scenarios, Czech, Estonian, Romanian, and
Slovene show their best performance with the mul-
tilingual model. Bulgarian and Serbian, on the
other hand, give somewhat better performance with
their optimal pairings under the bilingual model, but
their multilingual performance remains higher than
their average bilingual results. The performance of
English under the multilingual model is somewhat
lower, especially in the full lexicon scenario, where
it drops below monolingual performance. One pos-
sible explanation for this decrease lies in the fact that
English, by far, has the lowest trigram tag entropy of
all eight languages (Snyder et al, 2008). It is pos-
sible, therefore, that the signal it should be getting
from its own transitions is being drowned out by less
reliable information from other languages.
In order to test the performance of our model as
the number of languages increases, we ran the full
lexicon experiment with all possible subsets of the
eight languages. Figure 2 plots the average accuracy
as the number of languages varies. For comparison,
the monolingual and average bilingual baseline re-
sults are given, along with supervised monolingual
performance. Our multilingual model steadily gains
in accuracy as the number of available languages in-
Figure 2: Performance of the multilingual model as the
number of languages varies. Performance of the mono-
lingual and average bilingual baselines as well as a su-
pervised monolingual performance are given for compar-
ison.
creases. Interestingly, it even outperforms the bilin-
gual baseline (by a small margin) when only two lan-
guages are available, which may be attributable to
the more flexible non-parametric dependencies em-
ployed here. Finally, notice that the gap between
monolingual supervised and unsupervised perfor-
mance is cut by nearly two thirds under the unsu-
pervised multilingual model.
6 Conclusion
In this paper we?ve demonstrated that the benefits of
unsupervised multilingual learning increase steadily
with the number of available languages. Our model
scales gracefully as languages are added and effec-
tively incorporates information from them all, lead-
ing to substantial performance gains. In one experi-
ment, we cut the gap between unsupervised and su-
pervised performance by nearly two thirds. A fu-
ture challenge lies in incorporating constraints from
additional languages even when parallel text is un-
available.
Acknowledgments
The authors acknowledge the support of the National Sci-
ence Foundation (CAREER grant IIS-0448168 and grant IIS-
0835445). Thanks to Tommi Jaakkola and members of the
MIT NLP group for helpful discussions. Any opinions, find-
ings, or recommendations expressed above are those of the au-
thors and do not necessarily reflect the views of the NSF.
90
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. The Annals of Statistics, 2:1152?1174, Novem-
ber.
Trevor Cohn and Mirella Lapata. 2007. Machine trans-
lation by triangulation: Making effective use of multi-
parallel corpora. In Proceedings of ACL.
T. Erjavec. 2004. MULTEXT-East version 3: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Fourth International Conference on Lan-
guage Resources and Evaluation, LREC, volume 4,
pages 1535?1538.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of new
morpho-syntactically annotated resources. In Pro-
ceedings of LREC, pages 549?554.
Dmitriy Genzel. 2005. Inducing a multilingual dictio-
nary from a parallel multitext in related languages. In
Proceedings of the HLT/EMNLP, pages 875?882.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
W. K. Hastings. 1970. Monte Carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57:97?109.
Franz Josef Och and Hermann Ney. 2001. Statistical
multi-source translation. In MT Summit 2001, pages
253?258.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In Proceedings of the
EMNLP, pages 1041?1050.
Masao Utiyama and Hitoshi Isahara. 2006. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Proceedings of NAACL/HLT,
pages 484?491.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1?8.
91
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 352?359,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Conditional Modality Fusion for Coreference Resolution
Jacob Eisenstein and Randall Davis
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139 USA
{jacobe,davis}@csail.mit.edu
Abstract
Non-verbal modalities such as gesture can
improve processing of spontaneous spoken
language. For example, similar hand ges-
tures tend to predict semantic similarity, so
features that quantify gestural similarity can
improve semantic tasks such as coreference
resolution. However, not all hand move-
ments are informative gestures; psycholog-
ical research has shown that speakers are
more likely to gesture meaningfully when
their speech is ambiguous. Ideally, one
would attend to gesture only in such cir-
cumstances, and ignore other hand move-
ments. We present conditional modality
fusion, which formalizes this intuition by
treating the informativeness of gesture as a
hidden variable to be learned jointly with
the class label. Applied to coreference
resolution, conditional modality fusion sig-
nificantly outperforms both early and late
modality fusion, which are current tech-
niques for modality combination.
1 Introduction
Non-verbal modalities such as gesture and prosody
can increase the robustness of NLP systems to the
inevitable disfluency of spontaneous speech. For ex-
ample, consider the following excerpt from a dia-
logue in which the speaker describes a mechanical
device:
?So this moves up, and it ? everything moves up.
And this top one clears this area here, and goes all
the way up to the top.?
The references in this passage are difficult to
disambiguate, but the gestures shown in Figure 1
make the meaning more clear. However, non-verbal
modalities are often noisy, and their interactions
with speech are complex (McNeill, 1992). Ges-
ture, for example, is sometimes communicative, but
other times merely distracting. While people have
little difficulty distinguishing between meaningful
gestures and irrelevant hand motions (e.g., self-
touching, adjusting glasses) (Goodwin and Good-
win, 1986), NLP systems may be confused by such
seemingly random movements. Our goal is to in-
clude non-verbal features only in the specific cases
when they are helpful and necessary.
We present a model that learns in an unsupervised
fashion when non-verbal features are useful, allow-
ing it to gate the contribution of those features. The
relevance of the non-verbal features is treated as a
hidden variable, which is learned jointly with the
class label in a conditional model. We demonstrate
that this improves performance on binary corefer-
ence resolution, the task of determining whether a
noun phrases refers to a single semantic entity. Con-
ditional modality fusion yields a relative increase of
73% in the contribution of hand-gesture features.
The model is not specifically tailored to gesture-
speech integration, and may also be applicable to
other non-verbal modalities.
2 Related work
Most of the existing work on integrating non-verbal
features relates to prosody. For example, Shriberg
et al (2000) explore the use of prosodic features for
sentence and topic segmentation. The first modal-
352
And this top one clears this area here, and goes
all the way up to the top...
2
So this moves up. And it ? everything moves up.
1
Figure 1: An example where gesture helps to disambiguate meaning.
ity combination technique that they consider trains a
single classifier with all modalities combined into a
single feature vector; this is sometimes called ?early
fusion.? Shriberg et al also consider training sepa-
rate classifiers and combining their posteriors, either
through weighted addition or multiplication; this is
sometimes called ?late fusion.? Late fusion is also
employed for gesture-speech combination in (Chen
et al, 2004). Experiments in both (Shriberg et al,
2000) and (Kim et al, 2004) find no conclusive win-
ner among early fusion, additive late fusion, and
multiplicative late fusion.
Toyama and Horvitz (2000) introduce a Bayesian
network approach to modality combination for
speaker identification. As in late fusion, modality-
specific classifiers are trained independently. How-
ever, the Bayesian approach also learns to predict
the reliability of each modality on a given instance,
and incorporates this information into the Bayes
net. While more flexible than the interpolation tech-
niques described in (Shriberg et al, 2000), training
modality-specific classifiers separately is still sub-
optimal compared to training them jointly, because
independent training of the modality-specific classi-
fiers forces them to account for data that they can-
not possibly explain. For example, if the speaker is
not gesturing meaningfully, it is counterproductive
to train a gesture-modality classifier on the features
at this instant; doing so can lead to overfitting and
poor generalization.
Our approach combines aspects of both early and
late fusion. As in early fusion, classifiers for all
modalities are trained jointly. But as in Toyama and
Horvitz?s Bayesian late fusion model, modalities can
be weighted based on their predictive power for spe-
cific instances. In addition, our model is trained to
maximize conditional likelihood, rather than joint
likelihood.
3 Conditional modality fusion
The goal of our approach is to learn to weight the
non-verbal features xnv only when they are rele-
vant. To do this, we introduce a hidden variable
m ? {?1, 1}, which governs whether the non-
verbal features are included. p(m) is conditioned on
a subset of features xm, which may belong to any
modality; p(m|xm) is learned jointly with the class
label p(y|x), with y ? {?1, 1}. For our coreference
resolution model, y corresponds to whether a given
pair of noun phrases refers to the same entity.
In a log-linear model, parameterized by weights
w, we have:
p(y|x;w) =
?
m
p(y,m|x;w)
=
?
m exp(?(y,m,x;w))?
y?,m exp(?(y
?,m,x;w))
.
Here, ? is a potential function representing the
compatibility between the label y, the hidden vari-
able m, and the observations x; this potential is pa-
rameterized by a vector of weights, w. The numera-
tor expresses the compatibility of the label y and ob-
servations x, summed over all possible values of the
hidden variablem. The denominator sums over both
m and all possible labels y?, yielding the conditional
probability p(y|x;w). The use of hidden variables
353
in a conditionally-trained model follows (Quattoni
et al, 2004).
This model can be trained by a gradient-based
optimization to maximize the conditional log-
likelihood of the observations. The unregularized
log-likelihood and gradient are given by:
l(w) =
X
i
ln(p(yi|xi;w)) (1)
=
X
i
ln
P
m exp(?(yi,m,xi;w))P
y?,m exp(?(y
?,m,xi;w))
(2)
?li
?wj
=
X
m
p(m|yi,xi;w)
?
?wj
?(yi,m,xi;w)
?
X
y?,m
p(m, y?|xi;w)
?
?wj
?(y?,m,xi;w)
The form of the potential function ? is where our
intuitions about the role of the hidden variable are
formalized. Our goal is to include the non-verbal
features xnv only when they are relevant; conse-
quently, the weight for these features should go to
zero for some settings of the hidden variable m. In
addition, verbal language is different when used in
combination with meaningful non-verbal commu-
nication than when it is used unimodally (Kehler,
2000; Melinger and Levelt, 2004). Thus, we learn
a different set of feature weights for each case: wv,1
when the non-verbal features are included, and wv,2
otherwise. The formal definition of the potential
function for conditional modality fusion is:
?(y,m,x;w) ?
{
y(wTv,1xv +w
T
nvxnv) +w
T
mxm m = 1
ywTv,2xv ?w
T
mxm m = ?1.
(3)
4 Application to coreference resolution
We apply conditional modality fusion to corefer-
ence resolution ? the problem of partitioning the
noun phrases in a document into clusters, where all
members of a cluster refer to the same semantic en-
tity. Coreference resolution on text datasets is well-
studied (e.g., (Cardie and Wagstaff, 1999)). This
prior work provides the departure point for our in-
vestigation of coreference resolution on spontaneous
and unconstrained speech and gesture.
4.1 Form of the model
The form of the model used in this application is
slightly different from that shown in Equation 3.
When determining whether two noun phrases core-
fer, the features at each utterance must be consid-
ered. For example, if we are to compare the simi-
larity of the gestures that accompany the two noun
phrases, it should be the case that gesture is relevant
during both time periods.
For this reason, we create two hidden variables,
m1 and m2; they indicate the relevance of ges-
ture over the first (antecedent) and second (anaphor)
noun phrases, respectively. Since gesture similarity
is only meaningful if the gesture is relevant during
both NPs, the gesture features are included only if
m1 = m2 = 1. Similarly, the linguistic feature
weights wv,1 are used when m1 = m2 = 1; oth-
erwise the weights wv,2 are used. This yields the
model shown in Equation 4.
The vector of meta features xm1 includes all
single-phrase verbal and gesture features from Ta-
ble 1, computed at the antecedent noun phrase;
xm2 includes the single-phrase verbal and gesture
features, computed at the anaphoric noun phrase.
The label-dependent verbal features xv include both
pairwise and single phrase verbal features from the
table, while the label-dependent non-verbal features
xnv include only the pairwise gesture features. The
single-phrase non-verbal features were not included
because they were not thought to be informative as
to whether the associated noun-phrase would partic-
ipate in coreference relations.
4.2 Verbal features
We employ a set of verbal features that is similar
to the features used by state-of-the-art coreference
resolution systems that operate on text (e.g., (Cardie
and Wagstaff, 1999)). Pairwise verbal features in-
clude: several string-match variants; distance fea-
tures, measured in terms of the number of interven-
ing noun phrases and sentences between the candi-
date NPs; and some syntactic features that can be
computed from part of speech tags. Single-phrase
verbal features describe the type of the noun phrase
(definite, indefinite, demonstrative (e.g., this ball),
or pronoun), the number of times it appeared in
the document, and whether there were any adjecti-
354
?(y,m1,m2,x;w) ?
{
y(wTv,1xv +w
T
nvxnv) +m1w
T
mxm1 +m2w
T
mxm2 , m1 = m2 = 1
ywTv,2xv +m1w
T
mxm1 +m2w
T
mxm2 , otherwise.
(4)
val modifiers. The continuous-valued features were
binned using a supervised technique (Fayyad and
Irani, 1993).
Note that some features commonly used for coref-
erence on the MUC and ACE corpora are not appli-
cable here. For example, gazetteers listing names of
nations or corporations are not relevant to our cor-
pus, which focuses on discussions of mechanical de-
vices (see section 5). Because we are working from
transcripts rather than text, features dependent on
punctuation and capitalization, such as apposition,
are also not applicable.
4.3 Non-verbal features
Our non-verbal features attempt to capture similar-
ity between the speaker?s hand gestures; similar ges-
tures are thought to suggest semantic similarity (Mc-
Neill, 1992). For example, two noun phrases may
be more likely to corefer if they are accompanied by
identically-located pointing gestures. In this section,
we describe features that quantify various aspects of
gestural similarity.
The most straightforward measure of similarity is
the Euclidean distance between the average hand po-
sition during each noun phrase ? we call this the
FOCUS-DISTANCE feature. Euclidean distance cap-
tures cases in which the speaker is performing a ges-
tural ?hold? in roughly the same location (McNeill,
1992).
However, Euclidean distance may not correlate
directly with semantic similarity. For example,
when gesturing at a detailed part of a diagram,
very small changes in hand position may be se-
mantically meaningful, while in other regions posi-
tional similarity may be defined more loosely. Ide-
ally, we would compute a semantic feature cap-
turing the object of the speaker?s reference (e.g.,
?the red block?), but this is not possible in gen-
eral, since a complete taxonomy of all possible ob-
jects of reference is usually unknown. Instead, we
use a hidden Markov model (HMM) to perform a
spatio-temporal clustering on hand position and ve-
locity. The SAME-CLUSTER feature reports whether
the hand positions during two noun phrases were
usually grouped in the same cluster by the HMM.
JS-DIV reports the Jensen-Shannon divergence, a
continuous-valued feature used to measure the simi-
larity in cluster assignment probabilities between the
two gestures (Lin, 1991).
The gesture features described thus far capture the
similarity between static gestures; that is, gestures
in which the hand position is nearly constant. How-
ever, these features do not capture the similarity be-
tween gesture trajectories, which may also be used
to communicate meaning. For example, a descrip-
tion of two identical motions might be expressed
by very similar gesture trajectories. To measure the
similarity between gesture trajectories, we use dy-
namic time warping (Huang et al, 2001), which
gives a similarity metric for temporal data that is
invariant to speed. This is reported in the DTW-
DISTANCE feature.
All features are computed from hand and body
pixel coordinates, which are obtained via computer
vision; our vision system is similar to (Deutscher et
al., 2000). The feature set currently supports only
single-hand gestures, using the hand that is farthest
from the body center. As with the verbal feature set,
supervised binning was applied to the continuous-
valued features.
4.4 Meta features
The role of the meta features is to determine whether
the gesture features are relevant at a given point in
time. To make this determination, both verbal and
non-verbal features are applied; the only require-
ment is that they be computable at a single instant
in time (unlike features that measure the similarity
between two NPs or gestures).
Verbal meta features Meaningful gesture has
been shown to be more frequent when the associated
speech is ambiguous (Melinger and Levelt, 2004).
Kehler finds that fully-specified noun phrases are
less likely to receive multimodal support (Kehler,
2000). These findings lead us to expect that pro-
355
Pairwise verbal features
edit-distance a numerical measure of the string simi-
larity between the two NPs
exact-match true if the two NPs have identical sur-
face forms
str-match true if the NPs are identical after re-
moving articles
nonpro-str true if i and j are not pronouns, and str-
match is true
pro-str true if i and j are pronouns, and str-
match is true
j-substring-i true if the anaphor j is a substring of
the antecedent i
i-substring-j true if i is a substring of j
overlap true if there are any shared words be-
tween i and j
np-dist the number of noun phrases between i
and j in the document
sent-dist the number of sentences between i and
j in the document
both-subj true if both i and j precede the first verb
of their sentences
same-verb true if the first verb in the sentences for
i and j is identical
number-match true if i and j have the same number
Single-phrase verbal features
pronoun true if the NP is a pronoun
count number of times the NP appears in the
document
has-modifiers true if the NP has adjective modifiers
indef-np true if the NP is an indefinite NP (e.g.,
a fish)
def-np true if the NP is a definite NP (e.g., the
scooter)
dem-np true if the NP begins with this, that,
these, or those
lexical features lexical features are defined for the most
common pronouns: it, that, this, and
they
Pairwise gesture features
focus-distance the Euclidean distance in pixels be-
tween the average hand position during
the two NPs
DTW-agreement a measure of the agreement of the hand-
trajectories during the two NPs, com-
puted using dynamic time warping
same-cluster true if the hand positions during the two
NPs fall in the same cluster
JS-div the Jensen-Shannon divergence be-
tween the cluster assignment likeli-
hoods
Single-phrase gesture features
dist-to-rest distance of the hand from rest position
jitter sum of instantaneous motion across NP
speed total displacement over NP, divided by
duration
rest-cluster true if the hand is usually in the cluster
associated with rest position
movement-cluster true if the hand is usually in the cluster
associated with movement
Table 1: The feature set
nouns should be likely to co-occur with meaningful
gestures, while definite NPs and noun phrases that
include adjectival modifiers should be unlikely to do
so. To capture these intuitions, all single-phrase ver-
bal features are included as meta features.
Non-verbal meta features Research on gesture
has shown that semantically meaningful hand mo-
tions usually take place away from ?rest position,?
which is located at the speaker?s lap or sides (Mc-
Neill, 1992). Effortful movements away from these
default positions can thus be expected to predict that
gesture is being used to communicate. We iden-
tify rest position as the center of the body on the
x-axis, and at a fixed, predefined location on the y-
axis. The DIST-TO-REST feature computes the av-
erage Euclidean distance of the hands from the rest
position, over the duration of the NP.
As noted in the previous section, a spatio-
temporal clustering was performed on the hand po-
sitions and velocities, using an HMM. The REST-
CLUSTER feature takes the value ?true? iff the most
frequently occupied cluster during the NP is the
cluster closest to rest position. In addition, pa-
rameter tying in the HMM forces all clusters but
one to represent static hold, with the remaining
cluster accounting for the transition movements be-
tween holds. Only this last cluster is permitted to
have an expected non-zero speed; if the hand is
most frequently in this cluster during the NP, then
the MOVEMENT-CLUSTER feature takes the value
?true.?
4.5 Implementation
The objective function (Equation 1) is optimized
using a Java implementation of L-BFGS, a quasi-
Newton numerical optimization technique (Liu and
Nocedal, 1989). Standard L2-norm regulariza-
tion is employed to prevent overfitting, with cross-
validation to select the regularization constant. Al-
though standard logistic regression optimizes a con-
vex objective, the inclusion of the hidden variable
renders our objective non-convex. Thus, conver-
gence to a global minimum is not guaranteed.
5 Evaluation setup
Dataset Our dataset consists of sixteen short dia-
logues, in which participants explained the behavior
356
of mechanical devices to a friend. There are nine
different pairs of participants; each contributed two
dialogues, with two thrown out due to recording er-
rors. One participant, the ?speaker,? saw a short
video describing the function of the device prior
to the dialogue; the other participant was tested on
comprehension of the device?s behavior after the di-
alogue. The speaker was given a pre-printed dia-
gram to aid in the discussion. For simplicity, only
the speaker?s utterances were included in these ex-
periments.
The dialogues were limited to three minutes in du-
ration, and most of the participants used the entire
allotted time. ?Markable? noun phrases ? those that
are permitted to participate in coreference relations
? were annotated by the first author, in accordance
with the MUC task definition (Hirschman and Chin-
chor, 1997). A total of 1141 ?markable? NPs were
transcribed, roughly half the size of the MUC6 de-
velopment set, which includes 2072 markable NPs
over 30 documents.
Evaluation metric Coreference resolution is of-
ten performed in two phases: a binary classifi-
cation phase, in which the likelihood of corefer-
ence for each pair of noun phrases is assessed;
and a partitioning phase, in which the clusters of
mutually-coreferring NPs are formed, maximizing
some global criterion (Cardie and Wagstaff, 1999).
Our model does not address the formation of noun-
phrase clusters, but only the question of whether
each pair of noun phrases in the document corefer.
Consequently, we evaluate only the binary classifi-
cation phase, and report results in terms of the area
under the ROC curve (AUC). As the small size of
the corpus did not permit dedicated test and devel-
opment sets, results are computed using leave-one-
out cross-validation, with one fold for each of the
sixteen documents in the corpus.
Baselines Three types of baselines were compared
to our conditional modality fusion (CMF) technique:
? Early fusion. The early fusion baseline in-
cludes all features in a single vector, ignor-
ing modality. This is equivalent to standard
maximum-entropy classification. Early fusion
is implemented with a conditionally-trained
linear classifier; it uses the same code as the
CMF model, but always includes all features.
? Late fusion. The late fusion baselines train
separate classifiers for gesture and speech, and
then combine their posteriors. The modality-
specific classifiers are conditionally-trained lin-
ear models, and again use the same code as the
CMF model. For simplicity, a parameter sweep
identifies the interpolation weights that maxi-
mize performance on the test set. Thus, it is
likely that these results somewhat overestimate
the performance of these baseline models. We
report results for both additive and multiplica-
tive combination of posteriors.
? No fusion. These baselines include the fea-
tures from only a single modality, and again
build a conditionally-trained linear classifier.
Implementation uses the same code as the CMF
model, but weights on features outside the tar-
get modality are forced to zero.
Although a comparison with existing state-of-the-
art coreference systems would be ideal, all such
available systems use verbal features that are inap-
plicable to our dataset, such as punctuation, capital-
ization, and gazetteers. The verbal features that we
have included are a representative sample from the
literature (e.g., (Cardie and Wagstaff, 1999)). The
?no fusion, verbal features only? baseline thus pro-
vides a reasonable representation of prior work on
coreference, by applying a maximum-entropy clas-
sifier to this set of typical verbal features.
Parameter tuning Continuous features are
binned separately for each cross-validation fold,
using only the training data. The regularization
constant is selected by cross-validation within each
training subset.
6 Results
Conditional modality fusion outperforms all other
approaches by a statistically significant margin (Ta-
ble 2). Compared with early fusion, CMF offers an
absolute improvement of 1.20% in area under the
ROC curve (AUC).1 A paired t-test shows that this
1AUC quantifies the ranking accuracy of a classifier. If the
AUC is 1, all positively-labeled examples are ranked higher than
all negative-labeled ones.
357
model AUC
Conditional modality fusion .8226
Early fusion .8109
Late fusion, multiplicative .8103
Late fusion, additive .8068
No fusion (verbal features only) .7945
No fusion (gesture features only) .6732
Table 2: Results, in terms of areas under the ROC
curve
2 3 4 5 6 7 80.79
0.795
0.8
0.805
0.81
0.815
0.82
0.825
0.83
log of regularization constant
AU
C
CMF
Early FusionSpeech Only
Figure 2: Conditional modality fusion is robust to
variations in the regularization constant.
result is statistically significant (p < .002, t(15) =
3.73). CMF obtains higher performance on fourteen
of the sixteen test folds. Both additive and multi-
plicative late fusion perform on par with early fu-
sion.
Early fusion with gesture features is superior to
unimodal verbal classification by an absolute im-
provement of 1.64% AUC (p < 4 ? 10?4, t(15) =
4.45). Thus, while gesture features improve coref-
erence resolution on this dataset, their effectiveness
is increased by a relative 73% when conditional
modality fusion is applied. Figure 2 shows how per-
formance varies with the regularization constant.
7 Discussion
The feature weights learned by the system to deter-
mine coreference largely confirm our linguistic in-
tuitions. Among the textual features, a large pos-
itive weight was assigned to the string match fea-
tures, while a large negative weight was assigned to
features such as number incompatibility (i.e., sin-
pronoun def dem indef "this" "it" "that" "they"modifiers?0.6
?0.5
?0.4
?0.3
?0.2
?0.1
0
0.1
0.2
0.3
0.4 Weights learned with verbal meta features
Figure 3: Weights for verbal meta features
gular versus plural). The system also learned that
gestures with similar hand positions and trajectories
were likely to indicate coreferring noun phrases; all
of our similarity metrics were correlated positively
with coreference. A chi-squared analysis found that
the EDIT DISTANCE was the most informative ver-
bal feature. The most informative gesture feature
was DTW-AGREEMENT feature, which measures
the similarity between gesture trajectories.
As described in section 4, both textual and gestu-
ral features are used to determine whether the ges-
ture is relevant. Among textual features, definite
and indefinite noun phrases were assigned nega-
tive weights, suggesting gesture would not be use-
ful to disambiguate coreference for such NPs. Pro-
nouns were assigned positive weights, with ?this?
and the much less frequently used ?they? receiving
the strongest weights. ?It? and ?that? received lower
weights; we observed that these pronouns were fre-
quently used to refer to the immediately preceding
noun phrase, so multimodal support was often un-
necessary. Last, we note that NPs with adjectival
modifiers were assigned negative weights, support-
ing the finding of (Kehler, 2000) that fully-specified
NPs are less likely to receive multimodal support. A
summary of the weights assigned to the verbal meta
features is shown in Figure 3. Among gesture meta
features, the weights learned by the system indicate
that non-moving hand gestures away from the body
are most likely to be informative in this dataset.
358
8 Future work
We have assumed that the relevance of gesture to
semantics is dependent only on the currently avail-
able features, and not conditioned on prior history.
In reality, meaningful gestures occur over contigu-
ous blocks of time, rather than at randomly dis-
tributed instances. Indeed, the psychology literature
describes a finite-state model of gesture, proceed-
ing from ?preparation,? to ?stroke,? ?hold,? and then
?retraction? (McNeill, 1992). These units are called
movement phases. The relevance of various gesture
features may be expected to depend on the move-
ment phase. During strokes, the trajectory of the
gesture may be the most relevant feature, while dur-
ing holds, static features such as hand position and
hand shape may dominate; during preparation and
retraction, gesture features are likely to be irrelevant.
The identification of these movement phases
should be independent of the specific problem of
coreference resolution. Thus, additional labels for
other linguistic phenomena (e.g., topic segmenta-
tion, disfluency) could be combined into the model.
Ideally, each additional set of labels would transfer
performance gains to the other labeling problems.
9 Conclusions
We have presented a new method for combining
multiple modalities, which we feel is especially rel-
evant to non-verbal modalities that are used to com-
municate only intermittently. Our model treats the
relevance of the non-verbal modality as a hidden
variable, learned jointly with the class labels. Ap-
plied to coreference resolution, this model yields a
relative increase of 73% in the contribution of the
gesture features. This gain is attained by identify-
ing instances in which gesture features are especially
relevant, and weighing their contribution more heav-
ily. We next plan to investigate models with a tem-
poral component, so that the behavior of the hidden
variable is governed by a finite-state transducer.
Acknowledgments We thank Aaron Adler, Regina
Barzilay, S. R. K. Branavan, Sonya Cates, Erdong Chen,
Michael Collins, Lisa Guttentag, Michael Oltmans, and Tom
Ouyang. This research is supported in part by MIT Project Oxy-
gen.
References
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. In Proceedings of EMNLP, pages 82?89.
Lei Chen, Yang Liu, Mary P. Harper, and Elizabeth Shriberg.
2004. Multimodal model integration for sentence unit de-
tection. In Proceedings of ICMI, pages 121?128.
Jonathan Deutscher, Andrew Blake, and Ian Reid. 2000. Artic-
ulated body motion capture by annealed particle filtering. In
Proceedings of CVPR, volume 2, pages 126?133.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-interval
discretization of continuousvalued attributes for classifica-
tion learning. In Proceedings of IJCAI-93, volume 2, pages
1022?1027. Morgan Kaufmann.
M.H. Goodwin and C. Goodwin. 1986. Gesture and co-
participation in the activity of searching for a word. Semiot-
ica, 62:51?75.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-7 coref-
erence task definition. In Proceedings of the Message Un-
derstanding Conference.
Xuedong Huang, Alex Acero, and Hsiao-Wuen Hon. 2001.
Spoken Language Processing. Prentice Hall.
Andrew Kehler. 2000. Cognitive status and form of reference
in multimodal human-computer interaction. In Proceedings
of AAAI, pages 685?690.
Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf.
2004. Detecting structural metadata with decision trees
and transformation-based learning. In Proceedings of HLT-
NAACL?04. ACL Press.
Jianhua Lin. 1991. Divergence measures based on the shannon
entropy. IEEE transactions on information theory, 37:145?
151.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45:503?528.
David McNeill. 1992. Hand and Mind. The University of
Chicago Press.
Alissa Melinger and Willem J. M. Levelt. 2004. Gesture and
communicative intention of the speaker. Gesture, 4(2):119?
141.
Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2004.
Conditional random fields for object recognition. In Neural
Information Processing Systems, pages 1097?1104.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur, and
Gokhan Tur. 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32.
Kentaro Toyama and Eric Horvitz. 2000. Bayesian modal-
ity fusion: Probabilistic integration of multiple vision al-
gorithms for head tracking. In Proceedings of ACCV ?00,
Fourth Asian Conference on Computer Vision.
359
Proceedings of ACL-08: HLT, pages 263?271,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Document-Level Semantic Properties from Free-text Annotations
S.R.K. Branavan Harr Chen Jacob Eisenstein Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, harr, jacobe, regina}@csail.mit.edu
Abstract
This paper demonstrates a new method for
leveraging free-text annotations to infer se-
mantic properties of documents. Free-text an-
notations are becoming increasingly abundant,
due to the recent dramatic growth in semi-
structured, user-generated online content. An
example of such content is product reviews,
which are often annotated by their authors
with pros/cons keyphrases such as ?a real bar-
gain? or ?good value.? To exploit such noisy
annotations, we simultaneously find a hid-
den paraphrase structure of the keyphrases, a
model of the document texts, and the underly-
ing semantic properties that link the two. This
allows us to predict properties of unannotated
documents. Our approach is implemented as
a hierarchical Bayesian model with joint in-
ference, which increases the robustness of the
keyphrase clustering and encourages the doc-
ument model to correlate with semantically
meaningful properties. We perform several
evaluations of our model, and find that it sub-
stantially outperforms alternative approaches.
1 Introduction
A central problem in language understanding is
transforming raw text into structured representa-
tions. Learning-based approaches have dramatically
increased the scope and robustness of this type of
automatic language processing, but they are typi-
cally dependent on large expert-annotated datasets,
which are costly to produce. In this paper, we show
how novice-generated free-text annotations avail-
able online can be leveraged to automatically infer
document-level semantic properties.
With the rapid increase of online content cre-
ated by end users, noisy free-text annotations have
pros/cons: great nutritional value
... combines it all: an amazing product, quick and
friendly service, cleanliness, great nutrition ...
pros/cons: a bit pricey, healthy
... is an awesome place to go if you are health con-
scious. They have some really great low calorie dishes
and they publish the calories and fat grams per serving.
Figure 1: Excerpts from online restaurant reviews with
pros/cons phrase lists. Both reviews discuss healthiness,
but use different keyphrases.
become widely available (Vickery and Wunsch-
Vincent, 2007; Sterling, 2005). For example, con-
sider reviews of consumer products and services.
Often, such reviews are annotated with keyphrase
lists of pros and cons. We would like to use these
keyphrase lists as training labels, so that the proper-
ties of unannotated reviews can be predicted. Hav-
ing such a system would facilitate structured access
and summarization of this data. However, novice-
generated keyphrase annotations are incomplete de-
scriptions of their corresponding review texts. Fur-
thermore, they lack consistency: the same under-
lying property may be expressed in many ways,
e.g., ?healthy? and ?great nutritional value? (see Fig-
ure 1). To take advantage of such noisy labels, a sys-
tem must both uncover their hidden clustering into
properties, and learn to predict these properties from
review text.
This paper presents a model that addresses both
problems simultaneously. We assume that both the
document text and the selection of keyphrases are
governed by the underlying hidden properties of the
document. Each property indexes a language model,
thus allowing documents that incorporate the same
263
property to share similar features. In addition, each
keyphrase is associated with a property; keyphrases
that are associated with the same property should
have similar distributional and surface features.
We link these two ideas in a joint hierarchical
Bayesian model. Keyphrases are clustered based
on their distributional and lexical properties, and a
hidden topic model is applied to the document text.
Crucially, the keyphrase clusters and document top-
ics are linked, and inference is performed jointly.
This increases the robustness of the keyphrase clus-
tering, and ensures that the inferred hidden topics
are indicative of salient semantic properties.
Our model is broadly applicable to many scenar-
ios where documents are annotated in a noisy man-
ner. In this work, we apply our method to a col-
lection of reviews in two categories: restaurants and
cell phones. The training data consists of review text
and the associated pros/cons lists. We then evaluate
the ability of our model to predict review properties
when the pros/cons list is hidden. Across a variety
of evaluation scenarios, our algorithm consistently
outperforms alternative strategies by a wide margin.
2 Related Work
Review Analysis Our approach relates to previous
work on property extraction from reviews (Popescu
et al, 2005; Hu and Liu, 2004; Kim and Hovy,
2006). These methods extract lists of phrases, which
are analogous to the keyphrases we use as input
to our algorithm. However, our approach is dis-
tinguished in two ways: first, we are able to pre-
dict keyphrases beyond those that appear verbatim
in the text. Second, our approach learns the rela-
tionships between keyphrases, allowing us to draw
direct comparisons between reviews.
Bayesian Topic Modeling One aspect of our
model views properties as distributions over words
in the document. This approach is inspired by meth-
ods in the topic modeling literature, such as Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), where
topics are treated as hidden variables that govern the
distribution of words in a text. Our algorithm ex-
tends this notion by biasing the induced hidden top-
ics toward a clustering of known keyphrases. Tying
these two information sources together enhances the
robustness of the hidden topics, thereby increasing
the chance that the induced structure corresponds to
semantically meaningful properties.
Recent work has examined coupling topic mod-
els with explicit supervision (Blei and McAuliffe,
2007; Titov and McDonald, 2008). However, such
approaches assume that the documents are labeled
within a predefined annotation structure, e.g., the
properties of food, ambiance, and service for restau-
rants. In contrast, we address free-text annotations
created by end users, without known semantic prop-
erties. Rather than requiring a predefined annotation
structure, our model infers one from the data.
3 Problem Formulation
We formulate our problem as follows. We assume
a dataset composed of documents with associated
keyphrases. Each document may be marked with
multiple keyphrases that express unseen semantic
properties. Across the entire collection, several
keyphrases may express the same property. The
keyphrases are also incomplete ? review texts of-
ten express properties that are not mentioned in their
keyphrases. At training time, our model has access
to both text and keyphrases; at test time, the goal is
to predict the properties supported by a previously
unseen document. We can then use this property list
to generate an appropriate set of keyphrases.
4 Model Description
Our approach leverages both keyphrase clustering
and distributional analysis of the text in a joint, hi-
erarchical Bayesian model. Keyphrases are drawn
from a set of clusters; words in the documents are
drawn from language models indexed by a set of
topics, where the topics correspond to the keyphrase
clusters. Crucially, we bias the assignment of hid-
den topics in the text to be similar to the topics rep-
resented by the keyphrases of the document, but we
permit some words to be drawn from other topics
not represented by the keyphrases. This flexibility in
the coupling allows the model to learn effectively in
the presence of incomplete keyphrase annotations,
while still encouraging the keyphrase clustering to
cohere with the topics supported by the text.
We train the model on documents annotated with
keyphrases. During training, we learn a hidden
topic model from the text; each topic is also asso-
264
? ? keyphrase cluster model
x ? keyphrase cluster assignment
s ? keyphrase similarity values
h ? document keyphrases
? ? document keyphrase topics
? ? probability of selecting ? instead of ?
c ? selects between ? and ? for word topics
? ? document topic model
z ? word topic assignment
? ? language models of each topic
w ? document words
? ? Dirichlet(?0)
x? ? Multinomial(?)
s?,?? ?
{
Beta(?=) if x? = x??
Beta(?6=) otherwise
?d = [?d,1 . . . ?d,K ]T
where
?d,k ?
{
1 if x? = k for any l ? hd
0 otherwise
? ? Beta(?0)
cd,n ? Bernoulli(?)
?d ? Dirichlet(?0)
zd,n ?
{
Multinomial(?d) if cd,n = 1
Multinomial(?d) otherwise
?k ? Dirichlet(?0)
wd,n ? Multinomial(?zd,n)
Figure 2: The plate diagram for our model. Shaded circles denote observed variables, and squares denote hyper
parameters. The dotted arrows indicate that ? is constructed deterministically from x and h.
ciated with a cluster of keyphrases. At test time,
we are presented with documents that do not con-
tain keyphrase annotations. The hidden topic model
of the review text is used to determine the proper-
ties that a document as a whole supports. For each
property, we compute the proportion of the docu-
ment?s words assigned to it. Properties with propor-
tions above a set threshold (tuned on a development
set) are predicted as being supported.
4.1 Keyphrase Clustering
One of our goals is to cluster the keyphrases, such
that each cluster corresponds to a well-defined prop-
erty. We represent each distinct keyphrase as a vec-
tor of similarity scores computed over the set of
observed keyphrases; these scores are represented
by s in Figure 2, the plate diagram of our model.1
Modeling the similarity matrix rather than the sur-
1We assume that similarity scores are conditionally inde-
pendent given the keyphrase clustering, though the scores are
in fact related. Such simplifying assumptions have been previ-
ously used with success in NLP (e.g., Toutanova and Johnson,
2007), though a more theoretically sound treatment of the sim-
ilarity matrix is an area for future research.
face forms allows arbitrary comparisons between
keyphrases, e.g., permitting the use of both lexical
and distributional information. The lexical com-
parison is based on the cosine similarity between
the keyphrase words. The distributional similar-
ity is quantified in terms of the co-occurrence of
keyphrases across review texts. Our model is inher-
ently capable of using any arbitrary source of simi-
larity information; for a discussion of similarity met-
rics, see Lin (1998).
4.2 Document-level Distributional Analysis
Our analysis of the document text is based on proba-
bilistic topic models such as LDA (Blei et al, 2003).
In the LDA framework, each word is generated from
a language model that is indexed by the word?s topic
assignment. Thus, rather than identifying a single
topic for a document, LDA identifies a distribution
over topics.
Our word model operates similarly, identifying a
topic for each word, written as z in Figure 2. To
tie these topics to the keyphrases, we deterministi-
cally construct a document-specific topic distribu-
265
tion from the clusters represented by the document?s
keyphrases ? this is ? in the figure. ? assigns equal
probability to all topics that are represented in the
keyphrases, and a small smoothing probability to
other topics.
As noted above, properties may be expressed in
the text even when no related keyphrase appears. For
this reason, we also construct a document-specific
topic distribution ?. The auxiliary variable c indi-
cates whether a given word?s topic is drawn from
the set of keyphrase clusters, or from this topic dis-
tribution.
4.3 Generative Process
In this section, we describe the underlying genera-
tive process more formally.
First we consider the set of all keyphrases ob-
served across the entire corpus, of which there are
L. We draw a multinomial distribution ? over the K
keyphrase clusters from a symmetric Dirichlet prior
?0. Then for the ?th keyphrase, a cluster assign-
ment x? is drawn from the multinomial ?. Finally,
the similarity matrix s ? [0, 1]L?L is constructed.
Each entry s?,?? is drawn independently, depending
on the cluster assignments x? and x?? . Specifically,
s?,?? is drawn from a Beta distribution with parame-
ters ?= if x? = x?? and ?6= otherwise. The parame-
ters ?= linearly bias s?,?? towards one (Beta(?=) ?
Beta(2, 1)), and the parameters ?6= linearly bias s?,??
towards zero (Beta(?6=) ? Beta(1, 2)).
Next, the words in each of the D documents
are generated. Document d has Nd words; zd,n is
the topic for word wd,n. These latent topics are
drawn either from the set of clusters represented by
the document?s keyphrases, or from the document?s
topic model ?d. We deterministically construct a
document-specific keyphrase topic model ?d, based
on the keyphrase cluster assignments x and the ob-
served keyphrases hd. The multinomial ?d assigns
equal probability to each topic that is represented by
a phrase in hd, and a small probability to other top-
ics.
As noted earlier, a document?s text may support
properties that are not mentioned in its observed
keyphrases. For that reason, we draw a document
topic multinomial ?d from a symmetric Dirichlet
prior ?0. The binary auxiliary variable cd,n deter-
mines whether the word?s topic is drawn from the
keyphrase model ?d or the document topic model
?d. cd,n is drawn from a weighted coin flip, with
probability ?; ? is drawn from a Beta distribution
with prior ?0. We have zd,n ? ?d if cd,n = 1,
and zd,n ? ?d otherwise. Finally, the word wd,n
is drawn from the multinomial ?zd,n , where zd,n in-
dexes a topic-specific language model. Each of the
K language models ?k is drawn from a symmetric
Dirichlet prior ?0.
5 Posterior Sampling
Ultimately, we need to compute the model?s poste-
rior distribution given the training data. Doing so
analytically is intractable due to the complexity of
the model, but sampling-based techniques can be
used to estimate the posterior. We employ Gibbs
sampling, previously used in NLP by Finkel et al
(2005) and Goldwater et al (2006), among others.
This technique repeatedly samples from the condi-
tional distributions of each hidden variable, eventu-
ally converging on a Markov chain whose stationary
distribution is the posterior distribution of the hid-
den variables in the model (Gelman et al, 2004).
We now present sampling equations for each of the
hidden variables in Figure 2.
The prior over keyphrase clusters ? is sampled
based on hyperprior ?0 and keyphrase cluster as-
signments x. We write p(? | . . .) to mean the prob-
ability conditioned on all the other variables.
p(? | . . .) ? p(? | ?0)p(x | ?),
= p(? | ?0)
L
?
?
p(x? | ?)
= Dir(?;?0)
L
?
?
Mul(x?;?)
= Dir(?;??),
where ??i = ?0 + count(x? = i). This update rule
is due to the conjugacy of the multinomial to the
Dirichlet distribution. The first line follows from
Bayes? rule, and the second line from the conditional
independence of each keyphrase assignment x? from
the others, given ?.
?d and ?k are resampled in a similar manner:
p(?d | . . .) ? Dir(?d;??d),
p(?k | . . .) ? Dir(?k; ??k),
266
p(x? | . . .) ? p(x? | ?)p(s | x?,x??, ?)p(z | ?, ?, c)
? p(x? | ?)
?
?
?
?? 6=?
p(s?,?? | x?, x?? , ?)
?
?
?
?
D
?
d
?
cd,n=1
p(zd,n | ?d)
?
?
= Mul(x?;?)
?
?
?
?? 6=?
Beta(s?,?? ;?x?,x?? )
?
?
?
?
D
?
d
?
cd,n=1
Mul(zd,n; ?d)
?
?
Figure 3: The resampling equation for the keyphrase cluster assignments.
where ??d,i = ?0 + count(zd,n = i ? cd,n = 0)
and ??k,i = ?0 +
?
d count(wd,n = i ? zd,n = k). In
building the counts for ??d,i, we consider only cases
in which cd,n = 0, indicating that the topic zd,n is
indeed drawn from the document topic model ?d.
Similarly, when building the counts for ??k, we con-
sider only cases in which the word wd,n is drawn
from topic k.
To resample ?, we employ the conjugacy of the
Beta prior to the Bernoulli observation likelihoods,
adding counts of c to the prior ?0.
p(? | . . .) ? Beta(?;??),
where ?? = ?0 +
[ ?
d count(cd,n = 1)
?
d count(cd,n = 0)
]
.
The keyphrase cluster assignments are repre-
sented by x, whose sampling distribution depends
on ?, s, and z, via ?. The equation is shown in Fig-
ure 3. The first term is the prior on x?. The second
term encodes the dependence of the similarity ma-
trix s on the cluster assignments; with slight abuse of
notation, we write ?x?,x?? to denote ?= if x? = x?? ,
and ?6= otherwise. The third term is the dependence
of the word topics zd,n on the topic distribution ?d.
We compute the final result of Figure 3 for each pos-
sible setting of x?, and then sample from the normal-
ized multinomial.
The word topics z are sampled according to
keyphrase topic distribution ?d, document topic dis-
tribution ?d, words w, and auxiliary variables c:
p(zd,n | . . .)
? p(zd,n | ?d, ?d, cd,n)p(wd,n | zd,n, ?)
=
{
Mul(zd,n; ?d)Mul(wd,n; ?zd,n) if cd,n = 1,
Mul(zd,n;?d)Mul(wd,n; ?zd,n) otherwise.
As with x?, each zd,n is sampled by computing
the conditional likelihood of each possible setting
within a constant of proportionality, and then sam-
pling from the normalized multinomial.
Finally, we sample each auxiliary variable cd,n,
which indicates whether the hidden topic zd,n is
drawn from ?d or ?d. The conditional probability
for cd,n depends on its prior ? and the hidden topic
assignments zd,n:
p(cd,n | . . .)
? p(cd,n | ?)p(zd,n | ?d, ?d, cd,n)
=
{
Bern(cd,n;?)Mul(zd,n; ?d) if cd,n = 1,
Bern(cd,n;?)Mul(zd,n;?d) otherwise.
We compute the likelihood of cd,n = 0 and cd,n = 1
within a constant of proportionality, and then sample
from the normalized Bernoulli distribution.
6 Experimental Setup
Data Sets We evaluate our system on reviews from
two categories, restaurants and cell phones. These
reviews were downloaded from the popular Epin-
ions2 website. Users of this website evaluate prod-
ucts by providing both a textual description of their
opinion, as well as concise lists of keyphrases (pros
and cons) summarizing the review. The statistics of
this dataset are provided in Table 1. For each of
the categories, we randomly selected 50%, 15%, and
35% of the documents as training, development, and
test sets, respectively.
Manual analysis of this data reveals that authors
often omit properties mentioned in the text from
the list of keyphrases. To obtain a complete gold
2http://www.epinions.com/
267
Restaurants Cell Phones
# of reviews 3883 1112
Avg. review length 916.9 1056.9
Avg. keyphrases / review 3.42 4.91
Table 1: Statistics of the reviews dataset by category.
standard, we hand-annotated a subset of the reviews
from the restaurant category. The annotation effort
focused on eight commonly mentioned properties,
such as those underlying the keyphrases ?pleasant
atmosphere? and ?attentive staff.? Two raters anno-
tated 160 reviews, 30 of which were annotated by
both. Cohen?s kappa, a measure of interrater agree-
ment ranging from zero to one, was 0.78 for this sub-
set, indicating high agreement (Cohen, 1960).
Each review was annotated with 2.56 properties
on average. Each manually-annotated property cor-
responded to an average of 19.1 keyphrases in the
restaurant data, and 6.7 keyphrases in the cell phone
data. This supports our intuition that a single se-
mantic property may be expressed using a variety of
different keyphrases.
Training Our model needs to be provided with the
number of clusters K . We setK large enough for the
model to learn effectively on the development set.
For the restaurant data ? where the gold standard
identified eight semantic properties ? we set K to
20, allowing the model to account for keyphrases not
included in the eight most common properties. For
the cell phones category, we set K to 30.
To improve the model?s convergence rate, we per-
form two initialization steps for the Gibbs sampler.
First, sampling is done only on the keyphrase clus-
tering component of the model, ignoring document
text. Second, we fix this clustering and sample the
remaining model parameters. These two steps are
run for 5,000 iterations each. The full joint model
is then sampled for 100,000 iterations. Inspection
of the parameter estimates confirms model conver-
gence. On a 2GHz dual-core desktop machine, a
multi-threaded C++ implementation of model train-
ing takes about two hours for each dataset.
Inference The final point estimate used for test-
ing is an average (for continuous variables) or a
mode (for discrete variables) over the last 1,000
Gibbs sampling iterations. Averaging is a heuris-
tic that is applicable in our case because our sam-
ple histograms are unimodal and exhibit low skew.
The model usually works equally well using single-
sample estimates, but is more prone to estimation
noise.
As previously mentioned, we convert word topic
assignments to document properties by examining
the proportion of words supporting each property. A
threshold for this proportion is set for each property
via the development set.
Evaluation Our first evaluation examines the ac-
curacy of our model and the baselines by compar-
ing their output against the keyphrases provided by
the review authors. More specifically, the model
first predicts the properties supported by a given re-
view. We then test whether the original authors?
keyphrases are contained in the clusters associated
with these properties.
As noted above, the authors? keyphrases are of-
ten incomplete. To perform a noise-free compari-
son, we based our second evaluation on the man-
ually constructed gold standard for the restaurant
category. We took the most commonly observed
keyphrase from each of the eight annotated proper-
ties, and tested whether they are supported by the
model based on the document text.
In both types of evaluation, we measure the
model?s performance using precision, recall, and F-
score. These are computed in the standard manner,
based on the model?s keyphrase predictions com-
pared against the corresponding references. The
sign test was used for statistical significance test-
ing (De Groot and Schervish, 2001).
Baselines To the best of our knowledge, this task
not been previously addressed in the literature. We
therefore consider five baselines that allow us to ex-
plore the properties of this task and our model.
Random: Each keyphrase is supported by a doc-
ument with probability of one half. This baseline?s
results are computed (in expectation) rather than ac-
tually run. This method is expected to have a recall
of 0.5, because in expectation it will select half of
the correct keyphrases. Its precision is the propor-
tion of supported keyphrases in the test set.
Phrase in text: A keyphrase is supported by a doc-
ument if it appears verbatim in the text. Because of
this narrow requirement, precision should be high
whereas recall will be low.
268
Restaurants Restaurants Cell Phones
gold standard annotation free-text annotation free-text annotation
Recall Prec. F-score Recall Prec. F-score Recall Prec. F-score
Random 0.500 0.300 ? 0.375 0.500 0.500 ? 0.500 0.500 0.489 ? 0.494
Phrase in text 0.048 0.500 ? 0.087 0.078 0.909 ? 0.144 0.171 0.529 ? 0.259
Cluster in text 0.223 0.534 0.314 0.517 0.640 ? 0.572 0.829 0.547 0.659
Phrase classifier 0.028 0.636 ? 0.053 0.068 0.963 ? 0.126 0.029 0.600 ? 0.055
Cluster classifier 0.113 0.622 ? 0.192 0.255 0.907 ? 0.398 0.210 0.759 0.328
Our model 0.625 0.416 0.500 0.901 0.652 0.757 0.886 0.585 0.705
Our model + gold clusters 0.582 0.398 0.472 0.795 0.627 ? 0.701 0.886 0.520 ? 0.655
Table 2: Comparison of the property predictions made by our model and the baselines in the two categories as evaluated
against the gold and free-text annotations. Results for our model using the fixed, manually-created gold clusterings are
also shown. The methods against which our model has significantly better results on the sign test are indicated with a
? for p <= 0.05, and ? for p <= 0.1.
Cluster in text: A keyphrase is supported by a
document if it or any of its paraphrases appears in
the text. Paraphrasing is based on our model?s clus-
tering of the keyphrases. The use of paraphrasing
information enhances recall at the potential cost of
precision, depending on the quality of the clustering.
Phrase classifier: Discriminative classifiers are
trained for each keyphrase. Positive examples are
documents that are labeled with the keyphrase;
all other documents are negative examples. A
keyphrase is supported by a document if that
keyphrase?s classifier returns positive.
Cluster classifier: Discriminative classifiers are
trained for each cluster of keyphrases, using our
model?s clustering. Positive examples are docu-
ments that are labeled with any keyphrase from the
cluster; all other documents are negative examples.
All keyphrases of a cluster are supported by a docu-
ment if that cluster?s classifier returns positive.
Phrase classifier and cluster classifier employ
maximum entropy classifiers, trained on the same
features as our model, i.e., word counts. The former
is high-precision/low-recall, because for any partic-
ular keyphrase, its synonymous keyphrases would
be considered negative examples. The latter broad-
ens the positive examples, which should improve re-
call. We used Zhang Le?s MaxEnt toolkit3 to build
these classifiers.
3http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
7 Results
Comparative performance Table 2 presents the
results of the evaluation scenarios described above.
Our model outperforms every baseline by a wide
margin in all evaluations.
The absolute performance of the automatic meth-
ods indicates the difficulty of the task. For instance,
evaluation against gold standard annotations shows
that the random baseline outperforms all of the other
baselines. We observe similar disappointing results
for the non-random baselines against the free-text
annotations. The precision and recall characteristics
of the baselines match our previously described ex-
pectations.
The poor performance of the discriminative mod-
els seems surprising at first. However, these re-
sults can be explained by the degree of noise in
the training data, specifically, the aforementioned
sparsity of free-text annotations. As previously de-
scribed, our technique allows document text topics
to stochastically derive from either the keyphrases or
a background distribution ? this allows our model
to learn effectively from incomplete annotations. In
fact, when we force all text topics to derive from
keyphrase clusters in our model, its performance de-
grades to the level of the classifiers or worse, with
an F-score of 0.390 in the restaurant category and
0.171 in the cell phone category.
Impact of paraphrasing As previously ob-
served in entailment research (Dagan et al, 2006),
paraphrasing information contributes greatly to im-
proved performance on semantic inference. This is
269
Figure 4: Sample keyphrase clusters that our model infers
in the cell phone category.
confirmed by the dramatic difference in results be-
tween the cluster in text and phrase in text baselines.
Therefore it is important to quantify the quality of
automatically computed paraphrases, such as those
illustrated in Figure 4.
Restaurants Cell Phones
Keyphrase similarity only 0.931 0.759
Joint training 0.966 0.876
Table 3: Rand Index scores of our model?s clusters, using
only keyphrase similarity vs. using keyphrases and text
jointly. Comparison of cluster quality is against the gold
standard.
One way to assess clustering quality is to com-
pare it against a ?gold standard? clustering, as con-
structed in Section 6. For this purpose, we use the
Rand Index (Rand, 1971), a measure of cluster sim-
ilarity. This measure varies from zero to one; higher
scores are better. Table 3 shows the Rand Indices
for our model?s clustering, as well as the clustering
obtained by using only keyphrase similarity. These
scores confirm that joint inference produces better
clusters than using only keyphrases.
Another way of assessing cluster quality is to con-
sider the impact of using the gold standard clustering
instead of our model?s clustering. As shown in the
last two lines of Table 2, using the gold clustering
yields results worse than using the model clustering.
This indicates that for the purposes of our task, the
model clustering is of sufficient quality.
8 Conclusions and Future Work
In this paper, we have shown how free-text anno-
tations provided by novice users can be leveraged
as a training set for document-level semantic infer-
ence. The resulting hierarchical Bayesian model
overcomes the lack of consistency in such anno-
tations by inducing a hidden structure of seman-
tic properties, which correspond both to clusters of
keyphrases and hidden topic models in the text. Our
system successfully extracts semantic properties of
unannotated restaurant and cell phone reviews, em-
pirically validating our approach.
Our present model makes strong assumptions
about the independence of similarity scores. We be-
lieve this could be avoided by modeling the genera-
tion of the entire similarity matrix jointly. We have
also assumed that the properties themselves are un-
structured, but they are in fact related in interest-
ing ways. For example, it would be desirable to
model antonyms explicitly, e.g., no restaurant review
should be simultaneously labeled as having good
and bad food. The correlated topic model (Blei and
Lafferty, 2006) is one way to account for relation-
ships between hidden topics; more structured repre-
sentations, such as hierarchies, may also be consid-
ered.
Finally, the core idea of using free-text as a
source of training labels has wide applicability, and
has the potential to enable sophisticated content
search and analysis. For example, online blog en-
tries are often tagged with short keyphrases. Our
technique could be used to standardize these tags,
and assign keyphrases to untagged blogs. The no-
tion of free-text annotations is also very broad ?
we are currently exploring the applicability of this
model to Wikipedia articles, using section titles as
keyphrases, to build standard article schemas.
Acknowledgments
The authors acknowledge the support of the NSF,
Quanta Computer, the U.S. Office of Naval Re-
search, and DARPA. Thanks to Michael Collins,
Dina Katabi, Kristian Kersting, Terry Koo, Brian
Milch, Tahira Naseem, Dan Roy, Benjamin Snyder,
Luke Zettlemoyer, and the anonymous reviewers for
helpful comments and suggestions. Any opinions,
findings, and conclusions or recommendations ex-
pressed above are those of the authors and do not
necessarily reflect the views of the NSF.
270
References
David M. Blei and John D. Lafferty. 2006. Correlated
topic models. In Advances in NIPS, pages 147?154.
David M. Blei and Jon McAuliffe. 2007. Supervised
topic models. In Advances in NIPS.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. Lecture Notes in Computer Science,
3944:177?190.
Morris H. De Groot and Mark J. Schervish. 2001. Prob-
ability and Statistics. Addison Wesley.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by Gibbs sampling. In
Proceedings of the ACL, pages 363?370.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, 2nd edi-
tion.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of ACL, pages
673?680.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of SIGKDD,
pages 168?177.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING/ACL, pages 483?490.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML, pages 296?304.
Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.
2005. OPINE: Extracting product features and opin-
ions from reviews. In Proceedings of HLT/EMNLP,
pages 339?346.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850, December.
Bruce Sterling. 2005. Order out of chaos: What is the
best way to tag, bag, and sort data? Give it to the
unorganized masses. http://www.wired.com/
wired/archive/13.04/view.html?pg=4.
Accessed April 21, 2008.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the ACL.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Advances in NIPS.
Graham Vickery and Sacha Wunsch-Vincent. 2007. Par-
ticipative Web and User-Created Content: Web 2.0,
Wikis and Social Networking. OECD Publishing.
271
Proceedings of ACL-08: HLT, pages 852?860,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Gestural Cohesion for Topic Segmentation
Jacob Eisenstein, Regina Barzilay and Randall Davis
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge MA 02139
{jacobe, regina, davis}@csail.mit.edu
Abstract
This paper explores the relationship between
discourse segmentation and coverbal gesture.
Introducing the idea of gestural cohesion, we
show that coherent topic segments are char-
acterized by homogeneous gestural forms and
that changes in the distribution of gestural
features predict segment boundaries. Gestu-
ral features are extracted automatically from
video, and are combined with lexical features
in a Bayesian generative model. The resulting
multimodal system outperforms text-only seg-
mentation on both manual and automatically-
recognized speech transcripts.
1 Introduction
When people communicate face-to-face, discourse
cues are expressed simultaneously through multiple
channels. Previous research has extensively studied
how discourse cues correlate with lexico-syntactic
and prosodic features (Hearst, 1994; Hirschberg and
Nakatani, 1998; Passonneau and Litman, 1997); this
work informs various text and speech processing
applications, such as automatic summarization and
segmentation. Gesture is another communicative
modality that frequently accompanies speech, yet it
has not been exploited for computational discourse
analysis.
This paper empirically demonstrates that gesture
correlates with discourse structure. In particular,
we show that automatically-extracted visual fea-
tures can be combined with lexical cues in a sta-
tistical model to predict topic segmentation, a fre-
quently studied form of discourse structure. Our
method builds on the idea that coherent discourse
segments are characterized by gestural cohesion; in
other words, that such segments exhibit homoge-
neous gestural patterns. Lexical cohesion (Halliday
and Hasan, 1976) forms the backbone of many ver-
bal segmentation algorithms, on the theory that seg-
mentation boundaries should be placed where the
distribution of words changes (Hearst, 1994). With
gestural cohesion, we explore whether the same idea
holds for gesture features.
The motivation for this approach comes from a
series of psycholinguistic studies suggesting that
gesture supplements speech with meaningful and
unique semantic content (McNeill, 1992; Kendon,
2004). We assume that repeated patterns in gesture
are indicative of the semantic coherence that charac-
terizes well-defined discourse segments. An advan-
tage of this view is that gestures can be brought to
bear on discourse analysis without undertaking the
daunting task of recognizing and interpreting indi-
vidual gestures. This is crucial because coverbal
gesture ? unlike formal sign language ? rarely fol-
lows any predefined form or grammar, and may vary
dramatically by speaker.
A key implementational challenge is automati-
cally extracting gestural information from raw video
and representing it in a way that can applied to dis-
course analysis. We employ a representation of vi-
sual codewords, which capture clusters of low-level
motion patterns. For example, one codeword may
correspond to strong left-right motion in the up-
per part of the frame. These codewords are then
treated similarly to lexical items; our model iden-
tifies changes in their distribution, and predicts topic
852
boundaries appropriately. The overall framework is
implemented as a hierarchical Bayesian model, sup-
porting flexible integration of multiple knowledge
sources.
Experimental results support the hypothesis that
gestural cohesion is indicative of discourse struc-
ture. Applying our algorithm to a dataset of face-
to-face dialogues, we find that gesture commu-
nicates unique information, improving segmenta-
tion performance over lexical features alone. The
positive impact of gesture is most pronounced
when automatically-recognized speech transcripts
are used, but gestures improve performance by a
significant margin even in combination with manual
transcripts.
2 Related Work
Gesture and discourse Much of the work on ges-
ture in natural language processing has focused
on multimodal dialogue systems in which the ges-
tures and speech may be constrained, e.g. (Johnston,
1998). In contrast, we focus on improving discourse
processing on unconstrained natural language be-
tween humans. This effort follows basic psycho-
logical and linguistic research on the communicative
role of gesture (McNeill, 1992; Kendon, 2004), in-
cluding some efforts that made use of automatically
acquired visual features (Quek, 2003). We extend
these empirical studies with a statistical model of the
relationship between gesture and discourse segmen-
tation.
Hand-coded descriptions of body posture shifts
and eye gaze behavior have been shown to correlate
with topic and turn boundaries in task-oriented dia-
logue (Cassell et al, 2001). These findings are ex-
ploited to generate realistic conversational ?ground-
ing? behavior in an animated agent. The seman-
tic content of gesture was leveraged ? again, for
gesture generation ? in (Kopp et al, 2007), which
presents an animated agent that is capable of aug-
menting navigation directions with gestures that de-
scribe the physical properties of landmarks along
the route. Both systems generate plausible and
human-like gestural behavior; we address the con-
verse problem of interpreting such gestures.
In this vein, hand-coded gesture features have
been used to improve sentence segmentation, show-
ing that sentence boundaries are unlikely to over-
lap gestures that are in progress (Chen et al, 2006).
Features that capture the start and end of gestures
are shown to improve sentence segmentation beyond
lexical and prosodic features alone. This idea of ges-
tural features as a sort of visual punctuation has par-
allels in the literature on prosody, which we discuss
in the next subsection.
Finally, ambiguous noun phrases can be resolved
by examining the similarity of co-articulated ges-
tures (Eisenstein and Davis, 2007). While noun
phrase coreference can be viewed as a discourse pro-
cessing task, we address the higher-level discourse
phenomenon of topic segmentation. In addition, this
prior work focused primarily on pointing gestures
directed at pre-printed visual aids. The current pa-
per presents a new domain, in which speakers do not
have access to visual aids. Thus pointing gestures
are less frequent than ?iconic? gestures, in which the
form of motion is the principle communicative fea-
ture (McNeill, 1992).
Non-textual features for topic segmentation Re-
search on non-textual features for topic segmenta-
tion has primarily focused on prosody, under the as-
sumption that a key prosodic function is to mark
structure at the discourse level (Steedman, 1990;
Grosz and Hirshberg, 1992; Swerts, 1997). The ul-
timate goal of this research is to find correlates of
hierarchical discourse structure in phonetic features.
Today, research on prosody has converged on
prosodic cues which correlate with discourse struc-
ture. Such markers include pause duration, fun-
damental frequency, and pitch range manipula-
tions (Grosz and Hirshberg, 1992; Hirschberg and
Nakatani, 1998). These studies informed the devel-
opment of applications such as segmentation tools
for meeting analysis, e.g. (Tur et al, 2001; Galley et
al., 2003).
In comparison, the connection between gesture
and discourse structure is a relatively unexplored
area, at least with respect to computational ap-
proaches. One conclusion that emerges from our
analysis is that gesture may signal discourse struc-
ture in a different way than prosody does: while spe-
cific prosodic markers characterize segment bound-
aries, gesture predicts segmentation through intra-
segmental cohesion. The combination of these two
853
modalities is an exciting direction for future re-
search.
3 Visual Features for Discourse Analysis
This section describes the process of building a rep-
resentation that permits the assessment of gestural
cohesion. The core signal-level features are based
on spatiotemporal interest points, which provide a
sparse representation of the motion in the video. At
each interest point, visual, spatial, and kinematic
characteristics are extracted and then concatenated
into vectors. Principal component analysis (PCA)
reduces the dimensionality to a feature vector of
manageable size (Bishop, 2006). These feature vec-
tors are then clustered, yielding a codebook of visual
forms. This video processing pipeline is shown in
Figure 1; the remainder of the section describes the
individual steps in greater detail.
3.1 Spatiotemporal Interest Points
Spatiotemporal interest points (Laptev, 2005) pro-
vide a sparse representation of motion in video. The
idea is to select a few local regions that contain high
information content in both the spatial and tempo-
ral dimensions. The image features at these regions
should be relatively robust to lighting and perspec-
tive changes, and they should capture the relevant
movement in the video. The set of spatiotemporal
interest points thereby provides a highly compressed
representation of the key visual features. Purely spa-
tial interest points have been successful in a variety
of image processing tasks (Lowe, 1999), and spa-
tiotemporal interest points are beginning to show
similar advantages for video processing (Laptev,
2005).
The use of spatiotemporal interest points is specif-
ically motivated by techniques from the computer
vision domain of activity recognition (Efros et al,
2003; Niebles et al, 2006). The goal of activity
recognition is to classify video sequences into se-
mantic categories: e.g., walking, running, jumping.
As a simple example, consider the task of distin-
guishing videos of walking from videos of jump-
ing. In the walking videos, the motion at most of
the interest points will be horizontal, while in the
jumping videos it will be vertical. Spurious vertical
motion in a walking video is unlikely to confuse the
classifier, as long as the majority of interest points
move horizontally. The hypothesis of this paper is
that just as such low-level movement features can be
applied in a supervised fashion to distinguish activi-
ties, they can be applied in an unsupervised fashion
to group co-speech gestures into perceptually mean-
ingful clusters.
The Activity Recognition Toolbox (Dolla?r et al,
2005)1 is used to detect spatiotemporal interest
points for our dataset. This toolbox ranks interest
points using a difference-of-Gaussians filter in the
spatial dimension, and a set of Gabor filters in the
temporal dimension. The total number of interest
points extracted per video is set to equal the number
of frames in the video. This bounds the complexity
of the representation to be linear in the length of the
video; however, the system may extract many inter-
est points in some frames and none in other frames.
Figure 2 shows the interest points extracted from
a representative video frame from our corpus. Note
that the system has identified high contrast regions
of the gesturing hand. From manual inspection,
the large majority of interest points extracted in our
dataset capture motion created by hand gestures.
Thus, for this dataset it is reasonable to assume that
an interest point-based representation expresses the
visual properties of the speakers? hand gestures. In
videos containing other sources of motion, prepro-
cessing may be required to filter out interest points
that are extraneous to gestural communication.
3.2 Visual Descriptors
At each interest point, the temporal and spatial
brightness gradients are constructed across a small
space-time volume of nearby pixels. Brightness gra-
dients have been used for a variety of problems in
computer vision (Forsyth and Ponce, 2003), and pro-
vide a fairly general way to describe the visual ap-
pearance of small image patches. However, even for
a small space-time volume, the resulting dimension-
ality is still quite large: a 10-by-10 pixel box across 5
video frames yields a 500-dimensional feature vec-
tor for each of the three gradients. For this reason,
principal component analysis (Bishop, 2006) is used
to reduce the dimensionality. The spatial location of
the interest point is added to the final feature vector.
1http://vision.ucsd.edu/?pdollar/research/cuboids doc/index.html
854
Figure 1: The visual processing pipeline for the extraction of gestural codewords from video.
Figure 2: Circles indicate the interest points extracted
from this frame of the corpus.
This visual feature representation is substantially
lower-level than the descriptions of gesture form
found in both the psychology and computer science
literatures. For example, when manually annotat-
ing gesture, it is common to employ a taxonomy
of hand shapes and trajectories, and to describe the
location with respect to the body and head (Mc-
Neill, 1992; Martell, 2005). Working with automatic
hand tracking, Quek (2003) automatically computes
perceptually-salient gesture features, such as sym-
metric motion and oscillatory repetitions.
In contrast, our feature representation takes the
form of a vector of continuous values and is not eas-
ily interpretable in terms of how the gesture actu-
ally appears. However, this low-level approach of-
fers several important advantages. Most critically,
it requires no initialization and comparatively little
tuning: it can be applied directly to any video with a
fixed camera position and static background. Sec-
ond, it is robust: while image noise may cause a
few spurious interest points, the majority of inter-
est points should still guide the system to an appro-
priate characterization of the gesture. In contrast,
hand tracking can become irrevocably lost, requiring
manual resets (Gavrila, 1999). Finally, the success
of similar low-level interest point representations at
the activity-recognition task provides reason for op-
timism that they may also be applicable to unsuper-
vised gesture analysis.
3.3 A Lexicon of Visual Forms
After extracting a set of low-dimensional feature
vectors to characterize the visual appearance at each
spatiotemporal interest point, it remains only to
convert this into a representation amenable to a
cohesion-based analysis. Using k-means cluster-
ing (Bishop, 2006), the feature vectors are grouped
into codewords: a compact, lexicon-like representa-
tion of salient visual features in video. The number
of clusters is a tunable parameter, though a system-
atic investigation of the role of this parameter is left
for future work.
Codewords capture frequently-occurring patterns
of motion and appearance at a local scale ? interest
points that are clustered together have a similar vi-
sual appearance. Because most of the motion in our
videos is gestural, the codewords that appear during
a given sentence provide a succinct representation of
the ongoing gestural activity. Distributions of code-
words over time can be analyzed in similar terms
to the distribution of lexical features. A change in
the distribution of codewords indicates new visual
kinematic elements entering the discourse. Thus, the
codeword representation allows gestural cohesion to
be assessed in much the same way as lexical cohe-
sion.
4 Bayesian Topic Segmentation
Topic segmentation is performed in a Bayesian
framework, with each sentence?s segment index en-
coded in a hidden variable, written zt. The hidden
variables are assumed to be generated by a linear
segmentation, such that zt ? {zt?1, zt?1 + 1}. Ob-
servations ? the words and gesture codewords ? are
855
generated by multinomial language models that are
indexed according to the segment. In this frame-
work, a high-likelihood segmentation will include
language models that are tightly focused on a com-
pact vocabulary. Such a segmentation maximizes
the lexical cohesion of each segment. This model
thus provides a principled, probabilistic framework
for cohesion-based segmentation, and we will see
that the Bayesian approach is particularly well-
suited to the combination of multiple modalities.
Formally, our goal is to identify the best possible
segmentation S, where S is a tuple: S = ?z, ?, ??.
The segment indices for each sentence are written
zt; for segment i, ?i and ?i are multinomial lan-
guage models over words and gesture codewords re-
spectively. For each sentence, xt and yt indicate
the words and gestures that appear. We will seek to
identify the segmentation S? = argmaxSp(S,x,y),
conditioned on priors that will be defined below.
p(S,x,y) = p(x,y|S)p(S)
p(x,y|S) =
?
i
p({xt : zt = i}|?i)p({yt : zt = i}|?i)
(1)
p(S) = p(z)
?
i
p(?i)p(?i) (2)
The language models ?i and ?i are multinomial
distributions, so the log-likelihood of the obser-
vations xt is log p(xt|?i) =
?W
j n(t, j) log ?i,j ,
where n(t, j) is the count of word j in sentence t,
and W is the size of the vocabulary. An analogous
equation is used for the gesture codewords. Each
language model is given a symmetric Dirichlet prior
?. As we will see shortly, the use of different pri-
ors for the verbal and gestural language models al-
lows us to weight these modalities in a Bayesian
framework. Finally, we model the probability of
the segmentation z by considering the durations of
each segment: p(z) =
?
i p(dur(i)|?). A negative-
binomial distribution with parameter ? is applied to
discourage extremely short or long segments.
Inference Crucially, both the likelihood (equa-
tion 1) and the prior (equation 2) factor into a prod-
uct across the segments. This factorization en-
ables the optimal segmentation to be found using
a dynamic program, similar to those demonstrated
by Utiyama and Isahara (2001) and Malioutov and
Barzilay (2006). For each set of segmentation points
z, the associated language models are set to their
posterior expectations, e.g., ?i = E[?|{xt : zt =
i}, ?].
The Dirichlet prior is conjugate to the multino-
mial, so this expectation can be computed in closed
form:
?i,j =
n(i, j) + ?
N(i) +W?
, (3)
where n(i, j) is the count of word j in segment
i and N(i) is the total number of words in seg-
ment i (Bernardo and Smith, 2000). The symmetric
Dirichlet prior ? acts as a smoothing pseudo-count.
In the multimodal context, the priors act to control
the weight of each modality. If the prior for the ver-
bal language model ? is high relative to the prior for
the gestural language model ? then the verbal multi-
nomial will be smoother, and will have a weaker im-
pact on the final segmentation. The impact of the
priors on the weights of each modality is explored
in Section 6.
Estimation of priors The distribution over seg-
ment durations is negative-binomial, with parame-
ters ?. In general, the maximum likelihood estimate
of the parameters of a negative-binomial distribu-
tion cannot be found in closed form (Balakrishnan
and Nevzorov, 2003). For any given segmentation,
the maximum-likelihood setting for ? is found via
a gradient-based search. This setting is then used
to generate another segmentation, and the process
is iterated until convergence, as in hard expectation-
maximization. The Dirichlet priors on the language
models are symmetric, and are chosen via cross-
validation. Sampling or gradient-based techniques
may be used to estimate these parameters, but this is
left for future work.
Relation to other segmentation models Other
cohesion-based techniques have typically focused
on hand-crafted similarity metrics between sen-
tences, such as cosine similarity (Galley et al, 2003;
Malioutov and Barzilay, 2006). In contrast, the
model described here is probabilistically motivated,
maximizing the joint probability of the segmentation
with the observed words and gestures. Our objec-
tive criterion is similar in form to that of Utiyama
and Isahara (2001); however, in contrast to this prior
856
work, our criterion is justified by a Bayesian ap-
proach. Also, while the smoothing in our approach
arises naturally from the symmetric Dirichlet prior,
Utiyama and Isahara apply Laplace?s rule and add
pseudo-counts of one in all cases. Such an approach
would be incapable of flexibly balancing the contri-
butions of each modality.
5 Evaluation Setup
Dataset Our dataset is composed of fifteen audio-
video recordings of dialogues limited to three min-
utes in duration. The dataset includes nine differ-
ent pairs of participants. In each video one of five
subjects is discussed. The potential subjects include
a ?Tom and Jerry? cartoon, a ?Star Wars? toy, and
three mechanical devices: a latchbox, a piston, and
a candy dispenser. One participant ? ?participant A?
? was familiarized with the topic, and is tasked with
explaining it to participant B, who is permitted to
ask questions. Audio from both participants is used,
but only video of participant A is used; we do not ex-
amine whether B?s gestures are relevant to discourse
segmentation.
Video was recorded using standard camcorders,
with a resolution of 720 by 480 at 30 frames per
second. The video was reduced to 360 by 240 gray-
scale images before visual analysis is applied. Audio
was recorded using headset microphones. No man-
ual postprocessing is applied to the video.
Annotations and data processing All speech was
transcribed by hand, and time stamps were obtained
using the SPHINX-II speech recognition system for
forced alignment (Huang et al, 1993). Sentence
boundaries are annotated according to (NIST, 2003),
and additional sentence boundaries are automati-
cally inserted at all turn boundaries. Commonly-
occurring terms unlikely to impact segmentation are
automatically removed by using a stoplist.
For automatic speech recognition, the default Mi-
crosoft speech recognizer was applied to each sen-
tence, and the top-ranked recognition result was re-
ported. As is sometimes the case in real-world ap-
plications, no speaker-specific training data is avail-
able. The resulting recognition quality is very poor,
yielding a word error rate of 77%.
Annotators were instructed to select segment
boundaries that divide the dialogue into coherent
topics. Segmentation points are required to coincide
with sentence or turn boundaries. A second annota-
tor ? who is not an author on any paper connected
with this research ? provided an additional set of
segment annotations on six documents. On this sub-
set of documents, the Pk between annotators was
.306, and the WindowDiff was .325 (these metrics
are explained in the next subsection). This is simi-
lar to the interrater agreement reported byMalioutov
and Barzilay (2006).
Over the fifteen dialogues, a total of 7458 words
were transcribed (497 per dialogue), spread over
1440 sentences or interrupted turns (96 per dia-
logue). There were a total of 102 segments (6.8
per dialogue), from a minimum of four to a maxi-
mum of ten. This rate of fourteen sentences or in-
terrupted turns per segment indicates relatively fine-
grained segmentation. In the physics lecture corpus
used by Malioutov and Barzilay (2006), there are
roughly 100 sentences per segment. On the ICSI
corpus of meeting transcripts, Galley et al (2003)
report 7.5 segments per meeting, with 770 ?poten-
tial boundaries,? suggesting a similar rate of roughly
100 sentences or interrupted turns per segment.
The size of this multimodal dataset is orders of
magnitude smaller than many other segmentation
corpora. For example, the Broadcast News corpus
used by Beeferman et al (1999) and others con-
tains two million words. The entire ICSI meeting
corpus contains roughly 600,000 words, although
only one third of this dataset was annotated for seg-
mentation (Galley et al, 2003). The physics lecture
corpus that was mentioned above contains 232,000
words (Malioutov and Barzilay, 2006). The task
considered in this section is thus more difficult than
much of the previous discourse segmentation work
on two dimensions: there is less training data, and a
finer-grained segmentation is required.
Metrics All experiments are evaluated in terms
of the commonly-used Pk (Beeferman et al, 1999)
and WindowDiff (WD) (Pevzner and Hearst, 2002)
scores. These metrics are penalties, so lower val-
ues indicate better segmentations. The Pk metric
expresses the probability that any randomly chosen
pair of sentences is incorrectly segmented, if they
are k sentences apart (Beeferman et al, 1999). Fol-
lowing tradition, k is set to half of the mean seg-
857
Method Pk WD
1. gesture only .486 .502
2. ASR only .462 .476
3. ASR + gesture .388 .401
4. transcript only .382 .397
5. transcript + gesture .332 .349
6. random .473 .526
7. equal-width .508 .515
Table 1: For each method, the score of the best perform-
ing configuration is shown. Pk and WD are penalties, so
lower values indicate better performance.
ment length. The WindowDiff metric is a varia-
tion of Pk (Pevzner and Hearst, 2002), applying a
penalty whenever the number of segments within the
k-sentence window differs for the reference and hy-
pothesized segmentations.
Baselines Two na??ve baselines are evaluated.
Given that the annotator has divided the dialogue
into K segments, the random baseline arbitrary
chooses K random segmentation points. The re-
sults of this baseline are averaged over 1000 itera-
tions. The equal-width baseline places boundaries
such that all segments contain an equal number of
sentences. Both the experimental systems and these
na??ve baselines were given the correct number of
segments, and also were provided with manually an-
notated sentence boundaries ? their task is to select
the k sentence boundaries that most accurately seg-
ment the text.
6 Results
Table 1 shows the segmentation performance for a
range of feature sets, as well as the two baselines.
Given only gesture features the segmentation results
are poor (line 1), barely outperforming the baselines
(lines 6 and 7). However, gesture proves highly ef-
fective as a supplementary modality. The combina-
tion of gesture with ASR transcripts (line 3) yields
an absolute 7.4% improvement over ASR transcripts
alone (line 4). Paired t-tests show that this result
is statistically significant (t(14) = 2.71, p < .01
for both Pk and WindowDiff). Even when man-
ual speech transcripts are available, gesture features
yield a substantial improvement, reducing Pk and
WD by roughly 5%. This result is statistically sig-
nificant for both Pk (t(14) = 2.00, p < .05) and
WD (t(14) = 1.94, p < .05).
Interactions of verbal and gesture features We
now consider the relative contribution of the verbal
and gesture features. In a discriminative setting, the
contribution of each modality would be explicitly
weighted. In a Bayesian generative model, the same
effect is achieved through the Dirichlet priors, which
act to smooth the verbal and gestural multinomials ?
see equation 3. For example, when the gesture prior
is high and verbal prior is low, the gesture counts are
smoothed, and the verbal counts play a greater role
in segmentation. When both priors are very high,
the model will simply try to find equally-sized seg-
ments, satisfying the distribution over durations.
The effects of these parameters can be seen in Fig-
ure 3. The gesture model prior is held constant at
its ideal value, and the segmentation performance
is plotted against the logarithm of the verbal prior.
Low values of the verbal prior cause it to domi-
nate the segmentation; this can be seen at the left
of both graphs, where the performance of the multi-
modal and verbal-only systems are nearly identical.
High values of the verbal prior cause it to be over-
smoothed, and performance thus approaches that of
the gesture-only segmenter.
Comparison to other models While much of
the research on topic segmentation focuses on writ-
ten text, there are some comparable systems that
also aim at unsupervised segmentation of sponta-
neous spoken language. For example, Malioutov
and Barzilay (2006) segment a corpus of classroom
lectures, using similar lexical cohesion-based fea-
tures. With manual transcriptions, they report a .383
Pk and .417 WD on artificial intelligence (AI) lec-
tures, and .298 Pk and .311 WD on physics lectures.
Our results are in the range bracketed by these two
extremes; the wide range of results suggests that seg-
mentation scores are difficult to compare across do-
mains. The segmentation of physics lectures was at
a very course level of granularity, while the segmen-
tation of AI lectures was more similar to our anno-
tations.
We applied the publicly-available executable for
this algorithm to our data, but performance was
poor, yielding a .417 Pk and .465 WD even when
both verbal and gestural features were available.
858
?3 ?2.5 ?2 ?1.5 ?1 ?0.5
0.32
0.34
0.36
0.38
0.4
0.42
log verbal prior
Pk
 
 
verbal?only
multimodal
?3 ?2.5 ?2 ?1.5 ?1 ?0.5
0.32
0.34
0.36
0.38
0.4
0.42
log verbal prior
W
D
 
 
verbal?only
multimodal
Figure 3: The multimodal and verbal-only performance using the reference transcript. The x-axis shows the logarithm
of the verbal prior; the gestural prior is held fixed at the optimal value.
This may be because the technique is not de-
signed for the relatively fine-grained segmentation
demanded by our dataset (Malioutov, 2006).
7 Conclusions
This research shows a novel relationship between
gestural cohesion and discourse structure. Automat-
ically extracted gesture features are predictive of dis-
course segmentation when used in isolation; when
lexical information is present, segmentation perfor-
mance is further improved. This suggests that ges-
tures provide unique information not present in the
lexical features alone, even when perfect transcripts
are available.
There are at least two possibilities for how ges-
ture might impact topic segmentation: ?visual punc-
tuation,? and cohesion. The visual punctuation view
would attempt to identify specific gestural patterns
that are characteristic of segment boundaries. This
is analogous to research that identifies prosodic sig-
natures of topic boundaries, such as (Hirschberg and
Nakatani, 1998). By design, our model is incapable
of exploiting such phenomena, as our goal is to in-
vestigate the notion of gestural cohesion. Thus, the
performance gains demonstrated in this paper can-
not be explained by such punctuation-like phenom-
ena; we believe that they are due to the consistent
gestural themes that characterize coherent topics.
However, we are interested in pursuing the idea of
visual punctuation in the future, so as to compare the
power of visual punctuation and gestural cohesion
to predict segment boundaries. In addition, the in-
teraction of gesture and prosody suggests additional
possibilities for future research.
The videos in the dataset for this paper are fo-
cused on the description of physical devices and
events, leading to a fairly concrete set of gestures.
In other registers of conversation, gestural form may
be driven more by spatial metaphors, or may con-
sist mainly of temporal ?beats.? In such cases, the
importance of gestural cohesion for discourse seg-
mentation may depend on the visual expressivity of
the speaker. We plan to examine the extensibility of
gesture cohesion to more naturalistic settings, such
as classroom lectures.
Finally, topic segmentation provides only an out-
line of the discourse structure. Richer models of dis-
course include hierarchical structure (Grosz and Sid-
ner, 1986) and Rhetorical Structure Theory (Mann
and Thompson, 1988). The application of gestural
analysis to such models may lead to fruitful areas of
future research.
Acknowledgments
We thank Aaron Adler, C. Mario Christoudias,
Michael Collins, Lisa Guttentag, Igor Malioutov,
Brian Milch, Matthew Rasmussen, Candace Sidner,
Luke Zettlemoyer, and the anonymous reviewers.
This research was supported by Quanta Computer,
the National Science Foundation (CAREER grant
IIS-0448168 and grant IIS-0415865) and the Mi-
crosoft Research Faculty Fellowship.
859
References
Narayanaswamy Balakrishnan and Valery B. Nevzorov.
2003. A primer on statistical distributions. John Wi-
ley & Sons.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Jose? M. Bernardo and Adrian F. M. Smith. 2000.
Bayesian Theory. Wiley.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-
more, Candace L. Sidner, and Charles Rich. 2001.
Non-verbal cues for discourse structure. In Proceed-
ings of ACL, pages 106?115.
Lei Chen, Mary Harper, and Zhongqiang Huang. 2006.
Using maximum entropy (ME) model to incorporate
gesture cues for sentence segmentation. In Proceed-
ings of ICMI, pages 185?192.
Piotr Dolla?r, Vincent Rabaud, Garrison Cottrell, and
Serge Belongie. 2005. Behavior recognition via
sparse spatio-temporal features. In ICCV VS-PETS.
Alexei A. Efros, Alexander C. Berg, Greg Mori, and Ji-
tendra Malik. 2003. Recognizing action at a distance.
In Proceedings of ICCV, pages 726?733.
Jacob Eisenstein and Randall Davis. 2007. Conditional
modality fusion for coreference resolution. In Pro-
ceedings of ACL, pages 352?359.
David A. Forsyth and Jean Ponce. 2003. Computer Vi-
sion: A Modern Approach. Prentice Hall.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. Proceedings of
ACL, pages 562?569.
Dariu M. Gavrila. 1999. Visual analysis of human move-
ment: A survey. Computer Vision and Image Under-
standing, 73(1):82?98.
Barbara Grosz and Julia Hirshberg. 1992. Some into-
national characteristics of discourse structure. In Pro-
ceedings of ICSLP, pages 429?432.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proceedings of
ICSLP.
Xuedong Huang, Fileno Alleva, Mei-Yuh Hwang, and
Ronald Rosenfeld. 1993. An overview of the Sphinx-
II speech recognition system. In Proceedings of ARPA
Human Language Technology Workshop, pages 81?
86.
Michael Johnston. 1998. Unification-based multimodal
parsing. In Proceedings of COLING, pages 624?630.
Adam Kendon. 2004. Gesture: Visible Action as Utter-
ance. Cambridge University Press.
Stefan Kopp, Paul Tepper, Kim Ferriman, and Justine
Cassell. 2007. Trading spaces: How humans and hu-
manoids use speech and gesture to give directions. In
Toyoaki Nishida, editor, Conversational Informatics:
An Engineering Approach. Wiley.
Ivan Laptev. 2005. On space-time interest points. In-
ternational Journal of Computer Vision, 64(2-3):107?
123.
David G. Lowe. 1999. Object recognition from local
scale-invariant features. In Proceedings of ICCV, vol-
ume 2, pages 1150?1157.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25?32.
Igor Malioutov. 2006. Minimum cut model for spoken
lecture segmentation. Master?s thesis, Massachusetts
Institute of Technology.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8:243?281.
Craig Martell. 2005. FORM: An experiment in the anno-
tation of the kinematics of gesture. Ph.D. thesis, Uni-
versity of Pennsylvania.
David McNeill. 1992. Hand and Mind. The University
of Chicago Press.
Juan Carlos Niebles, Hongcheng Wang, and Li Fei-Fei.
2006. Unsupervised Learning of Human Action Cate-
gories Using Spatial-Temporal Words. In Proceedings
of the British Machine Vision Conference.
NIST. 2003. The Rich Transcription Fall 2003 (RT-03F)
Evaluation plan.
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103?139.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36.
Francis Quek. 2003. The catchment feature model
for multimodal language analysis. In Proceedings of
ICCV.
Mark Steedman. 1990. Structure and intonation in spo-
ken language understanding. In Proceedings of ACL,
pages 9?16.
Marc Swerts. 1997. Prosodic features at discourse
boundaries of different strength. The Journal of the
Acoustical Society of America, 101:514.
Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, and
Elizabeth Shriberg. 2001. Integrating prosodic and
lexical cues for automatic topic segmentation. Com-
putational Linguistics, 27(1):31?57.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL, pages 491?498.
860
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958?967,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Reading to Learn: Constructing Features from Semantic Abstracts
Jacob Eisenstein
?
James Clarke
?
Dan Goldwasser
?
Dan Roth
??
?
Beckman Institute for Advanced Science and Technology,
?
Department of Computer Science
University of Illinois
Urbana, IL 61801
{jacobe,clarkeje,goldwas1,danr}@illinois.edu
Abstract
Machine learning offers a range of tools
for training systems from data, but these
methods are only as good as the underly-
ing representation. This paper proposes to
acquire representations for machine learn-
ing by reading text written to accommo-
date human learning. We propose a novel
form of semantic analysis called read-
ing to learn, where the goal is to obtain
a high-level semantic abstract of multi-
ple documents in a representation that fa-
cilitates learning. We obtain this abstract
through a generative model that requires
no labeled data, instead leveraging repe-
tition across multiple documents. The se-
mantic abstract is converted into a trans-
formed feature space for learning, result-
ing in improved generalization on a rela-
tional learning task.
1 Introduction
Machine learning offers a range of powerful tools
for training systems to act in complex environ-
ments, but these methods depend on a well-chosen
representation for features. For learning to suc-
ceed the representation often must be crafted with
knowledge about the application domain. This
poses a bottleneck, requiring expertise in both ma-
chine learning and the application domain. How-
ever, domain experts often express their knowl-
edge through text; one direct expression is through
text designed to aid human learning. In this paper
we exploit text written by domain experts in or-
der to build a more expressive representation for
learning. We term this approach reading to learn.
The following scenario demonstrates the moti-
vation for reading to learn. Imagine an agent given
a task within its world/environment. The agent has
no prior knowledge of the task but can perceive the
world through low-level sensors. Learning directly
from the sensors may be difficult, as interesting
tasks typically require a complex combination of
sensors. Our goal is to acquire domain knowledge
through the semantic analysis of text, so as to pro-
duce higher-level relations through combinations
of sensors.
As a concrete example consider the problem of
learning how to make legal moves in Freecell soli-
taire. Relevant sensors may indicate if an object
is a card or a freecell, whether a card is a certain
value, and whether two values are in sequence.
Although it is possible to express the rules with
a combination of sensors, learning this combina-
tion is difficult. Text can facilitate learning by pro-
viding relations at the appropriate level of gen-
eralization. For example, the sentence: ?You can
place a card on an empty freecell,? suggests not
only which sensors are useful together but also
how these sensors should be linked. Assuming the
sensors are represented as predicates, one possi-
ble relation this sentence suggests is: r(x, y) =
card(x) ? freecell(y) ? empty(y). Armed
with this new relation the agent?s learning task
may be simpler. Throughout the paper we refer to
low-level sensory input as sensor or predicate, and
to a higher level concept as a logical formula or re-
lation.
Our approach to semantic analysis does not re-
quire a complete semantic representation of the
text. We merely wish to acquire a semantic ab-
stract of a document or document collection, and
use the discovered relations to facilitate data-
driven learning. This will allow us to directly eval-
uate the contribution of the extracted relations for
learning.
We develop an approach to recover semantic ab-
stracts that uses minimal supervision: we assume
only a very small set of lexical glosses, which map
from words to sensors. This marks a substantial
departure from previous work on semantic pars-
ing, which requires either annotations of the mean-
ings of each individual sentence (Zettlemoyer and
Collins, 2005; Liang et al, 2009), or alignments
of sentences to grounded representations of the
958
world (Chen and Mooney, 2008). For the purpose
of learning, this approach may be inapplicable, as
such text is often written at a high level of abstrac-
tion that permits no grounded representation.
There are two properties of our setting that
make unsupervised learning feasible. First, it is
not necessary to extract a semantic representation
of each individual sentence, but rather a summary
of the semantics of the document collection. Er-
rors in the semantic abstract are not fatal, as long
it guides the learning component towards a more
useful representation. Second, we can exploit rep-
etition across documents, which should generally
express the same underlying meaning. Logical for-
mulae that are well-supported by multiple docu-
ments are especially likely to be useful.
The rest of this paper describes our approach
for recovering semantic abstracts and outlines how
we apply and evaluate this approach on the Free-
cell domain. The paper contributes the following
key ideas: (1) Interpreting abstract ?instructional?
text, written at a level that does not correspond
to concrete sensory inputs in the world, so that
no grounded representation is possible, (2) read-
ing to learn, a new setting in which extracted se-
mantic representations are evaluated by whether
they facilitate learning; (3) abstractive semantic
summarization, aimed at capturing broad seman-
tic properties of a multi-document dataset, rather
than a semantic parse of individual sentences; (4) a
novel, minimally-supervised generative model for
semantic analysis which leverages both lexical and
syntactic properties of text.
2 Approach Overview
We describe our approach to text analysis as mul-
tidocument semantic abstraction, with the goal of
discovering a compact set of logical formulae to
explain the text in a document collection. To this
end, we develop a novel generative model in which
natural language sentences (e.g., ?You can always
place cards in empty freecells?) are stochastically
generated from logical formulae (e.g., card(x)?
freecell(y) ? empty(y)). We formally define
a generative process that reflects our intuitions
about the relationship between formulae and sen-
tences (Section 3), and perform sampling-based
inference to recover the formulae most likely to
have generated the observed data (Section 4). The
top N such formulae can then be added as addi-
tional predicates for relational learning.
Our semantic representation consists of con-
junctions of literals, each of which includes a sin-
gle predicate (e.g., empty) and one or more vari-
ables (e.g., x). Predicates describe atomic seman-
tic concepts, while variables construct networks
of relationships between them. While the impor-
tance of the predicates is obvious, the variable
assignments also exert a crucial influence on the
semantics of the conjunction: modifying a sin-
gle variable in the formula above from empty(y)
to empty(x) yields a formula that is trivially
false for all groundings (since cards can never be
empty).
Thus, our generative model must account for the
influence of both predicates and variables on the
sentences in the documents. A natural choice is to
use the predicates to influence the lexical items,
while letting the variables determine the syntac-
tic structure. For example, the formula card(x)?
freecell(y) ? empty(y) contains three pred-
icates and two variables. The predicates influence
the lexical items in a direct way: we expect that
sentences generated from this formula will include
a member of the gloss set for each predicate ?
the sentence ?Put the cards on the empty free-
cells? should be more likely than ?Columns are
constructed by playing cards in alternating colors.?
The impact of the variables on the generative
process is more subtle. The sharing of the variable
y suggests a relationship between the predicates
freecell and empty. This should be realized
in the syntactic structure of the sentence. Model-
ing syntax using a dependency tree, we expect that
the glosses for predicates that share terms will ap-
pear in compact sub-trees, while predicates that do
not share terms should be more distant. One pos-
sible surface realization of this logical formula is
the sentence, ?Put the card on the empty freecell,?
whose dependency parse is shown in the left tree
of Figure 1. The glosses empty and freecell are im-
mediately adjacent, while card is more remote.
We develop two metrics that quantify the com-
pactness of a set of variable assignments with
respect to a dependency tree: excess terms, and
shared terms. The number of excess terms in a
subtree is the number of unique terms assigned
to words in the subtree, minus the maximum arity
of any predicate in the subtree. Shared terms arise
whenever a node has multiple subtrees which each
contain the same variable. We will use the alterna-
tive alignments in Figure 1 to provide a more de-
tailed explanation. In each tree, the variables are
written in the nodes belonging to the associated
lexical items; variables are written over arrows to
indicate membership in some node in the subtree.
Excess Terms Alignment A of Fig-
ure 1, corresponding to the formula
959
Put
card on
freecell
the empty
the
X
X
X
XXX
X XX
XX
X
X
X
Y
XXY
X XY
XY
Y
X
Y
Y
XYY
X YY
YY
Y
X
Y
Z
XYZ
X YZ
YZ
Z
Dependency tree Alignment A Alignment B Alignment C Alignment D
Figure 1: A dependency parse and four different variable assignments. Each literal is aligned to a word (a
node in the graph), and the associated variables are written in the box. Variables belonging to descendant
nodes are written over the arrows.
card(x)?freecell(x)?empty(x), has zero
excess terms in every subtree; there is a total of one
variable, and all the predicates are unary. In Align-
ment B, card(x) ? freecell(x) ? empty(y),
there are excess terms at the root, and in the top
two subtrees on the right-hand side. Alignment C
contains an excess term at only the root node.
Even though it contains the same number of
unique variables as Alignment B, it is not penal-
ized as harshly because the alignment of variables
better corresponds to the syntactic structure.
Alignment D contains the greatest number of
excess terms: two at the root of the tree, and one
in each of the top two subtrees on the right side.
Shared Terms According to the excess term
metric, the best choice is simply to introduce as
few variables as possible. For this reason, we also
penalize shared terms which occur when a node
has subtree children that share a variable. In Fig-
ure 1, Alignments A and B each contain a shared
term at the top node; Alignments C and D contain
no shared terms.
Overall, we note that Alignment B is penalized
on both metrics, as it contains both excess terms
and shared terms; the syntactic structure of the
sentence makes such a variable assignment rela-
tively improbable.
card(x) & freecell(y) & empty(y)
f(y)e(y)c(x)
f(y)e(y)c(x)
Put the card on the empty freecell
(a)
(b)
(c)
(d)
(e)
Figure 2: A graphical depiction of the generative
process by which sentences are produced from for-
mulae
3 Generative Model
These intuitions are formalized in a generative
account of how sentences are stochastically pro-
duced from a set of logical formulae. This gener-
ative story guides an inference procedure for re-
covering logical formulae that are likely to have
generated any observed set of texts, which is de-
scribed in Section 4.
The outline of the generative process is depicted
in Figure 2. For each sentence, we begin in step (a)
by drawing a formula f from a Dirichlet pro-
cess (Ferguson, 1973). The Dirichlet process de-
960
fines a non-parametric mixture model, and has the
effect of adaptively selecting the appropriate num-
ber of formulae to explain the observed sentences
in the corpus.
1
We then draw the sentence length
from some distribution over positive integers; as
the sentence length is always observed, we need
not define the distribution (step (b)). In step (c), a
dependency tree is drawn from a uniform distribu-
tion over spanning trees with a number of nodes
equal to the length of the sentence. In step (d) we
draw an alignment of the literals in f to nodes in
the dependency tree, written a
t
(f). The distribu-
tion over alignments is described in Section 3.1.
Finally, the aligned literals are used to generate the
words at each slot in the dependency tree. A more
formal definition of this process is as follows:
? Draw ?, the expected number of literals per
formula, from a Gamma distribution G(u, v).
? Draw an infinite set of formulae f . For each
formula f
i
,
? Draw the formula length #|f
i
| from a
Poisson distribution, n
i
? Poisson(?).
? Draw n
i
literals from a uniform distri-
bution.
? Draw pi, an infinite multinomial distribution
over formulae: pi ? GEM(pi
0
), where GEM
refers to the stick-breaking prior (Sethura-
man, 1994) and pi
0
= 1 is the concentra-
tion parameter. By attaching the multinomial
pi to the infinite set of formulae f , we cre-
ate a Dirichlet process. This is conventionally
writtenDP (pi
0
, G
0
), where the base distribu-
tionG
0
encodes only the distribution over the
number of literals, Poisson(?).
? For each of D documents, draw the number
of sentences T ? Poisson. For each of the T
sentences in the document,
? Draw a formula f ? DP (pi
0
, G
0
) from
the Dirichlet Process described above.
? Draw a sentence length #|s| ? Poisson.
? Draw a dependency graph t (a spanning
tree of size #|s|) from a uniform distri-
bution.
? Draw an alignment a
t
(f), an injective
mapping from literals in f to nodes in
the dependency structure t. The distribu-
tion over alignments is described in Sec-
tion 3.1.
1
There are many recent applications of Dirichlet pro-
cesses in natural language processing, e.g. Goldwater et al
(2006).
? Draw the sentence s from the formula
f and the alignment a(f). For each
word token w
i
? s is drawn from
p(w
i
|a
t
(f, i)), where a
t
(f, i) indicates
the (possibly empty) literal assigned
to slot i in the alignment a
t
(f) (Sec-
tion 3.2).
3.1 Distribution over Alignments
The distribution over alignments reflects our intu-
ition that when literals share variables, they will
be aligned to word slots that are nearby in the de-
pendency structure; literals that do not share vari-
ables should be more distant. This is formalized by
applying the concepts of excess terms and shared
terms defined in Section 2. After computing the
number of excess and shared terms in each sub-
tree t
i
, we can compute a local score (LS ) for that
subtree:
LS (a
t
(f); t
i
) = ? ?NShared(a
t
(f), t
i
)
+ ? ?NExcess(a
t
(f), t
i
) ? height(t
i
).
This scoring function can be applied recursively to
each subtree in t; the overall score of the tree is the
recursive sum,
score(a
t
(f); t) = LS (a
t
(f); t)+
n
?
i
score(a
t
(f); t
i
),
(1)
where t
i
indicates the i
th
subtree of t. We hypoth-
esize a generative process that produces all possi-
ble alignments, scores them using score(a
t
(f); t),
and selects an alignment with probability,
p(a
t
(f)) ? exp{?score(a
t
(f); t)}. (2)
In our experiments, we define the parameters ? =
1, ? = 1.
3.2 Generation of Lexical Items
Once the logical formula is aligned to the parse
structure, the generation of the lexical items in
the sentence is straightforward. For word slots to
which no literals are aligned, the lexical item is
drawn from a language model ?, estimated from
the entire document collection. For slots to which
at least one literal is aligned, we construct a lan-
guage model ? in which the probability mass is
divided equally among all glosses of aligned pred-
icates. The language model ? is used as a backoff,
so that there is a strong bias in favor of generating
glosses, but some probability mass is reserved for
the other lexical items.
961
4 Inference
This section describes a sampling-based inference
procedure for obtaining a set of formulae f that
explain the observed text s and dependency struc-
tures t. We perform Gibbs sampling over the
formulae assigned to each sentence. Using the
Chinese Restaurant Process interpretation of the
Dirichlet Process (Aldous, 1985), we marginalize
pi, the infinite multinomial over all possible for-
mulae: at each sampling step we select either an
existing formula, or stochastically generate a new
formula. After each full round of Gibbs sampling,
a set of Metropolis-Hastings moves are applied to
explore modifications of the formulae. This proce-
dure converges on a stationary Markov chain cen-
tered on a set of formulae that cohere well with the
lexical and syntactic properties of the text.
4.1 Assigning Sentences to Formulae
For each sentence s
i
and dependency tree t
i
, a hid-
den variable y
i
indicates the index of the formula
that generates the text. We can resample y
i
using
Gibbs sampling. In the non-parametric setting, y
i
ranges over all non-negative integers; the Chinese
Restaurant Process formulation marginalizes the
infinite-dimensional parameter pi, yielding a prior
based on the counts for each ?active? formula (to
which at least one other sentence is assigned), and
a pseudo-count representing all non-active formu-
lae. Given K formulae, the prior on selecting for-
mula j is:
p(y
i
= j|y
?i
, pi
0
) ?
{
n
?i
(j) j < K
pi
0
j = K,
(3)
where y
?i
refers to the assignments of all y other
than y
i
and n
?i
refers to the counts over these as-
signments. Each j < K identifies an existing for-
mula in f , to which at least one other sentence is
assigned. When j = K, this means a new formula
f
?
must be generated.
To perform Gibbs sampling, we draw from the
posterior distribution over y
i
,
p(y
i
|s
i
, t
i
f , f
?
,y
?i
, pi
0
) ?
p(y
i
|y
?i
, pi
0
)p(s
i
, t
i
|y
i
, f , f
?
),
where the first term is the prior defined in Equa-
tion 3 and the latter term is the likelihood of gener-
ating the parsed sentence ?s
i
, t
i
? from the formula
indexed by y
i
.
To compute the probability of a parsed sentence
given a formula, we sum over alignments,
p(s, t|f) =
?
a
t
(f)
p(s, t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(4)
applying the chain rule and independence assump-
tions from the generative model. The result is a
product of three terms: the likelihood of the lexi-
cal items given the aligned predicates (defined in
Section 3.2; the likelihood of the alignment given
the dependency tree and formula (defined in equa-
tion 2), and the probability of the dependency tree
given the formula, which is uniform.
Equation 4 takes a sum across alignments, but
most of the probability mass of p(s|a
t
(f)) will
be concentrated on alignments in which predicates
cover words that gloss them. Thus, we can apply
an approximation,
p(s, t|f) ?
N
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(5)
in which we draw N samples in which predicates
are aligned to their glosses whenever possible.
Similarly, Equation 2 quantifies the likelihood
of an alignment only to a constant of proportional-
ity; again, a sum over possible alignments is nec-
essary. We do not expect the prior on alignments
to be strongly peaked like the sentence likelihood,
so we approximate the normalization term by sam-
pling M alignments at random and extrapolating:
p(a
t
(f)|t, f) ? q(a
t
(f); t)
=
q(a
t
(f); t)
?
a
?
t
(f)
q(a
?
t
(f); t)
?
#|a
?
t
(f)|
M
q(a
t
(f); t)
?
M
a
?
t
(f)
q(a
?
t
(f); t)
,
where q(a
t
(f); t) = exp{?score(a
t
(f); t)}, de-
fined in Equation 2. In our experiments, we set N
to at most 10, and M = 20. Drawing larger num-
bers of samples had no discernible effect on sys-
tem output.
962
4.1.1 Generating new formulae
Chinese Restaurant Process sampling requires the
generation of new candidate formulae at each re-
sampling stage. To generate a new formula, we
first sample the number of literals. As described
in the generative story (Section 3), the number
of literals is drawn from a Poisson distribution
with parameter ?. We treat ? as unknown and
marginalize, using the Gamma hyperprior G(u, v).
Due to Poisson-Gamma conjugacy, this marginal-
ization can be performed analytically, yielding
a Negative-Binomial distribution with parameters
?u+
?
i
#|f
i
|, (1+K+v)
?1
?, where
?
i
#|f
i
| is
the sum of the number of literals in each formula,
and K is the number of formulae which generate
at least one sentence. In this sense, the hyperpriors
u and v act as pseudo counts. We set u = 3, v = 1,
reflecting a weak prior expectation of three literals
per predicate.
After drawing the size of the formula, the predi-
cates are selected from a uniform random distribu-
tion. Finally, the terms are assigned: at each slot,
we reuse a previous term with probability 0.5, un-
less none is available; otherwise a new term is gen-
erated.
4.2 Proposing changes to formulae
The assignment resampling procedure has the
ability to generate new formulae, thus exploring
the space of relational features. However, to ex-
plore this space more rapidly, we introduce four
Metropolis-Hastings moves that modify existing
formulae (Gilks, 1995): adding a literal, deleting
a literal, substituting a literal, and rearranging the
terms of the formula. For each proposed move, we
recompute the joint likelihood of the formula and
all aligned sentences. The move is stochastically
accepted based on the ratio of the joint likelihoods
of the new and old configurations, multiplied by a
Hastings correction.
The joint likelihood with respect to formula f
is computed as p(s, t, f) = p(f)
?
i
p(s
i
, t
i
|f).
The prior on f considers only the number of liter-
als, using a Negative-Binomial distribution as de-
scribed in section 4.1.1. The likelihood p(s
i
, t
i
|f)
is given in equation 4. The Hastings correction is
p?(f
?
? f)/p?(f ? f
?
), with p?(f ? f
?
) indicat-
ing the probability of proposing a move from f
to f
?
,and p?(f
?
? f) indicating the probability of
proposing the reverse move. The Hastings correc-
tions depend on the arity of the predicates being
added and removed; the derivation is straightfor-
ward but tedious. We plan to release a technical
report with complete details.
4.3 Summary of inference
The final inference procedure iterates between
Gibbs sampling of assignments of formulae to
sentences, and manipulating the formulae through
Metropolis-Hastings moves. A full iteration com-
prises proposing a move to each formula, and then
using Gibbs sampling to reconsider all assign-
ments. If a formula no longer has any sentences
assigned to it, then it is dropped from the active
set, and can no longer be selected in Gibbs sam-
pling ? this is standard in the Chinese Restaurant
Process.
Five separate Markov chains are maintained in
parallel. To allow the sampling procedure to con-
verge to a stationary distribution, each chain be-
gins with 100 iterations of ?burn-in? sampling,
without storing the output. At this point, we per-
form another 100 iterations, storing the state at the
end of each iteration.
2
All formulae are ranked ac-
cording to the cumulative number of sentences to
which they are assigned (across all five Markov
chains), aggregating the counts for multiple in-
stances of identical formulae. This yields a ranked
list of formulae which will be used in our frame-
work as features for relational learning.
5 Evaluation
Our experimental setup is designed to evaluate the
quality of the semantic abstraction performed by
our model. The logical formulae obtained by our
system are applied as features for relational learn-
ing of the rules of the game of Freecell solitaire.
We investigate whether these features enable bet-
ter generalization given varying number of train-
ing examples of Freecell game states. We also
quantify the specific role of syntax, lexical choice,
and feature expressivity in learning performance.
This section describes the details of this evalua-
tion.
5.1 Relational Learning
We perform relational learning using Inductive
Logic Programming (ILP), which constructs gen-
eralized rules by assembling smaller logical for-
mulae to explain observed propositional exam-
ples (Muggleton, 1995). The lowest level formu-
lae consist of basic sensors that describe the en-
vironment. ILP?s expressivity enables it to build
complex conjunctions of these building blocks,
but at the cost of tractability. Our evaluation asks
whether the logical formulae abstracted from text
2
Sampling for more iterations was not found to affect per-
formance on development data, and the model likelihood ap-
peared stationary after 100 iterations.
963
Predicate Glosses
card(x) card
tableau(x) column, tableau
freecell(x) freecell, cell
homecell(x) foundation, cell, homecell
value(x,y) ace, king, rank, 8, 3, 7, lowest,
highest
successor(x,y) higher, sequence, sequential
color(x,y) black, red, color
suit(x,y) suit, club, diamond, spade,
heart
on(x,y) onto
top(x,y) bottom, available, top
empty(x) empty
Table 1: Predicates in the Freecell world model,
with natural language glosses obtained from the
development set text.
can transform the representation to facilitate learn-
ing. We compare against both the sensor-level rep-
resentation as well as richer representations that do
not benefit from the full power of our model?s se-
mantic analysis.
The ALEPH
3
ILP system, which is primarily
based on PROGOL (Muggleton, 1995), was used
to induce the rules of game. The search parame-
ters remained constant for all experiments.
5.2 Resources
There are four types of resources required to work
in the reading-to-learn setting: a world model, in-
structional text, a small set of glosses that map
from text to elements of the world model, and la-
beled examples of correct and incorrect actions
in the world. In our experiments, we consider
the domain of Freecell solitaire, a popular card
game (Morehead and Mott-Smith, 1983) in which
cards are moved between various types of loca-
tions, depending on their suit and rank. We now
describe the resources for the Freecell domain in
more detail.
WorldModel Freecell solitaire can be described
formally using first order logic; we consider a
slightly modified version of the representation
from the Planning Domain Definition Language
(PDDL), which is used in automatic game-playing
competitions. Specifically, there are 87 constants:
52 cards, 16 locations, 13 values, four suits, and
two colors. These constants are combined with a
fixed set of 11 predicates, listed in Table 1.
Instructional Text Our approach relies on text
that describes how to operate in the Freecell soli-
taire domain. A total of five instruction sets were
3
Freely available from http://www.comlab.ox.
ac.uk/activities/machinelearning/Aleph/
obtained from the Internet. Due to the popular-
ity of the Microsoft implementation of Freecell,
instructions often contain information specific to
playing Freecell on a computer. We manually re-
moved sentences which did not focus on the card
aspects of Freecell (e.g., how to set up the board
and information regarding where to click to move
cards). In order to use our semantic abstraction
model, the instructions were part-of-speech tagged
with the Stanford POS Tagger (Toutanova and
Manning, 2000) and dependency parses were ob-
tained using Malt (Nivre, 2006).
Glosses Our reading to learn setting requires a
small set of glosses, which are surface forms com-
monly used to represent predicates from the world
model. We envision an application scenario in
which a designer manually specifies a few glosses
for each predicate. However, for the purposes of
evaluation, it would be unprincipled for the exper-
imenters to handcraft the ideal set of glosses. In-
stead, we gathered a development set of text and
annotated the lexical mentions of the world model
predicates in text. This annotation is used to ob-
tain glosses to apply to the evaluation text. This
approximates a scenario in which the designer has
a reasonable idea of how the domain will be de-
scribed in text, but no prior knowledge of the spe-
cific details of the text instructions. Our exper-
iments used glosses that occurred two or more
times in the instructions: this yields a total of 32
glosses for 11 predicates, as shown in Table 1.
Evaluation game data Ultimately, the seman-
tic abstraction obtained from the text is applied
to learning on labeled examples of correct and
incorrect actions in the world model. For evalu-
ation, we automatically generated a set of move
scenarios: game states with one positive example
(a legal move) and one negative example (an ille-
gal move). To avoid bias in the data we generate
an equal number of move scenarios from each of
three types: moves to the freecells, homecells, and
tableaux. For our experiments we vary the number
of move scenarios in the training set; the develop-
ment and test sets consist of 900 and 1500 move
scenarios respectively.
5.3 Evaluation Settings
We compare four different feature sets, which
will be provided to the ALEPH ILP learner. All
feature sets include the sensor-level predicates
shown in Table 1. The FULL-MODEL feature
set alo includes the top logical formulae ob-
tained in our model?s semantic abstract (see Sec-
964
tion 4.3). The NO-SYNTAX feature set is obtained
from a variant of our model in which the in-
fluence of syntax is removed by setting parame-
ters ?, ? = 0. The SENSORS-ONLY feature set
uses only the sensor-level predicates. Finally, the
RELATIONAL-RANDOM feature set is constructed
by replacing each feature in the FULL-MODEL set
with a randomly generated relational feature of
identical expressivity (each predicate is replaced
by a randomly chosen alternative with identical
arity; terms are also assigned randomly). This en-
sures that any performance gains obtained by our
model were not due merely to the greater expres-
sivity of its relational features. The number of fea-
tures included in each scenario is tuned on a de-
velopment set of test examples.
The performance metric assesses the ability
of the ILP learner to classify proposed Freecell
moves as legal or illegal. As the evaluation set
contains an equal number of positive and negative
examples, accuracy is the appropriate metric. The
training scenarios are randomly generated; we re-
peat each run 50 times and average our results. For
the RELATIONAL-RANDOM feature set ? in which
predicates and terms are chosen randomly ? we
also regenerate the formulae per run.
6 Results
Table 2 shows a comparison of the results
using the setup described above. Our FULL-
MODEL achieves the best performance at ev-
ery training set size, consistently outperforming
the SENSORS-ONLY representation by an abso-
lute difference of three to four percent. This
demonstrates the semantic abstract obtained by
our model does indeed facilitate machine learning
in this domain.
RELATIONAL-RANDOM provides a baseline of
relational features with equal expressivity to those
chosen by our model, but with the predicates and
terms selected randomly. We consistently outper-
form this baseline, demonstrate that the improve-
ment obtained over the sensors only representation
is not due merely to the added expressivity of our
features.
The third row compares against NO-SYNTAX,
a crippled version of our model that incorpo-
rates lexical features but not the syntactic struc-
ture. The results are stronger than the SENSORS-
ONLY and RELATIONAL-RANDOM baselines, but
still weaker than our full system. This demon-
strates the syntactic features incorporated by our
model result in better semantic representations of
the underlying text.
Features Number of training scenarios
15 30 60 120
SENSORS-ONLY 79.12 88.07 92.77 93.73
RELATIONAL-RANDOM 82.72 89.14 93.08 94.17
NO-SYNTAX 80.98 89.79 94.11 97.04
FULL-MODEL 82.89 91.00 95.23 97.45
Table 2: Results as number of training examples
varied. Each value represents the accuracy of the
induced rules obtained with the given feature set.
card(x
1
) ? tableau(x
2
)
card(x
1
) ? freecell(x
2
)
homecell(x
1
) ? value(x
2
,x
3
)
empty(x
1
) ? freecell(x
1
)
card(x
1
) ? top(x
1
,x
2
)
card(x
1
) ? homecell(x
2
)
freecell(x
1
) ? homecell(x
2
)
card(x
1
) ? tableau(x
1
)
card(x
1
) ? top(x
2
,x
1
)
homecell(x
1
)
card(x
1
) ? homecell(x
1
)
color(x
1
,x
2
) ? value(x
3
,x
4
)
suit(x
1
,x
2
) ? value(x
3
,x
4
)
value(x
1
,x
2
) ? value(x
3
,x
4
)
homecell(x
1
) ? successor(x
2
,x
3
)
Figure 3: The top 15 features recovered by the se-
mantic abstraction of our full model.
Figure 3 shows the top 15 formulae recovered
by the full model running on the evaluation text.
Features such as empty(x
1
) ? freecell(x
1
)
are useful because they reuse variables to ensure
that objects have key properties ? in this case, en-
suring that a freecell is empty. Other features, such
as homecell(x
1
) ? value(x
2
, x
3
), help to fo-
cus the search on useful conjunctions of predicates
(in Freecell, the legality of playing a card on a
homecell depends on the value of the card). Note
that three of these 15 formulae are trivially use-
less, in that they are always false: e.g., card(x
1
)
? tableau(x
1
). This illustrates the importance
of term assignment in obtaining useful features
for learning. In the NO-SYNTAX system, which
ignores the relationship between term assignment
and syntactic structure, eight of the top 15 formu-
lae were trivially useless due to term incompatibil-
ity.
7 Related Work
This paper draws on recent literature on extract-
ing logical forms from surface text (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005; Downey
et al, 2005; Liang et al, 2009), interpreting lan-
guage in the context of a domain (Chen and
Mooney, 2008), and using an actionable domain
to guide text interpretation (Branavan et al, 2009).
We differentiate our research in several dimen-
sions:
965
Language Interpretation Instructional text de-
scribes generalized statements about entities in
the domain and the way they interact, thus the
text does not correspond directly to concrete sen-
sory inputs in the world (i.e., a specific world
state). Our interpretation captures these general-
izations as first-order logic statements that can be
evaluated given a specific state. This contrasts to
previous work which interprets ?directions? and
thus assumes a direct correspondence between text
and world state (Branavan et al, 2009; Chen and
Mooney, 2008).
Supervision Our work avoids supervision in the
form of labeled examples, using only a minimal
set of natural language glosses per predicate. Pre-
vious work also considered the supervision signal
obtained by interpreting natural language in the
context of a formal domain. Branavan et al (2009)
use feedback from a world model as a supervi-
sion signal. Chen and Mooney (2008) use tempo-
ral alignment of text and grounded descriptions of
the world state. In these approaches, concrete do-
main entities are grounded in language interpreta-
tion, and therefore require only a propositional se-
mantic representation. Previous approaches for in-
terpreting generalized natural language statements
are trained from labeled examples (Zettlemoyer
and Collins, 2005; Lu et al, 2008).
Level of analysis We aim for an abstractive
semantic summary across multiple documents,
whereas other approaches attempt to produce log-
ical forms for individual sentences (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005). We
avoid the requirement that each sentence have a
meaningful interpretation within the domain, al-
lowing us to handle relatively unstructured text.
Evaluation We do not evaluate the representa-
tions obtained by our model; rather we assess
whether these representations improve learning
performance. This is similar to work on Geo-
Query (Wong and Mooney, 2007; Ge and Mooney,
2005), and also to recent work on following step-
by-step directions (Branavan et al, 2009). While
these evaluations are performed on the basis of in-
dividual sentences, actions, or system responses,
we evaluate the holistic semantic analysis obtained
by our system.
Model We treat surface text as generated from a
latent semantic description. Lu et al (2008) ap-
ply a generative model, but require a complete
derivation from semantics to the lexical represen-
tation, while we favor a more flexible semantic
analysis that can be learned without annotation
and applied to noisy text. More similar is the work
of Liang et al (2009), which models the gener-
ation of semantically-relevant fields using lexical
and discourse features. Our approach differs by
accounting for syntax, which enables a more ex-
pressive semantic representation that includes un-
grounded variables.
Relational learning The output of our semantic
analysis is applied to learning in a structured rela-
tional space, using ILP. A key difficulty with ILP
is that the increased expressivity dramatically ex-
pands the hypothesis space, and it is widely agreed
that some learning bias is required for ILP to be
tractable (N?edellec et al, 1996; Cumby and Roth,
2003). Our work can be viewed as a new method
for acquiring such bias from text; moreover, our
approach is not specialized for ILP and may be
used to transform the feature space in other forms
of relational learning as well (Roth and Yih, 2001;
Cumby and Roth, 2003; Richardson and Domin-
gos, 2006).
8 Conclusion
This paper demonstrates a new setting for seman-
tic analysis, which we term reading to learn. We
handle text which describes the world in gen-
eral terms rather than refereing to concrete enti-
ties in the domain. We obtain a semantic abstract
of multiple documents, using a novel, minimally-
supervised generative model that accounts for both
syntax and lexical choice. The semantic abstract
is represented as a set of predicate logic formu-
lae, which are applied as higher-order features for
learning. We demonstrate that these features im-
prove learning performance, and that both the lex-
ical and syntactic aspects of our model yield sub-
stantial contributions.
In the current setup, we produce an ?overgener-
ated? semantic representation comprised of useful
features for learning but also some false positives.
Learning in our system can be seen as the process
of pruning this representation by selecting useful
formulae based on interaction with the training
data. In the future we hope to explore ways to in-
terleave semantic analysis with exploration of the
learning domain, by using the environment as a
supervision signal for linguistic analysis.
Acknowledgments We thank Gerald DeJong,
Julia Hockenmaier, Alex Klementiev and the
anonymous reviewers for their helpful feedback.
This work is supported by DARPA funding under
the Bootstrap Learning Program and the Beckman
Institute Postdoctoral Fellowship.
966
References
Aldous, David J. 1985. Exchangeability and re-
lated topics. Lecture Notes in Math 1117:1?198.
Branavan, S. R. K., Harr Chen, Luke Zettle-
moyer, and Regina Barzilay. 2009. Reinforce-
ment learning for mapping instructions to ac-
tions. In Proceedings of the Joint Conference
of the Association for Computational Linguis-
tics and International Joint Conference on Nat-
ural Language Processing Processing (ACL-
IJCNLP 2009). Singapore.
Chen, David L. and Raymond J. Mooney. 2008.
Learning to sportscast: A test of grounded lan-
guage acquisition. In Proceedings of 25th In-
ternational Conference on Machine Learning
(ICML 2008). Helsinki, Finland, pages 128?
135.
Cumby, Chad and Dan Roth. 2003. On kernel
methods for relational learning. In Proceed-
ings of the Twentieth International Conference
(ICML 2003). Washington, DC, pages 107?114.
Downey, Doug, Oren Etzioni, and Stephen Soder-
land. 2005. A probabilistic model of redun-
dancy in information extraction. In Proceedings
of the International Joint Conference on Arti-
ficial Intelligence (IJCAI 2005). pages 1034?
1041.
Ferguson, Thomas S. 1973. A bayesian analysis
of some nonparametric problems. The Annals
of Statistics 1(2):209?230.
Ge, Ruifang and Raymond J. Mooney. 2005. A
statistical semantic parser that integrates syn-
tax and semantics. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL-2005). Ann Arbor,
MI, pages 128?135.
Gilks, Walter R. 1995. Markov Chain Monte
Carlo in Practice. Chapman & Hall/CRC.
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics
(COLING-ACL 2006). Sydney, Australia, pages
673?680.
Liang, Percy, Michael Jordan, and Dan Klein.
2009. Learning semantic correspondences with
less supervision. In Proceedings of the Joint
Conference of the Association for Computa-
tional Linguistics and International Joint Con-
ference on Natural Language Processing Pro-
cessing (ACL-IJCNLP 2009). Singapore.
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning representa-
tions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2008).
Honolulu, Hawaii, pages 783?792.
Morehead, Albert H. and Geoffrey Mott-Smith.
1983. The Complete Book of Solitaire and Pa-
tience Games. Bantam.
Muggleton, Stephen. 1995. Inverse entailment and
progol. New Generation Computing Journal
13:245?286.
N?edellec, C., C. Rouveirol, H. Ad?e, F. Bergadano,
and B. Tausend. 1996. Declarative bias in ILP.
In L. De Raedt, editor, Advances in Inductive
Logic Programming, IOS Press, pages 82?103.
Nivre, Joakim. 2006. Inductive dependency pars-
ing. Springer.
Richardson, Matthew and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107?136.
Roth, Dan and Wen-tau Yih. 2001. Relational
learning via propositional algorithms: An infor-
mation extraction case study. In Proceedings of
the International Joint Conference on Artificial
Intelligence (IJCAI 2001). pages 1257?1263.
Sethuraman, Jayaram. 1994. A constructive def-
inition of dirichlet priors. Statistica Sinica
4(2):639?650.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used
in a maximum entropy part-of-speech tagger.
In Proceedings of the Joint SIGDAT Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora
(EMNLP/VLC-2000). pages 63?70.
Wong, Yuk Wah and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic
parsing with lambda calculus. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 2007). Prague,
Czech Republic, pages 128?135.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form:
Structured classification with probabilistic cat-
egorial grammars. In Proceedings of the 21st
Conference on Uncertainty in Artificial Intelli-
gence (UAI 2005). pages 658?666.
967
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 353?361,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion
Jacob Eisenstein
Beckman Institute for Advanced Science and Technology
University of Illinois
Urbana, IL 61801
jacobe@illinois.edu
Abstract
This paper presents a novel unsupervised
method for hierarchical topic segmentation.
Lexical cohesion ? the workhorse of unsu-
pervised linear segmentation ? is treated as
a multi-scale phenomenon, and formalized
in a Bayesian setting. Each word token is
modeled as a draw from a pyramid of la-
tent topic models, where the structure of the
pyramid is constrained to induce a hierarchi-
cal segmentation. Inference takes the form
of a coordinate-ascent algorithm, iterating be-
tween two steps: a novel dynamic program
for obtaining the globally-optimal hierarchi-
cal segmentation, and collapsed variational
Bayesian inference over the hidden variables.
The resulting system is fast and accurate, and
compares well against heuristic alternatives.
1 Introduction
Recovering structural organization from unformat-
ted texts or transcripts is a fundamental problem
in natural language processing, with applications to
classroom lectures, meeting transcripts, and chat-
room logs. In the unsupervised setting, a variety
of successful systems have leveraged lexical cohe-
sion (Halliday and Hasan, 1976) ? the idea that
topically-coherent segments display consistent lex-
ical distributions (Hearst, 1994; Utiyama and Isa-
hara, 2001; Eisenstein and Barzilay, 2008). How-
ever, such systems almost invariably focus on linear
segmentation, while it is widely believed that dis-
course displays a hierarchical structure (Grosz and
Sidner, 1986). This paper introduces the concept of
multi-scale lexical cohesion, and leverages this idea
in a Bayesian generative model for hierarchical topic
segmentation.
The idea of multi-scale cohesion is illustrated
by the following two examples, drawn from the
Wikipedia entry for the city of Buenos Aires.
There are over 150 city bus lines called Colec-
tivos ... Colectivos in Buenos Aires do not
have a fixed timetable, but run from 4 to sev-
eral per hour, depending on the bus line and
time of the day.
The Buenos Aires metro has six lines, 74 sta-
tions, and 52.3 km of track. An expansion
program is underway to extend existing lines
into the outer neighborhoods. Track length is
expected to reach 89 km...
The two sections are both part of a high-level seg-
ment on transportation. Words in bold are charac-
teristic of the subsections (buses and trains, respec-
tively), and do not occur elsewhere in the transporta-
tion section; words in italics occur throughout the
high-level section, but not elsewhere in the article.
This paper shows how multi-scale cohesion can be
captured in a Bayesian generative model and ex-
ploited for unsupervised hierarchical topic segmen-
tation.
Latent topic models (Blei et al, 2003) provide a
powerful statistical apparatus with which to study
discourse structure. A consistent theme is the treat-
ment of individual words as draws from multinomial
language models indexed by a hidden ?topic? asso-
ciated with the word. In latent Dirichlet alocation
(LDA) and related models, the hidden topic for each
word is unconstrained and unrelated to the hidden
topic of neighboring words (given the parameters).
In this paper, the latent topics are constrained to pro-
duce a hierarchical segmentation structure, as shown
in Figure 1.
353
? ?
w1?...?wT
?8?6 ?7?1 ?2 ?3 ?4 ?5
Figure 1: Each word wt is drawn from a mixture of the
language models located above t in the pyramid.
These structural requirements simplify inference,
allowing the language models to be analytically
marginalized. The remaining hidden variables are
the scale-level assignments for each word token.
Given marginal distributions over these variables, it
is possible to search the entire space of hierarchical
segmentations in polynomial time, using a novel dy-
namic program. Collapsed variational Bayesian in-
ference is then used to update the marginals. This
approach achieves high quality segmentation on
multiple levels of the topic hierarchy.
Source code is available at http://people.
csail.mit.edu/jacobe/naacl09.html.
2 Related Work
The use of lexical cohesion (Halliday and Hasan,
1976) in unsupervised topic segmentation dates back
to Hearst?s seminal TEXTTILING system (1994).
Lexical cohesion was placed in a probabilistic
(though not Bayesian) framework by Utiyama and
Isahara (2001). The application of Bayesian topic
models to text segmentation was investigated first
by Blei and Moreno (2001) and later by Purver et
al. (2006), using HMM-like graphical models for
linear segmentation. Eisenstein and Barzilay (2008)
extend this work by marginalizing the language
models using the Dirichlet compound multinomial
distribution; this permits efficient inference to be
performed directly in the space of segmentations.
All of these papers consider only linear topic seg-
mentation; we introduce multi-scale lexical cohe-
sion, which posits that the distribution of some
words changes slowly with high-level topics, while
others change rapidly with lower-level subtopics.
This gives a principled mechanism to model hier-
archical topic segmentation.
The literature on hierarchical topic segmentation
is relatively sparse. Hsueh et al (2006) describe a
supervised approach that trains separate classifiers
for topic and sub-topic segmentation; more relevant
for the current work is the unsupervised method
of Yaari (1997). As in TEXTTILING, cohesion is
measured using cosine similarity, and agglomerative
clustering is used to induce a dendrogram over para-
graphs; the dendrogram is transformed into a hier-
archical segmentation using a heuristic algorithm.
Such heuristic approaches are typically brittle, as
they include a number of parameters that must be
hand-tuned. These problems can be avoided by
working in a Bayesian probabilistic framework.
We note two orthogonal but related approaches
to extracting nonlinear discourse structures from
text. Rhetorical structure theory posits a hierarchi-
cal structure of discourse relations between spans of
text (Mann and Thompson, 1988). This structure is
richer than hierarchical topic segmentation, and the
base level of analysis is typically more fine-grained
? at the level of individual clauses. Unsupervised
approaches based purely on cohesion are unlikely to
succeed at this level of granularity.
Elsner and Charniak (2008) propose the task of
conversation disentanglement from internet chat-
room logs. Unlike hierarchical topic segmentation,
conversational threads may be disjoint, with un-
related threads interposed between two utterances
from the same thread. Elsner and Charniak present a
supervised approach to this problem, but the devel-
opment of cohesion-based unsupervised methods is
an interesting possibility for future work.
3 Model
Topic modeling is premised on a generative frame-
work in which each word wt is drawn from a multi-
nomial ?yt , where yt is a hidden topic indexing the
language model that generates wt. From a modeling
standpoint, linear topic segmentation merely adds
the constraint that yt ? {yt?1, yt?1 + 1}. Segmen-
tations that draw boundaries so as to induce com-
pact, low-entropy language models will achieve a
354
high likelihood. Thus topic models situate lexical
cohesion in a probabilistic setting.
For hierarchical segmentation, we take the hy-
pothesis that lexical cohesion is a multi-scale phe-
nomenon. This is represented with a pyramid of lan-
guage models, shown in Figure 1. Each word may be
drawn from any language model above it in the pyra-
mid. Thus, the high-level language models will be
required to explain words throughout large parts of
the document, while the low-level language models
will be required to explain only a local set of words.
A hidden variable zt indicates which level is respon-
sible for generating the word wt.
Ideally we would like to choose the segmentation
y? = argmaxyp(w|y)p(y). However, we must deal
with the hidden language models ? and scale-level
assignments z. The language models can be inte-
grated out analytically (Section 3.1). Given marginal
likelihoods for the hidden variables z, the globally
optimal segmentation y? can be found using a dy-
namic program (Section 4.1). Given a segmentation,
we can estimate marginals for the hidden variables,
using collapsed variational inference (Section 4.2).
We iterate between these procedures in an EM-like
coordinate-ascent algorithm (Section 4.4) until con-
vergence.
3.1 Language models
We begin the formal presentation of the model with
some notation. Each word wt is modeled as a single
draw from a multinomial language model ?j . The
language models in turn are drawn from symmetric
Dirichlet distributions with parameter ?. The num-
ber of language models is written K; the number of
words is W ; the length of the document is T ; and
the depth of the hierarchy is L.
For hierarchical segmentation, the vector yt indi-
cates the segment index of t at each level of the topic
hierarchy; the specific level of the hierarchy respon-
sible for wt is given by the hidden variable zt. Thus,
y(zt)t is the index of the language model that gener-
ates wt.
With these pieces in place, we can write the ob-
servation likelihood,
p(w|y, z,?) =
T?
t
p(wt|?y(zt)t )
=
K?
j
?
{t:y(zt)t =j}
p(wt|?j),
where we have merely rearranged the product to
group terms that are drawn from the same language
model. As the goal is to obtain the hierarchical seg-
mentation and not the language models, the search
space can be reduced by marginalizing ?. The
derivation is facilitated by a notational convenience:
xj represents the lexical counts induced by the set
of words {wt : y(zt)t = j}.
p(w|y, z, ?) =
K?
j
?
d?jp(?j |?)p(xj |?j)
=
K?
j
pdcm(xj ;?)
=
K?
j
?(W?)
?(?Wi xji + ?)
W?
i
?(xji + ?)
?(?) .
(1)
Here, pdcm indicates the Dirichlet compound
multinomial distribution (Madsen et al, 2005),
which is the closed form solution to the integral over
language models. Also known as the multivariate
Polya distribution, the probability density function
can be computed exactly as a ratio of gamma func-
tions. Here we use a symmetric Dirichlet prior ?,
though asymmetric priors can easily be applied.
Thus far we have treated the hidden variables
z as observed. In fact we will compute approxi-
mate marginal probabilities Qzt(zt), written ?t` ?
Qzt(zt = `). Writing ?x?Qz for the expectation of x
under distribution Qz , we approximate,
?pdcm(xj ;?)?Qz ? pdcm(?xj?Qz ;?)
?xj(i)?Qz =
?
{t:j?yt}
L?
`
?(wt = i)?(y(`)t = j)?t`,
355
where xj(i) indicates the count for word type i gen-
erated from segment j. In the outer sum, we con-
sider all t for possibly drawn from segment j. The
inner sum goes over all levels of the pyramid. The
delta functions take the value one if the enclosed
Boolean expression is true and zero otherwise, so
we are adding the fractional counts ?t` only when
wt = i and y(`)t = j.
3.2 Prior on segmentations
Maximizing the joint probability p(w,y) =
p(w|y)p(y) leaves the term p(y) as a prior on seg-
mentations. This prior can be used to favor segmen-
tations with the desired granularity. Consider a prior
of the form p(y) = ?L`=1 p(y(`)|y(`?1)); for nota-
tional convenience, we introduce a base level such
that y(0)t = t, where every word is a segmentation
point. At every level ` > 0, the prior is a Markov
process, p(y(`)|y(`?1)) = ?Tt p(y(`)t |y(`)t?1,y(`?1)).
The constraint y(`)t ? {y(`)t?1, y(`)t?1 + 1} ensures a
linear segmentation at each level. To enforce hierar-
chical consistency, each y(`)t can be a segmentation
point only if t is also a segmentation point at the
lower level ` ? 1. Zero probability is assigned to
segmentations that violate these constraints.
To quantify the prior probability of legal segmen-
tations, assume a set of parameters d`, indicating
the expected segment duration at each level. If t
is a valid potential segmentation point at level `
(i.e., y(`?1)t = 1 + y(`?1)t?1 ), then the prior probabil-
ity of a segment transition is r` = d`?1/d`, with
d0 = 1. If there are N segments in level ` and
M ? N segments in level ` ? 1, then the prior
p(y(`)|y(`?1)) = rN` (1 ? r`)M?N , as long as the
hierarchical segmentation constraint is obeyed.
For the purposes of inference it will be prefer-
able to have a prior that decomposes over levels and
segments. In particular, we do not want to have to
commit to a particular segmentation at level ` be-
fore segmenting level ` + 1. The above prior can
be approximated by replacing M with its expecta-
tion ?M?d`?1 = T/d`?1. Then a single segment
ranging from wu to wv (inclusive) will contribute
log r` + v?ud`?1 log(1? r`) to the log of the prior.
4 Inference
This section describes the inference for the segmen-
tation y, the approximate marginals QZ , and the hy-
perparameter ?.
4.1 Dynamic programming for hierarchical
segmentation
While the model structure is reminiscent of a facto-
rial hidden Markov model (HMM), there are impor-
tant differences that prevent the direct application of
HMM inference. Hidden Markov models assume
that the parameters of the observation likelihood dis-
tributions are available directly, while we marginal-
ize them out. This has the effect of introducing de-
pendencies throughout the state space: the segment
assignment for each yt contributes to lexical counts
which in turn affect the observation likelihoods for
many other t?. However, due to the left-to-right na-
ture of segmentation, efficient inference of the opti-
mal hierarchical segmentation (given the marginals
QZ) is still possible.
Let B(`)[u, v] represent the log-likelihood of
grouping together all contiguous words wu . . . wv?1
at level ` of the segmentation hierarchy. Using xt
to indicate a vector of zeros with one at the position
wt, we can express B more formally:
B(`)[u, v] = log pdcm
( v?
t=u
xt?t`
)
+ log r` + v ? u? 1d`?1 log(1? r`).
The last two terms are from the prior p(y), as ex-
plained in Section 3.2. The value of B(`)[u, v] is
computed for all u, all v > u, and all `.
Next, we compute the log-likelihood of the op-
timal segmentation, which we write as A(L)[0, T ].
This matrix can be filled in recursively:
A(`)[u, v] = max
u?t<v
B(`)[t, v] +A(`?1)[t, v] +A(`)[u, t].
The first term adds in the log probability of the
segment from t to v at level `. The second term re-
turns the best score for segmenting this same interval
at a more detailed level of segmentation. The third
term recursively segments the interval from u to t at
the same level `. The boundary case A(`)[u, u] = 0.
356
4.1.1 Computational complexity
The sizes of A and B are each O(LT 2). The ma-
trix A can be constructed by iterating through the
layers and then iterating: u from 1 to T ; v from u+1
to T ; and t from u to v + 1. Thus, the time cost for
filling A is O(LT 3). For computing the observation
likelihoods in B, the time complexity isO(LT 2W ),
where W is the size of the vocabulary ? by keeping
cumulative lexical counts, we can compute B[u, v]
without iterating from u to v.
Eisenstein and Barzilay (2008) describe a dy-
namic program for linear segmentation with a
space complexity of O(T ) and time complexities of
O(T 2) to compute the A matrix and O(TW ) to fill
the B matrix.1 Thus, moving to hierarchical seg-
mentation introduces a factor of TL to the complex-
ity of inference.
4.1.2 Discussion
Intuitively, efficient inference is possible because
the location of each segment boundary affects the
likelihood of only the adjoining segments at the
same level of the hierarchy, and their ?children? at
the lower levels of the hierarchy. Thus, the observa-
tion likelihood at each level decomposes across the
segments of the level. This is due to the left-to-right
nature of segmentation ? in general it is not possible
to marginalize the language models and still perform
efficient inference in HMMs. The prior (Section 3.2)
was designed to decompose across segments ? if, for
example, p(y) explicitly referenced the total number
of segments, inference would be more difficult.
A simpler inference procedure would be a greedy
approach that makes a fixed decision about the top-
level segmentation, and then applies recursion to
achieve segmentation at the lower levels. The greedy
approach will not be optimal if the best top-level
segmentation leads to unsatisfactory results at the
lower levels, or if the lower levels could help to
disambiguate high-level segmentation. In contrast,
the algorithm presented here maximizes the overall
score across all levels of the segmentation hierarchy.
1The use of dynamic programming for linear topic segmen-
tation goes back at least to (Heinonen, 1998); however, we are
aware of no prior work on dynamic programming for hierarchi-
cal segmentation.
4.2 Scale-level marginals
The hidden variable zt represents the level of the
segmentation hierarchy from which the word wt is
drawn. Given language models ?, each wt can
be thought of as a draw from a Bayesian mixture
model, with zt as the index of the component that
generates wt. However, as we are marginalizing
the language models, standard mixture model infer-
ence techniques do not apply. One possible solu-
tion would be to instantiate the maximum a posteri-
ori language models after segmenting, but we would
prefer not to have to commit to specific language
models. Collapsed Gibbs sampling (Griffiths and
Steyvers, 2004) is another possibility, but sampling-
based solutions may not be ideal from a performance
standpoint.
Recent papers by Teh et al (2007) and Sung et
al. (2008) point to an appealing alternative: col-
lapsed variational inference (called latent-state vari-
ational Bayes by Sung et al). Collapsed variational
inference integrates over the parameters (in this
case, the language models) and computes marginal
distributions for the latent variables, Qz. However,
due to the difficulty of computing the expectation
of the normalizing term, these marginal probabili-
ties are available only in approximation.
More formally, we wish to compute the approx-
imate distribution Qz(z) = ?Tt Qzt(zt), factoriz-
ing across all latent variables. As is typical in vari-
ational approaches, we fit this distribution by opti-
mizing a lower bound on the data marginal likeli-
hood p(w, z|y) ? we condition on the segmentation
y because we are treating it as fixed in this part of
the inference. The lower bound can be optimized by
iteratively setting,
Qzt(zt) ? exp
{
?logP (x, z|y)??Qzt
}
,
indicating the expectation under Qz?t for all t? 6= t.Due to the couplings across z, it is not possible
to compute this expectation directly, so we use the
first-order approximation described in (Sung et al,
2008). In this approximation, the value Qzt(zt = `)
? which we abbreviate as ?t` ? takes the form of
the likelihood of the observation wt, given a mod-
ified mixture model. The parameters of the mixture
model are based on the priors and the counts of w
357
and ? for all t? 6= t:
?t` ? ?`
x??t` (wt)?W
i x??t` (i)
(2)
x??t` (i) = ?`(i) +
?
t? 6=t
?t?`?(wt? = i). (3)
The first term in equation 2 is the set of compo-
nent weights ?`, which are fixed at 1/L for all `. The
fraction represents the posterior estimate of the lan-
guage models: standard Dirichlet-multinomial con-
jugacy gives a sum of counts plus a Dirichlet prior
(equation 3). Thus, the form of the update is ex-
tremely similar to collapsed Gibbs sampling, except
that we maintain the full distribution over zt rather
than sampling a specific value. The derivation of this
update is beyond the scope of this paper, but is sim-
ilar to the mixture of Bernoullis model presented in
Section 5 of (Sung et al, 2008).
Iterative updates of this form are applied until the
change in the lower bound is less than 10?3. This
procedure appears at step 5a of algorithm 1.
4.3 Hyperparameter estimation
The inference procedure defined here includes two
parameters: ?, the symmetric Dirichlet prior on the
language models; and d, the expected segment du-
rations. The granularity of segmentation is consid-
ered to be a user-defined characteristic, so there is
no ?right answer? for how to set this parameter. We
simply use the oracle segment durations, and pro-
vide the same oracle to the baseline methods where
possible. As discussed in Section 6, this parameter
had little effect on system performance.
The ? parameter controls the expected sparsity of
the induced language models; it will be set automat-
ically. Given a segmentation y and hidden-variable
marginals ?, we can maximize p(?,w|y, ?) =
pdcm(w|y, ?, ?)p(?) through gradient descent. The
Dirichlet compound multinomial has a tractable gra-
dient, which can be computed using scaled counts,
x?j = ?t:y(zt)t =j ?tjxt (Minka, 2003). The scaledcounts are taken for each segment j across the entire
segmentation hierarchy. The likelihood p(x?|?) then
has the same form as equation 1, with the xji terms
replaced by x?ji. The gradient of the log-likelihood
Algorithm 1 Complete segmentation inference
1. Input text w; expected durations d.
2. ? ? INITIALIZE-GAMMA(w)
3. y?? EQUAL-WIDTH-SEG(w, d)
4. ?? .1
5. Do
(a) ? ? ESTIMATE-GAMMA(y?,w, ?, ?)
(b) ?? ESTIMATE-ALPHA(y?,w, ?)
(c) y? SEGMENT(w, ?, ?, d)
(d) If y = y? then return y
(e) Else y?? y
is thus a sum across segments,
d`/d? =
K?
j
W (?(W?)??(?))
+
W?
i
?(x?ji + ?)??(W?+
W?
i
x?ji).
Here, ? indicates the digamma function, which
is the derivative of the log gamma function. The
prior p(?) takes the form of a Gamma distribution
with parameters G(1, 1), which has the effect of dis-
couraging large values of ?. With these parame-
ters, the gradient of the Gamma distribution with re-
spect to ? is negative one. To optimize ?, we inter-
pose an epoch of L-BFGS (Liu and Nocedal, 1989)
optimization after maximizing ? (Step 5b of algo-
rithm 1).
4.4 Combined inference procedure
The final inference procedure alternates between up-
dating the marginals ?, the Dirichlet prior ?, and the
MAP segmentation y?. Since the procedure makes
hard decisions on ? and the segmentations y, it
can be thought of as a form of Viterbi expectation-
maximization (EM). When a repeated segmentation
is encountered, the procedure terminates. Initializa-
tion involves constructing a segmentation y? in which
each level is segmented uniformly, based on the ex-
pected segment duration d`. The hidden variable
marginals ? are initialized randomly. While there
is no guarantee of finding the global maximum, lit-
tle sensitivity to the random initialization of ? was
observed in preliminary experiments.
The dynamic program described in this section
achieves polynomial time complexity, but O(LT 3)
358
can still be slow when T is the number of word to-
kens in a large document such as a textbook. For
this reason, we only permit segment boundaries to
be placed at gold-standard sentence boundaries. The
only change to the algorithm is that the tables A
and B need contain only cells for each sentence
rather than for each word token ? hidden variable
marginals are still computed for each word token.
Implemented in Java, the algorithm runs in roughly
five minutes for a document with 1000 sentences on
a dual core 2.4 GHz machine.
5 Experimental Setup
Corpora The dataset for evaluation is drawn from
a medical textbook (Walker et al, 1990).2 The text
contains 17083 sentences, segmented hierarchically
into twelve high-level parts, 150 chapters, and 520
sub-chapter sections. Evaluation is performed sep-
arately on each of the twelve parts, with the task of
correctly identifying the chapter and section bound-
aries. Eisenstein and Barzilay (2008) use the same
dataset to evaluate linear topic segmentation, though
they evaluated only at the level of sections, given
gold standard chapter boundaries.
Practical applications of topic segmentation typ-
ically relate to more informal documents such as
blogs or speech transcripts (Hsueh et al, 2006), as
formal texts such as books already contain segmen-
tation markings provided by the author. The premise
of this evaluation is that textbook corpora provide a
reasonable proxy for performance on less structured
data. However, further clarification of this point is
an important direction for future research.
Metrics All experiments are evaluated in terms
of the commonly-used Pk and WindowDiff met-
rics (Pevzner and Hearst, 2002). Both metrics pass a
window through the document, and assess whether
the sentences on the edges of the window are prop-
erly segmented with respect to each other. Win-
dowDiff is stricter in that it requires that the number
of intervening segments between the two sentences
be identical in the hypothesized and the reference
segmentations, while Pk only asks whether the two
sentences are in the same segment or not. This eval-
2The full text of this book is available for free download at
http://onlinebooks.library.upenn.edu.
uation uses source code provided by Malioutov and
Barzilay (2006).
Experimental system The joint hierarchical
Bayesian model described in this paper is called
HIERBAYES. It performs a three-level hierarchical
segmentation, in which the lowest level is for sub-
chapter sections, the middle level is for chapters, and
the top level spans the entire part. This top-level has
the effect of limiting the influence of words that are
common throughout the document.
Baseline systems As noted in Section 2, there is
little related work on unsupervised hierarchical seg-
mentation. However, a straightforward baseline is
a greedy approach: first segment at the top level,
and then recursively feed each top-level segment to
the segmenter again. Any linear segmenter can be
plugged into this baseline as a ?black box.?
To isolate the contribution of joint inference, the
greedy framework can be combined with a one-level
version of the Bayesian segmentation algorithm de-
scribed here. This is equivalent to BAYESSEG,
which achieved the best reported performance on the
linear segmentation of this same dataset (Eisenstein
and Barzilay, 2008). The hierarchical segmenter
built by placing BAYESSEG in a greedy algorithm
is called GREEDY-BAYES.
To identify the contribution of the Bayesian
segmentation framework, we can plug in alter-
native linear segmenters. Two frequently-cited
systems are LCSEG (Galley et al, 2003) and
TEXTSEG (Utiyama and Isahara, 2001). LC-
SEG optimizes a metric of lexical cohesion based
on lexical chains. TEXTSEG employs a probabilis-
tic segmentation objective that is similar to ours,
but uses maximum a posteriori estimates of the lan-
guage models, rather than marginalizing them out.
Other key differences are that they set ? = 1, and
use a minimum description length criterion to deter-
mine segmentation granularity. Both of these base-
lines were run using their default parametrization.
Finally, as a minimal baseline, UNIFORM pro-
duces a hierarchical segmentation with the ground
truth number of segments per level and uniform du-
ration per segment at each level.
Preprocessing The Porter (1980) stemming algo-
rithm is applied to group equivalent lexical items. A
set of stop-words is also removed, using the same
359
chapter section average
# segs Pk WD # segs Pk WD Pk WD
HIERBAYES 5.0 .248 .255 8.5 .312 .351 .280 .303
GREEDY-BAYES 19.0 .260 .372 19.5 .275 .340 .268 .356
GREEDY-LCSEG 7.8 .256 .286 52.2 .351 .455 .304 .371
GREEDY-TEXTSEG 11.5 .251 .277 88.4 .473 .630 .362 .454
UNIFORM 12.5 .487 .491 43.3 .505 .551 .496 .521
Table 1: Segmentation accuracy and granularity. Both the Pk and WindowDiff (WD) metrics are penalties, so lower
scores are better. The # segs columns indicate the average number of segments at each level; the gold standard
segmentation granularity is given in the UNIFORM row, which obtains this granularity by construction.
list originally employed by several competitive sys-
tems (Utiyama and Isahara, 2001).
6 Results
Table 1 presents performance results for the joint
hierarchical segmenter and the three greedy base-
lines. As shown in the table, the hierarchical system
achieves the top overall performance on the harsher
WindowDiff metric. In general, the greedy seg-
menters each perform well at one of the two levels
and poorly at the other level. The joint hierarchical
inference of HIERBAYES enables it to achieve bal-
anced performance at the two levels.
The GREEDY-BAYES system achieves a slightly
better average Pk than HIERBAYES, but has a very
large gap between its Pk and WindowDiff scores.
The Pk metric requires only that the system cor-
rectly classify whether two points are in the same
or different segments, while the WindowDiff metric
insists that the exact number of interposing segments
be identified correctly. Thus, the generation of spu-
rious short segments may explain the gap between
the metrics.
LCSEG and TEXTSEG use heuristics to deter-
mine segmentation granularity; even though these
methods did not score well in terms of segmentation
accuracy, they were generally closer to the correct
granularity. In the Bayesian methods, granularity
is enforced by the Markov prior described in Sec-
tion 3.2. This prior was particularly ineffective for
GREEDY-BAYES, which gave nearly the same num-
ber of segments at both levels, despite the different
settings of the expected duration parameter d.
The Dirichlet prior ? was selected automatically,
but informal experiments with manual settings sug-
gest that this parameter exerts a stronger influence
on segmentation granularity. Low settings reflect an
expectation of sparse lexical counts and thus encour-
age short segments, while high settings reflect an ex-
pectation of evenly-distributed counts and thus lead
to long segments. Further investigation is needed
on how best to control segmentation granularity in a
Bayesian setting.
7 Discussion
While it is widely agreed that language often dis-
plays hierarchical topic structure (Grosz, 1977),
there have been relatively few attempts to extract
such structure automatically. This paper shows
that the lexical features that have been successfully
exploited in linear segmentation can also be used
to extract a hierarchical segmentation, due to the
phenomenon of multi-scale lexical cohesion. The
Bayesian methodology offers a principled proba-
bilistic formalization of multi-scale cohesion, yield-
ing an accurate and fast segmentation algorithm with
a minimal set of tunable parameters.
It is interesting to consider how multi-scale seg-
mentation might be extended to finer-grain seg-
ments, such as paragraphs. The lexical counts at the
paragraph level will be sparse, so lexical cohesion
alone is unlikely to be sufficient. Rather it may be
necessary to model discourse connectors and lexical
semantics explicitly. The development of more com-
prehensive Bayesian models for discourse structure
seems an exciting direction for future research.
Acknowledgments Thanks to Michel Galley, Igor
Malioutov, and Masao Utiyama for making their topic
segmentation systems publicly available, and to the
anonymous reviewers for useful feedback. This research
is supported by the Beckman Postdoctoral Fellowship.
360
References
David M. Blei and Pedro J. Moreno. 2001. Topic seg-
mentation with an aspect hidden markov model. In
SIGIR, pages 343?348.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Micha Elsner and Eugene Charniak. 2008. You Talk-
ing to Me? A Corpus and Algorithm for Conversation
Disentanglement. In Proceedings of ACL.
Michel Galley, Katheen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. pages 562?569.
T.L. Griffiths and M. Steyvers. 2004. Finding scientific
topics.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Barbara Grosz. 1977. The representation and use of fo-
cus in dialogue understanding. Technical Report 151,
Artificial Intelligence Center, SRI International.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9?16.
Oskari Heinonen. 1998. Optimal Multi-Paragraph Text
Segmentation by Dynamic Programming. In Proceed-
ings of ACL, pages 1484?1486.
P.Y. Hsueh, J. Moore, and S. Renals. 2006. Automatic
segmentation of multiparty dialogue. In Proccedings
of EACL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
R.E. Madsen, D. Kauchak, and C. Elkan. 2005. Model-
ing word burstiness using the Dirichlet distribution. In
Proceedings of ICML.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25?32.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8:243?281.
Thomas P. Minka. 2003. Estimating a dirichlet distri-
bution. Technical report, Massachusetts Institute of
Technology.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14:130?137.
M. Purver, T.L. Griffiths, K.P. Ko?rding, and J.B. Tenen-
baum. 2006. Unsupervised topic modelling for multi-
party spoken discourse. In Proceedings of ACL, pages
17?24.
Jaemo Sung, Zoubin Ghahramani, and Sung-Yang Bang.
2008. Latent-space variational bayes. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
30(12):2236?2242, Dec.
Y.W. Teh, D. Newman, and M. Welling. 2007. A Col-
lapsed Variational Bayesian Inference Algorithm for
Latent Dirichlet Allocation. In NIPS, volume 19, page
1353.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL, pages 491?498.
H. Kenneth Walker, W. Dallas Hall, and J. Willis Hurst,
editors. 1990. Clinical Methods : The History, Physi-
cal, and Laboratory Examinations. Butterworths.
Y. Yaari. 1997. Segmentation of Expository Texts by
Hierarchical Agglomerative Clustering. In Recent Ad-
vances in Natural Language Processing.
361
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277?1287,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Latent Variable Model for Geographic Lexical Variation
Jacob Eisenstein Brendan O?Connor Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,brendano,nasmith,epxing}@cs.cmu.edu
Abstract
The rapid growth of geotagged social media
raises new computational possibilities for in-
vestigating geographic linguistic variation. In
this paper, we present a multi-level generative
model that reasons jointly about latent topics
and geographical regions. High-level topics
such as ?sports? or ?entertainment? are ren-
dered differently in each geographic region,
revealing topic-specific regional distinctions.
Applied to a new dataset of geotagged mi-
croblogs, our model recovers coherent top-
ics and their regional variants, while identi-
fying geographic areas of linguistic consis-
tency. The model also enables prediction of
an author?s geographic location from raw text,
outperforming both text regression and super-
vised topic models.
1 Introduction
Sociolinguistics and dialectology study how lan-
guage varies across social and regional contexts.
Quantitative research in these fields generally pro-
ceeds by counting the frequency of a handful of
previously-identified linguistic variables: pairs of
phonological, lexical, or morphosyntactic features
that are semantically equivalent, but whose fre-
quency depends on social, geographical, or other
factors (Paolillo, 2002; Chambers, 2009). It is left to
the experimenter to determine which variables will
be considered, and there is no obvious procedure for
drawing inferences from the distribution of multiple
variables. In this paper, we present a method for
identifying geographically-aligned lexical variation
directly from raw text. Our approach takes the form
of a probabilistic graphical model capable of iden-
tifying both geographically-salient terms and coher-
ent linguistic communities.
One challenge in the study of lexical variation is
that term frequencies are influenced by a variety of
factors, such as the topic of discourse. We address
this issue by adding latent variables that allow us to
model topical variation explicitly. We hypothesize
that geography and topic interact, as ?pure? topi-
cal lexical distributions are corrupted by geographi-
cal factors; for example, a sports-related topic will
be rendered differently in New York and Califor-
nia. Each author is imbued with a latent ?region?
indicator, which both selects the regional variant of
each topic, and generates the author?s observed ge-
ographical location. The regional corruption of top-
ics is modeled through a cascade of logistic normal
priors?a general modeling approach which we call
cascading topic models. The resulting system has
multiple capabilities, including: (i) analyzing lexi-
cal variation by both topic and geography; (ii) seg-
menting geographical space into coherent linguistic
communities; (iii) predicting author location based
on text alone.
This research is only possible due to the rapid
growth of social media. Our dataset is derived from
the microblogging website Twitter,1 which permits
users to post short messages to the public. Many
users of Twitter also supply exact geographical co-
ordinates from GPS-enabled devices (e.g., mobile
phones),2 yielding geotagged text data. Text in
computer-mediated communication is often more
vernacular (Tagliamonte and Denis, 2008), and as
such it is more likely to reveal the influence of ge-
ographic factors than text written in a more formal
genre, such as news text (Labov, 1966).
We evaluate our approach both qualitatively and
quantitatively. We investigate the topics and regions
1http://www.twitter.com
2User profiles also contain self-reported location names, but
we do not use that information in this work.
1277
that the model obtains, showing both common-sense
results (place names and sports teams are grouped
appropriately), as well as less-obvious insights about
slang. Quantitatively, we apply our model to predict
the location of unlabeled authors, using text alone.
On this task, our model outperforms several alterna-
tives, including both discriminative text regression
and related latent-variable approaches.
2 Data
The main dataset in this research is gathered from
the microblog website Twitter, via its official API.
We use an archive of messages collected over the
first week of March 2010 from the ?Gardenhose?
sample stream,3 which then consisted of 15% of
all public messages, totaling millions per day. We
aggressively filter this stream, using only messages
that are tagged with physical (latitude, longitude)
coordinate pairs from a mobile client, and whose au-
thors wrote at least 20 messages over this period. We
also filter to include only authors who follow fewer
than 1,000 other people, and have fewer than 1,000
followers. Kwak et al (2010) find dramatic shifts
in behavior among users with social graph connec-
tivity outside of that range; such users may be mar-
keters, celebrities with professional publicists, news
media sources, etc. We also remove messages con-
taining URLs to eliminate bots posting information
such as advertising or weather conditions. For inter-
pretability, we restrict our attention to authors inside
a bounding box around the contiguous U.S. states,
yielding a final sample of about 9,500 users and
380,000 messages, totaling 4.7 million word tokens.
We have made this dataset available online.4
Informal text from mobile phones is challeng-
ing to tokenize; we adapt a publicly available tok-
enizer5 originally developed for Twitter (O?Connor
et al, 2010), which preserves emoticons and blocks
of punctuation and other symbols as tokens. For
each user?s Twitter feed, we combine all messages
into a single ?document.? We remove word types
that appear in fewer than 40 feeds, yielding a vocab-
ulary of 5,216 words. Of these, 1,332 do not appear
in the English, French, or Spanish dictionaries of the
3http://dev.twitter.com/pages/streaming_api
4http://www.ark.cs.cmu.edu/GeoTwitter
5http://tweetmotif.com
spell-checking program aspell.
Every message is tagged with a location, but most
messages from a single individual tend to come from
nearby locations (as they go about their day); for
modeling purposes we use only a single geographic
location for each author, simply taking the location
of the first message in the sample.
The authors in our dataset are fairly heavy Twit-
ter users, posting an average of 40 messages per day
(although we see only 15% of this total). We have
little information about their demographics, though
from the text it seems likely that this user set skews
towards teens and young adults. The dataset cov-
ers each of the 48 contiguous United States and the
District of Columbia.
3 Model
We develop a model that incorporates two sources
of lexical variation: topic and geographical region.
We treat the text and geographic locations as out-
puts from a generative process that incorporates both
topics and regions as latent variables.6 During infer-
ence, we seek to recover the topics and regions that
best explain the observed data.
At the base level of model are ?pure? topics (such
as ?sports?, ?weather?, or ?slang?); these topics are
rendered differently in each region. We call this gen-
eral modeling approach a cascading topic model; we
describe it first in general terms before moving to the
specific application to geographical variation.
3.1 Cascading Topic Models
Cascading topic models generate text from a chain
of random variables. Each element in the chain de-
fines a distribution over words, and acts as the mean
of the distribution over the subsequent element in
the chain. Thus, each element in the chain can be
thought of as introducing some additional corrup-
tion. All words are drawn from the final distribution
in the chain.
At the beginning of the chain are the priors, fol-
lowed by unadulerated base topics, which may then
be corrupted by other factors (such as geography or
time). For example, consider a base ?food? topic
6The region could be observed by using a predefined geo-
graphical decomposition, e.g., political boundaries. However,
such regions may not correspond well to linguistic variation.
1278
that emphasizes words like dinner and delicious;
the corrupted ?food-California? topic would place
weight on these words, but might place extra em-
phasis on other words like sprouts.
The path through the cascade is determined by a
set of indexing variables, which may be hidden or
observed. As in standard latent Dirichlet alocation
(Blei et al, 2003), the base topics are selected by
a per-token hidden variable z. In the geographical
topic model, the next level corresponds to regions,
which are selected by a per-author latent variable r.
Formally, we draw each level of the cascade from
a normal distribution centered on the previous level;
the final multinomial distribution over words is ob-
tained by exponentiating and normalizing. To ensure
tractable inference, we assume that all covariance
matrices are uniform diagonal, i.e., aI with a > 0;
this means we do not model interactions between
words.
3.2 The Geographic Topic Model
The application of cascading topic models to ge-
ographical variation is straightforward. Each doc-
ument corresponds to the entire Twitter feed of a
given author during the time period covered by our
corpus. For each author, the latent variable r cor-
responds to the geographical region of the author,
which is not observed. As described above, r se-
lects a corrupted version of each topic: the kth basic
topic has mean ?k, with uniform diagonal covari-
ance ?2k; for region j, we can draw the regionally-
corrupted topic from the normal distribution, ?jk ?
N(?k, ?
2
kI).
Because ? is normally-distributed, it lies not in
the simplex but in RW . We deterministically com-
pute multinomial parameters ? by exponentiating
and normalizing: ?jk = exp(?jk)/
?
i exp(?
(i)
jk ).
This normalization could introduce identifiability
problems, as there are multiple settings for ? that
maximize P (w|?) (Blei and Lafferty, 2006a). How-
ever, this difficulty is obviated by the priors: given
? and ?2, there is only a single ? that maximizes
P (w|?)P (?|?, ?2); similarly, only a single ?max-
imizes P (?|?)P (?|a, b2).
The observed latitude and longitude, denoted y,
are normally distributed and conditioned on the re-
gion, with mean ?r and precision matrix ?r indexed
by the region r. The region index r is itself drawn
from a single shared multinomial ?. The model is
shown as a plate diagram in Figure 1.
Given a vocabulary size W , the generative story
is as follows:
? Generate base topics: for each topic k < K
? Draw the base topic from a normal distribu-
tion with uniform diagonal covariance: ?k ?
N(a, b2I),
? Draw the regional variance from a Gamma
distribution: ?2k ? G(c, d).
? Generate regional variants: for each region
j < J ,
? Draw the region-topic ?jk from a normal
distribution with uniform diagonal covari-
ance: ?jk ? N(?k, ?
2
kI).
? Convert ?jk into a multinomial
distribution over words by ex-
ponentiating and normalizing:
?jk = exp
(
?jk
)
/
?W
i exp(?
(i)
jk ),
where the denominator sums over the
vocabulary.
? Generate regions: for each region j < J ,
? Draw the spatial mean ?j from a normal dis-
tribution.
? Draw the precision matrix ?j from a Wishart
distribution.
? Draw the distribution over regions ? from a sym-
metric Dirichlet prior, ? ? Dir(??1).
? Generate text and locations: for each document d,
? Draw topic proportions from a symmetric
Dirichlet prior, ? ? Dir(?1).
? Draw the region r from the multinomial dis-
tribution ?.
? Draw the location y from the bivariate Gaus-
sian, y ? N(?r,?r).
? For each word token,
? Draw the topic indicator z ? ?.
? Draw the word token w ? ?rz .
4 Inference
We apply mean-field variational inference: a fully-
factored variational distribution Q is chosen to min-
imize the Kullback-Leibler divergence from the
true distribution. Mean-field variational inference
with conjugate priors is described in detail else-
where (Bishop, 2006; Wainwright and Jordan,
2008); we restrict our focus to the issues that are
unique to the geographic topic model.
1279
? ?
w
z
?
D
Nd
y
r K
?
J
?
?
?
??2
?
?k log of base topic k?s distribution over word types
?2k variance parameter for regional variants of topic k
?jk region j?s variant of base topic ?k
?d author d?s topic proportions
rd author d?s latent region
yd author d?s observed GPS location
?j region j?s spatial center
?j region j?s spatial precision
zn token n?s topic assignment
wn token n?s observed word type
? global prior over author-topic proportions
? global prior over region classes
Figure 1: Plate diagram for the geographic topic model, with a table of all random variables. Priors (besides ?) are
omitted for clarity, and the document indices on z and w are implicit.
We place variational distributions over all latent
variables of interest: ?, z, r,?,?,?, ?2,?, and ?,
updating each of these distributions in turn, until
convergence. The variational distributions over ?
and ? are Dirichlet, and have closed form updates:
each can be set to the sum of the expected counts,
plus a term from the prior (Blei et al, 2003). The
variational distributions q(z) and q(r) are categor-
ical, and can be set proportional to the expected
joint likelihood?to set q(z) we marginalize over r,
and vice versa.7 The updates for the multivariate
Gaussian spatial parameters ? and ? are described
by Penny (2001).
4.1 Regional Word Distributions
The variational region-topic distribution ?jk is nor-
mal, with uniform diagonal covariance for tractabil-
ity. Throughout we will write ?x? to indicate the ex-
pectation of x under the variational distribution Q.
Thus, the vector mean of the distribution q(?jk) is
written ??jk?, while the variance (uniform across i)
of q(?) is written V(?jk).
To update the mean parameter ??jk?, we max-
imize the contribution to the variational bound L
from the relevant terms:
L
[??(i)jk ?]
= ?log p(w|?, z, r)?+?log p(?(i)jk |?
(i)
k , ?
2
k)?,
(1)
7Thanks to the na??ve mean field assumption, we can
marginalize over z by first decomposing across all Nd words
and then summing over q(z).
with the first term representing the likelihood of the
observed words (recall that ? is computed determin-
istically from ?) and the second term corresponding
to the prior. The likelihood term requires the expec-
tation ?log??, but this is somewhat complicated by
the normalizer
?W
i exp(?
(i)), which sums over all
terms in the vocabulary. As in previous work on lo-
gistic normal topic models, we use a Taylor approx-
imation for this term (Blei and Lafferty, 2006a).
The prior on ? is normal, so the contribution from
the second term of the objective (Equation 1) is
? 1
2??2k?
?(?(i)jk ? ?
(i)
k )
2?. We introduce the following
notation for expected counts: N(i, j, k) indicates the
expected count of term i in region j and topic k, and
N(j, k) =
?
iN(i, j, k). After some calculus, we
can write the gradient ?L/???((i))jk ? as
N(i, j, k)?N(j, k)??(i)jk ? ? ??
?2
k ?(??
(i)
jk ? ? ??
(i)
k ?),
(2)
which has an intuitive interpretation. The first two
terms represent the difference in expected counts for
term i under the variational distributions q(z, r) and
q(z, r, ?): this difference goes to zero when ?(i)jk per-
fectly matches N(i, j, k)/N(j, k). The third term
penalizes ?(i)jk for deviating from its prior ?
(i)
k , but
this penalty is proportional to the expected inverse
variance ???2k ?. We apply gradient ascent to maxi-
mize the objective L. A similar set of calculations
gives the gradient for the variance of ?; these are
described in an forthcoming appendix.
1280
4.2 Base Topics
The base topic parameters are?k and ?
2
k; in the vari-
ational distribution, q(?k) is normally distributed
and q(?2k) is Gamma distributed. Note that ?k and
?2k affect only the regional word distributions ?jk.
An advantage of the logistic normal is that the vari-
ational parameters over ?k are available in closed
form,
??(i)k ? =
b2
?J
j ??
(i)
jk ?+ ??
2
k?a
(i)
b2J + ??2k?
V(?k) = (b
?2 + J???2k ?)
?1,
where J indicates the number of regions. The ex-
pectation of the base topic ? incorporates the prior
and the average of the generated region-topics?
these two components are weighted respectively by
the expected variance of the region-topics ??2k? and
the prior topical variance b2. The posterior variance
V(?) is a harmonic combination of the prior vari-
ance b2 and the expected variance of the region top-
ics.
The variational distribution over the region-topic
variance ?2k has Gamma parameters. These param-
eters cannot be updated in closed form, so gradi-
ent optimization is again required. The derivation
of these updates is more involved, and is left for a
forthcoming appendix.
5 Implementation
Variational scheduling and initialization are impor-
tant aspects of any hierarchical generative model,
and are often under-discussed. In our implementa-
tion, the variational updates are scheduled as fol-
lows: given expected counts, we iteratively update
the variational parameters on the region-topics ? and
the base topics?, until convergence. We then update
the geographical parameters ? and ?, as well as the
distribution over regions ?. Finally, for each doc-
ument we iteratively update the variational param-
eters over ?, z, and r until convergence, obtaining
expected counts that are used in the next iteration
of updates for the topics and their regional variants.
We iterate an outer loop over the entire set of updates
until convergence.
We initialize the model in a piecewise fashion.
First we train a Dirichlet process mixture model on
the locations y, using variational inference on the
truncated stick-breaking approximation (Blei and
Jordan, 2006). This automatically selects the num-
ber of regions J , and gives a distribution over each
region indicator rd from geographical information
alone. We then run standard latent Dirichlet aloca-
tion to obtain estimates of z for each token (ignoring
the locations). From this initialization we can com-
pute the first set of expected counts, which are used
to obtain initial estimates of all parameters needed
to begin variational inference in the full model.
The prior a is the expected mean of each topic
?; for each term i, we set a(i) = logN(i) ? logN ,
where N(i) is the total count of i in the corpus and
N =
?
iN(i). The variance prior b
2 is set to 1, and
the prior on ?2 is the Gamma distribution G(2, 200),
encouraging minimal deviation from the base topics.
The symmetric Dirichlet prior on ? is set to 12 , and
the symmetric Dirichlet parameter on ? is updated
from weak hyperpriors (Minka, 2003). Finally, the
geographical model takes priors that are linked to the
data: for each region, the mean is very weakly en-
couraged to be near the overall mean, and the covari-
ance prior is set by the average covariance of clusters
obtained by running K-means.
6 Evaluation
For a quantitative evaluation of the estimated rela-
tionship between text and geography, we assess our
model?s ability to predict the geographic location of
unlabeled authors based on their text alone.8 This
task may also be practically relevant as a step toward
applications for recommending local businesses or
social connections. A randomly-chosen 60% of au-
thors are used for training, 20% for development,
and the remaining 20% for final evaluation.
6.1 Systems
We compare several approaches for predicting au-
thor location; we divide these into latent variable
generative models and discriminative approaches.
8Alternatively, one might evaluate the attributed regional
memberships of the words themselves. While the Dictionary of
American Regional English (Cassidy and Hall, 1985) attempts
a comprehensive list of all regionally-affiliated terms, it is based
on interviews conducted from 1965-1970, and the final volume
(covering Si?Z) is not yet complete.
1281
6.1.1 Latent Variable Models
Geographic Topic Model This is the full version
of our system, as described in this paper. To pre-
dict the unseen location yd, we iterate until con-
vergence on the variational updates for the hidden
topics zd, the topic proportions ?d, and the region
rd. From rd, the location can be estimated as y?d =
arg maxy
?J
j p(y|?j ,?j)q(rd = j). The develop-
ment set is used to tune the number of topics and to
select the best of multiple random initializations.
Mixture of Unigrams A core premise of our ap-
proach is that modeling topical variation will im-
prove our ability to understand geographical varia-
tion. We test this idea by fixing K = 1, running our
system with only a single topic. This is equivalent
to a Bayesian mixture of unigrams in which each au-
thor is assigned a single, regional unigram language
model that generates all of his or her text. The de-
velopment set is used to select the best of multiple
random initializations.
Supervised Latent Dirichlet Allocation In a
more subtle version of the mixture-of-unigrams
model, we model each author as an admixture of re-
gions. Thus, the latent variable attached to each au-
thor is no longer an index, but rather a vector on the
simplex. This model is equivalent to supervised la-
tent Dirichlet alocation (Blei and McAuliffe, 2007):
each topic is associated with equivariant Gaussian
distributions over the latitude and longitude, and
these topics must explain both the text and the ob-
served geographical locations. For unlabeled au-
thors, we estimate latitude and longitude by esti-
mating the topic proportions and then applying the
learned geographical distributions. This is a linear
prediction
f(z?d;a) = (z?
T
da
lat, z?Tda
lon)
for an author?s topic proportions z?d and topic-
geography weights a ? R2K .
6.1.2 Baseline Approaches
Text Regression We perform linear regression
to discriminatively learn the relationship between
words and locations. Using term frequency features
xd for each author, we predict locations with word-
geography weights a ? R2W :
f(xd;a) = (x
T
da
lat, xTda
lon)
Weights are trained to minimize the sum of squared
Euclidean distances, subject to L1 regularization:
?
d
(xTda
lat ? ylatd )
2 + (xTda
lon ? ylond )
2
+ ?lat||a
lat||1 + ?lon||a
lon||1
The minimization problem decouples into two sep-
arate latitude and longitude models, which we fit
using the glmnet elastic net regularized regres-
sion package (Friedman et al, 2010), which ob-
tained good results on other text-based prediction
tasks (Joshi et al, 2010). Regularization parameters
were tuned on the development set. The L1 penalty
outperformed L2 and mixtures of L1 and L2.
Note that for both word-level linear regression
here, and the topic-level linear regression in SLDA,
the choice of squared Euclidean distance dovetails
with our use of spatial Gaussian likelihoods in the
geographic topic models, since optimizing a is
equivalent to maximum likelihood estimation un-
der the assumption that locations are drawn from
equivariant circular Gaussians centered around each
f(xd;a) linear prediction. We experimented with
decorrelating the location dimensions by projecting
yd into the principal component space, but this did
not help text regression.
K-Nearest Neighbors Linear regression is a poor
model for the multimodal density of human popula-
tions. As an alternative baseline, we applied super-
vised K-nearest neighbors to predict the location yd
as the average of the positions of the K most sim-
ilar authors in the training set. We computed term-
frequency inverse-document frequency features and
applied cosine similarity over their first 30 principal
components to find the neighbors. The choices of
principal components, IDF weighting, and neighbor-
hood size K = 20 were tuned on the development
set.
6.2 Metrics
Our principle error metrics are the mean and median
distance between the predicted and true location in
kilometers.9 Because the distance error may be dif-
ficult to interpret, we also report accuracy of classi-
9For convenience, model training and prediction use latitude
and longitude as an unprojected 2D Euclidean space. However,
properly measuring the physical distance between points on the
1282
Regression Classification accuracy (%)
System Mean Dist. (km) Median Dist. (km) Region (4-way) State (49-way)
Geographic topic model 900 494 58 24
Mixture of unigrams 947 644 53 19
Supervised LDA 1055 728 39 4
Text regression 948 712 41 4
K-nearest neighbors 1077 853 37 2
Mean location 1148 1018
Most common class 37 27
Table 1: Location prediction results; lower scores are better on the regression task, higher scores are better on the
classification task. Distances are in kilometers. Mean location and most common class are computed from the test set.
Both the geographic topic model and supervised LDA use the best number of topics from the development set (10 and
5, respectively).
fication by state and by region of the United States.
Our data includes the 48 contiguous states plus the
District of Columbia; the U.S. Census Bureau di-
vides these states into four regions: West, Midwest,
Northeast, and South.10 Note that while major pop-
ulation centers straddle several state lines, most re-
gion boundaries are far from the largest cities, re-
sulting in a clearer analysis.
6.3 Results
As shown in Table 1, the geographic topic model
achieves the strongest performance on all metrics.
All differences in performance between systems
are statistically significant (p < .01) using the
Wilcoxon-Mann-Whitney test for regression error
and the ?2 test for classification accuracy. Figure 2
shows how performance changes as the number of
topics varies.
Note that the geographic topic model and the mix-
ture of unigrams use identical code and parametriza-
tion ? the only difference is that the geographic topic
model accounts for topical variation, while the mix-
ture of unigrams sets K = 1. These results validate
our basic premise that it is important to model the
interaction between topical and geographical varia-
tion.
Text regression and supervised LDA perform es-
pecially poorly on the classification metric. Both
methods make predictions that are averaged across
Earth?s surface requires computing or approximating the great
circle distance ? we use the Haversine formula (Sinnott, 1984).
For the continental U.S., the relationship between degrees and
kilometers is nearly linear, but extending the model to a conti-
nental scale would require a more sophisticated approach.
10http://www.census.gov/geo/www/us_regdiv.pdf
0 5 10 15 20400
500
600
700
800
900
1000
1100
Number of topics
Me
dia
n e
rro
r (k
m)
 
 
Geographic Topic Model
Supervised LDA
Mean location
Figure 2: The effect of varying the number of topics on
the median regression error (lower is better).
each word in the document: in text regression, each
word is directly multiplied by a feature weight; in
supervised LDA the word is associated with a la-
tent topic first, and then multiplied by a weight. For
these models, all words exert an influence on the pre-
dicted location, so uninformative words will draw
the prediction towards the center of the map. This
yields reasonable distance errors but poor classifica-
tion accuracy. We had hoped that K-nearest neigh-
bors would be a better fit for this metric, but its per-
formance is poor at all values of K. Of course it is
always possible to optimize classification accuracy
directly, but such an approach would be incapable
of predicting the exact geographical location, which
is the focus of our evaluation (given that the desired
geographical partition is unknown). Note that the
geographic topic model is also not trained to opti-
mize classification accuracy.
1283
?basketball?
?popular
music?
?daily life? ?emoticons? ?chit chat?
PISTONS KOBE
LAKERS game
DUKE NBA
CAVS STUCKEY
JETS KNICKS
album music
beats artist video
#LAKERS
ITUNES tour
produced vol
tonight shop
weekend getting
going chilling
ready discount
waiting iam
:) haha :d :( ;) :p
xd :/ hahaha
hahah
lol smh jk yea
wyd coo ima
wassup
somethin jp
Boston
+ CELTICS victoryBOSTON
CHARLOTTE
playing daughter
PEARL alive war
comp
BOSTON ;p gna loveee
ese exam suttin
sippin
N. California+
THUNDER
KINGS GIANTS
pimp trees clap
SIMON dl
mountain seee 6am OAKLAND
pues hella koo
SAN fckn
hella flirt hut
iono OAKLAND
New York
+ NETS KNICKS BRONX iam cab oww wasssup nm
Los Angeles+
#KOBE
#LAKERS
AUSTIN
#LAKERS load
HOLLYWOOD
imm MICKEY
TUPAC
omw tacos hr
HOLLYWOOD
af papi raining
th bomb coo
HOLLYWOOD
wyd coo af nada
tacos messin
fasho bomb
Lake Erie
+
CAVS
CLEVELAND
OHIO BUCKS od
COLUMBUS
premiere prod
joint TORONTO
onto designer
CANADA village
burr
stink CHIPOTLE
tipsy
;d blvd BIEBER
hve OHIO
foul WIZ salty
excuses lames
officer lastnight
Table 2: Example base topics (top line) and regional variants. For the base topics, terms are ranked by log-odds
compared to the background distribution. The regional variants show words that are strong compared to both the base
topic and the background. Foreign-language words are shown in italics, while terms that are usually in proper nouns
are shown in SMALL CAPS. See Table 3 for definitions of slang terms; see Section 7 for more explanation and details
on the methodology.
Figure 3: Regional clustering of the training set obtained by one randomly-initialized run of the geographical topic
model. Each point represents one author, and each shape/color combination represents the most likely cluster as-
signment. Ellipses represent the regions? spatial means and covariances. The same model and coloring are shown in
Table 2.
1284
7 Analysis
Our model permits analysis of geographical vari-
ation in the context of topics that help to clarify
the significance of geographically-salient terms. Ta-
ble 2 shows a subset of the results of one randomly-
initialized run, including five hand-chosen topics (of
50 total) and five regions (of 13, as chosen automat-
ically during initialization). Terms were selected by
log-odds comparison. For the base topics we show
the ten strongest terms in each topic as compared to
the background word distribution. For the regional
variants, we show terms that are strong both region-
ally and topically: specifically, we select terms that
are in the top 100 compared to both the background
distribution and to the base topic. The names for the
topics and regions were chosen by the authors.
Nearly all of the terms in column 1 (?basketball?)
refer to sports teams, athletes, and place names?
encouragingly, terms tend to appear in the regions
where their referents reside. Column 2 contains sev-
eral proper nouns, mostly referring to popular mu-
sic figures (including PEARL from the band Pearl
Jam).11 Columns 3?5 are more conversational.
Spanish-language terms (papi, pues, nada, ese) tend
to appear in regions with large Spanish-speaking
populations?it is also telling that these terms ap-
pear in topics with emoticons and slang abbrevia-
tions, which may transcend linguistic barriers. Other
terms refer to people or subjects that may be espe-
cially relevant in certain regions: tacos appears in
the southern California region and cab in the New
York region; TUPAC refers to a rap musician from
Los Angeles, and WIZ refers to a rap musician from
Pittsburgh, not far from the center of the ?Lake Erie?
region.
A large number of slang terms are found to have
strong regional biases, suggesting that slang may
depend on geography more than standard English
does. The terms af and hella display especially
strong regional affinities, appearing in the regional
variants of multiple topics (see Table 3 for defini-
tions). Northern and Southern California use variant
spellings koo and coo to express the same meaning.
11This analysis is from an earlier version of our dataset that
contained some Twitterbots, including one from a Boston-area
radio station. The bots were purged for the evaluation in Sec-
tion 6, though the numerical results are nearly identical.
term definition
af as fuck (very)
coo cool
dl download
fasho for sure
gna going to
hella very
hr hour
iam I am
ima I?m going to
imm I?m
iono I don?t know
lames lame (not cool)
people
term definition
jk just kidding
jp just playing (kid-
ding)
koo cool
lol laugh out loud
nm nothing much
od overdone (very)
omw on my way
smh shake my head
suttin something
wassup what?s up
wyd what are you do-
ing?
Table 3: A glossary of non-standard terms from Ta-
ble 2. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
While research in perceptual dialectology does con-
firm the link of hella to Northern California (Bu-
choltz et al, 2007), we caution that our findings
are merely suggestive, and a more rigorous analysis
must be undertaken before making definitive state-
ments about the regional membership of individual
terms. We view the geographic topic model as an
exploratory tool that may be used to facilitate such
investigations.
Figure 3 shows the regional clustering on the
training set obtained by one run of the model. Each
point represents an author, and the ellipses represent
the bivariate Gaussians for each region. There are
nine compact regions for major metropolitan areas,
two slightly larger regions that encompass Florida
and the area around Lake Erie, and two large re-
gions that partition the country roughly into north
and south.
8 Related Work
The relationship between language and geography
has been a topic of interest to linguists since the
nineteenth century (Johnstone, 2010). An early
work of particular relevance is Kurath?s Word Geog-
raphy of the Eastern United States (1949), in which
he conducted interviews and then mapped the oc-
currence of equivalent word pairs such as stoop and
porch. The essence of this approach?identifying
variable pairs and measuring their frequencies?
remains a dominant methodology in both dialec-
1285
tology (Labov et al, 2006) and sociolinguis-
tics (Tagliamonte, 2006). Within this paradigm,
computational techniques are often applied to post
hoc analysis: logistic regression (Sankoff et al,
2005) and mixed-effects models (Johnson, 2009) are
used to measure the contribution of individual vari-
ables, while hierarchical clustering and multidimen-
sional scaling enable aggregated inference across
multiple variables (Nerbonne, 2009). However, in
all such work it is assumed that the relevant linguis-
tic variables have already been identified?a time-
consuming process involving considerable linguistic
expertise. We view our work as complementary to
this tradition: we work directly from raw text, iden-
tifying both the relevant features and coherent lin-
guistic communities.
An active recent literature concerns geotagged in-
formation on the web, such as search queries (Back-
strom et al, 2008) and tagged images (Crandall et
al., 2009). This research identifies the geographic
distribution of individual queries and tags, but does
not attempt to induce any structural organization of
either the text or geographical space, which is the
focus of our research. More relevant is the work
of Mei et al (2006), in which the distribution over
latent topics in blog posts is conditioned on the ge-
ographical location of the author. This is somewhat
similar to the supervised LDA model that we con-
sider, but their approach assumes that a partitioning
of geographical space into regions is already given.
Methodologically, our cascading topic model is
designed to capture multiple dimensions of variabil-
ity: topics and geography. Mei et al (2007) include
sentiment as a second dimension in a topic model,
using a switching variable so that individual word
tokens may be selected from either the topic or the
sentiment. However, our hypothesis is that individ-
ual word tokens reflect both the topic and the ge-
ographical aspect. Sharing this intuition, Paul and
Girju (2010) build topic-aspect models for the cross
product of topics and aspects. They do not impose
any regularity across multiple aspects of the same
topic, so this approach may not scale when the num-
ber of aspects is large (they consider only two as-
pects). We address this issue using cascading distri-
butions; when the observed data for a given region-
topic pair is low, the model falls back to the base
topic. The use of cascading logistic normal distri-
butions in topic models follows earlier work on dy-
namic topic models (Blei and Lafferty, 2006b; Xing,
2005).
9 Conclusion
This paper presents a model that jointly identifies
words with high regional affinity, geographically-
coherent linguistic regions, and the relationship be-
tween regional and topic variation. The key model-
ing assumption is that regions and topics interact to
shape observed lexical frequencies. We validate this
assumption on a prediction task in which our model
outperforms strong alternatives that do not distin-
guish regional and topical variation.
We see this work as a first step towards a unsuper-
vised methodology for modeling linguistic variation
using raw text. Indeed, in a study of morphosyn-
tactic variation, Szmrecsanyi (2010) finds that by
the most generous measure, geographical factors ac-
count for only 33% of the observed variation. Our
analysis might well improve if non-geographical
factors were considered, including age, race, gen-
der, income and whether a location is urban or ru-
ral. In some regions, estimates of many of these fac-
tors may be obtained by cross-referencing geogra-
phy with demographic data. We hope to explore this
possibility in future work.
Acknowledgments
We would like to thank Amr Ahmed, Jonathan Chang,
Shay Cohen, William Cohen, Ross Curtis, Miro Dud??k,
Scott Kiesling, Seyoung Kim, and the anonymous re-
viewers. This research was enabled by Google?s sup-
port of the Worldly Knowledge project at CMU, AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF IIS-0713379, and an Alfred P. Sloan
Fellowship.
References
L. Backstrom, J. Kleinberg, R. Kumar, and J. Novak.
2008. Spatial variation in search engine queries. In
Proceedings of WWW.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
D. M. Blei and M. I. Jordan. 2006. Variational infer-
ence for Dirichlet process mixtures. Bayesian Analy-
sis, 1:121?144.
1286
D. M. Blei and J. Lafferty. 2006a. Correlated topic mod-
els. In NIPS.
D. M. Blei and J. Lafferty. 2006b. Dynamic topic mod-
els. In Proceedings of ICML.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In NIPS.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
M. Bucholtz, N. Bermudez, V. Fung, L. Edwards, and
R. Vargas. 2007. Hella Nor Cal or totally So Cal?
the perceptual dialectology of California. Journal of
English Linguistics, 35(4):325?352.
F. G. Cassidy and J. H. Hall. 1985. Dictionary of Amer-
ican Regional English, volume 1. Harvard University
Press.
J. Chambers. 2009. Sociolinguistic Theory: Linguistic
Variation and its Social Significance. Blackwell.
D. J Crandall, L. Backstrom, D. Huttenlocher, and
J. Kleinberg. 2009. Mapping the world?s photos. In
Proceedings of WWW, page 761770.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. Regular-
ization paths for generalized linear models via coordi-
nate descent. Journal of Statistical Software, 33(1).
D. E. Johnson. 2009. Getting off the GoldVarb standard:
Introducing Rbrul for mixed-effects variable rule anal-
ysis. Language and Linguistics Compass, 3(1):359?
383.
B. Johnstone. 2010. Language and place. In R. Mesthrie
and W. Wolfram, editors, Cambridge Handbook of So-
ciolinguistics. Cambridge University Press.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proceedings of NAACL-HLT.
H. Kurath. 1949. A Word Geography of the Eastern
United States. University of Michigan Press.
H. Kwak, C. Lee, H. Park, and S. Moon. 2010. What
is Twitter, a social network or a news media? In Pro-
ceedings of WWW.
W. Labov, S. Ash, and C. Boberg. 2006. The Atlas of
North American English: Phonetics, Phonology, and
Sound Change. Walter de Gruyter.
W. Labov. 1966. The Social Stratification of English in
New York City. Center for Applied Linguistics.
Q. Mei, C. Liu, H. Su, and C. X Zhai. 2006. A proba-
bilistic approach to spatiotemporal theme pattern min-
ing on weblogs. In Proceedings of WWW, page 542.
Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai.
2007. Topic sentiment mixture: modeling facets and
opinions in weblogs. In Proceedings of WWW.
T. P. Minka. 2003. Estimating a Dirichlet distribution.
Technical report, Massachusetts Institute of Technol-
ogy.
J. Nerbonne. 2009. Data-driven dialectology. Language
and Linguistics Compass, 3(1).
B. O?Connor, M. Krieger, and D. Ahn. 2010. TweetMo-
tif: Exploratory search and topic summarization for
twitter. In Proceedings of ICWSM.
J. C. Paolillo. 2002. Analyzing Linguistic Variation: Sta-
tistical Models and Methods. CSLI Publications.
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics. In
Proceedings of AAAI.
W. D. Penny. 2001. Variational Bayes for d-dimensional
Gaussian mixture models. Technical report, Univer-
sity College London.
D. Sankoff, S. A. Tagliamonte, and E. Smith. 2005.
Goldvarb X: A variable rule application for Macintosh
and Windows. Technical report, Department of Lin-
guistics, University of Toronto.
R. W. Sinnott. 1984. Virtues of the Haversine. Sky and
Telescope, 68(2).
B. Szmrecsanyi. 2010. Geography is overrated. In
S. Hansen, C. Schwarz, P. Stoeckle, and T. Streck, ed-
itors, Dialectological and Folk Dialectological Con-
cepts of Space. Walter de Gruyter.
S. A. Tagliamonte and D. Denis. 2008. Linguistic ruin?
LOL! Instant messanging and teen language. Ameri-
can Speech, 83.
S. A. Tagliamonte. 2006. Analysing Sociolinguistic Vari-
ation. Cambridge University Press.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
E. P. Xing. 2005. On topic evolution. Technical Report
05-115, Center for Automated Learning and Discov-
ery, Carnegie Mellon University.
1287
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 61?72,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Log-Linear Model for Unsupervised Text Normalization
Yi Yang
School of Interactive Computing
Georgia Institute of Technology
yiyang@gatech.edu
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Abstract
We present a unified unsupervised statistical
model for text normalization. The relation-
ship between standard and non-standard to-
kens is characterized by a log-linear model,
permitting arbitrary features. The weights
of these features are trained in a maximum-
likelihood framework, employing a novel se-
quential Monte Carlo training algorithm to
overcome the large label space, which would
be impractical for traditional dynamic pro-
gramming solutions. This model is im-
plemented in a normalization system called
UNLOL, which achieves the best known re-
sults on two normalization datasets, outper-
forming more complex systems. We use the
output of UNLOL to automatically normalize
a large corpus of social media text, revealing a
set of coherent orthographic styles that under-
lie online language variation.
1 Introduction
Social media language can differ substantially from
other written text. Many of the attempts to character-
ize and overcome this variation have focused on nor-
malization: transforming social media language into
text that better matches standard datasets (Sproat et
al., 2001; Liu et al, 2011). Because there is lit-
tle available training data, and because social me-
dia language changes rapidly (Eisenstein, 2013b),
fully supervised training is generally not considered
appropriate for this task. However, due to the ex-
tremely high-dimensional output space ? arbitrary
sequences of words across the vocabulary ? it is
a very challenging problem for unsupervised learn-
ing. Perhaps it is for these reasons that the most suc-
cessful systems are pipeline architectures that cob-
ble together a diverse array of techniques and re-
sources, including statistical language models, de-
pendency parsers, string edit distances, off-the-shelf
spellcheckers, and curated slang dictionaries (Liu et
al., 2011; Han and Baldwin, 2011; Han et al, 2013).
We propose a different approach, performing nor-
malization in a maximum-likelihood framework.
There are two main sources of information to be
exploited: local context, and surface similarity be-
tween the observed strings and normalization can-
didates. We treat the local context using standard
language modeling techniques; we treat string simi-
larity with a log-linear model that includes features
for both surface similarity and word-word pairs.
Because labeled examples of normalized text
are not available, this model cannot be trained
in the standard supervised fashion. Nor can we
apply dynamic programming techniques for unsu-
pervised training of locally-normalized conditional
models (Berg-Kirkpatrick et al, 2010), as their com-
plexity is quadratic in the size of label space; in
normalization, the label space is the vocabulary it-
self, with at least 104 elements. Instead, we present
a new training approach using Monte Carlo tech-
niques to compute an approximate gradient on the
feature weights. This training method may be appli-
cable in other unsupervised learning problems with
a large label space.
This model is implemented in a normalization
system called UNLOL (unsupervised normalization
in a LOg-Linear model). It is a lightweight proba-
61
bilistic approach, relying only on a language model
for the target domain; it can be adapted to new
corpora text or new domains easily and quickly.
Our evaluations show that UNLOL outperforms the
state-of-the-art on standard normalization datasets.
In addition, we demonstrate the linguistic insights
that can be obtained from normalization, using
UNLOL to identify classes of orthographic transfor-
mations that form coherent linguistic styles.
2 Background
The text normalization task was introduced
by Sproat et al (2001), and attained popularity in
the context of SMS messages (Choudhury et al,
2007b). It has become still more salient in the era of
widespread social media, particularly Twitter. Han
and Baldwin (2011) formally define a normalization
task for Twitter, focusing on normalizations between
single tokens, and excluding multi-word tokens like
lol (laugh out loud). The normalization task has
been criticized by Eisenstein (2013b), who argues
that it strips away important social meanings. In
recent work, normalization has been shown to yield
improvements for part-of-speech tagging (Han
et al, 2013), parsing (Zhang et al, 2013), and
machine translation (Hassan and Menezes, 2013).
As we will show in Section 7, accurate automated
normalization can also improve our understanding
of the nature of social media language.
Supervised methods Early work on normaliza-
tion focused on labeled SMS datasets, using ap-
proaches such as noisy-channel modeling (Choud-
hury et al, 2007a) and machine translation (Aw
et al, 2006), as well as hybrid combinations of
spelling correction and speech recognition (Kobus
et al, 2008; Beaufort et al, 2010). This work
sought to balance language models (favoring words
that fit in context) with transformation models (fa-
voring words that are similar to the observed text).
Our approach can also be seen as a noisy channel
model, but unlike this prior work, no labeled data is
required.
Unsupervised methods Cook and Stevenson
(2009) manually identify several word formation
types within a noisy channel framework. They
parametrize each formation type with a small num-
ber of scalar values, so that all legal transformations
of a given type are equally likely. The scalar pa-
rameters are then estimated using expectation max-
imization. This work stands apart from most of the
other unsupervised models, which are pipelines.
Contractor et al (2010) use string edit distance
to identify closely-related candidate orthographic
forms and then decode the message using a language
model. Gouws et al (2011) refine this approach
by mining an ?exception dictionary? of strongly-
associated word pairs such as you/u. Like Con-
tractor et al (2010), we apply string edit distance,
and like Gouws et al (2011), we capture strongly
related word pairs. However, rather than applying
these properties as filtering steps in a pipeline, we
add them as features in a unified log-linear model.
Recent approaches have sought to improve accu-
racy by bringing more external resources and com-
plex architectures to bear. Han and Baldwin (2011)
begin with a set of string similarity metrics, and then
apply dependency parsing to identify contextually-
similar words. Liu et al (2011) extract noisy train-
ing pairs from the search snippets that result from
carefully designed queries to Google, and then train
a conditional random field (Lafferty et al, 2001) to
estimate a character-based translation model. They
later extend this work by adding a model of vi-
sual priming, an off-the-shelf spell-checker, and lo-
cal context (Liu et al, 2012a). Hassan and Menezes
(2013) use a random walk framework to capture
contextual similarity, which they then interpolate
with an edit distance metric. Rather than seek-
ing additional external resources or designing more
complex metrics of context and similarity, we pro-
pose a unified statistical model, which learns feature
weights in a maximum-likelihood framework.
3 Approach
Our approach is motivated by the following criteria:
? Unsupervised. We want to be able to train
a model without labeled data. At present, la-
beled data for Twitter normalization is avail-
able only in small quantities. Moreover, as
social media language is undergoing rapid
change (Eisenstein, 2013b), labeled datasets
may become stale and increasingly ill-suited to
new spellings and words.
62
? Low-resource. Other unsupervised ap-
proaches take advantage of resources such as
slang dictionaries and spell checkers (Han and
Baldwin, 2011; Liu et al, 2011). Resources
that characterize the current state of internet
language risk becoming outdated; in this paper
we investigate whether high-quality normaliza-
tion is possible without any such resources.
? Featurized. The relationship between any pair
of words can be characterized in a number of
different ways, ranging from simple character-
level rules (e.g., going/goin) to larger substi-
tutions (e.g., someone/sum1), and even to pat-
terns that are lexically restricted (e.g., you/u,
to/2). For these reasons, we seek a model that
permits many overlapping features to describe
candidate word pairs. These features may in-
clude simple string edit distance metrics, as
well as lexical features that memorize specific
pairs of standard and nonstandard words.
? Context-driven. Learning potentially arbitrary
word-to-word transformations without supervi-
sion would be impossible without the strong
additional cue of local context. For example,
in the phrase
give me suttin to believe in,
even a reader who has never before seen the
word suttin may recognize it as a phonetic
transcription of something. The relatively high
string edit distance is overcome by the strong
contextual preference for the word something
over orthographically closer alternatives such
as button or suiting. We can apply an arbi-
trary target language model, leveraging large
amounts of unlabeled data and catering to the
desired linguistic characteristics of the normal-
ized content.
? Holistic. While several prior approaches ?
such as normalization dictionaries ? operate at
the token level, our approach reasons over the
scope of the entire message. The necessity for
such holistic, joint inference and learning can
be seen by changing the example above to:
gimme suttin 2 beleive innnn.
None of these tokens are standard (except 2,
which appears in a nonstandard sense here), so
without joint inference, it would not be possi-
ble to use context to help normalize suttin.
Only by jointly reasoning over the entire mes-
sage can we obtain the correct normalization.
These desiderata point towards a featurized se-
quence model, which must be trained without la-
beled examples. While there is prior work on train-
ing sequence models without supervision (Smith
and Eisner, 2005; Berg-Kirkpatrick et al, 2010),
there is an additional complication not faced by
models for tasks such as part-of-speech tagging
and named entity recognition: the potential label
space of standard words is large, on the order of
at least 104. Naive application of Viterbi decod-
ing ? which is a component of training for both
Contrastive Estimation (Smith and Eisner, 2005)
and the locally-normalized sequence labeling model
of Berg-Kirkpatrick et al (2010) ? will be stymied
by Viterbi?s quadratic complexity in the dimension
of the label space. While various pruning heuris-
tics may be applied, we instead look to Sequen-
tial Monte Carlo (SMC), a randomized algorithm
which approximates the necessary feature expecta-
tions through weighted samples.
4 Model
Given a set of source-language sentences S =
{s1, s2, . . .} (e.g., Tweets), our goal is to trans-
duce them into target-language sentences T =
{t1, t2, . . .} (standard English). We are given a tar-
get language model P (t), which can be estimated
from some large set of unlabeled target-language
sentences. We denote the vocabularies of source lan-
guage and target language as ?S and ?T respectively.
We define a log-linear model that scores source
and target strings, with the form
P (s|t; ?) ? exp
(
?Tf(s, t)
)
. (1)
The desired conditional probability P (t|s) can be
obtained by combining this model with the target
language model, P (t|s) ? P (s|t; ?)P (t). Since no
labeled data is available, the parameters ? must be
estimated by maximizing the log-likelihood of the
source-language data. We define the log-likelihood
63
`?(s) for a source-language sentence s as follows:
`?(s) = logP (s) = log
?
t
P (s|t; ?)P (t)
We would like to maximize this objective by mak-
ing gradient-based updates.
?`?(s)
??
=
1
P (s)
?
t
P (t)
?
??
P (s|t; ?)
=
?
t
P (t|s)
(
f(s, t)?
?
s?
P (s?|t)f(s?, t)
)
= Et|s[f(s, t)? Es?|t[f(s
?, t)]]
(2)
We are left with a difference in expected feature
counts, as is typical in log-linear models. However,
unlike the supervised case, here both terms are ex-
pectations: the outer expectation is over all target se-
quences (given the observed source sequence), and
the nested expectation is over all source sequences,
given the target sequence. As the space of possible
target sequences t grows exponentially in the length
of the source sequence, it will not be practical to
compute this expectation directly.
Dynamic programming is the typical solution for
computing feature expectations, and can be applied
to sequence models when the feature function de-
composes locally. There are two reasons this will
not work in our case. First, while the forward-
backward algorithm would enable us to compute
Et|s, it would not give us the nested expectation
Et|s[Es?|t]; this is the classic challenge in training
globally-normalized log-linear models without la-
beled data (Smith and Eisner, 2005). Second, both
forward-backward and the Viterbi algorithm have
time complexity that is quadratic in the dimension of
the label space, at least 104 or 105. As we will show,
Sequential Monte Carlo (SMC) algorithms have a
number of advantages in this setting: they permit
the efficient computation of both the outer and inner
expectations, they are trivially parallelizable, and
the number of samples provides an intuitive tuning
tradeoff between accuracy and speed.
4.1 Sequential Monte Carlo approximation
Sequential Monte Carlo algorithms are a class of
sampling-based algorithms in which latent vari-
ables are sampled sequentially (Cappe et al, 2007).
They are particularly well-suited to sequence mod-
els, though they can be applied more broadly. SMC
algorithms maintain a set of weighted hypotheses;
the weights correspond to probabilities, and in our
case, the hypotheses correspond to target language
word sequences. Specifically, we approximate the
conditional probability,
P (t1:n|s1:n) ?
K?
k=1
?kn?tk1:n(t1:n),
where ?kn is the normalized weight of sample k at
word n (??kn is the unnormalized weight), and ?tk1:n is
a delta function centered at tk1:n.
At each step, and for each hypothesis k, a new
target word is sampled from a proposal distribution,
and the weight of the hypothesis is then updated. We
maintain feature counts for each hypothesis, and ap-
proximate the expectation by taking a weighted av-
erage using the hypothesis weights. The proposal
distribution will be described in detail later.
We make a Markov assumption, so that the emis-
sion probability P (s|t) decomposes across the ele-
ments of the sentence P (s|t) =
?N
n P (sn|tn). This
means that the feature functions f(s, t) must decom-
pose on each ?sn, tn? pair. We can then rewrite (1)
as
P (s|t; ?) =
N?
n
exp
(
?Tf(sn, tn)
)
Z(tn)
(3)
Z(tn) =
?
s
exp
(
?Tf(s, tn)
)
. (4)
In addition, we assume that the target language
model P (t) can be written as an N-gram language
model, P (t) =
?
n P (tn|tn?1, . . . tn?k+1). With
these assumptions, we can view normalization as
a finite state-space model in which the target lan-
guage model defines the prior distribution of the pro-
cess and Equation 3 defines the likelihood function.
We are able to compute the the posterior probabil-
ity P (t|s) using sequential importance sampling, a
member of the SMC family.
The crucial idea in sequential importance sam-
pling is to update the hypotheses tk1:n and their
weights ?kn so that they approximate the posterior
distribution at the next time step, P (t1:n+1|s1:n+1).
64
Assuming the proposal distribution has the form
Q(tk1:n|s1:n), the importance weights are given by
?kn ?
P (tk1:n|s1:n)
Q(tk1:n|s1:n)
(5)
In order to update the hypotheses recursively, we
rewrite P (t1:n|s1:n) as:
P (t1:n|s1:n) =
P (sn|t1:n, s1:n?1)P (t1:n|s1:n?1)
P (sn|s1:n?1)
=
P (sn|tn)P (tn|t1:n?1, s1:n?1)P (t1:n?1|s1:n?1)
P (sn|s1:n?1)
?P (sn|tn)P (tn|tn?1)P (t1:n?1|s1:n?1),
assuming a bigram language model. We further as-
sume the proposal distribution Q can be factored as:
Q(t1:n|s1:n) =Q(tn|t1:n?1, s1:n)Q(t1:n?1|s1:n?1)
=Q(tn|tn?1, sn)Q(t1:n?1|s1:n?1).
(6)
Then the unnormalized importance weights sim-
plify to a recurrence:
??kn =
P (sn|tkn)P (t
k
n|t
k
n?1)P (t
k
1:n?1|s1:n?1)
Q(tn|tn?1, sn)Q(tk1:n?1|s1:n?1)
(7)
=?kn?1
P (sn|tkn)P (t
k
n|t
k
n?1)
Q(tn|tn?1, sn)
(8)
Therefore, we can approximate the posterior dis-
tribution P (tn|s1:n) ?
?K
k=1 ?
k
n?tkn(tn), and com-
pute the outer expectation as follows:
Et|s[f(s, t)] =
K?
k=1
?kN
N?
n=1
f(sn, tkn) (9)
We compute the nested expectation using a non-
sequential Monte Carlo approximation, assuming
we can draw s`,k ? P (s|tkn).
Es|tk [f(s, t
k)] =
1
L
N?
n=1
L?
`=1
f(s`,kn , t
k
n)
This gives the overall gradient computation:
Et|s[f(s, t)? Es?|t[f(s
?, t)]] =
1
?K
k=1 ??
k
N
K?
k=1
??kN
?
N?
n=1
(
f(sn, tkn)?
1
L
L?
`=1
f(s`,kn , t
k
n)
)
(10)
where we sample tkn and update ?
k
n while mov-
ing from left-to-right, and sample s`,kn at each n.
Note that although the sequential importance sam-
pler moves left-to-right like a filter, we use only the
final weights ?N to compute the expectation. Thus,
the resulting expectation is based on the distribu-
tion P (s1:N |t1:N ), so that no backwards ?smooth-
ing? pass (Godsill et al, 2004) is needed to elim-
inate bias. Other applications of sequential Monte
Carlo make use of resampling (Cappe et al, 2007) to
avoid degeneration of the hypothesis weights, but we
found this to be unnecessary due to the short length
of Twitter messages.
4.2 Proposal distribution
The major computational challenge for dynamic
programming approaches to normalization is the
large label space, equal to the size of the target vo-
cabulary. It may appear that all we have gained
by applying sequential Monte Carlo is to convert
a computational problem into a statistical one: a
naive sampling approach will have little hope of
finding the small high-probability region of the high-
dimensional label space. However, sequential im-
portance sampling allows us to address this issue
through the proposal distribution, from which we
sample the candidate words tn. Careful design of the
proposal distribution can guide sampling towards
the high-probability space. In the asymptotic limit of
an infinite number of samples, any non-pathological
proposal distribution will ultimately arrive at the de-
sired estimate, but a good proposal distribution can
greatly reduce the number of samples needed.
Doucet et al (2001) note that the optimal pro-
posal ? which minimizes the variance of the im-
portance weights conditional on t1:n?1 and s1:n ?
has the following form:
Q(tkn|sn, t
k
n?1) =
P (sn|tkn)P (t
k
n|t
k
n?1)
?
t? P (sn|t
?)P (t?|tkn?1)
(11)
65
Sampling from this proposal requires computing
the normalized distribution P (sn|tkn); similarly, the
update of the hypothesis weights (Equation 8) re-
quires the calculation ofQ in its normalized form. In
each case, the total cost is the product of the vocabu-
lary sizes, O(#|?T |#|?S |), which is not tractable as
the vocabularies become large.
In low-dimensional settings, a convenient so-
lution is to set the proposal distribution equal
to the transition distribution, Q(tkn|sn, t
k
n?1) =
P (tkn|t
k
n?1, . . . , t
k
n?k+1). This choice is called the
?bootstrap filter,? and it has the advantage that the
weights ?(k) are exactly identical to the product
of emission likelihoods
?
n P (sn|t
k
n). The com-
plexity of computing the hypothesis weights is thus
O(#|?S |). However, because this proposal ignores
the emission likelihood, the bootstrap filter has very
little hope of finding a high-probability sample in
high-entropy contexts.
We strike a middle ground between efficiency and
accuracy, using a proposal distribution that is closely
related to the overall likelihood, yet is tractable to
sample and compute:
Q(tkn|sn, t
k
n?1)
def
=
P (sn|tkn)Z(t
k
n)P (t
k
n|t
k
n?1)
?
t? P (sn|t
?)Z(t?)P (t?|tkn?1)
=
exp
(
?Tf(sn, tn)
)
P (tkn|t
k
n?1)
?
t? exp
(
?Tf(sn, t?)
)
P (t?|tkn?1)
(12)
Here, we simply replace the likelihood distribu-
tion in (11) by its unnormalized version.
To update the unnormalized hypothesis weights
??kn, we have
??kn =?
k
n?1
?
t? exp
(
?Tf(sn, t?)
)
P (t?|tkn?1)
Z(tkn)
(13)
The numerator requires summing over all ele-
ments in ?T and the denominator Z(tkn) requires
summing over all elements in ?S , for a total cost of
O(#|?T |+ #|?S |).
4.3 Decoding
Given an input source sentence s, the decoding prob-
lem is to find a target sentence t that maximizes
P (t|s) ? P (s|t)P (t) =
?N
n P (sn|tn)P (tn|tn?1).
Feature name Description
word-word pair A set of binary features for each
source/target word pair ?s, t?
string similarity A set of binary features in-
dicating whether s is one of
the top N string similar non-
standard words of t, for N ?
{5, 10, 25, 50, 100, 250, 500, 1000}
Table 1: The feature set for our log-linear model
As with learning, we cannot apply the usual dy-
namic programming algorithm (Viterbi), because
of its quadratic cost in the size of the target lan-
guage vocabulary. This must be multiplied by
the cost of computing the normalized probability
P (sn|tn), resulting in a prohibitive time complexity
of O(#|?S |#|?T |2N).
We consider two approximate decoding algo-
rithms. The first is to simply apply the proposal dis-
tribution, with linear complexity in the size of the
two vocabularies. However, this decoder is not iden-
tical to P (t|s), because of the extra factor of Z(t)
in the numerator. Alternatively, we can apply the
proposal distribution for selecting target word can-
didates, then apply the Viterbi algorithm only within
these candidates. The total cost is O(#|?S |T 2N),
where T is the number of target word candidates we
consider; this will asymptotically approach P (t|s)
as T ? #|?T |. Our evaluations use the more expen-
sive proposal+Viterbi decoding, but accuracy with
the more efficient proposal-based decoding is very
similar.
4.4 Features
Our system uses the feature types described in Ta-
ble 1. The word pair features are designed to cap-
ture lexical conventions, e.g. you/u. We only con-
sider word pair features that fired during training.
The string similarity features rely on the similarity
function proposed by Contractor et al (2010), which
has proven effective for normalization in prior work.
We bin this similarity to create binary features indi-
cating whether a string s is in the top-N most similar
strings to t; this binning yields substantial speed im-
provements without negatively impacting accuracy.
66
5 Implementation and data
The model and inference described in the pre-
vious section are implemented in a software
system for normalizing text on twitter, called
UNLOL: unsupervised normalization in a LOg-
Linear model. The final system can process roughly
10,000 Tweets per hour. We now describe some im-
plementation details.
5.1 Normalization candidates
Most tokens in tweets do not require normalization.
The question of how to identify which words are
to be normalized is still an open problem. Follow-
ing Han and Baldwin (2011), we build a dictionary
of words which are permissible in the target domain,
and make no attempt to normalize source strings
that match these words. As with other comparable
approaches, we are therefore unable to normalize
strings like ill into I?ll. Our set of ?in-vocabulary?
(IV) words is based on the GNU aspell dictionary
(v0.60.6), containing 97,070 words. From this dic-
tionary, we follow Liu et al (2012a) and remove all
the words with a count of less than 20 in the Edin-
burgh Twitter corpus (Petrovic? et al, 2010) ? re-
sulting in a total of 52,449 target words. All sin-
gle characters except a and i are excluded, and rt
is treated as in-vocabulary. For all in-vocabulary
words, we define P (sn|tn) = ?(sn, tn), taking the
value of zero when sn 6= tn. This effectively pre-
vents our model from attempting to normalize these
words.
In addition to words that are in the target vocabu-
lary, there are many other strings that should not be
normalized, such as names and multiword shorten-
ings (e.g. going to/gonna).1 We follow prior work
and assume that the set of normalization candidates
is known in advance during test set decoding (Han et
al., 2013). However, the unlabeled training data has
no such information. Thus, during training we at-
tempt to normalize all tokens that (1) are not in our
lexicon of IV words, and (2) are composed of letters,
numbers and the apostrophe. This set includes con-
tractions like "gonna" and "gotta", which would not
appear in the test set, but are nonetheless normalized
1Whether multiword shortenings should be normalized is ar-
guable, but they are outside the scope of current normalization
datasets (Han and Baldwin, 2011).
during training. For each OOV token, we conduct a
pre-normalization step by reducing any repetitions
of more than two letters in the nonstandard words to
exactly two letters (e.g., cooool? cool).
5.2 Language modeling
The Kneser-Ney smoothed trigram target language
model is estimated with the SRILM toolkit Stolcke
(2002), using Tweets from the Edinburgh Twitter
corpus that contain no OOV words besides hash-
tags and username mentions (following (Han et al,
2013)). We use this language model for both training
and decoding. We occasionally find training con-
texts in which the trigram ?tn, tn?1, tn?2? is unob-
served in the language model data; features resulting
from such trigrams are not considered when comput-
ing the weight gradients.
5.3 Parameters
The Monte Carlo approximations require two pa-
rameters: the number of samples for sequential
Monte Carlo (K), and the number of samples for the
non-sequential sampler of the nested expectation (L,
from Equation 10). The theory of Monte Carlo ap-
proximation states that the quality of the approxima-
tion should only improve as the number of samples
increases; we obtained good results with K = 10
and L = 1, and found relatively little improvement
by increasing these values. The number of hypothe-
ses considered by the decoder is set to T = 10;
again, the performance should only improve with T ,
as we more closely approximate full Viterbi decod-
ing.
6 Experiments
Datasets We use two existing labeled Twitter
datasets to evaluate our approach. The first dataset
? which we call LWWL11, based on the names of
its authors Liu et al (2011) ? contains 3,802 indi-
vidual ?nonstandard? words (i.e., words that are not
in the target vocabulary) and their normalized forms.
The rest of the message in which the words is appear
is not available. As this corpus does not provide lin-
guistic context, its decoding must use a unigram tar-
get language model. The second dataset ? which
is called LexNorm1.1 by its authors Han and Bald-
win (2011) ? contains 549 complete tweets with
1,184 nonstandard tokens (558 unique word types).
67
Method Dataset Precision Recall F-measure
(Liu et al 2011)
LMML11
68.88 68.88 68.88
(Liu et al 2012) 69.81 69.81 69.81
UNLOL 73.04 73.04 73.04
(Han and Baldwin, 2011)
LexNorm 1.1
75.30 75.30 75.30
(Liu et al 2012) 84.13 78.38 81.15
(Hassan et al 2013) 85.37 56.4 69.93
UNLOL 82.09 82.09 82.09
UNLOL LexNorm 1.2 82.06 82.06 82.06
Table 2: Empirical results
In this corpus, we can decode with a trigram lan-
guage model.
Close analysis of LexNorm1.1 revealed some in-
consistencies in annotation (for example, y?all
and 2 are sometimes normalized to you and to,
but are left unnormalized in other cases). In ad-
dition, several annotations disagree with existing
resources on internet language and dialectal En-
glish. For example, smh is normalized to some-
how in LexNorm1.1, but internetslang.com
and urbandictionary.com assert that it stands
for shake my head, and this is evident from examples
such as smh at this girl. Similarly, finna
is normalized to finally in LexNorm1.1, but from
the literature on African American English (Green,
2002), it corresponds to fixing to (e.g., i?m finna
go home). To address these issues, we have pro-
duced a new version of this dataset, which we call
LexNorm1.2 (after consulting with the creators of
LexNorm1.1). LexNorm1.2 differs from version 1.1
in the annotations for 172 of the 2140 OOV words.
We evaluate on LexNorm1.1 to compare with prior
work, but we also present results on LexNorm1.2
in the hope that it will become standard in future
work on normalization in English. The dataset
is available at http://www.cc.gatech.edu/
~jeisenst/lexnorm.v1.2.tgz.
To obtain unlabeled training data, we randomly
sample 50 tweets from the Edinburgh Twitter cor-
pus Petrovic? et al (2010) for each OOV word. Some
OOV words appear less than 50 times in the cor-
pus, so we obtained more training tweets for them
through the Twitter search API.
Metrics Prior work on these datasets has assumed
perfect detection of words requiring normalization,
and has focused on finding the correct normalization
for these words (Han and Baldwin, 2011; Han et al,
2013). Recall has been defined as the proportion of
words requiring normalization which are normalized
correctly; precision is defined as the proportion of
normalizations which are correct.
Results We run our training algorithm for two it-
erations (pass the training data twice). The results
are presented in Table 2. Our system, UNLOL,
achieves the highest published F-measure on both
datasets. Performance on LexNorm1.2 is very simi-
lar to LexNorm1.1, despite the fact that roughly 8%
of the examples were relabeled.
In the normalization task that we consider, the to-
kens to be normalized are specified in advance. This
is the same task specification as in the prior work
against which we compare. At test time, our system
attempts normalizes all such tokens; every error is
thus both a false positive and false negative, so pre-
cision equals to recall for this task; this is also true
for Han and Baldwin (2011) and Liu et al (2011).
It is possible to trade recall for precision by re-
fusing to normalize words when the system?s confi-
dence falls below a threshold. A good setting of this
threshold can improve the F-measure, but we did not
report these results because we have no development
set for parameter tuning.
Regularization One potential concern is that the
number of non-zero feature weights will continually
increase until the memory cost becomes overwhelm-
ing. Although we did not run up against mem-
68
0 100000 200000 300000 400000number of features
79
80
81
82
83
F-m
eas
ure
?=1e?04
?=5e?05
?=1e?05
?=5e?06 ?=1e?06 ?=0e+00 ? Dataset F-measure # of features
10?4
LexNorm 1.1
79.05 9,281
5? 10?5 80.32 11,794
10?5 81.00 42,466
5? 10?6 82.52 74,744
10?6 82.35 241,820
0 82.26 369,366
5? 10?6 LexNorm 1.2 82.23 74,607
Figure 1: Effect of L1 regularization on the F-measure and the number of features with non-zero weights
ory limitations in the experiments producing the re-
sults in Table 2, this issue can be addressed through
the application of L1 regularization, which produces
sparse weight vectors by adding a penalty of ?||?||1
to the log-likelihood. We perform online optimiza-
tion of the L1-regularized log-likelihood by apply-
ing the truncated gradient method (Langford et al,
2009). We use an exponential decreasing learning
rate ?k = ?0?k/N , where k is the iteration counter
andN is the size of training data. We set ?0 = 1 and
? = 0.5. Experiments were run until 300,000 train-
ing instances were observed, with a final learning
rate of less than 1/32. As shown in Figure 1, a small
amount of regularization can dramatically decrease
the number of active features without harming per-
formance.
7 Analysis
We apply our normalization system to investi-
gate the orthographic processes underlying language
variation in social media. Using a dataset of 400,000
English language tweets, sampled from the month
of August in each year from 2009 to 2012, we ap-
ply UNLOL to automatically normalize each token.
We then treat these normalizations as labeled train-
ing data, and examine the Levenshtein alignment be-
tween the source and target tokens. This alignment
gives approximate character-level transduction rules
to explain each OOV token. We then examine which
rules are used by each author, constructing a matrix
of authors and rules.2
Factorization of the author-rule matrix reveals sets
of rules that tend to be used together; we might
call these rulesets ?orthographic styles.? We apply
non-negative matrix factorization (Lee and Seung,
2001), which characterizes each author by a vector
of k style loadings, and simultaneously constructs
k style dictionaries, which each put weight on dif-
ferent orthographic rules. Because the loadings are
constrained to be non-negative, the factorization can
be seen as sparsely assigning varying amounts of
each style to each author. We choose the factoriza-
tion that minimizes the Frobenius norm of the recon-
struction error, using the NIMFA software package
(http://nimfa.biolab.si/).
The resulting styles are shown in Table 3, for
k = 10; other values of k give similar overall re-
sults with more or less detail. The styles incor-
porate a number of linguistic phenomena, includ-
ing: expressive lengthening (styles 7-9; see Brody
and Diakopoulos, 2011); g- and t-dropping (style 5,
see Eisenstein 2013a) ; th-stopping (style 6); and
the dropping of several word-final vowels (styles
1-3). Some of these styles, such as t-dropping
and th-stopping, have direct analogues in spoken
language varieties (Tagliamonte and Temple, 2005;
Green, 2002), while others, like expressive length-
ening, seem more unique to social media. The re-
lationships between these orthographic styles and
social variables such as geography and demograph-
2We tried adding these rules as features and retraining the
normalization system, but this hurt performance.
69
style rules examples
1. you; o-dropping y/_ ou/_u *y/*_ o/_ u, yu, 2day, knw, gud, yur, wud, yuh, u?ve, toda,
everthing, everwhere, ourself
2. e-dropping, u/o be/b_ e/_ o/u e*/_* b, r, luv, cum, hav, mayb, bn, remembr, btween,
gunna, gud
3. a-dropping a/_ *a/*_ re/r_ ar/_r r, tht, wht, yrs, bck, strt, gurantee,
elementry, wr, rlly, wher, rdy, preciate,
neway
4. g-dropping g*/_* ng/n_ g/_ goin, talkin, watchin, feelin, makin
5. t-dropping t*/_* st/s_ t/_ jus, bc, shh, wha, gota, wea, mus, firts, jes,
subsistutes
6. th-stopping h/_ *t/*d th/d_ t/d dat, de, skool, fone, dese, dha, shid, dhat,
dat?s
7. (kd)-lengthening i_/id _/k _/d _*/k* idk, fuckk, okk, backk, workk, badd, andd,
goodd, bedd, elidgible, pidgeon
8. o-lengthening o_/oo _*/o* _/o soo, noo, doo, oohh, loove, thoo, helloo
9. e-lengthening _/i e_/ee _/e _*/e* mee, ive, retweet, bestie, lovee, nicee, heey,
likee, iphone, homie, ii, damnit
10. a-adding _/a __/ma _/m _*/a* ima, outta, needa, shoulda, woulda, mm,
comming, tomm, boutt, ppreciate
Table 3: Orthographic styles induced from automatically normalized Twitter text
ics must be left to future research, but they offer a
promising generalization of prior work that has fo-
cused almost exclusively on exclusively on lexical
variation (Argamon et al, 2007; Eisenstein et al,
2010; Eisenstein et al, 2011), with a few exceptions
for character-level features (Brody and Diakopoulos,
2011; Burger et al, 2011).
Note that style 10 is largely the result of mis-
taken normalizations. The tokens ima, outta, and
needa all refer to multi-word expressions in stan-
dard English, and are thus outside the scope of the
normalization task as defined by Han et al (2013).
UNLOL has produced incorrect single-token nor-
malizations for these terms: i/ima, out/outta, and
need/needa. But while these normalizations are
wrong, the resulting style nonetheless captures a co-
herent orthographic phenomenon.
8 Conclusion
We have presented a unified, unsupervised statistical
model for normalizing social media text, attaining
the best reported performance on the two standard
normalization datasets. The power of our approach
comes from flexible modeling of word-to-word re-
lationships through features, while exploiting con-
textual regularity to train the corresponding feature
weights without labeled data. The primary techni-
cal challenge was overcoming the large label space
of the normalization task; we accomplish this us-
ing sequential Monte Carlo. Future work may con-
sider whether sequential Monte Carlo can offer sim-
ilar advantages in other unsupervised NLP tasks. An
additional benefit of our joint statistical approach is
that it may be combined with other downstream lan-
guage processing tasks, such as part-of-speech tag-
ging (Gimpel et al, 2011) and named entity resolu-
tion (Liu et al, 2012b).
Acknowledgments
We thank the reviewers for thoughtful comments
on our submission. This work also benefitted
from discussions with Timothy Baldwin, Paul Cook,
Frank Dellaert, Arnoud Doucet, Micha Elsner, and
Sharon Goldwater. It was supported by NSF SOCS-
1111142.
References
S. Argamon, M. Koppel, J. Pennebaker, and J. Schler.
2007. Mining the blogosphere: age, gender, and the
varieties of self-expression. First Monday, 12(9).
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of ACL, pages 33?40.
70
Richard Beaufort, Sophie Roekhaut, Louise-Am?lie
Cougnon, and C?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of ACL, pages 770?
779.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL, pages 582?590.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on twit-
ter. In Proceedings of EMNLP.
Olivier Cappe, Simon J. Godsill, and Eric Moulines.
2007. An overview of existing methods and recent ad-
vances in sequential monte carlo. Proceedings of the
IEEE, 95(5):899?924, May.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007a. Investigation and model-
ing of the structure of texting language. Interna-
tional Journal on Document Analysis and Recognition,
10(3):157?174.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007b. Investigation and modeling of the structure of
texting language. International Journal of Document
Analysis and Recognition (IJDAR), 10(3-4):157?174.
Danish Contractor, Tanveer A. Faruquie, and L. Venkata
Subramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceedings of COLING, pages 189?196.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity, CALC ?09, pages
71?78, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
A. Doucet, N.J. Gordon, and V. Krishnamurthy. 2001.
Particle filters for state estimation of jump markov lin-
ear systems. Trans. Sig. Proc., 49(3):613?624, March.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of ACL.
Jacob Eisenstein. 2013a. Phonological factors in social
media writing. In Proceedings of the NAACL Work-
shop on Language Analysis in Social Media.
Jacob Eisenstein. 2013b. What to do about bad language
on the internet. In Proceedings of NAACL, pages 359?
369.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In Pro-
ceedings of ACL.
Simon J. Godsill, Arnaud Doucet, and Mike West. 2004.
Monte carlo smoothing for non-linear time series. In
Journal of the American Statistical Association, pages
156?168.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First Workshop on Unsu-
pervised Learning in NLP, EMNLP ?11.
Lisa J. Green. 2002. African American English: A
Linguistic Introduction. Cambridge University Press,
September.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: makn sens a #twitter. In
Proceedings of ACL, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lex-
ical normalization for social media text. ACM Trans-
actions on Intelligent Systems and Technology, 4(1):5.
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proceedings of ACL.
Catherine Kobus, Fran?ois Yvon, and G?raldine
Damnati. 2008. Normalizing sms: are two metaphors
better than one? In Proceedings of COLING, pages
441?448.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of ICML, pages 282?289.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. The
Journal of Machine Learning Research, 10:777?801.
D. D. Lee and H. S. Seung. 2001. Algorithms for Non-
Negative Matrix Factorization. In Advances in Neural
Information Processing Systems (NIPS), volume 13,
pages 556?562.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normaliz-
ing text messages without pre-categorization nor su-
pervision. In Proceedings of ACL, pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of ACL, pages 1035?1044.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang
Fu, and Furu Wei. 2012b. Joint inference of named
entity recognition and normalization for tweets. In
Proceedings of ACL.
71
Sa?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 354?362, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. Sproat, A.W. Black, S. Chen, S. Kumar, M. Os-
tendorf, and C. Richards. 2001. Normalization of
non-standard words. Computer Speech & Language,
15(3):287?333.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904.
Sali Tagliamonte and Rosalind Temple. 2005. New
perspectives on an ol? variable: (t,d) in british en-
glish. Language Variation and Change, 17:281?302,
September.
Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of ACL,
pages 1159?1168.
72
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891?896,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Discriminative Improvements to Distributional Sentence Similarity
Yangfeng Ji
School of Interactive Computing
Georgia Institute of Technology
jiyfeng@gatech.edu
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Abstract
Matrix and tensor factorization have been ap-
plied to a number of semantic relatedness
tasks, including paraphrase identification. The
key idea is that similarity in the latent space
implies semantic relatedness. We describe
three ways in which labeled data can im-
prove the accuracy of these approaches on
paraphrase classification. First, we design
a new discriminative term-weighting metric
called TF-KLD, which outperforms TF-IDF.
Next, we show that using the latent repre-
sentation from matrix factorization as features
in a classification algorithm substantially im-
proves accuracy. Finally, we combine latent
features with fine-grained n-gram overlap fea-
tures, yielding performance that is 3% more
accurate than the prior state-of-the-art.
1 Introduction
Measuring the semantic similarity of short units
of text is fundamental to many natural language
processing tasks, from evaluating machine transla-
tion (Kauchak and Barzilay, 2006) to grouping re-
dundant event mentions in social media (Petrovic?
et al, 2010). The task is challenging because of
the infinitely diverse set of possible linguistic real-
izations for any idea (Bhagat and Hovy, 2013), and
because of the short length of individual sentences,
which means that standard bag-of-words representa-
tions will be hopelessly sparse.
Distributional methods address this problem by
transforming the high-dimensional bag-of-words
representation into a lower-dimensional latent space.
This can be accomplished by factoring a matrix
or tensor of term-context counts (Turney and Pan-
tel, 2010); proximity in the induced latent space
has been shown to correlate with semantic similar-
ity (Mihalcea et al, 2006). However, factoring the
term-context matrix means throwing away a consid-
erable amount of information, as the original ma-
trix of size M ?N (number of instances by number
of features) is factored into two smaller matrices of
size M ?K and N ?K, with K  M,N . If the
factorization does not take into account labeled data
about semantic similarity, important information can
be lost.
In this paper, we show how labeled data can con-
siderably improve distributional methods for mea-
suring semantic similarity. First, we develop a
new discriminative term-weighting metric called
TF-KLD, which is applied to the term-context ma-
trix before factorization. On a standard paraphrase
identification task (Dolan et al, 2004), this method
improves on both traditional TF-IDF and Weighted
Textual Matrix Factorization (WTMF; Guo and
Diab, 2012). Next, we convert the latent repre-
sentations of each sentence pair into a feature vec-
tor, which is used as input to a linear SVM clas-
sifier. This yields further improvements and sub-
stantially outperforms the current state-of-the-art
on paraphrase classification. We then add ?fine-
grained? features about the lexical similarity of the
sentence pair. The combination of latent and fine-
grained features yields further improvements in ac-
curacy, demonstrating that these feature sets provide
complementary information on semantic similarity.
891
2 Related Work
Without attempting to do justice to the entire lit-
erature on paraphrase identification, we note three
high-level approaches: (1) string similarity metrics
such as n-gram overlap and BLEU score (Wan et
al., 2006; Madnani et al, 2012), as well as string
kernels (Bu et al, 2012); (2) syntactic operations
on the parse structure (Wu, 2005; Das and Smith,
2009); and (3) distributional methods, such as la-
tent semantic analysis (LSA; Landauer et al, 1998),
which are most relevant to our work. One appli-
cation of distributional techniques is to replace in-
dividual words with distributionally similar alterna-
tives (Kauchak and Barzilay, 2006). Alternatively,
Blacoe and Lapata (2012) show that latent word rep-
resentations can be combined with simple element-
wise operations to identify the semantic similarity
of larger units of text. Socher et al (2011) pro-
pose a syntactically-informed approach to combine
word representations, using a recursive auto-encoder
to propagate meaning through the parse tree.
We take a different approach: rather than repre-
senting the meanings of individual words, we di-
rectly obtain a distributional representation for the
entire sentence. This is inspired by Mihalcea et al
(2006) and Guo and Diab (2012), who treat sen-
tences as pseudo-documents in an LSA framework,
and identify paraphrases using similarity in the la-
tent space. We show that the performance of such
techniques can be improved dramatically by using
supervised information to (1) reweight the individ-
ual distributional features and (2) learn the impor-
tance of each latent dimension.
3 Discriminative feature weighting
Distributional representations (Turney and Pantel,
2010) can be induced from a co-occurrence ma-
trix W ? RM?N , where M is the number of in-
stances and N is the number of distributional fea-
tures. For paraphrase identification, each instance
is a sentence; features may be unigrams, or may
include higher-order n-grams or dependency pairs.
By decomposing the matrix W, we hope to obtain
a latent representation in which semantically-related
sentences are similar. Singular value decomposition
(SVD) is traditionally used to perform this factoriza-
tion. However, recent work has demonstrated the ro-
bustness of nonnegative matrix factorization (NMF;
Lee and Seung, 2001) for text mining tasks (Xu et
al., 2003; Arora et al, 2012); the difference from
SVD is the addition of a non-negativity constraint
in the latent representation based on non-orthogonal
basis.
WhileW may simply contain counts of distribu-
tional features, prior work has demonstrated the util-
ity of reweighting these counts (Turney and Pantel,
2010). TF-IDF is a standard approach, as the inverse
document frequency (IDF) term increases the impor-
tance of rare words, which may be more discrimi-
native. Guo and Diab (2012) show that applying a
special weight to unseen words can further improve-
ment performance on paraphrase identification.
We present a new weighting scheme, TF-KLD,
based on supervised information. The key idea is
to increase the weights of distributional features that
are discriminative, and to decrease the weights of
features that are not. Conceptually, this is similar
to Linear Discriminant Analysis, a supervised fea-
ture weighting scheme for continuous data (Murphy,
2012).
More formally, we assume labeled sentence pairs
of the form ?~w(1)i , ~w
(2)
i , ri?, where ~w
(1)
i is the bi-
narized vector of distributional features for the first
sentence, ~w(2)i is the binarized vector of distribu-
tional features for the second sentence, and ri ?
{0, 1} indicates whether they are labeled as a para-
phrase pair. Assuming the order of the sentences
within the pair is irrelevant, then for k-th distribu-
tional feature, we define two Bernoulli distributions:
? pk = P (w
(1)
ik |w
(2)
ik = 1, ri = 1). This is the
probability that sentence w(1)i contains feature
k, given that k appears in w(2)i and the two sen-
tences are labeled as paraphrases, ri = 1.
? qk = P (w
(1)
ik |w
(2)
ik = 1, ri = 0). This is the
probability that sentence w(1)i contains feature
k, given that k appears in w(2)i and the two sen-
tences are labeled as not paraphrases, ri = 0.
The Kullback-Leibler divergence KL(pk||qk) =
?
x pk(x) log
pk(x)
qk(x)
is then a measure of the discrim-
inability of feature k, and is guaranteed to be non-
892
0.0 0.2 0.4 0.6 0.8 1.0pk
0.0
0.2
0.4
0.6
0.8
1.0
1?q
k
neither
nor
not
fear
same
but
off
shares
studythen
0.200
0.200
0.400
0.400
0.600
0.600
0.800 0.800
1.000
1.000
Figure 1: Conditional probabilities for a few hand-
selected unigram features, with lines showing contours
with identical KL-divergence. The probabilities are es-
timated based on the MSRPC training set (Dolan et al,
2004).
negative.1 We use this divergence to reweight the
features in W before performing the matrix factor-
ization. This has the effect of increasing the weights
of features whose likelihood of appearing in a pair
of sentences is strongly influenced by the paraphrase
relationship between the two sentences. On the other
hand, if pk = qk, then the KL-divergence will be
zero, and the feature will be ignored in the ma-
trix factorization. We name this weighting scheme
TF-KLD, since it includes the term frequency and
the KL-divergence.
Taking the unigram feature not as an example, we
have pk = [0.66, 0.34] and qk = [0.31, 0.69], for a
KL-divergence of 0.25: the likelihood of this word
being shared between two sentence is strongly de-
pendent on whether the sentences are paraphrases.
In contrast, the feature then has pk = [0.33, 0.67]
and qk = [0.32, 0.68], for a KL-divergence of 3.9?
10?4. Figure 1 shows the distributions of these and
other unigram features with respect to pk and 1?qk.
The diagonal line running through the middle of the
plot indicates zero KL-divergence, so features on
this line will be ignored.
1We obtain very similar results with the opposite divergence
KL(qk||pk). However, the symmetric Jensen-Shannon diver-
gence performs poorly.
1 unigram recall
2 unigram precision
3 bigram recall
4 bigram precision
5 dependency relation recall
6 dependency relation precision
7 BLEU recall
8 BLEU precision
9 Difference of sentence length
10 Tree-editing distance
Table 1: Fine-grained features for paraphrase classifica-
tion, selected from prior work (Wan et al, 2006).
4 Supervised classification
While previous work has performed paraphrase clas-
sification using distance or similarity in the latent
space (Guo and Diab, 2012; Socher et al, 2011),
more direct supervision can be applied. Specifically,
we convert the latent representations of a pair of sen-
tences ~v1 and ~v2 into a sample vector,
~s(~v1, ~v2) =
[
~v1 + ~v2, |~v1 ? ~v2|
]
, (1)
concatenating the element-wise sum ~v1 +~v2 and ab-
solute difference |~v1 ? ~v2|. Note that ~s(?, ?) is sym-
metric, since ~s(~v1, ~v2) = ~s(~v2, ~v1). Given this rep-
resentation, we can use any supervised classification
algorithm.
A further advantage of treating paraphrase as a
supervised classification problem is that we can ap-
ply additional features besides the latent represen-
tation. We consider a subset of features identified
by Wan et al (2006), listed in Table 1. These fea-
tures mainly capture fine-grained similarity between
sentences, for example by counting specific unigram
and bigram overlap.
5 Experiments
Our experiments test the utility of the TF-
KLD weighting towards paraphrase classification,
using the Microsoft Research Paraphrase Corpus
(Dolan et al, 2004). The training set contains 2753
true paraphrase pairs and 1323 false paraphrase
pairs; the test set contains 1147 and 578 pairs, re-
spectively.
The TF-KLD weights are constructed from only
the training set, while matrix factorizations are per-
893
formed on the entire corpus. Matrix factorization on
both training and (unlabeled) test data can be viewed
as a form of transductive learning (Gammerman et
al., 1998), where we assume access to unlabeled test
set instances.2 We also consider an inductive setting,
where we construct the basis of the latent space from
only the training set, and then project the test set
onto this basis to find the corresponding latent rep-
resentation. The performance differences between
the transductive and inductive settings were gener-
ally between 0.5% and 1%, as noted in detail be-
low. We reiterate that the TF-KLD weights are never
computed from test set data.
Prior work on this dataset is described in sec-
tion 2. To our knowledge, the current state-of-the-
art is a supervised system that combines several ma-
chine translation metrics (Madnani et al, 2012), but
we also compare with state-of-the-art unsupervised
matrix factorization work (Guo and Diab, 2012).
5.1 Similarity-based classification
In the first experiment, we predict whether a pair
of sentences is a paraphrase by measuring their co-
sine similarity in latent space, using a threshold for
the classification boundary. As in prior work (Guo
and Diab, 2012), the threshold is tuned on held-out
training data. We consider two distributional feature
sets: FEAT1, which includes unigrams; and FEAT2,
which also includes bigrams and unlabeled depen-
dency pairs obtained from MaltParser (Nivre et al,
2007). To compare with Guo and Diab (2012), we
set the latent dimensionality to K = 100, which was
the same in their paper. Both SVD and NMF factor-
ization are evaluated; in both cases, we minimize the
Frobenius norm of the reconstruction error.
Table 2 compares the accuracy of a num-
ber of different configurations. The transductive
TF-KLD weighting yields the best overall accu-
racy, achieving 72.75% when combined with non-
negative matrix factorization. While NMF performs
slightly better than SVD in both comparisons, the
major difference is the performance of discrimina-
tive TF-KLD weighting, which outperforms TF-IDF
regardless of the factorization technique. When we
2Another example of transductive learning in NLP is
when Turian et al (2010) induced word representations from a
corpus that included both training and test data for their down-
stream named entity recognition task.
50 100 150 200 250 300 350 400K60
65
70
75
80
Accu
racy
 (%)
Feat1_TF-IDF_SVMFeat2_TF-IDF_SVMFeat1_TF-KLD_SVMFeat2_TF-KLD_SVM
Figure 2: Accuracy of feature and weighting combina-
tions in the classification framework.
perform the matrix factorization on only the training
data, the accuracy on the test set is 73.58%, with F1
score 80.55%.
5.2 Supervised classification
Next, we apply supervised classification, construct-
ing sample vectors from the latent representation as
shown in Equation 1. For classification, we choose
a Support Vector Machine with a linear kernel (Fan
et al, 2008), leaving a thorough comparison of clas-
sifiers for future work. The classifier parameter C is
tuned on a development set comprising 20% of the
original training set.
Figure 2 presents results for a range of latent di-
mensionalities. Supervised learning identifies the
important dimensions in the latent space, yielding
significantly better performance that the similarity-
based classification from the previous experiment.
In Table 3, we compare against prior published
work, using the held-out development set to select
the best value of K (again, K = 400). The best
result is from TF-KLD, with distributional features
FEAT2, achieving 79.76% accuracy and 85.87% F1.
This is well beyond all known prior results on this
task. When we induce the latent basis from only
the training data, we get 78.55% on accuracy and
84.59% F1, also better than the previous state-of-art.
Finally, we augment the distributional represen-
tation, concatenating the ten ?fine-grained? fea-
tures in Table 1 to the sample vectors described
in Equation 1. As shown in Table 3, the accu-
894
Factorization Feature set Weighting K Measure Accuracy (%) F1
SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33
NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14
WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported
SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19
NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48
Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo
and Diab (2012).
Acc. F1
Most common class 66.5 79.9
(Wan et al, 2006) 75.6 83.0
(Das and Smith, 2009) 73.9 82.3
(Das and Smith, 2009) with 18 features 76.1 82.7
(Bu et al, 2012) 76.3 not reported
(Socher et al, 2011) 76.8 83.6
(Madnani et al, 2012) 77.4 84.1
FEAT2, TF-KLD, SVM 79.76 85.87
FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96
Table 3: Supervised classification. Results from prior work are reprinted.
racy now improves to 80.41%, with an F1 score of
85.96%. When the latent representation is induced
from only the training data, the corresponding re-
sults are 79.94% on accuracy and 85.36% F1, again
better than the previous state-of-the-art. These re-
sults show that the information captured by the dis-
tributional representation can still be augmented by
more fine-grained traditional features.
6 Conclusion
We have presented three ways in which labeled
data can improve distributional measures of seman-
tic similarity at the sentence level. The main innova-
tion is TF-KLD, which discriminatively reweights
the distributional features before factorization, so
that discriminability impacts the induction of the la-
tent representation. We then transform the latent
representation into a sample vector for supervised
learning, obtaining results that strongly outperform
the prior state-of-the-art; adding fine-grained lexi-
cal features further increases performance. These
ideas may have applicability in other semantic sim-
ilarity tasks, and we are also eager to apply them to
new, large-scale automatically-induced paraphrase
corpora (Ganitkevitch et al, 2013).
Acknowledgments
We thank the reviewers for their helpful feedback,
and Weiwei Guo for quickly answering questions
about his implementation. This research was sup-
ported by a Google Faculty Research Award to the
second author.
References
Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012.
Learning Topic Models - Going beyond SVD. In
FOCS, pages 1?10.
Rahul Bhagat and Eduard Hovy. 2013. What Is a Para-
phrase? Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Fan Bu, Hang Li, and Xiaoyan Zhu. 2012. String Re-
writing kernel. In Proceedings of ACL, pages 449?
458. Association for Computational Linguistics.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
895
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing, pages
468?476, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised Construction of Large Paraphrase Corpora:
Exploiting Massively Parallel News Sources. In COL-
ING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Alexander Gammerman, Volodya Vovk, and Vladimir
Vapnik. 1998. Learning by transduction. In Proceed-
ings of the Fourteenth conference on Uncertainty in
artificial intelligence, pages 148?155. Morgan Kauf-
mann Publishers Inc.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL, pages 758?764.
Association for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012. Modeling Sentences
in the Latent Space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, pages 864?872, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of NAACL, pages 455?462. Association for Computa-
tional Linguistics.
Thomas Landauer, Peter W. Foltz, and Darrel Laham.
1998. Introduction to Latent Semantic Analysis. Dis-
cource Processes, 25:259?284.
Daniel D. Lee and H. Sebastian Seung. 2001. Al-
gorithms for Non-Negative Matrix Factorization. In
Advances in Neural Information Processing Systems
(NIPS).
Nitin Madnani, Joel R. Tetreault, and Martin Chodorow.
2012. Re-examining Machine Translation Metrics for
Paraphrase Identification. In HLT-NAACL, pages 182?
190. The Association for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In AAAI.
Kevin P. Murphy. 2012. Machine Learning: A Proba-
bilistic Perspective. The MIT Press.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of HLT-NAACL, pages 181?
189. Association for Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011.
Dynamic Pooling And Unfolding Recursive Autoen-
coders For Paraphrase Detection. In Advances in Neu-
ral Information Processing Systems (NIPS).
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word Representation: A Simple and General Method
for Semi-Supervised Learning. In ACL, pages 384?
394.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Seman-
tics. JAIR, 37:141?188.
Ssephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using Dependency-based Features to Take the
?Para-farce? out of Paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars. In
Proceedings of the ACL Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, pages
25?30. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
Clustering based on Non-Negative Matrix Factoriza-
tion. In SIGIR, pages 267?273.
896
Proceedings of NAACL-HLT 2013, pages 359?369,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
What to do about bad language on the internet
Jacob Eisenstein
jacobe@gatech.edu
School of Interactive Computing
Georgia Institute of Technology
Abstract
The rise of social media has brought compu-
tational linguistics in ever-closer contact with
bad language: text that defies our expecta-
tions about vocabulary, spelling, and syntax.
This paper surveys the landscape of bad lan-
guage, and offers a critical review of the NLP
community?s response, which has largely fol-
lowed two paths: normalization and domain
adaptation. Each approach is evaluated in the
context of theoretical and empirical work on
computer-mediated communication. In addi-
tion, the paper presents a quantitative analy-
sis of the lexical diversity of social media text,
and its relationship to other corpora.
1 Introduction
As social media becomes an increasingly important
application domain for natural language processing,
we encounter language that is substantially different
from many benchmark corpora. The following ex-
amples are all from the social media service Twitter:
? Work on farm Fri. Burning piles of brush
WindyFire got out of control. Thank God for
good naber He help get undr control Pants-
BurnLegWound. (Senator Charles Grassley)
? Boom! Ya ur website suxx bro
(Sarah Silverman)
? ...dats why pluto is pluto it can neva b a star
(Shaquille O?Neil)
? michelle obama great. job. and. whit all my.
respect she. look. great. congrats. to. her.
(Ozzie Guillen)
These examples are selected from celebrities (for
privacy reasons), but they contain linguistic chal-
lenges that are endemic to the medium, including
non-standard punctuation, capitalization, spelling,
vocabulary, and syntax. The consequences for lan-
guage technology are dire: a series of papers has
detailed how state-of-the-art natural language pro-
cessing (NLP) systems perform significantly worse
on social media text. In part-of-speech tagging, the
accuracy of the Stanford tagger (Toutanova et al,
2003) falls from 97% on Wall Street Journal text to
85% accuracy on Twitter (Gimpel et al, 2011). In
named entity recognition, the CoNLL-trained Stan-
ford recognizer achieves 44% F-measure (Ritter et
al., 2011), down from 86% on the CoNLL test
set (Finkel et al, 2005). In parsing, Foster et al
(2011) report double-digit decreases in accuracy for
four different state-of-the-art parsers when applied
to social media text.
The application of language technology to so-
cial media is potentially transformative, leveraging
the knowledge and perspectives of millions of peo-
ple. But to deliver on this potential, the problems
at the core of the NLP pipeline must be addressed.
A growing thread of research takes up this chal-
lenge, including a shared task and workshop on
?parsing the web,? with new corpora which appear
to sit somewhere between the Wall Street Journal
and Twitter on the spectrum of bad language (Petrov
and McDonald, 2012). But perhaps surprisingly,
very little of this research has considered why social
media language is so different. This review paper
attempts to shed some light on this question, sur-
veying a strong tradition of empirical and theoreti-
359
cal research on computer-mediated communication
(CMC). I argue that the two main computational ap-
proaches to dealing with bad language ? normaliza-
tion and domain adaptation ? are based on theories
of social media language that are not descriptively
accurate. I have worked and continue to work in
both of these areas, so I make this argument not as
a criticism of others, but in a spirit of self-reflection.
It is hoped that a greater engagement with sociolin-
guistic and CMC research will lead to new, nuanced
approaches to the challenge of bad language.
Why so much Twitter? Most of the examples
in this paper will focus on Twitter, a microblog-
ging service. Munro and Manning (2012) argue
that Twitter has unfairly dominated recent research,
at the expense of email and SMS text messages,
which they found to be both linguistically distinct
from Twitter and significantly more prevalent (in
2010). This matches earlier research arguing that
email contained relatively little ?neography,? com-
pared with text messages and chat (Anis, 2007).
A crucial advantage for Twitter is that it is public
by default, while SMS and email are private. This
makes Twitter data less problematic from a privacy
standpoint,1 far easier to obtain, and more amenable
to target applications such as large-scale mining of
events (Sakaki et al, 2010; Benson et al, 2011) and
opinions (Sauper et al, 2011). Similar argument
could be made on behalf of other public social me-
dia, such as blog comments (Ali-Hasan and Adamic,
2007), forums, and chatrooms (Paolillo, 2001). The
main advantage of Twitter over these media is con-
venience in gathering large datasets through a sin-
gle streaming interface. More comparative evalu-
ation is needed to determine linguistic similarities
and differences between Twitter and these other me-
dia; Section 4 presents an evaluation of the lexical
similarity between Twitter and political blogs.
2 A tour of bad language
While many NLP researchers and engineers have
wrestled with the difficulties imposed by bad lan-
guage, there has been relatively little considera-
tion of why language in social media is so differ-
ent from our other corpora. A survey of laypeo-
1boyd and Crawford (2012) note that ?public by default?
data still raises important ethical considerations.
ple found that more than half of the respondents
agreed with the following partial explanations for
non-standard spelling on the internet: ?people are
unsure of the correct spellings,? ?it?s faster,? ?it?s be-
come the norm,? and ?people want to represent their
own dialects and/or accents? (Jones, 2010). Let us
now consider the evidence for these and other poten-
tial explanations.
2.1 Illiteracy
Some commentators have fixated on the proposal
that the authors of non-standard language in social
media are simply unaware or incapable of using
more standard language (Thurlow, 2006). But em-
pirical research suggests that many users of bad lan-
guage are capable of using more traditional forms.
Drouin and Davis (2009) find no significant differ-
ences in the literacy scores of individuals who do
or do not use non-standard vocabulary in text mes-
sages. Tagliamonte and Denis (2008) review traces
of instant messaging conversations among students,
arguing that they ?pick and choose ... from the en-
tire stylistic repertoire of the language? in a way that
would be impossible without skilled command of
both formal and informal registers. While news text
is usually more carefully composed and edited than
much of the language in social media, there is little
evidence that bad language results from an inability
to speak anything else.
2.2 Length limits
In the case of Twitter, the limit of 140 characters for
each message is frequently cited as an explanation
for bad language (Finin et al, 2010). Does Twitter?s
character limit cause users to prefer shorter words,
such as u instead of you? If so, one might expect
shortening to be used most frequently in messages
that are near the 140-character limit. Using a dataset
of one million English-language tweets (Bamman et
al., 2012), I have computed the average length of
messages containing both standard words and their
non-standard alternatives, focusing on the top five
non-standard shortenings identified by the automatic
method of Gouws et al (2011a). The shortening ur
can substitute for both your and you?re. While wit
and bout are also spellings for standard words, man-
ual examination of one hundred randomly selected
examples for each surface form revealed only one
360
standard length alternative length
your 85.1? 0.4
ur 81.9? 0.6
you?re 90.0? 0.1
with 87.9? 0.3 wit 78.8? 0.7
going 82.7? 0.5 goin 72.2? 1.0
know 86.1? 0.4 kno 78.4? 1.0
about 88.9? 0.4 bout 74.5? 0.7
Table 1: Average length of messages containing standard
forms and their shortenings
case in which the standard meaning was intended for
wit, and none for bout.
The average message lengths are shown in Ta-
ble 1. In all five cases, the non-standard form tends
to be used in shorter messages ? not in long mes-
sages near the 140 character limit. Moreover, this
difference is substantially greater than the saving of
one or two characters offered by shortened form.
This is not consistent with the explanation that Twit-
ter?s character limit is the primary factor driving the
use of shortened forms. It is still possible that Twit-
ter?s length limitations might indirectly cause word
shortenings: for example, by legitimizing shortened
forms or causing authors to develop a habit of pre-
ferring them. But factors other than the length limit
must be recruited to explain why such conventions
or habits apply only to some messages and not oth-
ers.
2.3 Text input affordances
Text input affordances ? whether standard key-
boards or predictive entry on mobile devices ? play
a role in computer-mediated communication that is
perhaps under-appreciated. Gouws et al (2011b) in-
vestigate orthographic variation on Twitter, and find
differences across devices: for example, that mes-
sages from iPhones include more contractions than
messages from Blackberries, and that tweets sent
from the web browser are more likely to drop vow-
els. While each affordance facilitates some writ-
ing styles and inhibits others, the affordances them-
selves are unevenly distributed across users. For ex-
ample, older people may prefer standard keyboards,
and wealthier people may be more likely to own
iPhones. Affordances are a moving target: new de-
vices and software are constantly becoming avail-
able, the software itself may adapt to the user?s in-
put, and the user may adapt to the software and de-
vice.
2.4 Pragmatics
Emoticons are frequently thought of as introduc-
ing an expressive, non-verbal component into writ-
ten language, mirroring the role played by facial ex-
pressions in speech (Walther and D?Addario, 2001),
but they can also be seen as playing a pragmatic
function: marking an utterance as facetious, or
demonstrating a non-confrontational, less invested
stance (Dresner and Herring, 2010). In many cases,
phrasal abbreviations like lol (laugh out loud),
lmao (laughing my ass off ), smh (shake my head),
and ikr (i know, right?) play a similar role: yea she
dnt like me lol; lmao I?m playin son. A key differ-
ence from emoticons is that abbreviations can act
as constituents, as in smh at your ignorance. An-
other form of non-standard language is expressive
lengthening (e.g., coooolllllll), found by Brody and
Diakopoulos (2011) to indicate subjectivity and sen-
timent. In running dialogues ? such as in online
multiplayer games ? the symbols * and ? can play
an explicit pragmatic function (Collister, 2011; Col-
lister, 2012).
2.5 Social variables
A series of papers has documented the interac-
tions between social media text and social vari-
ables such as age (Burger and Henderson, 2006;
Argamon et al, 2007; Rosenthal and McKeown,
2011), gender (Burger et al, 2011; Rao et al, 2010),
race (Eisenstein et al, 2011), and location (Eisen-
stein et al, 2010; Wing and Baldridge, 2011). From
this literature, it is clear that many of the features
that characterize bad language have strong associa-
tions with specific social variables. In some cases,
these associations mirror linguistic variables known
from speech ? such as geographically-associated
lexical items like hella, or transcriptions of phono-
logical variables like ?g-dropping? (Eisenstein et al,
2010). But in other cases, apparently new lexical
items, such as the abbreviations ctfu, lls, and af,
acquire surprisingly strong associations with geo-
graphical areas and demographic groups (Eisenstein
et al, 2011).
A robust finding from the sociolinguistics litera-
ture is that non-standard forms that mark social vari-
361
ables, such as regional dialects, are often inhibited in
formal registers (Labov, 1972). For example, while
the Pittsburgh spoken dialect sometimes features the
address term yinz (Johnstone et al, 2006), one would
not expect to find many examples in financial re-
ports. Other investigators have found that much of
the content in Twitter concerns social events and self
presentation (Ramage et al, 2010), which may en-
courage the use of less formal registers in which
socially-marketed language is uninhibited.
The use of non-standard language is often seen
as a form of identity work, signaling authentic-
ity, solidarity, or resistance to norms imposed from
above (Bucholtz and Hall, 2005). In spoken lan-
guage, many of the linguistic variables that perform
identity work are phonological ? for example, Eck-
ert (2000) showed how the northern cities vowel
shift was used by a subset of suburban teenagers to
index affiliation with Detroit. The emergence of new
linguistic variables in social media suggests that this
identity work is as necessary in social media as it
is in spoken language. Some of these new variables
are transcriptions of existing spoken language vari-
ables: like finna, which transcribes fixing to. Oth-
ers ? abbreviations like ctfu and emoticons ? seem
to be linguistic inventions created to meet the needs
of social communication in a new medium. In an
early study of variation in social media, Paolillo
(1999) notes that code-switching between English
and Hindi also performs this type of identity work.
Finally, it is an uncomfortable fact that the text
in many of our most frequently-used corpora was
written and edited predominantly by working-age
white men. The Penn Treebank is composed of
professionally-written news text from 1989, when
minorities comprised 7.5% of the print journalism
workforce; the proportion of women in the journal-
ism workforce was first recorded in 1999, when it
was 37% (American Society of Newspaper Editors,
1999). In contrast, Twitter users in the USA con-
tain an equal proportion of men and women, and
a higher proportion of young adults and minorities
than in the population as a whole (Smith and Brewer,
2012). Such demographic differences are very likely
to lead to differences in language (Green, 2002;
Labov, 2001; Eckert and McConnell-Ginet, 2003).
Overall, the reasons for language diversity in so-
cial media are manifold, though some of the most
frequently cited explanations (illiteracy and length
restrictions) do not hold up to scrutiny. The in-
creasing prevalence of emoticons, phrasal abbrevi-
ations (lol, ctfu), and expressive lengthening may
reflect the increasing use of written language for
ephemeral social interaction, with the concomitant
need for multiple channels through which to express
multiple types of meaning. The fact many such neol-
ogisms are closely circumscribed in geography and
demographics may reflect diffusion through social
networks that are assortative on exactly these dimen-
sions (Backstrom et al, 2010; Thelwall, 2009). But
an additional consideration is that non-standard lan-
guage is deliberately deployed in the performance of
identity work and stancetaking. This seems a partic-
ularly salient explanation for the use of lexical vari-
ables that originate in spoken language (jawn, hella),
and for the orthographic transcription of phonolog-
ical variation (Eisenstein, 2013). Determining the
role and relative importance of social network diffu-
sion and identity work as factors in the diversifica-
tion of social media language is an exciting direction
for future research.
3 What can we do about it?
Having surveyed the landscape of bad language and
its possible causes, let us now turn to the responses
offered by the language technology research com-
munity.
3.1 Normalization
One approach to dealing with bad language is to
turn it good: ?normalizing? social media or SMS
messages to better conform to the sort of language
that our technology expects. Approaches to normal-
ization include the noisy-channel model (Cook and
Stevenson, 2009), string and distributional similar-
ity (Han and Baldwin, 2011; Han et al, 2012), se-
quence labeling (Choudhury et al, 2007; Liu et al,
2011a), and machine translation (Aw et al, 2006).
As this task has been the focus of substantial atten-
tion in recent years, labeled datasets have become
available and accuracies have climbed.
That said, it is surprisingly difficult to find a
precise definition of the normalization task. Writ-
ing before social media was a significant focus for
NLP, Sproat et al (2001) proposed to replace non-
362
standard words with ?the contextually appropriate
word or sequence of words.? In some cases, this
seems clear enough: we can rewrite dats why pluto
is pluto with that?s why... But it is not difficult to find
cases that are less clear, putting would-be normaliz-
ers in a difficult position. The labeled dataset of Han
and Baldwin (2011) addresses a more tractable sub-
set of the normalization problem, annotating only
token-to-token normalizations. Thus, imma ? a
transcription of I?m gonna, which in turn transcribes
I?m going to ? is not normalized in this dataset. Ab-
breviations like LOL and WTF are also not normal-
ized, even when they are used to abbreviate syntac-
tic constituents, as in wtf is the matter with you? Nor
are words like hella and jawn normalized, since they
have no obvious one-word transcription in standard
English. These decisions no doubt help to solidify
the reliability of the annotations, but they provide an
overly optimistic impression of the ability of string
edit distance and related similarity-based techniques
to normalize bad language. The resulting gold stan-
dard annotations seem little more amenable to au-
tomated parsing and information extraction than the
original text.
But if we critique normalization for not going
far enough, we must also ask whether it goes too
far. The logic of normalization presupposes that the
?norm? can be identified unambiguously, and that
there is a direct mapping from non-standard words
to the elements in this normal set. On closer exami-
nation, the norm reveals itself to be slippery. Whose
norm are we targeting? Should we normalize flvr to
flavor or flavour? Where does the normal end and
the abnormal begin? For example, Han and Baldwin
normalize ain to ain?t, but not all the way to isn?t.
While ain?t is certainly well-known to speakers of
Standard American English, it does not appear in the
Penn Treebank and probably could not be used in the
Wall Street Journal, except in quotation.
Normalization is often impossible without chang-
ing the meaning of the text. Should we normalize the
final word of ya ur website suxx bro to brother? At
the very least, this adds semantic ambiguity where
there was none before (is she talking to her biolog-
ical brother? or possibly to a monk?). Language
variation does not arise from passing standard text
through a noisy channel; it often serves a pragmatic
and/or stancetaking (Du Bois, 2007) function. Elim-
inating variation would strip those additional lay-
ers of meaning from whatever propositional content
might survive the normalization process. Sarah Sil-
verman?s ya ur website suxx bro can only be under-
stood as a critique from a caricatured persona ? the
type of person who ends sentences with bro. Sim-
ilarly, we can assume that Shaquille O?Neil is ca-
pable of writing that?s why Pluto is Pluto, but that
to do so would convey an undesirably didactic and
authoritative stance towards the audience and topic.
This is not to deny that there is great poten-
tial value in research aimed at understanding or-
thographic variation through a combination of lo-
cal context, string similarity, and related finite-state
machinery. Given the productivity of orthographic
substitutions in social media text, it is clear that lan-
guage technology must be made more robust. Nor-
malization may point the way towards such robust-
ness, even if we do not build an explicit normaliza-
tion component directly into the language process-
ing pipeline. Another potential benefit of this re-
search is to better understand the underlying ortho-
graphic processes that lead to the diversity of lan-
guage in social media, how these processes diffuse
over social networks, and how they impact compre-
hensibility for both the target and non-target audi-
ences.
3.2 Domain adaptation
Rather than adapting text to fit our tools, we may
instead adapt our tools to fit the text. A series of
papers has followed the mold of ?NLP for Twit-
ter,? including part-of-speech tagging (Gimpel et al,
2011; Owoputi et al, 2013), named entity recogni-
tion (Finin et al, 2010; Ritter et al, 2011; Liu et al,
2011b), parsing (Foster et al, 2011), dialogue mod-
eling (Ritter et al, 2010) and summarization (Sharifi
et al, 2010). These papers adapt various parts of the
natural language processing pipeline for social me-
dia text, and make use of a range of techniques:
? preprocessing to normalize expressive length-
ening, and eliminate or group all hashtags,
usernames, and URLs (Gimpel et al, 2011;
Foster et al, 2011)
? new labeled data, enabling the application of
semi-supervised learning (Finin et al, 2010;
Gimpel et al, 2011; Ritter et al, 2011)
363
? new annotation schemes specifically cus-
tomized for social media text (Gimpel et al,
2011)
? self-training on unlabeled social media
text (Foster et al, 2011)
? distributional features to address the sparsity
of bag-of-words features (Gimpel et al, 2011;
Owoputi et al, 2013; Ritter et al, 2011)
? joint normalization, incorporated directly into
downstream application (Liu et al, 2012)
? distant supervision, using named entity on-
tologies and topic models (Ritter et al, 2011)
Only a few of these techniques (normalization and
new annotation systems) are specific to social me-
dia; the rest can found in other domain adaptation
settings. Is domain adaptation appropriate for social
media? Darling et al (2012) argue that social me-
dia is not a coherent domain at all, and that a POS
tagger for Twitter will not necessarily generalize to
other social media. One can go further: Twitter it-
self is not a unified genre, it is composed of many
different styles and registers, with widely varying
expectations for the degree of standardness and di-
mensions of variation (Androutsopoulos, 2011). I
am the co-author on a paper entitled ?Part-of-speech
tagging for Twitter,? but if we take this title literally,
it is impossible on a trivial level: Twitter contains
text in dozens or hundreds of languages, including
many for which no POS tagger exists. Even within
a single language ? setting aside issues of code-
switching (Paolillo, 1996) ? Twitter and other so-
cial media can contain registers ranging from hash-
tag wordplay (Naaman et al, 2011) to the official
pronouncements of the British Monarchy. And even
if all good language is alike, bad language can be
bad in many different ways ? as Androutsopoulos
(2011) notes when contrasting the types of variation
encountered when ?visiting a gamer forum? versus
?joining the Twitter profile of a rap star.?
4 The lexical coherence of social media
The internal coherence of social media ? and its
relationship to other types of text ? can be quan-
tified in terms of the similarity of distributions over
bigrams. While there are many techniques for com-
paring word distributions, I apply the relatively sim-
ple method of counting out-of-vocabulary (OOV) bi-
grams. The relationship between OOV rate and do-
main adaptation has been explored by McClosky et
al. (2010), who use it as a feature to predict how well
a parser will perform when applied across domains.2
Specifically, the datasets A and B are compared
by counting the number of bigram tokens in A that
are unseen in B. The following corpora are com-
pared:
? Twitter-month: randomly selected tweets
from each month between January 2010 to Oc-
tober 2012 (Eisenstein et al, 2012).
? Twitter-hour: randomly selected tweets from
each hour of the day, randomly sampled during
the period from January 2010 to October 2012.
? Twitter-#: tweets in which the first token is a
hashtag. The hashtag itself is not included in
the bigram counts; see below for more details
on which bigrams are included.
? Twitter-@: tweets in which the first token is a
username. The username itself is not included
in the bigram counts.
? Penn Treebank: sections 2-21
? Infinite Jest: the text of the 1996 novel by
David Foster Wallace (Wallace, 2012). Con-
sists of only 482,558 tokens.
? Blog articles: A randomly-sampled subset
of the American political blog posts gathered
by Yano et al (2009).
? Blog comments: A randomly-selected subset
of comments associated with the blog posts de-
scribed above.
In all corpora, only fully alphabetic tokens are
counted; thus, all hashtags and usernames are dis-
carded. The Twitter text is tokenized using Tweet-
2A very recent study compares Twitter with other corpora,
using a number of alternative metrics, such as the use of high
and low frequency words, pronouns, and intensifiers (Hu et al,
2013). This is complementary to the present study, which fo-
cuses on the degree of difference in the lexical distributions of
corpora gathered from various media.
364
5 10 15 20
month gap
1.000
1.005
1.010
1.015
1.020
1.025
1.030
1.035
rel
ati
ve
 pr
op
ort
ion
 of
 OO
V b
igr
am
s
with NEs
without NEs
Figure 1: Lexical mismatch increases over time, as social
media language evolves.
motif;3 the Penn Treebank data uses the gold stan-
dard tokenization; Infinite Jest and the blog data are
tokenized using NLTK (Bird et al, 2009). All to-
kens are downcased, and sequences of three or more
consecutive identical characters are reduced to three
characters (e.g., coooool? coool). All Twitter cor-
pora are subject to the following filters: messages
must be from the United States and should be written
in English,4 they may not include hyperlinks (elim-
inating most marketing messages), they may not be
retweets, and the author must not have more than
1,000 followers or follow more than 1,000 people.
These criteria serve to eliminate text from celebri-
ties, businesses, or automated bots.
Twitter over time Figure 1 shows how the pro-
portion of out-of-vocabulary bigrams increases over
time. It is possible that the core features of language
are constant but the set of named entities that are
mentioned changes over time. To control for this,
the CMU Twitter Part-of-Speech tagger (Owoputi et
al., 2013) was used to identify named entity men-
tions, and they were replaced with a special token.
3https://github.com/brendano/tweetmotif
4Approximate language detection was performed as follows.
We first identify the 1000 most common words, then sort all au-
thors by the proportion of these types that they used, and elim-
inate the bottom 10%. This filtering mechanism eliminates in-
dividuals who never write in English, but a small amount of
foreign language still enters the dataset via code-switching au-
thors. The effect of more advanced language detection meth-
ods (Bergsma et al, 2012) on these results may be considered
in future work.
2 4 6 8 10 12
hour gap
1.000
1.005
1.010
1.015
1.020
1.025
1.030
1.035
1.040
rel
ati
ve
 pr
op
ort
ion
 of
 OO
V b
igr
am
s
with NEs
without NEs
Figure 2: Different times of day have unique lexical sig-
natures, reflecting differing topics and authors.
The OOV rate is standardized with respect to a one-
month time gap, where it is 24.4% when named en-
tities are included, and 21.3% when they are not.
These rates reach maxima at 25.2% and 22.0% re-
spectively, with dips at 12 and 24 months indicat-
ing cyclic yearly effects. While the proportion of
OOV tokens is smaller when named entities are not
included, the rate of growth is similar in each case.
The steadily increasingly rate of OOV bigrams sug-
gests that we cannot annotate our way out of the bad
language problem. An NLP system trained from
data gathered in January 2010 will be increasingly
outdated as time passes and social media language
continues to evolve.
One need not wait months to see language change
on Twitter: marked changes can be observed over
the course of a single day (Golder and Macy, 2011).
A quantitative comparison is shown in Figure 2.
Here the OOV rate is standardized with respect to
a one-hour gap, where it is 24.2% when named en-
tities are included, and 21.1% when they are not.
These rates rise monotonically as the time gap in-
creases, peaking at 25.1% and 21.9% respectively.
Such diurnal changes may reflect the diverse lan-
guage of the different types of authors who post
throughout the day.
Types of usage The Twitter-# and Twitter-@ cor-
pora are designed to capture the diversity of ways
in which social media is used to communicate.
Twitter-# contains tweets that begin with hashtags,
and are thus more likely to be part of running jokes
365
or trending topics (Naaman et al, 2011). Twitter-
@ contains tweets that begin with usernames ? an
addressing mechanism that is used to maintain dia-
logue threads on the site. These datasets are com-
pared with a set of randomly selected tweets from
June 2011, and with several other corpora: Penn
Treebank, the novel Infinite Jest, and text and com-
ments from political blogs. There was no attempt to
remove named entities from any of these corpora, as
such a comparison would merely reflect the different
accuracy levels of NER in each corpus.
The results are shown in Table 2. A few observa-
tions stand out. First, the Penn Treebank is the clear
outlier: a PTB dictionary has by far the most OOV
tokens for all three Twitter domains and Infinite Jest,
although it is a better match for the blog corpora
than Infinite Jest is. Second, the social media are
fairly internally coherent: the Twitter datasets bet-
ter match each other than any other corpus, with a
maximum OOV rate of 33.4 for Twitter-# against
Twitter-@, though this is significantly higher than
the OOV rate of 27.8 between two separate generic
Twitter samples drawn from the same month. Fi-
nally, the OOV rate increase between Twitter and
blogs ? also social media ? is substantial. Con-
trary to expectations, the Blog-body corpus was no
closer to the PTB standard than Blog-comment.
These results suggest that the Penn Treebank cor-
pus is so distant from social media that there are in-
deed substantial gains to be reaped by adapting from
news text towards generic Twitter or Blog target do-
mains. The internal differences within these social
media ? at least as measured by the distinctions
drawn in Table 2 ? are much smaller than the dif-
ferences between these corpora and the PTB stan-
dard. However, in the long run, the effectiveness
of this approach will be limited, as it is clear from
Figure 1 that social media is a moving target. Any
static system that we build today, whether by man-
ual annotation or automated adaptation, will see its
performance decay over time.
5 What to do next
Language is shaped by a constant negotiation be-
tween processes that encourage change and linguis-
tic diversity, and countervailing processes that en-
force existing norms. The decision of the NLP com-
munity to focus so much effort on news text is em-
inently justified on practical grounds, but has unin-
tended consequences not just for technology but for
language itself. By developing software that works
best for standard linguistic forms, we throw the
weight of language technology behind those forms,
and against variants that are preferred by disempow-
ered groups. By adopting a model of ?normaliza-
tion,? we declare one version of language to be the
norm, and all others to be outside that norm. By
adopting a model of ?domain adaptation,? we con-
fuse a medium with a coherent domain. Adapting
language technology towards the median Tweet can
improve accuracy on average, but it is certain to
leave many forms of language out.
Much of the current research on the relationship
between social media language and metadata has the
goal of using language to predict the metadata ?
revealing who is a woman or a man, who is from
Oklahoma or New Jersey, and so on. This perspec-
tive on social variables and personal identity ignores
the local categories that are often more linguisti-
cally salient (Eckert, 2008); worse, it strips individ-
uals of any agency in using language as a resource
to create and shape their identity (Coupland, 2007),
and conceals the role that language plays in creating
and perpetuating categories like gender (Bucholtz
and Hall, 2005). An alternative possibility is to re-
verse the relationship between language and meta-
data, using metadata to achieve a more flexible and
heterogeneous domain adaptation that is sensitive to
the social factors that shape variation. Such a re-
versal would help language technology to move be-
yond false dichotomies between normal and abnor-
mal text, source and target domains, and good and
bad language.
Acknowledgments
This paper benefitted from discussions with David
Bamman, Natalia Cecire, Micha Elsner, Sharon
Goldwater, Scott Kiesling, Brendan O?Connor,
Tyler Schnoebelen, and Yi Yang. Many thanks to
Brendan O?Connor and David Bamman for provid-
ing Twitter datasets, Tae Yano for the blog com-
ment dataset, and Byron Wallace for the Infinite Jest
dataset. Thanks also to the anonymous reviewers for
their helpful feedback.
366
Tw-June Tw-@ Tw-# Blog-body Blog-comment Infinite-Jest PTB
Tw-June 28.7 29.3 47.1 48.6 54.0 63.9
Tw-@ 25.9 29.7 47.8 49.9 56.3 66.4
Tw-# 29.8 33.4 49.6 51.0 54.7 66.2
Blog-body 41.9 44.1 43.8 27.2 49.1 48.0
Blog-comment 47.4 49.6 49.2 30.2 53.0 48.4
Infinite-Jest 49.4 51.1 49.9 48.3 47.4 55.5
PTB 72.2 73.1 72.7 64.5 61.9 71.9
Table 2: Percent OOV bigram tokens across corpora. Rows are the dataset providing the tokens, columns are the
dataset providing the dictionary.
References
Noor Ali-Hasan and Lada Adamic. 2007. Expressing so-
cial relationships on the blog through links and com-
ments. In Proceedings of ICWSM.
American Society of Newspaper Editors. 1999. 1999
Newsroom Census: Minority Employment Inches up in
Daily Newspapers. American Society of Newspaper
Editors, Reston, VA.
Jannis Androutsopoulos. 2011. Language change and
digital media: a review of conceptions and evidence.
In Nikolas Coupland and Tore Kristiansen, editors,
Standard Languages and Language Standards in a
Changing Europe. Novus, Oslo.
Jacques Anis. 2007. Neography: Unconventional
spelling in French SMS text messages. In Brenda
Danet and Susan C. Herring, editors, The multilingual
internet: Language, culture, and communication on-
line, pages 87 ? 115. Oxford University Press.
Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
age, gender, and the varieties of self-expression. First
Monday, 12(9).
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of ACL, pages 33?40.
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical prediction
with social and spatial proximity. In Proceedings of
WWW, pages 61?70.
David Bamman, Jacob Eisenstein, and Tyler Schnoebe-
len. 2012. Gender in twitter: Styles, stances, and
social networks. Technical Report 1210.4567, arXiv,
October.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74, June.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python. O?Reilly Me-
dia, Incorporated.
danah boyd and Kate Crawford. 2012. Critical questions
for big data. Information, Communication & Society,
15(5):662?679, May.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP.
Mary Bucholtz and Kira Hall. 2005. Identity and inter-
action: A sociocultural linguistic approach. Discourse
studies, 7(4-5):585?614.
John D. Burger and John C. Henderson. 2006. An explo-
ration of observable features related to blogger age. In
AAAI Spring Symposium: Computational Approaches
to Analyzing Weblogs.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on twit-
ter. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157?174.
Lauren B. Collister. 2011. *-repair in online discourse.
Journal of Pragmatics, 43(3):918?921, February.
Lauren B. Collister. 2012. The discourse deictics ? and
<-- in a world of warcraft community. Discourse,
Context & Media, 1(1):9?19, March.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text message normalization. In Pro-
ceedings of the NAACL-HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78.
Nikolas Coupland. 2007. Style (Key Topics in Sociolin-
guistics). Cambridge University Press, July.
367
William M. Darling, Michael J. Paul, and Fei Song.
2012. Unsupervised part-of-speech tagging in
noisy and esoteric domains with a syntactic-semantic
bayesian hmm. In Proceedings of EACL Workshop on
Semantic Analysis in Social Media.
Eli Dresner and Susan C. Herring. 2010. Functions of
the non-verbal in cmc: Emoticons and illocutionary
force. Communication Theory, 20(3):249?268.
Michelle Drouin and Claire Davis. 2009. R u txting? is
the use of text speak hurting your literacy? Journal of
Literacy Research, 41(1):46?67.
John W. Du Bois. 2007. The stance triangle. In Robert
Engelbretson, editor, Stancetaking in discourse, pages
139?182. John Benjamins Publishing Company, Ams-
terdam/Philadelphia.
Penelope Eckert and Sally McConnell-Ginet. 2003. Lan-
guage and Gender. Cambridge University Press, New
York.
Penelope Eckert. 2000. Linguistic variation as social
practice. Blackwell.
Penelope Eckert. 2008. Variation and the indexical field.
Journal of Sociolinguistics, 12(4):453?476.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of ACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words. Technical Report 1210.5268,
arXiv.
Jacob Eisenstein. 2013. Phonological factors in social
media writing. In Proceedings of the NAACL Work-
shop on Language Analysis in Social Media.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proceedings of ACL, pages 363?370.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From news to comment:
Resources and benchmarks for parsing the language
of web 2.0. In Proceedings of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In Pro-
ceedings of ACL.
Scott A. Golder and Michael W. Macy. 2011. Di-
urnal and seasonal mood vary with work, sleep,
and daylength across diverse cultures. Science,
333(6051):1878?1881, September.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011a.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82?90, July.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011b. Contextual bearing on linguistic
variation in social media. In Proceedings of the ACL
Workshop on Language in Social Media.
Lisa Green. 2002. African American English. Cam-
bridge University Press.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a# twitter. In
Proceedings of ACL, volume 1.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Auto-
matically constructing a normalisation dictionary for
microblogs. In Proceedings of EMNLP.
Yuheng Hu, Kartik Talamadupula, and Subbarao Kamb-
hampati. 2013. Dude, srsly?: The surprisingly for-
mal nature of twitter?s language. In Proceedings of
ICWSM.
Barbara Johnstone, Jennifer Andrus, and Andrew E
Danielson. 2006. Mobility, indexicality, and the en-
registerment of pittsburghese. Journal of English Lin-
guistics, 34(2):77?104.
Lucy Jones. 2010. The changing face of spelling on the
internet. Technical report, The English Spelling Soci-
ety.
William Labov. 1972. Sociolinguistic patterns.
Philadelphia: University of Pennsylvania Press.
William Labov. 2001. Principles of linguistic change.
Vol.2 : Social factors. Blackwell Publishers, Oxford.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of ACL, pages 71?76.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of ACL.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang
Fu, and Furu Wei. 2012. Joint inference of named en-
tity recognition and normalization for tweets. In Pro-
ceedings of ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Proceedings of NAACL, pages 28?36, June.
368
Robert Munro and Christopher D. Manning. 2012.
Short message communications: users, topics, and in-
language processing. In Proceedings of the 2nd ACM
Symposium on Computing for Development.
Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twit-
ter. Journal of the American Society for Information
Science and Technology, 62(5):902?918.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL.
John C. Paolillo. 1996. Language choice on
soc.culture.punjab. Electronic Journal of Communica-
tion/La Revue Electronique de Communication, 6(3).
John C. Paolillo. 1999. The virtual speech community:
Social network and language variation on irc. Journal
of Computer-Mediated Communication, 4(4):0.
John C. Paolillo. 2001. Language variation on internet
relay chat: A social network approach. Journal of So-
ciolinguistics, 5(2):180?213.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Daniel Ramage, Sue Dumais, and D. Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of ICWSM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of Workshop on
Search and mining user-generated contents.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of NAACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an experi-
mental study. In Proceedings of EMNLP.
Sara Rosenthal and Kathleen McKeown. 2011. Age pre-
diction in blogs: A study of style, content, and online
behavior in pre- and Post-Social media generations. In
Proceedings of ACL.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of WWW,
pages 851?860.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2011. Content models with attitude. In Proceedings
of ACL.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proceedings of NAACL.
Aaron Smith and Joanna Brewer. 2012. Twitter use
2012. Technical report, Pew Research Center, May.
Richard Sproat, Alan W Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech & Language, 15(3):287?333.
Sali A. Tagliamonte and Derek Denis. 2008. Linguistic
ruin? lol! instant messaging and teen language. Amer-
ican Speech, 83(1):3?34, March.
Mike Thelwall. 2009. Homophily in MySpace. J. Am.
Soc. Inf. Sci., 60(2):219?231.
Crispin Thurlow. 2006. From statistical panic to moral
panic: The metadiscursive construction and popular
exaggeration of new media language in the print me-
dia. J. Computer-Mediated Communication, pages
667?701.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Byron Wallace. 2012. Multiple narrative disentangle-
ment: Unraveling infinite jest. In Proceedings of
NAACL.
Joseph B. Walther and Kyle P. D?Addario. 2001. The
impacts of emoticons on message interpretation in
computer-mediated communication. Social Science
Computer Review, 19(3):324?347.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of ACL.
Tae Yano, William W. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of NAACL.
369
Proceedings of NAACL-HLT 2013, pages 808?813,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Discourse Connectors for Latent Subjectivity in Sentiment Analysis
Rakshit Trivedi
College of Computing
Georgia Institute of Technology
Atlanta, GA 30308, USA
rtrivedi6@gatech.edu
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA 30308, USA
jacobe@gatech.edu
Abstract
Document-level sentiment analysis can bene-
fit from fine-grained subjectivity, so that sen-
timent polarity judgments are based on the
relevant parts of the document. While fine-
grained subjectivity annotations are rarely
available, encouraging results have been ob-
tained by modeling subjectivity as a latent
variable. However, latent variable models
fail to capitalize on our linguistic knowledge
about discourse structure. We present a new
method for injecting linguistic knowledge into
latent variable subjectivity modeling, using
discourse connectors. Connector-augmented
transition features allow the latent variable
model to learn the relevance of discourse con-
nectors for subjectivity transitions, without
subjectivity annotations. This yields signif-
icantly improved performance on document-
level sentiment analysis in English and Span-
ish. We also describe a simple heuristic for
automatically identifying connectors when no
predefined list is available.
1 Introduction
Document-level sentiment analysis can benefit from
consideration of discourse structure. Voll and
Taboada (2007) show that adjective-based sentiment
classification is improved by examining topicality
(whether each sentence is central to the overall
point); Yessenalina et al (2010b) show that bag-of-
ngrams sentiment classification is improved by ex-
amining subjectivity (whether a sentence expresses
a subjective opinion or objective fact). However, it
is unclear how best to obtain the appropriate dis-
course analyses. Voll and Taboada (2007) find that
domain-independent discourse parsing (Soricut and
Marcu, 2003) offers little improvement for senti-
ment analysis, so they resort to training a domain-
specific model for identifying topic sentences in re-
views. But this requires a labeled dataset of topic
sentences, imposing a substantial additional cost.
Yessenalina et al (2010b) treat sentence level
subjectivity as a latent variable, automatically in-
ducing the ?annotator rationale? (Zaidan et al, 2007;
Yessenalina et al, 2010a) for each training sen-
tence so as to focus sentiment learning on the sub-
jective parts of the document. This yields sig-
nificant improvements over bag-of-ngrams super-
vised sentiment classification. Latent variable sub-
jectivity analysis is attractive because it requires
neither subjectivity annotations nor an accurate
domain-independent discourse parser. But while the
?knowledge-free? nature of this approach is appeal-
ing, it is unsatisfying that it fails to exploit decades
of research on discourse structure.
In this paper, we explore a lightweight approach
to injecting linguistic knowledge into latent variable
models of subjectivity. The entry point is a set of
discourse connectors: words and phrases that signal
a shift or continuation in the discourse structure.
Such connectors have been the subject of exten-
sive study in the creation of the Penn Discourse
Treebank (PDTB: Prasad et al 2008). The role
of discourse connectors in sentiment analysis can
be clearly seen in examples, such as ?It?s hard to
imagine the studios hiring another manic German
maverick to helm a cop thriller. But that?s exactly
why the movie is unmissable.? (Huddleston, 2010)
808
We present a new approach to incorporate
discourse connectors in a latent subjectivity
model (Yessenalina et al, 2010b). This approach
requires no manually-specified information about
the meaning of the connectors, just the connectors
themselves. Our approach builds on proximity
features, which give the latent variable model a way
to prefer or disprefer subjectivity and sentiment
transitions, usually with the goal of encouraging
smoothness across the document. By taking
the cross-product of these features with a set of
discourse connectors, we obtain a new set of
connector-augmented transition features, which
capture the way discourse connectors are used to
indicate subjectivity and sentiment transitions. The
model is thus able to learn that subjectivity shifts
are likely to be accompanied by connectors such as
however or nonetheless.
We present experiments in both English and Span-
ish showing that this method of incorporating dis-
course connectors yields significant improvements
in document-level sentiment analysis. In case no
list of connectors is available, we describe a sim-
ple heuristic for automatically identifying candidate
connector words. The automatically identified con-
nectors do not perform as well as the expert-defined
lists, but they still outperform a baseline method
that ignores discourse connectors (in English). This
demonstrates both the robustness of the approach
and the value of linguistic knowledge.
2 Model
Given accurate labels of the subjectivity of each
sentence, a document-level sentiment analyzer
could safely ignore the sentences marked as non-
subjective.1 This would be beneficial for training as
well as prediction, because the learning algorithm
would not be confused by sentences that contradict
the document label. But in general we cannot rely on
having access to sentence-level subjectivity annota-
tions. Instead, we treat subjectivity as a latent vari-
able, and ask the learner to impute its value. Given
document-level sentiment annotations and an initial
1Discourse parsing often focuses on sub-sentence elemen-
tary discourse units (Mann and Thompson, 1988). For sim-
plicity, we consider units at the sentence level only, and leave
finer-grained analysis for future work.
model, the learner can mark as non-subjective those
sentences whose analysis disagrees with the docu-
ment label.
More formally, each document has a label y ?
{?1, 1}, a set of sentences x, and a set of per-
sentence subjectivity judgments h ? {0, 1}S , where
S is the number of sentences. We compute a set
of features on these variables, and score each in-
stance by a weighted combination of the features,
wTf(y,x,h). At prediction time, we seek a label
y which achieves a high score given the observed x
and the ideal h.
y? = argmax
y
(
max
h
wTf(y,x,h)
)
. (1)
At training time, we seek weights w which
achieve a high score given all training examples
{x, y}t,
w? = argmax
w
?
t
max
h
wTf(yt,xt,h). (2)
We can decompose the feature vector into two
parts: polarity features fpol(y,x,h), and subjectiv-
ity features fsubj(x,h). The basic feature set decom-
poses across sentences, though the polarity features
involve the document-level polarity. For sentence i,
we have fpol(y,xi, hi) = yhixi: the bag-of-words
features for sentence i are multiplied by the docu-
ment polarity y ? {?1, 1} and the sentence sub-
jectivity hi ? {0, 1}. The weights wpol capture the
sentiment polarity of each possible word. As for the
subjectivity features, we simply have fsubj(xi, hi) =
hixi. The weights wsubj capture the subjectivity of
each word, with large values indicate positive sub-
jectivity.
However, these features do not capture transi-
tions between the subjectivity and sentiment of ad-
jacent sentences. For this reason, Yessenalina et al
(2010b) introduce an additional set of proximity fea-
tures, fprox(hi, hi?1), which are parametrized by the
subjectivity of both the current sentence i and the
previous sentence i? 1. The effect of these features
will be to learn a preference for consistency in the
subjectivity of adjacent sentences.
By augmenting the transition features with the
text xi, we allow this preference for consistency
to be modulated by discourse connectors. We de-
sign the transition feature vector ftrans(xi, hi, hi?1)
809
to contain two elements for every discourse connec-
tor, one for hi = hi?1, and one for hi 6= hi?1. For
example, the feature ?moreover, CONTINUE? fires
when sentence i starts with moreover and hi?1 =
hi,i. We would expect to learn a positive weight for
this feature, and negative weights for features such
as ?moreover, SHIFT? and ?however, CONTINUE?.
3 Experiments
To evaluate the utility of adding discourse connec-
tors to latent subjectivity sentiment analysis, we
compare several models on movie review datasets
in English and Spanish.
3.1 Data
We use two movie review datasets:
? 50,000 English-language movie reviews (Maas
et al, 2011). Each review has a rating from
1-10; we marked ratings of 5 or greater as pos-
itive. Half the dataset is used for test and half
for training. Parameter tuning is performed by
cross-validation.
? 5,000 Spanish-language movie reviews (Cruz
et al, 2008). Each review has a rating from
1-5; we marked 3-5 as positive. We randomly
created a 60/20/20 split for training, validation,
and test.
3.2 Connectors
We first consider single-word discourse connectors:
in English, we use a list of all 57 one-word con-
nectors from the Penn Discourse Tree Bank (Prasad
et al, 2008); in Spanish, we selected 25 one-word
connectors from a Spanish language education web-
site.2 We also consider multi-word connectors. Us-
ing the same sources, this expands the English set to
93 connectors, and Spanish set to 80 connectors.
In case no list of discourse connectors is avail-
able, we propose a simple technique for automati-
cally identifying potential connectors. We use a ?2
test to select words which are especially likely to ini-
tiate sentences. The top K words (with the lowest p
values) were added as potential connectors, where
K is equal to the number of ?true? connectors pro-
vided by the gold-standard resource.
2russell.famaf.unc.edu.ar/?laura/
shallowdisc4summ/discmar/
Finally, we consider a model with connector-
augmented transition features for all words in the
vocabulary. Thus, there are four connector sets:
? true-unigram-connectors: unigram connec-
tors from the Penn Discourse Treebank and the
Spanish language education website
? true-multiword-connectors: unigram and
multiword connectors from these same re-
sources
? auto-unigram-connectors: automatically-
selected connectors using the ?2 test
? all-unigram-connectors: all words are poten-
tial connectors
3.3 Systems
The connector-augmented transition features are in-
corporated into a latent variable support vector ma-
chine (SVM). We also consider two baselines:
? no-connectors: the same latent variable SVM,
but without the connector features. This is
identical to the prior work of Yessenalina et al
(2010b).
? SVM: a standard SVM binary classifier
The latent variable models require an initial guess
for the subjectivity of each sentence. Yessenalina et
al. (2010b) compare several initializations and find
the best results using OpinionFinder (Wilson et al,
2005). For the Spanish data, we performed initial
subjectivity analysis by matching against a publicly-
available full-strength Spanish lexicon set (Rosas et
al., 2012).
3.4 Implementation details
Both our implementation and the baselines are
built on the latent structural SVM (Yu and
Joachims, 2009; http://www.cs.cornell.
edu/?cnyu/latentssvm/), which is in turn
built on the SVM-Light distribution (http://
svmlight.joachims.org/). The regulariza-
tion parameter C was chosen by cross-validation.
4 Results
Table 1 shows the sentiment analysis accuracy with
each system and feature set. The best overall re-
sults in both language are given by the models with
810
system English Spanish
true-multiword-connectors 91.25 79.80
true-unigram-connectors 91.36 77.50
auto-connectors 90.22 76.90
all-unigram-connectors 87.60 74.30
No-connectors 88.21 76.42
SVM 84.79 69.44
0.84 0.85 0.86 0.87 0.88 0.89 0.90 0.91 0.92sentiment analysis accuracy
SVM
no-connectors
all-unigram
auto-unigram
true-unigram
true-multiword
English
0.70 0.75 0.80sentiment analysis accuracy
SVM
no-connectors
all-unigram
auto-unigram
true-unigram
true-multiword
Spanish
Figure 1: Document-level sentiment analysis accuracy.
The 95% confidence intervals are estimated from the cu-
mulative density function of the binomial distribution.
connector-augmented transition features. In En-
glish, the multiword and unigram connectors per-
form equally well, and significantly outperform all
alternatives at p < .05. The connector-based fea-
tures reduce the error rate of the latent subjectivity
SVM by 25%. In Spanish, the picture is less clear
because the smaller test set yields larger confidence
intervals, so that only the comparison with the SVM
classifier is significant at p < .05. Nonetheless,
the connector-augmented transition features give the
best accuracy, with an especially large improvement
obtained by the multiword connectors.
Next, we investigated the quality of the
automatically-induced discourse connectors.
The ?2 heuristic for selecting candidate connectors
gave results that were significantly better than the
no-connector baseline in English, though the
Figure 2: Precision-Recall curve for top-K discovered
connectors when compared with PDTB connector set
difference in Spanish was minimal. However, when
every word is included as a potential connectors, the
performance suffers, dropping below the accuracy
of the no-connector baseline. This shows that the
improvement in accuracy offered by the connector
features is not simply due to the increased flexibility
of the model, but depends on identifying a small set
of likely discourse connectors.
For a qualitative evalatuation, we ranked all
English-language unigram connectors by their fea-
ture weights, and list the top ten for each subjectivity
transition:
? SHIFT: however; though; but; if; unlike; al-
though; while; overall; nevertheless; still
? CONTINUATION: as; there; now; even; in; af-
ter; once; almost; because; so
Overall these word lists cohere with our intu-
itions, particularly the words associated with SHIFT
transitions: however, but, and nevertheless. As one
of the reviewers noted, some of the words associ-
ated with CONTINUATION transitions are better seen
as discourse cues rather than connectors, such as
now. Other words seem to connect two subsequent
clauses, e.g., if Nicholas Cage had played every role,
the film might have reached its potential. Incorporat-
ing such connectors must be left for future work.
Finally, in learning weights for each connector
feature, our model can be seen as discovering dis-
course connectors. We compare the highly weighted
discovered connectors from the all-unigram and
auto-unigram settings with the one-word connec-
tors from the Penn Discourse Tree Bank. The results
811
of this comparison are shown in Figure 2, which
traces a precision-recall curve by taking the top K
connectors for various values of K. The auto-
unigram model is able to identify many true con-
nectors from the Penn Discourse Treebank, while
the all-unigram model achieves low precision. This
graph helps to explain the large performance gap
between the auto-unigram and all-unigram fea-
ture sets; the all-unigram set includes too many
weak features, and the learning algorithm is not able
to distinguish the true discourse connectors. The
Spanish discourse connectors identified by this ap-
proach were extremely poor, possibly because so
many more of the Spanish connectors include mul-
tiple words.
5 Related Work
Polanyi and Zaenen (2006) noted the importance of
accounting for valence shifters in sentiment analy-
sis, identifying relevant connectors at the sentence
and discourse levels. They propose a heuristic ap-
proach to use shifters to modify the contributions
of sentiment words. There have been several sub-
sequent efforts to model within-sentence valence
shifts, including compositional grammar (Moilanen
and Pulman, 2007), matrix-vector products across
the sentence (Yessenalina and Cardie, 2011), and
methods that reason about polarity shifters within
the parse tree (Socher et al, 2012; Sayeed et al,
2012). The value of discourse structure towards pre-
dicting opinion polarity has also demonstrated in the
context of multi-party dialogues (Somasundaran et
al., 2009). Our approach functions at the discourse
level within single-author documents, so it is com-
plementary to this prior work.
Voll and Taboada (2007) investigate various tech-
niques for focusing sentiment analysis on sentences
that are central to the main topic. They obtain
negative results with the general-purpose SPADE
discourse parser (Soricut and Marcu, 2003), but
find that training a decision tree classifier to iden-
tify topic-central sentences yields positive results.
Wiebe (1994) argues that in coherent narratives, ob-
jectivity and subjectivity are usually consistent be-
tween adjacent sentences, an insight exploited by
Pang and Lee (2004) in a supervised system for
subjectivity analysis. Later work employed struc-
tured graphical models to model the flow of sub-
jectivity and sentiment over the course of the doc-
ument (Mao and Lebanon, 2006; McDonald et al,
2007). All of these approaches depend on labeled
training examples of subjective and objective sen-
tences, but Yessenalina et al (2010b) show that sub-
jectivity can be modeled as a latent variable, using a
latent variable version of the structured support vec-
tor machine (Yu and Joachims, 2009).
Our work can be seen as a combination of the
machine learning approach of Yessenalina et al
(2010b) with the insight of Polanyi and Zaenen
(2006) that connectors play a key role in transitions
between subjectivity and sentiment. Eisenstein and
Barzilay (2008) incorporated discourse connectors
into an unsupervised model of topic segmentation,
but this work only considered the role of such mark-
ers to differentiate adjoining segments of text, and
not to identify their roles with respect to one an-
other. That work was also not capable of learning
from supervised annotations in a downstream task.
In contrast, our approach uses document-level senti-
ment annotations to learn about the role of discourse
connectors in sentence-level subjectivity.
6 Conclusion
Latent variable machine learning is a powerful
tool for inducing linguistic structure directly from
data. However, adding a small amount of linguistic
knowledge can substantially improve performance.
We have presented a simple technique for combin-
ing a latent variable support vector machine with
a list of discourse connectors, by creating an aug-
mented feature set that combines the connectors
with pairwise subjectivity transition features. This
improves accuracy, even with a noisy list of connec-
tors that has been identified automatically. Possible
directions for future work include richer representa-
tions of discourse structure, and the combination of
discourse-level and sentence-level valence and sub-
jectivity shifters.
Acknowledgments
Thanks to the anonymous reviewers for their help-
ful feedback. This work was supported by a Google
Faculty Research Award.
812
References
Fermin L. Cruz, Jose A. Troyano, Fernando Enriquez,
and Javier Ortega. 2008. Clasificacio?n de documen-
tos basada en la opinio?n: experimentos con un cor-
pus de cr?ticas de cine en espanol. Procesamiento de
Lenguaje Natural, 41.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Tom Huddleston. 2010. Review of The Bad Lieutenant:
Port of Call New Orleans. Time Out, May 18.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of ACL.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3).
Yi Mao and Guy Lebanon. 2006. Isotonic condi-
tional random fields and local sentiment flow. In
B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
ACL.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of RANLP.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL.
Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifters. Computing attitude and affect in text:
Theory and applications.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Veronica Perez Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in spanish. In
Proceedings of LREC.
Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In Proceedings of
NAACL.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP-CoNLL.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
EMNLP.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL.
Kimberly Voll and Maite Taboada. 2007. Not all words
are created equal: Extracting semantic orientation as
a function of adjective relevance. In Proceedings of
Australian Conference on Artificial Intelligence.
Janyce M. Wiebe. 1994. Tracking point of view in nar-
rative. Computational Linguistics, 20(2).
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT-EMNLP: Interactive Demonstra-
tions.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of EMNLP.
Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010a.
Automatically generating annotator rationales to im-
prove sentiment classification. In Proceedings of ACL:
Short Papers.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010b. Multi-Level structured models for Document-
Level sentiment classification. In Proceedings of
EMNLP.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of ICML.
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In Proceedings
of HLT-NAACL.
813
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1365?1374,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Discovering Sociolinguistic Associations with Structured Sparsity
Jacob Eisenstein Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,nasmith,epxing}@cs.cmu.edu
Abstract
We present a method to discover robust and
interpretable sociolinguistic associations from
raw geotagged text data. Using aggregate de-
mographic statistics about the authors? geo-
graphic communities, we solve a multi-output
regression problem between demographics
and lexical frequencies. By imposing a com-
posite `1,? regularizer, we obtain structured
sparsity, driving entire rows of coefficients
to zero. We perform two regression studies.
First, we use term frequencies to predict de-
mographic attributes; our method identifies a
compact set of words that are strongly asso-
ciated with author demographics. Next, we
conjoin demographic attributes into features,
which we use to predict term frequencies. The
composite regularizer identifies a small num-
ber of features, which correspond to com-
munities of authors united by shared demo-
graphic and linguistic properties.
1 Introduction
How is language influenced by the speaker?s so-
ciocultural identity? Quantitative sociolinguistics
usually addresses this question through carefully
crafted studies that correlate individual demographic
attributes and linguistic variables?for example, the
interaction between income and the ?dropped r? fea-
ture of the New York accent (Labov, 1966). But
such studies require the knowledge to select the
?dropped r? and the speaker?s income, from thou-
sands of other possibilities. In this paper, we present
a method to acquire such patterns from raw data. Us-
ing multi-output regression with structured sparsity,
our method identifies a small subset of lexical items
that are most influenced by demographics, and dis-
covers conjunctions of demographic attributes that
are especially salient for lexical variation.
Sociolinguistic associations are difficult to model,
because the space of potentially relevant interactions
is large and complex. On the linguistic side there
are thousands of possible variables, even if we limit
ourselves to unigram lexical features. On the demo-
graphic side, the interaction between demographic
attributes is often non-linear: for example, gender
may negate or amplify class-based language differ-
ences (Zhang, 2005). Thus, additive models which
assume that each demographic attribute makes a lin-
ear contribution are inadequate.
In this paper, we explore the large space of po-
tential sociolinguistic associations using structured
sparsity. We treat the relationship between language
and demographics as a set of multi-input, multi-
output regression problems. The regression coeffi-
cients are arranged in a matrix, with rows indicating
predictors and columns indicating outputs. We ap-
ply a composite regularizer that drives entire rows
of the coefficient matrix to zero, yielding compact,
interpretable models that reuse features across dif-
ferent outputs. If we treat the lexical frequencies
as inputs and the author?s demographics as outputs,
the induced sparsity pattern reveals the set of lexi-
cal items that is most closely tied to demographics.
If we treat the demographic attributes as inputs and
build a model to predict the text, we can incremen-
tally construct a conjunctive feature space of demo-
graphic attributes, capturing key non-linear interac-
tions.
1365
The primary purpose of this research is ex-
ploratory data analysis to identify both the most
linguistic-salient demographic features, and the
most demographically-salient words. However, this
model also enables predictions about demographic
features by analyzing raw text, potentially support-
ing applications in targeted information extraction
or advertising. On the task of predicting demo-
graphics from text, we find that our sparse model
yields performance that is statistically indistinguish-
able from the full vocabulary, even with a reduction
in the model complexity an order of magnitude. On
the task of predicting text from author demograph-
ics, we find that our incrementally constructed fea-
ture set obtains significantly better perplexity than a
linear model of demographic attributes.
2 Data
Our dataset is derived from prior work in which
we gathered the text and geographical locations of
9,250 microbloggers on the website twitter.
com (Eisenstein et al, 2010). Bloggers were se-
lected from a pool of frequent posters whose mes-
sages include metadata indicating a geographical lo-
cation within a bounding box around the continen-
tal United States. We limit the vocabulary to the
5,418 terms which are used by at least 40 authors; no
stoplists are applied, as the use of standard or non-
standard orthography for stopwords (e.g., to vs. 2)
may convey important information about the author.
The dataset includes messages during the first week
of March 2010.
O?Connor et al (2010) obtained aggregate demo-
graphic statistics for these data by mapping geoloca-
tions to publicly-available data from the U. S. Cen-
sus ZIP Code Tabulation Areas (ZCTA).1 There
are 33,178 such areas in the USA (the 9,250 mi-
crobloggers in our dataset occupy 3,458 unique ZC-
TAs), and they are designed to contain roughly
equal numbers of inhabitants and demographically-
homogeneous populations. The demographic at-
tributes that we consider in this paper are shown
in Table 1. All attributes are based on self-reports.
The race and ethnicity attributes are not mutually
exclusive?individuals can indicate any number of
races or ethnicities. The ?other language? attribute
1http://www.census.gov/support/cen2000.
html
mean std. dev.
race & ethnicity
% white 52.1 29.0
% African American 32.2 29.1
% Hispanic 15.7 18.3
language
% English speakers 73.7 18.4
% Spanish speakers 14.6 15.6
% other language speakers 11.7 9.2
socioeconomic
% urban 95.1 14.3
% with family 64.1 14.4
% renters 48.9 23.4
median income ($) 42,500 18,100
Table 1: The demographic attributes used in this research.
aggregates all languages besides English and Span-
ish. ?Urban areas? refer to sets of census tracts or
census blocks which contain at least 2,500 residents;
our ?% urban? attribute is the percentage of individ-
uals in each ZCTA who are listed as living in an ur-
ban area. We also consider the percentage of indi-
viduals who live with their families, the percentage
who live in rented housing, and the median reported
income in each ZCTA.
While geographical aggregate statistics are fre-
quently used to proxy for individual socioeconomic
status in research areas such as public health (e.g.,
Rushton, 2008), it is clear that interpretation must
proceed with caution. Consider an author from a ZIP
code in which 60% of the residents are Hispanic:2
we do not know the likelihood that the author is His-
panic, because the set of Twitter users is not a rep-
resentative sample of the overall population. Polling
research suggests that users of both Twitter (Smith
and Rainie, 2010) and geolocation services (Zick-
uhr and Smith, 2010) are much more diverse with
respect to age, gender, race and ethnicity than the
general population of Internet users. Nonetheless,
at present we can only use aggregate statistics to
make inferences about the geographic communities
in which our authors live, and not the authors them-
selves.
2In the U.S. Census, the official ethnonym is Hispanic or
Latino; for brevity we will use Hispanic in the rest of this paper.
1366
3 Models
The selection of both words and demographic fea-
tures can be framed in terms of multi-output regres-
sion with structured sparsity. To select the lexical
indicators that best predict demographics, we con-
struct a regression problem in which term frequen-
cies are the predictors and demographic attributes
are the outputs; to select the demographic features
that predict word use, this arrangement is reversed.
Through structured sparsity, we learn models in
which entire sets of coefficients are driven to zero;
this tells us which words and demographic features
can safely be ignored.
This section describes the model and implemen-
tation for output-regression with structured sparsity;
in Section 4 and 5 we give the details of its applica-
tion to select terms and demographic features. For-
mally, we consider the linear equationY = XB+,
where,
? Y is the dependent variable matrix, with di-
mensions N ? T , where N is the number of
samples and T is the number of output dimen-
sions (or tasks);
? X is the independent variable matrix, with di-
mensions N ? P , where P is the number of
input dimensions (or predictors);
? B is the matrix of regression coefficients, with
dimensions P ? T ;
?  is a N ? T matrix in which each element is
noise from a zero-mean Gaussian distribution.
We would like to solve the unconstrained opti-
mization problem,
minimizeB ||Y ?XB||
2
F + ?R(B), (1)
where ||A||2F indicates the squared Frobenius norm?
i
?
j a
2
ij , and the function R(B) defines a norm
on the regression coefficients B. Ridge regres-
sion applies the `2 norm R(B) =
?T
t=1
??P
p b
2
pt,
and lasso regression applies the `1 norm R(B) =?T
t=1
?P
p |bpt|; in both cases, it is possible to de-
compose the multi-output regression problem, treat-
ing each output dimension separately. However, our
working hypothesis is that there will be substantial
correlations across both the vocabulary and the de-
mographic features?for example, a demographic
feature such as the percentage of Spanish speakers
will predict a large set of words. Our goal is to select
a small set of predictors yielding good performance
across all output dimensions. Thus, we desire struc-
tured sparsity, in which entire rows of the coefficient
matrix B are driven to zero.
Structured sparsity is not achieved by the lasso?s
`1 norm. The lasso gives element-wise sparsity, in
which many entries ofB are driven to zero, but each
predictor may have a non-zero value for some output
dimension. To drive entire rows of B to zero, we re-
quire a composite regularizer. We consider the `1,?
norm, which is the sum of `? norms across output
dimensions: R(B) =
?T
t maxp bpt (Turlach et al,
2005). This norm, which corresponds to a multi-
output lasso regression, has the desired property of
driving entire rows of B to zero.
3.1 Optimization
There are several techniques for solving the `1,?
normalized regression, including interior point
methods (Turlach et al, 2005) and projected gradi-
ent (Duchi et al, 2008; Quattoni et al, 2009). We
choose the blockwise coordinate descent approach
of Liu et al (2009) because it is easy to implement
and efficient: the time complexity of each iteration
is independent of the number of samples.3
Due to space limitations, we defer to Liu et al
(2009) for a complete description of the algorithm.
However, we note two aspects of our implementa-
tion which are important for natural language pro-
cessing applications. The algorithm?s efficiency is
accomplished by precomputing the matrices C =
X?TY? and D = X?TX?, where X? and Y? are the stan-
dardized versions ofX andY, obtained by subtract-
ing the mean and scaling by the variance. Explicit
mean correction would destroy the sparse term fre-
quency data representation and render us unable to
store the data in memory; however, we can achieve
the same effect by computing C = XTY ?N x?Ty?,
where x? and y? are row vectors indicating the means
3Our implementation is available at http://sailing.
cs.cmu.edu/sociolinguistic.html.
1367
ofX andY respectively.4 We can similarly compute
D = XTX?N x?Tx?.
If the number of predictors is too large, it may
not be possible to store the dense matrix D in mem-
ory. We have found that approximation based on the
truncated singular value decomposition provides an
effective trade-off of time for space. Specifically, we
compute XTX ?
USVT
(
USVT
)T
= U
(
SVTVSTUT
)
= UM.
Lower truncation levels are less accurate, but are
faster and require less space: for K singular val-
ues, the storage cost is O(KP ), instead of O(P 2);
the time cost increases by a factor of K. This ap-
proximation was not necessary in the experiments
presented here, although we have found that it per-
forms well as long as the regularizer is not too close
to zero.
3.2 Regularization
The regularization constant ? can be computed us-
ing cross-validation. As ? increases, we reuse the
previous solution of B for initialization; this ?warm
start? trick can greatly accelerate the computation
of the overall regularization path (Friedman et al,
2010). At each ?i, we solve the sparse multi-output
regression; the solution Bi defines a sparse set of
predictors for all tasks.
We then use this limited set of predictors to con-
struct a new input matrix X?i, which serves as the
input in a standard ridge regression, thus refitting
the model. The tuning set performance of this re-
gression is the score for ?i. Such post hoc refitting
is often used in tandem with the lasso and related
sparse methods; the effectiveness of this procedure
has been demonstrated in both theory (Wasserman
and Roeder, 2009) and practice (Wu et al, 2010).
The regularization parameter of the ridge regression
is determined by internal cross-validation.
4 Predicting Demographics from Text
Sparse multi-output regression can be used to select
a subset of vocabulary items that are especially in-
dicative of demographic and geographic differences.
4Assume without loss of generality that X and Y are scaled
to have variance 1, because this scaling does not affect the spar-
sity pattern.
Starting from the regression problem (1), the predic-
tors X are set to the term frequencies, with one col-
umn for each word type and one row for each author
in the dataset. The outputsY are set to the ten demo-
graphic attributes described in Table 1 (we consider
much larger demographic feature spaces in the next
section) The `1,? regularizer will drive entire rows
of the coefficient matrix B to zero, eliminating all
demographic effects for many words.
4.1 Quantitative Evaluation
We evaluate the ability of lexical features to predict
the demographic attributes of their authors (as prox-
ied by the census data from the author?s geograph-
ical area). The purpose of this evaluation is to as-
sess the predictive ability of the compact subset of
lexical items identified by the multi-output lasso, as
compared with the full vocabulary. In addition, this
evaluation establishes a baseline for performance on
the demographic prediction task.
We perform five-fold cross-validation, using the
multi-output lasso to identify a sparse feature set
in the training data. We compare against several
other dimensionality reduction techniques, match-
ing the number of features obtained by the multi-
output lasso at each fold. First, we compare against
a truncated singular value decomposition, with the
truncation level set to the number of terms selected
by the multi-output lasso; this is similar in spirit to
vector-based lexical semantic techniques (Schu?tze
and Pedersen, 1993). We also compare against sim-
ply selecting the N most frequent terms, and the N
terms with the greatest variance in frequency across
authors. Finally, we compare against the complete
set of all 5,418 terms. As before, we perform post
hoc refitting on the training data using a standard
ridge regression. The regularization constant for the
ridge regression is identified using nested five-fold
cross validation within the training set.
We evaluate on the refit models on the heldout
test folds. The scoring metric is Pearson?s correla-
tion coefficient between the predicted and true de-
mographics: ?(y, y?) = cov(y,y?)?y?y? , with cov(y, y?) in-
dicating the covariance and ?y indicating the stan-
dard deviation. On this metric, a perfect predictor
will score 1 and a random predictor will score 0. We
report the average correlation across all ten demo-
1368
102 103
0.16
0.18
0.2
0.22
0.24
0.26
0.28
number of features
av
er
age
 co
rre
lati
on
 
 
multi?output lasso
SVD
highest variance
most frequent
Figure 1: Average correlation plotted against the number
of active features (on a logarithmic scale).
graphic attributes, as well as the individual correla-
tions.
Results Table 2 shows the correlations obtained
by regressions performed on a range of different vo-
cabularies, averaged across all five folds. Linguistic
features are best at predicting race, ethnicity, lan-
guage, and the proportion of renters; the other de-
mographic attributes are more difficult to predict.
Among feature sets, the highest average correlation
is obtained by the full vocabulary, but the multi-
output lasso obtains nearly identical performance
using a feature set that is an order of magnitude
smaller. Applying the Fischer transformation, we
find that all correlations are statistically significant
at p < .001.
The Fischer transformation can also be used to
estimate 95% confidence intervals around the cor-
relations. The extent of the confidence intervals
varies slightly across attributes, but all are tighter
than ?0.02. We find that the multi-output lasso and
the full vocabulary regression are not significantly
different on any of the attributes. Thus, the multi-
output lasso achieves a 93% compression of the fea-
ture set without a significant decrease in predictive
performance. The multi-output lasso yields higher
correlations than the other dimensionality reduction
techniques on all of the attributes; these differences
are statistically significant in many?but not all?
cases. The correlations for each attribute are clearly
not independent, so we do not compare the average
across attributes.
Recall that the regularization coefficient was cho-
sen by nested cross-validation within the training
set; the average number of features selected is
394.6. Figure 1 shows the performance of each
dimensionality-reduction technique across the reg-
ularization path for the first of five cross-validation
folds. Computing the truncated SVD of a sparse ma-
trix at very large truncation levels is computationally
expensive, so we cannot draw the complete perfor-
mance curve for this method. The multi-output lasso
dominates the alternatives, obtaining a particularly
strong advantage with very small feature sets. This
demonstrates its utility for identifying interpretable
models which permit qualitative analysis.
4.2 Qualitative Analysis
For a qualitative analysis, we retrain the model on
the full dataset, and tune the regularization to iden-
tify a compact set of 69 features. For each identified
term, we apply a significance test on the relationship
between the presence of each term and the demo-
graphic indicators shown in the columns of the ta-
ble. Specifically, we apply the Wald test for compar-
ing the means of independent samples, while mak-
ing the Bonferroni correction for multiple compar-
isons (Wasserman, 2003). The use of sparse multi-
output regression for variable selection increases the
power of post hoc significance testing, because the
Bonferroni correction bases the threshold for sta-
tistical significance on the total number of compar-
isons. We find 275 associations at the p < .05 level;
at the higher threshold required by a Bonferroni cor-
rection for comparisons among all terms in the vo-
cabulary, 69 of these associations would have been
missed.
Table 3 shows the terms identified by our model
which have a significant correlation with at least one
of the demographic indicators. We divide words in
the list into categories, which order alphabetically
by the first word in each category: emoticons; stan-
dard English, defined as words with Wordnet entries;
proper names; abbreviations; non-English words;
non-standard words used with English. The cate-
gorization was based on the most frequent sense in
an informal analysis of our data. A glossary of non-
standard terms is given in Table 4.
Some patterns emerge from Table 3. Standard
English words tend to appear in areas with more
1369
vocabulary # features av
er
ag
e
w
hi
te
A
fr
.A
m
.
H
is
p.
E
ng
.l
an
g.
S
pa
n.
la
ng
.
ot
he
r
la
ng
.
ur
ba
n
fa
m
il
y
re
nt
er
m
ed
.i
nc
.
full 5418 0.260 0.337 0.318 0.296 0.384 0.296 0.256 0.155 0.113 0.295 0.152
multi-output lasso
394.6
0.260 0.326 0.308 0.304 0.383 0.303 0.249 0.153 0.113 0.302 0.156
SVD 0.237 0.321 0.299 0.269 0.352 0.272 0.226 0.138 0.081 0.278 0.136
highest variance 0.220 0.309 0.287 0.245 0.315 0.248 0.199 0.132 0.085 0.250 0.135
most frequent 0.204 0.294 0.264 0.222 0.293 0.229 0.178 0.129 0.073 0.228 0.126
Table 2: Correlations between predicted and observed demographic attributes, averaged across cross validation folds.
English speakers; predictably, Spanish words tend
to appear in areas with Spanish speakers and His-
panics. Emoticons tend to be used in areas with
many Hispanics and few African Americans. Ab-
breviations (e.g., lmaoo) have a nearly uniform
demographic profile, displaying negative correla-
tions with whites and English speakers, and posi-
tive correlations with African Americans, Hispanics,
renters, Spanish speakers, and areas classified as ur-
ban.
Many non-standard English words (e.g., dats)
appear in areas with high proportions of renters,
African Americans, and non-English speakers,
though a subset (haha, hahaha, and yep) display
the opposite demographic pattern. Many of these
non-standard words are phonetic transcriptions of
standard words or phrases: that?s?dats, what?s
up?wassup, I?m going to?ima. The relationship
between these transcriptions and the phonological
characteristics of dialects such as African-American
Vernacular English is a topic for future work.
5 Conjunctive Demographic Features
Next, we demonstrate how to select conjunctions of
demographic features that predict text. Again, we
apply multi-output regression, but now we reverse
the direction of inference: the predictors are demo-
graphic features, and the outputs are term frequen-
cies. The sparsity-inducing `1,? norm will select a
subset of demographic features that explain the term
frequencies.
We create an initial feature set f (0)(X) by bin-
ning each demographic attribute, using five equal-
frequency bins. We then constructive conjunctive
features by applying a procedure inspired by related
work in computational biology, called ?Screen and
Clean? (Wu et al, 2010). On iteration i:
? Solve the sparse multi-output regression prob-
lem Y = f (i)(X)B(i) + .
? Select a subset of features S(i) such that m ?
S(i) iff maxj |b
(i)
m,j | > 0. These are the row
indices of the predictors with non-zero coeffi-
cients.
? Create a new feature set f (i+1)(X), including
the conjunction of each feature (and its nega-
tion) in S(i) with each feature in the initial set
f (0)(X).
We iterate this process to create features that con-
join as many as three attributes. In addition to the
binned versions of the demographic attributes de-
scribed in Table 1, we include geographical infor-
mation. We built Gaussian mixture models over the
locations, with 3, 5, 8, 12, 17, and 23 components.
For each author we include the most likely cluster
assignment in each of the six mixture models. For
efficiency, the outputs Y are not set to the raw term
frequencies; instead we compute a truncated sin-
gular value decomposition of the term frequencies
W ? UVDT, and use the basis U. We set the trun-
cation level to 100.
5.1 Quantitative Evaluation
The ability of the induced demographic features to
predict text is evaluated using a traditional perplex-
ity metric. The same test and training split is used
from the vocabulary experiments. We construct a
language model from the induced demographic fea-
tures by training a multi-output ridge regression,
which gives a matrix B? that maps from demographic
features to term frequencies across the entire vocab-
ulary. For each document in the test set, the ?raw?
predicted language model is y?d = f(xd)B, which
is then normalized. The probability mass assigned
1370
w
hi
te
A
fr
.A
m
.
H
is
p.
E
ng
.l
an
g.
S
pa
n.
la
ng
.
ot
he
r
la
ng
.
ur
ba
n
fa
m
il
y
re
nt
er
m
ed
.i
nc
.
- - - + - + + +
;) - + - +
:( -
:) -
:d + - + - +
as - + -
awesome + - - - +
break - + - -
campus - + - -
dead - + - + + +
hell - + - -
shit - +
train - + +
will - + -
would + -
atlanta - + - -
famu + - + - - -
harlem - +
bbm - + - + + +
lls + - + - -
lmaoo - + + - + + + +
lmaooo - + + - + + + +
lmaoooo - + + - + + +
lmfaoo - + - + + +
lmfaooo - + - + + +
lml - + + - + + + + -
odee - + - + + +
omw - + + - + + + +
smfh - + + - + + + +
smh - + + +
w| - + - + + + +
con + - + +
la - + - +
si - + - +
dats - + - + -
deadass - + + - + + + +
haha + - -
hahah + -
hahaha + - - +
ima - + - + +
madd - - + +
nah - + - + + +
ova - + - +
sis - + +
skool - + - + + + -
wassup - + + - + + + + -
wat - + + - + + + + -
ya - + +
yall - +
yep - + - - - -
yoo - + + - + + + +
yooo - + - + +
Table 3: Demographically-indicative terms discovered by
multi-output sparse regression. Statistically significant
(p < .05) associations are marked with a + or -.
term definition
bbm Blackberry Messenger
dats that?s
dead(ass) very
famu Florida Agricultural
and Mechanical Univ.
ima I?m going to
lls laughing like shit
lm(f)ao+ laughing my (fucking)
ass off
lml love my life
madd very, lots
nah no
odee very
term definition
omw on my way
ova over
sis sister
skool school
sm(f)h shake my (fuck-
ing) head
w| with
wassup what?s up
wat what
ya your, you
yall you plural
yep yes
yoo+ you
Table 4: A glossary of non-standard terms from Ta-
ble 3. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
model perplexity
induced demographic features 333.9
raw demographic attributes 335.4
baseline (no demographics) 337.1
Table 5: Word perplexity on test documents, using
language models estimated from induced demographic
features, raw demographic attributes, and a relative-
frequency baseline. Lower scores are better.
to unseen words is determined through nested cross-
validation. We compare against a baseline language
model obtained from the training set, again using
nested cross-validation to set the probability of un-
seen terms.
Results are shown in Table 5. The language mod-
els induced from demographic data yield small but
statistically significant improvements over the base-
line (Wilcoxon signed-rank test, p < .001). More-
over, the model based on conjunctive features signif-
icantly outperforms the model constructed from raw
attributes (p < .001).
5.2 Features Discovered
Our approach discovers 37 conjunctive features,
yielding the results shown in Table 5. We sort all
features by frequency, and manually select a sub-
set to display in Table 6. Alongside each feature,
we show the words with the highest and lowest log-
odds ratios with respect to the feature. Many of these
terms are non-standard; while space does not permit
a complete glossary, some are defined in Table 4 or
in our earlier work (Eisenstein et al, 2010).
1371
feature positive terms negative terms
1 geo: Northeast m2 brib mangoville soho odeee fasho #ilovefamu foo coo fina
2 geo: NYC mangoville lolss m2 brib wordd bahaha fasho goofy #ilovefamu
tacos
4 geo: South+Midwest renter? 0.615 white? 0.823 hme muthafucka bae charlotte tx odeee m2 lolss diner mangoville
7 Afr. Am.> 0.101 renter> 0.615 Span. lang.> 0.063 dhat brib odeee lolss wassupp bahaha charlotte california ikr en-
ter
8 Afr. Am.? 0.207 Hispanic> 0.119 Span. lang.> 0.063 les ahah para san donde bmore ohio #lowkey #twitterjail
nahhh
9 geo: NYC Span. lang.? 0.213 mangoville thatt odeee lolss
buzzin
landed rodney jawn wiz golf
12 Afr. Am.> 0.442 geo: South+Midwest white? 0.823 #ilovefamu panama midtermswillies #lowkey knoe esta pero odeee hii
15 geo: West Coast other lang.> 0.110 ahah fasho san koo diego granted pride adore phat pressure
17 Afr. Am.> 0.442 geo: NYC other lang.? 0.110 lolss iim buzzin qonna qood foo tender celebs pages pandora
20 Afr. Am.? 0.207 Span. lang.> 0.063 white> 0.823 del bby cuando estoy muscle knicks becoming uncomfortablelarge granted
23 Afr. Am.? 0.050 geo: West Span. lang.? 0.106 leno it?d 15th hacked government knicks liquor uu hunn homee
33 Afr. Am.> 0.101 geo: SF Bay Span. lang.> 0.063 hella aha california bay o.o aj everywhere phones shift re-gardless
36 Afr. Am.? 0.050 geo: DC/Philadelphia Span. lang.? 0.106 deh opens stuffed yaa bmore hmmmmm dyin tea cousin hella
Table 6: Conjunctive features discovered by our method with a strong sparsity-inducing prior, ordered by frequency.
We also show the words with high log-odds for each feature (postive terms) and its negation (negative terms).
In general, geography was a strong predictor, ap-
pearing in 25 of the 37 conjunctions. Features 1
and 2 (F1 and F2) are purely geographical, captur-
ing the northeastern United States and the New York
City area. The geographical area of F2 is completely
contained by F1; the associated terms are thus very
similar, but by having both features, the model can
distinguish terms which are used in northeastern ar-
eas outside New York City, as well as terms which
are especially likely in New York.5
Several features conjoin geography with demo-
graphic attributes. For example, F9 further refines
the New York City area by focusing on communities
that have relatively low numbers of Spanish speak-
ers; F17 emphasizes New York neighborhoods that
have very high numbers of African Americans and
few speakers of languages other than English and
Spanish. The regression model can use these fea-
tures in combination to make fine-grained distinc-
tions about the differences between such neighbor-
hoods. Outside New York, we see that F4 combines
a broad geographic area with attributes that select at
least moderate levels of minorities and fewer renters
(a proxy for areas that are less urban), while F15
identifies West Coast communities with large num-
5Mangoville and M2 are clubs in New York; fasho and coo
were previously found to be strongly associated with the West
Coast (Eisenstein et al, 2010).
bers of speakers of languages other than English and
Spanish.
Race and ethnicity appear in 28 of the 37 con-
junctions. The attribute indicating the proportion of
African Americans appeared in 22 of these features,
strongly suggesting that African American Vernac-
ular English (Rickford, 1999) plays an important
role in social media text. Many of these features
conjoined the proportion of African Americans with
geographical features, identifying local linguistic
styles used predominantly in either African Amer-
ican or white communities. Among features which
focus on minority communities, F17 emphasizes the
New York area, F33 focuses on the San Francisco
Bay area, and F12 selects a broad area in the Mid-
west and South. Conversely, F23 selects areas with
very few African Americans and Spanish-speakers
in the western part of the United States, and F36 se-
lects for similar demographics in the area of Wash-
ington and Philadelphia.
Other features conjoined the proportion of
African Americans with the proportion of Hispan-
ics and/or Spanish speakers. In some cases, features
selected for high proportions of both African Amer-
icans and Hispanics; for example, F7 seems to iden-
tify a general ?urban minority? group, emphasizing
renters, African Americans, and Spanish speakers.
Other features differentiate between African Ameri-
1372
cans and Hispanics: F8 identifies regions with many
Spanish speakers and Hispanics, but few African
Americans; F20 identifies regions with both Span-
ish speakers and whites, but few African Americans.
F8 and F20 tend to emphasize more Spanish words
than features which select for both African Ameri-
cans and Hispanics.
While race, geography, and language predom-
inate, the socioeconomic attributes appear in far
fewer features. The most prevalent attribute is the
proportion of renters, which appears in F4 and F7,
and in three other features not shown here. This at-
tribute may be a better indicator of the urban/rural
divide than the ?% urban? attribute, which has a
very low threshold for what counts as urban (see
Table 1). It may also be a better proxy for wealth
than median income, which appears in only one of
the thirty-seven selected features. Overall, the se-
lected features tend to include attributes that are easy
to predict from text (compare with Table 2).
6 Related Work
Sociolinguistics has a long tradition of quantitative
and computational research. Logistic regression has
been used to identify relationships between demo-
graphic features and linguistic variables since the
1970s (Cedergren and Sankoff, 1974). More re-
cent developments include the use of mixed factor
models to account for idiosyncrasies of individual
speakers (Johnson, 2009), as well as clustering and
multidimensional scaling (Nerbonne, 2009) to en-
able aggregate inference across multiple linguistic
variables. However, all of these approaches assume
that both the linguistic indicators and demographic
attributes have already been identified by the re-
searcher. In contrast, our approach focuses on iden-
tifying these indicators automatically from data. We
view our approach as an exploratory complement to
more traditional analysis.
There is relatively little computational work on
identifying speaker demographics. Chang et al
(2010) use U.S. Census statistics about the ethnic
distribution of last names as an anchor in a latent-
variable model that infers the ethnicity of Facebook
users; however, their paper analyzes social behav-
ior rather than language use. In unpublished work,
David Bamman uses geotagged Twitter text and U.S.
Census statistics to estimate the age, gender, and
racial distributions of various lexical items.6 Eisen-
stein et al (2010) infer geographic clusters that are
coherent with respect to both location and lexical
distributions; follow-up work by O?Connor et al
(2010) applies a similar generative model to demo-
graphic data. The model presented here differs in
two key ways: first, we use sparsity-inducing regu-
larization to perform variable selection; second, we
eschew high-dimensional mixture models in favor of
a bottom-up approach of building conjunctions of
demographic and geographic attributes. In a mix-
ture model, each component must define a distribu-
tion over all demographic variables, which may be
difficult to estimate in a high-dimensional setting.
Early examples of the use of sparsity in natu-
ral language processing include maximum entropy
classification (Kazama and Tsujii, 2003), language
modeling (Goodman, 2004), and incremental pars-
ing (Riezler and Vasserman, 2004). These papers all
apply the standard lasso, obtaining sparsity for a sin-
gle output dimension. Structured sparsity has rarely
been applied to language tasks, but Duh et al (2010)
reformulated the problem of reranking N -best lists
as multi-task learning with structured sparsity.
7 Conclusion
This paper demonstrates how regression with struc-
tured sparsity can be applied to select words and
conjunctive demographic features that reveal soci-
olinguistic associations. The resulting models are
compact and interpretable, with little cost in accu-
racy. In the future we hope to consider richer lin-
guistic models capable of identifying multi-word ex-
pressions and syntactic variation.
Acknowledgments We received helpful feedback
from Moira Burke, Scott Kiesling, Seyoung Kim, Andre?
Martins, Kriti Puniyani, and the anonymous reviewers.
Brendan O?Connor provided the data for this research,
and Seunghak Lee shared a Matlab implementation of
the multi-output lasso, which was the basis for our C
implementation. This research was enabled by AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF CAREER IIS-1054319, NSF IIS-
0713379, an Alfred P. Sloan Fellowship, and Google?s
support of the Worldly Knowledge project at CMU.
6http://www.lexicalist.com
1373
References
Henrietta J. Cedergren and David Sankoff. 1974. Vari-
able rules: Performance as a statistical reflection of
competence. Language, 50(2):333?355.
Jonathan Chang, Itamar Rosenn, Lars Backstrom, and
Cameron Marlow. 2010. ePluribus: Ethnicity on so-
cial networks. In Proceedings of ICWSM.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
`1-ball for learning in high dimensions. In Proceed-
ings of ICML.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. n-best rerank-
ing by multitask learning. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
Metrics.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model of ge-
ographic lexical variation. In Proceedings of EMNLP.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization paths for generalized linear
models via coordinate descent. Journal of Statistical
Software, 33(1):1?22.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proceedings of NAACL-HLT.
Daniel E. Johnson. 2009. Getting off the GoldVarb
standard: Introducing Rbrul for mixed-effects variable
rule analysis. Language and Linguistics Compass,
3(1):359?383.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP.
William Labov. 1966. The Social Stratification of En-
glish in New York City. Center for Applied Linguis-
tics.
Han Liu, Mark Palatucci, and Jian Zhang. 2009. Block-
wise coordinate descent procedures for the multi-task
lasso, with applications to neural semantic basis dis-
covery. In Proceedings of ICML.
John Nerbonne. 2009. Data-driven dialectology. Lan-
guage and Linguistics Compass, 3(1):175?198.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science.
Ariadna Quattoni, Xavier Carreras, Michael Collins, and
Trevor Darrell. 2009. An efficient projection for `1,?
regularization. In Proceedings of ICML.
John R. Rickford. 1999. African American Vernacular
English. Blackwell.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and `1 regularization for re-
laxed maximum-entropy modeling. In Proceedings of
EMNLP.
Gerard Rushton, Marc P. Armstrong, Josephine Gittler,
Barry R. Greene, Claire E. Pavlik, Michele M. West,
and Dale L. Zimmerman, editors. 2008. Geocoding
Health Data: The Use of Geographic Codes in Cancer
Prevention and Control, Research, and Practice. CRC
Press.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Aaron Smith and Lee Rainie. 2010. Who tweets? Tech-
nical report, Pew Research Center, December.
Berwin A. Turlach, William N. Venables, and Stephen J.
Wright. 2005. Simultaneous variable selection. Tech-
nometrics, 47(3):349?363.
Larry Wasserman and Kathryn Roeder. 2009. High-
dimensional variable selection. Annals of Statistics,
37(5A):2178?2201.
Larry Wasserman. 2003. All of Statistics: A Concise
Course in Statistical Inference. Springer.
Jing Wu, Bernie Devlin, Steven Ringquist, Massimo
Trucco, and Kathryn Roeder. 2010. Screen and clean:
A tool for identifying interactions in genome-wide as-
sociation studies. Genetic Epidemiology, 34(3):275?
285.
Qing Zhang. 2005. A Chinese yuppie in Beijing: Phono-
logical variation and the construction of a new profes-
sional identity. Language in Society, 34:431?466.
Kathryn Zickuhr and Aaron Smith. 2010. 4% of online
Americans use location-based services. Technical re-
port, Pew Research Center, November.
1374
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184?193,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bootstrapping a Unified Model of Lexical and Phonetic Acquisition
Micha Elsner
melsner0@gmail.com
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Jacob Eisenstein
jacobe@gmail.com
School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA, 30308, USA
Abstract
During early language acquisition, infants must
learn both a lexicon and a model of phonet-
ics that explains how lexical items can vary
in pronunciation?for instance ?the? might be
realized as [Di] or [D@]. Previous models of ac-
quisition have generally tackled these problems
in isolation, yet behavioral evidence suggests
infants acquire lexical and phonetic knowledge
simultaneously. We present a Bayesian model
that clusters together phonetic variants of the
same lexical item while learning both a lan-
guage model over lexical items and a log-linear
model of pronunciation variability based on ar-
ticulatory features. The model is trained on
transcribed surface pronunciations, and learns
by bootstrapping, without access to the true
lexicon. We test the model using a corpus of
child-directed speech with realistic phonetic
variation and either gold standard or automati-
cally induced word boundaries. In both cases
modeling variability improves the accuracy of
the learned lexicon over a system that assumes
each lexical item has a unique pronunciation.
1 Introduction
Infants acquiring their first language confront two
difficult cognitive problems: building a lexicon of
word forms, and learning basic phonetics and phonol-
ogy. The two tasks are closely related: knowing what
sounds can substitute for one another helps in clus-
tering together variant pronunciations of the same
word, while knowing the environments in which par-
ticular words can occur helps determine which sound
changes are meaningful and which are not (Feldman
(a) intended: /ju want w2n/ /want e kUki/
(b) surface: [j@ waP w2n] [wan @ kUki]
(c) unsegmented: [j@waPw2n] [wan@kUki]
(d) idealized: /juwantw2n/ /wantekUki/
Figure 1: The utterances you want one? want a cookie?
represented (a) using a canonical phonemic encoding for
each word and (b) as they might be pronounced phoneti-
cally. Lines (c) and (d) remove the word boundaries (but
not utterance boundaries) from (b) and (a), respectively.
et al, 2009). For instance, if an infant who already
knows the word [ju] ?you? encounters a new word
[j@], they must decide whether it is a new lexical item
or a variant of the word they already know. Evidence
for the correct conclusion comes from the pronun-
ciation (many English vowels are reduced to [@] in
unstressed positions) and the context?if the next
word is ?want?, ?you? is a plausible choice.
To date, most models of infant language learn-
ing have focused on either lexicon-building or pho-
netic learning in isolation. For example, many mod-
els of word segmentation implicitly or explicitly
build a lexicon while segmenting the input stream
of phonemes into word tokens; in nearly all cases
the phonemic input is created from an orthographic
transcription using a phonemic dictionary, thus ab-
stracting away from any phonetic variability (Brent,
1999; Venkataraman, 2001; Swingley, 2005; Gold-
water et al, 2009, among others). As illustrated
in Figure 1, these models attempt to infer line (a)
from line (d). However, (d) is an idealization: real
speech has variability, and behavioral evidence sug-
gests that infants are still learning about the phonetics
and phonology of their language even after beginning
to segment words, rather than learning to neutralize
184
the variations first and acquiring the lexicon after-
wards (Feldman et al, 2009, and references therein).
Based on this evidence, a more realistic model of
early language acquisition should propose a method
of inferring the intended forms (Figure 1a) from the
unsegmented surface forms (1c) while also learning a
model of phonetic variation relating the intended and
surface forms (a) and (b). Previous models with sim-
ilar goals have learned from an artificial corpus with
a small vocabulary (Driesen et al, 2009; Ra?sa?nen,
2011) or have modeled variability only in vowels
(Feldman et al, 2009); to our knowledge, this paper
is the first to use a naturalistic infant-directed corpus
while modeling variability in all segments, and to
incorporate word-level context (a bigram language
model). Our main contribution is a joint lexical-
phonetic model that infers intended forms from seg-
mented surface forms; we test the system using in-
put with either gold standard word boundaries or
boundaries induced by an existing unsupervised seg-
mentation model (Goldwater et al, 2009). We show
that in both cases modeling variability improves the
accuracy of the learned lexicon over a system that
assumes each intended form has a unique surface
form.
Our model is conceptually similar to those used
in speech recognition and other applications: we
assume the intended tokens are generated from a bi-
gram language model and then distorted by a noisy
channel, in particular a log-linear model of phonetic
variability. But unlike speech recognition, we have
no ?intended-form, surface-form? training pairs to
train the phonetic model, nor even a dictionary of
intended-form strings to train the language model.
Instead, we initialize the noise model using feature
weights based on universal linguistic principles (e.g.,
a surface phone is likely to share articulatory features
with the intended phone) and use a bootstrapping
process to iteratively infer the intended forms and
retrain the language model and noise model. While
we do not claim that the particular inference mech-
anism we use is cognitively plausible, our positive
results further support the claim that infants can and
do acquire phonetics and the lexicon in concert.
2 Related work
Our work is inspired by the lexical-phonetic model
of Feldman et al (2009). They extend a model for
clustering acoustic tokens into phonetic categories
(Vallabha et al, 2007) by adding a lexical level that
simultaneously clusters word tokens (which contain
the acoustic tokens) into lexical entries. Including
the lexical level improves the model?s phonetic cat-
egorization, and a follow-up study on artificial lan-
guage learning (Feldman, 2011) supports the claim
that human learners use lexical knowledge to distin-
guish meaningful from unimportant phonetic con-
trasts. Feldman et al (2009) use a real-valued rep-
resentation for vowels (formant values), but assume
no variability in consonants, and treat each word to-
ken independently. In contrast, our model uses a
symbolic representation for sounds, but models vari-
ability in all segment types and incorporates a bigram
word-level language model.
To our knowledge, the only other lexicon-building
systems that also learn about phonetic variability are
those of Driesen et al (2009) and Ra?sa?nen (2011).
These systems learn to represent lexical items and
their variability from a discretized representation of
the speech stream, but they are tested on an artifi-
cial corpus with only 80 vocabulary items that was
constructed so as to ?avoid strong word-to-word de-
pendencies? (Ra?sa?nen, 2011). Here, we use a natu-
ralistic corpus, demonstrating that lexical-phonetic
learning is possible in this more general setting and
that word-level context information is important for
doing so.
Several other related systems work directly from
the acoustic signal and many of these do use natu-
ralistic corpora. However, they do not learn at both
the lexical and phonetic/acoustic level. For example,
Park and Glass (2008), Aimetti (2009), Jansen et al
(2010), and McInnes and Goldwater (2011) present
lexicon-building systems that use hard-coded acous-
tic similarity measures rather than learning about
variability, and they only extract and cluster a few
frequent words. On the phonetic side, Varadarajan et
al. (2008) and Dupoux et al (2011) describe systems
that learn phone-like units but without the benefit of
top-down information.
A final line of related work is on word segmenta-
tion. In addition to the models mentioned in Section
1, which use phonemic input, a few models of word
segmentation have been tested using phonetic input
(Fleck, 2008; Rytting, 2007; Daland and Pierrehum-
bert, 2010). However, they do not cluster segmented
185
Figure 2: Our generative model of the surface tokens s
from intended tokens x, which occur with left and right
contexts l and r.
word tokens into lexical items (none of these mod-
els even maintains an explicit lexicon), nor do they
model or learn from phonetic variation in the input.
3 Lexical-phonetic model
Our lexical-phonetic model is defined using the stan-
dard noisy channel framework: first a sequence of
intended word tokens is generated using a language
model, and then each token is transformed by a proba-
bilistic finite-state transducer to produce the observed
surface sequence. In this section, we present the
model in a hierarchical Bayesian framework to em-
phasize its similarity to existing models, in particu-
lar those of Feldman et al (2009) and Goldwater et
al. (2009). In our actual implementation, however,
we use approximation and MAP point estimates to
make our inference process more tractable; we dis-
cuss these simplifications in Section 4.
Our observed data consists of a (segmented) se-
quence of surface words s1 . . . sn. We wish to re-
cover the corresponding sequence of intended words
x1 . . . xn. As shown in Figure 2, si is produced from
xi by a transducer T : si ? T (xi), which models
phonetic changes. Each xi is sampled from a dis-
tribution ? which represents word frequencies, and
its left and right context words, li and ri, are drawn
from distributions conditioned on xi, in order to cap-
ture information about the environments in which
xi appears: li ? PL(xi), ri ? PR(xi). Because the
number of word types is not known in advance, ? is
drawn from a Dirichlet process DP (?), and PL(x)
and PR(x) have Pitman-Yor priors with concentra-
tion parameter 0 and discount d (Teh, 2006).
Our generative model of xi is unusual for two rea-
sons. First, we treat each xi independently rather
than linking them via a Markov chain. This makes
the model deficient, since li overlaps with xi?1 and
so forth, generating each token twice. During in-
ference, however, we will never compute the joint
probability of all the data at once, only the prob-
abilities of subsets of the variables with particular
intended word forms u and v. As long as no two of
these words are adjacent, the deficiency will have no
effect. We make this independence assumption for
computational reasons?when deciding whether to
merge u and v into a single lexical entry, we compute
the change in estimated probability for their contexts,
but not the effect on other words for which u and v
themselves appear as context words.
Also unusual is that we factor the joint probabil-
ity (l, x, r) as p(x)p(l|x)p(r|x) rather than as a left-
to-right chain p(l)p(x|l)p(r|x). Given our indepen-
dence assumption above, these two quantities are
mathematically equivalent, so the difference matters
only because we are using smoothed estimates. Our
factorization leads to a symmetric treatment of left
and right contexts, which simplifies implementation:
we can store all the context parameters locally as
PL(?|x) rather than distributed over various P (x|?).
Next, we explain our transducer T . A weighted
finite-state transducer (WFST) is a variant of a finite-
state automaton (Pereira et al, 1994) that reads an
input string symbol-by-symbol and probabilistically
produces an output string; thus it can be used to
specify a conditional probability on output strings
given an input. Our WFST (Figure 3) computes a
weighted edit distance, and is implemented using
OpenFST (Allauzen et al, 2007). It contains a state
for each triplet of (previous, current, next) phones;
conditioned on this state, it emits a character out-
put which can be thought of as a possible surface
realization of current in its particular environment.
The output can be the empty string , in which case
current is deleted. The machine can also insert char-
acters at any point in the string, by transitioning to an
insert state (previous, , current) and then returning
while emitting some new character.
The transducer is parameterized by the probabil-
ities of the arcs. For instance, all arcs leaving the
state (?, D, i) consume the character D and emit some
character c with probability p(c|?, D, i). Following
186
Figure 3: The fragment of the transducer responsible for
input string [Di] ?the?. ?...? represents an output arc for
each possible character, including the empty string ; ? is
the word boundary marker.
Dreyer et al (2008), we parameterize these distribu-
tions with a log-linear model. The model features are
based on articulatory phonetics and distinguish three
dimensions of sound production: voicing, place of
articulation and manner of articulation.
Features are generated from four positional tem-
plates (Figure 4): (curr)?out, (prev, curr)?out,
(curr, next)?out and (prev, curr, next)?out. Each
template is instantiated once per articulatory dimen-
sion, with prev, curr, next and out replaced by their
values for that dimension: for instance, there are
two voicing values, voiced and unvoiced1 and the
(curr)?out template for [D] producing [d] would
be instantiated as (voiced)?voiced. To capture
trends specific to particular sounds, each template
is instantiated again using the actual symbol for
curr and articulatory values for everything else (e.g.,
[D]?unvoiced). An additional template,?out, cap-
tures the marginal frequency of the output symbol.
There are also faithfulness features, same-sound,
same-voice, same-place and same-manner which
check if curr is exactly identical to out or shares
the exact value of a particular feature.
Our choice of templates and features is based on
standard linguistic principles: we expect that chang-
ing only a single articulatory dimension will be more
acceptable than changing several, and that the artic-
ulatory dimensions of context phones are important
because of assimilatory and dissimilatory processes
(Hayes, 2011). In modern phonetics and phonology,
these generalizations are usually expressed as Opti-
mality Theory constraints; log-linear models such as
ours have previously been used to implement stochas-
1We use seven place values and five manner values (stop,
nasal stop, fricative, vowel, other). Empty segments like  and ?
are assigned a special value ?no-value? for all features.
Figure 4: Some features generated for (?, D, i)? d. Each
black factor node corresponds to a positional template.
The features instantiated for the (curr)?out and ?out
template are shown in full, and we show some of the
features for the (curr,next)?out template.
tic Optimality Theory models (Goldwater and John-
son, 2003; Hayes and Wilson, 2008).
4 Inference
Global optimization of the model posterior is diffi-
cult; instead we use Viterbi EM (Spitkovsky et al,
2010; Allahverdyan and Galstyan, 2011). We begin
with a simple initial transducer and alternate between
two phases: clustering together surface forms, and
reestimating the transducer parameters. We iterate
this procedure until convergence (when successive
clustering phases find nearly the same set of merges);
this tends to take about 5 or 6 iterations.
In our clustering phase, we improve the model
posterior as much as possible by greedily making
type merges, where, for a pair of intended word forms
u and v, we replace all instances of xi = u with
xi = v. We maintain the invariant that each intended
word form?s most common surface form must be
itself; this biases the model toward solutions with
low distortion in the transducer.
4.1 Scoring merges
We write the change in the log posterior probability
of the model resulting from a type merge of u to v as
?(u, v), which factors into two terms, one depending
on the surface string and the transducer, and the other
depending on the string of intended words. In order to
ensure that each intended word form?s most common
surface form is itself, we define ?(u, v) = ?? if u
is more common than v.
We write the log probability of x being transduced
to s as T (s|x). If we merge u into v, we no longer
187
need to produce any surface forms from u, but instead
we must derive them from v. If #(?) counts the
occurrences of some event in the current state of the
model, the transducer component of ? is:
?T =
?
s
#(xi=u, si=s)(T (s|v)? T (s|u)) (1)
This term is typically negative, voting against a
merge, since u is more similar to itself than to v.
The language modeling term relating to the in-
tended string again factors into multiple components.
The probability of a particular li, xi, ri can be broken
into p(xi)p(li|xi)p(ri|xi) according to the model.
We deal first with the p(xi) unigram term, consid-
ering all tokens where xi ? {u, v} and computing
the probability pu = p(xi = u|xi ? {u, v}). By
definition of a Dirichlet process, the marginal over a
subset of the variables will be Dirichlet, so for ? > 1
we have the MAP estimate:
pu =
#(xi=u) + ?? 1
#(xi ? {u, v}) + 2(?? 1)
(2)
pv = p(xi = v|xi ? {u, v}) is computed similarly.
If we decide to merge u into v, however, the proba-
bility p(xi = v|xi ? {u, v}) becomes 1. The change
in log-probability resulting from the merge is closely
related to the entropy of the distribution:
?U = ?#(xi=u) log(pu)?#(xi=v) log(pv) (3)
This change must be positive and favors merging.
Next, we consider the change in probability from
the left contexts (the derivations for right contexts are
equivalent). If u and v are separate words, we gen-
erate their left contexts from different distributions
p(l|u) and p(l|v), while if they are merged, we must
generate all the contexts from the same distribution
p(l|{u, v}). This change is:
?L =
?
l
#(l, u){log(p(l|{u, v}))? log(p(l|u)}
+
?
l
#(l, v){log(p(l|{u, v}))? log(p(l|v)}
In a full Bayesian model, we would integrate over
the parameters of these distributions; instead, we
use Kneser-Ney smoothing (Kneser and Ney, 1995)
which has been shown to approximate the MAP solu-
tion of a hierarchical Pitman-Yor model (Teh, 2006;
Goldwater et al, 2006). The Kneser-Ney discount2
d is a tunable parameter of our system, and con-
trols whether the term favors merging or not. If d is
small, p(l|u) and p(l|v) are close to their maximum-
likelihood estimates, and ?L is similar to a Jensen-
Shannon divergence; it is always negative and dis-
courages mergers. As d increases, however, p(l|u)
for rare words approaches the prior distribution; in
this case, merging two words may result in better
posterior parameters than estimating both separately,
since the combined estimate loses less mass to dis-
counting.
Because neither the transducer nor the language
model are perfect models of the true distribution,
they can have incompatible dynamic ranges. Often,
the transducer distribution is too peaked; to remedy
this, we downweight the transducer probability by
?, a parameter of our model, which we set to .5.
Downweighting of the acoustic model versus the LM
is typical in speech recognition (Bahl et al, 1980).
To summarize, the full change in posterior is:
?(u, v) = ?U + ?L + ?R + ??T (4)
There are four parameters. The transducer regular-
ization r = 1 and unigram prior ? = 2, which we
set ad-hoc, have little impact on performance. The
Kneser-Ney discount d = 2 and transducer down-
weight ? = .5 have more influence and were tuned
on development data.
4.2 Clustering algorithm
In the clustering phase, we start with an initial solu-
tion in which each surface form is its own intended
pronunciation and iteratively improve this solution
by merging together word types, picking (approxi-
mately) the best merger at each point.
We begin by computing a set of candidate mergers
for each surface word type u. This step saves time
by quickly rejecting mergers which are certain to get
very low transducer scores. We reject a pair u, v if
the difference in their length is greater than 4, or if
both words are longer than 4 segments, but, when
we consider them as unordered bags of segments, the
Dice coefficient between them is less than .5.
For each word u and all its candidates v, we com-
pute ?(u, v) as in Equation 4. We keep track of the
2We use one discount, rather than several as in modified KN.
188
Input: vocabulary of surface forms u
Input: C(u): candidate intended forms of u
Output: intend(u): intended form of u
foreach u ? vocab do
// initialization
v?(u)? argmaxv ?C(u) ?(u, v);
??(u)? ?(u, v?(u))
intend(u)? u
add u to queue Q with priority ??(u))
while top(Q) > ?? do
u? pop(Q)
recompute v?(u),??(u)
if ??(u) > 0 then
// merge u with best merger
intend(u)? v?(u)
update ?(x, u) ?x : v?(x) = u
remove u from C(x) ?x
update ?(x, v) ?x : v?(x) = v
update ?(v, x) ?x ? C(v)
if updated ? > ?? for any words then
reset ??, v? for those words
// (these updates can
increase a word?s priority
from ??)
else if ??(u) 6= ?? then
// reject but leave in queue
??(u)? ??
Algorithm 1: Our clustering phase.
current best target v?(u) and best score ??(u), using
a priority queue. At each step of the algorithm, we
pop the u with the current best ??(u), recompute
its scores, and then merge it with v?(u) if doing so
would improve the model posterior. In an exact al-
gorithm, we would then need to recompute most of
the other scores, since merging u and v?(u) affects
other words for which u and v?(u) are candidates,
and also words for which they appear in the context
set. However, recomputing all these scores would be
extremely time-consuming.3 Therefore, we recom-
pute scores for only those words where the previous
best merger was either u or v?(u). (If the best merge
would not improve the probability, we reject it, but
since its score might increase if we merge v?(u), we
leave u in the queue, setting its ? score to ??; this
score will be updated if we merge v?(u).)
Since we recompute the exact scores ?(u, v) im-
mediately before merging u, the algorithm is guaran-
3The transducer scores can be cached since they depend only
on surface forms, but the language model scores cannot.
teed never to reduce the posterior probability. It can
potentially make changes in the wrong order, since
not all the ?s are recomputed in each step, but most
changes do not affect one another, so performing
them out of order has no impact. Empirically, we
find that mutually exclusive changes (usually of the
form (u, v) and (v, w)) tend to differ enough in initial
score that they are evaluated in the correct order.
4.3 Training the transducer
To train the transducer on a set of mappings between
surface and intended forms, we find the maximum-
probability state sequence for each mapping (another
application of Viterbi EM) and extract features for
each state and its output. Learning weights is then
a maximum-entropy problem, which we solve using
Orthant-wise Limited-memory Quasi-Newton.4
To construct our initial transducer, we first learn
weights for the marginal distribution on surface
sounds by training the max-ent system with only the
bias features active. Next, we manually set weights
(Table 1) for insertions and deletions, which do not
appear on the surface, and for faithfulness features.
Other features get an initial weight of 0.
5 Experiments
5.1 Dataset
Our corpus is a processed version of the Bernstein-
Ratner corpus (Bernstein-Ratner, 1987) from
CHILDES (MacWhinney, 2000), which contains or-
thographic transcriptions of parent-child dyads with
infants aged 13-23 months. Brent and Cartwright
(1996) created a phonemic version of this corpus
by extracting all infant-directed utterances and con-
verted them to a phonemic transcription using a dic-
tionary. This version, which contains 9790 utterances
(33399 tokens, 1321 types), is now standard for word
segmentation, but contains no phonetic variability.
Since producing a close phonetic transcription of
this data would be impractical, we instead construct
an approximate phonetic version using information
from the Buckeye corpus (Pitt et al, 2007). Buckeye
is a corpus of adult-directed conversational Ameri-
can English, and has been phonetically transcribed
4We use the implementation of Andrew and Gao (2007) with
an l2 regularizer and regularization parameter r = 1; although
this could be tuned, in practice it has little effect on results.
189
Feature Weight
output-is-x marginal p(x)
output-is- 0
same-sound 5
same-{place,voice, manner} 2
insertion -3
Table 1: Initial transducer weights.
?about? ahbawt:15, bawt:9, ihbawt:4, ahbawd:4, ih-
bawd:4, ahbaat:2, baw:1, ahbaht:1, erbawd:1,
bawd:1, ahbaad:1, ahpaat:1, bah:1, baht:1,
ah:1, ahbahd:1, ehbaat:1, ahbaed:1, ihbaht:1,
baot:1
?wanna? waanah:94, waanih:37, wahnah:16, waan:13,
wahneh:8, wahnih:5, wahney:3, waanlih:3,
wehnih:2, waaneh:2, waonih:2, waaah:1,
wuhnih:1, wahn:1, waantah:1, waanaa:1,
wowiy:1, waaih:1, wah:1, waaniy:1
Table 2: Empirical distribution of pronunciations of
?about? and ?wanna? in our dataset.
by hand to indicate realistic pronunciation variability.
To create our phonetic corpus, we replace each phone-
mic word in the Bernstein-Ratner-Brent corpus with
a phonetic pronunciation of that word sampled from
the empirical distribution of pronunciations in Buck-
eye (Table 2). If the word never occurs in Buckeye,
we use the original phonemic version.
Our corpus is not completely realistic as a sam-
ple of child-directed speech. Since each pronuncia-
tion is sampled independently, it lacks coarticulation
and prosodic effects, and the distribution of pronun-
ciations is derived from adult-directed rather than
child-directed speech. Nonetheless, it represents pho-
netic variability more realistically than the Bernstein-
Ratner-Brent corpus, while still maintaining the lexi-
cal characteristics of infant-directed speech (as com-
pared to the Buckeye corpus, with its much larger
vocabulary and more complex language model).
We conduct our development experiments on the
first 8000 input utterances, holding out the remain-
ing 1790 for evaluation. For evaluation experiments,
we run the system on all 9790 utterances, reporting
scores on only the last 1790.
5.2 Metrics
We evaluate our results by generalizing the three
segmentation metrics from Goldwater et al (2009):
word boundary F-score, word token F-score, and
lexicon (word type) F-score.
0 1 2 3 4 5Iteration
75
76
77
78
79
80
81
82
Token F
Lexicon F
Figure 5: System scores over 5 iterations.
In our first set of experiments we evaluate how
well our system clusters together surface forms de-
rived from the same intended form, assuming gold
standard word boundaries. We do not evaluate the
induced intended forms directly against the gold stan-
dard intended forms?we want to evaluate cluster
memberships and not labels. Instead we compute
a one-to-one mapping between our induced lexical
items and the gold standard, maximizing the agree-
ment between the two (Haghighi and Klein, 2006).
Using this mapping, we compute mapped token F-
score5 and lexicon F-score.
In our second set of experiments, we use unknown
word boundaries and evaluate the segmentations. We
report the standard word boundary F and unlabeled
word token F as well as mapped F. The unlabeled to-
ken score counts correctly segmented tokens, whether
assigned a correct intended form or not.
5.3 Known word boundaries
We first run our system with known word boundaries
(Table 3). As a baseline, we treat every surface token
as its own intended form (none). This baseline has
fairly high accuracy; 65% of word tokens receive
the most common pronunciation for their intended
form.6 As an upper bound, we find the best intended
form for each surface type (type ubound). This cor-
rectly resolves 91% of tokens; the remaining error is
due to homophones (surface types corresponding to
more than one intended form). We also test our sys-
5When using the gold word boundaries, the precision and
recall are equal and this is is the same as the accuracy; in seg-
mentation experiments the two differ, because with fewer seg-
mentation boundaries, the system proposes fewer tokens. Only
correctly segmented tokens which are also mapped to the correct
form count as matches.
6The lexicon recall is not quite 100% because one rare word
appears only as a homophone of another word.
190
System Tok F Lex P Lex R Lex F
none 65.4 50.2 99.7 66.7
initializer 75.2 83.2 73.3 78.0
system 79.2 87.1 75.9 81.1
oracle trans. 82.7 88.7 83.8 86.2
type ubound 91.0 97.5 98.0 97.7
Table 3: Results on 1790 utterances (known boundaries).
Boundaries Unlabeled Tokens
P R F P R F
no var. 90.1 80.3 84.9 74.5 68.7 71.5
w/var. 70.4 93.5 80.3 56.5 69.7 62.4
Table 4: Degradation in dpseg segmentation perfor-
mance caused by pronunciation variation.
Mapped Tokens Lexicon (types)
P R F P R F
none 39.8 49.0 43.9 37.7 49.1 42.6
init 42.2 52.0 56.5 50.1 40.8 45.0
sys 44.2 54.5 48.8 48.6 43.1 45.7
Table 5: Results on 1790 utterances (induced boundaries).
tem using an oracle transducer (oracle trans.)?the
transducer estimated from the upper-bound mapping.
This scores 83%, showing that our articulatory fea-
ture set captures most, but not all, of the available
information. At the beginning of bootstrapping, our
system (init) scores 75%, but this improves to 79%
after five iterations of reestimation (system). Most
learning occurs in the first two or three iterations
(Figure 5).
To determine the importance of different parts of
our system, we run a few ablation tests on develop-
ment data. Context information is critical to obtain
a good solution; setting ?L and ?R to 0 lowers our
dev token F-score from 83% to 75%. Initializing
all feature weights to 0 yields a poor initial solution
(18% dev token F instead of 75%), but after learn-
ing the result is only slightly lower than using the
weights in Table 1 (78% rather than 80%), showing
that the system is quite robust to initialization.
5.4 Unknown word boundaries
As a simple extension of our model to the case of
unknown word boundaries, we interleave it with an
existing model of word segmentation, dpseg (Gold-
water et al, 2009).7 In each iteration, we run the
segmenter, then bootstrap our model for five itera-
tions on the segmented output. We then concatenate
the intended word sequence proposed by our model
to produce the next iteration?s segmenter input.
Phonetic variation is known to reduce the perfor-
mance of dpseg (Fleck, 2008; Boruta et al, 2011)
and our experiments confirm this (Table 4). Using
induced word boundaries also makes it harder to
recover the lexicon (Table 5), lowering the baseline
F-score from 67% to 43%. Nevertheless, our system
improves the lexicon F-score to 46%, with token F
rising from 44% to 49%, demonstrating the system?s
ability to work without gold word boundaries. Un-
fortunately, performing multiple iterations between
the segmenter and lexical-phonetic learner has little
further effect; we hope to address this issue in future.
6 Conclusion
We have presented a noisy-channel model that si-
multaneously learns a lexicon, a bigram language
model, and a model of phonetic variation, while us-
ing only the noisy surface forms as training data.
It is the first model of lexical-phonetic acquisition
to include word-level context and to be tested on an
infant-directed corpus with realistic phonetic variabil-
ity. Whether trained using gold standard or automati-
cally induced word boundaries, the model recovers
lexical items more effectively than a system that as-
sumes no phonetic variability; moreover, the use of
word-level context is key to the model?s success. Ul-
timately, we hope to extend the model to jointly infer
word boundaries along with lexical-phonetic knowl-
edge, and to work directly from acoustic input. How-
ever, we have already shown that lexical-phonetic
learning from a broad-coverage corpus is possible,
supporting the claim that infants acquire lexical and
phonetic knowledge simultaneously.
Acknowledgements
This work was supported by EPSRC grant
EP/H050442/1 to the second author.
7dpseg1.2 from http://homepages.inf.ed.ac.
uk/sgwater/resources.html
191
References
Guillaume Aimetti. 2009. Modelling early language
acquisition skills: Towards a general statistical learning
mechanism. In Proceedings of the Student Research
Workshop at EACL.
Armen Allahverdyan and Aram Galstyan. 2011. Compar-
ative analysis of Viterbi training and ML estimation for
HMMs. In Advances in Neural Information Processing
Systems (NIPS).
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of L1-regularized log-linear models. In ICML ?07.
Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert
Mercer. 1980. Language-model/acoustic-channel-
model balance mechanism. Technical disclosure bul-
letin Vol. 23, No. 7b, IBM, December.
Nan Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
L. Boruta, S. Peperkamp, B. Crabbe?, E. Dupoux, et al
2011. Testing the robustness of online word segmenta-
tion: effects of linguistic diversity and phonetic varia-
tion. ACL HLT 2011, page 1.
Michael Brent and Timothy Cartwright. 1996. Distribu-
tional regularity and phonotactic constraints are useful
for segmentation. Cognition, 61:93?125.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105, February.
R. Daland and J.B. Pierrehumbert. 2010. Learning
diphone-based segmentation. Cognitive Science.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Joris Driesen, Louis ten Bosch, and Hugo Van hamme.
2009. Adaptive non-negative matrix factorization in
a computational model of language acquisition. In
Proceedings of Interspeech.
E. Dupoux, G. Beraud-Sudreau, and S. Sagayama. 2011.
Templatic features for modeling phoneme acquisition.
In Proceedings of the 33rd Annual Cognitive Science
Society.
Naomi Feldman, Thomas Griffiths, and James Morgan.
2009. Learning phonetic categories by learning a lexi-
con. In Proceedings of the 31st Annual Conference of
the Cognitive Science Society (CogSci).
Naomi Feldman. 2011. Interactions between word and
speech sound categorization in language acquisition.
Ph.D. thesis, Brown University.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of ACL-08: HLT, pages
130?138, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Sharon Goldwater and Mark Johnson. 2003. Learning OT
constraint rankings using a maximum entropy model.
In J. Spenader, A. Eriksson, and Osten Dahl, editors,
Proceedings of the Stockholm Workshop on Variation
within Optimality Theory, pages 111?120, Stockholm.
Stockholm University.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006. Interpolating between types and tokens by esti-
mating power-law generators. In Advances in Neural
Information Processing Systems (NIPS) 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. In In 46th
Annual Meeting of the ACL, pages 398?406.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39(3):379?440.
Bruce Hayes. 2011. Introductory Phonology. John Wiley
and Sons.
A. Jansen, K. Church, and H. Hermansky. 2010. Towards
spoken term discovery at scale with zero resources. In
Proceedings of Interspeech, pages 1676?1679.
R. Kneser and H. Ney. 1995. Improved backing-off for M-
gram language modeling. In Proc. ICASSP ?95, pages
181?184, Detroit, MI, May.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Vol 2: The Database. Lawrence
Erlbaum Associates, Mahwah, NJ, 3rd edition.
Fergus R. McInnes and Sharon Goldwater. 2011. Un-
supervised extraction of recurring words from infant-
directed speech. In Proceedings of the 33rd Annual
Conference of the Cognitive Science Society.
A. S. Park and J. R. Glass. 2008. Unsupervised pat-
tern discovery in speech. IEEE Transactions on Audio,
Speech and Language Processing, 16:186?197.
192
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their ap-
plication to human language processing. In HLT.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech (2nd release).
Okko Ra?sa?nen. 2011. A computational model of word
segmentation from continuous speech using transitional
probabilities of atomic acoustic events. Cognition,
120(2):28.
Anton Rytting. 2007. Preserving Subsegmental Varia-
tion in Modeling Word Segmentation (Or, the Raising
of Baby Mondegreen). Ph.D. thesis, The Ohio State
University.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D. Manning. 2010. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 9?17, Up-
psala, Sweden, July. Association for Computational
Linguistics.
D. Swingley. 2005. Statistical clustering and the contents
of the infant vocabulary. Cognitive Psychology, 50:86?
132.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 985?992, Sydney,
Australia, July. Association for Computational Linguis-
tics.
G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker, and
S. Amano. 2007. Unsupervised learning of vowel
categories from infant-directed speech. Proceedings
of the National Academy of Sciences, 104(33):13273?
13278.
B. Varadarajan, S. Khudanpur, and E. Dupoux. 2008. Un-
supervised learning of acoustic sub-word units. In Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics on Human Language
Technologies: Short Papers, pages 165?168. Associa-
tion for Computational Linguistics.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
193
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 13?24,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Representation Learning for Text-level Discourse Parsing
Yangfeng Ji
School of Interactive Computing
Georgia Institute of Technology
jiyfeng@gatech.edu
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Abstract
Text-level discourse parsing is notoriously
difficult, as distinctions between discourse
relations require subtle semantic judg-
ments that are not easily captured using
standard features. In this paper, we present
a representation learning approach, in
which we transform surface features into
a latent space that facilitates RST dis-
course parsing. By combining the machin-
ery of large-margin transition-based struc-
tured prediction with representation learn-
ing, our method jointly learns to parse dis-
course while at the same time learning a
discourse-driven projection of surface fea-
tures. The resulting shift-reduce discourse
parser obtains substantial improvements
over the previous state-of-the-art in pre-
dicting relations and nuclearity on the RST
Treebank.
1 Introduction
Discourse structure describes the high-level or-
ganization of text or speech. It is central to
a number of high-impact applications, such as
text summarization (Louis et al, 2010), senti-
ment analysis (Voll and Taboada, 2007; Somasun-
daran et al, 2009), question answering (Ferrucci
et al, 2010), and automatic evaluation of student
writing (Miltsakaki and Kukich, 2004; Burstein
et al, 2013). Hierarchical discourse representa-
tions such as Rhetorical Structure Theory (RST)
are particularly useful because of the computa-
tional applicability of tree-shaped discourse struc-
tures (Taboada and Mann, 2006), as shown in Fig-
ure 1.
Unfortunately, the performance of discourse
parsing is still relatively weak: the state-of-the-art
F-measure for text-level relation detection in the
RST Treebank is only slightly above 55% (Joty
when profit was $107.8 million on sales of $435.5 million.
The projections are in the neighborhood of 50 cents a share to 75 cents, compared with a restated $1.65 a share a year earlier,
CIRCUMSTANCE
COMPARISON
Figure 1: An example of RST discourse structure.
et al, 2013). While recent work has introduced
increasingly powerful features (Feng and Hirst,
2012) and inference techniques (Joty et al, 2013),
discourse relations remain hard to detect, due in
part to a long tail of ?alternative lexicalizations?
that can be used to realize each relation (Prasad et
al., 2010). Surface and syntactic features are not
capable of capturing what are fundamentally se-
mantic distinctions, particularly in the face of rel-
atively small annotated training sets.
In this paper, we present a representation learn-
ing approach to discourse parsing. The core idea
of our work is to learn a transformation from a
bag-of-words surface representation into a latent
space in which discourse relations are easily iden-
tifiable. The latent representation for each dis-
course unit can be viewed as a discriminatively-
trained vector-space representation of its meaning.
Alternatively, our approach can be seen as a non-
linear learning algorithm for incremental struc-
ture prediction, which overcomes feature sparsity
through effective parameter tying. We consider
several alternative methods for transforming the
original features, corresponding to different ideas
of the meaning and role of the latent representa-
tion.
Our method is implemented as a shift-reduce
discourse parser (Marcu, 1999; Sagae, 2009).
Learning is performed as large-margin transition-
based structure prediction (Taskar et al, 2003),
while at the same time jointly learning to project
the surface representation into latent space. The
13
resulting system strongly outperforms the prior
state-of-the-art at labeled F-measure, obtaining
raw improvements of roughly 6% on relation la-
bels and 2.5% on nuclearity. In addition, we show
that the latent representation coheres well with the
characterization of discourse connectives in the
Penn Discourse Treebank (Prasad et al, 2008).
2 Model
The core idea of this paper is to project lexical fea-
tures into a latent space that facilitates discourse
parsing. In this way, we can capture the meaning
of each discourse unit, without suffering from the
very high dimensionality of a lexical representa-
tion. While such feature learning approaches have
proven to increase robustness for parsing, POS
tagging, and NER (Miller et al, 2004; Koo et al,
2008; Turian et al, 2010), they would seem to
have an especially promising role for discourse,
where training data is relatively sparse and ambi-
guity is considerable. Prasad et al (2010) show
that there is a long tail of alternative lexicalizations
for discourse relations in the Penn Discourse Tree-
bank, posing obvious challenges for approaches
based on directly matching lexical features ob-
served in the training data.
Based on this observation, our goal is to learn
a function that transforms lexical features into
a much lower-dimensional latent representation,
while simultaneously learning to predict discourse
structure based on this latent representation. In
this paper, we consider a simple transformation
function, linear projection. Thus, we name the ap-
proach DPLP: Discourse Parsing from Linear Pro-
jection. We apply transition-based (incremental)
structured prediction to obtain a discourse parse,
training a predictor to make the correct incremen-
tal moves to match the annotations of training data
in the RST Treebank. This supervision signal is
then used to learn both the weights and the projec-
tion matrix in a large-margin framework.
2.1 Shift-reduce discourse parsing
We construct RST Trees using shift-reduce pars-
ing, as first proposed by Marcu (1999). At each
point in the parsing process, we maintain a stack
and a queue; initially the stack is empty and the
first elementary discourse unit (EDU) in the docu-
ment is at the front of the queue.
1
The parser can
1
We do not address segmentation of text into elemen-
tary discourse units in this paper. Standard classification-
Notation Explanation
V Vocabulary for surface features
V Size of V
K Dimension of latent space
w
m
Classification weights for class m
C Total number of classes, which correspond to
possible shift-reduce operations
A Parameter of the representation function (also
the projection matrix in the linear representa-
tion function)
v
i
Word count vector of discourse unit i
v Vertical concatenation of word count vectors
for the three discourse units currently being
considered by the parser
? Regularization for classification weights
? Regularization for projection matrix
?
i
Slack variable for sample i
?
i,m
Dual variable for sample i and class m
?
t
Learning rate at iteration t
Table 1: Summary of mathematical notation
then choose either to shift the front of the queue
onto the top of the stack, or to reduce the top two
elements on the stack in a discourse relation. The
reduction operation must choose both the type of
relation and which element will be the nucleus.
So, overall there are multiple reduce operations
with specific relation types and nucleus positions.
Shift-reduce parsing can be learned as a classifi-
cation task, where the classifier uses features of
the elements in the stack and queue to decide what
move to take. Previous work has employed deci-
sion trees (Marcu, 1999) and the averaged percep-
tron (Collins and Roark, 2004; Sagae, 2009) for
this purpose. Instead, we employ a large-margin
classifier, because we can compute derivatives of
the margin-based objective function with respect
to both the classifier weights as well as the projec-
tion matrix.
2.2 Discourse parsing with projected features
More formally, we denote the surface feature vo-
cabulary V , and represent each EDU as the nu-
meric vector v ? N
V
, where V = #|V| and the n-
th element of v is the count of the n-th surface fea-
ture in this EDU (see Table 1 for a summary of no-
tation). During shift-reduce parsing, we consider
features of three EDUs:
2
the top two elements on
based approaches can achieve a segmentation F-measure
of 94% (Hernault et al, 2010); a more complex rerank-
ing model does slightly better, at 95% F-Measure with
automatically-generated parse trees, and 96.6% with gold an-
notated trees (Xuan Bach et al, 2012). Human agreement
reaches 98% F-Measure.
2
After applying a reduce operation, the stack will include
a span that contains multiple EDUs. We follow the strong
14
the stack (v
1
and v
2
), and the front of the queue
(v
3
). The vertical concatenation of these vectors
is denoted v = [v
1
;v
2
;v
3
]. In general, we can
formulate the decision function for the multi-class
shift-reduce classifier as
m? = argmax
m?{1,...,C}
w
>
m
f(v;A) (1)
where w
m
is the weight for the m-th class
and f(v;A) is the representation function
parametrized by A. The score for class m (in
our case, the value of taking the m-th shift-
reduce operation) is computed by the inner prod-
uct w
>
m
f(v;A). The specific shift-reduce opera-
tion is chosen by maximizing the decision value in
Equation 1.
The representation function f(v;A) can be de-
fined in any form; for example, it could be a non-
linear function defined by a neural network model
parametrized by A. We focus on the linear projec-
tion,
f(v;A) = Av, (2)
where A ? R
K?3V
is projects the surface repre-
sentation v of three EDUs into a latent space of
size K  V .
Note that by setting
?
w
>
m
= w
>
m
A, the decision
scoring function can be rewritten as
?
w
>
m
v, which
is linear in the original surface features. Therefore,
the expressiveness of DPLP is identical to a linear
separator in the original feature space. However,
the learning problem is considerably different. If
there are C total classes (possible shift-reduce op-
erations), then a linear classifier must learn 3V C
parameters, while DPLP must learn (3V + C)K
parameters, which will be smaller under the as-
sumption that K < C  V . This can be seen
as a form of parameter tying on the linear weights
?
w
m
, which allows statistical strength to be shared
across training instances. We will consider special
cases of A that reduce the parameter space still
further.
2.3 Special forms of the projection matrix
We consider three different constructions for the
projection matrix A.
? General form: In the general case, we place
compositionality criterion of Marcu (1996) and consider only
the nuclear EDU of the span. Later work may explore the
composition of features between the nucleus and satellite.
no special constraint on the form of A.
f(v;A) = A
?
?
v
1
v
2
v
3
?
?
(3)
This form is shown in Figure 2(a).
? Concatenation form: In the concatenation
form, we choose a block structure for A, in
which a single projection matrix B is applied
to each EDU:
f(v;A) =
[
B 0 0
0 B 0
0 0 B
][
v
1
v
2
v
3
]
(4)
In this form, we transform the representa-
tion of each EDU separately, but do not at-
tempt to represent interrelationships between
the EDUs in the latent space. The number
of parameters in A is
1
3
KV . Then, the total
number of parameters, including the decision
weights {w
m
}, in this form is (
V
3
+ C)K.
? Difference form. In the difference form, we
explicitly represent the differences between
adjacent EDUs, by constructing A as a block
difference matrix,
f(v;A) =
[
C ?C 0
C 0 ?C
0 0 0
][
v
1
v
2
v
3
]
, (5)
The result of this projection is that the la-
tent representation has the form [C(v
1
?
v
2
);C(v
1
?v
3
)], representing the difference
between the top two EDUs on the stack, and
between the top EDU on the stack and the
first EDU in the queue. This is intended
to capture semantic similarity, so that reduc-
tions between related EDUs will be preferred.
Similarly, the total number of parameters to
estimate in this form is (V + 2C)
K
3
.
3 Large-Margin Learning Framework
We apply a large margin structure prediction ap-
proach to train the model. There are two pa-
rameters that need to be learned: the classifica-
tion weights {w
m
}, and the projection matrix A.
As we will see, it is possible to learn {w
m
} us-
ing standard support vector machine (SVM) train-
ing (holding A fixed), and then make a simple
gradient-based update to A (holding {w
m
} fixed).
By interleaving these two operations, we arrive at
a saddle point of the objective function.
15
A W
y
v1 from stack v 2 from stack v 3 from queue
(a) General form
A W
y
v1 from stack v 2 from stack v 3 from queue
(b) Concatenation form
A W
y
v1 from stack v 2 from stack v 3 from queue
(c) Difference form
Figure 2: Decision problem with different representation functions
Specifically, we formulate the following con-
strained optimization problem,
min
{w
1:C
,?
1:l
,A}
?
2
C
?
m=1
?w
m
?
2
2
+
l
?
i=1
?
i
+
?
2
?A?
2
F
s.t. (w
y
i
?w
m
)
>
f(v
i
;A) ? 1? ?
y
i
=m
? ?
i
,
? i,m
(6)
where m ? {1, . . . , C} is the index of the
shift-reduce decision taken by the classifier (e.g.,
SHIFT, REDUCE-CONTRAST-RIGHT, etc), i ?
{1, ? ? ? , l} is the index of the training sample, and
w
m
is the vector of classification weights for class
m. The slack variables ?
i
permit the margin con-
straint to be violated in exchange for a penalty, and
the delta function ?
y
i
=m
is unity if y
i
= m, and
zero otherwise.
As is standard in the multi-class linear
SVM (Crammer and Singer, 2001), we can solve
the problem defined in Equation 6 via Lagrangian
optimization:
L({w
1:C
, ?
1:l
,A, ?
1:l,1:C
}) =
?
2
C
?
m=1
?w
m
?
2
2
+
l
?
i=1
?
i
+
?
2
?A?
2
F
+
?
i,m
?
i,m
{
(w
>
m
?w
>
y
i
)f(v
i
;A) + 1? ?
y
i
=m
? ?
i
}
s.t. ?
i,m
? 0 ?i,m
(7)
Then, to optimize L, we need to find a saddle
point, which would be the minimum for the vari-
ables {w
1:C
, ?
1:l
} and the projection matrix A,
and the maximum for the dual variables {?
1:l,1:C
}.
If A is fixed, then the optimization problem is
equivalent to a standard multi-class SVM, in the
transformed feature space f(v
i
;A). We can obtain
the weights {w
1:C
} and dual variables {?
1:l,1:C
}
from a standard dual-form SVM solver. We then
update A, recompute {w
1:C
} and {?
1:l,1:C
}, and
iterate until convergence. This iterative procedure
is similar to the latent variable structural SVM (Yu
and Joachims, 2009), although the specific details
of our learning algorithm are different.
3.1 Learning Projection Matrix A
We update A while holding fixed the weights and
dual variables. The derivative of L with respect to
A is
?L
?A
= ?A+
?
i,m
?
i,m
(w
>
m
?w
>
y
i
)
?f(v
i
;A)
?A
= ?A+
?
i,m
?
i,m
(w
m
?w
y
i
)v
i
>
(8)
Setting
?L
?A
= 0, we have the closed-form solution,
A =
1
?
?
i,m
?
i,m
(w
m
?w
y
i
)v
i
>
=
1
?
?
i,j
(w
y
i
?
?
m
?
i,m
w
m
)v
i
>
,
(9)
because the dual variables for each instance must
sum to one,
?
m
?
i,m
= 1.
Note that for a given i, the matrix (w
y
i
?
?
m
?
i,m
w
m
)v
i
>
is of (at most) rank-1. There-
fore, the solution of A can be viewed as the lin-
ear combination of a sequence of rank-1 matrices,
where each rank-1 matrix is defined by distribu-
tional representation v
i
and the weight difference
between the weight of true label w
y
i
and the ?ex-
pected? weight
?
m
?
i,m
w
m
.
One property of the dual variables is that
f(v
i
;A) is a support vector only if the dual vari-
able ?
i,y
i
< 1. Since the dual variables for each
instance are guaranteed to sum to one, we have
w
y
i
?
?
m
?
i,m
w
m
= 0 if ?
i,y
i
= 1. In other
words, the contribution from non support vectors
to the projection matrix A is 0. Then, we can fur-
ther simplify the updating equation as
A =
1
?
?
v
i
?SV
(w
y
i
?
?
m
?
i,m
w
m
)v
i
>
(10)
This is computationally advantageous since many
instances are not support vectors, and it shows that
the discriminatively-trained projection matrix only
incorporates information from each instance to the
extent that the correct classification receives low
confidence.
16
Algorithm 1 Mini-batch learning algorithm
Input: Training set D, Regularization parame-
ters ? and ? , Number of iteration T , Initializa-
tion matrix A
0
, and Threshold ?
while t = 1, . . . , T do
Randomly choose a subset of training sam-
ples D
t
from D
Train SVM with A
t?1
to obtain {w
(t)
m
} and
{?
(t)
i,m
}
Update A
t
using Equation 11 with ?
t
=
1
t
if
?A
t
?A
t?1
?
F
?A
2
?A
1
?
F
< ? then
Return
end if
end while
Re-train SVM with D and the final A
Output: Projection matrix A, SVM classifier
with weights w
3.2 Gradient-based Learning for A
Solving the quadratic programming defined by the
dual form of the SVM is time-consuming, espe-
cially on a large-scale dataset. But if we focus on
learning the projection matrix A, we can speed up
learning by sampling only a small proportion of
the training data to compute an approximate op-
timum for {w
1:C
, ?
1:l,1:C
}, before each update of
A. This idea is similar to the mini-batch learning,
which has been used in large-scale SVM problem
(Nelakanti et al, 2013) and deep learning models
(Le et al, 2011).
Specifically, in iteration t, the algorithm ran-
domly chooses a subset of training samples D
t
to
train the model. We cannot make a closed-form
update to A based on this small sample, but we
can take an approximate gradient step,
A
t
= (1? ?
t
?)A
t?1
+
?
t
{
?
v
i
?SV(D
t
)
(
w
(t)
y
i
?
?
m
?
(t)
i,m
w
(t)
m
)
v
i
>
}
,
(11)
where ?
t
is a learning rate. In iteration t, we
choose ?
t
=
1
t
. After convergence, we obtain the
weights w by applying the SVM over the entire
dataset, using the final A. The algorithm is sum-
marized in Algorithm 1 and more details about im-
plementation will be clarified in Section 4. While
minibatch learning requires more iterations, the
SVM training is much faster in each batch, and the
overall algorithm is several times faster than using
the entire training set for each update.
4 Implementation
The learning algorithm is applied in a shift-reduce
parser, where the training data consists of the
(unique) list of shift and reduce operations re-
quired to produce the gold RST parses. On test
data, we choose parsing operations in an online
fashion ? at each step, the parsing algorithm
changes the status of the stack and the queue ac-
cording the selected transition, then creates the
next sample with the updated status.
4.1 Parameters and Initialization
There are three free parameters in our approach:
the latent dimension K, and regularization pa-
rameters ? and ? . We consider the values K ?
{30, 60, 90, 150}, ? ? {1, 10, 50, 100} and ? ?
{1.0, 0.1, 0.01, 0.001}, and search over this space
using a development set of thirty document ran-
domly selected from within the RST Treebank
training data. We initialize each element of A
0
to a uniform random value in the range [0, 1]. For
mini-batch learning, we fixed the batch size to be
500 training samples (shift-reduce operations) in
each iteration.
4.2 Additional features
As described thus far, our model considers only
the projected representation of each EDU in its
parsing decisions. But prior work has shown that
other, structural features can provide useful in-
formation (Joty et al, 2013). We therefore aug-
ment our classifier with a set of simple feature
templates. These templates are applied to individ-
ual EDUs, as well as pairs of EDUs: (1) the two
EDUs on top of the stack, and (2) the EDU on top
of the stack and the EDU in front of the queue.
The features are shown in Table 2. In computing
these features, all tokens are downcased, and nu-
merical features are not binned. The dependency
structure and POS tags are obtained from MALT-
Parser (Nivre et al, 2007).
5 Experiments
We evaluate DPLP on the RST Discourse Tree-
bank (Carlson et al, 2001), comparing against
state-of-the-art results. We also investigate the in-
formation encoded by the projection matrix.
5.1 Experimental Setup
Dataset The RST Discourse Treebank (RST-
DT) consists of 385 documents, with 347 for train-
17
Feature Examples
Words at beginning and end of the EDU
?BEGIN-WORD-STACK1 = but?
?BEGIN-WORD-STACK1-QUEUE1 = but, the?
POS tag at beginning and end of the EDU
?BEGIN-TAG-STACK1 = CC?
?BEGIN-TAG-STACK1-QUEUE1 = CC, DT?
Head word set from each EDU. The set includes words
whose parent in the depenency graph is ROOT or is not
within the EDU (Sagae, 2009).
?HEAD-WORDS-STACK2 = working?
Length of EDU in tokens ?LEN-STACK1-STACK2 = ?7, 8??
Distance between EDUs ?DIST-STACK1-QUEUE1 = 2?
Distance from the EDU to the beginning of the document ?DIST-FROM-START-QUEUE1 = 3?
Distance from the EDU to the end of the document ?DIST-FROM-END-STACK1 = 1?
Whether two EDUs are in the same sentence ?SAME-SENT-STACK1-QUEUE1 = True?
Table 2: Additional features for RST parsing
ing and 38 for testing in the standard split. As
we focus on relational discourse parsing, we fol-
low prior work (Feng and Hirst, 2012; Joty et al,
2013), and use gold EDU segmentations. The
strongest automated RST segmentation methods
currently attain 95% accuracy (Xuan Bach et al,
2012).
Preprocessing In the RST-DT, most nodes have
exactly two children, one nucleus and one satellite.
For non-binary relations, we use right-branching
to binarize the tree structure. For multi-nuclear
relations, we choose the left EDU as ?head?
EDU. The vocabulary V includes all unigrams af-
ter down-casing. No other preprocessing is per-
formed. In total, there are 16250 unique unigrams
in V .
Fixed projection matrix baselines Instead of
learning from data, a simple way to obtain a pro-
jection matrix is to use matrix factorization. Re-
cent work has demonstrated the effectiveness of
non-negative matrix factorization (NMF) for mea-
suring distributional similarity (Dinu and Lapata,
2010; Van de Cruys and Apidianaki, 2011). We
can construct B
nmf
in the concatenation form
of the projection matrix by applying NMF to the
EDU-feature matrix, M ?WH. As a result, W
describes each EDU with aK-dimensional vector,
and H describes each word with a K-dimensional
vector. We can then construct B
nmf
by taking
the pseudo-inverse of H, which then projects from
word-count vectors into the latent space.
Another way to construct B is to use neural
word embeddings (Collobert and Weston, 2008).
In this case, we can view the product Bv as a com-
position of the word embeddings, using the simple
additive composition model proposed by Mitchell
and Lapata (2010). We used the word embeddings
from Collobert and Weston (2008) with dimension
{25, 50, 100}. Grid search over heldout training
data was used to select the optimum latent dimen-
sion for both the NMF and word embedding base-
lines. Note that the size K of the resulting projec-
tion matrix is three times the size of the embed-
ding (or NMF representation) due to the concate-
nate construction.
We also consider the special case where A = I.
Competitive systems We compare our approach
with HILDA (Hernault et al, 2010) and TSP (Joty
et al, 2013). Joty et al (2013) proposed two dif-
ferent approaches to combine sentence-level pars-
ing models: sliding windows (TSP SW) and 1
sentence-1 subtree (TSP 1-1). In the comparison,
we report the results of both approaches. All re-
sults are based on the same gold standard EDU
segmentation. We cannot compare with the re-
sults of Feng and Hirst (2012), because they do
not evaluate on the overall discourse structure, but
rather treat each relation as an individual classifi-
cation problem.
Metrics To evaluate the parsing performance,
we use the three standard ways to measure the per-
formance: unlabeled (i.e., hierarchical spans) and
labeled (i.e., nuclearity and relation) F-score, as
defined by Black et al (1991). The application
of this approach to RST parsing is described by
Marcu (2000b).
3
To compare with previous works
on RST-DT, we use the 18 coarse-grained relations
defined in (Carlson et al, 2001).
3
We implemented the evaluation metrics by ourselves.
Together with the DPLP system, all codes are published on
https://github.com/jiyfeng/DPLP
18
Method Matrix Form +Features K Span Nuclearity Relation
Prior work
1. HILDA (Hernault et al, 2010) 83.0 68.4 54.8
2. TSP 1-1 (Joty et al, 2013) 82.47 68.43 55.73
3. TSP SW (Joty et al, 2013) 82.74 68.40 55.71
Our work
4. Basic features A = 0 Yes 79.43 67.98 52.96
5. Word embeddings Concatenation No 75 75.28 67.14 53.79
6. NMF Concatenation No 150 78.57 67.66 54.80
7. Bag-of-words A = I Yes 79.85 69.01 60.21
8. DPLP Concatenation No 60 80.91 69.39 58.96
9. DPLP Difference No 60 80.47 68.61 58.27
10. DPLP Concatenation Yes 60 82.08 71.13 61.63
11. DPLP General Yes 30 81.60 70.95 61.75
Human annotation 88.70 77.72 65.75
Table 3: Parsing results of different models on the RST-DT test set. The results of TSP and HILDA are
reprinted from prior work (Joty et al, 2013; Hernault et al, 2010).
5.2 Experimental Results
Table 3 presents RST parsing results for DPLP and
some alternative systems. All versions
of DPLP outperform the prior state-of-the-art
on nuclearity and relation detection. This includes
relatively simple systems whose features are
simply a projection of the word count vectors
for each EDU (lines 7 and 8). The addition of
the features from Table 2 improves performance
further, leading to absolute F-score improvement
of around 2.5% in nuclearity and 6% in relation
prediction (lines 9 and 10).
On span detection, DPLP performs slightly
worse than the prior state-of-the-art. These sys-
tems employ richer syntactic and contextual fea-
tures, which might be especially helpful for span
identification. As shown by line 4 of the re-
sults table, the basic features from Table 2 pro-
vide most of the predictive power for spans; how-
ever, these features are inadequate at the more
semantically-oriented tasks of nuclearity and re-
lation prediction, which benefit substantially from
the projected features. Since correctly identifying
spans is a precondition for nuclearity and relation
prediction, we might obtain still better results by
combining features from HILDA and TSP with the
representation learning approach described here.
Lines 5 and 6 show that discriminative learning
of the projection matrix is crucial, as fixed projec-
tions obtained from NMF or neural word embed-
dings perform substantially worse. Line 7 shows
that the original bag-of-words representation to-
gether with basic features could give us some ben-
efit on discourse parsing, but still not as good as
results from DPLP. From lines 8 and 9, we see
that the concatenation construction is superior to
the difference construction, but the comparison
between lines 10 and 11 is inconclusive on the
merits of the general form of A. This suggests
that using the projection matrix to model interre-
lationships between EDUs does not substantially
improve performance, and the simpler concatena-
tion construction may be preferred.
Figure 3 shows how performance changes for
different latent dimensions K. At each value of
K, we employ grid search over a development set
to identify the optimal regularizers ? and ? . For
the concatenation construction, performance is not
overly sensitive to K. For the general form of A,
performance decreases with large K. Recall from
Section 2.3 that this construction has nine times as
many parameters as the concatenation form; with
large values of K, it is likely to overfit.
5.3 Analysis of Projection Matrix
Why does projection of the surface features im-
prove discourse parsing? To answer this question,
we examine what information the projection ma-
trix is learning to encoded. We take the projec-
tion matrix from the concatenation construction
and K = 60 as an example for case study. Re-
calling the definition in equation 4, the projection
matrix A will be composed of three identical sub-
matrices B ? R
20?V
. The columns of the B ma-
trix can be viewed as 20-dimensional descriptors
of the words in the vocabulary.
For the purpose of visualization, we further re-
duce the dimension of latent representation from
K = 20 to 2 dimensions using t-SNE (van der
Maaten and Hinton, 2008). One further simpli-
19
30 60 90 150K76
77
78
79
80
81
82
83
84
F-sco
re
Concatenation DPLPGeneral DPLPTSP 1-1 (Joty, et al, 2013)HILDA (Hernault, et al, 2010)
(a) Span
30 60 90 150K65
66
67
68
69
70
71
72
F-sco
re
Concatenation DPLPGeneral DPLPTSP 1-1 (Joty, et al, 2013)HILDA (Hernault, et al, 2010)
(b) Nuclearity
30 60 90 150K
50
52
54
56
58
60
62
F-sco
re
Concatenation DPLPGeneral DPLPTSP 1-1 (Joty, et al, 2013)HILDA (Hernault, et al, 2010)
(c) Relation
Figure 3: The performance of our parser over different latent dimension K. Results for DPLP include
the additional features from Table 3
fication for visualization is we consider only the
top 1000 frequent unigrams in the RST-DT train-
ing set. For comparison, we also apply t-SNE to
the projection matrix B
nmf
recovered from non-
negative matrix factorization.
Figure 4 highlights words that are related to dis-
course analysis. Among the top 1000 words, we
highlight the words from 5 major discourse con-
nective categories provided in Appendix B of the
PDTB annotation manual (Prasad et al, 2008):
CONJUNCTION, CONTRAST, PRECEDENCE, RE-
SULT, and SUCCESSION. In addition, we also
highlighted two verb categories from the top 1000
words: modal verbs and reporting verbs, with their
inflections (Krestel et al, 2008).
From the figure, it is clear DPLP has learned a
projection matrix that successfully groups several
major discourse-related word classes: particularly
modal and reporting verbs; it has also grouped
succession and precedence connectives with some
success. In contrast, while NMF does obtain com-
pact clusters of words, these clusters appear to be
completely unrelated to discourse function of the
words that they include. This demonstrates the
value of using discriminative training to obtain the
transformed representation of the discourse units.
6 Related Work
Early work on document-level discourse parsing
applied hand-crafted rules and heuristics to build
trees in the framework of Rhetorical Structure
Theory (Sumita et al, 1992; Corston-Oliver, 1998;
Marcu, 2000a). An early data-driven approach
was offered by Schilder (2002), who used distribu-
tional techniques to rate the topicality of each dis-
course unit, and then chose among underspecified
discourse structures by placing more topical sen-
tences near the root. Learning-based approaches
were first applied to identify within-sentence dis-
course relations (Soricut and Marcu, 2003), and
only later to cross-sentence relations at the docu-
ment level (Baldridge and Lascarides, 2005). Of
particular relevance to our inference technique are
incremental discourse parsing approaches, such
as shift-reduce (Sagae, 2009) and A* (Muller et
al., 2012). Prior learning-based work has largely
focused on lexical, syntactic, and structural fea-
tures, but the close relationship between discourse
structure and semantics (Forbes-Riley et al, 2006)
suggests that shallow feature sets may struggle
to capture the long tail of alternative lexicaliza-
tions that can be used to realize discourse rela-
tions (Prasad et al, 2010; Marcu and Echihabi,
2002). Only Subba and Di Eugenio (2009) incor-
porate rich compositional semantics into discourse
parsing, but due to the ambiguity of their seman-
tic parser, they must manually select the correct
semantic parse from a forest of possiblities.
Recent work has succeeded in pushing the state-
of-the-art in RST parsing by innovating on sev-
eral fronts. Feng and Hirst (2012) explore rich
linguistic linguistic features, including lexical se-
mantics and discourse production rules suggested
by Lin et al (2009) in the context of the Penn Dis-
course Treebank (Prasad et al, 2008). Muller et
al. (2012) show that A* decoding can outperform
both greedy and graph-based decoding algorithms.
Joty et al (2013) achieve the best prior results
on RST relation detection by (i) jointly perform-
ing relation detection and classification, (ii) per-
forming bottom-up rather than greedy decoding,
and (iii) distinguishing between intra-sentence and
inter-sentence relations. Our approach is largely
orthogonal to this prior work: we focus on trans-
20
although until howeveralsothough
but
thuslater
cancouldwouldshouldandwhenafter so
once willmight may
beforethen
sayssay reportedsaid sayingbelievethink
mustasked
report
(a) Latent representation of words from projection learning
with K = 20.
butwould whenalso maycan then must
mightoncehoweversothoughthus
although
shouldlateruntilwillbefore aftercould
andsayssaid say
asked saying thinkbelieve
report
ConjunctionContrastPrecedenceResultSuccessionModal verbReporting verb
(b) Latent representation of words from non-negative matrix
factorization with K = 20.
Figure 4: t-SNE Visualization on latent representations of words.
forming the lexical representation of discourse
units into a latent space to facilitate learning. As
shown in Figure 4(a), this projection succeeds
at grouping words with similar discourse func-
tions. We might expect to obtain further improve-
ments by augmenting this representation learning
approach with rich syntactic features (particularly
for span identification), more accurate decoding,
and special treatment of intra-sentence relations;
this is a direction for future research.
Discriminative learning of latent features for
discourse processing can be viewed as a form
of representation learning (Bengio et al, 2013).
Also called Deep Learning, such approaches
have recently been applied in a number of NLP
tasks (Collobert et al, 2011; Socher et al, 2012).
Of particular relevance are applications to the de-
tection of semantic or discourse relations, such
as paraphrase, by comparing sentences in an in-
duced latent space (Socher et al, 2011; Guo and
Diab, 2012; Ji and Eisenstein, 2013). In this work,
we show how discourse structure annotations can
function as a supervision signal to discriminatively
learn a transformation from lexical features to a la-
tent space that is well-suited for discourse parsing.
Unlike much of the prior work on representation
learning, we induce a simple linear transforma-
tion. Extension of our approach by incorporating
a non-linear activation function is a natural topic
for future research.
7 Conclusion
We have presented a framework to perform dis-
course parsing while jointly learning to project to
a low-dimensional representation of the discourse
units. Using the vector-space representation of
EDUs, our shift-reduce parsing system substan-
tially outperforms existing systems on nuclearity
detection and discourse relation identification. By
adding some additional surface features, we ob-
tain further improvements. The low dimensional
representation also captures basic intuitions about
discourse connectives and verbs, as shown in Fig-
ure 4(a).
Deep learning approaches typically apply a
non-linear transformation such as the sigmoid
function (Bengio et al, 2013). We have con-
ducted a few unsuccessful experiments with the
?hard tanh? function proposed by Collobert and
Weston (2008), but a more complete exploration
of non-linear transformations must wait for future
work. Another direction would be more sophis-
ticated composition of the surface features within
each elementary discourse unit, such as the hierar-
chical convolutional neural network (Kalchbren-
ner and Blunsom, 2013) or the recursive tensor
network (Socher et al, 2013). It seems likely that
a better accounting for syntax could improve the
latent representations that our method induces.
Acknowledgments
We thank the reviewers for their helpful feedback,
particularly for the connection to multitask learn-
ing. We also want to thank Kenji Sagae and
Vanessa Wei Feng for the helpful discussion via
email communication. This research was sup-
ported by Google Faculty Research Awards to the
second author.
21
References
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning, pages 96?103.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation Learning: A Review and New
Perspectives. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(8):1798?1828.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Phil Harrison, Don Hin-
dle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991.
A Procedure for Quantitatively Comparing the Syn-
tactic Coverage of English Grammars. In Speech
and Natural Language: Proceedings of a Workshop
Held at Pacific Grove, California, February 19-22,
1991, pages 306?311.
Jill Burstein, Joel Tetreault, and Martin Chodorow.
2013. Holistic discourse coherence annotation
for noisy essay writing. Dialogue & Discourse,
4(2):34?52.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a Discourse-tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Proceedings of Second SIGdial Work-
shop on Discourse and Dialogue.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, page 111. Association for Computa-
tional Linguistics.
R. Collobert and J. Weston. 2008. A Unified Architec-
ture for Natural Language Processing: Deep Neural
Networks with Multitask Learning. In ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural Lan-
guage Processing (Almost) from Scratch. Journal of
Machine Learning Research, 12:2493?2537.
Simon Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization, pages 9?15.
Koby Crammer and Yoram Singer. 2001. On the Algo-
rithmic Implementation of Multiclass Kernel-based
Vector Machines. Journal of Machine Learning Re-
search, 2:265?292.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing Distributional Similarity in Context. In EMNLP,
pages 1162?1172.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistic Features. In
Proceedings of ACL.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al 2010. Building Watson: An overview
of the DeepQA project. AI magazine, 31(3):59?79.
Katherine Forbes-Riley, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23(1):55?
106.
Weiwei Guo and Mona Diab. 2012. Modeling Sen-
tences in the Latent Space. In Proceedings of ACL,
pages 864?872, Jeju Island, Korea, July. Association
for Computational Linguistics.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1?33.
Yangfeng Ji and Jacob Eisenstein. 2013. Discrimina-
tive Improvements to Distributional Sentence Simi-
larity. In EMNLP, pages 891?896, Seattle, Washing-
ton, USA, October. Association for Computational
Linguistics.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of ACL.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of ACL-HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Ralf Krestel, Sabine Bergler, and Ren?e Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In LREC,
Marrakech, Morocco, May. European Language Re-
sources Association (ELRA).
Quoc V. Le, Jiquan Ngiam, Adam Coates, Abhik
Lahiri, Bobby Prochnow, and Andrew Y. Ng. 2011.
On Optimization Methods for Deep Learning. In
ICML.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing Implicit Discourse Relations in the
Penn Discourse Treebank. In EMNLP.
Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147?156. Association for Computa-
tional Linguistics.
22
Daniel Marcu and Abdessamad Echihabi. 2002. An
Unsupervised Approach to Recognizing Discourse
Relations. In Proceedings of ACL, pages 368?375,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Daniel Marcu. 1996. Building Up Rhetorical Structure
Trees. In Proceedings of AAAI.
Daniel Marcu. 1999. A Decision-Based Approach to
Rhetorical Parsing. In Proceedings of ACL, pages
365?372, College Park, Maryland, USA, June. As-
sociation for Computational Linguistics.
Daniel Marcu. 2000a. The Rhetorical Parsing of Un-
restricted Texts: A Surface-based Approach. Com-
putational Linguistics, 26:395?448.
Daniel Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name Tagging with Word Clusters and Dis-
criminative Training. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL, pages
337?342, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Philippe Muller, Stergos Afantenos, Pascal Denis, and
Nicholas Asher. 2012. Constrained Decoding for
Text-Level Discourse Parsing. In Coling, pages
1883?1900, Mumbai, India, December. The COL-
ING 2012 Organizing Committee.
Anil Kumar Nelakanti, Cedric Archambeau, Julien
Mairal, Francis Bach, and Guillaume Bouchard.
2013. Structured Penalties for Log-Linear Lan-
guage Models. In EMNLP, pages 233?243, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
LREC.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Realization of discourse relations by other
means: alternative lexicalizations. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1023?1031. Asso-
ciation for Computational Linguistics.
Kenji Sagae. 2009. Analysis of Discourse Structure
with Syntactic Dependencies and Data-Driven Shift-
Reduce Parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT),
pages 81?84, Paris, France, October. Association for
Computational Linguistics.
Frank Schilder. 2002. Robust discourse parsing via
discourse markers, topicality and position. Natural
Language Engineering, 8(3):235?255.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In NIPS.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In EMNLP.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing using Syntactic and Lexical Infor-
mation. In NAACL.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive Discourse Parser that uses Rich Linguistic In-
formation. In NAACL-HLT, pages 566?574, Boul-
der, Colorado, June. Association for Computational
Linguistics.
K. Sumita, K. Ono, T. Chino, T. Ukita, and S. Amano.
1992. A discourse structure analyzer for Japanese
text. In Proceedings International Conference on
Fifth Generation Computer Systems, pages 1133?
1140.
Maite Taboada and William C Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse stud-
ies, 8(4):567?588.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In NIPS.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representation: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of ACL, pages 384?394.
Tim Van de Cruys and Marianna Apidianaki. 2011.
Latent Semantic Word Sense Induction and Disam-
biguation. In Proceedings of ACL, pages 1476?
1485, Portland, Oregon, USA, June. Association for
Computational Linguistics.
23
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2759?2605, November.
Kimberly Voll and Maite Taboada. 2007. Not all
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. In Pro-
ceedings of Australian Conference on Artificial In-
telligence.
Ngo Xuan Bach, Nguyen Le Minh, and Akira Shimazu.
2012. A Reranking Model for Discourse Segmenta-
tion using Subtree Features. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 160?168.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169?1176.
ACM.
24
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265?271,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
POS induction with distributional and morphological information
using a distance-dependent Chinese restaurant process
Kairit Sirts
Institute of Cybernetics at
Tallinn University of Technology
sirts@ioc.ee
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Micha Elsner
Department of Linguistics
The Ohio State University
melsner0@gmail.com
Sharon Goldwater
ILCC, School of Informatics
University of Edinburgh
sgwater@inf.ed.ac.uk
Abstract
We present a new approach to inducing the
syntactic categories of words, combining
their distributional and morphological prop-
erties in a joint nonparametric Bayesian
model based on the distance-dependent
Chinese Restaurant Process. The prior
distribution over word clusterings uses a
log-linear model of morphological similar-
ity; the likelihood function is the probabil-
ity of generating vector word embeddings.
The weights of the morphology model
are learned jointly while inducing part-of-
speech clusters, encouraging them to co-
here with the distributional features. The
resulting algorithm outperforms competi-
tive alternatives on English POS induction.
1 Introduction
The morphosyntactic function of words is reflected
in two ways: their distributional properties, and
their morphological structure. Each information
source has its own advantages and disadvantages.
Distributional similarity varies smoothly with syn-
tactic function, so that words with similar syntactic
functions should have similar distributional proper-
ties. In contrast, there can be multiple paradigms
for a single morphological inflection (such as past
tense in English). But accurate computation of
distributional similarity requires large amounts of
data, which may not be available for rare words;
morphological rules can be applied to any word
regardless of how often it appears.
These observations suggest that a general ap-
proach to the induction of syntactic categories
should leverage both distributional and morpho-
logical features (Clark, 2003; Christodoulopoulos
et al, 2010). But these features are difficult to
combine because of their disparate representations.
Distributional information is typically represented
in numerical vectors, and recent work has demon-
strated the utility of continuous vector represen-
tations, or ?embeddings? (Mikolov et al, 2013;
Luong et al, 2013; Kim and de Marneffe, 2013;
Turian et al, 2010). In contrast, morphology is
often represented in terms of sparse, discrete fea-
tures (such as morphemes), or via pairwise mea-
sures such as string edit distance. Moreover, the
mapping between a surface form and morphology
is complex and nonlinear, so that simple metrics
such as edit distance will only weakly approximate
morphological similarity.
In this paper we present a new approach for in-
ducing part-of-speech (POS) classes, combining
morphological and distributional information in a
non-parametric Bayesian generative model based
on the distance-dependent Chinese restaurant pro-
cess (ddCRP; Blei and Frazier, 2011). In the dd-
CRP, each data point (word type) selects another
point to ?follow?; this chain of following links
corresponds to a partition of the data points into
clusters. The probability of word w
1
following w
2
depends on two factors: 1) the distributional simi-
larity between all words in the proposed partition
containing w
1
and w
2
, which is encoded using a
Gaussian likelihood function over the word embed-
dings; and 2) the morphological similarity between
w
1
and w
2
, which acts as a prior distribution on the
induced clustering. We use a log-linear model to
capture suffix similarities between words, and learn
the feature weights by iterating between sampling
and weight learning.
We apply our model to the English section of
the the Multext-East corpus (Erjavec, 2004) in or-
der to evaluate both against the coarse-grained and
265
fine-grained tags, where the fine-grained tags en-
code detailed morphological classes. We find that
our model effectively combines morphological fea-
tures with distributional similarity, outperforming
comparable alternative approaches.
2 Related work
Unsupervised POS tagging has a long history in
NLP. This paper focuses on the POS induction
problem (i.e., no tag dictionary is available), and
here we limit our discussion to very recent sys-
tems. A review and comparison of older systems
is provided by Christodoulopoulos et al (2010),
who found that imposing a one-tag-per-word-type
constraint to reduce model flexibility tended to
improve system performance; like other recent
systems, we impose that constraint here. Recent
work also shows that the combination of morpho-
logical and distributional information yields the
best results, especially cross-linguistically (Clark,
2003; Berg-Kirkpatrick et al, 2010). Since then,
most systems have incorporated morphology in
some way, whether as an initial step to obtain pro-
totypes for clusters (Abend et al, 2010), or as
features in a generative model (Lee et al, 2010;
Christodoulopoulos et al, 2011; Sirts and Alum?ae,
2012), or a representation-learning algorithm (Yat-
baz et al, 2012). Several of these systems use a
small fixed set of orthographic and/or suffix fea-
tures, sometimes obtained from an unsupervised
morphological segmentation system (Abend et al,
2010; Lee et al, 2010; Christodoulopoulos et al,
2011; Yatbaz et al, 2012). Blunsom and Cohn?s
(2011) model learns an n-gram character model
over the words in each cluster; we learn a log-
linear model, which can incorporate arbitrary fea-
tures. Berg-Kirkpatrick et al (2010) also include
a log-linear model of morphology in POS induc-
tion, but they use morphology in the likelihood
term of a parametric sequence model, thereby en-
couraging all elements that share a tag to have the
same morphological features. In contrast, we use
pairwise morphological similarity as a prior in a
non-parametric clustering model. This means that
the membership of a word in a cluster requires only
morphological similarity to some other element in
the cluster, not to the cluster centroid; which may
be more appropriate for languages with multiple
morphological paradigms. Another difference is
that our non-parametric formulation makes it un-
necessary to know the number of tags in advance.
3 Distance-dependent CRP
The ddCRP (Blei and Frazier, 2011) is an extension
of the CRP; like the CRP, it defines a distribution
over partitions (?table assignments?) of data points
(?customers?). Whereas in the regular CRP each
customer chooses a table with probability propor-
tional to the number of customers already sitting
there, in the ddCRP each customer chooses another
customer to follow, and sits at the same table with
that customer. By identifying the connected compo-
nents in this graph, the ddCRP equivalently defines
a prior over clusterings.
If c
i
is the index of the customer followed by
customer i, then the ddCRP prior can be written
P (c
i
= j) ?
{
f(d
ij
) if i 6= j
? if i = j,
(1)
where d
ij
is the distance between customers i and j
and f is a decay function. A ddCRP is sequential if
customers can only follow previous customers, i.e.,
d
ij
=? when i > j and f(?) = 0. In this case,
if d
ij
= 1 for all i < j then the ddCRP reduces to
the CRP.
Separating the distance and decay function
makes sense for ?natural? distances (e.g., the num-
ber of words between word i and j in a document,
or the time between two events), but they can also
be collapsed into a single similarity function. We
wish to assign higher similarities to pairs of words
that share meaningful suffixes. Because we do not
know which suffixes are meaningful a priori, we
use a maximum entropy model whose features in-
clude all suffixes up to length three that are shared
by at least one pair of words. Our prior is then:
P (c
i
= j|w, ?) ?
{
e
w
T
g(i,j)
if i 6= j
? if i = j,
(2)
where g
s
(i, j) is 1 if suffix s is shared by ith and
jth words, and 0 otherwise.
We can create an infinite mixture model by com-
bining the ddCRP prior with a likelihood function
defining the probability of the data given the cluster
assignments. Since we are using continuous-valued
vectors (word embeddings) to represent the distri-
butional characteristics of words, we use a multi-
variate Gaussian likelihood. We will marginalize
over the mean ? and covariance ? of each clus-
ter, which in turn are drawn from Gaussian and
inverse-Wishart (IW) priors respectively:
? ? IW (?
0
,?
0
) ? ? N (?
0
,
?
/
?
0
) (3)
266
The full model is then:
P (X,c,?,?|?,w, ?) (4)
=
K
?
k=1
P (?
k
|?)p(?
k
|?
k
,?)
?
n
?
i=1
(P (c
i
|w, ?)P (x
i
|?
z
i
,?
z
i
)),
where ? are the hyperparameters for (?,?) and z
i
is the (implicit) cluster assignment of the ith word
x
i
. With a CRP prior, this model would be an infi-
nite Gaussian mixture model (IGMM; Rasmussen,
2000), and we will use the IGMM as a baseline.
4 Inference
The Gibbs sampler for the ddCRP integrates over
the Gaussian parameters, sampling only follower
variables. At each step, the follower link c
i
for a
single customer i is sampled, which can implicitly
shift the entire block of n customers fol(i) who fol-
low i into a new cluster. Since we marginalize over
the cluster parameters, computing P (c
i
= j) re-
quires computing the likelihood P (fol(i),X
j
|?),
where X
j
are the k customers already clustered
with j. However, if we do not merge fol(i)
with X
j
, then we have P (X
j
|?) in the overall
joint probability. Therefore, we can decompose
P (fol(i),X
j
|?) = P (fol(i)|X
j
,?)P (X
j
|?) and
need only compute the change in likelihood due to
merging in fol(i):
1
:
P (fol(i)|X
j
,?) = pi
?nd/2
?
d/2
k
|?
k
|
?
k
/2
?
d/2
n+k
|?
n+k
|
?
n+k
/2
?
d
?
i=1
?
(
?
n+k
+1?i
2
)
?
(
?
k
+1?i
2
)
, (5)
where the hyperparameters are updated as ?
n
=
?
0
+ n, ?
n
= ?
0
+ n, and
?
n
=
?
0
?
0
+ x?
?
0
+ n
(6)
?
n
= ?
0
+Q+ ?
0
?
0
?
0
T
? ?
n
?
n
?
T
n
, (7)
where Q =
?
n
i=1
x
i
x
T
i
.
Combining this likelihood term with the prior,
the probability of customer i following j is
P (c
i
= j|X
,
?,w, ?)
? P (fol(i)|X
j
,?)P (c
i
= j|w, ?). (8)
1
http://www.stats.ox.ac.uk/
?
teh/re-
search/notes/GaussianInverseWishart.pdf
Our non-sequential ddCRP introduces cycles
into the follower structure, which are handled in the
sampler as described by Socher et al (2011). Also,
the block of customers being moved around can po-
tentially be very large, which makes it easy for the
likelihood term to swamp the prior. In practice we
found that introducing an additional parameter a
(used to exponentiate the prior) improved results?
although we report results without this exponent as
well. This technique was also used by Titov and
Klementiev (2012) and Elsner et al (2012).
Inference also includes optimizing the feature
weights for the log-linear model in the ddCRP
prior (Titov and Klementiev, 2012). We interleave
L-BFGS optimization within sampling, as in Monte
Carlo Expectation-Maximization (Wei and Tanner,
1990). We do not apply the exponentiation parame-
ter a when training the weights because this proce-
dure affects the follower structure only, and we do
not have to worry about the magnitude of the like-
lihood. Before the first iteration we initialize the
follower structure: for each word, we choose ran-
domly a word to follow from amongst those with
the longest shared suffix of up to 3 characters. The
number of clusters starts around 750, but decreases
substantially after the first sampling iteration.
5 Experiments
Data For our experiments we used the English
word embeddings from the Polyglot project (Al-
Rfou? et al, 2013)
2
, which provides embeddings
trained on Wikipedia texts for 100,000 of the most
frequent words in many languages.
We evaluate on the English part of the Multext-
East (MTE) corpus (Erjavec, 2004), which provides
both coarse-grained and fine-grained POS labels
for the text of Orwell?s ?1984?. Coarse labels con-
sist of 11 main word classes, while the fine-grained
tags (104 for English) are sequences of detailed
morphological attributes. Some of these attributes
are not well-attested in English (e.g. gender) and
some are mostly distinguishable via semantic anal-
ysis (e.g. 1st and 2nd person verbs). Many tags are
assigned only to one or a few words. Scores for the
fine-grained tags will be lower for these reasons,
but we argue below that they are still informative.
Since Wikipedia and MTE are from different
domains their lexicons do not fully overlap; we
2
https://sites.google.com/site/rmyeid/
projects/polyglot
267
Wikipedia tokens 1843M
Multext-East tokens 118K
Multext-East types 9193
Multext-East & Wiki types 7540
Table 1: Statistics for the English Polyglot word embeddings
and English part of MTE: number of Wikipedia tokens used
to train the embeddings, number of tokens/types in MTE, and
number of types shared by both datasets.
take the intersection of these two sets for training
and evaluation. Table 1 shows corpus statistics.
Evaluation With a few exceptions (Biemann,
2006; Van Gael et al, 2009), POS induction sys-
tems normally require the user to specify the num-
ber of desired clusters, and the systems are evalu-
ated with that number set to the number of tags in
the gold standard. For corpora such as MTE with
both fine-grained and coarse-grained tages, pre-
vious evaluations have scored against the coarse-
grained tags. Though coarse-grained tags have
their place (Petrov et al, 2012), in many cases
the distributional and morphological distinctions
between words are more closely aligned with the
fine-grained tagsets, which typically distinguish
between verb tenses, noun number and gender,
and adjectival scale (comparative, superlative, etc.),
so we feel that the evaluation against fine-grained
tagset is more relevant here. For better comparison
with previous work, we also evaluate against the
coarse-grained tags; however, these numbers are
not strictly comparable to other scores reported on
MTE because we are only able to train and evalu-
ate on the subset of words that also have Polyglot
embeddings. To provide some measure of the dif-
ficulty of the task, we report baseline scores using
K-means clustering, which is relatively strong base-
line in this task (Christodoulopoulos et al, 2011).
There are several measures commonly used for
unsupervised POS induction. We report greedy
one-to-one mapping accuracy (1-1) (Haghighi and
Klein, 2006) and the information-theoretic score V-
measure (V-m), which also varies from 0 to 100%
(Rosenberg and Hirschberg, 2007). In previous
work it has been common to also report many-to-
one (m-1) mapping but this measure is particularly
sensitive to the number of induced clusters (more
clusters yield higher scores), which is variable for
our models. V-m can be somewhat sensitive to the
number of clusters (Reichart and Rappoport, 2009)
but much less so than m-1 (Christodoulopoulos
et al, 2010). With different number of induced
and gold standard clusters the 1-1 measure suffers
because some induced clusters cannot be mapped
to gold clusters or vice versa. However, almost half
the gold standard clusters in MTE contain just a
few words and we do not expect our model to be
able to learn them anyway, so the 1-1 measure is
still useful for telling us how well the model learns
the bigger and more distinguishable classes.
In unsupervised POS induction it is standard to
report accuracy on tokens even when the model it-
self works on types. Here we report also type-based
measures because these can reveal differences in
model behavior even when token-based measures
are similar.
Experimental setup For baselines we use K-
means and the IGMM, which both only learn from
the word embeddings. The CRP prior in the IGMM
has one hyperparameter (the concentration param-
eter ?); we report results for ? = 5 and 20. Both
the IGMM and ddCRP have four hyperparameters
controlling the prior over the Gaussian cluster pa-
rameters: ?
0
, ?
0
, ?
0
and ?
0
. We set the prior scale
matrix ?
0
by using the average covariance from
a K-means run with K = 200. When setting the
average covariance as the expected value of the IW
distribution the suitable scale matrix can be com-
puted as ?
0
= E [X] (?
0
? d? 1), where ?
0
is the
prior degrees of freedom (which we set to d + 10)
and d is the data dimensionality (64 for the Poly-
glot embeddings). We set the prior mean ?
0
equal
to the sample mean of the data and ?
0
to 0.01.
We experiment with three different priors for the
ddCRP model. All our ddCRP models are non-
sequential (Socher et al, 2011), allowing cycles
to be formed. The simplest model, ddCRP uni-
form, uses a uniform prior that sets the distance
between any two words equal to one.
3
The second
model, ddCRP learned, uses the log-linear prior
with weights learned between each two Gibbs iter-
ations as explained in section 4. The final model,
ddCRP exp, adds the prior exponentiation. The ?
parameter for the ddCRP is set to 1 in all experi-
ments. For ddCRP exp, we report results with the
exponent a set to 5.
Results and discussion Table 2 presents all re-
sults. Each number is an average of 5 experiments
3
In the sequential case this model would be equivalent to
the IGMM (Blei and Frazier, 2011). Due to the nonsequen-
tiality this equivalence does not hold, but we do expect to see
similar results to the IGMM.
268
Fine types Fine tokens Coarse tokens
Model K Model K-means Model K-means Model K-means
K-means 104 or 11 16.1 / 47.3 - 39.2 / 62.0 - 44.4 / 45.5 -
IGMM, ? = 5 55.6 41.0 / 45.9 23.1 / 49.5 48.0 / 64.8 37.2 / 61.0 48.3 / 58.3 40.8 / 55.0
IGMM, ? = 20 121.2 35.0 / 47.1 14.7 / 46.9 50.6 / 67.8 44.7 / 65.5 48.7 / 60.0 48.3 / 57.9
ddCRP uniform 80.4 50.5 / 52.9 18.6 / 48.2 52.4 / 68.7 35.1 / 60.3 52.1 / 62.2 40.3 / 54.2
ddCRP learned 89.6 50.1 / 55.1 17.6 / 48.0 51.1 / 69.7 39.0 / 63.2 48.9 / 62.0 41.1 / 55.1
ddCRP exp, a = 5 47.2 64.0 / 60.3 25.0 / 50.3 55.1 / 66.4 33.0 / 59.1 47.8 / 55.1 36.9 / 53.1
Table 2: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokens
using coarse-grained tags. For each model we present the number of induced clusters K (or fixed K for K-means) and 1-1 / V-m
scores. The second column under each evaluation setting gives the scores for K-means with K equal to the number of clusters
induced by the model in that row.
with different random initializations. For each eval-
uation setting we provide two sets of scores?first
are the 1-1 and V-m scores for the given model,
second are the comparable scores for K-means run
with the same number of clusters as induced by the
non-parametric model.
These results show that all non-parametric mod-
els perform better than K-means, which is a strong
baseline in this task (Christodoulopoulos et al,
2011). The poor performace of K-means can be
explained by the fact that it tends to find clusters
of relatively equal size, although the POS clus-
ters are rarely of similar size. The common noun
singular class is by far the largest in English, con-
taining roughly a quarter of the word types. Non-
parametric models are able to produce cluster of
different sizes when the evidence indicates so, and
this is clearly the case here.
From the token-based evaluation it is hard to
say which IGMM hyperparameter value is better
even though the number of clusters induced differs
by a factor of 2. The type-base evaluation, how-
ever, clearly prefers the smaller value with fewer
clusters. Similar effects can be seen when com-
paring IGMM and ddCRP uniform. We expected
these two models perform on the same level, and
their token-based scores are similar, but on the type-
based evaluation the ddCRP is clearly superior. The
difference could be due to the non-sequentiality,
or becuase the samplers are different?IGMM en-
abling resampling only one item at a time, ddCRP
performing blocked sampling.
Further we can see that the ddCRP uniform and
learned perform roughly the same. Although the
prior in those models is different they work mainly
using the the likelihood. The ddCRP with learned
prior does produce nice follower structures within
each cluster but the prior is in general too weak
compared to the likelihood to influence the cluster-
ing decisions. Exponentiating the prior reduces the
number of induced clusters and improves results,
as it can change the cluster assignment for some
words where the likelihood strongly prefers one
cluster but the prior clearly indicates another.
The last column shows the token-based evalua-
tion against the coarse-grained tagset. This is the
most common evaluation framework used previ-
ously in the literature. Although our scores are not
directly comparable with the previous results, our
V-m scores are similar to the best published 60.5
(Christodoulopoulos et al, 2010) and 66.7 (Sirts
and Alum?ae, 2012).
In preliminary experiments, we found that di-
rectly applying the best-performing English model
to other languages is not effective. Different lan-
guages may require different parametrizations of
the model. Further study is also needed to verify
that word embeddings effectively capture syntax
across languages, and to determine the amount of
unlabeled text necessary to learn good embeddings.
6 Conclusion
This paper demonstrates that morphology and dis-
tributional features can be combined in a flexi-
ble, joint probabilistic model, using the distance-
dependent Chinese Restaurant Process. A key ad-
vantage of this framework is the ability to include
arbitrary features in the prior distribution. Future
work may exploit this advantage more thoroughly:
for example, by using features that incorporate
prior knowledge of the language?s morphological
structure. Another important goal is the evaluation
of this method on languages beyond English.
Acknowledgments: KS was supported by the
Tiger University program of the Estonian Infor-
mation Technology Foundation for Education. JE
was supported by a visiting fellowship from the
Scottish Informatics & Computer Science Alliance.
We thank the reviewers for their helpful feedback.
269
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2010.
Improved unsupervised pos induction through pro-
totype discovery. In Proceedings of the 48th An-
nual Meeting of the Association of Computational
Linguistics, pages 1298?1307.
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proceedings of the Thir-
teenth Annual Conference on Natural Language
Learning, pages 183?192, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre B. C?ot?e, John
DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590.
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7?12.
David M Blei and Peter I Frazier. 2011. Distance
dependent chinese restaurant processes. Journal of
Machine Learning Research, 12:2461?2488.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association of Computational
Linguistics, pages 865?874.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for part-of-speech induction using multiple features.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the European chapter of the
ACL.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association of Computational
Linguistics.
Toma?z Erjavec. 2004. MULTEXT-East version 3:
Multilingual morphosyntactic specifications, lexi-
cons and corpora. In LREC.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013. Deriving adjectival scales from continuous
space word representations. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2010. Simple type-level unsupervised pos tag-
ging. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
853?861.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representations
with recursive neural networks for morphology. In
Proceedings of the Thirteenth Annual Conference on
Natural Language Learning.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of Human
Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 746?751.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Carl Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Process-
ing Systems 12, Cambridge, MA. MIT Press.
Roi Reichart and Ari Rappoport. 2009. The nvi cluster-
ing evaluation measure. In Proceedings of the Ninth
Annual Conference on Natural Language Learning,
pages 165?173.
A. Rosenberg and J. Hirschberg. 2007. V-measure:
A conditional entropy-based external cluster evalua-
tion measure. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 410?42.
Kairit Sirts and Tanel Alum?ae. 2012. A hierarchi-
cal Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 407?416.
Richard Socher, Andrew L Maas, and Christopher D
Manning. 2011. Spectral chinese restaurant pro-
cesses: Nonparametric clustering based on similar-
ities. In Proceedings of the Fifteenth International
Conference on Artificial Intelligence and Statistics,
pages 698?706.
270
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 678?687, Singapore.
Greg CG Wei and Martin A Tanner. 1990. A
monte carlo implementation of the em algorithm
and the poor man?s data augmentation algorithms.
Journal of the American Statistical Association,
85(411):699?704.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 940?951.
271
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 415?420,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Modeling Factuality Judgments in Social Media Text
Sandeep Soni Tanushree Mitra Eric Gilbert Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
soni.sandeepb@gmail.com, {tmitra3,gilbert,jeisenst}@cc.gatech.edu
Abstract
How do journalists mark quoted content
as certain or uncertain, and how do read-
ers interpret these signals? Predicates such
as thinks, claims, and admits offer a range
of options for framing quoted content ac-
cording to the author?s own perceptions of
its credibility. We gather a new dataset
of direct and indirect quotes from Twit-
ter, and obtain annotations of the perceived
certainty of the quoted statements. We
then compare the ability of linguistic and
extra-linguistic features to predict readers?
assessment of the certainty of quoted con-
tent. We see that readers are indeed influ-
enced by such framing devices ? and we
find no evidence that they consider other
factors, such as the source, journalist, or
the content itself. In addition, we examine
the impact of specific framing devices on
perceptions of credibility.
1 Introduction
Contemporary journalism is increasingly con-
ducted through social media services like Twit-
ter (Lotan et al, 2011; Hermida et al, 2012). As
events unfold, journalists and political commen-
tators use quotes ? often indirect ? to convey
potentially uncertain information and claims from
their sources and informants, e.g.,
Figure 1: Indirect quotations in Twitter
A key pragmatic goal of such messages is to
convey the provenance and uncertainty of the
quoted content. In some cases, the author may also
introduce their own perspective (Lin et al, 2006)
through the use of framing (Greene and Resnik,
2009). For instance, consider the use of the word
claims in Figure 1, which conveys the author?s
doubt about the indirectly quoted content.
Detecting and reasoning about the certainty of
propositional content has been identified as a key
task for information extraction, and is now sup-
ported by the FactBank corpus of annotations for
newstext (Saur?? and Pustejovsky, 2009). However,
less is known about this phenomenon in social
media ? a domain whose endemic uncertainty
makes proper treatment of factuality even more
crucial (Morris et al, 2012). Successful automa-
tion of factuality judgments could help to detect
online rumors (Qazvinian et al, 2011), and might
enable new applications, such as the computation
of reliability ratings for ongoing stories.
This paper investigates how linguistic resources
and extra-linguistic factors affect perceptions of
the certainty of quoted information in Twitter. We
present a new dataset of Twitter messages that use
FactBank predicates (e.g., claim, say, insist) to
scope the claims of named entity sources. This
dataset was annotated by Mechanical Turk work-
ers who gave ratings for the factuality of the
scoped claims in each Twitter message. This en-
ables us to build a predictive model of the fac-
tuality annotations, with the goal of determining
the full set of relevant factors, including the pred-
icate, the source, the journalist, and the content
of the claim itself. However, we find that these
extra-linguistic factors do not predict readers? fac-
tuality judgments, suggesting that the journalist?s
own framing plays a decisive role in the cred-
ibility of the information being conveyed. We
explore the specific linguistic feature that affect
factuality judgments, and compare our findings
with previously-proposed groupings of factuality-
related predicates.
415
say tell thin
k
acco
rd
sugg
est claim beli
ev adm
it ask pred
ict
repo
rt
exp
lain den
i
hop
e
lear
n
insis
t
hea
r
won
der feel stat
e
disc
ov
forg
et
asse
rt
gue
ss
obse
rv
mai
ntai
n
dou
bt
Cue Words
0
50
100
150
Cou
nts
Figure 2: Count of cue words in our dataset. Each
word is patterned according to its group, as shown
in Figure 3.
Rep
ort Beli
ef
Kno
wled
ge
Dou
bt
Perc
epti
on
Cue Groups
0
100
200
300
400
500
600
Cou
nts
Figure 3: Count of cue groups in our dataset
2 Text data
We gathered a dataset of Twitter messages from
103 professional journalists and bloggers who
work in the field of American Politics.
1
Tweets
were gathered using Twitter?s streaming API, ex-
tracting the complete permissible timeline up to
February 23, 2014. A total of 959,754 tweets were
gathered, and most were written in early 2014.
Our interest in this text is specifically in quoted
content ? including ?indirect? quotes, which may
include paraphrased quotations, as in the examples
in Figure 1. While labeled datasets for such quotes
have been created (O?Keefe et al, 2012; Pareti,
2012), these are not freely available at present. In
any case, the relevance of these datasets to Twitter
text is currently unproven. Therefore, rather than
train a supervised model to detect quotations, we
apply a simple dependency-based heuristic.
? We focus on tweets that contain any member of
a list of source-introducing predicates (we bor-
row the terminology of Pareti (2012) and call
this the CUE). Our complete list ? shown in
Table 1 ? was selected mainly from the exam-
ples presented by Saur?? and Pustejovsky (2012),
1
We used the website http://muckrack.com.
Report say, report, tell, told, observe, state,
accord, insist, assert, claim, main-
tain, explain, deny
Knowledge learn, admit, discover, forget, forgot
Belief think, thought, predict, suggest,
guess, believe
Doubt doubt, wonder, ask, hope
Perception sense, hear, feel
Table 1: Lemmas of source-introducing predicates
(cues) and groups (Saur??, 2008).
but with reference also to Saur???s (2008) dis-
sertation for cues that are common in Twitter.
The Porter Stemmer is applied to match inflec-
tions, e.g. denies/denied; for irregular cases
not handled by the Porter Stemmer (e.g., for-
get/forgot), we include both forms. We use the
CMU Twitter Part-of-Speech Tagger (Owoputi
et al, 2013) to select only instances in the verb
sense. Figure 2 shows the distribution of the
cues and Figure 3 shows the distribution of the
cue groups. For cues that appear in multiple
groups, we chose the most common group.
? We run the Stanford Dependency parser to
obtain labeled dependencies (De Marneffe et
al., 2006), requiring that the cue has outgoing
edges of the type NSUBJ (noun subject) and
CCOMP (clausal complement). The subtree
headed by the modifier of the CCOMP relation
is considered the claim; the subtree headed by
the modifier of the NSUBJ relation is consid-
ered the source. See Figure 4 for an example.
? We use a combination of regular expressions
and dependency rules to capture expressions
of the type ?CLAIM, according to SOURCE.?
Specifically, the PCOMP path from according
is searched for the pattern according to
*
.
The text that matches the * is the source and the
remaining text other than the source is taken as
the claim.
? Finally, we restrict consideration to tweets in
which the source contains a named entity or
twitter username. This eliminates expressions
of personal belief such as I doubt Obama will
win, as well as anonymous sources such as
Team sources report that Lebron has demanded
a trade to New York. Investigating the factual-
ity judgments formed in response to such tweets
is clearly an important problem for future re-
search, but is outside the scope of this paper.
This heuristic pipeline may miss many relevant
tweets, but since the overall volume is high, we
416
Source Cue ClaimI guess, since FBI claims it couldn?t match Tsarnaev, we can assume ...
nsubj
mark
ccomp
nsubj
aux+neg dobj
Figure 4: Dependency parse of an example message, with claim, source, and cue.
Total journalists 443
Total U.S. political journalists 103
Total tweets 959754
Tweets with cues 172706
Tweets with source and claims 40615
Total tweets annotated 1265
Unique sources in annotated dataset 766
Unigrams in annotated dataset 1345
Table 2: Count Statistics of the entire data col-
lected and the annotated dataset
Figure 5: Turk annotation interface
prioritize precision. The resulting dataset is sum-
marized in Table 2.
3 Annotation
We used Amazon Mechanical Turk (AMT) to col-
lect ratings of claims. AMT has been widely used
by the NLP community to collect data (Snow et
al., 2008), with ?best practices? defined to help
requesters best design Turk jobs (Callison-Burch
and Dredze, 2010). We followed these guidelines
to perform pilot experiments to test the instruction
set and the quality of responses. Based on the pi-
lot study we designed Human Intelligence Tasks
(HITs) to annotate 1265 claims.
Each HIT contained a batch of ten tweets and
rewarded $0.10 per hit. To ensure quality con-
trol we required the Turkers to have at least 85%
hit approval rating and to reside in the United
States, because the Twitter messages in our dataset
were related to American politics. For each tweet,
we obtained five independent ratings from Turk-
ers satisfying the above qualifications. The rat-
ings were based on a 5-point Likert scale rang-
ing from ?[-2] Certainly False? to ?[2] Certainly
True? and allowing for ?[0] Uncertain?. We also
allowed for ?Not Applicable? option to capture
ratings where the Turkers did not have sufficient
knowledge about the statement or if the statement
was not really a claim. Figure 6 shows the set of
instructions provided to the Turkers, and Figure 5
illustrates the annotation interface.
2
We excluded tweets for which three or more
Turkers gave a rating of ?Not Applicable,? leaving
us with a dataset of 1170 tweets. Within this set,
the average variance per tweet (excluding ?Not
Applicable? ratings) was 0.585.
4 Modeling factuality judgments
Having obtained a corpus of factuality ratings, we
now model the factors that drive these ratings.
4.1 Predictive accuracy
First, we attempt to determine the impact of vari-
ous predictive features on rater judgments of fac-
tuality. We consider the following features:
? Cue word: after stemming
? Cue word group: as given in Table 1
? Source: represented by the named entity or
username in the source field (see Figure 4)
? Journalist: represented by their Twitter ID
? Claim: represented by a bag-of-words vector
from the claim field (Figure 4)
These features are used as predictors in a series
of linear ridge regressions, where the dependent
variable is the mean certainty rating. We throw
out tweets that were rated as ?not applicable? by a
majority of raters, but otherwise ignore ?not appli-
cable? ratings of the remaining tweets. The goal
of these regressions is to determine which fea-
tures are predictive of raters? factuality judgments.
The ridge regression regularization parameter was
tuned via cross-validation in the training set. We
used the bootstrap to obtain multiple training/test
2
The data is available at https://www.github.
com/jacobeisenstein/twitter-certainty.
417
Figure 6: User instructions for the annotation task
Features Error
Baseline .442
Cue word .404*
Cue word group .42
Source .447
Journalist .444
Claim .476
Cue word + cue word group .404*
All features .420
Table 3: Linear regression error rates for each fea-
ture group. * indicates improvement over the base-
line at p < .05.
splits (70% training), which were used for signifi-
cance testing.
Table 3 reports mean average error for each fea-
ture group, as well as a baseline that simply re-
ports the mean rating across the training set. Each
accuracy was compared with the baseline using a
paired z-test. Only the cue word features pass this
test at p < .05. The other features do not help,
even in combination with the cue word.
While these findings must be interpreted with
caution, they suggest that readers ? at least, Me-
chanical Turk workers ? use relatively little inde-
pendent judgment to assess the validity of quoted
text that they encounter on Twitter. Of course,
richer linguistic models, more advanced machine
learning, or experiments with more carefully-
selected readers might offer a different view. But
the results at hand are most compatible with the
conclusion that readers base their assessments of
factuality only on the framing provided by the
journalist who reports the quote.
4.2 Cue words and cue groups
Given the importance of cue words as a sig-
nal for factuality, we want to assess the factual-
ity judgments induced by each cue. A second
question is whether proposed groupings of cue
words into groups cohere with such perceptions.
Saur?? (2008) describes several classes of source-
introducing predicates, which indicate how the
source relates to the quoted claim. These classes
are summarized in Table 1, along with frequently-
occuring cues from our corpus. We rely on Fact-
Bank to assign the cue words to classes; the only
word not covered by FactBank was sense, which
we placed in predicates of perception.
We performed another set of linear regressions,
again using the mean certainty rating as the de-
pendent variable. In this case, there was no train-
ing/test split, so confidence intervals on the result-
ing parameters are computed using the analytic
closed form. We performed two such regressions:
first using only the individual cues as predictors,
and then using only the cue groups. Results are
shown in Figures 7 and 8; Figure 7 includes only
cues which appear at least ten times, although all
cues were included in the regression.
The cues that give the highest factuality coef-
ficients are learn and admit, which are labeled as
predicates of knowledge. These cues carry a sub-
stantial amount of framing, as they purport to de-
scribe the private mental state of the source. The
word admit often applies to statements that are
perceived as damaging to the source, such as Bill
Gates admits Control-Alt-Delete was a mistake;
since there can be no self-interest behind such
statements, they may be perceived as more likely
to be true.
Several of the cues with the lowest factuality co-
efficients are predicates of belief: suggest, predict
and think. The words suggest, think, and believe
also purport to describe the private mental state of
the source, but their framing function is the op-
posite of the predicates of knowledge: they im-
ply that it is important to mark the claim as the
source?s belief, and not a widely-accepted fact.
For example, Mubarak clearly believes he has the
military leadership?s support.
A third group of interest are the predicates of
report, which have widely-varying certainty coef-
ficients. The cues according, report, say, and tell
418
lear
n
adm
it
acc
ord rep
ort hea
r say tell
exp
lain ask insi
st
beli
ev clai
m
sug
ges
t
pre
dict hop
e
thin
k den
i
Cue Words
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Coe
ffici
ent
s' E
stim
ate
s
Figure 7: Linear regression coefficients for
frequently-occurring cue words. Each word is pat-
terned according to its group, shown in Figure 8.
Kno
wle
dge
Per
cep
tion Rep
ort
Dou
bt
Bel
ief
Cue Groups
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Coe
ffici
ent
s' E
stim
ate
s
Figure 8: Linear regression coefficients for cue
word group.
are strongly predictive of certainty, but the cues
claim and deny convey uncertainty. Both accord-
ing and report are often used in conjunction with
impersonal and institutional sources, e.g., Cuc-
cinelli trails McAuliffe by 24 points , according to
a new poll. In contrast, insist, claim, and deny im-
ply that there is uncertainty about the quoted state-
ment, e.g., Christie insists that Fort Lee Mayor
was never on my radar. In this case, the fact that
the predicate indicates a report is not enough to
determine the framing: different sorts of reports
carry radically different perceptions of factuality.
5 Related work
Factuality and Veridicality The creation of
FactBank (Saur?? and Pustejovsky, 2009) has en-
abled recent work on the factuality (or ?veridical-
ity?) of event mentions in text. Saur?? and Puste-
jovsky (2012) propose a two-dimensional factual-
ity annotation scheme, including polarity and cer-
tainty; they then build a classifier to predict an-
notations of factuality from statements in Fact-
Bank. Their work on source-introducing predi-
cates provides part of the foundation for this re-
search, which focuses on quoted statements in so-
cial media text. de Marneffe et al (2012) conduct
an empirical evaluation of FactBank ratings from
Mechanical Turk workers, finding a high degree of
disagreement between raters. They also construct
a statistical model to predict these ratings. We are
unaware of prior work comparing the contribution
of linguistic and extra-linguistic predictors (e.g.,
source and journalist features) for factuality rat-
ings. This prior work also does not measure the
impact of individual cues and cue classes on as-
sessment of factuality.
Credibility in social media Recent work in the
area of computational social science focuses on
understanding credibility cues on Twitter. Such
studies have found that users express concern over
the credibility of tweets belonging to certain topics
(politics, news, emergency). By manipulating sev-
eral features of a tweet, Morris et al (2012) found
that in addition to content, users often use addi-
tional markers while assessing the tweet credibil-
ity, such as the user name of the source. The search
for reliable signals of information credibility in so-
cial media has led to the construction of automatic
classifiers to identify credible tweets (Castillo et
al., 2011). However, this prior work has not ex-
plored the linguistic basis of factuality judgments,
which we show to depend on framing devices such
as cue words.
6 Conclusion
Perceptions of the factuality of quoted content are
influenced by the cue words used to introduce
them, while extra-linguistic factors, such as the
source and the author, did not appear to be rele-
vant in our experiments. This result is obtained
from real tweets written by journalists; a natural
counterpart study would be to experimentally ma-
nipulate this framing to see if the same perceptions
apply. Another future direction would be to test
whether the deployment of cue words as framing
devices reflects the ideology of the journalist. We
are also interested to group multiple instances of
the same quote (Leskovec et al, 2009), and exam-
ine how its framing varies across different news
outlets and over time.
Acknowledgments: This research was supported
by DARPA-W911NF-12-1-0043 and by a Compu-
tational Journalism research award from Google.
We thank the reviewers for their helpful feedback.
419
References
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon?s me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12.
Association for Computational Linguistics.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th international conference
on World wide web, pages 675?684. ACM.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Marie C. de Marneffe, Christopher D. Manning, and
Christopher Potts. 2012. Did it happen? the prag-
matic complexity of veridicality assessment. Com-
put. Linguist., 38(2):301?333, June.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503?511, Boulder, Colorado, June.
Association for Computational Linguistics.
Alfred Hermida, Seth C Lewis, and Rodrigo Zamith.
2012. Sourcing the arab spring: A case study of
andy carvins sources during the tunisian and egyp-
tian revolutions. In international symposium on on-
line journalism, Austin, TX, April, pages 20?21.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In Proceedings of the 15th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 497?506. ACM.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: Identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL-X ?06, pages 109?116, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Gilad Lotan, Erhardt Graeff, Mike Ananny, Devin
Gaffney, Ian Pearce, et al 2011. The arab spring?
the revolutions were tweeted: Information flows dur-
ing the 2011 tunisian and egyptian revolutions. In-
ternational Journal of Communication, 5:31.
Meredith Ringel Morris, Scott Counts, Asta Roseway,
Aaron Hoff, and Julia Schwarz. 2012. Tweeting
is believing?: understanding microblog credibility
perceptions. In Proceedings of the ACM 2012 con-
ference on Computer Supported Cooperative Work,
pages 441?450. ACM.
Tim O?Keefe, Silvia Pareti, James R Curran, Irena Ko-
prinska, and Matthew Honnibal. 2012. A sequence
labelling approach to quote attribution. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 790?
799. Association for Computational Linguistics.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Silvia Pareti. 2012. A database of attribution relations.
In LREC, pages 3213?3217.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589?1599.
Association for Computational Linguistics.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Saur?? and James Pustejovsky. 2012. Are you
sure that this happened? assessing the factuality de-
gree of events in text. Comput. Linguist., 38(2):261?
299, June.
Roser Saur??. 2008. A Factuality Profiler for Eventual-
ities in Text. Ph.D. thesis, Brandeis University.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
420
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538?544,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Fast Easy Unsupervised Domain Adaptation
with Marginalized Structured Dropout
Yi Yang and Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
{yiyang, jacobe}@gatech.edu
Abstract
Unsupervised domain adaptation often re-
lies on transforming the instance represen-
tation. However, most such approaches
are designed for bag-of-words models, and
ignore the structured features present in
many problems in NLP. We propose a
new technique called marginalized struc-
tured dropout, which exploits feature
structure to obtain a remarkably simple
and efficient feature projection. Applied
to the task of fine-grained part-of-speech
tagging on a dataset of historical Por-
tuguese, marginalized structured dropout
yields state-of-the-art accuracy while in-
creasing speed by more than an order-of-
magnitude over previous work.
1 Introduction
Unsupervised domain adaptation is a fundamen-
tal problem for natural language processing, as
we hope to apply our systems to datasets unlike
those for which we have annotations. This is par-
ticularly relevant as labeled datasets become stale
in comparison with rapidly evolving social media
writing styles (Eisenstein, 2013), and as there is
increasing interest in natural language processing
for historical texts (Piotrowski, 2012). While a
number of different approaches for domain adap-
tation have been proposed (Pan and Yang, 2010;
S?gaard, 2013), they tend to emphasize bag-of-
words features for classification tasks such as sen-
timent analysis. Consequently, many approaches
rely on each instance having a relatively large
number of active features, and fail to exploit the
structured feature spaces that characterize syn-
tactic tasks such as sequence labeling and pars-
ing (Smith, 2011).
As we will show, substantial efficiency im-
provements can be obtained by designing domain
adaptation methods for learning in structured fea-
ture spaces. We build on work from the deep
learning community, in which denoising autoen-
coders are trained to remove synthetic noise from
the observed instances (Glorot et al, 2011a). By
using the autoencoder to transform the original
feature space, one may obtain a representation
that is less dependent on any individual feature,
and therefore more robust across domains. Chen
et al (2012) showed that such autoencoders can
be learned even as the noising process is analyt-
ically marginalized; the idea is similar in spirit
to feature noising (Wang et al, 2013). While
the marginalized denoising autoencoder (mDA) is
considerably faster than the original denoising au-
toencoder, it requires solving a system of equa-
tions that can grow very large, as realistic NLP
tasks can involve 10
5
or more features.
In this paper we investigate noising functions
that are explicitly designed for structured feature
spaces, which are common in NLP. For example,
in part-of-speech tagging, Toutanova et al (2003)
define several feature ?templates?: the current
word, the previous word, the suffix of the current
word, and so on. For each feature template, there
are thousands of binary features. To exploit this
structure, we propose two alternative noising tech-
niques: (1) feature scrambling, which randomly
chooses a feature template and randomly selects
an alternative value within the template, and (2)
structured dropout, which randomly eliminates
all but a single feature template. We show how it
is possible to marginalize over both types of noise,
and find that the solution for structured dropout is
substantially simpler and more efficient than the
mDA approach of Chen et al (2012), which does
not consider feature structure.
We apply these ideas to fine-grained part-of-
speech tagging on a dataset of Portuguese texts
from the years 1502 to 1836 (Galves and Faria,
2010), training on recent texts and evaluating
538
on older documents. Both structure-aware do-
main adaptation algorithms perform as well as
standard dropout ? and better than the well-
known structural correspondence learning (SCL)
algorithm (Blitzer et al, 2007) ? but structured
dropout is more than an order-of-magnitude faster.
As a secondary contribution of this paper, we
demonstrate the applicability of unsupervised do-
main adaptation to the syntactic analysis of histor-
ical texts.
2 Model
In this section we first briefly describe the de-
noising autoencoder (Glorot et al, 2011b), its ap-
plication to domain adaptation, and the analytic
marginalization of noise (Chen et al, 2012). Then
we present three versions of marginalized denois-
ing autoencoders (mDA) by incorporating differ-
ent types of noise, including two new noising pro-
cesses that are designed for structured features.
2.1 Denoising Autoencoders
Assume instances x
1
, . . . ,x
n
, which are drawn
from both the source and target domains. We will
?corrupt? these instances by adding different types
of noise, and denote the corrupted version of x
i
by
?
x
i
. Single-layer denoising autoencoders recon-
struct the corrupted inputs with a projection matrix
W : R
d
? R
d
, which is estimated by minimizing
the squared reconstruction loss
L =
1
2
n
?
i=1
||x
i
?W
?
x
i
||
2
. (1)
If we write X = [x
1
, . . . ,x
n
] ? R
d?n
, and we
write its corrupted version
?
X, then the loss in (1)
can be written as
L(W) =
1
2n
tr
[
(
X?W
?
X
)
>
(
X?W
?
X
)
]
.
(2)
In this case, we have the well-known closed-
form solution for this ordinary least square prob-
lem:
W = PQ
?1
, (3)
where Q =
?
X
?
X
>
and P = X
?
X
>
. After ob-
taining the weight matrix W, we can insert non-
linearity into the output of the denoiser, such as
tanh(WX). It is also possible to apply stack-
ing, by passing this vector through another autoen-
coder (Chen et al, 2012). In pilot experiments,
this slowed down estimation and had little effect
on accuracy, so we did not include it.
High-dimensional setting Structured predic-
tion tasks often have much more features than
simple bag-of-words representation, and perfor-
mance relies on the rare features. In a naive im-
plementation of the denoising approach, both P
and Q will be dense matrices with dimension-
ality d ? d, which would be roughly 10
11
ele-
ments in our experiments. To solve this problem,
Chen et al (2012) propose to use a set of pivot
features, and train the autoencoder to reconstruct
the pivots from the full set of features. Specifi-
cally, the corrupted input is divided to S subsets
?
x
i
=
[
(
?
x)
1
i
>
, . . . , (
?
x)
S
i
>
]
>
. We obtain a projec-
tion matrix W
s
for each subset by reconstructing
the pivot features from the features in this subset;
we can then use the sum of all reconstructions as
the new features, tanh(
?
S
s=1
W
s
X
s
).
Marginalized Denoising Autoencoders In the
standard denoising autoencoder, we need to gen-
erate multiple versions of the corrupted data
?
X
to reduce the variance of the solution (Glorot et
al., 2011b). But Chen et al (2012) show that it
is possible to marginalize over the noise, analyt-
ically computing expectations of both P and Q,
and computing
W = E[P]E[Q]
?1
, (4)
where E[P] =
?
n
i=1
E[x
i
?
x
>
i
] and E[Q] =
?
n
i=1
E[
?
x
i
?
x
>
i
]. This is equivalent to corrupting
the data m?? times. The computation of these
expectations depends on the type of noise.
2.2 Noise distributions
Chen et al (2012) used dropout noise for domain
adaptation, which we briefly review. We then de-
scribe two novel types of noise that are designed
for structured feature spaces, and explain how they
can be marginalized to efficiently compute W.
Dropout noise In dropout noise, each feature is
set to zero with probability p > 0. If we define
the scatter matrix of the uncorrupted input as S =
XX
>
, the solutions under dropout noise are
E[Q]
?,?
=
{
(1? p)
2
S
?,?
if ? 6= ?
(1? p)S
?,?
if ? = ?
, (5)
and
E[P]
?,?
= (1? p)S
?,?
, (6)
539
where ? and ? index two features. The form of
these solutions means that computing W requires
solving a system of equations equal to the num-
ber of features (in the naive implementation), or
several smaller systems of equations (in the high-
dimensional version). Note also that p is a tunable
parameter for this type of noise.
Structured dropout noise In many NLP set-
tings, we have several feature templates, such as
previous-word, middle-word, next-word, etc, with
only one feature per template firing on any token.
We can exploit this structure by using an alterna-
tive dropout scheme: for each token, choose ex-
actly one feature template to keep, and zero out all
other features that consider this token (transition
feature templates such as ?y
t
, y
t?1
? are not con-
sidered for dropout). Assuming we haveK feature
templates, this noise leads to very simple solutions
for the marginalized matrices E[P] and E[Q],
E[Q]
?,?
=
{
0 if ? 6= ?
1
K
S
?,?
if ? = ?
(7)
E[P]
?,?
=
1
K
S
?,?
, (8)
ForE[P], we obtain a scaled version of the scat-
ter matrix, because in each instance
?
x, there is ex-
actly a 1/K chance that each individual feature
survives dropout. E[Q] is diagonal, because for
any off-diagonal entry E[Q]
?,?
, at least one of ?
and ? will drop out for every instance. We can
therefore view the projection matrix W as a row-
normalized version of the scatter matrix S. Put
another way, the contribution of ? to the recon-
struction for ? is equal to the co-occurence count
of ? and ?, divided by the count of ?.
Unlike standard dropout, there are no free
hyper-parameters to tune for structured dropout.
Since E[Q] is a diagonal matrix, we eliminate the
cost of matrix inversion (or of solving a system of
linear equations). Moreover, to extend mDA for
high dimensional data, we no longer need to di-
vide the corrupted input
?
x to several subsets.
1
For intuition, consider standard feature dropout
with p =
K?1
K
. This will look very similar to
structured dropout: the matrix E[P] is identical,
and E[Q] has off-diagonal elements which are
scaled by (1 ? p)
2
, which goes to zero as K is
1
E[P] is an r by d matrix, where r is the number of pivots.
large. However, by including these elements, stan-
dard dropout is considerably slower, as we show in
our experiments.
Scrambling noise A third alternative is to
?scramble? the features by randomly selecting al-
ternative features within each template. For a fea-
ture ? belonging to a template F , with probability
p we will draw a noise feature ? also belonging
to F , according to some distribution q. In this
work, we use an uniform distribution, in which
q
?
=
1
|F |
. However, the below solutions will also
hold for other scrambling distributions, such as
mean-preserving distributions.
Again, it is possible to analytically marginal-
ize over this noise. Recall that E[Q] =
?
n
i=1
E[
?
x
i
?
x
>
i
]. An off-diagonal entry in the ma-
trix
?
x
?
x
>
which involves features ? and ? belong-
ing to different templates (F
?
6= F
?
) can take four
different values (x
i,?
denotes feature ? in x
i
):
? x
i,?
x
i,?
if both features are unchanged,
which happens with probability (1? p)
2
.
? 1 if both features are chosen as noise features,
which happens with probability p
2
q
?
q
?
.
? x
i,?
or x
i,?
if one feature is unchanged and
the other one is chosen as the noise feature,
which happens with probability p(1 ? p)q
?
or p(1? p)q
?
.
The diagonal entries take the first two values
above, with probability 1 ? p and pq
?
respec-
tively. Other entries will be all zero (only one
feature belonging to the same template will fire
in x
i
). We can use similar reasoning to compute
the expectation of P. With probability (1 ? p),
the original features are preserved, and we add the
outer-product x
i
x
>
i
; with probability p, we add the
outer-product x
i
q
>
. Therefore E[P] can be com-
puted as the sum of these terms.
3 Experiments
We compare these methods on historical Por-
tuguese part-of-speech tagging, creating domains
over historical epochs.
3.1 Experiment setup
Datasets We use the Tycho Brahe corpus to
evaluate our methods. The corpus contains a total
of 1,480,528 manually tagged words. It uses a set
of 383 tags and is composed of various texts from
540
historical Portuguese, from 1502 to 1836. We di-
vide the texts into fifty-year periods to create dif-
ferent domains. Table 1 presents some statistics of
the datasets. We hold out 5% of data as develop-
ment data to tune parameters. The two most recent
domains (1800-1849 and 1750-1849) are treated
as source domains, and the other domains are tar-
get domains. This scenario is motivated by train-
ing a tagger on a modern newstext corpus and ap-
plying it to historical documents.
Dataset
# of Tokens
Total Narrative Letters Dissertation Theatre
1800-1849 125719 91582 34137 0 0
1750-1799 202346 57477 84465 0 60404
1700-1749 278846 0 130327 148519 0
1650-1699 248194 83938 115062 49194 0
1600-1649 295154 117515 115252 62387 0
1550-1599 148061 148061 0 0 0
1500-1549 182208 126516 0 55692 0
Overall 1480528 625089 479243 315792 60404
Table 1: Statistics of the Tycho Brahe Corpus
CRF tagger We use a conditional random field
tagger, choosing CRFsuite because it supports
arbitrary real valued features (Okazaki, 2007),
with SGD optimization. Following the work of
Nogueira Dos Santos et al (2008) on this dataset,
we apply the feature set of Ratnaparkhi (1996).
There are 16 feature templates and 372, 902 fea-
tures in total. Following Blitzer et al (2006), we
consider pivot features that appear more than 50
times in all the domains. This leads to a total of
1572 pivot features in our experiments.
Methods We compare mDA with three alterna-
tive approaches. We refer to baseline as training
a CRF tagger on the source domain and testing on
the target domain with only base features. We also
include PCA to project the entire dataset onto a
low-dimensional sub-space (while still including
the original features). Finally, we compare against
Structural Correspondence Learning (SCL; Blitzer
et al, 2006), another feature learning algorithm.
In all cases, we include the entire dataset to com-
pute the feature projections; we also conducted ex-
periments using only the test and training data for
feature projections, with very similar results.
Parameters All the hyper-parameters are de-
cided with our development data on the training
set. We try different low dimension K from 10 to
2000 for PCA. Following Blitzer (2008) we per-
form feature centering/normalization, as well as
rescaling for SCL. The best parameters for SCL
are dimensionality K = 25 and rescale factor
? = 5, which are the same as in the original pa-
per. For mDA, the best corruption level is p = 0.9
for dropout noise, and p = 0.1 for scrambling
noise. Structured dropout noise has no free hyper-
parameters.
3.2 Results
Table 2 presents results for different domain adap-
tation tasks. We also compute the transfer ra-
tio, which is defined as
adaptation accuracy
baseline accuracy
, shown in
Figure 1. The generally positive trend of these
graphs indicates that adaptation becomes progres-
sively more important as we select test sets that are
more temporally remote from the training data.
In general, mDA outperforms SCL and PCA,
the latter of which shows little improvement over
the base features. The various noising approaches
for mDA give very similar results. However, struc-
tured dropout is orders of magnitude faster than
the alternatives, as shown in Table 3. The scram-
bling noise is most time-consuming, with cost
dominated by a matrix multiplication.
Method PCA SCL
mDA
dropout structured scambling
Time 7,779 38,849 8,939 339 327,075
Table 3: Time, in seconds, to compute the feature
transformation
4 Related Work
Domain adaptation Most previous work on do-
main adaptation focused on the supervised setting,
in which some labeled data is available in the tar-
get domain (Jiang and Zhai, 2007; Daum?e III,
2007; Finkel and Manning, 2009). Our work fo-
cuses on unsupervised domain adaptation, where
no labeled data is available in the target domain.
Several representation learning methods have been
proposed to solve this problem. In structural corre-
spondence learning (SCL), the induced represen-
tation is based on the task of predicting the pres-
ence of pivot features. Autoencoders apply a sim-
ilar idea, but use the denoised instances as the la-
tent representation (Vincent et al, 2008; Glorot et
al., 2011b; Chen et al, 2012). Within the con-
text of denoising autoencoders, we have focused
541
Task baseline PCA SCL
mDA
dropout structured scrambling
from 1800-1849
? 1750 89.12 89.09 89.69 90.08 90.08 90.01
? 1700 90.43 90.43 91.06 91.56 91.57 91.55
? 1650 88.45 88.52 87.09 88.69 88.70 88.57
? 1600 87.56 87.58 88.47 89.60 89.61 89.54
? 1550 89.66 89.61 90.57 91.39 91.39 91.36
? 1500 85.58 85.63 86.99 88.96 88.95 88.91
from 1750-1849
? 1700 94.64 94.62 94.81 95.08 95.08 95.02
? 1650 91.98 90.97 90.37 90.83 90.84 90.80
? 1600 92.95 92.91 93.17 93.78 93.78 93.71
? 1550 93.27 93.21 93.75 94.06 94.05 94.02
? 1500 89.80 89.75 90.59 91.71 91.71 91.68
Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849.
Figure 1: Transfer ratio for adaptation to historical text
on dropout noise, which has also been applied as
a general technique for improving the robustness
of machine learning, particularly in neural net-
works (Hinton et al, 2012; Wang et al, 2013).
On the specific problem of sequence labeling,
Xiao and Guo (2013) proposed a supervised do-
main adaptation method by using a log-bilinear
language adaptation model. Dhillon et al (2011)
presented a spectral method to estimate low di-
mensional context-specific word representations
for sequence labeling. Huang and Yates (2009;
2012) used an HMM model to learn latent rep-
resentations, and then leverage the Posterior Reg-
ularization framework to incorporate specific bi-
ases. Unlike these methods, our approach uses a
standard CRF, but with transformed features.
Historical text Our evaluation concerns syntac-
tic analysis of historical text, which is a topic of in-
creasing interest for NLP (Piotrowski, 2012). Pen-
nacchiotti and Zanzotto (2008) find that part-of-
speech tagging degrades considerably when ap-
plied to a corpus of historical Italian. Moon and
Baldridge (2007) tackle the challenging problem
of tagging Middle English, using techniques for
projecting syntactic annotations across languages.
Prior work on the Tycho Brahe corpus applied su-
pervised learning to a random split of test and
training data (Kepler and Finger, 2006; Dos San-
tos et al, 2008); they did not consider the domain
adaptation problem of training on recent data and
testing on older historical text.
5 Conclusion and Future Work
Denoising autoencoders provide an intuitive so-
lution for domain adaptation: transform the fea-
tures into a representation that is resistant to the
noise that may characterize the domain adaptation
process. The original implementation of this idea
produced this noise directly (Glorot et al, 2011b);
later work showed that dropout noise could be an-
alytically marginalized (Chen et al, 2012). We
take another step towards simplicity by showing
that structured dropout can make marginalization
even easier, obtaining dramatic speedups without
sacrificing accuracy.
Acknowledgments : We thank the reviewers for
useful feedback. This research was supported by
National Science Foundation award 1349837.
542
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?06, pages 120?128, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.
John Blitzer. 2008. Domain Adaptation of Natural
Language Processing Systems. Ph.D. thesis, Uni-
versity of Pennsylvania.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In John Langford and
Joelle Pineau, editors, Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML-
12), ICML ?12, pages 767?774. ACM, New York,
NY, USA, July.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In ACL, volume 1785, page 1787.
Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In NIPS, volume 24, pages 199?207.
C??cero Nogueira Dos Santos, Ruy L Milidi?u, and
Ra?ul P Renter??a. 2008. Portuguese part-of-speech
tagging using entropy guided transformation learn-
ing. In Computational Processing of the Portuguese
Language, pages 143?152. Springer.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In Proceedings of NAACL,
Atlanta, GA.
Jenny Rose Finkel and Christopher D Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 602?610. Association for Computa-
tional Linguistics.
Charlotte Galves and Pablo Faria. 2010. Ty-
cho Brahe Parsed Corpus of Historical Por-
tuguese. http://www.tycho.iel.unicamp.br/ ty-
cho/corpus/en/index.html.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011a. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011b. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 513?520.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, pages 495?503. Association for Compu-
tational Linguistics.
Fei Huang and Alexander Yates. 2012. Biased rep-
resentation learning for domain adaptation. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1313?1323. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In ACL,
volume 2007, page 22.
F?abio N Kepler and Marcelo Finger. 2006. Comparing
two markov methods for part-of-speech tagging of
portuguese. In Advances in Artificial Intelligence-
IBERAMIA-SBIA 2006, pages 482?491. Springer.
Taesun Moon and Jason Baldridge. 2007. Part-of-
speech tagging for middle english through align-
ment and projection of parallel diachronic texts. In
EMNLP-CoNLL, pages 390?399.
C??cero Nogueira Dos Santos, Ruy L. Milidi?u, and
Ra?ul P. Renter??a. 2008. Portuguese part-of-speech
tagging using entropy guided transformation learn-
ing. In Proceedings of the 8th international con-
ference on Computational Processing of the Por-
tuguese Language, PROPOR ?08, pages 143?152,
Berlin, Heidelberg. Springer-Verlag.
Naoaki Okazaki. 2007. Crfsuite: a fast implementa-
tion of conditional random fields (crfs).
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345?1359.
Marco Pennacchiotti and Fabio Massimo Zanzotto.
2008. Natural language processing across time:
An empirical investigation on italian. In Advances
in Natural Language Processing, pages 371?382.
Springer.
Michael Piotrowski. 2012. Natural language process-
ing for historical texts. Synthesis Lectures on Hu-
man Language Technologies, 5(2):1?157.
543
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, April 16.
Noah A Smith. 2011. Linguistic structure prediction.
Synthesis Lectures on Human Language Technolo-
gies, 4(2):1?274.
Anders S?gaard. 2013. Semi-supervised learning and
domain adaptation in natural language processing.
Synthesis Lectures on Human Language Technolo-
gies, 6(2):1?103.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international
conference on Machine learning, pages 1096?1103.
ACM.
Sida I. Wang, Mengqiu Wang, Stefan Wager, Percy
Liang, and Christopher D. Manning. 2013. Fea-
ture noising for log-linear structured prediction. In
Empirical Methods in Natural Language Processing
(EMNLP).
Min Xiao and Yuhong Guo. 2013. Domain adapta-
tion for sequence labeling tasks with a probabilis-
tic language adaptation model. In Sanjoy Dasgupta
and David Mcallester, editors, Proceedings of the
30th International Conference on Machine Learn-
ing (ICML-13), volume 28, pages 293?301. JMLR
Workshop and Conference Proceedings.
544
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 19?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Social Links from Latent Topics in Microblogs?
Kriti Puniyani and Jacob Eisenstein and Shay Cohen and Eric P. Xing
School of Computer Science
Carnegie Mellon University
{kpuniyan,jacobeis,scohen,epxing}@cs.cmu.edu
1 Introduction
Language use is overlaid on a network of social con-
nections, which exerts an influence on both the topics
of discussion and the ways that these topics can be ex-
pressed (Halliday, 1978). In the past, efforts to under-
stand this relationship were stymied by a lack of data, but
social media offers exciting new opportunities. By com-
bining large linguistic corpora with explicit representa-
tions of social network structures, social media provides
a new window into the interaction between language and
society. Our long term goal is to develop joint sociolin-
guistic models that explain the social basis of linguistic
variation.
In this paper we focus on microblogs: internet jour-
nals in which each entry is constrained to a few words
in length. While this platform receives high-profile at-
tention when used in connection with major news events
such as natural disasters or political turmoil, less is
known about the themes that characterize microblogging
on a day-to-day basis. We perform an exploratory anal-
ysis of the content of a well-known microblogging plat-
form (Twitter), using topic models to uncover latent se-
mantic themes (Blei et al, 2003). We then show that these
latent topics are predictive of the network structure; with-
out any supervision, they predict which other microblogs
a user is likely to follow, and to whom microbloggers will
address messages. Indeed, our topical link predictor out-
performs a competitive supervised alternative from tra-
ditional social network analysis. Finally, we explore the
application of supervision to our topical link predictor,
using regression to learn weights that emphasize topics
of particular relevance to the social network structure.
2 Data
We acquired data from Twitter?s streaming ?Gardenhose?
API, which returned roughly 15% of all messages sent
over a period of two weeks in January 2010. This com-
?We thank the reviews for their helpful suggestions and Brendan
O?Connor for making the Twitter data available.
prised 15GB of compressed data; we aimed to extract a
representative subset by first sampling 500 people who
posted at least sixteen messages over this period, and
then ?crawled? at most 500 randomly-selected followers
of each of these original authors. The resulting data in-
cludes 21,306 users, 837,879 messages, and 10,578,934
word tokens.
Text Twitter contains highly non-standard orthography
that poses challenges for early-stage text processing.1 We
took a conservative approach to tokenization, splitting
only on whitespaces and apostrophes, and eliminating
only token-initial and token-final punctuation characters.
Two markers are used to indicate special tokens: #, indi-
cating a topic (e.g. #curling); and @, indicating that the
message is addressed to another user. Topic tokens were
included after stripping the leading #, but address tokens
were removed. All terms occurring less than 50 times
were removed, yielding a vocabulary of 11,425 terms.
Out-of-vocabulary items were classified as either words,
URLs, or numbers. To ensure a fair evaluation, we re-
moved ?retweets? ? when a user reposts verbatim the
message of another user ? if the original message author
is also part of the dataset.
Links We experiment with two social graphs extracted
from the data: a follower graph and a communication
graph. The follower graph places directed edges between
users who have chosen to follow each other?s updates;
the message graph places a directed edge between users
who have addressed messages to each other (using the @
symbol). Huberman et al (2009) argue that the commu-
nication graph captures direct interactions and is thus a
more accurate representation of the true underlying social
structure, while the follower graph contains more con-
nections than could possibly be maintained in a realistic
social network.
1For example, some tweets use punctuation for tokenization (You
look like a retired pornstar!lmao) while others
use punctuation inside the token (lOv!n d!s th!ng call3d
l!f3).
19
Figure 1: Mean rank of test links (lower is better), reported over 4-fold cross-validation. Common-neighbors is a network-based
method that ignores text; the LDA (Latent Dirichlet Allocation) methods are grouped by number of latent topics.
3 Method
We constructed a topic model over twitter messages,
identifying the latent themes that characterize the cor-
pus. In standard topic modeling methodology, topics de-
fine distributions over vocabulary items, and each docu-
ment contains a set of latent topic proportions (Blei et al,
2003). However, the average message on Twitter is only
sixteen word tokens, which is too sparse for traditional
topic modeling; instead, we gathered together all of the
messages from a given user into a single document. Thus
our model learns the latent topics that characterize au-
thors, rather than messages.
Authors with similar topic proportions are likely to
share interests or dialect, suggesting potential social con-
nections. Author similarity can be quantified without
supervision by taking the dot product of the topic pro-
portions. If labeled data is available (a partially ob-
served network), then regression can be applied to learn
weights for each topic. Chang and Blei (2009) describe
such a regression-based predictor, which takes the form
exp
(
??T (z?i ? z?j) ? (z?i ? z?j)? ?
)
, denoting the pre-
dicted strength of connection between authors i and j.
Here z?i (z?j) refers to the expected topic proportions for
user i (j), ? is a vector of learned regression weights, and
? is an intercept term which is only necessary if a the link
prediction function must return a probability. We used
the updates from Chang and Blei to learn ? in a post hoc
fashion, after training the topic model.
4 Results
We constructed topic models using an implemen-
tation of variational inference2 for Latent Dirich-
let Allocation (LDA). The results of the run with
the best variational bound on 50 topics can be
found at http://sailing.cs.cmu.edu/
socialmedia/naacl10ws/. While many of
the topics focus on content (for example, electronics
and sports), others capture distinct languages and even
dialect variation. Such dialects are particularly evident in
2http://www.cs.princeton.edu/?blei/lda-c
stopwords (you versus u). Structured topic models that
explicitly handle these two orthogonal axes of linguistic
variation are an intriguing possibility for future work.
We evaluate our topic-based approach for link predic-
tion on both the message and follower graphs, compar-
ing against an approach that only considers the network
structure. Liben-Nowell and Kleinberg (2003) perform
a quantitative comparison of such approaches, finding
that the relatively simple technique of counting the num-
ber of shared neighbors between two nodes is a surpris-
ingly competitive predictor of whether they are linked;
we call this approach common-neighbors. We evaluate
this method and our own supervised LDA+regression ap-
proach by hiding half of the edges in the graph, and pre-
dicting them from the other half.
For each author in the dataset, we apply each method
to rank all possible links; the evaluation computes the av-
erage rank of the true links that were held out (for our
data, a random baseline would score 10653 ? half the
number of authors in the network). As shown in Figure
1, topic-based link prediction outperforms the alternative
that considers only the graph structure. Interestingly, post
hoc regression on the topic proportions did not consis-
tently improve performance, though joint learning may
do better (e.g., Chang and Blei, 2009). The text-based ap-
proach is especially strong on the message graph, while
the link-based approach is more competitive on the fol-
lowers graph; a model that captures both features seems
a useful direction for future work.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet aloca-
tion. Journal of Machine Learning Research, 3:993?1022.
J. Chang and D. Blei. 2009. Hierarchical relational models for
document networks. Annals of Applied Statistics.
M.A.K. Halliday. 1978. Language as social semiotic: The
social interpretation of language and meaning. University
Park Press.
Bernardo Huberman, Daniel M. Romero, and Fang Wu. 2009.
Social networks that matter: Twitter under the microscope.
First Monday, 14(1?5), January.
D. Liben-Nowell and J. Kleinberg. 2003. The link prediction
problem for social networks. In Proc. of CIKM.
20
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 2?12,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Databases of Named Entities from Bayesian Nonparametrics
Jacob Eisenstein Tae Yano William W. Cohen Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,taey,wcohen,nasmith,epxing}@cs.cmu.edu
Abstract
We present a nonparametric Bayesian ap-
proach to extract a structured database of enti-
ties from text. Neither the number of entities
nor the fields that characterize each entity are
provided in advance; the only supervision is
a set of five prototype examples. Our method
jointly accomplishes three tasks: (i) identify-
ing a set of canonical entities, (ii) inferring a
schema for the fields that describe each entity,
and (iii) matching entities to their references in
raw text. Empirical evaluation shows that the
approach learns an accurate database of enti-
ties and a sensible model of name structure.
1 Introduction
Consider the task of building a set of structured
records from a collection of text: for example, ex-
tracting the names of people or businesses from
blog posts, where each full name decomposes into
fields corresponding to first-name, last-name, title,
etc. To instruct a person to perform this task, one
might begin with a few examples of the records to
be obtained; assuming that the mapping from text to
records is relatively straightforward, no additional
instruction would be necessary. In this paper, we
present a method for training information extraction
software in the same way: starting from a small table
of partially-complete ?prototype? records (Table 1),
our system learns to add new entries and fields to
the table, while simultaneously aligning the records
to text.
We assume that the dimensionality of the database
is unknown, so that neither the number of entries
John McCain Sen. Mr.
George Bush W. Mr.
Hillary Clinton Rodham Mrs.
Barack Obama Sen.
Sarah Palin
Table 1: A set of partially-complete prototype records,
which constitutes the only supervision for the system.
nor the number of fields is specified in advance. To
accommodate this uncertainty, we apply a Bayesian
model which is nonparametric along three dimen-
sions: the assignment of text mentions to entities
(making popular entries more likely while always al-
lowing new entries); the alignment of individual text
tokens to fields (encouraging the re-use of common
fields, but permitting the creation of new fields); and
the assignment of values to entries in the database
itself (encouraging the reuse of values across entries
in a given field). By adaptively updating the con-
centration parameter of stick-breaking distribution
controlling the assignment of values to entries in the
database, our model can learn domain-specific infor-
mation about each field: for example, that titles are
often repeated, while names are more varied.
Our system?s input consists of a very small proto-
type table and a corpus of text which has been au-
tomatically segmented to identify names. Our de-
sired output is a set of structured records in which
each field contains a single string ? not a distribu-
tion over strings, which would be more difficult to
interpret. This requirement induces a tight proba-
bilistic coupling between the assignment of text to
cells in the table, so special care is required to ob-
2
tain efficient inference. Our procedure alternates
between two phases. In the first phase, we per-
form collapsed Gibbs sampling on the assignments
of string mentions to rows and columns in the table,
while marginalizing the values of the table itself. In
the second phase, we apply Metropolis-Hastings to
swap the values of columns in the table, while simul-
taneously relabeling the affected strings in the text.
Our model performs three tasks: it constructs a
set of entities from raw text, matches mentions in
text with the entities to which they refer, and discov-
ers general categories of tokens that appear in names
(such as titles and first names). We are aware of
no existing system that performs all three of these
tasks jointly. We evaluate on a dataset of political
blogs, measuring our system?s ability to discover
a set of reference entities (recall) while maintain-
ing a compact number of rows and columns (pre-
cision). With as few as five partially-complete pro-
totype examples, our approach gives accurate tables
that match well against a manually-annotated refer-
ence list. Our method outperforms a baseline single-
link clustering approach inspired by one of the most
successful entries (Elmacioglu et al, 2007) in the
SEMEVAL ?Web People Search? shared task (Ar-
tiles et al, 2007).
2 Task Definition
In this work, we assume that a bag of M mentions
in text have been identified. The mth mention wm
is a sequence of contiguous word tokens (its length
is denoted Nm) understood to refer to a real-world
entity. The entities (and the mapping of mentions to
entities) are not known in advance. While our focus
in this paper is names of people, the task is defined
in a more generic way.
Formally, the task is to construct a table x where
rows correspond to entities and columns to func-
tional fields. The number of entities and the num-
ber of fields are not prespecified. x?,j denotes the
jth column of x, and xi,j is a single word type fill-
ing the cell in row i, column j. An example is Ta-
ble 1, where the fields are first-name, last-name, ti-
tle, middle-name, and so on. In addition to the table,
we require that each mention be mapped to an en-
tity (i.e., a row in the table). Success at this task
therefore requires (i) identifying entities, (ii) discov-
ering the internal structure of mentions (effectively
canonicalizing them), and (iii) mapping mentions
to entities (therefore resolving coreference relation-
ships among mentions). Note that this task differs
from previous work on knowledge base population
(e.g., McNamee, 2009) because the schema is not
formally defined in advance; rather, the number of
fields and their meaning must be induced from just
a few prototype examples.
To incorporate partial supervision, a subset of the
table x is specified manually by an annotator. We
denote this subset of ?prototypes? by x?; for entries
that are unspecified by the user, we write x?i,j = ?.
Prototypes are not assumed to provide complete in-
formation for any entity.
3 Model
We now craft a nonparametric generative story that
explains both the latent table and the observed men-
tions. The model incorporates three nonparamet-
ric components, allowing an unbounded number of
rows (entities) and columns (fields), as well as an un-
bounded number of values per column (field values).
A plate diagram for the graphical model is shown in
Figure 1.
A key point is that the column distributions ?
range over possible values at the entity level, not
over mentions in text. For example, ?2 might be
the distribution over possible last names and ?3 the
distribution over elected office titles. Note that ?2
would contain a low value for the last name Obama
? which indicates that few people have this last
name ? even though a very high proportion of men-
tions in our data include the string Obama.
The user-generated entries (x?) can still be treated
as the outcome of the generative process: using ex-
changeability, we treat these entries as the first sam-
ples drawn in each column. In this work, we treat
them as fully observed, but it is possible to treat
them as noisy and incorporate a stochastic depen-
dency between xi,j and x?i,j .
4 Inference
We now develop sampling-based inference for the
model described in the previous section. We be-
gin with a token-based collapsed Gibbs sampler, and
then add larger-scale Metropolis-Hastings moves.
3
? ?2
x ? ?
w r ?r
c ?r
?c ?c
Figure 1: A plate diagram for the
text-and-tables graphical model.
The upper plate is the table x, and
the lower plate is the set of textual
mentions. Notation is defined in the
generative model to the right.
? Generate the table entries. For each column j,
? Draw a concentration parameter ?j from a log-normal distribution,
log?j ? N (?, ?2).
? Draw a distribution over strings from a Dirichlet process ?j ?
DP(?j , G0), where the base distribution G0 is a uniform distribution
over strings in a fixed character alphabet, up to an arbitrary finite length.
? For each row i, draw the entry xi,j ? ?j .
? Generate the text mentions.
? Draw a prior distribution over rows from a stick-breaking distribution,
?r ? Stick(?r).
? Draw a prior distribution over columns from a stick-breaking distribu-
tion, ?c ? Stick(?c).
? For each mention wm,
? Draw a row in the table rm ? ?r.
? For each word token wm,n (n ? {1, . . . , Nm}),
? Draw a column in the table cm,n ? ?c.
? Set the text wm,n = xrm,cm,n .
4.1 Gibbs sampling
A key aspect of the generative process is that the
word token wm,n is completely determined by the
table x and the row and column indicators rm and
cm,n: given that a token was generated by row i
and column j of the table, it must be identical to
the value of xi,j . Using Bayes? rule, we can reverse
this deterministic dependence: given the values for
the row and column indices, the entries in the table
are restricted to exact matches with the text men-
tions that they generate. This allows us to marginal-
ize the unobserved entries in the table. We can also
marginalize the distributions ?r, ?c, and ?j , using
the standard collapsed Gibbs sampling equations for
Dirichlet processes. Thus, sampling the row and col-
umn indices is all that is required to explore the en-
tire space of model configurations.
4.1.1 Conditional probability for word tokens
The conditional sampling distributions for both
rows and columns will marginalize the table (be-
sides the prototypes x?). To do this, we must be
able to compute P (wm,n | rm = i, cm,n =
j, x?,w?(m,n), r?m, c?(m,n), ?j), which represents
the probability of generating word wm,n, given
rm = i and cm,n = j. The notation w?(m,n), r?m,
and c?m,n represent the words, row indices, and col-
umn indices for all mentions besides wm,n. For sim-
plicity, we will elide these variables in much of the
subsequent notation.
We first consider the case where we have a user-
specified entry for the row and column ?i, j?? that
is, if x?ij 6= ?. Then the probability is simply,
P (wm,n | rm = i, cm,n = j, x?, . . .) =
{
1, if x?ij = wm,n
0, if x?ij 6= wm,n.
(1)
Because the table cell xij is observed, we do not
marginalize over it; we have a generative probability
of one if the word matches, and zero otherwise. If
the table cell xij is not specified by the user, then we
marginalize over its possible values. For any given
xij , the probability P (wm,n | xij , rm = i, cm,n =
j) is still a delta function, so we have:
?
P (wm,n | xrm,cm,n)P (xrm,cm,n | . . .) dxrm,cm,n
= P (x = wm,n | w?(m,n), r?m, c?(m,n), x?, . . .)
The integral is equal to the probability of the value
of the cell xrm,cm,n being identical to the string
wm,n, given assignments to all other variables. To
compute this probability, we again must consider
two cases: if the cell xi,j has generated some other
string wm?,n? then its value must be identical to that
4
string; otherwise it is unknown. More formally, for
any cell ?i, j?, if ?wm?,n? : rm? = i ? cm?,n? =
j ? ?m?, n?? 6= ?m,n?, then P (xi,j = wm?,n?) = 1;
all other strings have zero probability. If xi,j has not
generated any other entry, then its probability is con-
ditioned on the other elements of the table x. The
known elements of this table are themselves deter-
mined by either the user entries x? or the observa-
tionsw?(m,n). We can define these known elements
as x?, where x?ij = ? if x?ij = ? ? @?m,n? : rm =
i ? cm,n = j. Then we can apply the standard Chi-
nese restaurant process marginalization to obtain:
P (xij | x??(i,j), ?) =
{ N(x??(i,j)=xij)
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) > 0
?
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) = 0
(2)
In our implementation, we maintain the table x?,
updating it as we resample the row and column as-
signments. To construct the conditional distribution
for any given entry, we first consult this table, and
then compute the probability in Equation 2 for en-
tries where x?ij = ?.
4.1.2 Sampling columns
We can now derive sampling equations for the
column indices cm,n. We first apply Bayes? rule
to obtain P (cm,n | wm,n, rm, . . .) ? P (cm,n |
c?(m,n), ?c)?P (wm,n | cm,n, rm, x?, . . .). The like-
lihood term P (wm,n | cm,n, . . .) is defined in the
previous section; we can compute the first factor us-
ing the standard Dirichlet process marginalization
over ?c. Writing N(c?(m,n) = j) for the count of
occurrences of column j in the set c?(m,n), we ob-
tain
P (cm,n = j | c?(m,n), ?c) =
{ N(c?(m,n)=j)
N(c?(m,n))+?c
, if N(c?(m,n) = j) > 0
?c
N(c?(m,n))+?c
, if N(c?(m,n) = j) = 0
(3)
4.1.3 Sampling rows
In principle the row indicators can be sampled
identically to the columns, with the caveat that the
generative probability P (wm | rm, . . .) is a product
across all Nm tokens in wm.1 However, because of
1This relies on the assumption that the values of {cm,n} are
mutually independent given c?m. Future work might apply
the tight probabilistic coupling between the row and
column indicators, straightforward Gibbs sampling
mixes slowly. Instead, we marginalize the column
indicators while sampling r. Only the likelihood
term is affected by this change:
P (wm | rm,w?m, r?m, . . .)
=
?
j
P (c = j | c?m, ?c)P (wm,n | cm,n = j, rm, x?, ?).
(4)
The tokens are conditionally independent given the
row, so we factor and then explicitly marginalize
over each cm,n. The chain rule gives the form in
Equation 4, which contains terms for the prior over
columns and the likelihood of the word; these are
defined in Equations 2 and 3. Note that neither the
inferred table x? nor the heldout column counts c?m
include counts from any of the cells in row m.
4.2 Column swaps
Suppose that during initialization, we encounter the
string Barry Obama before encountering Barack
Obama. We would then put Barry in the first-name
column, and put Barack in some other column for
nicknames. After making these initial decisions,
they would be very difficult to undo using Gibbs
sampling ? we would have to first shift all instances
of Barry to another column, then move an instance
of Barack to the first-name column, and then move
the instances of Barry to the nickname column. To
rectify this issue, we perform sampling on the table
itself, swapping the columns of entries in the table,
while simultaneously updating the relevant column
indices of the mentions.
In the proposal, we select at random a row t and
indices i and j. In the table, we will swap xt,i with
xt,j ; in the text we will swap the values of each cm,n
whenever rm = t and cm,n = i or j. This pro-
posal is symmetric, so no Hastings correction is re-
quired. Because we are simultaneously updating the
table and the column indices, the generative likeli-
hood of the words is unchanged; the only changes
a more structured model of the ways that fields are combined
when mentioning an entity. For example, a first-order Markov
model could learn that family names often follow given names,
but the reverse rarely occurs (in English).
5
in the overall likelihood come from the column in-
dices and the values of the cells in the table. Letting
x?, c? indicate the state of the table and column in-
dices after the proposed move, we will accept with
probability,
Paccept(x? x?) = min
(
1,
P (c?)P (x?)
P (c)P (x)
)
(5)
We first consider the ratio of the table probabili-
ties, P (x
?|?)
P (x|?) . Recall that each column of x is drawn
from a Dirichlet process; appealing to exchangeabil-
ity, we can treat the row t as the last element drawn,
and compute the probabilities P (xt,i | x?(t,i), ?i),
with x?(t,i) indicating the elements of the column i
excluding row t. This probability is given by Equa-
tion 2. For a swap of columns i and j, we compute
the ratio:
P (xt,i | x?(t,j), ?j)P (xt,j | x?(t,i), ?i)
P (xt,i | x?(t,i), ?i)P (xt,j | x?(t,j), ?j)
(6)
Next we consider the ratio of the column proba-
bilities, P (c
?)
P (c) . Again we can apply exchangeabil-
ity, P (c) = P ({cm : rm = t} | {cm? : rm? 6=
t})P ({cm? : rm? 6= t}). The second term P ({cm? :
rm? 6= t}) is unaffected by the move, and so is iden-
tical in both the numerator and denominator of the
likelihood ratio; probabilities from columns other
than i and j also cancel in this way. The remaining
ratio can be simplified to,
(
P (c = j | c?t, ?c)
P (c = i | c?t, ?c)
)N(r=t?c=i)?N(r=t?c=j)
(7)
where the counts N() are from the state of the sam-
pler before executing the proposed move. The prob-
ability P (c = i | c?t, ?c) is defined in Equation 3,
and the overall acceptance ratio for column swaps is
the product of (6) and (7).
4.3 Hyperparameters
The concentration parameters ?r and ?c help to con-
trol the number of rows and columns in the ta-
ble, respectively. These parameters are updated to
their maximum likelihood values using gradient-
based optimization, so our overall inference pro-
cedure is a form of Monte Carlo Expectation-
Maximization (Wei and Tanner, 1990).
The concentration parameters ?j control the di-
versity of each column in the table: if ?j is low then
we expect a high degree of repetition, as with titles;
if ?j is high then we expect a high degree of diver-
sity. When the sampling procedure adds a new col-
umn, there is very little information for how to set
its concentration parameter, as the conditional like-
lihood will be flat. Consequently, greater care must
be taken to handle these priors appropriately.
We place a log-normal hyperprior on the col-
umn concentration parameters, log?j ? N (?, ?2).
The parameters of the log-normal are shared across
columns, which provides additional information to
constrain the concentration parameters of newly-
created columns. We then use Metropolis-Hastings
to sample the values of each ?j , using the joint like-
lihood,
P (?j , x?(j) | ?, ?2) ?
exp(?(log?j ? ?)2)?
kj
j ?(?j)
2?2?(nj + ?j)
,
where x?(j) is column j of the inferred table, nj is
the number of specified entries in column j of the
table x? and kj is the number of unique entries in
the column; see Rasmussen (2000) for a derivation.
After repeatedly sampling several values of ?j for
each column in the table, we update ? and ?2 to their
maximum-likelihood estimates.
5 Temporal Prominence
Andy Warhol predicted, ?in the future, everyone will
be world-famous for fifteen minutes.? A model of
temporal dynamics that accounts for the fleeting and
fickle nature of fame might yield better performance
for transient entities, like Joe the Plumber. Among
several alternatives for modeling temporal dynamics
in latent variable models, we choose a simple non-
parametric approach: the recurrent Chinese restau-
rant process (RCRP; Ahmed and Xing, 2008). The
core idea of the RCRP is that time is partitioned into
epochs, with a unique Chinese restaurant process in
each epoch. Each CRP has a prior which takes the
form of pseudo-counts computed from the counts in
previous epochs. We employ the simplest version of
the RCRP, a first-order Markov model in which the
prior for epoch t is equal to the vector of counts for
epoch t? 1:
6
P (r(t)m = i|r
(t)
1...m?1, r
(t?1), ?r) ?
{
N(r(t)1...m?1 = i) + N(r
(t?1) = i), if > 0;
?r, otherwise.
(8)
The count of row i in epoch t ? 1 is written
N(r(t?1) = i); the count in epoch t for mentions
1 to m ? 1 is written N(r(t)1...m?1 = i). As before,
we can apply exchangeability to treat each mention
as the last in the epoch, so during inference we can
replace this with the count N(r(t)?m). Note that there
is zero probability of drawing an entity that has no
counts in epochs t or t ? 1 but exists in some other
epoch; the probability mass ?r is reserved for draw-
ing a new entity, and the chance of this matching
some existing entity from another epoch is vanish-
ingly small.
During Gibbs sampling, we also need to consider
the effect of r(t)m on the subsequent epoch t + 1.
While space does not permit a derivation, the result-
ing probability is proportional to
P (r(t+1)|r(t)?m, r
(t)
m = i, ?r) ?
?
???
???
1 if N(r(t+1) = i) = 0,
N(r(t+1)=i)
?r
if N(r(t)?m = i) = 0,
1 + N(r
(t+1)=i)
N(r(t)?m=i)
if N(r(t)?m = i) > 0.
(9)
This favors entities which are frequent in epoch
t+ 1 but infrequent in epoch t.
The move to a recurrent Chinese restaurant pro-
cess does not affect the sampling equations for the
columns c, nor the concentration parameters of the
table, ?. The only part of the inference procedure
that needs to be changed is the optimization of the
hyperparameter ?r; the log-likelihood is now the
sum across all epochs, and each epoch makes a con-
tribution to the gradient.
6 Evaluation Setup
Our model jointly performs three tasks: identifying
a set of entities, discovering the set of fields, and
matching mention strings with the entities and fields
to which they refer. We are aware of no prior work
that performs these tasks jointly, nor any dataset that
is annotated for all three tasks.2 Consequently, we
focus our quantitative evaluation on what we take to
be the most important subtask: identifying the enti-
ties which are mentioned in raw text. We annotate
a new dataset of blog text for this purpose, and de-
sign precision and recall metrics to reward systems
that recover as much of the reference set as possi-
ble, while avoiding spurious entities and fields. We
also perform a qualitative analysis, noting the areas
where our method outperforms string matching ap-
proaches, and where there is need for further im-
provement.
Data Evaluation was performed on a corpus
of blogs describing United States politics in
2008 (Eisenstein and Xing, 2010). We ran the Stan-
ford Named Entity Recognition system (Finkel et
al., 2005) to obtain a set of 25,000 candidate men-
tions which the system judged to be names of peo-
ple. We then pruned strings that appeared fewer than
four times and eliminated strings with more than
seven tokens (these were usually errors). The result-
ing dataset has 19,247 mentions comprising 45,466
word tokens, and 813 unique mention strings.
Gold standard We develop a reference set of 100
entities for evaluation. This set was created by sort-
ing the unique name strings in the training set by fre-
quency, and manually merging strings that reference
the same entity. We also manually discarded strings
from the reference set if they resulted from errors in
the preprocessing pipeline (tokenization and named
entity recognition). Each entity is represented by
the set of all word tokens that appear in its refer-
ences; there are a total of 231 tokens for the 100 en-
tities. Most entities only include first and last names,
though the most frequent entities have many more:
for example, the entity Barack Obama has known
names: {Barack, Obama, Sen., Mr.}.
Metrics We evaluate the recall and precision of
a system?s response set by matching against the
reference set. The first step is to create a bipar-
tite matching between response and reference enti-
ties.3 Using a cost function that quantifies the sim-
2Recent work exploiting Wikipedia disambiguation pages
for evaluating cross-document coreference suggests an appeal-
ing alternative for future work (Singh et al, 2011).
3Bipartite matchings are typical in information extraction
evaluation metrics (e.g., Doddington et al, 2004).
7
ilarity of response and reference entities, we opti-
mize the matching using the Kuhn-Munkres algo-
rithm (Kuhn, 1955). For recall, the cost function
counts the number of shared word tokens, divided
by the number of word tokens in the reference enti-
ties; the recall is one minus the average cost of the
best matching (with a cost of one for reference enti-
ties that are not matched, and no cost for unmatched
response entities). Precision is computed identically,
but we normalize by the number of word tokens in
the response entity. Precision assigns a penalty of
one to unmatched response entities and no penalty
for unmatched reference entities.
Note that this metric grossly underrates the preci-
sion of all systems: the reference set is limited to 100
entities, but it is clear that our text mentions many
other people. This is harsh but fair: all systems are
penalized equally for identifying entities that are not
present in the reference set, and the ideal system will
recover the fifty reference entities (thus maximizing
recall) while keeping the table as compact as possi-
ble (thus maximizing precision). However, the raw
precision values have little meaning outside the con-
text of a direct comparison under identical experi-
mental conditions.
Systems The initial seed set for our system con-
sists of a partial annotation of five entities (Table 1)
? larger seed sets did not improve performance. We
run the inference procedure described in the previ-
ous section for 20,000 iterations, and then obtain a
final database by taking the intersection of the in-
ferred tables x? obtained at every 100 iterations, start-
ing with iteration 15,000. To account for variance
across Markov chains, we perform three different
runs. We evaluate a non-temporal version of our
model (as described in Sections 3 and 4), and a tem-
poral version with 5 epochs. For the non-temporal
version, a non-parallel C implementation had a wall
clock sampling time of roughly 16 hours; the tem-
poral version required 24 hours.
We compare against a baseline that incrementally
clusters strings into entities using a string edit dis-
tance metric, based on the work of Elmacioglu et
al. (2007). Starting from a configuration in which
each unique string forms its own cluster, we incre-
mentally merge clusters using the single-link crite-
rion, based on the minimum Jaccard edit distance
0.2 0.3 0.4 0.5 0.6 0.70
0.1
0.2
0.3
recall
pre
cis
ion
 
 
baseline
atemporal model
temporal model
Figure 2: The precision and recall of our models, as com-
pared to the curve defined by the incremental clustering
baseline. Each point indicates a unique sampling run.
Bill Clinton Benazir Bhutto
Nancy Pelosi Speaker
John Kerry Sen. Roberts
Martin King Dr. Jr. Luther
Bill Nelson
Table 2: A subset of the entity database discovered by
our model, hand selected to show highlight interesting
success and failure cases.
between each pair of clusters. This yields a series of
outputs that move along the precision-recall curve,
with precision increasing as the clusters encompass
more strings. There is prior work on heuristics for
selecting a stopping point, but we compare our re-
sults against the entire precision-recall curve (Man-
ning et al, 2008).
7 Results
The results of our evaluation are shown in Figure 2.
All sampling runs from our models lie well beyond
the precision-recall curve defined by the baseline
system, demonstrating the ability to achieve reason-
able recall with a far more compact database. The
baseline system can achieve nearly perfect recall by
creating one entity per unique string, but as it merges
strings to improve precision, its recall suffers sig-
nificantly. As noted above, perfect precision is not
possible on this task, because the reference set cov-
ers only a subset of the entities that appear in the
data. However, the numbers do measure the ability
to recover the reference entities in the most compact
table possible, allowing a quantitative comparison of
our models and the baseline approach.
8
Table 2 shows a database identified by the atem-
poral version of our model. The most densely-
populated columns in the table correspond to well-
defined name parts: columns 1 and 2 are almost
exclusively populated with first and last names re-
spectively, and column 3 is mainly populated by ti-
tles. The remaining columns are more of a grab
bag. Column 4 correctly captures Jr. for Martin
Luther King; column 5 correctly captures Luther,
but mistakenly contains Roberts (thus merging the
John Kerry and John Roberts entities), and Bhutto
(thus helping to merge the Bill Clinton and Benazir
Bhutto entities).
The model successfully distinguishes some, but
not all, of the entities that share tokens. For example,
the model separates Bill Clinton from Bill Nelson;
it also separates John McCain from John Kerry
(whom it mistakenly merges with John Roberts).
The ability to distinguish individuals who share first
names is due in part to the model attributing a low
concentration parameter to first names, meaning that
some repetition in the first name column is expected.
The model correctly identifies several titles and al-
ternative names, including the rare title Speaker for
Nancy Pelosi; however, it misses others, such as the
Senator title for Bill Nelson. This may be due in
part to the sample merging procedure used to gener-
ate this table, which requires that a cell contain the
same value in at least 80% of the samples.
Many errors may be attributed to slow mixing.
After mistakenly merging Bhutto and Clinton at
an early stage, the Gibbs sampler ? which treats
each mention independently ? is unable to sep-
arate them. Given that several other mentions of
Bhutto are already in the row occupied by Clin-
ton, the overall likelihood would benefit little from
creating a new row for a single mention, though
moving all such mentions simultaneously would re-
sult in an improvement. Larger scale Metropolis-
Hastings moves, such as split-merge or type-based
sampling (Liang et al, 2010) may help.
8 Related Work
Information Extraction A tradition of research
in information extraction focuses on processing raw
text to fill in the fields of manually-defined tem-
plates, thus populating databases of events or re-
lations (McNamee and Dang, 2009). While early
approaches focused on surface-level methods such
as wrapper induction (Kushmerick et al, 1997),
more recent work in this area includes Bayesian
nonparametrics to select the number of rows in the
database (Haghighi and Klein, 2010a). However,
even in such nonparametric work, the form of the
template and the number of slots are fixed in ad-
vance. Our approach differs in that the number of
fields and their meaning is learned from data. Recent
work by Chambers and Jurafsky (2011) approaches
a related problem, applying agglomerative cluster-
ing over sentences to detect events, and then clus-
tering syntactic constituents to induce the relevant
fields of each event entity. As described in Section 6,
our method performs well against an agglomerative
clustering baseline, though a more comprehensive
comparison of the two approaches is an important
step for future work.
Name Segmentation and Structure A related
stream of research focuses specifically on names:
identifying them in raw text, discovering their struc-
ture, and matching names that refer to the same en-
tity. We do not undertake the problem of named en-
tity recognition (Tjong Kim Sang, 2002), but rather
apply an existing NER system as a preprocessing
step (Finkel et al, 2005). Typical NER systems
do not attempt to discover the internal structure of
names or a database of canonical names, although
they often use prefabricated ?gazetteers? of names
and name parts as features to improve performance
(Borthwick et al, 1998; Sarawagi and Cohen, 2005).
Charniak (2001) shows that it is possible to learn a
model of name structure, either by using coreference
information as labeled data, or by leveraging a small
set of hand-crafted constraints. Elsner et al (2009)
develop a nonparametric Bayesian model of name
structure using adaptor grammars, which they use to
distinguish types of names (e.g., people, places, and
organizations). Li et al (2004) use a set of manually-
crafted ?transformations? of name parts to build a
model of how a name might be rendered in multi-
ple different ways. While each of these approaches
bears on one or more facets of the problem that we
consider here, none provides a holistic treatment of
name disambiguation and structure.
9
Resolving Mentions to Entities The problem of
resolving mentions to entities has been approach
from a variety of different perspectives. There is
an extensive literature on probabilistic record link-
age, in which database records are compared to de-
termine if they are likely to have the same real-world
referents (e.g., Felligi and Sunter, 1969; Bilenko
et al, 2003). Most approaches focus on pairwise
assessments of whether two records are the same,
whereas our method attempts to infer a single coher-
ent model of the underlying relational data. Some
more recent work in record linkage has explicitly
formulated the task of inferring a latent relational
model of a set of observed datasets (e.g., Cohen
et al, 2000; Pasula et al, 2002; Bhattacharya and
Getoor, 2007); however, to our knowledge, these
prior models have all exploited some predefined
database schema (i.e., set of columns), which our
model does not require. Many of these prior mod-
els have been applied to bibliographic data, where
different conventions and abbreviations lead to im-
perfect matches in different references to the same
publication. In our task, we consider name mentions
in raw text; such mentions are short, and may not
offer as many redundant clues for linkage as biblio-
graphic references.
In natural language processing, coreference res-
olution is the task of grouping entity mentions
(strings), in one or more documents, based on their
common referents in the world. Although much of
coreference resolution has on the single document
setting, there has been some recent work on cross-
document coreference resolution (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). The problem we consider
is related to cross-document coreference, although
we take on the additional challenge of providing
a canonicalized name for each referent (the corre-
sponding table row), and in inferring a structured
representation of entity names (the table columns).
For this reason, our evaluation focuses on the in-
duced table of entities, rather than the clustering of
mention strings. The best coreference systems de-
pend on carefully crafted, problem-specific linguis-
tic features (Bengtson and Roth, 2008) and exter-
nal knowledge (Haghighi and Klein, 2010b). Future
work might consider how to exploit such features for
the more holistic information extraction setting.
9 Conclusion
This paper presents a Bayesian nonparametric ap-
proach to recover structured records from text. Us-
ing only a small set of prototype records, we are able
to recover an accurate table that jointly identifies en-
tities and internal name structure. In our view, the
main advantage of a Bayesian approach compared
to more heuristic alternatives is that it facilitates in-
corporation of additional information sources when
available. In this paper, we have considered one
such additional source, incorporating temporal con-
text using the recurrent Chinese restaurant process.
We envision enhancing the model in several other
respects. One promising direction is the incorpo-
ration of name structure, which could be captured
using a first-order Markov model of the transitions
between name parts. In the nonparametric setting,
a transition matrix is unbounded along both dimen-
sions, and this can be handled by a hierarchical
Dirichlet process (HDP; Teh et al2006).4 We en-
vision other potential applications of the HDP: for
example, learning ?topics? of entities which tend to
appear together (i.e., given a mention of Mahmoud
Abbas in the American press, a mention of Ben-
jamin Netanyahu is likely), and handling document-
specific burstiness (i.e., given that an entity is men-
tioned once in a document, it is much more likely
to be mentioned again). Finally, we would like
to incorporate lexical context from the sentences in
which each entity is mentioned, which might help to
distinguish, say, computer science researchers who
share names with former defense secretaries or pro-
fessional basketball players.
Acknowledgments This research was enabled
by AFOSR FA95501010247, DARPA grant
N10AP20042, ONR N000140910758, NSF DBI-
0546594, IIS-0713379, IIS-0915187, IIS-0811562,
an Alfred P. Sloan Fellowship, and Google?s support
of the Worldly Knowledge project at CMU. We
thank the reviewers for their thoughtful feedback.
4One of the reviewers proposed to draw entire column se-
quences from a Dirichlet process. Given the relatively small
number of columns and canonical name forms, this may be a
straightforward and effective alternative to the HDP.
10
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
restaurant process with applications to evolutionary
clustering. In International Conference on Data Min-
ing.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: establishing a
benchmark for the web people search task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 64?69. Associa-
tion for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 294?303, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Indrajit Bhattacharya and Lise Getoor. 2007. Collec-
tive entity resolution in relational data. ACM Trans.
Knowl. Discov. Data, 1(1), March.
Mikhail Bilenko, William W. Cohen, Stephen Fien-
berg, Raymond J. Mooney, and Pradeep Ravikumar.
2003. Adaptive name-matching in information in-
tegration. IEEE Intelligent Systems, 18(5):16?23,
September/October.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Sixth
Workshop on Very Large Corpora New Brunswick,
New Jersey. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proceedings of the
Second Meeting of the North American Chapter of the
Association for Computational Linguistics.
William W. Cohen, Henry Kautz, and David McAllester.
2000. Hardening soft information sources. In Pro-
ceedings of the Sixth International Conference on
Knowledge Discovery and Data Mining, pages 255?
259.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program: Tasks, data, and evaluation. In 4th
international conference on language resources and
evaluation (LREC?04).
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan,
and Dongwon Lee. 2007. Psnus: Web people name
disambiguation by simple clustering with rich features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 268?
271, Prague, Czech Republic, June. Association for
Computational Linguistics.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 164?172, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
I. P. Felligi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Society,
64:1183?1210.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010a. An entity-level
approach to information extraction. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort
?10, pages 291?295, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010b. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Nicholas Kushmerick, Daniel S. Weld, and Robert
Doorenbos. 1997. Wrapper induction for information
extraction. In Proceedings of IJCAI.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In Proceedings of AAAI, pages
419?424.
11
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-Based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 573?581, Los Angeles, California,
June. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Text Analysis Conference (TAC).
Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Rus-
sell, and Ilya Shpitser. 2002. Identity uncertainty and
citation matching. In Advances in Neural Processing
Systems 15, Vancouver, British Columbia. MIT Press.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Carl E. Rasmussen. 2000. The Infinite Gaussian Mixture
Model. In In Advances in Neural Information Process-
ing Systems 12, volume 12, pages 554?560.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information ex-
traction. In Lawrence K. Saul, Yair Weiss, and Le?on
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1185?1192. MIT Press,
Cambridge, MA.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581, December.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the Sixth
Conference on Natural Language Learning.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo Implementation of the EM Algorithm and the
Poor Man?s Data Augmentation Algorithms. Journal
of the American Statistical Association, 85(411):699?
704.
12
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 11?19,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Phonological Factors in Social Media Writing
Jacob Eisenstein
jacobe@gatech.edu
School of Interactive Computing
Georgia Institute of Technology
Abstract
Does phonological variation get transcribed
into social media text? This paper investigates
examples of the phonological variable of con-
sonant cluster reduction in Twitter. Not only
does this variable appear frequently, but it dis-
plays the same sensitivity to linguistic context
as in spoken language. This suggests that when
social media writing transcribes phonological
properties of speech, it is not merely a case of
inventing orthographic transcriptions. Rather,
social media displays influence from structural
properties of the phonological system.
1 Introduction
The differences between social media text and other
forms of written language are a subject of increas-
ing interest for both language technology (Gimpel
et al, 2011; Ritter et al, 2011; Foster et al, 2011)
and linguistics (Androutsopoulos, 2011; Dresner and
Herring, 2010; Paolillo, 1996). Many words that
are endogenous to social media have been linked
with specific geographical regions (Eisenstein et al,
2010; Wing and Baldridge, 2011) and demographic
groups (Argamon et al, 2007; Rao et al, 2010; Eisen-
stein et al, 2011), raising the question of whether
this variation is related to spoken language dialects.
Dialect variation encompasses differences at multi-
ple linguistic levels, including the lexicon, morphol-
ogy, syntax, and phonology. While previous work
on group differences in social media language has
generally focused on lexical differences, this paper
considers the most purely ?spoken? aspect of dialect:
phonology.
Specifically, this paper presents evidence against
the following two null hypotheses:
? H0: Phonological variation does not impact so-
cial media text.
? H1: Phonological variation may introduce new
lexical items into social media text, but not the
underlying structural rules.
These hypotheses are examined in the context of
the phonological variable of consonant cluster reduc-
tion (also known as consonant cluster simplification,
or more specifically, -/t,d/ deletion). When a word
ends in cluster of consonant sounds ? for exam-
ple, mist or missed ? the cluster may be simplified,
for example, to miss. This well-studied variable has
been demonstrated in a number of different English
dialects, including African American English (Labov
et al, 1968; Green, 2002), Tejano and Chicano En-
glish (Bayley, 1994; Santa Ana, 1991), and British
English (Tagliamonte and Temple, 2005); it has also
been identified in other languages, such as Quebe-
cois French (Co?te?, 2004). While some previous work
has cast doubt on the influence of spoken dialects on
written language (Whiteman, 1982; Thompson et al,
2004), this paper presents large-scale evidence for
consonant cluster reduction in social media text from
Twitter ? in contradiction of the null hypothesis H0.
But even if social media authors introduce new
orthographic transcriptions to capture the sound of
language in the dialect that they speak, such innova-
tions may be purely lexical. Phonological variation
is governed by a network of interacting preferences
that include the surrounding linguistic context. Do
11
these structural aspects of phonological variation also
appear in written social media?
Consonant cluster reduction is a classic example
of the complex workings of phonological variation:
its frequency depends on the morphology of the word
in which it appears, as well as the phonology of the
preceding and subsequent segments. The variable
is therefore a standard test case for models of the
interaction between phonological preferences (Guy,
1991). For our purposes, the key point is that con-
sonant cluster reduction is strongly inhibited when
the subsequent phonological segment begins with a
vowel. The final t in left is more likely to be deleted
in I left the house than in I left a tip. Guy (1991)
writes, ?prior studies are unanimous that a following
consonant promotes deletion more readily than a fol-
lowing vowel,? and more recent work continues to
uphold this finding (Tagliamonte and Temple, 2005).
Consonant cluster reduction thus provides an op-
portunity to test the null hypothesis H1. If the intro-
duction of phonological variation into social media
writing occurs only on the level of new lexical items,
that would predict that reduced consonant clusters
would be followed by consonant-initial and vowel-
initial segments at roughly equal rates. But if conso-
nant cluster reduction is inhibited by adjacent vowel-
initial segments in social media text, that would argue
against H1. The experiments in this paper provide ev-
idence of such context-sensitivity, suggesting that the
influence of phonological variation on social media
text must be deeper than the transcription of invidual
lexical items.
2 Word pairs
The following word pairs were considered:
? left / lef
? just / jus
? with / wit
? going / goin
? doing / doin
? know / kno
The first two pairs display consonant cluster re-
duction, specifically t-deletion. As mentioned above,
consonant cluster reduction is a property of African
American English (AAE) and several other English
dialects. The pair with/wit represents a stopping
of the interdental fricative, a characteristic of New
York English (Gordon, 2004), rural Southern En-
glish (Thomas, 2004), as well as AAE (Green, 2002).
The next two pairs represent ?g-dropping?, the re-
placement of the velar nasal with the coronal nasal,
which has been associated with informal speech in
many parts of the English-speaking world.1 The final
word pair know/kno does not differ in pronunciation,
and is included as a control.
These pairs were selected because they are all
frequently-used words, and because they cover a
range of typical ?shortenings? in social media and
other computer mediated communication (Gouws et
al., 2011). Another criterion is that each shortened
form can be recognized relatively unambiguously.
Although wit and wan are standard English words,
close examination of the data did not reveal any ex-
amples in which the surface forms could be construed
to indicate these words. Other words were rejected
for this reason: for example, best may be reduced
to bes, but this surface form is frequently used as an
acronym for Blackberry Enterprise Server.
Consonant cluster reduction may be combined
with morphosyntactic variation, particularly in
African American English. Thompson et al (2004)
describe several such cases: zero past tense (mother
kiss(ed) them all goodbye), zero plural (the children
made their bed(s)), and subject-verb agreement (then
she jump(s) on the roof ). In each of these cases, it is
unclear whether it is the morphosyntactic or phono-
logical process that is responsible for the absence of
the final consonant. Words that feature such ambigu-
ity, such as past, were avoided.
Table 1 shows five randomly sampled examples
of each shortened form. Only the relevant portion
of each message is shown. From consideration of
many examples such as these, it is clear that the
shortened forms lef, jus, wit, goin, doin, kno refer to
the standard forms left, just, with, going, doing, know
in the overwhelming majority of cases.
1Language Log offers an engaging discussion of the
linguistic and cultural history of ?g-dropping.? http:
//itre.cis.upenn.edu/?myl/languagelog/
archives/000878.html
12
1. ok lef the y had a good workout
(ok, left the YMCA, had a good workout)
2. @USER lef the house
3. eat off d wol a d rice and lef d meat
(... left the meat)
4. she nah lef me
(she has not left me)
5. i lef my changer
6. jus livin this thing called life
7. all the money he jus took out the bank
8. boutta jus strt tweatin random shxt
(about to just start tweeting ...)
9. i jus look at shit way different
10. u jus fuckn lamee
11. fall in love wit her
12. i mess wit pockets
13. da hell wit u
(the hell with you)
14. drinks wit my bro
15. don?t fuck wit him
16. a team that?s goin to continue
17. what?s goin on tonight
18. is reign stil goin down
19. when is she goin bck 2 work?
20. ur not goin now where
(you?re not going nowhere)
21. u were doin the same thing
22. he doin big things
23. i?m not doin shit this weekend
24. oh u doin it for haiti huh
25. i damn sure aint doin it in the am
26. u kno u gotta put up pics
27. i kno some people bout to be sick
28. u already kno
29. you kno im not ugly pendeja
30. now i kno why i?m always on netflix
Table 1: examples of each shortened form
3 Data
Our research is supported by a dataset of microblog
posts from the social media service Twitter. This ser-
vice allows its users to post 140-character messages.
Each author?s messages appear in the newsfeeds of
individuals who have chosen to ?follow? the author,
though by default the messages are publicly available
to anyone on the Internet. Twitter has relatively broad
penetration across different ethnicities, genders, and
income levels. The Pew Research Center has repeat-
edly polled the demographics of Twitter (Smith and
Brewer, 2012), finding: nearly identical usage among
women (15% of female internet users are on Twit-
ter) and men (14%); high usage among non-Hispanic
Blacks (28%); an even distribution across income and
education levels; higher usage among young adults
(26% for ages 18-29, 4% for ages 65+).
Twitter?s streaming API delivers an ongoing ran-
dom sample of messages from the complete set of
public messages on the service. The data in this
study was gathered from the public ?Gardenhose?
feed, which is claimed to be approximately 10% of
all public posts; however, recent research suggests
that the sampling rate for geolocated posts is much
higher (Morstatter et al, 2013). This data was gath-
ered over a period from August 2009 through the
end of September 2012, resulting in a total of 114
million messages from 2.77 million different user
accounts (Eisenstein et al, 2012).
Several filters were applied to ensure that the
dataset is appropriate for the research goals of this pa-
per. The dataset includes only messages that contain
geolocation metadata, which is optionally provided
by smartphone clients. Each message must have a
latitude and longitude within a United States census
block, which enables the demographic analysis in
Section 6. Retweets are excluded (both as identified
in the official Twitter API, as well as messages whose
text includes the token ?RT?), as are messages that
contain a URL. Grouping tweets by author, we retain
only authors who have fewer than 1000 ?followers?
(people who have chosen to view the author?s mes-
sages in their newsfeed) and who follow fewer than
1000 individuals.
Specific instances of the word pairs are acquired by
using grep to identify messages in which the short-
ened form is followed by another sequence of purely
13
alphabetic characters. Reservoir sampling (Vitter,
1985) was used to obtain a randomized set of at most
10,000 messages for each word. There were only 753
examples of the shortening lef ; for all other words we
obtain the full 10,000 messages. For each shortened
word, an equal number of samples for the standard
form were obtained through the same method: grep
piped through a reservoir sampler. Each instance
of the standard form must also be followed by a
purely alphabetic string. Note that the total number
of instances is slightly higher than the number of
messages, because a word may appear multiple times
within the same message. The counts are shown in
Table 2.
4 Analysis 1: Frequency of vowels after
word shortening
The first experiment tests the hypothesis that con-
sonant clusters are less likely to be reduced when
followed by a word that begins with a vowel letter.
Table 2 presents the counts for each term, along with
the probability that the next segment begins with the
vowel. The probabilities are accompanied by 95%
confidence intervals, which are computed from the
standard deviation of the binomial distribution.All
differences are statistically significant at p < .05.
The simplified form lef is followed by a vowel
only 19% of the time, while the complete form left is
followed by a vowel 35% of the time. The absolute
difference for jus and just is much smaller, but with
such large counts, even this 2% absolute difference
is unlikely to be a chance fluctuation.
The remaining results are more mixed. The short-
ened form wit is significantly more likely to be fol-
lowed by a vowel than its standard form with. The
two ?g dropping? examples are inconsistent, and trou-
blingly, there is a significant effect in the control
condition. For these reasons, a more fine-grained
analysis is pursued in the next section.
A potential complication to these results is that
cluster reduction may be especially likely in specific
phrases. For example, most can be reduced to mos,
but in a sample of 1000 instances of this reduction,
72% occurred within a single expression: mos def.
This phrase can be either an expression of certainty
(most definitely), or a reference to the performing
artist of the same name. If mos were observed to
word N N(vowel) P(vowel)
lef 753 145 0.193 ? 0.028
left 757 265 0.350 ? 0.034
jus 10336 939 0.091 ? 0.006
just 10411 1158 0.111 ? 0.006
wit 10405 2513 0.242 ? 0.008
with 10510 2328 0.222 ? 0.008
doin 10203 2594 0.254 ? 0.008
doing 10198 2793 0.274 ? 0.009
goin 10197 3194 0.313 ? 0.009
going 10275 1821 0.177 ? 0.007
kno 10387 3542 0.341 ? 0.009
know 10402 3070 0.295 ? 0.009
Table 2: Term counts and probability with which the fol-
lowing segment begins with a vowel. All differences are
significant at p < .05.
be more likely to be followed by a consonant-initial
word than most, this might be attributable to this one
expression.
An inverse effect could explain the high likelihood
that goin is followed by a vowel. Given that the
author has chosen an informal register, the phrase
goin to is likely to be replaced by gonna. One might
hypothesize the following decision tree:
? If formal register, use going
? If informal register,
? If next word is to, use gonna
? else, use goin
Counts for each possibility are shown in Table 3;
these counts are drawn from a subset of the 100,000
messages and thus cannot be compared directly with
Table 2. Nonetheless, since to is by far the most
frequent successor to going, a great deal of going?s
preference for consonant successors can be explained
by the word to.
5 Analysis 2: Logistic regression to control
for lexical confounds
While it is tempting to simply remove going to and
goin to from the dataset, this would put us on a slip-
pery slope: where do we draw the line between lexi-
cal confounds and phonological effects? Rather than
14
total ... to percentage
going 1471 784 53.3%
goin 470 107 22.8%
gonna 1046 n/a n/a
Table 3: Counts for going to and related phrases in the first
100,000 messages in the dataset. The shortened form goin
is far less likely to be followed by to, possibly because of
the frequently-chosen gonna alternative.
word ?? ?? z p
lef/left -0.45 0.10 -4.47 3.9? 10?6
jus/just -0.43 0.11 -3.98 3.4? 10?5
wit/with -0.16 0.03 -4.96 3.6? 10?7
doin/doing 0.08 0.04 2.29 0.011
goin/going -0.07 0.05 -1.62 0.053
kno/know -0.07 0.05 -1.23 0.11
Table 4: Logistic regression coefficients for the VOWEL
feature, predicting the choice of the shortened form. Nega-
tive values indicate that the shortened form is less likely if
followed by a vowel, when controlling for lexical features.
excluding such examples from the dataset, it would
be preferable to apply analytic techniques capable of
sorting out lexical and systematic effects. One such
technique is logistic regression, which forces lexical
and phonological factors to compete for the right to
explain the observed orthographic variations.2
The dependent variable indicates whether the
word-final consonant cluster was reduced. The inde-
pendent variables include a single feature indicating
whether the successor word begins with a vowel, and
additional lexical features for all possible successor
words. If the orthographic variation is best explained
by a small number of successor words, the phono-
logical VOWEL feature will not acquire significant
weight.
Table 4 presents the mean and standard deviation
of the logistic regression coefficient for the VOWEL
feature, computed over 1000 bootstrapping itera-
tions (Wasserman, 2005).3 The coefficient has the
2(Stepwise) logistic regression has a long history in varia-
tionist sociolinguistics, particularly through the ubiquitous VAR-
BRUL software (Tagliamonte, 2006).
3An L2 regularization parameter was selected by randomly
sampling 50 training/test splits. Average accuracy was between
58% and 66% on the development data, for the optimal regular-
ization coefficient.
largest magnitude in cases of consonant cluster re-
duction, and the associated p-values indicate strong
statistical significance. The VOWEL coefficient is
also strongly significant for wit/with. It reaches the
p < .05 threshold for doin/doing, although in this
case, the presence of a vowel indicates a preference
for the shortened form doin ? contra the raw fre-
quencies in Table 2. The coefficient for the VOWEL
feature is not significantly different from zero for
goin/going and for the control kno/know. Note that
since we had no prior expectation of the coefficient
sign in these cases, a two-tailed test would be most
appropriate, with critical value ? = 0.025 to estab-
lish 95% confidence.
6 Analysis 3: Social variables
The final analysis concerns the relationship between
phonological variation and social variables. In spo-
ken language, the word pairs chosen in this study
have connections with both ethnic and regional di-
alects: consonant cluster reduction is a feature of
African-American English (Green, 2002) and Te-
jano and Chicano English (Bayley, 1994; Santa Ana,
1991); th-stopping (as in wit/with) is a feature of
African-American English (Green, 2002) as well as
several regional dialects (Gordon, 2004; Thomas,
2004); the velar nasal in doin and goin is a property
of informal speech. The control pair kno/know does
not correspond to any sound difference, and thus
there is no prior evidence about its relationship to
social variables.
The dataset includes the average latitude and lon-
gitude for each user account in the corpus. It is possi-
ble to identify the county associated with the latitude
and longitude, and then to obtain county-level de-
mographic statistics from the United States census.
An approximate average demographic profile for
each word in the study can be constructed by ag-
gregating the demographic statistics for the counties
of residence of each author who has used the word.
Twitter users do not comprise an unbiased sample
from each county, so this profile can only describe the
demographic environment of the authors, and not the
demographic properties of the authors themselves.
Results are shown in Figure 1. The confidence
intervals reflect the Bonferroni correction for mul-
tiple comparison, setting ? = 0.05/48. The con-
15
lef left jus just wit with goin going doin doing kno know
16
18
20
22
24
26
28
30
%
 b
la
ck
lef left jus just wit with goin going doin doing kno know
60
62
64
66
68
70
72
74
%
 w
hi
te
lef left jus just wit with goin going doin doing kno know
14
16
18
20
22
24
%
 h
isp
an
ic
lef left jus just wit with goin going doin doing kno know
2000
4000
6000
8000
10000
12000
14000
16000
po
p.
 d
en
sit
y
Figure 1: Average demographics of the counties in which users of each term live, with 95% confidence intervals
16
sonant cluster reduction examples are indeed pre-
ferred by authors from densely-populated (urban)
counties with more African Americans, although
these counties tend to prefer all of the non-standard
variants, including the control pair kno/know. Con-
versely, the non-standard variants have aggregate
demographic profiles that include fewer European
Americans. None of the differences regarding the
percentage of Hispanics/Latinos are statistically sig-
nificant. Overall, these results show an associa-
tion between non-standard orthography and densely-
populated counties with high proportions of African
Americans, but we find no special affinity for conso-
nant cluster reduction.
7 Related work
Previous studies of the impact of dialect on writ-
ing have found relatively little evidence of purely
phonological variation in written language. White-
man (1982) gathered an oral/written dataset of inter-
view transcripts and classroom compositions. In the
written data, there are many examples of final con-
sonant deletion: verbal -s (he go- to the pool), plural
-s (in their hand-), possessive -s (it is Sally- radio),
and past tense -ed. However, each of these deletions
is morphosyntactic rather than purely phonological.
They are seen by Whiteman as an omission of the
inflectional suffix, rather than as a transcription of
phonological variation, which she finds to be very
rare in cases where morphosyntactic factors are not in
play. She writes, ?nonstandard phonological features
rarely occur in writing, even when these features are
extremely frequent in the oral dialect of the writer.?
Similar evidence is presented by Thompson et al
(2004), who compare the spoken and written lan-
guage of 50 third-grade students who were identi-
fied as speakers of African American English (AAE).
While each of these students produced a substantial
amount of AAE in spoken language, they produced
only one third as many AAE features in the written
sample. Thompson et al find almost no instances
of purely phonological features in writing, including
consonant cluster reduction ? except in combina-
tion with morphosyntactic features, such as zero past
tense (e.g. mother kiss(ed) them all goodbye). They
propose the following explanation:
African American students have models
for spoken AAE; however, children do not
have models for written AAE... students
likely have minimal opportunities to ex-
perience AAE in print (emphasis in the
original).
This was written in 2004; in the intervening years,
social media and text messages now provide many
examples of written AAE. Unlike classroom settings,
social media is informal and outside the scope of
school control. Whether the increasing prevalence of
written AAE will ultimately lead to widely-accepted
writing systems for this and other dialects is an in-
triguing open question.
8 Conclusions and future work
The experiments in this paper demonstrate that
phonology impacts social media orthography at the
word level and beyond. I have discussed examples of
three such phenomena: consonant cluster reduction,
th-stopping, and the replacement of the velar nasal
with the coronal (?g-dropping?). Both consonant
cluster reduction and th-stopping are significantly in-
fluenced by the phonological context: their frequency
depends on whether the subsequent segment begins
with a vowel. This indicates that when social media
authors transcribe spoken language variation, they
are not simply replacing standard spellings of indi-
vidual words. The more difficult question ? how
phonological context enters into writing ? must be
left for future work.
There are several other avenues along which to con-
tinue this research. The sociolinguistic literature de-
scribes a number of other systematic factors that im-
pact consonant cluster reduction (Guy, 1991; Taglia-
monte and Temple, 2005), and a complete model that
included all such factors might shed additional light
on this phenomenon. In such work it is typical to dis-
tinguish between different types of consonants; for
example, Tagliamonte and Temple (2005) distinguish
obstruents, glides, pauses, and the liquids /r/ and /l/.
In addition, while this paper has equated consonant
letters with consonant sounds, a more careful analy-
sis might attempt to induce (or annotate) the pronun-
ciation of the relevant words. The speech synthesis
literature offers numerous such methods (Bisani and
Ney, 2008), though social media text may pose new
17
challenges, particularly for approaches that are based
on generalizing from standard pronunciation dictio-
naries.
One might also ask whether the phonological sys-
tem impacts all authors to the same extent. Labov
(2007) distinguishes two forms of language change:
transmission, where successive generations of chil-
dren advance a sound change, and diffusion, where
language contact leads adults to ?borrow? aspects
of other languages or dialects. Labov marshalls ev-
idence from regional sound changes to show that
transmission is generally more structural and reg-
ular, while diffusion is more superficial and irreg-
ular; this may be attributed to the ability of child
language learners to recognize structural linguistic
patterns. Does phonological context impact transcrip-
tion equally among all authors in our data, or can we
identify authors whose use of phonological transcrip-
tion is particularly sensitive to context?
Acknowledgments
Thanks to Brendan O?Connor for building the Twitter
dataset that made this research possible. Thanks to
the reviewers for their helpful comments.
References
Jannis Androutsopoulos. 2011. Language change and
digital media: a review of conceptions and evidence. In
Nikolas Coupland and Tore Kristiansen, editors, Stan-
dard Languages and Language Standards in a Chang-
ing Europe. Novus, Oslo.
S. Argamon, M. Koppel, J. Pennebaker, and J. Schler.
2007. Mining the blogosphere: age, gender, and the
varieties of self-expression. First Monday, 12(9).
Robert Bayley. 1994. Consonant cluster reduction
in tejano english. Language Variation and Change,
6(03):303?326.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conversion.
Speech Commun., 50(5):434?451, May.
Marie-He?le`ne Co?te?. 2004. Consonant cluster simplifica-
tion in Que?bec French. Probus: International journal
of Latin and Romance linguistics, 16:151?201.
E. Dresner and S.C. Herring. 2010. Functions of the
nonverbal in cmc: Emoticons and illocutionary force.
Communication Theory, 20(3):249?268.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith, and
Eric P. Xing. 2010. A latent variable model for geo-
graphic lexical variation. In Proceedings of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011.
Discovering sociolinguistic associations with structured
sparsity. In Proceedings of ACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words, October.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From news to comment:
Resources and benchmarks for parsing the language of
web 2.0. In Proceedings of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Di-
panjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: annotation, features, and experiments. In Proceed-
ings of ACL.
Matthew J. Gordon, 2004. A Handbook of Varieties of
English, chapter New York, Philadelphia, and other
northern cities, pages 282?299. Volume 1 of Kortmann
et al (Kortmann et al, 2004).
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy text.
In Proceedings of the First workshop on Unsupervised
Learning in NLP, pages 82?90, Edinburgh, Scotland,
July. Association for Computational Linguistics.
Lisa J. Green. 2002. African American English: A
Linguistic Introduction. Cambridge University Press,
September.
Gregory R. Guy. 1991. Contextual conditioning in
variable lexical phonology. Language Variation and
Change, 3:223?239, June.
Bernd Kortmann, Edgar W. Schneider, and Kate Burridge
et al, editors. 2004. A Handbook of Varieties of En-
glish, volume 1. Mouton de Gruyter, Berlin, Boston.
William Labov, Paul Cohen, Clarence Robins, and John
Lewis. 1968. A study of the Non-Standard english
of negro and puerto rican speakers in new york city.
Technical report, United States Office of Education,
Washington, DC.
William Labov. 2007. Transmission and diffusion. Lan-
guage, 83(2):344?387.
Fred Morstatter, Jurgen Pfeffer, Huan Liu, and Kathleen M.
Carley. 2013. Is the sample good enough? comparing
data from twitter?s streaming api with twitter?s firehose.
In Proceedings of ICWSM.
John C. Paolillo. 1996. Language choice on
soc.culture.punjab. Electronic Journal of Communi-
cation/La Revue Electronique de Communication, 6(3).
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of Workshop on
Search and mining user-generated contents.
18
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of EMNLP.
Otto Santa Ana. 1991. Phonetic simplification processes
in the English of the barrio: A cross-generational soci-
olinguistic study of the Chicanos of Los Angeles. Ph.D.
thesis, University of Pennsylvania.
Aaron Smith and Joanna Brewer. 2012. Twitter use 2012.
Technical report, Pew Research Center, May.
Sali Tagliamonte and Rosalind Temple. 2005. New per-
spectives on an ol? variable: (t,d) in british english.
Language Variation and Change, 17:281?302, Septem-
ber.
Sali A. Tagliamonte. 2006. Analysing Sociolinguistic
Variation. Cambridge University Press.
Erik R Thomas, 2004. A Handbook of Varieties of English,
chapter Rural Southern white accents, pages 87?114.
Volume 1 of Kortmann et al (Kortmann et al, 2004).
Connie A. Thompson, Holly K. Craig, and Julie A. Wash-
ington. 2004. Variable production of african american
english across oracy and literacy contexts. Language,
speech, and hearing services in schools, 35(3):269?282,
July.
Jeffrey S. Vitter. 1985. Random sampling with a reservoir.
ACM Trans. Math. Softw., 11(1):37?57, March.
Larry Wasserman. 2005. All of Nonparametric Statistics
(Springer Texts in Statistics). Springer, October.
Marcia F. Whiteman. 1982. Dialect influence in writing.
In Marcia Farr Whiteman and Carl, editors, Writing:
The Nature, Development, and Teaching of Written
Communication, volume 1: Variation in writing. Rout-
ledge, October.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids. In
Proceedings of ACL.
19
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 97?106,
Baltimore, Maryland USA, June 27, 2014.
c
?2014 Association for Computational Linguistics
Mining Themes and Interests in the Asperger?s and Autism Community
Yangfeng Ji, Hwajung Hong, Rosa Arriaga, Agata Rozga, Gregory Abowd, Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
{jiyfeng,hwajung,arriaga,agata,abowd,jacobe}@gatech.edu
Abstract
Discussion forums offer a new source of
insight for the experiences and challenges
faced by individuals affected by mental
disorders. Language technology can help
domain experts gather insight from these
forums, by aggregating themes and user
behaviors across thousands of conversa-
tions. We present a novel model for web
forums, which captures both thematic con-
tent as well as user-specific interests. Ap-
plying this model to the Aspies Central fo-
rum (which covers issues related to As-
perger?s syndrome and autism spectrum
disorder), we identify several topics of
concern to individuals who report being on
the autism spectrum. We perform the eval-
uation on the data collected from Aspies
Central forum, including 1,939 threads,
29,947 posts and 972 users. Quantita-
tive evaluations demonstrate that the top-
ics extracted by this model are substan-
tially more than those obtained by Latent
Dirichlet Allocation and the Author-Topic
Model. Qualitative analysis by subject-
matter experts suggests intriguing direc-
tions for future investigation.
1 Introduction
Online forums can offer new insights on men-
tal disorders, by leveraging the experiences of af-
fected individuals ? in their own words. Such
insights can potentially help mental health profes-
sionals and caregivers. Below is an example dia-
logue from the Aspies Central forum,
1
where indi-
viduals who report being on the autism spectrum
(and their families and friends) exchange advice
and discuss their experiences:
1
http://www.aspiescentral.com
? User A: Do you feel paranoid at work?
. . . What are some situations in which you
think you have been unfairly treated?
? User B: Actually I am going through some-
thing like that now, and it is very difficult to
keep it under control. . .
? User A: Yes, yes that is it. Exactly . . . I think
it might be an Aspie trait to do that, I mean
over think everything and take it too literally?
? User B: It probably is an Aspie trait. I?ve
been told too that I am too hard on myself.
Aspies Central, like other related forums, has
thousands of such exchanges. However, aggregat-
ing insight from this wealth of information poses
obvious challenges. Manual analysis is extremely
time-consuming and labor-intensive, thus limiting
the scope of data that can be considered. In addi-
tion, manual coding systems raise validity ques-
tions, because they can tacitly impose the pre-
existing views of the experimenter on all sub-
sequent analysis. There is therefore a need for
computational tools that support large-scale ex-
ploratory textual analysis of such forums.
In this paper, we present a tool for automati-
cally mining web forums to explore textual themes
and user interests. Our system is based on Latent
Dirichlet Allocation (LDA; Blei et al, 2003), but is
customized for this setting in two key ways:
? By modeling sparsely-varying topics, we can
easily recover key terms of interest, while
retaining robustness to large vocabulary and
small counts (Eisenstein et al., 2011).
? By modeling author preference by topic, we
can quickly identify topics of interest for each
user, and simultaneously recover topics that
better distinguish the perspectives of each au-
thor.
The key technical challenge in this work lies in
bringing together several disparate modalities into
97
a single modeling framework: text, authorship,
and thread structure. We present a joint Bayesian
graphical model that unifies these facets, discov-
ering both an underlying set of topical themes,
and the relationship of these themes to authors.
We derive a variational inference algorithm for
this model, and apply the resulting software on a
dataset gathered from Aspies Central.
The topics and insights produced by our system
are evaluated both quantitatively and qualitatively.
In a blind comparison with LDA and the author-
topic model (Steyvers et al., 2004), both subject-
matter experts and lay users find the topics gener-
ated by our system to be substantially more coher-
ent and relevant. A subsequent qualitative analysis
aligns these topics with existing theory about the
autism spectrum, and suggests new potential in-
sights and avenues for future investigation.
2 Aspies Central Forum
Aspies Central (AC) is an online forum for indi-
viduals on the autism spectrum, and has publicly
accessible discussion boards. Members of the site
do not necessarily have to have an official diag-
nosis of autism or a related condition. Neurotyp-
ical individuals (people not on the autism spec-
trum) are also allowed to participate in the fo-
rum. The forum includes more than 19 discussion
boards with subjects ranging from general discus-
sions about the autism spectrum to private discus-
sions about personal concerns. As of March 2014,
AC hosts 5,393 threads, 89,211 individual posts,
and 3,278 members.
AC consists of fifteen public discussion boards
and four private discussion boards that require
membership. We collected data only from
publicly-accessible discussion boards. In addition,
we excluded discussion boards that were website-
specific (announcement-and-introduce-yourself),
those mainly used by family and friends of in-
dividuals on the spectrum (friends-and-family) or
researchers (autism-news-and-research), and one
for amusement (forum-games). Thus, we focused
on ten discussion boards (aspergers-syndrome-
Autism-and-HFA, PDD-NOS-social-anxiety-and-
others, obsessions-and-interests, friendships-and-
social-skills, education-and-employment, love-
relationships-and-dating, autism-spectrum-help-
and-support, off-topic-discussion, entertainment-
discussion, computers-technology-discussion), in
which AC users discuss their everyday expe-
? ?d
zdpn
wdpn
m
?k
ad
?
bi
yik ?N PD KAK
Figure 1: Plate diagram. Shaded notes represent observed
variables, clear nodes represent latent variables, arrows in-
dicate probabilistic dependencies, and plates indicate repeti-
tion.
riences, concerns, and challenges. Using the
python library Beautiful Soup, we collected 1,939
threads (29,947 individual posts) from the discus-
sion board archives over a time period from June
1, 2010 to July 27, 2013. For a given post, we
extracted associated metadata such as the author
identifier and posting timestamps.
3 Model Specification
Our goal is to develop a model that captures the
preeminent themes and user behaviors from traces
of user behaviors in online forums. The model
should unite textual content with authorship and
thread structure, by connecting these observed
variables through a set of latent variables rep-
resenting conceptual topics and user preferences.
In this section, we present the statistical specifi-
cation of just such a model, using the machinery
of Bayesian graphical models. Specifically, the
model descibes a stochastic process by which the
observed variables are emitted from prior proba-
bility distributions shaped by the latent variables.
By performing Bayesian statistical inference in
this model, we can recover a probability distribu-
tion around the latent variables of interest.
We now describe the components of the model
that generate each set of observed variables. The
model is shown as a plate diagram in Figure 1, and
the notation is summarized in Table 1.
3.1 Generating the text
The part of the model which produces the text it-
self is similar to standard latent Dirichlet alloca-
tion (LDA) (Blei et al., 2003). We assume a set
of K latent topics, which are distributions over
each word in a finite vocabulary. These topics are
98
Symbol Description
D number of threads
P
d
number of posts in thread d
N
p
number of word tokens in post p
? parameter of topic distribution of threads
?
d
the multinomial distribution of topics specific to the thread d
z
dpn
the topic associated with the nth token in post p of thread d
w
dpn
the nth token in post p of thread d
a
d
authorship distribution for question post and answer posts in
thread d respectively
y
ik
the topic-preference indicator of author i on topic k
b
i
the Gaussian distribution of author i?s selection bias
?
k
topic k in log linear space
m background topic
? topic weights matrix
?
2
?
variance of feature weights
?
2
b
variance of selection bias
? prior probability of authors? preference on any topic
Table 1: Mathematical notations
shared among all D threads in the collection, but
each thread has its own distribution over the top-
ics.
We make use of the SAGE parametrization for
generative models of text (Eisenstein et al., 2011).
SAGE uses adaptive sparsity to induce topics that
deviate from a background word distribution in
only a few key words, without requiring a regular-
ization parameter. The background distribution is
written m, and the deviation for topic k is written
?
k
, so that Pr(w = v|?
k
,m) ? exp (m
v
+ ?
kv
).
Each word tokenw
dpn
(the n
th
word in post p of
thread d) is generated from the probability distri-
bution associated with a single topic, indexed by
the latent variable z
dpn
? {1 . . .K}. This latent
variable is drawn from a prior ?
d
, which is the
probability distribution over topics associated with
all posts in thread d.
3.2 Generating the author
We have metadata indicating the author of each
post, and we assume that users are more likely
to participate in threads that relate to their topic-
specific preference. In addition, some people may
be more or less likely to participate overall. We
extend the LDA generative model to incorporate
each of these intuitions.
For each author i, we define a latent preference
vector y
i
, where y
ik
? {0, 1} indicates whether
the author i prefers to answer questions about
topic k. We place a Bernoulli prior on each y
ik
, so
that y
ik
? Bern(?), where Bern(y; ?) = ?
y
(1 ?
?)
(1?y)
. Induction of y is one of the key infer-
ence tasks for the model, since this captures topic-
specific preference.
It is also a fact that some individuals will partic-
ipate in a conversation regardless of whether they
have anything useful to add. To model this gen-
eral tendency, we add an ?bias? variable b
i
? R.
When b
i
is negative, this means that author i will
be reluctant to participate even when she does have
relevant interests.
Finally, various topics may require different lev-
els of preference; some may capture only general
knowledge that many individuals are able to pro-
vide, while others may be more obscure. We in-
troduce a diagonal topic-weight matrix ?, where
?
kk
= ?
k
? 0 is the importance of preference for
topic k. We can easily generalize the model by in-
cluding non-zero off-diagonal elements, but leave
this for future work.
The generative distribution for the observed au-
thor variable is a log-linear function of y and b:
Pr(a
di
= 1|?
d
,y,?, b) =
exp(?T
d
?y
i
+ b
i
)
?
A
j=1
exp(?T
d
?y
j
+ b
j
)
(1)
This distribution is multinomial over authors; each
author?s probability of responding to a thread de-
pends on the topics in the thread (?
d
), the author?s
preference on those topics (y
i
), the importance of
preference for each topic (?), and the bias parame-
ter b
i
. We exponentiate and then normalize, yield-
ing a multinomial distribution.
The authorship distribution in Equation (1)
refers to a probability of user i authoring a single
response post in thread d (we will handle question
posts next). Let us construct a binary vector a
(r)
d
,
where it is 1 if author i has authored any response
posts in thread d, and zero otherwise. The proba-
bility distribution for this vector can be written
P (a(r)
d
|?
d
,y,?, b) ?
A?
i=1
(
exp(?T
d
?y
i
+ b
i
)
?
A
j=1
exp(?T
d
?y
j
+ b
j
)
)
a
(r)
di
(2)
One of the goals of this model is to distinguish
frequent responders (i.e., potential experts) from
individuals who post questions in a given topic.
Therefore, we make the probability of author i ini-
tiating thread d depend on the value 1 ? y
ki
for
each topic k. We write the binary vector a
(q)
d
,
where a
(q)
di
= 1 if author i has written the ques-
tion post, and zero otherwise. Note that there can
only be one question post, so a
(q)
d
is an indicator
vector. Its probability is written as
p(a(q)
d
|?
d
,y,?, b) ?
A?
i=1
(
exp(?T
d
?(1? y
i
) + b
i
)
?
A
j=1
exp(?T
d
?(1? y
j
) + b
j
)
)
a
(q)
di
(3)
99
We can put these pieces together for a complete
distribution over authorship for thread d:
P (a
d
, |?
d
,y,?, b) ?
A?
i=1
(
exp(?T
d
?y
i
+ b
i
)
?
A
j=1
exp(?T
d
?y
j
+ b
j
)
)
a
(r)
di
?
A?
i=1
(
exp(?T
d
?(1? y
i
) + b
i
)
?
A
j=1
exp(?T
d
?(1? y
j
) + b
j
)
)
a
(q)
di
(4)
where a
d
= {a
(q)
d
,a
(r)
d
}. The probability
p(a
d
|?
d
,y,?, b) combines the authorship distri-
bution of authors from question post and answer
posts in thread d. The identity of the original ques-
tion poster does not appear in the answer vector,
since further posts are taken to be refinements of
the original question.
This model is similar in spirit to super-
vised latent Dirichlet allocation (sLDA) (Blei and
McAuliffe, 2007). However, there are two key dif-
ferences. First, sLDA uses point estimation to ob-
tain a weight for each topic. In contrast, we per-
form Bayesian inference on the author-topic pref-
erence y. Second, sLDA generates the metadata
from the dot-product of the weights and
?
z, while
we use ? directly. The sLDA paper argues that
there is a risk of overfitting, where some of the top-
ics serve only to explain the metadata and never
generate any of the text. This problem does not
arise in our experiments.
3.3 Formal generative story
We are now ready to formally define the generative
process of our model:
1. For each topic k
(a) Set the word probabilities ?
k
=
exp(m+?
k
)?
i
exp(m
i
+?
ki
)
2. For each author i
(a) Draw the selection bias b
i
? N (0, ?
2
b
)
(b) For each topic k
i. Draw the author-topic preference
level y
ik
? Bern(?)
3. For each thread d
(a) Draw topic proportions ?
d
? Dir(?)
(b) Draw the author vector a
d
from Equa-
tion (4)
(c) For each post p
i. For each word in this post
A. Draw topic assignment z
dpn
?
Mult(?
d
)
B. Draw word
w
dpn
? Mult(?
z
dpn
)
4 Inference and estimation
The purpose of inference and estimation is to re-
cover probability distributions and point estimates
for the quantities of interest: the content of the
topics, the assignment of topics to threads, au-
thor preferences for each topic, etc. While recent
progress in probabilistic programming has im-
proved capabilities for automating inference and
estimation directly from the model specification,
2
here we develop a custom algorithm, based on
variational mean field (Wainwright and Jordan,
2008). Specifically, we approximate the distribu-
tion over topic proportions, topic indicators, and
author-topic preference P (?, z,y|w,a,x) with a
mean field approximation
q(?,z,y|?, ?, ?) =
A?
i=1
K?
k=1
q(y
ik
|?
ik
)
D?
d=1
P
d?
p=1
N
p,d?
n=1
q(z
dpn
|?
dpn
)
D?
d=1
q(?
d
|?
d
)
(5)
where P
d
is the number of posts in thread d, K
is the number of topics, and N
p
is the number of
word tokens in post P
d
. The variational parame-
ters of q(?) are ?, ?, ?. We will write ??? to indicate
an expectation under the distribution q(?, z,y).
We employ point estimates for the variables
b (author selection bias), ? (topic-time feature
weights), ? (topic-word log-probability devia-
tions), and diagonal elements of ? (topic weights).
The estimation of ? follows the procedure defined
in SAGE (Eisenstein et al., 2011); we explain the
estimation of the remaining parameters below.
Given the variational distribution in Equation
(5), the inference on our topic model can be for-
mulated as constrained optimization of this bound.
min L(?, ?, ?; b,?,?)
s.t.?
dk
? 0 ?d, k
?
dpn
? 0,
?
k
?
dpnk
= 1 ?d, p, n
0 ? ?
ik
? 1 ?i, k
?
k
? 0 ?k
(6)
The constraints are due to the parametric form
of the variational approximation: q(?
d
|?
d
) is
Dirichlet, and requires non-negative parameters;
2
see http://probabilistic-programming.
org/
100
q(z
dpn
|?
dpn
) is multinomial, and requires that
?
dpn
lie on the K ? 1 simplex; q(y
ik
|?
ik
) is
Bernoulli and requires that ?
ik
be between 0 and
1. In addition, as a topic weight, ?
k
should also be
non-negative.
Algorithm 1 One pass of the variational inference
algorithm for our model.
for d = 1, . . . , D do
while not converged do
for p = 1, . . . , P
d
do
for n = 1, . . . , N
p,d
do
Update ?
dpnk
using Equation (7) for each k =
1, . . . ,K
end for
end for
Update ?
dk
by optimizing Equation (6) with Equa-
tion (10) for each k = 1, . . . ,K
end while
end for
for i = 1, . . . , A do
Update ?
ik
by optimizing Equation (6) with Equa-
tion (13) for each k = 1, . . . ,K
Update
?
b
i
by optimizing Equation (6) with Equa-
tion (14)
end for
for k = 1, . . . ,K do
Update ?
k
with Equation (15)
end for
4.1 Word-topic indicators
With the variational distribution in Equation (5),
the inference on ?
dpn
for a given token n in post p
of thread d is same as in LDA. For the nth token
in post p of thread d,
?
dpnk
? ?
kw
dpn
exp(?log ?
dk
?) (7)
where ? is defined in the generative story and
?log ?
dk
? is the expectation of log ?
dk
under the
distribution q(?
dk
|?
d
),
?log ?
dk
? = ?(?
dk
)??(
K
?
k=1
?
dk
) (8)
where ?(?) is the Digamma function, the first
derivative of the log-gamma function.
For the other variational parameters ? and ?, we
can not obtain a closed form solution. As the con-
straints on these parameters are all convex with re-
spect to each component, we employed a projected
quasi-Newton algorithm proposed in (Schmidt et
al., 2009) to optimize L in Equation (6). One pass
of the variational inference procedure is summa-
rized in Algorithm 1.Since every step in this algo-
rithm will not decrease the variational bound, the
overall algorithm is guaranteed to converge.
4.2 Document-topic distribution
The inference for document-topic proportions is
different from LDA, due to the generation of the
author vector a
d
, which depends on ?
d
. For a
given thread d, the part of the bound associated
with the variational parameter ?
d
is
L
?
d
= ?log p(?
d
|?
d
)?+ ?log p(a
d
|?
d
,y,?, b)?
+
P
d?
p=1
N
p,d?
n=1
?log p(z
dpn
|?
d
)? ? ?q(?
d
|?
d
)?
(9)
and the derivative of L
?
d
with respect to ?
dk
is
dL
?
d
d?
dk
= ?
?
(?
dk
)(?
dk
+
P
d?
p=1
N
p,d?
n=1
?
dpnk
? ?
dk
)
??
?
(
K?
k=1
?
dk
)
K?
k=1
(?
dk
+
P
d?
p=1
N
p,d?
n=1
?
dpnk
? ?
dk
)
+
d
d?
dk
?log p(a
d
|?
d
,y,?, b)? ,
(10)
where ?
?
(?) is the trigramma function. The first
two lines of Equation (10) are identical to LDA?s
variational inference, which obtains a closed-form
solution by setting ?
dk
= ?
dk
+
?
p,n
?
dpnk
. The
additional term for generating the authorship vec-
tor a
d
eliminates this closed-form solution and
forces us to turn to gradient-based optimization.
The expectation on the log probability of the
authorship involves the expectation on the log
partition function, which we approximate using
Jensen?s inequality. We then derive the gradient,
?
??
dk
?log p(a
d
|?
d
,y,?, b)?
? ?
k
(
A?
i=1
a
(r)
di
?
ik
?A
(r)
d
A?
i=1
?
ik
?
a
(r)
di
|?
d
,y
?
)
? ?
k
(
A?
i=1
a
(q)
di
?
ik
?
A?
i=1
?
ik
?
a
(q)
di
|?
d
,y
?
)
(11)
The convenience variable A
(r)
d
counts the number
of distinct response authors in thread d; recall that
there can be only one question author. The nota-
tion
?
a
(r)
di
|?
d
,y
?
=
exp(
?
?
T
?
? ?y
i
?+ b
i
)
?
j
exp(
?
?
T
?
? ?y
j
?+ b
j
)
,
represents the generative probability of a
(r)
di
= 1
under the current variational distributions q(?
d
)
and q(y
i
). The notation
?
a
(q)
di
|?
d
,y
?
is analo-
gous, but represents the question post indicator
a
(q)
di
.
101
4.3 Author-topic preference
The variational distribution over author-topic
preference is q(y
ik
|?
ik
); as this distribution is
Bernoulli, ?y
ik
? = ?
ik
, the parameter itself prox-
ies for the topic-specific author preference ? how
much author i prefers to answer posts on topic k.
The part of the variational bound the relates to
the author preferences is
L
?
=
D?
d=1
?log p(a
d
|?
d
,y,?, b)?
+
A?
i=1
K?
k=1
?p(y
ik
|?)? ?
A?
i=1
K?
k=1
?q(y
ik
|?
ik
)?
(12)
For author i on topic k, the derivative of
?log p(a
d
|?
d
,y,?, b)? for document d with re-
spect to ?
ik
is
d
d?
ik
?logP (a
d
|?
d
,y,?, b)?
? ??
dk
??
k
(
a
(r)
di
?
?
a
(r)
di
|?
d
,y
?
? a
(q)
di
+
?
a
(q)
di
|?
d
,y
?)
,
(13)
where ??
dk
? =
?
dk?
k
?
?
dk
?
. Thus, participating as a
respondent increases ?
ik
to the extent that topic k
is involved in the thread; participating as the ques-
tioner decreases ?
ik
by a corresponding amount.
4.4 Point estimates
We make point estimates of the following param-
eters: author selection bias b
i
and topic-specific
preference weights ?
k
. All updates are based
on maximum a posteriori estimation or maximum
likelihood estimation.
Selection bias For the selection bias b
i
of au-
thor i given a thread d, the objective function in
Equation (6) with the prior of b
i
? N (0, ?
2
b
) is
minimized by a quasi-Newton algorithm with the
following derivative
?
?b
i
?logP (a
d
|?
d
,y,?, b)? ? a(r)
d,i
?
?
a
(r)
di
|?
d
,y
?
+ a
(q)
d,i
?
?
a
(q)
di
|?
d
,y
? (14)
The zero-mean Gaussian prior shrinks b
i
towards
zero by subtracting b
i
/?
2
b
from this gradient. Note
that the gradient in Equation (14) is non-negative
whenever author i participates in thread d. This
means any post from this author, whether question
posts or answer posts, will have a positive contri-
bution of the author?s selection bias. This means
that any activity in the forum will elevate the se-
lection bias b
i
, but will not necessarily increase the
imputed preference level.
Topic weights The topic-specific preference
weight ?
k
is updated by considering the derivative
of variational bound with respect to ?
k
?L
??
k
=
D
?
d=1
?
??
k
?p(a
d
|?
d
,y,?, b)? (15)
where for a given document d,
?
??
k
?log p(a
d
|?
d
,y,?, b)? ? ??
dk
??
k
?
A?
i=1
?
ik
(
a
(r)
i
? a
(q)
i
+
?
a
(q)
di
|?
d
,y
?
?A
(r)
d
?
a
(r)
di
|?
d
,y
?)
Thus, ?
k
will converge at a value where the ob-
served posting counts matches the expectations
under ?log p(a
d
|?
d
,y,?, b)?.
5 Quantitative Evaluation
To validate the topics identified by the model,
we performed a manual evaluation, combining the
opinions of both novices as well as subject matter
experts in Autism and Asberger?s Syndrome. The
purpose of the evaluation is to determine whether
the topics induced by the proposed model are more
coherent than topics from generic alternatives such
as LDA and the author-topic model, which are not
specifically designed for forums.
5.1 Experiment Setup
Preprocessing Preprocessing was minimal. We
tokenized texts using white space and removed
punctuations at the beginning/end of each token.
We removed words that appear less than five
times, resulting in a vocabulary of the 4903 most
frequently-used words.
Baseline Models We considered two baseline
models in the evaulation. The first baseline model
is latent Dirichlet allocation (LDA), which consid-
ers only the text and ignores the metadata (Blei
et al., 2003). The second baseline is the Author-
Topic (AT) model, which extends LDA by associ-
ating authors with topics (Rosen-Zvi et al., 2004;
Steyvers et al., 2004). Both baselines are im-
plemented in the Matlab Topic Modeling Tool-
box (Steyvers and Griffiths, 2005).
Parameter Settings For all three models, we set
K = 50. Our model includes the three tunable
parameters ?, the Bernoulli prior on topic-specific
expertise; ?
2
b
, the variance prior on use selection
102
bias; and ?, the prior on document-topic distri-
bution. In the following experiments, we chose
? = 0.2, ?
2
b
= 1.0, ? = 1.0. LDA and AT share
two parameters, ?, the symmetric Dirichlet prior
for document-topic distribution; ?, the symmetric
Dirichlet prior for the topic-word distribution. In
both models, we set ? = 3.0 and ? = 0.01. All
parameters were selected in advance of the experi-
ments; further tuning of these paramters is left for
future work.
5.2 Topic Coherence Evaluation
To be useful, a topic model should produce topics
that human readers judge to be coherent. While
some automated metrics have been shown to co-
here with human coherence judgments (Newman
et al., 2010), it is possible that naive raters might
have different judgments from subject matter ex-
perts. For this reason, we focused on human eval-
uation, including both expert and novice opinions.
One rater, R1, is an author of the paper (HH) and
a Ph.D. student focusing on designing technology
to understand and support individuals with autism
spectrum disorder. The remaining three raters are
not authors of the paper and are not domain ex-
perts.
In the evaluation protocol, raters were presented
with batteries of fifteen topics, from which they
were asked to select the three most coherent. In
each of the ten batteries, there were five topics
from each model, permuted at random. Thus, af-
ter completing the task, all 150 topics ? 50 topics
from each model ? were rated. The user interface
of topic coherence evaluation is given in Figure 2,
including the specific prompt.
We note that this evaluation differs from the
?intrusion task? proposed by Chang et al. (2009),
in which raters are asked to guess which word
was randomly inserted into a topic. While the in-
trusion task protocol avoids relying on subjective
judgments of the meaning of ?coherence,? it pre-
vents expert raters from expressing a preference
for topics that might be especially useful for anal-
ysis of autism spectrum disorder. Prior work has
also shown that the variance of these tasks is high,
making it difficult to distinguish between models.
Table 2 shows, for each rater, the percentage of
topics were chosen from each model as the most
coherent within each battery. On average, 80% of
the topics were chosen from our proposed model.
If all three models are equally good at discover-
Figure 2: The user interface of topic coherence
evaluation.
Rater
Model R1 R2 R3 R4 Average
Our model 70% 93% 80% 77% 80%
AT 17% 7% 13% 10% 12%
LDA 13% 0% 7% 13% 8%
Table 2: Percentage of the most coherent topics that are
selected from three different topic models: our model, the
Author-Topic Model (AT), and latent Dirichlet allocation
(LDA).
ing coherent topics, the average percentage across
three models should be roughly equal. Note that
the opinion of the expert rater R1 is generally sim-
ilar to the other three raters.
6 Analysis of Aspies Central Topics
In this section, we further use our model to ex-
plore more information about the Aspies Central
forum. We want to examine whether the autism-
related topics identified the model can support re-
searchers to gain qualitative understanding of the
needs and concerns of autism forum users. We are
also interested in understanding the users? behav-
ioral patterns on autism-related topics. The anal-
ysis task has three components: first we will de-
scribe the interesting topics from the autism do-
main perpective. Then we will find out the pro-
portion of each topic, including autism related top-
ics. Finally, in order to understand the user activ-
ity patterns on these autism related topics we will
derive the topic-specific preference ranking of the
users from our model.
103
Index Proportion Top keywords Index Proportion Top keywords
1 1.7% dont im organization couldnt construction 2 2.6% yah supervisor behavior taboo phone
3 2.2% game watched games fallout played 4 3.5% volunteering esteem community art self
5 1.1% nobody smell boss fool smelling 6 3.2% firefox razor blades pc console
7 3.4% doesn?t it?s mandarin i?ve that?s 8 2.1% diagnosed facessenses visualize visual
9 1.7% obsessions bookscollecting library authors 10 2.6% ptsd central cure neurotypical we
11 1.2% stims mom nails lip shoes 12 1.8% classroom campus tag numbers exams
13 1.6% battery hawke charlie ive swing 14 1.9% divorce william women marryrates
15 0.1% chocolate pdd milk romance nose 16 5.8% kinda holland neccesarily employment bucks
17 0.6% eat burgers jokes memory foods 18 2.4% dryer martial dream wake schedule
19 3.7% depression beleive christianity buddhism becouse 20 1.4% grudges pairs glasses museum frames
21 0.4% alma star gods alien sun 22 2.6% facebook profiles befriend friendships friends
23 0.4% trilogy sci-fi cartoon iphone grandma 24 2.7% flapping stuffed toes curse animal
25 1.5% empathy smells compassion emotions emotional 26 1.7% males evolution females originally constructive
27 0.5% list dedicate lists humor song 28 4.6% nts aspies autie qc intuitive
29 2.7% captain i?m film anime that?s 30 3.6% homeless pic wild math laugh
31 3.3% shave exhausting during terrified products 32 5.6% you?re you your yourself hiring
33 4.6% dictionary asks there?re offend fog 34 1.5% grade ed school 7th diploma
35 1.0% cave blonde hair bald disney 36 1.9% diagnosis autism syndrome symptoms aspergers
37 1.3% song joanna newsom rap favorites 38 1.8% poetry asleep children ghosts lots
39 2.1% heat iron adhd chaos pills 40 3.6% bike zone rides zoning worrying
41 1.2% uk maths team teams op 42 0.8% book books read reading kindle
43 1.0% husband narcissist husband?s he hyper 44 1.1% songs guitar drums music synth
45 1.3% autism disorder spectrum disorders pervasive 46 0.7% dog noise dogs barking noisy
47 0.6% relationship women relationships sexual sexually 48 0.9% weed marijuana pot smoking fishing
49 0.9% him he his bernard je 50 2.0% her she she?s kyoko she?ll
Table 3: 50 topics identified by our model. The ?proportion? columns show the topic proportions in the
dataset. Furthermore, 14 topics are highlighted as interesting topics for autism research.
Table 3 shows all 50 topics from our model. For
each topic, we show the top five words related to
this topic. We further identified fourteen topics
(highlighted with blue color), which are particu-
larly relevant to understand autism.
Among the identified topics, there are three
popular topics discussed in the Aspies Central fo-
rum: topic 4, topic 19 and topic 31. From the top
word list, we identified that topic 4 is composed
of keywords related to psychological (e.g., self-
esteem, art) and social (e.g., volunteering, com-
munity) well-being of the Aspies Central users.
Topic 19 includes discussion on mental health
issues (e.g., depression) and religious activities
(e.g., believe, christianity, buddhism) as coping
strategies. Topic 31 addresses a specific personal
hygiene issue ? helping people with autism learn
to shave. This might be difficult for individuals
with sensory issues: for example, they may be
terrified by the sound and vibration generated by
the shaver. For example, topic 22 is about mak-
ing friends and maintaining friendship; topic 12 is
about educational issues ranging from seeking ed-
ucational resources to improving academic skills
and adjusting to college life.
In addition to identifying meaningful topics, an-
other capability of our model is to discover users?
topic preferences and expertise. Recall that, for
user i and topic k, our model estimates a author-
topic preference variable ?
ik
. Each ?
ik
ranges
from 0 to 1, indicating the probability of user i to
Topic User index
5 USER 1, USER 2, USER 3, USER 4, USER 5
8 USER 1, USER 2, USER 6, USER 5, USER 7
12 USER 1, USER 2, USER 4, USER 8, USER 3
19 USER 1, USER 2, USER 3, USER 4, USER 7
22 USER 1, USER 2, USER 3, USER 9, USER 7
31 USER 1, USER 3, USER 2, USER 6, USER 10
36 USER 1, USER 2, USER 4, USER 3, USER 11
45 USER 1, USER 3, USER 4, USER 12, USER 13
47 USER 2, USER 14, USER 15, USER 16 , USER 6
48 USER 5, USER 4, USER 6, USER 9, USER 2
Table 4: The ranking of user preference on some interest-
ing topics (we replace user IDs with user indices to avoid
any privacy-related issue). USER 1 is the moderator of this
forum. In total, our model identifies 16 user with high topic-
specific preference from 10 interesting topics. For the other
4 interesting topics, there is no user with significantly high
preference.
answer a question on topic k. As we set the prior
probability of author-topic preference to be 0.2,
we show topic-author pairs for which ?
ik
> 0.2
in Table 4.
The dominance of USER 1 in these topics is ex-
plained by the fact that this user is the moderator
of the forum. Besides, we also find some other
users participating in most of the interesting top-
ics, such as USER 2 and USER 3. On the other
hand, users like USER 14 and USER 15 only show
up in few topics. This observation is supported by
their activities on discussion boards. Searching on
the Aspies Certral forum, we found most answer
posts of user USER 15 are from the board ?love-
104
relationships-and-dating?.
7 Related Work
Social media has become an important source of
health information (Choudhury et al., 2014). For
example, Twitter has been used both for mining
both public health information (Paul and Dredze,
2011) and for estimating individual health sta-
tus (Sokolova et al., 2013; Teodoro and Naaman,
2013). Domain-specific online communities, such
Aspies Central, have their own advantages, tar-
geting specific issues and featuring more close-
knit and long-term relationships among mem-
bers (Newton et al., 2009).
Previous studies on mining health information
show that technical models and tools from com-
putational linguistics are helpful for both under-
standing contents and providing informative fea-
tures. Sokolova and Bobicev (2011) use sentiment
analysis to analyze opinions expressed in health-
related Web messages; Hong et al. (2012) focus
on lexical differences to automatically distinguish
schizophrenic patients from healthy individuals.
Topic models have previously been used to
mine health information: Resnik et al. (2013) use
LDA to improve the prediction for neuroticism
and depression on college students, while Paul and
Dredze (2013) customize their factorial LDA to
model the joint effect of drug, aspect, and route
of administration. Most relevantly for the current
paper, Nguyen et al. (2013) use LDA to discover
autism-related topics, using a dataset of 10,000
posts from ten different autism commnities. How-
ever, their focus was on automated classification of
communities as autism-related or not, rather than
on analysis and on providing support for qualita-
tive autism researchers. The applicability of the
model developed in our paper towards classifica-
tion tasks is a potential direction for future re-
search.
In general, topic models capture latent themes
in document collections, characterizing each doc-
ument in the collection as a mixture of topics (Blei
et al., 2003). A natural extension of topic mod-
els is to infer the relationships between topics and
metadata such as authorship or time. A relatively
simple approach is to represent authors as an ag-
gregation of the topics in all documents they have
written (Wagner et al., 2012). More sophisticated
topic models, such as Author-Topic (AT) model
(Rosen-Zvi et al., 2004; Steyvers et al., 2004) as-
sume that each document is generated by a mix-
ture of its authors? topic distributions. Our model
can be viewed as one further extension of topic
models by incorporating more metadata informa-
tion (authorship, thread structure) in online fo-
rums.
8 Conclusion
This paper describes how topic models can offer
insights on the issues and challenges faced by in-
dividuals on the autism spectrum. In particular,
we demonstrate that by unifying textual content
with authorship and thread structure metadata, we
can obtain more coherent topics and better under-
stand user activity patterns. This coherence is val-
idated by manual annotations from both experts
and non-experts. Thus, we believe that our model
provides a promising mechanism to capture be-
havioral and psychological attributes relating to
the special populations affected by their cognitive
disabilities, some of which may signal needs and
concerns about their mental health and social well-
being.
We hope that this paper encourages future ap-
plications of topic modeling to help psychologists
understand the autism spectrum and other psycho-
logical disorders ? and we hope to obtain further
validation of our model through its utility in such
qualitative research. Other directions for future
work include replication of our results across mul-
tiple forums, and applications to other conditions
such as depression and attention deficit hyperac-
tivity disorder (ADHD).
Acknowledgments
This research was supported by a Google Faculty
Award to the last author. We thank the three re-
viewers for their detailed and helpful suggestions
to improve the paper.
References
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised Topic Models. In NIPS.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
Tea Leaves: How Humans Interpret Topic Models.
In Yoshua Bengio, Dale Schuurmans, John D. Laf-
ferty, Christopher K. I. Williams, and Aron Culotta,
105
editors, NIPS, pages 288?296. Curran Associates,
Inc.
Munmun De Choudhury, Meredith Ringel Morris, and
Ryen W. White. 2014. Seeking and Sharing Health
Information Online: Comparing Search Engines and
Social Media. In Procedings of CHI.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse Additive Generative Models of Text. In
ICML.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal Differences in Autobiographical Narratives from
Schizophrenic Patients and Healthy Controls. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
37?47. Association for Computational Linguistics,
July.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100?108. Association for Computa-
tional Linguistics.
A. Taylor Newton, Adam D.I. Kramer, and Daniel N.
McIntosh. 2009. Autism online: a comparison
of word usage in bloggers with and without autism
spectrum disorders. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 463?466. ACM.
Thin Nguyen, Dinh Phung, and Svetha Venkatesh.
2013. Analysis of psycholinguistic processes and
topics in online autism communities. In Multimedia
and Expo (ICME), 2013 IEEE International Confer-
ence on, pages 1?6. IEEE.
Michael J. Paul and Mark Dredze. 2011. You Are
What You Tweet: Analyzing Twitter for Public
Health. In ICWSM.
Michael J. Paul and Mark Dredze. 2013. Drug Ex-
traction from the Web: Summarizing Drug Expe-
riences with Multi-Dimensional Topic Models. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 168?178, Atlanta, Georgia, June. Association
for Computational Linguistics.
Philip Resnik, Anderson Garron, and Rebecca Resnik.
2013. Using Topic Modeling to Improve Prediction
of Neuroticism and Depression in College Students.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The Author-Topic
Model for Authors and Documents. In UAI.
Mark Schmidt, Ewout van den Berg, Michael P. Fried-
lander, and Kevin Muphy. 2009. Optimizing Costly
Functions with Simple Constraints: A Limited-
Memory Projected Quasi-Netton Algorithm. In AIS-
TATS.
Marina Sokolova and Victoria Bobicev. 2011. Sen-
timents and Opinions in Health-related Web mes-
sages. In Proceedings of the International Confer-
ence Recent Advances in Natural Language Pro-
cessing 2011, pages 132?139, Hissar, Bulgaria,
September. RANLP 2011 Organising Committee.
Marina Sokolova, Stan Matwin, Yasser Jafer, and
David Schramm. 2013. How Joe and Jane Tweet
about Their Health: Mining for Personal Health In-
formation on Twitter. In Proceedings of the In-
ternational Conference Recent Advances in Natu-
ral Language Processing RANLP 2013, pages 626?
632, Hissar, Bulgaria, September. INCOMA Ltd.
Shoumen, BULGARIA.
Mark Steyvers and Thomas Griffiths. 2005. Matlab
Topic Modeling Toolbox 1.4.
Mark Steyvers, Padhraic Smyth, and Thomas Griffiths.
2004. Probabilistic Author-Topic Models for Infor-
mation Discovery. In KDD.
Rannie Teodoro and Mor Naaman. 2013. Fitter with
Twitter: Understanding Personal Health and Fitness
Activity in Social Media. In Proceedings of the
7th International Conference on Weblogs and Social
Media.
Claudia Wagner, Vera Liao, Peter Pirolli, Les Nel-
son, and Markus Strohmaier. 2012. It?s not in
their tweets: Modeling topical expertise of Twitter
users. In ASE/IEEE International Conference on So-
cial Computing.
Martin J. Wainwright and Michael I. Jordan. 2008.
Graphical models, exponential families, and varia-
tional inference. Foundations and Trends in Ma-
chine Learning, 1(1-2):1?305.
106
