Coling 2010: Poster Volume, pages 1507?1514,
Beijing, August 2010
Predicting Discourse Connectives for Implicit Discourse Relation
Recognition
Zhi-Min Zhou and Yu Xu
East China Normal University
51091201052@ecnu.cn
Zheng-Yu Niu
Toshiba China R&D Center
zhengyu.niu@gmail.com
Man Lan and Jian Su
Institute for Infocomm Research
sujian@i2r.a-star.edu.sg
Chew Lim Tan
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Existing works indicate that the absence
of explicit discourse connectives makes
it difficult to recognize implicit discourse
relations. In this paper we attempt to
overcome this difficulty for implicit rela-
tion recognition by automatically insert-
ing discourse connectives between argu-
ments with the use of a language model.
Then we propose two algorithms to lever-
age the information of these predicted
connectives. One is to use these pre-
dicted implicit connectives as additional
features in a supervised model. The other
is to perform implicit relation recognition
based only on these predicted connectives.
Results on Penn Discourse Treebank 2.0
show that predicted discourse connectives
help implicit relation recognition and the
first algorithm can achieve an absolute av-
erage f-score improvement of 3% over a
state of the art baseline system.
1 Introduction
Discourse relation analysis is to automatically
identify discourse relations (e.g., explanation re-
lation) that hold between arbitrary spans of text.
This analysis may be a part of many natural lan-
guage processing systems, e.g., text summariza-
tion system, question answering system. If there
are discourse connectives between textual units
to explicitly mark their relations, the recognition
task on these texts is defined as explicit discourse
relation recognition. Otherwise it is defined as im-
plicit discourse relation recognition.
Previous study indicates that the presence of
discourse connectives between textual units can
greatly help relation recognition. In Penn Dis-
course Treebank (PDTB) corpus (Prasad et al,
2008), the most general senses, i.e., Comparison
(Comp.), Contingency (Cont.), Temporal (Temp.)
and Expansion (Exp.), can be disambiguated in
explicit relations with more than 90% f-scores
based only on the discourse connectives explicitly
used to signal the relation (Pitler and Nenkova.,
2009b).
However, for implicit relations, there are no
connectives to explicitly mark the relations, which
makes the recognition task quite difficult. Some of
existing works attempt to perform relation recog-
nition without hand-annotated corpora (Marcu
and Echihabi, 2002), (Sporleder and Lascarides,
2008) and (Blair-Goldensohn, 2007). They use
unambiguous patterns such as [Arg1, but Arg2]
to create synthetic examples of implicit relations
and then use [Arg1, Arg2] as an training example
of an implicit relation. Another research line is
to exploit various linguistically informed features
under the framework of supervised models, (Pitler
et al, 2009a) and (Lin et al, 2009), e.g., polarity
features, semantic classes, tense, production rules
of parse trees of arguments, etc.
Our study on PDTB test data shows that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we simply mapped the ground
truth implicit connective of each test instance to
its most frequent sense. It indicates the impor-
tance of connective information for implicit rela-
tion recognition. However, so far there is no previ-
ous study attempting to use such kind of connec-
tive information for implicit relation. One possi-
1507
ble reason is that implicit connectives do not ex-
ist in unannotated real texts. Another evidence
of the importance of connectives for implicit re-
lations is shown in PDTB annotation. The PDTB
annotation consists of inserting a connective ex-
pression that best conveys the inferred relation by
the readers. Connectives inserted in this way to
express inferred relations are called implicit con-
nectives, which do not exist in real texts. These
evidences inspire us to consider two interesting re-
search questions:
(1) Can we automatically predict implicit connec-
tives between arguments?
(2) How to use the predicted implicit connectives
to build an automatic discourse relation analysis
system?
In this paper we address these two questions as
follows: (1) We insert appropriate discourse con-
nectives between two textual units with the use of
a language model. Here we train the language
model on large amount of raw corpora without
the use of any hand-annotated data. (2) Then we
present two algorithms to use these predicted con-
nectives for implicit relation recognition. One is
to use these connectives as additional features in a
supervised model. The other is to perform relation
recognition based only on these connectives.
We performed evaluation of the two algorithms
and a baseline system on PDTB 2.0 corpus. Ex-
perimental results showed that using predicted
discourse connectives as additional features can
significantly improve the performance of implicit
discourse relation recognition. Specifically, the
first algorithm achieved an absolute average f-
score improvement of 3% over a state of the art
baseline system.
The rest of this paper is organized as follows.
Section 2 describes the two algorithms for implicit
discourse relation recognition. Section 3 presents
experiments and results on PDTB data. Section
4 reviews related work. Section 5 concludes this
work.
2 Our Algorithms for Implicit Discourse
Relation Recognition
2.1 Prediction of implicit connectives
Explicit discourse relations are easily identifiable
due to the presence of discourse connectives be-
tween arguments. (Pitler and Nenkova., 2009b)
showed that in PDTB corpus, the most general
senses, i.e., Comparison (Comp.), Contingency
(Cont.), Temporal (Temp.) and Expansion (Exp.),
can be disambiguated in explicit relations with
more than 90% f-scores based only on discourse
connectives.
But for implicit relations, there are no connec-
tives to explicitly mark the relations, which makes
the recognition task quite difficult. PDTB data
provides implicit connectives that are inserted be-
tween paragraph-internal adjacent sentence pairs
not marked by any of explicit connectives. The
availability of ground-truth implicit connectives
makes it possible to evaluate the contribution of
these connectives for implicit relation recognition.
Our initial study on PDTB data show that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we obtained the sense of each
test example by mapping each ground truth im-
plicit connective to its most frequent sense. We
see that connective information is an important
knowledge source for implicit relation recogni-
tion. However these implicit connectives do not
exist in real texts. In this paper we overcome this
difficulty by inserting a connective between two
arguments with the use of a language model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two ar-
guments, denoted as Arg1 and Arg2. Typically,
there are two possible positions for most of im-
plicit connectives1, i.e., the position before Arg1
and the position between Arg1 and Arg2. Given a
set of possible implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as PPL(Sci,j). According
1For parallel connectives, e.g., if . . . then. . . , the two con-
nectives will take the two arguments together, so there is only
one possible combination for connectives and arguments.
1508
to the value of PPL(Sci,j) (the lower the better),
we can rank these sentences and select the con-
nectives in top N sentences as implicit connec-
tives for this argument pair. The language model
may be trained on large amount of unannotated
corpora that can be cheaply acquired, e.g., North
American News corpus.
2.2 Using predicted implicit connectives as
additional features
We predict implicit connectives on both training
set and test set. Then we can use the predicted
implicit connectives as additional features for su-
pervised implicit relation recognition. Previous
works exploited various linguistically informed
features under the framework of supervised mod-
els. In this paper, we include 9 types of features
in our system due to their superior performance
in previous studies, e.g., polarity features, seman-
tic classes of verbs, contextual sense, modality,
inquirer tags of words, first-last words of argu-
ments, cross-argument word pairs, ever used in
(Pitler et al, 2009a), production rules of parse
trees of arguments used in (Lin et al, 2009), and
intra-argument word pairs inspired by the work of
(Saito et al, 2006).
Here we provide the details of the 9 features,
shown as follows:
Verbs: Similar to the work in (Pitler et al,
2009a), the verb features consist of the number of
pairs of verbs in Arg1 and Arg2 if they are from
the same class based on their highest Levin verb
class level (Dorr, 2001). In addition, the average
length of verb phrase and the part of speech tags
of main verb are also included as verb features.
Context: If the immediately preceding (or fol-
lowing) relation is an explicit, its relation and
sense are used as features. Moreover, we use an-
other feature to indicate if Arg1 leads a paragraph.
Polarity: We use the number of positive,
negated positive, negative and neutral words in ar-
guments and their cross product as features. For
negated positives, we locate the negated words in
text span and then define the closely behind posi-
tive word as negated positive.
Modality: We look for modal words including
their various tenses or abbreviation forms in both
arguments. Then we generate a feature to indicate
the presence or absence of modal words in both
arguments and their cross product.
Inquirer Tags: Inquirer Tags extracted from
General Inquirer lexicon (Stone et al, 1966) con-
tains positive or negative classification of words.
In fact, its fine-grained categories, such as Fall
versus Rise, or Pleasure versus Pain, can indi-
cate the relation between two words, especially
for verbs. So we choose the presence or absence
of 21 pair categories with complementary relation
in Inquirer Tags as features. We also include their
cross production as features.
FirstLastFirst3: We choose the first and last
words of each argument as features, as well as the
pair of first words, the pair of last words, and the
first 3 words in each argument. In addition, we ap-
ply Porter?s Stemmer (Porter, 1980) to each word
before preparation of these features.
Production Rule: According to (Lin et al,
2009), we extract all the possible production rules
from arguments, and check whether the rules ap-
pear in Arg1, Arg2 and both arguments. We re-
move the rules occurring less than 5 times in train-
ing data.
Cross-argument Word Pairs: We perform the
Porter?s stemming (Porter, 1980), and then group
all words from Arg1 and Arg2 into two sets W1
and W2 respectively. Then we generate any possi-
ble word pair (wi, wj) (wi ? W1, wj ? W2). We
remove the word pairs with less than 5 times.
Intra-argument Word Pairs: Let
Q1 = (q1, q2, . . . , qn) be the word se-
quence of Arg1. The intra-argument word
pairs for Arg1 is defined as WP1 =
((q1, q2), (q1, q3), . . . , (q1, qn), (q2, q3), . . . ,
(qn?1, qn)). We extract all the intra-argument
word pairs from Arg1 and Arg2 and remove word
pairs appearing less than 5 times in training data.
2.3 Relation recognition based only on
predicted implicit connectives
After the prediction of implicit connectives, we
can address the implicit relation recognition task
with the methods for explicit relation recogni-
tion due to the presence of implicit connectives,
e.g., sense classification based only on connec-
tives (Pitler and Nenkova., 2009b). The work of
(Pitler and Nenkova., 2009b) showed that most
1509
of connectives are unambiguous and it is possible
to obtain high performance in prediction of dis-
course sense due to the simple mapping relation
between connectives and senses. Given two ex-
amples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey Comparison and Contingency sense
respectively. In most cases, we can easily recog-
nize the relation sense by the appearance of dis-
course connective since it can be interpreted in
only one way. That means, the ambiguity of the
mapping between sense and connective is quite
few.
We count the frequency of sense tags for each
possible connective on PDTB training data for im-
plicit relation. Then we build a sense recognition
model by simply mapping each connective to its
most frequent sense. Here we do not perform con-
nective prediction on training data. During test-
ing, we use the language model to insert implicit
connectives into each test argument pair. Then we
perform relation recognition by mapping each im-
plicit connective to its most frequent sense.
3 Experiments and Results
3.1 Experiments
3.1.1 Data sets
In this work we used the PDTB 2.0 corpus for
evaluation of our algorithms. Following the work
of (Pitler et al, 2009a), we used sections 2-20 as
training set, sections 21-22 as test set, and sec-
tions 0-1 as development set for parameter opti-
mization. For comparison with the work of (Pitler
et al, 2009a), we ran four binary classification
tasks to identify each of the main relations (Cont.,
Comp., Exp., and Temp.) from the rest. For each
relation, we used equal numbers of positive and
negative examples as training data2. The negative
examples were chosen at random from sections 2-
20. We used all the instances in sections 21 and
22 as test set, so the test set is representative of
2Here the numbers of training and test instances for Ex-
pansion relation are different from those in (Pitler et al,
2009a). The reason is that we do not include instances of
EntRel as positive examples.
the natural distribution. The numbers of positive
and negative instances for each sense in different
data sets are listed in Table 1.
Table 1: Statistics of positive and negative sam-
ples in training, development and test sets for each
relation.
Relation Train Dev Test
Pos/Neg Pos/Neg Pos/Neg
Comp. 1927/1927 191/997 146/912
Cont. 3375/3375 292/896 276/782
Exp. 6052/6052 651/537 556/502
Temp. 730/730 54/1134 67/991
In this work we used LibSVM toolkit to con-
struct four linear SVM models for a baseline sys-
tem and the system in Section 2.2.
3.1.2 A baseline system
We first built a baseline system, which used 9
types of features listed in Section 2.2.
We tuned the numbers of firstLastFirst3, cross-
argument word pair, intra-argument word pair on
development set. Finally we set the frequency
threshold at 3, 5 and 5 respectively.
3.1.3 Prediction of implicit connectives
To predict implicit connectives, we adopt the
following two steps:(1) train a language model;
(2) select top N implicit connectives.
Step 1: We used SRILM toolkit to train the lan-
guage models on three benchmark news corpora,
i.e., New York part in the BLLIP North Ameri-
can News, Xin and Ltw parts of English Gigaword
(4th Edition). We also tried different values for
n in n-gram model. The parameters were tuned
on the development set to optimize the accuracy
of prediction. In this work we chose 3-gram lan-
guage model trained on NY corpus.
Step 2: We combined each instance?s Arg1 and
Arg2 with connectives extract from PDTB2 (100
in all). There are two types of connectives, sin-
gle connective (e.g. because and but) and paral-
lel connective (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of pos-
sible implicit connectives {ci}, for single connec-
tive {ci}, we constructed two synthetic sentences,
ci+Arg1+Arg2 and Arg1+ci+Arg2. In case of
1510
parallel connective, we constructed one synthetic
sentence like ci1+Arg1+ci2+Arg2.
As a result, we can get 198 synthetic sentences
for each argument pair. Then we converted all
words to lower cases and used the language model
trained in the above step to calculate perplexity
on sentence level. The perplexity scores were
ranked from low to high. For example, we got the
perplexity (ppl) for two sentences as follows:
(1) but this is an old story, we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 652.837
(2) this is an old story, but we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 583.514
We considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is, the
presence and absence of the specific connective.
According to the value of PPL(Sci,j) (the
lower the better), we selected the connectives in
top N sentences as implicit connectives for this
argument pair. In order to get the optimal N value,
we tried various values of N on development set
and selected the minimum value of N so that the
ground-truth connectives appeared in top N con-
nectives. The final N value is set to 60 based on
the trade-off between performance and efficiency.
3.1.4 Using predicted connectives as
additional features
This system combines the predicted implicit
connectives as additional features and the 9 types
of features in an supervised framework. The 9
types of features are listed as shown in Section 2.2
and tuned on development set.
We combined predicted connectives with the
best subset features from the development data set
with respect to f-score. In our experiment of se-
lecting best subset features, single features rather
than the combination of several features achieved
much higher scores. So we combine single fea-
tures with predicted connectives as final features.
3.1.5 Using only predicted connectives for
implicit relation recognition
We built two variants for the algorithm in Sec-
tion 2.3. One is to use the data for explicit re-
lations in PDTB sections 2-20 as training data.
The other is to use the data for implicit relations
in PDTB sections 2-20 as training data. Given
training data, we obtained the most frequent sense
for each connective appearing in the training data.
Then given test data, we recognized the sense of
each argument pair by mapping each predicted
connective to its most frequent sense. In this
work we conducted another experiment to see the
upper-bound performance of this algorithm. Here
we performed recognition based on ground-truth
implicit connectives and used the data for implicit
relations as training data.
3.2 Results
3.2.1 Result of baseline system
Table 2 summarizes the best performance
achieved by the baseline system in compari-
son with previous state-of-the-art performance
achieved in (Pitler et al, 2009a). The first two
lines in the table show their best results using sin-
gle feature and using combined feature subset. It
indicates that the performance of using combined
feature subset is higher than that using single fea-
ture alone.
From this table, we can find that our base-
line system has a comparable result on Contin-
gency and Temporal. On Comparison, our system
achieved a better performance around 9% f-score
higher than their best result. However, for Expan-
sion, they expanded both training and testing sets
by including EntRel relation as positive examples,
which makes it impossible to perform direct com-
parison. Generally, our baseline system is reason-
able and thus the consequent experiments on it are
reliable.
3.2.2 Result of algorithm 1: using predicted
connectives as additional features
Table 3 summarizes the best performance
achieved by the baseline system and the first al-
gorithm (i.e., baseline + Language Model) on test
set. The second and third column show the best
performance achieved by the baseline system and
1511
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test
set.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Using the best single feature (Pitler et al, 2009a) 21.01(52.59) 36.75(62.44) 71.29(59.23) 15.93(61.20)
Using the best feature subset (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
the first algorithm using predicted connectives as
additional features.
Table 3: Performance comparison of the algo-
rithm in Section 2.2 with the baseline system on
test set.
Rela- Features Baseline Baseline+LM
tion F1 (Acc) F1 (Acc)
Comp. Production Rule 30.72(78.26) 31.08(68.15)
Context 24.66(42.25) 27.64(53.97)
InquirerTags 23.31(73.25) 27.87(55.48)
Polarity 21.11(40.64) 23.64(52.36)
Modality 17.25(80.06) 26.17(55.20)
Verbs 25.00(53.50) 31.79(58.22)
Cont. Prodcution Rule 45.38(40.17) 47.16(48.96)
Context 37.61(44.70) 34.74(48.87)
Polarity 35.57(50.00) 43.33(33.74)
InquirerTags 38.04(41.49) 42.22(36.11)
Modality 32.18(66.54) 35.26(55.58)
Verbs 40.44(54.06) 42.04(32.23)
Exp. Context 48.34(54.54) 68.32(53.02)
FirstLastFirst3 65.95(57.94) 68.94(53.59)
InquirerTags 61.29(52.84) 68.49(53.21)
Modality 64.36(56.14) 68.9(52.55)
Polarity 49.95(50.38) 68.62(53.40)
Verbs 52.95(53.31) 70.11(54.54)
Temp. Context 13.52(64.93) 16.99(79.68)
FirstLastFirst3 15.75(66.64) 19.70(64.56)
InquirerTags 8.51(83.74) 19.20(56.24)
Modality 16.46(29.96) 19.97(54.54)
Polarity 16.29(51.42) 20.30(55.48)
Verbs 13.88(54.25) 13.53(61.34)
From this table, we found that this additional
feature obtained from language model showed
significant improvements in almost four relations.
Specifically, the top two improvements are on Ex-
pansion and Temporal relations, which improved
4.16% and 3.84% in f-score respectively. Al-
though on Comparison relation there is only a
slight improvement (+1.07%), our two best sys-
tems both got around 10% improvements of f-
score over a state-of-the-art system in (Pitler et al,
2009a). As a whole, the first algorithm achieved
3% improvement of f-score over a state of the art
baseline system. All these results indicate that
predicted implicit connectives can help improve
the performance.
3.2.3 Result of algorithm 2: using only
predicted connectives for implicit
relation recognition
Table 4 summarizes the best performance
achieved by the second algorithm in comparison
with the baseline system on test set.
The experiment showed that the baseline sys-
tem using just gold-truth implicit connectives can
achieve an f-score of 91.8% for implicit relation
recognition. It once again proved that implicit
connectives make significant contributions for im-
plicit relation recognition. This also encourages
our future work on finding the most suitable con-
nectives for implicit relation recognition.
From this table, we found that, using only pre-
dicted implicit connectives achieved an compara-
ble performance to (Pitler et al, 2009a), although
it was still a bit lower than our best baseline. But
we should bear in mind that this algorithm only
uses 4 features for implicit relation recognition
and these 4 features are easy computable and fast
run, which makes the system more practical in ap-
plication. Furthermore, compared with other al-
gorithms which require hand-annotated data for
training, the performance of this second algorithm
is acceptable if we take into account that no la-
beled data is used for model training.
3.3 Analysis
Experimental results on PDTB showed that using
the predicted implicit connectives significantly
improves the performance of implicit discourse
relation recognition. Our first algorithm achieves
an average f-score improvement of 3% over a
state of the art baseline system. Specifically, for
the relations: Comp., Cont., Exp., Temp., our
first algorithm can achieve 1.07%, 1.78%, 4.16%,
3.84% f-score improvements over a state of the
art baseline system. Since (Pitler et al, 2009a)
1512
Table 4: Performance comparison of the algorithm in Section 2.3 with the baseline system on test set.
System Comp. vs. Other Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
Our algorithm with training data for explicit relation 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97)
Our algorithm with training data for implicit relation 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51)
Sense recognition using gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07)
used different selection of instances for Expan-
sion sense3, we cannot make a direct compari-
son. However, we achieve the best f-score around
70%, which provide 5% improvements over our
baseline system. On the other hand, the second
proposed algorithm using only predicted connec-
tives still achieves promising results for each rela-
tion. Specifically, the model for the Comparison
relation achieves an f-score of 26.02% (5% over
the previous work in (Pitler et al, 2009a)). Fur-
thermore, the models for Contingency and Tem-
poral relation achieve 35.72% and 13.76% f-score
respectively, which are comparable to the previ-
ous work in (Pitler et al, 2009a). The model for
Expansion relation obtains an f-score of 64.95%,
which is only 1% less than our baseline system
which consists of ten thousands of features.
4 Related Work
Existing works on automatic recognition of dis-
course relations can be grouped into two cat-
egories according to whether they used hand-
annotated corpora.
One research line is to perform relation recog-
nition without hand-annotated corpora.
(Marcu and Echihabi, 2002) used a pattern-
based approach to extract instances of discourse
relations such as Contrast and Elaboration from
unlabeled corpora. Then they used word-pairs be-
tween two arguments as features for building clas-
sification models and tested their model on artifi-
cial data for implicit relations.
There are other efforts that attempt to extend the
work of (Marcu and Echihabi, 2002). (Saito et al,
2006) followed the method of (Marcu and Echi-
habi, 2002) and conducted experiments with com-
bination of cross-argument word pairs and phrasal
3They expanded the Expansion data set by adding ran-
domly selected EntRel instances by 50%, which is consid-
ered to significantly change data distribution.
patterns as features to recognize implicit relations
between adjacent sentences in a Japanese corpus.
They showed that phrasal patterns extracted from
a text span pair provide useful evidence in the re-
lation classification. (Sporleder and Lascarides,
2008) discovered that Marcu and Echihabi?s mod-
els do not perform as well on implicit relations as
one might expect from the test accuracies on syn-
thetic data. (Blair-Goldensohn, 2007) extended
the work of (Marcu and Echihabi, 2002) by re-
fining the training and classification process using
parameter optimization, topic segmentation and
syntactic parsing.
(Lapata and Lascarides, 2004) dealt with tem-
poral links between main and subordinate clauses
by inferring the temporal markers linking them.
They extracted clause pairs with explicit temporal
markers from BLLIP corpus as training data.
Another research line is to use human-
annotated corpora as training data, e.g., the RST
Bank (Carlson et al, 2001) used by (Soricut and
Marcu, 2003), adhoc annotations used by (?),
(Baldridge and Lascarides, 2005), and the Graph-
Bank (Wolf et al, 2005) used by (Wellner et al,
2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2008) bene-
fits the researchers with a large discourse anno-
tated corpora, using a comprehensive scheme for
both implicit and explicit relations. (Pitler et al,
2009a) performed implicit relation classification
on the second version of the PDTB. They used
several linguistically informed features, such as
word polarity, verb classes, and word pairs, show-
ing performance increases over a random classi-
fication baseline. (Lin et al, 2009) presented an
implicit discourse relation classifier in PDTB with
the use of contextual relations, constituent Parse
Features, dependency parse features and cross-
argument word pairs.
1513
In comparison with existing works, we investi-
gated a new knowledge source, implicit connec-
tives, for implicit relation recognition. Moreover,
our two models can exploit both labeled and un-
labeled data by training a language model on un-
labeled data and then using this language model
to generate implicit connectives for recognition
models trained on labeled data.
5 Conclusions
In this paper we use a language model to auto-
matically generate implicit connectives and then
present two methods to use these connectives for
recognition of implicit relations. One method is to
use these predicted implicit connectives as addi-
tional features in a supervised model and the other
is to perform implicit relation recognition based
only on these predicted connectives. Results on
Penn Discourse Treebank 2.0 show that predicted
discourse connectives help implicit relation recog-
nition and the first algorithm achieves an absolute
average f-score improvement of 3% over a state of
the art baseline system.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, Col-
lege Park, MD,2001.
R. Girju. 2003. Automatic detection of causal rela-
tions for question answering. In ACL 2003 Work-
shops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
EMNLP.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th ACL.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th ACL.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. 1980. An algorithm for suffix stripping. In
Program, vol. 14, no. 3, pp.130-137.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. Sentence Level Discourse
Parsing using Syntactic and Lexical Information.
Proceedings of HLT/NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
1514
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 226?229,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
ECNU: Effective Semantic Relations Classification without Complicated
Features or Multiple External Corpora
Yuan Chen
?
, Man Lan
?,?
, Jian Su
?
, Zhi Min Zhou
?
, Yu Xu
?
?
East China Normal University, Shanghai, PRC.
?
Institute for Infocomm Research, Singapore.
lanman.sg@gmail.com
Abstract
This paper describes our approach to the
automatic identification of semantic rela-
tions between nominals in English sen-
tences. The basic idea of our strategy
is to develop machine-learning classifiers
which: (1) make use of class-independent
features and classifier; (2) make use of
a simple and effective feature set without
high computational cost; (3) make no use
of external annotated or unannotated cor-
pus at all. At SemEval 2010 Task 8 our
system achieved an F-measure of 75.43%
and a accuracy of 70.22%.
1 Introduction
Knowledge extraction of semantic relations be-
tween pairs of nominals from English text is one
important application both as an end in itself and
as an intermediate step in various downstream
NLP applications, such as information extraction,
summarization, machine translation, QA etc. It is
also useful for many auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing and discourse relation processing.
In the past decade, semantic relation classifica-
tion has attracted a lot of interest from researchers
and a wide variety of relation classification
schemes exist in the literature. However, most
research work is quite different in definition of
relations and granularities of various applications.
That is, there is little agreement on relation
inventories. SemEval 2010 Task 8 (Hendrickx
et al, 2008) provides a new standard benchmark
for semantic relation classification to a wider
community, where it defines 9 relations includ-
ing CAUSE-EFFECT, COMPONENT-WHOLE,
CONTENT-CONTAINER, ENTITY-DESTINATION,
ENTITY-ORIGIN, INSTRUMENT-AGENCY,
MEMBER-COLLECTION, MESSAGE-TOPIC,
PRODUCT-PRODUCER, and a tenth pseudo-
relation OTHER (where relation is not one of the
9 annotated relations).
Unlike the previous semantic relation task in
SemEval 2007 Task 4, the current evaluation pro-
vides neither query pattern for each sentence nor
manually annotated word sense (in WordNet se-
mantic) for each nominals. Since its initiative is
to provide a more realistic real-world application
design that is practical, any classification system
must be usable without too much effort. It needs
to be easily computable. So we need to take into
account the following special considerations.
1. The extracted features for relation are ex-
pected to be easily computable. That is, the
steps in the feature extraction process are to
be simple and direct for the purpose of reduc-
ing errors possibly introduced by many NLP
tools. Furthermore, a unified (global) feature
set is set up for all relations rather than for
each relation.
2. Most previous work at SemEval 2007 Task
4 leveraged on external theauri or corpora
(whether unannotated or annotated) (Davi-
dov and Rappoport, 2008), (Costello, 2007),
(Beamer et al, 2007) and (Nakov and Hearst,
2008) that make the task adaption to different
domains and languages more difficult, since
they would not have such manually classified
or annotated corpus available. From a practi-
cal point of view, our system would make use
of less resources.
3. Most previous work at Semeval 2007 Task
4 constructed several local classifiers on dif-
ferent algorithms or different feature subsets,
one for each relation (Hendrickx et al, 2007)
and (Davidov and Rappoport, 2008). Our ap-
proach is to build a global classifier for all
relations in practical NLP settings.
226
Based on the above considerations, the idea of
our system is to make use of external resources as
less as possible. The purpose of this work is two-
fold. First, it provides an overview of our simple
and effective process for this task. Second, it com-
pares different features and classification strate-
gies for semantic relation.
Section 2 presents the system description. Sec-
tion 3 describes the results and discussions. Sec-
tion 4 concludes this work.
2 System Description
2.1 Features Extraction
For each training and test sentence, we reduce the
annotated target entities e1 and e2 to single nouns
noun1 and noun2, by keeping their last nouns only,
which we assume to be heads.
We create a global feature set for all relations.
The features extracted are of three types, i.e., lex-
ical, morpho-syntactic and semantic. The feature
set consists of the following 6 types of features.
Feature set 1: Lemma of target entities e1
and e2. The lemma of the entities annotated in
the given sentence.
Feature set 2: Stem and POS of words be-
tween e1 and e2. The stem and POS tag of the
words between two nominals. First all the words
between two nominals were extracted and then the
Porter?s stemming was performed to reduce words
to their base forms (Porter, 1980). Meanwhile,
OpenNLP postag tool was used to return part-of-
speech tagging for each word.
Feature set 3: syntactic pattern derived from
syntactic parser between e1 and e2. Typically,
the verb phrase or preposition phrase which con-
tain the nominals are important for relation clas-
sification. Therefore, OpenNLP Parser was per-
formed to do full syntactic parsing for each sen-
tence. Then for each nominal, we look for its par-
ent node in the syntactic tree until the parent node
is a verb phrase or preposition phrase. Then the
label of this phrase and the verb or preposition of
this phrase were extracted as the syntactic features.
Besides, we also extracted other 3 feature types
with the aid of WordNet.
Feature set 4: WordNet semantic class of e1
and e2. The WordNet semantic class of each an-
notated entity in the relation. If the nominal has
two and more words, then we examine the seman-
tic class of ?w1 w2? in WordNet. If no result re-
turned from WordNet, we examine the semantic
class of head in the nominal. Since the cost of
manually WSD is expensive, the system simply
used the first (most frequent) noun senses for those
words.
Feature set 5: meronym-holonym relation
between e1 and e2. The meronym-holonym
relation between nominals. These information
are quite important for COMPONENT-WHOLE and
MEMBER-COLLECTION relations. WordNet3.0
provides meronym and holonym information for
some nouns. The features are extracted in the fol-
lowing steps. First, for nominal e1, we extract its
holonym from WN and for nominal e2, we extract
its Synonyms/Hypernyms. Then, the system will
check if there is same word between e1?s holonym
and e2?s synonym & hypernym. The yes or no
result will be a binary feature. If yes, we also ex-
amine the type of this match is ?part of ? or ?mem-
ber of ? in holonym result. Then this type is also
a binary feature. After that, we exchange the posi-
tion of e1 and e2 and perform the same process-
ing. By creating these features, the system can
also take the direction of relations into account.
Feature set 6: hyponym-hypernym rela-
tion between nominal and the word of ?con-
tainer?. This feature is designed for CONTENT-
CONTAINER relation. For each nominal, WordNet
returns its hypernym set. Then the system examine
if the hypernym set contains the word ?container?.
The result leads to a binary feature.
2.2 Classifier Construction
Our system is to build up a global classifier based
on global feature set for all 9 non-Other relations.
Generally, for this multi-class task, there are two
strategies for building classifier, which both con-
struct classifier on a global feature set. The first
scheme is to treat this multi-class task as an multi-
way classification. Since each pair of nominals
corresponds to one relation, i.e., single label clas-
sification, we build up a 10-way SVM classifier for
all 10 relations. Here, we call it multi-way clas-
sification. That is, the system will construct one
single global classifier which can classify 10 rela-
tions simultaneously in a run. The second scheme
is to split this multi-class task into multiple binary
classification tasks. Thus, we build 9 binary SVM
classifiers, one for each non-Other relation. Noted
that in both strategies the classifiers are built on
global feature set for all relations. For the sec-
ond multiple binary classification, we also exper-
227
imented on different prob. thresholds, i.e., 0.25
and 0.5. Furthermore, in order to reduce errors
and boost performance, we also adopt the major-
ity voting strategy to combine different classifiers.
3 Results and Discussion
3.1 System Configurations and Results
The classifiers for all relations were optimized
independently in a number of 10-fold cross-
validation (CV) experiments on the provided train-
ing sets. The feature sets and learning algorithms
which were found to obtain the highest accuracies
for each relation were then used when applying the
classifiers to the unseen test data.
Table 1 summaries the 7 system configurations
we submitted and their performance on the test
data.
Among the above 7 system, SR5 system shows
the best macro-averaged F1 measure. Table 2 de-
scribes the statistics and performance obtained per
relation on the SR5 system.
Table 3 shows the performance of these 7 sys-
tems on the test data as a function of training set
size.
3.2 Discussion
The first three systems are based on three feature
sets, i.e.,F1-F3, with different classification strat-
egy. The next three systems are based on all six
feature sets with different classification strategy.
The last system adopts majority voting scheme on
the results of four systems, i.e., SR1, SR2, SR4
and SR5. Based on the above series of exper-
iments and results shown in the above 3 tables,
some interesting observations can be found as fol-
lows.
Obviously, although we did not perform WSD
on each nominal and only took the first noun sense
as semantic class, WordNet significantly improved
the performance. This result is consistent with
many previous work on Semeval 2007 Task 4 and
once again it shows that WordNet is important
for semantic relation classification. Specifically,
whether for multi-way classification or multiple
binary classification, the systems involved features
extracted from WordNet performed better than the
others not involved WN, for example, SR4 better
than SR1 (74.82% vs 60.08%), SR5 better than
SR2 (75.43% vs 72.59%), SR6 better than SR3
(72.19% vs 68.50%).
Generally, the performance of multiple binary
classifier is better than multi-way classifier. That
means, given a global feature set for 9 relations,
the performance of 9 binary classifiers is better
than a 10-way classifier. Specifically, when F1-F3
are involved, SR2 (72.59%) and SR3 (68.50%) are
both better than SR1 (60.08%). However, when
F1-F6 feature sets are involved, the performance
of SR4 is between that of SR5 and SR6 in terms of
macro-averaged F
1
measure. With respect to ac-
curacy measure (Acc), SR4 system performs the
best.
Moreover, for multiple binary classification, the
threshold of probability has impact on the perfor-
mance. Generally, the system with prob. threshold
0.25 is better than that with 0.5, for example, SR2
better than SR3 (72.59% vs 68.50%), SR5 better
than SR6 (75.43% vs 72.19%).
As an ensemble system, SR7 combines the re-
sults of SR1, SR2, SR4 and SR5. However, this
majority voting strategy has not shown significant
improvements. The possible reason may be that
these classifiers come from a family of SVM clas-
sifiers and thus the random errors are not signifi-
cantly different.
Besides, one interesting observation is that SR4
system achieved the top 2 performance on TD1
data amongst all participating systems. This
shows that, even with less training data, SR4 sys-
tem achieves good performance.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D.
?
O S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Ro-
mano and S. Szpakowicz. SemEval-2010 Task 8:
Multi-Way Classification of Semantic Relations Be-
tween Pairs of Nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, pp.94-
99, 2010, Uppsala, Sweden.
D. Davidov and A. Rappoport. Classification of
Semantic Relationships between Nominals Using
Pattern Clusters. Proceedings of ACL-08: HLT,
pp.227-235, 2008.
F. J. Costello. UCD-FC: Deducing semantic rela-
tions using WordNet senses that occur frequently
228
Run Feature Set Classifier P (%) R (%) F
1
(%) Acc (%)
SR1 F1-F3 multi-way classification 70.69 58.05 60.08 57.05
SR2 F1-F3 multiple binary (prob. threshold =0.25) 74.02 71.61 72.59 67.10
SR3 F1-F3 multiple binary (prob. threshold =0.5) 80.25 60.92 68.50 62.02
SR4 F1-F6 multi-way classification 75.72 74.16 74.82 70.52
SR5 F1-F6 multiple binary (prob. threshold =0.25) 75.88 75.29 75.43 70.22
SR6 F1-F6 multiple binary (prob. threshold =0.5) 83.08 64.72 72.19 65.81
SR7 F1-F6 majority voting based on SR1, SR2, SR4 and SR5 74.83 75.97 75.21 70.15
Table 1: Summary of 7 system configurations and performance on the test data. Precision, Recall, F1
are macro-averaged for system?s performance on 9 non-Other relations and evaluated with directionality
taken into account.
Run Total # P (%) R (%) F
1
(%) Acc (%)
Cause-Effect 328 83.33 86.89 85.07 86.89
Component-Whole 312 74.82 65.71 69.97 65.71
Content-Container 192 79.19 81.25 80.21 81.25
Entity-Destination 292 79.38 86.99 83.01 86.99
Entity-Origin 258 81.01 81.01 81.01 81.01
Instrument-Agency 156 63.19 58.33 60.67 58.33
Member-Collection 233 73.76 83.26 78.23 83.26
Message-Topic 261 75.2 73.18 74.17 73.18
Product-Producer 231 73.06 61.04 66.51 61.04
Other 454 38.56 40.09 39.31 40.09
Micro-Average 76.88 76.27 76.57 70.22
Macro-Average 75.88 75.29 75.43 70.22
Table 2: Performance obtained per relation on SR5 system. Precision, Recall, F1 are macro-averaged for
system?s performance on 9 non-Other relations and evaluated with directionality taken into account.
Run TD1 TD2 TD3 TD4
F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%)
SR1 52.13 49.50 56.58 54.84 58.16 56.16 60.08 57.05
SR2 46.24 38.90 47.99 40.45 69.83 64.67 72.59 67.10
SR3 39.89 34.56 42.29 36.66 65.47 59.59 68.50 62.02
SR4 67.95 63.45 70.58 66.14 72.99 68.94 74.82 70.52
SR5 49.32 41.59 50.70 42.77 72.63 67.72 75.43 70.22
SR6 42.88 36.99 45.54 39.57 69.87 64.00 72.19 65.81
SR7 58.67 52.71 58.87 53.18 72.79 68.09 75.21 70.15
Table 3: Performance of these 7 systems on the test data as a function of training set size. The four
training subsets, TD1, TD2, TD3 and TD4, have 1000, 2000, 4000 and 8000 (complete) training samples
respectively. F1 is macro-averaged for system?s performance on 9 non-Other relations and evaluated
with directionality taken into account.
in a database of noun-noun compounds. ACL Se-
mEval?07 Workshop, pp.370C373, 2007.
B. Beamer, S. Bhat, B. Chee, A. Fister, A. Rozovskaya
and R.Girju. UIUC: A knowledge-rich approach
to identifying semantic relations between nominals.
ACL SemEval?07 Workshop, pp.386-389, 2007.
I. Hendrickx, R. Morante, C. Sporleder and A. Bosch.
ILK: machine learning of semantic relations with
shallow features and almost no data. ACL Se-
mEval?07 Workshop, pp.187C190, 2007.
P. Nakov and M. A. Hearst. Solving Relational Simi-
larity Problems Using the Web as a Corpus. In Pro-
ceedings of ACL, pp.452-460, 2008.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
229
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139?146,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The Effects of Discourse Connectives Prediction on Implicit Discourse
Relation Recognition
Zhi Min Zhou?, Man Lan?,?, Zheng Yu Niu?, Yu Xu?, Jian Su?
?East China Normal University, Shanghai, PRC.
?Baidu.com Inc., Beijing, PRC.
?Institute for Infocomm Research, Singapore.
51091201052@ecnu.cn, lanman.sg@gmail.com
Abstract
Implicit discourse relation recognition is
difficult due to the absence of explicit
discourse connectives between arbitrary
spans of text. In this paper, we use lan-
guage models to predict the discourse con-
nectives between the arguments pair. We
present two methods to apply the pre-
dicted connectives to implicit discourse
relation recognition. One is to use the
sense frequency of the specific connec-
tives in a supervised framework. The
other is to directly use the presence of the
predicted connectives in an unsupervised
way. Results on PDTB2 show that using
language model to predict the connectives
can achieve comparable F-scores to the
previous state-of-art method. Our method
is quite promising in that not only it has
a very small number of features but also
once a language model based on other re-
sources is trained it can be more adaptive
to other languages and domains.
1 Introduction
Discourse relation analysis involves identifying
the discourse relations (e.g., the comparison re-
lation) between arbitrary spans of text, where
the discourse connectives (e.g., ?however?, ?be-
cause?) may or may not explicitly exist in the text.
This analysis is one important application both as
an end in itself and as an intermediate step in var-
ious downstream NLP applications, such as text
summarization, question answering etc.
As discussed in (Pitler and Nenkova., 2009b),
although explicit discourse connectives may have
two types of ambiguity, i.e., one is discourse or
non-discourse usage (?once? can be either a tem-
poral connective or a word meaning ?formerly?),
the other is discourse relation sense ambiguity
(?since? can serve as either a temporal or causal
connective), their study shows that for explicit
discourse relations in Penn Discourse Treebank
(PDTB) corpus, the most general 4 senses, i.e.,
Comparison (Comp.), Contingency (Cont.), Tem-
poral (Temp.) and Expansion (Exp.), can be eas-
ily addressed by the presence of discourse con-
nectives and a simple method only considering the
sense frequency of connectives can achieve more
than 93% accuracy. This indicates the importance
of connectives for discourse relation recognition.
However, with implicit discourse relation
recognition, there is no connective between the
textual arguments, which results in a very difficult
task. In recent years, a multitude of efforts have
been employed to solve this task. One approach
is to exploit various linguistically informed fea-
tures extracted from human-annotated corpora in
a supervised framework (Pitler et al, 2009a) and
(Lin et al, 2009). Another approach is to perform
recognition without human-annotated corpora by
creating synthetic examples of implicit relations in
an unsupervised way (Marcu and Echihabi, 2002).
Moreover, our initial study on PDTB implicit
relation data shows that the averaged F-score for
the most general 4 senses can reach 91.8% when
we obtain the sense of test examples by map-
ping each implicit connective to its most frequent
sense (i.e., sense recognition using gold-truth im-
plicit connectives). This high F-score performance
again proves that the connectives are very crucial
source for implicit relation recognition.
In this paper, we present a new method to ad-
dress the problem of recognizing implicit dis-
course relation. This method is inspired by the
above observations, especially the two gold-truth
results, which reveals that discourse connectives
are very important signals for discourse relation
recognition. Our basic idea is to recover the im-
plicit connectives (not present in real text) be-
tween two spans of text with the use of a language
139
model trained on large amount of raw data without
any human-annotation. Then we use these pre-
dicted connectives to generate feature vectors in
two ways for implicit discourse relation recogni-
tion. One is to use the sense frequency of the spe-
cific connectives in a supervised framework. The
other is to directly use the presence of the pre-
dicted connectives in an unsupervised way.
We performed evaluation on explicit and im-
plicit relation data sets in the PDTB 2 corpus. Ex-
perimental results showed that the two methods
achieved comparable F-scores to the state-of-art
methods. It indicates that the method using lan-
guage model to predict connectives is very useful
in solving this task.
The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our methods for implicit discourse relation
recognition. Section 4 presents experiments and
results. Section 5 offers some conclusions.
2 Related Work
Existing works on automatic recognition of im-
plicit discourse relations fall into two categories
according to whether the method is supervised or
unsupervised.
Some works perform relation recognition with
supervised methods on human-annotated corpora,
for example, the RST Bank (Carlson et al, 2001)
used by (Soricut and Marcu, 2003), adhoc anno-
tations used by (Girju, 2003) and (Baldridge and
Lascarides, 2005), and the GraphBank (Wolf et al,
2005) used by (Wellner et al, 2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2006) has sig-
nificantly expanded the discourse-annotated cor-
pora available to researchers, using a comprehen-
sive scheme for both implicit and explicit rela-
tions. (Pitler et al, 2009a) performed implicit re-
lation classification on the second version of the
PDTB. They used several linguistically informed
features, such as word polarity, verb classes, and
word pairs, showing performance increases over a
random classification baseline. (Lin et al, 2009)
presented an implicit discourse relation classifier
in PDTB with the use of contextual relations, con-
stituent Parse Features, dependency parse features
and cross-argument word pairs. Although both of
two methods achieved the state of the art perfor-
mance for automatical recognition of implicit dis-
course relations, due to lack of human-annotated
corpora, their approaches are not very useful in the
real word.
Another line of research is to use the unsuper-
vised methods on unhuman-annotated corpus.
(Marcu and Echihabi, 2002) used several pat-
terns to extract instances of discourse relations
such as contrast and elaboration from unlabeled
corpora. Then they used word-pairs between argu-
ments as features for building classification mod-
els and tested their model on artificial data for im-
plicit relations.
Subsequently other studies attempt to ex-
tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered that
Marcu and Echihabi?s models do not perform as
well on implicit relations as one might expect
from the test accuracy on synthetic data. (Gold-
ensohn, 2007) extended the work of (Marcu and
Echihabi, 2002) by refining the training and clas-
sification process using parameter optimization,
topic segmentation and syntactic parsing. (Saito
et al, 2006) followed the method of (Marcu and
Echihabi, 2002) and conducted experiments with
a combination of cross-argument word pairs and
phrasal patterns as features to recognize implicit
relations between adjacent sentences in a Japanese
corpus.
Previous work showed that with the use of some
patterns, structures, or the pairs of words, rela-
tion classification can be performed using unsu-
pervised methods.
In contrast to existing work, we investigated a
new knowledge source, i.e., implicit connectives
predicted using a language model, for implicit re-
lation recognition. Moreover, this method can
be applied in both supervised and unsupervised
ways by generating features on labeled and unla-
beled training data and then performing implicit
discourse connectives recognition.
3 Methodology
3.1 Predicting implicit connectives via a
language model
Previous work (Pitler and Nenkova., 2009b)
showed that with the presence of discourse con-
nectives, explicit discourse relations in PDTB can
be easily identified with more than 90% F-score.
Our initial study on PDTB human-annotated im-
plicit relation data shows that the averaged F-score
for the most general 4 senses can reach 91.8%
when we simply map each implicit connective to
140
its most frequent sense. These high F-scores indi-
cate that the connectives are very crucial source of
information for both explicit and implicit relation
recognition. However, for implicit relations, there
are no explicitly discourse connectives in real text.
This built-in absence makes the implicit relation
recognition task quite difficult. In this work we
overcome this difficulty by inserting connectives
into the two arguments with the use of a language
model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two
arguments, denoted as Arg1 and Arg2. Typi-
cally, there are two possible positions for most
of implicit connectives, i.e., the position before
Arg1 and the position between Arg1 and Arg2.
Given a set of implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as Ppl(Sci,j). According to
the value of Ppl(Sci,j) (the lower the better), we
can rank these sentences and select the connec-
tives in top N sentences as implicit connectives
for this argument pair. Here the language model
may be trained on any large amount of unanno-
tated corpora that can be cheaply acquired. Typi-
cally, a large corpora with the same domain as the
test data will be used for training language model.
Therefore, we chose news corpora, such as North
American News Corpora.
After that, we use the top N predicted connec-
tives to generate different feature vectors and per-
form the classification in two ways. One is to use
the sense frequency of predicted connectives in a
supervised framework. The other is to directly use
the presence of the predicted connectives in an un-
supervised way. The two approaches are described
as follows.
3.2 Using sense frequency of predicted
discourse connectives as features
After the above procedure, we get a sorted set of
predicted discourse connectives. Due to the pres-
ence of an implicit connective, the implicit dis-
course relation recognition task can be addressed
with the methods for explicit relation recognition,
e.g., sense classification based only on connectives
(Pitler et al, 2009a). Inspired by their work, the
first approach is to use sense frequency of pre-
dicted discourse connectives as features. We take
the connective with the lowest perplexity value
(i.e., top 1 connective) as the real connective for
the arguments pair. Then we count the sense
frequency of this connective on the training set.
Figure 1 illustrates the procedure of generating
predicted discourse connective from a language
model and calculating its sense frequency from
training data. Here the calculation of sense fre-
quency of connective is based on the annotated
training data which has labeled discourse rela-
tions, thus this method is a supervised one.
Figure 1: Procedure of generating a predicted dis-
course connective and its sense frequency from the
training set and a language model.
Then we can directly use the sense frequency
to generate a 4-feature vector to perform the clas-
sification. For example, the sense frequency of
the connective but in the most general 4 senses
can be counted from training set as 691, 6, 49,
2, respectively. For a given pair of arguments,
if but is predicted as the top 1 connective based
on a language model, a 4-dimension feature vec-
tor (691, 6, 49, 2) is generated for this pair and
used for training and test procedure. Figure 2
and 3 show the training and test procedure for this
method.
Figure 2: Training procedure for the first ap-
proach.
141
Figure 3: Test procedure for the first approach.
3.3 Using presence or absence of predicted
discourse connective as features
(Pitler et al, 2008) showed that most connectives
are unambiguous and it is possible to obtain high-
accuracy in prediction of discourse senses due to
the simple mapping relation between connectives
and senses. Given two examples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey the Comparison and Contingency
senses respectively. In most cases, we can easily
recognize the relation sense by the appearance of
a discourse connective since it can be interpreted
in only one way. That means the ambiguity of
the mapping between sense and connective is quite
low. Therefore, the second approach is to use only
the presence of the top N predicted discourse con-
nectives to generate a feature vector for a given
pair of arguments.
4 Experiment
4.1 Data sets
We used PDTB as our data set to perform the eval-
uation of our methods. The corpus contains anno-
tations of explicit and implicit discourse relations.
The first evaluation is performed on the annotated
implicit data set. Following the work of (Pitler et
al., 2009a), we used sections 2-20 as the training
set, sections 21-22 as the test set and sections 0-
1 as the development set for parameter optimiza-
tion (e.g., N value). The second evaluation is per-
formed on the annotated explicit data set. We fol-
low the method used in (Sporleder and Lascarides,
2008) to remove the discourse connective from the
explicit instances and consider these processed in-
stances as implicit ones.
We constructed four binary classifiers to recog-
nize each main senses (i.e., Cont., Cont., Exp.,
Temp.) from the rest. For each sense we used
equal numbers of positive and negative instances
in training set. The negative instances were cho-
sen at random from the rest of training set. For
both evaluations all instances in sections 21-22
were used as test set. Table 1 lists the numbers
of positive and negative instances for each sense
in training, development and test sets of implicit
and explicit relation data sets.
4.2 Evaluation and classifier
To evaluate the performance of above systems, we
used two widely-used measures, F-score ( i.e., F1)
and accuracy. In addition, in this work we used
the LIBSVM toolkit to construct four linear SVM
classifiers for each sense.
4.3 Preprocessing
We used the SRILM toolkit to build a language
model and calculated the perplexity value for each
training and test sample. The steps are described
as follows. First, since perplexity is an intrin-
sic score to measure the similarity between train-
ing and test samples, in order to fit the restric-
tion of perplexity we chose 3 widely-used cor-
pora in the Newswire domain to train the language
model, i.e., (1) the New York part of BLLIP North
American News Text (Complete), (2) the Xin and
(3) the Ltw parts of the English Gigaword Fourth
Edition. For the BLLIP corpus with 1,796,386
automatically parsed English sentences, we con-
verted the parsed sentences into original textual
data. Some punctuation marks such as commas,
periods, minuses, right/left parentheses are con-
verted into their original form. For the Xin and
Ltw parts, we only used the Sentence Detector
toolkit in OpenNLP to split each sentence. Finally
we constructed 3-, 4- and 5-grams language mod-
els from these three corpora. Table 2 lists statis-
tics of different n-grams in the different language
models and different corpora.
Next, for each instance we combined its Arg1
and Arg2 with connectives obtained from PDTB.
There are two types of connectives, single con-
nectives (e.g. ?because? and ?but?) and paral-
142
Table 1: Statistics of positive and negative instances for each sense in training, development and test sets
of implicit and explicit relation data sets.
Implicit Explicit
Comp. Cont. Exp. Temp. Comp. Cont. Exp. Temp.
Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663
Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247
Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124
Table 2: Statistics of different n-grams in the dif-
ferent language models and different corpora.
n-gram BLLIP - Gigaword- Gigaword-
New York Xin Ltw
1-gram 1638156 2068538 2276491
2-grams 26156851 23961796 33504873
3-grams 80876435 77799100 101855639
4-grams 127142452 134410879 159791916
5-grams 146454530 168166195 183794771
lel connectives (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of
possible implicit connectives {ci}, for a single
connective ci, we constructed two synthetic sen-
tences, ci+Arg1+Arg2 and Arg1+ci+Arg2. In case
of parallel connectives, we constructed one syn-
thetic sentence like ci,1+Arg1+ci,2+Arg2.
As a result, we obtain 198 synthetic sentences
(|ci| ? 2 for single connective or |ci| for parallel
connective) for each pair of arguments. Then we
converted all words to lower cases and used the
language model trained in the above step to calcu-
late its perplexity (the lower the better) value on
sentence level. The sentences were ranked from
low to high according to their perplexity scores.
For example, given a sentence with arguments pair
as follows:
Arg1: it increased its loan-loss reserves by $93
million after reviewing its loan portfolio,
Arg2: before the loan-loss addition it had operat-
ing profit of $10 million for the quarter.
we got the perplexity (Ppl) values for this argu-
ments pair in combination with two connectives
(but and by comparison) in two positions as fol-
lows:
1. but + Arg1 + Arg2: Ppl= 349.622
2. Arg1 + but + Arg2: Ppl= 399.339
3. by comparison + Arg1 + Arg2: Ppl= 472.206
4. Arg1 + by comparison + Arg2: Ppl= 543.051
In our second approach described in Section
3.3, we considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is,
the presence or absence of the specific connective.
According to the value of Ppl(Sci,j), we tried var-
ious N values on development set to get the opti-
mal N value.
4.4 Results
Table 3 summarizes the best performance
achieved using gold-truth implicit connectives,
the previous state-of-art performance achieved
by (Pitler et al, 2009a) and our approaches.
The first line shows the result by mapping the
gold-truth implicit connectives directly to the
relation?s sense. The second line presents the best
result of (Pitler et al, 2009a). One thing worth
mentioning here is that for the Expansion relation,
(Pitler et al, 2009a) expanded both training and
test sets by including EntRel relation as positive
examples, which makes it impossible to perform
direct comparison. The third and fourth lines
show the best results using our first approach,
where the sense frequency is counted on explicit
and implicit training set respectively. The last line
shows the best result of our second approach only
considering the presence of top N connectives.
Table 4 summarizes the best performance using
gold-truth explicit connectives reported in (Pitler
and Nenkova., 2009b) and our two approaches.
Figure 4 shows the curves of averaged F-scores
on implicit connective classification with differ-
ent n-gram language models. From this figure we
can see that all 4-grams language models achieved
around 0.5% better averaged F-score than 3-grams
models. And except for Ltw corpus, other 5-grams
models achieved lower averaged F-score than 4-
grams models. Specially the 5-grams result of
New York corpus is much lower than its 3-grams
result.
Figure 5 shows the averaged F-scores of dif-
ferent top N on the New York corpus with 3-,
4- and 5-grams language models. The essential
143
Table 3: Best result of implicit relations compared with state-of-art methods.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using
gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)
Best result in (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)
Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)
Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)
Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)
Table 4: Best result of explicit relation conversion to implicit relation compared with results using the
same method.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using gold-truth
explicit connectives in (Pitler et al, 2009a) N/A N/A N/A N/A N/A(93.67)
Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)
Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)
0 10 20 30 40 50 60 70 80 90 100110120130140150160170180190200
30.0
30.5
31.0
31.5
32.0
32.5
33.0
33.5
34.0
34.5
 
 NY 3-gram
 NY 4-gram
 NY 5-gram
Top N value
A
v
e
r
a
g
e
d
 
F
-
S
c
o
r
e
Figure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with different
top N values.
trend of these curves cannot be summarized in
one sentence. But we can see that the best aver-
aged F-scores mostly appeared in the range from
100 ? 160. For 4-grams and 5-grams models, the
system achieved the top averaged F-scores when
N = 20 as well.
4.5 Discussion
Experimental results on PDTB showed that using
predicted connectives achieved the comparable F-
scores of the state-of-art method.
From Table 3 we can find that our results are
closely to the best performance of previous state-
of-art methods in terms of averaged F-score. On
the Comparison sense, our first approach has an
improvement of more than 4% F-score on the pre-
vious state-of-art method (Pitler et al, 2009a). As
we mentioned before, for the Expansion sense,
they included EntRel relation to expand the train-
ing set and test set, which makes it impossible to
perform a direct comparison. Since the positive in-
stances size has been increased by 50%, they may
achieve a higher F-score than our approach. For
other relations, our best performance is slightly
lower than theirs. While bearing in mind that our
approach only uses a very small amount of fea-
tures for implicit relation recognition. Compared
144
3-gram 4-gram 5-gram
31.0
31.2
31.4
31.6
31.8
32.0
32.2
32.4
32.6
 
 New York
 Xin
 Ltw
n-gram
A
v
e
r
a
g
e
d
 
F
-
s
c
o
r
e
Figure 4: Curves of averaged F-score on implicit
connective classification with n-Gram language
model.
with other approaches involving thousands of fea-
tures, our method is quite promising.
From Table 4 we observe comparable averaged
F-score (39.98% F-score) on explicit relation data
set to that on implicit relation data set. Previ-
ously, (Sporleder and Lascarides, 2008) also used
the same conversion method to perform implicit
relation recognition on different corpora and their
best result is around 33.69% F-score. Although
the two results cannot be compared directly due to
different data sets, the magnitude of performance
quantities is comparable and reliable.
By comparing with the above different systems,
we find several useful observations. First, our
method using predicted implicit connectives via a
language model can help the task of implicit dis-
course relation recognition. The results are com-
parable to the previous state-of-art studies. Sec-
ond, our method has a lot of advantages, i.e., a
very small amount of features (several or no more
than 200 vs. ten thousand), easy computation
(only based on the trained language model vs. us-
ing a lot of NLP tools to extract a large amount of
linguistically informed features) and fast running,
which makes it more practical in real world appli-
cation. Furthermore, since the language model can
be trained on many corpora whether annotated or
unannotated, this method is more adaptive to other
languages and domains.
5 Conclusions
In this paper we have presented an approach to
implicit discourse relation recognition using pre-
dicted implicit connectives via a language model.
The predicted connectives have been used for im-
plicit relation recognition in two ways, i.e., super-
vised and unsupervised framework. Results on the
Penn Discourse Treebank 2.0 show that the pre-
dicted discourse connectives can help implicit re-
lation recognition and the two algorithms achieve
comparable F-scores with the state-of-art method.
In addition, this method is quite promising due to
its simple, easy to retrieve, fast run and increased
adaptivity to other languages and domains.
Acknowledgments
We thank the reviewers for their helpful com-
ments and Jonathan Ginzburg for his mentor-
ing. This work is supported by grants from
National Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, College
Park, MD,2001.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL 2003 Workshops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics.
145
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.
Lee, A. Joshi. 2008. Easily Identifiable Dis-
course Relations. Coling 2008: Companion vol-
ume: Posters.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. Proceedings of the COLING/ACL Work-
shop on Sentiment and Subjectivity in Text.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. Proceedings of the Human Language Technol-
ogy and North American Association for Computa-
tional Linguistics Conference.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
146
