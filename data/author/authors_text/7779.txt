Multilingual and cross-lingual news topic tracking 
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, Emilia K?sper & Irina Temnikova
Joint Research Centre, European Commission 
T.P. 267, Via E. Fermi 1 
21020 Ispra (VA), Italy 
http://www.jrc.it/langtech 
Firstname.Lastname@jrc.it 
 
 
Abstract 
We are presenting a working system for automated 
news analysis that ingests an average total of 7600 
news articles per day in five languages. For each 
language, the system detects the major news stories 
of the day using a group-average unsupervised ag-
glomerative clustering process. It also tracks, for 
each cluster, related groups of articles published 
over the previous seven days, using a cosine of 
weighted terms. The system furthermore tracks re-
lated news across languages, in all language pairs 
involved. The cross-lingual news cluster similarity 
is based on a linear combination of three types of 
input: (a) cognates, (b) automatically detected ref-
erences to geographical place names and (c) the re-
sults of a mapping process onto a multilingual clas-
sification system. A manual evaluation showed that 
the system produces good results.  
1 Introduction 
Most large organisations, companies and politi-
cal parties have a department analysing the news 
on a daily basis. Motivations differ, but often these 
organisations want to know how they and their 
leading members are represented in the news, or 
they need to know whether there has been any 
event they ought to know about. Examples of ex-
isting news gathering and analysis systems are In-
formedia1 and the Europe Media Monitor (Best et 
al. 2002). DARPA has taken an interest in the do-
main and launched, in 1996, the Topic Detection 
and Tracking task2 (TDT) under the TIDES pro-
gram. It distinguishes three major tasks: (a) seg-
mentation of a continuous information flow (e.g. 
spoken news) into individual news items, (b) de-
tection of breaking news, i.e. of a new subject that 
has not previously been discussed, and (c) topic 
tracking, i.e. the identification of related news over 
time. Our task is the analysis of a multilingual col-
lection of written news articles, which means that 
segmentation (task a) is of no relevance. Neither 
do we present here work on the detection of new 
                                                     
1 http://www.informedia.cs.cmu.edu/ 
2 http://www.nist.gov/speech/tests/tdt/ 
topics (task b). Instead, we focus on the topic 
tracking task (c), and especially on the novel as-
pect of cross-lingual tracking.  
The aim of our work is to provide an automati-
cally generated overview over the major news of 
each day (midnight to midnight) in the languages 
English, German, French, Spanish and Italian. The 
corpus consists of news items gathered from a 
large number of internet news sites world-wide, 
and of various subscription news wires (Best et al 
2002). The texts are thus from hundreds of differ-
ent sources (feeds) which often discuss the same 
events. Newspapers often publish the news they 
receive from press agencies with no or few 
amendments. The corpus of news articles thus con-
tains not only summaries of the same events writ-
ten by different journalists, but also many dupli-
cates and near duplicates of the same original text 
which need to be eliminated from the collection.  
In order to identify the major news, we identify 
clusters of similar news items, i.e. news items that 
deal with the same subject. All subjects that trigger 
a large number of news articles from various feeds 
are of interest. The related news thus do not neces-
sarily have to discuss events, i.e. things that happen 
at a particular time and place (e.g. the 11/03 Ma-
drid bombing), but they can also be a thread of dis-
cussions on the same subject, such as the campaign 
for the US presidential elections.  
In section 2, we summarise other work on topic 
tracking, on cross-lingual news linking and on fea-
ture extraction methods. Section 3 describes the 
multilingual news corpus and the text feature ex-
traction used for the document representation. In 
section 4, we present the process and evaluation of 
major news identification. Section 5 is dedicated to 
the multi-monolingual topic tracking process and 
its evaluation. Section 6 describes the cross-lingual 
linking of related clusters of major news, plus 
evaluation results. Section 7 points to future work.  
2 Related work 
Allan et al (1998) identify new events and then 
track the topic like in an information filtering task 
by querying new documents against the profile of 
the newly detected topic. Topics are represented as 
a vector of stemmed words and their TF.IDF val-
ues, only considering nouns, verbs, adjectives and 
numbers. In their experiments, using between 10 
and 20 features produced optimal results. Schultz 
(1999) took the alternative approach of clustering 
texts with a single-linkage unsupervised agglom-
erative clustering method, using cosine similarity 
and TF.IDF for term weighting. He concludes that 
?a successful clustering algorithm must incorporate 
a representation for a cluster itself as group aver-
age clustering does?. We followed Schultz? advice. 
Unlike Schultz, however, we use the log-likelihood 
test for term weighting as this measure seems to be 
better when dealing with varying text sizes (Kil-
garriff 1996). We do not consider parts-of-speech, 
lemmatisation or stemming, as we do not have ac-
cess to linguistic resources for all the languages we 
need to work with, but we use an extensive list of 
stop words.  
Approaches to cross-lingual topic tracking are 
rather limited. Possible solutions for this task are to 
either translate documents or words from one lan-
guage into the other, or to map the documents in 
both languages onto some multilingual reference 
system such as a thesaurus. Wactlar (1999) used 
bilingual dictionaries to translate Serbo-Croatian 
words and phrases into English and using the trans-
lations as a query on the English texts to find simi-
lar texts. In TDT-3, only four systems tried to es-
tablish links between documents written in differ-
ent languages. All of them tried to link English and 
Chinese-Mandarin news articles by using Machine 
Translation (e.g. Leek et al 1999). Using a ma-
chine translation tool before carrying out the topic 
tracking resulted in a 50% performance loss, com-
pared to monolingual topic tracking.  
Friburger & Maurel (2002) showed that the iden-
tification and usage of proper names, and espe-
cially of geographical references, significantly im-
proves document similarity calculation and cluster-
ing. Hyland et al (1999) clustered news and de-
tected topics exploiting the unique combinations of 
various named entities to link related documents. 
However, according to Friburger & Maurel (2002), 
the usage of named entities alone is not sufficient.  
Our own approach to cross-lingual topic track-
ing, presented in section 6, is therefore based on 
three kinds of information. Two of them exploit 
the co-occurrence of named entities in related news 
stories: (a) cognates (i.e. words that are the same 
across languages, including names) and (b) geo-
graphical references. The third component, (c) a 
process mapping texts onto a multilingual classifi-
cation scheme, provides an additional, more con-
tent-oriented similarity measure. Pouliquen et al 
(2003) showed that mapping texts onto a multilin-
gual classification system can be very successful 
for the task of identifying document translations. 
This approach should thus also be an appropriate 
measure to identify similar documents in other 
languages, such as news discussing the same topic. 
3 Feature extraction for document represen-
tation 
The similarity measure for monolingual news 
item clustering, discussed in section 4, is a cosine 
of weighted terms (see 3.1) enriched with informa-
tion about references to geographical place names 
(see 3.2).  Related news are tracked over time by 
calculating the cosine of their cluster representa-
tions, while setting certain thresholds (section 5). 
The cross-lingual linking of related clusters, as de-
scribed in section 6, additionally uses the results of 
a mapping process onto a multilingual classifica-
tion scheme (see 3.3).  
The news corpus consists of a daily average of 
3350 English news items, 2100 German, 870 Ital-
ian, 800 French and 530 Spanish articles, coming 
from over three hundred different internet sources.  
3.1 Keyword identification 
For monolingual applications, we represent 
documents by a weighted list of their terms. For 
the weighting, we use the log-likelihood test, 
which is said to perform better than the alternatives 
TF.IDF or chi-square when comparing documents 
of different sizes (Kilgarriff 1996). The reference 
corpus was produced with documents of the same 
type, i.e. news articles. It is planned to update the 
reference word frequency list daily or weekly so as 
to take account of the temporary news bias towards 
specific subjects (e.g. the Iraq war). We set the p-
value to 0.01 in order to limit the size of the vector 
to the most important words. Furthermore, we use 
a large list of stop words that includes not only 
function words, but also many other words that are 
not useful to represent the contents of a document. 
We do not consider part-of-speech information and 
do not carry out stemming or lemmatisation, in 
order to increase the speed of the process and to be 
able to include new languages quickly even if we 
do not have linguistic resources for them. Cluster-
ing results do not seem to suffer from this lack of 
linguistic normalisation, but when we extend the 
system to more highly inflected languages, we will 
have to see whether lemmatisation will be neces-
sary. The result of the keyword identification proc-
ess is thus a representation of each incoming news 
article in a vector space.  
3.2 Geographical Place Name Recognition 
For place name recognition, we use a system that 
has been developed by Pouliquen et al (2004). 
Compared to other named entity recognition sys-
tems, this tool has the advantage that it recognises 
exonyms (foreign language equivalences, e.g. Ven-
ice vs. Venezia) and that it disambiguates between 
places with the same name (e.g. Paris in France vs. 
the other 13 places called Paris in the world). 
However, instead of using the city and region 
names as they are mentioned in the article, each 
place name simply adds to the country score of 
each article. The idea behind this is that the place 
names themselves are already contained in the list 
of keywords. By adding the country score sepa-
rately, we heighten the impact of the geographical 
information on the clustering process.  
The country scores are calculated as follows: for 
each geographical place name identified for a 
given country, we add one to the country counter. 
We then normalise this value using the log-
likelihood value, using the average country counter 
in a large number of other news articles as a refer-
ence base. As with keywords, we plan to update 
the country counter reference frequency list on a 
daily or weekly basis. The resulting normalised 
country score has the same format as the keyword 
list so that it can simply be added to the document 
vector space representation.  
3.3 Mapping documents onto a multilingual 
classification scheme 
For the semantic mapping of news articles, we 
use an existing system developed by Pouliquen et 
al. (2003), which maps documents onto a multilin-
gual thesaurus called Eurovoc. Eurovoc is a wide-
coverage classification scheme with approximately 
6000 hierarchically organised classes. Each of the 
classes has exactly one translation in the currently 
22 languages for which it exists. The system car-
ries out category-ranking classification using Ma-
chine Learning methods. In an inductive process, it 
builds a profile-based classifier by observing the 
manual classification on a training set of docu-
ments with only positive examples. The outcome 
of the mapping process is a ranked list of the 100 
most pertinent Eurovoc classes. Due to the multi-
lingual nature of Eurovoc, this representation is 
independent of the text language so that it is very 
suitable for cross-lingual document similarity cal-
culation, as was shown by Pouliquen et al (2003).  
4 Clustering of news articles 
In this process, larger groups of similar articles 
are grouped into clusters. Unlike in document clas-
sification, clustering is a bottom-up, unsupervised 
process, because the document classes are not 
known beforehand. 
4.1 Building a dendrogram 
In the process, we build a hierarchical clustering 
tree (dendrogram), using an agglomerative algo-
rithm (Jain et al 1999). In a first step, (1) we cal-
culate the similarity between each document pair 
in the collection (i.e. one full day of news in one 
language), applying the cosine formula to the 
document vector pairs. The vector for each single 
document consists of its keywords and their log-
likelihood values, enhanced with the country pro-
file as described in sections 3.1 and 3.2. (2) When 
two or more documents have a cosine similarity of 
90% or more, we eliminate all but one of them as 
we assume that they are duplicates or near-
duplicates, i.e. they are exact copies or slightly 
amended versions of the same news wire. (3) We 
then combine the two most similar documents into 
a cluster, for which we calculate a new representa-
tion by merging the two vectors into one. For the 
node combining the two documents, we also have 
an intra-cluster similarity value showing the degree 
to which the two documents are similar. For the 
rest of the clustering process, this node will be 
treated like a single document, with the exception 
that it will have twice the weight of a single docu-
ment when being merged with another document 
or cluster of documents. We iteratively repeat steps 
(1) and (3) so as to include more and more docu-
ments into the binary dendrogram until all docu-
ments are included. The resulting dendrogram will 
have clusters of articles that are similar, and a list 
of keywords and their weight for each cluster. The 
degree of similarity for each cluster is shown by its 
intra-cluster similarity value.  
4.2 Cluster extraction to identify main events 
In a next step, we search the dendrogram for the 
major news clusters of the day, by identifying all 
sub-clusters of documents that fulfil the following 
conditions: (a) the intra-cluster similarity (cluster 
cohesiveness) is above the threshold of 50%; (b) 
the number X of articles in the cluster is at least 
0.6% of the total number of articles of that lan-
guage per day; (c) the number Y of different feeds 
is at least half the minimum number of articles per 
cluster (Y = X/2).  
The threshold of 50% in (a) was chosen because 
it guarantees that most related articles are included 
in the cluster, while unrelated ones are mostly ex-
cluded (see section 4.3). The minimum number of 
articles per cluster in (b) was chosen to limit the 
number of major news clusters per day. We re-
quested a minimum number of different news 
feeds (c) so as to be sure that the news items are of 
general interest and that we are not dealing with 
some newspaper-specific or local issues.  
With the current settings, the system produces an 
average of 9 English major news clusters per day, 
11 Italian, 16 German, 20 French and 21 Spanish. 
The varying numbers indicate that the settings 
should probably be changed so as to produce a 
similar number of major news clusters per day in 
the various languages. Most likely, the minimum 
number of feeds should have an upper maximum 
value for languages like English with thousands of 
news articles per day.  
For each cluster, we have the following informa-
tion: number of articles, number of sources (feeds), 
intra-cluster similarity measure and keywords. Us-
ing our group-average approach we also have the 
centroid of the cluster (i.e. the vector of features 
that represents the cluster). For each cluster, we 
compute the article that is most similar to the cen-
troid (short: the centroid article). We use the title 
of this centroid article as the title for the cluster 
and we present this article to the users as a first 
document to read about the contents of the whole 
cluster.  
The collection of clusters is mainly presented to 
the users as a flat and independent list of clusters. 
However, as we realised that some of the clusters 
are more related than others (e.g. with the recent 
interest in Iraq, there are often various clusters 
covering different aspects of the political situation 
of the country), we position clusters with an inter-
cluster similarity of over 30% closer to each other 
when presenting them to the users.  
4.3 Evaluation of the monolingual clustering 
The evaluation of clustering results is rather 
tricky. According to Joachims (2003), clustering 
results can be evaluated using a variety of different 
ways: (a) let the market decide (select the winner); 
(b) ask end users; (c) measure the ?tightness? or 
?purity? of clusters; (d) use human-identified clus-
ters to evaluate system-generated ones. The last 
solution (d) is out of our reach because it is very 
resource-consuming; several evaluators would be 
needed for cross-checking the human judgement. 
The ?market? (a) and user groups (b) will use and 
evaluate our system in the near future, but we need 
to evaluate the system prior to showing it to a large 
number of customers. We therefore focus on 
method (c) by letting a person judge how consis-
tently the articles of each cluster treat the same 
story.  
We evaluated the major clusters of English news 
articles (using the 50% intra-cluster similarity 
threshold) produced for the seven-day period start-
ing 9 March 2004. During this period, 71 clusters 
containing 1072 news articles were produced. The 
evaluator was asked to decide, for each cluster and 
on a four-grade scale, to what extent the clustered 
articles were related to the centroid article. Com-
paring the clustered articles to the centroid article 
was chosen over evaluating the homogeneity of the 
cluster because it is both easier and closer to the 
real-life situation of the users: users will enter the 
cluster via the centroid article and will judge the 
other articles according to whether or not they con-
tain the information they expect. The evaluation 
scale distinguishes the following ratings:  
 
(0) wrong link, e.g. Madrid football results vs. 
Madrid elections; this is a hypothetical exam-
ple as no such link was found.  
(1) loosely connected story, e.g. Welsh documen-
tary on drinking vs. alcohol policy in Britain; 
(2) interlinked news stories, e.g. 11/03 Madrid 
bombing vs. elections of the Spanish Prime 
Minister Zapatero vs. Spanish decision to pull 
troops out of Iraq; 
(3) same news story. 
 
In the evaluation, 91.5% of the articles were 
rated as good (3), 7.7% were rated as interlinked 
(2) and 0.8% were rated as loosely connected. No 
wrong links were found. 47 of the 71 clusters only 
contained good articles (3). Loosely connected ar-
ticles (1) were distributed evenly. No more than 
two  articles of this rating were found in a single 
cluster. They never amounted to more than 17% of 
all articles in a cluster (2 out of 12 articles).  
An evaluation of the clusters produced on one 
day?s data with 30% and 40% intra-cluster similar-
ity thresholds showed that the performance de-
creased drastically. In 30%-clusters, we found sev-
eral wrong links (category 0), while no such wrong 
links were found in the 50%-clusters. The total 
number of wrong (0) or loosely connected (1) arti-
cles went up from one (in the 50%-cluster for that 
day) to 37. Furthermore, the worst clusters con-
tained over 50% of such unrelated articles. The 
40%-clusters were of a slightly better quality, but 
they still were clearly less good than the 50%-
clusters: The percentage of wrong (0) and loosely 
connected (1) articles only went up from 0.8% (in 
the 50%-clusters) to 4%, but some of the 40%-
clusters still had more bad (category 0 or 1) than 
good (category 2 or 3) articles. These numbers 
confirm that our choice of the 50% intra-cluster 
similarity threshold is most useful. 
We have not produced a quantitative evaluation 
of the miss rate of the clustering process (i.e. the 
number of related articles not included in the clus-
ter, showing the recall). However, a full-text 
search of the relevant proper names in the rest of 
the news collection showed that the clustering 
process missed very few related articles. In any 
case, from our users? point of view, it is much 
more important to know the major news stories of 
a specific day than being able to access all articles 
on the subject.  
Statistical evaluation showed no correlation be-
tween cluster size and accuracy. However, cate-
gory (2) results were more frequently found in 
clusters pertaining to news stories that go on for a 
long time, such as the US presidential elections. 
These stories get wide coverage without being 
?breaking news?, and many of the articles involved 
are commentaries. Some of the category (2) results 
were also found in stories around the Madrid 
bombing and its consequences: some articles dis-
cussed the bombing itself on 11 March (number of 
dead, investigation, mourning); others discussed 
the fact that, in the 14 March elections, the Spanish 
people elected the socialists as they felt that former 
Prime Minister Aznar?s politics were partially re-
sponsible for this tragedy; yet other articles dis-
cussed the post-election consequences such as the 
decision of the new Socialist government to pull 
out the Spanish troops from Iraq, etc. Many of the 
articles touched upon several of these issues. Arti-
cles were rated as good (3) if they had at least one 
core topic in common with the centroid article.  
5 Monolingual linking of news over time 
Establishing automatic links between the major 
clusters of news published in one language in the 
last 24 hours and the news published in previous 
days can help users in their analysis of events. Es-
tablishing historical links between related news 
stories is the third of the TDT tasks (see the intro-
duction in section 1).  
We track topics by calculating the cosine simi-
larity between all major news clusters of one day 
with all major news clusters of the previous days, 
currently up to a maximum distance of seven days. 
The input for the similarity calculation is the clus-
ter vector produced by the monolingual clustering 
process (see section 4.2). The output for each pair-
wise similarity calculation is a similarity value be-
tween 0 and 1. Whether we decide that two clusters 
are related or not depends on the similarity thresh-
old we set. We found that related clusters over time 
have an extremely high similarity, often around 
90%, which shows that the vocabulary used in 
news stories over time changes very little. For test-
ing purposes, we set the threshold very low, at 
15%, so that we could determine a useful threshold 
during the evaluation process.  
5.1 Evaluation of historical linking 
We evaluated the historical links for the 136 
English clusters of major news produced for the 
two-week period starting on 9 March 2004, look-
ing at the seven-day window preceding the day for 
which each major news cluster was identified. The 
total number of historical links found for this pe-
riod is 228, i.e. on average 1.68 historical links per 
major news cluster. However, for 42 of the 136 
major news clusters, the system did not find any 
related news clusters with a similarity of 15% or 
more.  
We made a binary distinction between ?closely 
related articles? (+) and ?unrelated, or not so re-
lated articles? (?).The evaluation results at varying 
cosine similarity thresholds, displayed in Table 1, 
show that there is no threshold which includes all 
good clusters and excludes all bad ones. Setting the 
threshold at 40% would mean that 173 (135+24+ 
14) of the 203 good clusters (86%) would be found 
while three bad ones would also be shown to the 
user. Setting the threshold at the more inclusive 
level of 20% would mean that 199 of the 203 good 
clusters (98%) would be found, but the number of 
unrelated ones would increase to 17.  
 
Similarity + Related  ? Unrelated 
15 ? 19% 4 8 
20 ? 39% 26 14 
40 ? 59% 14 2 
60 ? 79% 24 0 
80 ? 100% 135 1 
Total 203 25 
Table 1: Evaluation, for varying similarity thresh-
olds, of the automatically detected links between 
major news of the day and the major news pub-
lished in the seven days before. The distinction 
was binary: Related (+) or Not (so) related (?). 
6 Cross-lingual linking of news clusters 
News analysts and employees in press rooms 
and public relations departments often want to see 
how the same news is discussed in different coun-
tries. To allow easy access to related news in other 
languages, we establish cross-lingual links between 
the clusters of major news stories. As major news 
in one country sometimes is only minor news in 
another, we calculate a second, alternative group of 
news clusters for each language and each day, con-
taining a larger number of smaller clusters. To get 
this alternative group of clusters, we set the intra-
cluster similarity to 25% and require that the news 
of the cluster come from at least two different 
news sources. These conditions are much weaker 
than the requirements described in section 4.2. For 
each major news cluster (50% intra-cluster similar-
ity) per day and per language, we thus try to find 
related news in the other languages among any of 
the smaller clusters produced with the 25% intra-
cluster similarity requirement.  
We use three types of input for the calculation of 
cross-lingual cluster similarity: (a) the vector of 
keywords, as described in section 3.1, not en-
hanced with geographical information, (b) the 
country score vector, as described in section 3.2, 
and (c) the vector of Eurovoc descriptors, as de-
scribed in section 3.3. The impact of the three 
components is currently set to 20%, 30% and 50% 
respectively. Using the Eurovoc vector alone 
would give very high similarity values for, say, 
news about elections in France and in the United 
States. By adding the country score, a considerable 
weight in the cross-lingual similarity calculation is 
given to the countries that are mentioned in each 
news cluster. The overlap between the keyword 
vectors of documents in two different languages 
will, of course, be extremely little, but it increases 
with the number of named entities that the docu-
ments have in common. According to Gey (2000), 
30% of content-bearing words in journalistic text 
are proper names.  
The system ignores individual articles, but calcu-
lates the similarity between whole clusters of the 
different languages. The country score and the 
Eurovoc descriptor vector are thus assigned to the 
cluster as a whole, treating all articles of each clus-
ter like one big bag of words. 
6.1 Evaluation of cross-lingual cluster links 
The evaluation for the cross-lingual linking was 
carried out on the same corpus as the evaluation of 
the historical links, i.e. taking the 136 English ma-
jor news clusters as a starting point. Cross-lingual 
cluster links were evaluated for two languages, 
English to French and English to Italian. The 
evaluation was again binary, i.e. clusters were ei-
ther judged as being ?closely related? (+) or ?unre-
lated, or not so related? (?). For 31 English clus-
ters, no French cluster was found. Similarly, for 32 
English clusters, no Italian cluster was found. This 
means that for almost 25% of the English-speaking 
major news stories (31/136), there was no equiva-
lent news cluster in the other languages.  
For the remaining English clusters, a total of 131 
French and 133 Italian clusters were detected by 
the system, i.e. on average more than one for each 
English cluster. However, when several related 
news clusters were found, only the one with the 
highest score was considered in the evaluation.  
Table 2 not only shows that the English-Italian 
links are less reliable than the English-French ones 
(the Italian document representation is inferior to 
the French one because we spent less effort on op-
timising the Italian keyword assignment), but also 
that the quality of cross-lingual links is generally 
lower than the historical links presented in sec-
tion 5.1. If we set the threshold for identifying re-
lated news across languages to 30%, the system 
catches 74 of the 75 good French clusters (99%) 
and 67 of the 69 Italian clusters (97%). However, 
the system then also proposes 13 bad French and 
12 bad Italian clusters to the users. Setting the 
threshold higher would decrease the number of 
wrong hits. However, we decided to use the 
threshold of 30% because we consider it important 
for users to be able to find related news in other 
languages. Furthermore, unrelated clusters are usu-
ally very easy to detect just by looking at the title 
of the cluster.  
 
Similarity FR +  FR ? IT + IT ? 
15 ? 19% 0 7 0 1 
20 ? 29% 1 6 2 11 
30 ? 39% 5 6 7 8 
40 ? 49% 16 4 13 5 
50 ? 59% 19 1 18 6 
60 ? 100% 34 1 29 1 
Total 75 25 69 32 
Table 2: Evaluation, for varying similarity thresh-
olds, of the automatically detected cross-lingual 
links between English major news and French (FR) 
or Italian (IT) news of the same day. The distinc-
tion was binary: Related (+) or Not (so) related (?). 
7 Conclusion and future work 
We have shown that our system can rather accu-
rately identify clusters of major news per day in 
five languages and that it can link these clusters to 
related news over time (topic tracking). The most 
interesting and novel feature of the system is, how-
ever, that it can also identify related news across 
languages, without translating articles or using bi-
lingual dictionaries. This cross-lingual cluster simi-
larity is achieved by a combination of three feature 
sets, which currently have an impact of 50%, 30% 
and 20%, respectively: the main feature set is the 
mapping onto the multilingual classification 
scheme Eurovoc; the others are the countries re-
ferred to in the articles (direct mention of the coun-
try, or of a smaller place name of that country) and 
the cognates (same strings used in the articles 
across languages, i.e. mainly named entities). The 
evaluation has shown that the results are good, but 
that the cross-lingual linking performs less well 
than the monolingual historical linking of related 
news clusters. Users felt that the system performs 
well enough for it to go online soon, for usage by a 
large user community of several thousand people. 
Improvements to the system will nevertheless be 
sought.  
Future work will include testing different set-
tings concerning the relative impact of the three 
components, as well as detecting and using more 
named entities such as absolute and relative date 
expressions, proper names, etc. A further aim is to 
extend the system to another six languages. 
The usage of cognate similarity could be im-
proved. Currently it will not work with Greek, for 
instance, except for a few proper names. We would 
therefore like to experiment with multi-lingual 
stemming methods to exploit the existence of simi-
lar words across languages such as English ele-
phant, French ?l?phant, Spanish and Italian ele-
fante and German Elefant.  
Several customer groups requested an advanced 
news analysis that distinguishes between articles 
about concrete events and articles commenting 
about these events. We will explore this issue, but 
it is very likely that this distinction will require a 
syntactic analysis of the news and cannot be made 
with our bag-of-words approach. 
Finally, we intend to work on breaking news de-
tection, i.e. detecting new events, as opposed to 
detecting major news. This work will require 
working on smaller time windows than the current 
24-hour window.  
8 Acknowledgements 
We would like to thank the Web Technology group 
of the Joint Research Centre for their collaboration 
and for giving us access to their valuable multilin-
gual news collection. Our special thanks goes to 
Clive Best, Erik van der Goot, Ken Blackler and 
Teofilo Garcia. We would also like to thank our 
former colleague Johan Hagman for introducing us 
to the methods and usefulness of cluster analysis.  
References  
Allan James, Ron Papka & Victor Lavrenko 
(1998). On-line New Event Detection and Track-
ing. Proceedings of the 21st Annual International 
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 37-
45. Melbourne, Australia 
Best Clive, Erik van der Goot, Monica de Paola, 
Teofilo Garcia & David Horby (2002). Europe 
Media Monitor ? EMM. JRC Technical Note No. 
I.02.88. Ispra, Italy.  
Friburger N. & D. Maurel (2002). Textual Similar-
ity Based on Proper Names. Proceedings of the 
workshop Mathematical/Formal Methods in In-
formation Retrieval (MFIR?2002) at the 25th 
ACM SIGIR Conference, pp. 155-167. Tampere, 
Finland. 
Gey Frederic (2000). Research to Improve Cross-
Language Retrieval ? Position Paper for CLEF. 
In C. Peters (ed.): Cross-Language Information 
Retrieval and Evaluation, Workshop of Cross-
Language Evaluation Forum (CLEF?2000), Lis-
bon, Portugal. Lecture Notes in Computer Sci-
ence 2069, Springer. 
Hyland R., C. Clifton & R. Holland (1999). Geo-
NODE: Visualizing News in Geospatial Context. 
In Afca99. 
Jain A., M. Murty & P. Flynn (1999). Data cluster-
ing: a review. Pages 264 
Joachims Thorsten (2003). Representing and Ac-
cessing Digital Information. Available at http:// 
www.cs.cornell.edu/Courses/cs630/2003fa/lectur
es/tclust.pdf 
Kilgarriff A. (1996) Which words are particularly 
characteristic of a text? A survey of statistical 
approaches. Proceedings of the AISB Workshop 
on Language Engineering for Document Analy-
sis and Recognition. Sussex, 04/1996, pp. 33-40.  
Leek Tim, Hubert Jin, Sreenivasa Sista & Richard 
Schwartz (1999). The BBN Crosslingual Topic 
Detection and Tracking System. In 1999 TDT 
Evaluation System Summary Papers. 
http://www.nist.gov/speech/tests/tdt/tdt99/papers 
Pouliquen Bruno, Ralf Steinberger & Camelia Ig-
nat (2003). Automatic identification of document 
translations in large multilingual document col-
lections. Proceedings of the International Con-
ference Recent Advances in Natural Language 
Processing (RANLP'2003), pp. 401-408. Borov-
ets, Bulgaria, 10 - 12 September 2003. 
Pouliquen Bruno, Ralf Steinberger, Camelia Ignat 
& Tom de Groeve (2004). Geographical Infor-
mation Recognition and Visualisation in Texts 
Written in Various Languages. Proceedings of 
the 2004 ACM Symposium on Applied Comput-
ing, Session on Information Access and Retrieval 
(Nicosia, Cyprus), Volume 2 of 2, pages 1051-
1058. New York.  
Schultz J. Michael & Mark Liberman (1999). 
Topic detection and Tracking using idf-weighted 
Cosine Coefficient. DARPA Broadcast News 
Workshop Proceedings.  
Wactlar H.D. (1999). New Directions in Video In-
formation Extraction and Summarization. In 
Proceedings of the 10th DELOS Workshop, Sa-
norini, Greece, 24-25 June 1999. 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 145?148
Manchester, August 2008
Online-Monitoring of Security-Related Events
Martin Atkinson, Jakub Piskorski, Bruno Pouliquen
Ralf Steinberger, Hristo Tanev, Vanni Zavarella
Joint Research Centre of the European Commission
Institute for the Protection and Security of the Citizen
Via Fermi 2749, 21027 Ispra (VA), Italy
firstname.lastname@jrc.it
Abstract
This paper presents a fully operational
real-time event extraction system which is
capable of accurately and efficiently ex-
tracting violent and natural disaster events
from vast amount of online news articles
per day in different languages. Due to the
requirement that the system must be mul-
tilingual and easily extendable, it is based
on a shallow linguistic analysis. The event
extraction results can be viewed on a pub-
licly accessible website.
1 Introduction
Gathering information about violent and natural
disaster events from online news is of paramount
importance to better understand conflicts and to
develop global monitoring systems for the auto-
matic detection of precursors for threats in the
fields of conflict and health. This paper reports
on a fully operational live event extraction system
to detect information on violent events and natural
disasters in large multilingual collections of online
news articles collected by the news aggregation
system Europe Media Monitor (Best et al, 2005),
http://press.jrc.it/overview.html.
Although a considerable amount of work on the
automatic extraction of events has been reported,
it still appears to be a lesser studied area in com-
parison to the somewhat easier tasks of named-
entity and relation extraction. Two comprehensive
examples of the current functionality and capabil-
ities of event extraction technology dealing with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the identification of disease outbreaks and con-
flict incidents are given in (Grishman et al, 2002)
and (King and Lowe, 2003) respectively. The most
recent trends and developments in this area are re-
ported in (Ashish et al, 2006)
In order to be capable of processing vast
amounts of textual data in real time (as in the case
of EMM)we follow a linguistically lightweight ap-
proach and exploit clustered news at various pro-
cessing stages (pattern learning, information fu-
sion, geo-tagging, etc.). Consequently, only a tiny
fraction of each text is analysed. In a nutshell, our
system deploys simple 1 and 2-slot extraction pat-
terns to identify event-relevant entities. These pat-
terns are semi-automatically acquired in a boot-
strapping manner by using clustered news data.
Next, information about events scattered over dif-
ferent documents is integrated by applying voting
heuristics. The results of the core event extraction
system are integrated into a real-world global mon-
itoring system. Although we mainly cover the se-
curity domain, the techniques deployed in our sys-
tem can be applied to other domains, such as for
instance tracking business-related events for risk
assessment.
In the remaining part of this paper we give a
brief overview of the real-time event extraction
processing chain and describe the particularities of
selected subcomponents. Finally, the online appli-
cation is presented.
2 Real-time Event Extraction Process
The real-time event extraction processing chain is
depicted in Figure 1. First, news articles are gath-
ered by dedicated software for electronic media
monitoring, namely the EMM system (Best et al,
2005). EMM receives an average of 50,000 news
articles per day from about 1,500 news sources in
145
over 40 languages, and regularly checks for up-
dates of news. Secondly, the input data is grouped
into news clusters ideally including documents
on one topic or event. Then, clusters describing
security-related events are selected using keyword-
based heuristics. For each such cluster, the system
tries to detect and extract only the main event by
analysing all documents in the cluster.
EMM
News
Clustering / 
Geo Tag
Text Pre-
Processing 
Pattern 
Matching
Information 
Aggregation 
Events
NEXUS
Figure 1: Real-time processing chain.
Next, each cluster is processed by our core event
extraction engine. For each detected violent event,
it produces a frame, whose main slots are: date and
location, number of killed, injured or kidnapped
people, actors, type of event, weapons used, etc.
In an initial step, each document in the cluster
is linguistically pre-processed in order to produce
a more abstract representation of the texts. This
encompasses: fine-grained tokenisation, sentence
splitting, matching of known named entities, la-
belling of key terms and phrases like action words
(e.g. kill, shoot) and person groups.
Once texts are grouped into clusters and lin-
guistically pre-processed, the pattern engine ap-
plies a cascade of extraction grammars (consisting
of 1 and 2-slot extraction patterns) on each docu-
ment within a cluster. For creating extraction pat-
terns, we apply a blend of machine learning and
knowledge-based techniques. The extraction pat-
terns are matched against the first sentence and the
title of each article from the cluster. By processing
only the top sentence and the title, the system is
more likely to capture facts about the most impor-
tant event in the cluster. Even if we fail to detect
a single piece of information in one document in a
cluster, the same information is likely to be found
in another document of the cluster, where it may
be expressed in a different way.
Finally, since information about events is scat-
tered over different articles, the last step con-
sists of cross-document cluster-level information
fusion, i.e., we aggregate and validate information
extracted locally from each single article in the
same cluster. For this purpose, simple voting-like
heuristics are deployed.
Every ten minutes, EMM clusters the articles
found during the last four hours. The event extrac-
tion engine analyses each of these clusters. The
event information is thus always up-to-date. The
output of the event extraction engine constitutes
the input for a global monitoring system.
3 Geo-tagging Clusters
Challenges for geo-tagging clusters are that place
names can be homographic with person names and
with other place names. We solve the former am-
biguity by first identifying person names found
in our automatically populated database of known
people and organisations. For the latter ambiguity,
we adopted a cluster-centric approach by weight-
ing all place names found in a cluster and by select-
ing the one with the highest score. For each cluster,
we thus first establish all possible candidate loca-
tions by looking up in the texts all place, province,
region and country names found in a multilingual
gazetteer (including name variants). The weights
of the locations are then based on the place name
significance (e.g., a capital city scores higher than
a village) and on the place name hierarchy (i.e. if
the province or region to which the place belongs
are also mentioned in the text, it scores higher).
4 Pattern Acquisition
For pattern acquisition, we deploy a weakly super-
vised bootstrapping algorithm (Tanev and Oezden-
Wennerberg, 2008) similar in spirit to the one de-
scribed in (Yangarber, 2003), which involves some
manual validation. Contrary to other approaches,
the learning phase exploits the knowledge to which
cluster the news items belong. Intuitively, this
guarantees better precision of the learned patterns.
In particular, for each event-specific semantic role
(e.g. killed), a separate cycle of learning iterations
is executed (usually up to three) in order to learn
1-slot extraction patterns. Each cluster includes ar-
ticles from different sources about the same news
story. Therefore, we assume that each entity ap-
pears in the same semantic role (actor, victim, in-
jured) in the context of one cluster. An auto-
matic procedure for syntactic expansion comple-
ments the learning. This procedure accepts a man-
ually provided list of words which have identical
(or similar) syntactic usage patterns (e.g. killed,
assassinated, murdered, etc.). It then generates
new patterns from the old ones by substituting for
each other the words in the list. After 1-slot pat-
terns are acquired, some of them are used to man-
ually create 2-slot patterns like X shot Y.
146
5 Pattern matching engine
In order to guarantee that massive amounts of tex-
tual data can be processed in real time, we have
developed ExPRESS (Piskorski, 2007), an effi-
cient extraction pattern engine, which is capable of
matching thousands of patterns against MB-sized
texts within seconds. The pattern specification lan-
guage is a blend of two previously introduced IE-
oriented grammar formalisms, namely JAPE used
in GATE (Cunningham et al, 2000) and XTDL,
used in SPROUT (Dro?zd?zy?nski et al, 2004).
A single pattern is a regular expression over flat
feature structures (FS), i.e., non-recursive typed
feature structures without structure sharing, where
features are string-valued and ? unlike in XTDL
types ? are not organised in a hierarchy. Each such
regular expression is associated with a list of FSs
which constitute the output specification. Like in
XTDL, we deploy variables and functional oper-
ators for forming slot values and for establishing
contact with the ?outer world?. Further, we adapted
JAPEs feature of associating patterns with mul-
tiple actions, i.e., producing multiple annotations
(possibly nested). An empirical comparison of the
run-time behaviour of the new formalism against
the other 2 revealed that significant speed-ups can
be achieved (at least 30 times faster). ExPRESS
comes with a pool of highly efficient core linguis-
tic processing resources (Piskorski, 2008).
6 Information Aggregation
Once single pieces of information are extracted by
the pattern engine, they are merged into event de-
scriptions by applying an information aggregation
algorithm. This algorithm assumes that each clus-
ter reports at most one main event of interest. It
takes as input the text entities extracted from one
news cluster with their semantic roles and consid-
ers the sentences from which these entities are ex-
tracted. If one and the same entity has two roles as-
signed, a preference is given to the role assigned by
the most reliable group of patterns (e.g., 2-slot pat-
terns are more reliable). Another ambiguity which
has to be resolved arises from the contradictory in-
formation which news sources give about the num-
ber of victims. We use an ad-hoc heuristic for
computing the most probable estimation for these
numbers, i.e., firstly the largest group of numbers
which are close to each other is selected and sec-
ondly the number closest to the average in that
group is chosen. After this estimation is com-
puted, the system discards from each news clus-
ter all the articles whose reported victim numbers
significantly differ from the estimated numbers for
the whole cluster. Additionally, some victim arith-
metic is applied, i.e., a small taxonomy of person
classes is used to sum victim numbers (e.g., gun-
men and terrorists belong to the same class ofNon-
GovernmentalArmedGroup).
7 Event Classification
After the single pieces of information are assem-
bled into the event description, an event classifica-
tion is performed. Some of the most used event
classes are Terrorist Attack, Bombing, Shooting,
Air Attack, etc. The classification algorithm uses
a blend of keyword matching and domain spe-
cific rules. As an example, consider the following
domain-specific rule: if the event description in-
cludes named entities, which are assigned the se-
mantic role kidnapped, as well as entities which
are assigned the semantic role released, then the
type of the event is Hostage Release, rather than
Kidnapping. If the event refers to kidnapped peo-
ple and at the same time the news articles contain
words like video or videotape, then the event type
is Hostage Video Release. The second rule has a
higher priority, therefore it impedes the Hostage
Release rule to fire erroneously, when the release
of a hostage video is reported.
8 Monitoring Events
The core event extraction engine for English is
fully operational since December 2007. There are
two online applications running on top of it which
allow monitoring events. The first one is a dedi-
cated webpage using the Google Maps JavaScript
API (see Figure 2). It is publicly accessible at:
http://press.jrc.it/geo?type=event
&format=html&language=en and provides
an instant overview of what is occurring where in
the world. A small problem with this application
is that it overlays and hides events that are close to
each other.
The second application shows the same events
using the Google Earth client application. The
geo-located data is transmitted via the Keyhole
Markup Language (KML) format
1
supported di-
rectly by Google Earth.
2
The application is re-
1
http://code.google.com/apis/kml/documentation/
2
In order to run it, start Google Earth with KML:
http://press.jrc.it/geo?type=event&format=kml&language=en
147
Figure 2: Event visualisation with Google Maps
stricted to displaying at most half the globe, but
it allows expanding overlaid events.
Since it is important for stakeholders to be
quickly and efficiently informed about the type and
gravity of the event, various icons are used to rep-
resent the type or group of events visually (see Fig-
ure 3). We use general forms of icons for violent
events and specific forms of icons for natural and
man-made disasters. For violent events, the gen-
eral form represents the major consequence of the
event, except for kidnappings, where specific icons
are used. Independently of the type of event, all
icons are sized according to the damage caused,
i.e. it is dependent on the number of victims in-
volved in the event. Also, to highlight the events
with a more significant damage, a border is drawn
around the icon to indicate that a threshold of peo-
ple involved has been passed.
The online demo is available for English, Italian
and French. We are currently working on adapt-
ing the event extraction engine to other languages,
including Russian, Spanish, Polish, German and
Arabic. A more thorough description of the sys-
tem can be found in (Tanev et al, 2008; Piskorski
et al, 2008).
References
Ashish, N., D. Appelt, D. Freitag, and D. Zelenko. 2006.
Proceedings of the workshop on Event Extraction and Syn-
thesis, held in conjunction with the AAAI 2006 conference.
Menlo Park, California, USA.
Best, C., E. van der Goot, K. Blackler, T. Garcia, and
D. Horby. 2005. Europe Media Monitor. Technical Re-
port EUR 22173 EN, European Commission.
Cunningham, H., D. Maynard, and V. Tablan. 2000. JAPE: a
Java Annotation Patterns Engine (Second Edition). Tech-
nical Report, CS?00?10, University of Sheffield, Depart-
ment of Computer Science.
?
Kidnap
K
A
Arrest
R
Release
V
Video
V
Man
Made
?Violent EventUndefined Violent EventKilled Violent EventInjured Violent EventKindnapped Violent EventArrest Hostage Release VideoRelease Violent EventNo Consequneces
Man Made
Disaster
Man Made 
Fire
Man Made
Explosion
ND
!
Natural
Dister
Volcanic 
Eruption
Tsunami Earthquake Landslide
?
Avalanche Tropical
Storm
Lightning
Strike
Storm
Snow
Storm
Flood Wild Fire
Heatwave
Key to Symbols
Consequence Significance (number of people involved)
No Circle  = up to 10 Red Circle = More than 100Yellow Circle= between 10 and 100
Humanitarian
Crisis
Trial
Unclassified
Figure 3: Key to event type icons and magnitude
indicators
Dro?zd?zy?nski, W., H.-U. Krieger, J. Piskorski, U. Sch?afer,
and F. Xu. 2004. Shallow Processing with Unification
and Typed Feature Structures ? Foundations and Appli-
cations. K?unstliche Intelligenz, 2004(1):17?23.
Grishman, R., S. Huttunen, and R. Yangarber. 2002. Real-
time Event Extraction for Infectious Disease Outbreaks.
Proceedings of the Human Language Technology Confer-
ence (HLT) 2002.
King, G. and W. Lowe. 2003. An Automated Information
Extraction Tool for International Conflict Data with Per-
formance as Good as Human Coders: A Rare Events Eval-
uation Design. International Organization, 57:617?642.
Piskorski, J., H. Tanev, M. Atkinson, and E. Van der Goot.
2008. Cluster-centric Approach to News Event Extraction.
In Proceedings of MISSI 2008, Wroclaw, Poland.
Piskorski, J. 2007. ExPRESS Extraction Pattern Recogni-
tion Engine and Specification Suite. In Proceedings of the
International Workshop Finite-State Methods and Natu-
ral language Processing 2007 (FSMNLP?2007), Potsdam,
Germany.
Piskorski, J. 2008. CORLEONE ? Core Linguistic Entity
Online Extraction. Technical report 23393 EN, Joint Re-
search Centre of the European Commission, Ispra, Italy.
Tanev, H. and P. Oezden-Wennerberg. 2008. Learning to
Populate an Ontology of Violent Events (in print). In
Fogelman-Soulie, F. and Perrotta, D. and Piskorski, J. and
Steinberger, R., editor, NATO Security through Science Se-
ries: Information and Communication Security. IOS Press.
Tanev, H., J. Piskorski, and M. Atkinson. 2008. Real-
Time News Event Extraction for Global Crisis Monitor-
ing. In Proceedings of the 13
th
International Conference
on Applications of Natural Language to Information Sys-
tems (NLDB 2008, Lecture Notes in Computer Science Vol.
5039), pages 207?218. Springer-Verlag Berlin Heidelberg.
Yangarber, R. 2003. Counter-Training in Discovery of Se-
mantic Patterns. In Proceedings of the 41
st
Annual Meet-
ing of the ACL.
148
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 25?30,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
ONTS: ?Optima? News Translation System
Marco Turchi?, Martin Atkinson?, Alastair Wilcox+, Brett Crawley,
Stefano Bucci+, Ralf Steinberger? and Erik Van der Goot?
European Commission - Joint Research Centre (JRC), IPSC - GlobeSec
Via Fermi 2749, 21020 Ispra (VA) - Italy
?[name].[surname]@jrc.ec.europa.eu
+[name].[surname]@ext.jrc.ec.europa.eu
brettcrawley@gmail.com
Abstract
We propose a real-time machine translation
system that allows users to select a news
category and to translate the related live
news articles from Arabic, Czech, Danish,
Farsi, French, German, Italian, Polish, Por-
tuguese, Spanish and Turkish into English.
The Moses-based system was optimised for
the news domain and differs from other
available systems in four ways: (1) News
items are automatically categorised on the
source side, before translation; (2) Named
entity translation is optimised by recog-
nising and extracting them on the source
side and by re-inserting their translation in
the target language, making use of a sep-
arate entity repository; (3) News titles are
translated with a separate translation sys-
tem which is optimised for the specific style
of news titles; (4) The system was opti-
mised for speed in order to cope with the
large volume of daily news articles.
1 Introduction
Being able to read news from other countries and
written in other languages allows readers to be
better informed. It allows them to detect national
news bias and thus improves transparency and
democracy. Existing online translation systems
such as Google Translate and Bing Translator1
are thus a great service, but the number of docu-
ments that can be submitted is restricted (Google
will even entirely stop their service in 2012) and
submitting documents means disclosing the users?
interests and their (possibly sensitive) data to the
service-providing company.
1http://translate.google.com/ and http:
//www.microsofttranslator.com/
For these reasons, we have developed our
in-house machine translation system ONTS. Its
translation results will be publicly accessible as
part of the Europe Media Monitor family of ap-
plications, (Steinberger et al 2009), which gather
and process about 100,000 news articles per day
in about fifty languages. ONTS is based on
the open source phrase-based statistical machine
translation toolkit Moses (Koehn et al 2007),
trained mostly on freely available parallel cor-
pora and optimised for the news domain, as stated
above. The main objective of developing our in-
house system is thus not to improve translation
quality over the existing services (this would be
beyond our possibilities), but to offer our users a
rough translation (a ?gist?) that allows them to get
an idea of the main contents of the article and to
determine whether the news item at hand is rele-
vant for their field of interest or not.
A similar news-focused translation service is
?Found in Translation? (Turchi et al 2009),
which gathers articles in 23 languages and trans-
lates them into English. ?Found in Translation? is
also based on Moses, but it categorises the news
after translation and the translation process is not
optimised for the news domain.
2 Europe Media Monitor
Europe Media Monitor (EMM)2 gathers a daily
average of 100,000 news articles in approximately
50 languages, from about 3,400 hand-selected
web news sources, from a couple of hundred spe-
cialist and government websites, as well as from
about twenty commercial news providers. It vis-
its the news web sites up to every five minutes to
2http://emm.newsbrief.eu/overview.html
25
search for the latest articles. When news sites of-
fer RSS feeds, it makes use of these, otherwise
it extracts the news text from the often complex
HTML pages. All news items are converted to
Unicode. They are processed in a pipeline struc-
ture, where each module adds additional informa-
tion. Independently of how files are written, the
system uses UTF-8-encoded RSS format.
Inside the pipeline, different algorithms are im-
plemented to produce monolingual and multilin-
gual clusters and to extract various types of in-
formation such as named entities, quotations, cat-
egories and more. ONTS uses two modules of
EMM: the named entity recognition and the cate-
gorization parts.
2.1 Named Entity Recognition and Variant
Matching.
Named Entity Recognition (NER) is per-
formed using manually constructed language-
independent rules that make use of language-
specific lists of trigger words such as titles
(president), professions or occupations (tennis
player, playboy), references to countries, regions,
ethnic or religious groups (French, Bavarian,
Berber, Muslim), age expressions (57-year-old),
verbal phrases (deceased), modifiers (former)
and more. These patterns can also occur in
combination and patterns can be nested to capture
more complex titles, (Steinberger and Pouliquen,
2007). In order to be able to cover many different
languages, no other dictionaries and no parsers or
part-of-speech taggers are used.
To identify which of the names newly found
every day are new entities and which ones are
merely variant spellings of entities already con-
tained in the database, we apply a language-
independent name similarity measure to decide
which name variants should be automatically
merged, for details see (Pouliquen and Stein-
berger, 2009). This allows us to maintain a
database containing over 1,15 million named en-
tities and 200,000 variants. The major part of
this resource can be downloaded from http:
//langtech.jrc.it/JRC-Names.html
2.2 Category Classification across
Languages.
All news items are categorized into hundreds of
categories. Category definitions are multilingual,
created by humans and they include geographic
regions such as each country of the world, organi-
zations, themes such as natural disasters or secu-
rity, and more specific classes such as earthquake,
terrorism or tuberculosis,
Articles fall into a given category if they sat-
isfy the category definition, which consists of
Boolean operators with optional vicinity opera-
tors and wild cards. Alternatively, cumulative
positive or negative weights and a threshold can
be used. Uppercase letters in the category defi-
nition only match uppercase words, while lower-
case words in the definition match both uppercase
and lowercase words. Many categories are de-
fined with input from the users themselves. This
method to categorize the articles is rather sim-
ple and user-friendly, and it lends itself to dealing
with many languages, (Steinberger et al 2009).
3 News Translation System
In this section, we describe our statistical machine
translation (SMT) service based on the open-
source toolkit Moses (Koehn et al 2007) and its
adaptation to translation of news items.
Which is the most suitable SMT system for
our requirements? The main goal of our system
is to help the user understand the content of an ar-
ticle. This means that a translated article is evalu-
ated positively even if it is not perfect in the target
language. Dealing with such a large number of
source languages and articles per day, our system
should take into account the translation speed, and
try to avoid using language-dependent tools such
as part-of-speech taggers.
Inside the Moses toolkit, three different
statistical approaches have been implemented:
phrase based statistical machine translation (PB-
SMT) (Koehn et al 2003), hierarchical phrase
based statistical machine translation (Chiang,
2007) and syntax-based statistical machine trans-
lation (Marcu et al 2006). To identify the
most suitable system for our requirements, we
run a set of experiments training the three mod-
els with Europarl V4 German-English (Koehn,
2005) and optimizing and testing on the News
corpus (Callison-Burch et al 2009). For all of
them, we use their default configurations and they
are run under the same condition on the same ma-
chine to better evaluate translation time. For the
syntax model we use linguistic information only
on the target side. According to our experiments,
in terms of performance the hierarchical model
26
performs better than PBSMT and syntax (18.31,
18.09, 17.62 Bleu points), but in terms of transla-
tion speed PBSMT is better than hierarchical and
syntax (1.02, 4.5, 49 second per sentence). Al-
though, the hierarchical model has the best Bleu
score, we prefer to use the PBSMT system in our
translation service, because it is four times faster.
Which training data can we use? It is known
in statistical machine translation that more train-
ing data implies better translation. Although, the
number of parallel corpora has been is growing
in the last years, the amounts of training data
vary from language pair to language pair. To
train our models we use the freely available cor-
pora (when possible): Europarl (Koehn, 2005),
JRC-Acquis (Steinberger et al 2006), DGT-
TM3, Opus (Tiedemann, 2009), SE-Times (Ty-
ers and Alperen, 2010), Tehran English-Persian
Parallel Corpus (Pilevar et al 2011), News
Corpus (Callison-Burch et al 2009), UN Cor-
pus (Rafalovitch and Dale, 2009), CzEng0.9 (Bo-
jar and Z?abokrtsky?, 2009), English-Persian paral-
lel corpus distributed by ELRA4 and two Arabic-
English datasets distributed by LDC5. This re-
sults in some language pairs with a large cover-
age, (more than 4 million sentences), and other
with a very small coverage, (less than 1 million).
The language models are trained using 12 model
sentences for the content model and 4.7 million
for the title model. Both sets are extracted from
English news.
For less resourced languages such as Farsi and
Turkish, we tried to extend the available corpora.
For Farsi, we applied the methodology proposed
by (Lambert et al 2011), where we used a large
language model and an English-Farsi SMT model
to produce new sentence pairs. For Turkish we
added the Movie Subtitles corpus (Tiedemann,
2009), which allowed the SMT system to in-
crease its translation capability, but included sev-
eral slang words and spoken phrases.
How to deal with Named Entities in transla-
tion? News articles are related to the most impor-
tant events. These names need to be efficiently
translated to correctly understand the content of
an article. From an SMT point of view, two main
issues are related to Named Entity translation: (1)
such a name is not in the training data or (2) part
3http://langtech.jrc.it/DGT-TM.html
4http://catalog.elra.info/
5http://www.ldc.upenn.edu/
of the name is a common word in the target lan-
guage and it is wrongly translated, e.g. the French
name ?Bruno Le Maire? which risks to be trans-
lated into English as ?Bruno Mayor?. To mitigate
both the effects we use our multilingual named
entity database. In the source language, each news
item is analysed to identify possible entities; if
an entity is recognised, its correct translation into
English is retrieved from the database, and sug-
gested to the SMT system enriching the source
sentence using the xml markup option 6 in Moses.
This approach allows us to complement the train-
ing data increasing the translation capability of
our system.
How to deal with different language styles
in the news? News title writing style contains
more gerund verbs, no or few linking verbs,
prepositions and adverbs than normal sentences,
while content sentences include more preposi-
tion, adverbs and different verbal tenses. Starting
from this assumption, we investigated if this phe-
nomenon can affect the translation performance
of our system.
We trained two SMT systems, SMTcontent
and SMTtitle, using the Europarl V4 German-
English data as training corpus, and two dif-
ferent development sets: one made of content
sentences, News Commentaries (Callison-Burch
et al 2009), and the other made of news ti-
tles in the source language which were trans-
lated into English using a commercial transla-
tion system. With the same strategy we gener-
ated also a Title test set. The SMTtitle used a
language model created using only English news
titles. The News and Title test sets were trans-
lated by both the systems. Although the perfor-
mance obtained translating the News and Title
corpora are not comparable, we were interested
in analysing how the same test set is translated
by the two systems. We noticed that translat-
ing a test set with a system that was optimized
with the same type of data resulted in almost 2
Blue score improvements: Title-TestSet: 0.3706
(SMTtitle), 0.3511 (SMTcontent); News-TestSet:
0.1768 (SMTtitle), 0.1945 (SMTcontent). This
behaviour was present also in different language
pairs. According to these results we decided
to use two different translation systems for each
language pair, one optimized using title data
6http://www.statmt.org/moses/?n=Moses.
AdvancedFeatures#ntoc4
27
and the other using normal content sentences.
Even though this implementation choice requires
more computational power to run in memory two
Moses servers, it allows us to mitigate the work-
load of each single instance reducing translation
time of each single article and to improve transla-
tion quality.
3.1 Translation Quality
To evaluate the translation performance of ONTS,
we run a set of experiments where we translate a
test set for each language pair using our system
and Google Translate. Lack of human translated
parallel titles obliges us to test only the content
based model. For German, Spanish and Czech we
use the news test sets proposed in (Callison-Burch
et al 2010), for French and Italian the news test
sets presented in (Callison-Burch et al 2008),
for Arabic, Farsi and Turkish, sets of 2,000 news
sentences extracted from the Arabic-English and
English-Persian datasets and the SE-Times cor-
pus. For the other languages we use 2,000 sen-
tences which are not news but a mixture of JRC-
Acquis, Europarl and DGT-TM data. It is not
guarantee that our test sets are not part of the train-
ing data of Google Translate.
Each test set is translated by Google Translate
- Translator Toolkit, and by our system. Bleu
score is used to evaluate the performance of both
systems. Results, see Table 1, show that Google
Translate produces better translation for those lan-
guages for which large amounts of data are avail-
able such as French, German, Italian and Spanish.
Surprisingly, for Danish, Portuguese and Polish,
ONTS has better performance, this depends on
the choice of the test sets which are not made of
news data but of data that is fairly homogeneous
in terms of style and genre with the training sets.
The impact of the named entity module is ev-
ident for Arabic and Farsi, where each English
suggested entity results in a larger coverage of
the source language and better translations. For
highly inflected and agglutinative languages such
as Turkish, the output proposed by ONTS is poor.
We are working on gathering more training data
coming from the news domain and on the pos-
sibility of applying a linguistic pre-processing of
the documents.
Source L. ONTS Google T.
Arabic 0.318 0.255
Czech 0.218 0.226
Danish 0.324 0.296
Farsi 0.245 0.197
French 0.26 0.286
German 0.205 0.25
Italian 0.234 0.31
Polish 0.568 0.511
Portuguese 0.579 0.424
Spanish 0.283 0.334
Turkish 0.238 0.395
Table 1: Automatic evaluation.
4 Technical Implementation
The translation service is made of two compo-
nents: the connection module and the Moses
server. The connection module is a servlet im-
plemented in Java. It receives the RSS files,
isolates each single news article, identifies each
source language and pre-processes it. Each news
item is split into sentences, each sentence is to-
kenized, lowercased, passed through a statisti-
cal compound word splitter, (Koehn and Knight,
2003), and the named entity annotator module.
For language modelling we use the KenLM im-
plementation, (Heafield, 2011).
According to the language, the correct Moses
servers, title and content, are fed in a multi-
thread manner. We use the multi-thread version
of Moses (Haddow, 2010). When all the sentences
of each article are translated, the inverse process
is run: they are detokenized, recased, and untrans-
lated/unknown words are listed. The translated ti-
tle and content of each article are uploaded into
the RSS file and it is passed to the next modules.
The full system including the translation mod-
ules is running in a 2xQuad-Core with In-
tel Hyper-threading Technology processors with
48GB of memory. It is our intention to locate
the Moses servers on different machines. This is
possible thanks to the high modularity and cus-
tomization of the connection module. At the mo-
ment, the translation models are available for the
following source languages: Arabic, Czech, Dan-
ish, Farsi, French, German, Italian, Polish, Por-
tuguese, Spanish and Turkish.
28
Figure 1: Demo Web site.
4.1 Demo
Our translation service is currently presented on
a demo web site, see Figure 1, which is available
at http://optima.jrc.it/Translate/.
News articles can be retrieved selecting one of the
topics and the language. All the topics are as-
signed to each article using the methodology de-
scribed in 2.2. These articles are shown in the left
column of the interface. When the button ?Trans-
late? is pressed, the translation process starts and
the translated articles appear in the right column
of the page.
The translation system can be customized from
the interface enabling or disabling the named
entity, compound, recaser, detokenizer and un-
known word modules. Each translated article is
enriched showing the translation time in millisec-
onds per character and, if enabled, the list of un-
known words. The interface is linked to the con-
nection module and data is transferred using RSS
structure.
5 Discussion
In this paper we present the Optima News Trans-
lation System and how it is connected to Eu-
rope Media Monitor application. Different strate-
gies are applied to increase the translation perfor-
mance taking advantage of the document struc-
ture and other resources available in our research
group. We believe that the experiments described
in this work can result very useful for the develop-
ment of other similar systems. Translations pro-
duced by our system will soon be available as part
of the main EMM applications.
The performance of our system is encouraging,
but not as good as the performance of web ser-
vices such as Google Translate, mostly because
we use less training data and we have reduced
computational power. On the other hand, our in-
house system can be fed with a large number of
articles per day and sensitive data without includ-
ing third parties in the translation process. Per-
formance and translation time vary according to
the number and complexity of sentences and lan-
guage pairs.
The domain of news articles dynamically
changes according to the main events in the world,
while existing parallel data is static and usually
associated to governmental domains. It is our in-
tention to investigate how to adapt our translation
system updating the language model with the En-
glish articles of the day.
Acknowledgments
The authors thank the JRC?s OPTIMA team for
its support during the development of ONTS.
References
O. Bojar and Z. Z?abokrtsky?. 2009. CzEng0.9: Large
Parallel Treebank with Rich Annotation. Prague
Bulletin of Mathematical Linguistics, 92.
C. Callison-Burch and C. Fordyce and P. Koehn and
C. Monz and J. Schroeder. 2008. Further Meta-
Evaluation of Machine Translation. Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, pages 70?106. Columbus, US.
C. Callison-Burch, and P. Koehn and C. Monz and J.
Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 1?28. Athens, Greece.
C. Callison-Burch, and P. Koehn and C. Monz and K.
Peterson and M. Przybocki and O. Zaidan. 2009.
Findings of the 2010 Joint Workshop on Statisti-
cal Machine Translation and Metrics for Machine
Translation. Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 17?53. Uppsala, Sweden.
D. Chiang. 2005. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2): pages 201?
228. MIT Press.
B. Haddow. 2010. Adding multi-threaded decoding to
moses. The Prague Bulletin of Mathematical Lin-
guistics, 93(1): pages 57?66. Versita.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. Proceedings of the Sixth
Workshop on Statistical Machine Translation, Ed-
inburgh, UK.
29
P. Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of
the Machine Translation Summit X, pages 79-86.
Phuket, Thailand.
P. Koehn and F. J. Och and D. Marcu. 2003. Statistical
phrase-based translation. Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 48?54. Edmon-
ton, Canada.
P. Koehn and K. Knight. 2003. Empirical methods
for compound splitting. Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics, pages 187?193. Bu-
dapest, Hungary.
P. Koehn and H. Hoang and A. Birch and C. Callison-
Burch and M. Federico and N. Bertoldi and B.
Cowan and W. Shen and C. Moran and R. Zens
and C. Dyer and O. Bojar and A. Constantin and E.
Herbst 2007. Moses: Open source toolkit for sta-
tistical machine translation. Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, demonstration session, pages 177?180.
Columbus, Oh, USA.
P. Lambert and H. Schwenk and C. Servan and S.
Abdul-Rauf. 2011. SPMT: Investigations on Trans-
lation Model Adaptation Using Monolingual Data.
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 284?293. Edinburgh,
Scotland.
D. Marcu and W. Wang and A. Echihabi and K.
Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language phrases.
Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
48?54. Edmonton, Canada.
M. Pilevar and H. Faili and A. Pilevar. 2011. TEP:
Tehran English-Persian Parallel Corpus. Compu-
tational Linguistics and Intelligent Text Processing,
pages 68?79. Springer.
B. Pouliquen and R. Steinberger. 2009. Auto-
matic construction of multilingual name dictionar-
ies. Learning Machine Translation, pages 59?78.
MIT Press - Advances in Neural Information Pro-
cessing Systems Series (NIPS).
A. Rafalovitch and R. Dale. 2009. United nations
general assembly resolutions: A six-language par-
allel corpus. Proceedings of the MT Summit XIII,
pages 292?299. Ottawa, Canada.
R. Steinberger and B. Pouliquen. 2007. Cross-lingual
named entity recognition. Lingvistic? Investiga-
tiones, 30(1) pages 135?162. John Benjamins Pub-
lishing Company.
R. Steinberger and B. Pouliquen and A. Widiger and
C. Ignat and T. Erjavec and D. Tufis? and D. Varga.
2006. The JRC-Acquis: A multilingual aligned par-
allel corpus with 20+ languages. Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 2142?2147. Genova,
Italy.
R. Steinberger and B. Pouliquen and E. van der Goot.
2009. An Introduction to the Europe Media Monitor
Family of Applications. Proceedings of the Infor-
mation Access in a Multilingual World-Proceedings
of the SIGIR 2009 Workshop, pages 1?8. Boston,
USA.
J. Tiedemann. 2009. News from OPUS-A Collection
of Multilingual Parallel Corpora with Tools and
Interfaces. Recent advances in natural language
processing V: selected papers from RANLP 2007,
pages 309:237.
M. Turchi and I. Flaounas and O. Ali and T. DeBie
and T. Snowsill and N. Cristianini. 2009. Found in
translation. Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discov-
ery in Databases, pages 746?749. Bled, Slovenia.
F. Tyers and M.S. Alperen. 2010. South-East Euro-
pean Times: A parallel corpus of Balkan languages.
Proceedings of the LREC workshop on Exploita-
tion of multilingual resources and tools for Central
and (South) Eastern European Languages, Valletta,
Malta.
30
Proceedings of the ACL 2010 Conference Short Papers, pages 382?386,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Wrapping up a Summary:
from Representation to Generation
Josef Steinberger and Marco Turchi and
Mijail Kabadjov and Ralf Steinberger
EC Joint Research Centre
21027, Ispra (VA), Italy
{Josef.Steinberger, Marco.Turchi,
Mijail.Kabadjov, Ralf.Steinberger}
@jrc.ec.europa.eu
Nello Cristianini
University of Bristol,
Bristol, BS8 1UB, UK
nello@support-vector.net
Abstract
The main focus of this work is to investi-
gate robust ways for generating summaries
from summary representations without re-
curring to simple sentence extraction and
aiming at more human-like summaries.
This is motivated by empirical evidence
from TAC 2009 data showing that human
summaries contain on average more and
shorter sentences than the system sum-
maries. We report encouraging prelimi-
nary results comparable to those attained
by participating systems at TAC 2009.
1 Introduction
In this paper we adopt the general framework
for summarization put forward by Spa?rck-Jones
(1999) ? which views summarization as a three-
fold process: interpretation, transformation and
generation ? and attempt to provide a clean in-
stantiation for each processing phase, with a par-
ticular emphasis on the last, summary-generation
phase often omitted or over-simplified in the main-
stream work on summarization.
The advantages of looking at the summarization
problem in terms of distinct processing phases are
numerous. It not only serves as a common ground
for comparing different systems and understand-
ing better the underlying logic and assumptions,
but it also provides a neat framework for devel-
oping systems based on clean and extendable de-
signs. For instance, Gong and Liu (2002) pro-
posed a method based on Latent Semantic Anal-
ysis (LSA) and later J. Steinberger et al (2007)
showed that solely by enhancing the first source
interpretation phase, one is already able to pro-
duce better summaries.
There has been limited work on the last sum-
mary generation phase due to the fact that it is
unarguably a very challenging problem. The vast
amount of approaches assume simple sentence se-
lection, a type of extractive summarization, where
often the summary representation and the end
summary are, indeed, conflated.
The main focus of this work is, thus, to in-
vestigate robust ways for generating summaries
from summary representations without recurring
to simple sentence extraction and aiming at more
human-like summaries. This decision is also mo-
tivated by empirical evidence from TAC 2009 data
(see table 1) showing that human summaries con-
tain on average more and shorter sentences than
the system summaries. The intuition behind this is
that, by containing more sentences, a summary is
able to capture more of the important content from
the source.
Our initial experimental results show that our
approach is feasible, since it produces summaries,
which when evaluated against the TAC 2009 data1
yield ROUGE scores (Lin and Hovy, 2003) com-
parable to the participating systems in the Sum-
marization task at TAC 2009. Taking into account
that our approach is completely unsupervised and
language-independent, we find our preliminary re-
sults encouraging.
The remainder of the paper is organised as fol-
lows: in the next section we briefly survey the
related work, in ?3 we describe our approach to
summarization, in ?4 we explain how we tackle
the generation step, in ?5 we present and discuss
our experimental results and towards the end we
conclude and give pointers to future work.
2 Related Work
There is a large body of literature on summariza-
tion (Hovy, 2005; Erkan and Radev, 2004; Kupiec
et al, 1995). The most closely related work to the
approach presented hereby is work on summariza-
tion attempting to go beyond simple sentence ex-
1http://www.nist.gov/tac/
382
traction and to a lesser degree work on sentence
compression. We survey below work along these
lines.
Although our approach is related to sentence
compression (Knight and Marcu, 2002; Clarke
and Lapata, 2008), it is subtly different. Firstly, we
reduce the number of terms to be used in the sum-
mary at a global level, not at a local per-sentence
level. Secondly, we directly exploit the resulting
structures from the SVD making the last genera-
tion step fully aware of previous processing stages,
as opposed to tackling the problem of sentence
compression in isolation.
A similar approach to our sentence reconstruc-
tion method has been developed by Quirk et al
(2004) for paraphrase generation. In their work,
training and test sets contain sentence pairs that
are composed of two different proper English sen-
tences and a paraphrase of a source sentence is
generated by finding the optimal path through a
paraphrases lattice.
Finally, it is worth mentioning that we are aware
of the ?capsule overview? summaries proposed by
Boguraev and Kennedy (1997) which is similar to
our TSR (see below), however, as opposed to their
emphasis on a suitable browsing interface rather
than producing a readable summary, we precisely
attempt the latter.
3 Three-fold Summarization:
Interpretation, Transformation and
Generation
We chose the LSA paradigm for summarization,
since it provides a clear and direct instantiation of
Spa?rck-Jones? three-stage framework.
In LSA-based summarization the interpreta-
tion phase takes the form of building a term-by-
sentence matrix A = [A1, A2, . . . , An], where
each column Aj = [a1j , a2j , . . . , anj ]T represents
the weighted term-frequency vector of sentence j
in a given set of documents. We adopt the same
weighting scheme as the one described in (Stein-
berger et al, 2007), as well as their more general
definition of term entailing not only unigrams and
bigrams, but also named entities.
The transformation phase is done by applying
singular value decomposition (SVD) to the initial
term-by-sentence matrix defined as A = U?V T .
The generation phase is where our main contri-
bution comes in. At this point we depart from stan-
dard LSA-based approaches and aim at produc-
ing a succinct summary representation comprised
only of salient terms ? Term Summary Represen-
tation (TSR). Then this TSR is passed on to an-
other module which attempts to produce complete
sentences. The module for sentence reconstruc-
tion is described in detail in section 4, in what fol-
lows we explain the method for producing a TSR.
3.1 Term Summary Representation
To explain how a term summary representation
(TSR) is produced, we first need to define two con-
cepts: salience score of a given term and salience
threshold. Salience score for each term in matrix
A is given by the magnitude of the corresponding
vector in the matrix resulting from the dot product
of the matrix of left singular vectors with the diag-
onal matrix of singular values. More formally, let
T = U ? ? and then for each term i, the salience
score is given by |~Ti|. Salience threshold is equal
to the salience score of the top kth term, when all
terms are sorted in descending order on the basis
of their salience scores and a cutoff is defined as a
percentage (e.g., top 15%). In other words, if the
total number of terms is n, then 100?k/n must be
equal to the percentage cutoff specified.
The generation of a TSR is performed in two
steps. First, an initial pool of sentences is selected
by using the same technique as in (Steinberger and
Jez?ek, 2009) which exploits the dot product of the
diagonal matrix of singular values with the right
singular vectors: ? ? V T .2 This initial pool of sen-
tences is the output of standard LSA approaches.
Second, the terms from the source matrix A are
identified in the initial pool of sentences and those
terms whose salience score is above the salience
threshold are copied across to the TSR. Thus, the
TSR is formed by the most (globally) salient terms
from each one of the sentences. For example:
? Extracted Sentence: ?Irish Prime Minister Bertie
Ahern admitted on Tuesday that he had held a series of
private one-on-one meetings on the Northern Ireland
peace process with Sinn Fein leader Gerry Adams, but
denied they had been secret in any way.?
? TSR Sentence at 10%: ?Irish Prime Minister
Bertie Ahern Tuesday had held one-on-one meetings
Northern Ireland peace process Sinn Fein leader Gerry
Adams?3
2Due to space constraints, full details on that step are
omitted here, see (Steinberger and Jez?ek, 2009).
3The TSR sentence is stemmed just before feeding it to
the reconstruction module discussed in the next section.
383
Average Human System At 100% At 15% At 10% At 5% At 1%
number of: Summaries Summaries
Sentences/summary 6.17 3.82 3.8 3.95 4.39 5.18 12.58
Words/sentence 15.96 25.01 26.24 25.1 22.61 19.08 7.55
Words/summary 98.46 95.59 99.59 99.25 99.18 98.86 94.96
Table 1: Summary statistics on TAC?09 data (initial summaries).
Metric LSAextract At 100% At 15% At 10% At 5% At 1%
ROUGE-1 0.371 0.361 0.362 0.365 0.372 0.298
ROUGE-2 0.096 0.08 0.081 0.083 0.083 0.083
ROUGE-SU4 0.131 0.125 0.126 0.128 0.131 0.104
Table 2: Summarization results on TAC?09 data (initial summaries).
4 Noisy-channel model for sentence
reconstruction
This section describes a probabilistic approach to
the reconstruction problem. We adopt the noisy-
channel framework that has been widely used in a
number of other NLP applications. Our interpre-
tation of the noisy channel consists of looking at a
stemmed string without stopwords and imagining
that it was originally a long string and that some-
one removed or stemmed some text from it. In our
framework, reconstruction consists of identifying
the original long string.
To model our interpretation of the noisy chan-
nel, we make use of one of the most popular
classes of SMT systems: the Phrase Based Model
(PBM) (Zens et al, 2002; Och and Ney, 2001;
Koehn et al, 2003). It is an extension of the noisy-
channel model and was introduced by Brown et al
(1994), using phrases rather than words. In PBM,
a source sentence f is segmented into a sequence
of I phrases f I = [f1, f2, . . . fI ] and the same is
done for the target sentence e, where the notion of
phrase is not related to any grammatical assump-
tion; a phrase is an n-gram. The best translation
ebest of f is obtained by:
ebest = argmaxe p(e|f) = argmaxe
I?
i=1
?(fi|ei)
??
d(ai ? bi?1)
?d
|e|?
i=1
pLM (ei|e1 . . . ei?1)
?LM
where ?(fi|ei) is the probability of translating
a phrase ei into a phrase fi. d(ai ? bi?1) is
the distance-based reordering model that drives
the system to penalize substantial reorderings of
words during translation, while still allowing some
flexibility. In the reordering model, ai denotes the
start position of the source phrase that was trans-
lated into the ith target phrase, and bi?1 denotes
the end position of the source phrase translated
into the (i?1th) target phrase. pLM (ei|e1 . . . ei?1)
is the language model probability that is based on
the Markov chain assumption. It assigns a higher
probability to fluent/grammatical sentences. ??,
?LM and ?d are used to give a different weight to
each element (for more details see (Koehn et al,
2003)).
In our reconstruction problem, the difference
between the source and target sentences is not in
terms of languages, but in terms of forms. In fact,
our source sentence f is a stemmed sentence with-
out stopwords, while the target sentence e is a
complete English sentence. ?Translate? means to
reconstruct the most probable sentence e given f
inserting new words and reproducing the inflected
surface forms of the source words.
4.1 Training of the model
In Statistical Machine Translation, a PBM system
is trained using parallel sentences, where each sen-
tence in a language is paired with another sentence
in a different language and one is the translation of
the other.
In the reconstruction problem, we use a set, S1
of 2,487,414 English sentences extracted from the
news. This set is duplicated, S2, and for each sen-
tence in S2, stopwords are removed and the re-
maining words are stemmed using Porter?s stem-
mer (Porter, 1980). Our stopword list contains 488
words. Verbs are not included in this list, because
they are relevant for the reconstruction task. To
optimize the lambda parameters, we select 2,000
pairs as development set.
384
An example of training sentence pair is:
? Source Sentence: ?royal mail ha doubl profit 321
million huge fall number letter post?
? Target Sentence: ?royal mail has doubled its prof-
its to 321 million despite a huge fall in the number of
letters being posted?
In this work we use Moses (Koehn et al, 2007),
a complete phrase-based translation toolkit for
academic purposes. It provides all the state-of-the-
art components needed to create a phrase-based
machine translation system. It contains different
modules to preprocess data, train the Language
Models and the Translation Models.
5 Experimental Results
For our experiments we made use of the TAC
2009 data which conveniently contains human-
produced summaries against which we could eval-
uate the output of our system (NIST, 2009).
To begin our inquiry we carried out a phase
of exploratory data analysis, in which we mea-
sured the average number of sentences per sum-
mary, words per sentence and words per summary
in human vs. system summaries in the TAC 2009
data. Additionally, we also measured these statis-
tics of summaries produced by our system at five
different percentage cutoffs: 100%, 15%, 10%,
5% and 1%. 4 The results from this exploration
are summarised in table 1. The most notable thing
is that human summaries contain on average more
and shorter sentences than the system summaries
(see 2nd and 3rd column from left to right). Sec-
ondly, we note that as the percentage cutoff de-
creases (from 4th column rightwards) the charac-
teristics of the summaries produced by our system
are increasingly more similar to those of the hu-
man summaries. In other words, within the 100-
word window imposed by the TAC guidelines, our
system is able to fit more (and hence shorter) sen-
tences as we decrease the percentage cutoff.
Summarization performance results are shown
in table 2. We used the standard ROUGE evalu-
ation (Lin and Hovy, 2003) which has been also
used for TAC. We include the usual ROUGE met-
rics: R1 is the maximum number of co-occurring
unigrams, R2 is the maximum number of co-
occurring bigrams and RSU4 is the skip bigram
measure with the addition of unigrams as counting
4Recall from section ?3 that the salience threshold is a
function of the percentage cutoff.
unit. The last five columns of table 2 (from left to
right) correspond to summaries produced by our
system at various percentage cutoffs. The 2nd col-
umn, LSAextract, corresponds to the performance
of our system at producing summaries by sentence
extraction only.5
In the light of the above, the decrease in per-
formance from column LSAextract to column ?At
100%? can be regarded as reconstruction error.6
Then, as we decrease the percentage cutoff (from
4th column rightwards) we are increasingly cover-
ing more of the content comprised by the human
summaries (as far as the ROUGE metrics are able
to gauge this, of course). In other words, the im-
provement of content coverage makes up for the
reconstruction error, and at 5% cutoff we already
obtain ROUGE scores comparable to LSAextract.
This suggests that if we improve the quality of our
sentence reconstruction we would potentially end
up with a better performing system than a typical
LSA system based on sentence selection. Hence,
we find these results very encouraging.
Finally, we admittedly note that by applying a
percentage cutoff on the initial term set and further
performing the sentence reconstruction we gain in
content coverage, to a certain extent, on the ex-
pense of sentence readability.
6 Conclusion
In this paper we proposed a novel approach to
summary generation from summary representa-
tion based on the LSA summarization framework
and on a machine-translation-inspired technique
for sentence reconstruction.
Our preliminary results show that our approach
is feasible, since it produces summaries which re-
semble better human summaries in terms of the av-
erage number of sentences per summary and yield
ROUGE scores comparable to the participating
systems in the Summarization task at TAC 2009.
Bearing in mind that our approach is completely
unsupervised and language-independent, we find
our results promising.
In future work we plan on working towards im-
proving the quality of our sentence reconstruction
step in order to produce better and more readable
sentences.
5These are, effectively, what we called initial pool of sen-
tences in section 3, before the TSR generation.
6The only difference between the two types of summaries
is the reconstruction step, since we are including 100% of the
terms.
385
References
B. Boguraev and C. Kennedy. 1997. Salience-
based content characterisation of text documents. In
I. Mani, editor, Proceedings of the Workshop on In-
telligent and Scalable Text Summarization at the An-
nual Joint Meeting of the ACL/EACL, Madrid.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1994. The mathematic of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 31:273?318.
G. Erkan and D. Radev. 2004. LexRank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analy-
sis. In Proceedings of ACM SIGIR, New Orleans,
US.
E. Hovy. 2005. Automated text summarization. In
Ruslan Mitkov, editor, The Oxford Handbook of
Computational Linguistics, pages 583?598. Oxford
University Press, Oxford, UK.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
?03, pages 48?54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of ACL ?07, demonstration session.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proceedings of the ACM
SIGIR, pages 68?73, Seattle, Washington.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, Edmonton, Canada.
NIST, editor. 2009. Proceeding of the Text Analysis
Conference, Gaithersburg, MD, November.
F. Och and H. Ney. 2001. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL ?02, pages
295?302, Morristown, NJ, USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation.
In Proceedings of EMNLP, volume 149. Barcelona,
Spain.
K. Spa?rck-Jones. 1999. Automatic summarising: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization.
MIT Press.
J. Steinberger and K. Jez?ek. 2009. Update summariza-
tion based on novel topic distribution. In Proceed-
ings of the 9th ACM DocEng, Munich, Germany.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jez?ek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management,
43(6):1663?1680. Special Issue on Text Summari-
sation (Donna Harman, ed.).
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of KI
?02, pages 18?32, London, UK. Springer-Verlag.
386
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 49?56
Manchester, August 2008
Story tracking: linking similar news over time and across languages 
Bruno Pouliquen & Ralf Steinberger 
European Commission 
Joint Research Centre 
Via E. Fermi 2749, 21027 Ispra, Italy 
Firstname.Lastname@jrc.it 
Olivier Deguernel 
Temis S.A. 
Tour Gamma B, 193-197 rue de Bercy 
75582 Paris Cedex, France 
Olivier.Deguernel@temis.com
Abstract 
The Europe Media Monitor system 
(EMM) gathers and aggregates an aver-
age of 50,000 newspaper articles per day 
in over 40 languages. To manage the in-
formation overflow, it was decided to 
group similar articles per day and per 
language into clusters and to link daily 
clusters over time into stories. A story 
automatically comes into existence when 
related groups of articles occur within a 
7-day window. While cross-lingual links 
across 19 languages for individual news 
clusters have been displayed since 2004 
as part of a freely accessible online appli-
cation (http://press.jrc.it/NewsExplorer), 
the newest development is work on link-
ing entire stories across languages. The 
evaluation of the monolingual aggrega-
tion of historical clusters into stories and 
of the linking of stories across languages 
yielded mostly satisfying results. 
1 Introduction 
Large amounts of information are published 
daily on news web portals around the world. Pre-
senting the most important news on simple, 
newspaper-like pages is enough when the user 
wants to be informed about the latest news. 
However, such websites do not provide a long-
term view on how any given story or event de-
veloped over time. Our objective is to provide 
users with a fully automatic tool that groups in-
dividual news articles every day into clusters of 
related news and to aggregate the daily clusters 
into stories, by linking them to the related ones 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
identified in the previous weeks and months. In 
our jargon, stories are thus groups of articles 
talking about a similar event or theme over time. 
We work with the daily clusters computed by the 
NewsExplorer application (Pouliquen et al 
2004). For each daily cluster in currently nine-
teen languages, the similarity to all clusters pro-
duced during the previous seven days is com-
puted and a link is established if the similarity is 
above a certain threshold. It is on the basis of 
these individual links that stories are built, i.e. 
longer chains of news clusters related over time. 
The current NewsExplorer application addition-
ally identifies for all news clusters, whether there 
are related clusters in the other languages. These 
daily cross-lingual links are used to link the 
longer-lasting stories across languages. 
After a review of related work (Section  1 2), 
we will present the Europe Media Monitor 
(EMM) system and its NewsExplorer application 
(section  3). We will then provide details on the 
process to build the multi-monolingual stories 
(Section  4) and on the more recent work on link-
ing stories across languages (Section  5). Sec-
tion  6 presents evaluation results both for the 
monolingual story compilation and for the estab-
lishment of cross-lingual links. Section  7 con-
cludes and points to future work.  
2 Related work 
The presented work falls into the two fields of 
Topic Detection and Tracking and cross-lingual 
document similarity calculation.  
2.1 Topic detection and tracking (TDT) 
TDT was promoted and meticulously defined by 
the US-American DARPA programme (see 
Wayne 2000). An example explaining the TDT 
concept was that of the Oklahoma City bombing 
in 1995, where not only the bombing, but also 
the related memorial services, investigations, 
prosecution etc. were supposed to be captured. 
49
Human evaluators will often differ in their opin-
ion whether a given document belongs to a topic 
or not, especially as ?topic? can be defined 
broadly (e.g. the Iraq war and the following pe-
riod of insurgence) or more specifically. For in-
stance, the capture and prosecution of Saddam 
Hussein, individual roadside bombings and air 
strikes, or the killing of Al Qaeda leader Abu 
Musab al-Zarqawi could either be seen as indi-
vidual topics or as part of the Iraq war. This 
fuzziness regarding what is a ?topic? makes a 
formal evaluation rather difficult. Our system is 
more inclusive and will thus include all the men-
tioned sub-events into one topic (story). A sepa-
rate clustering system was developed as part of 
the EMM-NewsBrief (http://press.jrc.it/NewsBrief/), 
which produces more short-lived and thus more 
specific historical cluster links. 
2.2 Cross-lingual linking of documents 
Since 2000, the TDT task was part of the TIDES 
programme (Translingual Information Detection, 
Extraction and Summarisation), which focused 
on cross-lingual information access. The goal of 
TIDES was to enable English-speaking users to 
access, correlate and interpret multilingual 
sources of real-time information and to share the 
essence of this information with collaborators. 
The purpose of our own work includes the topic 
detection and tracking as well as the cross-
lingual aspect. Main differences between our 
own work and TIDES are that we need to moni-
tor more languages, that we are interested in all 
cross-lingual links (as opposed to targeting only 
English), and that we use different methods to 
establish cross-lingual links (see Section 5). 
All TDT and TIDES participants used either 
Machine Translation (MT; e.g. Leek et al 1999) 
or bilingual dictionaries (e.g. Wactlar 1999) for 
the cross-lingual tasks. Performance was always 
lower for cross-lingual topic tracking (Wayne 
2000). An interesting insight was formulated in 
the ?native language hypothesis? by Larkey et al
(2004), which states that topic tracking works 
better in the original language than in (ma-
chine-)translated collections. Various partici-
pants stated that the usage of named entities 
helped (Wayne 2000). Taking these insights into 
account, we always work in the source language 
and make intensive use of named entities. 
Outside TDT, an additional two approaches 
for linking related documents across languages 
have been proposed, both of which use bilingual 
vector space models: Landauer & Littman (1991) 
used bilingual Lexical Semantic Analysis and Vi-
nokourov et al (2002) used Kernel Canonical 
Correlation Analysis. These and the approaches 
using MT or bilingual dictionaries have in com-
mon that they require bilingual resources and are 
thus not easily scalable for many language pairs. 
For N languages, there are N*(N-1)/2 language 
pairs (e.g. for 20 languages, there are 190 lan-
guage pairs and 380 language pair directions). 
Due to the multilinguality requirement in the 
European Union (EU) context (there are 23 offi-
cial EU languages as of 2007), Steinberger et al 
(2004) proposed to produce an interlingual docu-
ment (or document cluster) representation based 
on named entities (persons, organisations, disam-
biguated locations), units of measurement, multi-
lingual specialist taxonomies (e.g. medicine), 
thesauri and other similar resources that may help 
produce a language-independent document repre-
sentation. Similarly to Steinberger et al (2004), 
the work described in the following sections 
equally goes beyond the language pair-specific 
approach, but it does not make use of the whole 
range of information types.  
In Pouliquen et al (2004), we showed how 
NewsExplorer links individual news clusters 
over time and across languages, but without ag-
gregating the clusters into the more compact and 
high-level representations (which we call sto-
ries). This new level of abstraction was achieved 
by exploiting the monolingual and cross-lingual 
cluster links and by adding additional filtering 
heuristics to eliminate wrong story candidate 
clusters. As a result, long-term developments can 
now be visualised in timelines and users can ex-
plore the development of events over long time 
periods (see Section  4.2). Additionally, meta-
information for each story can be compiled 
automatically, including article and cluster statis-
tics as well as lists of named entities associated 
to a given story.  
2.3 Commercial applications 
Compared to commercial or other publicly accessi-
ble news analysis and navigation applications, the 
one presented here is unique in that it is the only 
one offering automatic linking of news items re-
lated either historically or across languages. The 
news aggregators Google News 
(http://news.google.com) and Yahoo! News 
(http://news.yahoo.com/), for instance, deliver daily 
news in multiple languages, but do not link the 
found articles over time or across languages. The 
monolingual English language applications Day-
Life (http://www.daylife.com/), SiloBreaker 
(http://www.silobreaker.com/), and NewsVine 
50
Figure 1. Example of historical links between 
clusters: The graph shows the cosine similarity 
between today?s English language cluster (Final 
hole being drilled ?) and seven clusters identi-
fied during five previous days. Only clusters with 
a similarity above 0.5 will be retained. 
(http://www.newsvine.com/) do not link related news 
over time either. NewsTin (http://www.newstin.com) 
is the only one to offer more languages (ten) and to 
categorise news into a number of broad categories, 
but  they, again, do not link related news over time 
or across languages.  
3 Europe Media Monitor (EMM) & 
NewsExplorer 
EMM has been gathering multilingual news arti-
cles from many different web portals since 2002. 
It?s NewsBrief application has since displayed 
the world?s most recent news items on its public 
web servers (http://emm.jrc.it/overview.html). 
Every day, and for each of 19 languages sepa-
rately, EMM?s NewsExplorer application groups 
related articles into clusters. Clusters are com-
puted using a group average agglomerative bot-
tom-up clustering algorithm (similar to Schultz 
& Liberman 1999). Each article is represented as 
a vector of keywords with the keywords being 
the words of the text (except stop words) and 
their weight being the log-likelihood value com-
puted using word frequency lists based on sev-
eral years of news. We additionally enrich the 
vector space representation of each cluster with 
country information (see Pouliquen et al, 2004), 
based on log-likelihood-weighted, automatically 
recognised and disambiguated location and coun-
try names (see Pouliquen et al 2006).  
Each computed daily cluster consists of its 
keywords (i.e. the average log-likelihood weight 
for each word) and the title of the cluster?s me-
doid (i.e. the article closest to the centroid of the 
cluster). In addition we enrich the cluster with 
features that will be used in further processes. 
These include the cluster size, lists of persons, 
organisations, geo-locations and subject domain 
codes (see Section  5). 
When comparing two clusters in the same lan-
guage, the keywords offer a good representation 
(especially when the keywords are enriched with 
the country information). Section  5 will show 
that the additional ingredients are useful to com-
pare two clusters in different languages. 
4 Building stories enriched with meta-
information 
For each language separately and for each individ-
ual cluster of the day, we compute the cosine simi-
larity with all clusters of the past 7 days (see Fig-
ure 1). Similarity is based on the keywords associ-
ated with each cluster. If the similarity between the 
keyword vectors of two clusters is above the em-
pirically derived threshold of 0.5, clusters are 
linked. This optimised threshold was established by 
evaluating cluster linking in several languages (see 
Pouliquen et al 2004). A cluster can be linked to 
several previous clusters, and it can even be linked 
to two different clusters of the same day. 
4.1 Building stories by linking clusters over 
time 
Stories are composed of several clusters. If a new 
cluster is similar to clusters that are part of a 
story, it is likely that this new cluster is a con-
tinuation of the existing story. For the purpose of 
building stories, individual and yet unlinked clus-
ters of the previous seven days are treated like 
(single cluster) stories. If clusters have not been 
linked to within seven days, they remain individ-
ual clusters that are not part of a story. Building 
stories out of clusters is done using the following 
incremental algorithm (for a given day): 
for each cluster c  
 for each story s  
  score[s]=0; 
 for each cluster cp (linked to c) 
  if (s: story containing cp) then 
  score[s] += (1-score[s])*sim(cp,s); 
  endif 
 endfor 
 endfor 
 if (s: story having the maximum score) 
 then 
  add c to story s (with sim score[s]) 
 else // not similar to any story 
  create new story containing only c 
 endif 
endfor 
51
Lang Biggest title Keywords 
En US Airways won't pursue Delta 
forever 
United states / Doug Parker, Delta Airlines / airways, offer, emerge, 
grinstein, bid, regulatory, creditors, bankruptcy, atlanta, increased 
It Stop al massacro di balene. Il 
mondo contro il Giappone 
Australia, N. Zealand, Japan/ Greenpeace International, John Ho-
ward/ caccia, megattere, balene, sydney, acqua, mesi, antartico, salti 
Es Mayor operaci?n contra la por-
nograf?a infantil en Internet en la 
historia de Espa?a 
Guardia Civil, Fernando Herrero Tejedor / pornograf?a, imputa-
dos, mayor, cinco, delito, internet, registros, siete, inform?tica, sci 
De Australian Open: "Tommynator" 
mit Gala-Vorstellung 
Russia, Australia, United states / Australian Open, Mischa Zverev 
/ satz, tennis, deutschen, bozoljac, erstrunden, melbourne, kohl-
schreiber, Donnerstag 
Fr Il faut aider l'Afrique ? se mon-
dialiser, dit Jacques Chirac 
Jacques Chirac, African Union / afrique, sommet, continent, pr?si-
dent, cannes, darfour, ?tat, pays, conf?rence, chefs, omar 
Table 1. Examples of stories, their biggest titles and their corresponding keywords. Countries are dis-
played in italic, person and organisation names in boldface. 
with sim(cp,s) being the similarity of the cluster 
to the story (the first cluster of a story gets a sim 
of 1, the following depend on the score com-
puted by the algorithm). 
When deciding whether a new cluster should 
be part of an existing story, the challenge is to 
combine the similarities of the new cluster with 
each of the clusters in the story. As stories 
change over time and the purpose is to link the 
newest events to existing stories, the new cluster 
is only compared to the story?s clusters of the last 
7 days. A seven-day window is intuitive and 
automatically takes care of fluctuations regarding 
the number of articles during the week (week-
ends are quieter). In the algorithm to determine 
whether the new cluster is linked to the story, the 
similarity score is computed incrementally: The 
score is the similarity of the new cluster with the 
latest cluster of the story (typically yesterday?s) 
plus the similarity of the new cluster with the 
story?s cluster of the day before multiplied with a 
reducing factor (1-scorei-1), plus the similarity of 
the new cluster with the story?s cluster of yet an-
other day before multiplied with a reducing fac-
tor (1-scorei-2), etc. The reducing factor helps to 
keep the similarity score between the theoretical 
values 0 (unrelated) and 1 (highly related): 
??
?
<<??
==
? )70(),()1(
)0(0
1 iscsimscore
i
score
ii
i
 
If the final score is above the threshold of 0.5, 
the cluster gets linked to the existing story. 
Otherwise it remains unlinked. The story building 
algorithm is language-independent and could thus 
be applied to all of the 19 NewsExplorer lan-
guages. Currently, it is run every day (in 
sequential order) in the following nine languages: 
Dutch, English, French, German, Italian, 
Portuguese, Slovene, Spanish and Swedish. 
Out of the daily average of 970 new clusters 
(average computed for all nine languages over a 
period of one month), only 281 get linked to an 
existing story (29%) and 90 contribute to a new 
story (9%). The remaining 599 clusters (62%) 
remain unlinked singleton clusters. A small num-
ber of stories are very big and go on over a long 
time. This reflects big media issues such as the 
Iraq insurgence, the Iran-nuclear negotiations 
and the Israel-Palestine conflict. The latter is the 
currently longest story ever (see 
http://press.jrc.it/NewsExplorer/storyedition/en/RTERadio-
5f47a76fe35215964cbab22dcbc88d7b.html).  
4.2 Aggregating and displaying information 
about each story 
For each story, daily updated information gets 
stored in the NewsExplorer knowledge base. 
This includes (a) the title of the first cluster of 
the story (i.e. the title of the medoid article of 
that first cluster); (b) the title of the biggest clus-
ter of the story (i.e. the cluster with most arti-
cles); (c) the most frequently mentioned person 
names in the story (related people); (d) the per-
son names most highly associated to the story 
(associated people, see below); (e) the most fre-
quently mentioned other names in the story 
(mostly organisations, but also events such as 
Olympics, World War II, etc.); (f) the countries 
most frequently referred to in the story (either 
directly with the country name or indirectly, e.g. 
by referring to a city in that country); (g) a list of 
keywords describing the story (see below). This 
meta-information is exported every day into 
XML files for display on NewsExplorer. The 
public web pages display up to 13 keywords, in-
cluding up to three country names and up to two 
person or organisation names (see Table 1). To 
52
see examples of all meta-information types for 
each story, see the NewsExplorer pages.  
Stories are currently accessible through three 
different indexes (see Figure 2): the stories of the 
week, the stories of the month and the biggest 
stories (all displayed on the main page of 
NewsExplorer). The biggest stories are ordered 
by the number of clusters they contain without 
any consideration of the beginning date or the 
end date. The stories of the month present stories 
that started within the last 30 days, stories of the 
week those that started within the last seven 
days. 
For each story, a time line graph (a flash ap-
plication taking an XML export as input) is pro-
duced automatically, allowing users to see trends 
and to navigate and explore the story (Figure 3). 
While a story can have more than one cluster on 
a given day, the graph only displays the largest 
cluster for that day. 
The story?s keyword signature is computed us-
ing the keywords appearing in most of the con-
stituent clusters. If any of the keywords repre-
sents a country, it will be displayed first. A filter-
ing function eliminates keywords that are part of 
one of the selected entities. For instance, if a se-
lected entity is George W. Bush and a selected 
country is Iraq, the keywords Bush, George, 
Iraqi, etc. will not be displayed. 
 
Figure 2. Examples of English language stories, as on the NewsExplorer main page (2.04. 2008). 
As mentioned in the previous paragraph, a 
story?s related entities are those that have been 
mentioned most frequently. This typically in-
cludes many media VIPs. Associated entities are 
names that appear in this particular story, but are 
not so frequently mentioned in news clusters out-
side this story, according to the following, 
TF.IDF-like formula:  
?
?
=
Sc
i
i
ecfreSrelated ),(),(  
)),(log(1(
)1)),(min(log(
),(
),( eSC
efr
ecfr
eSass Sc
i
i +?=
?
?
with fr(e) being the number of clusters the entity 
appears in (in a collection of three years of news) 
and C(S,e) being the number of clusters in the 
story S mentioning the entity. Inversely, the 
NewsExplorer person and organisation pages 
also display, for each entity, the biggest stories 
they are involved in.  
Figure 3. Sample of a short story timeline. When
mousing over the graph, title, date and cluster
size for that day are displayed. A simple click al-
lows to jump to the relevant cluster, enabling us-
ers to explore the story. Available on page
http://press.jrc.it/NewsExplorer/storyedition/en/guardi
an-ee9f870100be631c0147646d29222de9.html.
5 Cross-lingual cluster and story linking 
For each daily cluster in nine NewsExplorer lan-
guages, the similarity to clusters in the other 18 
languages is computed. To achieve this, we pro-
duce three different language-independent vector 
representations for each cluster (for details, see 
Pouliquen et al 2004): a weighted list of Euro-
voc subject domain descriptors (eurov, available 
only for EU languages), a frequency list of per-
son and organisation names (ent), and a weighted 
list of direct or indirect references to countries 
(geo). As a fourth ingredient, we also make use 
of language-dependent keyword lists because 
even monolingual keywords sometimes match 
53
across languages due to cognate words (cog), etc. 
(e.g. tsunami, airlines, Tibet etc.). The overall 
similarity clsim for two clusters c? and c?? in dif-
ferent languages is calculated using a linear 
combination of the four cosine similarities, using 
the values for ???? &,, as 0.4, 0.3, 0.2 and 
0.1, respectively (see Figure 4): 
),(.),(.
),(.),(),(
cccogccent
ccgeocceurovccclsim
???+???+
???+????=???
??
??
 
5.1 Filtering and refining cross-lingual clus-
ter links 
The process described in the previous paragraphs 
produces some unwanted cross-lingual links. We 
also observed that not all cross-lingual links are 
transitive although they should be. We thus de-
veloped an additional filtering and link weighting 
algorithm to improve matters, whose basic idea 
is the following: When clusters are linked in 
more than two languages, our assumption is: If 
cluster A is linked to cluster B and cluster C, 
then cluster B should also be linked to cluster C. 
We furthermore assume that if cluster B is not 
linked to cluster C, then cluster B is less likely to 
be linked to cluster A. The new algorithm thus 
checks these ?inter-links? and calculates a new 
similarity value which combines the standard 
similarity (described in 5.0) with the number of 
inter-links. The formula punishes links to an iso-
lated cluster (i.e. links to a target language clus-
ter which itself is not linked to other linked lan-
guages) and raises the score for inter-linked clus-
ters (i.e. links to a target language cluster which 
itself is linked to other linked languages). The 
new similarity score uses the formula: 
)(
)(
).,(),(
CEl
CCl
CCclsimCCmclsi ?
????=????    
with Cl(C) being the number of computed cross-
lingual links and El(C) being the number of ex-
pected cross-links (i.e. all cross-language links 
observed when looking at all languages). For in-
stance, if a cluster is linked to three languages 
and these are linked to a further three, then 
Cl(C?)=3 and El(C?)=6.  
Figure 4. Example of the similarity calculation 
for an English and a French cluster. The overall 
similarity for these two clusters, based on the lin-
ear combination of four  different vectors, is 0.46.  
5.2 Linking whole stories across languages  
The stories contain clusters which are themselves 
linked to clusters in other languages (see 5.1). 
This information can be used to compute the 
similarity between two whole stories in different 
languages. The formula is quite simple: 
     ?
????????
????=???
ScSc
ji
ji
ccmclsiSSSclsim
,
),(),(  
with S' and S'' being two stories in different lan-
guages, and c' and c'' being constituent clusters. 
Cross-lingual cluster similarity values are only 
added if they are above the threshold of 0.15. 
Table 2 shows an English story and its links in 
seven languages. 
As the evaluation results in Section 6 show, this 
formula produces reasonable results, but it has 
some limitations. Firstly, it relies exclusively on 
Lang. 
  
Biggest title 
 
Nb. of 
clusters
Nb. of  
articles 
Common 
clusters 
Simi-
larity 
En Rescuers injured at mine collapse 17 200 --- --- 
Pt EUA: mineiros presos numa mina continuam incontact?veis 12 63 7 2.1363 
Es Colapsa mina en EE.UU. 5 24 3 0.9138 
De USA: Sechs Bergleute eingeschlossen 3 28 2 0.7672 
Nl Mijnwerkers vast na aardbeving in Utah 2 7 2 0.6082 
Fr Le sauvetage de mineurs dans l'Utah tourne au drame 3 16 2 0.5541 
Nl Reddingswerkers omgekomen in mijn Utah 2 12 2 0.4644 
Sv Mystisk "ub?t" unders?ks i New York 4 16 2 0.3681 
Table 2. Example of cross-lingual links between the English language US mine collapse story and stories 
in seven other languages. The Swedish story, which has the lowest similarity score, is actually unrelated. 
54
daily cross-lingual links, whereas stories are not 
necessarily reported on the same day across lan-
guages. Secondly, we might be able to produce 
better results by making use of the available 
meta-information at story level described in Sec-
tion 4.2. We are thus planning to refine this for-
mula in future work.  
Type of story  
N
um
be
r o
f 
st
or
ie
s 
N
b 
of
 c
or
re
ct
 
cr
os
s-
lin
gu
al
 
lin
ks
 
N
um
be
r o
f 
cr
os
s-
lin
gu
al
 
lin
ks
 
Pr
ec
is
io
n 
All stories 112 275 465 0.59 
Stories containing at 
least 5 clusters 
39 145 232 0.62 
Stories containing at 
least 10 clusters 
11 75 100 0.75 
10 top stories in 4 
languages 
40 235 270 0.87 
Table 4. Evaluation of cross-lingual story linking. 
6 Evaluation 
Evaluating such a system is not straightforward 
as there is a lot of room for  interpretation re-
garding the relatedness of clusters and stories. 
Cluster consistency evaluation and the monolin-
gual and cross-lingual linking of individual clus-
ters using a very similar approach has already 
been evaluated in Pouliquen et al (2004).  
In order to evaluate the precision for the story 
building in four languages, we have evaluated 
the relatedness of the individual components (the 
clusters) with the story itself. We compiled a list 
of 330 randomly selected stories (in the 4 lan-
guages English, German, Italian and Spanish) 
and asked an expert to judge if each of the clus-
ters is linked to the main story. For each story, 
we thus have a ratio of 'correctly linked' clusters 
(see Table 3). The average ratio corresponds to 
the precision of the story tracking system. There 
clearly is room for improvement, but we found the 
results good enough to display the automatically 
identified stories as part of the live application.  
We did make an attempt at evaluating also the 
recall for story building, but soon found out that 
the results would not make sense. The idea was 
to carry out a usage-oriented evaluation for the 
situation in which users are looking for any story 
of their choice using their own search words (e.g. 
Oscar and nomination, Pavarotti and death, 
etc.). It was found that relevant stories did indeed 
exist for almost every query. However, the re-
sults would entirely depend on the type of story 
the evaluator is looking for and on the evalua-
tor?s capacity to identify significant search 
words. We can thus not present results for the re-
call evaluation of the story tracking system. 
The purpose of a second test was to evaluate 
the accuracy of the cross-lingual story linking. 
For that purpose, we evaluated those 112 multi-
lingual stories out of the 330 stories in the previ-
ous experiment that had cross-lingual links to 
any of the languages Dutch, English, French, 
German, Italian, Portuguese, Spanish or Swedish. 
Table 4 shows that only 59% of the automati-
cally established cross-lingual story links were 
accurate, but that the situation improves when 
looking at stories consisting of more clusters, i.e. 
5 or 10. This trend was confirmed by a separate 
study evaluating only the cross-lingual links for 
the 10 largest stories in the same four languages, 
into the same eight other languages: 87% of the 
cross-lingual links were correct. Note that ? for 
these large stories ? the cross-lingual links were 
96.5% complete (270 out of 280 possible links 
were present). Further insights from this evalua-
tion are that there are only two out of the 40 top 
stories that should be merged (there are two Eng-
lish top stories on Israel) and that there is one 
cluster in each of the four languages which 
should be split (all China-related news merges 
into one story). It is clear that more experiments 
are needed to improve the cross-lingual links for 
smaller stories. We have not evaluated the recall 
of the cross-lingual story linking as recall evalua-
tion is very time-consuming and we first want to 
optimise the algorithm.  
7 Conclusion and Future Work 
Lan-
guage 
Number 
of stories 
Correct com-
ponents 
All com-
ponents 
Preci
sion 
German 93 249 265 0.94
English 113 490 570 0.86
Spanish 33 78 91 0.86
Italian 91 239 299 0.80
All  330 1056 1225 0.86
Table 3. Evaluation of the monolingual linking 
of clusters into stories for four languages. 
The story tracking system has been running for 
two years. There is definitely space for improve-
ment as unrelated clusters are sometimes part of 
a story, but informal positive user feedback 
makes us believe that users already find the cur-
rent results useful. An analysis of the web logs 
shows that more than 400 separate visitors per 
day look at story-related information, split quite 
evenly across the different languages (Table 5).  
55
 The story tracking algorithm is rather sensi-
tive to the starting date for the process: Different 
starting dates may result in different stories and 
certain starting dates may result in having two 
separate parallel stories talking about very 
closely related subjects. Another issue is the 
seven-day window: We may want to extend the 
window as it happens occasionally that a story 
?dies? because no related articles are published 
on the subject for a week, and that another story 
talking about the same subject starts 8 days later. 
Finally, our algorithm should try to cope with the 
fact that stories can split or merge (an issue not 
currently dealt with), but this is a non-trivial issue. 
Regarding the cross-lingual linking, the current 
results are encouraging, but not sufficient. The ac-
curacy needs to be improved before the results can 
go online. The most promising idea here is to 
make use of each story?s meta-information (lists 
of related persons, organisations, countries and 
keywords at story level) and to allow a time de-
lay in the publication of stories across languages. 
However, the application has high potential, as it 
will provide users with (graphically visualisable) 
information on how the media report events 
across languages and countries.  
In a separate effort, a ?live? news clustering 
system has been developed within EMM, which 
groups the news as they come in during the day 
(see http://press.jrc.it/NewsBrief/). This process 
needs to be integrated with the daily and more 
long-term story tracking process so that users can 
explore the history and the background for cur-
rent events.  
Acknowledgements 
We thank the Web Mining and Intelligence team 
and our team leader Erik van der Goot for the valu-
able news data and the robust web sites. A special 
thanks to Jenya Belyaeva for her evaluation. 
References 
Landauer Thomas & Michael Littman (1991). A Sta-
tistical Method for Language-Independent Repre-
sentation of the Topical Content of Text Segments. 
Proceedings of the 11th International Conference 
?Expert Systems and Their Applications?, vol. 8: 
pp. 77-85.  
Larkey Leah, Fangfang Feng, Margaret Connell, Vic-
tor Lavrenko (2004). Language-specific Models in 
Multilingual Topic Tracking. Proceedings of the 
27th annual international ACM SIGIR conference 
on Research and development in information re-
trieval, pp. 402-409. 
Leek Tim, Hubert Jin, Sreenivasa Sista & Richard 
Schwartz (1999). The BBN Crosslingual Topic De-
tection and Tracking System. In 1999 TDT Evalua-
tion System Summary Papers.  
Pouliquen Bruno, Ralf Steinberger, Camelia Ignat, 
Emilia K?sper & Irina Temnikova (2004). Multi-
lingual and cross-lingual news topic tracking. In: 
Proceedings of the 20th International Conference on 
Computational Linguistics, Vol. II, pp. 959-965.   
Pouliquen Bruno, Marco Kimler, Ralf Steinber-
ger,  Camelia Ignat, Tamara Oellinger, Ken Black-
ler, Flavio Fuart, Wajdi Zaghouani, Anna Widiger, 
Ann-Charlotte Forslund & Clive Best (2006). Geo-
coding multilingual texts: Recognition, Disam-
biguation and Visualisation. Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation (LREC'2006), pp. 53-58.  
Schultz J. Michael & Mark Liberman (1999). Topic 
detection and Tracking using idf-weighted Cosine 
Coefficient. DARPA Broadcast News Workshop 
Proceedings. 
Steinberger Ralf, Bruno Pouliquen & Camelia Ignat 
(2004). Providing cross-lingual information access 
with knowledge-poor methods. In: Andrej Brodnik, 
Matja? Gams & Ian Munro (eds.): Informatica. An 
international Journal of Computing and Informat-
ics. Vol. 28-4, pp. 415-423. Special Issue 'Informa-
tion Society in 2004'.  
Lang Hits Pct 
Hits/
day Visits 
Visits 
/day Pct 
De  59993 14% 2143 1611 58 13% 
En  164557 38% 5877 2273 81 19% 
Es  49360 11% 1763 1431 51 12% 
Fr  56023 13% 2001 1514 54 12% 
It  29445 7% 1052 1425 51 12% 
Nl  25175 6% 899 1242 44 10% 
Pt  42933 10% 1533 2170 78 18% 
Sv  7284 2% 260 575 21 5% 
Total: 434770  15527 12241 437  
Table 5. Number of connections to story-related 
NewsExplorer web pages only, and distribution 
per language (period 1-28/06/2008). Only visits 
from different IP addresses were counted. 
Vinokourov Alexei, John Shawe-Taylor, Nello Cristi-
anini (2002). Inferring a semantic representation of 
text via cross-language correlation analysis. Ad-
vances of Neural Information Processing Systems 15. 
Wactlar Howard (1999). New Directions in Video In-
formation Extraction and Summarization. Proceed-
ings of the 10th DELOS Workshop.  
Wayne Charles (2000). Multilingual topic detection 
and tracking: Successful research enabled by cor-
pora and evaluation. Proceedings of 2nd Interna-
tional Conference on Language Resources and 
Evaluation. 
56
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 28?36,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Creating Sentiment Dictionaries via Triangulation
Josef Steinberger,
Polina Lenkova, Mohamed Ebrahim,
Maud Ehrmann, Ali Hurriyetoglu,
Mijail Kabadjov, Ralf Steinberger,
Hristo Tanev and Vanni Zavarella
EC Joint Research Centre
21027, Ispra (VA), Italy
Name.Surname@jrc.ec.europa.eu
Silvia Va?zquez
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona
silvia.vazquez@upf.edu
Abstract
The paper presents a semi-automatic approach
to creating sentiment dictionaries in many lan-
guages. We first produced high-level gold-
standard sentiment dictionaries for two lan-
guages and then translated them automatically
into third languages. Those words that can
be found in both target language word lists
are likely to be useful because their word
senses are likely to be similar to that of the
two source languages. These dictionaries can
be further corrected, extended and improved.
In this paper, we present results that verify
our triangulation hypothesis, by evaluating tri-
angulated lists and comparing them to non-
triangulated machine-translated word lists.
1 Introduction
When developing software applications for senti-
ment analysis or opinion mining, there are basi-
cally two main options: (1) writing rules that assign
sentiment values to text or text parts (e.g. names,
products, product features), typically making use of
dictionaries consisting of sentiment words and their
positive or negative values, and (2) inferring rules
(and sentiment dictionaries), e.g. using machine
learning techniques, from previously annotated doc-
uments such as product reviews annotated with an
overall judgment of the product. While movie or
product reviews for many languages can frequently
be found online, sentiment-annotated data for other
fields are not usually available, or they are almost
exclusively available for English. Sentiment dictio-
naries are also mostly available for English only or,
if they exist for other languages, they are not com-
parable, in the sense that they have been developed
for different purposes, have different sizes, are based
on different definitions of what sentiment or opinion
means.
In this paper, we are addressing the resource bot-
tleneck for sentiment dictionaries, by developing
highly multilingual and comparable sentiment dic-
tionaries having similar sizes and based on a com-
mon specification. The aim is to develop such dic-
tionaries, consisting of typically one or two thou-
sand words, for tens of languages, although in this
paper we only present results for eight languages
(English, Spanish, Arabic, Czech, French, German,
Italian and Russian). The task raises the obvious
question how the human effort of producing this re-
source can be minimized. Simple translation, be it
using standard dictionaries or using machine trans-
lation, is not very efficient as most words have two,
five or ten different possible translations, depending
on context, part-of-speech, etc.
The approach we therefore chose is that of trian-
gulation. We first produced high-level gold-standard
sentiment dictionaries for two languages (English
and Spanish) and then translated them automatically
into third languages, e.g. French. Those words that
can be found in both target language word lists (En
Fr and Es Fr) are likely to be useful because their
word senses are likely to be similar to that of the
two source languages. These word lists can then be
used as they are or better they can be corrected, ex-
tended and improved. In this paper, we present eval-
uation results verifying our triangulation hypothesis,
by evaluating triangulated lists and comparing them
28
to non-triangulated machine-translated word lists.
Two further issues need to be addressed. The
first one concerns morphological inflection. Auto-
matic translation will yield one word form (often,
but not always the base form), which is not suffi-
cient when working with highly inflected languages:
A single English adjective typically has four Spanish
or Italian word forms (two each for gender and for
number) and many Russian word forms (due to gen-
der, number and case distinctions). The target lan-
guage word lists thus need to be expanded to cover
all these morphological variants with minimal effort
and considering the number of different languages
involved without using software, such as morpho-
logical analysers or generators. The second issue
has to do with the subjectivity involved in the human
annotation and evaluation effort. First of all, it is im-
portant that the task is well-defined (this is a chal-
lenge by itself) and, secondly, the inter-annotator
agreement for pairs of human evaluators working on
different languages has to be checked in order to get
an idea of the natural variation involved in such a
highly subjective task.
Our main field of interest is news opinion min-
ing. We would like to answer the question how cer-
tain entities (persons, organisations, event names,
programmes) are discussed in different media over
time, comparing different media sources, media in
different countries, and media written in different
languages. One possible end product would be a
graph showing how the popularity of a certain en-
tity has changed over time across different languages
and countries. News differs significantly from those
text types that are typically analysed in opinion min-
ing work, i.e. product or movie reviews: While a
product review is about a product (e.g. a printer)
and its features (e.g. speed, price or printing qual-
ity), the news is about any possible subject (news
content), which can by itself be perceived to be pos-
itive or negative. Entities mentioned in the news can
have many different roles in the events described.
If the method does not specifically separate positive
or negative news content from positive or negative
opinion about that entity, the sentiment analysis re-
sults will be strongly influenced by the news context.
For instance, the automatically identified sentiment
towards a politician would most likely to be low if
the politician is mentioned in the context of nega-
tive news content such as bombings or disasters. In
our approach, we therefore aim to distinguish news
content from sentiment values, and this distinction
has an impact on the sentiment dictionaries: unlike
in other approaches, words like death, killing, award
or winner are purposefully not included in the sen-
timent dictionaries as they typically represent news
content.
The rest of the paper is structured as follows: the
next section (2) describes related work, especially
in the context of creating sentiment resources. Sec-
tion 3 gives an overview of our approach to dic-
tionary creation, ranging from the automatic learn-
ing of the sentiment vocabulary, the triangulation
process, the expansion of the dictionaries in size
and regarding morphological inflections. Section 4
presents a number of results regarding dictionary
creation using simple translation versus triangula-
tion, morphological expansion and inter-annotator
agreement. Section 5 summarises, concludes and
points to future work.
2 Related Work
Most of the work in obtaining subjectivity lexicons
was done for English. However, there were some
authors who developed methods for the mapping of
subjectivity lexicons to other languages. Kim and
Hovy (2006) use a machine translation system and
subsequently use a subjectivity analysis system that
was developed for English. Mihalcea et al (2007)
propose a method to learn multilingual subjective
language via cross-language projections. They use
the Opinion Finder lexicon (Wilson et al, 2005)
and two bilingual English-Romanian dictionaries to
translate the words in the lexicon. Since word am-
biguity can appear (Opinion Finder does not mark
word senses), they filter as correct translations only
the most frequent words. The problem of translat-
ing multi-word expressions is solved by translating
word-by-word and filtering those translations that
occur at least three times on the Web. Another ap-
proach in obtaining subjectivity lexicons for other
languages than English was explored in Banea et al
(2008b). To this aim, the authors perform three dif-
ferent experiments, with good results. In the first
one, they automatically translate the annotations of
the MPQA corpus and thus obtain subjectivity an-
29
notated sentences in Romanian. In the second ap-
proach, they use the automatically translated entries
in the Opinion Finder lexicon to annotate a set of
sentences in Romanian. In the last experiment, they
reverse the direction of translation and verify the as-
sumption that subjective language can be translated
and thus new subjectivity lexicons can be obtained
for languages with no such resources. Finally, an-
other approach to building lexicons for languages
with scarce resources is presented in Banea et al
(2008a). In this research, the authors apply boot-
strapping to build a subjectivity lexicon for Roma-
nian, starting with a set of seed subjective entries,
using electronic bilingual dictionaries and a training
set of words. They start with a set of 60 words per-
taining to the categories of noun, verb, adjective and
adverb obtained by translating words in the Opin-
ion Finder lexicon. Translations are filtered using a
measure of similarity to the original words, based on
Latent Semantic Analysis (Landauer and Dumais,
1997) scores. Wan (2008) uses co-training to clas-
sify un-annotated Chinese reviews using a corpus
of annotated English reviews. He first translates
the English reviews into Chinese and subsequently
back to English. He then performs co-training using
all generated corpora. Banea et al (2010) translate
the MPQA corpus into five other languages (some
with a similar ethimology, others with a very differ-
ent structure). Subsequently, they expand the fea-
ture space used in a Naive Bayes classifier using the
same data translated to 2 or 3 other languages. Their
conclusion is that expanding the feature space with
data from other languages performs almost as well
as training a classifier for just one language on a
large set of training data.
3 Approach Overview
Our approach to dictionary creation starts with semi-
automatic way of colleting subjective terms in En-
glish and Spanish. These pivot language dictionaries
are then projected to other languages. The 3rd lan-
guage dictionaries are formed by the overlap of the
translations (triangulation). The lists are then man-
ually filtered and expanded, either by other relevant
terms or by their morphological variants, to gain a
wider coverage.
3.1 Gathering Subjective Terms
We started with analysing the available English
dictionaries of subjective terms: General Inquirer
(Stone et al, 1966), WordNet Affect (Strapparava
and Valitutti, 2004), SentiWordNet (Esuli and Se-
bastiani, 2006), MicroWNOp (Cerini et al, 2007).
Additionally, we used the resource of opinion words
with associated polarity from Balahur et al (2009),
which we denote as JRC Tonality Dictionary. The
positive effect of distinguishing two levels of inten-
sity was shown in (Balahur et al, 2010). We fol-
lowed the idea and each of the emloyed resources
was mapped to four categories: positive, negative,
highly positive and highly negative. We also got
inspired by the results reported in that paper and
we selected as the base dictionaries the combination
of MicroWNOp and JRC Tonality Dictionary which
gave the best results. Terms in those two dictionar-
ies were manually filtered and the other dictionar-
ies were used as lists of candidates (their highly fre-
quent terms were judged and the relevant ones were
included in the final English dictionary). Keeping in
mind the application of the dictionaries we removed
at this step terms that are more likely to describe bad
or good news content, rather than a sentiment to-
wards an entity. In addition, we manually collected
English diminishers (e.g. less or approximately), in-
tensifiers (e.g. very or indeed) and invertors (e.g.
not or barely). The English terms were translated to
Spanish and the same filtering was performed. We
extended all English and Spanish lists with the miss-
ing morphological variants of the terms.
3.2 Automatic Learning of Subjective Terms
We decided to expand our subjective term lists by
using automatic term extraction, inspired by (Riloff
and Wiebe, 2003). We look at the problem of ac-
quisition of subjective terms as learning of seman-
tic classes. Since we wanted to do this for two dif-
ferent languages, namely English and Spanish, the
multilingual term extraction algorithm Ontopopulis
(Tanev et al, 2010) was a natural choice.
Ontopopulis performs weakly supervised learning
of semantic dictionaries using distributional similar-
ity. The algorithm takes on its input a small set of
seed terms for each semantic class, which is to be
learnt, and an unannotated text corpus. For example,
30
if we want to learn the semantic class land vehicles,
we can use the seed set - bus, truck, and car. Then
it searches for the terms in the corpus and finds lin-
ear context patterns, which tend to co-occur imme-
diately before or after these terms. Some of the
highest-scored patterns, which Ontopopulis learned
about land vehicles were driver of the X, X was
parked, collided with another X, etc. Finally, the
algorithm searches for these context patterns in the
corpus and finds other terms which tend to fill the
slot of the patterns (designated by X). Considering
the land vehicles example, new terms which the sys-
tem learned were van, lorry, taxi, etc. Ontopop-
ulis is similar to the NOMEN algorithm (Lin et al,
2003). However, Ontopopulis has the advantage to
be language-independent, since it does not use any
form of language-specific processing, nor does it use
any language-specific resources, apart from a stop
word list.
In order to learn new subjective terms for each
of the languages, we passed the collected subjective
terms as an input to Ontopopulis. For English, we
divided the seed set in two classes: class A ? verbs
and class B ? nouns and adjectives. It was necessary
because each of these classes has a different syn-
tactic behaviour. It made sense to do the same for
Spanish, but we did not have enough Spanish speak-
ers available to undertake this task, therefore we put
together all the subjective Spanish words - verbs, ad-
jectives and nouns in one class. We ran Ontopopulis
for each of the three classes - the class of subjective
Spanish words and the English classes A and B. The
top scored 200 new learnt terms were taken for each
class and manually reviewed.
3.3 Triangulation and Expansion
After polishing the pivot language dictionaries we
projected them to other languages. The dictionaries
were translated by Google translator because of its
broad coverage of languages. The overlapping terms
between English and Spanish translations formed
the basis for further manual efforts. In some cases
there were overlapping terms in English and Span-
ish translations but they differed in intensity. There
was the same term translated from an English posi-
tive term and from a Spanish very positive term. In
these cases the term was assigned to the positive cat-
egory. However, more problematic cases arose when
the same 3rd language term was assigned to more
than one category. There were also cases with dif-
ferent polarity. We had to review them manually.
However, there were still lots of relevant terms in the
translated lists which were not translated from the
other language. These complement terms are a good
basis for extending the coverage of the dictionaries,
however, they need to be reviewed manually. Even if
we tried to include in the pivot lists all morpholog-
ical variants, in the triangulation output there were
only a few variants, mainly in the case of highly in-
flected languages. To deal with morphology we in-
troduced wild cards at the end of the term stem (*
stands for whatever ending and for whatever char-
acter). This step had to be performed carefully be-
cause some noise could be introduced. See the Re-
sults section for examples. Although this step was
performed by a human, we checked the most fre-
quent terms afterwards to avoid irrelavant frequent
terms.
4 Results
4.1 Pivot dictionaries
We gathered and filtered English sentiment terms
from the available corpora (see Section 3.1). The
dictionaries were then translated to Spanish (by
Google translator) and filtered afterwards. By ap-
plying automatic term extraction, we enriched the
sets of terms by 54 for English and 85 for Spanish,
after evaluating the top 200 candidates suggested by
the Ontopolulis tool for each language. The results
are encouraging, despite the relevance of the terms
(27% for English and 42.5% for Spanish where
some missing morphological variants were discov-
ered) does not seem to be very high, considering the
fact that we excluded the terms already contained
in the pivot lists. If we took them into account, the
precision would be much better. The initial step re-
sulted in obtaining high quality pivot sentiment dic-
tionaries for English and Spanish. Their statistics
are in table 1. We gathered more English terms than
Spanish (2.4k compared to 1.7k). The reason for
that is that some translations from English to Span-
ish have been filtered. Another observation is that
there is approximately the same number of negative
terms as positive ones, however, much more highly
negative than highly positive terms. Although the
31
Language English Spanish
HN 554 466
N 782 550
P 772 503
HP 171 119
INT 78 62
DIM 31 27
INV 15 10
TOTAL 2.403 1.737
Table 1: The size of the pilot dictionaries. HN=highly
negative terms, N=negative, P=positive, HP=highly posi-
tive, INV=invertors, DIM=diminishers, INV=invertors.
frequency analysis we carried out later showed that
even if there are fewer highly positive terms, they are
more frequent than the highly negative ones, which
results in almost uniform distribution.
4.2 Triangulation and Expansion
After running triangulation to other languages the
resulted terms were judged for relevance. Native
speakers could suggest to change term?s category
(e.g. negative to highly negative) or to remove it.
There were several reasons why the terms could
have been marked as ?non-sentiment?. For instance,
the term could tend to describe rather negative news
content than negative sentiment towards an entity
(e.g. dead, quake). In other cases the terms were
too ambiguous in a particular language. Examples
from English are: like or right.
Table 2 shows the quality of the triangulated dic-
tionaries. In all cases except for Italian we had only
one annotator assessing the quality. We can see that
the terms were correct in around 90% cases, how-
ever, it was a little bit worse in the case of Russian
in which the annotator suggested to change category
very often.
Terms translated from English but not from Span-
ish are less reliable but, if reviewed manually, the
dictionaries can be expanded significantly. Table 3
gives the statistics concerning these judgments. We
can see that their correctness is much lower than in
the case of the triangulated terms - the best in Italian
(54.4%) and the worst in Czech (30.7%). Of course,
the translation performance affects the results here.
However, this step extended the dictionaries by ap-
proximately 50%.
When considering terms out of context, the most
common translation error occurs when the original
word has several meanings. For instance, the En-
glish word nobility refers to the social class of no-
bles, as well as to the quality of being morally good.
In the news context we find this word mostly in the
second meaning. However, in the Russian triangu-
lated list we have found dvoryanstvo , which refers
to a social class in Russian. Likewise, we need to
keep in mind that a translation of a monosemantic
word might result polysemantic in the target lan-
guage, thereby leading to confusion. For example,
the Italian translation of the English word champion
campione is more frequently used in Italian news
context in a different meaning - sample, therefore
we must delete it from our sentiment words list for
Italian. Another difficulty we might encounter es-
pecially when dealing with inflectional languages is
the fact that a translation of a certain word might be
homographic with another word form in the target
language. Consider the English negative word ban-
dit and its Italian translation bandito, which is more
frequently used as a form of the verb bandire (to an-
nounce) in the news context. Also each annotator
had different point of view on classifying the bor-
derline cases (e.g. support, agreement or difficult).
Two main reasons are offered to explain the low
performance in Arabic. On the one hand, it seems
that some Google translation errors will be repeated
in different languages if the translated words have
the same etymological root. For example both words
? the English fresh and the Spanish fresca ? are
translated to the Arabic as YK
Yg. meaning new. The
Other reason is a more subtle one and is related to
the fact that Arabic words are not vocalized and to
the way an annotator perceive the meaning of a given
word in isolation. To illustrate this point, consider
the Arabic word ? J. ?A
	
J ?? @ , which could be used
as an adjective, meaning appropriate, or as a noun,
meaning The occasion. It appears that the annotator
would intuitively perceive the word in isolation as a
noun and not as an adjective, which leads to disre-
garding the evaluative aspects of a given word.
We tried to include in the pivot dictionaries all
morphological variants of the terms. However, in
highly inflected languages there are much more vari-
ants than those translated from English or Spanish.
32
We manually introduced wild cards to capture the
variants. We had to be attentive when compiling
wild cards for languages with a rich inflectional sys-
tem, as we might easily get undesirable words in the
output. To illustrate this, consider the third person
plural of the Italian negative word perdere (to lose)
perdono, which is also homographic with the word
meaning forgiveness in English. Naturally, it could
happen that the wildcard captures a non-sentiment
term or even a term with a different polarity. For in-
stance, the pattern care% would capture either care,
careful, carefully, but also career or careless. That
is way we perform the last manual checking after
matching the lists expanded by wildcards against a
large number of texts. The annotators were unable
to check all the variants, but only the most frequent
terms, which resulted in reviewing 70-80% of the
term mentions. This step has been performed for
only English, Czech and Russian so far. Table 5
gives the statistics. By introducing the wildcards,
the number of distinct terms grew up significantly
- 12x for Czech, 15x for Russian and 4x for En-
glish. One reason why it went up also for English
is that we captured compounds like: well-arranged,
well-balanced, well-behaved, well-chosen by a sin-
gle pattern. Another reason is that a single pat-
tern can capture different POSs: beaut% can cap-
ture beauty, beautiful, beautifully or beautify. Not
all of those words were present in the pivot dictio-
naries. For dangerous cases like care% above we
had to rather list all possible variants than using a
wildcard. This is also the reason why the number
of patterns is not much lower than the number of
initial terms. Even if this task was done manually,
some noise was added into the dictionaries (92-94%
of checked terms were correct). For example, highly
positive pattern hero% was introduced by an anno-
tator for capturing hero, heroes, heroic, heroical or
heroism. If not checked afterwards heroin would
score highly positively in the sentiment system. An-
other example is taken from Russian: word meaning
to steal ukra% - might generate Ukraine as one most
frequent negative word in Russian.
4.3 How subjective is the annotation?
Sentiment annotation is a very subjective task. In ad-
dition, annotators had to judge single terms without
any context: they had to think about all the senses of
Metric Percent Agreement Kappa
HN 0.909 0.465
N 0.796 0.368
P 0.714 0.281
HP 0.846 0
N+HN 0.829 0.396
P+HP 0.728 0.280
ALL 0.766 0.318
Table 6: Inter-annotator agreement on checking the trian-
gulated list. In the case of HP all terms were annotated as
correct by one of the annotators resulting in Kappa=0.
Metric Percent Agreement Kappa
HN 0.804 0.523
N 0.765 0.545
P 0.686 0.405
HP 0.855 0.669
N+HN 0.784 0.553
P+HP 0.783 0.559
ALL 0.826 0.614
Table 7: Inter-annotator agreement on checking the can-
didates. In ALL diminishers, intensifiers and invertors
are included as well.
the term. Only if the main sense was subjective they
agreed to leave it in the dictionary. Another sub-
jectivity level was given by concentrating on distin-
guishing news content and news sentiment. Defining
the line between negative and highly negative terms,
and similarly with positive, is also subjective. In the
case of Italian we compared judgments of two anno-
tators. The figures of inter-annotator agreement of
annotating the triangulated terms are in table 6 and
the complement terms in table 7. Based on the per-
cent agreement the annotators agree a little bit less
on the triangulated terms (76.6%) compared to the
complement terms (82.6%). However, if we look at
Kappa figures, the difference is clear. Many terms
translated only from English were clearly wrong
which led to a higher agreement between the annota-
tors (0.318 compared to 0.614). When looking at the
difference between positive and negative terms, we
can see that there was higher agreement on the neg-
ative triangulated terms then on the positive ones.
33
Language Triangulated Correct Removed Changed category
Arabic 926 606 (65.5%) 316 (34.1%) 4 (0.4%)
Czech 908 809 (89.1%) 68 (7.5%) 31 (3.4%)
French 1.085 956 (88.1%) 120 (11.1%) 9 (0.8%)
German 1.053 982 (93.3%) 50 (4.7%) 21 (2.0%)
Italian 1.032 918 (89.0%) 36 (3.5%) 78 (7.5%)
Russian 966 816 (84.5%) 49 (5.1%) 101 (10.4%)
Table 2: The size and quality of the triangulated dictionaries. Triangulated=No. of terms coming directly from triangu-
lation, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive from triangulation, but annotator changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 1.092 335 (30.7%) 675 (61.8%) 82 (7.5%)
French 1.226 617 (50.3%) 568 (46.3%) 41 (3.4%)
German 1.182 548 (46.4%) 610 (51.6%) 24 (2.0%)
Italian 1.069 582 (54.4%) 388 (36.3%) 99 (9.3%)
Russian 1.126 572 (50.8%) 457 (40.6%) 97 (8.6%)
Table 3: The size and quality of the candidate terms (translated from English but not from Spanish). Terms=No. of
terms translated from English but not from Spanish, Correct=terms annotated as correct, Removed=terms not relevant
to sentiment analysis, Change category=terms in wrong category (e.g., positive in the original list, but annotator
changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 2.000 1.144 (57.2%) 743 (37.2%) 113 (5.6%)
French 2.311 1.573 (68.1%) 688 (29.8%) 50 (2.1%)
German 2.235 1.530 (68.5%) 660 (29.5%) 45 (2.0%)
Italian 2.101 1.500 (71.4%) 424 (20.2%) 177 (8.4%)
Russian 2.092 1.388 (66.3%) 506 (24.2%) 198 (9.5%)
Table 4: The size and quality of the translated terms from English. Terms=No. of (distinct) terms translated from En-
glish, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive in the original list, but annotator changed the category to highly positive).
Language Initial terms Patterns Matched terms
Count Correct Checked
Czech 1.257 1.063 15.604 93.0% 74.4%
English 2.403 2.081 10.558 93.8% 81.1%
Russian 1.586 1.347 33.183 92.2% 71.0%
Table 5: Statistics of introducing wild cards and its evaluation. Initial terms=checked triangulated terms extended by
relevant translated terms from English, Patterns=number of patterns after introducing wildcards, Matched terms=terms
matched in the large corpus - their count and correctness + checked=how many mentions were checked (based on the
fact that the most frequent terms were annotated).
34
4.4 Triangulation vs. Translation
Table 4 present the results of simple translation from
English (summed up numbers from tables 2 and 3).
We can directly compare it to table 2 where only
results of triangulated terms are reported. The per-
formance of triangulation is significantly better than
the performance of translation in all languages. The
highest difference was in Czech (89.1% and 57.2%)
and the lowest was in Italian (89.0% and 71.4%).
As a task-based evaluation we used the triangu-
lated/translated dictionaries in the system analysing
news sentiment expressed towards entities. The sys-
tem analyses a fixed word window around entity
mentions. Subjective terms are summed up and the
resulting polarity is attached to the entity. Highly
negative terms score twice more than negative, di-
minishers lower and intensifiers lift up the score. In-
vertors invert the polarity but for instance inverted
highly positive terms score as only negative pre-
venting, for instance, not great to score as worst.
The system searches for the invertor only two words
around the subjective term.
We ran the system on 300 German sentences
taken from news gathered by the Europe Media
Monitor (EMM)1. In all these cases the system at-
tached a polarity to an entity mention. We ran it with
three different dictionaries - translated terms from
English, raw triangulated terms (without the man-
ual checking) and the checked triangulated terms.
This pilot experiment revealed the difference in per-
formance on this task. When translated terms were
used there were only 41.6% contexts with correct
polarity assigned by the system, with raw triangu-
lated terms 56.5%, and with checked triangulated
terms 63.4%. However, the number does not contain
neutral cases that would increase the overall perfor-
mance. There are lots of reasons why it goes wrong
here: the entity may not be the target of the sub-
jective term (we do not use parser because of deal-
ing with many languages and large amounts of news
texts), the system can miss or apply wrongly an in-
vertor, the subjective term is used in different sense,
and irony is hard to detect.
1http://emm.newsbrief.eu/overview.html
4.5 State of progress
We finished all the steps for English, Czech and Rus-
sian. French, German, Italian and Spanish dictio-
naries miss only the introduction of wild cards. In
Arabic we have checked only the triangulated terms.
For other 7 languages (Bulgarian, Dutch, Hungarian,
Polish, Portuguese, Slovak and Turkish) we have
only projected the terms by triangulation. However,
we have capabilities to finish all the steps also for
Bulgarian, Dutch, Slovak and Turkish. We haven?t
investigated using more than two pivot languages for
triangulation. It would probably results in more ac-
curate but shortened dictionaires.
5 Conclusions
We presented our semi-automatic approach and cur-
rent state of work of producing multilingual senti-
ment dictionaries suitable of assessing the sentiment
in news expressed towards an entity. The triangula-
tion approach works significantly better than simple
translation but additional manual effort can improve
it a lot in both recall and precision. We believe that
we can predict the sentiment expressed towards an
entity in a given time period based on large amounts
of data we gather in many languages even if the per-
case performance of the sentiment system as on a
moderate level. Now we are working on improving
the dictionaries in all the discussed languages. We
also run experiments to evaluate the system on vari-
ous languages.
Acknowledgments
We thank Alexandra Balahur for her collaboration
and useful comments. This research was partly sup-
ported by a IULA-Universitat Pompeu Fabra grant.
35
References
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
and Bruno Pouliquen. 2009. Opinion mining from
newspaper quotations. In Proceedings of the Work-
shop on Intelligent Analysis and Processing of Web
News Content at the IEEE / WIC / ACM International
Conferences on Web Intelligence and Intelligent Agent
Technology (WI-IAT).
A. Balahur, R. Steinberger, M. Kabadjov, V. Zavarella,
E. van der Goot, M. Halkia, B. Pouliquen, and
J. Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of LREC?10.
C. Banea, R. Mihalcea, and J. Wiebe. 2008a. A boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC.
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan.
2008b. Multilingual subjectivity analysis using ma-
chine translation. In Proceedings of EMNLP.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of COLING.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Andrea Sanso`,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, English linguis-
tics. Franco Angeli, Milano, IT.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceeding of the 6th International Conference on Lan-
guage Resources and Evaluation, Italy, May.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of the ACL Workshop on
Sentiment and Subjectivity in Text.
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of the ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104:211?240.
W. Lin, R. Yangarber, and R. Grishman. 2003. Boot-
strapped learning of semantic classes from positive
and negative examples. In Proceedings of the ICML-
2003 Workshop on The Continuum from Labeled to
Unlabeled Data, Washington DC.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of ACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceeding of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The general inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics, M.I.T. Press, Cambridge, MA.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceeding of the
4th International Conference on Language Resources
and Evaluation, pages 1083?1086, Lisbon, Portugal,
May.
H. Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-
rski, M. Atkinson, and R.Steinberger. 2010. Exploit-
ing machine learning techniques to build an event ex-
traction system for portuguese and spanish. Lingua-
matica: Revista para o Processamento Automatico das
Linguas Ibericas.
X. Wan. 2008. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics and 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP.
36
Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 71?78,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Experiments to Improve Named Entity Recognition on Turkish Tweets
Dilek K?uc? ?uk and Ralf Steinberger
European Commission, Joint Research Centre
Via E. Fermi 2749
21027 Ispra (VA), Italy
firstname.lastname@jrc.ec.europa.eu
Abstract
Social media texts are significant informa-
tion sources for several application areas
including trend analysis, event monitor-
ing, and opinion mining. Unfortunately,
existing solutions for tasks such as named
entity recognition that perform well on
formal texts usually perform poorly when
applied to social media texts. In this pa-
per, we report on experiments that have the
purpose of improving named entity recog-
nition on Turkish tweets, using two dif-
ferent annotated data sets. In these ex-
periments, starting with a baseline named
entity recognition system, we adapt its
recognition rules and resources to better
fit Twitter language by relaxing its capital-
ization constraint and by diacritics-based
expansion of its lexical resources, and we
employ a simplistic normalization scheme
on tweets to observe the effects of these on
the overall named entity recognition per-
formance on Turkish tweets. The evalua-
tion results of the system with these differ-
ent settings are provided with discussions
of these results.
1 Introduction
Analysis of social media texts, particularly mi-
croblog texts like tweets, has attracted recent at-
tention due to significance of the contained in-
formation for diverse application areas like trend
analysis, event monitoring, and opinion mining.
Tools for well-studied problems like named entity
recognition (NER) are usually employed as com-
ponents within these social media analysis appli-
cations. For instance, in (Abel et al., 2011), named
entities extracted from tweets are used to deter-
mine trending topics for user modeling within the
context of personalized recommender systems and
in (Ritter et al., 2012), named entities in tweets
are used to complement the events extracted by an
open domain event extraction system for Twitter.
However, existing NER solutions for well-formed
text types like news articles are reported to suf-
fer from considerable performance degradations
when they are ported to social media texts, mainly
due to the peculiarities of this latter text type (Rit-
ter et al., 2011).
In this paper, we report on our NER experiments
on Turkish tweets in order to determine facilitating
and impeding factors during the development of a
NER system for Turkish tweets which can be used
in social media analysis applications. We carry
out these experiments on two tweet data sets an-
notated with named entities. After the initial eval-
uation results of a rule-based NER system (K?uc??uk
and Yaz?c?, 2009) on these data sets, we gradually
present the performance results achieved by the
extended versions of the system together with dis-
cussions of these results. For these experiments,
we first perform two system adaptations, i.e., re-
laxing the capitalization constraint of the system
and diacritics-based expansion of the system?s lex-
ical resources. Next, we incorporate a simplistic
tweet normalization scheme into the NER proce-
dure. After the evaluation of these extensions, we
provide discussions on the plausible features of a
NER system tailored to Turkish tweets.
The rest of the paper is organized as follows:
In Section 2, we review the literature on NER on
tweets and NER on Turkish texts. In Section 3, we
present our NER experiments on Turkish tweets.
Directions of future work are outlined in Section 4
and finally Section 5 concludes the paper.
2 Related Work
There are several recent studies presenting ap-
proaches for NER on microblog texts, especially
on tweets in English. Among these studies, in
(Ritter et al., 2011), a NER system tailored to
71
tweets, called T-NER, is presented which employs
Conditional Random Fields (CRF) for named en-
tity segmentation and labelled topic modelling for
subsequent classification, using Freebase dictio-
naries. A hybrid approach to NER on tweets is
presented in (Liu et al., 2011) where k-Nearest
Neighbor and CRF based classifiers are sequen-
tially applied. In (Liu et al., 2012), a factor graph
based approach is proposed that jointly performs
NER and named entity normalization on tweets.
An unsupervised approach that performs only
named entity extraction on tweets using resources
like Wikipedia is described in (Li et al., 2012). A
clustering-based approach for NER on microtexts
is presented in (Jung, 2012), a lightweight filter
based approach for NER on tweets is described
in (de Oliveira et al., 2013), and a series of NER
experiments on targeted tweets in Polish is pre-
sented in (Piskorski and Ehrmann, 2013). Finally,
an adaptation of the ANNIE component of GATE
framework to microblog texts, called TwitIE, is
described in (Bontcheva et al., 2013).
Considering NER research on Turkish texts,
various approaches have been employed so far
including those based on using Hidden Markov
Models (HMM) (T?ur et al., 2003), on manually
engineered recognition rules (K?uc??uk and Yaz?c?,
2009; K?uc??uk and Yaz?c?, 2012), on rule learning
(Tatar and C?icekli, 2011), and on CRFs (Yeniterzi,
2011; S?eker and Eryi?git, 2012). All of these ap-
proaches have been proposed for news texts and
the CRF-based approach (S?eker and Eryi?git, 2012)
is reported to outperform the previous proposals
with a balanced F-Measure of about 91%.
To the best of our knowledge, there are only
two studies on NER from Turkish tweets. In
(C?elikkaya et al., 2013), the CRF-based NER sys-
tem (S?eker and Eryi?git, 2012) is evaluated on in-
formal text types and is reported to achieve an
F-Measure of 19% on tweets. In (K?uc??uk et al.,
2014), a tweet data set in Turkish annotated with
named entities is presented. The adaptation of a
multilingual rule-based NER system (Pouliquen
and Steinberger, 2009) to Turkish, which achieves
an F-Measure of about 61% on a news article data
set, gets an F-Measure of 37% on this tweet data
set, and after extending the resources of the NER
system with frequently appearing person and orga-
nization names in Turkish news articles, the corre-
sponding scores increase to about 69% and 43%,
respectively (K?uc??uk et al., 2014).
Table 1: NE Statistics on the Data Sets.
Frequency in
NE Type Tweet Set-1 Tweet Set-2
Person 457 774
Location 282 191
Organization 241 409
All PLOs 980 1,374
Date 201 342
Time 5 25
Money 16 13
Percent 9 3
All NEs 1,211 1,757
3 Named Entity Recognition
Experiments
The NER experiments are performed using the
rule-based NER system (K?uc??uk and Yaz?c?, 2009)
which makes use of a set of lexical resources,
i.e., lists of person/location/organization names
(henceforth referred to as PLOs), and patterns for
the extraction of named entities (NEs) of type
PLOs and time/date/money/percent expressions
(K?uc??uk and Yaz?c?, 2009). The system is pro-
posed for news articles which is a considerably
well-formed text type usually with proper capital-
ization of the initial letters of PLOs and separa-
tion of these PLOs from their suffixes with apos-
trophes
1
. Yet, as even such well-formed texts may
be lacking these important indicators of PLOs, the
system can be configured to make use of the cap-
italization clue or not, and it includes a simplistic
morphological analyzer to check the suffixes at the
end of PLO candidates and thereby validate these
candidates (K?uc??uk and Yaz?c?, 2009).
This NER system achieves a balanced F-
Measure of 78.7% (without giving any credit to
partial extractions) on a news article data set of
about 20K tokens obtained from the METU Turk-
ish corpus (Say et al., 2002) where the annotated
form of this data set includes a total of 1,613 NEs.
Within the course of the current study, we have
evaluated this system on two tweet data sets in
Turkish where statistical information about these
data sets are provided in Table 1. The first one,
which is referred to as Tweet Set?1 in Table 1,
is presented in (K?uc??uk et al., 2014) and comprises
2,320 tweets with about 20K tokens. The sec-
ond data set (Tweet Set?2) includes about 5K
1
An example inflected named entity of location name type
(a city name) in Turkish which takes the dative case suffix
(?ya) is Ankara?ya (meaning to Ankara) where the ini-
tial letter of the named entity is properly capitalized and the
case suffix is accordingly separated from the entity with an
apostrophe.
72
tweets with about 50K tokens and is described in
(C?elikkaya et al., 2013).
3.1 Initial Experiments
We have first evaluated the system?s performance
on the data sets without any extensions to the exist-
ing NER system. Table 2 presents these evaluation
results using the commonly employed metrics of
precision, recall, and balanced F-Measure, with-
out giving any credit to partially extracted NEs.
Table 3 displays those results with the same met-
rics this time giving credit to partial extractions
with the constraint that the NE type within the sys-
tem output and the answer key must be the same,
where these metrics have been employed in stud-
ies like (Maynard et al., 2001).
The evaluation results in Table 2 and Table
3 are in line with the common finding reported
in the literature that the NER systems for com-
paratively well-formed text types face consider-
able performance decreases when they are eval-
uated on tweets. This observation is usually at-
tributed to the peculiarities of tweet texts such as
common grammatical/spelling errors and deliber-
ate contractions. With strict metrics, the system is
reported to achieve an F-Measure rate of 78.7%.
When it is ported to tweets, the best overall F-
Measure rates achieved are 53.23% and 44.25%
on Tweet Set?1 and Tweet Set?2, respectively,
while the corresponding best F-Measure rates for
only PLOs are 47.76% and 36.63%, respectively,
all with strict metrics. The difference between
the results for PLOs and the overall results also
confirms that the system recognizes temporal and
numerical expressions (within its scope) with de-
cent performance, compared to the recognition of
PLOs.
The F-Measure rates obtained when partial ex-
tractions are also given credit are about 5% higher
than those obtained without giving any credit to
partially extracted NEs. This increase is impor-
tant due to pragmatic reasons as these partially
extracted NEs can help conveniently filter tweet
streams and retrieve relevant subsets of tweets in
several application settings.
3.2 NER Experiments with Rule/Resource
Adaptations
Tweet texts possess the following peculiarities
usually as opposed to other formal text types:
? Grammatical/spelling errors are common,
like incorrectly writing proper names all in
lowercase letters. A Turkish example illus-
trating a spelling error is the use of geliyoooo
instead of geliyor (meaning is coming).
? Contracted word forms are commonly used
instead of full forms, like referring to the
football club called Fenerbahc?e as Fener
only, where the latter contracted form is also
homonymous to a common name in Turkish
(meaning lantern).
? For the particular case of Turkish tweets,
non-accentuated characters (c, g, i, o, s, and
u) are often utilized instead of the corre-
sponding Turkish characters with diacritics
(c?, ?g, ?, ?o, s?, and ?u). An example of this phe-
nomenon is writing cunku instead of the cor-
rect form, c??unk ?u (meaning because).
Considering the above features, in order to im-
prove the initial NER performance on Turkish
tweets, we have tested two adaptations of the rule-
based NER system. The details of these adapta-
tions and the corresponding evaluation results are
presented in the following subsections.
3.2.1 Relaxing the Capitalization Constraint
of the System
As proper capitalization of PLOs is usually lack-
ing in tweets, we have evaluated the NER sys-
tem with its capitalization feature turned off, so
that the system considers all tokens (no matter
whether their initial character is capitalized or not)
as valid NE candidates. The initial evaluation re-
sults of the system with this setting are provided
in Table 2 and Table 3 within the rows where
the Capitalization column has a corresponding
OFF value. The results for these two capitaliza-
tion settings are also similarly provided in Tables
4-6 which present the evaluation results described
in the upcoming sections.
The results in Table 2 and Table 3 demonstrate
that relaxing the capitalization constraint (i.e., not
using the capitalization clue) during the NER pro-
cedure on Turkish tweets consistently improves
performance for PLOs on both data sets. The im-
provement obtained with this relaxation is more
dramatic on Tweet Set?2 and for this data set
the overall results are accordingly better than those
obtained when the capitalization clue is used. It
should again be noted that the NER system uses a
73
Table 2: Initial NER Evaluation Results (Strict Metrics).
Data Set Capitalization Metric Person Location Organization Overall for PLOs Overall for 7 Types
Tweet Set-1
ON
P (%) 52.82 77.78 72.34 64.16 71.13
R (%) 32.82 49.65 28.22 36.53 42.53
F (%) 40.49 60.61 40.60 46.55 53.23
OFF
P (%) 36.73 71.72 58.70 49.29 56.21
R (%) 43.33 62.06 33.61 46.33 50.45
F (%) 39.76 66.54 42.74 47.76 53.18
Tweet Set-2
ON
P (%) 55.79 58.68 72.06 58.86 65.62
R (%) 20.54 37.17 11.98 20.31 30.85
F (%) 30.03 45.51 20.55 30.19 41.97
OFF
P (%) 35.61 45.53 40.72 38.31 46.27
R (%) 38.37 61.26 16.63 35.08 42.40
F (%) 36.94 52.23 23.61 36.63 44.25
Table 3: Initial NER Evaluation Results (Partial Metrics).
Data Set Capitalization Metric Person Location Organization Overall for PLOs Overall for 7 Types
Tweet Set-1
ON
P (%) 65.33 86.05 88.37 75.98 80.74
R (%) 39.38 54.01 32.34 41.87 47.13
F (%) 49.14 66.37 47.35 53.99 59.52
OFF
P (%) 42.83 78.68 69.11 56.25 62.49
R (%) 50.92 67.71 38.00 52.55 55.72
F (%) 46.53 72.78 49.04 54.34 58.91
Tweet Set-2
ON
P (%) 69.79 61.34 74.63 68.27 72.51
R (%) 24.28 38.62 12.25 22.65 33.31
F (%) 36.03 47.40 21.05 34.02 45.65
OFF
P (%) 41.82 48.41 41.99 43.21 50.91
R (%) 45.10 65.59 17.06 39.38 46.45
F (%) 43.40 55.71 24.26 41.21 48.58
simplistic morphological analyzer to validate suf-
fixes added at the ends of the NEs, thereby the sys-
tem does not overgenerate with this new setting,
although the precision rates decrease considerably
in return to corresponding increases in the recall
rates. To summarize, together with the fact that
about 25.1% of all PLOs within Tweet Set?1 are
lacking proper capitalization (K?uc??uk et al., 2014),
these findings suggest that the ability to relax this
capitalization constraint is a convenient feature of
a practical NER system for Turkish tweets. An
alternative feature would be to automatically cor-
rect the capitalization of NEs instead, as a pre-
processing step.
3.2.2 Diacritics-Based Expansion of the
Lexical Resources
In Turkish tweet texts, words including Turkish
characters with diacritics are often, usually ei-
ther erroneously or deliberately for pragmatic rea-
sons such as to type faster, spelled with their non-
diacritic equivalents, as pointed out above. There-
fore, we expand the entries in the lexical resources
of the NER system to include both diacritic and
non-diacritic variants of these entries. For in-
stance, the Turkish name of the island Cyprus,
K?br?s, may appear in tweets as K?bris, Kibr?s,
or Kibris, as well. As this example denotes, for
each existing entry with n such Turkish-specific
characters, 2
n
entries (including the original en-
try) are included in the ultimate expanded forms
of the lexical resources, since each such character
may be used as it is or may be replaced with its
equivalent.
During this expansion stage, we have applied
a filtering procedure over these newly considered
2
n
? 1 entries to check whether they are homony-
mous to common names in Turkish. This fil-
tering procedure basically checks whether an ex-
pansion candidate is within a list of unique, sup-
posedly well-formed, Turkish words comprising
about 1,140,208 items including inflected forms
(Zemberek, 2010), and if it is, then this candidate
is discarded to avoid overgeneration during the ac-
tual NER procedure.
We have tested this new version of the sys-
tem with expanded lexical resources and the corre-
sponding evaluation results are provided in Table
4 and Table 5, using the strict and partial evalua-
tion metrics, respectively. Both strict and partial
evaluation results denote that the performance of
the system is improved after this diacritics-based
expansion of the system resources. The best re-
sults are obtained when this expansion is com-
bined with the relaxation of the capitalization con-
straint, for PLOs on Tweet Set?1, and both for
PLOs and all 7 NE types on Tweet Set?2. Sim-
ilar to the points made in the previous section,
this diacritics-based expansion scheme stands as
a promising feature of an ultimate NER system
for Turkish tweets, also considering the fact that
74
Table 4: NER Evaluation Results After Diacritics-Based Expansion of Resources (Strict Metrics).
Data Set Capitalization Metric Person Location Organization Overall for PLOs Overall for 7 Types
Tweet Set-1
ON
P (%) 53.00 78.80 73.20 64.89 71.95
R (%) 32.82 51.42 29.46 37.35 44.26
F (%) 40.54 62.23 42.01 47.41 54.81
OFF
P (%) 36.17 71.31 59.03 48.95 56.16
R (%) 43.76 63.48 35.27 47.35 52.35
F (%) 39.60 67.17 44.16 48.13 54.19
Tweet Set-2
ON
P (%) 58.22 58.73 70.67 60.20 67.29
R (%) 22.87 38.74 12.96 22.13 34.89
F (%) 32.84 46.69 21.90 32.36 45.95
OFF
P (%) 36.80 44.61 32.43 37.61 46.24
R (%) 43.41 62.83 17.60 38.43 47.64
F (%) 39.83 52.17 22.82 38.01 46.93
Table 5: NER Evaluation Results After Diacritics-Based Expansion of Resources (Partial Metrics).
Data Set Capitalization Metric Person Location Organization Overall for PLOs Overall for 7 Types
Tweet Set-1
ON
P (%) 65.58 87.46 88.76 76.81 81.44
R (%) 39.38 56.12 33.62 42.80 48.98
F (%) 49.21 68.37 48.77 54.97 61.17
OFF
P (%) 42.21 79.17 69.00 56.02 62.49
R (%) 51.56 69.85 39.70 53.88 57.90
F (%) 46.42 74.22 50.40 54.93 60.11
Tweet Set-2
ON
P (%) 71.48 61.29 72.97 69.07 73.68
R (%) 26.68 40.21 13.24 24.51 37.47
F (%) 38.86 48.56 22.41 36.18 49.67
OFF
P (%) 42.26 47.07 33.33 41.75 50.23
R (%) 50.14 66.76 18.04 42.65 51.72
F (%) 45.86 55.21 23.41 42.20 50.96
about 6.3% of all NEs in Tweet Set?1 are writ-
ten in characters with missing diacritics. A plausi-
ble alternative to this feature would be to perform
diacritics-based correction (or, normalization) as
presented in studies like (Mihalcea, 2002) prior to
the actual NER procedure. Similar approaches can
be tested on tweets in other languages having com-
mon characters with diacritics.
3.3 Tweet Normalization
Tweet normalization has emerged as an important
research problem (Han and Baldwin, 2011), the
solutions to which can readily be used in systems
for sentiment analysis and NER (as considered in
studies such as (Liu et al., 2012)), among others.
In order to observe the effects of normalization
on NER performance on Turkish tweets, we have
first experimented with a simplistic tweet normal-
ization scheme which aims at decreasing repeated
characters in words, as repetition of characters in
tweets is a frequent means to express stress. The
scheme is outlined below:
1. In order to determine the list of valid Turk-
ish words with consecutively repeated char-
acters, we have employed the list of Turk-
ish unique words (Zemberek, 2010), that we
have previously utilized during the diacritics-
based resource expansion procedure in Sec-
tion 3.2.2. Within this list, 74,262 words
(about 6.5% of the list) turn out to have con-
secutively repeated characters.
2. Using this sublist as a reference resource, we
have implemented the actual simplistic nor-
malization scheme: if a word in a tweet has
consecutively repeated character sequences
and the word is not included within the afore-
mentioned sublist, then all of these character
sequences are contracted to single character
instances. For instance, with this procedure,
the token zamaanlaaa is correctly replaced
with zamanla (meaning with time) and
mirayyy is correctly replaced with miray (a
proper person name).
The employment of the above normalization
scheme prior to the actual NER procedure has
led to slightly poorer results as some NEs which
should not be normalized through this scheme are
normalized instead. For instance, the city name
C?anakkale is changed to C?anakale during the
normalization procedure and it is missed by the
subsequent NER procedure. Hence, we employ
a three-phase pipelined NER approach where we
first run the NER procedure on the input text, then
employ the normalization scheme on the NER out-
put, and finally run the NER procedure again on
the normalization output, in order to avoid that the
normalization step corrupts well-formed NEs that
can readily be extracted by the system.
The performance of this ultimate NER pipeline,
with the capitalization feature turned off during
75
both of the actual NER phases, is evaluated only
on Tweet Set?1. Therefore, the performance
evaluations of the first NER phase correspond to
the previously presented results in the rows 4-6 of
Table 2 and Table 3, with strict and partial versions
of the metrics, respectively.
Below we summarize our findings regarding the
intermediate normalization procedure employed,
based on its evaluation results. Although some of
these findings are not directly relevant for the pur-
poses of the NER procedure, we provide them for
the completeness of the discussion on the normal-
ization of Turkish tweets.
? Excluding the normalization cases which in-
volve non-alphabetical characters only (like
normalizing >>>>>> to >), those that result
in a normalized form with a single alphabet-
ical character (like normalizing oooooo to
o), and those that involve emotion expres-
sions (like normalizing :DDDDD to :D), the
number of resulting instances considered for
performance evaluation is 494.
? The number of normalization instances in
which an incorrect token is precisely con-
verted into its corresponding valid form is
253, so, the precision of the overall normal-
ization scheme is 51.21%.
? 117 of the incorrect cases are due to the fact
that the token that is considered for normal-
ization is a valid but foreign token (such as
normalizing Harry to Hary, jennifer to
jenifer, full to ful, and tweet to twet).
Hence, these cases account for a decrease of
23.68% in the precision of the normalization
scheme.
? 15 of the incorrect instances are due to the
fact that Turkish characters with diacritics
are not correctly used, hence they cannot be
found within the reference sublist of valid
Turkish words, and subsequently considered
by the normalization procedure, although
they could instead be subject to a diacritics-
based normalization, as pointed out at the end
of Section 3.2.2. For instance, s?iir (mean-
ing poem) is incorrectly written as siir in
a tweet and since it, in this incorrect form,
cannot be found on the reference sublist, it is
erroneously changed to sir. There are also
other incorrect instances in which superflu-
ous characters are correctly removed with the
normalization procedure, yet the resulting to-
ken is still not in its correct form as a subse-
quent diacritics-based correction is required.
Though they are not considerably frequent
(as we only consider here tokens with consec-
utively repeated characters), these instances
serve to confirm that the restoration of dia-
critics should be considered along with other
forms of normalization.
? Some other frequent errors made by the nor-
malization scheme are due to incorrect to-
kenization as whitespaces to separate to-
kens can be missing due to writing errors or
the tendency to write some phrases hashtag-
like. An example case is incorrectly writ-
ing the adverb, demek ki (meaning so or
that means), as demekki in a tweet, which
in turn is erroneously changed to demeki
during normalization. This token, demekki,
should not be considered within this type of
normalization at all, although it needs pro-
cessing to be transformed into its correct
form, demek ki.
To summarize, the normalization scheme can
be enhanced considering the above points, where
proper treatment of non-Turkish tokens and the
consideration of diacritics-based issues stand as
the most promising directions of improvement.
Other more elaborate ways of normalizing tweets,
as presented in studies such as (Han and Bald-
win, 2011), should also be tested together with
the NER procedure, to observe their ultimate con-
tribution. Along the way, a normalization dictio-
nary for Turkish can be compiled, following stud-
ies like (Han et al., 2012).
The evaluation results of the ultimate three-
phase NER pipeline are provided in Table 6, with
the systems?s capitalization feature turned off in
both NER phases. Within the first three rows, the
results with the strict evaluation metrics are dis-
played while the last three rows present those re-
sults obtained with the partial versions. When we
examine the individual NER results after the in-
corporation of normalization scheme in details, we
observe that there are cases where incorrectly nor-
malizing some common names or slang/contracted
words leads to them being extracted as NEs during
the second NER phase. In order to prevent such
76
Table 6: Evaluation Results of the NER Pipeline with Normalization, on Tweet Set?1.
Metric Type Metric Person Location Organization Overall for PLOs Overall for 7 Types
Strict
P (%) 36.45 71.72 58.99 48.94 55.91
R (%) 44.42 62.06 34.02 46.94 51.20
F (%) 40.04 66.54 43.16 47.92 53.45
Partial
P (%) 42.32 78.68 69.35 55.73 62.04
R (%) 52.07 67.71 38.43 53.18 56.48
F (%) 46.69 72.78 49.45 54.43 59.13
false positives, the ways of improving the normal-
ization procedure discussed above can be imple-
mented and thereby less errors will be propagated
into the second NER phase.
Though the overall results in Table 6 are slightly
better than their counterparts when normalization
is not employed, we cannot derive sound conclu-
sions about the contribution of this normalization
scheme to the overall NER procedure. The slight
improvement is also an expected result as the size
of the test data set is quite small and the number
of NEs to be recognized after this type of nor-
malization is already limited since only about 1%
of all PLOs in Tweet Set?1 have incorrectly re-
peated consecutive characters. Yet, the results are
still promising in that with a more elaborate nor-
malization procedure evaluated on larger corpora,
more dramatic increases in the NER performance
can be obtained on Turkish tweets.
4 Future Work
Directions of future work based on the current
study include the following:
? Following the points made throughout Sec-
tion 3, several normalization schemes also in-
volving case and diacritics restoration can be
implemented and incorporated into the NER
procedure on tweets.
? Since tweet texts are short and informal, they
often lack contextual clues needed to perform
an efficient NER procedure. Additionally,
there is a tendency to mention new and pop-
ular NEs in tweets which might be missed by
a NER system with static lexical resources.
Hence, extending the lexical resources of
the NER system with contemporary up-to-
date NEs automatically obtained from Turk-
ish news articles can be considered. For this
purpose, we can readily employ resources
like JRC-Names (Steinberger et al., 2011), a
publicly available continuously-updated NE
and name variant dictionary, as a source of
up-to-date NEs in Turkish.
5 Conclusion
In this study, we target the problem of named en-
tity recognition on Turkish tweets. We have car-
ried out experiments starting with a rule-based
recognition system and gradually extended it in
two directions: adapting the rules/resources of
the system and introducing a tweet normalization
scheme into the recognition procedure. Thereby,
we present our findings on named entity recogni-
tion on Turkish tweets in addition to those on the
normalization of Turkish tweets. Based on these
findings, we outline some desirable features of a
named entity recognition system tailored to Turk-
ish tweets. Future work includes the employment
and testing of more elaborate tweet normalization
procedures along the way, on larger tweet data
sets, in addition to evaluating the system after its
resources are automatically extended with dictio-
naries of up-to-date named entities.
Acknowledgments
This study is supported in part by a postdoctoral
research grant from T
?
UB
?
ITAK.
References
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011. Analyzing Temporal Dynamics in Twitter
Profiles for Personalized Recommendations in the
Social Web. In Proceedings of the 3rd ACM Inter-
national Web Science Conference.
Kalina Bontcheva, Leon Derczynski, Adam Funk,
Mark Greenwood, Diana Maynard, and Niraj
Aswani. 2013. TwitIE: An Open-Source Informa-
tion Extraction Pipeline for Microblog Text. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing.
G?okhan C?elikkaya, Dilara Toruno?glu, and G?uls?en
Eryi?git. 2013. Named Entity Recognition on Real
Data: A Preliminary Investigation for Turkish. In
Proceedings of the 7th International Conference
on Application of Information and Communication
Technologies.
G?okhan A. S?eker and G?uls?en Eryi?git. 2012. Ini-
tial Explorations on Using CRFs for Turkish Named
77
Entity Recognition. In Proceedings of the Inter-
national Conference on Computational Linguistics,
pages 2459?2474.
Diego Marinho de Oliveira, Alberto H.F. Laender,
Adriano Veloso, and Altigran S. da Silva. 2013.
FS-NER: A Lightweight Filter-Stream Approach to
Named Entity Recognition on Twitter Data. In Pro-
ceedings of the 22nd International Conference on
World Wide Web Companion, pages 597?604.
Bo Han and Timothy Baldwin. 2011. Lexical Nor-
malisation of Short Text Messages: Makn Sens a
#twitter. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically Constructing a Normalisation Dictio-
nary for Microblogs. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Jason J. Jung. 2012. Online Named Entity Recog-
nition Method for Microtexts in Social Networking
Services: A Case Study of Twitter. Expert Systems
with Applications, 39(9):8066?8070.
Dilek K?uc??uk and Adnan Yaz?c?. 2009. Named En-
tity Recognition Experiments on Turkish Texts. In
T. Andreasen et al., editor, Proceedings of the Inter-
national Conference on Flexible Query Answering
Systems, volume 5822 of Lecture Notes in Computer
Science, pages 524?535.
Dilek K?uc??uk and Adnan Yaz?c?. 2012. A Hybrid
Named Entity Recognizer for Turkish. Expert Sys-
tems with Applications, 39(3):2733?2742.
Dilek K?uc??uk, Guillaume Jacquet, and Ralf Stein-
berger. 2014. Named Entity Recognition on Turkish
Tweets. In Proceedings of the Language Resources
and Evaluation Conference.
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, An-
witaman Datta, Aixin Sun, and Bu-Sung Lee. 2012.
TwiNER: Named Entity Recognition in Targeted
Twitter Stream. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 721?
730.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing Named Entities in
Tweets. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, pages
359?367.
Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu,
and Xiangyang Zhou. 2012. Joint Inference of
Named Entity Recognition and Normalization for
Tweets. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, pages 526?535.
Diana Maynard, Valentin Tablan, Cristan Ursu, Hamish
Cunningham, and Yorick Wilks. 2001. Named en-
tity recognition from diverse text types. In Proceed-
ings of the Conference on Recent Advances in Natu-
ral Language Processing.
Rada F. Mihalcea. 2002. Diacritics Restoration:
Learning from Letters versus Learning from Words.
In Proceedings of the 3rd International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 339?348.
Jakub Piskorski and Maud Ehrmann. 2013. On Named
Entity Recognition in Targeted Twitter Streams in
Polish. In Proceedings of the ACL Workshop on
Balto-Slavic Natural Language Processing.
Bruno Pouliquen and Ralf Steinberger. 2009. Auto-
matic Construction of Multilingual Name Dictionar-
ies. In C. Goutte et al., editor, Learning Machine
Translation, Advances in Neural Information Pro-
cessing Systems Series, pages 59?78. MIT Press.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named Entity Recognition in Tweets: An
Experimental Study. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1524?1534.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open Domain Event Extraction from Twit-
ter. In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 1104?1112.
Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut
?
Ozge. 2002. Development of a Corpus and a Tree-
bank for Present-Day Written Turkish. In Proceed-
ings of the 11th International Conference of Turkish
Linguistics.
Ralf Steinberger, Bruno Pouliquen, Mijail Alexandrov
Kabadjov, Jenya Belyaeva, and Erik Van der Goot.
2011. JRC-Names: A Freely Available, Highly
Multilingual Named Entity Resource. In Proceed-
ings of the Conference on Recent Advances in Natu-
ral Language Processing.
Serhan Tatar and
?
Ilyas C?icekli. 2011. Automatic
Rule Learning Exploiting Morphological Features
for Named Entity Recognition in Turkish. Journal
of Information Science, 37(2):137?151.
G?okhan T?ur, Dilek Hakkani-T?ur, and Kemal Oflazer.
2003. A Statistical Information Extraction Sys-
tem for Turkish. Natural Language Engineering,
9(2):181?210.
Reyyan Yeniterzi. 2011. Exploiting Morphology in
Turkish Named Entity Recognition System. In Pro-
ceedings of the ACL Student Session, pages 105?
110.
Zemberek. 2010. Turkish Unique Word List of Zem-
berek NLP Library for Turkic Languages. Available
at http://zemberek.googlecode.com/
files/full.txt.tr.tar.gz.
78
