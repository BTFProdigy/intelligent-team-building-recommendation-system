Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 261?264,
New York City, June 2006. c?2006 Association for Computational Linguistics
SmartNotes: Implicit Labeling of Meeting Data
through User Note?Taking and Browsing
Satanjeev Banerjee
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
banerjee@cs.cmu.edu
Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
air@cs.cmu.edu
Abstract
We have implemented SmartNotes, a sys-
tem that automatically acquires labeled
meeting data as users take notes during
meetings and browse the notes afterwards.
Such data can enable meeting understand-
ing components such as topic and ac-
tion item detectors to automatically im-
prove their performance over a sequence
of meetings. The SmartNotes system con-
sists of a laptop based note taking appli-
cation, and a web based note retrieval sys-
tem. We shall demonstrate the functional-
ities of this system, and will also demon-
strate the labeled data obtained during typ-
ical meetings and browsing sessions.
1 Goals of the SmartNotes System
Most institutions hold a large number of meetings
every day. Several of these meetings are important,
and meeting participants need to recall the details
of the discussions at a future date. In a previous
survey (Banerjee et al, 2005) of busy professors at
Carnegie Mellon University we showed that meeting
participants needed to recall details of past meetings
on average about twice a month. Performing such
retrieval is not an easy task. It is time consuming;
in our study participants took on average between
15 minutes to an hour to recall the information they
were seeking. Further, the quality of the retrieval
is dependent on whether or not the participants had
access to the notes at the meeting. On a scale of 0
to 5, with 5 denoting complete satisfaction with re-
trieval results, participants reported a satisfaction of
3.4 when they did not have notes, and 4.0 when they
did.
Despite the prevalence of important meetings and
the importance of notes, there is a relative paucity of
technology to help meeting participants take notes
easily at meetings. Some commercial applications
allow users to take notes (e.g. OneNote1) and even
record audio/video (e.g. Quindi2), but no product at-
tempts to automatically take notes. Our long term
goal is to create a system that makes note?taking
easier by performing tasks such as automatically
highlighting portions of the meeting that are likely
to be important to the user, automatically detecting
?note?worthy? phrases spoken during the meeting,
etc.
To perform such note taking, the system needs to
form an understanding of the meeting. Our short
term goal is to create a system that can detect the
topics of discussion, the action items being dis-
cussed, and the roles of the meeting participants.
Additionally, these components must adapt to spe-
cific users and groups of users since different people
will likely take different notes at the same meeting.
Thus we wish to implement the note taking system
in such a way that the user?s interactions with the
system result in labeled meeting data that can then
be used to adapt and improve the meeting under-
standing components.
Towards these goals, we have built SmartNotes
which helps users easily record and retrieve notes.
1http://office.microsoft.com/onenote
2http://www.quindi.com
261
The system also records the user interactions to form
labeled meeting data that can later be used to auto-
matically improve the meeting understanding com-
ponents. In the next section we describe the meeting
understanding components in more detail. Next we
describe SmartNotes itself, and show how it is cur-
rently helping users take and retrieve notes, while
acquiring labeled data to aid each of the meeting un-
derstanding components. Finally we end with a dis-
cussion of what functionality we plan to demonstrate
at the conference.
2 Automatic Meeting Understanding
Topic detection and segmentation: We are at-
tempting to automatically detect the topics being
discussed at meetings. This task consists of two sub-
tasks: discovering the points in a meeting when the
topic changes, and then associating a descriptive la-
bel to the segment between two topic shifts. Our cur-
rent strategy for topic shift detection (Banerjee and
Rudnicky, 2006a) is to perform an edge detection
using such features as speech activity (who spoke
when and for how long), the words that each per-
son spoke, etc. For labeling, we are currently sim-
ply associating the agenda item names recorded in
the notes with the segments they are most relevant
to, as decided by a tf.idf matching technique. Topic
detection is particularly useful during meeting infor-
mation retrieval; (Banerjee et al, 2005) showed that
when users wish to retrieve information from past
meetings, they are typically interested in a specific
discussion topic, as opposed to an entire meeting.
Action item detection: An obvious application
of meeting understanding is the automatic discovery
and recording of action items as they are discussed
during a meeting. Arguably one of the most impor-
tant outcomes of a meeting are the action items de-
cided upon, and automatically recording them could
be a huge benefit especially to those participants that
are likely to not note them down and consequently
forget about them later on.
Meeting participant role detection: Each meet-
ing participant plays a variety of roles in an insti-
tution. These roles can be based on their function
in the institution (managers, assistants, professors,
students, etc), or based on their expertise (speech
recognition experts, facilities experts, etc). Our cur-
rent strategy for role detection (Banerjee and Rud-
nicky, 2006b) is to train detectors on hand labeled
data. Our next step is to perform discovery of new
roles through clustering techniques. Detecting such
roles has several benefits. First, it allows us to build
prior expectations of a meeting between a group of
participants. For example, if we know person A is
a speech recognition expert and person B a speech
synthesis expert, a reasonable expectation is that
when they meet they are likely to talk about tech-
nologies related speech processing. Consequently,
we can use this expectation to aid the action item
detection and the topic detection in that meeting.
3 SmartNotes: System Description
We have implemented SmartNotes to help users
take multi?media notes during meetings, and re-
trieve them later on. SmartNotes consists of two ma-
jor components: The note taking application which
meeting participants use to take notes during the
meeting, and the note retrieval application which
users use to retrieve notes at a later point.
3.1 SmartNotes Note Taking Application
The note taking application is a stand?alone system,
that runs on each meeting participant?s laptop, and
allows him to take notes during the meeting. In ad-
dition to recording the text notes, it also records the
participant?s speech, and video, if a video camera is
connected to the laptop. This system is an extension
of the Carnegie Mellon Meeting Recorder (Banerjee
et al, 2004).
Figure 1 shows a screen?shot of this application.
It is a server?client application, and each participant
logs into a central server at the beginning of each
meeting. Thus, the system knows the precise iden-
tity of each note taker as well as each speaker in
the meeting. This allows us to avoid the onerous
problem of automatically detecting who is speaking
at any time during the meeting. Further, after log-
ging on, each client automatically synchronizes it-
self with a central NTP time server. Thus the time
stamps that each client associates with its recordings
are all synchronized, to facilitate merging and play
back of audio/video during browsing (described in
the next sub?section).
Once logged in, each participant?s note taking
262
Figure 1: Screen shot of the SmartNotes note?taking client
area is split into two sections: a shared note taking
area, and a private note taking area. Notes written
in the shared area are viewable by all meeting par-
ticipants. This allows meeting participants to share
the task of taking notes during a meeting: As long as
one participant has recorded an important point dur-
ing a meeting, the other participants do not need to,
thus making the note taking task easier for the group
as a whole. Private notes that a participant does not
wish to share with all participants can be taken in the
private note taking area.
The interface has a mechanism to allow meeting
participants to insert an agenda into the shared area.
Once inserted, the shared area is split into as many
boxes as there are agenda items. Participants can
then take notes during the discussion of an agenda
item in the corresponding agenda item box. This
is useful to the participants because it organizes the
notes as they are being taken, and, additionally, the
notes can later be retrieved agenda item by agenda
item. Thus, the user can access all notes he has taken
in different meetings regarding ?buying a printer?,
without having to see the notes taken for the other
agenda items in each such meeting.
In addition to being useful to the user, this act of
inserting an agenda and then taking notes within the
relevant agenda item box results in generating (un-
beknownst to the participant) labeled data for the
topic detection component. Specifically, if we de-
fine each agenda item as being a separate ?topic?,
and make the assumption that notes are taken ap-
proximately concurrent with the discussion of the
contents of the notes, then we can conclude that
there is a shift in the topic of discussion at some
point between the time stamp on the last note in
an agenda item box, and the time stamp on the first
note of the next agenda item box. This information
can then be used to improve the performance of the
topic shift detector. The accuracy of the topic shift
data thus acquired depends on the length of time be-
tween the two time points. Since this length is easy
to calculate automatically, this information can be
factored into the topic detector trainer.
The interface also allows participants to enter ac-
tion items through a dedicated action item form.
Again the advantage of such a form to the partici-
pants is that the action items (and thus the notes) are
better organized: After the meeting, they can per-
form retrieval on specific fields of the action items.
For example, they can ask to retrieve all the action
items assigned to a particular participant, or that are
due a particular day, etc.
In addition to being beneficial to the participant,
the action item form filling action results in gener-
ating labeled data for the action item detector.
Specifically, if we make the assumption that an ac-
tion item form filling action is preceded by a discus-
sion of the action item, then the system can couple
the contents of the form with all the speech within
a window of time before the form filling action, and
use this pair as a data point to retrain its action item
detector.
3.2 SmartNotes Note Retrieval Website
As notes and audio/video are recorded on each indi-
vidual participant?s laptop, they also get transferred
over the internet to a central meeting server. This
transfer occurs in the background without any in-
tervention from the user, utilizes only the left?over
bandwidth beyond the user?s current bandwidth us-
age, and is robust to system shut?downs, crashes,
etc. This process is described in more detail in
(Banerjee et al, 2004).
Once the meeting is over and all the data has been
transferred to the central server, meeting participants
can use the SmartNotes multi?media notes retrieval
system to view the notes and access the recorded
audio/video. This is a web?based application that
uses the same login process as the stand?along note
263
Figure 2: Screen shot of the SmartNotes website
taking system. Users can view a list of meetings
they have recorded using the SmartNotes applica-
tion in the past, and then for each meeting, they can
view the shared notes taken at the meeting. Figure
2 shows a screen shot of such a notes browsing ses-
sion. Additionally, participants can view their own
private notes taken during the meeting.
In addition to viewing the notes, they can also ac-
cess all recorded audio/video, indexed by the notes.
That is, they can access the audio/video recorded
around the time that the note was entered. Further
they can specify how many minutes before and af-
ter the note they wish to access. Since the server
has the audio from each meeting participant?s audio
channel, the viewer of the notes can choose to listen
to any one person?s channel, or a combination of the
audio channels. The merging of channels is done in
real time and is achievable because their time stamps
have been synchronized during recording.
In the immediate future we plan to implement a
simple key?word based search on the notes recorded
in all the recorded meetings (or in one specific meet-
ing). This search will return notes that match the
search using a standard tf.idf approach. The user
will also be provided the option of rating the qual-
ity of the search retrieval on a one bit satisfied/not?
satisfied scale. If the user chooses to provide this
rating, it can be used as a feedback to improve the
search. Additionally, which parts of the meeting the
user chooses to access the audio/video from can be
used to form a model of the parts of the meetings
most relevant to the user. This information can help
the system tailor its retrieval to individual prefer-
ences.
4 The Demonstration
We shall demonstrate both the SmartNotes note tak-
ing client as well as the SmartNotes note?retrieval
website. Specifically we will perform 2 minute long
mock meetings between 2 or 3 demonstrators. We
will show how notes can be taken, how agendas can
be created and action items noted. We will then
show how the notes and the audio/video from the 2
minute meeting can be accessed through the Smart-
Notes note retrieval website. We shall also show the
automatically labeled data that gets created both dur-
ing the mock meeting, as well as during the brows-
ing session. Finally, if time permits, we shall show
results on how much we can improve the meeting
understanding components? capabilities through la-
beled meeting data automatically acquired through
participants? use of SmartNotes at CMU and other
institutions that are currently using the system.
References
S. Banerjee and A. I. Rudnicky. 2006a. A texttiling
based approach to topic boundary detection in multi?
participant conversations. Submitted for publication.
S. Banerjee and A. I. Rudnicky. 2006b. You are what
you say: Using meeting participants? speech to detect
their roles and expertise. In Analyzing Conversations
in Text and Speech Workshop at HLT?NAACL 2006,
New York City, USA, June.
S. Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Pato-
dia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso,
A. Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky.
2004. Creating multi-modal, user?centric records of
meetings with the Carnegie Mellon meeting recorder
architecture. In Proceedings of the ICASSP Meeting
Recognition Workshop, Montreal, Canada.
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic?level annotations to meeting
browsing. In Proceedings of the Tenth International
Conference on Human-Computer Interaction, Rome,
Italy, September.
264
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 73?76, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SenseRelate::TargetWord ? A Generalized Framework
for Word Sense Disambiguation
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
sidd@cs.utah.edu
Satanjeev Banerjee
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15213
satanjeev@cmu.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Abstract
We have previously introduced a method
of word sense disambiguation that com-
putes the intended sense of a target word,
using WordNet-based measures of seman-
tic relatedness (Patwardhan et al, 2003).
SenseRelate::TargetWord is a Perl pack-
age that implements this algorithm. The
disambiguation process is carried out by
selecting that sense of the target word
which is most related to the context words.
Relatedness between word senses is mea-
sured using the WordNet::Similarity Perl
modules.
1 Introduction
Many words have different meanings when used in
different contexts. Word Sense Disambiguation is
the task of identifying the intended meaning of a
given target word from the context in which it is
used. (Lesk, 1986) performed disambiguation by
counting the number of overlaps between the dic-
tionary definitions (i.e., glosses) of the target word
and those of the neighboring words in the con-
text. (Banerjee and Pedersen, 2002) extended this
method of disambiguation by expanding the glosses
of words to include glosses of related words, accord-
ing to the structure of WordNet (Fellbaum, 1998).
In subsequent work, (Patwardhan et al, 2003) and
(Banerjee and Pedersen, 2003) proposed that mea-
suring gloss overalps is just one way of determin-
ing semantic relatedness, and that word sense dis-
ambiguation can be performed by finding the most
related sense of a target word to its surrounding con-
text using a wide variety of measures of relatedness.
SenseRelate::TargetWord is a Perl package that
implements these ideas, and is able to disambiguate
a target word in context by finding the sense that is
most related to its neighbors according to a speci-
fied measure. A user of this package is able to make
a variety of choices for text preprocessing options,
context selection, relatedness measure selection and
the selection of an algorithm for computing the over-
all relatedness between each sense of the target word
and words in the surrounding context. The user can
customize each of these choices to fit the needs of
her specific disambiguation task. Further, the vari-
ous sub-tasks in the package are implemented in a
modular fashion, allowing the user to easily replace
a module with her own module if needed.
The following sections describe the generalized
framework for Word Sense Disambiguation, the ar-
chitecture and usage of SenseRelate::TargetWord,
and a description of the user interfaces (command
line and GUI).
2 The Framework
The package has a highly modular architecture. The
disambiguation process is divided into a number of
smaller sub-tasks, each of which is represented by
a separate module. Each of the sequential sub-tasks
or stages accepts data from a previous stage, per-
forms a transformation on the data, and then passes
on the processed data structures to the next stage in
the pipeline. We have created a protocol that defines
the structure and format of the data that is passed
between the stages. The user can create her own
73
Relatedness 
Measure
Context
Target Sense
Preprocessing
Format Filter
Sense Inventory
Context Selection Postprocessing
Pick Sense
Figure 1: A generalized framework for Word Sense Disambiguation.
modules to perform any of these sub-tasks as long
as the modules adhere to the protocol laid down by
the package.
Figure 1 projects an overview of the architecture
of the system and shows the various sub-tasks that
need to be performed to carry out word sense dis-
ambiguation. The sub-tasks in the dotted boxes are
optional. Further, each of the sub-tasks can be per-
formed in a number of different ways, implying that
the package can be customized in a large number of
ways to suit different disambiguation needs.
2.1 Format Filter
The filter takes as input file(s) annotated in the
SENSEVAL-2 lexical sample format, which is an
XML?based format that has been used for both the
SENSEVAL-2 and SENSEVAL-3 exercises. A file in
this format includes a number of instances, each one
made up of 2 to 3 lines of text where a single tar-
get word is designated with an XML tag. The fil-
ter parses the input file to build data structures that
represent the instances to be disambiguated, which
includes a single target word and the surrounding
words that define the context.
2.2 Preprocessing
SenseRelate::TargetWord expects zero or more text
preprocessing modules, each of which perform a
transformation on the input words. For example, the
Compound Detection Module identifies sequences
of tokens that form compound words that are known
as concepts to WordNet (such as ?New York City?).
In order to ensure that compounds are treated as a
single unit, the package replaces them in the instance
with the corresponding underscore?connected form
(?New York City?).
Multiple preprocessing modules can be chained
together, the output of one connected to the input of
the next, to form a single preprocessing stage. For
example, a part of speech tagging module could be
added after compound detection.
2.3 Context Selection
Disambiguation is performed by finding the sense of
the target word that is most related to the words in
its surrounding context. The package allows for var-
ious methods of determining what exactly the sur-
rounding context should consist of. In the current
implementation, the context selection module uses
an n word window around the target word as con-
text. The window includes the target word, and ex-
tends to both the left and right. The module selects
the n? 1 words that are located closest to the target
word, and sends these words (and the target) on to
the next module for disambiguation. Note that these
words must all be known to WordNet, and should
not include any stop?words.
However, not all words in the surrounding context
are indicative of the correct sense of the target word.
An intelligent selection of the context words used in
the disambiguation process could potentially yield
much better results and generate a solution faster
than if all the nearby words were used. For exam-
ple, we could instead select the nouns from the win-
dow of context that have a high term?frequency to
document?frequency ratio. Or, we could identify
lexical chains in the surrounding context, and only
include those words that are found in chains that in-
clude the target word.
74
2.4 Sense Inventory
After having reduced the context to n words, the
Sense Inventory stage determines the possible senses
of each of the n words. This list can be obtained
from a dictionary, such as WordNet. A thesaurus
could also be used for the purpose. Note however,
that the subsequent modules in the pipeline should
be aware of the codes assigned to the word senses.
In our system, this module first decides the base
(uninflected) form of each of the n words. It then
retrieves all the senses for each word from the sense
inventory. We use WordNet for our sense inventory.
2.5 Postprocessing
Some optional processing can be performed on the
data structures generated by the Sense Inventory
module. This would include tasks such as sense
pruning, which is the process of removing some
senses from the inventory, based on simple heuris-
tics, algorithms or options. For example, the user
may decide to preclude all verb senses of the target
word from further consideration in the disambigua-
tion process.
2.6 Identifying the Sense
The disambiguation module takes the lists of senses
of the target word and those of the context words and
uses this information to pick one sense of the tar-
get word as the answer. Many different algorithms
could be used to do this. We have modules Local
and Global that (in different ways) determine the re-
latedness of each of the senses of the target word
with those of the context words, and pick the most
related sense as the answer. These are described
in greater detail by (Banerjee and Pedersen, 2002),
but in general the Local method compares the target
word to its neighbors in a pair-wise fashion, while
the Global method carries out an exhaustive compar-
ison between all the senses of the target word and all
the senses of the neighbors.
3 Using SenseRelate::TargetWord
SenseRelate::TargetWord can be used via the
command-line interface provided by the utility pro-
gram called disamb.pl. It provides a rich variety of
options for controlling the process of disambigua-
tion. Or, it can be embedded into Perl programs,
by including it as a module and calling its various
methods. Finally, there is a graphical interface to
the package that allows a user to highlight a word in
context to be disambiguated.
3.1 Command Line
The command-line interface disamb.pl takes as input
a SENSEVAL-2 formatted lexical sample file. The
program disambiguates the marked up word in each
instance and prints to screen the instance ID, along
with the disambiguated sense of the target word.
Command line options are available to control the
disambiguation process. For example, a user can
specify which relatedness measure they would like
to use, whether disambiguation should be carried out
using Local or Global methods, how large a win-
dow of context around the target word is to be used,
and whether or not all the parts of speech of a word
should be considered.
3.2 Programming Interface
SenseRelate::TargetWord is distributed as a Perl
package. It is programmed in object-oriented Perl
as a group of Perl classes. Objects of these classes
can be instantiated in user programs, and meth-
ods can be called on these objects. The pack-
age requires that the Perl interface to WordNet,
WordNet::QueryData1 be installed on the system.
The disambiguation algorithms also require that the
semantic relatedness measures WordNet::Similarity
(Pedersen et al, 2004) be installed.
3.3 Graphical User Interface
We have developed a graphical interface for the
package in order to conveniently access the disam-
biguation modules. The GUI is written in Gtk-Perl
? a Perl API to the Gtk toolkit. Unlike the command
line interface, the graphical interface is not tied to
any input file format. The interface allows the user to
input text, and to select the word to disambiguate. It
also provides the user with numerous configuration
options corresponding to the various customizations
described above.
1http://search.cpan.org/dist/WordNet-QueryData
75
4 Related Work
There is a long history of work in Word Sense Dis-
ambiguation that uses Machine Readable Dictionar-
ies, and are highly related to our approach.
One of the first approaches was that of (Lesk,
1986), which treated every dictionary definition of
a concept as a bag of words. To identify the in-
tended sense of the target word, the Lesk algorithm
would determine the number of word overlaps be-
tween the definitions of each of the meanings of the
target word, and those of the context words. The
meaning of the target word with maximum defini-
tion overlap with the context words was selected as
the intended sense.
(Wilks et al, 1993) developed a context vector
approach for performing word sense disambigua-
tion. Their algorithm built co-occurrence vectors
from dictionary definitions using Longman?s Dictio-
nary of Contemporary English (LDOCE). They then
determined the extent of overlap between the sum of
the vectors of the words in the context and the sum
of the vectors of the words in each of the definitions
(of the target word). For vectors, the extent of over-
lap is defined as the dot product of the vectors. The
meaning of the target word that had the maximum
overlap was selected as the answer.
More recently, (McCarthy et al, 2004) present a
method that performs disambiguation by determing
the most frequent sense of a word in a particular do-
main. This is based on measuring the relatedness
of the different possible senses of a target word (us-
ing WordNet::Similarity) to a set of words associated
with a particular domain that have been identified
using distributional methods. The relatedness scores
between a target word and the members of this set
are scaled by the distributional similarity score.
5 Availability
SenseRelate::TargetWord is written in Perl and is
freely distributed under the Gnu Public License. It
is available via SourceForge, an Open Source de-
velopment platform2, and the Comprehensive Perl
Archive Network (CPAN)3.
2http://senserelate.sourceforge.net
3http://search.cpan.org/dist/WordNet-SenseRelate-
TargetWord
6 Acknowledgements
This research is partially supported by a National
Science Foundation Faculty Early CAREER Devel-
opment Award (#0092784).
References
S. Banerjee and T. Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using Word-
Net. In Proceedings of the Third International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, Mexico City, February.
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Pro-
ceedings of the Eighteenth International Conference
on Artificial Intelligence (IJCAI-03), Acapulco, Mex-
ico, August.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from a ice cream cone. In Proceedings of SIGDOC
?86.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics (CICLING-03), Mex-
ico City, Mexico, February.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::Similarity - Measuring the Re-
latedness of Concepts. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and
B. Slator. 1993. Providing machine tractable dictio-
nary tools. In J. Pustejovsky, editor, Semantics and
the Lexicon. Kluwer Academic Press, Dordrecht and
Boston.
76
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
METEOR: An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments 
 
 
Satanjeev Banerjee Alon Lavie 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213 Pittsburgh, PA 15213 
banerjee+@cs.cmu.edu alavie@cs.cmu.edu 
Abstract 
We describe METEOR, an automatic 
metric for machine translation evaluation 
that is based on a generalized concept of 
unigram matching between the machine-
produced translation and human-produced 
reference translations. Unigrams can be 
matched based on their surface forms, 
stemmed forms, and meanings; further-
more, METEOR can be easily extended to 
include more advanced matching strate-
gies.  Once all generalized unigram 
matches between the two strings have 
been found, METEOR computes a score 
for this matching using a combination of 
unigram-precision, unigram-recall, and a 
measure of fragmentation that is designed 
to directly capture how well-ordered the 
matched words in the machine translation 
are in relation to the reference.  We 
evaluate METEOR by measuring the cor-
relation between the metric scores and 
human judgments of translation quality.  
We compute the Pearson R correlation 
value between its scores and human qual-
ity assessments of the LDC TIDES 2003 
Arabic-to-English and Chinese-to-English 
datasets.  We perform segment-by-
segment correlation, and show that 
METEOR gets an R correlation value of 
0.347 on the Arabic data and 0.331 on the 
Chinese data.  This is shown to be an im-
provement on using simply unigram-
precision, unigram-recall and their har-
monic F1 combination. We also perform 
experiments to show the relative contribu-
tions of the various mapping modules. 
 
1 Introduction 
Automatic Metrics for machine translation (MT) 
evaluation have been receiving significant atten-
tion in the past two years, since IBM's BLEU met-
ric was proposed and made available (Papineni et 
al 2002).  BLEU and the closely related NIST met-
ric (Doddington, 2002) have been extensively used 
for comparative evaluation of the various MT sys-
tems developed under the DARPA TIDES research 
program, as well as by other MT researchers.  The 
utility and attractiveness of automatic metrics for 
MT evaluation has consequently been widely rec-
ognized by the MT community.  Evaluating an MT 
system using such automatic metrics is much 
faster, easier and cheaper compared to human 
evaluations, which require trained bilingual evalua-
tors.  In addition to their utility for comparing the 
performance of different systems on a common 
translation task, automatic metrics can be applied 
on a frequent and ongoing basis during system de-
velopment, in order to guide the development of 
the system based on concrete performance im-
provements. 
Evaluation of Machine Translation has tradi-
tionally been performed by humans.  While the 
main criteria that should be taken into account in 
assessing the quality of MT output are fairly intui-
tive and well established, the overall task of MT 
evaluation is both complex and task dependent.  
MT evaluation has consequently been an area of 
significant research in itself over the years.  A wide 
range of assessment measures have been proposed, 
not all of which are easily quantifiable. Recently 
developed frameworks, such as FEMTI (King et al 
2003), are attempting to devise effective platforms 
for combining multi-faceted measures for MT 
evaluation in effective and user-adjustable ways.  
While a single one-dimensional numeric metric 
cannot hope to fully capture all aspects of MT 
65
evaluation, such metrics are still of great value and 
utility.  
In order to be both effective and useful, an 
automatic metric for MT evaluation has to satisfy 
several basic criteria.  The primary and most intui-
tive requirement is that the metric have very high 
correlation with quantified human notions of MT 
quality.  Furthermore, a good metric should be as 
sensitive as possible to differences in MT quality 
between different systems, and between different 
versions of the same system.  The metric should be 
consistent (same MT system on similar texts 
should produce similar scores), reliable (MT sys-
tems that score similarly can be trusted to perform 
similarly) and general (applicable to different MT 
tasks in a wide range of domains and scenarios).  
Needless to say, satisfying all of the above criteria 
is extremely difficult, and all of the metrics that 
have been proposed so far fall short of adequately 
addressing most if not all of these requirements.  
Nevertheless, when appropriately quantified and 
converted into concrete test measures, such re-
quirements can set an overall standard by which 
different MT evaluation metrics can be compared 
and evaluated.  
In this paper, we describe METEOR1, an auto-
matic metric for MT evaluation which we have 
been developing.  METEOR was designed to ex-
plicitly address several observed weaknesses in 
IBM's BLEU metric.   It is based on an explicit 
word-to-word matching between the MT output 
being evaluated and one or more reference transla-
tions.  Our current matching supports not only 
matching between words that are identical in the 
two strings being compared, but can also match 
words that are simple morphological variants of 
each other (i.e. they have an identical stem), and 
words that are synonyms of each other.  We envi-
sion ways in which this strict matching can be fur-
ther expanded in the future, and describe these at 
the end of the paper.  Each possible matching is 
scored based on a combination of several features.  
These currently include unigram-precision, uni-
gram-recall, and a direct measure of how out-of-
order the words of the MT output are with respect 
to the reference.   The score assigned to each indi-
vidual sentence of MT output is derived from the 
best scoring match among all matches over all ref-
erence translations.  The maximal-scoring match-
                                                           
1 METEOR: Metric for Evaluation of Translation with Explicit ORdering 
ing is then also used in order to calculate an aggre-
gate score for the MT system over the entire test 
set.  Section 2 describes the metric in detail, and 
provides a full example of the matching and scor-
ing.  
In previous work (Lavie et al, 2004), we com-
pared METEOR with IBM's BLEU metric and it?s 
derived NIST metric, using several empirical 
evaluation methods that have been proposed in the 
recent literature as concrete means to assess the 
level of correlation of automatic metrics and hu-
man judgments.  We demonstrated that METEOR 
has significantly improved correlation with human 
judgments.  Furthermore, our results demonstrated 
that recall plays a more important role than preci-
sion in obtaining high-levels of correlation with 
human judgments.  The previous analysis focused 
on correlation with human judgments at the system 
level.  In this paper, we focus our attention on im-
proving correlation between METEOR score and 
human judgments at the segment level. High-levels 
of correlation at the segment level are important 
because they are likely to yield a metric that is sen-
sitive to minor differences between systems and to 
minor differences between different versions of the 
same system.  Furthermore, current levels of corre-
lation at the sentence level are still rather low, of-
fering a very significant space for improvement.  
The results reported in this paper demonstrate that 
all of the individual components included within 
METEOR contribute to improved correlation with 
human judgments.  In particular, METEOR is 
shown to have statistically significant better corre-
lation compared to unigram-precision, unigram-
recall and the harmonic F1 combination of the two. 
We are currently in the process of exploring 
several further enhancements to the current 
METEOR metric, which we believe have the po-
tential to significantly further improve the sensitiv-
ity of the metric and its level of correlation with 
human judgments.  Our work on these directions is 
described in further detail in Section 4. 
 
2 The METEOR Metric 
2.1 Weaknesses in BLEU Addressed in 
METEOR 
The main principle behind IBM?s BLEU metric 
(Papineni et al 2002) is the measurement of the 
66
overlap in unigrams (single words) and higher or-
der n-grams of words, between a translation being 
evaluated and a set of one or more reference trans-
lations.  The main component of BLEU is n-gram 
precision: the proportion of the matched n-grams 
out of the total number of n-grams in the evaluated 
translation.  Precision is calculated separately for 
each n-gram order, and the precisions are com-
bined via a geometric averaging.  BLEU does not 
take recall into account directly.  Recall ? the pro-
portion of the matched n-grams out of the total 
number of n-grams in the reference translation, is 
extremely important for assessing the quality of 
MT output, as it reflects to what degree the transla-
tion covers the entire content of the translated sen-
tence.  BLEU does not use recall because the 
notion of recall is unclear when matching simulta-
neously against a set of reference translations 
(rather than a single reference).  To compensate for 
recall, BLEU uses a Brevity Penalty, which penal-
izes translations for being ?too short?.  The NIST 
metric is conceptually similar to BLEU in most 
aspects, including the weaknesses discussed below. 
BLEU and NIST suffer from several weak-
nesses, which we attempt to address explicitly in 
our proposed METEOR metric: 
The Lack of Recall:  We believe that the fixed 
brevity penalty in BLEU does not adequately com-
pensate for the lack of recall.  Our experimental 
results strongly support this claim. 
Use of Higher Order N-grams: Higher order 
N-grams are used in BLEU as an indirect measure 
of a translation?s level of grammatical well-
formedness.  We believe an explicit measure for 
the level of grammaticality (or word order) can 
better account for the importance of grammatical-
ity as a factor in the MT metric, and result in better 
correlation with human judgments of translation 
quality. 
Lack of Explicit Word-matching Between 
Translation and Reference:  N-gram counts don?t 
require an explicit word-to-word matching, but this 
can result in counting incorrect ?matches?, particu-
larly for common function words. 
Use of Geometric Averaging of N-grams: 
Geometric averaging results in a score of ?zero? 
whenever one of the component n-gram scores is 
zero.  Consequently, BLEU scores at the sentence 
(or segment) level can be meaningless.  Although 
BLEU was intended to be used only for aggregate 
counts over an entire test-set (and not at the sen-
tence level), scores at the sentence level can be 
useful indicators of the quality of the metric.  In 
experiments we conducted, a modified version of 
BLEU that uses equal-weight arithmetic averaging 
of n-gram scores was found to have better correla-
tion with human judgments. 
2.2 The METEOR Metric 
METEOR was designed to explicitly address the 
weaknesses in BLEU identified above.  It evaluates 
a translation by computing a score based on ex-
plicit word-to-word matches between the transla-
tion and a reference translation. If more than one 
reference translation is available, the given transla-
tion is scored against each reference independ-
ently, and the best score is reported. This is 
discussed in more detail later in this section.   
Given a pair of translations to be compared (a 
system translation and a reference translation), 
METEOR creates an alignment between the two 
strings. We define an alignment as a mapping be-
tween unigrams, such that every unigram in each 
string maps to zero or one unigram in the other 
string, and to no unigrams in the same string. Thus 
in a given alignment, a single unigram in one string 
cannot map to more than one unigram in the other 
string. This alignment is incrementally produced 
through a series of stages, each stage consisting of 
two distinct phases. 
In the first phase an external module lists all the 
possible unigram mappings between the two 
strings. Thus, for example, if the word ?computer? 
occurs once in the system translation and twice in 
the reference translation, the external module lists 
two possible unigram mappings, one mapping the 
occurrence of ?computer? in the system translation 
to the first occurrence of ?computer? in the refer-
ence translation, and another mapping it to the sec-
ond occurrence. Different modules map unigrams 
based on different criteria. The ?exact? module 
maps two unigrams if they are exactly the same 
(e.g. ?computers? maps to ?computers? but not 
?computer?). The ?porter stem? module maps two 
unigrams if they are the same after they are 
stemmed using the Porter stemmer (e.g.: ?com-
puters? maps to both ?computers? and to ?com-
puter?). The ?WN synonymy? module maps two 
unigrams if they are synonyms of each other.  
In the second phase of each stage, the largest 
subset of these unigram mappings is selected such 
67
that the resulting set constitutes an alignment as 
defined above (that is, each unigram must map to 
at most one unigram in the other string). If more 
than one subset constitutes an alignment, and also 
has the same cardinality as the largest set, 
METEOR selects that set that has the least number 
of unigram mapping crosses. Intuitively, if the two 
strings are typed out on two rows one above the 
other, and lines are drawn connecting unigrams 
that are mapped to each other, each line crossing is 
counted as a ?unigram mapping cross?. Formally, 
two unigram mappings (ti, rj) and (tk, rl) (where ti 
and tk are unigrams in the system translation 
mapped to unigrams rj and rl in the reference trans-
lation respectively) are said to cross if and only if 
the following formula evaluates to a negative 
number:  
(pos(ti) ? pos(tk)) * (pos(rj) ? pos(rl)) 
where pos(tx) is the numeric position of the uni-
gram tx in the system translation string, and pos(ry) 
is the numeric position of the unigram ry in the ref-
erence string. For a given alignment, every pair of 
unigram mappings is evaluated as a cross or not, 
and the alignment with the least total crosses is 
selected in this second phase. Note that these two 
phases together constitute a variation of the algo-
rithm presented in (Turian et al 2003). 
Each stage only maps unigrams that have not 
been mapped to any unigram in any of the preced-
ing stages. Thus the order in which the stages are 
run imposes different priorities on the mapping 
modules employed by the different stages. That is, 
if the first stage employs the ?exact? mapping 
module and the second stage employs the ?porter 
stem? module, METEOR is effectively preferring 
to first map two unigrams based on their surface 
forms, and performing the stemming only if the 
surface forms do not match (or if the mapping 
based on surface forms was too ?costly? in terms 
of the total number of crosses). Note that 
METEOR is flexible in terms of the number of 
stages, the actual external mapping module used 
for each stage, and the order in which the stages 
are run. By default the first stage uses the ?exact? 
mapping module, the second the ?porter stem? 
module and the third the ?WN synonymy? module.  
In section 4 we evaluate each of these configura-
tions of METEOR.  
Once all the stages have been run and a final 
alignment has been produced between the system 
translation and the reference translation, the 
METEOR score for this pair of translations is 
computed as follows.  First unigram precision (P) 
is computed as the ratio of the number of unigrams 
in the system translation that are mapped (to uni-
grams in the reference translation) to the total num-
ber of unigrams in the system translation. 
Similarly, unigram recall (R) is computed as the 
ratio of the number of unigrams in the system 
translation that are mapped (to unigrams in the ref-
erence translation) to the total number of unigrams 
in the reference translation. Next we compute 
Fmean by combining the precision and recall via a 
harmonic-mean (van Rijsbergen, 1979) that places 
most of the weight on recall.  We use a harmonic 
mean of P and 9R.  The resulting formula used is: 
PR
PRFmean
9
10
+=  
Precision, recall and Fmean are based on uni-
gram matches. To take into account longer 
matches, METEOR computes a penalty for a given 
alignment as follows. First, all the unigrams in the 
system translation that are mapped to unigrams in 
the reference translation are grouped into the few-
est possible number of chunks such that the uni-
grams in each chunk are in adjacent positions in 
the system translation, and are also mapped to uni-
grams that are in adjacent positions in the reference 
translation. Thus, the longer the n-grams, the fewer 
the chunks, and in the extreme case where the en-
tire system translation string matches the reference 
translation there is only one chunk. In the other 
extreme, if there are no bigram or longer matches, 
there are as many chunks as there are unigram 
matches. The penalty is then computed through the 
following formula: 
3
_#
#*5.0 ???
?
???
?=
matchedunigrams
chunksPenalty  
For example, if the system translation was ?the 
president spoke to the audience? and the reference 
translation was ?the president then spoke to the 
audience?, there are two chunks: ?the president? 
and ?spoke to the audience?. Observe that the pen-
alty increases as the number of chunks increases to 
a maximum of 0.5. As the number of chunks goes 
to 1, penalty decreases, and its lower bound is de-
cided by the number of unigrams matched. The 
parameters if this penalty function were deter-
mined based on some experimentation with de-
68
veopment data, but have not yet been trained to be 
optimal. 
Finally, the METEOR Score for the given 
alignment is computed as follows:  
 
)1(* PenaltyFmeanScore ?=  
 
This has the effect of reducing the Fmean by the 
maximum of 50% if there are no bigram or longer 
matches. 
For a single system translation, METEOR com-
putes the above score for each reference transla-
tion, and then reports the best score as the score for 
the translation. The overall METEOR score for a 
system is calculated based on aggregate statistics 
accumulated over the entire test set, similarly to 
the way this is done in BLEU.  We calculate ag-
gregate precision, aggregate recall, an aggregate 
penalty, and then combine them using the same 
formula used for scoring individual segments. 
3 Evaluation of the METEOR Metric 
3.1. Data 
We evaluated the METEOR metric and compared 
its performance with BLEU and NIST on the 
DARPA/TIDES 2003 Arabic-to-English and Chi-
nese-to-English MT evaluation data released 
through the LDC as a part of the workshop on In-
trinsic and Extrinsic Evaluation Measures for MT 
and/or Summarization, at the Annual Meeting of 
the Association of Computational Linguistics 
(2005). The Chinese data set consists of 920 sen-
tences, while the Arabic data set consists of 664 
sentences. Each sentence has four reference trans-
lations.  Furthermore, for 7 systems on the Chinese 
data and 6 on the Arabic data, every sentence 
translation has been assessed by two separate hu-
man judges and assigned an Adequacy and a Flu-
ency Score.  Each such score ranges from one to 
five (with one being the poorest grade and five the 
highest).  For this paper, we computed a Combined 
Score for each translation by averaging the ade-
quacy and fluency scores of the two judges for that 
translation.  We also computed an average System 
Score for each translation system by averaging the 
Combined Score for all the translations produced 
by that system. (Note that although we refer to 
these data sets as the ?Chinese? and the ?Arabic? 
data sets, the MT evaluation systems analyzed in 
this paper only evaluate English sentences pro-
duced by translation systems by comparing them to 
English reference sentences). 
3.2 Comparison with BLEU and NIST MT 
Evaluation Algorithms  
In this paper, we are interested in evaluating 
METEOR as a metric that can evaluate translations 
on a sentence-by-sentence basis, rather than on a 
coarse grained system-by-system basis. The stan-
dard metrics ? BLEU and NIST ? were however 
designed for system level scoring, hence comput-
ing sentence level scores using BLEU or the NIST 
evaluation mechanism is unfair to those algo-
rithms. To provide a point of comparison however, 
table 1 shows the system level correlation between 
human judgments and various MT evaluation algo-
rithms and sub components of METEOR over the 
Chinese portion of the Tides 2003 dataset. Specifi-
cally, these correlation figures were obtained as 
follows: Using each algorithm we computed one 
score per Chinese system by calculating the aggre-
gate scores produced by that algorithm for that sys-
tem. We also obtained the overall human judgment 
for each system by averaging all the human scores 
for that system?s translations. We then computed 
the Pearson correlation between these system level 
human judgments and the system level scores for 
each algorithm; these numbers are presented in 
table 1.  
 
System ID Correlation 
BLEU 0.817 
NIST 0.892 
Precision 0.752 
Recall 0.941 
F1 0.948 
Fmean 0.952 
METEOR 0.964 
 
Table 1: Comparison of human/METEOR correlation 
with BLEU and NIST/human correlations 
 
Observe that simply using Recall as the MT 
evaluation metric results in a significant improve-
ment in correlation with human judgment over 
both the BLEU and the NIST algorithms. These 
correlations further improve slightly when preci-
sion is taken into account (in the F1 measure), 
69
when the recall is weighed more heavily than pre-
cision (in the Fmean measure) and when a penalty 
is levied for fragmented matches (in the main 
METEOR measure).  
3.3 Evaluation Methodology  
As mentioned in the previous section, our main 
goal in this paper is to evaluate METEOR and its 
components on their translation-by-translation 
level correlation with human judgment. Towards 
this end, in the rest of this paper, our evaluation 
methodology is as follows: For each system, we 
compute the METEOR Score for every translation 
produced by the system, and then compute the cor-
relation between these individual scores and the 
human assessments (average of the adequacy and 
fluency scores) for the same translations. Thus we 
get a single Pearson R value for each system for 
which we have human assessments. Finally we 
average the R values of all the systems for each of 
the two language data sets to arrive at the overall 
average correlation for the Chinese dataset and the 
Arabic dataset. This number ranges between -1.0 
(completely negatively correlated) to +1.0 (com-
pletely positively correlated).  
We compare the correlation between human as-
sessments and METEOR Scores produced above 
with that between human assessments and preci-
sion, recall and Fmean scores to show the advan-
tage of the various components in the METEOR 
scoring function. Finally we run METEOR using 
different mapping modules, and compute the corre-
lation as described above for each configuration to 
show the effect of each unigram mapping mecha-
nism. 
3.4 Correlation between METEOR Scores 
and Human Assessments 
 
System ID Correlation 
ame 0.331 
ara 0.278 
arb 0.399 
ari 0.363 
arm 0.341 
arp 0.371 
Average 0.347 
 
Table 2: Correlation between METEOR Scores and 
Human Assessments for the Arabic Dataset 
We computed sentence by sentence correlation 
between METEOR Scores and human assessments 
(average of adequacy and fluency scores) for each 
translation for every system. Tables 2 and 3 show 
the Pearson R correlation values for each system, 
as well as the average correlation value per lan-
guage dataset.  
 
System ID Correlation 
E09 0.385 
E11 0.299 
E12 0.278 
E14 0.307 
E15 0.306 
E17 0.385 
E22 0.355 
Average 0.331 
 
Table 3: Correlation between METEOR Scores and 
Human Assessments for the Chinese Dataset 
3.5 Comparison with Other Metrics 
We computed translation by translation correla-
tions between human assessments and other met-
rics besides the METEOR score, namely precision, 
recall and Fmean. Tables 4 and 5 show the correla-
tions for the various scores.  
 
Metric Correlation 
Precision 0.287 
Recall 0.334 
Fmean 0.340 
METEOR 0.347 
 
Table 4: Correlations between human assessments and 
precision, recall, Fmean and METEOR Scores, aver-
aged over systems in the Arabic dataset 
 
 
Metric Correlation 
Precision 0.286 
Recall 0.320 
Fmean 0.327 
METEOR 0.331 
 
Table 5: Correlations between human assessments and 
precision, recall, Fmean and METEOR Scores, aver-
aged over systems in the Chinese dataset 
 
We observe that recall by itself correlates with 
human assessment much better than precision, and 
that combining the two using the Fmean formula 
70
described above results in further improvement. By 
penalizing the Fmean score using the chunk count 
we get some further marginal improvement in cor-
relation. 
3.6 Comparison between Different Map-
ping Modules 
To observe the effect of various unigram mapping 
modules on the correlation between the METEOR 
score and human assessments, we ran METEOR 
with different sequences of stages with different 
mapping modules in them. In the first experiment 
we ran METEOR with only one stage that used the 
?exact? mapping module. This module matches 
unigrams only if their surface forms match. (This 
module does not match unigrams that belong to a 
list of ?stop words? that consist mainly of function 
words). In the second experiment we ran 
METEOR with two stages, the first using the ?ex-
act? mapping module, and the second the ?Porter? 
mapping module. The Porter mapping module 
matches two unigrams to each other if they are 
identical after being passed through the Porter 
stemmer. In the third experiment we replaced the 
Porter mapping module with the WN-Stem map-
ping module. This module maps two unigrams to 
each other if they share the same base form in 
WordNet. This can be thought of as a different 
kind of stemmer ? the difference from the Porter 
stemmer is that the word stems are actual words 
when stemmed through WordNet in this manner. 
In the last experiment we ran METEOR with three 
stages, the first two using the exact and the Porter 
modules, and the third the WN-Synonymy map-
ping module.  This module maps two unigrams 
together if at least one sense of each word belongs 
to the same synset in WordNet. Intuitively, this 
implies that at least one sense of each of the two 
words represent the same concept. This can be 
thought of as a poor-man?s synonymy detection 
algorithm that does not disambiguate the words 
being tested for synonymy. Note that the 
METEOR scores used to compute correlations in 
the other tables (1 through 4) used exactly this se-
quence of stages.  
Tables 6 and 7 show the correlations between 
METEOR scores produced in each of these ex-
periments and human assessments for both the 
Arabic and the Chinese datasets. On both data sets, 
adding either stemming modules to simply using 
the exact matching improves correlations. Some 
further improvement in correlation is produced by 
adding the synonymy module.  
 
Mapping module sequence 
used (Arabic) 
Correlation 
Exact 0.312 
Exact, Porter 0.329 
Exact, WN-Stem 0.330 
Exact, Porter, WN-Synonym 0.347 
 
Table 6: Comparing correlations produced by different 
module stages on the Arabic dataset. 
 
Mapping module sequence 
used (Chinese) 
Correlation 
Exact 0.293 
Exact, Porter 0.318 
Exact, WN-Stem 0.312 
Exact, Porter, WN-Synonym 0.331 
 
Table 7: Comparing correlations produced by different 
module stages, on the Chinese dataset 
3.7 Correlation using Normalized Human 
Assessment Scores 
One problem with conducting correlation ex-
periments with human assessment scores at the 
sentence level is that the human scores are noisy ? 
that is, the levels of agreement between human 
judges on the actual sentence level assessment 
scores is not extremely high.  To partially address 
this issue, the human assessment scores were nor-
malized by a group at the MITRE Corporation.  To 
see the effect of this noise on the correlation, we 
computed the correlation between the METEOR 
Score (computed using the stages used in the 4th 
experiment in section 7 above) and both the raw 
human assessments as well as the normalized hu-
man assessments.  
 
 Arabic Dataset 
Chinese 
Dataset 
Raw human as-
sessments 0.347 0.331 
Normalized hu-
man assessments 0.403 0.365 
 
Table 8: Comparing correlations between METEOR 
Scores and both raw and normalized human assessments 
 
71
Table 8 shows that indeed METEOR Scores cor-
relate better with normalized human assessments. 
In other words, the noise in the human assessments 
hurts the correlations between automatic scores 
and human assessments. 
4 Future Work 
The METEOR metric we described and evaluated 
in this paper, while already demonstrating great 
promise, is still relatively simple and na?ve.  We 
are in the process of enhancing the metric and our 
experimentation in several directions: 
Train the Penalty and Score Formulas on 
Data: The formulas for Penalty and METEOR 
score were manually crafted based on empirical 
tests on a separate set of development data. How-
ever, we plan to optimize the formulas by training 
them on a separate data set, and choosing that for-
mula that best correlates with human assessments 
on the training data.  
Use Semantic Relatedness to Map Unigrams:   
So far we have experimented with exact mapping, 
stemmed mapping and synonymy mapping be-
tween unigrams. Our next step is to experiment 
with different measures of semantic relatedness to 
match unigrams that have a related meaning, but 
are not quite synonyms of each other.  
More Effective Use of Multiple Reference 
Translations:  Our current metric uses multiple 
reference translations in a weak way: we compare 
the translation with each reference separately and 
select the reference with the best match.  This was 
necessary in order to incorporate recall in our met-
ric, which we have shown to be highly advanta-
geous.  As our matching approach improves, the 
need for multiple references for the metric may in 
fact diminish.  Nevertheless, we are exploring 
ways in which to improve our matching against 
multiple references.  Recent work by (Pang et al 
2003) provides the mechanism for producing se-
mantically meaningful additional ?synthetic? refer-
ences from a small set of real references.  We plan 
to explore whether using such synthetic references 
can improve the performance of our metric. 
Weigh Matches Produced by Different Mod-
ules Differently: Our current multi-stage approach 
prefers metric imposes a priority on the different 
matching modules. However, once all the stages 
have been run, unigrams mapped through different 
mapping modules are treated the same.  Another 
approach to treating different mappings differently 
is to apply different weights to the mappings pro-
duced by different mapping modules. Thus ?com-
puter? may match ?computer? with a score of 1, 
?computers? with a score of 0.8 and ?workstation? 
with a score of 0.3. As future work we plan to de-
velop a version of METEOR that uses such 
weighting schemes. 
Acknowledgements 
We acknowledge Kenji Sagae and Shyamsundar 
Jayaraman for their work on the METEOR system. 
We also wish to thank John Henderson and Wil-
liam Morgan from MITRE for providing us with 
the normalized human judgment scores used for 
this work. 
References  
George Doddington. 2002. Automatic Evaluation of 
Machine Translation Quality using N-gram Co-
occurrence Statistics.  In Proceedings of 2nd Human 
Language Technologies Conference (HLT-02). San 
Diego, CA. pp. 128-132.  
Margaret King, Andrei Popescu-Belis and Eduard 
Hovy. 2003.  FEMTI: Creating and Using a Frame-
work for MT Evaluation.  In Proceedings of MT 
Summit IX, New Orleans, LA. Sept. 2003. pp. 224-
231. 
Alon Lavie, Kenji Sagae and Shyamsundar Jayaraman, 
2004.  The Significance of Recall in Automatic Met-
rics for MT Evaluation.  In Proceedings of AMTA-
2004, Washington DC.  September 2004. 
Bo Pang, Kevin Knight and Daniel Marcu. 2003.  Syn-
tax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New 
Sentences. In Proceedings of HLT-NAACL 2003.  
Edmonton, Canada. May 2003. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002.  BLEU: a Method for Automatic 
Evaluation of Machine Translation.  In Proceedings 
of the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL-02).  Philadelphia, 
PA. July 2002. pp. 311-318. 
Joseph P. Turian, Luke Shen and I. Dan Melamed. 
2003.  Evaluation of Machine Translation and its 
Evaluation.  In Proceedings of MT Summit IX, New 
Orleans, LA. Sept. 2003.  pp. 386-393. 
C. van Rijsbergen. 1979.  Information Retrieval.  But-
terworths.  London, England. 2nd Edition. 
72
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 23?30,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
You Are What You Say: Using Meeting Participants? Speech
to Detect their Roles and Expertise
Satanjeev Banerjee
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
banerjee@cs.cmu.edu
Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
air@cs.cmu.edu
Abstract
Our goal is to automatically detect the
functional roles that meeting participants
play, as well as the expertise they bring to
meetings. To perform this task, we build
decision tree classifiers that use a combi-
nation of simple speech features (speech
lengths and spoken keywords) extracted
from the participants? speech in meetings.
We show that this algorithm results in a
role detection accuracy of 83% on unseen
test data, where the random baseline is
33.3%. We also introduce a simple aggre-
gation mechanism that combines evidence
of the participants? expertise from multi-
ple meetings. We show that this aggre-
gation mechanism improves the role de-
tection accuracy from 66.7% (when ag-
gregating over a single meeting) to 83%
(when aggregating over 5 meetings).
1 Introduction
A multitude of meetings are organized every day
around the world to discuss and exchange impor-
tant information, to make decisions and to collab-
oratively solve problems. Our goal is to create sys-
tems that automatically understand the discussions
at meetings, and use this understanding to assist
meeting participants in various tasks during and af-
ter meetings. One such task is the retrieval of infor-
mation from previous meetings, which is typically
a difficult and time consuming task for the human
to perform (Banerjee et al, 2005). Another task is
to automatically record the action items being dis-
cussed at meetings, along with details such as when
the action is due, who is responsible for it, etc.
Meeting analysis is a quickly growing field of
study. In recent years, research has focussed on au-
tomatic speech recognition in meetings (Stolcke et
al., 2004; Metze et al, 2004; Hain et al, 2005), ac-
tivity recognition (Rybski and Veloso, 2004), auto-
matic meeting summarization (Murray et al, 2005),
meeting phase detection (Banerjee and Rudnicky,
2004) and topic detection (Galley et al, 2003). Rela-
tively little research has been performed on automat-
ically detecting the roles that meeting participants
play as they participate in meetings. These roles can
be functional (e.g. the facilitator who runs the meet-
ing, and the scribe who is the designated note taker
at the meeting), discourse based (e.g. the presenter,
and the discussion participant), and expertise related
(e.g. the hardware acquisition expert and the speech
recognition research expert). Some roles are tightly
scoped, relevant to just one meeting or even a part
of a meeting. For example, a person can be the fa-
cilitator of one meeting and the scribe of another, or
the same person can be a presenter for one part of
the meeting and a discussion participant for another
part. On the other hand, some roles have a broader
scope and last for the duration of a project. Thus
a single person may be the speech recognition ex-
pert in a project and have that role in all meetings
on that project. Additionally, the same person can
play multiple roles, e.g. the scribe can be a speech
recognition expert too.
Automatic role detection has many benefits, espe-
23
cially when used as a source of constraint for other
meeting understanding components. For example,
detecting the facilitator of the meeting might help
the automatic topic detection module if we know
that facilitators officially change topics and move the
discussion from one agenda item to the next. Know-
ing who the speech recognition expert is can help
the automatic action item detector: If an action item
regarding speech recognition has been detected but
the responsible person field has not been detected,
the module may place a higher probability on the
speech recognition expert as being the responsible
person for that action item. Additionally, detecting
who is an expert in which field can have benefits of
its own. For example, it can be used to automatically
direct queries on a particular subject to the person
deemed most qualified to answer the question, etc.
Basic information such as participant role and ex-
pertise needs to be robustly extracted if it is to be of
use to the more sophisticated stages of understand-
ing. Accordingly, we have based our role detection
algorithm on simple and highly accurate speech fea-
tures, as described in section 5.1.2.
(Banerjee and Rudnicky, 2004) describes the au-
tomatic detection of discourse roles in meetings.
These roles included presenter (participants who
make formal presentations using either slides or
the whiteboard), discussion participant (participants
involved in a discussion marked by frequent turn
changes), observer (participants not speaking, but
nevertheless consuming information during a pre-
sentation or discussion), etc. In this paper we focus
on automatically detecting the functional and exper-
tise based roles that participants play in a meeting.
In the next section we describe the data that is used
in all our role detection work in this paper. In subse-
quent sections we describe the role detection algo-
rithm in more detail, and present evaluation results.
2 The Y2 Meeting Scenario Data
Our research work is part of the Cognitive Assistant
that Learns and Organizes project (CALO, 2003). A
goal of this project is to create an artificial assis-
tant that can understand meetings and use this un-
derstanding to assist meeting participants during and
after meetings. Towards this goal, data is being col-
lected by creating a rich multimodal record of meet-
ings (e.g. (Banerjee et al, 2004)). While a large
part of this data consists of natural meetings (that
would have taken place even if they weren?t being
recorded), a small subset of this data is ?scenario
driven? ? the Y2 Scenario Data.
Meeting # Typical scenario
1 Hiring Joe: Buy a computer and
find office space for him
2 Hiring Cindy and Fred: Buy com?
puters & find office space for them
3 Buy printer for Joe, Cindy and Fred
4 Buy a server machine for Joe,
Cindy and Fred
5 Buy desktop and printer for the
meeting leader
Table 1: Typical Scenario Instructions
The Y2 Scenario Data consists of meetings be-
tween groups of 3 or 4 participants. Each group par-
ticipated in a sequence of up to 5 meetings. Each
sequence had an overall scenario ? the purchasing
of computing hardware and the allocation of office
space for three newly hired employees. Participants
were told to assume that the meetings in the se-
quence were being held one week apart, and that be-
tween any two meetings ?progress? was made on the
action items decided at each meeting. Participants
were given latitude to come up with their own sto-
ries of what ?progress? was made between meetings.
At each meeting, participants were asked to review
progress since the last meeting and make changes to
their decisions if necessary. Additionally, an extra
topic was introduced at each meeting, as shown in
table 1.
In each group of participants, one participant
played the role of the manager who has control over
the funds and makes the final decisions on the pur-
chases. The remaining 2 or 3 participants played the
roles of either the hardware acquisition expert or the
building facilities expert. The role of the hardware
expert was to make recommendations on the buying
of computers and printers, and to actually make the
purchases once a decision was made to do so. Sim-
ilarly the role of the building expert was to make
recommendations on which rooms were available to
fit the new employees into. Despite this role assign-
24
ment, all participants were expected to contribute to
discussions on all topics.
To make the meetings as natural as possible, the
participants were given control over the evolution of
the story, and were also encouraged to create con-
flicts between the manager?s demands and the advice
that the experts gave him. For example, managers
sometimes requested that all three employees be put
in a single office, but the facilities expert announced
that no 3 person room was available, unless the man-
ager was agreeable to pay extra for them. These
conflicts led to extended negotiations between the
participants. To promote fluency, participants were
instructed to use their knowledge of existing facili-
ties and equipment instead of inventing a completely
fictitious set of details (such as room numbers).
The data we use in this paper consists of 8 se-
quences recorded at Carnegie Mellon University and
at SRI International between 2004 and 2005. One of
these sequences has 4 meetings, the remaining have
5 meetings each, for a total of 39 meetings. 4 of
these sequences had a total of 3 participants each;
the remaining 4 sequences had a total of 4 partici-
pants each. On average each meeting was 15 min-
utes long. We partitioned this data into two roughly
equal sets, the training set containing 4 meeting se-
quences, and the test set containing the remaining
4 sets. Although a few participants participated in
multiple meetings, there was no overlap of partici-
pants between the training and the test set.
3 Functional Roles
Meeting participants have functional roles that en-
sure the smooth conduct of the meeting, with-
out regard to the specific contents of the meeting.
These roles may include that of the meeting leader
whose functions typically include starting the meet-
ing, establishing the agenda (perhaps in consulta-
tion with the other participants), making sure the
discussions remain on?agenda, moving the discus-
sion from agenda item to agenda item, etc. Another
possible functional role is that of a the designated
meeting scribe. Such a person may be tasked with
the job of taking the official notes or minutes for the
meeting.
Currently we are attempting to automatically de-
tect the meeting leader for a given meeting. In our
data (as described in section 2) the participant play-
ing the role of the manager is always the meeting
leader. In section 5 we describe our methodology
for automatically detecting the meeting leader.
4 Expertise
Typically each participant in a meeting makes con-
tributions to the discussions at the meeting (and to
the project or organization in general) based on their
own expertise or skill set. For example, a project
to build a multi?modal note taking application may
include project members with expertise in speech
recognition, in video analysis, etc. We define ex-
pertise based roles as roles based on skills that are
relevant to participants? contributions to the meeting
discussions and the project or organization in gen-
eral. Note that the expertise role a participant plays
in a meeting is potentially dependent on the exper-
tise roles of the other participants in the meeting,
and that a single person may play different expertise
roles in different meetings, or even within a single
meeting. For example, a single person may be the
?speech recognition expert? on the note taking appli-
cation project that simply uses off?the?shelf speech
recognition tools to perform note taking, but a ?noise
cancellation? expert on the project that is attempting
to improve the in?house speech recognizer. Auto-
matically detecting each participant?s roles can help
such meeting understanding components as the ac-
tion item detector.
Ideally we would like to automatically discover
the roles that each participant plays, and cluster
these roles into groups of similar roles so that
the meeting understanding components can transfer
what they learn about particular participants to other
(and newer) participants with similar roles. Such a
role detection mechanism would need no prior train-
ing data about the specific roles that participants
play in a new organization or project. Currently
however, we have started with a simplified partici-
pant role detection task where we do have training
data pertinent to the specific roles that meeting par-
ticipants play in the test set of meetings. As men-
tioned in section 2, our data consists of people play-
ing two kinds of expertise?based roles ? that of a
hardware acquisition expert, and that of a building
facilities expert. In the next section we discuss our
25
methodology of automatically detecting these roles
from the meeting participants? speech.
5 Methodology
Given a sequence of longitudinal meetings, we de-
fine our role detection task as a three?way classi-
fication problem, where the input to the classifier
consists of features extracted from the speech of a
particular participant over the given meetings, and
the output is a probability distribution over the three
possible roles. Note that although a single par-
ticipant can simultaneously play both a functional
and an expertise?based role, in the Y2 Scenario
Data each participant plays exactly one of the three
roles. We take advantage of this situation to simplify
the problem to the three way classification defined
above. We induce a decision tree (Quinlan, 1986)
classifier from hand labeled data. In the next sub-
section we describe the steps involved in training the
decision tree role classifier, and in the subsequent
subsection we describe how the trained decision tree
is used to arrive at a role label for each meeting par-
ticipant.
5.1 Training
5.1.1 Keyword List Creation
One of the sources of information that we wish
to employ to perform functional and expertise role
detection is the words that are spoken by each par-
ticipant over the course of the meetings. Our ap-
proach to harness this information source is to use
labeled training data to first create a set of words
most strongly associated with each of the three roles,
and then use only these words during the feature ex-
traction phase to detect each participant?s role, as de-
scribed in section 5.1.2.
We created this list of keywords as follows. Given
a training set of meeting sequences, we aggregated
for each role all the speech from all the participants
who had played that role in the training set. We then
split this data into individual words and removed
stop words ? closed class words (mainly articles and
prepositions) that typically contain less information
pertinent to the task than do nouns and verbs. For all
words across all the three roles, we computed the de-
gree of association between each word and each of
the three roles, using the chi squared method (Yang
and Pedersen, 1997), and chose the top 200 high
scoring word?role pairs. Finally we manually exam-
ined this list of words, and removed additional words
that we deemed to not be relevant to the task (essen-
tially identifying a domain?specific stop list). This
reduced the list to a total of 180 words. The 5 most
frequently occurring words in this list are: computer,
right, need, week and space. Intuitively the goal of
this keyword selection pre?processing step is to save
the decision tree role classifier from having to auto-
matically detect the important words from a much
larger set of words, which would require more data
to train.
5.1.2 Feature Extraction
The input to the decision tree role classifier is a set
of features abstracted from a specific participant?s
speech. One strategy is to extract exactly one set of
features from all the speech belonging to a partici-
pant across all the meetings in the meeting sequence.
However, this approach requires a very large num-
ber of meetings to train. Our chosen strategy is to
sample the speech output by each participant multi-
ple times over the course of the meeting sequence,
classify each such sample, and then aggregate the
evidence over all the samples to arrive at the overall
likelihood that a participant is playing a certain role.
To perform the sampling, we split each meeting
in the meeting sequence into a sequence of contigu-
ous windows each n seconds long, and then compute
one set of features from each participant?s speech
during each window. The value of n is decided
through parametric tests (described in section 7.1).
If a particular participant was silent during the en-
tire duration of a particular window, then features
are extracted from that silence.
Note that in the above formulation, there is no
overlap (nor gap) between successive windows. In
a separate set of experiments we used overlapping
windows. That is, given a window size, we moved
the window by a fixed step size (less than the size
of the window) and computed features from each
such overlapping window. The results of these
experiments were no better than those with non?
overlapping windows, and so for the rest of this pa-
per we simply report on the results with the non?
overlapping windows.
Given a particular window of speech of a partic-
26
ular participant, we extract the following 2 speech
length based features:
? Rank of this participant (among this meet-
ing?s participants) in terms of the length of his
speech during this window. Thus, if this partic-
ipant spoke the longest during the window, he
has a feature value of 1, if he spoke for the sec-
ond longest number of times, he has a feature
value of 2, etc.
? Ratio of the length of speech of this participant
in this window to the total length of speech
from all participants in this window. Thus if
a participant spoke for 3 seconds, and the to-
tal length of speech from all participants in
this window was 6 seconds, his feature value
is 0.5. Together with the rank feature above,
these two features capture the amount of speech
contributed by each participant to the window,
relative to the other participants.
In addition, for each window of speech of a par-
ticular participant, and for each keyword in our list
of pre?decided keywords, we extract the following
2 features:
? Rank of this participant (among this meeting?s
participants) in terms of the number of times
this keyword was spoken. Thus if in this win-
dow of time, this participant spoke the keyword
printer more often than any of the other partic-
ipants, then his feature value for this keyword
is 1.
? Ratio of the number of times this participant
uttered this keyword in this window to the total
number of times this keyword was uttered by
all the participants during this window. Thus
if a participant spoke the word printer 5 times
in this window, and in total all participants said
the word printer 7 times, then his feature value
for this keyword is 5/7. Together with the key-
word rank feature above, these two features
capture the number of times each participant
utters each keyword, relative to the other par-
ticipants.
Thus for each participant, for each meeting win-
dow, we extract two features based on the lengths
of speech, and 2 ? 180 features for each of the 180
keywords, for a total of 362 features. The true output
label for each such data point is the role of that par-
ticipant in the meeting sequence. We used these data
points to induce a classifier using the Weka Java im-
plementation (Witten and Frank, 2000) of the C4.5
decision tree learning algorithm (Quinlan, 1986).
This classifier takes features as described above as
input, and outputs class membership probabilities,
where the classes are the three roles. Note that for
the experiments in this paper we extract these fea-
tures from the manual transcriptions of the speech
of the meeting participants. In the future we plan to
perform these experiments using the transcriptions
output by an automatic speech recognizer.
5.2 Detecting Roles in Unseen Data
5.2.1 Classifying Windows of Unseen Data
Detecting the roles of meeting participants in un-
seen data is performed as follows: First the unseen
test data is split into windows of the same size as was
used during the training regime. Then the speech ac-
tivity and keywords based features are extracted (us-
ing the same keywords as was used during the train-
ing) for each participant in each window. Finally
these data points are used as input into the trained
decision tree, which outputs class membership prob-
abilities for each participant in each window.
5.2.2 Aggregating Evidence to Assign One Role
Per Participant
Thus for each participant we get as many proba-
bility distributions (over the three roles) as there are
windows in the test data. The next step is to aggre-
gate these probabilities over all the windows and ar-
rive at a single role assignment per participant. We
employ the simplest possible aggregation method:
We compute, for each participant, the average prob-
ability of each role over all the windows, and then
normalize the three average role probabilities so cal-
culated, so they still sum to 1. In the future we plan
to experiment with more sophisticated aggregation
mechanisms that jointly optimize the probabilities of
the different participants, instead of computing them
independently.
At this point, we could assign to each participant
his highest probability role. However, we wish to
ensure that the set of roles that get assigned to the
27
participants in a particular meeting are as diverse
as possible (since typically meetings are forums at
which different people of different expertise con-
vene to exchange information). To ensure such di-
versity, we apply the following heuristic. Once we
have all the average probabilities for all the roles for
each participant in a sequence of meetings, we as-
sign roles to participants in stages. At each stage
we consider all participants not yet assigned roles,
and pick that participant?role pair, say (p, r), that
has the highest probability value among all pairs un-
der consideration. We assign participant p the role r,
and then discount (by a constant multiplicative fac-
tor) the probability value of all participant?role pairs
(pi, rj) where pi is a participant not assigned a role
yet, and rj = r. This makes it less likely (but not
impossible) that another participant will be assigned
this same role r again. This process is repeated until
all participants have been assigned a role each.
6 Evaluation
We evaluated the algorithm by computing the accu-
racy of the detector?s role predictions. Specifically,
given a meeting sequence we ran the algorithm to
assign a role to each meeting participant, and com-
puted the accuracy by calculating the ratio of the
number of correct assignments to the total number
of participants in the sequence. Note that it is also
possible to evaluate the window?by?window clas-
sification of the decision tree classifiers; we report
results on this evaluation in section 7.1.
To evaluate this participant role detection algo-
rithm, we first trained the algorithm on the training
set of meetings. The training phase included key-
word list creation, window size optimization, and
the actual induction of the decision tree. On the
training data, a window size of 300 seconds resulted
in the highest accuracy over the training set. The test
at the root of the induced tree was whether the par-
ticipant?s rank in terms of speech lengths was 1, in
which case he was immediately classified as a meet-
ing leader. That is, the tree learnt that the person
who spoke the most in a window was most likely
the meeting leader. Other tests placed high in the
tree included obvious ones such as testing for the
keywords computer and printer to classify a partici-
pant as a hardware expert.
We then tested this trained role detector on the
testing set of meetings. Recall that the test set had
5 meeting sequences, each consisting of 5 meetings
and a total of 20 meeting participants. Over this test
set we obtained a role detection accuracy of 83%.
A ?classifier? that randomly assigns one of the three
roles to each participant in a meeting (without re-
gard to the roles assigned to the other participants in
the same meeting) would achieve a classification ac-
curacy of 33.3%. Thus, our algorithm significantly
beats the random classifier baseline. Note that as
mentioned earlier, the experiments in this paper are
based on the manually transcribed speech.
7 Further Experiments
7.1 Optimizing the Window Size
As mentioned above, one of the variables to be tuned
during the training phase is the size of the window
over which to extract speech features. We ran a se-
quence of experiments to optimize this window size,
the results of which are summarized in figure 1. In
this set of experiments, we performed the evaluation
on two levels of granularity. The larger granularity
level was the ?meeting sequence? granularity, where
we ran the usual evaluation described above. That
is, for each participant we first used the classifier to
obtain probability distributions over the 3 roles on
every window, and then aggregated these distribu-
tions to reach a single role assignment for the par-
ticipant over the entire meeting sequence. This role
was compared to the true role of the participant to
measure the accuracy of the algorithm. The smaller
granularity level was the ?window? level, where af-
ter obtaining the probability distribution over the
three roles for a particular window of a particu-
lar participant, we picked the role with the high-
est probability, and assigned it to the participant for
that window. Therefore, for each window we had
a role assignment that we compared to the true role
of the participant, resulting in an accuracy value for
the classifier for every window for every participant.
Note that the main difference between evaluation at
these two granularity levels is that in the ?window?
granularity, we did not have any aggregation of evi-
dence across multiple windows.
For different window sizes, we plotted the accu-
racy values obtained on the test set for the two evalu-
28
 40
 50
 60
 70
 80
 90
 100
 0  100  200  300  400  500  600  700  800  900  1000
Ac
cu
ra
cy
Window Size
Accuracy meeting sequence
Accuracy window
Figure 1: Effect of Different Window Sizes on Detection Ac-
curacy
ation granularities, as shown in figure 1. Notice that
by aggregating the evidence across the windows, the
detection accuracy improves for all window sizes.
This is to be expected since in the window gran-
ularity, the classifier has access to only the infor-
mation contained in a single window, and is there-
fore more error prone. However by merging the ev-
idence from many windows, the accuracy improves.
As window sizes increase, detection accuracy at the
window level improves, because the classifier has
more evidence at its disposal to make the decision.
However, detection at the meeting sequence level
gets steadily worse, potentially because the larger
the window size, the fewer the data points it has to
aggregate evidence from. These lines will eventu-
ally meet when the window size equals the size of
the entire meeting sequence.
A valid concern with these results is the high level
of noise, particularly in the aggregated detection ac-
curacy over the meeting sequence. One reason for
this is that there are far fewer data points at the meet-
ing sequence level than at the window level. With
larger data sets (more meeting sequences as well as
more participants per meeting) these results may sta-
bilize. Additionally, given the small amount of data,
our feature set is quite large, so a more aggressive
feature set reduction might help stabilize the results.
7.2 Automatic Improvement over Unseen Data
One of our goals is to create an expertise based role
detector system that improves over time as it has ac-
cess to more and more meetings for a given par-
 40
 50
 60
 70
 80
 90
 100
 1  2  3  4  5
Ac
cu
ra
cy
Window Size
Accuracy
Figure 2: Accuracy versus Number of Meetings over which
Roles were Detected
ticipant. This is especially important because the
roles that a participant plays can change over time;
we would like our system to be able to track these
changes. In the Y2 Scenario Data that we have used
in this current work, the roles do not change from
meeting to meeting. However observe that our evi-
dence aggregation algorithm fuses information from
all the meetings in a specific sequence of meetings
to arrive at a single role assignment for each partici-
pant.
To quantify the effect of this aggregation we com-
puted the role detection accuracy using different
numbers of meetings from each sequence. Specif-
ically, we computed the accuracy of the role detec-
tion over the test data using only the last meeting of
each sequence, only the last 2 meetings of each se-
quence, and so on until we used every meeting in ev-
ery sequence. The results are summarized in figure
2. When using only the last meeting in the sequence
to assign roles to the participants, the accuracy is
only 66.7%, when using the last two meetings, the
accuracy is 75%, and using the last three, four or
all meetings results in an accuracy of 83%. Thus,
the accuracy improves as we have more meetings to
combine evidence from, as is expected. However
the accuracy levels off at 83% when using three or
more meetings, perhaps because there is no new in-
formation to be gained by adding a fourth or a fifth
meeting.
8 Conclusions and Future Work
In this paper we have discussed our current approach
to detecting the functional and expertise based roles
of meeting participants. We have induced decision
29
trees that use simple and robust speech based fea-
tures to perform the role detection. We have used
a very simple evidence aggregation mechanism to
arrive at a single role assignment per meeting partic-
ipant over a sequence of meetings, and have shown
that we can achieve up to 83% accuracy on unseen
test data using this mechanism. Additionally we
have shown that by aggregating evidence across a
sequence of meetings, we perform better than if we
were to use a single meeting to perform the role de-
tection. As future work we plan to remove the con-
straints that we have currently imposed ? namely, we
will attempt to learn new roles in test data that do not
exist in training data. Additionally, we will attempt
to use this role information as inputs to downstream
meeting understanding tasks such as automatic topic
detection and action item detection.
9 Acknowledgements
This work was supported by DARPA grant NBCH-
D-03-0010. The content of the information in this
publication does not necessarily reflect the position
or the policy of the US Government, and no official
endorsement should be inferred.
References
S. Banerjee and A. I. Rudnicky. 2004. Using simple
speech-based features to detect the state of a meet-
ing and the roles of the meeting participants. In Pro-
ceedings of the 8th International Conference on Spo-
ken Language Processing (Interspeech 2004 ? ICSLP),
Jeju Island, Korea.
S. Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Pato-
dia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso,
A. Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky.
2004. Creating multi-modal, user?centric records of
meetings with the Carnegie Mellon meeting recorder
architecture. In Proceedings of the ICASSP Meeting
Recognition Workshop, Montreal, Canada.
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic?level annotations to meeting
browsing. In Proceedings of the Tenth International
Conference on Human-Computer Interaction, Rome,
Italy, September.
CALO. 2003. http://www.ai.sri.com/project/CALO.
M. Galley, K. McKeown, E. Fosler-Lussier, and Hongyan
Jing. 2003. Discourse segmentation of multi?party
conversation. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics, vol-
ume 1, pages 562 ? 569, Sapporo, Japan.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, and S. Renals. 2005. Transcrip-
tion of conference room meetings: An investigation.
In Proceedings of Interspeech 2005, Lisbon, Portugal,
September.
F. Metze, Q. Jin, C. Fugen, K. Laskowski, Y. Pan, and
T. Schultz. 2004. Issues in meeting transcription ?
the isl meeting transcription system. In Proceedings of
the 8th International Conference on Spoken Language
Processing (Interspeech 2004 ? ICSLP), Jeju Island,
Korea.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. In Proceedings
of Interspeech 2005, Lisbon, Portugal, September.
J. Quinlan. 1986. Induction of decision trees. Machine
Learning, 1:81?106.
Paul E. Rybski and Manuela M. Veloso. 2004. Using
sparse visual data to model human activities in meet-
ings. In Workshop on Modeling Other Agents from
Observations, International Joint Conference on Au-
tonomous Agents and Multi-Agent Systems.
A. Stolcke, C. Wooters, N. Mirghafori, T. Pirinen, I. Bu-
lyko, D. Gelbart, M. Graciarena, S. Otterson, B. Pe-
skin, and M. Ostendorf. 2004. Progress in meeting
recognition: The icsi?sri?uw spring 2004 evaluation
system. In NIST RT04 Meeting Recognition Work-
shop, Montreal.
I. Witten and E. Frank. 2000. Data Mining - Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan?Kaufmann, San Francisco,
CA.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proceedings
of the International Conference on Machine Learn-
ing, pages 412?420, Nashville, US. Morgan Kauf-
mann Publishers.
30
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 390?393,
Prague, June 2007. c?2007 Association for Computational Linguistics
UMND1: Unsupervised Word Sense Disambiguation Using Contextual
Semantic Relatedness
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112.
sidd@cs.utah.edu
Satanjeev Banerjee
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15217.
banerjee@cs.cmu.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812.
tpederse@d.umn.edu
Abstract
In this paper we describe an unsuper-
vised WordNet-based Word Sense Disam-
biguation system, which participated (as
UMND1) in the SemEval-2007 Coarse-
grained English Lexical Sample task. The
system disambiguates a target word by using
WordNet-based measures of semantic relat-
edness to find the sense of the word that
is semantically most strongly related to the
senses of the words in the context of the tar-
get word. We briefly describe this system,
the configuration options used for the task,
and present some analysis of the results.
1 Introduction
WordNet::SenseRelate::TargetWord1 (Patwardhan
et al, 2005; Patwardhan et al, 2003) is an unsuper-
vised Word Sense Disambiguation (WSD) system,
which is based on the hypothesis that the intended
sense of an ambiguous word is related to the
words in its context. For example, if the ?financial
institution? sense of bank is intended in a context,
then it is highly likely the context would contain
related words such as money, transaction, interest
rate, etc. The algorithm, therefore, determines
the intended sense of a word (target word) in a
given context by measuring the relatedness of each
sense of that word with the words in its context.
The sense of the target word that is most related
to its context is selected as the intended sense of
the target word. The system uses WordNet-based
1http://senserelate.sourceforge.net
measures of semantic relatedness2 (Pedersen et
al., 2004) to measure the relatedness between the
different senses of the target word and the words in
its context.
This system is completely unsupervised and re-
quires no annotated data for training. The lexical
database WordNet (Fellbaum, 1998) is the only re-
source that the system uses to measure the related-
ness between words and concepts. Thus, our system
is classified under the closed track of the task.
2 System Description
Our WSD system consists of a modular framework,
which allows different algorithms for the different
subtasks to be plugged into the system. We divide
the disambiguation task into two primary subtasks:
context selection and sense selection. The context
selection module tries to select words from the con-
text that are most likely to be indicative of the sense
of the target word. The sense selection module then
uses the set of selected context words to choose one
of the senses of the target word as the answer.
Figure 1 shows a block schematic of the system,
which takes SemEval-2007 English Lexical Sample
instances as input. Each instance is a made up of
a few English sentences, and one word from these
sentences is marked as the target word to be dis-
ambiguated. The system processes each instance
through multiple modules arranged in a sequential
pipeline. The final output of the pipeline is the sense
that is most appropriate for the target word in the
given context.
2http://wn-similarity.sourceforge.net
390
Instance
Preprocessing
Format Filter
Target Sense
Context Selection
Postprocessing
Sense Selection Relatedness Measure
Figure 1: System Architecture
2.1 Data Preparation
The input text is first passed through a format fil-
ter, whose task is to parse the input XML file. This
is followed by a preprocessing step. Each instance
passed to the preprocessing stage is first segmented
into words, and then all compound words are iden-
tified. Any sequence of words known to be a com-
pound in WordNet is combined into a single entity.
2.2 Context Selection
Although each input instance consists of a large
number of words, only a few of these are likely to
be useful for disambiguating the target word. We
use the context selection algorithm to select a subset
of the context words to be used for sense selection.
By removing the unimportant words, the computa-
tional complexity of the algorithm is reduced.
In this work, we use the NearestWords context
selection algorithm. This algorithm algorithm se-
lects 2n + 1 content words surrounding the target
word (including the target word) as the context. A
stop list is used to identify closed-class non-content
words. Additionally, any word not found in Word-
Net is also discarded. The algorithm then selects n
content words before and n content words follow-
ing the target word, and passes this unordered set of
2n + 1 words to the Sense Selection module.
2.3 Sense Selection Algorithm
The sense selection module takes the set of words
output by the context selection module, one of which
is the target word to be disambiguated. For each of
the words in this set, it retrieves a list of senses from
WordNet, based on which it determines the intended
sense of the target word.
The package provides two main algorithms for
Sense Selection: the local and the global algorithms,
as described in previous work (Banerjee and Peder-
sen, 2002; Patwardhan et al, 2003). In this work,
we use the local algorithm, which is faster and was
shown to perform as well as the global algorithm.
The local sense selection algorithm measures the
semantic relatedness of each sense of the target word
with the senses of the words in the context, and se-
lects that sense of the target word which is most re-
lated to the context word-senses. Given the 2n + 1
context words, the system scores each sense of the
target word. Suppose the target word t has T senses,
enumerated as t1, t2, . . . , tT . Also, suppose w1, w2,
. . . , w2n are the words in the context of t, each hav-
ing W1, W2, . . . , W2n senses, respectively. Then for
each ti a score is computed as
score(ti) =
2n
?
j=1
max
k=1 to Wj
(relatedness(ti, wjk))
where wjk is the kth sense of word wj . The sense ti
of target word t with the highest score is selected as
the intended sense of the target word.
The relatedness between two word senses is com-
puted using a measure of semantic relatedness de-
fined in the WordNet::Similarity software package
(Pedersen et al, 2004), which is a suite of Perl mod-
ules implementing a number WordNet-based mea-
sures of semantic relatedness. For this work, we
used the Context Vector measure (Patwardhan and
Pedersen, 2006). The relatedness of concepts is
computed based on word co-occurrence statistics
derived from WordNet glosses. Given two WordNet
senses, this module returns a score between 0 and 1,
indicating the relatedness of the two senses.
Our system relies on WordNet as its sense inven-
tory. However, this task used OntoNotes (Hovy et
al., 2006) as the sense inventory. OntoNotes word
senses are groupings of similar WordNet senses.
Thus, we used the training data answer key to gen-
erate a mapping between the OntoNotes senses of
the given lexical elements and their corresponding
WordNet senses. We had to manually create the
mappings for some of the WordNet senses, which
had no corresponding OntoNotes senses. The sense
selection algorithm performed all of its computa-
tions with respect to the WordNet senses, and finally
the OntoNotes sense corresponding to the selected
WordNet sense of the target word was output as the
391
answer for each instance.
3 Results and Analysis
For this task, we used the freely available Word-
Net::SenseRelate::TargetWord v0.10 and the Word-
Net::Similarity v1.04 packages. WordNet v2.1 was
used as the underlying knowledge base for these.
The context selection module used a window size
of five (including the target word). The semantic re-
latedness of concepts was measured using the Con-
text Vector measure, with configuration options as
defined in previous research (Patwardhan and Ped-
ersen, 2006). Since we always predict exactly one
sense for each instance, the precision and recall val-
ues of all our experiments were always the same.
Therefore, in this section we will use the name ?ac-
curacy? to mean both precision and recall.
3.1 Overall Results, and Baselines
The overall accuracy of our system on the test data
is 0.538. This represents 2,609 correctly disam-
biguated instances, out of a total of 4,851 instances.
As baseline, we compare against the random al-
gorithm where for each instance, we randomly pick
one of the WordNet senses for the lexical element
in that instance, and report the OntoNotes senseid it
maps to as the answer. This algorithm gets an ac-
curacy of 0.417. Thus, our algorithm gets an im-
provement of 12% absolute (29% relative) over this
random baseline.
Additionally, we compare our algorithm against
the WordNet SenseOne algorithm. In this algorithm,
we pick the first sense among the WordNet senses
of the lexical element in each instance, and report
its corresponding OntoNotes sense as the answer for
that instance. This algorithm leverages the fact that
(in most cases) the WordNet senses for a particular
word are listed in the database in descending order
of their frequency of occurrence in the corpora from
which the sense inventory was created. If the new
test data has a similar distribution of senses, then this
algorithm amounts to a ?majority baseline?. This
algorithm achieves an accuracy of 0.681 which is
15% absolute (27% relative) better than our algo-
rithm. Although this seemingly na??ve algorithm out-
performs our algorithm, we choose to avoid using
this information in our algorithms because it repre-
sents a large amount of human supervision in the
form of manual sense tagging of text, whereas our
goal is to create a purely unsupervised algorithm.
Additionally, our algorithms can, with little change,
work with other sense inventories besides WordNet
that may not have this information.
3.2 Results Disaggregated by Part of Speech
In our past experience, we have found that av-
erage disambiguation accuracy differs significantly
between words of different parts of speech. For the
given test data, we separately evaluated the noun and
verb instances. We obtained an accuracy of 0.399
for the noun targets and 0.692 for the verb targets.
Thus, we find that our algorithm performs much bet-
ter on verbs than on nouns, when evaluated using the
OntoNotes sense inventory. This is different from
our experience with SENSEVAL data from previous
years where performance on nouns was uniformly
better than that on verbs. One possible reason for the
better performance on verbs is that the OntoNotes
sense inventory has, on average, fewer senses per
verb word (4.41) than per noun word (5.71). How-
ever, additional experimentation is needed to more
fully understand the difference in performance.
3.3 Results Disaggregated by Lexical Element
To gauge the accuracy of our algorithm on different
words (lexical elements), we disaggregated the re-
sults by individual word. Table 1 lists the accuracy
values over instances of individual verb lexical ele-
ments, and Table 2 lists the accuracy values for noun
lexical elements. Our algorithm gets all instances
correct for 13 verb lexical elements, and for none of
the noun lexical elements. More generally, our al-
gorithm gets an accuracy of 50% or more on 45 out
of the 65 verb lexical elements, and on 15 out of the
35 noun lexical elements. For nouns, when the ac-
curacy results are viewed in sorted order (as in Table
2), one can observe a sudden degradation of results
between the accuracy of the word system.n ? 0.443
? and the word source.n ? 0.257. It is unclear why
there is such a jump; there is no such sudden degra-
dation in the results for the verb lexical elements.
4 Conclusions
This paper describes our system UMND1, which
participated in the SemEval-2007 Coarse-grained
392
Word Accuracy Word Accuracy
remove 1.000 purchase 1.000
negotiate 1.000 improve 1.000
hope 1.000 express 1.000
exist 1.000 estimate 1.000
describe 1.000 cause 1.000
avoid 1.000 attempt 1.000
affect 1.000 say 0.969
explain 0.944 complete 0.938
disclose 0.929 remember 0.923
allow 0.914 announce 0.900
kill 0.875 occur 0.864
do 0.836 replace 0.800
maintain 0.800 complain 0.786
believe 0.764 receive 0.750
approve 0.750 buy 0.739
produce 0.727 regard 0.714
propose 0.714 need 0.714
care 0.714 feel 0.706
recall 0.667 examine 0.667
claim 0.667 report 0.657
find 0.607 grant 0.600
work 0.558 begin 0.521
build 0.500 keep 0.463
go 0.459 contribute 0.444
rush 0.429 start 0.421
raise 0.382 end 0.381
prove 0.364 enjoy 0.357
see 0.296 set 0.262
promise 0.250 hold 0.250
lead 0.231 prepare 0.222
join 0.222 ask 0.207
come 0.186 turn 0.048
fix 0.000
Table 1: Verb Lexical Element Accuracies
English Lexical Sample task. The system is based
on WordNet::SenseRelate::TargetWord, which is a
freely available unsupervised Word Sense Disam-
biguation software package. The system uses
WordNet-based measures of semantic relatedness to
select the intended sense of an ambiguous word. The
system required no training data and using WordNet
as its only knowledge source achieved an accuracy
of 54% on the blind test set.
Acknowledgments
This research was partially supported by a National
Science Foundation Early CAREER Development
award (#0092784).
References
S. Banerjee and T. Pedersen. 2002. An Adapted Lesk Al-
gorithm for Word Sense Disambiguation Using Word-
Net. In Proceedings of the Third International Con-
Word Accuracy Word Accuracy
policy 0.949 people 0.904
future 0.870 drug 0.870
space 0.857 capital 0.789
effect 0.767 condition 0.765
job 0.692 bill 0.686
area 0.676 base 0.650
management 0.600 power 0.553
development 0.517 chance 0.467
exchange 0.459 order 0.456
part 0.451 president 0.446
system 0.443 source 0.257
network 0.218 state 0.208
share 0.192 rate 0.186
hour 0.167 plant 0.109
move 0.085 point 0.080
value 0.068 defense 0.048
position 0.044 carrier 0.000
authority 0.000
Table 2: Noun Lexical Element Accuracies
ference on Intelligent Text Processing and Computa-
tional Linguistics, pages 136?145, Mexico City, Mex-
ico, February.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the ACL, pages 57?60, New York, NY, June.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8, Trento, Italy, April.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing Measures of Semantic Relatedness for Word Sense
Disambiguation. In Proceedings of the Fourth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, pages 241?257, Mex-
ico City, Mexico, February.
S. Patwardhan, T. Pedersen, and S. Banerjee. 2005.
SenseRelate::TargetWord - A Generalized Framework
for Word Sense Disambiguation. In Proceedings of
the Twentieth National Conference on Artificial In-
telligence (Intelligent Systems Demonstrations), pages
1692?1693, Pittsburgh, PA, July.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics Demonstrations, pages
38?41, Boston, MA, May.
393
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 71?78,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Detecting the Noteworthiness of Utterances in Human Meetings 
 
 
Satanjeev Banerjee 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
banerjee@cs.cmu.edu  
Alexander I. Rudnicky 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
air@cs.cmu.edu 
  
 
Abstract 
Our goal is to make note-taking easier in 
meetings by automatically detecting 
noteworthy utterances in verbal ex-
changes and suggesting them to meeting 
participants for inclusion in their notes. 
To show feasibility of such a process we 
conducted a Wizard of Oz study where 
the Wizard picked automatically tran-
scribed utterances that he judged as 
noteworthy, and suggested their contents 
to the participants as notes. Over 9 meet-
ings, participants accepted 35% of these 
suggestions. Further, 41.5% of their notes 
at the end of the meeting contained Wi-
zard-suggested text. Next, in order to per-
form noteworthiness detection automati-
cally, we annotated a set of 6 meetings 
with a 3-level noteworthiness annotation 
scheme, which is a break from the binary 
?in summary?/ ?not in summary? labe-
ling typically used in speech summariza-
tion. We report Kappa of 0.44 for the 3-
way classification, and 0.58 when two of 
the 3 labels are merged into one. Finally, 
we trained an SVM classifier on this an-
notated data; this classifier?s performance 
lies between that of trivial baselines and 
inter-annotator agreement.  
1 Introduction 
We regularly exchange information verbally with 
others over the course of meetings. Often we 
need to access this information afterwards. Typi-
cally we record the information we consider im-
portant by taking notes. Note taking at meetings 
is a difficult task, however, because the partici-
pant must summarize and write down the infor-
mation in a way such that it is comprehensible 
afterwards, while paying attention to and partici-
pating in the ongoing discussion. Our goal is to 
make note-taking easier by automatically extract-
ing noteworthy items from spoken interactions in 
real time, and proposing them to the humans for 
inclusion in their notes.   
Judging which pieces of information in a 
meeting are noteworthy is a very subjective task. 
The subjectivity of this task is likely to be more 
acute than even that of meeting summarization, 
where low inter-annotator agreement is typical 
e.g. (Galley, 2006), (Liu & Liu, 2008), (Penn & 
Zhu, 2008), etc ? whether a piece of information 
should be included in a participant?s notes de-
pends not only on its importance, but also on 
factors such as the participant?s need to remem-
ber, his perceived likelihood of forgetting, etc. 
To investigate whether it is feasible even for a 
human to predict what someone else might find 
noteworthy in a meeting, we conducted a Wizard 
of Oz-based user study where a human suggested 
notes (with restriction) to meeting participants 
during the meeting. We concluded from this 
study (presented in section 2) that this task ap-
pears to be feasible for humans.  
Assuming feasibility, we then annotated 6 
meetings with a 3-level noteworthiness scheme. 
Having 3 levels instead of the typical 2 allows us 
to explicitly separate utterances of middling 
noteworthiness from those that are definitely 
noteworthy or not noteworthy, and allows us to 
encode more human knowledge than a 2-level 
scheme. We describe this annotation scheme in 
more detail in section 3, and show high inter-
annotator agreement compared to that typically 
reported in the summarization literature. Finally 
in sections 4 and 5 we use this annotated data to 
train and test a simple Support Vector Machine-
based predictor of utterance noteworthiness. 
2 Can Humans Do this Task?  
As mentioned in the introduction, given the de-
gree of subjectivity involved in identifying note-
71
worthy utterances, it is reasonable to ask whether 
the notes-suggestion task can be accomplished 
by humans, let alne by automatic systems. That 
is, we ask the question: Is it possible for a human 
to identify noteworthy utterances in a meeting 
such that  
(a) For at least some fraction of the suggestions, 
one or more meeting participants agree that 
the suggested notes should indeed be in-
cluded in their notes, and 
(b) The fraction of suggested notes that meeting 
participants find noteworthy is high enough 
that, over a sequence of meetings, the meet-
ing participants do not learn to simply ignore 
the suggestions.  
Observe that this task is more restricted than that 
of generic note-taking. While a human who is 
allowed to summarize discussions and produce 
to-the-point notes is likely to be useful, we as-
sume here that our system will not be able to 
create such abstractive summaries. Rather, our 
goal here is to explore the feasibility of an ex-
tractive summarization system that simply picks 
noteworthy utterances and suggests their con-
tents to the participants. To answer this question, 
we conducted a Wizard of Oz-based pilot user 
study, as follows. 
2.1 Wizard of Oz Study Design 
We designed a user study in which a human Wi-
zard listened to the utterances being uttered dur-
ing the meeting, identified noteworthy utter-
ances, and suggested their contents to one or 
more participants for inclusion in their notes. In 
order to minimize differences between the Wi-
zard and the system (except for the Wizard?s 
human-level ability to judge noteworthiness), we 
restricted the Wizard in the following ways: 
(a) The Wizard was allowed to only suggest the 
contents of individual utterances to the par-
ticipants, and not summarize the contents of 
multiple utterances.  
(b) The Wizard was allowed to listen to the 
meeting speech, but when suggesting the 
contents of an utterance to the participants, 
he was restricted to using a real-time auto-
matic transcription of the utterance. (He was 
allowed to withhold suggestions because 
they were too erroneously transcribed.) 
(c) In order to be closer to a system that has lit-
tle or no ?understanding? of the meetings, 
we chose a human (to play the role of the 
Wizard) who had not participated in the 
meetings before, and thus had little prior 
knowledge of the meetings? contents.  
2.2 Notes Suggestion Interface 
In order to suggest notes to meeting participants 
during a meeting ? either automatically or 
through a Wizard ? we have modified the 
SmartNotes system, whose meeting recording 
and note-taking features have been described 
earlier in (Banerjee & Rudnicky, 2007). Briefly, 
each meeting participant comes to the meeting 
with a laptop running SmartNotes. At the begin-
ning of the meeting, each participant?s Smart-
Notes client connects to a server, authenticates 
the participant and starts recording and transmit-
ting his speech to the server. In addition, Smart-
Notes also provides meeting participants with a 
note-taking interface that is split into two major 
panes. In the ?notes? pane the participant types 
his notes that are then recorded for research pur-
poses. In the ?suggestions? pane, Wizard-
suggested notes are displayed. If at any time dur-
ing the meeting a participant double-clicks on 
one of the suggested notes in the ?suggestions? 
pane, its text gets included in his notes in the 
?notes? pane. The Wizard uses a different appli-
cation to select real-time utterance transcriptions, 
and insert them into each participant?s ?sugges-
tions? pane. (While we also experimented with 
having the Wizard target his suggestions at indi-
vidual participants, we do not report on those 
experiments here; those results were similar to 
the ones presented below.)  
2.3 Results 
We conducted the Wizard of Oz study on 9 
meetings that all belonged to the same sequence. 
That is, these meetings featured a largely over-
lapping group of participants who met weekly to 
discuss progress on a single project. The same 
person played the role of the Wizard in each of 
these 9 meetings. The meetings were on average 
33 minutes long, and there were 3 to 4 partici-
pants in each meeting. Although we have not 
evaluated the accuracy of the speech recognizer 
on these particular meetings, the typical average 
word error rate for these speakers is around 0.4 ? 
i.e., 4 out of 10 words are incorrectly transcribed.  
On average, the Wizard suggested the contents 
of 7 utterances to the meeting participants, for a 
total of 63 suggestions across the 9 meetings. Of 
these 63 suggestions, 22 (34.9%) were accepted 
by the participants and included in their notes. 
Thus on average, about 2.5 Wizard-suggested 
notes were accepted and included in participants? 
notes in each meeting. On average, meeting par-
ticipants took a total of 5.9 lines of notes per 
72
meeting; thus, 41.5% of the notes in each meet-
ing were Wizard-suggested.  
It cannot be ascertained if the meeting partici-
pants would have written the suggested notes on 
their own if they weren?t suggested to them. 
However the fact that some Wizard-suggested 
notes were accepted implies that the participants 
probably saw some value in including those sug-
gestions in their notes. Further, there was no 
drop-off in the fraction of meeting notes that was 
Wizard-suggested: the per-meeting average per-
centage of notes that was Wizard-suggested was 
around 41% for both the first 4 meetings, as well 
as the last 5. This implies that despite a seeming-
ly low acceptance rate (35%), participants did 
not ?give up? on the suggestions, but continued 
to make use of them over the course of the 9-
meeting meeting sequence. We conclude that an 
extractive summarization system that detects 
noteworthy utterances and suggests them to 
meeting participants can be perceived as useful 
by the participants, if the detection of noteworthy 
utterances is ?accurate enough?. 
3 Meeting Data Used in this Paper 
Assuming the feasibility of an extraction-based 
notes suggestion system, we turn our attention to 
developing a system that can automatically 
detect the noteworthiness of an utterance. Our 
goal here is to learn to do this task over a se-
quence of related meetings. Towards this end, we 
have recorded sequences of natural meetings ? 
meetings that would have taken place even if 
they weren?t being recorded. Meetings in each 
sequence featured largely overlapping participant 
sets and topics of discussion. For each meeting, 
we used SmartNotes (Banerjee & Rudnicky, 
2007) (described in section 2 above) to record 
both the audio from each participant as well as 
his notes. The audio recording and the notes 
were both time stamped, associated with the par-
ticipant?s identity, and uploaded to the meeting 
server. After the meeting was completed the au-
dio was manually segmented into utterances and 
transcribed both manually and using a speech 
recognizer (more details in section 5.2).  
In this paper we use a single sequence of 6 
meetings held between April and June of 2006. 
(These were separate from the ones used for the 
Wizard of Oz study above.) The meetings were 
on average 28 minutes and 43 seconds long (? 3 
minutes and 48 seconds standard error) counting 
from the beginning of the first recorded utterance 
to the end of the last one. On average each meet-
ing had 28 minutes and 38 seconds of speech ? 
this includes overlapped speech when multiple 
participants spoke on top of each other. Across 
the 6 meetings there were 5 unique participants; 
each meeting featured between 2 and 4 of these 
participants (average: 3.5 ? 0.31).  
The meetings had, on average, 633.67 (? 
85.60) utterances each, for a total of 3,796 utter-
ances across the 6 meetings. (In this paper, these 
3,796 utterances form the units of classification.) 
As expected, utterances varied widely in length. 
On average, utterances were 2.67 ? 0.18 seconds 
long and contained 7.73 (? 0.44) words.  
4 Multilevel Noteworthiness Annotation 
In order to develop approaches to automatically 
identify noteworthy utterances, we have manual-
ly annotated each utterance in the meeting data 
with its degree of ?noteworthiness?. While re-
searchers in the related field of speech summari-
zation typically use a binary labeling ? ?in sum-
mary? versus ?out of summary? (e.g. (Galley, 
2006), (Liu & Liu, 2008), (Penn & Zhu, 2008), 
etc) ? we have observed that there are often 
many utterances that are ?borderline? at best, and 
the decision to label them as ?in summary? or 
?out? is arbitrary. Our approach instead has been 
to create three levels of noteworthiness. Doing so 
allows us to separate the ?clearly noteworthy? 
utterances from the ?clearly not noteworthy?, 
and to label the rest as being between these two 
classes. (Of course, arbitrary choices must still 
be made between the edges of these three 
classes. However, having three levels preserves 
more information in the labels than having two, 
and it is always possible to create two labels 
from the three, as we do in later sections.)  
These multilevel noteworthiness annotations 
were done by two annotators. One of them ?
denoted as ?annotator 1? ? had attended each of 
the meetings, while the other ? ?annotator 2? ? 
had not attended any of the meetings. Although 
annotator 2 was given a brief overview of the 
general contents of the meetings, his understand-
ing of the meeting was expected to be lower than 
that of the other annotator. By using such an an-
notator, our aim was to identify utterances that 
were ?obviously noteworthy? even to a human 
being who lacks a deep understanding of the con-
text of the meetings. (In section 5.2 we describe 
how we merge the two sets of annotations.)  
The annotators were asked to make a 3-level 
judgment about the relative noteworthiness of 
each utterance. That is, for each utterance, the 
73
annotators were asked to decide whether a note-
suggestion system should ?definitely show? the 
contents of the utterance to the meeting partici-
pants, or definitely not show (labeled as ?don?t 
show?). Utterances that did not quite belong to 
either category were asked to be labeled as 
?maybe show?. Utterances labeled ?definitely 
show? were thus at the highest level of notewor-
thiness, followed by those labeled ?maybe show? 
and those labeled ?don?t show?.  Note that we 
did not ask the annotators to label utterances di-
rectly in terms of noteworthiness. Anecdotally, 
we have observed that asking people to label ut-
terances with their noteworthiness leaves the task 
insufficiently well defined because the purpose 
of the labels is unclear. On the other hand, asking 
users to identify utterances they would have in-
cluded in their notes leads to annotators taking 
into account the difficulty of writing particular 
notes, which is also not desirable for this set of 
labels. Instead, we asked annotators to directly 
perform (in some sense) the task that the even-
tual notes-assistance system will perform. 
In order to gain a modicum of agreement in 
the annotations, the two annotators discussed 
their annotation strategies after annotating each 
of the first two meetings (but not after the later 
meetings). A few general annotation patterns 
emerged, as follows: Utterances labeled 
?definitely show? typically included: 
(a) Progress on action items since the last week.  
(b) Concrete plans of action for the next week.  
(c) Announcements of deadlines. 
(d) Announcements of bugs in software, etc. 
In addition, utterances that contained the crux 
of any seemingly important discussion were 
labeled as ?definitely show?. On the other hand, 
utterances that contained no information worth 
including in the notes (by the annotators? 
judgment) were labeled as ?don?t show?. 
Utterances that did contain some additional 
elaborations of the main point, but without which 
the main point could still be understood by future 
readers of the notes were typically labeled as 
?maybe show?. 
Table 1 shows the distribution of the three la-
bels across the full set of 3,796 utterances in the 
dataset for both annotators. Both annotators la-
beled only a small percentage of utterances as 
?definitely show?, a larger fraction as ?maybe 
show? and most utterances as ?don?t show?. Al-
though the annotators were not asked to shoot for 
a certain distribution, observe that they both la-
beled a similar fraction of utterances as ?definite-
ly show?. On the other hand, annotator 2, who 
did not attend the meetings, labeled 50% more 
utterances as ?maybe show? than annotator 1 
who did attend the meetings. This difference is 
likely due to the fact that annotator 1 had a better 
understanding of the utterances in the meeting, 
and was more confident in labeling utterances as 
?don?t show? than annotator 2 who, not having 
attended the meetings, was less sure of some ut-
terances, and thus more inclined to label them as 
?maybe show?.  
 
Annotator 
# 
Definitely 
show 
Maybe 
show 
Don?t 
show 
1 13.5% 24.4% 62.1% 
2 14.9% 38.8% 46.3% 
Table 1: Distribution of Labels for Each Annotator 
4.1 Inter-Annotator Kappa Agreement 
To gauge the level of agreement between the two 
annotators, we compute the Kappa score. Given 
labels from different annotators on the same data, 
this metric quantifies the difference between the 
observed agreement between the labels and the 
expected agreement, with larger values denoting 
stronger agreement.  
For the 3-way labeling task, the two annota-
tors achieve a Kappa agreement score of 0.44 (? 
0.04). This seemingly low number is typical of 
agreement scores obtained in meeting summari-
zation. (Liu & Liu, 2008) reported Kappa agree-
ment scores between 0.11 and 0.35 across 6 an-
notators while (Penn & Zhu, 2008) with 3 anno-
tators achieved Kappa of 0.383 and 0.372 on ca-
sual telephone conversations and lecture speech. 
(Galley, 2006) reported inter-annotator agree-
ment of 0.323 on data similar to ours. 
To further understand where the disagree-
ments lie, we converted the 3-way labeled data 
into 2 different 2-way labeled datasets by merg-
ing two labels into one. First we evaluate the de-
gree of agreement the annotators have in separat-
ing utterances labeled ?definitely show? from the 
other two levels. We do so by re-labeling all ut-
terances not labeled ?definitely show? with the 
label ?others?. For the ?definitely show? versus 
?others? labeling task, the annotators achieve an 
inter-annotator agreement of 0.46. Similarly we 
compute the agreement in separating utterances 
labeled ?do not show? from the two other labels 
? in this case the Kappa value is 0.58. This im-
plies that it is easier to agree on the separation 
between ?do not show? and the other classes, 
than between ?definitely show? and the other 
classes.  
74
4.2 Inter-Annotator Accuracy, Prec/Rec/F 
Another way to gauge the agreement between the 
two sets of annotations is to compute accuracy, 
precision, recall and f-measure between them. 
That is, we can designate one annotator?s labels 
as the ?gold standard?, and use the other annota-
tor?s labels to find, for each of the 3 labels, the 
number of utterances that are true positives, false 
positives, and false negatives. Using these num-
bers we can compute precision as the ratio of 
true positives to the sum of true and false posi-
tives, recall as the ratio of true positives to the 
sum of true positives and false negatives, and f-
measure as the harmonic mean of precision and 
recall. (Designating the other annotator?s labels 
as ?gold standard? simply swaps the precision 
and recall values, and keeps f-measure the same). 
Accuracy is the number of utterances that have 
the same label from the two annotators, divided 
by the total number of utterances.  
Table 2 shows the evaluation over the 6-
meeting dataset using annotator 1?s data as ?gold 
standard?. The standard error for each cell is less 
than 0.08. Observe in Table 2 that while both the 
?definitely show? and ?maybe show? classes 
have nearly equal f-measure, the precision and 
recall values for the ?maybe show? class are 
much farther apart from each other than those for 
the ?definitely show? class. This is due to the 
fact that while both annotators label a similar 
number of utterances as ?definitely show?, they 
label very different numbers of utterances as 
?maybe show?. If the same accuracy, precision, 
recall and f-measure scores are computed for the 
?definitely show? vs. ?others? split, the accuracy 
jumps to 87%, possibly because of the small size 
of the ?definitely show? category. The accuracy 
remains at 78% for the ?don?t show? vs. ?others? 
split.  
 
 Definitely 
show 
Maybe 
show 
Don?t 
show 
Precision 0.57 0.70 0.70 
Recall 0.53  0.46 0.93 
F-measure 0.53  0.54 0.80 
Accuracy 69% 
Table 2 Inter-Annotator Agreement using Accuracy Etc.  
4.3 Inter-Annotator Rouge Scores 
Annotations can also be evaluated by computing 
the ROUGE metric (Lin, 2004). ROUGE, a pop-
ular metric for summarization tasks, compares 
two summaries by computing precision, recall 
and f-measure over ngrams that overlap between 
them. Following previous work on meeting 
summarization (e.g. (Xie, Liu, & Lin, 2008), 
(Murray, Renals, & Carletta, 2005), etc), we re-
port evaluation using ROUGE-1 F-measure, 
where the value ?1? implies that overlapping un-
igrams are used to compute the metric. Unlike 
previous research that had one summary from 
each annotator per meeting, our 3-level annota-
tion allows us to have 2 different summaries: (a) 
the text of all the utterances labeled ?definitely 
show? and, (b) the text of all the utterances la-
beled either ?definitely show? or ?maybe show?.  
On average (across both annotators over the 6 
meetings) the ?definitely show? utterance texts 
are 18.72% the size of the texts of all the utter-
ances in the meetings, while the ?definitely or 
maybe show? utterance texts are 61.6%. Thus, 
these two texts represent two distinct points on 
the compression scale. The average R1 F-
measure score is 0.62 over the 6 meetings when 
comparing the ?definitely show? texts of the two 
annotators. This is twice the R1 score ? 0.3 ? of 
the trivial baseline of simply labeling every ut-
terance as ?definitely show?. The inter-annotator 
R1 F-measure for the ?definitely or maybe show? 
texts is 0.79, marginally higher than the trivial 
?all utterances? baseline of 0.71. In the next sec-
tion, we compare the scores achieved by the au-
tomatic system against these inter-annotator and 
trivial baseline scores.  
5 Automatic Label Prediction  
So far we have presented the annotation of the 
meeting data, and various analyses thereof. In 
this section we present our approach for the 
automatic prediction of these labels. We apply a 
classification based approach to the problem of 
predicting the noteworthiness level of an 
utterance, similar to (Banerjee & Rudnicky, 
2008). We use leave-one-meeting-out cross 
validation: for each meeting m, we train the 
classifier on manually labeled utterances from 
the other 5 meetings, and test the classifier on the 
utterances of meeting m. We then average the 
results across the 6 meetings. Given the small 
amount of data, we do not test on separate data, 
nor do we perform any tuning.  
Using the 3-level annotation described above, 
we train a 3-way classifier to label each utterance 
with one of the multilevel noteworthiness labels. 
In addition, we use the two 2-way merged-label 
annotations ? ?definitely show? vs. others and 
?don?t show? vs. others ? to train two more 2-
way classifiers. In each of these classification 
75
problems we use the same set of features and the 
same classification algorithms described below.  
5.1 Features Used 
Ngram features: As has been shown by 
(Banerjee & Rudnicky, 2008), the strongest 
features for noteworthiness detection are ngram 
features, i.e. features that capture the occurrence 
of ngrams (consecutive occurrences of one or 
more words) in utterances. Each ngram feature 
represents the presence or absence of a single 
specific ngram in an utterance. E.g., the ngram 
feature ?action item? represents the occurrence 
of the bigram ?action item? in a given utterance. 
Unlike (Banerjee & Rudnicky, 2008) where each 
ngram feature captured the frequency of a 
specific ngram in an utterance, in this paper we 
use boolean-valued ngram features to capture the 
presence/absence of ngrams in utterances. We do 
so because in tests on separate data, boolean-
valued features out-performed frequency-based 
features, perhaps due to data sparseness. Before 
ngram features are extracted, utterances are 
normalized: partial words, non-lexicalized filler 
words (like ?umm?, ?uh?), punctuations, 
apostrophes and hyphens are removed, and all 
remaining words are changed to upper case. Next, 
the vocabulary of ngrams is defined as the set of 
ngrams that occur at least 5 times in the entire 
dataset of meetings, for ngram sizes of 1 through 
6 word tokens. Finally, the occurrences of each 
of these vocabulary ngrams in an utterance are 
recorded as the feature vector for that utterance. 
In the dataset used in this paper, there are 694 
unique unigrams that occur at least 5 times 
across the 6 meetings, 1,582 bigrams, 1,065 
trigrams, 1,048 4-grams, 319 5-grams and 102 6-
grams. In addition to these ngram features, for 
each utterance we also include the number of Out 
of Vocabulary ngram ? ngrams that occur less 
than 5 times across all the meetings.  
Overlap-based Features: We assume that we 
have access to the text of the agenda of the test 
meeting, and also the text of the notes taken by 
the participants in previous meetings (but not 
those taken in the test meeting). Since these 
artifacts are likely to contain important keywords 
we compute two sets of overlaps features. In the 
first set we compute the number of ngrams that 
overlap between each utterance and the meeting 
agenda. That is, for each utterance we count the 
number of unigrams, bigrams, trigrams, etc that 
also occur in the agenda of that meeting. 
Similarly in the second set we compute the 
number of ngrams in each utterance that also 
occur in the notes of previous meetings. Finally, 
we compute the degree of overlap between this 
utterance and other utterances in the meeting. 
The motivation for this last feature is to find 
utterances that are repeats (or near-repeats) of 
other utterances ? repetition may correlate with 
importance.  
Other features: In addition to the ngram and 
ngram overlap features, we also include term 
frequency ? inverse document frequency (tf-idf) 
features to capture the information content of the 
ngrams in the utterance. Specifically we compute 
the TF-IDF of each ngram (of sizes 1 through 5) 
in the utterance, and include the maximum, 
minimum, average and standard deviation of 
these values as features of the utterance. We also 
include speaker-based features to capture who is 
speaking when. We include the identity of the 
speaker of the current utterance and those of the 
previous and next utterances as features. Lastly 
we include the length of the utterance (in seconds) 
as a feature.  
5.2 Evaluation Results 
In this paper we use a Support Vector Machines-
based classifier, which is a popular choice for 
extractive meeting summarization, e.g. (Xie, Liu, 
& Lin, 2008); we use a linear kernel in this pa-
per. In the results reported here we use the output 
of the Sphinx speech recognizer, using speaker-
independent acoustic models, and language mod-
els trained on publicly available meeting data. 
The word error rate was around 44% ? more  
details of the speech recognition process are in 
(Huggins-Daines & Rudnicky, 2007). For train-
ing purposes, we merged the annotations from 
the two annotators by choosing a ?middle or 
lower ground? for all disagreements. Thus, if for 
an utterance the two labels are ?definitely show? 
and ?don?t show?, we set the merged label as the 
middle ground of ?maybe show?. On the other 
hand if the two labels were on adjacent levels, 
we chose the lower one ? ?maybe show? when 
the labels were ?definitely show? and ?maybe 
show?, and ?don?t show? when the labels were 
?maybe show? and ?don?t show?. Thus only ut-
terances that both annotators labeled as ?definite-
ly show? were also labeled as ?definitely show? 
in the merged annotation. We plan to try other 
merging strategies in the future. For testing, we 
evaluated against each annotator?s labels sepa-
rately, and averaged the results. 
 
 
 
76
 Definitely 
show 
Maybe 
show 
Don?t 
show 
Precision 0.21 0.47 0.72 
Recall 0.16  0.40 0.79 
F-measure 0.16  0.43 0.75 
Accuracy 61.4% 
Table 3 Results of the 3-Way Classification 
Table 3 presents the accuracy, precision, recall 
and f-measure results of the 3-way classification 
task. (We use the Weka implementation of SVM 
that internally devolves the 3-way classification 
task into a sequence of pair-wise classifications. 
We use the final per-utterance classification 
here.) Observe that the overall accuracy of 
61.4% is only 11% lower relative to the accuracy 
obtained by comparing the two annotators? anno-
tations (69%, Table 2). However, the precision, 
recall and f-measure values for the ?definitely 
show? class are substantially lower for the pre-
dicted labels than the agreement between the two 
annotators. The numbers are closer for the ?may-
be show? and the ?don?t show? classes. This im-
plies that it is more difficult to accurately detect 
utterances labeled ?definitely show? than it is to 
detect the other classes. One reason for this dif-
ference is the size of each utterance class. Utter-
ances labeled ?definitely show? are only around 
14% of all utterances, thus there is less data for 
this class than the others. We also ran the algo-
rithm using manually transcribed data, and found 
improvement in only the ?Definitely show? class 
with an f-measure of 0.21. This improvement is 
perhaps because the speech recognizer is particu-
larly prone to getting names and other technical 
terms wrong, which may be important clues of 
noteworthiness. 
Table 4 presents the ROUGE-1 F-measure 
scores averaged over the 6 meetings. (ROUGE is 
described briefly in section 4.3 and in detail in 
(Lin, 2004)). Similar to the inter-annotator 
agreement computations, we computed ROUGE 
between the text of the utterances labeled ?defi-
nitely show? by the system against that of utter-
ances labeled ?definitely show? by the two anno-
tators. (We computed the scores separately 
against each of the annotators in turn and then 
averaged the two values.) We did the same thing 
for the set of utterances labeled either ?definitely 
show? or ?maybe show?. Observe that the R1-F 
score for the ?definitely show? comparison is 
nearly 50% relative higher than the trivial base-
line of labeling every utterance as ?definitely 
show?. However the score is 30% lower than the 
corresponding inter-annotator agreement. The 
corresponding R1-Fmeasure score using manual 
transcriptions is only marginally better ? 0.47. 
The set of utterances labeled either definitely or 
maybe shows (second row of table 4) does not 
outperform the all-utterances baseline when us-
ing automatic transcriptions, but does so with 
manual transcriptions, whose R1-F value is 0.74.  
 
Comparing What R1-Fmeasure 
Definitely show 0.43 
Definitely or maybe show 0.63 
Table 4 ROUGE Scores for the 3-Way Classification 
These results show that while the detection of 
definitely show utterances is better than the trivi-
al baselines even when using automatic tran-
scriptions, there is a lot of room for improve-
ment, as compared to human-human agreement. 
Although direct comparisons to other results 
from the meeting summarization literature are 
difficult because of the difference in the datasets, 
numerically it appears that our results are similar 
to those obtained previously. (Xie, Liu, & Lin, 
2008) uses Rouge-1 F-measure solely, and 
achieve scores between 0.6 to 0.7. (Murray, 
Renals, & Carletta, 2005) also achieve Rouge-1 
scores in the same range with manual transcripts.  
The trend in the results for the two 2-way clas-
sifications is similar to the trend for the inter an-
notator agreements. Just as inter-annotator accu-
racy increased to 87% for the ?definitely show? 
vs. ?others? classification, so does accuracy of 
the predicted labels increase to 88.3%. The f-
measure for the ?definitely show? class falls to 
0.13, much lower than the inter-annotator f-
measure of 0.53. For the ?don?t show? vs. ?oth-
ers? classification, the automatic system achieves 
an accuracy of 66.6%. For the ?definitely plus 
maybe? class, the f-measure is 0.59, which is 
22% relatively lower than the inter-annotator f-
measure for that class. (As with the 3-way classi-
fication, these results are all slightly worse than 
those obtained using manual transcriptions.) 
5.3 Useful Features 
In order to understand which features contribute 
most to these results, we used the Chi-Squared 
test of association to find features that are most 
strongly correlated to the 3 output classes. The 
best features are those that measure word over-
laps between the utterances and the text in the 
agenda labels and the notes in previous meetings. 
This is not a surprising finding ? the occurrence 
of an ngram in an agenda label or in a previous 
note is highly indicative of its importance, and 
77
consequently that of the utterances that contain 
that ngram. Max and average TF-IDF scores are 
also highly ranked features. These features score 
highly for utterances with seldom-used words, 
signifying the importance of those utterances. 
Domain independent ngrams such as ?action 
item? are strongly correlated with noteworthiness, 
as are a few domain dependent ngrams such as 
?time shift problem?. These latter features 
represent knowledge that is transferred from ear-
lier meetings to latter ones in the same sequence. 
The identity of the speaker of the utterance does 
not seem to correlate well with the utterance?s 
noteworthiness, although this finding could 
simply be an artifact of this particular dataset. 
6 Related Work  
Noteworthiness detection is closely related to 
meeting summarization. Extractive techniques 
are popular, e.g. (Murray, Renals, & Carletta, 
2005), and many algorithms have been attempted 
including SVMs (Xie, Liu, & Lin, 2008), Gaus-
sian Mixture Models and Maximal Marginal Re-
levance (Murray, Renals, & Carletta, 2005), and 
sequence labelers (Galley, 2006). Most ap-
proaches use a mixture of ngram features, and 
other structural and semantic features ? a good 
evaluation of typical features can be found in 
(Xie, Liu, & Lin, 2008). Different evaluation 
techniques have also been tried, with ROUGE 
often being shown as at least adequate (Liu & 
Liu, 2008). Our work is an application and ex-
tension of the speech summarization field to the 
problem of assistive note-taking.  
7 Conclusions and Future Work  
In our work we investigated the problem of de-
tecting the noteworthiness of utterances pro-
duced in meetings. We conducted a Wizard-of-
Oz-based user study to establish the usefulness 
of extracting the text of utterances and suggest-
ing these as notes to the meeting participants. We 
showed that participants were willing to accept 
about 35% of these suggestions over a sequence 
of 9 meetings. We then presented a 3-level note-
worthiness annotation scheme that breaks with 
the tradition of 2-way ?in/out of summary? anno-
tation. We showed that annotators have strong 
agreement for separating the highest level of 
noteworthiness from the other levels. Finally we 
used these annotations as labeled data to train a 
Support Vector Machine-based classifier which 
performed better than trivial baselines but not as 
well as inter-annotator agreement levels.  
     For future work, we plan to use automatic 
noteworthiness predictions to suggest notes to 
meeting participants during meetings. We are 
also interested in training the noteworthiness de-
tector directly from the notes that participants 
took in previous meetings, thus reducing the 
need for manually annotated data. 
Reference 
Banerjee, S, and A. I. Rudnicky. "Segmenting Meet-
ings into Agenda Items by Extracting Implicit Su-
pervision from Human Note-Taking." Proceedings 
of the International Conference on Intelligent User 
Interfaces. Honolulu, HI, 2007. 
Banerjee, Satanjeev, and A. I. Rudnicky. "An Extrac-
tive-Summarization Baseline for the Automatic 
Detection of Noteworthy Utterances in Multi-Party 
Human-Human Dialog." IEEE Workshop on Spo-
ken Language Technology. Goa, India, 2008. 
Galley, Michel. "A Skip-Chain Conditional Random 
Field for Ranking Meeting Utterances by Impor-
tance." Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing. Syd-
ney, Australia, 2006. 
Huggins-Daines, David, and A. I. Rudnicky. "Impli-
citly Supervised Language Model Adaptation for 
Meeting Transcription." Proceedings of the HLT-
NAACL. Rochester, NY. 2007. 
Lin, Chin-Yew. "ROUGE: A Package for Automatic 
Evaluation of Summaries." Proceedings of the 
ACL-04 Workshop: Text Summarization Branches 
Out. Barcelona, Spain: Association for Computa-
tional Linguistics, 2004. 74-81. 
Liu, Feifan, and Y. Liu. "Correlation between 
ROUGE and Human Evaluation of Extractive 
Meeting Summaries." Proceedings of ACL-HLT. 
Columbus, OH, 2008. 
Murray, Gabriel, S. Renals, and J. Carletta. "Extrac-
tive Summarization of Meeting Recordings." Pro-
ceedings of Interspeech. Lisbon, Portugal, 2005. 
Penn, Gerald, and X. Zhu. "A Critical Reassessment 
of Evaluation Baselines for Speech Summariza-
tion." Proceedings of ACL-HLT. Columbus, OH, 
2008.  
Xie, Shasha, Y. Liu, and H. Lin. "Evaluating the Ef-
fectiveness of Features and Sampling in Extractive 
Meeting Summarization." IEEE Workshop on 
Spoken Language Technology. Goa, India, 2008. 
78
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 99?107,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using the Amazon Mechanical Turk to Transcribe and  
Annotate Meeting Speech for Extractive Summarization 
Matthew Marge   Satanjeev Banerjee   Alexander I. Rudnicky  
School of Computer Science, Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
{mrmarge,banerjee,air}@cs.cmu.edu 
 
Abstract 
Due to its complexity, meeting speech pro-
vides a challenge for both transcription and 
annotation. While Amazon?s Mechanical Turk 
(MTurk) has been shown to produce good re-
sults for some types of speech, its suitability 
for transcription and annotation of spontane-
ous speech has not been established. We find 
that MTurk can be used to produce high-
quality transcription and describe two tech-
niques for doing so (voting and corrective). 
We also show that using a similar approach, 
high quality annotations useful for summari-
zation systems can also be produced. In both 
cases, accuracy is comparable to that obtained 
using trained personnel.  
1 Introduction 
Recently, Amazon?s Mechanical Turk (MTurk) has 
been shown to produce useful transcriptions of 
speech data; Gruenstein et al (2009) have success-
fully used MTurk to correct the transcription out-
put from a speech recognizer, while Novotney and 
Callison-Burch (2010) used MTurk for transcrib-
ing a corpus of conversational speech. These stu-
dies suggest that transcription, formerly considered 
to be an exacting task requiring at least some train-
ing, could be carried out by casual workers. How-
ever, only fairly simple transcription tasks were 
studied.  
 We propose to assess the suitability of MTurk 
for processing more challenging material, specifi-
cally recordings of meeting speech. Spontaneous 
speech can be difficult to transcribe because it may 
contain false starts, disfluencies, mispronunciations 
and other defects. Similarly for annotation, meet-
ing content may be difficult to follow and conven-
tions difficult to apply consistently.  
 Our first goal is to ascertain whether MTurk 
transcribers can accurately transcribe spontaneous 
speech, containing speech errors and of variable 
utterance length.  
 Our second goal is to use MTurk for creating 
annotations suitable for extractive summarization 
research, specifically labeling each utterance as 
either ?in-summary? or ?not in-summary?. Among 
other challenges, this task cannot be decomposed 
into small independent sub-tasks?for example, 
annotators cannot be asked to annotate a single 
utterance independent of other utterances. To our 
knowledge, MTurk has not been previously ex-
plored for the purpose of summarization annota-
tion.  
2 Meeting Speech Transcription Task 
We recently explored the use of MTurk for tran-
scription of short-duration clean speech (Marge et 
al., 2010) and found that combining independent 
transcripts using ROVER yields very close agree-
ment with a gold standard (2.14%, comparable to 
expert agreement). But simply collecting indepen-
dent transcriptions seemed inefficient: the ?easy? 
parts of each utterance are all transcribed the same. 
In the current study our goal is determine whether 
a smaller number of initial transcriptions can be 
used to identify easy- and difficult-to-transcribe 
regions, so that the attention of subsequent tran-
scribers can be focused on the more difficult re-
gions.  
2.1 Procedure 
In this corrective strategy for transcription, we 
have two turkers to independently produce tran-
scripts. A word-level minimum edit distance me-
tric is then used to align the two transcripts and 
locate disagreements. These regions are replaced 
with underscores, and new turkers are asked to 
transcribe those regions.  
Utterances were balanced for transcription dif-
ficulty (measured by the native English back-
99
ground of the speaker and utterance length). For 
the first pass transcription task, four sets of jobs 
were posted for turkers to perform, with each pay-
ing $0.01, $0.02, $0.04, or $0.07 per approved 
transcription. Payment was linearly scaled with the 
length of the utterance to be transcribed at a rate of 
$0.01 per 10 seconds of speech, with an additional 
payment of $0.01 for providing feedback. In each 
job set, there were 12 utterances to be transcribed 
(yielding a total of 24 jobs available given two 
transcribers per utterance). Turkers were free to 
transcribe as many utterances as they could across 
all payment amounts. 
After acquiring two transcriptions, we aligned 
them, identified points of disagreement and re-
posted the transcripts and the audio as part of a 
next round of job sets. Payment amounts were kept 
the same based on utterance length. In this second 
pass of transcriptions, three turkers were recruited 
to correct and amend each transcription. Thus, a 
total of five workers worked on every transcription 
after both iterations of the corrective task. In our 
experiment 23 turkers performed the first phase of 
the task, and 28 turkers the corrective task (4 
workers did both passes).  
2.2 First and Second Pass Instructions 
First-pass instructions asked turkers to listen to 
utterances with an embedded audio player pro-
vided with the HIT. Turkers were instructed to 
transcribe every word heard in the audio and to 
follow guidelines for marking speaker mispronun-
ciations and false starts. Filled pauses (?uh?, ?um?, 
etc.) were not to be transcribed in the first pass. 
Turkers could replay the audio as many times as 
necessary. 
In the second pass, turkers were instructed to 
focus on the portions of the transcript marked with 
underscores, but also to correct any other words 
they thought were incorrect. The instructions also 
asked turkers to identify three types of filler words: 
?uh?, ?um?, and ?lg? (laughter). We selected this 
set since they were the most frequent in the gold 
standard transcripts. Again, turkers could replay 
the audio.  
2.3 Speech Corpus 
The data were sampled from a previously-collected 
corpus of natural meetings (Banerjee and Rud-
nicky, 2007). The material used in this paper 
comes from four speakers, two native English 
speakers and two non-Native English speakers (all 
male). We selected 48 audio clips; 12 from each of 
the four speakers. Within each speaker's set of 
clips, we further divided the material into four 
length categories: ~5, ~10, ~30 and ~60 sec. The 
speech material is conversational in nature; the 
gold standard transcriptions of this data included 
approximately 15 mispronunciations and 125 false 
starts. Table 1 presents word count information 
related to the utterances in each length category. 
 
Utterance 
Length 
Word Count 
(mean) 
Standard  
Deviation 
Utterance 
Count 
5 sec 14  5.58 12  
10 sec 24.5  7.26 12  
30 sec 84  22.09 12  
60 sec 146.6  53.17 12  
 Table 1. Utterance characteristics. 
3 Meeting Transcription Analysis 
Evaluation of first and second pass corrections was 
done by calculating word error rate (WER) with a 
gold standard, obtained using the transcription 
process described in (Bennett and Rudnicky, 
2002). Before doing so, we normalized the candi-
date MTurk transcriptions as follows: spell-
checking (with included domain-specific technical 
terms), and removal of punctuation (periods, com-
mas, etc.). Apostrophes were retained. 
 
Table 2. WER across transcription iterations. 
3.1 First-Pass Transcription Results 
Results from aligning our first-pass transcriptions 
with a gold standard are shown in the second col-
umn of Table 2. Overall error rate was 23.8%, 
which reveals the inadequacy of individual turker 
transcriptions, if no further processing is done. 
(Remember that first-pass transcribers were asked 
to leave out fillers even though the gold standard 
contained them, thus increasing WER). 
Utterance 
Length 
First-Pass 
WER 
Second-Pass 
WER 
ROVER-3 
WER 
5 sec. 31.5% 19.8% 15.3% 
10 sec. 26.7% 20.3% 13.8% 
30 sec. 20.8% 16.9% 15.0% 
60 sec. 24.3% 17.1% 15.4% 
Aggregate 23.8% 17.5% 15.1% 
100
In this first pass, speech from non-native speak-
ers was transcribed more poorly (25.4% WER) 
than speech from native English speakers (21.7% 
WER). In their comments sections, 17% of turkers 
noted the difficulty in transcribing non-native 
speakers, while 13% found native English speech 
difficult. More than 80% of turkers thought the 
amount of work ?about right? for the payment re-
ceived.  
3.2 Second-Pass Transcription Results 
The corrective process greatly improved agreement 
with our expert transcriptions. Aggregate WER 
was reduced from 23.8% to 17.5% (27% relative 
reduction) when turkers corrected initial transcripts 
with highlighted disagreements (third column of 
Table 2). In fact, transcriptions after corrections 
were significantly more accurate than initial tran-
scriptions (F(1, 238) = 13.4, p < 0.05). With re-
spect to duration, the WER of the 5-second utter-
ances had the greatest improvement, a relative re-
duction of WER by 37%.  Transcription alignment  
with the gold standard experienced a 39% im-
provement to 13.3% for native English speech, and 
a 19% improvement to 20.6% for non-native Eng-
lish speech (columns 2 and 3 of Table 3).  
 We found that 30% of turkers indicated that the 
second-pass correction task was difficult, as com-
pared with 15% for the first-pass transcription task. 
Work amount was perceived to be about right 
(85% of the votes) in this phase, similar to the first. 
3.3 Combining Corrected Transcriptions 
In order to improve the transcriptions further, we 
combined the three second-pass transcriptions of 
each utterance using ROVER?s word-level voting 
scheme (Fiscus, 1997). The WER of the resulting 
transcripts are presented in the fourth column of 
Table 2. Aggregate WER was further reduced by 
14% relative to 15.1%. This result is close to typ-
ical disagreement rates of 6-12% reported in the 
literature (Roy and Roy, 2009). The best im-
provements using ROVER were found with the 
transcriptions of the shorter utterances: WER 
from the second-pass of 5-second utterances tran-
scriptions was reduced by 23% to 15.3%. The 10-
second utterance transcriptions experienced the 
best improvement, 32%, to a WER of 13.8%. 
 Although segmenting audio into shorter seg-
ments may yield fast turnaround times, we found 
that utterance length is not a significant factor in 
determining alignment between combined, cor-
rected transcriptions and gold-standard transcrip-
tions (F(3, 44) = 0.16, p = 0.92). We speculate that 
longer utterances show good accuracy due to the 
increased context available to transcribers.  
Table 3. WER across transcription iterations based on 
speaker background. 
3.4 Error Analysis 
Out of 3,281 words (48 merged transcriptions of 
48 utterances), 496 were errors. Among the errors 
were 37 insertions, 315 deletions, and 144 substitu-
tions. Thus the most common error was to miss a 
word.  
 Further analysis revealed that two common cas-
es of errors occurred: the misplacement or exclu-
sion of filler words (even though the second phase 
explicitly instructed turkers to insert filler words) 
and failure to transcribe words considered to be out 
of the range of the transcriber?s vocabulary, such 
as technical terms and foreign names. Filler words 
accounted for 112 errors (23%). Removing fillers 
from both the combined transcripts and the gold 
standard improved WER by 14% relative to 
13.0%. Further, WER for native English speech 
transcriptions was reduced to 8.9%. This difference 
was however not statistically significant (F(1,94) = 
1.64, p = 0.2). 
 Turkers had difficulty transcribing uncommon 
words, technical terms, names, acronyms, etc. 
(e.g., ?Speechalyzer?, ?CTM?, ?PQs?). Investiga-
tion showed that at least 41 errors (8%) could be 
attributed to this out-of-vocabulary problem. It is 
unclear if there is any way to completely eradicate 
such errors, short of asking the original speakers. 
3.5 Comparison to One-Pass Approach 
Although the corrective model provides significant 
gain from individual transcriptions, this approach 
is logistically more complex. We compared it to 
our one-pass approach, in which five turkers inde-
pendently transcribe all utterances (Marge et al, 
2010). Five new transcribers per utterance were 
recruited for this task (yielding 240 transcriptions). 
Speaker 
Background 
First-Pass 
WER 
Second-
Pass WER 
ROVER-3 
WER 
Native  21.7% 13.3% 10.8% 
Non-native 25.4% 20.6% 18.4% 
101
Individual error rate was 24.0%, comparable to the 
overall error rate for the first step of the corrective 
approach (Table 2).  
 After combining all five transcriptions with 
ROVER, we found similar gains to the corrective 
approach: an overall improvement to 15.2% error 
rate. Thus both approaches can effectively produce 
high-quality transcriptions. We speculate that if 
higher accuracy is required, the corrective process 
could be extended to iteratively re-focus effort on 
the regions of greatest disagreement. 
3.6 Latency 
Although payment scaled with the duration of ut-
terances, we observed a consistent disparity in tur-
naround time. All HITs were posted at the same 
time in both iterations (Thursday afternoon, EST). 
Turkers were able to transcribe 48 utterances twice 
in about a day in the first pass for the shorter utter-
ances (5- and 10-second utterances), while it took 
nearly a week to transcribe the 30- and 60-second 
utterances. Turkers were likely discouraged by the 
long duration of the transcriptions compounded 
with the nature of the speech. To increase turna-
round time on lengthy utterances, we speculate that 
it may be necessary to scale payment non-linearly 
with length (or another measure of perceived ef-
fort). 
3.7 Conclusion 
Spontaneous speech, even in long segments, can 
indeed be transcribed on MTurk with a level of 
accuracy that approaches expert agreement rates 
for spontaneous speech. However, we expect seg-
mentation of audio materials into smaller segments 
would yield fast turnaround time, and may keep 
costs low. In addition, we find that ROVER works 
more effectively on shorter segments because 
lengths of candidate transcriptions are less likely to 
have large disparities. Thus, multiple transcriptions 
per utterance can be utilized best when their 
lengths are shorter.  
4 Annotating for Summarization  
4.1 Motivation 
Transcribing audio data into text is the first step 
towards making information contained in audio 
easily accessible to humans. A next step is to con-
dense the information in the raw transcription, and 
produce a short summary that includes the most 
important information. Good summaries can pro-
vide readers with a general sense of the meeting, or 
help them to drill down into the raw transcript (or 
the audio itself) for additional information. 
4.2 Annotation Challenges  
Unfortunately, summary creation is a difficult task 
because ?importance? is inherently subjective and 
varies from consumer to consumer. For example, 
the manager of a project, browsing a summary of a 
meeting, might be interested in all agenda items, 
whereas a project participant may be interested in 
only those parts of the meeting that pertain to his 
portion of the project.  
Despite this subjectivity, the usefulness of a 
summary is clear, and audio summarization is an 
active area of research. Within this field, two kinds 
of human annotations are generally created?
annotators are either asked to write a short sum-
mary of the audio, or they are asked to label each 
transcribed utterance as either ?in summary? or 
?out of summary?. The latter annotation is particu-
larly useful for training and evaluating extractive 
summarization systems?systems that create sum-
maries by selecting a subset of the utterances.  
Due to the subjectivity involved, we find very 
low inter-annotator agreement for this labeling 
task. Liu and Liu (2008) reported Kappa agreement 
scores of between 0.11 and 0.35 across 6 annota-
tors, Penn and Zhu (2008) reported 0.38 on tele-
phone conversation and 0.37 on lecture speech, 
using 3 annotators, and Galley (2006) reported 
0.32 on meeting data. Such low levels of agree-
ment imply that the resulting training data is likely 
to contain a great deal of ?noise??utterances la-
beled ?in summary? or ?out of summary?, when in 
fact they are not good examples of those classes. 
Disagreements arise due to the fact that utter-
ance importance is a spectrum.  While some utter-
ances are clearly important or unimportant, there 
are many utterances that lie between these ex-
tremes. In order to label utterances as either ?in-
summary? or not, annotators must choose an arbi-
trary threshold at which to make this decision. 
Simply asking annotators to provide a continuous 
?importance value? between 0 and 1 is also likely 
to be infeasible as the exact value for a given utter-
ance is difficult to ascertain. 
102
4.3 3-Class Formulation 
One way to alleviate this problem is to redefine the 
task as a 3-class labeling problem. Annotators can 
be asked to label utterances as either ?important?, 
?unimportant? or ?in-between?. Although this for-
mulation creates two decision boundaries, instead 
of the single one in the 2-class formulation, the 
expectation is that a large number of utterances 
with middling importance will simply be assigned 
to the ?in between? class, thus reducing the amount 
of noise in the data. Indeed we have shown (Baner-
jee and Rudnicky, 2009) that in-house annotators 
achieve high inter-annotator agreement when pro-
vided with the 3-class formulation. 
Another way to alleviate the problem of low 
agreement is to obtain annotations from many an-
notators, and identify the utterances that a majority 
of the annotators appear to agree on; such utter-
ances may be considered as good examples of their 
class. Using multiple annotators is typically not 
feasible due to cost. In this paper we investigate 
using MTurk to create 3-class-based summariza-
tion annotations from multiple annotators per 
meeting, and to combine and filter these annota-
tions to create high quality labels. 
5 Using Mechanical Turk for Annotations 
5.1 Challenges of Using Mechanical Turk 
Unlike some other tasks that require little or no 
context in order to perform the annotation, summa-
rization annotation requires a great deal of context. 
It is unlikely that an annotator can determine the 
importance of an utterance without being aware of 
neighboring utterances. Moreover, the appropriate 
length of context for a given utterance is likely to 
vary. Presenting all contiguous utterances that dis-
cuss the same topic might be appropriate, but 
would require manual segmentation of the meeting 
into topics. In this paper we experiment with show-
ing all utterances of a meeting. This is a challenge 
however, because MTurk is typically applied to 
quick low-cost tasks that need little context. It is 
unclear whether turkers would be willing to per-
form such a time-consuming task, even for higher 
payment. 
Another challenge for turkers is being able to 
understand the discussion well enough to perform 
the annotation. We experiment here with meetings 
that include significant technical content. While in-
house annotators can be trained over time to under-
stand the material well enough to perform the task, 
it is impractical to provide turkers with such train-
ing. We investigate the degree to which turkers can 
provide summarization annotation with minimal 
training.  
5.2 Data Used 
We selected 5 recorded meetings for our study. 
These meetings were not scripted?and would 
have taken place even if they weren?t being rec-
orded. They were project meetings containing dis-
cussions about software deliverables, problems, 
resolution plans, etc. The contents included tech-
nical jargon and concepts that non-experts are un-
likely to grasp by reading the meeting transcript 
alone.  
The 5 meetings had 2 to 4 participants each 
(mean: 3.5). For all meetings, the speech from each 
participant was recorded separately using head-
mounted close-talking microphones. We manually 
split these audio streams into utterances?ensuring 
that utterances did not have more than a 0.5 second 
pause in them, and then transcribed them using an 
established process (Bennett and Rudnicky, 2002). 
The meetings varied widely in length from 15 mi-
nutes and 282 utterances to 40 minutes and 948 
utterances (means: 30 minutes, 610 utterances). 
There were 3,052 utterances across the 5 meetings, 
each containing an mean of 7 words. The utter-
ances in the meetings were annotated using the 3-
class formulation by two in-house annotators. 
Their inter-annotator agreement is presented along 
with the rest of the evaluation results in Section 6. 
0
10
20
30
40
50
60
Important Neutral Unimportant
%
 o
f U
tt
er
an
ce
s
In-house Mturk
Figure 1. Label distribution of in-house and MTurk 
annotators. 
 
103
5.3 HIT Design and Instructions 
We instructed turkers to imagine that someone else 
(not them) was going to eventually write a report 
about the meeting, and it was their task to identify 
those utterances that should be included in the re-
port. We asked annotators to label utterances as 
?important? if they should be included in the report 
and ?unimportant? otherwise. In addition, utter-
ances that they thought were of medium impor-
tance and that may or may not need to be included 
in the report were to be labeled as ?neutral?. We 
provided examples of utterances in each of these 
classes. For the ?important? class, for instance, we 
included ?talking about a problem? and ?discuss-
ing future plan of action? as examples. For the ?un-
important? class, we included ?off topic joking?, 
and for the ?neutral? class ?minute details of an 
algorithm? was an example. 
In addition to these instructions and examples, 
we gave turkers a general guideline to the effect 
that in these meetings typically 1/4th of the utter-
ances are ?important?, 1/4th ?neutral? and the rest 
?unimportant?. As we discuss in section 6, it is 
unclear whether most turkers followed this guide-
line. 
Following these instructions, examples and tips, 
we provided the text of the utterances in the form 
of an HTML table. Each row contained a single 
utterance, prefixed with the name of the speaker. 
The row also contained three radio buttons for the 
three classes into which the annotator was asked to 
classify the utterance. Although we did not ensure 
that annotators annotated every utterance before 
submitting their work, we observed that for 95% of 
the utterances every annotator did provide a judg-
ment; we ignore the remaining 5% of the utter-
ances in our evaluation below. 
5.4 Number of Turkers and Payment 
For each meeting, we used 5 turkers and paid each 
one the same. That is, we did not vary the payment 
amount as an experimental variable. We calculated 
the amount to pay for a meeting based on in the 
length of that meeting. Specifically, we multiplied 
the number of utterances by 0.13 US cents to arrive 
at the payment. This resulted in payments ranging 
from 35 cents to $1.25 per meeting (mean 79 
cents). The effective hourly rate (based on how 
much time turkers took to actually finish each job) 
was $0.87. 
6 Annotation Results 
6.1 Label Distribution 
We first examine the average distribution of labels 
across the 3 classes. Figure 1 shows the distribu-
tions (expressed as percentages of the number of 
utterances) for in-house and MTurk annotators, 
averaged across the 5 meetings. Observe that the 
distribution for the in-house annotators is far more 
skewed away from a uniform 33% assignment, 
whereas the label distribution of turkers is less 
skewed. The likely reason for this difference is that 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
In house v 
In house
Turker v 
Turker
In house v 
Turker
Ka
pp
a 
Ag
re
em
en
t
Figure 3. Agreement with in-house annotators when 
turker annotations are merged through voting. 
0.0
0.2
0.4
0.6
0.8
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40 Fraction of data w
ith agreem
ent criteria
Ka
pp
a 
Ag
re
em
en
t
Agreement Fraction of data
Figure 2. Average kappa agreement between in-house 
annotators, turkers, and in-house annotators and turkers. 
104
turkers have a poorer understanding of the meet-
ings, and are more likely than in-house annotators 
to make arbitrary judgments about utterances. This 
poor understanding perhaps also explains the large 
difference in the percentage of utterances labeled 
as important?for many utterances that are difficult 
to understand, turkers probably play it safe by 
marking it important.  
The error bars represent the standard deviations of 
these averages, and capture the difference in label 
distribution from meeting to meeting. While different 
meetings are likely to inherently have different ratios 
of the 3 classes, observe that the standard deviations 
for the in-house annotators are much lower than those 
for the turkers. For example, the percentage of utter-
ances labeled ?important? by in-house annotators 
varies from 9% to 22% across the 5 meetings, whe-
reas it varies from 30% to 57% for turkers, a much 
wider range. These differences in standard deviation 
persist for each meeting as well?that is, for any giv-
en meeting, the label distribution of the turkers varies 
much more between each other than the distribution 
of the in-house annotators. 
6.2 Inter-Annotator Agreement 
Figure 2 shows the kappa values for pairs of anno-
tators, averaged across the 5 meetings, while the 
error bars represent the standard deviations. The 
kappa between the two in-house annotators. (0.4) 
is well within the range of values reported in the 
summarization literature (see section 4). The kappa 
values range from 0.24 to 0.50 across the 5 meet-
ings. The inter-annotator agreement between pairs 
of turkers, averaged across the 10 possible pairs 
per meeting (5 choose 2), and across the 5 meet-
ings show that turkers tend to agree less between 
each other than in-house annotators, although this 
kappa (0.28) is still within the range of typical 
agreement (this kappa has lower variance because 
the sample size is larger). The kappa between in-
house annotators and turkers1 (0.19) is on the low-
er end of the scale but remains within the range of 
agreement reported in the literature, suggesting 
that Mechanical Turk may be a useful tool for 
summarization.  
                                                          
1
 For each meeting, we measure agreement between every 
possible pair of annotators such that one of the annotators was 
an in-house annotator, and the other a turker. Here we present 
the average agreement across all such pairs, and across all the 
meetings. 
6.3 Agreement after Voting 
We consider merging the annotations from mul-
tiple turkers using a simple voting scheme as fol-
lows. For each utterance, if 3, 4 or 5 annotators 
labeled the utterance with the same class, we la-
beled the utterance with that class. For utterances 
in which 2 annotators voted for one class, 2 for 
another and 1 for the third, we randomly picked 
from one of the classes in which 2 annotators voted 
the same way. We then computed agreement be-
tween this ?voted turker? and each of the two in-
house annotators, and averaged across the 5 meet-
ings. Figure 3 shows these agreement values. The 
left-most point on the ?Kappa Agreement? curve 
shows the average agreement obtained using indi-
vidual turkers (0.19) while the second point shows 
the agreement with the ?voted turker? (0.22). This 
is only a marginal improvement, implying that 
simply voting and using all the data does not im-
prove much over the average agreement of indi-
vidual annotators.  
 The agreement does improve when we consider 
only those utterances that a clear majority of anno-
tators agreed on. The 3rd, 4th and 5th points on the 
?Agreement? curve plot the average agreement 
when considering only those utterances that at least 
3, 4 and 5 turkers agreed on. The ?Fraction of da-
ta? curve plots the fraction of the meeting utter-
ances that fit these agreement criteria. For 
utterances that at least 3 turkers agreed on, the 
kappa agreement value with in-house annotators is 
0.25, and this represents 84% of the data. For about 
50% of the data 4 of 5 turkers agreed, and these 
utterances had a kappa of 0.32. Finally utterances 
for which annotators were unanimous had a kappa 
of 0.37, but represented only 22% of the data. It is 
particularly encouraging to note that although the 
amount of data reduces as we focus on utterances 
that more and more turkers agree on, the utterances 
so labeled are not dominated by any one class. For 
example, among utterances that 4 or more turkers 
agree on, 48% belong to the important class, 48% 
to unimportant class, and the remaining 4% to the 
neutral class. These results show that with voting, 
it is possible to select a subset of utterances that 
have higher agreement rates, implying that they are 
annotated with higher confidence. For future work 
we will investigate whether a summarization sys-
tem trained on only the highly agreed-upon data 
outperforms one trained on all the annotation data. 
105
7 Conclusions 
In this study, we found that MTurk can be used to 
create accurate transcriptions of spontaneous meet-
ing speech when using a two-stage corrective 
process. Our best technique yielded a disagreement 
rate of 15.1%, which is competitive with reported 
disagreement in the literature of 6-12%. We found 
that both fillers and out-of-vocabulary words 
proved troublesome. We also observed that the 
length of the utterance being transcribed wasn?t a 
significant factor in determining WER, but that the 
native language of the speaker was indeed a signif-
icant factor.  
 We also experimented with using MTurk for the 
purpose of labeling utterances for extractive sum-
marization research. We showed that despite the 
lack of training, turkers produce labels with better 
than random agreement with in-house annotators. 
Further, when combined using voting, and with the 
low-agreement utterances filtered out, we can iden-
tify a set of utterances that agree significantly bet-
ter with in-house annotations.  
 In summary, MTurk appears to be a viable re-
source for producing transcription and annotation 
of meeting speech. Producing high-quality outputs, 
however, may require the use of techniques such as 
ensemble voting and iterative correction or refine-
ment that leverage performance of the same task 
by multiple workers. 
References 
 
S. Banerjee and A. I. Rudnicky. 2007. Segmenting 
 meetings into agenda items by extracting implicit 
 supervision from human note-taking. In 
 Proceedings of IUI.  
S. Banerjee and A. I. Rudnicky. 2009. Detecting the 
 noteworthiness of utterances in human meetings.  In 
 Proceedings of SIGDial.  
C. Bennett and A. I. Rudnicky. 2002. The Carnegie 
 Mellon Communicator corpus. In Proceedings of 
 ICSLP.  
J. G. Fiscus. 1997. A post-processing system to yield     
 word error rates: Recognizer Output Voting Error 
 Reduction (ROVER). In Proceedings of ASRU 
 Workshop.  
M. Galley. (2006). A skip-chain conditional ran-
 dom field for ranking meeting utterances by im
 portance. In Proceedings of EMNLP.  
 
 
A. Gruenstein, I. McGraw, and A. Sutherland. 2009. A 
 self-transcribing speech corpus: collecting 
 continuous speech with an online educational game. 
 In Proceedings of SLaTE Workshop.  
F. Liu and Y. Liu. 2008. Correlation between 
 ROUGE and human evaluation of extractive 
 meeting summaries. In Proceedings of ACL-HLT.  
M. Marge, S. Banerjee, and A. I. Rudnicky. 2010.    
 Using the Amazon Mechanical Turk for 
 transcription of spoken language. In Proceedings 
 of ICASSP.  
S. Novotney and C. Callison-Burch. 2010. Cheap, fast 
 and good enough: Automatic speech recognition 
 with non-expert transcription. In Proceedings of 
 NAACL. 
G. Penn and X. Zhu. 2008. A critical reassessment of 
 evaluation baselines for speech  summarization. In 
 Proceedings of ACL-HLT.  
B. Roy and D. Roy. 2009. Fast transcription of un-
 structured audio recordings. In Proceedings of In
 terspeech. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
106
Appendix 
 
Transcription task HIT type 1: 
 
 
 
Transcription task HIT type 2: 
 
 
 
Annotation task HIT: 
 
 
107
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 131?133,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
The Ngram Statistics Package (Text::NSP) - A Flexible Tool for Identifying
Ngrams, Collocations, and Word Associations
Ted Pedersen?
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Satanjeev Banerjee
Twitter, Inc.
795 Folsom Street
San Francisco, CA 94107
Bridget T. McInnes
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Saiyam Kohli
SDL Language Weaver, Inc.
6060 Center Drive, Suite 150
Los Angeles, CA 90045
Mahesh Joshi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
Ying Liu
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Abstract
The Ngram Statistics Package (Text::NSP)
is freely available open-source software that
identifies ngrams, collocations and word as-
sociations in text. It is implemented in Perl
and takes advantage of regular expressions to
provide very flexible tokenization and to allow
for the identification of non-adjacent ngrams.
It includes a wide range of measures of associ-
ation that can be used to identify collocations.
1 Introduction
The identification of multiword expressions is a key
problem in Natural Language Processing. Despite
years of research, there is still no single best way
to proceed. As such, the availability of flexible and
easy to use toolkits remains important. Text::NSP
is one such package, and includes programs for
counting ngrams (count.pl, huge-count.pl), measur-
ing the association between the words that make up
an ngram (statistic.pl), and for measuring correlation
between the rankings of ngrams created by differ-
ent measures (rank.pl). It is also able to identify n-
th order co-occurrences (kocos.pl) and pre?specified
compound words in text (find-compounds.pl).
This paper briefly describes each component of
NSP. Additional details can be found in (Banerjee
and Pedersen, 2003) or in the software itself, which
is freely available from CPAN 1 or Sourceforge 2.
?Contact author : tpederse@d.umn.edu. Note that authors
Banerjee, McInnes, Kohli and Joshi contributed to Text::NSP
while they were at the University of Minnesota, Duluth.
1http://search.cpan.org/dist/Text-NSP/
2http://sourceforge.net/projects/ngram/
2 count.pl
The program count.pl takes any number of plain
text files or directories of such files and counts the
total number of ngrams as well their marginal to-
tals. It provides the ability to define what a token
may be using regular expressions (via the --token
option). An ngram is an ordered sequence of n to-
kens, and under this scheme tokens may be almost
anything, including space separated strings, charac-
ters, etc. Also, ngrams may be made up of nonadja-
cent tokens due to the --window option that allows
users to specify the number of tokens within which
an ngram must occur.
Counting is done using hashes in Perl which are
memory intensive. As a result, NSP also provides
the huge-count.pl program and various other huge-
*.pl utilities that carry out count.pl functionality us-
ing hard drive space rather than memory. This can
scale to much larger amounts of text, although usu-
ally taking more time in the process.
By default count.pl treats ngrams as ordered se-
quences of tokens; dog house is distinct from house
dog. However, it may be that order does not always
matter, and a user may simply want to know if two
words co-occur. In this case the combig.pl program
adjusts counts from count.pl to reflect an unordered
count, where dog house and house dog are consid-
ered the same. Finally, find-compounds.pl allows a
user to specify a file of already known multiword ex-
pressions (like place names, idioms, etc.) and then
identify all occurrences of those in a corpus before
running count.pl
131
3 statistic.pl
The core of NSP is a wide range of measures of
association that can be used to identify interest-
ing ngrams, particularly bigrams and trigrams. The
measures are organized into families that share com-
mon characteristics (which are described in detail in
the source code documentation). This allows for an
object oriented implementation that promotes inher-
itance of common functionality among these mea-
sures. Note that all of the Mutual Information mea-
sures are supported for trigrams, and that the Log-
likelihood ratio is supported for 4-grams. The mea-
sures in the package are shown grouped by family
in Table 1, where the name by which the measure is
known in NSP is in parentheses.
Table 1: Measures of Association in NSP
Mutual Information (MI)
(ll) Log-likelihood Ratio (Dunning, 1993)
(tmi) true MI (Church and Hanks, 1990)
(pmi) Pointwise MI (Church and Hanks, 1990)
(ps) Poisson-Stirling (Church, 2000)
Fisher?s Exact Test (Pedersen et al, 1996)
(leftFisher) left tailed
(rightFisher) right tailed
(twotailed) two tailed
Chi-squared
(phi) Phi Coefficient (Church, 1991)
(tscore) T-score (Church et al, 1991)
(x2) Pearson?s Chi-Squared (Dunning, 1993)
Dice
(dice) Dice Coefficient (Smadja, 1993)
(jaccard) Jaccard Measure
(odds) Odds Ratio (Blaheta and Johnson, 2001)
3.1 rank.pl
One natural experiment is to compare the output of
statistic.pl for the same input using different mea-
sures of association. rank.pl takes as input the out-
put from statistic.pl for two different measures, and
computes Spearman?s Rank Correlation Coefficient
between them. In general, measures within the same
family correlate more closely with each other than
with measures from a different family. As an ex-
ample tmi and ll as well as dice and jaccard differ
by only constant terms and therefore produce identi-
cal rankings. It is often worthwhile to conduct ex-
ploratory studies with multiple measures, and the
rank correlation can help recognize when two mea-
sures are very similar or different.
4 kocos.pl
In effect kocos.pl builds a word network by finding
all the n-th order co-occurrences for a given literal
or regular expression. This can be viewed somewhat
recursively, where the 3-rd order co-occurrences of
a given target word are all the tokens that occur with
the 2-nd order co-occurrences, which are all the to-
kens that occur with the 1-st order (immediate) co-
occurrences of the target. kocos.pl outputs chains of
the form king -> george -> washington,
where washington is a second order co-occurrence
(of king) since both king and washington are first
order co-occurrences of george. kocos.pl takes as
input the output from count.pl, combig.pl, or statis-
tic.pl.
5 API
In addition to command line support, Test::NSP of-
fers an extensive API for Perl programmers. All of
the measures described in Table 1 can be included
in Perl programs as object?oriented method calls
(Kohli, 2006), and it is also easy to add new mea-
sures or modify existing measures within a program.
6 Development History of Text::NSP
The Ngram Statistics Package was originally imple-
mented by Satanjeev Banerjee in 2000-2002 (Baner-
jee and Pedersen, 2003). Amruta Purandare in-
corporated NSP into SenseClusters (Purandare and
Pedersen, 2004) and added huge-count.pl, com-
big.pl and kocos.pl in 2002-2004. Bridget McInnes
added the log-likelihood ratio for longer ngrams
in 2003-2004 (McInnes, 2004). Saiyam Kohli
rewrote the measures of association to use object-
oriented methods in 2004-2006, and also added
numerous new measures for bigrams and trigams
(Kohli, 2006). Mahesh Joshi improved cross plat-
form support and created an NSP wrapper for Gate
in 2005-2006. Ying Liu wrote find-compounds.pl
and rewrote huge-count.pl in 2010-2011.
132
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistics Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D. Blaheta and M. Johnson. 2001. Unsupervised learn-
ing of multi-word verbs. In ACL/EACL Workshop on
Collocations, pages 54?60, Toulouse, France.
K. Church and P. Hanks. 1990. Word association norms,
mutual information and lexicography. Computational
Linguistics, pages 22?29.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Us-
ing statistics in lexical analysis. In U. Zernik, editor,
Lexical Acquisition: Exploiting On-Line Resources to
Build a Lexicon. Lawrence Erlbaum Associates, Hills-
dale, NJ.
K. Church. 1991. Concordances for parallel text. In
Seventh Annual Conference of the UW Centre for New
OED and Text Research, Oxford, England.
K. Church. 2000. Empirical estimates of adaptation:
The chance of two noriegas is closer to p/2 than p2.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
180?186, Saarbru?cken, Germany.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
S. Kohli. 2006. Introducing an object oriented design to
the ngram statistics package. Master?s thesis, Univer-
sity of Minnesota, Duluth, July.
B. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master?s thesis,
University of Minnesota, Duluth, December.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence,
pages 455?460, Portland, OR, August.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
133
