Learning Word Clusters from Data Types 
Pao lo  A l legr in i ,  S imonet ta  Montemagn i ,  V i to  P i r re l l i  
Ist ituto di Linguistica Comlmtazionale - CNR 
Via della Faggiola 32, Pisa, Italy 
{ allegrii),simo,vito } @ilc.pi.cnr.it 
Abst ract  
The paper illustrates a linguistic knowledge ac- 
quisition model making use of data types, in- 
finite nlenlory, and an inferential mechanism 
tbr inducing new intbrmation Dora known data. 
The mode\] is colnpared with standard stochas- 
tic lnethods applied to data tokens, and tested 
on a task of lexico semantic lassification. 
1 I n t roduct ion  and  Background 
Of late, consideral)le interest has been raised by 
the use of local syntactic contexts to automati- 
cally in&me lexico-semantic classes from parsed 
corI)ora (Pereira and Tishby 1992; Pereira ct 
al. 1993; Rooth 1995; Rooth et al 1999). This 
family of approaches takes a 1)air of words (usu- 
ally a verb plus a noun), and a syntactic rela- 
tion holding 1)etween the two in context (llSll- 
ally the ol)ject), and calculates its token distri- 
bution in a training corI)us. These (:omits de- 
line the range of more or less tyi)ical syntac- 
tic collocates elected by a verb. The semat> 
tic similarity between words is then delined in 
terms of sul)stitutability in local contexts (see 
also Grefenstette 1994; I.,in 1998): two verbs 
are semantically close if they typically share the 
same range of collocates; conversely, two nouns 
are semantically close if they take l)art in the 
same tyl)e of selection dependencies, i.e. if they 
arc selected t)y the same verbs, with the same 
function. \]~q'onl this perspective, a syntactically 
asymmetric relation (a dependency) is reinter- 
preted as a semantic co-selection, where each 
term of the relation can be defined with respect 
to the other. 
This symmetric similarity metric is often ac- 
(;omi)anied by the non trivial assuml)tion that 
th, c ,semantic classification of both vcrb,s and 
nouns be symmetric too. This is enforced by 
maximizing I7 ?tj), with  
to 
1--1 
where p(Ck) is the probability of class Ck 1)e- 
ing tbund in the training corpus, and p(vi\[Ck) 
and p(nj\[Ci~) define the probability that verb vi 
and noun nj be associated with the semantic 
dimension (or meaning component) of class C/~. 
Intuitively, the joint distribution of fimctiona.lly 
mmotated verb-noun pairs is accounted for t)y 
assuming thai; each l)air meml)er indel)endently 
correlates with the same semantic dimension, or 
selection type (Rooth 1995), a concel)tual pair 
de.fining all pairs in the class: <g. "scalar mo- 
tion", "communicative action" etc. 
The al)proach has the potential of dealing 
with polysemous words: as the same word can 
in principle belong to more than one (-lass, there 
are good reasons to expect hat the correspond- 
ing selection type positively correlates with one 
and only one sense of polysemous words. A fur- 
ther t)omls of the al)I)roach is that it makes it ex- 
i)licit the perspectivizing factor underlying the 
discovered similarity of words in a class. 
On a less positive side, poorly selective verbs 
(i.e. verbs which potentially combine with any 
1101111) Sllch as give, find or get tend to stick to- 
gether in highly probable classes, 1)ut apl)ear to 
stake out rather uIfinlbrmative senlantic dimen- 
sions, relating a motley collection of 11011118, such 
as part, way, reason and problem (Rooth 1995), 
whose only cominonality is the property of be- 
ing freely interchangeable in the context of the 
above-mentioned verbs. 
Another related issue is how many such di- 
nlens ions are necessary to account br the entire 
variety of senses attested in the training corpus. 
This is an empirical question, but we contend 
an inipor~ant one, as the usal)ility of the result;- 
lag (:lasses heavily dot)ends (m it. It is (:ommon 
knowledge that verbs can 1)c, exceedingly (-hoosy 
ill tile way they select their collocates. Hence, 
one is Mlowed to use the class Ck to make t)re - 
dictions about the set of collocates of a verb 
v~, only if P(v,:\]C~) is sufficiently high. Con- 
versely, if CI~ hal)pens to poorly (:orrelate with 
any verb, the set of nouns in Ck is unlikely 1;o 
reflect any lexical selection. This coml)ounds 
wii;h the 1)rol)lent hat the meaning of a verb vi 
can significantly involve more lihalt Olte Selltall- 
tic (timension: at i;he plesenl; stage of research 
in (:Olnlmtat;iollal lexical SO, lnanti('s, Ire scholar 
has shown what fun(:tion relal;es l;lm nmaning 
components of vi to its sehx:tional behaviom'. 
There is wide room for flu'ther resear(:h in this 
area, but truly ext)lorative tools are still needed. 
Finally, the des(:ribe(t method is a(:ul;c, ly 
t)rone to the 1)roblem of si)arse data. A ll;l)ough 
i,(cl,,,) is rightly ext)e(:ted to converg(, faster 
,:ha:, p(,,l.,), still (:o,,vergcu(:e of,(Cl ' , , )  ,:al, b(, 
ex(:(;edingly sh)w with low frequen('y nouns. It 
is moo|; i;ll;~I; sieving more and more (:()rims (tat~ 
is a solution in all (:ases, its word fl'cquen(:y is 
highly sensitive to changes in text genre, topic 
and domain (S(:hiitze and Pede, rsen 1993). 
2 The  approach  
Ih~re we illustrate a (lifl'er(mt; at)l)roa('h t() a(> 
(is|ring lexico semant;i(" (:lasses from sy~,ta('t;i- 
(:ally lo(-al ('ontexts. Like the family of sto(:has- 
tie metho(ts of se(:tion 1, we make use of ~t 
simibu:ity ntel;ric 1)ased on sul)stitui;ability ill 
(ve, rb,noun,flntction) tril)les. We also share the 
assumption that lexi(:o semantic lasses are in- 
herently multidimensional, as they heavily de- 
pend on cxis|;ence of a perspectivizing factor. 
Yet, we depart from other assmnt)tions. Clas- 
sification of verl)s an(l n(mns is asymmetric: 
two IIOIIIIS {Ll'e similar if (;hey collo(:ate with as 
many semantically diverse vcrl)s as possible in 
as many (tifli:rent syntactic ontexts as l)ossit)le. 
The converse apl)lies to verbs. In other words, 
semantic similarity of nom:s is not conditional 
on the similarity of their ac(:oml)anying verbs, 
and vicevcrsa. In a sells(',, classification brc, aks 
th, c symmetry: maximization of the silnilarity 
of nouns (verbs) may cause minimization of the 
similarity of i;heir accoml)anying verbs (nouns). 
A (:lass where a maximum of noun similarity 
correlates with a lni~ximmn of verb similarity 
cm~ be uninforniative, as exeml)litied above by 
the ease of t)oorly selective verbs. 
Secondly, we assmne (following Fodor 1998) 
that the number of t)erst)ectivizing factors gov- 
erning lexieal selection may have the order of 
magnitude of the lexicon itself. The use of 
global semantic dimensions may smooth out lex- 
ical t)refcrences. This is hardly what we need 
to semantically annotate lexical l)reti;rences. A 
more conservative al)proa(:h to the t)rol)lem, in- 
ducing local semantic ('lasses, (:an (:oml)ine al)- 
1)licability to real language 1)recessing l)roblen~s 
with the fln'l;lmr t)omls of exploring a relatively 
mmharted territory. 
Thirdly, p(vi, nj) ai)t)ears to be too sensitive 
to changes in text genre, tol)ic lind domain to 
be eXl)ectcd to converge reliably. We prefer to 
ground a similarity metric oIx measuring the cor- 
relation among verb noun, type, s ratlw, r than |e- 
ke, as, tbr two basic reasons: i) verl) noun types 
arc (tis(:retc, ;m(t less l)rone t;o random varia- 
tion in it (parsed) (:orpus, ii) verl) noun tyl)eS 
(:ml reliably l)e a(:(tuir(xl from highly intbrm~ttive 
trot hardly redundant knowledge sources uch as 
h~xi(:a n(1 encyclot)aedias. 
Finally, our information refit tbr measuring 
wor(t similarity is not a (:Oul)le of context 
sharing pairs (e.g. (set, sta.ndard,obj) and 
(sct, re.cord,obj)) but a qv, ad'r'uplc, of such con- 
text;s, tbrme(t 1)y (:ombining two verbs with two 
. ( , . ) ) s  ( , . , .  
a.d  ), 
such that they enter an av, ah)gical proportion. 
2.1 The  analogica l  p ropor t ion  
In the t)resenl; conl;ext, an anah)gieal prot)ortion 
(hereafl;er AP)  is a quadrui)le of flmctionally 
mmotated t)airs resulting from tile combination 
of any two ltOllllS 'l~, i and ~3.j wit;h any two verbs 
v/,. and vt su(:h as (2) holds: 
(v~.,ni,f,,,) : (v~.,nj,L,,)= 
(v,,ni, fn) : (vt,nj,fiz), (2) 
where terms along the two diagonals can sw~p 
t)lace in the 1)rot)ortion , and identity of sub- 
s(:ript indi(:ates identity of wflues. Three aspects 
of (2) are worth eml)hasizing in this context. 
First, it does not require that the stone syn- 
tactic time|ion hold 1)etween all pairs, but only 
that time|ions be pairwisc identical. Moreover, 
(2) does not cover all possible syntactic ontexts 
where hi, uj, "vk and vt may coral)|he, but only 
th, ose where verb and .function values co-vary. 
(.set, standard, obj) : (,set, record, obj) = 
(meet, standard, obj) : (,,,.tet:,record, x) (3) 
We call this constraint ile "same-verb-same 
flmction" principle. As we will see in section 2.3, 
the principle has important consequences on tile 
sort of similarity induced by (2). Finally, if one 
uses subscripts as tbrmal constraints on type 
identity, then any term can be derived from (2) 
if the values of all other terms are known. For 
example given tile partially instantiated propof  
tion in (3), the last term is filled in unambigu- 
ously by substituting x = fn = obj. 
AP  is an important generalization of the 
inter-substitutabil ity assumption, as it extends 
tile assumption to cases of flmctionally hetero- 
geneous verb-nonn pairs. Intuitively, an AP 
says that, for two nouns to be taken as sys- 
tematically similar, one has to be ready to 'ase 
th, cm interchangeably in at lea,st wo different lo- 
cal contexts. This is where the inferential and 
the classificatory perspectives meet. 
2.2 Mathemat ica l  background 
We gave reasons for defining the similarity met- 
ric as a flmction of verb-noun type correlation 
rather than verb noun token correlation. In 
this section we sketch the mathematical frame- 
work underlying this assumption, to show that, 
tbr a set of verb nonn pairs with a unique syn- 
tactic function, AP  is the smallest C that sat- 
isfies eq.(1). 
Eq.(1) says that vi and nj are conditionally 
independent given C, meaning that their corre- 
lation only det)ends on the probability of their 
belonging to C, as tbrmally restated in eq.(4). 
p(n, vlC) = p(nlC)p(vlC) (4) 
In passing from token to type frequency, we as- 
sume that a projection operator simply assigns 
a mfitbrm type probability to each event (pair) 
with a nonzero token probability in the train- 
ing corpus. From a learning perspective, this 
corresponds to the assumption that an infinite 
memory filters out events already seen during 
training. The type probability pT(n,v) is de- 
fined as in eq.(5), where Np is the number of 
different pairs attested in the training corpus. 
pT(n,v) = 1/Np if the pair is attested, 
pT(n,v) = 0 otherwise. (5) 
By eq.(4), pT(n, vlC ) ? 0 if and only if 
pT(nlC) ~ 0 and pT(vlC) ~ O. This amounts 
to saying that all verbs in C are freely inter- 
changeable in the context of all nouns in C, and 
viceversa. We will hereafter efer to C as a sub- 
stitutability island ( SI). AP  can accordingly be 
looked at as the minimal SI. 
The strength of correlation of nouns and 
verbs in each S I  can be measured as a sum- 
mation over the strength of all APs where they 
enter. Formally, one can define a correlation 
score or(v, n) as the probability of v and n be- 
ing attested in a pair. This can be derived from 
our definition of pr(v ,n) ,  as shown in eq.(6), 
by substituting pT(n,v) = pr(v)pT(nlv) and 
pT(nlv ) = 1/w(v), where w(a) is tile type fi'e- 
quency of a (i.e. number of different attested 
pairs containing a). 
1 
- = = 
G 
(6) N,, G 
Eq.(6), after simplification, yields the tbllowing 
o< (7) 
By the same token, the correlation flmction 
c7(AP) relative to the 4 possit)le pairs in AP 
is calculated as 
cr(dP) = 1)7'('/)11~3,1 )PSF (~?,2 \[V\] )pT(V2 I'rl,2)PT(7~l \[?)2 ) 
(3( \[C0(~%1)C0(Vl )C0(~'1,2)C0('02)\] -1. (g) 
Eq.(8) captures the intuition that the corre- 
lation score between verbs and nouns in AP 
is an inverse function of their type frequency. 
Nouns and verbs with high type frequency oc- 
cur in many different pairs: the less selective 
they are, the smaller their semantic contribu- 
tion to cr(AP). 
Our preference tbr a(AP) over el(v, n) under- 
lies the definition of correlation score of S I  given 
in eq.(9) (see also section 4). 
-_- Z (9) 
APESI 
2.3 Breaking the symmetry  
In section 2.2 we assumed, for the sake of sim- 
plicity, that verbs and nouns are possibly re- 
lated through one syntactic function only. In a 
10 
proportion like (2), however, l;he syntactic time- 
tion is allowed to wtry. Nonetheless each rclal;ed 
S\] contains nouns which always combine with 
a given verb with one and the .same syntactic 
./:unction. Clearly, the Sallle is no|; true of verbs. 
Suppose that an S1 contains two verbs vk and vt 
(say drive and pierce) and two nouns ni and nj 
(say nail and peg) that ~re respecl;ively el)jeer 
and subject of 'vl~ and yr. The type of similar- 
ity in the resulting n(mn and verb clusters is 
of a completely ditti;rent nature: in the case of 
n(mns, we acquire dist'rib,utionally parallel words 
(e.g. nail and peg); in the case of verbs, we 
get distrib'utionaIly correlated words (say drive 
and pierce) which are not interchangeable in the 
same conl;exl;. Mixing the two types of distribu- 
tional similarity in the same class makes little 
sense. Hereafter, we will aim at maximizing the 
similarity of disl;ributionally parallel nouns. In 
doing so, we will use functionally hel;erogencous 
contexts as in (2). This breaks classitication 
symlne|,ry, and there is no guarantee I;hal; se- 
mantically coher(mt verb clust;ers be rcl;m'ncd. 
3 The  method  
The section illusl;ratcs an at)plication of the 
principles of section 2 1;() 1;t5o task of clustering 
the set of object;s ot! a vo, rl) on t;he basis of a 
repository of flmctionally mmol;al;ed cont(;xts. 
3.1 The  knowledge  base 
The training evidence is a Knowledge. Base 
(KB)  of flm('tionally anno|;ated verb noun 
1)airs, ins|;mltiating a wide rallg~e of syntactic re- 
lations: 
a) vert)objecl;, e.g. (,~'a,,,.~,,,'c, ~, 'ol, lc,,z, obj) 
'cause-1)roblenl'; 
b) verb subject, e.g. (capitare, pwblcma subj) 
'occur-problem'; 
c) verb prepositional_complement, e.g. (recap- 
pare, probIcma, in,) 'run_into-problenl'. 
The KaY contains 43,000 pair types, auto- 
matically extracted from different knowledge 
sources: dictionaries, both bilingual and mono- 
lingual (Montemagni 1995), and a corpus of ti- 
nancial newspapers (Federici st al. 1998). The 
two sources rettect two ditt'erent modes of lexi- 
cal usage: dictionaries give typical examples of 
use of a word, and rmming corpora attest ac- 
tual usage of words in specific enfl)edding do- 
mains. These differences have all impact on 
the typology of senses which the two sources 
1)rovi(le evidence for. General dictionaries tes- 
titly all possible senses of a given word; tyl)ical 
word collocates acquired from dictionaries tend 
to cover the entire range of possible senses of 
a headword. On the other hand, unrestricted 
texts reIlect actual usage and possibly bear wit- 
hess to senses which are relevant to a specific 
domain only. 
3.2 The  i nput  words  
There is abundant psycholinguistic evidence 
that semantic similarity between words is em- 
inently conI;exl; sensitive (Miller and Charles 
1991). Moreover, in many language processing 
tasks, word similarity is typically judged rela- 
tive to an actual context, as in the cases of 
syntactic disambiguation (both structural and 
fulwtional), word sense disambiguation, and se- 
lection of the contextually approt)riate transla- 
1;ion equiwflent of a word given its neighl)ouring 
words. Finally, close examination of real data 
shows that (titl'erellt word senses elect classes of 
complements according to different dilnensions 
of semantic similarity. This is so pervasive, that 
it soon be(:omes impossit)le to t)rovide an efl'ec- 
live account of these, dimensions independently 
of the sense in question. 
Ewduation of botll accuracy and usability of 
any autontatic classitication of words into se- 
mantic clusters cammt lint artificially elude th(; 
1)asic question "similar in what respc, ct;?". Our 
choice of input words retlects these concerns. 
\?e automatically clustered the set of objects of 
a given verb, as they arc attested in a test co l  
pus. This yields local lexico seman{;ic lasses, 
i.e. conditional on the selected verb head, as 
opposed to global classes, i.e. built once and 
tbr all to accomlt tbr the collocates of any verb. 
Among the practical advantages of local clas- 
sitication we should at least mention the follow- 
ing two. Choice of a verb head as a perspec- 
tivizing factor considerably reduces the possi- 
bility that the same polysemous object collocate 
is used in different senses with the same verb. 
Fnrthermore, the resulting clusters can give in- 
tormal;ion about the senses, or meaning facets, 
of the verb head. 
a.a Ident i f icat ion and ranking of  noun 
clusters 
For the sake of concreteness, let us consider the 
tbllowing object-collocates of the Italian verb 
11 
{APPESANTIMENTO~ CRESCITA, 
FLESSIONE, RIALZO} 
{CRESCITA, FLESSIONE} 
{CRESCITA, GUAI0} 
{CRESCITA, PROBLEMA} 
{CRESCITA, RITARD0} 
{FLESSIONE, PROBLEMA} 
{FLESSIONE, RIALZO} 
{GUAI0, PROBLEMA} 
{RIDIMENSIONAMENT0, RITARD0} 
Figure h Some Sis 
headword causarc. 
: CAUSARE/O, REGISTRARE/O 
: CAUSARE/O, EVIDENZIARE/S, 
MEDIARE/O, MOSTRARE/O~ 
PRESENTARE/S, PRESENTIRE/O 
REGISTRARE/O, REGISTRARE/$ 
: CAUSARE/0, PROVOCARE/0 
AVERE/S~ CAUSARE/0, EVIDENZIARE/0 
PORRE/S, PRESENTARE/S 
CAUSARE/O, USARE/S 
CAUSARE/0, PRESENTARE/S, STARE/S 
CAUSARE/0, REGISTRARE/0 
REGISTRARE/S, SUBIRE/0 
CAUSARE/0, CAVARE -- SI/S, INCAPPARE/S 
CAUSARE/O, GIUSTIFICARE/O 
relative to the collocates of the 
causare 'cause', as they are found in a test cor- 
pus: 
appcsantimento 'increase in weight', 
crescita 'growth', flessionc 'decrease', 
guaio 'trouble', p~vblcma 'prol)lem', rialzo 
'rise', ridimensionamcnto 'reduction', 
ritardo 'delay', turbolenza 'turbulence'. 
Clustering these input words requires prelim- 
inary identification of Substitutability Islands 
(Sis). An example of SI  is the quadruple 
tbrmed by the verb pair causare 'cause' and in- 
eapparc 'rnn into' and the noun pair guaio 'trou- 
ble' and problema 'problem', where menfl)ers of 
the same pair are inter-substitutable in context, 
give:: the constraints entbrced by the AP type 
in (2). Note that guaio and problema are ob- 
jects of eausare, and prepositional complements 
(headed by in 'in') of incappare. This makes it 
possible to maximize the sinfilarity of trouble 
and problem across fimctionally heterogeneous 
contexts. 
Bigger Sis than the one just shown will form 
as many APs as there are quadruples of col:- 
textually interchangeable nouns and verbs. We 
consider a lexico-semantic cluster of nouns the 
projection of an SI  onto the set of nouns. Fig.1 
illustrates a sample of noun clusters (between 
curly brackets) projected from a set of Sis, to- 
gether with a list of the verbs tbund in the same 
Sis (the suffix 'S' stands tbr subject, and 'O' 
for object). Due to the asymmetry of classifica- 
tion, verbs in Sis are not taken to tbrm part of 
a lexico-semantic cluster in the same sense as 
nonns  are .  
7.04509e-O5{GUAIO,PKOBLEMA} 
7.01459e-O5{RIDIMENSIONAHENTO,RITARDO} 
4.65858e-O5{CRESCITA,FLESSIONE} 
I.T5699e-O5{FLESSIONE,RIALZO} 
9.49509e-O6{APPESANTIMENTO,CRESClTA,FLESSIONE,RIALZO} 
1.88964e-O6{CRESCITA,GUAIO} 
1.19814e-O6{CRESCITA,RITARDO} 
8.84254e-OT{CRESCITA,PROBLEMA} 
6,TI41e-OT{FLESSIONE,PROBLEMA} 
Figure 2: Nine top-most scored noun clusters 
Not all projected noun clusters exhibit the 
same degree of semantic oherence. Intuitively, 
the cluster {appesantimento crescita flessione 
riaIzo} 'increase in weight, growth, decrease, 
rise' is semantically more appeMing tlmn the 
cluster {crescita problema} 'growth problem' 
(Fig.l). 
A quantitative measure of the sen:antic ohe- 
sion of a noun cluster CN is give:: by the con'e- 
lation score ~(SI) of the SI  of which UN is a 
projection. In Fig.2 noun clusters are ranked by 
decreasing vahms of cr(SI), calculated according 
to eq.(9). 
3.4 Cent ro id  ident i f i ca t ion  
Noun clusters of Figs.1 and 2 are admittedly 
considerably fine grained. A coarser grain can 
be attained trivially through set union of inte:'- 
secting clusters. In fact, what we want to obtain 
is a set of mazimally orthogonal and semanti- 
cally coherent noun classes, under the assump- 
tion that these (:lasses highly correlate with the 
principal meaning components of the verb head 
of which input nouns are objects. 
In the algorithm ewfluated here this is 
achieved in two steps: i) first, we select the 
best possible centroids of the prospective classes 
among the noun clusters of Fig.2; secondly, 
ii) we lmnp outstanding clusters (i.e. clusters 
which have not been selected in step i)) around 
the identified centroids. In what tbllows, we will 
only focus on step i). Results and evaluation of 
step ii) are reported in (Allegrini et al 2000). 
In step i) we assume that centroids are 
disjunctively defined, maximally coherent 
classes; hence, there exists no pair of intersect- 
ing centroids. The best possible selection of 
centroids will include non intersecting clusters 
with the highest possible cumulative score. In 
practice, the best centroid corresponds to the 
12 
cluster with the topmost cr(SI). The second 
best centroid is the cluster with the second 
highest o-(SI) and no intersection with the first; 
centroid, and so on (the i-th centroid is 1;11(', 
i.-th highest chlster with no intersection with 
the tirst i - 1. centroids) until all clusters in the 
rank are used Ill). Clusters selected as centroids 
in the causare example above ~tre: {GUAIO 
PROBLEMA}, {RIDIMENSIONAMENTO RITARDO}, 
{CaP.SCITA FL~.SS-rONE}. 
Clearly, this is not the only t)ossible strategy 
t'or centroid selection, lint certainly a suitabh; 
one giv(;n our assulnl)tions mM goals. To stun 
Ul) , the targeted classification is local, i.('., con- 
ditional on a specific verl~ head, and orthogonal , 
i.c. it aims at identifying maximally disjulmtive 
classes with high correlation with the principal 
meaning ('Oral)orients of the vert) head. This 
strategy h;ads to identificalfion of the different 
senses, or possibly me,ruing facets, of a vert). 
In tlteir turn, noun clusters may capture sul)- 
tle semm~tic distinctions. For instance,, a dis- 
tinction is made between incremental eveni;s or 
results of incrt'anental e, vents, which 1)resut)l)OSe 
~ scalar dimension (as in the ease of {cre,,s'cita 
,/lcssione} 'growth, decrease') and re, scheduling 
eve, nts, where a change occurs with respect to 
a previously planned event or object (see the 
centroid {ridimensiov, amento  ritardo} :reduc- 
tion debw' ). 
4 Exper iment  and  eva luat ion  
We, were able to extract all S l 's relative to the 
entire K\]3.  However, we report here an intr ins ic 
evaluation of the accuracy of acquired ce.ntroids 
which involves ()lily a small subset of our results, 
since provision of a refhrence class tyl)ology is 
extremely labour intensive. 1 
We consider 20 Italian verbs and their object 
collocates.gThe object collocates were automat- 
ically extracted fi'om the "Italian SPAIIKLE 
Reference Corpus", a corpus of Italian financial 
1For an extrinsic evahlation of the proposed similar- 
ity measure the reader is referred to (Montemagni et aI. 
1996; Briscoe ct al. 1999; Federici ct al. 1999a). 
2The |x,'st verbs are: a.qgiungcrc 'add', aiutare 'hell)' , 
a,sl)cttarc 'expect', cambiarc 'change', C(t*taO,?'t: t(:}tllSe:~ 
chicdcrc 'ask', considc.rarc 'consider', dare. 'give', dc- 
ciderc 'decide', fornive 'provide', muoverc. 'move', pcrm~'.- 
ttere 'allow', portarc 'bring', p~wlurrc 'produce', sccglicrc 
'choose', sentirc 'feel', stabilire 'establislF, tagliarc 'cut', 
terminate 'end', trovarc 'find'. 
newspapers of about one million word tokens 
(Federici ct al. 1998). 
l?or each test verb, an indetmndent classifica- 
tion of its collocates was created lnanually, by 
partitioning the collocates into disjoint sets of 
semantically coherent lexical preferences, each 
set pointing to distinct senses of the test; verb, 
according to a reference monolingual dictionary 
(Garzanti 1984). This considerably reduces the 
anlount of subjectivity inevitably involved in 
the creation of a reference partition, and mini- 
mizes the probability that more than one sense 
of a t)olysemous noun can appear in the same 
class of collocates. 
The inferred centroids, selected from clusters 
ranked by c~(SI) defined as in (9), are t)rojected 
~gainst he reference classification. Precision is 
delined as the ratio between 1;t1(; mmflmr of con- 
troids t)roperly inchlded in one reference class 
and l;he nmnber of inferred centroids. Recall is 
defined as the ratio between the number of relhr- 
time classes which properly inchlde at least one 
centroid an(t the nmnber of all reference classes. 
Fig.3 shows results for the sets of object collo- 
cates of 1)olysemous {;est verbs only, as lttOltOSe- 
mous verbs trivially yMd 100% precision recall. 
An average, wflue over the sets of object col- 
locates of all verbs is also shown, with 86% 
88% of precision recall. Another average value 
is also l)lotted (as a black ul)right triangle), ol)- 
l:ained \])y ranking n(mn clusters l)y ~(S\]) calcu- 
lated as ill (10). This average wflue (53% 53% 
precision recall) provides a sort of baseline of 
the difliculty of the task, and sheds consider- 
able light on the use of APs ,  rather than simple 
verb noun pairs, as inforlnation units ibr mea- 
suring internal cohesion of centroids. 
 (si) =_ (10) 
(n,v)G91 
5 Conc lus ion  
We described a linguistic knowledge acquisition 
model and tested it; on a word classification task. 
The main points of our prot)osal are: 
? classification is asymmetric, grounded on 
principles of machine learning with intinite 
memory; 
? the algorithm is explorative and non- 
reductionist; no a priori  model of class dis- 
13 
' i . . . . . . . . . . . .  T L . . . . . . . . .  i:i:  
! AIUTARE 
I ICAMBIARE 
0.9 CHIEDERE 
CONSIDERARE ? 
- bARE 
' ,?DECIDERE 
0.8 PORTARE 
PRODURRE 
? SCEGLIERE ~ SENTIRE STABILIRE 
~TAGLIARE \ [ :1~ ? 
~:TFIOVA RE 
0.6 ~'AVERAGE(AP) 
~AVERAGE(pai 0 
i ? i 
o.5 ::.i ? ' 1  
I 
i 
0.3 0.4 0.5 0.6 0,7 0.8 0.9 1 
PRECISION 
d < ~ 0.7 
Figure 3: Centroid precision and recall for object 
collocates of polysemous verbs. 
tril)ution is assmned; 
? classification is modelled z~s the task of 
forming a web of context dependent se- 
mantic associations among words; 
? the approach uses a context--sensitive no- 
tion of semantic similarity; 
? the approach rests on the notion of analog- 
ical proportion, which proves to t)e a reli- 
able intbrmation refit for measuring seman- 
tic similarity; 
? analogical t)roportions are harder to track 
down than simple pairs, and interconnected 
in a highly complex way; yet, reliance on 
data types, as opposed to token frequen- 
cies, makes the proposed method comtm- 
rationally tractable and resistant o data 
sparseness. 
References  
Allegrini P., Montemagni S., Pirrelli V. (2000) Con- 
trolled Bootstrapt)ing of Lexico-semantic Classes 
as a Bridge between Paradigmatic and Syntag- 
matic Knowledge: Methodology and Evaluation. 
In Precccdings of LREC-2000, Athens, Greece 
May-June 2000, pp. 601-608. 
Briseoe T., McCarthy D., Carroll J., Allegrini P., 
Calzolari N., Federici S., Montemagni S., Pir- 
relli V., Abney S., Beil F., Carroll G., Light M., 
Prescher D., Riezler S., Rooth M. (5999) Acqui- 
sition System for Syntactic and Semantic Type 
and Selection. Deliverable 7.2. WP 7, EC project 
SPARKLE "Shallow Parsing and Knowledge Ex- 
traction for Language Engineering" (LE-2111). 
Federici, S., Montemagni, S., Pirrelli, V. (1999) 
SENSE: an Analogy based Word Sense Disam- 
biguation System. In M. Light and M. Pahner 
(eds.), Special Issue of Natural Language Engi- 
neerin 9on Lexical Semantic Tagging. 
Federici, S., Montemagni, S., Pirrelli, V., Calzolari, 
N. (1998) Analogy-based Extraction of Lexical 
Knowledge from Corpora: the SPARKLE Expe- 
rience. In Proceedings of LREC-1998, Granada, 
SP, May 1998. 
Fodor, J.A. (1998) Concepts. Where Co.qnitivc Sci- 
ence Went Wzvng. Clarendon Press, Oxtbrd, 
1998. 
Garzanti (1984) ll Nuovo Dizionario Italiano 
Garzanti. Garzanti, Milano, 1984. 
Grefenstette, G. (1994) Explorations in Automatic 
Thesaurus Discovery. Kluwer Acadenfic Publish- 
ers, Boston, 1994. 
Lin D. (1998) Automatic Retrieval and Clustering 
of Similar Words. In Proceedings of COLING- 
A CL'98, Montreal, Canada, August 1998. 
Miller, G.A., Charles, W.G. (1991) Contextual Cor- 
relates of Semantic SimilariW. In Language and 
Cognitive Processes, 6 (1), pp. 1-28. 
Montemagni, S. (1.995) Subject and Object in Italian 
Sentence Processing. PhD Dissertation, UMIST, 
Manchester, UK, 1995. 
MontemagIfi, S., Federici, S., Pirrelli, V. (1996) 
Resolving syntactic ambiguities with lexico 
semantic patterns: an analogy-based apl)roach. 
In Proceedings of COLING-96, Copenhagen, Au- 
gust 1996, pp. 376 381. 
Pereira, F., Tishby, N. (1992) Distributional Sim- 
ilarity, Phase Transitions and Hierarchical Clus- 
tering. In Working Notes, Fall Symposium Series. 
AAAI, pp. 54 64. 
Pereira, F., Tishby, N., Lee, L. (1993) Distrilmtional 
Clustering Of English Words. In Proceedings of 
the 30th Annual Meeting of the Association for 
Computational Linguistics, pp. 183 190. 
Rooth, M. (Ms) Two-dimensional clusters in gram- 
matical relations. In Symposium on 12epresenta- 
tion and Acquisition of Lcxical Knowledge: Pol- 
ysemy, Ambiguity and Gcncrativity, AAAI 1995 
Spring Symposium Series, Stanford University. 
Rooth, M., Riezler, S., Prescher, D., Carroll, G., 
Bell, F. (1999) Inducing aSemanticallt Annotated 
Lexicon via EM-Based Clustering. In Procccdings 
of the 37th Annual Meeting of the Association for' 
Computational Linguistics, Maryland, USA, June 
1999, I)P. 104-111. 
Schfitze, H., Pedersen, J. (1993) A vector model 
for syntagmatic and paradigmatic relatedness. In 
Proceedings of the 9th Annual Confcrcncc of the 
UW Centrc for" the New OED and Text 12cscarch, 
Oxford, England, 1993, pp. 104-113. 
14 
Grammar and Lexicon in the Robust Parsing of Italian 
Towards a Non-Na?ve Interplay 
 
Roberto  
BARTOLINI 
Istituto di Linguistica 
Computazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
Alessandro  
LENCI 
Universit? di Pisa 
Via Santa Maria 36 
56100 PISA (Italy) 
Simonetta  
MONTEMAGNI 
Istituto di Linguistica Com-
putazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
Vito  
PIRRELLI 
Istituto di Linguistica 
Computazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
 
{roberto.bartolini, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it 
 
Abstract 
In the paper we report a qualitative evalua-
tion of the performance of a dependency 
analyser of Italian that runs in both a non-
lexicalised and a lexicalised mode. Results 
shed light on the contribution of types of 
lexical information to parsing.    
Introduction 
It is widely assumed that rich computational 
lexicons form a fundamental component of reli-
able parsing architectures and that lexical infor-
mation can only have beneficial effects on 
parsing. Since the beginning of work on broad-
coverage parsing  (Jensen 1988a, 1988b), the 
key issue has been how to make effective use of 
lexical information. In this paper we put these 
assumptions to the test by addressing the follow-
ing questions: to what extent should a lexicon be 
trusted for parsing? What is the neat contribution 
of lexical information to overall parse success? 
We present here the results of a preliminary 
evaluation of the interplay between lexical and 
grammatical information in parsing Italian using 
a robust parsing system based on an incremental 
approach to shallow syntactic analysis. The sys-
tem can run in both a non-lexicalised and a lexi-
calised mode. Careful analysis of the results 
shows that contribution of lexical information to 
parse success is more selective than commonly 
assumed,  thus raising the parallel issues of how 
to promote a more effective integration between 
parsers and lexicons and how to develop better 
lexicons for parsing.  
1 Syntactic parsing lexicons 
Syntactic lexical information generally feeds 
parsing systems distilled in subcategorization 
frames. Subcategorization is a formal specifica-
tion of a predicate phrasal context in terms of the 
type of arguments syntactically selected by the 
predicate entry (e.g. the verb hit selects for a 
subject NP and an object NP). Lexical frames 
commonly include: i.) number of selected argu-
ments, ii.) syntactic categories of their possible 
realization (NP, PP, etc.), iii.) lexical constraints 
on the argument realization (e.g. the preposition 
heading a PP complement), and iv.) the argu-
ment functional role. Other types of syntactic in-
formation that are also found in syntactic 
lexicons are: argument optionality, verb control, 
auxiliary selection, order constraints, etc. On the 
other hand, collocation-based lexical informa-
tion is only rarely provided by computational 
lexicons, a gap often lamented in robust parsing 
system development. 
A number of syntactic computational lexi-
cons are nowadays available to the NLP com-
munity. Important examples are LDOCE 
(Procter 1987), ComLex (Grishman et al 1994), 
PAROLE (Ruimy et al 1998). These lexicons 
are basically hand-crafted by expert lexicogra-
phers, and their natural purpose is to provide 
general purpose, domain-independent syntactic 
information, covering the most frequent entries 
and frames. On the other hand, parsing systems 
often complement general lexicons with corpus-
driven, automatically harvested syntactic infor-
mation (Federici et al 1998b, Briscoe 2001, 
Korhonen 2002). Automatic acquisition of sub-
categorization frames allows systems to access 
highly context dependent constructions, to fill in 
possible lexical gaps and eventually rely on fre-
quency information to tune the relative impact of 
specific frames (Carroll et al 1998). 
Lexicon coverage is usually regarded as the 
main parameter affecting use of lexical informa-
tion for parsing. However, the real comparative 
impact of the type (rather than the mere quan-
tity) of lexical information has been seldom dis-
cussed. Our results show that the contribution of 
various lexical information types to parse suc-
cess is not uniform. The experiment focuses on a 
particular subset of the information available in 
syntactic lexicons - the representation of PP 
complements in lexical frames - tested on the 
task of PP-attachment. The reason for this 
choice is that this piece of information occupies 
a central and dominant position in existing lexi-
cons. For instance in the Italian PAROLE lexi-
con, more than one third of verb frames contain 
positions realized by a PP, and this percentage 
raises up to the near totality noun-headed 
frames. 
2 Robust Parsing of Italian 
The general architecture of the Italian parsing 
system used for testing adheres to the following 
principles: 1) modular approach to parsing, 2) 
underspecified output (whenever required), 3) 
cautious use of lexical information, generally re-
sorted to in order to refine and/or further specify 
analyses already produced on the basis of 
grammatical information. These principles un-
derlie other typical robust parsing architectures 
(Chanod 2001, Briscoe and Carroll 2002). 
The system consists of i.) CHUNK-IT 
(Federici et al 1998a), a battery of finite state 
automata for non-recursive text segmentation 
(chunking), and ii.) IDEAL (Lenci et al 2001), a 
dependency-based analyser of the full range of 
intra-sentential functional relations (e.g. subject, 
object, modifier, complement, etc.). CHUNK-IT 
requires a minimum of lexical knowledge: 
lemma, part of speech and morpho-syntactic fea-
tures. IDEAL includes in turn two main compo-
nents: (i.) a Core Dependency Grammar of 
Italian; (ii.) a syntactic lexicon of ~26,400 sub-
categorization frames for nouns, verbs and ad-
jectives derived from the Italian PAROLE 
syntactic lexicon (Ruimy et al 1998). The 
IDEAL Core Grammar is formed by ~100 rules 
(implemented as finite state automata) covering 
major syntactic phenomena,1 and organized into 
structurally-based rules and lexically-based 
rules. IDEAL adopts a slightly simplified ver-
sion of the FAME annotation scheme (Lenci et 
al. 2000), where functional relations are head-
based and hierarchically organised to make pro-
vision for underspecified representations of 
highly ambiguous functional analyses. This fea-
ture allows IDEAL to tackle cases where lexical 
information is incomplete, or where functional 
relations cannot be disambiguated conclusively 
(e.g. in the case of the argument vs. adjunct dis-
tinction). A ?confidence score? is associated 
with some of the identified dependency relations 
to determine a plausibility ranking among dif-
ferent possible analyses. 
In IDEAL, lexico-syntactic information inter-
venes only after possibly underspecified de-
pendency relations have been identified on the 
basis of structural information only. At this sec-
ond stage, the lexicon is accessed to provide ex-
tra conditions on parsing, so that the first stage 
parse can be non-monotonically altered in vari-
ous ways (see section 3.3). This strategy mini-
mises the impact of lexical gaps (whether at the 
level of lemma or of the associated subcategori-
zation frames) on the system performance (in 
particular on its coverage). 
3 The Experiment 
3.1 The Test Corpus (TC) 
The test corpus contains a selection of sentences 
extracted from the balanced partition of the Ital-
ian Syntactic Semantic Treebank (ISST, Mon-
temagni et al 2000), including articles from 
 
1 Adjectival and adverbial modification; negation; (non-
extraposed) sentence arguments (subject, object, indirect 
object); causative and modal constructions; predicative 
constructions; PP complementation and modification; em-
bedded finite and non-finite clauses; control of infinitival 
subjects; relative clauses (main cases); participial construc-
tions; adjectival coordination; noun-noun coordination 
(main cases); PP-PP coordination (main cases); cliticiza-
tion. 
contemporary Italian newspapers and periodicals 
covering a high variety of topics (politics, econ-
omy, culture, science, health, sport, leisure, etc.). 
TC consists of 23,919 word tokens, correspond-
ing to 721 sentences (with a mean sentence 
length of 33.18 words, including punctuation to-
kens). The mean number of grammatical rela-
tions per sentence is 18. 
3.2 The Baseline Parser (BP) 
The baseline parser is a non-lexicalised version 
of IDEAL including structurally-based rules 
only. The mean number of grammatical relations 
per sentence detected by BP in TC is 15. 
The output of the baseline parser is shallow in 
different respects. First, it contains underspeci-
fied analyses, resorted to whenever available 
structural information does not allow for a more 
specific syntactic interpretation: e.g. at this level, 
no distinction is made between arguments and 
modifiers, which are all generically tagged as 
?complements?. Concerning attachment, the sys-
tem tries all structurally-compatible attachment 
hypotheses and ranks them according to a confi-
dence score. Strong preference is given to 
rightmost attachments: e.g. a prepositional com-
plement is attached with the highest confidence 
score (50) to the closest, or rightmost, available 
lexical head. In the evaluation reported in section 
4, we consider top-ranked dependents only, i.e. 
those enforcing rightmost attachment. Moreover, 
in matching the relations yielded by the parser 
with the ISST relations in TC we make allowance 
for one level of subsumption, i.e. a BP relation can 
be one level higher than its ISST counterpart in 
the hierarchy of dependency relations. Finally, the 
BP output is partial with respect to those depend-
encies (e.g. a that-clause or a direct object) that 
would be very difficult to identify with a suffi-
cient degree of confidence through structurally-
based rules only.  
3.3 The Lexically-Augmented Parser (LAP) 
The lexically-augmented version of IDEAL in-
cludes both structurally-based and lexically-
based rules (using the PAROLE lexicon). In this 
lexically-augmented configuration, IDEAL first 
tries to identify as many dependencies as possi-
ble with structural information. Lexically-based 
rules intervene later to refine and/or complete 
structurally-based analyses. Those structurally-
based hypotheses that find support in the lexicon 
are assigned the highest score (60). The contri-
bution of lexically-based rules is non-monotonic:
old relations can eventually be downgraded, as 
they happen to score, in the newly ranked list of 
possible relations, lower than their lexically-
based alternatives. Furthermore, specification of 
a former underspecified relation is always ac-
companied by a re-ranking of the relations iden-
tified for a given sentence; from this re-ranking, 
restructuring (e.g. reattachment of complements) 
of the final output may follow. 
LAP output thus includes: 
a) fully specified dependency relations: e.g. an 
underspecified dependency relation such as 
?complement? (COMP), identified by a struc-
turally-based rule, is rewritten, when lexi-
cally-supported, as ?indirect object? (OBJI) 
and assigned a higher confidence value; 
b) new dependency relations: this is the case, 
for instance, of that-clauses, direct objects 
and other relation types whose identification 
is taken to be too difficult and noisy without 
support of lexical evidence; 
c) underspecified dependency relations, for 
those cases that find no lexical support. 
The mean number of grammatical relations per 
sentence detected by LAP in TC is 16. In the 
evaluation of section 4, we consider top-ranked 
dependents only (confidence score  50), corre-
sponding to either lexically-supported dependency 
relations or ? in their absence ? to rightmost at-
tachments. Again, in matching the relations 
yielded by the parser with the ISST relations in 
TC we make allowance for one level of subsump-
tion. 
4 Analysis of Results 
The parsing outputs of BP and LAP were com-
pared and projected against ISST annotation to 
assess the contribution of lexical information to 
parse success. In this paper, we focus on the 
evaluation of how and to which extent lexico-
syntactic information contributes to identifica-
tion of the proper attachment of prepositional 
complements. For an assessment of the role and 
impact of lexical information in the analysis of 
dependency pairs headed by specific words, the 
interested reader is referred to Bartolini et al
(2002). 
4.1 Quantitative Evaluation 
Table 1 summarises the results obtained by the 
two different parsing configurations (BP and 
LAP) on the task of attaching prepositional 
complements (PC). Prepositional complements 
are classified with respect to the governing head: 
PC_VNA refers to all prepositional comple-
ments governed by V(erbal), N(ominal) or 
A(djectival) heads. PC_V is the subset with a 
V(erbal) head and PC_N the subset with a 
N(ominal) head. For each PC class, precision, 
recall and f score figures are given for the differ-
ent parsing configurations. Precision is defined 
as the ratio of correctly identified dependency 
relations over all relations found by the parser 
(prec = correctly identified relations / total num-
ber of identified relations); recall refers to the ra-
tio of correctly identified dependency relations 
over all relations in ISST (recall = correctly 
identified relations / ISST relations). Finally, the 
overall performance of the parsing systems is 
described in terms of the f score, computed as 
follows: 2 prec recall / prec + recall. 
 
BP LAP ISST 
Prec recall F score Prec recall f score 
PC_VNA 3458 75,53 57,40 65,23 74,82 61,02 67,22
PC_V 1532 75,43 45,50 56,76 74,23 49,50 61,22
PC_N 1835 73,53 80,82 77,00 72,76 81,36 76,82
Table 1. Prepositional complement attachment in BP and LAP 
 
Table 2. Lexicalised attachments 
 
To focus on the role of the lexicon in either con-
firming or revising structure-based dependen-
cies, lexically-supported attachments are singled 
out for evaluation in Table 2. Their cumulative 
frequency counts are reported in the first three 
columns of Table 2 (?Lexicalised attachments?), 
together with their distribution per head catego-
ries. Lexicalised attachments include both those 
structure-based attachments that happen to be 
confirmed lexically (?Confirmed attachments?), 
and restructured attachments, i.e. when a prepo-
sitional complement previously attached to the 
closest available head to its left is eventually re-
assigned as the dependent of a farther head, on 
the basis of lexicon look-up (?Restructured at-
tachments?). Table 2 thus shows the impact of 
lexical information on the task of PP attachment. 
In most cases, 89% of the total of lexicalised at-
tachments, LAP basically confirms dependency 
relations already assigned at the previous stage. 
Newly discovered attachments, which are de-
tected thanks to lexicon look-up and re-ranking,  
amount to only 11% of all lexicalised attach-
ments, less than 3% of all PP attachments 
yielded by LAP.  
4.3 Discussion 
4.3.1 Recall and precision on noun and verb 
heads 
Let us consider the output of BP first. The strik-
ing difference in the recall of noun-headed vs 
verb-headed prepositional attachments (on com-
parable levels of precision, rows 2 and 3 of Ta-
ble 1) prompts the suggestion that the typical 
context of use of a noun is more easily described 
in terms of local, order-contingent criteria (e.g. 
rightmost attachment) than a verb context is. We 
can give at least three reasons for that. First, 
frame bearing nouns tend to select fewer argu-
 Lexicalised atts Confirmed atts Restructured atts 
total OK prec Total OK prec total OK prec 
PP_VNA 919 819 89,12 816 771 94,49 103 65 63,11
PP_V 289 244 84,43 201 194 96,52 88 61 69,32
PP_N 629 575 91,41 614 577 93,97 15 4 26,67
ments than verbs do. In our lexicon, 1693 verb-
headed frames out of 6924 have more than one 
non subject argument (24.4%), while there being 
only 1950 noun-headed frames out of 15399 
with more than one argument (12.6%). In TC, of 
2300 head verb tokens, 328 exhibit more than 
one non subject argument (14%). Rightmost at-
tachment trivially penalises such argument 
chains, where some arguments happen to be 
overtly realised in context one or more steps re-
moved from their heads. The second reason is 
sensitive to language variation: verb arguments 
tend to be dislocated more easily than noun ar-
guments, as dislocation heavily depends on sen-
tence-level (hence main verb-level) phenomena 
such as shift of topic or emphasis. In Italian, 
topic-driven argument dislocation in preverbal 
position is comparatively frequent and repre-
sents a problem for the baseline parser, which 
works on a head-first assumption. Thirdly, verbs 
are typically modified by a wider set of syntactic 
satellites than nouns are, such as temporal and 
circumstantial modifiers (Dik 1989). For exam-
ple, deverbal nouns do not inherit the possible 
temporal modifiers of their verb base (I run the 
marathon in three hours, but *the run of the 
marathon in three hours). Modifiers of this sort 
tend to be distributed in the sentence much more 
freely than ordinary arguments.  
4.3.2 Impact of the lexicon on recall 
Of the three above mentioned factors, only the 
first one has an obvious lexical character. We 
can provide a rough estimate of the impact of 
lexical information on the performance of LAP. 
The lexicon filter contributes a 9% increase of 
recall on verb complements (4% over 45.5%), 
by correctly reattaching to the verbal head those 
arguments (61) that were wrongly attached to 
their immediately preceding constituent by BP. 
This leads to an overall 49.5% recall. All re-
maining false negatives (about 48%) are i) either 
verb modifiers or ii) proper verb arguments ly-
ing out of the reach of structure-based criteria, 
due to syntactic phenomena such as complement 
dislocation, complex coordination, parenthetic 
constructions and ellipsis. We shall return to a 
more detailed analysis of false negatives in sec-
tion 4.3.4. In the case of noun complements, use 
of lexical information produces a negligible in-
crease of recall: 0.6% ( 0.5% over 80.8%). This 
is not surprising, as our test corpus contains very 
few cases of noun-headed argument chains, 
fewer than we could expect if the probability of 
their occurrence reflected the (uniform) type dis-
tribution of noun frames in the lexicon. The vast 
majority of noun-headed false negatives, as we 
shall see in more detail in a moment, is repre-
sented by modifiers. 
4.3.3 Impact of the lexicon on precision 
Reattachment is enforced by LAP when the 
preposition introducing a candidate complement 
in context is found in the lexical frame of its 
head. Table 2 shows that ~37% of the 103 re-
structured attachments proposed by the lexicon 
are wrong. Even more interestingly, there is a 
strong asymmetry between nouns and verbs. 
With verb heads, precision of lexically-driven 
reattachments is fairly high (~70%), nonetheless 
lower than precision of rightmost attachment 
(~75%). In the case of noun heads, the number 
of lexically reattached dependencies is instead 
extremely low. The percentage of mistakes is  
high, with precision dropping to 26.6%. 
The difference in the total number of restruc-
tured attachment may be again due to the richer 
complementation patterns exhibited by verbs in 
the lexicon. However, while in the case of verbs 
lexical information produces a significant im-
provement on restructured attachment precision, 
this contribution drops considerably for nouns. 
The main reason for this situation is that nouns 
tend to select semantically vacuous prepositions 
such as of much more often than verbs do. In our 
lexicon, out of 4157 frames headed by a noun, 
4015 contain the preposition di as an argument 
introducer (96.6%). Di is in fact an extremely 
polysemous preposition, heading, among others, 
also possessive phrases and other kinds of modi-
fiers. This trivially increases the number of cases 
of attachment ambiguity and eventually the pos-
sibility of getting false positives. Conversely, as 
shown by the number of confirmed attachments 
in Table 2, the role of lexical information in fur-
ther specifying an attachment with no restructur-
ing is almost uniform across nouns and verbs. 
4.3.4 False negatives  
The vast majority of undetected verb comple-
ments (80.6%) are modifiers of various kind. 
The remaining set of false negatives consists of 
48 complements (7.7%), 30 indirect objects 
(4.8%) and 43 oblique arguments (6.9%). Most 
such complements are by-phrases in passive 
constructions which are not as such very diffi-
cult to detect but just happen to fall out of the 
current coverage of LAP. More interestingly, 2/3 
of the remaining false negatives elude LAP be-
cause they are overtly realised far away from 
their verb head, often to its left. Most of these 
constructions involve argument dislocation and 
ellipsis. We can thus preliminarily conclude that 
argument dislocation and ellipsis accounts for 
about 14% of false negatives (7% over 50%). 
Finally, the number of false negatives due to at-
tachment ambiguity is almost negligible in the 
case of verbal heads. 
On the other hand, the impact of undetected 
modifiers of a verbal head on attachment recall 
is considerable. The most striking feature of this 
large subset is the comparative sparseness of 
modifiers introduced by di (of): 31 out of 504 
(6.2%). At a closer scrutiny, the majority of 
these di-phrases are either phraseological adver-
bial modifiers (di recente ?of late?, del resto ?be-
sides? etc.) or quasi-arguments headed by 
participle forms. Notably, 227 undetected modi-
fiers (45% of the total) are selected by semanti-
cally heavy and complex (possibly 
discontinuous) prepositions (davanti a ?in front 
of?, in mezzo a ?amid?, verso ?towards?, intorno 
a ?around?, contro ?against?, da ... a ?from ... to? 
etc.). As to the remaining 241 undetected modi-
fiers (48%), they are introduced by ?light? 
prepositions such as a ?to?, in ?in? and da ?from?. 
Although this 48% contains a number of diffi-
cult attachments, one can identify subsets of 
fairly reliable modifiers by focusing on the noun 
head introduced by the preposition, which usu-
ally gives a strong indication of the nature of the 
modifier, especially in the case of measure, tem-
poral and locative expressions.  
4.3.5 False positives 
Table 2 shows a prominent asymmetry in the 
precision of confirmed and restructured attach-
ments. Wrong restructured attachments are 
mainly due to a misleading match between the 
preposition introducing a PC and that introduc-
ing a slot in the lexical frame of its candidate 
head (~85%). This typically occurs with ?light? 
prepositions (e.g. di, a, etc.). Most notably, in a 
relevant subset of these mistakes, the verb or 
noun head belongs to an idiomatic multi-word 
expression. In the case of confirmed attach-
ments, about one third of false positives (~5%) 
involve multi-word expressions, in particular 
compound terms such as presidente del consig-
lio ?prime minister?, where the rightmost ele-
ment of the compound is wrongly selected as the 
head of the immediately following PP. In both 
restructured and confirmed attachments, the re-
maining cases (on average ~4%) are due to 
complex syntactic structures (e.g. appositive 
constructions, complex coordination, ellipsis 
etc.) which are outside the coverage of the cur-
rent grammar.  
Conclusion 
Larger lexicons are not necessarily better for 
parsing. The issue of the interplay of lexicon and 
grammar, although fairly well understood at the 
level of linguistic theory, still remains to be fully 
investigated at the level of parsing. In this paper, 
we tried to scratch the surface of the problem 
through a careful analysis of the performance of 
an incremental dependency analyser of Italian, 
which can run in both a non-lexicalised and a 
lexicalised mode.  
The contribution of lexical information to 
parse success is unevenly distributed over both 
part of speech categories and frame types. For 
reasons abundantly illustrated in section 4, the 
frames of noun heads are not quite as useful as 
those of verb heads, especially when available 
information is only syntactic. Moreover, while 
information on verb transitivity or clause em-
bedding is crucial to filter out noisy attachments, 
information on the preposition introducing the 
oblique complement or the indirect object of a 
verb can be misleading, and should thus be used 
for parsing with greater care. The main reason is 
that failure to register in the lexicon all possible 
prepositions actually found in real texts may 
cause undesired over-filtering of genuine 
arguments (false negatives). In many cases, 
argument prepositions are actually selected by 
the lexical head of the subcategorised argument, 
rather than by its subcategorising verb. Simi-
larly, while information about argument option-
ality vs obligatoriness is seldom confirmed in 
real language use, statistical preferences on the 
order of argument realisation can be very useful. 
Most current lexicons say very little about 
temporal and circumstantial modifiers, but much 
more can be said about them that is useful to 
parsing. First, some prepositions only occur to 
introduce verb modifiers. These semantically 
heavy prepositions, often consisting of more 
than one lexical item, play a fundamental role in 
the organization of written texts, and certainly 
deserve a special place in a parsing-oriented 
lexicon. Availability of this type of lexical in-
formation could pave the way to the develop-
ment of specialised ?mini-parsers? of those 
satellite modifiers whose structural position in 
the sentence is subject to considerable variation. 
These mini-parsers could benefit from informa-
tion about semantically-based classes of nouns, 
such as locations, measure terms, or temporal 
expressions, which should also contain indica-
tion of the preposition they are typically intro-
duced by. Clearly, this move requires 
abandoning the prejudice that lexical informa-
tion should only flow from the head to its 
dependents. Finally, availability of large 
repertoires of multi word units (both complex 
prepositions and compound terms) appears to 
have a large impact on improving parse preci-
sion.  
There is no doubt that harvesting such a wide 
range of lexical information in the quantity 
needed for accurate parsing will require exten-
sive recourse to bootstrapping methods of lexi-
cal knowledge acquisition from real texts.     
References  
Bartolini R., Lenci A., Montemagni S, Pirrelli V. 
(2002) The Lexicon-Grammar Balance in Robust 
Parsing of Italian, in Proceedings of the 3rd Inter-
national Conference on Language Resources and 
Evaluation, Las Palmas, Gran Canaria. 
Briscoe, E.J. (2001) From dictionary to corpus to 
self-organizing dictionary: learning valency asso-
ciations in the face of variation and change, in 
Proceedings of Corpus Linguistics 2001, Lancaster 
University, pp. 79-89. 
Briscoe T., Carroll J., (2002) Robust Accurate Statis-
tical Annotation of General Text, in Proceedings of 
the 3rd International Conference on Language Re-
sources and Evaluation, Las Palmas, Gran Canaria. 
Carroll, J., Minnen G., Briscoe E.J. (1998) Can sub-
categorisation probabilities help a statistical 
parser?, in Proceedings of the 6th ACL/SIGDAT 
Workshop on Very Large Corpora, Montreal, Can-
ada. 118-126. 
Chanod J.P. (2001) Robust Parsing and Beyond, in 
J.C. Junqua and G. van Noord (eds.) Robustness in 
Language and Speech Technology, Dordrecht, 
Kluwer, pp. 187-204. 
Federici, S., Montemagni, S., Pirrelli, V. (1998a) 
Chunking Italian: Linguistic and Task-oriented 
Evaluation, in Proceedings of the LREC Workshop 
on ?Evaluation of Parsing Systems?, Granada, 
Spain. 
Federici, S., Montemagni, S., Pirrelli, V., Calzolari, 
N. (1998b) Analogy-based Extraction of Lexical 
Knowledge from Corpora: the SPARKLE Experi-
ence, in Proceedings of the 1st International Con-
ference on Language resources and Evaluation, 
Granada, Spain. 
Grishman, R., Macleod C., Meyers A. (1994) 
COMLEX Syntax: Building a Computational Lexi-
con, in Proceedings of Coling 1994, Kyoto. 
Jensen K. (1988a) Issues in Parsing, in A. Blaser 
(ed.), Natural Language at the Computer, Springer 
Verlag, Berlin, pp. 65-83. 
Jensen K. (1988b) Why computational grammarians 
can be skeptical about existing linguistic theories,
in Proceedings of COLING-88, pp. 448-449. 
Lenci, A., Bartolini, R., Calzolari, N., Cartier, E. 
(2001) Document Analysis, MLIS-5015 MUSI, De-
liverable D3.1,. 
Lenci, A., Montemagni, S., Pirrelli, V., Soria, C. 
(2000) Where opposites meet. A Syntactic Meta-
scheme for Corpus Annotation and Parsing 
Evaluation, in Proceedings of the 2nd International 
Conference on Language Resources and Evalua-
tion, Athens, Greece. 
Montemagni S., Barsotti F., Battista M., Calzolari N., 
Corazzari O., Zampolli A., Fanciulli F., Massetani 
M., Raffaelli R., Basili R., Pazienza M.T., Saracino 
D., Zanzotto F., Mana N., Pianesi F., Delmonte R. 
(2000) The Italian Syntactic-Semantic Treebank: 
Architecture, Annotation, Tools and Evaluation, in 
Proceedings of the COLING Workshop on ?Lin-
guistically Interpreted Corpora (LINC-2000)?, 
Luxembourg, 6 August 2000, pp. 18-27. 
Procter, P. (1987) Longman Dictionary of Contempo-
rary English, Longman, London. 
Ruimy, N., Corazzari, O., Gola, E., Spanu, A., Cal-
zolari, N., Zampolli, A. (1998) The European LE-
PAROLE Project: The Italian Syntactic Lexicon, in 
Proceedings of the 1st International Conference on 
Language resources and Evaluation, Granada, 
Spain, 1998. 
 
Non-locality all the way through:  
Emergent Global Constraints in the Italian Morphological Lexicon 
Vito Pirrelli 
Istituto di Linguistica Computazionale 
CNR, Pisa, Italy 
vito.pirrelli@ilc.cnr.it 
Basilio Calderone 
Laboratorio di Linguistica  
Scuola Normale Superiore, Pisa, Italy 
b.calderone@sns.it 
Ivan Herreros 
Istituto di Linguistica Computazionale 
CNR, Pisa, Italy 
ivan.herreros@ilc.cnr.it 
Michele Virgilio 
Dipartimento di Fisica 
Universit? degli Studi di Pisa, Italy 
virgilio@df.unipi.it 
 
Abstract 
The paper reports on the behaviour of a Koho-
nen map of the mental lexicon, monitored 
through different phases of acquisition of the 
Italian verb system. Reported experiments ap-
pear to consistently reproduce emergent global 
ordering constraints on memory traces of in-
flected verb forms, developed through princi-
ples of local interactions between parallel 
processing neurons. 
1 Introduction 
Over the last 15 years, considerable evidence has 
accrued on the critical role of paradigm-based rela-
tions as an order-principle imposing a non-local 
organising structure on word forms memorised in 
the speaker?s mental lexicon, facilitating their re-
tention, accessibility and use, while permitting the 
spontaneous production and analysis of novel 
words. A number of theoretical models of the men-
tal lexicon have been put forward to deal with the 
role of these global constraints in i) setting an up-
per bound on the number of possible forms a 
speaker is ready to produce (Stemberger and Car-
stairs, 1988), ii) accounting for reaction times in 
lexical decision and related tasks (Baayen et al 
1997; Orsolini and Marslen-Wilson, 1997 and oth-
ers), iii) explaining production errors by both 
adults and children (Bybee and Slobin, 1982; By-
bee and Moder; 1983; Orsolini et al, 1998) and iv) 
accounting for human acceptability judgements 
and generalisations over nonce verb stems (Say 
and Clahsen, 2001). While most of these models 
share some core assumptions, they appear to 
largely differ on the role played by lexical relations 
in word storage, access and processing. According 
to the classical view (e.g. Taft, 1988) the relation-
ship between regularly inflected forms is directly 
encoded as lexical procedures linking inflectional 
affixation to separately encoded lexical roots. Ir-
regular word forms, on the other hand, are stored 
in full (Prasada and Pinker, 1993). In contrast to 
this view, associative models of morphological 
processing claim that words in the mental lexicon 
are always listed as full forms, establishing an in-
terconnected network of largely redundant linguis-
tic data reflecting similarities in meaning and form 
(Bybeee, 1995). 
Despite the great deal of experimental evidence 
now available, however, we still seem to know too 
little of the dynamic interplay between morpho-
logical learning and the actual working of the 
speaker?s lexicon to draw conclusive inferences 
from experimental findings. Associative models, 
for example, are generally purported to be unable 
to capture morpheme-based effects of morphologi-
cal storage and access. Thus, if humans are shown 
to access the mental lexicon through morphemes, 
so the argument goes, then associative models of 
the mental lexicon cannot be true. In fact, if asso-
ciative models can simulate emergent morpheme-
based effects of lexical organisation through stor-
age of full forms, then this conclusion is simply 
unwarranted.  
We believe that computer simulations of mor-
phology learning can play a role in this dispute. 
However, there have been comparatively few at-
tempts to model the way global ordering principles 
of lexical organisation interact with (local) proc-
essing strategies in morphology learning. In the 
present paper, we intend to simulate a biologically-
inspired process of paradigm-based self-
organisation of inflected verb forms in a Kohonen 
map of the Italian mental lexicon, built on the basis 
of local processes of memory access and updating. 
Before we go into that, we briefly overview rele-
vant machine learning work from this perspective. 
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
2 Background 
Lazy learning methods such as the nearest 
neighbour algorithm (van den Bosch et al, 1996) 
or the analogy-based approach (Pirrelli and 
Federici, 1994; Pirrelli and Yvon, 1999) require 
full storage of supervised data, and make on-line 
use of them with no prior or posterior lexical struc-
turing. This makes this class of algorithms flexible 
and efficient, but comparatively noise-sensitive 
and rather poor in simulating emergent learning 
phenomena. There is no explicit sense in which the 
system learns how to map new exemplars to al-
ready memorised ones, since the mapping function 
does not change through time and the only incre-
mental pay-off lies in the growing quantity of in-
formation stored in the exemplar data-base.   
Decision tree algorithms (Quinlan, 1986), on the 
other hand, try to build the shortest hierarchical 
structure that best classifies the training data, using 
a greedy heuristics to select the most discrimina-
tive attributes near the root of the hierarchy. As 
heuristics are based on a locally optimal splitting 
of all training data, adding new training data may 
lead to a dramatic reorganisation of the hierarchy, 
and nothing is explicitly learned from having built 
a decision tree at a previous learning stage (Ling 
and Marinov, 1993). 
To tackle the issue of word structure more 
squarely, there has been a recent upsurge of inter-
est in global paradigm-based constraints on mor-
phology learning, as a way to minimise the range 
of inflectional or derivational endings heuristically 
inferred from raw training data (Goldsmith, 2001; 
Gaussier, 1999; Baroni, 2000). It should be noted, 
however, that global, linguistically-inspired con-
straints of this sort do not interact with morphology 
learning in any direct way. Rather, they are typi-
cally used as global criteria for optimal conver-
gence on an existing repertoire of minimally re-
dundant sets of paradigmatically related mor-
phemes. Candidate morpheme-like units are ac-
quired independently of paradigm-based con-
straints, solely on the basis of local heuristics. 
Once more, there is no clear sense in which global 
constraints form integral part of learning. 
Of late, considerable attention has been paid to 
aspects of emergent morphological structure and 
continuous compositionality in multi-layered per-
ceptrons. Plaut et al (1996) show how a neural 
network comes to be sensitive to degrees of com-
positionality on the basis of exposure to examples 
of inputs and outputs from a word-reading task. 
Systematic input-output pairs tend to establish a 
clear one-to-one correlation between parts of input 
and parts of output representations, thus develop-
ing strongly compositional analyses. By the same 
token, a network trained on inputs with graded 
morphological structure develops representations 
with corresponding degrees of compositionality 
(Rueckl and Raveh, 1999). It must be appreciated 
that most such approaches to incremental com-
postionality are task-oriented and highly super-
vised. Arguably, a better-motivated and more ex-
planatory approach should be based on self-
organisation of input tokens into morphologically 
natural classes and their time-bound specialisation 
as members of one such class, with no external su-
pervision. Kohonen?s Self-Organising Maps 
(SOMs) (Kohonen, 1995) simulate self-
organisation by structuring input knowledge on a 
(generally) two-dimensional grid of neurons, 
whose activation values can be inspected by the 
researcher both instantaneously and through time. 
In the remainder of this paper we show that we can 
use SOMs to highlight interesting aspects of global 
morphological organisation in the learning of Ital-
ian conjugation, incrementally developed through 
local interactions between parallel processing neu-
rons.   
3 SOMs 
SOMs can project input tokens, represented as 
data points of an n-dimensional input space, onto a 
generally two-dimensional output space (the map 
grid) where similar input tokens are mapped onto 
nearby output units. Each output unit in the map is 
associated with a distinct prototype vector, whose 
dimensionality is equal to the dimensionality of in-
put vectors. As we shall see, a prototype vector is 
an approximate memory trace of recurring inputs, 
and plays the role of linking its corresponding out-
put unit to a position in the input space. Accord-
ingly, each output unit takes two positions: one in 
the input space (through its prototype vector) and 
one in the output space (its co-ordinates on the 
map grid).  
SOMs were originally conceived of as computer 
models of somatotopic brain maps. This explains 
why output units are also traditionally referred to 
as neurons. Intuitively, a prototype vector repre-
sents the memorised input pattern to which its as-
sociated neuron is most sensitive. Through learn-
ing, neurons gradually specialise in selectively be-
ing associated with specific input patterns. More-
over, memorised input patterns tend to cluster on 
the map grid so as to reflect natural classes in the 
input space.  
These interesting results are obtained through it-
erative unsupervised exposure to input tokens. At 
each learning step, a SOM is exposed to a single 
input token and goes through the following two 
stages: a) competitive neuron selection, and b) 
adaptive adjustment of prototype vectors. As we 
shall see in more detail in the remainder of this 
section, both stages are local and incremental in 
some crucial respects.1  
3.1 Stage 1: competitive selection 
Let vx be the n-dimension vector representation 
of the current input. At this stage, the distance be-
tween each prototype vector and vx is computed. 
The output unit b that happens to be associated 
with the prototype vector vb closest to vx is selected 
as the best matching unit. More formally: 
 { }ixbx vvvv ??? min ,  
 
where   is also known as the quantization error 
scored by vb relative to vx. Intuitively, this is to say 
that, although b is the map neuron reacting most 
sensitively to the current stimulus, b is not (yet) 
perfectly attuned to vx.  
Notably, the quantization error is a local distance 
function, as it involves two vector representations 
at a time. Hence, competitive selection is blind to 
general structural properties of the input space, 
such as the comparative role of each dimension in 
discriminating input tokens. This makes competi-
tive selection prone to errors due to accidental or 
spurious similarity between the input vector and 
SOM prototype vectors.     
3.2 Stage 2: adaptive adjustment 
After the winner unit b is selected at time t, the 
SOM locally adapts prototype vectors to the cur-
rent stimulus. Vector adaptation applies locally, 
within a kernel area of radius r, centred on the po-
sition of b on the map grid. Both vb(t) (vb at time t) 
and the prototype vectors associated with b?s ker-
nel units are adjusted to make them more similar to 
vx(t) (vx at time t). In particular, for each prototype 
vector vi in b?s kernel and the input vector vx, the 
following adaptive function is used 
 [ ])()()()1( tvtvhtvtv ixbiii ?+=+  , 
 
where hbi is the neighbourhood kernel centred 
around the winner unit b at time t, a non-increasing 
function of both time and the distance between the 
input vi and the winner vector vb. As learning time 
progresses, however, hbi decreases, and prototype 
vector updates become less sensitive to input con-
ditions, according to the following: 
 
                                                     
1 This marks a notable difference between SOMs and 
other classical projection techniques such as Vector 
Analysis or Multi-dimensional Scaling, which typically 
work on the basis of global constraints on the overall 
distribution of input data (e.g. by finding the space pro-
jection that maximizes data variance/co-variance). 
 )(),()( ttllhth ibbi ???= , 
 
where lb and li are, respectively, the position of b 
and its kernel neurons on the map grid, and ?(t) is 
the learning rate at time t, a monotonically decreas-
ing function of t. Interaction of these functions 
simulates effects of memory entrenchment and 
proto-typicality of early input data. 
3.3 Summary  
The dynamic interplay between locality and in-
crementality makes SOMs plausible models of 
neural computation and data compression. Their 
sensitivity to frequency effects in the distribution 
of input data allows the researcher to carefully test 
their learning behaviour in different time-bound 
conditions. Learning makes output units increas-
ingly more reactive to already experienced stimuli 
and thus gradually more competitive for selection. 
If an output unit is repeatedly selected by system-
atically occurring input tokens, it becomes associ-
ated with a more and more faithful vector represen-
tation of a stimulus or class of stimuli, to become 
an attractor for its neighbouring area on the map. 
As a result, the most parsimonious global organisa-
tion of input data emerges that is compatible with 
a) the size of the map grid, b) the dimensionality of 
output units and c) the distribution of input data.  
This intriguing dynamics persuaded us to use 
SOMs to simulate the emergence of non-local lexi-
cal constraints from local patterns of interconnec-
tivity between vector representations of full word 
forms. The Italian verb system offers a particularly 
rich material to put this hypothesis to the challeng-
ing test of a computer simulation. 
4 The Italian Verb System  
The Italian conjugation is a complex inflectional 
system, with a considerable number of classes of 
regular, subregular and irregular verbs exhibiting 
different probability densities (Pirrelli, 2000; Pir-
relli and Battista, 2000). Traditional descriptive 
grammars (e.g. Serianni, 1988) identify three main 
conjugation classes (or more simply conjugations), 
characterised by a distinct thematic vowel (TV), 
which appears between the verb root and the in-
flectional endings. First conjugation verbs have the 
TV -a- (parl-a-re 'speak'), second conjugation 
verbs have the TV -e- (tem-e-re 'fear'), and third 
conjugation verbs -i- (dorm-i-re 'sleep'). The first 
conjugation is by far the largest class of verbs 
TYPE EXAMPLE ENGLISH GLOSS 
[isk]-insertion + palatalization fi"nisko/fi"niSSi/fi"njamo (I)/(you)/(we) end 
[g]-insertion + diphthongization "vEngo/"vjEni/ve"njamo (I)/(you)/(we) come 
ablauting + velar palatalization "Esko/"ESSi/uS"Samo (I)/(you)/(we) go out 
[r]-drop + diphthongization "mwojo/"mwori/mo"rjamo (I)/(you)/(we) die 
Table 1. Variable stem alternations in the Italian present indicative. 
(73% of all verbs listed in De Mauro et al, 1993), 
almost all of which are regular. Only very few 1st 
conjugation verbs have irregularly inflected verb 
forms: andare 'go', dare 'give', stare 'stay' and fare 
?do, make?. It is also the only truly productive 
class. Neologisms and foreign loan words all fall 
into it. The second conjugation has far fewer 
members (17%), which are for the most part ir-
regular (around 95%). The third conjugation is the 
smallest class (10%). It is mostly regular (around 
10% of its verbs are irregular) and only partially 
productive. 
Besides this macro-level of paradigmatic or-
ganisation, Italian subregular verbs also exhibit 
ubiquitous patterns of stem alternations, whereby 
a change in paradigm slot triggers a simultaneous 
change of verb stem and inflectional ending, as 
illustrated in Table 1 for the present indicative ac-
tive. Pirrelli and Battista (2000) show that phe-
nomena of Italian stem alternation, far from being 
accidental inconsistencies of the Italian morpho-
phonology, define stable and strikingly conver-
gent patterns of variable stem formation (Aronoff, 
1994) throughout the entire verb system. The pat-
terns partition subregular Italian verbs into 
equivalence micro-classes. In turn, this can be in-
terpreted as suggesting that inter-class consistency 
plays a role in learning and may have exerted a 
convergent pressure in the history of the Italian 
verb system. If a speaker has heard a verb only in 
ambiguous inflections (i.e. inflections that are in-
dicators of more than one verb micro-class), (s)he 
will need to guess, in order to produce unambigu-
ous forms. Guesses are made on the basis of fre-
quently attested verb micro-classes (Albright, 
2002). 
5 Computer simulations 
The present experiments were carried out using 
the SOM toolbox (Vesanto et al, 2000), devel-
oped at the Neural Networks Research Centre of 
Helsinki University of Technology. The toolbox 
partly forced some standard choices in the training 
protocol, as discussed in more detail in the follow-
ing sections. In particular, we complied with Ko-
honen?s view of SOM training as consisting of 
two successive phases: a) rough training and b) 
fine-tuning. The implications of this view will be 
discussed in more detail later in the paper.  
5.1 Input data 
Our input data are inflected verb forms written 
in standard Italian orthography. Since Italian or-
thography is, with a handful of exceptions, consis-
tently phonological, we expect to replicate the 
same results with phonologically transcribed verb 
forms.  
Forms are incrementally sampled from a train-
ing data set, according to their probability densi-
ties in a free text corpus of about 3 million words. 
Input data cover a fragment of Italian verb inflec-
tion, including, among others, present indicative 
active, future indicative active, infinitive and past 
participle forms, for a total of 10 different inflec-
tions. The average length of training forms is 8.5, 
with a max value of 18.  
Following Plunkett and Marchman (1993), we 
assume than the map is exposed to a gradually 
growing lexicon. At epoch 1, the map learns in-
flected forms of the 5 most frequent verb types. At 
each ensuing epoch, five more verb types are 
added to the training data, according to their rank 
in a list of decreasingly frequent verb types. As an 
overall learning session consists of 100 epochs, 
the map is eventually exposed to a lexicon of 500 
verb types, each seen in ten different inflections. 
Although forms are sampled according to their 
corpus distributions, we hypothesise that the range 
of inflections in which verb tokens are seen by the 
map remains identical across verb types. This is 
done to throw paradigmatic effects in sharper re-
lief and responds to the (admittedly simplistic) as-
sumption that the syntactic patterns forming the 
linguistic input to the child do not vary across 
verb types. 
Each input token is localistically encoded as an 
8*16 matrix of values drawn from the set {1, -1}. 
Column vectors represent characters, and rows 
give the random encoding of each character, en-
suring maximum independence of character vec-
tor representations. The first eight columns in the 
matrix represent the first left-aligned characters of 
the form in question. The remaining eight col-
umns stand for the eight (right-aligned) final char-
acters of the input form.   
 a) b) 
Figure 1. Early self-organisation of a SOM for roots (a) and endings (b) of Italian verbs (epoch 10). 
 
 
a) b) 
Figure 2. Late self-organization of a SOM for roots (a) and endings (b) of Italian verbs (epoch 100).  
 
5.2 Training protocol 
At each training epoch, the map is exposed to a 
total of 3000 input tokens. As the range of 
different inflected forms from which input tokens 
are sampled is fairly limited (especially at early 
epochs), forms are repeatedly shown to the map. 
Following Kohonen (1995), a learning epoch 
consists of two phases. In the first rough training 
phase, the SOM is exposed to the first 1500 
tokens. In this phase, values of ? (the learning 
rate) and neighbourhood kernel radius r are made 
vary as a linear decreasing function of the time 
epoch, from max ? = 0.1 and r = 20 (epoch 1), to 
? = 0.02 and r = 10 (epoch 100). In the second 
fine-tuning phase of each epoch, on the other 
hand, ? is kept to 0.02 and r = 3. 
5.3 Simulation 1: Critical transitions in lexi-
cal organisation 
Figures 1 and 2 contain snapshots of the Italian 
verb map taken at the beginning and the end of 
training (epochs 1 and 100). The snapshots are 
Unified distance matrix (U-matrix, Ultsch and 
Siemon, 1990) representations of the Italian SOM. 
They are used to visualise distances between neu-
rons. In a U-matrix representation, the distance 
between adjacent neurons is calculated and pre-
sented with different colourings between adjacent 
positions on the map. A dark colouring between 
neurons signifies that their corresponding proto-
type vectors are close to each other in the input 
space. Dark colourings thus highlight areas of the 
map whose units react consistently to the same 
stimuli. A light colouring between output units, on 
the other hand, corresponds to a large distance (a 
gap) between their corresponding prototype vec-
tors. In short, dark areas can be viewed as clusters, 
and light areas as chaotically reacting cluster 
separators. This type of pictorial presentation is 
useful when one wants to inspect the state of 
knowledge developed by the map through learn-
ing.  
For each epoch, we took two such snapshots: i) 
one of prototype vector dimensions representing 
the initial part of a verb form (approximately its 
verb root, Figures 1.a and 2.a), and ii) one of pro-
totype vector dimensions representing the verb fi-
nal part (approximately, its inflectional endings, 
Figure 1.b and 2.b). 
5.3.1 Discussion 
Data storage on a Kohonen map is a dynamic 
process whereby i) output units tend to consis-
tently become more reactive to classes of input 
data, and ii) vector prototypes which are adjacent 
in the input space tend to cluster in topologically 
connected subareas of the map. 
Self-organisation is thus an emergent property, 
based on local (both in time and space) principles 
of prototype vector adaptation. At the outset, the 
map is a tabula rasa, i.e. it has no notion whatso-
ever of Italian inflectional morphology. This has 
two implications. First, before training sets in, 
output units are associated with randomly initial-
ised sequences of characters. Secondly, prototype 
vectors are randomly associated with map neu-
rons, so that two contiguous neurons on the map 
may be sensitive to very different stimulus pat-
terns.  
Figure 1 shows that, after the first training ep-
och, the map started by organising memorised in-
put patterns lexically, grouping them around their 
(5) roots. Each root is an attractor of lexically re-
lated stimuli, that nonetheless exhibit fairly het-
erogeneous endings (see Figure 1.b). 
At learning epoch 100, on the other hand, the 
topological organisation of the verb map is the 
mirror image of that at epoch 10 (Figures 2.a and 
2.b). In the course of learning, root attractors are 
gradually replaced by ending attractors. Accord-
ingly, vector prototypes that used to cluster 
around their lexical root appear now to stick to-
gether by morpho-syntactic categories such as 
tense, person and number. One can conceive of 
each connected dark area of map 2.b as a slot in 
an abstract inflectional paradigm, potentially as-
sociated with many forms that share an inflec-
tional ending but differ in their roots.  
 
 
root 
ending 
Figure 3. Average quantization error for an increasing number of input verbs  
 
The main reason for this morphological organisa-
tion to emerge at a late learning stage rests in the 
distribution of training data. At the beginning, the 
map is exposed to a small set of verbs, each of 
which is inflected in 10 different forms. Forms 
with the same ending tend to be fewer than forms 
with the same root. As the verb vocabulary grows 
(say of the order of about 50 different verbs), 
however, the principles of morphological (as op-
posed to lexical) organisation allow for more 
compact and faithful data storage, as reflected by 
a significant reduction in the map average quanti-
zation error (Figure 3). Many different forms can 
be clustered around comparatively few endings, 
and the latter eventually win out as local paradig-
matic attractors.  
Figure 4 (overleaf) is a blow-up of the map area 
associated with infinitive and past participle end-
ings. The map shows the content of the last three 
characters of each prototype vector. Since past 
participle forms occur in free texts more often 
than infinitives, they have a tendency to take a 
proportionally larger area of the map (due to the 
so-called magnification factor). Interestingly 
enough, past participles ending in -ato occupy one 
third of the whole picture, witnessing the promi-
nent role played by regular first conjugation verbs 
in the past participle inflection. 
Another intriguing feature of the map is the way 
the comparatively connected area of the past par-
ticiple is carved out into tightly interconnected 
micro-areas, corresponding to subregular verb 
forms (e.g. corso ?run?, scosso ?shaken? and chie-
sto ?answered?). Rather than lying outside of the 
morpho-phonological realm (as exceptions to the 
?TV + to? default rule), subregular forms of this 
kind seem here to draw the topological borders of 
the past participle domain, thus defining a con-
tinuous chain of morphological family resem-
blances. Finally, by analogy-based continuity, the 
map comes to develop a prototype vector for the 
non existing (but paradigmatically consistent) past 
participle ending -eto.2  This ?spontaneous? over-
generalization is the by-product of graded, over-
lapping morpheme-based memory traces. 
In general, stem frequency may have had a re-
tardatory effect on the critical transition from a 
lexical to a paradigm-based organisation. For the 
same reason, high-frequency forms are eventually 
memorised as whole words, as they can success-
fully counteract the root blurring effect produced 
by the chaotic overlay of past participle forms of 
different verbs, which are eventually attracted to 
the same map area. This turns out to be the case 
for very frequent past participles such as stato 
?been? and fatto ?done?. As a final point, a more 
detailed analysis of memory traces in the past par-
ticiple area of the map is likely to highlight sig-
nificant stem patterns in the subregular micro-
classes. If confirmed, this should provide fresh 
evidence supporting the existence of prototypical 
morphonological stem patterns consistently select-
ing specific subregular endings (Albright, 2002). 
5.4 Simulation 2: Second level map 
A SOM projects n-dimensional data points onto 
grid units of reduced dimensionality (usually 2). 
We can take advantage of this data compression to 
train a new SOM with complex representations 
consisting of the output units of a previously 
trained SOM. The newly trained SOM is a second 
level projection of the original data points.  
To test the consistency of the paradigm-based 
organisation of the map in Figure 2, we trained a 
                                                     
2 While Italian regular 1st  and 3rd conjugation verbs 
present a thematic vowel in their past participle end-
ings (-ato and  -ito respectively), regular 2 conjugation 
past participles (TV -e-) end, somewhat unexpectedly, 
in -uto. 
novel SOM with verb type vectors. Each such 
vector contains all 10 inflected forms of the same 
verb type, encoded through the co-ordinates of 
their best-matching units in the map grid of Figure 
2. The result of the newly trained map is given in 
Figure 5. 
 
 
Figure 4. The past participle and infinitive areas 
 
5.4.1 Discussion  
Figure 5 consistently pictures the three-fold 
macrostructure of the Italian verb system (section 
2) as three main horizontal areas going across the 
map top-to-bottom.  
 
 
Figure 5: A second level map 
 
Besides, we can identify other micro-areas, 
somewhat orthogonal to the main ones.The most 
significant such micro-class (circled by a dotted 
line) contains so-called [g]-inserted verbs (Pirrelli, 
2000; Fanciullo, 1998), whose forms exhibit a 
characteristic [g]/0 stem alternation, as in 
vengo/venite ?I come, you come (plur.)? and 
tengo/tenete ?I have/keep, you have/keep (plur.)?. 
The class straddles the 2nd and 3rd conjugation 
areas, thus pointing to a convergent phenomenon 
affecting a portion of the verb system (the present 
indicative and subjunctive) where the distinction 
between 2nd and 3rd conjugation inflections is 
considerably (but not completely) blurred. All in 
all, Italian verbs appear to fall not only into 
equivalence classes based on the selection of 
inflectional endings (traditional conjugations), but 
also into homogeneous micro-classes reflecting 
processes of variable stem formation. 
Identification of the appropriate micro-class is a 
crucial problem in Italian morphology learning. 
Our map appears to be in a position to tackle it 
reliably. 
Note finally the very particular position of the 
verb stare ?stay? on the grid. Although stare is a 
1st conjugation verb, it selects some 2nd conjuga-
tion endings (e.g. stessimo ?that we stayed (subj.)? 
and stette ?(s)he stayed?). This is captured in the 
map, where the verb is located halfway between 
the 1st and 2nd conjugation areas. 
6 Conclusion and future work 
The paper offered a series of snapshots of the 
dynamic behaviour of a Kohonen map of the men-
tal lexicon taken in different phases of acquisition 
of the Italian verb system. The snapshots consis-
tently portray the emergence of global ordering 
constraints on memory traces of inflected verb 
forms, at different levels of linguistic granularity.  
Our simulations highlight not only morphologi-
cally natural classes of input patterns (reminiscent 
of the hierarchical clustering of perceptron input 
units on the basis of their hidden layer activation 
values) and selective specialisation of neurons and 
prototype vector dimensions in the map, but also 
other non-trivial aspects of memory organisation. 
We observe that the number of neighbouring units 
involved in the memorisation of a specific mor-
phological class is proportional to both type fre-
quency of the class and token frequency of its 
members. Token frequency also affects the en-
trenchment of memory areas devoted to storing 
individual forms, so that highly frequent forms are 
memorised in full, rather than forming part of a 
morphological cluster.  
In our view, the solid neuro-physiological basis 
of SOMs? processing strategies and the consider-
able psycho-linguistic and linguistic evidence in 
favour of global constraints in morphology learn-
ing make the suggested approach an interesting 
medium-scale experimental framework, mediating 
between small-scale neurological structures and 
large-scale linguistic evidence. In the end, it 
would not be surprising if more in-depth computa-
tional analyses of this sort will give strong indica-
tions that associative models of the morphological 
lexicon are compatible with a ?realistic? interpre-
tation of morpheme-based decomposition and ac-
cess of inflected forms in the mental lexicon. Ac-
cording to this view, morphemes appear to play a 
truly active role in lexical indexing, as they ac-
quire an increasingly dominant position as local 
attractors through learning. This may sound trivial 
to the psycholinguistic community. Nonetheless, 
only very few computer simulations of morphol-
ogy learning have so far laid emphasis on the im-
portance of incrementally acquiring structure from 
morphological data (as opposed ? say ? to simply 
memorising more and more input examples) and 
on the role of acquired structure in lexical organi-
sation. Most notably for our present concerns, the 
global ordering constraints imposed by morpho-
logical structure in a SOM are the by-product of 
purely local strategies of memory access, process-
ing and updating, which are entirely compatible 
with associative models of morphological learn-
ing. After all, the learning child is not a linguist 
and it has no privileged perspective on all relevant 
data. It would nonetheless be somewhat reassur-
ing to observe that its generalisations and ordering 
constraints come very close to a linguist?s ontol-
ogy. 
The present work also shows some possible 
limitations of classical SOM architectures. The 
propensity of SOMs to fully memorise input data 
only at late learning stages (in the fine-tuning 
phase) is not fully justified in our context. Like-
wise, the hypothesis of a two-staged learning 
process, marked by a sharp discontinuity at the 
level of kernel radius length, has little psycholin-
guistic support. Furthermore, multiple classifica-
tions are only minimally supported by SOMs. As 
we saw, a paradigm-based organisation actually 
replaces the original lexical structure. This is not 
entirely desirable when we deal with complex 
language tasks. In order to tackle these potential 
problems, the following changes are currently be-
ing implemented:  
 
? endogenous modification of radius length as 
a function of the local distance between the 
best matching prototype vector and the cur-
rent stimulus; the smaller the distance the 
smaller the effect of adaptive updating on 
neighbouring vectors  
? adaptive vector-distance function; as a neu-
ron becomes more sensitive to an input pat-
tern, it also develops a sensitivity to specific 
input dimensions; differential sensitivity, 
however, is presently not taken into account 
when measuring the distance between two 
vectors; we suggest weighting vector di-
mensions, so that distances on some dimen-
sions are valued higher than distances on 
other dimensions   
?  ?self-feeding? SOMs for multiple classifi-
cation tasks; when an incoming stimulus 
has been matched by the winner unit only 
partially, the non matching part of the same 
stimulus is fed back to the map; this is in-
tended to allow ?recognition? of more than 
one morpheme within the same input form    
? more natural input representations, address-
ing the issue of time and space-invariant 
features in character sequences. 
References  
Albright, Adam. 2002. Islands of reliability for regular 
morphology: Evidence from Italian. Language, 
78:684-709. 
Aronoff, Mark. 1994. Morphology by Itself. M.I.T. 
Press, Cambridge, USA. 
Baayen, Harald, Ton Dijkstra and Robert Schreuder. 
1997. Singulars and Plurals in Dutch: Evidence for a 
Parallel Dual Route Model. Journal of Memory and 
Language, 36:94-117. 
Baroni, Marco. 2000. Distributional cues in morpheme 
discovery: A computational model and empirical 
evidence. Ph.D. dissertation, UCLA. 
Bosch van den, Antal, Walter Daelemans, Ton Wei-
jters. 1996. Morphological Analysis as Classifica-
tion: an Inductive-learning approach. In Proceedings 
of NEMLAP II , K. Oflazer and H. Somers, eds., 
pages 79-89, Ankara. 
Bybee, Joan. 1995. Regular Morphology and the Lexi-
con. Language and Cognitive Processes, 10 (5): 
425-455. 
Bybee, Joan and Dan I. Slobin. 1982. Rules and Sche-
mas in the Development and Use of the English Past 
Tense. Language, 58:265-289. 
Bybee, Joan and Carol Lynn Moder. 1983. Morpholo-
gical Classes as Natural Categories. Language, 
59:251-270. 
De Mauro, Tullio, Federico Mancini, Massimo Vedo-
velli and Miriam Voghera. 1993. Lessico di frequen-
za dell'italiano parlato. Etas Libri, Milan. 
Fanciullo, Franco. 1998. Per una interpretazione dei 
verbi italiani a ?inserto? velare. Archivio Glottologi-
co Italiano, LXXXIII(II):188-239. 
Gaussier, Eric. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In 
Proceedings of the Workshop on Unsupervised 
Learning in Natural Language Processing, pages 
24-30, University of Maryland.  
Goldsmith, John. 2001. Unsupervised Learning of the 
Morphology of a Natural Language. Computational 
Linguistics, 27(2):153-198. 
Kohonen, Teuvo. 1995. Self-Organizing Maps. 
Springer, Berlin. 
Ling, Charles X. and Marin Marinov. 1993. Answering 
the Connectionist Challenge: a Symbolic Model of 
Learning the Past Tense of English Verbs. Cogni-
tion, 49(3):235-290. 
Orsolini, Margherita and William Marslen-Wilson.  
1997. Universals in Morphological Representations: 
Evidence from Italian. Language and Cognitive 
Processes, 12(1):1-47. 
Orsolini, Margherita, Rachele Fanari and Hugo Bo-
wles. 1998. Acquiring regular and irregular inflec-
tion in a language with verbal classes. Language 
and Cognitive Processes, 13(4):452-464. 
Pirrelli, Vito. 2000. Paradigmi in Morfologia. Istituti 
Editoriali e Poligrafici Internazionali, Pisa. 
Pirrelli, Vito and Federici Stefano. 1994. "Deriva-
tional" Paradigms in Morphonology. In Proceedings 
of Coling 94, pages 234-240,  Kyoto. 
Pirrelli, Vito and Fran?ois Yvon. 1999. The hidden di-
mension: a paradigmatic view of data driven NLP. 
Journal of Experimental and Theoretical Artificial 
Intelligence, 11:391-408. 
Pirrelli, Vito and Marco Battista. 2000. The Paradig-
matic Dimension of Stem Allomorphy in Italian In-
flection. Italian Journal of Linguistics, 12(2):307-
380. 
Plaut, David C., James L. McClelland, Mark S. Sei-
denberg and Karalyn Patterson. 1996. Understand-
ing Normal and Impaired Word Reading: Computa-
tional Principles in Quasi-regular Domains. Psycho-
logical Review , 103:56-115. 
Plunkett, Kim and Virginia Marchman. 1993. From 
rote learning to system building: Acquiring verb 
morphology in children and connectionist nets. 
Cognition, 48:21-69. 
Prasada, Sandeep and Steven Pinker. 1993. Generaliza-
tions of regular and irregular morphology. Language 
and Cognitive Processes, 8:1-56. 
Rueckl, Jay G. and Michal Raveh. 1999. The Influence 
of Morphological Regularities on the Dynamics of 
Connectionist Networks. Brain and Language, 
68:110-117. 
Say, Tessa and Harald Clahsen. 2001. Words, Rules 
and Stems in the Italian Mental Lexicon. In ?Storage 
and computation in the language faculty?, S. Noote-
boom, F. Weerman and  F. Wijnen, eds., pages 75-
108, Kluwer Academic Publishers, Dordrecht. 
Serianni, Luca. 1988. Grammatica italiana: italiano 
comune e lingua letteraria. UTET, Turin. 
Stemberger, Joseph P. and Andrew Carstairs. 1988. A 
Processing Constraint on Inflectional Homonymy. 
Linguistics, 26:601-61. 
Taft, Marcus. 1988. A morphological-decomposition 
model of lexical representation. Linguistics, 26:657-
667. 
Ultsch, Alfred and H. Peter Siemon. 1990. Kohonen's 
Self-Organizing Feature Maps for Exploratory Data 
Analysis. In ?Proceedings of INNC'90. International 
Neural Network Conference1990?, pages 305-308, 
Dordrecht 
Vesanto, Juha, Johan Himberg, Esa Alhoniemi, and 
Juha Parhankangas. 2000. SOM Toolbox for Matlab 
5 . Report A57, Helsinki University of Technology, 
Neural Networks Research Centre, Espoo, Finland. 
 
 
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 72?81,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Climbing the path to grammar: a maximum entropy model of 
subject/object learning 
 
 
Felice Dell?Orletta Alessandro Lenci Simonetta Montemagni Vito Pirrelli 
Dept. of Computer Science Dept. of Linguistics ILC-CNR ILC-CNR 
University of Pisa University of Pis a Area della Ricerca Area della Ricerca 
Largo Pontecorvo 3 
56100 Pisa (Italy) 
Via Santa Maria 36 
56100 Pisa (Italy) 
Via Moruzzi 1 
56100 Pisa (Italy) 
Via Moruzzi 1 
56100 Pisa (Italy) 
 
{felice.dellorletta, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it 
 
 
 
 
Abstract 
In this paper, we discuss an applic ation of 
Maximum Entropy to modeling the acqui-
sition of subject and object processing in 
Italian. The model is able to learn from 
corpus data a set of experimentally and 
theoretically well-motivated linguistic 
constraints, as well as their relative sali-
ence in Italian grammar development and 
processing. The model is also shown to 
acquire robust syntactic generalizations 
by relying on the evidence provided by a 
small number of high token frequency 
verbs only. These results are consistent 
with current research focusing on the role 
of high frequency verbs in allowing chil-
dren to converge on the most salient con-
straints in the grammar. 
1 Introduction 
Current research in language learning supports the 
view that developing grammatical competence in-
volve mastering and integrating multiple, parallel, 
probabilistic constraints defined over different 
types of linguistic (and non linguistic) information 
(Seidenberg and MacDonald 1999, MacWhinney 
2004). This is particularly clear when we focus on 
the core of grammatical deve lopment, namely the 
ability to properly identify syntactic relations. Psy-
cholinguistic evidence shows that children learn to 
identify sentence subjects and direct objects by 
combining various types of probabilistic cues, such 
as word order, noun animacy, definiteness, agree-
ment, etc. The relative prominence of each of these 
cues during the development of a child?s syntactic 
competence can considerably vary cross-
linguistically, mirroring their relative salience in 
the adult grammar system (cf. Bates et al 1984). 
If grammatical constraints are inherently prob-
abilistic (Manning 2003), the path through which 
the child acquires adult grammar competence can 
be viewed as the process of building a stochastic 
model out of the linguistic input. Consistently with 
?usage-based? approaches to language acquisition 
(cf. Tomasello, 2000) grammatical constraints 
would thus emerge from language use thanks to the 
child?s ability to keep track of statistical regulari-
ties in linguistic cues. In turn, this raises the issue 
of how children are able to exploit the statistical 
distribution of cues in the linguistic input. Various 
types of cross-linguistic evidence converge on the 
hypothesis that children are actually able to take 
great advantage of the highly skewed distribution 
of naturalistic language data. Goldberg et al 
(2004), Matthews et al (2003), Ninio (1999) 
among the others argue that verbs with high token 
frequency in the input have a facilitatory effect in 
allowing children to derive robust syntactic gener-
alizations even from surprisingly minimal input. 
According to this model, syntactic learning is 
driven by a small pool of verbs occurring with the 
highest token frequency: they approximately corre-
spond to so-called ?light verbs? such as English 
go, give , want etc. These verbs would act as ?cata-
72
lysts? in allowing children to converge on the most 
salient grammar constraints of the language they 
are acquiring. 
In computational linguistics, Maximum Entropy 
models have proven to be robust statistical learning 
algorithms that perform well in a number of proc-
essing tasks (cf. Ratnaparkhi 1998). In this paper, 
we discuss successful application of a Maximum 
Entropy (ME) model to the processing of Italian 
syntactic relations. We believe that this discussion 
is of general interest for two basic reasons. First, 
the model is able to learn, from corpus data, a set 
of experimentally and theoretically well-motivated 
linguistic constraints, as well as their relative sali-
ence in the processing of Italian. This suggests that 
it is possible for a child to bootstrap and use this 
type of knowledge on the basis of a specific distri-
bution of real language data, a conclusion that 
bears on the question of the role and type of innate 
inductive biases. Secondly, the model is also 
shown to acquire robust syntactic generalizations 
by relying on the evidence provided by a small 
number of high token frequency verbs only. With 
some qualifications, this evidence sheds light on 
the interaction between highly skewed language 
data distributions and language maturation. Robust 
grammar generalizations emerge on the basis of 
exposure to early, statist ically stable and lexically 
underspecified evidence, thus providing a reliable 
backbone to children?s syntactic development and 
later lexical organization.  
In the following section we first broach the 
general problem of parsing subjects and objects in 
Italian. Section 3 describes an ME model of the 
problem. Section 4 and 5 are devoted to a detailed 
empirical analysis of the interaction of different 
feature configurations and of the interplay between 
verb token frequency and relevant generalizations. 
Conclusions are drawn in the final discussion. 
2 Subjects and Objects in Italian 
Children that learn how to process subjects and 
objects in Italian are confronted with a twofold 
challenge: i) the relatively free order of Italian sen-
tence constituents and ii) the possible absence of 
an overt subject. The existence of a preferred Sub-
ject Verb Object (SVO) order in Italian main 
clauses does not rule out all other possible permu-
tations of these units: in fact, they are all attested, 
albeit with considerable differences in distribution 
and degree of markedness (Bartolini et al 2004).1 
Moreover, because of pro-drop, an Italian Verb 
Noun (VN) sequence can either be interpreted as a 
VO construction with subject omission (e.g. ha 
dichiarato guerra ?(he) declared war?) or as an 
instance of postverbal subject (VS, e.g. ha di-
chiarato Giovanni ?John declared?). Symmetri-
cally, an NV sequence is potentially ambiguous 
between SV and OV: compare il bambino ha man-
giato  ?the child ate? with il gelato ha mangiato ?the 
ice-cream, (he) ate?. 
These grammatical facts are in keeping with 
what we know about Italian children?s parsing 
strategies. Bates et al (1984) show that while, in 
English, word order is by and large the most effec-
tive cue for subject-object identification (hence-
forth SOI) both in syntactic processing and during 
the child?s syntactic development, the same cue 
plays second fiddle in Italian. Bates and colleagues 
bring empirical evidence supporting the hypothesis 
that Italian children show extreme reliance on NV 
agreement and, secondly, on noun animacy, rather 
than word order. They conclude that the following 
syntactic constraints dominance hierarchy is opera-
tive in Italian: agreement > animacy > word order. 
The fact that animacy can reliably be resorted 
to in Italian SOI receives indirect confirmation 
from corpus data. We looked at the distribution of 
animate subjects and objects in the Italian Syntac-
tic Semantic Treebank (ISST, Montemagni et al, 
2003), a 300,000 tokens syntactically annotated 
corpus, including articles from contemporary Ita l-
ian newspapers and periodicals covering a broad 
variety of topics. Subjects and objects in ISST 
were automatically annotated for animacy using 
the SIMPLE Italian computational lexicon (Lenci 
et al 2000) as a background semantic resource. 
The annotation was then checked manually. Cor-
pus analysis highlights a strong asymmetry in the 
distribution of animate nouns in subject and object 
roles: over 56.6% of ISST subjects are animate 
(out of a total number of 12,646), while only the 
11.1% of objects are animate (out of a total number 
of 5,559). Such an overwhelming preference for 
inanimate ob jects in adult language data makes 
animacy play a very important role in SOI, both as 
a key developmental factor in the bootstrapping of 
the syntax-semantics mapping and as a reliable 
                                                               
1 In the present paper we restrict ourselves to the case of de-
clarative main clauses. 
73
processing cue, consistently with psycholinguistic 
data. 
On the other hand, the distribution of word or-
der configurations in the same corpus shows an-
other interesting asymmetry. NV sequences receive 
an SV interpretation in 95.6% of the cases, and an 
object interpretation in the remaining 4.4% (most 
of which are clitic and relative pronouns, whose 
preverbal pos ition is grammatically constrained). 
The situation is quite different when we turn to VN 
sequences, where verb-object pairs represent 
73.4% of the cases, with verb-subject pairs repre-
senting the remaining 26.6%. We infer that ? at 
least in standard written Italian ? VS is a much 
more consistently used construction than OV, and 
that the role of word order in Italian parsing is not 
a marginal one across the board, but rather relative 
to VN contexts only. In NV constructions there is a 
strong preference for a subject interpretation, and 
this suggests a more dynamic dominance hierarchy 
of Italian syntactic constraints than the one pro-
vided above. 
As for agreement, it represents conclusive evi-
dence for SOI only when a nominal constituent and 
a verb do not agree in number and/or person (as in 
leggono il libro ?(they) read the book?). On the 
contrary, when noun and verb share the same per-
son and number the impact of agreement on SOI is 
neutralised, as in il bambino legge il libro ?the 
child reads the book? or in ha dichiarato il presi-
dente ?the president declared?. Although this ambi-
guity arises in specific contexts (i.e. when the verb 
is used in the third person singular or plural and the 
subject/object candidate agrees with it), it is inter-
esting to note that in ISST: third person verb forms 
cover 95.6% of all finite verb forms; and, more 
interestingly for our present concerns, 87.9% of all 
VN and NV pairs involving a third person verb 
form contains an agreeing noun. From this we con-
clude that the contribution of agreement to our 
problem is fairly limited, as lack  of agreement 
shows up only in a limited number of contexts. 
All in all, corpus data lend support to the idea 
that in Italian SOI is governed by a complex inter-
play of probabilistic constraints of a different na-
ture (morpho-syntactic, semantic, word order etc.). 
Moreover, distributional asymmetries in language 
data seem to provide a fairly reliable statistical ba-
sis upon which relevant probabilistic constraints 
can be bootstrapped and combined consistently. In 
the following section we shall present a ME model 
of how constraints and their interaction can be 
bootstrapped from language data. 
3 A Maximum Entropy model of SOI 
The Maximum Entropy (ME) framework offers a 
mathematically  sound way to build a probabilistic 
model for SOI, which combines different linguistic 
cues. Given a linguistic context c and an outcome 
a?A that depends on c, in the ME framework the 
conditional probability distribution p(a|c) is esti-
mated on the basis of the assumption that no a pri-
ori constraints must be met other than those related 
to a set of features f j(a,c) of c, whose distribution is 
derived from the training data. It can be proven 
that the probability distribution p satisfying the 
above assumption is the one with the highest en-
tropy, is unique and has the following exponential 
form (Berger et al 1996): 
(1) ?
=
=
k
j
cajf
jcZ
cap
1
),(
)(
1)|( a  
where Z(c) is a normalization factor, f j(a,c) are the 
values of k features of the pair (a,c) and correspond 
to the linguistic cues of c that are relevant to pre-
dict the outcome a. Features are extracted from the 
training data and define the constraints that the 
probabilistic model p must satisfy. The parameters 
of the distribution a1, ?, ak correspond to weights 
associated with the features, and determine the 
relevance of each feature in the overall model. In 
the experiments reported below feature weights 
have been estimated with the Generative Iterative 
Scaling (GIS) algorithm implemented in the AMIS 
software (Miyao and Tsujii 2002). 
We model SOI as the task of predicting the cor-
rect syntactic function f  ? {subject, object} of a 
noun occurring in a given syntactic context s. This 
is equivalent to build the conditional probability 
distribution p(f |s) of having a syntactic function f  
in a syntactic context s . Adopting the ME ap-
proach, the distribution p can be rewritten in the 
parametric form of (1), with features correspond-
ing to the linguistic contextual cues relevant to 
SOI. The context s  is a pair <vs , ns>, where vs is 
the verbal head and ns its nominal dependent in s. 
This notion of s departs from more traditional 
ways of describing an SOI context as a triple of 
one verb and two nouns in a certain syntactic con-
figuration (e.g, SOV or VOS, etc.). In fact, we as-
sume that SOI can be stated in terms of the more 
74
local task of establishing the grammatical function 
of a noun n observed in a verb-noun pair. This 
simplifying assumption is consistent with the claim 
in MacWhinney et al (1984) that SVO word order 
is actually derivative from SV and VO local pat-
terns and downplays the role of the transitive com-
plex construction in sentence processing. Evidence 
in favour of this hypothesis also comes from cor-
pus data: in ISST, there are 4,072 complete sub-
ject-verb-object-configurations, a small number if 
compared to the 11,584 verb tokens appearing with 
either a subject or an object only. Due to the com-
parative sparseness of canonical SVO constructions 
in Italian, it seems more reasonable to assume that 
children should pay a great deal of attention to 
both SV and VO units as cues in sentence percep-
tion (Matthews et al 2004). Reconstruction of the 
whole lexical SVO pattern can accordingly be seen 
as the end point of an acquisition process whereby 
smaller units are re-analyzed as being part of more 
comprehensive constructions. This hypothesis is 
more in line with a distributed view of canonical 
constructions as derivative of more basic local po-
sitional patterns, working together to yield more 
complex and abstract constructions. Last but not 
least, assuming verb-noun pairs as the relevant 
context for SOI allows us to simultaneously model 
the interaction of word order variation with pro-
drop in Italian. 
4 Feature selection 
The most important part of any ME model is the 
selection of the context features whose weights are 
to be estimated from data distributions. Our feature 
selection strategy is grounded on the main assump-
tion that features should correspond to linguisti-
cally and psycholinguistically well-motivated 
contextual cues. This allows us to evaluate the 
probabilistic model also with respect to its ability 
to replicate psycholinguistic experimental results 
and to be consistent with linguistic generalizations. 
Features are binary functions fki,f  (f ,s), which 
test whether a certain cue ki for the function f  oc-
curs in the context s . For our ME model of SOI, 
we have selected the following types of features: 
Word order tests the position of the noun wrt the 
verb, for instance: 
(2)
?
?
? ==
otherwise
postposnounif
subjf subjpost 0
.1
),(,
ss  
Animacy  tests whether the noun in s  is animate or 
inanimate (cf. ?.2). The centrality of this cue in 
Italian is widely supported by psycholinguistic 
evidence. Another source of converging evidence 
comes from functional and typological linguistic 
research. For instance, Aissen (2003) argues for 
the universal value of the following hierarchy rep-
resenting the relative markedness of the associa-
tions between grammatical functions and animacy 
degrees (with each item in these scale been less 
marked than the elements to its right): 
Animacy Markedness Hierarchy 
Subj/Human > Subj/Animate > Subj/Inanimate 
Obj/Inanimate > Obj/Animate > Obj/Human 
Markedness hierarchies have also been interpreted 
as probabilistic constraints estimated form corpus 
data (Bresnan et al 2001, ?vrelid 2004). In our 
ME model we have used a reduced version of the 
animacy markedness hierarchy in which human 
and animate nouns have been both subsumed under 
the general class animate. 
Definiteness tests the degree of ?referentiality? of 
the noun in a context pair s . Like for animacy, 
definiteness has been claimed to be associated with 
grammatical functions, giving rise to the following 
universal markedness hierarchy Aissen (2003): 
Definiteness Markedness Hierarchy 
Subj/Pro > Subj/Name > Subj/Def > Subj/Indef 
Obj/Indef > Obj/Def > Obj/Name > Obj/Pro 
According to this hierarchy, subjects with a low 
degree of definiteness are more marked than sub-
jects with a high degree of definiteness (for objects 
the reverse pattern holds). Given the importance 
assigned to the definiteness markedness hierarchy 
in current linguistic research, we have included the 
definiteness cue in the ME model. It is worth re-
marking that, unlike animacy, in psycholinguistic 
experiments definiteness has not been assigned any 
effective role in SOI. This makes testing this cue in 
a computational model even more interesting, as a 
way to evaluate its effective contribution to Italian 
SOI. In our experiments, we have used a ?com-
pact? version of the definiteness scale: the defi-
niteness cue tests whether the noun in the context 
75
pair i) is a name or a pronoun ii) has a definite arti-
cle iii), has an indefinite article or iv) is a ?bare? 
noun (i.e. with no article). It is worth saying that 
?bare? nouns are usually placed at the bottom end 
of the definiteness scale. 
The three types of features above only refer to 
nominal cues in the context pairs. Nevertheless, 
specific lexical properties of the verb can also be 
resorted to in SOI. The probability for ns to be sub-
ject or object may also depend on the specific lexi-
cal preferences of vs. To take this lexical factor 
into account, we add a set of lexical cues to the 
three general feature types above. Lexical cues test 
animacy with respect to a specific verb vk: 
(3) 
?
?
?
?
?
=?==
otherwise
animnvvif
subjf ksubjkvanim
0
1
),(,,
sss  
Lexical features provide evidence of the propensity 
of a given verb to have an animate (inanimate) 
subject or object. In fact, the verb argument struc-
ture and thematic properties may well influence the 
possible distribution of animate (inanimate) sub-
jects and objects, thus overriding more general 
tendencies. By including lexical cues, we are thus 
able to test the interplay of lexical constraints with 
general grammatical ones. 
Note that in our ME model we have not in-
cluded agreement as a feature, in spite of its 
prominent role in Italian. The fact that agreement 
is often inconclusive for SOI (?.2) suggests that 
children must also acquire the ability to deal with 
the interplay of various concurrent constraints, 
none of which is singularly sufficient for the task 
completion this type of competence. It is exactly 
this area of syntactic competence that we wanted to 
explore with the experiments reported below (cf. 
MacWhinney et al 1984, who similarly abstract 
from the dominant role of case in German SOI). 
5 Testing feature configurations for SOI 
The ME model for Italian SOI has been trained on 
18,205 verb-subject/object pairs extracted from 
ISST. The training set was obtained by extracting 
all verb-subject and verb-object dependencies 
headed by an active verb occurring in a finite ver-
bal construction and by excluding all cases where 
the position of the nominal constituent was gram-
matically constrained (e.g. clitic objects, relative 
clauses). Two different feature configurations have 
been used for training: 
-  non-lexical feature configuration (NLC), in-
cluding only general features acting as global 
constraints: namely word order, noun animacy 
and noun definiteness; 
- lexical feature configuration (LC), including 
word order, noun animacy and definiteness, 
and information about the verb head.  
The test corpus consists of 645 verb-noun pairs 
extracted from contexts where agreement happens 
to be neutralized. Of them, 446 contained a subject 
(either pre- or post-verbal) and 199 contained an 
object (either pre- or post-verbal). The two feature 
configurations were evaluated by calculating the 
percentage of correctly assigned relations over the 
total number of test pairs (accuracy). As our model 
always assigns one syntactic relation to each test 
pair, accuracy equals both standard precision and 
recall. Finally, we have assumed a baseline score 
of 69%, corresponding to the result yielded by a 
dumb model assigning to each test pair the most 
frequent relation in the training corpus, i.e. subject. 
5.1 Non-lexical feature configuration 
Our first experiment was carried out with NLC. 
The accuracy on the test corpus is 91.5%; most 
errors (i.e. 96.4%) relate to the postverbal position, 
with 44 mistaken subjects (42 inanimate) and 9 
mistaken objects (all animate). The score was con-
firmed by a 10-fold cross-validation on the whole 
training set (89.3% accuracy). 
A further way to evaluate the goodness of the 
model is by inspecting the weights associated with 
feature values (Table 1). 
 Subj Obj 
Preverbal 1,34E+00 2,10E-02 
Postverbal 5,21E-01 1,47E+00 
Anim 1,28E+00 3,34E-01 
Inanim 8,60E-01 1,21E+00 
PronName  1,22E+00 5,75E-01 
DefArt 1,05E+00 1,00E+00 
IndefArt 8,33E-01 1,16E+00 
NoArticle 9,46E-01 1,07E+00 
Table 1 ? Feature value weights in NLC  
The grey cells in Table 1 highlight the preference 
of each feature value for either subject or object 
identif ication: e.g. preverbal subjects are strongly 
preferred over preverbal objects; animate subjects 
76
are preferred over animate objects, etc. Interest-
ingly, if we rank the Anim and Inanim values for 
subjects and objects, we can observe tha t they dis-
tribute consistently with the Animacy Markedness 
Hierarchy reported in ?.4: Subj /Anim > 
Subj/Inanim and Obj/Inanim > Obj/Anim. Simi-
larly, by ranking the values of the definiteness fea-
tures in the Subj column by decreasing weight 
values we obtain the following ordering: Pron-
Name > DefArt > IndefArt > NoArt, which nicely 
fits in with the Definiteness Markedness Hierarchy 
in ?.4. The so-called ?markedness reversal? is ob-
served if we focus on the values for the same fea-
tures in the Obj column: the PronName feature 
represents the most marked option, followed by 
DefArt. The only exception is represented by the 
relative ordering of IndefArt and NoArt which 
however show very close values. 
Evaluating feature salience 
In order to evaluate the most reliable cues in Italian 
SOI, we have analysed the model predictions for 
different bundles of feature values. For each of the 
16 different bundles (b) attested in the data, we 
have estimated p(subj|b) and p(obj|b): 
b p(subj|b) p(obj|b) 
Pre Anim IndefArt 0,994 0,006
Pre Anim DefArt 0,996 0,004
Pre Anim NoArt 0,995 0,005
Pre Anim PronName 0,998 0,002
Pre Inanim IndefArt 0,970 0,030
Pre Inanim DefArt 0,979 0,021
Pre Inanim NoArt 0,976 0,024
Pre Inanim PronName 0,990 0,010
Post Anim IndefArt 0,495 0,505
Post Anim DefArt 0,589 0,411
Post Anim NoArt 0,546 0,454
Post Anim PronName  0,743 0,257
Post Inanim IndefArt 0,153 0,847
Post Inanim DefArt 0,209 0,791
Post Inanim NoArt 0,182 0,818
Post Inanim PronName 0,348 0,652
Table 2 ? Subj/obj probabilities by different bundles 
The model shows a neat preference for subject 
when the noun is preverbal. Instead, when the noun 
is postverbal, function assignment is de facto de-
cided by the noun animacy. Conversely, definite-
ness features have a much more secondary role: 
they can re-enforce (or weaken) the preference ex-
pressed by animacy, but they do not have the 
strength to determine SOI. 
The relative salience of the different constraints 
acting on SOI can also be inferred by comparing 
the weights associated with individual feature val-
ues. For instance, Goldwater and Johnson (2003) 
show that ME can be successfully applied to learn 
constraint rankings in Optimality Theory, by as-
suming the parameter weights a1, ?, ak as the 
ranking values of the constraints. The following 
table lists the 16 general constraints of the model 
by increasing weight values: 
 
Feature Weight 
Preverbal_Obj 2,10E-02
Anim_Obj 3,34E-01
Postverbal_Subj 5,21E-01
ProName_Obj 5,75E-01
IndefArt_Subj 8,33E-01
Inanim_Subj 8,60E-01
NoArticle_Subj 9,46E-01
ArtDef_Obj 1,00E+00
DefArt_Subj 1,05E+00
NoArticle_Obj 1,07E+00
IndefArt_Obj 1,16E+00
Inanim_Obj 1,21E+00
PronName_Subj 1,22E+00
Anim_Subj 1,28E+00
Preverbal_Subj 1,34E+00
Postverbal_Obj 1,47E+00
Table 3 ? Constraint weights ranking 
The rankings in Table 3 can be used to derive the 
relative salience of each constraint. Lower ranked 
constraints correspond to more marked syntactic 
configurations that are then disfavoured in SOI. 
Notice that the two animacy constraints Anim_Obj 
and Anim_Subj are respectively placed near the 
bottom and the top end of the scale. Notwithstand-
ing the low position of Postverbal_Subj, animacy 
is thus able to override the word order constraint 
and to produce a strong tendency to identify ani-
mate nouns as subjects, even when they appear in 
postverbal position (cf. Table 2 above). The con-
straint ranking thus confirms the interplay between 
animacy and word order in Italian, with the former 
playing a decisive role in assigning the syntactic 
function of postverbal nouns. On the other hand, 
77
the constraints involving noun definiteness occupy 
a more intermediate position in the general rank-
ing, with very close values. This is again consistent 
with the less decisive role of this feature type in 
SOI, as shown above. 
5.2 Lexical feature configuration 
In this experiment the general features reported in 
Table 1 have been integrated with 4,316 verb-
specific features as the ones exemplified below for 
the verb dire ?say?: 
dire_animSog 1.228213e+00 
dire_noanimSog 7.028484e-01 
dire_animOgg 3.645964e-01 
dire_noanimOgg 1.321887e+00 
whose associated weights show the strong prefer-
ence of this verb to take animate subjects as op-
posed to inanimate ones as well as a preference for 
inanimate objects with respect to animate ones. 
The results achieved with LC on the test corpus 
show a significant improvement with respect to 
those obtained with NLC: the accuracy is now 
95.5%, with a  4% improvement, confirmed by a 
10-fold cross-validation (94.9%). Also in this case, 
most of the errors relate to the pos tverbal position 
(i.e. 27 out of 29), partitioned into 26 mistaken 
subjects and 1 mistaken object. Lexical features 
have been resorted to to solve most of the NLC 
errors (i.e. 34 out of 55). It is interesting to note 
however that lexical features can also be mislead-
ing. The LC results include 8 new errors, suggest-
ing that lexical features do not always provide 
conclusive evidence: in fact, in 185 cases out of 
645 test VN pairs (i.e. 28.7% of the cases) general 
features are preferred over lexical ones. It is also 
worth mentioning that the ranking of general ani-
macy and definiteness features in LC actually fits 
in with the respective markedness hierarchies even 
with a better approximation than the one produced 
by NLC.  Finally, the relative prominence of the 
different global features confirms the trend in Ta-
ble 2, with word order being predominant in pre-
verbal pos ition and animacy playing a major role 
with postverbal nouns. 
Both feature configurations of the ME model 
thus appear to comply with linguistic and psycho-
linguistic generalizations on SOI. On the linguistic 
side, the constraints learnt by the model are consis-
tent with universal markedness hierarchies for 
grammatical relations. Secondly, the prominence 
of the various constraints in the model fits in well 
with psycholinguistic data. Consistently with the 
results in Bates et al (1984), the model confirms 
the great impact of noun animacy in Italian, al-
though in this case its key role seems to be more 
directly limited to the postverbal position. Con-
versely, the preverbal position is by itself a very 
strong cue for subject interpretation. 
6 High frequency verbs and SOI  
Frequency is known to play a major influence in 
language learning. In morphology, for example, 
highly frequent lexical items tend to be shorter 
forms, more readily accessible in the mental lexi-
con, independently stored as whole items (Stem-
berger and MacWhinney 1986) and fairly resistant 
to morphological overgeneralization through time, 
thus establishing a correlation between irregular 
inflected forms and frequency. Frequency has also 
been assigned a key role in the acquisition of syn-
tactic constructions. In fact, Goldberg (1998) and 
Ninio (1999) have independently argued for the 
existence of a causal relation between early expo-
sure to highly frequent light verbs and acquisition 
of abstract syntax-semantics mappings (construc-
tions). Light verbs such as want, put and go tend to 
be very frequent, because they are applicable in a 
wider range of contexts and are learned and used at 
an early language maturation stage The main idea 
is that children?s early use of these high frequency 
verbs is conducive to the acquisition of abstract 
constructional properties generalizing over particu-
lar instances. 
Goldberg et al (2004) motivate this hypothesis 
by observing that light verbs have high input fre-
quency in the child?s developmental environment 
and, at the same time, exhibit a low degree of se-
mantic specialization. Hence, she argues, it takes a 
little abstraction step for a child to jump from ac-
tual instances of use of light verbs to the syntax-
semantics association of their underlying construc-
tion. On the other hand, Ninio (1999) grounds the 
facilitatory role of highly frequent verbs on their 
being ?pathbreaking? prototypes of the construc-
tion they instantiate, since they are the best models 
of the relevant combinatorial and semantic proper-
ties of their construction in a relatively undiluted 
fashion. However, in the case of light verb con-
structions, the correlation between high frequency 
78
and construction prototypicality and extension is 
tenuous. In fact, it is difficult to argue that frequent 
light verbs such as see, want or do exhibit a high 
degree of both semantic and constructional trans i-
tivity (Goldberg et al 2004). This is reminiscent of 
the morphological behaviour of very frequent word 
forms in inflectional languages, as most of these 
forms are highly fused and show a general ten-
dency towards irregular inflection and low mor-
phological prototypicality. Furthermore, it is 
difficult to reconcile the ?pathbreaking? view with 
the observation that frequently observed linguistic 
units are memorized in full, as unanalyzed wholes. 
6.1 Testing the role of frequency 
To address these open issues and put the alleged 
?pathbreaking? role of light verbs to the challeng-
ing test of a probabilistic model, we carried out a 
second battery of experiments to learn the general, 
non-lexical constraints from two training corpora 
of roughly equivalent size where overall type and 
token verb frequencies were controlled for. Both 
corpora are a subset of the original training set: 
1. skewed frequency corpus (SF) ? it includes 
5,261 context pairs, obtained by selecting 15 verbs 
occurring more than 100 times in ISST (figures in 
parentheses give their token frequency): essere 
?be? (2406), avere ?have? (708), fare  ?do, make? 
(527), dire ?say, tell? (275), dare ?give? (173), ve-
dere ?see? (134), andare ?go? (126), sembrare 
?seem? (124), cercare ?try? (122), mettere ?put? 
(122), portare ?take? (121), trovare ?find? (112), 
volere ?want? (105), lasciare ?leave? (105), riuscire 
?manage? (101). It is worth noticing that this set 
includes typical ?pathbreaking? verbs; 
2. balanced frequency corpus (BF) ? this corpus 
includes 5,373 context pairs selected in such a way 
to ensure that every verb type in the original train-
ing set is attested in BF and occurs at most 6 times. 
For verbs occurring with a higher frequency, the 
pairs to be included in BF have been randomly se-
lected. 
Thus SF and BF represent two opposite training 
situations: SF contains few types with very high 
token frequencies, while BF contains a high num-
ber of verb types (i.e. 1457), with very low and 
uniform token frequency. These training sets re-
semble the structure of linguistic input used by 
Goldberg et al (2004) for their experiments. In 
that case, one group of subjects was exposed to 
linguistic inputs in which some verbs occurred 
with a much higher frequency than the others; a 
second group of subjects was instead exposed to 
linguistic stimuli in which every verb occurred 
with roughly equal frequency. Therefore, by train-
ing our ME model on SF and BF we are able  to 
evaluate the effective role of high token frequency 
verbs in driving syntactic learning.  
The ME model with the general features only 
(i.e. NLC) was first trained on SF, and then tested 
on the 645-pair corpus in ?.5, showing a 90% ac-
curacy. The same ME model was then trained on 
BF, and then tested on the 645-pair corpus, scoring 
a 87% accuracy. The ME model trained on the 
skewed frequency data thus outperforms the model 
trained on BF in a statistically significant way (?2 = 
4.97; a=0.05; p-value = 0.025). 
By using a training set formed only by the verbs 
with the highest token frequency, the model has 
thus been able to acquire robust syntactic con-
straints for SOI. Once these constraints have been 
applied to unseen events, the model has achieved a 
performance comparable to the one of the general 
models in ?.5. This is somehow even more signif i-
cant if we consider that the training set was now 
formed by less than one-third of the pairs on which 
the models in ?.5 were trained. Data quantity aside, 
the most relevant fact is that it is the way verb fre-
quencies are distributed to determine the learning 
path, with a significant positive effect produced by 
high token frequency verbs. In the model trained 
on SF, feature ranking is also governed by mark-
edness relations, and the relative prominence of the 
various constraints is utterly similar to the one dis-
cussed in ?.5. In other terms, the results of this ex-
periment prove that frequent verbs are actually 
able to act as ?catalysts? of the syntactic acquis i-
tion process. It is possible for children to converge 
on the correct generalizations governing SOI in 
Italian, just by relying on the linguistic evidence 
provided by the most frequent verbs. 
This view suggests a way out of the apparent 
paradox of the ?pathbreaking? hypothesis: highly 
frequent verbs can be assumed to provide stable 
and consistent multiple probabilistic cues for the 
assignment of subject/object relations. The exis-
tence of pos itional patterns that occur with high 
token frequency may well provide a deeply en-
trenched and highly salient set of distributional 
cues that act as probabilistic constraints on con-
structional generalizations. We hypothesize that 
similar constructions of other less frequent verbs 
79
are processed, for lack of more specific overriding 
information, in the light of these constraints. Since 
processing is the result of a ?conspiracy? of dis-
tributed constraints, ?pathbreaking? prototypes 
need not be real construction exemplars but highly 
schematic patterns. We proved that highly frequent 
local positional patterns offer the right sort of con-
straint conspiracy. 
7 General discussion 
It appears that the distributional evidence of high 
frequency light verbs may well provide a solid 
cognitive anchor for sweeping perceptual generali-
zations on the syntax-semantics mapping. These 
generalizations are local, in that they involve pos i-
tional NV and VN pairs only, and are perceptual as 
they address the issue of identifying appropriate 
syntactic relations by relying on perceptual fea-
tures of linguistic contexts, such as position, ani-
macy, etc. On the basis of these findings, one can 
reasonably argue that complex lexical construc-
tions (in the sense of Goldberg 1998) are built 
upon these local patterns, by combining them in 
those contexts where the presence of a particular 
verb licenses such a combination.  
The two feature configurations discussed in ?.5 
(i.e. NLC and LC) can thus be viewed as two suc-
cessive steps along the path that leads towards the 
emergence of complex, lexically-driven construc-
tions. This can actually be modeled as the incre-
mental process of adding more and more lexical 
constraints to early lexicon-free generalizations 
(based on word order, animacy, definiteness etc.). 
As a result of such additional constraints, the pres-
ence of an intransitive verb may completely rule 
out the object interpretation of a VN pattern, flying 
in the face of a general bias towards viewing VN as 
a transitive pattern. This picture is compatible with 
the well-known observation that constructions are 
used rather conservatively by children at early 
stages of language maturation (Tomasello 2000). 
In fact, if early generalizations are mainly percep-
tual and local, we do not expect them to be used in 
production, at least until the child reaches a stage 
where they are combined into bigger lexically-
driven constructions. 
ME has proven to be a sound computational 
learning framework to simulate the interplay of 
complex probabilistic constraints in language. Our 
experiments confirm linguistic generalizations and 
psycholinguistic data for subjects and objects in 
Italian, while raising new interesting issues at the 
same time. This is the case of the role of definite-
ness in SOI. In fact, the model features neatly re-
produce the definiteness markedness hierarchy, but 
definiteness does not appear to be really influential 
for subject and object processing. Various hy-
potheses are compatible with such results, inclu d-
ing that definiteness is not a cue on which speakers 
rely for SOI in Italian. Another more interesting 
possibility is that definiteness constraints may in-
deed play a decisive role when the learner is asked 
to assign subject and object relations in the context 
of a more complex construction than a simple NV 
pair. Suppose that both nouns of a noun-noun-verb 
triple are amenable to a subject interpretation, but 
that one of them is a more likely subject than the 
other due to its being part of a definite noun 
phrase. Then, it is reasonable to expect that the 
model would select the definite noun phrase as the 
subject in the triple and opt for an object interpre-
tation of the other candidate noun phrase.  
As part of our future work, we plan to train the 
ME model on a more realistic corpus of parental 
input to Italian children, available in the CHILDES 
database (MacWhinney, 2000: http://childes.psy. 
cmu.edu/data/Romance/Italian). In fact, there is 
converging evidence that the use of particular con-
structions in parental speech is largely dominated 
by the use of each construction with one specific, 
highly frequent verb (e.g. go for the intransitive 
construction). The same trends noted in mother?s 
speech to children are mirrored in children?s early 
speech (Goldberg et al, 2004). Quochi (in prepara-
tion) reports a similar distributional pattern for the 
caused motion and intransitive motion verbs in two 
Italian CHILDES corpora (named ?Italian-
Antelmi? and ?Italian-Calambrone?). If these find-
ings are confirmed, the high accuracy of our ME 
model trained on the skewed frequency corpus 
(SF) allows us to expect an equally high accuracy 
when training the model on evidence from Italian 
parental speech.  
This brings us to another related point: lack of 
correction/supervision in parental input. Since our 
ME model heavily relies on previously classified 
noun-verb pairs, we can legitimately wonder how 
easily it can be extended to simulate child language 
learning in an unsupervised mode. In fact, it should 
be appreciated that, in our experiments, compar-
tively little rests on supervised classification. Iden-
80
tification of the contextually-relevant subject is, for 
lack of explicit morphosyntactic clues such as 
agreement and diathesis, simply a matter of guess-
ing the more likely agent of the action expressed 
by the verb on the basis of semantic and pragmatic 
features such as animacy, definiteness and noun 
position to the verb. Mutatis mutandis, the same 
holds for object identification. It is then highly 
likely that salient evidence for the correct sub-
ject/object classific ation comes to the child from 
direct observation of the situation described by a 
sentence. It is such systematic coupling of linguis-
tic evidence from the sentence with perceptual evi-
dence of the situation described by the sentence 
that can assist the child in developing interface 
notions such as subject, object and the like.  
References 
Aissen J., 2003. Differential object marking: iconicity 
vs. economy. Natural Language and Linguistic The-
ory, 21: 435-483. 
Bartolini R., Lenci A., Montemagni S., Pirrelli V., 2004. 
Hybrid constraints for robust parsing: First experi-
ments and evaluation. LREC2004: 859-862. 
Bates E., MacWhinney B., Caselli C., Devescovi A., 
Natale F., Venza V., 1984. A crosslinguistic study of 
the development of sentence interpretation strategies. 
Child Development, 55: 341-354. 
Berger A., Della Pietra S., Della Pietra V., 1996. A 
maximum entropy approach to natural language 
processing. Computational Linguistics 22(1): 39-71 
Bresnan J., Dingare D., Manning C. D., 2001. Soft con-
straints mirror hard constraints: voice and person in 
English and Lummi. Proceedings of the LFG01 Con-
ference , Hong Kong: 13-32. 
Goldberg A. E., 1998. The emergence of the semantics 
of argument structure constructions. In B. MacWhin-
ney (e d.), The Emergence of Language . Lawrence 
Erlbaum Associates, Hillsdale, N. J.: 197-212. 
Goldberg A. E., Casenhiser D., Sethuraman N., 2004. 
Learning argument structure generalizations, Cogni-
tive Linguistics. 
Goldwater S., Johnson M. 2003. Learning OT Con-
straint Rankings Using a Maximum Entropy Model. 
In Spenader J., Eriksson A., Dahl ?. (eds.), Proceed-
ings of the Stockholm Workshop on Variation within 
Optimality Theory. April 26-27, 2003, Stockholm 
University: 111-120. 
Lenci A. et al, 2000. SIMPLE: A Ge neral Framework 
for the Development of Multilingual Lexicons. Inter-
national Journal of Lexicography, 13 (4): 249-263. 
Manning C. D., 2003. Probabilistic syntax. In R. Bod, J. 
Hay, S. Jannedy (eds), Probabilistic Linguistics,  
MIT Press, Cambridge MA: 289-341. 
MacWhinney, B., 2000. The CHILDES project: Tools 
for analyzing talk. Third Edition. Mahwah, NJ: La w-
rence Erlbaum Associates 
MacWhinney B., Bates E., Kliegl R., 1984. Cue validity 
and sentence interpretation in English, German, and 
Italian. Journal of Verbal Learning and Verbal Be-
havior, 23: 127-150. 
MacWhinney B., 2004. A unified model of language 
acquisition. In J. Kroll & A. De Groot (eds.), Hand-
book of bilingualism: Psycholinguistic approaches, 
Oxford University Press, Oxford. 
Matthews D., Lieven E., Theakston A., Tomasello M., 
in press, The role of frequency in the acquisition of 
English word order, Cognitive Development. 
Miyao Y., Tsujii J., 2002. Maximum entropy estimation 
for feature forests. Proc. HLT2002. 
Montemagni S. et al 2003. Building the Italian syntac-
tic-semantic treebank. In Abeill? A. (ed.) Treebanks. 
Building and Using Parsed Corpora , Kluwer, 
Dordrecht: 189-210. 
Ninio, A. 1999. Pathbreaking verbs in syntactic devel-
opment and the question of prototypical transitivity. 
Journal of Child Language, 26: 619- 653. 
?vrelid L., 2004. Disambiguation of syntactic functions 
in Norwegian: modeling variation in word order in-
terpretations conditioned by animacy and definite-
ness. Proceedings of the 20th Scandinavian 
Conference of Linguistics, Helsinki. 
Quochi, V., (in preparation). A constructional analysis 
of parental speech: The role of frequency and predic-
tion in language acquisition, evidence from Italian. 
Ratnaparkhi A., 1998. Maximum Entropy Models for 
Natural Language Ambiguity Resolution. Ph.D. Dis-
sertation, University of Pennsylvania. 
Seidenberg M. S., MacDonald M. C. 1999. A probabil-
istic constraints approach to language acquisition and 
processing. Cognitive Science  23(4): 569-588. 
Stemberger, J., MacWhinney, B. 1986. Frequency and 
the lexical storage of regularly inflected forms. 
Memory and Cognition, 14:17-26. 
Tomasello M., 2000. Do young children have adult syn-
tactic competence? Cognition , 74: 209-253. 
81
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 21?28,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Probing the space of grammatical variation: 
induction of cross-lingual grammatical constraints from treebanks 
 
 
Felice Dell?Orletta 
Universit? di Pisa, Dipartimento di 
Informatica - Largo B. Pontecorvo 3 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
felice.dellorletta@ilc.cnr.it 
Alessandro Lenci 
Universit? di Pisa, Dipartimento di 
Linguistica - Via Santa Maria 36 
56100 Pisa, Italy 
 
alessandro.lenci@ilc.cnr.it 
Simonetta Montemagni 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
simonetta.montemagni@ilc.cnr.it 
Vito Pirrelli 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
vito.pirrelli@ilc.cnr.it 
 
  
Abstract 
The paper reports on a detailed 
quantitative analysis of distributional 
language data of both Italian and Czech, 
highlighting the relative contribution of a 
number of distributed grammatical 
factors to sentence-based identification of 
subjects and direct objects. The work 
uses a Maximum Entropy model of 
stochastic resolution of conflicting 
grammatical constraints and is 
demonstrably capable of putting 
explanatory theoretical accounts to the 
test of usage-based empirical verification. 
1 Introduction 
The paper illustrates the application of a 
Maximum Entropy (henceforth MaxEnt) model 
(Ratnaparkhi 1998) to the processing of subjects 
and direct objects in Italian and Czech. The 
model makes use of richly annotated Treebanks 
to determine the types of linguistic factors 
involved in the task and weigh up their relative 
salience. In doing so,  we set ourselves a two-
fold goal. On the one hand, we intend to discuss 
the use of Treebanks to discover typologically 
relevant and linguistically motivated factors and 
assess the relative contribution of the latter to 
cross-linguistic parsing issues. On the other 
hand, we are interested in testing the empirical 
plausibility of constraint-resolution models of 
language processing (see infra) when confronted 
with real language data.  
Current research in natural language learning 
and processing supports the view that 
grammatical competence consists in mastering 
and integrating multiple, parallel constraints 
(Seidenberg and MacDonald 1999, MacWhinney 
2004). Moreover, there is growing consensus on 
two major properties of grammatical constraints: 
i.) they are probabilistic ?soft constraints? 
(Bresnan et al 2001), and ii.) they have an 
inherently functional nature, involving different 
types of linguistic (and non linguistic) 
information (syntactic, semantic, etc.). These 
features emerge particularly clearly in dealing 
with one of the core aspects of grammar 
learning: the ability to identify syntactic relations 
in text. Psycholinguistic evidence shows that 
speakers learn to identify sentence subjects and 
direct objects by combining various types of 
probabilistic, functional cues, such as word 
order, noun animacy, definiteness, agreement, 
etc. An important observation is that the relative 
prominence of each such cue can considerably 
vary cross-linguistically. Bates et al (1984), for 
example, argue that while, in English, word order 
is the most effective cue for Subject-Object 
Identification (henceforth SOI) both in syntactic 
processing and during the child?s syntactic 
development, the same cue plays second fiddle in 
relatively free phrase-order languages such as 
Italian or German. 
If grammatical constraints are inherently 
probabilistic (Manning 2003), the path through 
which adult grammar competence is acquired can 
be viewed as the process of building a stochastic 
model out of the linguistic input. In 
computational linguistics, MaxEnt models have 
21
proven to be robust statistical learning algorithms 
that perform well in a number of processing 
tasks. Being supervised learning models, they  
require richly annotated data as training input. 
Before we turn to the use of Treebanks for 
training a MaxEnt model for SOI, we first 
analyse the range of linguistic factors that are 
taken to play a significant role in the task.   
2 Subjects and objects in Czech and 
Italian 
Grammatical relations - such as subject (S) and 
direct object (O) - are variously encoded in 
languages, the two most widespread strategies 
being: i) structural encoding through word order, 
and ii) morpho-syntactic marking. In turn, 
morpho-syntactic marking can apply either on 
the noun head only, in the form of case 
inflections, or on both the noun and the verb, in 
the form of agreement marking (Croft 2003). 
Besides formal coding, the distribution of 
subjects and object is also governed by semantic 
and pragmatic factors, such as noun animacy, 
definiteness, topicality, etc. As a result, there 
exists a variety of linguistic clues jointly co-
operating in making a particular noun phrase the 
subject or direct object of a sentence. Crucially 
for our present purposes, cross-linguistic 
variation does not only concern the particular 
strategy used to encode S and O, but also the 
relative strength that each factor plays in a given 
language. For instance, while English word order 
is by and large the dominant clue to identify S 
and O, in other languages the presence of a rich 
morphological system allows word order to have 
a much looser connection with the coding of 
grammatical relations, thus playing a secondary 
role in their identification. Moreover, there are 
languages where semantic and pragmatic 
constraints such as animacy and/or definiteness 
play a predominant role in the processing of 
grammatical relations. A large spectrum of 
variations exists, ranging from languages where 
S must have a higher degree of animacy and/or 
definiteness relative to O, to languages where 
this constraint only takes the form of a softer 
statistical preference (cf. Bresnan et al 2001). 
The goal of this paper is to explore the area of 
this complex space of grammar variation through 
careful assessment of the distribution of S and O 
tokens in Italian and Czech. For our present 
analysis, we have used a MaxEnt statistical 
model trained on data extracted from two 
syntactically annotated corpora: the Prague 
Dependency Treebank (PDT, Bohmova et al 
2003) for Czech, and the Italian Syntactic 
Semantic Treebank (ISST, Montemagni et al 
2003) for Italian. These corpora have been 
chosen not only because they are the largest 
syntactically annotated resources for the two 
languages, but also because of their high degree 
of comparability, since they both adopt a 
dependency-based annotation scheme. 
Czech and Italian provide an interesting 
vantage point for the cross-lingual analysis of 
grammatical variation. They are both Indo-
European languages, but they do not belong to 
the same family: Czech is a West Slavonic 
language, while Italian is a Romance language. 
For our present concerns, they appear to share 
two crucial features: i) the free order of 
grammatical relations with respect to the verb; ii) 
the possible absence of an overt subject. 
Nevertheless, they also greatly differ due to: the 
virtual non-existence of case marking in Italian 
(with the only marginal exception of personal 
pronouns), and the degree of phrase-order 
freedom in the two languages. Empirical 
evidence supporting the latter claim is provided 
in Table 1, which reports data extracted from 
PDT and ISST. Notice that although in both 
languages S and O can occur either pre-verbally 
or post-verbally, Czech and Italian greatly differ 
in their propensity to depart from the (unmarked) 
SVO order. While in Italian preverbal O is 
highly infrequent (1.90%), in Czech more than 
30% of O tokens occur before the verb. The 
situation is similar but somewhat more balanced 
in the case of S, which occurs post-verbally in 
22.21% of the Italian cases, and  in  40% of 
Czech ones. For sure, one can argue that, in 
spoken Italian, the number of pre-verbal objects 
is actually higher, because of the greater number 
of left dislocations and topicalizations occurring 
in informal speech. However reasonable, the 
observation does not explain away the 
distributional differences in the two corpora, 
since both PDT and ISST contain written 
language only. We thus suggest that there is clear 
empirical evidence in favour of a systematic, 
higher phrase-order freedom in Czech, arguably 
related to the well-known correlation of Czech 
constituent placement with sentence information 
structure, with the element carrying new 
information showing a tendency to occur 
sentence-finally (Stone 1990). For our present 
concerns, however, aspects of information 
structure, albeit central in Czech grammar, were 
not taken into  account, as they  happen not to  be 
22
 
   Czech Italian 
    Subj Obj Subj Obj 
Pre 59.82% 30.27% 77.79% 1.90% 
Post 40.18% 69.73% 22.21% 98.10% Pos 
All 100.00% 100.00% 100.00% 100.00% 
Agr 98.50% 56.54% 97.73% 58.33% 
NoAgr 1.50% 43.46% 2.27% 41.67% Agr 
All 100.00% 100.00% 100.00% 100.00% 
Anim 34.10% 15.42% 50.18% 10.67% 
NoAnim 65.90% 84.58% 49.82% 89.33% Anim 
All 100.00% 100.00% 100.00% 100.00% 
Table 1 ?Distribution of Czech and Italian S and O wrt word order, 
agreement and noun animacy 
 Czech 
 Subj Obj 
Nominative 53.83% 0.65% 
Accusative 0.15% 28.30% 
Dative 0.16% 9.54% 
Genitive 0.22% 2.03% 
Instrumental 0.01% 3.40% 
Ambiguous 45.63% 56.08% 
All 100.00% 100.00% 
Table 2 - Distribution of Czech S and O 
wrt case 
marked-up in the Italian corpus.  
According to the data reported in Table 1, 
Czech and Italian show similar correlation 
patterns between animacy and grammatical 
relations. S and O in ISST were automatically 
annotated for animacy using the SIMPLE Italian 
computational lexicon (Lenci et al 2000) as a 
background semantic resource. The annotation 
was then checked manually. Czech S and O were 
annotated for animacy using Czech WordNet 
(Pala and Smrz 2004); it is worth remarking that 
in Czech animacy annotation was done only 
automatically, without any manual revision. 
Italian shows a prominent asymmetry in the 
distribution of animate nouns in subject and 
object roles: over 50% of ISST subjects are 
animate, while only 10% of the objects are 
animate. Such a trend is also confirmed in Czech 
? although to a lesser extent - with 34.10% of 
animate subjects vs. 15.42% of objects.1 Such an 
overwhelming preference for animate subjects in 
corpus data suggests that animacy may play a 
very important role for S and O identification in 
both languages. 
Corpus data also provide interesting evidence 
concerning the actual role of morpho-syntactic 
constraints in the distribution of grammatical 
relations. Prima facie, agreement and case are 
the strongest and most directly accessible clues 
for S/O processing, as they are marked both 
overtly and locally. This is also confirmed by 
psycholinguistic evidence, showing that subjects 
tend to rely on these clues to identify S/O. 
However, it should be observed that agreement 
can be relied upon conclusively in S/O 
processing only when a nominal constituent and 
                                               
1 In fact, the considerable difference in animacy distribution 
between the two languages might only be an artefact of the 
way we annotated Czech nouns semantically, on the basis of 
their context-free classification in the Czech WordNet. 
a verb do not agree in number and/or person (as 
in leggono il libro ?(they) read the book?). 
Conversely, when N and V share the same 
person and number, no conclusion can be drawn, 
as trivially shown by a sentence like il bambino 
legge il libro ?the child reads the book?. In ISST, 
more than 58% of O tokens agree with their 
governing V, thus being formally 
indistinguishable from S on the basis of 
agreement features. PDT also exhibits a similar 
ratio, with 56% of O tokens agreeing with their 
verb head. Analogous considerations apply to 
case marking, whose perceptual reliability is 
undermined by morphological syncretism,  
whereby different  cases are realized through the 
same marker. Czech data reveal the massive 
extent of this phenomenon and its impact on SOI. 
As reported in Table 2, more than 56% of O 
tokens extracted from PDT are formally 
indistinguishable from S in case ending. 
Similarly, 45% of S tokens are formally 
indistinguishable from O uses on the same 
ground. All in all, this means that in 50% of the 
cases a Czech noun can not be understood as the 
S/O of a sentence by relying on overt case 
marking only. 
To sum up, corpus data lend support to the 
idea that in both Italian and in Czech SOI is 
governed by a complex interplay of probabilistic 
constraints of a different nature (morpho-
syntactic, semantic, word order, etc.) as the latter 
are neither singly necessary nor jointly sufficient 
to attack the processing task at hand. It is 
tempting to hypothesize that the joint distribution 
of these data can provide a statistically reliable 
basis upon which relevant probabilistic 
constraints are bootstrapped and combined 
consistently. This should be possible due to i) the 
different degrees of clue salience in the two 
languages and ii) the functional need to minimize 
23
processing ambiguity in ordinary communicative 
exchanges. With reference to the latter point, for 
example, we may surmise that a speaker will be 
more inclined to violate one constraint on S/O 
distribution (e.g. word order) when another clue 
is available (e.g. animacy) that strongly supports 
the intended interpretation only. The following 
section illustrates how a MaxEnt model can be 
used to model these intuitions by bootstrapping 
constraints and their interaction from language 
data. 
3 Maximum Entropy modelling 
The MaxEnt framework offers a mathematically 
sound way to build a probabilistic model for SOI, 
which combines different linguistic cues. Given 
a linguistic context c and an outcome a?A that 
depends on c, in the MaxEnt framework the 
conditional probability distribution p(a|c) is 
estimated on the basis of the assumption that no 
a priori constraints must be met other than those 
related to a set of features fj(a,c) of c, whose 
distribution is derived from the training data. It 
can be proven that the probability distribution p 
satisfying the above assumption is the one with 
the highest entropy, is unique and has the 
following exponential form (Berger et al 1996): 
(1) ?
=
=
k
j
caf
j
j
cZcap 1
),(
)(
1)|( a  
where Z(c) is a normalization factor, fj(a,c) are 
the values of k features of the pair (a,c) and 
correspond to the linguistic cues of c that are 
relevant to predict the outcome a. Features are 
extracted from the training data and define the 
constraints that the probabilistic model p must 
satisfy. The parameters of the distribution ?1, ?, 
?k correspond to weights associated with the 
features, and determine the relevance of each 
feature in the overall model. In the experiments 
reported below feature weights have been 
estimated with the Generative Iterative Scaling 
(GIS) algorithm implemented in the AMIS 
software (Miyao and Tsujii 2002). 
We model SOI as the task of predicting the 
correct syntactic function ? ? {subject, object} 
of a noun occurring in a given syntactic context 
?. This is equivalent to building the conditional 
probability distribution p(?|?) of having a 
syntactic function ? in a syntactic context ?. 
Adopting the MaxEnt approach, the distribution 
p can be rewritten in the parametric form of (1), 
with features corresponding to the linguistic 
contextual cues relevant to SOI. The context ? is 
a pair <v?, n?>, where v? is the verbal head and n? 
its nominal dependent in ?. This notion of ? 
departs from more traditional ways of describing 
an SOI context as a triple of one verb and two 
nouns in a certain syntactic configuration (e.g, 
SOV or VOS, etc.). In fact, we assume that SOI 
can be stated in terms of the more local task of 
establishing the grammatical function of a noun 
n observed in a verb-noun pair. This simplifying 
assumption is consistent with the claim in 
MacWhinney et al (1984) that SVO word order 
is actually derivative from SV and VO local 
patterns and downplays the role of the transitive 
complex construction in sentence processing. 
Evidence in favour of this hypothesis also comes 
from corpus data: for instance, in ISST complete 
subject-verb-object configurations represent only 
26% of the cases, a small percentage if compared 
to the 74% of verb tokens appearing with either a 
subject or an object only; a similar situation can 
be observed in PDT where complete subject-
verb-object configurations occur in only 20% of 
the cases. Due to the comparative sparseness of 
canonical SVO constructions in Czech and 
Italian, it seems more reasonable to assume that 
children should pay a great deal of attention to 
both SV and VO units as cues in sentence 
perception (Matthews et al in press). 
Reconstruction of the whole lexical SVO pattern 
can accordingly be seen as the end point of an 
acquisition process whereby smaller units are re-
analyzed as being part of more comprehensive 
constructions. This hypothesis is more in line 
with a distributed view of canonical 
constructions as derivative of more basic local 
positional patterns, working together to yield 
more complex and abstract constructions. Last 
but not least, assuming verb-noun pairs as the 
relevant context for SOI allows us to 
simultaneously model the interaction of word 
order variation with pro-drop. 
4 Feature selection 
The most important part of any MaxEnt model is 
the selection of the context features whose 
weights are to be estimated from data 
distributions. Our feature selection strategy is 
grounded on the main assumption that features 
should correspond to theoretically and 
typologically well-motivated contextual cues. 
This allows us to evaluate the probabilistic 
model also with respect to its consistency with 
current linguistic generalizations. In turn, the 
model can be used as a probe into the 
correspondence between theoretically motivated 
24
generalizations and usage-based empirical 
evidence.  
Features are binary functions fki,? (?,?), which 
test whether a certain cue ki for the feature ? 
occurs in the context ?. For our MaxEnt model, 
we have selected different features types that test 
morpho-syntactic, syntactic, and semantic key 
dimensions in determining the distribution of S 
and O. 
 
Morpho-syntactic features. These include N-V 
agreement, for Italian and Czech, and case, only 
for Czech. The combined use of such features 
allow us not only to test the impact of morpho-
syntactic information on SOI, but also to analyze 
patterns of cross-lingual variation stemming 
from language specific morphological 
differences, e.g. lack of case marking in Italian. 
 
Word order. This feature essentially test the 
position of the noun wrt the verb, for instance: 
(2)
??
? == otherwise
postposnounifsubjf subjpost 0
.1),(, ss  
 
Animacy. This is the main semantic feature, 
which tests whether the noun in ? is animate or 
inanimate (cf. section 2). The centrality of this 
cue for grammatical relation assignment is 
widely supported by typological evidence (cf. 
Aissen 2003, Croft 2003). The Animacy 
Markedness Hierarchy - representing the relative 
markedness of the associations between 
grammatical functions and animacy degrees ? is 
actually assigned the role of a functional 
universal principle in grammar. The hierarchy is 
reported below, with each item in these scales 
been less marked than the elements to its right: 
 
Animacy Markedness Hierarchy 
Subj/Human > Subj/Animate > Subj/Inanimate 
Obj/Inanimate > Obj/Animate > Obj/Human 
 
Markedness hierarchies have also been 
interpreted as probabilistic constraints estimated 
from corpus data (Bresnan et al 2001). In our 
MaxEnt model we have used a reduced version 
of the animacy markedness hierarchy in which 
human and animate nouns have been both 
subsumed under the general class animate. 
 
Definiteness tests the degree of ?referentiality? of 
the noun in a context pair ?. Like for animacy, 
definiteness has been claimed to be associated 
with grammatical functions, giving rise to the 
following universal markedness hierarchy Aissen 
(2003): 
 
Definiteness Markedness Hierarchy 
Subj/Pro > Subj/Name > Subj/Def > Subj/Indef 
Obj/Indef > Obj/Def > Obj/Name > Obj/Pro 
 
According to this hierarchy, subjects with a low 
degree of definiteness are more marked than 
subjects with a high degree of definiteness (for 
objects the reverse pattern holds). Given the 
importance assigned to the definiteness 
markedness hierarchy in current linguistic 
research, we have included the definiteness cue 
in the MaxEnt model. In our experiments, for 
Italian we have used a compact version of the 
definiteness scale: the definiteness cue tests 
whether the noun in the context pair i) is a name 
or a pronoun ii) has a definite article iii), has an 
indefinite article or iv) is a bare noun (i.e. with 
no article). It is worth saying that bare nouns are 
usually placed at the bottom end of the 
definiteness scale. Since in Czech there is no 
article, we only make a distinction between 
proper names and common nouns. 
5 Testing the model 
The Italian MaxEnt model was trained on 14,643 
verb-subject/object pairs extracted from ISST. 
For Czech, we used a training corpus of 37,947 
verb-subject/object pairs extracted from PDT. In 
both cases, the training set was obtained by 
extracting all verb-subject and verb-object 
dependencies headed by an active verb, with the 
exclusion of all cases where the position of the 
nominal constituent was grammatically 
determined (e.g. clitic objects, relative clauses). 
It is interesting to note that in both training sets 
the proportion of subjects and objects relations is 
nearly the same: 63.06%-65.93% verb-subject 
pairs and 36.94%-34.07% verb-object pairs for 
Italian and Czech respectively. 
The test corpus consists of a set of verb-noun 
pairs randomly extracted from the reference 
Treebanks: 1,000 pairs for Italian and 1,373 for 
Czech. For Italian, 559 pairs contained a subject 
and 441 contained an object; for Czech, 905 
pairs contained a subject and 468 an object. 
Evaluation was carried out by calculating the 
percentage of  correctly  assigned  relations  over 
the total number of test pairs (accuracy). As our 
model always assigns one syntactic relation to 
each test pair, accuracy equals both standard 
precision and recall. 
25
  Czech Italian 
  Subj Obj Subj Obj 
Preverb 1.99% 19.40% 0.00% 6.90% 
Postverb 71.14% 7.46% 71.55% 21.55% 
Anim 0.50% 3.98% 6.90% 21.55% 
Inanim 72.64% 22.89% 64.66% 6.90% 
Nomin 0.00% 1.00% 
Genitive 0.50% 0.00% 
Dative 1.99% 0.00% 
Accus 0.00% 0.00% 
Instrum 0.00% 0.00% 
Ambig 70.65% 25.87% 
Na 
Agr 70.15% 25.87% 61.21% 12.07% 
NoAgr 2.99% 0.50% 7.76% 1.72% 
NAAgr 0.00% 0.50% 2.59% 14.66% 
Table 3 ? Types of errors for Czech and Italian 
 
 Czech Italian 
 Subj Obj Subj Obj 
Preverb 1.24E+00 5.40E-01 1.31E+00 2.11E-02 
Postverb 8.77E-01 1.17E+00 5.39E-01 1.38E+00 
Anim 1.16E+00 6.63E-01 1.28E+00 3.17E-01 
Inanim 1.03E+00 9.63E-01 8.16E-01 1.23E+00 
PronName 1.13E+00 7.72E-01 1.13E+00 8.05E-01 
DefArt 1.01E+00 1.02E+00 
IndefArt 6.82E-01 1.26E+00 
NoArticle 
1.05E+00 9.31E-01 
9.91E-01 1.02E+00 
Nomin 1.23E+00 2.22E-02 
Genitive 2.94E-01 1.51E+00 
Dative 2.85E-02 1.49E+00 
Accus 8.06E-03 1.39E+00 
Instrum 3.80E-03 1.39E+00 
Na 
Agr 1.18E+00 6.67E-01 1.28E+00 4.67E-01 
NoAgr 7.71E-02 1.50E+00 1.52E-01 1.58E+00 
NAAgr 3.75E-01 1.53E+00 2.61E-01 1.84E+00 
Table 4 - Feature value weights in NLC for Czech and 
Italian
We have assumed a baseline score of 56% for 
Italian and of 66% for Czech, corresponding to 
the result yielded by a naive model   assigning   
to  each   test   pair  the   most frequent relation 
in the training corpus, i.e. subject. Experiments 
were carried out with the general features 
illustrated in section 4: verb agreement, case (for 
Czech only), word order, noun animacy and 
noun definiteness. 
Accuracy on the test corpus is 88.4% for 
Italian and 85.4% for Czech. A detailed error 
analysis for the two languages is reported in 
Table 3, showing that in both languages subject 
identification appears to be particularly 
problematic. In Czech, it appears that the 
prototypically mistaken subjects are post-verbal 
(71.14%), inanimate (72.64%), ambiguously 
case-marked (70.65%) and agreeing with the 
verb (70.15%), where reported percentages refer 
to the whole error set. Likewise, Italian mistaken 
subjects can be described thus: they typically 
occur in post-verbal position (71.55%), are 
mostly inanimate (64.66%) and agree with the 
verb (61.21%). Interestingly, in both languages, 
the highest number of errors occurs when a) N 
has the least prototypical syntactic and semantic 
properties for O or S (relative to word order and 
noun animacy) and b) morpho-syntactic features 
such as agreement and case are neutralised. This 
shows that MaxEnt is able to home in on the core 
linguistic properties that govern the distribution 
of S and O in Italian and Czech, while remaining 
uncertain in the face of somewhat peripheral and 
occasional cases. 
A further way to evaluate the goodness of fit 
of our model is by inspecting the weights 
associated with feature values for the two 
languages. They are reported in Table 4, where 
grey cells highlight the preference of each 
feature value for either subject or object 
identification. In both languages agreement with 
the verb strongly relates to the subject relation. 
For Czech, nominative case is strongly 
associated with subjects while the other cases 
with objects. Moreover, in both languages 
preverbal subjects are strongly preferred over 
preverbal objects; animate subjects are preferred 
over animate objects; pronouns and proper 
names are typically subjects.  
Let us now try to relate these feature values to 
the Markedness Hierarchies reported in section 
4. Interestingly enough, if we rank the Italian 
Anim and Inanim values for subjects and objects, 
we observe that they distribute consistently with 
the Animacy Markedness Hierarchy: Subj/Anim 
> Subj/Inanim and Obj/Inanim > Obj/Anim. This 
is confirmed by the Czech results. Similarly, by 
ranking the Italian values for the definiteness 
features in the Subj column by decreasing weight 
values we obtain the following ordering: 
PronName > DefArt > IndefArt > NoArt, which 
nicely fits in with the Definiteness Markedness 
Hierarchy in section 4. The so-called 
?markedness reversal? is replicated with a good 
degree of approximation, if we focus on the 
values for the same features in the Obj column: 
the PronName feature represents the most 
marked option, followed by IndefArt, DefArt and 
NoArt (the latter two showing the same feature 
value). The exception here is represented by the 
relative ordering of IndefArt and DefArt which 
however show very close values. The same 
26
seems to hold for Czech, where the feature 
ordering for Subj is PronName > 
DefArt/IndefArt/NoArt and the reverse is 
observed for Obj.  
5.1 Evaluating comparative feature salience 
The relative salience of the different constraints 
acting on SOI can be inferred by comparing the 
weights associated with individual feature 
values. For instance, Goldwater and Johnson 
(2003) show that MaxEnt can successfully be 
applied to learn constraint rankings in Optimality 
Theory, by assuming the parameter weights <?1, 
?, ?k> as the ranking values of the constraints.  
Table 5 illustrates the constraint ranking for 
the two languages, ordered by decreasing weight 
values for both S and O. Note that, although not 
all constraints are applicable in both languages, 
the weights associated with applicable 
constraints exhibit the same relative salience in 
Czech and Italian. This seems to suggest the 
existence of a rather dominant (if not universal) 
salience scale of S and O processing constraints, 
in spite of the considerable difference in the 
marking strategies adopted by the two languages. 
As the relative weight of each constraint 
crucially depends on its overall interaction with 
other constraints on a given processing task, 
absolute weight values can considerably vary 
from language to language, with a resulting 
impact on the distribution of S and O 
constructions. For example, the possibility of 
overtly and unambiguously marking a direct 
object with case inflection makes wider room for 
preverbal use of objects in Czech. Conversely, 
lack of case marking in Italian considerably 
limits the preverbal distribution of direct objects.   
This evidence, however, appears to be an 
epiphenomenon of the interaction of fairly stable 
and invariant preferences, reflecting common 
functional tendencies in language processing. As 
shown in Table 5, if constraint ranking largely 
confirms the interplay between animacy and 
word order in Italian, Czech does not contradict 
it but rather re-modulate it somewhat, due to the 
?perturbation? factors introduced by its richer 
battery of case markers. 
6 Conclusions 
Probabilistic language models, machine language 
learning algorithms and linguistic theorizing all 
appear to support a view of language processing 
as a process of dynamic, on-line resolution of 
conflicting grammatical constraints. We begin to 
gain considerable insights into the complex 
process of bootstrapping nature and behaviour of 
these constraints upon observing their actual 
distribution in perceptually salient contexts. In 
our view of things, this trend outlines a 
promising framework providing fresh support to 
usage-based models of language acquisition 
through mathematical and computational 
simulations. Moreover, it allows scholars to 
investigate patterns of cross-linguistic 
typological variation that crucially depend on the 
appropriate setting of model parameters. Finally, 
it promises to solve, on a principled basis, 
traditional performance-oriented cruces of 
grammar theorizing such as degrees of human 
acceptability of ill-formed grammatical 
constructions (Hayes 2000) and the inherently 
graded compositionality of linguistic 
constructions such as morpheme-based words 
and word-based phrases (Bybee 2002, Hay and 
Baayen 2005).  
We argue that the current availability of 
comparable, richly annotated corpora and of 
mathematical tools and models for corpus 
exploration make time ripe for probing the space 
of grammatical variation, both intra- and inter-
linguistically, on unprecedented levels of 
sophistication and granularity. All in all, we 
anticipate that such a convergence is likely to 
have a twofold impact: it is bound to shed light 
on the integration of performance and 
competence factors in language study; it will 
make mathematical models of language 
increasingly able to accommodate richer and 
richer language evidence, thus putting 
explanatory theoretical accounts to the test of a 
usage-based empirical verification. 
In the near future, we intend to pursue two 
parallel lines of development. First we would 
like to increase the context-sensitiveness of our 
processing task by integrating binary 
grammatical constraints into the broader context 
of multiply conflicting grammar relations. This 
way, we will be in a position to capture the 
constraint that a (transitive) verb has at most one 
subject and one object, thus avoiding multiple 
assignment of subject (object) relations in the 
same context. Suppose, for example, that both 
nouns in a noun-noun-verb triple are amenable to 
a subject interpretation, but that one of them is a 
more likely subject than the other. Then, it is 
reasonable to expect the model to process the 
less likely subject candidate as the object of the 
verb in the triple. Another promising line of 
development is based on the observation that the 
27
order in which verb arguments appear in context 
is also lexically governed: in Italian, for 
example, report verbs show a strong tendency to 
select subjects post-verbally. Dell?Orletta et al 
(2005) report a substantial improvement on the 
model performance on Italian SOI when lexical 
information is taken into account, as a lexicalized 
MaxEnt model appears to integrate general 
constructional and semantic biases with 
lexically-specific preferences. In a cross-lingual 
perspective, comparable evidence of lexical 
constraints on word order would allow us to 
discover language-wide invariants in the lexicon-
grammar interplay.   
References 
Bates E., MacWhinney B., Caselli C., Devescovi A., 
Natale F., Venza V. 1984. A crosslinguistic study 
of the development of sentence interpretation 
strategies. Child Development, 55: 341-354. 
Bohmova A., Hajic J., Hajicova E., Hladka B. 2003. 
The Prague Dependency Treebank: Three-Level 
Annotation Scenario, in A. Abeille (ed.) 
Treebanks: Building and Using Syntactically 
Annotated Corpora, Kluwer Academic Publishers, 
pp. 103-128. 
Bybee J. 2002. Sequentiality as the basis of 
constituent structure. in T. Giv?n and B. Malle 
(eds.) The Evolution of Language out of Pre-
Language, Amsterdam: John Benjamins. 107-132. 
Croft W. 2003. Typology and Universals. Second 
Edition, Cambridge University Press, Cambridge. 
Bresnan J., Dingare D., Manning C. D. 2001. Soft 
constraints mirror hard constraints: voice and 
person in English and Lummi. Proceedings of the 
LFG01 Conference, Hong Kong: 13-32. 
Dell?Orletta F., Lenci A., Montemagni S., Pirrelli V. 
2005. Climbing the path to grammar: a maximum 
entropy model of subject/object learning. 
Proceedings of the ACL-2005 Workshop 
?Psychocomputational Models of Human 
Language Acquisition?, University of Michigan, 
Ann Arbour (USA), 29-30 June 2005. 
Hay J., Baayen R.H. 2005. Shifting paradigms: 
gradient structure in morphology, Trends in 
Cognitive Sciences, 9(7): 342-348. 
Hayes B. 2000. Gradient Well-Formedness in 
Optimality Theory, in Joost Dekkers, Frank van 
der Leeuw and Jeroen van de Weijer (eds.) 
Optimality Theory: Phonology, Syntax, and 
Acquisition, Oxford University Press, pp. 88-120. 
Lenci A. et al 2000. SIMPLE: A General Framework 
for the Development of Multilingual Lexicons. 
International Journal of Lexicography, 13 (4): 
249-263. 
MacWhinney B. 2004. A unified model of language 
acquisition. In J. Kroll & A. De Groot (eds.), 
Handbook of bilingualism: Psycholinguistic 
approaches, Oxford University Press, Oxford. 
Manning C. D. 2003. Probabilistic syntax. In R. Bod, 
J. Hay, S. Jannedy (eds), Probabilistic Linguistics,  
MIT Press, Cambridge MA: 289-341. 
Miyao Y., Tsujii J. 2002. Maximum entropy 
estimation for feature forests. Proc. HLT2002. 
Montemagni S. et al 2003. Building the Italian 
syntactic-semantic treebank. In Abeill? A. (ed.) 
Treebanks. Building and Using Parsed Corpora, 
Kluwer, Dordrecht: 189-210. 
Ratnaparkhi A. 1998. Maximum Entropy Models for 
Natural Language Ambiguity Resolution. Ph.D. 
Dissertation, University of Pennsylvania. 
   
Constraints for S  Constraints for O 
Feature Italian Czech  Feature Italian Czech 
Preverbal 1.31E+00 1.24E+00  Genitive na 1.51E+00 
Nomin na 1.23E+00  NoAgr 1.58E+00 1.50E+00 
Agr 1.28E+00 1.18E+00  Dative na 1.49E+00 
Anim 1.28E+00 1.16E+00  Accus na 1.39E+00 
Inanim 8.16E-01 1.03E+00  Instrum na 1.39E+00 
Postverbal 5.39E-01 8.77E-01  Postverbal 1.38E+00 1.17E+00 
Genitive na 2.94E-01  Inanim 1.23E+00 9.63E-01 
NoAgr 1.52E-01 7.71E-02  Agr 4.67E-01 6.67E-01 
Dative na 2.85E-02  Anim 3.17E-01 6.63E-01 
Accus na 8.06E-03  Preverbal 2.11E-02 5.40E-01 
Instrum na 3.80E-03  Nomin na 2.22E-02 
Table 5 ? Ranked constraints for S and O in Czech and Italian 
28
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 36?43,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
The PAIS
`
A Corpus of Italian Web Texts
Verena Lyding
?
verena.lyding@eurac.edu
Egon Stemle
?
egon.stemle@eurac.edu
Claudia Borghetti
?
claudia.borghetti@unibo.it
Marco Brunello
?
marcobrunello84@gmail.com
Sara Castagnoli
?
s.castagnoli@unibo.it
Felice Dell?Orletta
?
felice.dellorletta@ilc.cnr.it
Henrik Dittmann
?
henrik.dittmann@bordet.be
Alessandro Lenci
?
alessandro.lenci@ling.unipi.it
Vito Pirrelli
?
vito.pirrelli@ilc.cnr.it
Abstract
PAIS
`
A is a Creative Commons licensed,
large web corpus of contemporary Italian.
We describe the design, harvesting, and
processing steps involved in its creation.
1 Introduction
This paper provides an overview of the PAIS
`
A cor-
pus of Italian web texts and an introductory de-
scription of the motivation, procedures and facili-
ties for its creation and delivery.
Developed within the PAIS
`
A project, the cor-
pus is intended to meet the objective to help over-
come the technological barriers that still prevent
web users from making use of large quantities of
contemporary Italian texts for language and cul-
tural education, by creating a comprehensive and
easily accessible corpus resource of Italian.
The initial motivation of the initiative stemmed
from the awareness that any static repertoire of
digital data, however carefully designed and de-
veloped, is doomed to fast obsolescence, if con-
tents are not freely available for public usage, con-
tinuously updated and checked for quality, incre-
mentally augmented with new texts and annota-
tion metadata for intelligent indexing and brows-
ing. These requirements brought us to design a
resource that was (1) freely available and freely
re-publishable, (2) comprehensively covering con-
temporary common language and cultural content
and (3) enhanced with a rich set of automatically-
annotated linguistic information to enable ad-
vanced querying and retrieving of data. On top
?
EURAC Research Bolzano/Bozen, IT
?
University of Bologna, IT
?
University of Leeds, UK
?
Institute of Computational Linguistics ?Antonio Zam-
polli? - CNR, IT
?
Institut Jules Bordet, BE
?
University of Pisa, IT
of that, we set out to develop (4) a dedicated in-
terface with a low entry barrier for different target
groups. The end result of this original plan repre-
sents an unprecedented digital language resource
in the Italian scenario.
The main novelty of the PAIS
`
A web corpus is
that it exclusively draws on Creative Commons li-
censed data, provides advanced linguistic annota-
tions with respect to corpora of comparable size
and corpora of web data, and invests in a carefully
designed query interface, targeted at different user
groups. In particular, the integration of richly an-
notated language content with an easily accessible,
user-oriented interface makes PAIS
`
A a unique and
flexible resource for language teaching.
2 Related Work
The world wide web, with its inexhaustible
amount of natural language data, has become an
established source for efficiently building large
corpora (Kilgarriff and Grefenstette, 2003). Tools
are available that make it convenient to bootstrap
corpora from the web based on mere seed term
lists, such as the BootCaT toolkit (Baroni and
Bernardini, 2004). The huge corpora created by
the WaCky project (Baroni et al., 2009) are an ex-
ample of such an approach.
A large number of papers have recently been
published on the harvesting, cleaning and pro-
cessing of web corpora.
1
However, freely avail-
able, large, contemporary, linguistically anno-
tated, easily accessible web corpora are still miss-
ing for many languages; but cf. e.g. (G?en?ereux
et al., 2012) and the Common Crawl Foundations
(CCF) web crawl
2
.
1
cf. the Special Interest Group of the Association for
Computational Linguistics on Web as Corpus (SIGWAC)
http://sigwac.org.uk/
2
CCF produces and maintains a repository of web crawl
data that is openly accessible: http://commoncrawl.
org/
36
3 Corpus Composition
3.1 Corpus design
PAIS
`
A aimed at creating a comprehensive corpus
resource of Italian web texts which adheres to the
criteria laid out in section 1. For these criteria to
be fully met, we had to address a wide variety of
issues covering the entire life-cycle of a digital text
resource, ranging from robust algorithms for web
navigation and harvesting, to adaptive annotation
tools for advanced text indexing and querying and
user-friendly accessing and rendering online inter-
faces customisable for different target groups.
Initially, we targeted a size of 100M tokens, and
planned to automatically annotate the data with
lemma, part-of-speech, structural dependency, and
advanced linguistic information, using and adapt-
ing standard annotation tools (cf. section 4). In-
tegration into a querying environment and a dedi-
cated online interface were planned.
3.2 Licenses
A crucial point when planning to compile a cor-
pus that is free to redistribute without encounter-
ing legal copyright issues is to collect texts that are
in the public domain or at least, have been made
available in a copyleft regime. This is the case
when the author of a certain document decided to
share some rights (copy and/or distribute, adapt
etc.) on her work with the public, in a way that
end users do not need to ask permission to the cre-
ator/owner of the original work. This is possible
by employing licenses other than the traditional
?all right reserved? copyright, i.e. GNU, Creative
Commons etc., which found a wide use especially
on the web. Exploratory studies (Brunello, 2009)
have shown that Creative Commons licenses are
widely employed throughout the web (at least on
the Italian webspace), enough to consider the pos-
sibility to build a large corpus from the web ex-
clusively made of documents released under such
licenses.
In particular, Creative Commons provides five
basic ?baseline rights?: Attribution (BY), Share
Alike (SA), Non Commercial (NC), No Deriva-
tive Works (ND). The licenses themselves are
composed of at least Attribution (which can be
used even alone) plus the other elements, al-
lowing six different combinations:
3
(1) Attribu-
tion (CC BY), (2) Attribution-NonCommercial
3
For detailed descriptions of each license see http://
creativecommons.org/licenses/
(CC BY-NC), (3) Attribution-ShareAlike (CC BY-
SA), (4) Attribution-NoDerivs (CC BY-ND), (5)
Attribution-NonCommercial-ShareAlike (CC BY-
NC-SA), and (6) Attribution-NonCommercial-
NoDerivs (CC BY-NC-ND).
Some combinations are not possible because
certain elements are not compatible, e.g. Share
Alike and No Derivative Works. For our purposes
we decided to discard documents released with the
two licenses containing the No Derivative Works
option, because our corpus is in fact a derivative
work of collected documents.
3.3 The final corpus
The corpus contains approximately 388,000 docu-
ments from 1,067 different websites, for a total of
about 250M tokens. All documents contained in
the PAIS
`
A corpus date back to Sept./Oct. 2010.
The documents come from several web sources
which, at the time of corpus collection, provided
their content under Creative Commons license
(see section 3.2 for details). About 269,000 texts
are from Wikimedia Foundation projects, with
approximately 263,300 pages from Wikipedia,
2380 pages from Wikibooks, 1680 pages from
Wikinews, 740 pages from Wikiversity, 410 pages
from Wikisource, and 390 Wikivoyage pages.
The remaining 119,000 documents come
from guide.supereva.it (ca. 19,000),
italy.indymedia.org (ca. 10,000) and
several blog services from more than another
1,000 different sites (e.g. www.tvblog.it
(9,088 pages), www.motoblog.it (3,300),
www.ecowebnews.it (3,220), and
www.webmasterpoint.org (3,138).
Texts included in PAIS
`
A have an average length
of 683 words, with the longest text
4
counting
66,380 running tokens. A non exhaustive list of
average text lengths by source type is provided in
table 1 by way of illustration.
The corpus has been annotated for lemma, part-
of-speech and dependency information (see sec-
tion 4.2 for details). At the document level, the
corpus contains information on the URL of origin
and a set of descriptive statistics of the text, includ-
ing text length, rate of advanced vocabulary, read-
ability parameters, etc. (see section 4.3). Also,
each document is marked with a unique identifier.
4
The European Constitution from wikisource.org:
http://it.wikisource.org/wiki/Trattato_
che_adotta_una_Costituzione_per_l?Europa
37
Document source Avg text length
PAIS
`
A total 683 words
Wikipedia 693 words
Wikibooks 1844 words
guide.supereva.it 378 words
italy.indymedia.it 1147 words
tvblog.it 1472 words
motoblog.it 421 words
ecowebnews.it 347 words
webmasterpoint.org 332 words
Table 1: Average text length by source
The annotated corpus adheres to the stan-
dard CoNLL column-based format (Buchholz and
Marsi, 2006), is encoded in UTF-8.
4 Corpus Creation
4.1 Collecting and cleaning web data
The web pages for PAIS
`
A were selected in two
ways: part of the corpus collection was made
through CC-focused web crawling, and another
part through a targeted collection of documents
from specific websites.
4.1.1 Seed-term based harvesting
At the time of corpus collection (2010), we used
the BootCaT toolkit mainly because collecting
URLs could be based on the public Yahoo! search
API
5
, including the option to restrict search to CC-
licensed pages (including the possibility to specify
even the particular licenses). Unfortunately, Ya-
hoo! discontinued the free availability of this API,
and BootCaT?s remaining search engines do not
provide this feature.
An earlier version of the corpus was collected
using the tuple list originally employed to build
itWaC
6
. As we noticed that the use of this list, in
combination with the restriction to CC, biased the
final results (i.e. specific websites occurred very
often as top results) , we provided as input 50,000
medium frequent seed terms from a basic Italian
vocabulary list
7
, in order to get a wider distribu-
tion of search queries, and, ultimately, of texts.
As introduced in section 3.2, we restricted the
selection not just to Creative Commons-licensed
5
http://developer.yahoo.com/boss/
6
http://wacky.sslmit.unibo.it/doku.
php?id=seed_words_and_tuples
7
http://ppbm.paravia.it/dib_lemmario.
php
texts, but specifically to those licenses allowing
redistribution: namely, CC BY, CC BY-SA, CC
BY-NC-SA, and CC BY-NC.
Results were downloaded and automatically
cleaned with the KrdWrd system, an environment
for the unified processing of web content (Steger
and Stemle, 2009).
Wrongly CC-tagged pages were eliminated us-
ing a black-list that had been manually populated
following inspection of earlier corpus versions.
4.1.2 Targeted
In September 2009, the Wikimedia Foundation de-
cided to release the content of their wikis under
CC BY-SA
8
, so we decided to download the large
and varied amount of texts made available through
the Italian versions of these websites. This was
done using the Wikipedia Extractor
9
on official
dumps
10
of Wikipedia, Wikinews, Wikisource,
Wikibooks, Wikiversity and Wikivoyage.
4.2 Linguistic annotation and tools
adaptation
The corpus was automatically annotated with
lemma, part-of-speech and dependency infor-
mation, using state-of-the-art annotation tools
for Italian. Part-of-speech tagging was per-
formed with the Part-Of-Speech tagger described
in Dell?Orletta (2009) and dependency-parsed by
the DeSR parser (Attardi et al., 2009), using Mul-
tilayer Perceptron as the learning algorithm. The
systems used the ISST-TANL part-of-speech
11
and dependency tagsets
12
. In particular, the pos-
tagger achieves a performance of 96.34% and
DeSR, trained on the ISST-TANL treebank con-
sisting of articles from newspapers and period-
icals, achieves a performance of 83.38% and
87.71% in terms of LAS (labelled attachment
score) and UAS (unlabelled attachment score) re-
spectively, when tested on texts of the same type.
However, since Gildea (2001), it is widely ac-
knowledged that statistical NLP tools have a drop
of accuracy when tested against corpora differing
from the typology of texts on which they were
trained. This also holds true for PAIS
`
A: it contains
8
Previously under GNU Free Documentation License.
9
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
10
http://dumps.wikimedia.org/
11
http://www.italianlp.it/docs/
ISST-TANL-POStagset.pdf
12
http://www.italianlp.it/docs/
ISST-TANL-DEPtagset.pdf
38
lexical and syntactic structures of non-canonical
languages such as the language of social media,
blogs, forum posts, consumer reviews, etc. As re-
ported in Petrov and McDonald (2012), there are
multiple reasons why parsing the web texts is dif-
ficult: punctuation and capitalization are often in-
consistent, there is a lexical shift due to increased
use of slang and technical jargon, some syntactic
constructions are more frequent in web text than
in newswire, etc.
In order to overcome this problem, two main ty-
pologies of methods and techniques have been de-
veloped: Self-training (McClosky et al., 2006) and
Active Learning (Thompson et al., 1999).
For the specific purpose of the NLP tools adap-
tation to the Italian web texts, we adopted two dif-
ferent strategies for the pos-tagger and the parser.
For what concerns pos-tagging, we used an active
learning approach: given a subset of automatically
pos-tagged sentences of PAIS
`
A, we selected the
ones with the lowest likelihood, where the sen-
tence likelihood was computed as the product of
the probabilities of the assignments of the pos-
tagger for all the tokens. These sentences were
manually revised and added to the training corpus
in order to build a new pos-tagger model incor-
porating some new knowledge from the target do-
main.
For what concerns parsing, we used a self-
training approach to domain adaptation described
in Dell?Orletta et al. (2013), based on ULISSE
(Dell?Orletta et al., 2011). ULISSE is an unsu-
pervised linguistically-driven algorithm to select
reliable parses from a collection of dependency
annotated texts. It assigns to each dependency
tree a score quantifying its reliability based on a
wide range of linguistic features. After collect-
ing statistics about selected features from a cor-
pus of automatically parsed sentences, for each
newly parsed sentence ULISSE computes a reli-
ability score using the previously extracted feature
statistics. From the top of the parses (ranked ac-
cording to their reliability score) different pools of
parses were selected to be used for training. The
new training contains the original training set as
well as the new selected parses which include lex-
ical and syntactic characteristics specific of the tar-
get domain (Italian web texts). The parser trained
on this new training set improves its performance
when tested on the target domain.
We used this domain adaptation approach for
the following three main reasons: a) it is unsuper-
vised (i.e. no need for manually annotated training
data); b) unlike the Active Learning approach used
for pos-tagging, it does not need manual revision
of the automatically parsed samples to be used for
training; c) it was previously tested on Italian texts
with good results (Dell?Orletta et al., 2013).
4.3 Readability analysis of corpus documents
For each corpus document, we calculated several
text statistics indicative of the linguistic complex-
ity, or ?readability? of a text.
The applied measures include, (1) text length in
tokens, that is the number of tokens per text, (2)
sentences per text, that is a sentence count, and (3)
type-token ratio indicated as a percentage value.
In addition, we calculated (4) the advanced vo-
cabulary per text, that is a word count of the text
vocabulary which is not part of the the basic Ital-
ian vocabulary (?vocabolario di base?) for written
texts, as defined by De Mauro (1991)
13
, and (5)
the Gulpease Index (?Indice Gulpease?) (Lucisano
and Piemontese, 1988), which is a measure for the
readability of text that is based on frequency rela-
tions between the number of sentences, words and
letters of a text.
All values are encoded as metadata for the cor-
pus. Via the PAIS
`
A online interface, they can
be employed for filtering documents and building
subcorpora. This facility was implemented with
the principal target group of PAIS
`
A users in mind,
as the selection of language examples according
to their readability level is particularly relevant for
language learning and teaching.
4.4 Attempts at text classification for genre,
topic, and function
Lack of information about the composition of cor-
pora collected from the web using unsupervised
methods is probably one of the major limitations
of current web corpora vis-`a-vis more traditional,
carefully constructed corpora, most notably when
applications to language teaching and learning are
envisaged. This also holds true for PAIS
`
A, es-
13
The advanced vocabulary was calculated on the ba-
sis of a word list consisting of De Mauro?s ?vocabolario
fondamentale? (http://it.wikipedia.org/wiki/
Vocabolario_fondamentale) and ?vocabolario
di alto uso? (http://it.wikipedia.org/wiki/
Vocabolario_di_alto_uso), together with high
frequent function words not contained in those two lists.
39
pecially for the harvested
14
subcorpus that was
downloaded as described in section 4.1. We there-
fore carried out some experiments with the ulti-
mate aim to enrich the corpus with metadata about
text genre, topic and function, using automated
techniques.
In order to gain some insights into the com-
position of PAIS
`
A, we first conducted some man-
ual investigations. Drawing on existing literature
on web genres (e.g. (Santini, 2005; Rehm et al.,
2008; Santini et al., 2010)) and text classification
according to text function and topic (e.g. (Sharoff,
2006)), we developed a tentative three-fold taxon-
omy to be used for text classification. Following
four cycles of sample manual annotation by three
annotators, categories were adjusted in order to
better reflect the nature of PAIS
`
A?s web documents
(cf. (Sharoff, 2010) about differences between do-
mains covered in the BNC and in the web-derived
ukWaC). Details about the taxonomy are provided
in Borghetti et al. (2011). Then, we started to
cross-check whether the devised taxonomy was
indeed appropriate to describe PAIS
`
A?s composi-
tion by comparing its categories with data result-
ing from the application of unsupervised methods
for text classification.
Interesting insights have emerged so far re-
garding the topic category. Following Sharoff
(2010), we used topic modelling based on La-
tent Dirichlet Allocation for the detection of top-
ics: 20 clusters/topics were identified on the ba-
sis of keywords (the number of clusters to re-
trieve is a user-defined parameter) and projected
onto the manually defined taxonomy. This re-
vealed that most of the 20 automatically iden-
tified topics could be reasonably matched to
one of the 8 categories included in the tax-
onomy; exceptions were represented by clus-
ters characterised by proper nouns and gen-
eral language words such bambino/uomo/famiglia
(?child?/?man?/?family?) or credere/sentire/sperare
(?to believe?/?feel?/?hope?), which may in fact be
indicative of genres such as diary or personal com-
ment (e.g. personal blog). Only one of the cate-
gories originally included in the taxonomy ? natu-
ral sciences ? was not represented in the clusters,
which may indicate that there are few texts within
PAIS
`
A belonging to this domain. One of the ma-
14
In fact, even the nature of the targeted texts is not pre-
cisely defined: for instance, Wikipedia articles can actually
encompass a variety of text types such as biographies, intro-
ductions to academic theories etc. (Santini et al., 2010, p. 15)
jor advantages of topic models is that each corpus
document can be associated ? to varying degrees ?
to several topics/clusters: if encoded as metadata,
this information makes it possible not only to fil-
ter texts according to their prevailing domain, but
also to represent the heterogeneous nature of many
web documents.
5 Corpus Access and Usage
5.1 Corpus distribution
The PAIS
`
A corpus is distributed in two ways: it is
made available for download and it can be queried
via its online interface. For both cases, no restric-
tions on its usage apply other than those defined
by the Creative Commons BY-NC-SA license. For
corpus download, both the raw text version and the
annotated corpus in CoNLL format are provided.
The PAIS
`
A corpus together with all project-
related information is accessible via the project
web site at http://www.corpusitaliano.it
5.2 Corpus interface
The creation of a dedicated open online interface
for the PAIS
`
A corpus has been a declared primary
objective of the project.
The interface is aimed at providing a power-
ful, effective and easy-to-employ tool for mak-
ing full use of the resource, without having to go
through downloading, installation or registration
procedures. It is targeted at different user groups,
particularly language learners, teachers, and lin-
guists. As users of PAIS
`
A are expected to show
varying levels of proficiency in terms of language
competence, linguistic knowledge, and concern-
ing the use of online search tools, the interface
has been designed to provide four separate search
components, implementing different query modes.
Initially, the user is directed to a basic keyword
search that adopts a ?Google-style? search box.
Single search terms, as well as multi-word combi-
nations or sequences can be searched by inserting
them in a simple text box.
The second component is an advanced graph-
ical search form. It provides elaborated search
options for querying linguistic annotation layers
and allows for defining distances between search
terms as well as repetitions or optionally occurring
terms. Furthermore, the advanced search supports
regular expressions.
The third component emulates a command-line
search via the powerful CQP query language of
40
the Open Corpus Workbench (Evert and Hardie,
2011). It allows for complex search queries in
CQP syntax that rely on linguistic annotation lay-
ers as well as on metadata information.
Finally, a filter interface is presented in a fourth
component. It serves the purpose of retriev-
ing full-text corpus documents based on keyword
searches as well as text statistics (see section 4.3).
Like the CQP interface, the filter interface is also
supporting the building of temporary subcorpora
for subsequent querying.
By default, search results are displayed as
KWIC (KeyWord In Context) lines, centred
around the search expression. Each search hit can
be expanded to its full sentence view. In addition,
the originating full text document can be accessed
and its source URL is provided.
Based on an interactive visualisation for depen-
dency graphs (Culy et al., 2011) for each search
result a graphical representations of dependency
relations together with the sentence and associated
lemma and part-of-speech information can be gen-
erated (see Figure 1).
Figure 1: Dependency diagram
Targeted at novice language learners of Italian,
a filter for automatically restricting search results
to sentences of limited complexity has been in-
tegrated into each search component. When ac-
tivated, search results are automatically filtered
based on a combination of the complexity mea-
sures introduced in section 4.3.
5.3 Technical details
The PAIS
`
A online interface has been developed in
several layers: in essence, it provides a front-end
to the corpus as indexed in Open Corpus Work-
bench (Evert and Hardie, 2011). This corpus
query engine provides the fundamental search ca-
pabilities through the CQP language. Based on
the CWB/Perl API that is part of the Open Corpus
Workbench package, a web service has been de-
veloped at EURAC which exposes a large part of
the CQP language
15
through a RESTful API.
16
The four types of searches provided by the on-
line interface are developed on top of this web ser-
vice. The user queries are translated into CQP
queries and passed to the web service. In many
cases, such as the free word order queries in the
simple and advanced search forms, more than one
CQP query is necessary to produce the desired
result. Other functionalities implemented in this
layer are the management of subcorpora and the
filtering by complexity. The results returned by
the web service are then formatted and presented
to the user.
The user interface as well as the mechanisms
for translation of queries from the web forms into
CQP have been developed server-side in PHP.
The visualizations are implemented client-side in
JavaScript and jQuery, the dependency graphs
based on the xLDD framework (Culy et al., 2011).
5.4 Extraction of lexico-syntactic information
PAIS
`
A is currently used in the CombiNet project
?Word Combinations in Italian ? Theoretical and
descriptive analysis, computational models, lexi-
cographic layout and creation of a dictionary?.
17
The project goal is to study the combinatory prop-
erties of Italian words by developing advanced
computational linguistics methods for extracting
distributional information from PAIS
`
A.
In particular, CombiNet uses a pattern-based
approach to extract a wide range of multiword
expressions, such as phrasal lexemes, colloca-
tions, and usual combinations. POS n-grams
are automatically extracted from PAIS
`
A, and then
ranked according to different types of associa-
tion measures (e.g., pointwise mutual informa-
tion, log-likelihood ratios, etc.). Extending the
LexIt methodology (Lenci et al., 2012), CombiNet
also extracts distributional profiles from the parsed
layer of PAIS
`
A, including the following types of
information:
1. syntactic slots (subject, complements, modi-
15
To safeguard the system against malicious attacks, secu-
rity measures had to be taken at several of the layers, which
unfortunately also make some of the more advanced CQP fea-
tures inaccessible to the user.
16
Web services based on REST (Representational State
Transfer) principles employ standard concepts such as a URI
and standard HTTP methods to provide an interface to func-
tionalities on a remote host.
17
3-year PRIN(2010/2011)-project, coordination by Raf-
faele Simone ? University of Rome Tre
41
fiers, etc.) and subcategorization frames;
2. lexical sets filling syntactic slots (e.g. proto-
typical subjects of a target verb);
3. semantic classes describing selectional pref-
erences of syntactic slots (e.g. the direct obj.
of mangiare/?to eat? typically selects nouns
referring to food, while its subject selects an-
imate nouns); semantic roles of predicates.
The saliency and typicality of combinatory pat-
terns are weighted by means of different statisti-
cal indexes and the resulting profiles will be used
to define a distributional semantic classification of
Italian verbs, comparable to the one elaborated in
the VerbNet project (Kipper et al., 2008).
6 Evaluation
We performed post-crawl evaluations on the data.
For licensing, we analysed 200,534 pages that
were originally collected for the PAIS
`
A corpus,
and only 1,060 were identified as containing no
CC license link (99.95% with CC mark-up). Then,
from 10,000 randomly selected non-CC-licensed
Italian pages 15 were wrongly identified as CC li-
censed containing CC mark-up (0.15% error). For
language identification we checked the harvested
corpus part with the CLD2 toolkit
18
, and > 99%
of the data was identified as Italian.
The pos-tagger has been adapted to peculiari-
ties of the PAIS
`
A web texts, by manually correct-
ing sample annotation output and re-training the
tagger accordingly. Following the active learning
approach as described in section 4.2 we built a new
pos-tagger model based on 40.000 manually re-
vised tokens. With the new model, we obtained
an improvement in accuracy of 1% on a test-set
of 5000 tokens extracted from PAIS
`
A. Final tag-
ger accuracy reached 96.03%.
7 Conclusion / Future Work
In this paper we showed how a contemporary and
free language resource of Italian with linguistic
annotations can be designed, implemented and de-
veloped from the web and made available for dif-
ferent types of language users.
Future work will focus on enriching the cor-
pus with metadata by means of automatic clas-
sification techniques, so as to make a better as-
sessment of corpus composition. A multi-faceted
18
Compact Language Detection 2, http://code.
google.com/p/cld2/
approach combining linguistic features extracted
from texts (content/function words ratio, sentence
length, word frequency, etc.) and information
extracted from document URLs (e.g., tags like
?wiki?, ?blog?) might be particularly suitable for
genre and function annotation.
Metadata annotation will enable more advanced
applications of the corpus for language teaching
and learning purposes. In this respect, existing
exemplifications of the use of the PAIS
`
A inter-
face for language learning and teaching (Lyding et
al., 2013) could be followed by further pedagogi-
cal proposals as well as empowered by dedicated
teaching guidelines for the exploitation of the cor-
pus and its web interface in the class of Italian as
a second language.
In a more general perspective, we envisage
a tighter integration between acquisition of new
texts, automated text annotation and development
of lexical and language learning resources allow-
ing even non-specialised users to carve out and
develop their own language data. This ambitious
goal points in the direction of a fully-automatised
control of the entire life-cycle of open-access Ital-
ian language resources with a view to address an
increasingly wider range of potential demands.
Acknowledgements
The three years PAIS
`
A project
19
, concluded in
January 2013, received funding from the Italian
Ministry of Education, Universities and Research
(MIUR)
20
, by the FIRB program (Fondo per gli
Investimenti della Ricerca di Base)
21
.
References
G. Attardi, F. Dell?Orletta, M. Simi, and J. Turian.
2009. Accurate dependency parsing with a stacked
multilayer perceptron. In Proc. of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
M. Baroni and S. Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In Proc.
of LREC 2004, pages 1313?1316. ELDA.
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed
19
An effort of four Italian research units: University of
Bologna, CNR Pisa, University of Trento and European
Academy of Bolzano/Bozen.
20
http://www.istruzione.it/
21
http://hubmiur.pubblica.istruzione.
it/web/ricerca/firb
42
web-crawled corpora. Journal of LRE, 43(3):209?
226.
C. Borghetti, S. Castagnoli, and M. Brunello. 2011. I
testi del web: una proposta di classificazione sulla
base del corpus pais`a. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica.,
pages 147?170. Carocci, Roma.
M. Brunello. 2009. The creation of free linguistic cor-
pora from the web. In I. Alegria, I. Leturia, and
S. Sharoff, editors, Proc. of the Fifth Web as Corpus
Workshop (WAC5), pages 9?16. Elhuyar Fundazioa.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
Tenth Conf. Comput. Nat. Lang. Learn., number
June in CoNLL-X ?06, pages 149?164. Association
for Computational Linguistics.
C. Culy, V. Lyding, and H. Dittmann. 2011. xldd:
Extended linguistic dependency diagrams. In Proc.
of the 15th International Conference on Information
Visualisation IV2011, pages 164?169, London, UK.
T. De Mauro. 1991. Guida all?uso delle parole. Edi-
tori Riuniti, Roma.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2011.
Ulisse: an unsupervised algorithm for detecting re-
liable dependency parses. In Proc. of CoNLL 2011,
Conferences on Natural Language Learning, Port-
land, Oregon.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2013.
Unsupervised linguistically-driven reliable depen-
dency parses detection and self-training for adapta-
tion to the biomedical domain. In Proc. of BioNLP
2013, Workshop on Biomedical NLP, Sofia.
F. Dell?Orletta. 2009. Ensemble system for part-of-
speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
S. Evert and A. Hardie. 2011. Twenty-first century
corpus workbench: Updating a query architecture
for the new millennium. In Proc. of the Corpus Lin-
guistics 2011, Birmingham, UK.
M. G?en?ereux, I. Hendrickx, and A. Mendes. 2012.
A large portuguese corpus on-line: Cleaning and
preprocessing. In PROPOR, volume 7243 of Lec-
ture Notes in Computer Science, pages 113?120.
Springer.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333?347.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of english verbs.
Journal of LRE, 42:21?40.
A. Lenci, G. Lapesa, and G. Bonansinga. 2012. Lexit:
A computational resource on italian argument struc-
ture. In N. Calzolari, K. Choukri, T. Declerck,
M. U?gur Do?gan, B. Maegaard, J. Mariani, J. Odijk,
and S. Piperidis, editors, Proc. of LREC 2012, pages
3712?3718, Istanbul, Turkey, May. ELRA.
P. Lucisano and M. E. Piemontese. 1988. Gulpease:
una formula per la predizione della difficolt dei testi
in lingua italiana. Scuola e citt`a, 39(3):110?124.
V. Lyding, C. Borghetti, H. Dittmann, L. Nicolas, and
E. Stemle. 2013. Open corpus interface for italian
language learning. In Proc. of the ICT for Language
Learning Conference, 6th Edition, Florence, Italy.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL 2006, ACL, Sydney.
S. Petrov and R. McDonald. 2012. Overview of the
2012 shared task on parsing the web. In Proc. of
SANCL 2012, First Workshop on Syntactic Analysis
of Non-Canonical Language, Montreal.
G. Rehm, M. Santini, A. Mehler, P. Braslavski,
R. Gleim, A. Stubbe, S. Symonenko, M. Tavosanis,
and V. Vidulin. 2008. Towards a reference corpus of
web genres for the evaluation of genre identification
systems. In Proc. of LREC 2008, pages 351?358,
Marrakech, Morocco.
M. Santini, A. Mehler, and S. Sharoff. 2010. Riding
the Rough Waves of Genre on the Web. Concepts
and Research Questions. In A. Mehler, S. Sharoff,
and M. Santini, editors, Genres on the Web: Compu-
tational Models and Empirical Studies., pages 3?33.
Springer, Dordrecht.
M. Santini. 2005. Genres in formation? an ex-
ploratory study of web pages using cluster analysis.
In Proc. of the 8th Annual Colloquium for the UK
Special Interest Group for Computational Linguis-
tics (CLUK05), Manchester, UK.
S. Sharoff. 2006. Creating General-Purpose Corpora
Using Automated Search Engine Queries. In M. Ba-
roni and S. Bernardini, editors, Wacky! Working
Papers on the Web as Corpus, pages 63?98. Gedit,
Bologna.
S. Sharoff. 2010. Analysing similarities and differ-
ences between corpora. In 7th Language Technolo-
gies Conference, Ljubljana.
J. M. Steger and E. W. Stemle. 2009. KrdWrd ? The
Architecture for Unified Processing of Web Content.
In Proc. Fifth Web as Corpus Work., Donostia-San
Sebastian, Basque Country.
C. A. Thompson, M. E. Califf, and R. J. Mooney. 1999.
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML99, the Six-
teenth International Conference on Machine Learn-
ing, San Francisco, CA.
43
