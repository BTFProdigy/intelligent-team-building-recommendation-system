Cluster-Based Query Expansion for Statistical Question Answering
Lucian Vlad Lita ?
Siemens Medical Solutions
lucian.lita@siemens.com
Jaime Carbonell
Carnegie Mellon University
jgc@cs.cmu.edu
Abstract
Document retrieval is a critical component
of question answering (QA), yet little work
has been done towards statistical modeling
of queries and towards automatic generation
of high quality query content for QA. This
paper introduces a new, cluster-based query
expansion method that learns queries known
to be successful when applied to similar
questions. We show that cluster-based ex-
pansion improves the retrieval performance
of a statistical question answering system
when used in addition to existing query ex-
pansion methods. This paper presents exper-
iments with several feature selection meth-
ods used individually and in combination.
We show that documents retrieved using the
cluster-based approach are inherently differ-
ent than documents retrieved using existing
methods and provide a higher data diversity
to answers extractors.
1 Introduction
Information retrieval has received sporadic exam-
ination in the context of question answering (QA).
Over the past several years, research efforts have in-
vestigated retrieval quality in very controlled scenar-
ios under the question answering task. At a first
glance, document and passage retrieval is reason-
able when considering the fact that its performance
is often above 80% for this stage in the question
answering process. However, most often, perfor-
mance is measured in terms of the presence of at
? work done at Carnegie Mellon
least one relevant document in the retrieved docu-
ment set, regardless of relevant document density ?
where a document is relevant if it contains at least
one correct answer. More specifically, the retrieval
stage is considered successful even if there is a sin-
gle document retrieved that mentions a correct an-
swer, regardless of context. This performance mea-
sure is usually not realistic and revealing in question
answering.
In typical scenarios, information extraction is not
always able to identify correct answers in free text.
When successfully found, correct answers are not
always assigned sufficiently high confidence scores
to ensure their high ranks in the final answer set.
As a result, overall question answering scores are
still suffering and considerable effort is being di-
rected towards improving answer extraction and an-
swer merging, yet little attention is being directed
towards retrieval.
A closer look at retrieval in QA shows that the
types of documents retrieved are not always con-
ducive to correct answers given existing extraction
methods. It is not sufficient to retrieve a relevant
document if the answer is difficult to extract from its
context. Moreover, the retrieval techniques are often
very simple, consisting of extracting keywords from
questions, expanding them using conventional meth-
ods such as synonym expansion and inflectional ex-
pansion, and then running the queries through a re-
trieval engine.
In order to improve overall question answering
performance, additional documents and better doc-
uments need to be retrieved. More explicitly, infor-
mation retrieval needs to: a) generate query types
and query content that is designed to be successful
(high precision) for individual questions and b) en-
426
sure that the documents retrieved by the new queries
are different than the documents retrieved using con-
ventional methods. By improving retrieval along
these dimensions, we provide QA systems with ad-
ditional new documents, increasing the diversity and
the likelihood of extracting correct answers. In this
paper, we present a cluster-based method for ex-
panding queries with new content learned from the
process of answering similar questions. The new
queries are very different from existing content since
they are not based on the question being answered,
but on content learned from other questions.
1.1 Related Work
Experiments using the CMU Javelin (Collins-
Thompson et al, 2004) and Waterloo?s MultiText
(Clarke et al, 2002) question answering systems
corroborate the expected direct correlation between
improved document retrieval performance and QA
accuracy across systems. Effectiveness of the re-
trieval component was measured using question cov-
erage ? number of questions with at least one rele-
vant document retrieved ? and mean average preci-
sion. Results suggest that retrieval methods adapted
for question answering which include question anal-
ysis performed better than ad-hoc IR methods which
supports previous findings (Monz, 2003).
In question answering, queries are often ambigu-
ous since they are directly derived from the ques-
tion keywords. Such query ambiguity has been ad-
dressed in previous research (Raghavan and Allan,
2002) by extracting part of speech patterns and con-
structing clarification queries. Patterns are mapped
into manually generated clarification questions and
presented to the user. The results using the clarity
(Croft et al, 2001) statistical measure suggest that
query ambiguity is often reduced by using clarifica-
tion queries which produce a focused set of docu-
ments.
Another research direction that tailors the IR com-
ponent to question answering systems focuses on
query formulation and query expansion (Woods et
al., 2001). Taxonomic conceptual indexing system
based on morphological, syntactic, and semantic
features can be used to expand queries with inflected
forms, hypernyms, and semantically related terms.
In subsequent research (Bilotti et al, 2004), stem-
ming is compared to query expansion using inflec-
tional variants. On a particular question answering
controlled dataset, results show that expansion us-
ing inflectional variants produces higher recall than
stemming.
Recently (Riezler et al, 2007) used statistical ma-
chine translation for query expansion and took a step
towards bridging the lexical gap between questions
and answers. In (Terra et al, 2005) query expansion
is studied using lexical affinities with different query
formulation strategies for passage retrieval. When
evaluated on TREC datasets, the affinity replace-
ment method obtained significant improvements in
precision, but did not outperform other methods in
terms of recall.
2 Cluster-Based Retrieval for QA
In order to explore retrieval under question answer-
ing, we employ a statistical system (SQA) that
achieves good factoid performance on the TREC
QA task: for ? 50% of the questions a correct an-
swer is in the top highest confidence answer. Rather
than manually defining a complete answering strat-
egy ? the type of question, the queries to be run, the
answer extraction, and the answer merging meth-
ods ? for each type of question, SQA learns dif-
ferent strategies for different types of similar ques-
tions SQA takes advantage of similarity in training
data (questions and answers from past TREC evalua-
tions), and performs question clustering. Two meth-
ods are employed constraint-based clustering and
EM with similar performance. The features used
by SQA clustering are surface-form n-grams as well
as part of speech n-grams extracted from questions.
However, any clustering method can be employed in
conjunction with the methods presented in this pa-
per.
The questions in each cluster are similar in some
respect (i.e. surface form and syntax), SQA uses
them to learn a complete answering strategy. For
each cluster of training questions, SQA learns an an-
swering strategy. New questions may fall in more
than one cluster, so multiple answering strategies at-
tempt simultaneously to answer it.
In this paper we do not cover a particular ques-
tion answering system such as SQA and we do not
examine the whole QA process. We instead focus
on improving retrieval performance using a set of
427
similar questions. The methods presented here can
generalize when similar training questions are avail-
able. Since in our experiments we employ a cluster-
based QA system, we use individual clusters of simi-
lar questions as local training data for learning better
queries.
2.1 Expansion Using Individual Questions
Most existing question answering systems use IR in
a simple, straight-forward fashion: query terms are
extracted online from the test question and used to
construct basic queries. These queries are then ex-
panded from the original keyword set using statisti-
cal methods, semantic, and morphological process-
ing. Using these enhanced queries, documents (or
passages) are retrieved and the top K are further
processed. This approach describes the traditional
IR task and does not take advantage of specific con-
straints, requirements, and rich context available in
the QA process. Pseudo-relevance feedback is often
used in question answering in order to improve the
chances of retrieving relevant documents. In web-
based QA, often systems rely on retrieval engines
to perform the keyword expansion. Some question
answering systems associate additional predefined
structure or content based on the question classifi-
cation. However, there this query enhancement pro-
cess is static and does not use the training data and
the question answering context differently for indi-
vidual questions.
Typical question answering queries used in docu-
ment or passage retrieval are constructed using mor-
phological and semantic variations of the content
words in the question. However, these expanded
queries do not benefit from the underlying structure
of the question, nor do they benefit from available
training data, which provides similar questions that
we already know how to answer.
2.2 Expansion Based on Similar Questions
We introduce cluster-based query expansion
(CBQE), a new task-oriented method for query ex-
pansion that is complementary to existing strategies
and that leads to different documents which contain
correct answers. Our approach goes beyond single
question-based methods and takes advantage of
high-level correlations that appear in the retrieval
process for similar questions.
The central idea is to cluster available training
questions and their known correct answers in or-
der to exploit the commonalities in the retrieval pro-
cess. From each cluster of similar questions we
learn a different, shared query content that is used
in retrieving relevant documents - documents that
contain correct answers. This method leverages
the fact that answers to similar questions tend to
share contextual features that can be used to enhance
keyword-based queries. Experiments with question
answering data show that our expanded queries in-
clude a different type of content compared to and
in addition to existing methods. These queries have
training question clusters as a source for expansion
rather than an individual test question. We show that
CBQE is conducive to the retrieval of relevant doc-
uments, different than the documents that can be re-
trieved using existing methods.
We take advantage of the fact that for similar
training questions, good IR queries are likely to
share structure and content features. Such features
can be learned from training data and can then be
applied to new similar questions. Note that some of
these features cannot be generated through simple
query expansion, which does not takes advantage of
successful queries for training questions. Features
that generate the best performing queries across an
entire cluster are then included in a cluster-specific
feature set, which we will refer to as the query con-
tent model.
While pseudo-relevance feedback is performed
on-line for each test question, cluster-based rel-
evance feedback is performed across all training
questions in each individual cluster. Relevance feed-
back is possible for training data, since correct an-
swers are already known and therefore document
relevance can be automatically and accurately as-
sessed.
Algorithm 1 shows how to learn a query content
model for each individual cluster, in particular: how
to generate queries enhanced with cluster-specific
content, how to select the best performing queries,
and how to construct the query content model to be
used on-line.
Initially, simple keyword-based queries are for-
mulated using words and phrases extracted directly
from the free question keywords that do not appear
in the cluster definition. The keyword queries are
428
Algorithm 1 Cluster-based relevance feedback algorithm for
retrieval in question answering
1: extract keywords from training questions in a cluster and
build keyword-based queries; apply traditional query ex-
pansion methods
2: for all keyword-based query do
3: retrieve an initial set of documents
4: end for
5: classify documents into relevant and non-relevant
6: select top k most discriminative features (e.g. n-grams,
paraphrases) from retrieved documents (across all training
questions).
7: use the top k selected features to enhance keyword-based
queries ? adding one feature at a time (k new queries)
8: for all enhanced queries do
9: retrieve a second set of documents
10: end for
11: classify documents into relevant and non-relevant based
12: score enhanced queries according to relevant document
density
13: include in the query content model the top h features whose
corresponding enhanced queries performed best across all
training questions in the cluster ? up to 20 queries in our
implementation
then subjected to frequently used forms of query ex-
pansion such as inflectional variant expansion and
semantic expansion (table ??). Further process-
ing depends on the available and desired process-
ing tools and may generate variations of the origi-
nal queries: morphological analysis, part of speech
tagging, syntactic parsing. Synonym and hypernym
expansion and corpus-based techniques can be em-
ployed as part of the query expansion process, which
has been extensively studied (Bilotti et al, 2004).
The cluster-based query expansion has the advan-
tage of being orthogonal to traditional query expan-
sion and can be used in addition to pseudo-relevance
feedback. CBQE is based on context shared by sim-
ilar training questions in each cluster, rather than on
individual question keywords. Since cluster-based
expansion relies on different features compared to
traditional expansion, it leads to new relevant doc-
uments, different from the ones retrieved using the
existing expansion techniques.
3 The Query Content Model
Simple queries are run through a retrieval engine in
order to produce a set of potentially relevant docu-
ments. While this step may produce relevant doc-
uments, we would like to construct more focused
queries, likely to retrieve documents with correct an-
swers and appropriate contexts. The goal is to add
query content that increases retrieval performance
on training questions. Towards this end, we evaluate
the discriminative power of features (n-grams and
paraphrases), and select the ones positively corre-
lated with relevant documents and negatively corre-
lated with non-relevant documents. The goal of this
approach is to retrieve documents containing simple,
high precision answer extraction patterns. Features
Cluster: When did X start working for Y?
Simple Queries Query Content Model
X, Y ?X joined Y in?
X, Y, start, working ?X started working for Y?
X, Y, ?start working? ?X was hired by Y?
X, Y, working ?Y hired X?
. . . X, Y, ?job interview?
. . .
Table 1: Sample cluster-based expansion features
that best discriminate passages containing correct
answers from those that do not, are selected as
potential candidates for enhancing keyword-based
queries. For each question-answer pair, we gener-
ate enhanced queries by individually adding selected
features (e.g. Table 1) to simple queries. The result-
ing queries are subsequently run through a retrieval
engine and scored using the measure of choice (e.g.
average precision). The content features used to
construct the top h features and corresponding en-
hanced queries are included in the query content
model.
The query content model is a collection of fea-
tures used to enhance the content of queries which
are successful across a range of similar questions
(Table 1). The collection is cluster specific and not
question specific - i.e. features are derived from
training data and enhanced queries are scored us-
ing training question answer pairs. Building a query
content model does not preclude traditional query
expansion. Through the query content model we al-
low shared context to play a more significant role in
query generation.
4 Experiments With Cluster-Based
Retrieval
We tested the performance of cluster-based con-
tent enhanced queries and compared it to the per-
429
formance of simple keyword-based queries and to
the performance of queries expanded through syn-
onyms and inflectional variants. We also experiment
with several feature selection methods for identify-
ing content features conducive to successful queries.
These experiments were performed with a web-
based QA system which uses the Google API for
document retrieval and a constraint-based approach
for question clustering. Using this system we
retrieved ?300, 000 and built a document set of
?10GB. For each new question, we identify train-
ing questions that share a minimum surface struc-
ture (e.g. a size 3 skip-ngram in common) which
we consider to be the prototype of a loose cluster.
Each cluster represents a different, implicit notion of
question similarity based on the set of training ques-
tions it covers. Therefore different clusters lead to
different retrieval strategies. These retrieval experi-
ments are restricted to using only clusters of size 4 or
higher to ensure sufficient training data for learning
queries from individual clusters. All experiments
were performed using leave-one-out cross valida-
tion.
For evaluating the entire statistical question an-
swering system, we used all questions from TREC8-
12. One of the well-known problems in QA consists
of questions having several unknown correct an-
swers with multiple answer forms ? different ways
of expressing the same answer. Since we are lim-
ited to a set of answer keys, we avoid the this prob-
lem by using all temporal questions from this dataset
for evaluating individual stages in the QA process
(i.e. retrieval) and for comparing different expan-
sion methods. These questions have the advantage
of having a more restrictive set of possible answer
surface forms, which lead to a more accurate mea-
sure of retrieval performance. At the same time they
cover both more difficult questions such as ?When
was General Manuel Noriega ousted as the leader
of Panama and turned over to U.S. authorities??
as well as simpler questions such as ?What year
did Montana become a state??. We employed this
dataset for an in-depth analysis of retrieval perfor-
mance.
We generated four sets of queries and we tested
their performance. We are interested in observ-
ing to what extent different methods produce addi-
tional relevant documents. The initial set of queries
are constructed by simply using a bag-of-words ap-
proach on the question keywords. These queries
are run through the retrieval engine, each generating
100 documents. The second set of queries builds on
the first set, expanding them using synonyms. Each
word and potential phrase is expanded using syn-
onyms extracted from WordNet synsets. For each
enhanced query generated, 100 documents are re-
trieved. To construct the third set of queries, we ex-
pand the queries in the first two sets using inflec-
tional variants of all the content words (e.g. verb
conjugations and noun pluralization (Bilotti et al,
2004)). For each of these queries we also retrieve
100 documents.
When text corpora are indexed without using
stemming, simple queries are expanded to include
morphological variations of keywords to improve re-
trieval and extraction performance. Inflectional vari-
ants include different pluralizations for nouns (e.g.
report, reports) and different conjugations for verbs
(e.g. imagine, imagines, imagined, imagining). Un-
der local corpus retrieval inflectional expansion by-
passes the unrelated term conflation problem that
stemmers tend to have, but at the same time, recall
might be lowered if not all related words with the
same root are considered. For a web-based question
answering system, the type of retrieval depends on
the search-engine assumptions, permissible query
structure, query size limitation, and search engine
bandwidth (allowable volume of queries per time).
By using inflectional expansion with queries that tar-
get web search engines, the redundancy for support-
ing different word variants is higher, and has the
potential to increase answer extraction performance.
Finally, in addition to the previous expansion meth-
ods, we employ our cluster-based query expansion
method. These queries incorporate the top most
discriminative ngrams and paraphrases (section 4.1)
learned from the training questions covered by the
same cluster. Instead of further building an expan-
sion using the original question keywords, we ex-
pand using contextual features that co-occur with
answers in free text. For all the training ques-
tions in a cluster, we gather statistics about the co-
occurrence of answers and potentially beneficial fea-
tures. These statistics are then used to select the best
features and apply them to new questions whose an-
swers are unknown. Figure 1 shows that approx-
430
Figure 1: Cumulative effect of expansion methods
imately 90% of the questions consistently benefit
from cluster-based query expansion when compared
to approximately 75% of the questions when em-
ploying the other methods combined. Each question
can be found in multiple clusters of different reso-
lution. Since different clusters may lead to differ-
ent selected features, questions benefit from multi-
ple strategies and even though one cluster-specific
strategy cannot produce relevant documents, other
cluster-specific strategies may be able to.
The cluster-based expansion method can generate
a large number of contextual features. When com-
paring feature selection methods, we only select the
top 10 features from each method and use them to
enhance existing question-based queries. Further-
more, in order to retrieve, process, extract, and score
a manageable number of documents, we limited the
retrieval to 10 documents for each query. In Fig-
ure 1 we observe that even as the other methods
retrieve more documents, ? 90% of the questions
still benefit from the cluster-based method. In other
words, the cluster-based method generates queries
using a different type of content and in turn, these
queries retrieve a different set documents than the
other methods. This observation is true even if we
continue to retrieve up to 100 documents for sim-
ple queries, synonym-expanded queries, and inflec-
tional variants-expanded queries.
This result is very encouraging since it suggests
that the answer extraction components of ques-
tion answering systems are exposed to a different
type of relevant documents, previously inaccessible
to them. Through these new relevant documents,
cluster-based query expansion has the potential to
provide answer extraction with richer and more var-
ied sources of correct answers for 90% of the ques-
tions.
new relevant documents
simple 4.43 100%
synonyms 1.48 33.4%
inflect 2.37 53.43%
cluster 1.05 23.65%
all 9.33 210.45%
all - synonyms 7.88 177.69%
all - inflect 6.99 157.69%
all - cluster 8.28 186.80%
Table 2: Keyword-based (?simple?), synonym, inflectional
variant, and cluster-based expansion. Average number of new
relevant documents across instances at 20 documents retrieved.
Although expansion methods generate additional
relevant documents that simpler methods cannot ob-
tain, an important metric to consider is the den-
sity of these new relevant documents. We are in-
terested in the number/percentage of new relevant
documents that expansion methods contribute with.
Table 2 shows at retrieval level of twenty docu-
ments how different query generation methods per-
form. We consider keyword based methods to be the
baseline and add synonym expanded queries (?syn-
onym?), inflectional variants expanded queries (?in-
flect?) which build upon the previous two types of
queries, and finally the cluster enhanced queries
(?cluster?) which contain features learned from train-
ing data. We see that inflectional variants have
the most impact on the number of new documents
added, although synonym expansion and cluster-
based expansion also contribute significantly.
4.1 Feature Selection for CBQE
Content features are learned from the training data
based on observing their co-occurrences with cor-
rect answers. In order to find the most appropri-
ate content features to enhance our cluster-specific
queries, we have experimented with several feature
selection methods (Yang and Pederson, 1997): in-
formation gain, chi-square, and scaled chi-square
(phi). Information gain (IG) measures the reduction
in entropy for the pre presence/absence of an answer
in relevant passages, given an n-gram feature. Chi-
square (?2) is a non-parametric measure of associa-
431
tion that quantifies the passage-level association be-
tween n-gram features and correct answers.
Given any of the above methods, individual n-
gram scores are combined at the cluster level by av-
eraging over individual questions in the cluster. In
figure 2 we compare these feature selection meth-
ods on our dataset. The selected features are used to
enhance queries and retrieve additional documents.
We measure the fraction of question instances for
which enhanced queries obtain at least one new rel-
evant document. The comparison is made with the
document set generated by keyword-based queries,
synonym expansion, and inflectional variant expan-
sion. We also include in our comparison the com-
bination of all feature selection methods (?All?). In
0 20 40 60 80 100
0.5
0.55
0.6
0.65
0.7
0.75
Instances With Additional Relevant Documents
#docs retrieved
fra
ct
io
n 
of
 in
st
an
ce
s
 
 
All
Prec
IGain
Phi
Chi2
Figure 2: Selection methods for cluster-based expansion
this experiment, average precision on training data
proves to be the best predictor of additional relevant
documents: ?71% of the test questions benefit from
queries based on average precision feature selection.
However, the other feature selection methods also
obtain a high performance, benefiting ?68% of the
test question instances.
Since these feature selection methods have differ-
ent biases, we expect to observe a boost in perfor-
mance (73%) from merging their feature sets (Fig-
ure 2). In this case there is a trade-off between
a 2% boost in performance and an almost double
set of features and enhanced queries. This trans-
lates into more queries and more documents to be
processed. Although it is not the focus of this re-
search, we note that a clever implementation could
incrementally add features from the next best selec-
tion method only after the existing queries and doc-
uments have been processed. This approach lends
itself to be a good basis for utility-based models
and planning (Hiyakumoto et al, 2005). We in-
0.3 0.4 0.5 0.6 0.7
0.4
0.5
0.6
0.7
Cluster Enhanced Queries
feature selection score (train)
a
ve
ra
ge
 p
re
cis
io
n 
(re
trie
va
l)
 
 
Precision at   1
Precision at   5
Precision at 10
Figure 3: Average precision of cluster enhanced queries
vestigate to what extent the scores of the selected
features are meaningful and correlate with actual re-
trieval performance on test data by measuring the
average precision of these queries at different num-
ber of documents retrieved. Figure 3 shows preci-
sion at one, five, and ten documents retrieved. We
observe that feature scores correlate well with ac-
tual retrieval performance, a result confirmed by all
three retrieval levels, suggesting that useful features
learned. The average precision also increases with
more documents retrieved, which is a desirable qual-
ity in question answering.
4.2 Qualitative Results
The cluster-based relevance feedback process can be
used to discover several artifacts useful in question
answering. For several of the clusters, we observe
that the feature selection process consistently and
with high confidence selected features such as ?noun
NP1 has one meaning? where NP1 is the first noun
phrase in the question. The goal is to add such fea-
tures to the keyword-based queries to retrieve high
precision documents. Note that our example, NP1
would be different for different test questions.
The indirect reason for selecting such features is
in fact the discovery of authorities: websites that fol-
low a particular format and which have a particular
type of information, relevant to a cluster. In the ex-
ample above, the websites answers.com and word-
net.princeton.edu consistently included answers to
clusters relevant to a person?s biography. Simi-
larly, wikipedia.org often provides answers to def-
initional questions (e.g. ?what is uzo??). By includ-
432
ing non-intuitive phrases, the expansion ensures that
the query will retrieve documents from a particular
authoritative source ? during feature selection, these
authorities supplied high precision documents for all
training questions in a particular cluster, hence fea-
tures specific to these sources were identified.
Q: When did Bob Marley die? [A: answers.com]
The noun Bob Marley has one meaning:
Jamaican singer who popularized reggae (1945-81)
Born: 6 February 1945
Birthplace: St. Ann?s Parish, Jamaica
Died: 11 May 1981 (cancer)
Songs: Get Up, Stand Up, Redemption Song . . .
In this example, profiles for many entities men-
tioned in a question cluster were found on several
authority websites. Due to unlikely expansions such
as ?noun Bob Marley has one meaning? the entity
?Bob Marley?, the answer to the question ?When
did Bob Marley die?? can easily be found. In fact,
this observation has the potential to lead to a cluster-
based authority discovery method, in which certain
sources are given more credibility and are used more
frequently than others. For example, by observing
that for most questions in a cluster, the wikipedia site
covers at least one correct answer (ideally that can
actually be extracted), then it should be considered
(accessed) for test questions before other sources of
documents. Through this process, given a set of
questions processed using the IBQA approach, a set
of authority answer sources can be identified.
5 Conclusions & Future Work
We presented a new, cluster-based query expansion
method that learns query content which is success-
fully used in answering other similar questions. Tra-
ditional QA query expansion is based only on the
individual keywords in a question. In contrast, the
cluster-based expansion learns features from context
shared by similar training questions from a cluster.
Since the features of cluster-based expansion are
different from the features used in traditional query
expansion, they lead to new relevant documents that
are different from documents retrieved using exist-
ing expansion techniques. Our experiments show
that more than 90% of the questions benefit from
our cluster-based method when used in addition to
traditional expansion methods.
Retrieval in local corpora offers more flexibility
in terms of query structure and expressivity. The
cluster-based method can be extended to take advan-
tage of structure in addition to content. More specif-
ically, different query structures could benefit differ-
ent types of questions. However, learning structure
might require more training questions for each clus-
ter. Further research can also be done to improve
the methods of combining learned content into more
robust and generalizable queries. Finally we are in-
terested modifying our cluster-based expansion for
the purpose of automatically identifying authority
sources for different types of questions.
References
M. W. Bilotti, B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or morpho-
logical query expansion? In IR4QA, SIGIR Workshop.
C. Clarke, G. Cormack, G. Kemkes, M. Laszlo, T. Ly-
nam, E. Terra, and P. Tilker. 2002. Statistical selection
of exact answers.
K. Collins-Thompson, E. Terra, J. Callan, and C. Clarke.
2004. The effect of document retrieval quality on fac-
toid question-answering performance.
W.B. Croft, S. Cronen-Townsend, and V. Lavrenko.
2001. Relevance feedback and personalization: A lan-
guage modeling perspective. In DELOS-NSF Work-
shop on Personalization and Recommender Systems in
Digital Libraries.
L. Hiyakumoto, L.V. Lita, and E. Nyberg. 2005. Multi-
strategy information extraction for question answer-
ing.
C. Monz. 2003. From document retrieval to question
answering. In Ph. D. Dissertation, Universiteit Van
Amsterdam.
H. Raghavan and J. Allan. 2002. Using part-of-speech
patterns to reduce query ambiguity.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In ACL.
E. Terra, C.L., and A. Clarke. 2005. Comparing query
formulation and lexical affinity replacements in pas-
sage retrieval. In ELECTRA, SIGIR Workshop.
W.A. Woods, S.J. Green, P. Martin, and A. Houston.
2001. Aggressive morphology and lexical relations for
query expansion.
Y. Yang and J. Pederson. 1997. Feature selection in sta-
tistical learning of text categorizatio n.
433
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?44,
New York, June 2006. c?2006 Association for Computational Linguistics
Spectral Clustering for Example Based Machine Translation
Rashmi Gangadharaiah
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
rgangadh@andrew.cmu.edu
Ralf Brown
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
ralf@cs.cmu.edu
Jaime Carbonell
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
jgc@cs.cmu.edu
Abstract
Prior work has shown that generaliza-
tion of data in an Example Based Ma-
chine Translation (EBMT) system, re-
duces the amount of pre-translated text re-
quired to achieve a certain level of accu-
racy (Brown, 2000). Several word clus-
tering algorithms have been suggested to
perform these generalizations, such as k-
Means clustering or Group Average Clus-
tering. The hypothesis is that better con-
textual clustering can lead to better trans-
lation accuracy with limited training data.
In this paper, we use a form of spectral
clustering to cluster words, and this is
shown to result in as much as 29.08% im-
provement over the baseline EBMT sys-
tem.
1 Introduction
In EBMT, the source sentence to be translated
is matched against the source language sentences
present in a corpus of source-target sentence pairs.
When a partial match is found, the corresponding
target translations are obtained through subsenten-
tial alignment. These partial matches are put to-
gether to obtain the final translation by optimizing
translation and alignment scores and using a statisti-
cal target language model in the decoding process.
Prior work has shown that EBMT requires large
amounts of data (in the order of two to three mil-
lion words) (Brown, 2000) of pre-translated text, to
function reasonably well. Thus, some modification
of the basic EBMT method is required to make it ef-
fective when less data is available. In order to use
the available text efficiently, systems such as, (Veale
and Way, 1997) and (Brown, 1999), convert the ex-
amples in the corpus into templates against which
the new text can be matched. Thus, source-target
sentence pairs are converted to source-target gener-
alized template pairs. An example of such a pair is
shown below:
The session opened at 2p.m
La se?ance est ouverte a? 2 heures
The <event> <verb-past-tense> at <time>
La <event> <verb-past-tense> a <time>
This single template can be used to translate differ-
ent source sentences, including for example,
The session adjourned at 6p.m
The seminar opened at 8a.m
if ?session? and ?seminar? are both generalized to
?<event>?, ?opened? and ?adjourned? are both gen-
eralized to ?<verb-past-tense>? and finally ?6p.m?
and ?8a.m? are both generalized to ?<time>?.
The system used by (Brown, 1999) performs
its generalization using both equivalence classes of
words and a production rule grammar. This paper
describes the use of spectral clustering (Ng. et. al.,
2001; Zelnik-Manor and Perona, 2004), for auto-
mated extraction of equivalence classes. Spectral
clustering is seen to be superior to Group Average
Clustering (GAC) (Brown, 2000) both in terms of
semantic similarity of words falling in a single clus-
ter, and overall BLEU score (Papineni. et. al., 2002)
in a large scale EBMT system.
The next section explains the term vectors ex-
tracted for each word, which are then used to cluster
words into equivalence classes and provides an out-
line of the Standard GAC algorithm. Section 3 de-
scribes the spectral clustering algorithm used. Sec-
41
tion 4 lists results obtained in a full evaluation of the
algorithm. Section 5 concludes and discusses direc-
tions for future work.
2 Term vectors for clustering
Using a bilingual dictionary, usually created using
statistical methods such as those of (Brown et. al.,
1990) or (Brown, 1997), and the parallel text, a
rough mapping between source and target words can
be created. This word pair is then treated as an in-
divisible token for future processing. For each such
word pair we then accumulate counts for each to-
ken in the surrounding context of its occurrences
(N words, currently 3, immediately prior to and
N words immediately following). The counts are
weighted with respect to distance from occurrence,
with a linear decay (from 1 to 1/N) to give great-
est importance to the words immediately adjacent to
the word pair being examined. These counts form a
pseudo-document for each pair, which are then con-
verted into term vectors for clustering.
In this paper, we compare our algorithm against
the incremental GAC algorithm(Brown, 2000). This
method examines each word pair in turn, comput-
ing a similarity measure to every existing cluster.
If the best similarity measure is above a predeter-
mined threshold, the new word is placed in the cor-
responding cluster, otherwise a new cluster is cre-
ated if the maximum number of clusters has not yet
been reached.
3 Spectral clustering
Spectral clustering is a general term used to de-
scribe a group of algorithms that cluster points using
the eigenvalues of ?distance matrices? obtained from
data. In our case, the algorithm described in (Ng.
et. al., 2001) was performed with certain variations
that were proposed by (Zelnik-Manor and Perona,
2004) to compute the scaling factors automatically
and for the k-Means orthogonal treatment (Verma
and Meila, 2003) during the initialization. These
scaling factors help in self-tuning distances between
points according to the local statistics of the neigh-
borhoods of the points. The algorithm is briefly de-
scribed below.
1. Let S =s1, s2, ....sn, denote the term vectors to
be clustered into k classes.
2. Form the affinity matrix A defined by
Aij = exp(?d2(si, sj)/?i?j) for i 6= j
Aii = 1
Where, d(si, sj) = 1/(sim(si, sj) + )
sim(si, sj) is the Cosine similarity between si
and sj ,  is used to prevent the ratio from be-
coming infinity
?i is the set of local scaling parameters for si.
?i = d(si, sT ) where, sT is the T th neighbor of
point si for some fixed T (7 for this paper).
3. Define D to be the diagonal matrix given by,
Dii = ?jAij
4. Compute L = D?1/2AD?1/2
5. Select k eigenvectors corresponding to k
largest eigenvalues (k is presently an externally
set parameter). The eigenvectors are normal-
ized to have unit length. Form matrix U by
stacking all the eigenvectors in columns.
6. Form the matrix Y by normalizing U?s rows,
Yij = Uij/
?
(?jU2ij)
7. Perform k-Means clustering treating each row
of Y as a point in k dimensions. The k-Means
algorithm is initialized either with random cen-
ters or with orthogonal vectors.
8. After clustering, assign the point si to cluster c
if the corresponding row i of the matrix Y was
assigned to cluster c.
9. Sum the distances between the members and
the centroid of each cluster to obtain the classi-
fication cost.
10. Goto step 7, iterate for a fixed number of it-
erations. In this paper, 20 iterations were per-
formed with orthogonal k-Means initialization
and 5 iterations with random k-Means initial-
ization.
11. The clusters obtained from the iteration with
least classification cost are selected as the k
clusters.
4 Preliminary Results
The clusters obtained from the spectral clustering
method are seen by inspection to correspond to more
natural and intuitive word classes than those ob-
tained by GAC. Even though this is subjective and
not guaranteed to lead to improve translation perfor-
mance, it shows that maybe the increased power of
spectral clustering to represent non-convex classes
42
(non-convex in the term vector domain) could be
useful in a real translation experiment. Some ex-
ample classes are shown in Table 1. The first
class in an intuitive sense corresponds to measure-
ment units. We see that in the <units> case,
GAC misses some of the members which are ac-
tually distributed among many different classes and
hence these are not well generalized. In the second
class <months>, spectral clustering has primarily
the months in a single class whereas GAC adds a
number of seemingly unrelated words to the clus-
ter. The classes were all obtained by finding 80
clusters in a 20,000-sentence pair subset of the IBM
Hansard Corpus (Linguistic Data Consortium, 1997)
for spectral clustering. 80 was chosen as the number
of clusters since it gave the highest BLEU score in
the evaluation. For GAC, 300 clusters were used as
this gave the best performance.
To show the effectiveness of the clustering meth-
ods in an actual evaluation, we set up the following
experiment for an English to French translation task
on the Hansard corpus. The training data consists of
three sets of size 10,000 (set1), 20,000 (set2) and
30,000 (set3) sentence pairs chosen from the first
six files of the Hansard Corpus. Only sentences of
length 5 to 21 words were taken. Only words with
frequency of occurrence greater than 9 were chosen
for clustering because more contextual information
would be available when the word occurs frequently
and this would help in obtaining better clusters. The
test data was chosen to be a set of 500 sentences ob-
tained from files 20, 40, 60 and 80 of the Hansard
corpus with 125 sentences from each file. Each of
the methods was run with different number of clus-
ters and results are reported only for the optimal
number of clusters in each case.
The results in Table 2 show that spectral clus-
tering requires moderate amounts of data to get a
large improvement. For small amounts of data it is
slightly worse than GAC, but neither gives much im-
provement over the baseline. For larger amounts of
data, again both methods are very similar, though
spectral clustering is better. Finally, for moderate
amounts of data, when generalization is the most
useful, spectral clustering gives a significant im-
provement over the baseline as well as over GAC.
By looking at the clusters obtained with varying
amounts of data, it can be concluded that high pu-
Table 1: Clusters for <units> and <months>
Spectral clustering GAC
?adjourned? ?hre? ?adjourned? ?hre?
?cent? ?%?
?days? ?jours?
?families? ?familles? ?families? ?familles?
?hours? ?heures?
?million? ?millions? ?million? ?millions?
?minutes? ?minutes?
?o?clock? ?heures? ?o?clock? ?heures?
?p.m.? ?heures? ?p.m.? ?heures?
?p.m.? ?hre?
?people? ?personnes? ?people? ?personnes?
?per? ?%? ?per? ?%?
?times? ?fois? ?times? ?fois?
?years? ?ans?
?august? ?aou?t? ?august? ?aou?t?
?december? ?de?cembre? ?december? ?de?cembre?
?february? ?fe?vrier? ?february? ?fe?vrier?
?january? ?janvier? ?january? ?janvier?
?march? ?mars? ?march? ?mars?
?may? ?mai? ?may? ?mai?
?november? ?novembre? ?november? ?novembre?
?october? ?octobre? ?october? ?octobre?
?only? ?seulement? ?only? ?seulement?
?june? ?juin? ?june? ?juin?
?july? ?juillet? ?july? ?juillet?
?april? ?avril? ?april? ?avril?
?september? ?septembre? ?september? ?septembre?
?page? ?page?
?per? ?$?
?recognize? ?parole?
?recognized? ?parole?
?recorded? ?page?
?section? ?article?
?since? ?depuis? ?since? ?depuis?
?took? ?se?ance?
?under? ?loi?
43
Table 2: % Relative improvement over baseline EBMT
# clus is the number of clusters for best performance
GAC Spectral
% Rel imp #clus % Rel imp #clus
10k 3.33 50 1.37 20
20k 22.47 300 29.08 80
30k 2.88 300 3.88 200
rity clusters can be obtained with even just moderate
amounts of data.
5 Conclusions and future work
From the experimental results we see that spectral
clustering leads to relatively purer and more intu-
itive clusters. These clusters result in an improved
BLEU score in comparison with the clusters ob-
tained through GAC. GAC can only collect clusters
in convex regions in the term vector space, while
spectral clustering is not limited in this regard. The
ability of spectral clustering to represent non-convex
shapes arises due to the projection onto the eigen-
vectors as described in (Ng. et. al., 2001).
As future work, we would like to analyze the
variation in performance as the amount of data in-
creases. It is widely known that increasing the
amount of training data in a generalized EBMT sys-
tem eventually leads to saturation of performance,
where all clustering methods perform about as well
as baseline. Thus, all methods have an operating re-
gion where they are the most useful. We would like
to locate and extend this region for spectral cluster-
ing.
Also, it would be interesting to compare the clus-
ters obtained with spectral clustering and the Part of
Speech tags of the words in the same cluster, espe-
cially for languages such as English where good tag-
gers are available.
Finally, an important direction of research is in
automatically selecting the number of clusters for
the clustering algorithm. To do this, we could use
information from the eigenvalues or the distribution
of points in the clusters.
Acknowledgment
This work was funded by National Business Center
award NBCHC050082.
References
Andrew Ng, Michael Jordan, and Yair Weiss 2001. On
Spectral Clustering: Analysis and an algorithm. In Ad-
vances in Neural Information Processing Systems 14:
Proceeding of the 2001 Conference, pages 849-856,
Vancouver, British Columbia, Canada, December.
Deepak Verma and Marina Meila. 2003. Comparison of
Spectral Clustering Algorithms. http://www.ms.
washington.edu/?spectral/.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311-
318,Philadelphia, PA, July. http://acl.ldc.
upenn.edu/P/P02
Linguistic Data Consortium. 1997. Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.
edu/
L. Zelnik-Manor and P. Perona 2004 Self-Tuning Spec-
tral Clustering. In Advances in Neural Information
Processing Systems 17: Proceeding of the 2004 Con-
ference.
Peter Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F.
Jelinek, J. Lafferty, R. Mercer and P. Roossin. 1990.
A Statistical Approach to Machine Translation. Com-
putational Linguistics, 16:79-85.
Ralf D. Brown. 1997. Automated Dictionary Extrac-
tion for ?Knowledge-Free? Example-Based Transla-
tion. In Proceedings of the Seventh International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI-97), pages 111-118, Santa
Fe, New Mexico, July. http://www.cs.cmu.
edu/?ralf/papers.html
Ralf D. Brown. 1999. Adding Linguistic Knowledge
to a Lexical Example-Based Translation System. In
Proceedings of the Eighth International Conference
on Theoretical and Methodological Issues in Machine
Translation(TMI-99), pages 22-32, August. http:
//www.cs.cmu.edu/?ralf/papers.html
Ralf. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of Eighteenth
International Conference on Computational Linguis-
tics (COLING-2000), pages 125-131, Saarbru?cken,
Germany.
Tony Veale and Andy Way. 1997. Gaijin: A Template-
Driven Bootstrapping Approach to Example-Based
Machine Translation. In Proceedings of NeMNLP97,
New Methods in Natural Language Processing, Sofia,
Bulgaria, September. http://www.compapp.
dcu.ie/?tonyv/papers/gaijin.html.
44
Proceedings of NAACL HLT 2007, pages 324?331,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Probability-Based Rankers for Action-Item Detection
Paul N. Bennett
Microsoft Research?
One Microsoft Way
Redmond, WA 98052
paul.n.bennett@microsoft.com
Jaime G. Carbonell
Language Technologies Institute, Carnegie Mellon
5000 Forbes Ave
Pittsburgh, PA 15213
jgc@cs.cmu.edu
Abstract
This paper studies methods that automat-
ically detect action-items in e-mail, an
important category for assisting users in
identifying new tasks, tracking ongoing
ones, and searching for completed ones.
Since action-items consist of a short span
of text, classifiers that detect action-items
can be built from a document-level or a
sentence-level view. Rather than com-
mit to either view, we adapt a context-
sensitive metaclassification framework to
this problem to combine the rankings pro-
duced by different algorithms as well as
different views. While this framework is
known to work well for standard classi-
fication, its suitability for fusing rankers
has not been studied. In an empirical eval-
uation, the resulting approach yields im-
proved rankings that are less sensitive to
training set variation, and furthermore, the
theoretically-motivated reliability indica-
tors we introduce enable the metaclassi-
fier to now be applicable in any problem
where the base classifiers are used.
1 Introduction
From business people to the everyday person, e-
mail plays an increasingly central role in a modern
lifestyle. With this shift, e-mail users desire im-
proved tools to help process, search, and organize
the information present in their ever-expanding in-
boxes. A system that ranks e-mails according to the
?This work was performed primarily while the first author
was supported by Carnegie Mellon University.
From: Henry Hutchins <hhutchins@innovative.company.com>
To: Sara Smith; Joe Johnson; William Woolings
Subject: meeting with prospective customers
Hi All,
I?d like to remind all of you that the group from GRTY will
be visiting us next Friday at 4:30 p.m. The schedule is:
+ 9:30 a.m. Informal Breakfast and Discussion in Cafeteria
+ 10:30 a.m. Company Overview
+ 11:00 a.m. Individual Meetings (Continue Over Lunch)
+ 2:00 p.m. Tour of Facilities
+ 3:00 p.m. Sales Pitch
In order to have this go off smoothly, I would like to practice
the presentation well in advance. As a result, I will need each
of your parts by Wednesday.
Keep up the good work!
?Henry
Figure 1: An E-mail with Action-Item (italics added).
likelihood of containing ?to-do? or action-items can
alleviate a user?s time burden and is the subject of
ongoing research throughout the literature.
In particular, an e-mail user may not always pro-
cess all e-mails, but even when one does, some
emails are likely to be of greater response urgency
than others. These messages often contain action-
items. Thus, while importance and urgency are not
equal to action-item content, an effective action-item
detection system can form one prominent subcom-
ponent in a larger prioritization system.
Action-item detection differs from standard text
classification in two important ways. First, the user
is interested both in detecting whether an email
contains action-items and in locating exactly where
these action-item requests are contained within the
email body. Second, action-item detection attempts
324
to recover the sender?s intent ? whether she means
to elicit response or action on the part of the receiver.
In this paper, we focus on the primary problem
of presenting e-mails in a ranked order according to
their likelihood of containing an action-item. Since
action-items typically consist of a short text span ?
a phrase, sentence, or small passage ? supervised
input to a learning system can either come at the
document-level where an e-mail is labeled yes/no
as to whether it contains an action-item or at the
sentence-level where each span that is an action-
item is explicitly identified. Then, a corresponding
document-level classifier or aggregated predictions
from a sentence-level classifier can be used to esti-
mate the overall likelihood for the e-mail.
Rather than commit to either view, we use a com-
bination technique to capture the information each
viewpoint has to offer on the current example. The
STRIVE approach (Bennett et al, 2005) has been
shown to provide robust combinations of heteroge-
neous models for standard topic classification by
capturing areas of high and low reliability via the
use of reliability indicators.
However, using STRIVE in order to produce im-
proved rankings has not been previously studied.
Furthermore, while they introduce some reliabil-
ity indicators that are general for text classification
problems as well as ones specifically tied to na??ve
Bayes models, they do not address other classifica-
tion models. We introduce a series of reliability in-
dicators connected to areas of high/low reliability in
kNN, SVMs, and decision trees to allow the combi-
nation model to include such factors as the sparse-
ness of training example neighbors around the cur-
rent example being classified. In addition, we pro-
vide a more formal motivation for the role these vari-
ables play in the resulting metaclassification model.
Empirical evidence demonstrates that the result-
ing approach yields a context-sensitive combination
model that improves the quality of rankings gener-
ated as well as reducing the variance of the ranking
quality across training splits.
2 Problem Approach
In contrast to related combination work, we focus on
improving rankings through the use of a metaclass-
ification framework. In addition, rather than sim-
ply focusing on combining models from different
classification algorithms, we also examine combin-
ing models that have different views, in that both the
qualitative nature of the labeled data and the applica-
tion of the learned base models differ. Furthermore,
we improve upon work on context-sensitive com-
bination by introducing reliability indicators which
model the sensitivity of a classifier?s output around
the current prediction point. Finally, we focus on the
application of these methods to action-item data ?
a growing area of interest which has been demon-
strated to behave differently than more standard text
classification problems (e.g. topic) in the literature
(Bennett and Carbonell, 2005).
2.1 Action-Item Detection
There are three basic problems for action-item de-
tection. (1) Document detection: Classify an e-mail
as to whether or not it contains an action-item. (2)
Document ranking: Rank the e-mails such that all
e-mail containing action-items occur as high as pos-
sible in the ranking. (3) Sentence detection: Classify
each sentence in an e-mail as to whether or not it is
an action-item.
Here we focus on the document ranking problem.
Improving the overall ranking not only helps users
find e-mails with action-items quicker (Bennett and
Carbonell, 2005) but can decrease response times
and help ensure that key e-mails are not overlooked.
Since a typical user will eventually process all
received mail, we assume that producing a quality
ranking will more directly measure the impact on
the user than accuracy or F1. Therefore, we focus on
ROC curves and area under the curve (AUC) since
both reflect the quality of the ranking produced.
2.2 Combining Classifiers with Metaclassifiers
One of the most common approaches to classi-
fier combination is stacking (Wolpert, 1992). In
this approach, a metaclassifier observes a past his-
tory of classifier predictions to learn how to weight
the classifiers according to their demonstrated ac-
curacies and interactions. To build the history,
cross-validation over the training set is used to ob-
tain predictions from each base classifier. Next, a
metalevel representation of the training set is con-
structed where each example consists of the class
label and the predictions of the base classifiers. Fi-
nally, a metaclassifier is trained on the metalevel rep-
resentation to learn a model of how to combine the
base classifiers.
However, it might be useful to augment the his-
tory with information other than the predicted prob-
abilities. For example, during peer review, reviewers
325
class class
 
 


 
 


 
Metaclassifier
Reliability
Indicators
SVM
Unigram
 
 


 
 


w1
w2
w3
wn
? ? ? ? ? ?
r1
r2
rn
Figure 2: Architecture of STRIVE. In STRIVE, an additional layer of learning is added where the metaclassifier can use the context
established by the reliability indicators and the output of the base classifiers to make an improved decision.
typically provide both a 1-5 acceptance rating and a
1-5 confidence. The first of these is related to an es-
timate of class membership, P (?accept?? | paper),
but the second is closer to a measure of expertise or
a self-assessment of the reviewer?s reliability on an
example-by-example basis.
Automatically deriving such self-assessments for
classification algorithms is non-trivial. The Stacked
Reliability Indicator Variable Ensemble framework,
or STRIVE, demonstrates how to extend stacking by
incorporating such self-assessments as a layer of re-
liability indicators and introduces a candidate set of
functions (Bennett et al, 2005).
The STRIVE architecture is depicted in Figure 2.
From left to right: (1) a bag-of-words representation
of the document is extracted and used by the base
classifiers to predict class probabilities; (2) reliabil-
ity indicator functions use the predicted probabili-
ties and the features of the document to characterize
whether this document falls within the ?expertise?
of the classifiers; (3) a metalevel classifier uses the
base classifier predictions and the reliability indica-
tors to make a more reliable combined prediction.
From the perspective of improving action-item
rankings, we are interested in whether stacking or
striving can improve the quality of rankings. How-
ever, we hypothesize that striving will perform better
since it can learn a model that varies the combination
rule based on the current example and thus, better
capture when a particular classifier at the document-
level or sentence-level, bag-of-words or n-gram rep-
resentation, etc. will produce a reliable prediction.
2.3 Formally Motivating Reliability Indicators
While STRIVE has been shown to provide robust
combination for topic classification, a formal moti-
vation is lacking for the type of reliability indicators
that are the most useful in classifier combination.
Assume we restrict our choice of metaclassifier to
a linear model. One natural choice is to rank the
e-mails according to the estimated posterior proba-
bility, P? (class = action item | x), but in a linear
combination framework it is actually more conve-
nient to work with the estimated log-odds or logit
transform which is monotone in the posterior, ?? =
log P? (class=action item|x)1?P? (class=action item|x) (Kahn, 2004).
Now, consider applying a metaclassifier to a sin-
gle base classifier. Given only a classifier?s probabil-
ity estimates, a metaclassifier cannot improve on the
estimates if they are well-calibrated (DeGroot and
Fienberg, 1986). Thus a metaclassifier applied to
a single base classifier corresponds to recalibration
(Kahn, 2004).
Assume each of the n base models gives an un-
calibrated log-odds estimate ??i. Then the com-
bination model would have the form ???(x) =
W0(x)+
?n
i=1 Wi(x)??i(x) where the Wi are exam-
ple dependent weight functions that the combination
model learns. The obvious implication is that our
reliability indicators can be informed by the optimal
values for the weighting functions.
We can determine the optimal weights in a sim-
plified case with a single base classifier by assuming
we are given ?true? log-odds values, ?, and a family
of distributions ?x such that ?x = p(z | x)
encodes what is local to x by giving the probability
of drawing a point z near to x. We use ? instead of
?x for notational simplicity. Since ? encodes the
example dependent nature of the weights, we can
drop x from the weight functions. To find weights
that minimize the squared difference between the
true log-odds and the estimated log-odds in the ?
vicinity of x, we can solve a standard regression
problem, argminw0,w1 E?
[
(
w1 ?? + w0 ? ?
)2
]
.
Under the assumption VAR?
[
??
]
6= 0, this yields:
326
w0 = E?[?] ? w1E?
[
??
]
(1)
w1 =
COV?
[
??, ?
]
VAR?
[
??
] = ?????
??,?? (2)
where ? and ? are the stdev and correlation co-
efficient under ?, respectively. The first parame-
ter is a measure of calibration that addresses the
question, ?How far off on average is the estimated
log-odds from the true log-odds in the local con-
text?? The second is a measure of correlation, ?How
closely does the estimated log-odds vary with the
true log-odds?? Note that the second parameter de-
pends on the local sensitivity of the base classifier,
VAR1/2?
[
??
]
= ???. Although we do not have true
log-odds, we can introduce local density models to
estimate the local sensitivity of the model.
In particular, we introduce a series of relia-
bility indicators by first defining a ? distribu-
tion and either computing VAR?
[
??
]
, E?
[
??
]
or
the closely related terms VAR?
[
??(z) ? ??(x)
]
,
E?
[
??(z) ? ??(x)
]
. We use the resulting values for
an example as features for a linear metaclassifier.
Thus we use a context-dependent bias term but leave
the more general model for future work.
2.4 Model-Based Reliability Indicators
As discussed in Section 2.3, we wish to define local
distributions in order to compute the local sensitivity
and similar terms for the base classification models.
To do so, we define local distributions that have the
same ?flavor? as the base classification model.
First, consider the kNN classifier. Since we are
concerned with how the decision function would
change as we move locally around the current pre-
diction point, it is natural to consider a set of shifts
defined by the k neighbors. In particular, let di de-
note the document that has been shifted by a factor
?i toward the ith neighbor, i.e. di = d+?i(ni?d).
We use the largest ?i such that the closest neighbor
to the new point is the original document, i.e. the
boundary of the Voronoi cell (see Figure 3). Clearly,
?i will not exceed 0.5, and we can find it efficiently
using a simple bisection algorithm. Now, let ? be
a uniform point-mass distribution over the shifted
points and ??kNN, the output score of the kNN model.
?1.5 ?1 ?0.5 0 0.5 1 1.5 2
?1.5
?1
?0.5
0
0.5
1
1.5
2
1
2 3
4
5
6
x
Figure 3: Illustration of the kNN shifts produced for a predic-
tion point x using the numbered points as its neighborhood.
Given this definition of ?, it is now straight-
forward to compute the kNN based reliabil-
ity indicators: E?[??kNN(z) ? ??kNN(x)] and
Var1/2? [??kNN(z) ? ??kNN(x)].
Similarly, we define variables for the SVM class-
ifier by considering a document?s locality in terms
of nearby support vectors from the set of support
vectors, V . To determine ?i, we define it in terms
of the closest support vector in V to d. Let  be
half the distance to the nearest point in V , i.e.  =
1
2 minv?V ?v ? d?. Then ?i = ?vi?d? .
1 Thus, the
shift vectors are all rescaled to have the same length.
Now, we must define a probability for the shift. We
use a simple exponential based on  and the rela-
tive distance from the document to the support vec-
tor defining this shift. Let di ? ? where P?(di) ?
exp(??vi ? d? + 2) and
?V
i=1 P?(di) = 1.2
Given this definition of ?, we compute the
SVM based reliability indicators: E?[??SVM(z) ?
??SVM(x)] and Var1/2? [??SVM(z) ? ??SVM(x)].
Space prevents us from presenting all the deriva-
tions here. However, we also define decision-tree
based variables where the locality distribution gives
high probability to documents that would land in
nearby leaves. For a multinomial na??ve Bayes model
(NB), we define a distribution of documents iden-
tical to the prediction document except having an
occurrence of a single feature deleted. For the
multivariate Bernoulli na??ve Bayes (MBNB) model
1We assume that the minimum distance is not zero. If it is
zero, then we return zero for all of the variables.
2As is standard to handle different document lengths, we
take the distance between documents after they have been nor-
malized to the unit sphere.
327
that models feature presence/absence, we use a
distribution over all documents that has one pres-
ence/absence bit flipped from the prediction docu-
ment. It is interesting to note that the variables from
the na??ve Bayes models can be shown to be equiva-
lent to variables introduced by Bennett et al (2005)
? although those were derived in a different fashion
by analyzing the weight a single feature carries with
respect to the overall prediction.
Furthermore, from this starting point, we go on to
define similar variables of possible interest. Includ-
ing the two for each model described here, we define
10 kNN variables, 5 SVM variables, 2 decision-tree
variables, 6 NB model based variables, and 6 MBNB
variables. We describe these variables as well as im-
plementation details and computational complexity
results in (Bennett, 2006).
3 Experimental Analysis
3.1 Data
Our corpus consists of e-mails obtained from vol-
unteers at an educational institution and covers
subjects such as: organizing a research work-
shop, arranging for job-candidate interviews, pub-
lishing proceedings, and talk announcements. Af-
ter eliminating duplicate e-mails, the corpus con-
tains 744 messages with a total of 6301 automat-
ically segmented sentences. A human panel la-
beled each phrase or sentence that contained an
explicit request for information or action. 416 e-
mails have no action-items and 328 e-mails con-
tain action-items. Additional information such
as annotator agreement, distribution of message
length, etc. can be found in (Bennett and Car-
bonell, 2005). An anonymized corpus is available
at http://www.cs.cmu.edu/?pbennett/action-item-dataset.html.
3.2 Feature Representation
We use two types of feature representation: a bag-
of-words representation which uses all unigram to-
kens as the feature pool; and a bag-of-n-grams
where n includes all n-grams where n ? 4. For
both representations at both the document-level and
sentence-level, we used only the top 300 features by
the chi-squared statistic.
3.3 Document-Level Classifiers
kNN
We used a s-cut variant of kNN common in text
classification (Yang, 1999) and a tfidf-weighting
of the terms with a distance-weighted vote of the
neighbors to compute the output. k was set to be
2(dlog2 Ne + 1) where N is the number of training
points. 3 The score used as the uncalibrated log-
odds estimate of being an action-item is:
??kNN(x) =
?
n?kNN(x)|c(n)=actionitem
cos(x,n) ?
?
n?kNN(x)|c(n) 6=actionitem
cos(x,n).
SVM
We used a linear SVM as implemented in the
SVMlight package v6.01 (Joachims, 1999) with a
tfidf feature representation and L2-norm. All de-
fault settings were used. SVM?s margin score,
?
?iyi K(xi,xj), has been shown to empirically
behave like an uncalibrated log-odds estimate (Platt,
1999).
Decision Trees
For the decision-tree implementation, we used the
WinMine toolkit and refer to this as Dnet below (Mi-
crosoft Corporation, 2001). Dnet builds decision
trees using a Bayesian machine learning algorithm
(Chickering et al, 1997; Heckerman et al, 2000).
The estimated log-odds is computed from a Laplace
correction to the empirical probability at a leaf node.
Na??ve Bayes
We use a multinomial na??ve Bayes (NB) and a mul-
tivariate Bernoulli na??ve Bayes classifier (MBNB)
(McCallum and Nigam, 1998). For these classifi-
ers, we smoothed word and class probabilities us-
ing a Bayesian estimate (with the word prior) and
a Laplace m-estimate, respectively. Since these are
probabilistic, they issue log-odds estimates directly.
3.4 Sentence-Level Classifiers
Each e-mail is automatically segmented into sen-
tences using RASP (Carroll, 2002). Since the cor-
pus has fine grained labels, we can train classifiers
to classify a sentence. Each classifier in Section 3.3
is also used to learn a sentence classifier. However,
we then must make a document-level prediction.
In order to produce a ranking score, the con-
fidence that the document contains an action-item is:
??(d) =
{ 1
n(d)
?
s?d|pi(s)=1 ??(s), ?s?d|pi(s) = 1
1
n(d) maxs?d ??(s) o.w.
3This rule is not guaranteed be optimal for a particular value
of N but is motivated by theoretical results which show such a
rule converges to the optimal classifier as the number of training
points increases (Devroye et al, 1996).
328
where s is a sentence in document d, pi is the class-
ifier?s 1/0 prediction, ?? is the score the classifier as-
signs as its confidence that pi(s) = 1, and n(d) is
the greater of 1 and the number of (unigram) to-
kens in the document. In other words, when any
sentence is predicted positive, the document score
is the length normalized sum of the sentence scores
above threshold. When no sentence is predicted pos-
itive, the document score is the maximum sentence
score normalized by length. The length normaliza-
tion compensates for the fact that we are more likely
to emit a false positive the longer a document is.
3.5 Stacking
To examine the hypothesis that the reliability in-
dicators provide utility beyond the information
present in the output of the 20 base classifiers
(2 representations?2 views?5 classifiers), we con-
struct a linear stacking model which uses only the
base classifier outputs and no reliability indicators as
a baseline. For the implementation, we use SVMlight
with default settings. The inputs to this classifier are
normalized to have zero mean and a scaled variance.
3.6 Striving
Since we are constructing base classifiers for both
the bag-of-words and bag-of-n-grams representa-
tions, this gives 58 reliability indicators from com-
puting the variables in Section 2.4 for the document-
level classifiers (58 = 2 ? [6 + 6 + 10 + 5 + 2]).
Although the model-based indicators are defined
for each sentence prediction, to use them at the
document-level we must somehow combine the re-
liability indicators over each sentence. The simplest
method is to average each classifier-based indicator
across the sentences in the document. We do so and
thus obtain another 58 reliability indicators.
Furthermore, our model might benefit from some
of the structure a sentence-level classifier offers
when combining document predictions. Analogous
to the sensitivity of each base model, we can con-
sider such indicators as the mean and standard de-
viation of the classifier confidences across the sen-
tences within a document. For each sentence-level
base classifier, these become two more indicators
which we can benefit from when combining docu-
ment predictions. This introduces 20 more variables
(20 = 2 representations ? 2 ? 5 classifiers).
Finally, we include the 2 basic voting statistic
reliability-indicators (PercentPredictingPositive and
PercentAgreeWBest) that Bennett et al (2005) found
useful for topic classification. This yields a total of
138 reliability-indicators (138 = 58+ 20+ 58+ 2).
With the 20 classifier outputs, there are a total of 158
input features for striving to handle.
As with stacking, we use SVMlight with default
settings and normalize the inputs to this classifier to
have zero mean and a scaled variance.
3.7 Performance Measures
We wish to improve the rankings of the e-mails in
the inbox such that action-item e-mails occur higher
in the inbox. Therefore, we use the area under the
curve (AUC) of an ROC curve as a measure of rank-
ing performance. AUC is a measure of overall model
and ranking quality that has gained wider adoption
recently and is equivalent to the Mann-Whitney-
Wilcoxon sum of ranks test (Hanley and McNeil,
1982). To put improvement in perspective, we can
write our relative reduction in residual area (RRA)
as 1?AUC1?AUCbaseline . We present gains relative to the
best AUC performer (bRRA), and relative to perfect
dynamic selection performance, (dRRA), which as-
sumes we could accurately dynamically choose the
best classifier per cross-validation run.
The F1 measure is the harmonic mean of preci-
sion and recall and is common throughout text class-
ification (Yang and Liu, 1999). Although we are not
concerned with F1 performance here, some users of
the system might be interested in improving rank-
ing while having negligible negative effect on F1.
Therefore, we examine F1 to ensure that an improve-
ment in ranking will not come at the cost of a statis-
tically significant decrease in F1.
3.8 Experimental Methodology
To evaluate performance of the combination sys-
tems, we perform 10-fold cross-validation and com-
pute the average performance. For significance tests,
we use a two-tailed t-test (Yang and Liu, 1999)
to compare the values obtained during each cross-
validation fold with a p-value of 0.05.
We examine two hypotheses: Stacking will out-
perform all of the base classifiers; Striving will out-
perform all the base classifiers and stacking.
3.9 Results & Discussion
Table 1 presents the summary of results. The best
performer in each column is in bold. If a combi-
nation method statistically significantly outperforms
all base classifiers, it is underlined.
329
F1 AUC bRRA dRRA
Document-Level, Bag-of-Words Representation
Dnet 0.7398 0.8423 1.41 1.78
NB 0.6905 0.7537 2.27 2.91
MBNB 0.6729 0.7745 2.00 2.49
SVM 0.6918 0.8367 1.48 1.87
kNN 0.6695 0.7669 2.17 2.74
Document-Level, Ngram Representation
Dnet 0.7412 0.8473 1.38 1.77
NB 0.7361 0.8114 1.75 2.23
MBNB 0.7534 0.8537 1.30 1.61
SVM 0.7392 0.8640 1.24 1.59
kNN 0.7021 0.8244 1.62 2.01
Sentence-Level, Bag-of-Words Representation
Dnet 0.7793 0.8885 1.00 1.27
NB 0.7731 0.8645 1.21 1.50
MBNB 0.7888 0.8699 1.14 1.42
SVM 0.6985 0.8548 1.34 1.70
kNN 0.6328 0.6823 2.98 3.88
Sentence-Level, Ngram Representation
Dnet 0.7521 0.8723 1.13 1.42
NB 0.8012 0.8723 1.15 1.46
MBNB 0.8010 0.8777 1.10 1.38
SVM 0.7842 0.8620 1.23 1.58
kNN 0.6811 0.8078 1.76 2.29
Metaclassifiers
Stacking 0.7765 0.8996 0.88 1.12
STRIVE 0.7813 0.9145 0.76 0.94
Table 1: Base classifier and combiner performance
Now, we turn to the issue of whether combination
improves the ranking of the documents. Examining
the results in Table 1, we see that STRIVE statistically
significantly beats every other classifier according to
AUC. Stacking outperforms the base classifiers with
respect to AUC but not statistically significantly.
Examining F1, we see that neither combination
method outperforms the best base classifier, NB
(sent,ngram). If we examine the hypothesis of
whether this base classifier significantly outperforms
either combination method, the hypothesis is re-
jected. Thus, STRIVE improves the overall ranking
with a negligible effect on F1.
Finally, we compare the ROC curves of striving,
stacking, and two of the most competitive base class-
ifiers in Figure 4. We see that striving loses by a
slight amount to stacking early in the curve but still
 0.5
 0.6
 0.7
 0.8
 0.9  1
 0
 0.2
 0.4
 0.6
 0.8
 1
True Positive Rate
False Positive R
ate
M
BN
B
(sent
,ng
ram)
SV
M
(sent
,ng
ram)
Stacking
STR
IV
E
Figure 4: ROC curves (rotated).
beats the base classifiers. Later in the curve, it dom-
inates all the classifiers. If we examine the curves
using error bars, we see that the variance of STRIVE
drops faster than the other classifiers as we move fur-
ther along the x-axis. Thus, STRIVE?s ranking quality
varies less with changes to the training set.
4 Related Work
Several researchers have considered text classifi-
cation tasks similar to action-item detection. Co-
hen et al (2004) describe an ontology of ?speech
acts?, such as ?Propose a Meeting?, and attempt
to predict when an e-mail contains one of these
speech acts. Corston-Oliver et al (2004) con-
sider detecting items in e-mail to ?Put on a To-Do
List? using a sentence-level classifier. In earlier
work (Bennett and Carbonell, 2005), we demon-
strated that sentence-level classifiers typically out-
perform document-level classifiers on this problem
and examined the underlying reasons why this was
330
the case. Furthermore, we presented user studies
demonstrating that users identify action-items more
rapidly when using the system.
In terms of classifier combination, a wide variety
of work has been done in the arena. The STRIVE
metaclassification approach (Bennett et al, 2005)
extended Wolpert?s stacking framework (Wolpert,
1992) to use reliability indicators. In recent work,
Lee et al (2006) derive variance estimates for na??ve
Bayes and tree-augmented na??ve Bayes and use
them in the combination model. Our work comple-
ments theirs by laying groundwork for how to com-
pute variance estimates for models such as kNN that
have no obvious probabilistic component.
5 Future Work and Conclusion
While there are many interesting directions for fu-
ture work, the most interesting is to directly integrate
the sensitivity and calibration quantities derived into
the more general model discussed in Section 2.3.
In this paper, we took an existing approach to
context-dependent combination, STRIVE, that used
many ad hoc reliability indicators and derived a
formal motivation for classifier model-based local
sensitivity indicators. These new reliability indi-
cators are efficiently computable, and the resulting
combination outperformed a vast array of alterna-
tive base classifiers for ranking in an action-item de-
tection task. Furthermore, the combination results
yielded a more robust performance relative to varia-
tion in the training sets. Finally, we demonstrated
that the STRIVE method could be successfully ap-
plied to ranking.
Acknowledgments
This work was supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract No.
NBCHD030010. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the views of the Defense
Advanced Research Projects Agency (DARPA), or the Depart-
ment of Interior-National Business Center (DOI-NBC).
References
Paul N. Bennett and Jaime Carbonell. 2005. Feature repre-
sentation for effective action-item detection. In SIGIR ?05,
Beyond Bag-of-Words Workshop.
Paul N. Bennett, Susan T. Dumais, and Eric Horvitz. 2005.
The combination of text classifiers using reliability indica-
tors. Information Retrieval, 8:67?100.
Paul N. Bennett. 2006. Building Reliable Metaclassifiers for
Text Learning. Ph.D. thesis, CMU. CMU-CS-06-121.
John Carroll. 2002. High precision extraction of grammatical
relations. In COLING ?02.
D.M. Chickering, D. Heckerman, and C. Meek. 1997. A
Bayesian approach to learning Bayesian networks with lo-
cal structure. In UAI ?97.
William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell.
2004. Learning to classify email into ?speech acts?. In
EMNLP ?04.
Simon Corston-Oliver, Eric Ringger, Michael Gamon, and
Richard Campbell. 2004. Task-focused summarization of
email. In Text Summarization Branches Out: Proceedings of
the ACL ?04 Workshop.
Morris H. DeGroot and Stephen E. Fienberg. 1986. Comparing
probability forecasters: Basic binary concepts and multivari-
ate extensions. In P. Goel and A. Zellner, editors, Bayesian
Inference and Decision Techniques. Elsevier.
Luc Devroye, La?szlo? Gyo?rfi, and Ga?bor Lugosi. 1996. A Prob-
abilistic Theory of Pattern Recognition. Springer-Verlag,
New York, NY.
James A. Hanley and Barbara J. McNeil. 1982. The meaning
and use of the area under a recever operating characteristic
(roc) curve. Radiology, 143(1):29?36.
D. Heckerman, D.M. Chickering, C. Meek, R. Rounthwaite,
and C. Kadie. 2000. Dependency networks for inference,
collaborative filtering, and data visualization. JMLR, 1:49?
75.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In Bernhard Scho?lkopf, Christopher J. Burges, and
Alexander J. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Joseph M. Kahn. 2004. Bayesian Aggregation of Probabil-
ity Forecasts on Categorical Events. Ph.D. thesis, Stanford
University, June.
Chi-Hoon Lee, Russ Greiner, and Shaojun Wang. 2006. Using
query-specific variance estimates to combine bayesian class-
ifiers. In ICML ?06.
Andrew McCallum and Kamal Nigam. 1998. A comparison
of event models for naive bayes text classification. In AAAI
?98, Workshops. TR WS-98-05.
Microsoft Corporation. 2001. WinMine
Toolkit v1.0. http://research.microsoft.com/
?dmax/WinMine/ContactInfo.html.
John C. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likelihood
methods. In Alexander J. Smola, Peter Bartlett, Bern-
hard Scholkopf, and Dale Schuurmans, editors, Advances in
Large Margin Classifiers. MIT Press.
David H. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5:241?259.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In SIGIR ?99.
Yiming Yang. 1999. An evaluation of statistical approaches to
text categorization. Information Retrieval, 1(1/2):67?88.
331
Multi-Document Summarization By Sentence Extraction 
Jade Goldstein* Vibhu Mittal t Jaime Carbonell* Mark Kantrowitzt 
jade@cs.cmu.edu mittal@jprc.com jgc@cs.cmu.edu mkant@jprc.com 
*Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
U.S.A. 
tJust Research 
4616 Henry Street 
Pittsburgh, PA 15213 
U.S.A. 
Abstract 
This paper discusses a text extraction approach to multi- 
document summarization that builds on single-document 
summarization methods by using additional, available in-, 
formation about the document set as a whole and the 
relationships between the documents. Multi-document 
summarization differs from single in that the issues 
of compression, speed, redundancy and passage selec- 
tion are critical in the formation of useful summaries. 
Our approach addresses these issues by using domain- 
independent techniques based mainly on fast, statistical 
processing, a metric for reducing redundancy and maxi- 
mizing diversity in the selected passages, and a modular 
framework to allow easy parameterization for different 
genres, corpora characteristics and user requirements. 
1 Introduction 
With the continuing growth of online information, it 
has become increasingly important to provide improved 
mechanisms to find and present extual information ef- 
fectively. Conventional IR systems find and rank docu- 
ments based on maximizing relevance to the user query 
(Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; 
Salton, 1989). Some systems also include sub-document 
relevance assessments and convey this information to the 
user. More recently, single document summarization sys- 
tems provide an automated generic abstract or a query- 
relevant summary (TIPSTER, 1998a). i However, large- 
scale IR and summarization have not yet been truly in- 
tegrated, and the functionality challenges on a summa- 
rization system are greater in a true IR or topic-detection 
context (Yang et al, 1998; Allan et al, 1998). 
Consider the situation where the user issues a search 
query, for instance on a news topic, and the retrieval sys- 
tem finds hundreds of closely-ranked documents in re- 
sponse. Many of these documents are likely to repeat 
much the same information, while differing in certain 
i Most of these were based on statistical techniques applied to var- 
ious document entities; examples include frait, 1983; Kupiec et al, 
1995; Paice, 1990, Klavans and Shaw, 1995; MeKeown et al, 1995; 
Shaw, 1995; Aon? et al, 1997; Boguraev and Kennedy, 1997; Hovy 
and Lin, 1997; Mitra et al, 1997; Teufel and Moens, 1997; Barzilay 
and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Mor- 
tbn, 1998; Radev and McKeown, 1998; Strzalkowski etal., 1998). 
parts. Summaries of the individual documents would 
help, but are likely to be very similar to each other, un- 
less the summarization system takes into account other 
summaries that have already been generated. Multi- 
document summarization - capable of summarizing ei- 
ther complete documents sets, or single documents in the 
context of previously summarized ones - are likely to 
be essential in such situations. Ideally, multi-document 
summaries should contain the key shared relevant infor- 
mation among all the documents only once, plus other 
information unique to some of the individual documents 
that are directly relevant to the user's query. 
Though many of the same techniques used in single- 
document summarization can also be used in multi- 
document summarization, there are at least four signif- 
icant differences: 
1. The degree of redundancy in information contained 
within a group of topically-related articles is much 
higher than the degree of redundancy within an arti- 
cle, as each article is apt to describe the main point 
as well as necessary shared background. Hence 
anti-redundancy methods are more crucial. 
2. A group of articles may contain a temporal dimen- 
sion, typical in a stream of news reports about an 
unfolding event. Here later information may over- 
ride earlier more tentative or incomplete accounts. 
3. The compression ratio (i.e. the size of the summary 
with respect o the size of the document set) will 
typically be much smaller for collections of dozens 
or hundreds of topically related documents than 
for single document summaries. The SUMMAC 
evaluation (TIPSTER, 1998a) tested 10% compres- 
sion summaries, but in our work summarizing 200- 
document clusters, we find that compression to the 
1% or 0.1% level is required. Summarization be- 
comes significantly more difficult when compres- 
sion demands increase. 
4. The co-reference problem in summarization 
presents even greater challenges for multi- 
document han for single-document summariza- 
tion (Baldwin and Morton, 1998). 
This paper discusses an approach to multi-document 
summarization that builds on previous work in single- 
40 
I 
i 
l 
i 
i 
I 
! 
I 
i 
i 
l 
I 
I 
! 
I 
I 
! 
I, 
I 
document summarization by using additional, available 
information about the document set as a whole, the re- 
lationships between the documents, as well as individual 
documents. 
2 Background and Related Work 
Generating an effective summary requires the summa- 
rizer to select, evaluate, order and aggregate items of 
information according to their relevance to a particular 
subject or purpose. These tasks can either be approx- 
imated by IR techniques or done in greater depth with 
fuller natural language processing. Most previous work 
in summarization has attempted todeal with the issues by 
focusing more on a related, but simpler, problem. With 
text-span deletion the system attempts o delete "less im- 
portant" spans of text from the original document; the 
text that remains is deemed a summary. Work on auto- 
mated document summarization by text span extraction 
dates back at least to work at IBM in the fifties (Luhn, 
1958). Most of the work in sentence xtraction applied 
statistical techniques (frequency analysis, variance anal- 
ysis, etc.) to linguistic units such as tokens, names, 
anaphora, etc. More recently, other approaches have 
investigated the utility of discourse structure (Marcu, 
1997), the combination of information extraction and 
language generation (Klavans and Shaw, 1995; McKe- 
own et al, 1995), and using machine learning to find 
patterns in text (Teufel and Moens, 1997; Barzilay and 
Elhadad, 1997; Strzalkowski et al, 1998). 
Some of these approaches tosingle document summa- 
rization have been extended to deal with multi-document 
summarization (Mani and Bloedern, 1997; Goldstein and 
Carbonell, 1998; TIPSTER, 1998b; Radev and McKe- 
own, 1998; Mani and Bloedorn, 1999; McKeown et al, 
.!999; Stein et al, 1999). These include comparing tem- 
plates filled in by extracting information - using special- 
ized, domain specific knowledge sources - from the doc- 
"ument, and then generating natural language summaries 
from the templates (Radev and McKeown, 1998), com-- 
? paring named-entities - extracted using specialized lists 
- between documents and selecting the most relevant 
section (TIPSTER, 1998b), finding co-reference chains 
in the document set to identify common sections of inter- 
est (TIPSTER, 1998b), or building activation etworks 
of related lexical items (identity mappings, synonyms, 
hypernyms, etc.) to extract text spans from the document 
set (Mani and Bloedern, 1997). Another system (Stein et 
al., 1999) creates a multi-document summary from mul- 
tiple single document summaries, an approach that can 
be sub-optimal in some cases, due to the fact that the 
process of generating the final multi-document summary 
takes as input he individual summaries and not the com- 
plete documents. (Particularly if the single-document 
summaries can contain much overlapping information.) 
The Columbia University system (McKeown et al, 1999) 
creates amulti-document summary using machine learn- 
ing and statistical techniques to identify similar sections 
41 
and language generation to reformulate the summary. 
The focus of our approach is a multi-document system 
that can quickly summarize large clusters of similar doc- 
uments (on the order of thousands) while providing the 
key relevant useful information or pointers to such in- 
formation. Our system (1) primarily uses only domain- 
independent techniques, based mainly on fast, statistical 
processing, (2) explicitly deals with the issue of reducing 
redundancy without eliminating potential relevant infor- 
mation, and (3) contains parameterized modules, so that 
different genres or corpora characteristics an be taken 
into account easily. 
3 Requirements for Multi-Document 
Summarization 
There are two types of situations in which multi- 
document summarization would be useful: (1) the user 
is faced with a collection of dis-similar documents and 
wishes to assess the information landscape contained in 
the collection, or (2) there is a collection of topically- 
related ocuments, extracted from a larger more diverse 
collection as the result of a query, or a topically-cohesive 
cluster. In the first case, if the collection is large enough, 
it only makes ense to first cluster and categorize the doc- 
uments (Yang et al, 1999), and then sample from, or 
summarize ach cohesive cluster. Hence, a "summary" 
would constitute of a visualization of the information 
landscape, where features could be clusters or summaries 
thereof. In the second case, it is possible to build a syn- 
thetic textual summary containing the main point(s) of 
the topic, augmented with non-redundant background in- 
formation and/or query-relevant elaborations. This is the 
focus of our work reported here, including the necessity 
to eliminate redundancy among the information content 
of multiple related ocuments. 
Users' information seeking needs and goals vary 
tremendously. When a group of three people created a
multi-document summarization of 10 articles about he 
Microsoft Trial from a given day, one summary focused 
on the details presented in court, one on an overall gist 
of the day's events, and the third on a high level view of 
the goals and outcome of the trial. Thus, an ideal multi- 
document summarization would be able to address the 
different levels of detail, which is difficult without natu- 
ral language understanding. An interface for the summa- 
rization system needs to be able to permit he user to en- 
ter information seeking oals, via a query, a background 
interest profile and/or a relevance feedback mechanism. 
Following is a list of requirements for multi-document 
summarization: 
? clustering: The ability to cluster similar documents 
and passages to find related information. 
? coverage: The ability to find and extract he main 
points across documents. 
? anti-redundancy: The ability to minimize redun- 
dancy between passages in the summary. 
*. summary cohesion criteria: The ability to combine 
text passages in a useful manner for the reader.-This 
may include: 
- document ordering: All text segments of high- 
est ranking document, hen all segments from 
the next highest ranking document, etc. 
- news-story principle (rank ordering):present 
the most relevant and diverse information first 
so that the reader gets the maximal information 
content even if they stop reading the summary. 
- topic-cohesion: Group together the passages 
by topic clustering using passage similarity cri- 
teria and present the information by the cluster" 
centroid passage rank. 
- t ime line ordering: Text passages ordered 
based on the occurrence of events in time. 
* coherence: Summaries generated should be read- 
able and relevant to the user. 
. context: Include sufficient context so that the sum- 
mary is understandable to the reader. 
? identification of source inconsistencies: Articles of- 
ten have errors (such as billion reported as million, 
etc.); multi-document summarization must be able 
to recognize and report source inconsistencies. 
? summary updates: A new multi-document summary 
must take into account previous ummaries in gen- 
erating new summaries. In such cases, the system 
needs to be able to track and categorize vents. 
? effective user interfaces: 
- Attributability: The user needs to be able to 
easily access the source of a given passage. 
This could be the single document summary. 
- Relationship: The user needs to view related 
passages to the text passage shown, which can 
highlight source inconsistencies. 
- Source Selection: The user needs to be able to 
,- select or eliminate various sources. For exam- 
ple, the user may want to eliminate information 
from some less reliable foreign news reporting 
sources. 
- Context: The user needs to be able to zoom 
in on the context surrounding the chosen pas- 
sages. 
- Redirection: The user should be able to high- 
light certain parts of the synthetic summary 
and give a command to the system indicating 
that these parts are to be weighted heavily and 
that other parts are to be given a lesser weight. 
4 Types of Multi-Document Summarizers 
In the previous ection we discussed the requirements 
for a multi-document summarization system. Depend- 
ing on a user's information seeking goals, the user may 
want to create summaries that contain primarily the com- 
mon portions of the documents (their intersection) or an 
overview of the entire cluster of documents (a sampling. 
of the space that the documents span). A user may also 
want to have a highly readable summary, an overview of 
pointers (sentences or word lists) to further information, 
? or a combination of the two. Following is a list of  var- 
ious methods of creating multi-document summaries by 
extraction: 
1. Summary from Common Sections of Documents: 
Find the important relevant parts that the cluster of 
documents have in common (their intersection) and 
use that as a summary. 
2. Summary from Common Sections and Unique Sec- 
tions of Documents: Find the important relevant 
parts that the cluster of documents have in common 
and the relevant parts that are unique and use that as 
a summary. 
3. Centroid Document Summary: Create a single doc- 
ument summary from the centroid ocument in the 
? cluster. 
4. Centroid Document plus Outliers Summary: Cre- 
ate a single document summary from the centroid 
document in the cluster and add some representa- 
tion from outlier documents (passages or keyword 
extraction) to provide a fuller coverage of the docu- 
ment set. 2 
5. Latest Document plus Outliers Summary: Create 
a single document summary from the latest time 
stamped ocument in the cluster (most recent in- 
formation) and add some representation f outlier 
documents o provide a fuller coverage of the docu- 
ment set. 
6. Summary from Common Sections and Unique Sec- 
tions of Documents with Time Weighting Factor: 
Find the important relevant parts that the cluster of 
documents have in common and the relevant parts 
that are unique and weight all the information by 
the time sequence of the documents in which they 
appear and use the result as a summary. This al- 
lows the more recent, often updated information to 
be more likely to be included in the summary. 
There are also much more complicated types of sum- 
mary extracts which involve natural anguage process- 
ing and/or understanding. These types of summaries in- 
clude: (1) differing points of view within the document 
collection, (2) updates of information within the doc- 
ument collection, (3) updates of information from the 
document collection with respect o an already provided 
summary, (4) the development of an event or subtopic of 
2This is similar to the approach ofTextwise fHPSTER, 1998b), 
whose multi-document summary consists of the most relevant para- 
graph and specialized word lists. 
42 
I 
I 
I 
I 
l 
I 
I 
I 
I 
I 
I 
i 
an event (e.g., death tolls) over time, and (5) a compara- 
tive development of an event. 
Naturally, an ideal multi-document summary would 
include a natural language generation component to cre- 
ate cohesive readable summaries (Radev and McKeown, 
1998; McKeown et al, 1999). Our current focus is on 
the extraction of the relevant passages. 
5 System Design 
In the previous ections we discussed the requirements 
and types of multi-document summarization systems. 
This section discusses our current implementation of
a multi-document summarization system which is de- 
signed to produce summaries that emphasize "relevant 
novelty." Relevant novelty is a metric for minimizing re- 
dundancy and maximizing both relevance and diversity. 
A first approximation tomeasuring relevant novelty is to 
measure relevance and novelty independently and pro- 
vide a linear combination as the metric. We call this lin- 
ear combination "marginal relevance" .-- i.e., a text pas- 
sage has high marginal relevance if it is both relevant to 
the query and useful for a summary, while having mini- 
mal similarity to previously selected passages. Using this 
metric one can maximize marginal relevance in retrieval 
and summarization, hence we label our method "maxi- 
mal marginal relevance" (MMR) (Carboneli and Gold- 
stein, 1998). 
The Maximal Marginal Relevance Multi-Document 
(MMR-MD) metric is defined in Figure 1. Sirnl and 
Sire2 cover some of the properties that we discussed in 
Section 3. 3 
: For Sirnl, the first term is the cosine similarity metric 
for query and document. The second term computes a
coverage score for the passage by whether the passage 
is in one or more clusters and the size of the cluster. 
The third term reflects the information content of the pas- 
.sage by taking into account both statistical and linguis- 
tic features for summary inclusion (such as query expan- 
.sion, position of the passage in the document and pres- 
ence/absence of named-entities in the passage). The final 
term indicates the temporal sequence of the document in 
the collection allowing for more recent information to 
have higher weights. 
For Sire2, the first term uses the cosine similarity met- 
ric to compute the similarity between the passage and 
previously selected passages. (This helps the system to 
minimize the possibility of including passages similar to 
ones already selected.) The second term penalizes pas- 
sages that are part of clusters from which other passages 
have already been chosen. The third term penalizes doc- 
uments from which passages have already been selected; 
however, the penalty is inversely proportional to docu- 
ment length, to allow the possibility of longer documents 
3Sirnn and Sirn2 as previously defined in MMR for single- 
document summarization contained only the first term of each equa- 
tion: 
43 
contributing more passages. These latter two terms allow 
for a fuller coverage of the clusters and documents. 
Given the above definition, MMR-MD incrementally 
computes the standard relevance-ranked list- plus some 
additional scoring factors - when the parameter A= 1, and 
computes a maximal diversity ranking among the pas- 
sages in the documents when A=0. For intermediate val- 
ues of A in the interval \[0,1 \], a linear combination of both 
criteria is optimized. In order to sample the information 
space in the general vicinity of the query, small values of 
can be used; to focus on multiple, potentially overlap- 
ping or reinforcing relevant passages, A can be set to a 
value closer to 1. We found that a particularly effective 
search strategy for document retrieval is to start with a 
small A (e.g., A = .3) in order to understand the informa- 
tion space in the region of the query, and then to focus 
on the most important parts using a reformulated query 
(possibly via relevance feedback) and a larger value of 
(e.g., A = .7) (Carboneli and Goldstein, 1998). 
Our multi-document summarizer works as follows: 
? Segment he documents into passages, and index 
them using inverted indices (as used by the IR 
engine). Passages may be phrases, sentences, n- 
sentence chunks, or paragraphs. 
? Identify the passages relevant o the query using 
cosine similarity with a threshold below which the 
passages are discarded. 
? Apply the MMR-MD metric as defined above. De- 
pending on the desired length of the summary, se- 
lect a number of passages to compute passage re- 
dundancy using the cosine similarity metric and use 
the passage similarity scoring as a method of clus- 
tering passages. Users can select he number of pas- 
sages or the amount of compression. 
? Reassemble the selected passages into a summary 
document using one of the summary-cohesion cri- 
teria (see Section 3). 
The results reported in this paper are based on the use 
of the SMART search engine (Buckley, 1985) to compute 
cosine similarities (with a SMART weighting of lnn  for 
both queries and passages), stopwords eliminated from 
the indexed ata and stemming turned on. 
6 Discussion 
The TIPSTER evaluation corpus provided several sets of 
topical clusters to which we applied MMR-MD summa- 
rization. As an example, consider a set of 200 apartheid- 
related news-wire documents from the Associated Press 
and the Wall Street Journal, spanning the period from 
1988 to 1992. We used the TIPSTER provided topic de- 
scription as the query. These 200 documents were on 
an average 31 sentences in length, with a total of 6115 
sentences. We used the sentence as our summary unit. 
Generating a summary 10 sentences long resulted in a 
MMR-MD ~ Arg max \[A(Siml (Pii, Q, Cij, Di, D)) - (1 - A) max Sirn2 (Pij, Pnm, C, S, Di))\] 
Pij ER\S t - P,=.. ES 
Sire1 (P,.j, Q, Cij, Di, D) = wl *(Pij'Q)+w2*coverage(Pij, Cij)+wa*content(Pij)+w4*tirne_sequenee(Di, D) 
Sim2 ( Pij, Pare, C, S, Di ) = tOa * ( f f  i j  " Pnm) + rob * clusters_selected( (7ij, S) + we * documents_selected( Di , S) 
~ov~r~ge(Pi~,C) = ~ wk * Ikl 
kECi./ 
eonlent(Pij) = ~ wtvp,(W) 
WEPij 
tirnesiarap( D,,a=tim, ) - timestamp( Di ) 
time_sequ_ence ( Di, D) = timestamp( Dmaxtime ) - tiraestamp( D,nintime ) 
clusters_selected(C~, S) = IC~ n L.J cv=l 
v,w:P,,,~ES 
documents_selected(Di, S) = ~ = 
where 
Sire1 is the similarity metric for relevance ranking 
Sim~ is the anti-redundancy metric 
D is a document collection 
P is the passages from the documents in that collection (e.g., ~ j  is passage j from document Di) 
Q is a query or user profile 
R = IR(D, P, Q, 8), i.e., the ranked list of passages from documents retrieved by an IR system, given D, P, Q and a 
' relevance threshold O, below which it will not retrieve passages (O can be degree of match or number of passages) 
._5" is the subset of passages in R already selected 
R\S  is the set difference, i.e., the set of as yet unselected passages in R 
' C is the set of passage clusters for the set of documents 
(7vw is the subset of clusters of (7 that contains passage Pvw 
(7~ is the subset of clusters that contain passages from document D~ 
Ikl is the number of passages in the individual cluster k
IC~,~ N Cijl is the number of clusters in the intersection of (7,,,nand(Tij 
wi..are weights for the terms, which can be optimized 
W is a word in the passage/~j 
type is a particular type of word, e.g., city name 
IOil is the length of document i. 
Figure l: Definition of multi-document summarization algorithm - MMR-MD 
i 
I 
I 
I 
i 
I 
! 
I 
I 
I 
i 
! 
i 
sentence compression ratio of 0.2% and a character com- 
pression of 0.3%, approximately two orders of magni- 
tude different with compression ratios used in single doc- 
ument summarization. The results of summarizing this 
document set with a value of A set to I (effectively query 
relevance, but no MMR-MD) and A set to 0.3 (both query 
relevance and MMR-MD anti-redundancy) are shown in 
Figures 2 and 3 respectively. The summary in Figure 2 
clearly illustrates the need for reducing redundancy and 
maximizing novel information. 
Consider for instance, the summary shown in Figure 2. 
The fact that the ANC is fighting to overthrow the gov- 
44 
i. wsJg10204-0176:1 CAPE TOWN, South Africa - President EW. de Klerk's proposal to repeal the major pillars 
of apartheid rew a generally positive response from black leaders, but African National Congress leader Nelson 
Mandela called on the international community to continue conomic sanctions against South Africa until the 
government takes further steps. 
2. AP880803-0082:25 Three Canadian anti-apartheid groups issued a statement urging the government to sever 
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting 
the white-dominated government in South Africa. 
3. AP880803-0080:25 Three Canadian anti-apartheid groups issued a statement urging the government to sever 
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting 
the white-dominated government in South Africa. 
4. AP880802-0165:23 South Africa says the ANC, the main black group fighting to overthrow South Africa's white 
government, has seven major military bases in Angola, and the Pretoria government wants those bases closed 
down. 
5. AP880212-0060:14 ANGOP quoted the Angolan statement as saying the main causes of confict in the region 
are South Africa's "illegal occupation" of Namibia, South African attacks against its black-ruled neighbors and 
its alleged creation of armed groups to carry out "terrorist a~tivities" in those countries, and the denial of political 
rights to the black majority in South Africa. 
6. AP880823-0069:17 The ANC is the main guerrilla group fighting to overthrow the South African government 
and end apartheid, the system of racial segregation i which South Africa's black majority has no vote in national 
affairs. 
7. AP880803-0158:26 South Africa says the ANC, the main black group fighting to overthrow South Africa's white- 
led government, has seven major military bases in Angola, and it wants those bases closed down. 
8. AP880613-0126:15 The ANC is fighting to topple the South African government and its policy of apartheid, 
under which the nation's 26 million blacks have no voice in national affairs and the 5 million whites control the 
economy and dominate government. 
9. AP880212-0060:13 The African National Congress i the main rebel movement fighting South Africa's white-led 
government and SWAPO is a black guerrilla group fighting for independence for Namibia, which is administered 
by South Africa. 
I0. WSJ870129-0051:1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African 
National Congress, voiced concerns about Soviet influence on the black South African group and the ANC's use 
of violence in the struggle against apartheid. 
Figure 2: Sample multi-document summary with A = 1, news-story-principle ordering (rank order) 
? ernment is mentioned seven times (sentences #2,-#4,#6- 
#9),"which constitutes 70% of the sentences in the sum- 
mary. Furthermore, sentence #3 is an exact duplicate of  
sentence #2, and sentence #7 is almost identical to sen- 
tence #4. In contrast, the summary in Figure 3, generated 
using MMR-MD with a value of A set to 0.3 shows sig- 
nificant improvements in eliminating redundancy. The 
fact that the ANC is fighting to overthrow the govern- 
ment is mentioned only twice (sentences #3,#7), and one 
of these sentences has additional information in it. The 
new summary retained only three of  the sentences from 
the earlier summary. 
Counting clearly distinct propositions in both cases, 
yields a 60% greater information content for the MMR- 
MD case, though both summaries are equivalent in 
length. 
When these 200 documents were added to a set of 4 
other topics of 200 documents, yielding a document-set 
with 1000 documents, the query relevant multi-document 
summarization system produced exactly the same re- 
suits. 
We are currently working on constructing datasetsfor 
experimental evaluations of multi-document summariza- 
tion. In order to construct these data sets, we attempted 
to categorize user's information seeking goals for multi- 
document summarization (see Section 3). As can be seen 
in Figure 2, the standard IR technique of using a query to 
extract relevant passages i no longer sufficient for multi- 
document summarization due to redundancy. In addi- 
tion, query relevant extractions cannot capture temporal 
sequencing. The data sets will allow us to measure the 
effects of these, and other features, on multi-document 
summarization quality. 
Specifically, we are constructing sets of 10 documents, 
? which either contain a snapshot of  an event from mul- 
tiple sources or the unfoldment of an event over time. 
45 
I 
I 1. WSJ870129-0051 1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African Na- 
tional Congress, voiced concerns about Soviet influence on the black South African group and the ANC's use of 
violence in the struggle against apartheid. 
2. wsJgg0422-0133 44 (See related story: "ANC: Apartheid' s Foes - The Long Struggle: The ANC Is Banned, 
But It Is in the Hearts of a Nation's Blacks - -  In South Africa, the Group Survives Assassinations, Government 
Crackdowns n The Black, Green and Gold" - WSJ April 22, 1988) 
3. AP880803-0158 26 South Africa says the ANC, the main black group fighting to overthrow South Africa's white- 
led government, has seven major military bases in Angola, and it wants those bases closed own. 
4. AP880919-0052  But activist clergymen from South Africa said the pontiff should have spoken out more force- 
fully against their white-minority government's policies of apartheid, under which 26 million blacks have no say 
in national affairs. 
5. AP890821-0092 10 Besides ending the emergency and lifting bans on anti- apartheid groups and individual ac- 
tivists, the Harare summit's conditions included the removal of all troops from South Africa's black townships, 
releasing all political prisoners and ending political trials and executions, and a government commitment tofree 
political discussion. 
6. wsJg00503-0041 1  Pretoria and the ANC remain'far ap~t ontheir vision s for a post-apartheid South Africa: 
The ANC wants a simple one-man, one-vote majority rule system, while the government claims that will lead to 
black domination and insists on constitutional protection of the rights of minorities, including the whites. 
7. WSJ900807-0037 1 JOHANNESBURG, South Africa - The African National Congress uspended its 30-year 
armed struggle against he whiie minority government, clearing the way for the start of negotiations over a new 
constitution based on black-white power sharing. 
8. WSJ900924-011920 The African National Congress, South Africa's main black liberation group, forged its sanc- 
tions strategy as a means of pressuring the government toabandon white-minority rule. 
9. WSJ910702-0053 36 At a, meeting in South Africa this week, the African National Congress, the major black 
group, is expected to take a tough line again st the white-rnn government. 
10. wsJg10204-01761 CAPE TOWN, South Africa - President EW. de Klerk's proposal to repeal the major pillars 
of apartheid rew a generally positive response from black leaders, but African National Congress leader Nelson 
Mandela called on the international community to continue conomic sanctions against South Africa until the 
government takes further steps. 
Figure 3: Sample multi-document summary with A = 0.3, time-line ordering 
From these sets we are performing two types of exper- 
iments. In the first, we are examining how users put 
sentences into pre-defined clusters and how they create 
sentence based multi-document summaries. The result 
will also serve as a gold standard for system generated 
summaries - do our systems pick the same summary sen- 
tences as humans and are they picking sentences from 
the same clusters as humans? The second type Of exper- 
iment is designed to determine how users perceive the 
output summary quality. In this experiment, users are 
asked to rate the output sentences from the summarizer 
as good, okay or bad. For the okay or bad sentences, 
they are asked to provide a summary sentence from the 
document set that is "better", i.e., that makes a better set 
of  sentences to represent the information content of  the 
document set. We are comparing our proposed summa- 
rizer #6 in Section 4 to summarizer #1, the common por- 
tions of  the document sets with no anti-redundancy and 
summarizer #3, single document summary of  a centroid 
document using our single document summarizer (Gold- 
stein et al, 1999). 
7 Conc lus ions  and  Future  Work  
This paper presented a statistical method of  generating 
extraction based multi-document summaries. I t  builds 
upon previous work in single-document summarization 
and takes into account some of the major differences be- 
tween single-document and multi-document summariza- 
tion: (i) the need to carefully eliminate redundant infor- 
mation from multiple documents, and achieve high com- 
pression ratios, (ii) take into account information about 
document and passage similarities, and weight different 
passages accordingly, and (iii) take temporal information 
into account. 
Our approach differs from others in several ways: it 
is completely domain-independent, is based mainly on 
fast, statistical processing, it attempts to maximize the 
novelty of the information being selected, and different 
46 
I 
I 
I 
I 
I 
I 
! 
I 
! 
I 
! 
! 
I 
i 
I 
I 
! 
I 
! 
I 
I 
! 
I 
I 
genres or corpora characteristics an be taken into ac- 
count easily. Since our system is not based on the use of 
sophisticated natural language understanding or informa- 
tion extraction techniques, ummaries lack co-reference 
resolution, passages may be disjoint from one another, 
and in some cases may have false implicature. 
In future work, we will integrate work on multi- 
document summarization with work on clustering to pro- 
vide summaries for clusters produced by topic detection 
and tracking. We also plan to investigate how to gen- 
erate coherent temporally based event summaries. We 
will also investigate how users can effectively use multi- 
document summarization through interactive interfaces 
to browse and explore large document sets. 
References 
James Allan, Jaime Carbonell, George Doddington,, 
Jonathan Yamron, and Yiming Yang. 1998. Topic de- 
tection and tracking pilot study: Final report. In Pro- 
ceedings of the DARPA Broadcast News Transcription 
and Understanding Workshop. 
Chinatsu Aone, M. E. Okurowski, J. Gorlinsky, and 
B. Larsen. 1997. A scalable summarization sys- 
tem using robust NLP. In Proceedings of the 
ACL'97/EACL'97 Workshop on Intelligent Scalable 
Text Summarization, pages 66-73, Madrid, Spain. 
Breck Baldwin and Thomas S. Morton. 1998. Dy- 
namic coreference-based summarization. I Proceed- 
ings of the Third Conference on Empirical Methods in 
Natural Language Processing (EMNLP-3), Granada, 
Spain, June. 
Regina Barzilay and Michael Elhadad. 1997. Using lex- 
ical chains for text summarization. In Proceedings of 
the ACL'97/EACL'97 Workshop on Intelligent Scal- 
able Text Summarization, pages 10-17, Madrid, Spain. 
Branimir Boguraev and Chris Kennedy. 1997. Salience 
based content characterization f text documents. In 
Proceedings of the ACL'97/EACL'97 Workshop on 
Intelligent Scalable Text Summarization, pages 2-9,. 
Madrid, Spain. 
Chris Buckley. 1985. Implementation f the SMART in- 
formation retrieval system. Technical Report TR 85- 
686, Cornell University. 
Jaime G. Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversity-based reranking for reordering 
documents and producing summaries. In Proceedings 
of SIGIR-98, Melbourne, Australia, August. 
Jade Goldstein and Jaime Carbonell. 1998. The use 
of mmr and diversity-based reranking in document 
reranking and summarization. In Proceedings of the 
14th Twente Workshop on Language Technology in 
Multimedia Information Retrieval, pages 152-166, 
Enschede, the Netherlands, December. 
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and 
? Jaime G. Carbonell. 1999. Summarizing Text Doc- 
uments: Sentence Selection and Evaluation Metrics. 
Irf Proceedings of the 22nd International ACM SIGIR 
Conference on Research and Development in Informa- 
tion Retrieval (S1G1R-99), pages 121-128, Berkeley, 
CA. 
Eduard Hovy and Chin-Yew Lin. 1997. Automated text 
summarization i SUMMARIST. In ACUEACL-97 
Workshop on Intelligent Scalable Text Summarization, 
pages 18-24, Madrid, Spain, July. 
Judith L. Klavans and James Shaw. 1995. Lexical se- 
mantics in summarization. I  Proceedings of the First 
Annual Workshop of the IFIP Working Group FOR 
NLP and KR, Nantes, France, April. 
Julian M. Kupiec, Jan Pedersen, and Francine Chen. 
1995. A trainable document summarizer. In Proceed- 
ings of the 18th Annual Int. ACM/SIG1R Coaference 
on Research and Development in IR, pages 68-73, 
Seattle, WA, July. 
P. H. Luhn. 1958. Automatic reation of literature ab- 
stracts. IBM Journal, pages 159-165. 
Inderjeet Mani and Eric Bloedern. 1997. Multi- 
document summarization by graph search and merg- 
ing. In Proceedings of AAA1-97, pages 622--628. 
AAAI. 
Inderjeet Mani and Eric Bloedom. 1999. Summarizing 
similarities and differences among related ocuments. 
Information Retrieval, 1:35-67. 
Daniel'Marcu. 1997. From discourse structures to text 
summaries. In Proceedings of the ACL'97/EACL'97 
Workshop on Intelligent Scalable Text Summarization, 
pages 82-88, Madrid, Spain. 
Kathleen McKeown, Jacques Robin, and Karen Kukich. 
1995. Designing and evaluating a new revision-based 
model for summary generation. Info. Proc. and Man- 
agement, 31 (5). 
Kathleen McKeown, Judith Klavans, Vasileios Hatzivas- 
siloglou, Regina Barzilay, and Eleazar Eskin. 1999. 
Towards Multidocument Summarization by Reformu- 
lation: Progress and Prospects. In Proceedings of 
AAAI-99, pages 453--460, Orlando, FL, July. 
Mandar Mitra, Amit Singhal, and Chris Buckley. 1997. 
Automatic text summarization by paragraph extrac- 
tion. In ACL/EACL-97 Workshop on Intelligent Scal- 
able Text Summarization, pages 31-36, Madrid, Spain, 
July. 
Chris D. Paice. 1990. Constructing literature abstracts 
by computer: Techniques and prospects. Info. Proc. 
and Management, 26:171-186. 
Dragomir Radev and Kathy McKeown. 1998. Generat- 
ing natural language summaries from multiple online 
sources. Compuutational Linguistics. 
Gerald Salttm. 1970. Automatic processing of foreign 
language docuemnts. Journal of American Society for 
Information Sciences, 21:187-194. 
Gerald Salton. 1989. Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of Informa- 
tion by Computer. Addison-Wesley. 
47 
James Shaw. 1995. Conciseness through aggregation i  
text generation. In Proceedings of 33rd Association 
for Computational Linguistics, pages 329-331. 
Gees C. Stein, Tomek Strzalkowski, and G. Bowden 
Wise. 1999. Summarizing Multiple Documents Us- 
ing Text Extraction and Interactive Clustering. In Pro- 
ceedings of PacLing-99: The Pacific Rim Conference 
on Computational Linguistics, pages 200-208, Water- 
loo, Canada. 
Tomek Strzalkowski, Jin Wang, and Bowden Wise. 
1998. A robust practical text summarization system. 
In AAAI Intelligent Text Summarization Workshop, 
pages 26-30, Stanford, CA, March. 
J. I. Tait. 1983. Automatic Summarizing of English 
Texts. Ph.D. thesis, University of Cambridge, Cam- 
bridge, UK. 
Simone Teufel and Marc Moens. 1997. Sentence x- 
traction as a classification task. In ACL/EACL-97 
Workshop on Intelligent Scalable Text Summarization, 
pages 58-65, Madrid, Spain, July. 
TIPSTER. 1998a. Tipster text phase III 18-month work- 
shop notes, May. Fairfax, VA. 
TIPSTER. 1998b. Tipster text phase III 24-month work- 
shop notes, October. Baltimore, MD. 
Charles J. van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
Yiming Yang, Tom Pierce, and Jaime 13. Carbonell. 
1998. A study on retrospective and on-line event de- 
tection. In Proceedings of the 21th Ann lnt ACM SI- 
G1R Conference on Research and Development inIn- 
formation Retrieval (SIGIR'98), pages 28-36. 
:Yiming Yang, Jaime G. Carbonell, Ralf D. Brown, 
Tom Pierce, Brian T. Archibald, and Xin Liu. 1999. 
Learning approaches for topic detection and tracking 
. news events. IEEE Intelligent Systems, Special Issue 
on Applications of Intelligent Information Retrieval, 
14(4):32-43, July/August. 
48 
 
		
Unsupervised Induction of Natural Language Morphology Inflection Classes 
Christian Monson, Alon Lavie, Jaime Carbonell, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University  
5000 Forbes Ave. 
Pittsburgh, USA 15213 
{cmonson, alavie+, jgc+, lsl+}@cs.cmu.edu 
 
Abstract 
We propose a novel language-independent 
framework for inducing a collection of mor-
phological inflection classes from a monolin-
gual corpus of full form words.  Our approach 
involves two main stages.  In the first stage, 
we generate a large data structure of candidate 
inflection classes and their interrelationships.  
In the second stage, search and filtering tech-
niques are applied to this data structure, to 
identify a select collection of "true" inflection 
classes of the language.  We describe the basic 
methodology involved in both stages of our 
approach and present an evaluation of our 
baseline techniques applied to induction of 
major inflection classes of Spanish.  The pre-
liminary results on an initial training corpus 
already surpass an F1 of 0.5 against ideal 
Spanish inflectional morphology classes. 
1 Introduction 
Many natural language processing tasks, such as 
morphological analysis and parsing, have mature 
solutions when applied to resource-rich European 
and Asian languages.  Addressing these same tasks 
in less studied low-density languages, however, 
poses exciting challenges.   
These languages have limited available re-
sources: with perhaps a few million speakers there 
is likely no native speaker linguist and frequently 
there is little electronic text readily available.  To 
compound the difficulties, while low-density lan-
guages abound, comparatively little financial re-
sources are available to address their challenges.  
These considerations suggest developing systems 
to automatically induce solutions for NLP tasks in 
new languages. 
The AVENUE project (Lavie et al 2003; Car-
bonell et al, 2002; Probst et al, 2002) at Carnegie 
Mellon University seeks to apply automatic induc-
tion methods to develop rule-based machine trans-
lation systems between pairs of languages where 
one of the languages is low-density and the other is 
resource-rich.  We are currently pursuing MT sys-
tems with Mapudungun, an indigenous language 
spoken by 900,000 people in southern Chile and 
Argentina, and Aymara, spoken by 3 million peo-
ple in Bolivia, Peru, and northern Chile, as low-
density languages and Spanish the resource rich 
language. 
A vital first step in a rule-based machine transla-
tion system is morphological analysis.  This paper 
outlines a framework for automatic natural lan-
guage morphology induction inspired by the tradi-
tional and linguistic concept of inflection classes.  
Additional details concerning the candidate inflec-
tion class framework can be found in Monson 
(2004).  This paper then goes on to describe one 
implemented search strategy within this frame-
work, presenting both a simple summary of results 
and an in depth error analysis. 
While the intent of this research direction is to 
define techniques applicable to low-density lan-
guages, this paper employs English to illustrate the 
main conjectures and Spanish, a language with a 
reasonably complex morphological system, for 
quantitative analysis.  All experiments detailed in 
this paper are over a Spanish newswire corpus of 
40,011 tokens and 6,975 types. 
2 Previous Work 
It is possible to organize much of the recent 
work on unsupervised morphology induction by 
considering the bias each approach has toward dis-
covering morphologically related words that are 
also orthographically similar. 
At one end of the spectrum is the work of 
Yarowsky et al (2001), who derive a morphologi-
cal analyzer for a language, L, by projecting the 
morphological analysis of a resource-rich language 
onto L through a clever application of statistical 
machine translation style word alignment prob-
abilities.  The word alignments are trained over a 
sentence aligned parallel bilingual text for the lan-
guage pair.  While the probabilistic model they use 
to generalize their initial system contains a bias 
toward orthographic similarity, the unembellished 
algorithm contains no assumptions on the ortho-
graphic shape of related word forms. 
Next along the spectrum of orthographic similar-
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
ity bias is the work of Schone and Jurafsky (2000), 
who first acquire a list of pairs of potential mor-
phological variants (PPMV?s) using an ortho-
graphic similarity technique due to Gaussier 
(1999), in which pairs of words from a corpus vo-
cabulary with the same initial string are identified.  
They then apply latent semantic analysis (LSA) to 
score each PPMV with a semantic distance.  Pairs 
measuring a small distance, those whose potential 
variants tend to occur where a neighborhood of the 
nearest hundred words contains similar counts of 
individual high-frequency forms, are then pro-
posed as true morphological variants of one anther.  
In later work, Schone and Jurafsky (2001) extend 
their technique to identify not only suffixes but 
also prefixes and circumfixes by building both 
forward and backward tries over a corpus. 
Goldsmith (2001), by searching over a space of 
morphology models limited to substitution of suf-
fixes, ties morphology yet closer to orthography.  
Segmenting word forms in a corpus, Goldsmith 
creates an inventory of stems and suffixes.  Suf-
fixes which can interchangeably concatenate onto 
a set of stems form a signature.  After defining the 
space of signatures, Goldsmith searches for that 
choice of word segmentations resulting in a mini-
mum description length local optimum. 
Finally, the work of Harris (1955; 1967), and 
later Hafer and Weiss (1974), has direct bearing on 
the approach taken in this paper.  Couched in mod-
ern terms, their work involves first building tries 
over a corpus vocabulary, and then selecting, as 
morpheme boundaries, those character boundaries 
with high branching count in the tries. 
The work in this paper also has a strong bias to-
ward discovering morphologically related words 
that share a similar orthography.  In particular, the 
morphology model we use is, akin to Goldsmith, 
limited to suffix substitution.  The novel proposal 
we bring to the table, however, is a formalization 
of the full search space of all candidate inflection 
classes.  With this bulwark in place, defining 
search strategies for morpheme discovery becomes 
a natural and straightforward activity. 
3 Inflection Classes as Motivation 
When learning the morphology of a foreign lan-
guage, it is common for a student to study tables of 
inflection classes.  In Spanish, for example, a regu-
lar verb belongs to one of three inflection 
classes?verbs that take the -ar infinitive suffix 
inflect for various syntactic features with one set of 
suffixes, verbs that take the -er infinitive suffix 
realize the same set of syntactic features with a 
second set of suffixes, while -ir verbs take yet a 
third set. 
Carstairs-McCarthy formalizes the concept of an 
inflection class in chapter 16 of The Handbook of 
Morphology (1998).  In his terminology, a lan-
guage with inflectional morphology contains lex-
emes which occur in a variety of word forms.  
Each word form carries two pieces of information: 
1) Lexical content and 
2) Morphosyntactic properties. 
For example, the English word form gave ex-
presses the lexeme GIVE plus the morphosyntactic 
property Past, while gives expresses GIVE plus the 
properties 3rd Person, Singular, and Non-Past. 
A set of morphosyntactic properties realized 
with a single word form is defined to be a cell, 
while a paradigm is a set of cells exactly expressed 
by the word forms of some lexeme.   
A particular natural language may have many 
paradigms.  In English, a language with very little 
inflectional morphology, there are at least two 
paradigms, a noun paradigm consisting of two 
cells, Singular and Plural, and a paradigm for 
verbs, consisting of the five cells given (with one 
choice of naming convention) as the first column 
of Table 1. 
Lexemes that belong to the same paradigm may 
still differ in their morphophonemic realizations of 
various cells in that paradigm?each paradigm 
may have several associated inflection classes 
which specify, for the lexemes belonging to that 
inflection class, the surface instantiation for each 
cell of the paradigm.   
Three of the inflection classes within the English 
verb paradigm are found in Table 1 under the col-
umns labeled A through C.  Each inflection class 
Inflection Classes Verb 
Paradigm A B C 
Basic 
blame 
roam 
solve 
show 
sow 
saw 
sing 
ring 
3rd Person 
Singular     
Non-past 
-/z/ 
blames 
roams 
solves 
-/z/ 
shows 
sows 
saws 
-/z/ 
sings 
rings 
 
Past 
-/d/ 
blamed 
roamed 
solved 
-/d/ 
showed 
sowed 
sawed 
V? /eI/ 
sang 
rang 
 
Perfective       
or Passive 
-/d/ 
blamed 
roamed 
solved 
-/n/ 
shown 
sown 
sawn 
V? /?/ 
sung 
rung 
 
Progressive 
-/i?/ 
blaming 
roaming 
solving 
-/i?/ 
showing 
sowing 
sawing 
-/i?/ 
singing 
ringing 
 
 
Table 1: A few inflection classes of the Eng-
lish verb paradigm 
column consists of entries corresponding to the 
cells of the verb paradigm.  Each entry contains an 
informal notation for the morphophonemic process 
which the inflection class applies to the basic form 
of a lexeme and examples of word forms filling the 
corresponding paradigm cell. 
Inflection class A is one of the largest and most 
productive verb inflection classes in English, in-
flection class B contains the Perfective/Passive 
suffix -/n/, and C is a small ?irregular? inflection 
class of strong verbs. 
The task our morphology induction system en-
gages is exactly the discovery of the inflection 
classes of a natural language.  Unlike the analysis 
in Table 1, however, the rest of this paper treats 
word forms as simply strings of characters as op-
posed to strings of phonemes. 
4 Empirical Inflection Classes 
There are two stages in our approach to unsu-
pervised morphology induction.  First, we define a 
search space over a set of candidate inflection 
classes, and second, we search this space for those 
candidates most likely to be part of a true inflec-
tion class in the language.  In both stages of our 
approach we intentionally exploit the fact that suf-
fixes belonging to the same natural language in-
flection class frequently occur interchangeably on 
the same stems. 
4.1 Candidate Inflection Class Search Space 
To define a search space wherein we hope to 
identify inflection classes of a natural language, 
our algorithm accepts as input a monolingual cor-
pus for that language and proposes candidate mor-
pheme boundaries at every character boundary in 
every word form in the corpus vocabulary.  We 
call each string before a candidate morpheme 
boundary a candidate stem or c-stem, and each 
string after a boundary a c-suffix.  We define a 
candidate inflection class (CIC) to be a set of c-
suffixes for which there exists at least one c-stem, 
t, such that each c-suffix in the CIC concatenated 
to t produces a word form in the vocabulary.  For 
convenience, let the set of c-stems which generate 
a CIC, C, be called the adherent c-stems of C; let 
the number of adherent c-stems of C be C?s adher-
ent size; and let the size of the set of c-suffixes in 
C be the level of C.  We denote a CIC in this paper 
by a period delimited sequence of c-suffixes. 
While CIC?s effectively model suffix substitu-
tion on bound stems, we would also like to model 
suffix concatenation onto free stems.  To this end, 
the set of candidate morpheme boundaries our al-
gorithm proposes include those boundaries after 
the final character in each word form.  In this paper 
we assume a suffix, which we denote as ?, follows 
all word form final boundaries.  A CIC contains 
the ? c-suffix when each c-stem in the CIC can 
occur, not only bound to other c-suffixes in the 
CIC, but also as a free stem.  For generality, the 
boundary before the first character of each word 
form is also a candidate morpheme boundary. 
 Table 2 illustrates the type of CIC?s produced 
by our algorithm.  The CIC?s in this table, arranged 
in a systematic but arbitrary order, are each derived 
Vocabulary: blame
blames roams
blamed roamed
roaming
?.s.d
blame
?.s
blame.solve
?.d
blame
s.d s.ed.ing e.es.ing
blame roam solv
s.ed e.ing
roam solv
s s.ing es.ing
blame.roam.solve roam solv
d ed.ing ng
blame.roame roam roami.solvi
ing g
roam.solv roamin.solvin
lame
b
solves
solve
ame.ames.amed
bl
me.mes.med e.es.ed
solving
oams.oamed.oaming
bla blam r
me.mes e.es olve.olves.olvingame.ames
ame.amed
bla blam.solv sbl
me.med e.ed
...bla blam
bl
mes.med es.ed
bla blam
me e
bla blam.solv
amed
bl.ro
mes es
bla blam.solvbl
ames
med ed
bla.roa blam.roam
?
blame.blames.blamed.roams.roamed.roaming.solve.solves.solving
lame.lames.lamed
b
lame.lames
b
lame.lamed
b
lames.lamed
b
lames
b
lamed
b
bl
ames.amed
bl
ame
Table 2: Some of the CIC's, arranged in a systematic but arbitrary order, derived from a toy vo-
cabulary. Each entry is specified as a period delimited sequence of c-suffixes in bold above a   
period delimited sequence of adherent c-stems in italics 
from one or more forms in a small vocabulary con-
sisting of a subset of the word forms found under 
inflection class A from Table 1.  Proposing, as our 
procedure does, morpheme boundaries at every 
character boundary in every word form necessarily 
produces many ridiculous CIC?s, such as 
ame.ames.amed, from the forms blame, blames, 
and blamed and the c-stem bl.  Dispersed among 
the incorrect CIC?s generated by our algorithm, 
however, are also CIC?s that seem very reasonable, 
such as ?.s, from the c-stems blame and tease.   
Note that where Table 1 lists all the surface 
forms of the three lexemes BLAME, ROAM, and 
SOLVE, the vocabulary of Table 2 mimics the vo-
cabulary of a text corpus from a highly inflected 
language where we expect few, if any, lexemes to 
occur in the complete set of possible surface forms.  
Specifically, the vocabulary of Table 2 lacks the 
surface form blaming of the lexeme BLAME, solved 
of the lexeme SOLVE, and the root form roam of 
the lexeme ROAM.  Hence, while the reasonable 
CIC ?.s arises from the pairs of surface forms 
(blame, blames) and (solve, solves), there is no 
way for the form roams to contribute to the ?.s 
CIC because the surface form roam is missing 
from this vocabulary.  In other words, we lack evi-
dence for a ? suffix on the c-stem roam.  Also no-
tice that, as a result of English spelling rules, the 
CIC s.ed generated from the pair of surface forms 
(roams, roamed) is separate from each of the 
CIC?s s.d and es.ed generated from the pair of sur-
face forms (blames, blamed).   
Looking at Table 2, it is clear there is structure 
among the CIC?s.  In particular, at least two types 
of relations hold between CIC?s.  First, hierarchi-
cally, the c-suffixes of one CIC may be a superset 
of the c-suffixes of another CIC.  For example the 
c-suffixes in the CIC e.es.ed are a superset of the 
c-suffixes in the CIC e.ed.  Second, cutting across 
this hierarchical structure there is structure be-
tween CIC?s which propose different morpheme 
boundaries within the same word forms.  Compare 
the CIC?s me.mes.med and e.es.ed; each is de-
rived from exactly the triple of word forms blame, 
blames, and blamed, but differ in the placement of 
the hypothesized morpheme boundary.   
Taken together the hierarchical c-suffix set in-
clusion relations and the morpheme boundary rela-
tions impose a lattice structure on the space of 
CIC?s.  Figure 1 diagrams the CIC lattice over an 
interesting subset of the columns of Table 2.  Hier-
archical links, represented by solid lines, connect 
any given CIC often to more than one parent and 
more than one child.  The empty CIC (not pictured 
in Figure 1) can be considered the child of all level 
one CIC?s (including the ? CIC), but there is no 
universal parent of all top level CIC?s.  Moving up 
the lattice always results in a monotonic decrease 
in adherent size because a parent CIC requires 
each adherent c-stem to form a word with a super-
set of the c-suffixes of each child. 
Horizontal morpheme boundary links, dashed 
lines, connect a CIC, C, with a neighbor to the 
right if each c-suffix in C begins with the same 
character.  This entails that there is at most one 
morpheme boundary link leading to the right of 
each CIC.  There may, however, be as many links 
leading to the left as there are characters in the or-
thography.  The only CIC with depicted multiple 
left links in Figure 1 is ?, which has left links to 
the CIC?s e, s, and d.  A number of left links ema-
nating from the CIC?s in Figure 1 are not shown; 
among others absent from the figure is the left link 
from the CIC e.es leading to the CIC ve.ves with 
the adherent sol.  Since left links effectively divide 
a CIC into separate CIC?s, one for each character 
in the orthography, adherent count monotonically 
decreases as left links are followed. 
To better visualize what a CIC lattice looks like 
when derived from real data, Figure 2 contains a 
portion of a hierarchical lattice automatically gen-
erated from our Spanish newswire corpus.  Each 
entry in Figure 2 contains the c-suffixes compris-
ing the CIC, the adherent size of the CIC, and a 
sample of adherent c-stems.  The lattice in Figure 2 
covers: 
e.es 
blam 
solv 
e.ed 
blam 
es 
blam 
solv 
?.s.d 
blame 
?.s 
blame 
solve 
? 
blame 
blames 
blamed 
roams 
roamed 
roaming 
solve 
solves 
solving 
e.es.ed 
blam 
ed 
blam 
roam 
d 
blame 
roame 
?.d 
blame 
s.d 
blame 
s 
blame 
roam 
solve 
es.ed 
blam 
e 
blam 
solv 
me.mes 
bla 
me.med 
bla 
mes 
bla 
me.mes.med 
bla 
med 
bla 
roa 
mes.med 
bla 
me 
bla 
Figure 1: Portion of a CIC lattice from the 
toy vocabulary in Table 2 
c-suffix set inclusion links 
morpheme boundary links 
1) The productive Spanish inflection class for 
adjectives, a.as.o.os, covering the four adjec-
tive paradigm cells: feminine singular, femi-
nine plural, masculine singular, and mascu-
line plural, respectively, 
2) All possible CIC subsets of the adjective 
CIC, e.g. a.as.o, a.os, etc. and, 
3) The imposter CIC a.as.o.os.tro, together 
with its rogue descendents, a.tro, and tro.   
Other CIC?s that are descendents of a.as.o.os.tro 
and that contain the c-suffix tro do not supply ad-
ditional adherents and hence are not present either 
in Figure 2 or in our program?s representation of 
the CIC lattice.  The CIC?s a.as.tro and os.tro, for 
example, both have only the one adherent, cas, 
already possessed by their common ancestor 
a.as.o.os.tro.  Strictly speaking we have simplified 
for exposition, as the CIC a.as.o.os.tro is not actu-
ally present in the algorithm?s representation ei-
ther, because the c-stem cas occurred with a num-
ber of additional c-suffixes yielding the CIC: 
a.as.i.o.os.sandra.tanier.ter.tro.trol.  
4.2 Search 
Given the framework of CIC lattices, the key 
task for automatic morphology induction is to 
autonomously separate the nonsense CIC?s from 
the useful ones, thus identifying linguistically 
plausible inflection classes.  This section treats the 
CIC lattices as a hypothesis space of valid inflec-
tion classes and searches this space for CIC?s most 
likely to be true inflection classes in a language. 
There are many possible search strategies and 
heuristics applicable to the CIC lattice, and while 
for future work we intend to explore a variety of 
search techniques, this paper presents a reasonable 
and intuitive baseline search procedure.  We have 
investigated a series of algorithms which build 
upon each other.  Each algorithm employs a num-
ber of parameters which are tuned by hand.  These 
parameters are only interesting in so far as they 
help us find true CIC?s from among the many in 
the lattice.  The performance of each algorithm is 
described in section 6. 
4.2.1 Vertical Only 
 To motivate the general approach we have 
taken, compare the adherent sizes of the various 
CIC?s in Figure 2.  The target CIC a.as.o.os, corre-
sponding to the Spanish adjective inflection class, 
has 43 adherents.  Its various descendents must 
occur with monotonically increasing adherent 
sizes, but frequently a child will not more than 
double or triple its immediate parent?s adherent 
size, and never is there a difference greater than a 
factor of ten. Notice also the large adherent counts 
of the level one descendents of a.as.o.os, the 
smallest is as with 404 adherents.   
Contrast this behavior with that of CIC?s involv-
ing the spurious suffix tro.  The CIC a.as.o.os.tro 
occurs in the corpus with exactly one adherent, 
cas.  Additionally, the word forms cena, supper, 
and centro, center, occur yielding the CIC a.tro 
with two adherents.  In total tro is the final string 
of only 16 individual word forms. 
In general, we expect that true suffixes in a lan-
guage will both occur frequently and occur at-
tached to a large number of stems which also ac-
cept other suffixes from the same inflection class.  
These considerations led us to propose three pa-
rameters for our basic search strategy: 
L1 SIZE:  A level one adherent size cutoff 
TOP SIZE:  An absolute adherent size cutoff 
RATIO:  A parent-to-child adherent size      
ratio cutoff 
The L1 SIZE parameter requires a c-suffix to be 
frequent, while the TOP SIZE and RATIO parame-
ters require a suffix to be substitutable for other c-
suffixes in a reasonable number of c-stems. 
a.as.o.os 
43 
african 
cas 
jur?dic 
l 
... 
a.as.o.os.tro 
1 
cas 
a.as.os 
50 
afectad 
cas 
jur?dic 
l 
... 
a.as.o 
59 
cas 
citad 
jur?dic 
l 
... 
a.o.os 
105 
impuest 
indonesi 
italian 
jur?dic 
... 
a.as 
199 
huelg 
incluid 
industri 
inundad 
... 
a.os 
134 
impedid 
impuest 
indonesi 
inundad 
... 
as.os 
68 
cas 
implicad 
inundad 
jur?dic 
... 
a.o 
214 
id 
indi 
indonesi 
inmediat 
... 
as.o 
85 
intern 
jur?dic 
just 
l 
... 
a.tro 
2 
cas 
cen 
a 
1237 
huelg 
ib 
id 
iglesi 
... 
as 
404 
huelg 
huelguist 
incluid 
industri 
... 
os 
534 
humor?stic 
human 
h?gad 
impedid 
... 
o 
1139 
hub 
hug 
human 
huyend 
... 
tro 
16 
catas 
ce 
cen 
cua 
... 
as.o.os 
54 
cas 
implicad 
jur?dic 
l 
... 
 
Figure 2: Hierarchical CIC lattice automati-
cally derived from Spanish 
o.os 
268 
human 
implicad 
indici 
indocumentad 
... 
We apply these three parameters by beginning 
our search at the bottom of the lattice.  Each level 
one CIC with an adherent count larger than L1 
SIZE is placed in a list of path CIC?s.  Then for 
each path CIC, C, we remove C from the list of 
path CIC?s, and in turn consider each of C?s hier-
archical parents, Pi.  If Pi?s adherent size is at least 
TOP SIZE, and if the ratio of Pi?s adherent size to 
C?s adherent size is larger than RATIO, then Pi is 
placed in the list of path CIC?s.  If no parent of C 
can be placed in the list of path CIC?s, and if C?s 
level is greater than one, then C is placed in a list 
of selected CIC?s.  When there are no more CIC?s 
in the list of path CIC?s, the search ends and the 
CIC?s in the selected list are the CIC?s the algo-
rithm believes are true CIC?s of the language. 
As an illustration suppose we explored the lattice 
in Figure 2 with the following parameter settings: 
L1 SIZE:  100 
TOP SIZE:  2 
RATIO:  0.1 
Our search algorithm begins by comparing the 
adherent size of each level one CIC to L1 SIZE.  
The only level one CIC with an adherent count less 
than 100 is tro with 16 adherents, preventing tro 
from being placed in the list of path CIC?s.   
Each of the surviving level one CIC?s is then 
considered in turn.  The algorithm comes to the 
CIC a, where the ratios of adherent sizes between 
each of its parents a.tro, a.as, a.o, and a.os and 
itself are 0.002, 0.161, 0.173, and 0.108 respec-
tively.  Each of these ratios, except that between a 
and a.tro, at 0.002, is larger than 0.1.  And since 
the adherent sizes of a.as, a.o, and a.os are each 
larger than TOP SIZE, these three CIC?s are placed 
in the list of path CIC?s.   
From this point, every hierarchical link in Figure 
2 leading to the CIC a.as.o.os passes the TOP SIZE 
and RATIO cutoffs.  Thus the algorithm reaches a 
state where the only CIC in the list of path CIC?s is 
a.as.o.os.  When this good CIC is removed from 
the list of path CIC?s, the algorithm finds that its 
only parent is a.as.o.os.tro with its lone adherent.  
Since TOP SIZE requires a parent to have at least 
two adherents, a.as.o.os.tro cannot be placed in 
the list of path CIC?s.  As no parent can be placed 
in the list of path CIC?s, a.as.o.os is placed in the 
list of selected CIC?s?which is the desired out-
come.  The list of path CIC?s is now empty and the 
search ends. 
4.2.2 Horizontal Blocking 
 To improve performance over the Vertical Only 
algorithm we next incorporated knowledge from 
the horizontal morpheme boundary links.  Monson 
(2004) describes how morpheme boundary links in 
a CIC lattice can be thought of as branchings in a 
vocabulary trie where identical subtries are con-
flated.  Harris (1955) discusses how the branching 
count in a suffix trie can be exploited to identify 
morpheme boundaries.  We extend the spirit of 
Harris? work in our algorithm through the use of 
two search parameters: 
HORIZ RATIO: A cutoff over: 
sizeadherent 
character in  ending adherents of #
argmax cc  
HORIZ SIZE: An adherent size cutoff 
Left Blocking 
In the first variant of horizontal blocking we ap-
ply these two horizontal parameters when consid-
ering a CIC, C, removed from the list of path 
CIC?s.  If the adherent size of C is larger than 
HORIZ SIZE and the maximum percentage of ad-
herents of C that end in any one character is larger 
than HORIZ RATIO, then C is simply thrown out. 
For example, suppose we used the following 
horizontal parameter settings: 
HORIZ RATIO:  0.5 
HORIZ SIZE:  10 
 The CIC da.do in our Spanish corpus has 62 
adherents, 46, or a fraction of 0.742, of which end 
in the character a (ada and ado fill the feminine 
and masculine past participle cells for the -ar verb 
inflection class).  If our Left Blocking search algo-
rithm reached the CIC da.do, it would be dis-
carded because while its adherent size is larger 
than HORIZ SIZE more than half of its adherents 
end with the same character.  Notice that this algo-
rithm does not explicitly follow leftward mor-
pheme boundary links.  The rationale for this be-
havior is that ada.ado will likely be explored inde-
pendently by a separate vertical path.  In future 
experiments we intend to investigate the effect of 
ensuring that the CIC to the left is explored by 
overtly placing the leftward CIC in the list of path 
CIC?s. 
Right Blocking 
 So far we have only described an algorithm to 
block paths where the correct morpheme boundary 
is to the left of the current hypothesis.  There are 
also CIC?s where a morpheme boundary should be 
moved to the right. The CIC cada.cado with seven 
adherents is one such. 
Accordingly, whenever we encounter a CIC, C, 
all of whose c-suffixes begin with the same charac-
ter (e.g. c in cada.cado) our algorithm poses the 
question, if we were considering the CIC to the 
right (e.g. ada.ado) would we have triggered Left 
Blocking?  If Left Blocking would not have been 
triggered then we throw C out.  In other words, we 
prefer the rightmost possible morpheme boundary, 
unless there is some reason to believe the mor-
pheme boundary should be to the left. 
Taking a closer look at cada.cado, the CIC to its 
right, ada.ado, has 46 adherents of which the char-
acter c ends the most, 7 or a fraction of 0.152.  If 
we were using a HORIZ RATIO of 0.5 as in the pre-
vious section, Left Blocking would not be trig-
gered from ada.ado and so Right Blocking is trig-
gered, throwing out cada.cado.  On the other hand, 
if we were considering blocking ada.ado, where 
both c-suffixes begin with a, the HORIZ RATIO pa-
rameter would need to be larger than 0.742 before 
right blocking would throw out ada.ado.   
Right Blocking Recursive 
 In addition to standard Right Blocking we ex-
plored recursively looking at the next most right 
neighbor of a CIC if the immediate right neighbor 
falls below the HORIZ SIZE threshold.  The ration-
ale behind this variant stems from CIC?s such as 
icada.icado with 4 adherents, crit, publ, ratif, and 
ub.  Since icada.icado?s immediate right neighbor 
cada.cado has only 7 adherents itself we may not 
want to base our blocking decision on so little data.  
Instead we consider the CIC ada.ado, discussed in 
the previous section, which has a large enough ad-
herent size that we might feel confident in our 
judgment.   
Full Horizontal Blocking 
The final version of the search we tried was to 
combine Left Blocking and Right Blocking Recur-
sive while constraining both to use the same values 
for the HORIZ RATIO and HORIZ SIZE parameters. 
5 Evaluation 
To evaluate the performance of the various base-
line search strategies, we first decided on a stan-
dard set of six inflection classes for Spanish: two 
for nouns, ?.s and ?.es, one for adjectives, 
a.as.o.os, and three for verbs, corresponding to the 
traditional -ar, -er, and -ir verb inflection classes.  
We call these six inflection classes our set of stan-
dard IC?s.  We make no claim as to the truth or 
completeness of the set of standard inflection 
classes we used in this evaluation.  The standard 
IC?s we compiled were simply some of the most 
common suffixes filling some of the most common 
morphosyntactic properties marked in Spanish. 
We then defined measures of recall, precision, 
and fragmentation over these standard IC?s (Figure 
3).  As defined, recall measures the fraction of 
unique suffixes in the standard IC?s that are found 
within those selected CIC?s that are subsets of 
some inflection class in the standard; precision 
measures the fraction of unique suffixes among all 
the selected CIC?s that are found within those se-
lected CIC?s that are subsets of an inflection class 
in the standard; and fragmentation measures re-
dundancy, specifically calculating the ratio of the 
number of selected CIC?s that are subsets of stan-
dard IC?s to the number of inflection classes in the 
standard.  High values for recall and precision are 
desirable, while a fragmentation of exactly 1 im-
plies that the number of usefully selected CIC?s is 
the same as the number of inflection classes in the 
standard. 
6 Results and Error Analysis 
For each of the search variants described in sec-
tion 4.2 we executed a by-hand search over the 
relevant parameters for those settings that optimize 
the F1 measure (the harmonic mean of recall and 
precision).  The best performing parameter settings 
are presented in Table 3 while quantitative results 
using these settings are plotted in Figure 4.   
Examining the performance of each algorithm 
(Figure 4) reveals that the simple Vertical only 
search achieves a high precision at the expense of a 
low recall measure.  The simple Vertical search 
also gives the smallest fragmentation, which, when 
combined with the high precision score, indicates a 
conservative algorithm that selects few CIC?s.  The 
parameter settings which achieve the highest F1 for 
Left Block alone and Right Block alone each pro-
duce much higher recall than the simple Vertical 
search.  Right Block Recursive increases precision 
significantly over simple Right Block and achieves 
U
U
sIC' standard 
sIC' standard 
sCIC' selected 
 of elements
 if  of elements
Recall
?
?
?
?
=
I
I
C
I
ICC
U
U
sCIC' selected 
sIC' standard 
sCIC' selected 
 of elements
 if  of elements
Precision
?
?
?
?
=
C
I
C
C
ICC
 
 {
sIC' standard
ionFragmentat sIC' standard 
sCIC' selected 
 if 1 
 if 0 ?
?
?
?
?
=
I
C
IC
IC
 
Figure 3: Three performance measures to   
optimize 
the highest F1 measure of any search variant.  
While Full Horizontal Block also performs well, 
sharing the values of HORIZ RATIO and HORIZ 
SIZE forced a compromise between Left Block and 
Right Block Recursive that did not significantly 
outperform either algorithm alone. 
Of the 83 unique suffixes in the hand compiled 
standard inflection classes, 21 did not share a c-
stem with any other c-suffix in the Spanish news-
wire corpus used for this evaluation?placing an 
upper limit on recall of 0.75 for the search algo-
rithms presented in this paper. 
Examining the parameter settings that yielded 
the highest F1 measure for each search variant 
(Table 3) is also enlightening.  Early experiments 
with Vertical only search clearly demonstrated that 
a TOP SIZE of two, or restricting the CIC?s permit-
ted to be selected to those with at least two adher-
ents, always resulted in better performance than 
other possible settings.  A TOP SIZE of one places 
no restriction on the adherent size of a CIC, ram-
pantly selecting CIC?s, such as the level 10 CIC 
given at the end of section 4.1, that consist of 
many c-suffixes that happen to validly concatenate 
onto a single c-stem?obliterating reasonable pre-
cision.  Higher settings for TOP SIZE induce a 
graceful degradation in recall.  Thus all experi-
ments reported here used a TOP SIZE of two. 
Beyond TOP SIZE the only parameters available 
to the basic Vertical algorithm are L1 SIZE and 
RATIO, which provide only crude means to halt the 
search of bad paths.  In particular, if a level one 
CIC, C, has more than L1 SIZE adherents, and has 
some parent which passes the RATIO cutoff, then 
some ancestor of C will be selected by the algo-
rithm as a good CIC.  Hence, the Vertical only al-
gorithm ensures search gets off on the right foot by 
using the highest values for the L1 SIZE and RATIO 
parameters of any algorithm variant.  Performance 
falls off quickly above L1 SIZE settings of 192, 
indicating that this parameter in this algorithm is 
sensitive to the size of the training corpus. 
In contrast, the horizontal blocking search algo-
rithms have additional parameters available to cull 
out bad search paths, and can hence afford to use 
lower (and more stable) values for L1 SIZE and 
RATIO.  Recall that the Left Blocking algorithm 
discards paths determined to be using a morpheme 
boundary too far to the right, while the Right 
Blocking algorithm discards paths using mor-
pheme boundaries too far to the left.  Notice that 
since, as reasoned in section 4.1, adherent count 
monotonically decreases as morpheme boundary 
links are followed to the left, if the L1 SIZE cutoff 
blocks a particular CIC, C, all CIC?s to the left of 
C will also be blocked.  From these facts it follows 
that a large L1 SIZE will reject some paths result-
ing from morpheme boundaries chosen too far to 
the left, which would otherwise have been pursued 
in the Left Blocking algorithm.  The Right Block-
ing algorithm, however, receives no such benefit, 
and achieves its best performance by maximizing 
recall with a small L1 SIZE. 
Examining the best performing parameter values 
for the Right Blocking Recursive algorithm reveals 
a curious behavior in which low values for L1 SIZE 
and RATIO allow a permissive vertical search while 
stringent values of HORIZ RATIO and, particularly, 
HORIZ SIZE constrain the search.  One explanation 
for these facts might be that following the mono-
tonically increasing chain of CIC adherent sizes 
along right horizontal links allows the algorithm to 
Figure 4: Recall, Precision, F1 and Fragmen-
tation Results for each search algorithm:     
Vertical, Left Blocking, Right Blocking,   
Right Blocking Recursive, and                     
Full Horizontal Blocking 
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
V
LB
RB
RBR
FHB
A
lg
o
ri
th
m
Recall/Precision/F-Measure
0 3 6 9 12 15 18 21 24 27 30
Fragmentation
Recall Precision
F-Measure Fragmentation
Table 3: Hand tuned optimal parameter set-
tings for each search algorithm:                       
Vertical, Left Blocking, Right Blocking,       
Right Blocking Recursive, and                      
Full Horizontal Blocking 
Algorithm TOP SIZE 
L1 
SIZE RATIO 
HORIZ 
RATIO 
HORIZ 
SIZE 
V 2 192 0.3   
LB 2 64 0.1 0.3 3 
RB 2 27 0.2 0.5 27 
RBR 2 27 0.05 0.5 243 
FHB 2 27 0.2 0.3 3 
 
make intelligent blocking decisions backed by suf-
ficient data. 
The best performing parameter values for the 
Full Horizontal Search are a compromise between 
the well performing values for the Left Blocking 
and those for the Right Blocking algorithms.  This 
parameter value compromise does not draw benefit 
from the recursion in the Right Block Recursive 
algorithm, but instead employs Right Block as a 
replacement for the relatively higher L1 SIZE pa-
rameter in the Left Blocking algorithm. 
It is also interesting to examine CIC?s selected 
by the search algorithms.  Table 4 lists all of the 
CIC?s selected by the conservative Vertical search 
algorithm together with a random sample of CIC?s 
selected by Right Blocking Recursive, the algo-
rithm which reached the highest F1 measure of any 
algorithm variant.   
Perhaps the most striking feature of Table 4 is 
the extent to which the CIC?s overlap.  Very few 
individual c-suffixes occur in only one CIC.  Of all 
the CIC?s in Table 4, only ?.s and a.as.o.os, both 
among the CIC?s selected by the Vertical algo-
rithm, represent complete inflection classes in the 
standard IC?s.  The remaining CIC?s are proper 
subsets of various verbal inflection classes.  The 
overlapping nature of the selected CIC?s suggests 
an additional step, which we do not investigate 
here, of conflating CIC?s into a fewer number of 
meta-CIC?s. 
The only verbal inflection class for which sub-
sets are able to pass the large L1 SIZE cutoff im-
posed by the Vertical search algorithm is -ar, the 
most frequent of the three major inflection classes 
in Spanish.  The Right Blocking Recursive algo-
rithm on the other hand identifies significant por-
tions of all three verbal inflection classes.  
The c-suffixes appearing in italics in Table 4 
correspond to no suffix found in any standard IC.  
These alien c-suffixes fall into two categories. 
1) The c-suffixes aciones, aci?n, and adores 
are noun forming derivational suffixes.   
2) The remaining c-suffixes were formed by 
choosing a morpheme boundary too far to 
the right.   
It is the second type of mistake that the Left 
Blocking search algorithm was specifically de-
signed to address.  Unfortunately na?vely combin-
ing the Right Blocking Recursive with the Left 
Blocking algorithm did not improve performance.  
We expect that by using separate horizontal pa-
Vertical
ar er ir 23 of 23 Selected CIC's
? ?.s
? a.aba.ada.adas.ado.ar.as
? a.aba.ada.ado.ando.ar
? a.aba.ada.ado.ar.ar?.en.?
a.aciones.aci?n .ada.adas.ar.aron
? a.ada.adas.ado.ar.ar?
? a.ada.adas.ar.aron.?
? a.ada.ado.ar.aron.ar?.?
? a.ada.ado.ar.ar?.ar?n.en.?
? a.ada.ado.ar.o.?
? a.ada.ados.ar.aron.?
? a.ado.ar.ara.aron.e.?
? a.ado.ar.aron.?
? a.an.ar.?
? a.as.o.os
? ? ? ? a.as
? aba.ado.ando.ar.aron.ar?
? aba.ado.ar.aron.ar?.en
? ada.ado.ados.ar.aron.?
? ada.ado.ando.ar.aron.?
? ada.ado.ar.ar?.o.?
? ada.ado.ar.en.o.?
? ado.ar.aron.ar?.ar?n.en
N A
dj Verbs
 
Table 4: All of the CIC?s selected by the conservative Vertical search algorithm (left), and a random 
sample of CIC?s selected by the algorithm with best F1 measure, Right Blocking Recursive (right).  For 
each CIC row, a dot is placed in the columns representing standard IC?s for which that CIC is a subset.  
The c-suffixes in italics are in no standard IC. 
Right Blocking Recursive
ar er ir 23 of 204 Selected CIC's
?.ba.n.ndo
? a.aba.ado.ados.ar.ar?.ar?n
a.aciones.aci?n .adas.ado.ar
? a.ada.adas.ado.ar.ar?
? a.adas.ado.an.ar
? a.ado.ados.ar.?
? a.ado.an.arse.?
? a.ado.aron.arse.?
? aba.ada.ado.ar.o.os
aciones.aci?n .ado.ados
aciones .ado.ados.ar?
aci?n .ado.an.e
? ada.adas.ado.ados.aron.?
? ada.ado.ados.ar.o
ado.adores .o
? ado.ados.arse.e
? ado.ar.aron.arse.ar?
do.dos.ndo.r.ron
? ? e.ida.ido
? emos.ido.?a.?an
? ida.ido.idos.ir.i?
? ido.iendo.ir
? ido.ir.ro
N A
dj Verbs
 
rameters for left blocking and for right blocking 
we could combine these two algorithms in a less 
constrained fashion that would result in better 
overall performance. 
7 Future Work  
We believe the heuristic search strategy de-
scribed in this paper can be significantly improved 
upon.  We plan to investigate search strategies for 
both the vertical and horizontal links in our CIC 
lattices.  We currently have plans to employ statis-
tical independence and correlation tests to adjacent 
CIC?s as a guide to search (Monson, 2004).  Other 
search criteria we are considering are information 
gain and minimum description length measures. 
There are also modifications to the search strat-
egy that may significantly improve performance.  
For example, it may be advantageous to actively 
follow horizontal morpheme boundary links, in-
stead of merely blocking paths, when a morpheme 
boundary error is discovered.  The next immediate 
step we will take is to scale our implementation to 
investigate performance changes as we increase 
the size of our Spanish corpus. 
The intention of this work is to produce a lan-
guage independent morphology induction algo-
rithm.  Hence, we plan to apply this work to a vari-
ety of languages, both well studied resource-rich 
languages as well as low-density languages of in-
terest to the AVENUE project. 
8 Acknowledgements 
The research reported in this paper was funded 
in part by NSF grant number IIS-0121631. 
References 
Jaime Carbonell, Katharina Probst, Erik Peterson, 
Christian Monson, Alon Lavie, Ralf Brown, and 
Lori Levin.  2002. Automatic Rule Learning for 
Resource-Limited MT. In Proceedings of the 5th 
Conference of the Association for Machine 
Translation in the Americas (AMTA-02). 
Andrew Carstairs-McCarthy. 1998. ?Paradigmatic 
Structure: Inflectional Paradigms and Morpho-
logical Classes.? The Handbook of Morphology. 
Eds. Andrew Spencer and Arnold M. Zwicky. 
Blackwell Publishers Inc., Massachusetts, USA, 
322-334. 
?ric Gaussier. 1999. Unsupervised learning of 
derivational morphology from inflectional lexi-
cons. In Proceedings of ACL ?99 Workshop: Un-
supervised Learning in Natural Language Proc-
essing. 
John Goldsmith. 2001. Unsupervised learning of 
the morphology of a natural language. Computa-
tional Linguistics, 27(2): 153-198. 
Margaret A. Hafer and Stephen F. Weiss. 1974. 
Word segmentation by letter successor varieties. 
Information Storage and Retrieval, 10:371-385. 
Zellig Harris. 1955. From phoneme to morpheme. 
Language, 31:190-222. Reprinted in Harris 
1970. 
Zellig Harris. 1967. Morpheme boundaries within 
words: Report on a computer test. Transforma-
tion and Discourse Analysis Papers 73, Depart-
ment of Linguistics, University of Pennsylvania. 
Reprinted in Harris 1970. 
Zellig Harris. 1970. Papers in Structural and 
Transformational Linguistics. D. Reidel, 
Dordrecht, Holland. 
Alon Lavie, Stephan Vogel, Lori Levin, Erik Pe-
terson, Katharina Probst, Ariadna Font Llitj?s, 
Rachel Reynolds, Jaime Carbonell, and Richard 
Cohen. 2003. Experiments with a Hindi-to-
English Transfer-based MT System under a Mis-
erly Data Scenario. ACM Transactions on Asian 
Language Information Processing (TALIP), to 
appear in 2(2). 
Christian Monson. 2004. A Framework for Unsu-
pervised Natural Language Morphology Induc-
tion.  In Proceedings of the Student Workshop at 
ACL-04. 
Katharina Probst, Lori Levin, Erik Peterson, Alon 
Lavie, and Jaime Carbonell. 2002. MT for Re-
source-Poor Languages using Elicitation-based 
Learning of Syntactic Transfer Rules. Machine 
Translation, Special Issue on Embedded MT, 
17(4): 245-270. 
Patrick Schone and Daniel Jurafsky. 2000. Knowl-
edge-free Induction of Morphology Using Latent 
Semantic Analysis. In Proceedings of the Fourth 
Conference on Computational Natural Language 
Learning and of the Second Learning Language 
in Logic Workshop, 67-72. 
Patrick Schone and Daniel Jurafsky. 2001. Knowl-
edge-free Induction of Inflectional Morpholo-
gies. In Proceedings of the North American 
Chapter of the Association of Computational 
Linguistics. 183-191. 
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis 
tools via robust projection across aligned cor-
pora. In Proceedings of the Human Language 
Technology Conference, 161-168. 
 
Instance-Based Question Answering:
A Data-Driven Approach
Lucian Vlad Lita
Carnegie Mellon University
llita@cs.cmu.edu
Jaime Carbonell
Carnegie Mellon University
jgc@cs.cmu.edu
Abstract
Anticipating the availability of large question-
answer datasets, we propose a principled, data-
driven Instance-Based approach to Question An-
swering. Most question answering systems incor-
porate three major steps: classify questions accord-
ing to answer types, formulate queries for document
retrieval, and extract actual answers. Under our ap-
proach, strategies for answering new questions are
directly learned from training data. We learn mod-
els of answer type, query content, and answer ex-
traction from clusters of similar questions. We view
the answer type as a distribution, rather than a class
in an ontology. In addition to query expansion, we
learn general content features from training data and
use them to enhance the queries. Finally, we treat
answer extraction as a binary classification problem
in which text snippets are labeled as correct or in-
correct answers. We present a basic implementation
of these concepts that achieves a good performance
on TREC test data.
1 Introduction
Ever since Question Answering (QA) emerged as
an active research field, the community has slowly
diversified question types, increased question com-
plexity, and refined evaluation metrics - as reflected
by the TREC QA track (Voorhees, 2003). Starting
from successful pipeline architectures (Moldovan et
al., 2000; Hovy et al, 2000), QA systems have re-
sponded to changes in the nature of the QA task by
incorporating knowledge resources (Hermjakob et
al., 2000; Hovy et al, 2002), handling additional
types of questions, employing complex reasoning
mechanisms (Moldovan et al, 2003; Nyberg et al,
2003), tapping into external data sources such as
the Web, encyclopedias, databases (Dumais et al,
2002; Xu et al, 2003), and merging multiple agents
and strategies into meta-systems (Chu-Carroll et al,
2003; Burger et al, 2002).
In recent years, learning components have started
to permeate Question Answering (Clarke et al,
2003; Ravichandran et al, 2003; Echihabi and
Marcu, 2003). Although the field is still domi-
nated by knowledge-intensive approaches, compo-
nents such as question classification, answer extrac-
tion, and answer verification are beginning to be ad-
dressed through statistical methods. At the same
time, research efforts in data acquisition promise to
deliver increasingly larger question-answer datasets
(Girju et al, 2003; Fleischman et al, 2003). More-
over, Question Answering is expanding to different
languages (Magnini et al, 2003) and domains other
than news stories (Zweigenbaum, 2003). These
trends suggest the need for principled, statisti-
cally based, easily re-trainable, language indepen-
dent QA systems that take full advantage of large
amounts of training data.
We propose an instance-based, data-driven ap-
proach to Question Answering. Instead of classify-
ing questions according to limited, predefined on-
tologies, we allow training data to shape the strate-
gies for answering new questions. Answer mod-
els, query content models, and extraction models are
also learned directly from training data. We present
a basic implementation of these concepts and eval-
uate the performance.
2 Motivation
Most existing Question Answering systems classify
new questions according to static ontologies. These
ontologies incorporate human knowledge about the
expected answer (e.g. date, location, person), an-
swer type granularity (e.g. date, year, century), and
very often semantic information about the question
type (e.g. birth date, discovery date, death date).
While effective to some degree, these ontologies
are still very small, and inconsistent. Considerable
manual effort is invested into building and maintain-
ing accurate ontologies even though answer types
are arguably not always disjoint and hierarchical in
nature (e.g. ?Where is the corpus callosum?? ex-
pects an answer that is both location and body part).
The most significant drawback is that ontologies
are not standard among systems, making individual
component evaluation very difficult and re-training
for new domains time-consuming.
2.1 Answer Modeling
The task of determining the answer type of a ques-
tion is usually considered a hard 1 decision prob-
lem: questions are classified according to an an-
swer ontology. The classification (location, per-
son?s name, etc) is usually made in the beginning
of the QA process and all subsequent efforts are
focused on finding answers of that particular type.
Several existing QA systems implement feedback
loops (Harabagiu et al, 2000) or full-fledged plan-
ning (Nyberg et al, 2003) to allow for potential an-
swer type re-classification.
However, most questions can have multiple an-
swer types as well as specific answer type distribu-
tions. The following questions can accommodate
answers of types: full date, year, and decade.
Question Answer
When did Glen lift off in Friendship7? Feb. 20, 1962
When did Glen join NASA? 1959
When did Glen have long hair? the fifties
However, it can be argued that date is the most
likely answer type to be observed for the first ques-
tion, year the most likely type for the second ques-
tion, and decade most likely for the third ques-
tion. In fact, although the three questions can be
answered by various temporal expressions, the dis-
tributions over these expressions are quite different.
Existing answer models do not usually account for
these distributions, even though there is a clear po-
tential for better answer extraction and more refined
answer scoring.
2.2 Document Retrieval
When faced with a new question, QA systems usu-
ally generate few, carefully expanded queries which
produce ranked lists of documents. The retrieval
step, which is very critical in the QA process,
does not take full advantage of context information.
However, similar questions with known answers do
share context information in the form of lexical and
structural features present in relevant documents.
For example all questions of the type ?When was
X born?? find their answers in documents which
often contain words such as ?native? or ?record?,
phrases such as ?gave birth to X?, and sometimes
even specific parse trees.
Most IR research in Question Answering is fo-
cused on improving query expansion and structur-
1the answer is classified into a single class instead of gener-
ating a probability distribution over answers
ing queries in order to take advantage of specific
document pre-processing. In addition to automatic
query expansion for QA (Yang et al, 2003), queries
are optimized to take advantage of expansion re-
sources and document sources. Very often, these
optimizations are performed offline, based on the
type of question being asked.
Several QA systems associate this type of infor-
mation with question ontologies: upon observing
questions of a certain type, specific lexical features
are sought in the retrieved documents. These fea-
tures are not always automatically learned in order
to be used in query generation. Moreover, systems
are highly dependent on specific ontologies and be-
come harder to re-train.
2.3 Answer Extraction
Given a set of relevant documents, the answer ex-
traction step consists of identifying snippets of text
or exact phrases that answer the question. Manual
approaches to answer extraction have been mod-
erately successful in the news domain. Regular
expressions, rule and pattern-based extraction are
among the most efficient techniques for information
extraction. However, because of the difficulty in ex-
tending them to additional types of questions, learn-
ing methods are becoming more prevalent.
Current systems (Ravichandran et al, 2003) al-
ready employ traditional information extraction and
machine learning for extracting answers from rel-
evant documents. Boundary detection techniques,
finite state transducers, and text passage classifica-
tion are a few methods that are usually applied to
this task.
The drawback shared by most statistical answer
extractors is their reliance on predefined ontologies.
They are often tailored to expected answer types and
require type-specific resources. Gazetteers, ency-
clopedias, and other resources are used to generate
type specific features.
3 Related Work
Current efforts in data acquisition for Question An-
swering are becoming more and more common.
(Girju et al, 2003) propose a supervised algorithm
for part-whole relations extraction. (Fleischman et
al., 2003) also propose a supervised algorithm that
uses part of speech patterns and a large corpus to ex-
tract semantic relations for Who-is type questions.
Such efforts promise to provide large and dense
datasets required by instance based approaches.
Several statistical approaches have proven to be
successful in answer extraction. The statistical
agent presented in (Chu-Carroll et al, 2003) uses
TestQuestion: WhendidJohnGlenstartworkingatNASA?
WhendidJayLenogetajobattheNBC?
WhendidColumbusarriveathisdestination?
?
Whendid<NNP+><VB>?at??
Whendid?SonybeginitsV AIOcampaign?
Whendid?T omRidgeinitiatetheterr oralertsystem?
?
Whendid<NNP+><SYNSETto_initiate>??
WhendidBeethovendie?
WhendidMuhammadlive?
?
Whendid<NNP><VB>?
WhendidtheRaiderswintheirlastgame?
WhendidEMNLP celebrateits5 th anniversary?
?
Whendid<NNP+>??
Whendiddinosaurswalktheearth?
Whendidpeoplediscoverfir e?
?
Whendid<NN><VB><NP>?
Whendid<NP>??
Figure 1: Neighboring questions are clustered according to features they share.
maximum entropy and models answer correctness
by introducing a hidden variable representing the
expected answer type. Large corpora such as the
Web can be mined for simple patterns (Ravichan-
dran et al, 2003) corresponding to individual ques-
tion types. These patterns are then applied to test
questions in order to extract answers. Other meth-
ods rely solely on answer redundancy (Dumais et
al., 2002): high performance retrieval engines and
large corpora contribute to the fact that the most re-
dundant entity is very often the correct answer.
Predictive annotation (Prager et al, 1999) is one
of the techniques that bring together corpus process-
ing and smarter queries. Twenty classes of objects
are identified and annotated in the corpus, and cor-
responding labels are used to enhance IR queries.
Along the same lines, (Agichtein et al, 2001) pro-
pose a method for learning query transformations
in order to improve answer retrieval. The method
involves learning phrase features for question clas-
sification. (Wen and Zhang, 2003) address the prob-
lem of query clustering based on semantic similar-
ity and analyze several applications such as query
re-formulation and index-term selection.
4 An Instance-Based Approach
This paper presents a data driven, instance-based
approach for Question Answering. We adopt the
view that strategies required in answering new ques-
tions can be directly learned from similar train-
ing examples (question-answer pairs). Consider
a multi-dimensional space, determined by features
extracted from training data. Each training question
is represented as a data point in this space. Features
can range from lexical n-grams to parse trees ele-
ments, depending on available processing.
Each test question is also projected onto the fea-
ture space. Its neighborhood consists of training
instances that share a number of features with the
new data point. Intuitively, each neighbor is similar
in some fashion to the new question. The obvious
next step would be to learn from the entire neigh-
borhood - similar to KNN classification. However,
due to the sparsity of the data and because different
groups of neighbors capture different aspects of the
test question, we choose to cluster the neighborhood
instead. Inside the neighborhood, we build individ-
ual clusters based on internal similarity. Figure 1
shows an example of neighborhood clustering. No-
tice that clusters may also have different granularity
- i.e. can share more or less features with the new
question.
Cluster1
Models
AnswerSet 1
Cluster2
Models
AnswerSet 2
Cluster3
Models
AnswerSet 3
Clusterk
Models
AnswerSet k
Neighborhood
Cluster2
Cluster3
Clusterk
NewQuestion
NET agging
POS
Parsing
Cluster1
Figure 2: The new question is projected onto the multi-
dimensional feature space. A set of neighborhood clus-
ters are identified and a model is dynamically built for
each of them. Each model is applied to the test question
in order to produce its own set of candidate answers.
By clustering the neighborhood, we set the stage
for supervised methods, provided the clusters are
sufficiently dense. The goal is to learn models that
explain individual clusters. A model explains the
data if it successfully answers questions from its
corresponding cluster. For each cluster, a mod-
els is constructed and tailored to the local data.
Models generating high confidence answers are ap-
plied to the new question to produce answer candi-
dates (Figure 2) Since the test question belongs to
multiple clusters, it benefits from different answer-
seeking strategies and different granularities.
Answering clusters of similar questions involves
several steps: learning the distribution of the
expected answer type, learning the structure and
content of queries, and learning how to extract the
answer. Although present in most systems, these
steps are often static, manually defined, or based on
limited resources (section 2). This paper proposes a
set of trainable, cluster-specific models:
1. the Answer Model Ai learns the cluster-specific
distribution of answer types.
2. the Query Content Model Ui is trained to enhance
the keyword-based queries with cluster-specific
content conducive to better document retrieval.
This model is orthogonal to query expansion.
3. the Extraction Model Ei is dynamically built
for answer candidate extraction, by classifying
snippets of text whether they contain a correct
answer or not.
AnswerModel
QueryContentModel
ExtractionModel
ClusterModels
Training
Samples
(Q, A)
Figure 3: Three cluster-specific components are learned
in order to better retrieve relevant documents, model the
expected answer, and then extract it from raw text. Local
question-answer pairs (Q,A) are used as training data.
These models are derived directly from cluster
data and collectively define a focused strategy for
finding answers to similar questions (Figure 3).
4.1 The Answer Model
Learning cluster-specific answer type distributions
is useful not only in terms of identifying answers
in running text but also in answer ranking. A prob-
abilistic approach has the advantage of postponing
answer type decisions from early in the QA process
until answer extraction or answer ranking. It also
has the advantage of allowing training data to shape
the expected structure of answers.
The answer modeling task consists of learning
specific answer type distributions for each cluster of
questions. Provided enough data, simple techniques
such as constructing finite state machines or learn-
ing regular expressions are sufficient. The principle
can also be applied to current answer ontologies by
replacing the hard classification with a distribution
over answer types.
For high-density clusters, the problem of learn-
ing the expected answer type is reduced to learn-
ing possible answer types and performing a reliable
frequency count. However, very often clusters are
sparse (e.g. are based on rare features) and a more
reliable method is required. k-nearest training data
points Q1..Qk can be used in order to estimate the
probability that the test question q will observe an
answer type ?j :
P (?j , q) = ? ?
k
?
i=0
P (?j |Qi) ? ?(q,Qi) (1)
where P (?j , Qi) is the probability of observing
an answer of type ?j when asking question Qi.
?(q,Qi) represents a distance function between q
and Qi, and ? is a normalizing factor over the set
of all viable answer types in the neighborhood of q.
4.2 The Query Content Model
Current Question Answering systems use IR in a
straight-forward fashion. Query terms are extracted
and then expanded using statistical and semantic
similarity measures. Documents are retrieved and
the top K are further processed. This approach de-
scribes the traditional IR task and does not take ad-
vantage of specific constraints, requirements, and
rich context available in the QA process.
The data-driven framework we propose takes ad-
vantage of knowledge available at retrieval time
and incorporates it to create better cluster-specific
queries. In addition to query expansion, the goal is
to learn content features: n-grams and paraphrases
(Hermjakob et al, 2002) which yield better queries
when added to simple keyword-based queries. The
Query Content Model is a cluster-specific collec-
tion of content features that generate the best docu-
ment set (Table 1).
Cluster: When did X start working for Y?
Simple Queries Query Content Model
X, Y ?X joined Y in?
X, Y start working ?X started working for Y?
X, Y ?start working? ?X was hired by Y?
... ?Y hired X?
X, Y ?job interview?
...
Table 1: Queries based only on X and Y question
terms may not be appropriate if the two entities share
a long history. A focused, cluster-specific content model
is likely to generate more precise queries.
For training, simple keyword-based queries are
run through a retrieval engine in order to produce
a set of potentially relevant documents. Features
(n-grams and paraphrases) are extracted and scored
based on their co-occurrence with the correct an-
swer. More specifically, consider a positive class:
documents which contain the correct answer, and a
negative class: documents which do not contain the
answer. We compute the average mutual informa-
tion I(C;Fi) between a class of a document, and
the absence or presence of a feature fi in the doc-
ument (McCallum and Nigam, 1998). We let C be
the class variable and Fi the feature variable:
I(C;Fi) = H(C) ? H(C|Fi)
=
?
c?C
?
fi?0,1
P (c, fi) log
P (c, fi)
P (c)P (fi)
where H(C) is the entropy of the class variable and
H(C|Fi) is the entropy of the class variable condi-
tioned on the feature variable. Features that best dis-
criminate passages containing correct answers from
those that do not, are selected as potential candi-
dates for enhancing keyword-based queries.
For each question-answer pair, we generate can-
didate queries by individually adding selected fea-
tures (e.g. table 1) to the expanded word-based
query. The resulting candidate queries are subse-
quently run through a retrieval engine and scored
based on the number of passages containing cor-
rect answers (precision). The content features found
in the top u candidate queries are included in the
Query Content Model.
The Content Model is cluster specific and not in-
stance specific. It does not replace traditional query
expansion - both methods can be applied simulta-
neously to the test questions: specific keywords are
the basis for traditional query expansion and clus-
ters of similar questions are the basis for learning
additional content conducive to better document re-
trieval. Through the Query Content Model we al-
low shared context to play a more significant role in
query generation.
4.3 The Extraction Model
During training, documents are retrieved for each
question cluster and a set of one-sentence passages
containing a minimum number of query terms is
selected. The passages are then transformed into
feature vectors to be used for classification. The
features consist of n-grams, paraphrases, distances
between keywords and potential answers, simple
statistics such as document and sentence length, part
of speech features such as required verbs etc. More
extensive sets of features can be found in informa-
tion extraction literature (Bikel et al, 1999).
Under our data-driven approach, answer extrac-
tion consists of deciding the correctness of candi-
date passages. The task is to build a model that
accepts snippets of text and decides whether they
contain a correct answer.
A classifier is trained for each question cluster.
When new question instances arrive, the already
trained cluster-specific models are applied to new,
relevant text snippets in order to test for correctness.
We will refer to the resulting classifier scores as an-
swer confidence scores.
5 Experiments
We present a basic implementation of the instance-
based approach. The resulting QA system is fully
automatically trained, without human intervention.
Instance-based approaches are known to require
large, dense training datasets which are currently
under development. Although still sparse, the
subset of all temporal questions from the TREC
9-12 (Voorhees, 2003) datasets is relatively dense
compared to the rest of the question space. This
makes it a good candidate for evaluating our
instance-based QA approach until larger and denser
datasets become available. It is also broad enough
to include different question structures and varying
degrees of difficulty and complexity such as:
? ?When did Beethoven die??
? ?How long is a quarter in an NBA game??
? ?What year did General Montgomery lead the Allies
to a victory over the Axis troops in North Africa??
The 296 temporal questions and their correspond-
ing answer patterns provided by NIST were used
in our experiments. The questions were processed
with a part of speech tagger (Brill, 1994) and a
parser (Collins, 1999).
The questions were clustered using template-
style frames that incorporate lexical items, parser
labels, and surface form flags (Figure 1). Consider
the following question and several of its corre-
sponding frames:
?When did Beethoven die??
when did <NNP> die
when did <NNP> <VB>
when did <NNP> <Q>
when did <NP> <Q>
when did <Q>
where <NNP>,<NP>,<VB>,<Q> denote:
proper noun, noun phrase, verb, and generic ques-
tion term sequence, respectively. Initially, frames
are generated exhaustively for each question. Each
frame that applies to more than three questions is
then selected to represent a specific cluster.
One hundred documents were retrieved
for each query through the Google API
(www.google.com/api). Documents containing
the full question, question number, references to
TREC, NIST, AQUAINT, Question Answering and
other similar problematic content were filtered out.
When building the Query Content Model
keyword-based queries were initially formulated
and expanded. From the retrieved documents a set
of content features (n-grams and paraphrases) were
selected through average mutual information. The
features were added to the simple queries and a
new set of documents was retrieved. The enhanced
queries were scored and the corresponding top 10 n-
grams/paraphrases were included in the Query Con-
tent Model. The maximum n-gram and paraphrase
size for these features was set to 6 words.
The Extraction Model uses a support vector ma-
chine (SVM) classifier (Joachims, 2002) with a lin-
ear kernel. The task of the classifier is to decide if
text snippets contain a correct answer. The SVM
was trained on features extracted from one-sentence
passages containing at least one keyword from the
original question. The features consist of: distance
between keywords and potential answers, keyword
density in a passage, simple statistics such as doc-
ument and sentence length, query type, lexical n-
grams (up to 6-grams), and paraphrases.
We performed experiments using leave-one-out
cross validation. The system was trained and tested
without any question filtering or manual input. Each
cluster produced an answer set with correspond-
ing scores. Top 5 answers for each instance were
considered by a mean reciprocal rank (MRR) met-
ric over all N questions: MRRN =
?N
i=0
1
ranki ,
where ranki refers to the first correct occurrence in
the top 5 answers for question i. While not the fo-
cus of this paper, answer clustering algorithms are
likely to further improve performance.
6 Results
The most important step in our instance-based ap-
proach is identifying clusters of questions. Figure
4 shows the question distribution in terms of num-
ber of clusters. For example: 30 questions belong
to exactly 3 clusters. The number of clusters cor-
responding to a question can be seen as a measure
of how common the question is - the more clusters
a question has, the more likely it is to have a dense
neighborhood.
The resulting MRR is 0.447 and 61.5% ques-
tions have correct answers among the first five pro-
posed answers. This translates into results consis-
tently above the sixth highest score at each TREC
9-12. Our results were compared directly to the top
performing systems? results on the same temporal
2 3 4 5 6 7 8 9 larger
0
10
20
30
40
50
60
70
80
 Question Distribution With Number of Clusters
# clusters
# 
qu
es
tio
ns
(avg) 
Figure 4: Question distribution - each bar shows the
number of questions that belong to exactly c clusters.
1 2 3 4 5 6 7 8
0
10
20
30
40
50
60
70
80
Cluster Contribution to Top 10 Answers
# clusters
# 
qu
es
tio
ns
Figure 5: Number of clusters that contribute with cor-
rect answers to the final answer set - only the top 10 an-
swers were considered for each question.
question test set.
Figure 5 shows the degree to which clusters pro-
duce correct answers to test questions. Very often,
more than one cluster contributes to the final answer
set, which suggests that there is a benefit in cluster-
ing the neighborhood according to different similar-
ity features and granularity.
It is not surprising that cluster size is not cor-
related with performance (Figure 6). The overall
strategy learned from the cluster ?When did <NP>
die?? corresponds to an MRR of 0.79, while the
strategy learned from cluster ?How <Q>?? corre-
sponds to an MRR of 0.13. Even if the two clusters
generate strategies with radically different perfor-
mance, they have the same size - 10 questions are
covered by each cluster.
Figure 7 shows that performance is correlated
with answer confidence scores. The higher the con-
fidence threshold the higher the precision (MRR)
of the predicted answers. When small, unstable
clusters are ignored, the predicted MRR improves
considerably. Small clusters tend to produce unsta-
0 20 40 60 80 100 120
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Performance And Cluster Size
cluster size
M
R
R
Figure 6: Since training data is not uniformly distributed
in the feature space, cluster size is not well correlated
with performance. A specific cardinality may represent a
small and dense part cluster, or a large and sparse cluster.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Performance And Confidence Thresholds
confidence threshold
M
R
R
Cardinality 2+
Cardinality 3+
Cardinality 4+
Cardinality 5+
Figure 7: MRR of predicted answers varies with answer
confidence thresholds. There is a tradeoff between confi-
dence threshold and MRR . The curves represent differ-
ent thresholds for minimum cluster size.
ble strategies and have extremely low performance.
Often times structurally different but semantically
equivalent clusters have a higher cardinality and
much better performance. For example, the cluster
?What year did <NP> die?? has cardinality 2 and
a corresponding MRR of zero. However, as seen
previously, the cluster ?When did <NP> die?? has
cardinality 10 and a corresponding MRR of 0.79.
Table 2 presents an intuitive cluster and the top n-
grams and paraphrases with most information con-
tent. Each feature has also a corresponding average
mutual information score. These particular content
features are intuitive and highly indicative of a cor-
rect answer. However, in sparse clusters, the con-
tent features have less information content and are
more vague. For example, the very sparse cluster
?When was <Q>?? yields content features such as
?April?, ?May?, ?in the spring of?, ?back in? which
only suggest broad temporal expressions.
Cluster: When did <QTERM> die?
N-grams Paraphrases
0.81 his death in 0.80 <Q> died in
0.78 died on 0.78 <Q> died
0.77 died in 0.68 <Q> died on
0.75 death in 0.58 <Q> died at
0.73 of death 0.38 <Q> , who died
0.69 to his death 0.38 <Q> dies
0.66 died 0.38 <Q> died at the age of
0.63 , born on 0.38 <Q> , born
0.63 date of death 0.35 <Q> ?s death on
Table 2: Query Content Model: learning n-grams and
paraphrases for class ?When did <NP> die??, where
<Q> refers to a phrase in the original question.
7 Conclusions
This paper presents an principled, statistically
based, instance-based approach to Question An-
swering. Strategies and models required for answer-
ing new questions are directly learned from training
data. Since training requires very little human ef-
fort, relevant context, high information query con-
tent, and extraction are constantly improved with
the addition of more question-answer pairs.
Training data is a critical resource for this ap-
proach - clusters with very few data points are not
likely to generate accurate models. However, re-
search efforts involving data acquisition are promis-
ing to deliver larger datasets in the near future and
solve this problem. We present an implementation
of the instance-based QA approach and we eval-
uate it on temporal questions. The dataset is of
reasonable size and complexity, and is sufficiently
dense for applying instance-based methods. We per-
formed leave-one-out cross validation experiments
and obtained an overall mean reciprocal rank of
0.447. 61.5% of questions obtained correct answers
among the top five which is equivalent to a score in
the top six TREC systems on the same test set.
The experiments show that strategies derived
from very small clusters are noisy and unstable.
When larger clusters are involved, answer confi-
dence becomes correlated with higher predictive
performance. Moreover, when ignoring sparse data,
answering strategies tend to be more stable. This
supports the need for more training data as means to
improve the overall performance of the data driven,
instance based approach to question answering.
8 Current & Future Work
Data is the single most important resource for
instance-based approaches. Currently we are ex-
ploring large-scale data acquisition methods that
can provide the necessary training data density for
most question types, as well as the use of trivia
questions in the training process.
Our data-driven approach to Question Answering
has the advantage of incorporating learning com-
ponents. It is very easy to train and makes use of
very few resources. This property suggests that lit-
tle effort is required to re-train the system for dif-
ferent domains as well as other languages. We plan
to apply instance-based QA to European languages
and test this hypothesis using training data acquired
through unsupervised means.
More effort is required in order to better integrate
the cluster-specific models. Strategy overlap analy-
sis and refinement of local optimization criteria has
the potential to improve overall performance under
time constraints.
References
E. Agichtein, S. Lawrence, and L. Gravano. 2001.
Learning search engine specific query transfor-
mations for question answering. WWW.
D. Bikel, R. Schwartz, and R. Weischedel. 1999.
An algorithm that learns what?s in a name. Ma-
chine Learning.
E. Brill. 1994. Some advances in rule-based part of
speech tagging. AAAI.
J. Burger, L. Ferro, W. Greiff, J. Henderson,
M. Light, and S. Mardis. 2002. Mitre?s qanda
at trec-11. TREC.
J. Chu-Carroll, K. Czuba, J. Prager, and A. Itty-
cheriah. 2003. In question answering, two heads
are better than one. HLT-NAACL.
C. Clarke, G. Cormack, G. Kemkes, M. Laszlo,
T. Lynam, E. Terra, and P. Tilker. 2003. Statis-
tical selection of exact answers. TREC.
M. Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. Disertation.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002. Web question answering: Is more always
better? SIGIR.
A. Echihabi and D. Marcu. 2003. A noisy channel
approach to question answering. ACL.
M. Fleischman, E. Hovy, and A. Echihabi. 2003.
Offline strategies for online question answering:
Answering questions before they are asked. ACL.
R. Girju, D. Moldovan, and A. Badulescu. 2003.
Learning semantic constraints for the automatic
discovery of part-whole relations. HLT-NAACL.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
R. Bunescu, R. Girju, V. Rus, and P. Morarescu.
2000. Falcon: Boosting knowledge for answer
engines. TREC.
U. Hermjakob, E. Hovy, and C. Lin. 2000.
Knowledge-based question answering. TREC.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformu-
lation resource and web exploitation for question
answering. TREC.
E. Hovy, L. Gerber, U. Hermjakob, M. Junk, and
C.Y. Lin. 2000. Question answering in webclo-
pedia. TREC.
E. Hovy, U. Hermjakob, C. Lin, and D. Ravichan-
dran. 2002. Using knowledge to facilitate factoid
answer pinpointing. COLING.
T. Joachims. 2002. Learning to classify text using
support vector machines. Disertation.
B. Magnini, S. Romagnoli, A. Vallin, J. Herrera,
A. Penas, V. Peiado, F. Verdejo, and M. de Rijke.
2003. The multiple language question answering
track at clef 2003. CLEF.
A. McCallum and K. Nigam. 1998. A comparison
of event models for naive bayes text classifica-
tion. AAAI, Workshop on Learning for Text Cate-
gorization.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The
structure and performance of an open-domain
question answering system. ACL.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. ACL.
E. Nyberg, T. Mitamura, J. Callan, J. Carbonell,
R. Frederking, K. Collins-Thompson, L. Hiyaku-
moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,
A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and
B. Vand Durme. 2003. A multi strategy approach
with dynamic planning. TREC.
J. Prager, D. Radev, E. Brown, A. Coden, and
V. Samn. 1999. The use of predictive annotation
for question answering in trec8. TREC.
D. Ravichandran, A. Ittycheriah, and S. Roukos.
2003. Automatic derivation of surface text pat-
terns for a maximum entropy based question an-
swering system. HLT-NAACL.
E.M. Voorhees. 2003. Overview of the trec 2003
question answering track. TREC.
J.R. Wen and H.J. Zhang. 2003. Query clustering
in the web context. IR and Clustering.
J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec
2003 qa at bbn: Answering definitional ques-
tions. TREC.
H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003.
Structured use of external knowledge for event-
based open domain question answering. SIGIR.
P. Zweigenbaum. 2003. Question answering in
biomedicine. EACL.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 87?90,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Symmetric Probabilistic Alignment
Ralf D. Brown Jae Dong Kim Peter J. Jansen Jaime G. Carbonell
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{ralf,jdkim,pjj,jgc}@cs.cmu.edu
Abstract
We recently decided to develop a new
alignment algorithm for the purpose
of improving our Example-Based Ma-
chine Translation (EBMT) system?s per-
formance, since subsentential alignment is
critical in locating the correct translation
for a matched fragment of the input. Un-
like most algorithms in the literature, this
new Symmetric Probabilistic Alignment
(SPA) algorithm treats the source and tar-
get languages in a symmetric fashion.
In this short paper, we outline our basic
algorithm and some extensions for using
context and positional information, and
compare its alignment accuracy on the
Romanian-English data for the shared task
with IBM Model 4 and the reported results
from the prior workshop.
1 Symmetric Probabilistic Alignment
(SPA)
In subsentential alignment, mappings are produced
from words or phrases in the source language sen-
tence and those words or phrases in the target lan-
guage sentence that best express their meaning.
An alignment algorithm takes as input a bilingual
corpus consisting of corresponding sentence pairs
and strives to find the best possible alignment in the
second for selected n-grams (sequences of n words)
in the first language. The alignments are based on
a number of factors, including a bilingual dictionary
(preferably a probabilistic one), the position of the
words, invariants such as numbers and punctuation,
and so forth.
For our baseline algorithm, we make the follow-
ing simplifying assumptions, each of which we in-
tend to relax in future work, and the last of which
has already been partially relaxed:
1. A fixed bilingual probabilistic dictionary is
available.
2. Fragments (word sequences) are translated in-
dependently of surrounding context.
3. Contiguous fragments of source language text
are translated into contiguous fragments in the
target language text.
Unlike the work of (Marcu and Wong, 2002),
our alignment algorithm is not generative and does
not use the idea of a bag of concepts from which
the phrases in the sentence pair arise. It is, rather,
intended to find the corresponding target-language
phrase given a specific source-language phrase of in-
terest, as required by our EBMT system after find-
ing a match between the input and the training data
(Brown, 2004).
1.1 Baseline Algorithm
Our baseline algorithm is based on maximizing the
probability of bi-directional translations of individ-
ual words between a selected n-gram in the source
language and every possible n-gram in the corre-
sponding paired target language sentence. No posi-
tional preference assumptions are made, nor are any
length preservation assumptions made. That is, an
n-gram may translate to an m-gram, for any val-
ues of n or m bounded by the source and target
sentence lengths, respectively. Finally a smooth-
ing factor is used to avoid singularities (i.e. avoid-
ing zero-probabilities for unknown words, or words
never translated before in a way consistent with the
dictionary).
87
Given a source-language sentence
S1 : s0, s1, ..., si, ..., si+k, ..., sn (1)
in the bilingual corpus, where si, ..., si+k is a phrase
of interest, and the corresponding target language
sentence S2 is
S2 : t0, t1, ..., tj , ..., tj+l, ..., tm (2)
the values of j and l are to be determined.
Then the segment we try to obtain is the target
fragment F?T with the highest probability of all pos-
sible fragments of S2 to be a mutual translation with
the given source fragment, or
F?T = argmax{FT } (p(si, ..., si+k ? tj, ..., tj+l))
(3)
All possible segments can be checked in O(m2)
time, where m is the target language length, because
we will check m 1-word segments, m? 1 two-word
segments, and so on. If we bound the target language
n-grams to a maximal length k, then the complexity
is linear, i.e. O(km).
The score of the best possible alignment is com-
puted as follows: Let LT be the Target Language
Vocabulary, s a source word, ti be target segment
words, and V = {ti ? {LT }|i ? 1} the translation
word set of s,
We define the translation relation probability
p(Tr(s) ? {t0, t1, ..., tk}) as follows:
1. p(Tr(s) ? {t0, t1, ..., tk}) = max(p(ti|s))
for all ti ? {t0, t1, ..., tk} when {ti|ti ?
{t0, t1, ..., tk}} is not empty.
2. p(Tr(s) ? {t0, t1, ..., tk}) = 0 otherwise.
Then the score of the best alignment is
SF?T = max{FT }
SFT (4)
where the score can be written as two components
SFT = P1 ? P2 (5)
which can be further specified as
P1 =
( k
?
m=0
max (p (Tr(si+m) ? {tj...j+l}) , ?)
)
1
k+1
(6)
P2 =
( l
?
n=0
max (p (Tr(tj+n) ? {si...i+k}) , ?)
)
1
l+1
(7)
where ? is a very small probability used as a smooth-
ing value.
1.2 Length Penalty
The ratio between source and target segment (n-
gram) lengths should be comparable to the ratio be-
tween the lengths of the source and target sentences,
though certainly variation is possible. Therefore, we
add a penalty function to the alignment probability
that increases with the discrepancy between the two
ratios.
Let the length of the source language segment be
i and the length of a target language segment under
consideration be j. Given a source language sen-
tence length of n (in the corpus sentence containing
the fragment) and its corresponding target language
length of m. The expected target segment length is
then given by j? = i? mn . Further defining an allow-
able difference AD, our implementation calculates
the length penalty LP as follows, with the value of
the exponent determined empirically:
LPFT = min
?
?
(
|j ? j?|
AD
)4
, 1
?
? (8)
The score for a segment including the penalty func-
tion is then:
SFT ? SFT ? (1? LPFT ) (9)
Note that, as intended, the score is forced to 0 when
the length difference |j ? j?| > AD.
1.3 Distortion Penalty
For closely-related language pairs which tend to
have similar word orders, we introduce a distortion
penalty to penalize the alignment score of any can-
didate target fragment which is out of the expected
position range. First, we calculate CE , the expected
center of the candidate target fragment using CFS ,
the center of the source fragment and the ratio of
target- to source-sentence length.
CE = CFS ?
m
n (10)
88
Then we calculate an allowed distance limit of the
center Dallowed using a constant distance limit value
DL and the ratio of actual target sentence length to
average target sentence length.
Dallowed = DL ?
m
maverage
(11)
Let Dactual be the actual distance difference be-
tween the candidate target fragment?s center and the
expected center, and set
SFT ?
?
?
?
0, ifDactual ? Dallowed
SFT
(Dactual?Dallowed+1)2 , otherwise
(12)
Furthermore, we think that we can apply this
penalty to language pairs which have lower word-
order similarities than e.g. French-English. Because
there might exist certain positional relationships be-
tween such language pairs, if we can calculate the
expected position using each language?s sentence
structure, we can apply a distortion penalty to the
candidate alignments.
1.4 Anchor Context
If the adjacent words of the source fragment and the
candidate target fragment are translations of each
other, we expect that this alignment is more likely
to be correct. We boost SFT with the anchor context
alignment score SACp ,
SACp = P (si?1 ? tj?1) ? P (si+k ? tj+l) (13)
SFT ? (SFT )? ? (SACp)1?? (14)
Empirically, we found this combination gives the
best score for French-English when ? = 0.6 and
for Romanian-English when ? = 0.8, and leads to
better results than the similar formula
SFT ? ? ? SFT + (1? ?) ? SACp (15)
2 Experimental Design
In previous work (Kim et al, 2005), we tested our
alignment method on a set of French-English sen-
tence pairs taken from the Canadian Hansard corpus
and on a set of English-Chinese sentence pairs, and
compared the results to human alignments. For the
present workshop, we chose to use the Romanian-
English data which had been made available.
Due to a lack of time prior to the period of the
shared task, we merely re-used the parameters which
had been tuned for French-English, rather than tun-
ing the alignment parameters specifically for the de-
velopment data.
SPA was run under three experimental conditions.
In the first, labeled ?SPA (c)? in Tables 1 and 2, SPA
was instructed to examine only contiguous target
phrases as potential alignments for a given source
phrase. In the second, labeled ?SPA (n)?, a noncon-
tiguous target algnment consisting of two contigu-
ous segments with a gap between them was permit-
ted in addition to contiguous target algnments. The
third condition (?SPA (h)?) examined the impact of
a small amount of manual alignment information on
the selection of contiguous alignments. Unlike the
first two conditions, the presence of additional data
beyond the training corpus forces SPA(h) into the
Unlimited Resources track.
We had a native Romanian speaker hand-align
204 sentence pairs from the training corpus, and
extracted 732 distinct translation pairs from those
alignments, of which 450 were already present in
the automatically-generated dictionaries. The new
translation pairs were added to the dictionaries for
the SPA(h) condition and the translation probabili-
ties for the existing pairs were increased to reflect
the increased confidence in their correctness. Had
more time been available, we would have investi-
gated more sophisticated means of integrating the
human knowledge into the translation dictionaries.
3 Results and Conclusions
Table 1 compares the performance of SPA on what
is now the development data against the submissions
with the best AER values reported by (Mihalcea
and Pedersen, 2003) for the participants in the 2003
workshop, including CMU, MITRE, RALI, Univer-
sity of Alberta, and XRCE 1. As SPA generates only
SURE alignments, the values in Table 1 are SURE
alignments under the NO-NULL-Align scoring con-
dition for all systems except Fourday, which did not
generate SURE alignments.
Despite the fact that SPA was designed specifi-
cally for phrase-to-phrase alignments rather than the
1Citations for individual participants? papers have been
omitted for space reasons; all appear in the same proceedings.
89
Method Prec% Rec% F1% AER
SPA (c) 64.47 62.68 63.56 36.44
SPA (n) 64.38 62.70 63.53 36.47
SPA (h) 64.61 62.55 63.56 36.44
Fourday 52.83 42.86 47.33 52.67
UMD.RE.2 58.29 49.99 53.82 46.61
BiBr 70.65 55.75 62.32 41.39
Ralign 92.00 45.06 60.49 35.24
XRCEnolm 82.65 62.44 71.14 28.86
Table 1: Romanian-English alignment results (De-
velopment Set, NO-NULL-Align)
word-to-word alignments needed for the shared task
and was not tuned for this corpus, its performance is
competitive with the best of the systems previously
used for the shared task. We thus decided to submit
runs for the official 2005 evaluation, whose resulting
scores are shown in Table 2.
On the development set, noncontiguous align-
ments resulted in slightly lower precision than con-
tiguous alignments, which was not unexpected, but
recall does not increase enough to improve F1 or
AER. The modified dictionaries improved preci-
sion slightly, as anticipated, but lowered recall suffi-
ciently to have no net effect on F1 or AER.
The evaluation set proved to be very similar in dif-
ficulty to the development data, resulting in scores
that were very close to those achieved on the dev-test
set. Noncontiguous alignments again proved to have
a very small negative effect on AER resulting from
reduced precision, but this time the altered dictionar-
ies for SPA(h) resulted in a substantial reduction in
recall, considerably harming overall performance.
After the shared task was complete, we performed
some tuning of the alignment parameters for the
Romanian-English development test set, and found
that the French-English-tuned parameters were close
to optimal in performance. The AER on the develop-
ment test set for the SPA(c) contiguous alignments
condition decreased from 36.44% to 36.11% after
the re-tuning.
4 Future Work
Enhancements in the extraction of word-to-word
alignments from what is fundamentally a phrase-to-
phrase alignment algorithm could probably further
Method Prec% Recall% F1% AER%
SPA (c) 64.96 61.34 63.10 36.90
SPA (n) 64.91 61.34 63.07 36.93
SPA (h) 64.60 60.54 62.50 37.50
Table 2: Evaluation results (NO-NULL-Align)
improve results on the Romanian-English data. We
also intend to investigate principled, seamless inte-
gration of manual alignments and dictionaries with
probabilistic ones, since the ad hoc method proved
detrimental. Finally, a more detailed performance
analysis is in order, to determine whether the close
balance of precision and recall is inherent in the bidi-
rectionality of the algorithm or merely coincidence.
5 Acknowledgements
We would like to thank Lucian Vlad Lita for provid-
ing manual alignments.
References
Ralf D. Brown. 2004. A Modified Burrows-Wheeler
Transform for Highly-Scalable Example-Based Trans-
lation. In Machine Translation: From Real Users
to Research, Proceedings of the 6th Conference of
the Association for Machine Translation in the Amer-
icas (AMTA-2004), volume 3265 of Lecture Notes
in Artificial Intelligence, pages 27?36. Springer Ver-
lag, September-October. http://www.cs.cmu.-
edu/?ralf/papers.html.
Jae Dong Kim, Ralf D. Brown, Peter J. Jansen, and
Jaime G. Carbonell. 2005. Symmetric Probabilistic
Alignment for Example-Based Translation. In Pro-
ceedings of the Tenth Workshop of the European Asso-
cation for Machine Translation (EAMT-05), May. (to
appear).
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Machine
Translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), July. http://www.isi.edu/-
?marcu/papers.html.
Rada Mihalcea and Ted Pedersen. 2003. An Evalua-
tion Exercise for Word Alignment. In Proceedings of
the HLT-NAACL 2003 Workshop: Building and Using
Parallel Texts: Data Driven Machine Translation and
Beyond, pages 1?10. Association for Computational
Linguistics, May.
90
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 117?125,
Prague, June 2007. c?2007 Association for Computational Linguistics
ParaMor: Minimally Supervised Induction of Paradigm  
 Structure and Morphological Analysis 
 
Christian Monson, Jaime Carbonell, Alon Lavie, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA, USA 15213 
{cmonson, alavie+, jgc+, lsl+}@cs.cmu.edu 
Abstract 
Paradigms provide an inherent 
organizational structure to natural language 
morphology. ParaMor, our minimally 
supervised morphology induction 
algorithm, retrusses the word forms of raw 
text corpora back onto their paradigmatic 
skeletons; performing on par with state-of-
the-art minimally supervised morphology 
induction algorithms at morphological 
analysis of English and German. ParaMor 
consists of two phases. Our algorithm first 
constructs sets of affixes closely mimicking 
the paradigms of a language. And with 
these structures in hand, ParaMor then 
annotates word forms with morpheme 
boundaries. To set ParaMor?s few free 
parameters we analyze a training corpus of 
Spanish. Without adjusting parameters, we 
induce the morphological structure of 
English and German. Adopting the 
evaluation methodology of Morpho 
Challenge 2007 (Kurimo et al, 2007), we 
compare ParaMor?s morphological 
analyses with Morfessor (Creutz, 2006), a 
modern minimally supervised morphology 
induction system. ParaMor consistently 
achieves competitive F1 measures. 
1 Introduction 
Words in natural language (NL) have internal 
structure. Morphological processes derive new lex-
emes from old ones or inflect the surface form of 
lexemes to mark morphosyntactic features such as 
tense, number, person, etc. This paper address 
minimally supervised induction of productive natu-
ral language morphology from text. Minimally su-
pervised induction of morphology interests us both 
for practical and theoretical reasons. In linguistic 
theory, the morpheme is often defined as the 
smallest unit of language which conveys meaning. 
And yet, without annotating for meaning, recent 
work on minimally supervised morphology induc-
tion from written corpora has met with some suc-
cess (Creutz, 2006). We are curious how far this 
program can be pushed. From a practical perspec-
tive, minimally supervised morphology induction 
would help create morphological analysis systems 
for languages outside the traditional scope of NLP. 
However, to develop our method we induce the 
morphological structure of three well-understood 
languages, English, German, and Spanish. 
1.1 Inherent Structure in NL Morphology 
The approach we have taken to induce morpho-
logical structure has explicit roots in linguistic the-
ory. Cross-linguistically, natural language organ-
izes inflectional morphology into paradigms and 
inflection classes. A paradigm is a set of mutually 
exclusive operations that can be performed on a 
word form. Each mutually exclusive morphologi-
cal operation in a paradigm marks a lexeme for 
some set or cell of morphosyntactic features. An 
inflection class, meanwhile, specifies the proce-
dural details that a particular set of adherent lex-
emes follow to realize the surface form filling each 
paradigm cell. Each lexeme in a language adheres 
to a single inflection class for each paradigm the 
lexeme realizes. The lexemes belonging to an in-
flection class may have no relationship binding 
them together beyond an arbitrary morphological 
stipulation that they adhere to the same inflection 
class. But for this paper, an inflection class may 
117
also refer to a set of lexemes that inflect similarly 
for phonological or orthographic reasons. Working 
with text we intentionally blur phonology and or-
thography. 
A simple example will help illustrate paradigms, 
inflection classes, and the mutual exclusivity of 
cells. As shown in Table 1, all English verbs 
belong to a single common paradigm of five cells: 
One cell marks a verb for the morphosyntactic 
feature values present tense 3rd person, as in eats; 
another cell marks past tense, as in ate; a third cell 
holds a surface form typically used to mark 
progressive aspect, eating; a fourth produces a 
passive participle, eaten; and finally there is the 
unmarked cell, in this example eat.  
Aside from inflection classes each containing 
only a few irregular lexemes, such as that 
containing eat, there are no English verbal 
inflection classes that arbitrarily differentiate 
lexemes on purely morphological grounds. There 
are, however, several inflection classes that realize 
surface forms only for verbs with particular 
phonology or orthography. The ?silent-e? inflection 
class is one such. To adhere to the ?silent-e? 
inflection class a lexeme must fill the unmarked 
paradigm cell with a form that ends in an unspoken 
character e, as in dance. The other paradigm cells 
in the ?silent-e? inflection class are filled by 
applying orthographic rules such as:  
Progressive Aspect Cell ? replace the final e of 
the unmarked form with the string ing, 
dance  dancing  
Past Cell ? substitute ed, dance  danced  
Paradigm cells are mutually exclusive. In the Eng-
lish verbal paradigm, although English speakers 
can express progressive past actions with a 
grammatical construction, viz. was eating, there is 
no surface form of the lexeme eat that 
simultaneously fills both the progressive and the 
past cells of the verbal paradigm, *ateing. 
1.2 ParaMor 
Paradigms and inflection classes, the inherent 
structure of natural language morphology, form the 
basis of ParaMor, our minimally supervised 
morphological induction algorithm. In ParaMor?s 
first phase, we find sets of mutually exclusive 
strings which closely mirror the inflection classes 
of a language?although ParaMor does not 
differentiate between syncretic word forms of the 
same lexeme filling different paradigm cells, such 
as ed-suffixed forms which can fill either the past 
or the passive cells of English verbs. In ParaMor?s 
second phase we employ the structured knowledge 
contained within the discovered inflection classes 
to segment word forms into morpheme-like pieces.  
Languages employ a variety of morphological 
processes to arrive at grammatical word forms?
processes including suffix-, prefix-, and infixation, 
reduplication, and template filling. Furthermore, 
the application of word forming processes often 
triggers phonological (or orthographic) change, 
such a as the dropped final e of the ?silent-e? 
inflection class, see Table 1. Despite the wide 
range of morphological processes and their 
complicating concomitant phonology, a large caste 
of inflection classes, and hence paradigms, can be 
represented as mutually exclusive substring 
substitutions. In the ?silent-e? inflection class, for 
example, the word-final strings e.ed.es.ing can be 
substituted for one another to produce the surface 
forms that fill the paradigm cells of lexemes 
belonging to this inflection class. In this paper we 
focus on identifying word final suffix morphology. 
While we focus on suffixes, the methods we 
employ can be straightforwardly generalized to 
prefixes and ongoing work seeks to model 
sequences of concatenative morphemes. 
Inducing the morphology of a language from a 
naturally occurring text corpus is challenging. In 
languages with a rich morphological structure, sur-
face forms filling particular cells of an inflection 
class may be relatively rare. In the Spanish news-
wire text over which we developed ParaMor there 
are 50,000 unique types. Among these types, in-
Table 1: The English verbal paradigm, left col-
umn, and two inflection classes of the verbal 
paradigm. The verb eat fills the cells of its in-
flection class with the five surface forms 
shown in the second column. Verbs belonging 
to the ?silent-e? inflection class inflect follow-
ing the pattern of the third column. 
            Inflection Class Paradigm 
Cells ?eat? ?silent-e? 
Unmarked eat dance, erase, ? 
Present, 3rd eats dances, erases, ? 
Past Tense ate danced, erased, ? 
Progressive eating dancing, erasing, ? 
Passive eaten danced, erased, ? 
 
118
stances of first and second person verb forms are 
few. The suffix imos which fills the first person 
plural indicative present cell for the ir verbal in-
flection class of Spanish occurs on only 77 unique 
lexemes. And yet we aim to identify candidate in-
flection classes which closely model the true in-
flection classes of a language, covering as many 
inflectional paradigm cells as possible. 
Fortunately, we can leverage the paradigm struc-
ture of natural language morphology itself to retain 
many inflections which, because of data sparse-
ness, might be missed if considered in isolation. 
ParaMor begins with a recall-centric search for 
partial candidate inflection classes. Many of the 
candidates which result from this initial search are 
incorrect. But intermingled with the false positives 
are candidates which collectively model significant 
fractions of true inflection classes. Hence, Pa-
raMor?s next step is to cluster the initial partial 
candidate inflection classes into larger groups. This 
clustering effectively uses the larger correct initial 
candidates as nuclei to which smaller correct can-
didates accrete. With as many initial true candi-
dates as possible safely corralled with other candi-
dates covering the same inflection class, ParaMor 
completes the paradigm discovery phase by dis-
carding the large number of erroneous initially se-
lected candidate inflection classes. Finally, with a 
strong grasp on the paradigm structure, ParaMor 
straightforwardly segments the words of a corpus 
into morphemes. 
1.3 Related Work 
In this section we highlight previously proposed 
minimally supervised approaches to the induction 
of morphology that, like ParaMor, draw on the 
unique structure of natural language morphology. 
One facet of NL morphological structure com-
monly leveraged by morphology induction algo-
rithms is that morphemes are recurrent building 
blocks of words. Brent et al (1995), Goldsmith 
(2001), and Creutz (2006) emphasize the building 
block nature of morphemes when they each use 
recurring word segments to efficiently encode a 
corpus. These approaches then hypothesize that 
those recurring segments which most efficiently 
encode a corpus are likely morphemes. Another 
technique that exploits morphemes as repeating 
sub-word segments encodes the lexemes of a cor-
pus as a  character tree, i.e. trie, (Harris, 1955; 
Hafer and Weis, 1974), or as a finite state automa-
ton (FSA) over characters (Johnson, H. and Martin, 
2003; Altun and M. Johnson, 2001). A trie or FSA 
conflates multiple instances of a morpheme into a 
single sequence of states. Because the choice of 
possible succeeding characters is highly con-
strained within a morpheme, branch points in the 
trie or FSA are likely morpheme boundaries. Often 
trie similarities are used as a first step followed by 
further processing to identify morphemes (Schone 
and Jurafsky, 2001).  
The paradigm structure of NL morphology has 
also been previously leveraged. Goldsmith (2001) 
uses morphemes to efficiently encode a corpus, but 
he first groups morphemes into paradigm like 
structures he calls signatures. To date, the work 
that draws the most on paradigm structure is 
Snover (2002). Snover incorporates paradigm 
structure into a generative statistical model of 
morphology. Additionally, to discover paradigm 
like sets of suffixes, Snover designs and searches 
networks of partial paradigms. These networks are 
the direct inspiration for ParaMor?s morphology 
scheme networks described in section 2.1. 
2 ParaMor: Inflection Class Identification 
2.1 Search 
A Search Space: The first stage of ParaMor is a 
search procedure designed to identify partial in-
flection classes containing as many true productive 
suffixes of a language as possible. To search for 
these partial inflection classes we must first define 
a space to search over. In a naturally occurring 
corpus not all possible surface forms occur. In a 
corpus, each stem adhering to an inflection class 
will likely be observed in combination with only a 
subset of the suffixes in that inflection class. Each 
box in Figure 1 depicts a small portion of the em-
pirical co-occurrence of suffixes and stems from a 
Spanish newswire corpus of 50,000 types. Each 
box in this figure contains a list of suffixes at the 
top in bold, together with the total number, and a 
few examples (in italics), of stems that occurred in 
separate word forms with each suffix in that box. 
For example, the box containing the suffixes e, 
er?, ieron, and i? contains the stems deb and 
padec because the word forms debe, padece, de-
ber?, padecer?, etc. all occurred in the corpus. We 
call each possible pair of suffix and stem sets a 
scheme, and say that the e.er?.ieron.i? scheme 
covers the words debe, padece, etc. Note that a 
scheme contains both stems that occurred with ex-
actly the set of suffixes in that scheme, as well as 
119
stems that occurred with suffixes beyond just those 
in the scheme. For example, in addition to the four 
suffixes e, er?, ieron, and i?, the stem deb oc-
curred with the suffixes er and ido, as evident from 
the top left scheme e.er.er?.ido.ieron.i? which 
contains the stem deb. Intuitively, a scheme is a 
subset of the suffixes filling the paradigm cells of a 
true inflection class together with the stems that 
empirically occurred with that set of suffixes.  
The schemes in Figure 1 cover portions of the er 
and the ir Spanish verbal inflection classes. The 
top left scheme of the figure contains suffixes in 
the er inflection class, while the top center scheme 
contains suffixes in the ir inflection class. The six 
suffixes in the top left scheme and the six suffixes 
in the top center scheme are just a few of the 
suffixes in the full er and ir inflection classes. As 
is fairly common for inflection classes across 
languages, the sets of suffixes in the Spanish er 
and ir inflection classes overlap. That is, verbs that 
belong to the er inflection class can take as a suffix 
certain strings of characters that verbs belonging to 
the ir inflection class can also take. The suffixes 
that are unique to the er verb inflection class in the 
top left scheme are er and er?; while the unique 
suffixes for the ir class in the top center scheme are 
ir and ir?. In the third row of the figure, the 
scheme e.ido.ieron.i? contains only suffixes found 
in both the er and ir schemes. 
 While the example schemes in Figure 1 are cor-
rect and do occur in a real Spanish newswire cor-
pus, the schemes are atypically perfect. There is 
only one suffix appearing in Figure 1 that is not a 
true suffix of Spanish?azar in the upper right 
scheme. In unsupervised morphology induction we 
do not know a priori the correct suffixes of a lan-
guage. Hence, we form schemes by proposing can-
didate morpheme boundaries at every character 
boundary in every word, including the character 
boundary after the final character in each word 
form, to allow for empty suffixes. 
Schemes of suffixes and their exhaustively co-
occurring stems define a natural search space over 
partial inflection classes because schemes readily 
organize by the suffixes and stems they contain. 
We define a parent-child relationship between a 
parent scheme, P  and a child scheme C , when P  
contains all the suffixes that C  contains and when 
P  contains exactly one more suffix than C . In 
Figure 1, parent child relations are represented by 
solid lines connecting boxed schemes. The scheme 
e.er.er?.ido.ieron.i?, for example, is the parent of 
three depicted children in Figure 1, one of which is 
e.er.er?.ieron.i?.  
Our search strategy exploits a fundamental 
aspect of the relationship between parent and child 
schemes. Consider the number of stems in a parent 
scheme P  as compared to the number of stems in 
any one of its children C . Since P  contains all the 
suffixes which C  contains, and because P  only 
contains stems that occurred with every suffix in 
P , P  can at most contain exactly the stems C  
contains and typically will contain fewer. In the 
Spanish corpus from which the scheme network of 
Figure 1 was built, 32 stems occur in forms with 
each of the five suffixes e, er, er?, ieron, and i? 
attached. But only 28 of these 32 stems occur in 
yet another form involving ido?the stem deb did 
but the stems padec and romp did not, for example. 
A Search Strategy: To search for schemes 
which cover portions of the true inflection classes 
of a language, ParaMor?s search starts at the bot-
tom of the network. The lowest level in the scheme 
e.er.er?.ido.ieron.i? 
28: deb, escog, ofrec, roconoc, vend, ... 
e.ido.ieron.ir.ir?.i? 
28: asist, dirig, exig, ocurr, sufr, ... 
e.er?.ido.ieron.i? 
28: deb, escog, ... 
e.er.ido.ieron.i? 
46: deb, parec, recog... 
e.ido.ieron.ir?.i? 
28: asist, dirig, ... 
 
e.ido.ieron.ir.i? 
39: asist, bat, sal, ... 
e.er.er?.ieron.i? 
32: deb, padec, romp, ... 
e.ido.ieron.i? 
86: asist, deb, hund,... 
e.er?.ieron.i? 
32: deb, padec, ... 
er.ido.ieron.i? 
58: ascend, ejerc, recog, ... 
ido.ieron.ir.i? 
44: interrump, sal, ... 
Figure 1: A small portion of a morphology scheme network?our search space of partial empirical in-
flection classes. This network was built from a Spanish Newswire corpus of 50,000 types, 1.26 million 
tokens. Each box contains a scheme. The suffixes of each scheme appear in bold at the top of each box. 
The total number of adherent stems for each scheme, together with a few exemplar stems, is in italics. 
Stems are underlined if they do not appear in any parent shown in this figure. 
azar.e.ido.ieron.ir.i? 
1: sal 
120
network consists of schemes which contain exactly 
one suffix together with all the stems that occurred 
in the corpus with that suffix attached. ParaMor 
considers each one-suffix scheme in turn beginning 
with that scheme containing the most stems, work-
ing toward schemes containing fewer. From each 
bottom scheme, ParaMor follows a single greedy 
upward path from child to parent. As long as an 
upward path takes at least one step, making it to a 
scheme containing two or more alternating suf-
fixes, our search strategy accepts the terminal 
scheme of the path as likely modeling a portion of 
a true inflection class. 
Each greedily chosen upward step is based on 
two criteria. The first criterion considers the 
number of adherent stems in the current scheme as 
compared to its parents? adherent sizes. A variety 
of statistics could judge the stem-strength of parent 
schemes: ranging from simple ratios through 
(dis)similarity measures, such as the dice 
coefficient or mutual information, to full fledged 
statistical tests. After experimenting with a range 
of such statistics we found, somewhat surprisingly, 
that measuring the ratio of parent stem size to child 
stem size correctly identifies parent schemes which 
contain only true suffixes just as consistently as 
more sophisticated tests. While a full report of our 
experiments is beyond the scope of this paper, the 
short explanation of this behavior is data 
sparseness. Many upward search steps start from 
schemes containing few stems. And when little 
data is available no statistic is particularly reliable.  
Parent-child stem ratios have two additional 
computational advantages over other measures. 
First, they are quick to compute and second, the 
parent with the largest stem ratio is always that 
parent with the most stems. So, being greedy, each 
search step simply moves to that parent, P , with 
the most stems, as long as the parent-child stem 
ratio to P  is large. The threshold above which a 
stem ratio is considered large enough to warrant an 
upward step is a free parameter. As the goal of this 
initial search stage is to identify schemes contain-
ing as wide a variety of productive suffixes as pos-
sible, we want to set the parent-child stem ratio 
threshold as low as possible. But a ratio threshold 
that is too small will allow search paths to schemes 
containing unproductive and spurious suffixes. In 
practice, for Spanish, we have found that setting 
the parent-child stem ratio cutoff much below 0.25 
results in schemes that begin to include only mar-
ginally productive derivational suffixes. For this 
paper we leave the parent-child stem ratio cutoff 
parameter at 0.25.  
Alone, stem strength assessments of parent 
schemes, such as parent-child stem ratios, falter as 
a search path nears the top of the morphology 
scheme network. Monotonically decreasing adher-
ent stem size causes statistics that assess parents? 
stem-strength to become less and less reliable. 
Hence, the second criterion governing each search 
step helps to halt upward search paths before judg-
ing parents? worth becomes impossible. While 
there are certainly many possible stopping criteria, 
ParaMor?s policy stops each upward search path 
when there is no parent scheme with more stems 
than it has suffixes. We devised this halting condi-
tion for two reasons. First, requiring each path 
scheme to contain more stems than suffixes attains 
high suffix recall. High recall results from setting a 
low bar for upward movement at the bottom of the 
network. Search paths which begin from schemes 
whose single suffix is rare in the text corpus can 
often take one or two upward search steps and 
reach a scheme containing the necessary three or 
four stems. Second, this halting criterion requires 
the top scheme of search paths that climb high in 
the network to contain a comparatively large num-
ber of stems. Reigning in high-reaching search 
paths before the stem count falls too far, captures 
path-terminal schemes which cover a large number 
of word types. In the second stage of ParaMor?s 
inflection class identification phase these larger 
terminal schemes effectively vacuum up the useful 
smaller paths that result from the more rare suf-
fixes. Figure 2 contains examples of schemes se-
lected by ParaMor?s initial search. 
To evaluate ParaMor at paradigm identification, 
we hand compiled an answer key of the inflection 
classes of Spanish. This answer key contains nine 
productive inflection classes. Three contain the 
suffixes of the ar, er, and ir verbal inflection 
classes. There are two orthographically differenti-
ated inflection classes for nouns in the answer key: 
one for nouns that form the plural by adding s, and 
one for nouns that take es. Adjectives in Spanish 
inflect for gender and number. Arguably, gender 
and number each constitute separate paradigms, 
each with two cells. But here we conflated these 
into a single inflection class with four cells. Fi-
nally, there are three inflection classes in our an-
swer key covering Spanish clitics. Spanish verbal 
clitics behave orthographically as agglutinative 
sequences of suffixes.  
121
In a corpus of Spanish newswire text of 50,000 
types and 1.26 million tokens, the initial search 
identifies schemes containing 92% of all ideal in-
flectional suffixes of Spanish, or 98% of the ideal 
suffixes that occurred at least twice in the corpus. 
There are selected schemes which contain portions 
of each of the nine inflection classes in the answer 
key. The high recall of the initial search comes, of 
course, at the expense of precision. While there are 
nine inflection-classes and 87 unique suffixes in 
the hand-built answer key for Spanish, 8339 
schemes are selected containing 9889 unique can-
didate suffixes.  
2.2 Clustering Partial Inflection Classes 
While the third step of inflection class identifica-
tion, discussed in Section 2.3, directly improves 
the initial search?s low precision by filtering out 
bogus schemes, the second step, described here, 
conflates selected schemes which model portions 
of the same inflection class. Consider the fifth and 
twelfth schemes selected by ParaMor from our 
Spanish corpus, as shown in Figure 2. Both of 
these schemes contain a large number of suffixes 
from the Spanish ar verbal inflection class. And 
while each contains many overlapping suffixes, 
each possesses correct suffixes which the other 
does not. Meanwhile, the 1591st selected scheme 
contains four suffixes of the ir verbal inflection 
class, including the only instance of ir? that occurs 
in any selected scheme. Containing only six stems, 
the 1591st scheme could accidentally be filtered out 
during the third phase of inflection class identifica-
tion. Hence, the rationale for clustering initial se-
lected schemes is two fold. First, by consolidating 
schemes which cover portions of the same inflec-
tion class we produce sets of suffixes which more 
closely model the paradigm structure of natural 
language morphology. And, second, corralling cor-
rect schemes safeguards against losing unique suf-
fixes. 
The clustering of schemes presents two unique 
challenges. First, we must avoid over-clustering 
schemes which model distinct inflection classes. 
As noted in Section 2.1, it is common, cross-
linguistically, for the suffixes of inflection classes 
to overlap. Looking at Figure 2, we must be careful 
not to merge the 209th selected scheme, which 
models a portion of the er verbal inflection class, 
with the 1591st selected scheme, which models the 
ir class?despite these schemes sharing two suf-
fixes, ido and idos. As the second challenge, the 
many small schemes which the search strategy 
produces act as distractive noise during clustering. 
While small schemes containing correct suffixes 
do exist, e.g. the 1591st scheme, the vast majority 
of schemes containing few stems and suffixes are 
incorrect collections of word final strings that hap-
pen to occur in corpus word forms attached to a 
small number of shared initial strings. ParaMor?s 
clustering algorithm should, for example, avoid 
placing ?.s and ?.ipo, respectively the 1st and 
1590th selected schemes, in the same cluster. Al-
though ?.ipo shares the null suffix with the valid 
nominal scheme ?.s, the string ?ipo? is not a mor-
phological suffix of Spanish. 
To form clusters of related schemes while ad-
dressing both the challenge of observing a lan-
guage?s paradigm structure as well as the challenge 
of merging in the face of many small incorrectly 
selected schemes, ParaMor adapts greedy hierar-
chical agglomerative clustering. We modify vanilla 
bottom-up clustering by placing restrictions on 
which clusters are allowed to merge. The first re-
striction helps ensure that schemes modeling dis-
tinct but overlapping inflection classes remain 
separated. The restriction: do not place into the 
same cluster suffixes which share no stem in the 
corpus. This restriction retains separate clusters for 
separate inflection classes because a lexeme?s stem 
Figure 2: The suffixes of some schemes selected 
by the initial search over a Spanish corpus of 
50,000 types. While some selected schemes 
contain large numbers of correct suffixes, such 
as the 1st, 2nd, 5th, 12th, 209th, and 1591st selected 
schemes; many others are incorrect collections 
of word final strings. 
 1) ?.s 5501 stems 
 2) a.as.o.os 892 stems 
... 
 5) a.aba.aban.ada.adas.ado.ados.an.ando.   
ar.aron.arse.ar?.ar?n.? 25 stems 
... 
 12) a.aba.ada.adas.ado.ados.an.ando.ar.   
aron.ar?.ar?n.e.en.? 21 stems 
... 
 209) e.er.ida.idas.ido.idos.imiento.i? 9 stems 
... 
1590) ?.ipo 4 stems 
1591) ido.idos.ir.ir? 6 stems 
1592) ?.e.iu 4 stems 
1593) iza.izado.izan.izar.izaron.izar?n.iz? 
... 8 stems 
122
occurring with suffixes unique to that lexeme?s 
inflection class will not occur with suffixes unique 
to some other inflection class.  
Alone, requiring all pairs of suffixes in a cluster 
to occur in the corpus with some common stem 
will not prevent small bogus schemes, such as 
?.ipo from attaching to correct schemes, such as 
?.s?the ipo.s scheme contains two ?stems,? the 
word form initial strings ?ma? and ?t?. And so a 
second restriction is required. This second restric-
tion employs a heuristic specifically adapted to 
ParaMor?s initial search strategy. As discussed in 
Section 2.1, in addition to many schemes which 
contain only few suffixes, ParaMor?s initial net-
work search also identifies multiple overlapping 
schemes containing significant subsets of the suf-
fixes in an inflection class. The 5th, 12th, and 209th 
selected schemes of Figure 2 are three such larger 
schemes. ParaMor restricts cluster merges heuristi-
cally by requiring at least one large scheme for 
each small scheme the cluster contains, where we 
measure the size of a scheme as the number of 
unique word forms it covers. The threshold size 
above which schemes are considered large is the 
second of ParaMor?s two free parameters. The 
scheme size threshold is reused during ParaMor?s 
filtering stage. We discuss the unsupervised proce-
dure we use to set the size threshold when we pre-
sent the details of cluster filtering in Section 2.3. 
We have found that with these two cluster re-
strictions in place, the particular metric we use to 
measure the similarity of scheme-clusters does not 
significantly affect clustering. For the experiments 
we report here, we measure the similarity of 
scheme-clusters as the cosine between the sets of 
all possible stem-suffix pairs the clusters contain. 
A stem-suffix pair occurs in a cluster if some 
scheme belonging to that cluster contains both that 
stem and that suffix. With these adaptations, we 
allow agglomerative clustering to proceed until 
there are no more clusters that can legally be 
merged.  
2.3 Filtering of Inflection Classes 
With most valid schemes having found a safe ha-
ven in a cluster with other schemes modeling the 
same inflection class, we turn our attention to im-
proving scheme-cluster precision. ParaMor applies 
a series of filters, culling out unwanted scheme-
clusters. The first filter is closely related to the 
cluster restriction on scheme size discussed in Sec-
tion 2.2. ParaMor discards all unclustered schemes 
falling below the size threshold used during clus-
tering. Figure 3 graphs the number of Spanish clus-
ters which survive this size-based filtering step as 
the threshold size is varied. Figure 3 also contains 
a plot of the recall of unique Spanish suffixes as a 
function of this threshold. As the size threshold is 
increased the number of remaining clusters quickly 
drops. But suffix recall only slowly falls during the 
steep decline in cluster count, indicating ParaMor 
discards mostly bogus schemes containing illicit 
suffixes. Because recall is relatively stable, the ex-
act size threshold we use should have only a minor 
effect on ParaMor?s final morphological analyses. 
In fact, we have not fully explored the ramifica-
tions various threshold values have on the final 
morphological word segmentations, but have sim-
ply picked a reasonable setting, 37 covered word 
types. At this threshold, the number of scheme-
clusters is reduced by more than 98%, while the 
number of unique candidate suffixes in any cluster 
is reduced by more than 85%. Note that the initial 
number of selected schemes, 8339, falls outside the 
scale of Figure 3. 
Of the scheme-clusters which remain after size 
based filtering is complete, by far the largest cate-
gory of incorrect clusters contains schemes which, 
like the 1593rd selected scheme, shown in Figure 2, 
incorrectly hypothesize morpheme boundaries one 
or more characters to the left of the true boundary. 
To filter out these incorrectly segmented clusters 
we use a technique inspired by Harris (1955). For 
each initial string common to all suffixes in the 
cluster, for each scheme in the cluster, we examine 
the network scheme containing the suffixes formed 
by stripping the initial string from the scheme?s 
Figure 3: The # of clusters and their recall of 
unique Spanish suffixes as the scheme-cluster 
size cutoff is varied. The value of each function 
at the threshold we use in all experiments re-
ported in this paper is that of the larger symbol. 
0
200
400
600
800
1000
0 50 100 150
Scheme or Cluster Size
# 
o
f C
lu
st
er
s
0
0.2
0.4
0.6
0.8
1
R
ec
a
ll
# of Clusters
Recall
 
123
suffixes. We then measure the entropy of leftward 
trie characters of the stripped scheme. If the en-
tropy is large, then the character stripped scheme is 
likely at a morpheme boundary and the original 
scheme is likely modeling an incorrect morpheme 
boundary. This algorithm would throw out the 
1593rd selected scheme because the stems in the 
scheme a.ado.an.ar.aron.ar?n.? end in a wide 
variety of characters, yielding high trie entropy, 
and signaling a likely morpheme boundary. 
Because we apply morpheme boundary filtering 
after we have clustered, the redundancy of the 
many schemes in the cluster makes this filter quite 
robust, letting us set the cutoff parameter as low as 
we like avoiding another free parameter. 
2.4 Segmentation and Evaluation 
Word segmentation is our final step of morpholo-
gical analysis. ParaMor?s current segmentation 
algorithm is perhaps the most simple paradigm 
inspired segmentation algorithm possible. Essen-
tially, ParaMor strips off suffixes which likely par-
ticipate in a paradigm. To segment any word, w , 
ParaMor identifies all scheme-clusters that contain 
a non-empty suffix that matches a word final string 
of w . For each such matching suffix, Cf ? , 
where C is the cluster containing f , we strip f  
from w  obtaining a stem t . If there is some sec-
ond suffix Cf ??  such that ft ?.  is a word form 
found in either of the training or the test corpora, 
then ParaMor proposes a segmentation of w  be-
tween t  and f . ParaMor, here, identifies f  and 
f ?  as mutually exclusive suffixes from the same 
paradigm. If ParaMor finds no complex analysis, 
then we propose w  itself as the sole analysis of the 
word. Note that for each word form, ParaMor may 
propose multiple separate segmentation analyses 
each containing a single proposed stem and suffix. 
To evaluate ParaMor?s morphological segmenta-
tions we follow the methodology of Morpho Chal-
lenge 2007 (Kurimo et al, 2007), a minimally su-
pervised morphology induction competition. Word 
segmentations are evaluated in Morpho Challenge 
2007 by comparing against hand annotated mor-
phological analyses. The correctness of proposed 
morphological analyses is computed in Morpho 
Challenge 2007 by comparing pairs of word forms 
which share portions of their analyses. Recall is 
measured by first sampling pairs of words from the 
answer analyses which share a stem or morphosyn-
tactic feature and then noting if that pair of word 
forms shares a morpheme in any of their proposed 
analyses. Precision is measured analogously, sam-
pling morpheme-sharing pairs of words from the 
proposed analyses and noting if that pair of words 
shares a feature in any correct analysis of those 
words.  
We evaluate ParaMor on two languages not 
examined during the development of ParaMor?s 
induction algorithms: English and German. And 
we evaluate with each of these two languages at 
two tasks:  
1. Analyzing inflectional morphology alone 
2. Jointly analyzing inflectional and derivational 
morphology.  
We constructed Morpho Challenge 2007 style 
answer keys for each language and each task using 
the Celex database (Burnage, 1990). The English 
and German corpora we test over are the corpora 
available through Morpho Challenge 2007. The 
English corpus contains nearly 385,000 types, 
while the German corpus contains more than 1.26 
million types. ParaMor induced paradigmatic 
scheme-clusters over these larger corpora by 
reading just the top 50,000 most frequent types. 
But with the scheme-clusters in hand, ParaMor 
segmented all the types in each corpus. 
We compare ParaMor to Morfessor v0.9.2 
(Creutz, 2006), a state-of-the-art minimally super-
vised morphology induction algorithm. Morfessor 
has a single free parameter. To make for stiff com-
petition, we report results for Morfessor at that pa-
rameter setting which maximized F1 on each sepa-
rate test scenario. We did not vary the two free pa-
rameters of ParaMor, but hold each of ParaMor?s 
parameters at a setting which produced reasonable 
Spanish suffix sets, see sections 2.1-2.2. Table 2 
contains the evaluation results. To estimate the 
variance of our experimental results we measured 
Morpho Challenge 2007 style precision, recall, and 
F1 on multiple non-overlapping pairs of 1000 fea-
ture-sharing words.  
Neither ParaMor nor Morfessor arise in Table 2 
as clearly superior. Each algorithm outperforms the 
other at F1 in some scenario. Examining precision 
and recall is more illuminating. ParaMor attains 
particularly high recall of inflectional affixes for 
both English and German. We conjecture that Pa-
raMor?s strong performance at identifying inflec-
tional morphemes comes from closely modeling 
the natural paradigm structure of language. Con-
versely, Morfessor places its focus on precision 
and does not rely on any property exclusive to in-
flectional (or derivational) morphology. Hence, 
124
Morfessor attains high precision with reasonable 
recall when graded against an answer key contain-
ing both inflectional and derivational morphology. 
We are excited by ParaMor?s strong 
performance and are eager to extend our algorithm. 
We believe the precision of ParaMor?s simple 
segmentation algorithm can be improved by 
narrowing down the proposed analyses for each 
word to the most likely. Perhaps ParaMor and 
Morfessor?s vastly different strategies for 
morphology induction could be combined into a 
hybrid strategy more successful than either alone. 
And ambitiously, we hope to extend ParaMor to 
analyze languages with agglutinative sequences of 
affixes by generalizing the definition of a scheme.  
Acknowledgements 
The research reported in this paper was funded in 
part by NSF grant number IIS-0121631. 
References 
Altun, Yasemin, and Mark Johnson. "Inducing 
SFA with -Transitions Using Minimum 
Description Length." Finite State Methods in 
Natural Language Processing Workshop at 
ESSLLI Helsinki: 2001.  
Brent, Michael R., Sreerama K. Murthy, and 
Andrew Lundberg. "Discovering Morphemic 
Suffixes: A Case Study in MDL Induction." The 
Fifth International Workshop on Artificial Intel-
ligence and Statistics Fort Lauderdale, Florida, 
1995.  
Burnage, Gavin. Celex?A Guide for Users. 
Springer, Centre for Lexical information, 
Nijmegen, the Netherlands, 1990. 
Creutz, Mathias. ?Induction of the Morphology of 
Natural Language: Unsupervised Morpheme 
Segmentation with Application to Automatic 
Speech Recognition.? Ph.D. Thesis in Computer 
and Information Science, Report D13. Helsinki: 
University of Technology, Espoo, Finland, 2006. 
Goldsmith, John. "Unsupervised Learning of the 
Morphology of a Natural Language." Computa-
tional Linguistics 27.2 (2001): 153-198.  
Hafer, Margaret A., and Stephen F. Weiss. "Word 
Segmentation by Letter Successor Varieties." 
Information Storage and Retrieval 10.11/12 
(1974): 371-385. 
Harris, Zellig. "From Phoneme to Morpheme." 
Language 31.2 (1955): 190-222. Reprinted in 
Harris 1970. 
Harris, Zellig. Papers in Structural and 
Transformational Linguists. Ed. D. Reidel, 
Dordrecht 1970. 
Johnson, Howard, and Joel Martin. "Unsupervised 
Learning of Morphology for English and Inuk-
titut." Human Language Technology Conference 
/ North American Chapter of the Association for 
Computational Linguistics (HLT-NAACL). 
Edmonton, Canada: 2003. 
Kurimo, Mikko, Mathias Creutz, and Matti 
Varjokallio. ?Unsupervised Morpheme Analysis 
? Morpho Challenge 2007.? March 26, 2007. 
<http://www.cis.hut.fi/morphochallenge2007/> 
Schone, Patrick, and Daniel Jurafsky. "Know-
ledge-Free Induction of Inflectional Morpho-
logies." North American Chapter of the 
Association for Computational Linguistics 
(NAACL). Pittsburgh, Pennsylvania: 2001. 183-
191. 
Snover, Matthew G. "An Unsupervised Knowledge 
Free Algorithm for the Learning of Morphology 
in Natural Languages." Sever Institute of Tech-
nology, Computer Science Saint Louis, Mis-
souri: Washington University, M.S. Thesis, 
2002. 
Table 2: ParaMor segmentations compared to Morfessor?s (Creutz, 2006) evaluated for Precision, Recall, 
F1, and standard deviation of F1, , in four scenarios. Segmentations over English and German are each 
evaluated against correct morphological analyses consisting, on the left, of inflectional morphology 
only, and on the right, of both inflectional and derivational morphology. 
 Inflectional Morphology Only Inflectional & Derivational Morphology 
 English German English German 
 P R F1  P R F1  P R F1  P R F1  
Morfessor 53.3 47.0 49.9 1.3 38.7 44.2 41.2 0.8 73.6 34.0 46.5 1.1 66.9 37.1 47.7 0.7 
ParaMor 33.0 81.4 47.0 0.9 42.8 68.6 52.7 0.8 48.9 53.6 51.1 0.8 60.0 33.5 43.0 0.7 
 
125
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 49?58,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Evaluating an Agglutinative Segmentation Model for ParaMor 
Christian Monson, Alon Lavie, Jaime Carbonell, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15217, USA 
 {cmonson, alavie, jgc, lsl}@cs.cmu.edu
Abstract 
This paper describes and evaluates a modifica-
tion to the segmentation model used in the un-
supervised morphology induction system, Pa-
raMor. Our improved segmentation model 
permits multiple morpheme boundaries in a 
single word. To prepare ParaMor to effectively 
apply the new agglutinative segmentation 
model, two heuristics improve ParaMor?s pre-
cision. These precision-enhancing heuristics 
are adaptations of those used in other unsuper-
vised morphology induction systems, including 
work by Hafer and Weiss (1974) and Gold-
smith (2006). By reformulating the segmenta-
tion model used in ParaMor, we significantly 
improve ParaMor?s performance in all lan-
guage tracks and in both the linguistic evalua-
tion as well as in the task based information re-
trieval (IR) evaluation of the peer operated 
competition Morpho Challenge 2007. Para-
Mor?s improved morpheme recall in the lin-
guistic evaluations of German, Finnish, and 
Turkish is higher than that of any system which 
competed in the Challenge. In the three lan-
guages of the IR evaluation, our enhanced Pa-
raMor significantly outperforms, at average 
precision over newswire queries, a morpho-
logically na?ve baseline; scoring just behind the 
leading system from Morpho Challenge 2007 
in English and ahead of the first place system 
in German.  
1 Unsupervised Morphology Induction 
Analyzing the morphological structure of words 
can benefit natural language processing (NLP) ap-
plications from grapheme-to-phoneme conversion 
(Demberg et al, 2007) to machine translation 
(Goldwater and McClosky, 2005). But many of the 
world?s languages currently lack morphological 
analysis systems. Unsupervised induction could fa-
cilitate, for these lesser-resourced languages, the 
quick development of morphological systems from 
raw text corpora. Unsupervised morphology induc-
tion has been shown to help NLP tasks including 
speech recognition (Creutz, 2006) and information 
retrieval (Kurimo et al, 2007b). In this paper we 
work with languages like Spanish, German, and 
Turkish for which morphological analysis systems 
already exist. 
The baseline ParaMor algorithm which we ex-
tend here competed in the English and German 
tracks of Morpho Challenge 2007 (Monson et al, 
2007b). The peer operated competitions of the 
Morpho Challenge series standardize the evalua-
tion of unsupervised morphology induction algo-
rithms (Kurimo et al, 2007a; 2007b). The ParaMor 
algorithm showed promise in the 2007 Challenge, 
placing first in the linguistic evaluation of German. 
Developed after the close of Morpho Challenge 
2007, our improvements to the ParaMor algorithm 
could not officially compete in this Challenge. 
However, the Morpho Challenge 2007 Organizing 
Committee (Kurimo et al, 2008) graciously over-
saw the quantitative evaluation of our agglutinative 
version of ParaMor.  
1.1 Related Work 
A variety of approaches to unsupervised morphol-
ogy induction have shown promise in past work: 
Here we highlight three techniques which have 
been used in a number of unsupervised morphol-
ogy induction algorithms. Since character se-
quences are less predictable at morpheme bounda-
ries than within any particular morpheme (see dis-
cussion in section 2.1), a first unsupervised mor-
49
phology induction technique measures the predict-
ability of word-internal character sequences. Harris 
(1955) was the first to propose the branching factor 
of the character tree of a corpus vocabulary as a 
measure of character predictability. Character trees 
have been incorporated into a number of more re-
cently proposed unsupervised morphology induc-
tion systems (Schone and Jurafsky, 2001; Wicen-
towski, 2002; Goldsmith, 2006; Bordag, 2007). 
Johnson and Martin (2003) generalize from charac-
ter trees and model morphological character se-
quences with minimized finite state automata. 
Bernhard (2007) measures character predictability 
by directly computing transitional probabilities be-
tween substrings of words. 
A second successful technique has used the 
minimum description length principle to capture 
the morpheme as a recurrent structure of morphol-
ogy. The Linguistica system of Goldsmith (2006), 
the Morfessor system of Creutz (2006), and the 
system described in Brent et al (1995) take this 
approach. 
A third technique leverages inflectional para-
digms as the organizational structure of morphol-
ogy. The ParaMor algorithm, which this paper ex-
tends, joins Snover (2002), Zeman (2007), and 
Goldsmith?s Linguistica in building morphology 
models around the paradigm.  
ParaMor tackles three challenges that face mor-
phology induction systems which Goldsmith's Lin-
guistica algorithm does not yet address. First, sec-
tion 2.2 of this paper introduces an agglutinative 
segmentation model. This agglutinative model seg-
ments words into as many morphemes as the data 
justify. Although Goldsmith (2001) and Goldsmith 
and Hu (2004) discuss ideas for segmenting indi-
vidual words into more than two morphemes, the 
implemented Linguistica algorithm, as presented in 
Goldsmith (2006), permits at most a single mor-
pheme boundary in each word. Second, ParaMor 
decouples the task of paradigm identification from 
that of word segmentation (Monson et al, 2007b). 
In contrast, morphology models in Linguistica in-
herently encode both a belief about paradigm 
structure on individual words as well as a segmen-
tation of those words. Without ParaMor?s decoup-
ling of paradigm structure from specific segmenta-
tion models, our algorithm for agglutinative seg-
mentation (section 2.2) would not have been possi-
ble. Third, the evaluation of ParaMor in this paper 
is over much larger corpora than any published 
evaluation of Linguistica. Goldsmith (2006) seg-
ments the Brown corpus of English, which, after 
discarding numbers and punctuation, has a vocabu-
lary size of 47,607 types. Using Linguistica, Creutz 
(2006) successfully segments a Finnish corpus of 
250,000 tokens (approximately 130,000 types), but 
Creutz notes that Linguistica is memory intensive 
and not runable for larger corpora. In the evalua-
tions of Morpho Challenge 2007, ParaMor seg-
mented the words from corpora with over 42 mil-
lion tokens and vocabularies as large as 2.2 million 
types.  
2 ParaMor 
This section briefly outlines the high level struc-
ture of ParaMor as described in detail in Monson et 
al. (2007a; 2007b). ParaMor takes the inflectional 
paradigm as the basic building block of morphol-
ogy. A paradigm is a mutually substitutable set of 
morphological operations. For example, most ad-
jectives in Spanish inflect for two paradigms. First, 
adjectives are marked for gender: an a suffix 
marks feminine, an o masculine. Then Spanish ad-
jectives mark number: an s suffix signals plural, 
while no marking, ? in this paper, indicates singu-
lar. The four surface forms of the cross-product of 
the gender and number paradigms on the Spanish 
word for ?beautiful? are then: bello, bella, bellos, 
and bellas.  
ParaMor is a two stage algorithm. In the first 
stage, ParaMor identifies candidate paradigms 
which likely model suffixes of morphological pa-
radigms and their cross-products. Since some 70% 
of the world?s languages are significantly suffixing 
(Dryer, 2005), ParaMor only attempts to identify 
suffix paradigms. ParaMor?s first stage consists of 
three pipelined steps. In the first step, ParaMor 
searches a space of candidate partial paradigms, 
called schemes, for those which possibly model 
suffixes of true paradigms. The second step merges 
selected schemes which appear to model the same 
paradigm. And in the third step, ParaMor discards 
scheme clusters which likely do not model true 
paradigms.  
The second stage of the ParaMor algorithm 
segments word forms using the candidate para-
digms identified in the first stage. Section 2.2 of 
this paper introduces a new segmentation model 
for ParaMor?s second stage that allows more than 
one morpheme boundary in a single word?as is 
50
needed to correctly segment Spanish plural adjec-
tives. As this agglutinative segmentation model re-
lies on the paradigms learned in ParaMor?s first 
stage, section 2.1 presents solutions to two types of 
paradigm model error that the baseline ParaMor 
system makes. The solutions to these two error 
types are similar in nature to ideas proposed in the 
unsupervised morphology induction work of Hafer 
and Weiss (1974) and Goldsmith (2006). 
2.1 Precision at Paradigm Identification 
Table 1 presents 14 of the more than 8000 schemes 
identified during one baseline run of ParaMor?s 
scheme search step. Each row of Table 1 lists a 
scheme that was selected while searching over a 
Spanish newswire corpus of 50,000 types. On the 
far left of Table 1, the Rank column states the or-
dinal rank at which that row?s scheme was selected 
during the search procedure: the first scheme Pa-
raMor selects is ?.s; a.as.o.os is the second; ido.-
idos.ir.ir? is the 1566th selected scheme, etc. The 
right four columns of Table 1, present raw data on 
the selected schemes, giving the number of can-
didate suffixes in that scheme, the proposed suf-
fixes themselves, the number of candidate stems in 
the scheme, and a sample of those candidate stems. 
Each candidate stem in a ParaMor scheme forms a 
word that occured in the input corpus with each 
candidate suffix belonging to that scheme; for 
example, from the first selected scheme, the candi-
date stem apoyada joins to the candidate suffix s to 
form the word apoyadas ?supported (adjective 
feminine plural)??a word which occured in the 
Spanish newswire corpus.  
Between the rank on the left and the scheme 
details on the right of Table 1, are columns which 
categorize the scheme on its success, or failure, to 
model a true paradigm of Spanish. A dot appears in 
the columns marked Noun, Adjective, or Verb if the 
majority of the candidate suffixes in a row?s 
scheme attempt to model suffixes in a paradigm of 
that part of speech. A dot appears in the Derivation 
column if one or more candidate suffixes of the 
scheme models a Spanish derivational suffix. The 
Good column is marked if the candidate suffixes of 
a scheme take the surface form of true paradig-
matic suffixes. Initially selected schemes in Table 
1 that correctly capture suffixes of real Spanish 
paradigms are the 1st, 2nd, 5th, 13th, 30th, and 1566th 
selected schemes. While some smaller paradigms 
of Spanish are perfectly identified (including ?.s, 
which marks singular and plural on many nouns 
and adjectives, and the adjectival cross-product 
paradigm of gender and number, a.as.o.os) many 
selected schemes do not satisfactorily model Span-
ish suffixes. Incorrect schemes in Table 1 are 
marked in the Error columns.  
The vast majority of unsatisfactory paradigm 
models fail for one of two reasons. First, many 
schemes contain candidate suffixes which system-
Model of Error 
Verb 
Ra
nk
 
No
un
 
Ad
jec
tiv
e 
ar er ir 
De
riv
ati
on
 
Go
od
 
St
em
 In
ter
na
l 
Su
ffi
x I
nt
er
na
l 
Ch
an
ce
 Candidate Suffixes Candidate Stems 
1 ?  ?      ?     2 ?.s 5513 apoyada, barata, hombro, oficina, reo, ? 
2  ?      ?     4 a.as.o.os 899 apoyad, captad, dirigid, junt, pr?xim, ? 
3   ?       ?   14 ?.ba.ban.da.das.do.dos.n.ndo.r.ron.rse.r?.r?n 25 apoya, disputa, lanza, lleva, toma, ? 
5   ?     ?     15 a.aba.aban.ada.adas.ado.ados.an.ando.ar.aron.arse.ar?.ar?n.? 24 apoy, desarroll, disput, lanz, llev, ? 
11  ?     ?   ?    5 ta.tamente.tas.to.tos 22 cier, direc, ins?li, modes, sangrien, ? 
12   ?    ?    ?   14 ?.ba.ci?n.da.das.do.dos.n.ndo.r.ron.r?.r?n.r?a 16 acepta, concentra, fija, provoca, ? 
13   ?     ?     15 a.aba.ada.adas.ado.ados.an.ando.ar.aron.ar?.ar?n.e.en.? 20 apoy, declar, enfrent, llev, tom, ? 
30    ?  ?   ?     11 a.e.en.ida.idas.ido.idos.iendo.ieron.i?.?a 15 cumpl, escond, recib, transmit, vend, ? 
1000          ?  3 ?.g.gs 4 h, k, on, s 
1566     ?   ?     4 ido.idos.ir.ir? 6 conclu, cumpl, distribu, exclu, reun, segu 
2000      ?   ?    2 lia.liana 5 austra, ita, ju, sici, zu 
3000          ?  3 ?.a.anar 4 all, am, g, s 
4000          ?  3 ?.e.ince 4 l, pr, qu, v 
8000   ?      ?    2 trada.trarnos 3 concen, demos, encon 
               
 
Table 1. Candidate partial paradigms, or schemes, that the baseline ParaMor algorithm selected during its first step, 
search, of its first stage, paradigm identification. This baseline ParaMor run was over a Spanish newswire corpus of 
50,000 types. While some selected schemes contain suffixes from true paradigms, other schemes contain incorrectly 
segmented candidate suffixes. 
  
51
atically misanalyze word forms. These schemes 
consistently hypothesize either stem-internal or 
suffix-internal morpheme boundaries. Schemes 
which hypothesize incorrect morpheme boundaries 
include the 3rd, 11th, 12th, 2000th, and 8000th se-
lected schemes of Table 1. Among these, the 3rd 
and 12th selected schemes place morpheme boun-
daries internal to true suffixes. For example, the 3rd 
selected scheme contains truncated forms of suf-
fixes that occur correctly in the 5th selected 
scheme. Symmetrically, the candidate suffixes in 
the 11th, 2000th, and 8000th selected schemes hy-
pothesize morpheme boundaries internal to true 
Spanish stems, inadvertently including portions of 
stems within their suffix lists. In a random sample 
of 100 schemes from the 8240 schemes that the 
baseline ParaMor algorithm selects over our Span-
ish corpus, 59 schemes hypothesized an incorrect 
morpheme boundary. 
The second most prevalent reason for model 
failure occurs when the candidate suffixes of a 
scheme are related not by belonging to the same 
paradigm, but rather by a chance co-occurrence on 
a few candidate stems of the text. Schemes which 
arise from chance string collisions in Table 1 in-
clude the 1000th, 3000th, and 4000th selected 
schemes. The string lengths of the candidate stems 
and candidate suffixes of these chance schemes are 
often quite short. The longest candidate stem in 
any of the three chance-error schemes of Table 1 is 
three characters long; and all three selected 
schemes propose the suffix ?, which has length 
zero. Short stems and short suffixes in selected 
schemes are easily explained combinatorially: The 
inventory of possible strings grows exponentially 
with the length of the string. Because there just 
aren?t very many length one, length two, or even 
length three strings, it should come as no surprise 
when a variety of candidate suffixes happen to oc-
cur attached to the same set of short stems. In our 
random sample of 100 initially selected schemes, 
35 were erroneously selected as a result of a 
chance collision of word types. 
The next two sub-sections present solutions to 
the two types of paradigm model failure in the 
baseline algorithm that are exemplified in Table 1. 
These first two extensions aim to improve preci-
sion by reducing the number of schemes ParaMor 
erroneously selects. 
 
Correcting Morpheme Boundary Errors 
Most of the baseline selected schemes which incor-
rectly hypothesize a morpheme boundary do so at 
stem-internal positions. Indeed, in our random 
sample of 100 schemes, 51 of the 59 schemes with 
morpheme boundary errors incorrectly hypothe-
sized a boundary stem-internally. For this reason, 
the baseline ParaMor algorithm already discarded 
schemes that likely misplace a boundary stem-
internally (Monson et al, 2007b). Although there 
are fewer schemes that misplace a morpheme 
boundary suffix-internally, suffix-internal error 
schemes contain short suffixes that can generalize 
to segment a large number of word forms. (See 
section 2.2 for a description of ParaMor?s morpho-
logical segmentation model). To measure the in-
fluence of suffix-internal error schemes on mor-
pheme segmentation, we examined ParaMor?s 
baseline segmentations of a random sample of 100 
word forms from the 50,000 words of our Spanish 
corpus. In these 100 words, 82 morpheme bounda-
ries were introduced that should not have been. 
And 40 of these 82 incorrectly proposed bounda-
ries were placed by schemes which hypothesized a 
morpheme boundary internal to true suffixes.  
To address the problem of suffix-internal mis-
placed boundaries we adapt an idea originally pro-
posed by Harris (1955) and extended by Hafer and 
Weiss (1974): Take any string t. Let F be the set of 
strings such that for each Ff ? , t.f is a word form 
of a particular natural language. Harris noted that 
when the boundaries between t and each f fall at 
morpheme boundaries, the strings in F typically 
begin in a wide variety of characters; but when the 
t-f boundaries are morpheme-internal, each legiti-
mate word final string must first complete the er-
roneously split morpheme, and so the strings in F 
will begin with one of a very few characters. This 
argument similarly holds when the roles of t and f 
are reversed. Hafer and Weiss (1974) describe a 
number of variations to Harris? letter variety algo-
rithm. Their most successful variation uses entropy 
to measure character variety.  
Goldsmith?s (2006) Linguistica algorithm pio-
neered the use of entropy in a paradigm-based un-
supervised morphology induction system. Linguis-
tica measures the entropy of stem-final characters 
in a set of initially selected paradigm models. 
When entropy falls below a threshold, Linguistica 
considers relocating the morpheme boundary of 
52
each word covered by that paradigm model. If, af-
ter boundary relocation, the resulting description 
length of Linguistica?s morphology model de-
creases, Linguistica accepts the relocated bounda-
ries.  
To identify suffix-internal morpheme boundary 
errors among ParaMor?s initially selected schemes, 
we follow Hafer and Weiss (1974) and Goldsmith 
(2006) in using entropy as a measure of the variety 
in boundary-adjacent character distributions. In a 
ParaMor style scheme, the candidate stems form a 
set of word-initial strings, and the candidate suf-
fixes a set of word-final strings. If a scheme?s 
stems end in a very few unique characters, the 
scheme has likely hypothesized an incorrect suffix-
internal morpheme boundary. Consider the 3rd se-
lected scheme in Table 1. All 25 of the 3rd 
scheme?s stems end in the character ?a?. Conse-
quently, we measure the entropy of the distribution 
of final characters in each scheme?s candidate 
stems. Where Linguistica modifies paradigm mod-
els which appear to incorrectly place morpheme 
boundaries, our extension to ParaMor permanently 
removes schemes. To avoid introducing a free pa-
rameter, our extension to ParaMor flags a scheme 
as a likely boundary error only when virtually all 
of that scheme?s candidate stems end in the same 
character. We flag a scheme if its entropy is below 
a threshold set close to zero, 0.5. The baseline Pa-
raMor algorithm discards schemes which it be-
lieves hypothesize an incorrect stem-internal mor-
pheme boundary only after the scheme clustering 
step of ParaMor?s paradigm identification stage. 
Our extension follows suit: If we flag more than 
half of the schemes in a cluster as likely proposing 
a suffix-internal boundary, then we discard that 
cluster. Referencing Table 1, this first extension to 
ParaMor successfully removes both the 3rd and the 
12th selected schemes.  
Correcting Chance String Collision Errors 
Scheme errors due to chance string collisions are 
the second most prevalent error type. As described 
above, the string lengths of the candidate stems 
and suffixes of chance schemes are typically short. 
When the stems and suffixes of a scheme are short, 
then the underlying types which support a scheme 
are also short. Where the baseline ParaMor algo-
rithm explicitly builds schemes over all types in a 
corpus, we modify ParaMor to exclude short types 
from the vocabulary during morphology induction. 
Goldsmith (2006) also uses string-length thresh-
olds to restrict what paradigm models the Linguis-
tica algorithm produces. 
Excluding short types during ParaMor?s mor-
phology induction stage does not preclude short 
types from being analyzed as containing multiple 
morphemes during ParaMor?s segmentation stage. 
As section 2.2 describes, ParaMor?s segmentation 
algorithm is independent of the set of types from 
which schemes and scheme clusters are built. 
The string length that types must meet to join 
the induction vocabulary is a free parameter. Pa-
raMor is designed to identify the productive inflec-
tional paradigms of a language. Unless a paradigm 
is restricted to occur only with short stems, a pos-
sible but unusual scenario (as with the English ad-
jectival comparative, c.f. faster but *exquisiter) we 
can expect a productive paradigm to occur with a 
reasonable number of longer stems in a corpus. 
Hence, ParaMor needn?t be overly concerned 
about discarding short types. A qualitative exam-
ination of Spanish data suggested discarding types 
five characters or less in length; we use this cutoff 
in all experiments described in this paper. 
Excluding short types from the paradigm induc-
tion vocabulary virtually eliminates the entire cate-
gory of chance scheme. In a random sample of 100 
schemes that ParaMor selected when short types 
were excluded, only one scheme contained types 
related only by chance string similarity, down from 
35 when short types were not excluded. Returning 
to Table 1, excluding types five characters or less 
in length bars ten of the twelve word types which 
support the erroneous 3000th selected scheme ?.a.-
anar. Among the excluded types are valid Spanish 
words such as ganar ?to gain?. But also eliminated 
are several meaningless acronyms such as the sin-
gle letters g and s. Without these short types, Pa-
raMor rightly cannot select the 3000th scheme. 
2.2 Segmentation 
An Agglutinative Model 
With the improvement in scheme precision that re-
sults from the two extensions discussed in section 
2.1, we are ready to propose a more realistic model 
of morphology. ParaMor?s baseline segmentation 
algorithm distrusts ParaMor?s induced scheme 
models. The baseline algorithm assumes each word 
form can contain at most a single morpheme 
boundary. If it detects more than one morpheme 
53
boundary, then the baseline algorithm proposes a 
separate morphological analysis for each possible 
boundary. In contrast, our extended model of seg-
mentation vests more trust in the induced schemes, 
assuming that scheme clusters which propose dif-
ferent morpheme boundaries are simply modeling 
different valid morpheme boundaries. And our ex-
tension proposes a single morphological analysis 
containing all hypothesized morpheme boundaries.  
To detect morpheme boundaries, ParaMor 
matches each word, w, in the full vocabulary of a 
corpus against the clusters of schemes which are 
the final output of ParaMor?s paradigm identifica-
tion stage. When a suffix, f, of some scheme-
cluster, C, matches a word-final string of w, i.e. 
fuw .= , ParaMor attempts to replace f in turn with 
each suffix f ?  of C. If the string fu ?.  occurs in 
the full corpus vocabulary, then, on the basis of 
this paradigmatic evidence, ParaMor identifies a 
morpheme boundary in w between u and f . 
For example, to detect morpheme boundaries in 
the Spanish word apoyados ?supports (adjective 
masculine plural)?, ParaMor matches all word-
final strings of apoyados against the candidate suf-
fixes of ParaMor?s induced scheme clusters. The 
word-final strings of apoyados are s, os, dos, ados, 
yados, ?. The scheme clusters that our extended 
version of ParaMor induces include clusters which 
contain schemes very similar to the 1st, 2nd, and 5th 
baseline selected schemes, see Table 1. In particu-
lar, our extended ParaMor identifies separate 
scheme clusters that contain the candidate suffixes: 
s and ?; os and o; and ados and ado. Substituting 
? for s, o for os, or ado for ados yields the Spanish 
string apoyado ?supports (adjective masculine sin-
gular)?. It so happens, that apoyado does occur in 
our Spanish corpus, and so ParaMor has found 
paradigmatic evidence for three morpheme boun-
daries. Crucially, our ParaMor extension from sec-
tion 2.1 that removes schemes which hypothesize 
suffix internal morpheme boundaries correctly dis-
cards all schemes which contained the candidate 
suffix dos. Consequently, no scheme cluster exists 
to incorrectly suggest the morpheme boundary 
*apoya + dos, as the 3rd baseline selected scheme 
would have. Where ParaMor?s baseline segmenta-
tion algorithm would propose three separate analy-
ses of apoyados, one for each detected morpheme 
boundary: apoy +ados, apoyad +os, and apoyado 
+s; our extended segmentation algorithm produces 
the single correct analysis: apoy +ad +o +s.  
It is interesting to note that although each of Pa-
raMor?s individual paradigm models proposes a 
single morpheme boundary, our agglutinative seg-
mentation model can recover multiple boundaries 
in a single word. Using this idea it may be possible 
to quickly adapt Linguistica for agglutinative lan-
guages. Instead of interpreting the sets of stems 
and affixes that Goldsmith?s Linguistica algorithm 
produces as immediate segmentations of words, 
these signatures can be thought of as models of 
paradigms that may generalize to new words. 
Augmenting ParaMor?s Segmentations 
With its focus on the paradigm, ParaMor special-
izes at analyzing inflectional morphology (Monson 
et al, 2007a). Morpho Challenge 2007 requires al-
gorithms to analyze both inflectional and deriva-
tional morphology (Kurimo et al, 2007a; 2007b). 
To compete in the challenge, we combine Pa-
raMor?s morphological segmentations with seg-
mentations from Morfessor (Creutz, 2006), an un-
supervised morphology induction algorithm which 
learns both inflectional and derivational morphol-
ogy. We incorporate the segmentations from Mor-
fessor into the segmentations that the ParaMor sys-
tem produces by straightforwardly adding the Mor-
fessor segmentation for each word as an additional 
separate analysis to those ParaMor produces (Mon-
son et al, 2007b). Morfessor has one free parame-
ter, which we optimize separately for each lan-
guage of Morpho Challenge 2007.  
ParaMor also has several free parameters, in-
cluding the type length parameter and the parame-
ter over stem-final character entropy described in 
section 2.1. We do not adjust any of ParaMor?s pa-
rameters from language to language, but fix them 
at values that produce reasonable Spanish para-
digms and segmentations. As in Monson et al 
(2007b), to avoid adjusting ParaMor?s parameters 
we limit ParaMor?s paradigm induction vocabulary 
to 50,000 frequent types for each language.  
3 Evaluation 
To evaluate our extensions to the ParaMor algo-
rithm, we follow the methodology of the peer op-
erated Morpho Challenge 2007. All segmentations 
produced by our extensions were sent to the Mor-
pho Challenge Organizing Committee (Kurimo et 
al., 2008). The Organizing Committee evaluated 
our segmentations and returned the automatically 
54
calculated quantitative results. Using the evalua-
tion methodology of Morpho Challenge 2007 per-
mits us to compare our algorithms against the un-
supervised morphology induction systems which 
competed in the 2007 Challenge. Of the many al-
gorithms for unsupervised morphology induction 
discussed with the related work in section 1.1, five 
participated in Morpho Challenge 2007. Unless an 
algorithm has been given an explicit name, mor-
phology induction algorithms will be denoted in 
this paper by the name of their lead author. The 
five algorithms which participated in the 2007 
Challenge are: Bernhard (2007), Bordag (2007), 
Zeman (2007), Creutz?s (2006) Morfessor, and Pa-
raMor (2007b). 
Morpho Challenge 2007 had participating algo-
rithms analyze words in four languages: English, 
German, Finnish, and Turkish. The Challenge 
evaluated each algorithm?s morphological analyses 
in two ways. First, a linguistic evaluation measured 
each algorithm?s precision, recall, and F1 at mor-
pheme identification against an answer key of mor-
phologically analyzed word forms. Scores were 
normalized when a system proposed multiple 
analyses of a single word, as our combined Pa-
raMor-Morfessor submissions do. For further de-
tails on the linguistic evaluation in Morpho Chal-
lenge 2007, see Kurimo et al (2007a). The second 
evaluation of Morpho Challenge 2007 was a task 
based evaluation. Each algorithm?s analyses were 
imbedded in an information retrieval (IR) system. 
The IR evaluation consisted of queries over a lan-
guage specific collection of newswire articles. All 
word forms in all queries and all documents were 
replaced with the morphological decompositions of 
each individual analysis algorithm. Separate IR 
tasks were run for English, German, and Finnish, 
but not Turkish. For additional details on the IR 
evaluation of Morpho Challenge 2007 please refer-
ence Kurimo et al (2007b). 
Tables 2 and 3 present, respectively, the lin-
guistic and IR evaluation results. In these two ta-
bles, the top two rows contain results for segmen-
tations produced by versions of ParaMor that in-
clude our extensions. The topmost row in each ta-
ble, labeled ?+P +Seg?, gives the results for our 
fully augmented version of ParaMor, which in-
cludes our two extensions designed to improve 
precision as well as our new segmentation model 
which can propose multiple morpheme boundaries 
in a single analysis of a word form. The second 
row of each table, labeled ?+P ?Seg?, augments Pa-
raMor only with the two enhancements designed to 
improve precision. The third row of each table 
gives the Challenge results for the ParaMor base-
line algorithm. Rows four through seven of each 
table give scores from Morpho Challenge 2007 for 
the best performing unsupervised systems. If mul-
tiple versions of a single algorithm competed in the 
Challenge, the scores reported here are the highest 
F1 or Average Precision score of any algorithm 
variant at a particular task. In all test scenarios but 
Finnish IR, we produced Morfessor segmentations 
to augment ParaMor that are independent of the 
Morfessor runs which competed in Morpho Chal-
lenge. If our Morfessor runs gave a higher F1 or 
Average Precision, then we report this higher 
score. Finally, scores reported on rows eight and 
beyond are from reference algorithms that are not 
unsupervised. Reference algorithms appear in ital-
ics. A double line bisects both Table 2 and Table 3 
horizontally. All results which appear above the 
double line were evaluated after the final deadline 
of Morpho Challenge 2007. In particular, ParaMor 
officially competed only in the English and Ger-
man tracks of the Challenge.  
The Linguistic Evaluation 
Table 2 contains the results from the linguistic 
evaluation of Morpho Challenge. The Morpho 
Challenge Organizing Committee did not provide 
us with data on the statistical significance of the 
results for the enhanced versions of ParaMor. But 
most score differences are statistically signifi-
cant?All F1 differences of more than 0.5 between 
systems which officially competed in Morpho 
Challenge 2007 were statistically significant (Ku-
rimo et al, 2007a).  
In German, Finnish, and Turkish our fully en-
hanced version of ParaMor achieves a higher F1 
than any system that competed in Morpho Chal-
lenge 2007. In English, ParaMor?s precision score 
drags F1 under that of the first place system, Bern-
hard; In Finnish, the Bernhard system?s F1 is likely 
not statistically different from that of our system. 
Our final segmentation algorithm demonstrates 
consistent performance across all four languages. 
In Turkish, where the morpheme recall of other 
unsupervised systems is anomalously low, our al-
gorithm achieves a recall in a range similar to its 
recall scores for the other languages. ParaMor?s ul-
timate recall is double that of any other unsuper-
55
vised Turkish system, leading to an improvement 
in F1 over the next best system, Morfessor alone, 
of 13.5% absolute or 22.0% relative.  
In all four languages, as expected, the combina-
tion of removing short types from the training data, 
and the additional filtering of scheme clusters, 
?+P?, significantly improves precision scores over 
the ParaMor baseline. Allowing multiple mor-
pheme boundaries in a single word, ?+Seg?, in-
creases the number of words ParaMor believes 
share a morpheme. Some of these new words do in 
fact share a morpheme, some, in reality do not. 
Hence, our extension of ParaMor to agglutinative 
sequences of morphemes increases recall but low-
ers precision across all four languages. The effect 
of agglutinative segmentations on F1, however, dif-
fers with language. For the two languages which 
make limited use of suffix sequences, English and 
German, a model which hypothesizes multiple 
morpheme boundaries can only moderately in-
crease recall and does not justify, by F1, the many 
incorrect segmentations which result. On the other 
hand, an agglutinative model significantly im-
proves recall for true agglutinative languages like 
Finnish and Turkish, more than compensating in F1 
for the drop in precision over these languages. But 
in all four languages, the agglutinative version of 
ParaMor outperforms the baseline unenhanced ver-
sion at F1. 
The final row of Table 2 is the evaluation of a 
reference algorithm submitted by Tepper (2007). 
While not an unsupervised algorithm, Tepper?s 
reference parallels ParaMor in augmenting seg-
mentations produced by Morfessor. Where Pa-
raMor augments Morfessor with special attention 
to inflectional morphology, Tepper augments Mor-
fessor with hand crafted morphophonology rules 
that conflate multiple surface forms of the same 
underlying suffix. Like ParaMor, Tepper?s algo-
rithm significantly improves on Morfessor?s recall. 
With two examples of successful system augmen-
tation, we suggest that future research take a closer 
look at building on existing unsupervised mor-
phology induction systems. 
The IR Evaluation 
Turn now to results from the IR evaluation in Ta-
ble 3. Although ParaMor does not fair as well in 
Finnish, in German, the fully enhanced version of 
ParaMor places above the best system from the 
2007 Challenge, Bernhard, while our score on 
English rivals this same best system. Morpho Chal-
lenge 2007 did not measure the statistical signifi-
cance of uninterpolated average precision scores in 
the IR evaluation. It is not clear what feature of Pa-
raMor?s Finnish analyses causes comparatively 
low average precision. Perhaps it is simply that Pa-
raMor attains a lower morpheme recall over Fin-
nish than over English or German. And unfortu-
nately, Morpho Challenge 2007 did not run IR ex-
periments over the other agglutinative language in 
the competition, Turkish. When ParaMor does not 
combine multiple morpheme boundaries into a sin-
gle analysis, as in the baseline and ?+P ?Seg? sce-
Table 2. Unsupervised morphology induction systems evaluated for precision (P), recall (R), and F1 at morpheme 
identification using the methodology of the linguistic competition of Morpho Challenge 2007. 
English German Finnish Turkish 
 P R F1 P R F1 P R F1 P R F1 
 +P +Seg 50.6 63.3 56.3 49.5 59.5 54.1 49.8 47.3 48.5 51.9 52.1 52.0 
 +P ?Seg 56.2 60.9 58.5 57.4 53.5 55.4 60.5 33.9 43.5 62.0 38.2 47.3 
ParaMor  
&        
Morfessor 
Baseline 41.6 65.1 50.7 51.5 55.6 53.4 55.0 35.6 43.2 53.2 41.6 46.7 
Bernhard 61.6 60.0 60.8 49.1 57.4 52.9 59.7 40.4 48.2 73.7 14.8 24.7 
Bordag 59.7 32.1 41.8 60.5 41.6 49.3 71.3 24.4 36.4 81.3 17.6 28.9 
Morfessor 82.2 33.1 47.2 67.6 36.9 47.8 76.8 27.5 40.6 73.9 26.1 38.5 
Zeman 53.0 42.1 46.9 52.8 28.5 37.0 58.8 20.9 30.9 65.8 18.8 29.2 
Tepper 69.2 52.6 59.8 - - - 62.0 46.2 53.0 70.3 43.0 53.3 
 
56
narios, average precision is comparatively poor. 
Where the linguistic evaluation did not always pe-
nalize a system for proposing multiple partial 
analyses, real NLP applications, such as IR, can. 
The reference algorithms for the IR evaluation 
are: Dummy, no morphological analysis; Oracle, 
where all words in the queries and documents for 
which the linguistic answer key contains an entry 
are replaced with that answer; Porter, the standard 
English Porter stemmer; and Tepper described 
above. While the hand built Porter stemmer still 
outperforms the best unsupervised systems on Eng-
lish, these same best unsupervised systems outper-
form both the Dummy and Oracle references for all 
three evaluated languages?strong evidence that 
unsupervised induction algorithms are not only 
better than no morphological analysis, but that they 
are better than incomplete analysis as well.  
4 Conclusions and Future Directions 
Augmenting ParaMor with an agglutinative model 
of segmentation produces an unsupervised mor-
phology induction system with consistent and 
strong performance at morpheme identification 
across all four languages of Morpho Challenge 
2007. By first cleaning up the paradigm models 
that ParaMor learns, we raise ParaMor?s segmenta-
tion precision and allow the agglutinative model to 
significantly improve ParaMor?s morpheme recall.  
Looking forward to future improvements, we 
examined by hand the final set of scheme clusters 
that the current version of ParaMor produces over 
our newswire corpus of 50,000 Spanish types. Pa-
raMor?s paradigm identification stage outputs 41 
separate clusters. Among these final scheme clus-
ters are those which model all major productive 
paradigms of Spanish. In fact, there are often mul-
tiple scheme clusters which model portions of the 
same true paradigm. As an extreme case, 12 sepa-
rate scheme clusters contain suffixes from the 
Spanish ar verbal paradigm. Relaxing restrictions 
on ParaMor?s clustering algorithm (Monson et al, 
2007a) may address this paradigm fragmentation.  
The second significant shortcoming which sur-
faces among ParaMor?s 41 final scheme clusters is 
that ParaMor currently does not address morpho-
phonology. Among the final scheme clusters, 12 
attempt to model morphophonological change by 
incorporating the phonological change either into 
the stems or into the suffixes of the scheme cluster. 
But ParaMor currently has no mechanism for de-
tecting when a cluster is modeling morphophonol-
ogy. Perhaps ideas on morphophonology from 
Goldsmith (2006) could be adapted to work with 
the ParaMor algorithm. Finally, we plan to look at 
scaling the size of the vocabulary used both during 
paradigm induction and during morpheme segmen-
tation. We are particularly interested in the possi-
bility that ParaMor may  be able to identify para-
digms from much less data than 50,000 types. 
Acknowledgements 
We kindly thank Mikko Kurimo, Ville Turunen, 
Matti Varjokallio, and the full Organizing Com-
mittee of Morpho Challenge 2007, for running the 
evaluations of ParaMor. These dedicated workers 
produced impressively fast turn around for evalua-
tions on sometimes rather short notice. 
The research described in this paper was sup-
ported by NSF grants IIS-0121631 (AVENUE) and 
IIS-0534217 (LETRAS), with supplemental fund-
ing from NSF?s Office of Polar Programs and Of-
fice of International Science and Education. 
Table 3. Unsupervised morphology induction sys-
tems evaluated for uninterpolated average precision 
using the methodology of the IR competition of 
Morpho Challenge 2007. These results use Okapi 
term weighting (Kurimo et al, 2008b). 
*Only a subset of the words which occurred in the 
IR evaluation of this language was analyzed by this 
system.  
 Eng. Ger. Finn. Tur. 
 +P +Seg 39.3 48.4 42.6 - 
 +P ?Seg 35.1 43.1 37.1 - 
ParaMor 
&        
Morfessor 
Baseline 34.4 40.1 35.9 - 
Bernhard 39.4 47.3 49.2 - 
Bordag 34.0 43.1 43.1 - 
Morfessor 38.8 46.0 44.1 - 
Zeman  26.7*  25.7*  28.1* - 
Dummy 31.2 32.3 32.7 - 
Oracle 37.7 34.7 43.1 - 
Porter 40.8 - - - 
Tepper  37.3* - - - 
 
57
References 
Bernhard, Delphine. Simple Morpheme Labeling in Un-
supervised Morpheme Analysis. Working Notes for 
the CLEF 2007 Workshop. Budapest, Hungary, 2007. 
Bordag, Stefan. Unsupervised and Knowledge-free 
Morpheme Segmentation and Analysis. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007. 
Brent, Michael R., Sreerama K. Murthy, and Andrew 
Lundberg. Discovering Morphemic Suffixes: A Case 
Study in MDL Induction. The Fifth International 
Workshop on Artificial Intelligence and Statistics. 
Fort Lauderdale, Florida, 1995.  
Creutz, Mathias. Induction of the Morphology of Natu-
ral Language: Unsupervised Morpheme Segmenta-
tion with Application to Automatic Speech Recogni-
tion. Ph.D. Thesis. Computer and Information Sci-
ence, Report D13. Helsinki: University of Technol-
ogy, Espoo, Finland, 2006. 
Demberg, Vera, Helmut Schmid, and Gregor M?hler. 
Phonological Constraints and Morphological Pre-
processing for Grapheme-to-Phoneme Conversion. 
Association for Computational Linguistics. Prague, 
Czech Republic, 2007. 
Dryer, Matthew S. Prefixing vs. Suffixing in Inflec-
tional Morphology.  In The World Atlas of Language 
Structures. Eds. Martin Haspelmath, Matthew S. 
Dryer, David Gil, and Bernard Comrie. 2005. 
Goldsmith, John. Unsupervised Learning of the Mor-
phology of a Natural Language. Computational Lin-
guistics. 27.2:153-198. 2001. 
Goldsmith, John. An Algorithm for the Unsupervised 
Learning of Morphology. Natural Language Engi-
neering. 12.4:335-351. 2006. 
Goldsmith, John, and Yu Hu. From Signatures to Finite 
State Automata. Paper presented at the Midwest 
Computational Linguistics Colloquium. Blooming-
ton, Indiana, 2004. 
Goldwater, Sharon, and David McClosky. Improving 
Statistic MT through Morphological Analysis. Em-
pirical Methods in Natural Language Processing. 
Vancouver, Canada, 2005. 
Hafer, Margaret A. and Stephen F. Weiss. Word Seg-
mentation by Letter Successor Varieties. Information 
Storage and Retrieval, 10:371-385. 1974. 
Harris, Zellig. From Phoneme to Morpheme. Language 
31.2:190-222. 1955. Reprinted in Harris (1970). 
Harris, Zellig. Papers in Structural and Transforma-
tional Linguists. Ed. D. Reidel, Dordrecht. 1970. 
Johnson, Howard, and Joel Martin. Unsupervised 
Learning of Morphology for English and Inuktitut. 
Human Language Technology Conference / North 
American Chapter of the Association for Computa-
tional Linguistics. Edmonton, Canada, 2003. 
Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 
Unsupervised Morpheme Analysis Evaluation by a 
Comparison to a Linguistic Gold Standard ? Morpho 
Challenge 2007. Working Notes for the CLEF 2007 
Workshop. Budapest, Hungary, 2007a. 
Kurimo, Mikko, Mathias Creutz, and Ville Turunen. 
Unsupervised Morpheme Analysis Evaluation by IR 
Experiments ? Morpho Challenge 2007. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007b. 
Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 
Unsupervised Morpheme Analysis -- Morpho Chal-
lenge 2007. January 10, 2008. <http://www.cis.hut.-
fi/morphochallenge2007/>. 2008. 
Monson, Christian, Jaime Carbonell, Alon Lavie, and 
Lori Levin. ParaMor: Minimally Supervised Induc-
tion of Paradigm Structure and Morphological 
Analysis. Computing and Historical Phonology: The 
Ninth Meeting of the ACL Special Interest Group in 
Computational Morphology and Phonology. Prague, 
Czech Republic, 2007a. 
Monson, Christian, Jaime Carbonell, Alon Lavie, and 
Lori Levin. ParaMor: Finding Paradigms across 
Morphology. Working Notes for the CLEF 2007 
Workshop. Budapest, Hungary, 2007b. 
Schone, Patrick, and Daniel Jurafsky. Knowledge-Free 
Induction of Inflectional Morphologies. North 
American Chapter of the Association for Computa-
tional Linguistics. Pittsburgh, Pennsylvania, 2001. 
Snover, Matthew G. An Unsupervised Knowledge Free 
Algorithm for the Learning of Morphology in Natural 
Languages. M.S. Thesis. Computer Science, Sever 
Institute of Technology, Washington University, 
Saint Louis, Missouri, 2002. 
Tepper, Michael A. Using Hand-Written Rewrite Rules 
to Induce Underlying Morphology. Working Notes 
for the CLEF 2007 Workshop. Budapest, Hungary, 
2007. 
Wicentowski, Richard. Modeling and Learning Multi-
lingual Inflectional Morphology in a Minimally Su-
pervised Framework. Ph.D. Thesis. Johns Hopkins 
University, Baltimore, Maryland, 2002. 
Zeman, Daniel. Unsupervised Acquiring of Morpho-
logical Paradigms from Tokenized Text. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007. 
 
 
58
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 58?61,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Proactive Learning for Building Machine Translation Systems for Minority
Languages
Vamshi Ambati
vamshi@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Jaime Carbonell
jgc@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
Building machine translation (MT) for many
minority languages in the world is a serious
challenge. For many minor languages there is
little machine readable text, few knowledge-
able linguists, and little money available for
MT development. For these reasons, it be-
comes very important for an MT system to
make best use of its resources, both labeled
and unlabeled, in building a quality system.
In this paper we argue that traditional active
learning setup may not be the right fit for seek-
ing annotations required for building a Syn-
tax Based MT system for minority languages.
We posit that a relatively new variant of active
learning, Proactive Learning, is more suitable
for this task.
1 Introduction
Speakers of minority languages could benefit from
fluent machine translation (MT) between their native
tongue and the dominant language of their region.
But scarcity in capital and know-how has largely
restricted machine translation to the dominant lan-
guages of first world nations. To lower the barriers
surrounding MT system creation, we must reduce
the time and resources needed to develop MT for
new language pairs. Syntax based MT has proven
to be a good choice for minority language scenario
(Lavie et al, 2003). While the amount of paral-
lel data required to build such systems is orders of
magnitude smaller than corresponding phrase based
statistical systems (Koehn et al, 2003), the variety
of linguistic annotation required is greater. Syntax
based MT systems require lexicons that provide cov-
erage for the target translations, synchronous gram-
mar rules that define the divergences in word-order
across the language-pair. In case of minority lan-
guages one can only expect to find meagre amount
of such data, if any. Building such resources effec-
tively, within a constrained budget, and deploying an
MT system is the need of the day.
We first consider ?Active Learning? (AL) as a
framework for building annotated data for the task
of MT. However, AL relies on unrealistic assump-
tions related to the annotation tasks. For instance,
AL assumes there is a unique omniscient oracle. In
MT, it is possible and more general to have multiple
sources of information with differing reliabilities or
areas of expertise. A literate bilingual speaker with
no extra training can produce translations for word,
phrase or sentences and even align them. But it re-
quires a trained linguist to produce syntactic parse
trees. AL also assumes that the single oracle is per-
fect, always providing a correct answer when re-
quested. In reality, an oracle (human or machine)
may be incorrect (fallible) with a probability that
should be a function of the difficulty of the question.
There is also no notion of cost associated with the
annotation task, that varies across the input space.
But in MT, it is easy to see that length of a sentence
and cost of translation are superlinear. Also not all
annotation tasks for MT have the same level of dif-
ficulty or cost. For example, it is relatively cheap
to ask a bilingual speaker whether a word, phrase
or sentence was correctly translated by the system,
but a bit more expensive to ask for a correction. As-
sumptions like these render active learning unsuit-
58
able for our task at hand which is building an MT
system for languages with limited resources. We
make the case for ?Proactive Learning? (Donmez
and Carbonell, 2008) as a solution for this scenario.
In the rest of the paper, we discuss syntax based
MT approach in Section 2. In Section 3 we first
discuss active learning approaches for MT and de-
tail the characteristics of MT for minority languages
problem that render traditional active learning un-
suitable for practical purposes. In Section 4 we dis-
cuss proactive learning as a potential solution for the
current problem. We conclude with some challenges
that still remain in applying proactive learning for
MT.
2 Syntax Based Machine Translation
In recent years, corpus based approaches to ma-
chine translation have become predominant, with
Phrase Based Statistical Machine Translation (PB-
SMT) (Koehn et al, 2003) being the most ac-
tively progressing area. Recent research in syn-
tax based machine translation (Yamada and Knight,
2001; Chiang, 2005) incorporates syntactic informa-
tion to ameliorate the reordering problem faced by
PB-SMT approaches. While traditional approaches
to syntax based MT were dependent on availabil-
ity of manual grammar, more recent approaches op-
erate within the resources of PB-SMT and induce
hierarchical or linguistic grammars from existing
phrasal units, to provide better generality and struc-
ture for reordering (Yamada and Knight, 2001; Chi-
ang, 2005; Wu, 1997).
2.1 Resources for Syntax MT
Syntax based approaches to MT seek to leverage the
structure of natural language to automatically induce
MT systems. Depending upon the MT system and
the paradigm, the resource requirements may vary
and could also include modules such as morpholog-
ical analyzers, sense disambiguation modules, gen-
erators etc. A detailed discussion of the comprehen-
sive pipeline, may be out of the scope of this pa-
per, more so because such resources can not be ex-
pected in a low-resource language scenario. We only
focus on the quintessential set of modules for MT
pipeline - data acquisition, word-alignment, syntac-
tic analysis etc. The resources can broadly be cat-
egorized as ?monolingual? vs ?bilingual? depending
upon whether it requires knowledge in one language
or both languages for annotation. A sample of the
different kinds of data and annotation that is ex-
pected by an MT system is shown below. Each of
the additional information can be seen as extra an-
notations for the ?Source? sentence. The language
of target in the example is ?Hindi?.
? Source: John ate an apple
? Target: John ne ek seb khaya
? Alignment: (1,1),(2,5),(3,3),(4,4)
? SourceParse: (S (NP (NNP John)) (VP (VBD
ate) (NP (DT an) (NN apple))))
? Lexicon: (seb? apple),(ate? khaya)
? Grammar: VP: V NP? NP V
3 Active Learning for MT
Modern syntax based MT rides on the success of
both Statistical Machine Translation and Statistical
Parsing. Active learning has been applied to Statis-
tical Parsing (Hwa, 2004; Baldridge and Osborne,
2003) to improve sample selection for manual anno-
tation. In case of MT, active learning has remained
largely unexplored. Some attempts include training
multiple statistical MT systems on varying amounts
of data, and exploring a committee based selection
for re-ranking the data to be translated and included
for re-training. But this does not apply to training in
a low-resource scenario where data is scarce.
In the rest of the section we discuss the different
scenarios that arise in gathering of annotation for
MT under a traditional ?active learning? setup and
discuss the characteristics of the task that render it
difficult.
3.1 Multiple Oracles
For each of the sub-tasks of annotation, in reality
we have multiple sources of information or multi-
ple oracles. We can elicit translations for building
a parallel corpus from bilingual speakers who speak
both the languages with certain accuracy or from a
linguist who is well educated in the formal sense
of the languages. With the success of collabora-
tive sites like Amazon?s ?Mechanical Turk? 1, one
1http://www.mturk.com/
59
can provide the task of annotation to multiple ora-
cles on the internet (Snow et al, 2008). The task
of word alignment can be posed in a similar fash-
ion too. More interestingly, there are statistical tools
like GIZA 2 that take as input un-annotated paral-
lel data and propose automatic correspondences be-
tween words in the language-pair, giving scope to
?machine oracles?.
3.2 Varying Quality and Reliability
Oracles also vary on the correctness of the answers
they provide (quality) as well as their availability
(robustness) to answer. One typical distinction is
?human oracles? vs ?machine oracles?. Human or-
acle produce higher quality annotations when com-
pared to a machine oracle. We would prefer a tree
bank of parse trees that were manually created over
automatically generated tree banks. Similar is the
case with word-alignment and other tasks of trans-
lation. Some oracles are ?reluctant? to produce an
output, for example parsers tend to break on really
long sentences, but when they produce an output
we can associate some confidence with it about the
quality. One can expect a human oracle to produce
parse trees for long sentences, but the quality could
be questionable.
3.3 Non-uniform costs
Each of the annotation tasks has a non-uniform cost
associated with it, the distribution of which is de-
pendent upon the difficulty over the input space.
Clearly, length of the sentence is a good indicator of
the cost. It takes much longer to translate a sentence
of 100 words than to translate one with 10 words. It
takes at least twice as long to create word-alignment
correspondences for a sentence-pair with 40 tokens
than a pair with 20 tokens. Similarly, a human takes
much longer to manually create parse tree for a long
sentence than a short sentence.
It is also the case that not all oracles have the
same non-uniform cost distribution over the input
space. Some oracles are more expensive than the
others. For example a practicing linguist?s time is
perhaps costlier than that of an undergraduate who
is a bilingual speaker. As noticed above, this may
reflect upon the quality of annotation for the task,
2http://www.fjoch.com/GIZA++.html
but sometimes a tradeoff to make is cost vs qual-
ity. We can not afford to introduce a grammar rule
of low-quality into the system, but can possibly do
away with an incorrect word-correspondence link.
4 Proactive Learning
Proactive learning (Donmez and Carbonell, 2008) is
a generalization of active learning designed to re-
lax unrealistic assumptions and thereby reach prac-
tical applications. Active learning seeks to select the
most informative unlabeled instances and ask an om-
niscient oracle for their labels, so as to retrain the
learning algorithm maximizing accuracy. However,
the oracle is assumed to be infallible (never wrong),
indefatigable (always answers), individual (only one
oracle), and insensitive to costs (always free or al-
ways charges the same). Proactive learning relaxes
all these four assumptions, relying on a decision-
theoretic approach to jointly select the optimal or-
acle and instance, by casting the problem as a utility
optimization problem subject to a budget constraint.
maximize E[V (S)] subject to B
maxS?ULE[V (S)]? ?(
?
k
tk ? Ck)s.t
?
k
tk ? Ck = B
The above equation can be interpreted as maximiz-
ing the expected value of labeling the input set S
under the budget constraint B. The subscript k de-
notes the oracle from which the answer was elicited
under a cost function C. A greedy approximation of
the above results in the equation 1, where Ek[V (x)]
is the expected value of information of the example
x corresponding to oracle k. One can design inter-
esting functions that calculate V (x) in case of MT.
For example, selecting short sentences with an unre-
solved linguistic issue could maximize the utility of
the data at a low cost.
(x?, k?) = argmaxx?UEk[V (x)] subject to B (1)
We now turn to how proactive learning framework
helps solve the issues raised for active learning in
MT in section 3. We can address the issue of multi-
ple oracles where one oracle is fallible or reluctant to
answer, by factoring into Equation 2 its probability
60
function for returning an answer. The score returned
by such a factoring can be called the utility associ-
ated with that input for a particular oracle. We call
this U(x, k). A similar factorization can be done in
order to address the issue of oracles that are fallible.
U(x, k) = P (ans|x, k) ? V (x)? Ck
(x?, k?) = argmaxx?U U(x, k)
Since we do not have the P (ans/x, k) distribu-
tion information for each oracle, proactive learning
proposes to discover this in a discovery phase under
some allocated budget Bd. Once we have an esti-
mate from the discovery phase, the rest of the label-
ing proceeds according to the optimization function.
For more details of the algorithms refer (Donmez
and Carbonell, 2008). Finally, we can also relax the
assumption of uniform cost per annotation, but re-
placing the Ck term in the above equations with a
Cnon?unifk function denoting the non-uniform cost
function associated with the oracle.
5 Future Challenges
While proactive learning is a good framework for
building MT systems for minority languages, there
are however a few issues that still remain that need
careful attention.
Joint Utility: In a complex system like MT where
different models combine forces to produce the
translation we have a situation where we need to op-
timize not only for an input and the oracle, but also
the kind of annotation we would like to elicit. For
example given a particular translation model, we do
not know if the most optimal thing at a given point is
to seek more word-alignment annotation from a par-
ticular ?alignment oracle? or seek parse annotation
from a ?parsing oracle?.
Machine oracles vs Human oracles: The assump-
tion with an oracle is that the knowledge and exper-
tise of the oracle does not change over the course of
annotation. We do not assume that the oracle learns
over time and hence the speed of annotation or per-
haps the accuracy of annotation increases. This is
however very common with ?machine oracles?. For
example, an oracle that suggests automatic align-
ment of data using statistical concordances may ini-
tially be unreliable due to the less amount of data it is
trained on, but as it receives more data, the estimates
get better and so the system gets more reliable.
Evaluation: Performance of underlying system is
typically done by well understood metrics like pre-
cision/recall. However, evaluation of MT output
is quite subjective and automatic evaluation met-
rics may be too coarse to distinguish the nuances
of translation. This becomes quite important in
an online active learning setup, where we add an-
notated data incrementally, and the immediately
trained translation models are not sufficient to make
a difference in the scores of the evaluation metric.
References
Jason Baldridge and Miles Osborne. 2003. Active learn-
ing for hpsg parse selection. In Proc. of the HLT-
NAACL 2003, pages 17?24, Morristown, NJ, USA.
Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. 43rd ACL,
pages 263?270, Morristown, NJ, USA. Association for
Computational Linguistics.
Pinar Donmez and Jaime G. Carbonell. 2008. Proactive
learning: cost-sensitive active learning with multiple
imperfect oracles. In CIKM ?08, pages 619?628, New
York, NY, USA. ACM.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Comput. Linguist., 30(3):253?276.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the HLT/NAACL, Edomonton, Canada.
Alon Lavie, Stephan Vogel, Lori Levin, Erik Peter-
son, Katharina Probst, Ariadna Font Llitjo?s, Rachel
Reynolds, Jaime Carbonell, and Richard Cohen. 2003.
Experiments with a hindi-to-english transfer-based mt
system under a miserly data scenario. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 2(2):143?163.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the EMNLP 2008, pages 254?
263, Honolulu, Hawaii, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL ?01,
pages 523?530, Morristown, NJ, USA. Association for
Computational Linguistics.
61
Coling 2010: Poster Volume, pages 320?328,
Beijing, August 2010
Monolingual Distributional Profiles for Word Substitution in Machine
Translation
Rashmi Gangadharaiah
rgangadh@cs.cmu.edu
Ralf D. Brown
ralf@cs.cmu.edu
Language Technologies Institute,
Carnegie Mellon University
Jaime Carbonell
jgc@cs.cmu.edu
Abstract
Out-of-vocabulary (OOV) words present a
significant challenge for Machine Trans-
lation. For low-resource languages, lim-
ited training data increases the frequency
of OOV words and this degrades the qual-
ity of the translations. Past approaches
have suggested using stems or synonyms
for OOV words. Unlike the previous
methods, we show how to handle not just
the OOV words but rare words as well
in an Example-based Machine Transla-
tion (EBMT) paradigm. Presence of OOV
words and rare words in the input sentence
prevents the system from finding longer
phrasal matches and produces low qual-
ity translations due to less reliable lan-
guage model estimates. The proposed
method requires only a monolingual cor-
pus of the source language to find can-
didate replacements. A new framework
is introduced to score and rank the re-
placements by efficiently combining fea-
tures extracted for the candidate replace-
ments. A lattice representation scheme al-
lows the decoder to select from a beam
of possible replacement candidates. The
new framework gives statistically signif-
icant improvements in English-Chinese
and English-Haitian translation systems.
1 Introduction
An EBMT system makes use of a parallel corpus
to translate new sentences. Each input sentence
is matched against the source side of a training
corpus. When matches are found, the correspond-
ing translations in the target language are obtained
through sub-sentential alignment. In our EBMT
system, the final translation is obtained by com-
bining the partial target translations using a sta-
tistical target Language Model. EBMT systems,
like other data-driven approaches, require large
amounts of data to function well (Brown, 2000).
Having more training data is beneficial re-
sulting in log-linear improvement in translation
quality for corpus-based methods (EBMT, SMT).
Koehn (2002) shows translation scores for a num-
ber of language pairs with different training sizes
translated using the Pharaoh SMT toolkit (Koehn
et al, 2003). However, obtaining sizable paral-
lel corpora for many languages is time-consuming
and expensive. For rare languages, finding bilin-
gual speakers becomes especially difficult.
One of the main reasons for low quality transla-
tions is the presence of large number of OOV and
rare words (low frequency words in the training
corpus). Variation in domain and errors in spelling
increase the number of OOV words. Many of the
present translation systems either ignore these un-
known words or leave them untranslated in the fi-
nal target translation. When data is limited, the
number of OOV words increases, leading to the
poor performance of the translation models and
the language models due to the absence of longer
sequences of source word matches and less reli-
able language model estimates.
Approaches in the past have suggested using
stems or synonyms for OOV words as replace-
ments (Yang and Kirchhoff, 2006). Similarity
measures have been used to find words that are
closely related (Marton et al, 2009). For morpho-
320
logically rich languages, the OOV word is mor-
phologically analyzed and the stem is used as its
replacement (Popovic? and Ney, 2004).
This paper presents a simpler method inspired
by the Context-based MT approach (Carbonell et
al., 2006) to improve translation quality. The
method requires a large source language mono-
lingual corpus and does not require any other
language dependent resources to obtain replace-
ments. Approaches suggested in the past only
concentrated on finding replacements for the OOV
words and not the rare words. This paper pro-
poses a unified method to find possible replace-
ments for OOV words as well as rare words based
on the context in which these words appear. In
the case of rare words, the translated sentence is
traced back to find the origin of the translations
and the target translations of the replacements are
replaced with the translations of the rare words. In
the case of OOV words, the target translations are
replaced by the OOV word itself. The main idea
for adopting this approach is the belief that the
EBMT system will be able to find longer phrasal
matches and that the language model will be able
to give better probability estimates while decod-
ing if it is not forced to fragment text at OOV and
rare-word boundaries. This method is highly ben-
eficial for low-resource languages that do not have
morphological analysers or Part-of-Speech (POS)
taggers and in cases where the similarity measures
proposed in the past do not find closely related
words for certain OOV words.
The rest of the paper is organized as follows.
The next section (Section 2) discusses related
work in handling OOV words. Section 3 describes
the method adopted in this paper. Section 4 de-
scribes the experimental setup. Section 5 reports
the results obtained with the new framework for
English-Chinese and English-Haitian translation
systems. Section 6 concludes and suggests pos-
sible future work.
2 Related Work
Orthographic and morpho-syntactic techniques
for preprocessing training and test data have been
shown to reduce OOV word rates. Popovic?
and Ney (2004) demonstrated this on rich mor-
phological languages in an SMT system. They
introduced different types of transformations to
the verbs to reduce the number of unseen word
forms. Habash (2008) addresses spelling, name-
transliteration OOVs and morphological OOVs in
an Arabic-English Machine Translation system.
Phrases with the OOV replacements in the phrase
table of a phrase-based SMT system were ?recy-
cled? to create new phrases in which the replace-
ments were replaced by the OOV words.
Yang and Kirchhoff (2006) proposed a back-
off model for phrase-based SMT that translated
word forms in the source language by hierarchi-
cal morphological phrase level abstractions. If
an unknown word was found, the word was first
stemmed and the phrase table entries for words
sharing the same stem were modified by replacing
the words with their stems. If a phrase entry or a
single word phrase was found, the corresponding
translation was used, otherwise the model backed
off to the next level and applied compound split-
ting to the unknown word. The phrase table in-
cluded phrasal entries based on full word forms as
well as stemmed and split counterparts.
Vilar et al (2007) performed the translation
process treating both the source and target sen-
tences as a string of letters. Hence, there are
no unknown words when carrying out the actual
translation of a test corpus. The word-based sys-
tem did most of the translation work and the letter-
based system translated the OOV words.
The method proposed in this work to han-
dle OOV and rare words is very similar to the
method adopted by Carbonell et al (2006) to gen-
erate word and phrasal synonyms in their Context-
based MT system. Context-based MT does not
require parallel text but requires a large monolin-
gual target language corpus and a fullform bilin-
gual dictionary. The main principle is to find those
n-gram candidate translations from a large target
corpus that contain as many potential word and
phrase translations of the source text from the dic-
tionary and fewer spurious content words. The
overlap decoder combines the target n-gram trans-
lation candidates by finding maximal left and right
overlaps with the translation candidates of the pre-
vious and following n-grams. When the overlap
decoder does not find coherent sequences of over-
lapping target n-grams, more candidate transla-
321
tions are obtained by substituting words or phrases
in the target n-grams by their synonyms.
Barzilay and McKeown (2001) and Callison-
Burch et al (2006) extracted paraphrases from
monolingual parallel corpus where multiple trans-
lations were present for the same source. The syn-
onym generation in Carbonell et al (2006) differs
from the above in that it does not require paral-
lel resources containing multiple translations for
the same source language. In Carbonell et al
(2006), a list of paired left and right contexts that
contain the desired word or phrase are extracted
from the monolingual corpus. The same corpus
is used to find other words and phrases that fit the
paired contexts in the list. The idea is based on the
distributional hypothesis which states that words
with similar meanings tend to appear in similar
contexts (Harris, 1954). Hence, their approach
performed synonym generation on the target lan-
guage to find translation candidates that would
provide maximal overlap during decoding.
Marton et al (2009) proposed an approach sim-
ilar to Carbonell et al (2006) to obtain replace-
ments for OOV words, where monolingual dis-
tributional profiles for OOV words were con-
structed. Hence, the approach was applied on the
source language side as opposed to Carbonell et
al. (2006) which worked on the target language.
Only similarity scores and no other features were
used to rank the paraphrases (or replacements)
that occured in similar contexts. The high rank-
ing paraphrases were used to augment the phrase
table of phrase-based SMT.
All of the previously suggested methods only
handle OOV words (except Carbonell et al (2006)
which handles low frequency target phrases) and
no attempt is made to handle rare words. Many of
the methods explained above directly modify the
training corpus (or phrase table in phrase-based
SMT) increasing the size of the corpus. Our
method clusters words and phrases based on their
context as described by Carbonell et al (2006) but
uses the clustered words as replacements for not
just the OOV words but also for the rare words
on the source language side. Our method does
not make use of any morphological analysers,
POS taggers or manually created dictionaries
as they may not be available for many rare or
low-resource languages. The translation of the
replacements in the final decoded target sentence
is replaced by the translation of the original word
(or the source word itself in the OOV case),
hence, we do not specifically look for synonyms.
The only condition for a word to be a candidate
replacement is that its left and right context need
to match with that of the OOV/rare-word. Hence,
the clustered words could have different semantic
relations. For example,
(cluster1):?laugh, giggle, chuckle, cry, weep?
where ?laugh, giggle, chuckle? are synonyms and
?cry, weep? are antonyms of ?laugh?.
Clusters can also contain hypernyms (or hy-
ponyms), meronyms (or holonyms), troponyms
and coordinate terms along with synonyms and
antonyms. For example,
(cluster2):?country, region, place, area, dis-
trict, state, zone, United States, Canada, Korea,
Malaysia?.
where ?country? is a hypernym of ?United
States/Canada/Korea/Malaysia?. ?district? is a
meronym of ?state?. ?United States, Canada,
Korea, Malaysia? are coordinate terms sharing
?country? as their hypernym.
The contributions made by the paper are three-
fold: first, replacements are found for not just the
OOV words but for the rare words as well. Sec-
ond, the framework used allows scoring replace-
ments based on multiple features to permit op-
timization. Third, instead of directly modifying
the training corpus by replacing the candidate re-
placements by the OOV words, a new representa-
tion scheme is used for the test sentences to effi-
ciently handle a beam of possible replacements.
3 Proposed Method
Like Marton et al (2009), only a large monolin-
gual corpus is required to extract candidate re-
placements. To retrieve more replacements, the
monolingual corpus is pre-processed by first gen-
eralizing numbers, months and years by NUM-
BER, MONTH and YEAR tags, respectively.
322
3.1 OOV and Rare words
Words in the test sentence (new source sentence
to be translated) that do not appear in the training
corpus are called OOV words. Words in the test
sentence that appear less thanK times in the train-
ing corpus are considered as rare words (in this
paper K = 3). The method presented in the fol-
lowing sections holds for both OOV as well as rare
words. In the case of rare words, the final transla-
tion is postprocessed (Section 3.7) to include the
translation of the rare word.
The procedure adopted will be explained with
a real example T (the rest of the sentence is
removed for the sake of clarity) encountered in
the test data with ?hawks? as the OOV word,
T :a mobile base , hitting three hawks with
one arrow over the past few years ...
3.2 Context
As the goal is to obtain longer target phrasal trans-
lations for the test sentence before decoding,
only words that fit the left and right context of the
OOV/rare-word in the test sentence are extracted.
Unlike Marton et al (2009) where a context list
for each OOV is generated from the contexts
of their replacements, this paper uses only the
left and right context of the OOV/rare-word.
The default window size for the context is five
words (two words to the left and two words to the
right of the OOV/rare-word). If the windowed
words contain only function words, the window
is incremented until at least one content word is
present in the resulting context. This enables one
to find sensible replacements that fit the context
well. The contexts for T are:
Left-context (L): hitting three
Right-context (R): with one arrow
The above contexts are further processed to
generalize the numbers by a NUMBER tag
to produce more candidate replacements. The
resulting contexts are now:
Left-context (L): hitting NUMBER
Right-context (R): with NUMBER arrow
As a single L ? R context is used, a far
smaller number of replacements are extracted.
3.3 Finding Candidate replacements
The monolingual corpus (ML) of the source lan-
guage is used to find words and phrases (Xk) that
fitLXkR i.e., withL as its left context and/orR as
its right context. The maximum length for Xk is
set to 3 currently. The replacements are further fil-
tered to obtain only those replacements that con-
tain at least one content word. As illustrated ear-
lier, the resulting replacement candidates are not
necessarily synonyms.
3.4 Features
A local context of two to three words to the left
of an OOV/rare-word (wordi) and two to three
words to the right of wordi contain sufficient
clues for the word,wordi. Hence, local contextual
features are used to score each of the replacement
candidates (Xi,k) of wordi. Each Xi,k extracted
in the previous step is converted to a feature vector
containing 11 contextual features. Certainly more
features can be extracted with additional knowl-
edge sources. The framework allows adding more
features, but for the present results, only these 11
features were used.
As our aim is to assist the translation system in
finding longer target phrasal matches, the features
are constructed from the occurrence statistics of
Xi,k from the bilingual training corpus (BL). If a
candidate replacement does not occur in the BL,
then it is removed from the list of possible replace-
ment candidates.
Frequency counts for the features of a partic-
ular replacement, Xi,k, extracted in the context
of Li,?2Li,?1 (two preceding words of wordi)
and Ri,+1Ri,+2 (two following words of wordi)
(the remaining words in the left and right context
of wordi are not used for feature extraction) are
obtained as follows:
f1: frequency of Xi,kRi,+1
f2: frequency of Li,?1Xi,k
f3: frequency of Li,?1Xi,kRi,+1
f4: frequency of Li,?2Li,?1Xi,k
f5: frequency of Xi,kRi,+1Ri,+2
f6: frequency of Li,?2Li,?1Xi,kRi,+1
323
f7: frequency of Li,?1Xi,kRi,+1Ri,+2
f8: frequency of Li,?2Li,?1Xi,kRi,+1Ri,+2
f9: frequency of Xi,k in ML
f10: frequency of Xi,k in BL
f11: number of feature values (f1, ..f10) > 0
f11 is a vote feature which counts the num-
ber of features (f1 ... f10) that have a value
greater than zero. The features are normalized
to fall within [0, 1]. The sentences in ML, BL
and test data are padded with two begin markers
and two end markers for obtaining counts for
OOV/rare-words that appear at the beginning or
end of a test sentence.
3.5 Representation
Before we go on to explaining the lattice repre-
sentation, we would like to make a small clarifica-
tion in the terminalogy used. In the MT commu-
nity, a lattice usually refers to the list of possible
partially-overlapping target translations for each
possible source n?gram phrase in the input sen-
tence. Since we are using the term lattice to also
refer to the possible paths through the input sen-
tence, we will call the lattice used by the decoder,
the ?decoding lattice?. The lattice obtained from
the input sentence representing possible replace-
ment candidates will be called the ?input lattice?.
An input lattice (Figure 1) is constructed with
a beam of replacements for the OOV and rare
words. Each replacement candidate is given a
score (Eqn 1) indicating the confidence that a suit-
able replacement is found. The numbers in Fig-
ure 1 indicate the start and end indices (based
on character counts) of the words in the test sen-
tence. In T , two replacements were found for the
word ?hawks?: ?homers? and ?birds?. However,
?homers? was not found in the BL and hence, it
was removed from the replacement list.
The input lattice also includes the OOV word
with a low score (Eqn 2). This allows the EBMT
system to also include the OOV/rare-word dur-
ing decoding. In the Translation Model of the
EBMT system, this test lattice is matched against
the source sentences in the bilingual training cor-
pus. The matching process would now also look
for phrases with ?birds? and not just ?hawks?.
When a match is found, the corresponding trans-
  
T?:?????a?mobile?base?,?hitting?three?? hawks?with?one?arrow?.....input?lattice:0? 0? (???a???)1? 6? (???mobile???)7? 10? (???base???)11? 11? (??,??)12? 18? (???hitting???)13? 17? (???three???)18? 22? (???hawks?????0.0026)18? 22? (???birds???????0.9974)23? 26? (???with???)27? 29? (???one???)30? 34? (???arrow???)??????????????
Figure 1: Lattice of the input sentence T contain-
ing replacements for OOV words.
  
OOV/Rare word Candidate ReplacementsSpelling errorskrygyzstan kyrgyzstan,...
yusukuni yasukuni,..
kilomaters kilometers, miles, km, ...Coordinate termssomoa india, turkey, germany, russia, japan,...
ear body, arms, hands, feet, mind, car, ...
buyers dealer, inspector, the experts, smuggler,.Synonymsplummet drop, dropped, fell, ....Synonyms and Antonymsoptimal worse, better, minimal,....
Figure 2: Sample English candidate replacements
obtained.
lation in the target language is obtained through
sub-sentential alignment (Section 3.7). The scores
on the input lattice are later used by the decoder
(Section 3.7). Each replacement Xi,k for the
OOV/rare-word (wordi) is scored with a logistic
function (Bishop, 2006) to convert the dot product
of the features and weights (~? ? ~fi,k) to a score be-
tween 0 and 1 (Eqn 1 and Eqn 2).
p?(Xi,k|wordi) =
exp(~?? ~fi,k)
1+
?
j=1...S exp(~?? ~fi,j)
(1)
324
p?(wordi) =
1
1 +
?
j=1...S exp(~? ? ~fi,j)
(2)
where, ~fi,j is the feature vector for the jth replace-
ment candidate of wordi, S is the number of re-
placements, ~? is the weight vector indicating the
importance of the corresponding features.
3.6 Tuning feature weights
We would like to select those feature weights (~?)
which would lead to the least expected loss in
translation quality (Eqn 3). ?log(BLEU) (Pap-
ineni et al, 2002) is used to calculate the expected
loss over a development set. As this objective
function has many local minima and is piecewise
constant, the surface is smoothed using the L2-
norm regularization. Powell?s algorithm (Powell,
1964) with grid-based line optimization is used to
find the best weights. 7 different random guesses
are used to initialize the algorithm.
min
?
E?[L(ttune)] + ? ? ||?||2 (3)
The algorithm assumes that partial derivates of
the function are not available. Approximations of
the weights (?1, ..?N ) are generated successively
along each of the N standard base vectors. The
procedure is iterated with a stopping criteria based
on the amount of change in the weights and the
change in the loss. A cross-validation set (in ad-
dition to the regularization term) is used to pre-
vent overfitting at the end of each iteration of the
Powell?s algorithm. This process is repeated with
different values of ? , as in Deterministic Anneal-
ing (Rose, 1998). ? is initialized with a high value
and is halved after each process.
3.7 System Description
The EBMT system finds phrasal matches for the
test (or input) sentence from the source side of
the bilingual corpus. The corresponding tar-
get phrasal translations are obtained through sub-
sentential alignment. When an input lattice is
given instead of an input sentence, the system per-
forms the same matching process for all possible
phrases obtained from the input lattice. Hence,
the system also finds matches for source phrases
that contain the replacements for the OOV/rare-
word. Only the top C ranking replacement candi-
? ?
??????a????mobile??base???,??hitting???three???hawks????with???????one???????arrow??....????????????????????? ?????birds
       ?? ??   ? ?  ? ?
? ?? ?,?? ???? hawks
???three?birds??
??
Decoding?Lattice
 ? ? ?????three?birds?with?one?arrow??
Figure 3: Lattice containing possible phrasal tar-
get translations for the test sentence T .
dates for every OOV/rare word are used in build-
ing the input lattice. The optimal value of C was
empirically found to be 2. On examining the ob-
tained input lattices, the proposed method found
replacements for at the most 3 OOV/rare words in
each test sentence (Section 4). Hence, the number
of possible paths through the input lattice is not
substantially large.
The target translations of all the source phrases
are placed on a common decoding lattice. An
example of a decoding lattice for example T is
given in Figure 3. The system is now able to find
longer matches (? three birds with one arrow ?
and ? three birds ?) which was not possible earlier
with the OOV word, ?hawks?. The local order-
ing information between the translations of ?three
birds? and ?with one arrow? is well captured due
to the retrieval of the longer source phrasal match,
?three birds with one arrow?. Our ultimate goal
is to obtain translations for such longer n?gram
source phrases boosting the confidence of both the
translation model and the language model.
The decoder used in this paper (Brown, 2003)
works on this decoding lattice of possible
phrasal target translations (or fragments) for
source phrases present in the input lattice to gen-
erate the target translation. Similar to Pharaoh
(Koehn et al, 2003), the decoder uses multi-
level beam search with a priority queue formed
based on the number of source words translated.
Bonuses are given for paths that have overlapping
fragments. The total score (TS) for a path (Eqn
4) through the translation lattice is the arithmetic
average of the scores for each target word in the
325
path. The EBMT engine assigns each candidate
phrasal translation a quality score computed as
a log-linear combination of alignment score and
translation probability. The alignment score indi-
cates the engine?s confidence that the right target
translation has been chosen for a source phrase.
The translation probability is the proportion of
times each distinct alternative translation was en-
countered out of all the translations. If the path
includes a candidate replacement, the log of the
score, p?(wi), given for a candidate replacement
is incorporated into TS as an additional term with
a weight wt5.
TS = 1t
t?
i=1
[wt1 log(bi) + wt2 log(peni)
+wt3 log(qi) + wt4 log(P (wi|wi?2, wi?1))
+1I(wi=replacement)wt5 log(p?(wi)) ] (4)
where, t is the number of target words in the path,
wtj indicates the importance of each score, bi is
the bonus factor given for long phrasal matches,
peni is the penalty factor for source and target
phrasal-length mismatches, qi is the quality score
and P (wi|wi?2, wi?1) is the LM score. The pa-
rameters of the EBMT system (wtj) are tuned on
a development set.
The target translation is postprocessed to in-
clude the translation of the OOV/rare-word with
the help of the best path information from the
decoder. In the case of OOV words, since the
translation is not available, the OOV word is put
back into the final output translation in place of
the translation of its replacement. In the output
translation of the test example T , the translation
of ?birds? is replaced by the word, ?hawks?. For
rare words, knowing that the translation of the rare
word may not be correct (due to poor alignment
statistics), the target translation of the replacement
is replaced by the translation of the rare word
obtained from the dictionary. If the rare word
has multiple translations, the translation with the
highest score is chosen.
4 Experimental Setup
As we are interested in improving the per-
formance of low-resource EBMT, the English-
Haitian (Eng-Hai) newswire data (Haitian Cre-
ole, CMU, 2010) containing 15,136 sentence-
pairs was used. To test the performance in other
languages, we simulated sparsity by choosing less
training data for English-Chinese (Eng-Chi). For
the Eng-Chi experiments, we extracted 30k train-
ing sentence pairs from the FBIS (NIST, 2003)
corpus. The data was segmented using the Stan-
ford segmenter (Tseng et al, 2005). Although
we are only interested in small data sets, we also
performed experiments with a larger data set of
200k. 5-gram Language Models were built from
the target half of the training data with Kneser-
Ney smoothing. For the monolingual English cor-
pus, 9 million sentences were collected from the
Hansard Corpus (LDC, 1997) and FBIS data.
EBMT system without OOV/rare-word han-
dling is chosen as the Baseline system. The pa-
rameters of the EBMT system are tuned with 200
sentence pairs for both Eng-Chi and Eng-Hai. The
tuned EBMT parameters are used for the Base-
line system and the system with OOV/rare-word
handling. The feature weights for the proposed
method are then tuned on a seperate development
set of 200 sentence-pairs with source sentences
containing at least 1 OOV/rare-word. The cross-
validation set for this purpose is made up of 100
sentence-pairs. In the OOV case, 500 sentence
pairs containing at least 1 OOV word are used for
testing. For the rare word handling experiments,
500 sentence pairs containing at least 1 rare word
are used for testing.
To assess the translation quality, 4-gram word-
based BLEU is used for Eng-Hai and 3-gram
word-based BLEU is used for Eng-Chi. Since
BLEU scores have a few limitations, the NIST and
TER metrics are also used. The test data used for
comparing the system handling OOV words and
the Baseline (without OOV word handling) is dif-
ferent from the test data used for comparing the
system handling rare words and the Baseline sys-
tem (without rare word handling). In the former
case, the test data handles only OOV words and
in the latter, the test data only handles rare words.
Hence, the test data for both the cases do not com-
pletely overlap. As we are interested in determin-
ing whether handling rare words in test sentences
is useful, we keep both the test data sets seper-
ate and assess the improvements obtained by only
326
OOV/Rare system TER BLEU NISTOOV Baseline 77.89 18.61 4.8525Handling OOV 76.95 19.32 4.9664Rare Baseline 74.23 22.84 5.3803Handling Rare 74.02 23.12 5.4406
Table 1: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Hai.
handling OOV words and by only handling rare
words over their corresponding Baselines. As fu-
ture work, it would be interesting to create one test
data set to handle both OOV and rare words to see
the overall gain.
The test set is further split into 5 files and the
Wilcoxon (Wilcoxon, 1945) Signed-Rank test is
used to find the statistical significance.
5 Results
Sample replacements found are given in Figure 2.
For both Eng-Chi and Eng-Hai experiments, only
the top C ranking replacement candidates were
used. The value of C was tuned on the develop-
ment set and the optimal value was found to be
2. Translation quality scores obtained on the test
data with 30k and 200k Eng-Chi training data sets
are given in Table 2. Table 1 shows the results
obtained on Eng-Hai. Statistically significant im-
provements (p < 0.0001) were seen by handling
OOV words as well as rare words over their cor-
responding baselines.
As the goal of the approach was to obtain longer
target phrasal matches, we counted the number of
n-grams for each value of n present on the de-
coding lattice in the 30k Eng-Chi case. The sub-
plots: A and B in Figure 4, shows the frequency
of n-grams for higher values of n (for n > 5)
when handling OOV and rare words. The plots
clearly show the increase in number of longer tar-
get phrases when compared to the phrases ob-
tained by the baseline systems.
Since the BLEU and NIST scores were com-
puted only up to 3-grams, we further found the
number of n-gram matches (for n > 3) in the
final translation of the test data with respect to
the reference translations (subplots: C and D).
As expected, a larger number of longer n?gram
matches were found. For the OOV case, matches
6 7 8 9 10 11 12 13 14 150
20004000
60008000
1000012000
n?gram
#n?gr
ams o
n the 
decod
ing la
ttice
 
 BaselineHandling OOV words
6 7 8 9 10 11 12 13 14 150
5000
10000
15000
n?gram 
 BaselineHandling Rare words
4 5 6 7 8 9 10 110
50
100
150
n?gram 
 BaselineHandling OOV words
4 5 6 7 8 9 10 110
10
20
30
40
n?gram
#corre
ctly tr
ansla
ted n?
grams
 
 BaselineHandling Rare words
A C
B D
Figure 4: A, B: number of n-grams found for in-
creasing values of n on the decoding lattice. C, D:
number of target n-gram matches for increasing
values of n with respect to the reference transla-
tions.
OOV/Rare Training system TER BLEU NISTdata sizeOOV 30k Baseline 82.03 14.12 4.118630k Handling OOV 80.97 14.78 4.1798200k Baseline 79.41 19.90 4.6822200k Handling OOV 77.66 20.50 4.7654Rare 30k Baseline 82.09 15.36 4.362630k Handling Rare 80.02 16.03 4.4314200k Baseline 78.04 20.96 4.9647200k Handling Rare 77.35 21.17 5.0122
Table 2: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Chi.
up to 9-grams were found where the baseline only
found matches up to 8-grams.
6 Conclusion and Future Work
A simple approach to improve translation quality
by handling both OOV and rare words was pro-
posed. The framework allowed scoring and rank-
ing each replacement candidate efficiently.
The method was tested on two language pairs
and statistically significant improvements were
seen in both cases. The results showed that rare
words also need to be handled to see improve-
ments in translation quality.
In this paper, the proposed method was only ap-
plied on words, as future work we would like to
extend it to OOV and rare-phrases as well.
327
References
R. Barzilay and K. McKeown 2001. Extracting para-
phrases from a parallel corpus. In Proceedings
of the 39th Annual Meeting of the Association for
Computaional Linguistics, pp. 50-57.
C. M. Bishop 2006. Pattern Recognition and Machine
Learning, Springer.
R. D. Brown, R. Hutchinson, P. N. Bennett, J. G. Car-
bonell, P. Jansen. 2003. Reducing Boundary Fric-
tion Using Translation-Fragment Overlap. In Pro-
ceedings of The Ninth Machine Translation Summit,
pp. 24-31.
R. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of The Inter-
national Conference on Computational Linguistics,
pp. 125-131.
C. Callison-Burch, P. Koehn and M. Osborne. 2006.
Improved Statistical Machine Translation Using
Paraphrases. In Proceedings of The North Ameri-
can Chapter of the Association for Computational
Linguistics, pp. 17-24.
J. Carbonell, S. Klien, D. Miller, M. Steinbaum, T.
Grassiany and J. Frey. 2006. Context-Based Ma-
chine Translation Using Paraphrases. In Proceed-
ings of The Association for Machine Translation in
the Americas, pp. 8-12.
N. Habash. 2008. Four Techniques for On-
line Handling of Out-of-Vocabulary Words in
Arabic-English Statistical Machine Translation. In
Proceedings of Association for Computational
Linguistics-08: HLT, pp. 57-60.
Public release of Haitian Creole lan-
guage data by Carnegie Mellon, 2010.
http://www.speech.cs.cmu.edu/haitian/
Z. Harris. 1954. Distributional structure. Word,
10(23): 146-162.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder for
Phrase-Based Statistical Machine Translation Mod-
els. The Association for Machine Translation.
P. Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical Phrase-Based Translation. In Proceedings of
HLT:The North American Chapter of the Associa-
tion for Computational Linguistics.
P. Koehn 2002 Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished,
http://www.isi.edu/koehn/publications/europarl/
Linguistic Data Consortium. 1997 Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.edu/
Y. Marton, C. Callison-Burch and P. Resnik. 2009.
Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceed-
ing of The Empirical Methods in Natural Language
Processing, pp. 381-390.
NIST. 2003. Machine translation evaluation.
http://nist.gov/speech/tests/mt/
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of The Associa-
tion for Computational Linguistics. pp. 311-318.
M. Popovic? and H. Ney. 2004. Towards the use of
Word Stems and Suffixes for Statistical Machine
Translation. In Proceedings of The International
Conference on Language Resources and Evalua-
tion.
M. J. D. Powell. 1964. An efficient method for find-
ing the minimum of a function of several variables
without calculating derivatives Computer Journal.
Volume 7, pp. 152-162.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related
optimization problems. In Proceedings of The In-
stitute of Electrical and Electronics Engineers, pp.
2210-2239.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky and C.
Manning. 2005. A Conditional Random Field
Word Segmenter. Fourth SIGHAN Workshop on
Chinese Language Processing.
D. Vilar, J. Peter, and H. Ney. 2007. Can we translate
letters? In Proceedings of Association Computa-
tional Linguistics Workshop on SMT, pp. 33-39.
M. Yang and K. Kirchhoff. 2006. Phrase-based
back-off models for machine translation of highly
inflected languages. In Proceedings of European
Chapter of the ACL, 41-48.
F. Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics, 1, 80-83. tool:
http://faculty.vassar.edu/lowry/wilcoxon.html
328
Proceedings of the ACL 2010 Conference Short Papers, pages 365?370,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Active Learning-Based Elicitation for Semi-Supervised Word Alignment
Vamshi Ambati, Stephan Vogel and Jaime Carbonell
{vamshi,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Semi-supervised word alignment aims to
improve the accuracy of automatic word
alignment by incorporating full or par-
tial manual alignments. Motivated by
standard active learning query sampling
frameworks like uncertainty-, margin- and
query-by-committee sampling we propose
multiple query strategies for the alignment
link selection task. Our experiments show
that by active selection of uncertain and
informative links, we reduce the overall
manual effort involved in elicitation of
alignment link data for training a semi-
supervised word aligner.
1 Introduction
Corpus-based approaches to machine translation
have become predominant, with phrase-based sta-
tistical machine translation (PB-SMT) (Koehn et
al., 2003) being the most actively progressing area.
The success of statistical approaches to MT can
be attributed to the IBM models (Brown et al,
1993) that characterize word-level alignments in
parallel corpora. Parameters of these alignment
models are learnt in an unsupervised manner us-
ing the EM algorithm over sentence-level aligned
parallel corpora. While the ease of automati-
cally aligning sentences at the word-level with
tools like GIZA++ (Och and Ney, 2003) has en-
abled fast development of SMT systems for vari-
ous language pairs, the quality of alignment is typ-
ically quite low for language pairs like Chinese-
English, Arabic-English that diverge from the in-
dependence assumptions made by the generative
models. Increased parallel data enables better es-
timation of the model parameters, but a large num-
ber of language pairs still lack such resources.
Two directions of research have been pursued
for improving generative word alignment. The
first is to relax or update the independence as-
sumptions based on more information, usually
syntactic, from the language pairs (Cherry and
Lin, 2006; Fraser and Marcu, 2007a). The sec-
ond is to use extra annotation, typically word-level
human alignment for some sentence pairs, in con-
junction with the parallel data to learn alignment
in a semi-supervised manner. Our research is in
the direction of the latter, and aims to reduce the
effort involved in hand-generation of word align-
ments by using active learning strategies for care-
ful selection of word pairs to seek alignment.
Active learning for MT has not yet been ex-
plored to its full potential. Much of the litera-
ture has explored one task ? selecting sentences
to translate and add to the training corpus (Haf-
fari and Sarkar, 2009). In this paper we explore
active learning for word alignment, where the in-
put to the active learner is a sentence pair (S, T )
and the annotation elicited from human is a set of
links {aij , ?si ? S, tj ? T}. Unlike previous ap-
proaches, our work does not require elicitation of
full alignment for the sentence pair, which could
be effort-intensive. We propose active learning
query strategies to selectively elicit partial align-
ment information. Experiments in Section 5 show
that our selection strategies reduce alignment error
rates significantly over baseline.
2 Related Work
Researchers have begun to explore models that
use both labeled and unlabeled data to build
word-alignment models for MT. Fraser and Marcu
(2006) pose the problem of alignment as a search
problem in log-linear space with features com-
ing from the IBM alignment models. The log-
365
linear model is trained on available labeled data
to improve performance. They propose a semi-
supervised training algorithm which alternates be-
tween discriminative error training on the la-
beled data to learn the weighting parameters and
maximum-likelihood EM training on unlabeled
data to estimate the parameters. Callison-Burch
et al (2004) also improve alignment by interpolat-
ing human alignments with automatic alignments.
They observe that while working with such data
sets, alignments of higher quality should be given
a much higher weight than the lower-quality align-
ments. Wu et al (2006) learn separate models
from labeled and unlabeled data using the standard
EM algorithm. The two models are then interpo-
lated to use as a learner in the semi-supervised
algorithm to improve word alignment. To our
knowledge, there is no prior work that has looked
at reducing human effort by selective elicitation of
partial word alignment using active learning tech-
niques.
3 Active Learning for Word Alignment
Active learning attempts to optimize performance
by selecting the most informative instances to la-
bel where ?informativeness? is defined as maximal
expected improvement in accuracy. The objective
is to select optimal instance for an external expert
to label and then run the learning method on the
newly-labeled and previously-labeled instances to
minimize prediction or translation error, repeat-
ing until either the maximal number of external
queries is reached or a desired accuracy level is
achieved. Several studies (Tong and Koller, 2002;
Nguyen and Smeulders, 2004; Donmez and Car-
bonell, 2008) show that active learning greatly
helps to reduce the labeling effort in various clas-
sification tasks.
3.1 Active Learning Setup
We discuss our active learning setup for word
alignment in Algorithm 1. We start with an un-
labeled dataset U = {(Sk, Tk)}, indexed by k,
and a seed pool of partial alignment links A0 =
{akij , ?si ? Sk, tj ? Tk}. This is usually an empty
set at iteration t = 0. We iterate for T itera-
tions. We take a pool-based active learning strat-
egy, where we have access to all the automatically
aligned links and we can score the links based
on our active learning query strategy. The query
strategy uses the automatically trained alignment
model Mt from current iteration t for scoring the
links. Re-training and re-tuning an SMT system
for each link at a time is computationally infeasi-
ble. We therefore perform batch learning by se-
lecting a set of N links scored high by our query
strategy. We seek manual corrections for the se-
lected links and add the alignment data to the
current labeled data set. The word-level aligned
labeled data is provided to our semi-supervised
word alignment algorithm for training an align-
ment model Mt+1 over U .
Algorithm 1 AL FOR WORD ALIGNMENT
1: Unlabeled Data Set: U = {(Sk, Tk)}
2: Manual Alignment Set : A0 = {akij ,?si ?
Sk, tj ? Tk}
3: Train Semi-supervised Word Alignment using
(U , A0)?M0
4: N : batch size
5: for t = 0 to T do
6: Lt = LinkSelection(U ,At,Mt,N )
7: Request Human Alignment for Lt
8: At+1 = At + Lt
9: Re-train Semi-Supervised Word Align-
ment on (U,At+1)?Mt+1
10: end for
We can iteratively perform the algorithm for a
defined number of iterations T or until a certain
desired performance is reached, which is mea-
sured by alignment error rate (AER) (Fraser and
Marcu, 2007b) in the case of word alignment. In
a more typical scenario, since reducing human ef-
fort or cost of elicitation is the objective, we iterate
until the available budget is exhausted.
3.2 Semi-Supervised Word Alignment
We use an extended version of MGIZA++ (Gao
and Vogel, 2008) to perform the constrained semi-
supervised word alignment. Manual alignments
are incorporated in the EM training phase of these
models as constraints that restrict the summation
over all possible alignment paths. Typically in the
EM procedure for IBM models, the training pro-
cedure requires for each source sentence position,
the summation over all positions in the target sen-
tence. The manual alignments allow for one-to-
many alignments and many-to-many alignments
in both directions. For each position i in the source
sentence, there can be more than one manually
aligned target word. The restricted training will
allow only those paths, which are consistent with
366
the manual alignments. Therefore, the restriction
of the alignment paths reduces to restricting the
summation in EM.
4 Query Strategies for Link Selection
We propose multiple query selection strategies for
our active learning setup. The scoring criteria is
designed to select alignment links across sentence
pairs that are highly uncertain under current au-
tomatic translation models. These links are diffi-
cult to align correctly by automatic alignment and
will cause incorrect phrase pairs to be extracted in
the translation model, in turn hurting the transla-
tion quality of the SMT system. Manual correc-
tion of such links produces the maximal benefit to
the model. We would ideally like to elicit the least
number of manual corrections possible in order to
reduce the cost of data acquisition. In this section
we discuss our link selection strategies based on
the standard active learning paradigm of ?uncer-
tainty sampling?(Lewis and Catlett, 1994). We use
the automatically trained translation model ?t for
scoring each link for uncertainty, which consists of
bidirectional translation lexicon tables computed
from the bidirectional alignments.
4.1 Uncertainty Sampling: Bidirectional
Alignment Scores
The automatic Viterbi alignment produced by
the alignment models is used to obtain transla-
tion lexicons. These lexicons capture the condi-
tional distributions of source-given-target P (s/t)
and target-given-source P (t/s) probabilities at the
word level where si ? S and tj ? T . We de-
fine certainty of a link as the harmonic mean of the
bidirectional probabilities. The selection strategy
selects the least scoring links according to the for-
mula below which corresponds to links with max-
imum uncertainty:
Score(aij/s
I
1, t1
J) =
2 ? P (tj/si) ? P (si/tj)
P (tj/si) + P (si/tj)
(1)
4.2 Confidence Sampling: Posterior
Alignment probabilities
Confidence estimation for MT output is an in-
teresting area with meaningful initial exploration
(Blatz et al, 2004; Ueffing and Ney, 2007). Given
a sentence pair (sI1, t
J
1 ) and its word alignment,
we compute two confidence metrics at alignment
link level ? based on the posterior link probability
as seen in Equation 5. We select the alignment
links that the initial word aligner is least confi-
dent according to our metric and seek manual cor-
rection of the links. We use t2s to denote com-
putation using higher order (IBM4) target-given-
source models and s2t to denote source-given-
target models. Targeting some of the uncertain
parts of word alignment has already been shown
to improve translation quality in SMT (Huang,
2009). We use confidence metrics as an active
learning sampling strategy to obtain most informa-
tive links. We also experimented with other con-
fidence metrics as discussed in (Ueffing and Ney,
2007), especially the IBM 1 model score metric,
but it did not show significant improvement in this
task.
Pt2s(aij , t
J
1 /s
I
1) =
pt2s(tj/si,aij?A)
?M
i pt2s(tj/si)
(2)
Ps2t(aij , s
I
1/t
J
1 ) =
ps2t(si/tj ,aij?A)
?N
i ps2t(si/tj)
(3)
Conf1(aij/S, T ) =
2?Pt2s?Ps2t
Pt2s+Ps2t
(4)
(5)
4.3 Query by Committee
The generative alignments produced differ based
on the choice of direction of the language pair. We
useAs2t to denote alignment in the source to target
direction and At2s to denote the target to source
direction. We consider these alignments to be two
experts that have two different views of the align-
ment process. We formulate our query strategy
to select links where the agreement differs across
these two alignments. In general query by com-
mittee is a standard sampling strategy in active
learning(Freund et al, 1997), where the commit-
tee consists of any number of experts, in this case
alignments, with varying opinions. We formulate
a query by committee sampling strategy for word
alignment as shown in Equation 6. In order to
break ties, we extend this approach to select the
link with higher average frequency of occurrence
of words involved in the link.
Score(aij) = ? (6)
where ? =
?
?
?
2 aij ? As2t ?At2s
1 aij ? As2t ?At2s
0 otherwise
4.4 Margin Sampling
The strategy for confidence based sampling only
considers information about the best scoring link
367
conf(aij/S, T ). However we could benefit from
information about the second best scoring link as
well. In typical multi-class classification prob-
lems, earlier work shows success using such a
?margin based? approach (Scheffer et al, 2001),
where the difference between the probabilities as-
signed by the underlying model to the first best
and second best labels is used as a sampling cri-
teria. We adapt such a margin-based approach to
link-selection using the Conf1 scoring function
discussed in the earlier sub-section. Our margin
technique is formulated below, where a?1ij and
a?2ij are potential first best and second best scor-
ing alignment links for a word at position i in the
source sentence S with translation T . The word
with minimum margin value is chosen for human
alignment. Intuitively such a word is a possible
candidate for mis-alignment due to the inherent
confusion in its target translation.
Margin(i) =
Conf1(a?1ij/S, T ) ?Conf1(a?2ij/S, T )
5 Experiments
5.1 Data Setup
Our aim in this paper is to show that active learn-
ing can help select the most informative alignment
links that have high uncertainty according to a
given automatically trained model. We also show
that fixing such alignments leads to the maximum
reduction of error in word alignment, as measured
by AER. We compare this with a baseline where
links are selected at random for manual correction.
To run our experiments iteratively, we automate
the setup by using a parallel corpus for which the
gold-standard human alignment is already avail-
able. We select the Chinese-English language pair,
where we have access to 21,863 sentence pairs
along with complete manual alignment.
5.2 Results
We first automatically align the Cn-En corpus us-
ing GIZA++ (Och and Ney, 2003). We then
use the learned model in running our link selec-
tion algorithm over the entire corpus to determine
the most uncertain links according to each active
learning strategy. The links are then looked up in
the gold-standard human alignment database and
corrected. In case a link is not present in the
gold-standard data, we introduce a NULL align-
ment, else we propose the alignment as given in
Figure 1: Performance of active sampling strate-
gies for link selection
the gold standard. We select the partial align-
ment as a set of alignment links and provide it to
our semi-supervised word aligner. We plot per-
formance curves as number of links used in each
iteration vs. the overall reduction of AER on the
corpus.
Query by committee performs worse than ran-
dom indicating that two alignments differing in
direction are not sufficient in deciding for uncer-
tainty. We will be exploring alternative formula-
tions to this strategy. We observe that confidence
based metrics perform significantly better than the
baseline. From the scatter plots in Figure 1 1 we
can say that using our best selection strategy one
achieves similar performance to the baseline, but
at a much lower cost of elicitation assuming cost
per link is uniform.
We also perform end-to-end machine transla-
tion experiments to show that our improvement
of alignment quality leads to an improvement of
translation scores. For this experiment, we train
a standard phrase-based SMT system (Koehn et
al., 2007) over the entire parallel corpus. We tune
on the MT-Eval 2004 dataset and test on a subset
of MT-Eval 2004 dataset consisting of 631 sen-
tences. We first obtain the baseline score where
no manual alignment was used. We also train a
configuration using gold standard manual align-
ment data for the parallel corpus. This is the max-
imum translation accuracy that we can achieve by
any link selection algorithm. We now take the
best link selection criteria, which is the confidence
1X axis has number of links elicited on a log-scale
368
System BLEU METEOR
Baseline 18.82 42.70
Human Alignment 19.96 44.22
Active Selection 20% 19.34 43.25
Table 1: Alignment and Translation Quality
based method and train a system by only selecting
20% of all the links. We observe that at this point
we have reduced the AER from 37.09 AER to
26.57 AER. The translation accuracy as measured
by BLEU (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007) also shows improve-
ment over baseline and approaches gold standard
quality. Therefore we achieve 45% of the possible
improvement by only using 20% elicitation effort.
5.3 Batch Selection
Re-training the word alignment models after elic-
iting every individual alignment link is infeasible.
In our data set of 21,863 sentences with 588,075
links, it would be computationally intensive to re-
train after eliciting even 100 links in a batch. We
therefore sample links as a discrete batch, and train
alignment models to report performance at fixed
points. Such a batch selection is only going to be
sub-optimal as the underlying model changes with
every alignment link and therefore becomes ?stale?
for future selections. We observe that in some sce-
narios while fixing one alignment link could po-
tentially fix all the mis-alignments in a sentence
pair, our batch selection mechanism still samples
from the rest of the links in the sentence pair. We
experimented with an exponential decay function
over the number of links previously selected, in
order to discourage repeated sampling from the
same sentence pair. We performed an experiment
by selecting one of our best performing selection
strategies (conf ) and ran it in both configurations
- one with the decay parameter (batchdecay) and
one without it (batch). As seen in Figure 2, the
decay function has an effect in the initial part of
the curve where sampling is sparse but the effect
gradually fades away as we observe more samples.
In the reported results we do not use batch decay,
but an optimal estimation of ?staleness? could lead
to better gains in batch link selection using active
learning.
Figure 2: Batch decay effects on Conf-posterior
sampling strategy
6 Conclusion and Future Work
Word-Alignment is a particularly challenging
problem and has been addressed in a completely
unsupervised manner thus far (Brown et al, 1993).
While generative alignment models have been suc-
cessful, lack of sufficient data, model assump-
tions and local optimum during training are well
known problems. Semi-supervised techniques use
partial manual alignment data to address some of
these issues. We have shown that active learning
strategies can reduce the effort involved in elicit-
ing human alignment data. The reduction in ef-
fort is due to careful selection of maximally un-
certain links that provide the most benefit to the
alignment model when used in a semi-supervised
training fashion. Experiments on Chinese-English
have shown considerable improvements. In future
we wish to work with word alignments for other
language pairs like Arabic and English. We have
tested out the feasibility of obtaining human word
alignment data using Amazon Mechanical Turk
and plan to obtain more data reduce the cost of
annotation.
Acknowledgments
This research was partially supported by DARPA
under grant NBCHC080097. Any opinions, find-
ings, and conclusions expressed in this paper are
those of the authors and do not necessarily reflect
the views of the DARPA. The first author would
like to thank Qin Gao for the semi-supervised
word alignment software and help with running
experiments.
369
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for machine
translation. In Proceedings of Coling 2004, pages 315?
321, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In ACL 2004, page
175, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative train-
ing. In Proceedings of the COLING/ACL on Main con-
ference poster sessions, pages 105?112, Morristown, NJ,
USA.
Pinar Donmez and Jaime G. Carbonell. 2008. Optimizing es-
timated loss reduction for active sampling in rank learning.
In ICML ?08: Proceedings of the 25th international con-
ference on Machine learning, pages 248?255, New York,
NY, USA. ACM.
Alexander Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In ACL-44: Pro-
ceedings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, pages 769?
776, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Alexander Fraser and Daniel Marcu. 2007a. Getting the
structure right for word alignment: LEAF. In Proceedings
of the 2007 Joint Conference on EMNLP-CoNLL, pages
51?60.
Alexander Fraser and Daniel Marcu. 2007b. Measuring word
alignment quality for statistical machine translation. Com-
put. Linguist., 33(3):293?303.
Yoav Freund, Sebastian H. Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine. Learning., 28(2-3):133?168.
Qin Gao and Stephan Vogel. 2008. Parallel implementa-
tions of word alignment tool. In Software Engineering,
Testing, and Quality Assurance for Natural Language Pro-
cessing, pages 49?57, Columbus, Ohio, June. Association
for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2009. Active learn-
ing for multilingual statistical machine translation. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the AFNLP,
pages 181?189, Suntec, Singapore, August. Association
for Computational Linguistics.
Fei Huang. 2009. Confidence measure for word alignment.
In Proceedings of the Joint ACL and IJCNLP, pages 932?
940, Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of the
HLT/NAACL, Edomonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL Demon-
stration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an auto-
matic metric for mt evaluation with high levels of corre-
lation with human judgments. In WMT 2007, pages 228?
231, Morristown, NJ, USA.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In In Proceed-
ings of the Eleventh International Conference on Machine
Learning, pages 148?156. Morgan Kaufmann.
Hieu T. Nguyen and Arnold Smeulders. 2004. Active learn-
ing using pre-clustering. In ICML.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In ACL 2002, pages 311?318, Mor-
ristown, NJ, USA.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel.
2001. Active hidden markov models for information ex-
traction. In IDA ?01: Proceedings of the 4th Interna-
tional Conference on Advances in Intelligent Data Anal-
ysis, pages 309?318, London, UK. Springer-Verlag.
Simon Tong and Daphne Koller. 2002. Support vector ma-
chine active learning with applications to text classifica-
tion. Journal of Machine Learning, pages 45?66.
Nicola Ueffing and Hermann Ney. 2007. Word-level con-
fidence estimation for machine translation. Comput. Lin-
guist., 33(1):9?40.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and unlabeled
data. In Proceedings of the COLING/ACL on Main con-
ference poster sessions, pages 913?920, Morristown, NJ,
USA. Association for Computational Linguistics.
370
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 10?17,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Active Semi-Supervised Learning for Improving Word Alignment
Vamshi Ambati, Stephan Vogel and Jaime Carbonell
{vamshi,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Word alignment models form an important
part of building statistical machine transla-
tion systems. Semi-supervised word align-
ment aims to improve the accuracy of auto-
matic word alignment by incorporating full
or partial alignments acquired from humans.
Such dedicated elicitation effort is often ex-
pensive and depends on availability of bilin-
gual speakers for the language-pair. In this
paper we study active learning query strate-
gies to carefully identify highly uncertain or
most informative alignment links that are pro-
posed under an unsupervised word alignment
model. Manual correction of such informative
links can then be applied to create a labeled
dataset used by a semi-supervised word align-
ment model. Our experiments show that using
active learning leads to maximal reduction of
alignment error rates with reduced human ef-
fort.
1 Introduction
The success of statistical approaches to Machine
Translation (MT) can be attributed to the IBM mod-
els (Brown et al, 1993) that characterize word-
level alignments in parallel corpora. Parameters of
these alignment models are learnt in an unsupervised
manner using the EM algorithm over sentence-level
aligned parallel corpora. While the ease of auto-
matically aligning sentences at the word-level with
tools like GIZA++ (Och and Ney, 2003) has enabled
fast development of statistical machine translation
(SMT) systems for various language pairs, the qual-
ity of alignment is typically quite low for language
pairs that diverge from the independence assump-
tions made by the generative models. Also, an im-
mense amount of parallel data enables better estima-
tion of the model parameters, but a large number of
language pairs still lack parallel data.
Two directions of research have been pursued for
improving generative word alignment. The first is to
relax or update the independence assumptions based
on more information, usually syntactic, from the
language pairs (Cherry and Lin, 2006). The sec-
ond is to use extra annotation, typically word-level
human alignment for some sentence pairs, in con-
junction with the parallel data to learn alignment in
a semi-supervised manner. Our research is in the
direction of the latter, and aims to reduce the effort
involved in hand-generation of word alignments by
using active learning strategies for careful selection
of word pairs to seek alignment.
Active learning for MT has not yet been explored
to its full potential. Much of the literature has ex-
plored one task ? selecting sentences to translate
and add to the training corpus (Haffari et al, 2009).
In this paper we explore active learning for word
alignment, where the input to the active learner is
a sentence pair (sJ1 , t
I
1), present in two different lan-
guages S = {s?} and T = {t?}, and the annotation
elicited from human is a set of links {(j, i) : j =
0 ? ? ? J ; i = 0 ? ? ? I}. Unlike previous approaches,
our work does not require elicitation of full align-
ment for the sentence pair, which could be effort-
intensive. We use standard active learning query
strategies to selectively elicit partial alignment infor-
mation. This partial alignment information is then
fed into a semi-supervised word aligner which per-
10
forms an improved word alignment over the entire
parallel corpus.
Rest of the paper is organized as follows. We
present related work in Section 2. Section 3 gives
an overview of unsupervised word alignment mod-
els and its semi-supervised improvisation. Section 4
details our active learning framework with discus-
sion of the link selection strategies in Section 5. Ex-
periments in Section 6 have shown that our selection
strategies reduce alignment error rates significantly
over baseline. We conclude with discussion on fu-
ture work.
2 Related Work
Semi-supervised learning is a broader area of Ma-
chine Learning, focusing on improving the learn-
ing process by usage of unlabeled data in conjunc-
tion with labeled data (Chapelle et al, 2006). Many
semi-supervised learning algorithms use co-training
framework, which assumes that the dataset has mul-
tiple views, and training different classifiers on a
non-overlapping subset of these features provides
additional labeled data (Zhu, 2005). Active query
selection for training a semi-supervised learning al-
gorithm is an interesting method that has been ap-
plied to clustering problems. Tomanek and Hahn
(2009) applied active semi supervised learning to
the sequence-labeling problem. Tur et al (2005) de-
scribe active and semi-supervised learning methods
for reducing labeling effort for spoken language un-
derstanding. They train supervised classification al-
gorithms for the task of call classification and apply
it to a large unlabeled dataset to select the least con-
fident instances for human labeling.
Researchers have begun to explore semi-
supervised word alignment models that use both
labeled and unlabeled data. Fraser and Marcu
(2006) pose the problem of alignment as a search
problem in log-linear space with features coming
from the IBM alignment models. The log-linear
model is trained on the available labeled data
to improve performance. They propose a semi-
supervised training algorithm which alternates
between discriminative error training on the la-
beled data to learn the weighting parameters and
maximum-likelihood EM training on unlabeled
data to estimate the parameters. Callison-Burch et
al. (2004) also improve alignment by interpolating
human alignments with automatic alignments. They
observe that while working with such datasets,
alignments of higher quality should be given a much
higher weight than the lower-quality alignments.
Wu et al (2006) learn separate models from labeled
and unlabeled data using the standard EM algo-
rithm. The two models are then interpolated as a
learner in the semi-supervised AdaBoost algorithm
to improve word alignment.
Active learning has been applied to various fields
of Natural Language Processing like statistical pars-
ing, entity recognition among others (Hwa, 2004;
Tang et al, 2001; Shen et al, 2004). In case of
MT, the potential of active learning has remained
largely unexplored. For Statistical Machine Transla-
tion, application of active learning has been focused
on the task of selecting the most informative sen-
tences to train the model, in order to reduce cost
of data acquisition. Recent work in this area dis-
cussed multiple query selection strategies for a Sta-
tistical Phrase Based Translation system (Haffari et
al., 2009). Their framework requires source text to
be translated by the system and the translated data
is used in a self-training setting to train MT models.
To our knowledge, we are not aware of any work
that has looked at reducing human effort by selec-
tive elicitation of alignment information using active
learning techniques.
3 Word Alignment
3.1 IBM models
IBM models provide a generative framework for
performing word alignment of parallel corpus.
Given two strings from source and target languages
sJ1 = s1, ? ? ? , sj , ? ? ? sJ and t
I
1 = t1, ? ? ? , ti, ? ? ? tI ,
an alignment A is defined as a subset of the Carte-
sian product of the word indices as shown in Eq 1.
In IBM models, since alignment is treated as a func-
tion, all the source positions must be covered exactly
once (Brown et al, 1993).
A ? {(j, i) : j = 0 ? ? ? J ; i = 0 ? ? ? I} (1)
For the task of translation, we would ideally want
to model P (sI1|t
J
1 ), which is the probability of ob-
serving source sentence sI1 given target sentence t
J
1 .
This requires a lot of parallel corpus for estimation
11
and so it is then factored over the word alignment
A for the sentence pair, which is a hidden variable.
Word alignment is therefore a by-product in the pro-
cess of modeling translation. We can also represent
the same under some parameterization of ?, which
is the model we are interested to estimate.
P (sJ1 |t
I
1) =
?
aJ1
Pr(sJ1 , A|t
J
1 ) (2)
=
?
A
p?(s
J
1 , A|t
I
1) (3)
Given a parallel corpus U of sentence pairs
{(sk, tk) : k = 1, ? ? ? ,K} the parameters can be
estimated by maximizing the conditional likelihood
over the data. IBM models (Brown et al, 1993) from
1 to 5 are different ways of factoring the probability
model to estimate the parameter set ?. For example
in the simplest of the models, IBM model 1, only the
lexical translation probability is considered treating
each word being translated independent of the other
words.
?? = argmax
?
K?
k=1
?
A
p?(sk, A|tk) (4)
The parameters of the model above are estimated
as ??, using the EM algorithm. We can also extract
the Viterbi alignment ,A?, for all the sentence pairs,
which is the alignment with the highest probability
under the current model parameters ?:
A? = argmax
A
p??(s
J
1 , A|t
I
1) (5)
The alignment models are asymmetric and dif-
fer with the choice of translation direction. We can
therefore perform the above after switching the di-
rection of the language pair and obtain models and
Viterbi alignments for the corpus as represented be-
low:
?? = argmax
?
K?
k=1
?
a
p?(tk, a|sk) (6)
A? = argmax
A
p??(t
I
1, A|s
J
1 ) (7)
Given the Viterbi alignment for each sentence
pair in the parallel corpus, we can also compute the
word-level alignment probabilities using simple rel-
ative likelihood estimation for both the directions.
As we will discuss in Section 5, the alignments and
the computed lexicons form an important part of our
link selection strategies.
P (sj/ti) =
?
s count(ti, sj ; A?)?
s count(ti)
(8)
P (ti/sj) =
?
s count(ti, sj ; A?)?
s count(sj)
(9)
We perform all our experiments on a symmetrized
alignment that combines the bidirectional align-
ments using heuristics as discussed in (Koehn et al,
2007). We represent this alignment as A = {aij :
i = 0 ? ? ? J ? sJ1 ; j = 0 ? ? ? I ? t
I
1}.
3.2 Semi-Supervised Word Alignment
We use an extended version of MGIZA++ (Gao
and Vogel, 2008) to perform the constrained semi-
supervised word alignment. To get full benefit
from the manual alignments, MGIZA++ modifies all
alignment models used in the standard training pro-
cedure, i.e. the IBM1, HMM, IBM3 and IBM4 mod-
els. Manual alignments are incorporated in the EM
training phase of these models as constraints that
restrict the summation over all possible alignment
paths. Typically in the EM procedure for IBM mod-
els, the training procedure requires for each source
sentence position, the summation over all positions
in the target sentence. The manual alignments al-
low for one-to-many alignments and many-to-many
alignments in both directions. For each position i
in the source sentence, there can be more than one
manually aligned target word. The restricted train-
ing will allow only those paths, which are consistent
with the manual alignments. Therefore, the restric-
tion of the alignment paths reduces to restricting the
summation in EM.
4 Active Learning for Word Alignment
Active learning attempts to optimize performance
by selecting the most informative instances to la-
bel, where ?informativeness? is defined as maximal
expected improvement in accuracy. The objective
is to select optimal instance for an external expert
to label and then run the learning method on the
newly-labeled and previously-labeled instances to
minimize prediction or translation error, repeating
until either the maximal number of external queries
12
is reached or a desired accuracy level is achieved.
Several studies (Tong and Koller, 2002; Nguyen
and Smeulders, 2004; Donmez and Carbonell, 2008)
show that active learning greatly helps to reduce the
labeling effort in various classification tasks.
We discuss our active learning setup for word
alignment in Algorithm 1. We start with an un-
labeled dataset U = {(Sk, Tk)}, indexed by k,
and a seed pool of partial alignment links A0 =
{akij , ?si ? Sk, tj ? Tk}. Each a
k
ij represents an
alignment link from a sentence pair k that connects
source word si with tj .
This is usually an empty set at iteration t = 0. We
iterate for T iterations. We take a pool-based active
learning strategy, where we have access to all the au-
tomatically aligned links and we can score the links
based on our active learning query strategy. The
query strategy uses the automatically trained align-
ment model ?t from the current iteration t, for scor-
ing the links. Re-training and re-tuning an SMT sys-
tem for each link at a time is computationally infea-
sible. We therefore perform batch learning by se-
lecting a set of N links scored high by our query
strategy. We seek manual corrections for the se-
lected links and add the alignment data to the cur-
rent labeled dataset. The word-level aligned labeled
dataset is then provided to our semi-supervised word
alignment algorithm, which uses it to produces the
alignment model ?t+1 for U .
Algorithm 1 AL FOR WORD ALIGNMENT
1: Unlabeled Data Set: U = {(sk, tk)}
2: Manual Alignment Set : A0 = {akij ,?si ?
Sk, tj ? Tk}
3: Train Semi-supervised Word Alignment using
(U , A0)? ?0
4: N : batch size
5: for t = 0 to T do
6: Lt = LinkSelection(U ,At,?t,N )
7: Request Human Alignment for Lt
8: At+1 = At + Lt
9: Re-train Semi-Supervised Word Align-
ment on (U,At+1)? ?t+1
10: end for
We can iteratively perform the algorithm for a de-
fined number of iterations T or until a certain desired
performance is reached, which is measured by align-
ment error rate (AER) (Fraser and Marcu, 2007) in
the case of word alignment. In a more typical sce-
nario, since reducing human effort or cost of elici-
tation is the objective, we iterate until the available
budget is exhausted.
5 Query Strategies for Link Selection
We propose multiple query selection strategies for
our active learning setup. The scoring criteria is
designed to select alignment links across sentence
pairs that are highly uncertain under current au-
tomatic translation models. These links are diffi-
cult to align correctly by automatic alignment and
will cause incorrect phrase pairs to be extracted in
the translation model, in turn hurting the transla-
tion quality of the SMT system. Manual correction
of such links produces the maximal benefit to the
model. We would ideally like to elicit the least num-
ber of manual corrections possible in order to reduce
the cost of data acquisition. In this section we dis-
cuss our link selection strategies based on the stan-
dard active learning paradigm of ?uncertainty sam-
pling?(Lewis and Catlett, 1994). We use the au-
tomatically trained translation model ?t for scoring
each link for uncertainty. In particular ?t consists of
bidirectional lexicon tables computed from the bidi-
rectional alignments as discussed in Section 3.
5.1 Uncertainty based: Bidirectional
Alignment Scores
The automatic Viterbi alignment produced by the
alignment models is used to obtain translation lexi-
cons, as discussed in Section 3. These lexicons cap-
ture the conditional distributions of source-given-
target P (s/t) and target-given-source P (t/s) prob-
abilities at the word level where si ? S and tj ? T .
We define certainty of a link as the harmonic mean
of the bidirectional probabilities. The selection strat-
egy selects the least scoring links according to the
formula below which corresponds to links with max-
imum uncertainty:
Score(aij/sI1, t
J
1 ) =
2 ? P (tj/si) ? P (si/tj)
P (tj/si) + P (si/tj)
(10)
5.2 Confidence Based: Posterior Alignment
probabilities
Confidence estimation for MT output is an interest-
ing area with meaningful initial exploration (Blatz
13
et al, 2004; Ueffing and Ney, 2007). Given a sen-
tence pair (sI1, t
J
1 ) and its word alignment, we com-
pute two confidence metrics at alignment link level ?
based on the posterior link probability and a simple
IBM Model 1 as seen in Equation 13. We select the
alignment links that the initial word aligner is least
confident according to our metric and seek manual
correction of the links. We use t2s to denote com-
putation using higher order (IBM4) target-given-
source models and s2t to denote source-given-target
models. Targeting some of the uncertain parts of
word alignment has already been shown to improve
translation quality in SMT (Huang, 2009). In our
current work, we use confidence metrics as an ac-
tive learning sampling strategy to obtain most infor-
mative links. We also experiment with other con-
fidence metrics as discussed in (Ueffing and Ney,
2007), especially the IBM 1 model score metric
which showed some improvement as well.
Pt2s(aij , tJ1 /s
I
1) =
pt2s(tj/si, aij ? A)
?M
i pt2s(tj/si)
(11)
Ps2t(aij , sI1/t
J
1 ) =
ps2t(si/tj , aij ? A)
?N
i pt2s(tj/si)
(12)
Conf(aij/S, T ) =
2 ? Pt2s ? Ps2t
Pt2s + Ps2t
(13)
5.3 Agreement Based: Query by Committee
The generative alignments produced differ based on
the choice of direction of the language pair. We use
As2t to denote alignment in the source to target di-
rection and At2s to denote the target to source direc-
tion. We consider these alignments to be two experts
that have two different views of the alignment pro-
cess. We formulate our query strategy to select links,
where the agreement differs across these two align-
ments. In general query by committee is a standard
sampling strategy in active learning(Freund et al,
1997), where the committee consists of any number
of experts with varying opinions, in this case align-
ments in different directions. We formulate a query
by committee sampling strategy for word alignment
as shown in Equation 14. In order to break ties, we
extend this approach to select the link with higher
average frequency of occurrence of words involved
in the link.
Language Sentences Words
Src Tgt
Ch-En 21,863 424,683 524,882
Ar-En 29,876 630,101 821,938
Table 1: Corpus Statistics of Human Data
Alignment Automatic Links Manual Links
Ch-En 491,887 588,075
Ar-En 786,223 712,583
Table 2: Alignment Statistics of Human Data
Score(aij) = ? where (14)
? =
?
?
?
2 aij ? At2s ?At2s
1 aij ? At2s ?At2s
0 otherwise
6 Experiments
6.1 Data Analysis
To run our active learning and semi-supervised word
alignment experiments iteratively, we simulate the
setup by using a parallel corpus for which the
gold standard human alignment is already available.
We experiment with two language pairs - Chinese-
English and Arabic-English. Corpus-level statistics
for both language pairs can be seen in Table 1 and
their alignment link level statistics can be seen in
Table 2. Both datasets were released by LDC as part
of the GALE project.
Chinese-English dataset consists of 21,863 sen-
tence pairs with complete manual alignment. The
human alignment for this dataset is much denser
than the automatic word alignment. On an aver-
age each source word is linked to more than one
target word. Similarly, the Arabic-English dataset
consisting of 29,876 sentence pairs also has a denser
manual alignment. Automatic word alignment in
both cases was computed as a symmetrized version
of the bidirectional alignments obtained from using
GIZA++ (Och and Ney, 2003) in each direction sep-
arately.
6.2 Word Alignment Results
We first perform an unsupervised word alignment of
the parallel corpus. We then use the learned model
14
Figure 1: Chinese-English: Link Selection Results
in running our link selection algorithm over the en-
tire alignments to determine the most uncertain links
according to each active learning strategy. The links
are then looked up in the gold standard human align-
ment database and corrected. In scenarios where
an alignment link is not present in the gold stan-
dard data for the source word, we introduce a NULL
alignment constraint, else we select all the links as
given in the gold standard. The aim of our work is to
show that active learning can help in selecting infor-
mative alignment links, which if manually labeled
can reduce the overall alignment error rate of the
given corpus. We, therefore measure the reduction
of alignment error rate (AER) of a semi-supervised
word aligner that uses this extra information to align
the corpus. We plot performance curves for both
Chinese-English, Figure 1 and Arabic-English, Fig-
ure 2, with number of manual links elicited on x-axis
and AER on y-axis. In each iteration of the experi-
ment, we gradually increase the number of links se-
lected from gold standard and make them available
to the semi-supervised word aligner and measure the
overall reduction of AER on the corpus. We com-
pare our link selection strategies to a baseline ap-
proach, where links are selected at random for man-
ual correction.
All our approaches perform equally or better than
the baseline for both language pairs. Query by
committee (qbc) performs similar to the baseline in
Chinese-English and only slightly better for Arabic-
Figure 2: Arabic-English: Link Selection Results
English. This could be due to our committee con-
sisting of two alignments that differ only in direc-
tion and so are not sufficient in deciding for uncer-
tainty. We will be exploring alternative formulations
to this strategy. Confidence based and uncertainty
based metrics perform significantly better than the
baseline in both language pairs. We can interpret the
improvements in two ways. For the same number
of manual alignments elicited, our selection strate-
gies select links that provide higher reduction of er-
ror when compared to the baseline. An alternative
interpretation is that assuming a uniform cost per
link, our best selection strategy achieves similar per-
formance to the baseline, at a much lower cost of
elicitation.
6.3 Translation Results
We also perform end-to-end machine translation ex-
periments to show that our improvement of align-
ment quality leads to an improvement of translation
scores. For Chinese-English, we train a standard
phrase-based SMT system (Koehn et al, 2007) over
the available 21,863 sentences. We tune on the MT-
Eval 2004 dataset and test on a subset of MT-Eval
2005 dataset consisting of 631 sentences. The lan-
guage model we use is built using only the English
side of the parallel corpus. We understand that this
language model is not the optimal choice, but we
are interested in testing the word alignment accu-
racy, which primarily affects the translation model.
15
Cn-En BLEU METEOR
Baseline 18.82 42.70
Human Alignment 19.96 44.22
Active Selection 20% 19.34 43.25
Table 3: Effect of Alignment on Translation Quality
We first obtain the baseline score by training in an
unsupervised manner, where no manual alignment
is used. We also train a configuration, where we
substitute the final word alignment with gold stan-
dard manual alignment for the entire parallel corpus.
This is an upper bound on the translation accuracy
that can be achieved by any alignment link selec-
tion algorithm for this dataset. We now take our
best link selection criteria, which is the confidence
based method and re-train the MT system after elic-
iting manual information for only 20% of the align-
ment links. We observe that at this point we have
reduced the AER from 37.09 to 26.57. The trans-
lation accuracy reported in Table 3, as measured by
BLEU (Papineni et al, 2002) and METEOR (Lavie
and Agarwal, 2007), also shows significant improve-
ment and approaches the quality achieved using gold
standard data. We did not perform MT experiments
with Arabic-English dataset due to the incompatibil-
ity of tokenization schemes between the manually
aligned parallel corpora and publicly available eval-
uation sets.
7 Conclusion
Word-Alignment is a particularly challenging prob-
lem and has been addressed in a completely unsuper-
vised manner thus far (Brown et al, 1993). While
generative alignment models have been successful,
lack of sufficient data, model assumptions and lo-
cal optimum during training are well known prob-
lems. Semi-supervised techniques use partial man-
ual alignment data to address some of these issues.
We have shown that active learning strategies can
reduce the effort involved in eliciting human align-
ment data. The reduction in effort is due to care-
ful selection of maximally uncertain links that pro-
vide the most benefit to the alignment model when
used in a semi-supervised training fashion. Experi-
ments on Chinese-English have shown considerable
improvements.
8 Future Work
In future, we wish to work with word alignments for
other language pairs as well as study the effect of
manual alignments by varying the size of available
parallel data. We also plan to obtain alignments from
non-experts over online marketplaces like Amazon
Mechanical Turk to further reduce the cost of an-
notation. We will be experimenting with obtain-
ing full-alignment vs. partial alignment from non-
experts. Our hypothesis is that, humans are good
at performing tasks of smaller size and so we can
extract high quality alignments in the partial align-
ment case. Cost of link annotation in our current
work is assumed to be uniform, but this needs to
be revisited. We will also experiment with active
learning techniques for identifying sentence pairs
with very low alignment confidence, where obtain-
ing full-alignment is equivalent to obtaining multi-
ple partial alignments.
Acknowledgments
This research was partially supported by DARPA
under grant NBCHC080097. Any opinions, find-
ings, and conclusions expressed in this paper are
those of the authors and do not necessarily reflect the
views of the DARPA. The first author would like to
thank Qin Gao for the semi-supervised word align-
ment software and help with running experiments.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of Coling 2004,
pages 315?321, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In ACL 2004, page
175, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
16
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 105?112, Morris-
town, NJ, USA.
Pinar Donmez and Jaime G. Carbonell. 2008. Opti-
mizing estimated loss reduction for active sampling in
rank learning. In ICML ?08: Proceedings of the 25th
international conference on Machine learning, pages
248?255, New York, NY, USA. ACM.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 769?776, Morristown, NJ, USA.
Association for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293?303.
Yoav Freund, Sebastian H. Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine. Learning., 28(2-
3):133?168.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In Proceedings of HLT NAACL
2009, pages 415?423, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Fei Huang. 2009. Confidence measure for word align-
ment. In Proceedings of the Joint ACL and IJCNLP,
pages 932?940, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Comput. Linguist., 30(3):253?276.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL Demonstration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT 2007,
pages 228?231, Morristown, NJ, USA.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In In
Proceedings of the Eleventh International Conference
on Machine Learning, pages 148?156. Morgan Kauf-
mann.
Hieu T. Nguyen and Arnold Smeulders. 2004. Active
learning using pre-clustering. In ICML.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL 2002, pages 311?
318, Morristown, NJ, USA.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In ACL ?04: Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589, Morristown, NJ,
USA. Association for Computational Linguistics.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
ACL ?02, pages 120?127, Morristown, NJ, USA.
Katrin Tomanek and Udo Hahn. 2009. Semi-supervised
active learning for sequence labeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1039?1047, Suntec, Singapore, August. Association
for Computational Linguistics.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. Journal of Machine Learning, pages 45?66.
Gokhan Tur, Dilek Hakkani-Tr, and Robert E. Schapire.
2005. Combining active and semi-supervised learning
for spoken language understanding. Speech Commu-
nication, 45(2):171 ? 186.
Nicola Ueffing and Hermann Ney. 2007. Word-level
confidence estimation for machine translation. Com-
put. Linguist., 33(1):9?40.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boosting
statistical word alignment using labeled and unlabeled
data. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 913?920, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
X. Zhu. 2005. Semi-Supervised Learning Lit-
erature Survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
17
Active Learning with Multiple Annotations for Comparable Data
Classification Task
Vamshi Ambati, Sanjika Hewavitharana, Stephan Vogel and Jaime Carbonell
{vamshi,sanjika,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Supervised learning algorithms for identify-
ing comparable sentence pairs from a domi-
nantly non-parallel corpora require resources
for computing feature functions as well as
training the classifier. In this paper we pro-
pose active learning techniques for addressing
the problem of building comparable data for
low-resource languages. In particular we pro-
pose strategies to elicit two kinds of annota-
tions from comparable sentence pairs: class
label assignment and parallel segment extrac-
tion. We also propose an active learning strat-
egy for these two annotations that performs
significantly better than when sampling for ei-
ther of the annotations independently.
1 Introduction
The state-of-the-art Machine Translation (MT) sys-
tems are statistical, requiring large amounts of paral-
lel corpora. Such corpora needs to be carefully cre-
ated by language experts or speakers, which makes
building MT systems feasible only for those lan-
guage pairs with sufficient public interest or finan-
cial support. With the increasing rate of social media
creation and the quick growth of web media in lan-
guages other than English makes it relevant for lan-
guage research community to explore the feasibility
of Internet as a source for parallel data. (Resnik and
Smith, 2003) show that parallel corpora for a variety
of languages can be harvested on the Internet. It is to
be observed that a major portion of the multilingual
web documents are created independent of one an-
other and so are only mildly parallel at the document
level.
There are multiple challenges in building compa-
rable corpora for consumption by the MT systems.
The first challenge is to identify the parallelism be-
tween documents of different languages which has
been reliably done using cross lingual information
retrieval techniques. Once we have identified a sub-
set of documents that are potentially parallel, the
second challenge is to identify comparable sentence
pairs. This is an interesting challenge as the avail-
ability of completely parallel sentences on the inter-
net is quite low in most language-pairs, but one can
observe very few comparable sentences among com-
parable documents for a given language-pair. Our
work tries to address this problem by posing the
identification of comparable sentences from com-
parable data as a supervised classification problem.
Unlike earlier research (Munteanu and Marcu, 2005)
where the authors try to identify parallel sentences
among a pool of comparable documents, we try to
first identify comparable sentences in a pool with
dominantly non-parallel sentences. We then build
a supervised classifier that learns from user annota-
tions for comparable corpora identification. Train-
ing such a classifier requires reliably annotated data
that may be unavailable for low-resource language
pairs. Involving a human expert to perform such
annotations is expensive for low-resource languages
and so we propose active learning as a suitable tech-
nique to reduce the labeling effort.
There is yet one other issue that needs to be solved
in order for our classification based approach to
work for truly low-resource language pairs. As we
will describe later in the paper, our comparable sen-
tence classifier relies on the availability of an ini-
69
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 69?77,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
tial seed lexicon that can either be provided by a hu-
man or can be statistically trained from parallel cor-
pora (Och and Ney, 2003). Experiments show that a
broad coverage lexicon provides us with better cov-
erage for effective identification of comparable cor-
pora. However, availability of such a resource can
not be expected in very low-resource language pairs,
or even if present may not be of good quality. This
opens an interesting research question - Can we also
elicit such information effectively at low costs? We
propose active learning strategies for identifying the
most informative comparable sentence pairs which a
human can then extract parallel segments from.
While the first form of supervision provides us
with class labels that can be used for tuning the fea-
ture weights of our classifier, the second form of su-
pervision enables us to better estimate the feature
functions. For the comparable sentence classifier to
perform well, we show that both forms of supervi-
sion are needed and we introduce an active learning
protocol to combine the two forms of supervision
under a single joint active learning strategy.
The rest of the paper is organized as follows. In
Section 2 we survey earlier research as relevant to
the scope of the paper. In Section 3 we discuss the
supervised training setup for our classifier. In Sec-
tion 4 we discuss the application of active learning to
the classification task. Section 5 discusses the case
of active learning with two different annotations and
proposes an approach for combining them. Section 6
presents experimental results and the effectiveness
of the active learning strategies. We conclude with
further discussion and future work.
2 Related Work
There has been a lot of interest in using compara-
ble corpora for MT, primarily on extracting paral-
lel sentence pairs from comparable sources (Zhao
and Vogel, 2002; Fung and Yee, 1998). Some work
has gone beyond this focussing on extracting sub-
sentential fragments from noisier comparable data
(Munteanu and Marcu, 2006; Quirk et al, 2007).
The research conducted in this paper has two pri-
mary contributions and so we will discuss the related
work as relevant to each of them.
Our first contribution in this paper is the appli-
cation of active learning for acquiring comparable
data in the low-resource scenario, especially rele-
vant when working with low-resource languages.
There is some earlier work highlighting the need
for techniques to deal with low-resource scenar-
ios.(Munteanu and Marcu, 2005) propose bootstrap-
ping using an existing classifier for collecting new
data. However, this approach works when there is
a classifier of reasonable performance. In the ab-
sence of parallel corpora to train lexicons human
constructed dictionaries were used as an alternative
which may, however, not be available for a large
number of languages. Our proposal of active learn-
ing in this paper is suitable for highly impoverished
scenarios that require support from a human.
The second contribution of the paper is to ex-
tend the traditional active learning setup that is suit-
able for eliciting a single annotation. We highlight
the needs of the comparable corpora scenario where
we have two kinds of annotations - class label as-
signment and parallel segment extraction and pro-
pose strategies in active learning that involve multi-
ple annotations. A relevant setup is multitask learn-
ing (Caruana, 1997) which is increasingly becom-
ing popular in natural language processing for learn-
ing from multiple learning tasks. There has been
very less work in the area of multitask active learn-
ing. (Reichart et al, 2008) proposes an extension of
the single-sided active elicitation task to a multi-task
scenario, where data elicitation is performed for two
or more independent tasks at the same time. (Settles
et al, 2008) propose elicitation of annotations for
image segmentation under a multi-instance learning
framework.
Active learning with multiple annotations also has
similarities to the recent body of work in learn-
ing from instance feedback and feature feedback
(Melville et al, 2005). (Druck et al, 2009) pro-
pose active learning extensions to the gradient ap-
proach of learning from feature and instance feed-
back. However, in the comparable corpora problem
although the second annotation is geared towards
learning better features by enhancing the coverage
of the lexicon, the annotation itself is not on the fea-
tures but for extracting training data that is then used
to train the lexicon.
70
3 Supervised Comparable Sentence
Classification
In this section we discuss our supervised training
setup and the classification algorithm. Our classifier
tries to identify comparable sentences from among a
large pool of noisy comparable sentences. In this pa-
per we define comparable sentences as being trans-
lations that have around fifty percent or more trans-
lation equivalence. In future we will evaluate the ro-
bustness of the classifier by varying levels of noise
at the sentence level.
3.1 Training the Classifier
Following (Munteanu and Marcu, 2005), we use a
Maximum Entropy classifier to identify comparable
sentences. The classifier probability can be defined
as:
Pr(ci|S, T ) = 1Z(S, T )exp
?
?
n?
j=1
?jfij(ci, S, T )
?
?
where (S, T ) is a sentence pair, ci is the class, fij
are feature functions and Z(S) is a normalizing fac-
tor. The parameters ?i are the weights for the fea-
ture functions and are estimated by optimizing on a
training data set. For the task of classifying a sen-
tence pair, there are two classes, c0 = comparable
and c1 = non parallel. A value closer to one for
Pr(c1|S, T ) indicates that (S, T ) are comparable.
To train the classifier we need comparable sen-
tence pairs and non-parallel sentence pairs. While
it is easy to find negative examples online, ac-
quiring comparable sentences is non-trivial and re-
quires human intervention. (Munteanu and Marcu,
2005) construct negative examples automatically
from positive examples by pairing all source sen-
tences with all target sentences. We, however, as-
sume the availability of both positive and negative
examples to train the classifier. We use the GIS
learning algorithm for tuning the model parameters.
3.2 Feature Computation
The features are defined primarily based on trans-
lation lexicon probabilities. Rather than computing
word alignment between the two sentences, we use
lexical probabilities to determine alignment points
as follows: a source word s is aligned to a target
word t if p(s|t) > 0.5. Target word alignment is
computed similarly. Long contiguous sections of
aligned words indicate parallelism. We use the fol-
lowing features:
? Source and target sentence length ratio
? Source and target sentence length difference
? Lexical probability score, similar to IBM
model 1
? Number of aligned words
? Longest aligned word sequence
? Number of un-aligned words
Lexical probability score, and alignment features
generate two sets of features based on translation
lexica obtained by training in both directions. Fea-
tures are normalized with respect to the sentence
length.
Figure 1: Seed parallel corpora size vs. Classifier perfor-
mance in Urdu-English language pair
In our experiments we observe that the most in-
formative features are the ones involving the prob-
abilistic lexicon. However, the comparable corpora
obtained for training the classifier cannot be used for
automatically training a lexicon. We, therefore, re-
quire the availability of an initial seed parallel cor-
pus that can be used for computing the lexicon and
the associated feature functions. We notice that the
size of the seed corpus has a large influence on the
accuracy of the classifier. Figure 1 shows a plot with
71
the initial size of the corpus used to construct the
probabilistic lexicon on x-axis and its effect on the
accuracy of the classifier on y-axis. The sentences
were drawn randomly from a large pool of Urdu-
English parallel corpus and it is clear that a larger
pool of parallel sentences leads to a better lexicon
and an improved classifier.
4 Active Learning with Multiple
Annotations
4.1 Cost Motivation
Lack of existing annotated data requires reliable
human annotation that is expensive and effort-
intensive. We propose active learning for the prob-
lem of effectively acquiring multiple annotations
starting with unlabeled data. In active learning, the
learner has access to a large pool of unlabeled data
and sometimes a small portion of seed labeled data.
The objective of the active learner is then to se-
lect the most informative instances from the unla-
beled data and seek annotations from a human ex-
pert, which it then uses to retrain the underlying su-
pervised model for improving performance.
A meaningful setup to study multi annotation ac-
tive learning is to take into account the cost involved
for each of the annotations. In the case of compara-
ble corpora we have two annotation tasks, each with
cost modelsCost1 andCost2 respectively. The goal
of multi annotation active learning is to select the
optimal set of instances for each annotation so as to
maximize the benefit to the classifier. Unlike the tra-
ditional active learning, where we optimize the num-
ber of instances we label, here we optimize the se-
lection under a provided budget Bk per iteration of
the active learning algorithm.
4.2 Active Learning Setup
We now discuss our active learning framework for
building comparable corpora as shown in Algo-
rithm 1. We start with an unlabeled dataset U0 =
{xj =< sj , tj >} and a seed labeled dataset L0 =
{(< sj , tj >, ci)}, where c ? 0, 1 are class la-
bels with 0 being the non-parallel class and 1 being
the comparable data class. We also have T0 = {<
sk, tk >} which corresponds to parallel segments
or sentences identified from L0 that will be used in
training the probabilistic lexicon. Both T0 and L0
can be very small in size at the start of the active
learning loop. In our experiments, we tried with as
few as 50 to 100 sentences for each of the datasets.
We perform an iterative budget motivated active
learning loop for acquiring labeled data over k it-
erations. We start the active learning loop by first
training a lexicon with the available Tk and then us-
ing that we train the classifier over Lk. We, then
score all the sentences in the Uk using the model ?
and apply our selection strategy to retrieve the best
scoring instance or a small batch of instances. In the
simplest case we annotate this instance and add it
back to the tuning set Ck for re-training the classi-
fier. If the instance was a comparable sentence pair,
then we could also perform the second annotation
conditioned upon the availability of the budget. The
identified sub-segments (ssi , tti) are added back to
the training data Tk used for training the lexicon in
the subsequent iterations.
Algorithm 1 ACTIVE LEARNING SETUP
1: Given Unlabeled Comparable Corpus: U0
2: Given Seed Parallel Corpus: T0
3: Given Tuning Corpus: L0
4: for k = 0 to K do
5: Train Lexicon using Tk
6: ? = Tune Classifier using Ck
7: while Cost < Bk do
8: i = Query(Uk,Lk,Tk,?)
9: ci = Human Annotation-1 (si, ti)
10: (ssi ,tti) = Human Annotation-2 xi
11: Lk = Ck ? (si, ti, ci)
12: Tk = Tk ? (ssi, tti)
13: Uk = Uk - xi
14: Cost = Cost1 + Cost2
15: end while
16: end for
5 Sampling Strategies for Active Learning
5.1 Acquiring Training Data for Classifier
Our selection strategies for obtaining class labels for
training the classifier uses the model in its current
state to decide on the informative instances for the
next round of iterative training. We propose the fol-
lowing two sampling strategies for this task.
72
5.1.1 Certainty Sampling
This strategy selects instances where the current
model is highly confident. While this may seem
redundant at the outset, we argue that this crite-
ria can be a good sampling strategy when the clas-
sifier is weak or trained in an impoverished data
scenario. Certainty sampling strategy is a lot sim-
ilar to the idea of unsupervised approaches like
boosting or self-training. However, we make it a
semi-supervised approach by having a human in the
loop to provide affirmation for the selected instance.
Consider the following scenario. If we select an
instance that our current model prefers and obtain
a contradicting label from the human, then this in-
stance has a maximal impact on the decision bound-
ary of the classifier. On the other hand, if the label
is reaffirmed by a human, the overall variance re-
duces and in the process, it also helps in assigning
higher preference for the configuration of the deci-
sion boundary. (Melville et al, 2005) introduce a
certainty sampling strategy for the task of feature
labeling in a text categorization task. Inspired by
the same we borrow the name and also apply this
as an instance sampling approach. Given an in-
stance x and the classifier posterior distribution for
the classes as P (.), we select the most informative
instance as follows:
x? = argmaxxP (c = 1|x)
5.1.2 Margin-based Sampling
The certainty sampling strategy only considers the
instance that has the best score for the comparable
sentence class. However we could benefit from in-
formation about the second best class assigned to
the same instance. In the typical multi-class clas-
sification problems, earlier work shows success us-
ing such a ?margin based? approach (Scheffer et al,
2001), where the difference between the probabil-
ities assigned by the underlying model to the first
best and second best classes is used as the sampling
criteria.
Given a classifier with posterior distribution
over classes for an instance P (c = 1|x),
the margin based strategy is framed as x? =
argminxP (c1|x)? P (c2|x), where c1 is the best
prediction for the class and c2 is the second best
prediction under the model. It should be noted that
for binary classification tasks with two classes, the
margin sampling approach reduces to an uncertainty
sampling approach (Lewis and Catlett, 1994).
5.2 Acquiring Parallel Segments for Lexicon
Training
We now propose two sampling strategies for the sec-
ond annotation. Our goal is to select instances that
could potentially provide parallel segments for im-
proved lexical coverage and feature computation.
5.2.1 Diversity Sampling
We are interested in acquiring clean parallel seg-
ments for training a lexicon that can be used in fea-
ture computation. It is not clear how one could use a
comparable sentence pair to decide the potential for
extracting a parallel segment. However, it is highly
likely that if such a sentence pair has new cover-
age on the source side, then it increases the chances
of obtaining new coverage. We, therefore, propose
a diversity based sampling for extracting instances
that provide new vocabulary coverage . The scor-
ing function tc score(s) is defined below, where
V oc(s) is defined as the vocabulary of source sen-
tence s for an instance xi =< si, ti >, T is the set
of parallel sentences or segments extracted so far.
tc score(s) =
|T |?
s=1
sim(s, s?) ? 1|T | (1)
sim(s, s?) = |(V oc(s) ? V oc(s?)| (2)
5.2.2 Alignment Ratio
We also propose a strategy that provides direct in-
sight into the coverage of the underlying lexicon and
prefers a sentence pair that is more likely to be com-
parable. We call this alignment ratio and it can be
easily computed from the available set of features
discussed in Section 3 as below:
a score(s) = #unalignedwords#alignedwords (3)
s? = argmaxsa score(s) (4)
This strategy is quite similar to the diversity based
approach as both prefer selecting sentences that have
73
a potential to offer new vocabulary from the com-
parable sentence pair. However while the diver-
sity approach looks only at the source side coverage
and does not depend upon the underlying lexicon,
the alignment ratio utilizes the model for computing
coverage. It should also be noted that while we have
coverage for a word in the sentence pair, it may not
make it to the probabilistically trained and extracted
lexicon.
5.3 Combining Multiple Annotations
Finally, given two annotations and corresponding
sampling strategies, we try to jointly select the sen-
tence that is best suitable for obtaining both the an-
notations and is maximally beneficial to the classi-
fier. We select a single instance by combining the
scores from the different selection strategies as a
geometric mean. For instance, we consider a mar-
gin based sampling (margin) for the first annota-
tion and a diversity sampling (tc score) for the sec-
ond annotation, we can jointly select a sentence that
maximizes the combined score as shown below:
total score(s) = margin(s) ? tc score(s) (5)
s? = argmaxstotal score(s) (6)
6 Experiments and Results
6.1 Data
This research primarily focuses on identifying com-
parable sentences from a pool of dominantly non-
parallel sentences. To our knowledge, there is a
dearth of publicly available comparable corpora of
this nature. We, therefore, simulate a low-resource
scenario by using realistic assumptions of noise
and parallelism at both the corpus-level and the
sentence-level. In this section we discuss the pro-
cess and assumptions involved in the creation of our
datasets and try to mimic the properties of real-world
comparable corpora harvested from the web.
We first start with a sentence-aligned parallel cor-
pus available for the language pair. We then divide
the corpus into three parts. The first part is called
the ?sampling pool? and is set aside to use for draw-
ing sentences at random. The second part is used
to act as a non-parallel corpus. We achieve non-
parallelism by randomizing the mapping of the tar-
get sentences with the source sentences. This is a
slight variation of the strategy used in (Munteanu
and Marcu, 2005) for generating negative examples
for their classifier. The third part is used to synthe-
size a comparable corpus at the sentence-level. We
perform this by first selecting a parallel sentence-
pair and then padding either sides by a source and
target segment drawn independently from the sam-
pling pool. We control the length of the non-parallel
portion that is appended to be lesser than or equal
to the original length of the sentence. Therefore, the
resulting synthesized comparable sentence pairs are
guaranteed to contain at least 50% parallelism.
We use this dataset as the unlabeled pool from
which the active learner selects instances for label-
ing. Since the gold-standard labels for this corpus
are already available, which gives us better control
over automating the active learning process, which
typically requires a human in the loop. However,
our active learning strategies are in no way limited
by the simulated data setup and can generalize to the
real world scenario with an expert providing the la-
bels for each instance.
We perform our experiments with data from two
language pairs: Urdu-English and Spanish-English.
For Urdu-English, we use the parallel corpus NIST
2008 dataset released for the translation shared task.
We start with 50,000 parallel sentence corpus from
the released training data to create a corpus of
25,000 sentence pairs with 12,500 each of compa-
rable and non-parallel sentence pairs. Similarly, we
use 50,000 parallel sentences from the training data
released by the WMT 2008 datasets for Spanish-
English to create a corpus of 25,000 sentence pairs.
We also use two held-out data sets for training and
tuning the classifier, consisting of 1000 sentence
pairs (500 non-parallel and 500 comparable).
6.2 Results
We perform two kinds of evaluations: the first, to
show that our active learning strategies perform well
across language pairs and the second, to show that
multi annotation active learning leads to a good im-
provement in performance of the classifier.
6.2.1 How does the Active Learning perform?
In section 5, we proposed multiple active learn-
ing strategies for both eliciting both kinds of annota-
tions. A good active learning strategy should select
74
instances that contribute to the maximal improve-
ment of the classifier. The effectiveness of active
learning is typically tested by the number of queries
the learner asks and the resultant improvement in
the performance of the classifier. The classifier per-
formance in the comparable sentence classification
task can be computed as the F-score on the held out
dataset. For this work, we assume that both the an-
notations require the same effort level and so assign
uniform cost for eliciting each of them. Therefore
the number of queries is equivalent to the total cost
of supervision.
Figure 2: Active learning performance for the compara-
ble corpora classification in Urdu-English language-pair
Figure 3: Active learning performance for the compara-
ble corpora classification in Spanish-English language-
pair
Figure 2 shows our results for the Urdu-English
language pair, and Figure 3 plots the Spanish-
English results with the x-axis showing the total
number of queries posed to obtain annotations and
the y-axis shows the resultant improvement in accu-
racy of the classifier. In these experiments we do
not actively select for the second annotation but ac-
quire the parallel segment from the same sentence.
We compare this over a random baseline where the
sentence pair is selected at random and used for elic-
iting both annotations at the same time.
Firstly, we notice that both our active learn-
ing strategies: certainty sampling and margin-based
sampling perform better than the random baseline.
For the Urdu-English language pair we can see that
for the same effort expended (i.e 2000 queries) the
classifier has an increase in accuracy of 8 absolute
points. For Spanish-English language pair the ac-
curacy improvement is 6 points over random base-
line. Another observation from Figure 3 is that for
the classifier to reach an fixed accuracy of 68 points,
the random sampling method requires 2000 queries
while the from the active selection strategies require
significantly less effort of about 500 queries.
6.2.2 Performance of Joint Selection with
Multiple Annotations
We now evaluate our joint selection strategy that
tries to select the best possible instance for both
the annotations. Figure 4 shows our results for the
Urdu-English language pair, and Figure 5 plots the
Spanish-English results for active learning with mul-
tiple annotations. As before, the x-axis shows the
total number of queries posed, equivalent to the cu-
mulative effort for obtaining the annotations and the
y-axis shows the resultant improvement in accuracy
of the classifier.
We evaluate the multi annotation active learning
against two single-sided baselines where the sam-
pling focus is on selecting instances according to
strategies suitable for one annotation at a time. The
best performing active learning strategy for the class
label annotations is the certainty sampling (annot1)
and so for one single-sided baseline, we use this
baseline. We also obtain the second annotation for
the same instance. By doing so, we might be se-
lecting an instance that is sub-optimal for the sec-
ond annotation and therefore the resultant lexicon
may not maximally benefit from the instance. We
also observe, from our experiments, that the diver-
sity based sampling works well for the second anno-
75
tation and alignment ratio does not perform as well.
So, for the second single-sided baseline we use the
diversity based sampling strategy (annot2) and get
the first annotation for the same instance. Finally
we compare this with the joint selection approach
proposed earlier that combines both the annotation
strategies (annot1+annot2). In both the language
pairs we notice that joint selection for both anno-
tations performs better than the baselines.
Figure 4: Active learning with multiple annotations and
classification performance in Urdu-English
Figure 5: Active learning with multiple annotations and
classification performance in Spanish-English
7 Conclusion and Future Work
In this paper, we proposed active learning with mul-
tiple annotations for the challenge of building com-
parable corpora in low-resource scenarios. In par-
ticular, we identified two kinds of annotations: class
labels (for identifying comparable vs. non-parallel
data) and clean parallel segments within the com-
parable sentences. We implemented multiple inde-
pendent strategies for obtaining each of the abve in
a cost-effective manner. Our active learning experi-
ments in a simulated low-resource comparable cor-
pora scenario across two language pairs show signif-
icant results over strong baselines. Finally we also
proposed a joint selection strategy that selects a sin-
gle instance which is beneficial to both the annota-
tions. The results indicate an improvement over sin-
gle strategy baselines.
There are several interesting questions for future
work. Throughout the paper we assumed uniform
costs for both the annotations, which will need to
be verified with human subjects. We also hypoth-
esize that obtaining both annotations for the same
sentence may be cheaper than getting them from two
different sentences due to the overhead of context
switching. Another assumption is that of the exis-
tence of a single contiguous parallel segment in a
comparable sentence pair, which needs to be veri-
fied for corpora on the web.
Finally, active learning assumes availability of an
expert to answer the queries. Availability of an ex-
pert for low-resource languages and feasibility of
running large scale experiments is difficult. We,
therefore, have started working on crowdsourcing
these annotation tasks on Amazon Mechanical Turk
(MTurk) where it is easy to find people and quickly
run experiments with real people.
Acknowledgement
This material is based upon work supported in part
by the U. S. Army Research Laboratory and the U.
S. Army Research Office under grant W911NF-10-
1-0533, and in part by NSF under grant IIS 0916866.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Rich Caruana. 1997. Multitask learning. In Machine
Learning, pages 41?75.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active learning by labeling features. In Pro-
ceedings of Conference on Empirical Methods in Nat-
76
ural Language Processing (EMNLP 2009), pages 81?
90.
Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of ACL 2010.
Pascale Fung and Lo Yen Yee. 1998. An IR approach for
translating new words from nonparallel, comparable
texts. In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics, pages
414?420, Montreal, Canada.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In In
Proceedings of the Eleventh International Conference
on Machine Learning, pages 148?156. Morgan Kauf-
mann.
Prem Melville, Foster Provost, Maytal Saar-Tsechansky,
and Raymond Mooney. 2005. Economical active
feature-value acquisition through expected utility esti-
mation. In UBDM ?05: Proceedings of the 1st interna-
tional workshop on Utility-based data mining, pages
10?16, New York, NY, USA. ACM.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 81?88, Sydney, Aus-
tralia.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345?1359, October.
Chris Quirk, Raghavendra U. Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of the Machine Translation Summit XI, pages
377?384, Copenhagen, Denmark.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguis-
tic annotations. In Proceedings of ACL-08: HLT,
pages 861?869, Columbus, Ohio, June. Association
for Computational Linguistics.
Philip Resnik and Noah A. Smith. 2003. The web as a
parallel corpus. Comput. Linguist., 29(3):349?380.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for informa-
tion extraction. In IDA ?01: Proceedings of the 4th
International Conference on Advances in Intelligent
Data Analysis, pages 309?318, London, UK. Springer-
Verlag.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In In Advances in
Neural Information Processing Systems (NIPS, pages
1289?1296. MIT Press.
Bing Zhao and Stephan Vogel. 2002. Full-text story
alignment models for chinese-english bilingual news
corpora. In Proceedings of the ICSLP ?02, September.
77
