Proceedings of NAACL HLT 2007, Companion Volume, pages 9?12,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Conquest ? an Open-Source Dialog System for Conferences 
 
Dan Bohus, Sergio Grau Puerto, David Huggins-Daines, Venkatesh Keri,  
Gopala Krishna,  Rohit Kumar, Antoine Raux, Stefanie Tomko 
School of Computer Science 
Carnegie Mellon University 
{ dbohus, sgrau, dhuggins, vkeri, gopalakr, rohitk, antoine, stef }@ cs.cmu.edu 
 
 
  
Abstract 
We describe ConQuest, an open-source, 
reusable spoken dialog system that pro-
vides technical program information dur-
ing conferences. The system uses a 
transparent, modular and open infrastruc-
ture, and aims to enable applied research 
in spoken language interfaces. The con-
ference domain is a good platform for ap-
plied research since it permits periodical 
redeployments and evaluations with a real 
user-base. In this paper, we describe the 
system?s functionality, overall architec-
ture, and we discuss two initial deploy-
ments.  
1 Introduction  
Conducting applied spoken language interface re-
search is generally a costly endeavor. Developing, 
deploying and maintaining real-world spoken lan-
guage interfaces requires an existing infrastructure, 
a significant amount of engineering effort, and can 
greatly benefit from the availability of certain re-
sources such as transcribed in-domain data.  
In an effort to enable applied research and to 
lower this high cost of entry, we have developed 
ConQuest (Conference Questions) an open-source 
spoken dialog system that provides access to 
schedule and technical program information during 
conferences. We believe the conference domain 
has a number of good properties for applied re-
search: it includes a number of tasks of different 
complexities, it provides regular access to a real-
world user population; it permits periodical rede-
ployments and evaluations and therefore can pro-
vide a natural common evaluation task for the 
spoken language interfaces community.  
The ConQuest system is constructed on top of 
the open, transparent and modular Olympus dialog 
system framework (2007), and can be easily reused 
across different conferences. To date, the system 
has been deployed in two conferences: InterSpeech 
2006 and IJCAI 2007. Together with corpora col-
lected from these deployments, the system is freely 
available for download (Conquest, 2007).  
We begin by describing the ConQuest function-
ality in the next section. Then, in section 3 we pro-
vide an overview of the system architecture and 
discuss the development process. In section 4 we 
briefly discuss the two deployment efforts. Finally, 
in section 5 we discuss related work and draw a 
number of conclusions.   
2 Functionality 
As Figure 1 illustrates, ConQuest is a mixed-
initiative spoken dialog system that provides ac-
cess to schedule and technical program information 
during conferences.  
Users can browse the schedule and find details 
about various papers or sessions of interest by pro-
viding identifying information, such as topics, ses-
sion names, special events, paper titles, author 
names, specific dates and times, specific locations, 
or a combination thereof (e.g. turns 2, 4, 14). Addi-
tionally, the system also allows uses to listen to 
current announcements and to cast their votes for a 
Best Paper Award (e.g. turns 10-17).  
The interaction is mixed-initiative; at any point, 
the user may switch the current focus of the con-
versation (e.g. turn 8). ConQuest employs an adap-
tive information presentation strategy that allows 
9
users to easily navigate the schedule (see turns 3, 5 
and 15). The system uses a rich repertoire of error 
recovery strategies to handle potential errors, in-
cluding several fall-back strategies (e.g. turn 13).  
3 System Architecture  
The ConQuest system was built using RavenClaw/ 
Olympus (2007), an open-source framework that 
facilitates research and development in task ori-
ented conversational spoken language interfaces. 
Olympus consists of a collection of components 
for recognition, language understanding, dialog 
management, language generation, speech synthe-
sis, etc., and the corresponding communication 
infrastructure. To date, Olympus has been used to 
develop and deploy a number of other systems 
spanning different domains and interaction types 
(Bohus and Rudnicky, 2003).  
A key characteristic of the Olympus framework 
is a clear separation between the domain independ-
ent programs (or components) and domain specific 
resources. This decoupling promotes reusability 
and significantly lessens the system development 
effort. In ConQuest, the authoring effort was fo-
cused on developing resources such as the lexicon, 
language model, grammar, dialog task specifica-
tion, etc. Some interesting, unanticipated engineer-
ing challenges we faced during development were 
dealing with foreign names and accented charac-
ters and performing text normalization on various 
fields (e.g. Alex Smith and Alexander Smith are 
the same author), while at the same time ensuring 
consistency between these various resources. Be-
low, we briefly comment of each component and 
the corresponding resources. Figure 2 provides a 
top-level architectural view. 
Speech Recognition. ConQuest uses a recogni-
tion server coupled to a set of parallel recognition 
engines: two SPHINX-II decoders (Huang et al, 
1992) that use gender-specific acoustic models, 
and a DTMF (touch-tone decoder). Each recogni-
tion engine uses class-based (e.g. paper titles, au-
thor names, etc.), state-specific trigram-language 
models. We started with an initial language model 
built using data collected with an early text-only 
prototype. We then internally deployed a speech 
based system, collected more data, transcribed it, 
and used it to retrain the language models. The 
Figure 1. A sample interaction with ConQuest 
1 S: Welcome to ConQuest [?] I can give you information about papers, sessions, special events and announcements at Inter-
Speech 2006. You can also cast a vote for the best paper award. How can I help you? 
2 U: are there any papers on spoken dialog systems today 
3 S:  There are 23 papers on spoken dialog systems today. These papers are being presented in 3 sessions: Dialog and Discourse, 
Language Modeling for Spoken Dialog Systems and Spoken Dialog Systems I. To browse this list of papers, please tell me the 
author name, the paper title, the time or the venue you?re looking for. If you?re not interested in any of these papers say ?quit?. 
4 U: which ones are in the afternoon 
5 S:  There will be 11 papers on spoken dialog systems this afternoon. Would you like to hear information about all these papers? 
6 U: yes 
7 S:  The list of papers on spoken dialog systems this afternoon is: Paper 1 - ?Dynamic Extension of a Grammar-based Dialogue 
System: Constructing an All-Recipes Knowing Robot?, Paper 2 ?  [?] Would you like more information about any of these? 
8 U: tell me more about the third one 
9 S:  Paper 3 is titled ?System- versus user-initiative dialog strategy for driver information systems?, and was presented yesterday at 2 
pm in the poster session in Allegheny Ballroom Foyer. This paper was written by Chantal Ackermann, and Marion Libossek. 
Would you like more information about any of these? 
8 U: no when is the banquet 
9 S:  The Banquet will be held Wednesday from 7pm to 9:30 pm in the Heintz History Center. The Heinz History Center is located on 
Smallman street between 12th and 13th streets. The History Center is a museum that relates and interprets 250 years of history 
in the Western Pennsylvania area. Is there anything else I can do for you? 
10 U: yes I?d like to cast a vote 
11 S:  Please tell me the title of the paper you?d like to vote for.. 
12 U: I don?t really know the title but I think it had Diane as one of the main authors ? [non-understanding] 
13 S:  Sorry, I didn?t catch that. Please dial the first six letters from the title of the paper you would like to vote for.  
14 U: I don?t know it it?s a paper by Diane Litman 
15 S:  There are 3 papers by Diane Litman. Paper 1 ? ?Discourse Structure and Speech Recognition Problems?, Paper 2 ? [?], Paper 
3 ? [?] Would you like to vote for any of these? 
16 U: Discourse Structure and Speech Recognition Problems 
17 S:  Okay, I?ve recorded your vote for the paper entitled ?Discourse Structure and Speech Recognition Problems? What else can I do 
for you?  
18 U: [?] 
10
final language models used during the InterSpeech 
deployment were trained from on a corpus of 6350 
utterances. The system operated with a lexicon of 
4795 words, which included 659 lexicalized (con-
catenated) paper titles, and 1492 lexicalized author 
names, and 78 lexicalized session names. The pro-
nunciations were generated using CMU Dictionary 
and later manually corrected.  
Language understanding. The system uses the 
Phoenix (Ward and Issar, 1994) robust parser to 
extract concepts from the recognition results. A 
domain-specific shallow semantic grammar was 
developed and concatenated with a domain-
independent grammar for generic expressions like 
[Yes], [No], [Date], [Time], etc.  
Dialog management. ConQuest uses a Raven-
Claw-based dialog manager (Bohus and Rudnicky, 
2003). We developed a dialog task specification 
for the conference schedule domain, expressed as a 
hierarchical plan for the interaction, which the 
RavenClaw engine uses to drive the dialog. In the 
process, the RavenClaw engine automatically pro-
vides additional generic conversational skills such 
as error recovery strategies and support for various 
universal dialog mechanisms (e.g. repeat, start-
over, what-can-I-say, etc.)  
Backend/Database. A backend agent looks up 
schedule information from the database (stored as 
a flat text file). The backend agent also performs 
domain specific pre-lookup normalization (e.g. 
mapping author names to their canonical forms), 
and post-lookup processing of the returned records 
(e.g. clustering papers by sessions). The database 
file serves as starting point for constructing a 
number of other system resources (e.g. language 
model classes, lexicon, etc.)  
Figure 2. The Olympus dialog system reference architecture (a typical system) 
Temporal reference resolution agent. Apart 
from the database agent, the dialog manager also 
communicates with an agent that resolves temporal 
expressions (e.g. tomorrow at four p.m.) into ca-
nonical forms.  
Language generation. ConQuest uses Rosetta, 
a template-based language generation component. 
The authoring effort at this level consisted of writ-
ing various templates for the different system ques-
tions and information presentation prompts.  
Speech synthesis. ConQuest uses the Cepstral 
(2005) speech synthesis engine, configured with an 
open-domain unit selection voice. We manually 
checked and corrected pronunciations for author 
names, various technical terms and abbreviations.  
4 Development and Deployment 
The first development of ConQuest system was 
done for the Interspeech 2006 conference held in 
Pittsburgh, PA. The iterative development process 
involved regular interaction with potential users 
i.e. researchers who regularly attend conferences. 
Seven developers working half time participated in 
this development for about three months. An esti-
mated one man-year of effort was spent. This esti-
mate does not include the effort involved in 
transcribing the data collected after the conference. 
Two systems were deployed at the Interspeech 
2006 conference: a desktop system using a close-
talking microphone placed by the registration desk, 
and a telephone-based system. Throughout the 
conference we collected a corpus of 174 sessions. 
We have orthographically transcribed the user ut-
11
terances and are currently analyzing the data; we 
plan to soon release it to the community, together 
with detailed statistics, the full system logs as well 
as the full system source code (Conquest, 2007). 
Following Interspeech 2006, ConQuest was re-
deployed at IJCAI 2007 conference held in Hy-
derabad, India. The second deployment took an 
estimated two man-months: three developers work-
ing half-time for over a month. The significant 
parts of the second deployment involved incorpo-
rating scheduling data for the IJCAI 2007 and im-
plementing two new requirements i.e. support for 
workshops and Indian English speech recognition. 
The IJCAI development had fewer iterations than 
the first effort. The two desktop systems set up at 
the conference venue collected 129 sessions of 
data. This data is currently being transcribed and 
will soon be released to the community through the 
Conquest website (Conquest, 2007). 
Through these two deployments of ConQuest 
the system specifications have been refined and we 
expect the development time to asymptote to less 
than a month after a few more deployments. 
5 Discussion and Conclusion  
Our primary goal in developing ConQuest was to 
enable research by constructing and releasing an 
open-source, full-fledged dialog system, as well as 
an initial corpus collected with this system. The 
system is built on top of an open, transparent and 
modular infrastructure that facilitates research in 
spoken language interfaces (Olympus, 2007).  
There have been a number of other efforts to 
collect and publish dialog corpora, for instance 
within the DARPA Communicator project. A more 
recent project, that operates in a domain similar to 
ConQuest is DiSCoH, a Dialog System for Confer-
ence Help developed by researchers at AT&T, 
ICSI and Edinburgh University, and deployed dur-
ing the SLT-2006 workshop (Adreani et al, 2006). 
While their goals are similar, i.e. to enable re-
search, DiSCoH and ConQuest differ in a number 
of dimensions. Functionality-wise, DiSCoH offers 
general conference information about the venue, 
accommodation options and costs, paper submis-
sion, etc., while ConQuest provides access to the 
technical schedule and allows participants to vote 
for a best paper award. DiSCoH is built using 
AT&T technology and a call-routing approach; 
ConQuest relies on a plan-based dialog manage-
ment framework (RavenClaw) and an open-source 
infrastructure (Olympus). Finally, the DiSCoH ef-
fort aims to develop a richly annotated dialog cor-
pus to be used for research; ConQuest?s aim is to 
provide both the full system and an initial tran-
scribed and annotated corpus to the community. 
The conference domain is interesting in that it 
allows for frequent redeployment and in theory 
provides regular access to a certain user-base. It 
should therefore facilitate research and periodical 
evaluations. Unfortunately, the dialog corpora col-
lected so far using DiSCoH and ConQuest have 
been somewhat smaller than our initial expecta-
tions. We believe this is largely due to the fact that 
the systems provide information that is already 
accessible to users by other means (paper confer-
ence program, web-sites, etc.). Perhaps combining 
the functionalities of these two systems, and ex-
panding into directions where the system provides 
otherwise hard-to-access information (e.g. local 
restaurants, transportation, etc.) would lead to in-
creased traffic.  
References  
Adreani, G., Di Fabbrizio, G., Gilbert, M., Gillick, D., 
Hakkani-Tur, D., and Lemon, O., 2006 Let?s DiS-
CoH: Collecting an Annotated Open Corpus with 
Dialogue Acts and Reward Signals for Natural Lan-
guage Helpdesk, in Proceedings of IEEE SLT-2006 
Workshop, Aruba Beach, Aruba. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proceedings of 
Eurospeech 2003, Geneva, Switzerland. 
Cepstral, LLC, 2005, SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Conquest, 2007, http://www.conquest-dialog.org. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Olympus/RavenClaw web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proceedings of the ARPA Human Language Tech-
nology Workshop, pages 213?216, Plainsboro, NJ. 
12
Proceedings of NAACL HLT 2007, Companion Volume, pages 73?76,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Implicitly Supervised Language Model Adaptation for Meeting
Transcription
David Huggins-Daines
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dhuggins@cs.cmu.edu
Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
air@cs.cmu.edu
Abstract
We describe the use of meeting metadata,
acquired using a computerized meeting
organization and note-taking system, to
improve automatic transcription of meet-
ings. By applying a two-step language
model adaptation process based on notes
and agenda items, we were able to re-
duce perplexity by 9% and word error rate
by 4% relative on a set of ten meetings
recorded in-house. This approach can be
used to leverage other types of metadata.
1 Introduction
Automatic transcription of multi-party conversa-
tions such as meetings is one of the most difficult
tasks in automatic speech recognition. In (Morgan
et al, 2003) it is described as an ?ASR-complete?
problem, one that presents unique challenges for ev-
ery component of a speech recognition system.
Though much of the literature on meeting tran-
scription has focused on the unique acoustic mod-
eling and segmentation problems incurred by meet-
ing transcription, language modeling for meetings
is an interesting problem as well. Though meet-
ing speech is spontaneous in nature, the vocabulary
and phrasing in meetings can be very specialized
and often highly technical. Speaking style can vary
greatly between speakers, and the discourse struc-
ture of multi-party interaction gives rise to cross-
speaker effects that are difficult to model with stan-
dard N-gram models (Ji and Bilmes, 2004).
Speech in meetings has one crucial advantage
over many other transcription tasks, namely that it
does not occur in isolation. Meetings are scheduled
and discussed in advance, often via e-mail. People
take notes and create agendas for meetings, and of-
ten read directly from electronic presentation mate-
rials. The structure of meetings can be exploited -
topics can be segmented both temporally and across
speakers, and these shifting topics can be modeled
as sub-languages.
We examine the effect of leveraging one partic-
ular type of external information, namely the writ-
ten agendas and meeting minutes, and we demon-
strate that, by using off-line language model adapta-
tion techniques, these can significantly (p < 0.01)
improve language modeling and speech recognition
accuracy. The language in the notes and agendas is
very similar to that used by the speakers, hence we
consider this to be a form of semi-supervised or im-
plicitly supervised adaptation.
2 Corpus
The SmartNotes system, described in (Banerjee and
Rudnicky, 2007) is a collaborative platform for
meeting organization, recording, and note-taking.
As part of our research into meeting segmentation
and recognition, we have collected a series of 10 un-
scripted meetings using SmartNotes. These meet-
ings themselves are approximately 30 minutes in
length (ranging from 1745 to 7208 words) with three
regular participants, and consist of discussions and
reporting on our ongoing research. The meetings
are structured around the agendas and action items
constructed through the SmartNotes interface. The
73
agenda itself is largely constant from meeting to
meeting, while each meeting typically reviews dis-
cusses the previous week?s action items. Each par-
ticipant is equipped with a laptop computer and an
individual headset microphone.
Each meeting was manually transcribed and seg-
mented for training and testing purposes. The tran-
scription includes speaker identification and timing
information. As part of the meeting, participants are
encouraged to take notes and define action items.
These are automatically collected on a server along
with timestamp information. In (Banerjee and Rud-
nicky, 2007), it was shown that timestamped text of
this kind is useful for topic segmentation of meet-
ings. In this work, we have not attempted to take
advantage of the timing information, nor have we
attempted to perform any topic segmentation. Given
the small quantity of text available from the notes,
we feel that the type of static language model adap-
tation presented here is most feasible when done at
the entire meeting level. A cache language model
(Kuhn and Mori, 1990) may be able to capture the
(informally attested) locality effects between notes
and speech.
Since the notes are naturalistic text, often con-
taining shorthand, abbreviations, numbers, punctu-
ation, and so forth, we preprocess them by running
them through the text-normalization component of
the Festival1 speech synthesis system and extracting
the resulting string of individual words. This yielded
an average of 252 words of adaptation data for each
of the 10 meetings.
3 System Description
Unless otherwise noted, all language models eval-
uated here are trigram models using Katz smooth-
ing (Katz, 1987) and Good-Turing discounting. Lin-
ear interpolation of multiple source models was per-
formed by maximizing the likelihood over a held-out
set of adaptation data.
For automatic transcription, our acoustic mod-
els consist of 5000 tied triphone states (senones),
each using a 64-component Gaussian mixture model
with diagonal covariance matrices. The input fea-
tures consist of 13-dimensional MFCC features,
delta, and delta-delta coefficients. These models
1http://www.festvox.org/
Corpus # Words Perplexity
Fisher English 19902585 178.41
Switchboard-I 2781951 215.52
ICSI (75 Meetings) 710115 134.94
Regular Meetings 266043 111.76
Switchboard Cellular 253977 280.81
CallHome English 211377 272.19
NIST Meetings 136932 199.40
CMU (ISL Meetings) 107235 292.86
Scenario Meetings 36694 306.43
Table 1: Source Corpora for Language Model
are trained on approximately 370 hours of speech
data, consisting of the ICSI meeting corpus (Mor-
gan et al, 2003), the HUB-4 Broadcast News cor-
pus, the NIST pilot meeting corpus, the WSJ CSR-
0 and CSR-1 corpora,2 the CMU Arctic TTS cor-
pora (Kominek and Black, 2004), and a corpus of 32
hours of meetings previously recorded by our group
in 2004 and 2005.
Our baseline language model is based on a linear
interpolation of source language models built from
conversational and meeting speech corpora, using a
held-out set of previously recorded ?scenario? meet-
ings. These meetings are unscripted, but have a fixed
topic and structure, which is a fictitious scenario in-
volving the hiring of new researchers. The source
language models contain a total of 24 million words
from nine different corpora, as detailed in Table 1.
The ?Regular Meetings? and ?Scenario Meetings?
were collected in-house and consist of the same 32
hours of meetings mentioned above, along with the
remainder of the scenario meetings. We used a vo-
cabulary of 20795 words, consisting of all words
from the locally recorded, ICSI, and NIST meetings,
combined with the Switchboard-I vocabulary (with
the exception of words occurring less than 3 times).
The Switchboard and Fisher models were pruned by
dropping singleton trigrams.
4 Interpolation and Vocabulary Closure
We created one adapted language model for each
meeting using a two-step process. First, the source
language models were re-combined using linear in-
terpolation to minimize perplexity on the set of notes
2All corpora are available through http://ldc.upenn.edu/
74
Meeting Baseline Interpolated Closure
04/17 90.05 85.96 84.41
04/24 90.16 85.54 81.88
05/02 94.27 89.24 89.19
05/12 110.95 101.68 87.13
05/18 85.78 81.50 78.04
05/23 97.51 93.07 94.39
06/02 109.70 104.49 101.90
06/12 96.80 92.88 91.05
06/16 93.93 87.71 79.17
06/20 97.19 93.88 92.48
Mean 96.57 91.59 (-5.04) 87.96 (-8.7)
S.D. 8.61 7.21 (1.69) 7.40 (6.2)
p n/a < 0.01 < 0.01
Table 2: Adaptation Results: Perplexity
for each meeting. Next, the vocabulary was ex-
panded using the notes. In order to accomplish
this, a trigram language model was trained from the
notes themselves and interpolated with the output of
the previous step using a small, fixed interpolation
weight ? = 0.1. It should be noted that this also
has the effect of slightly boosting the probabilities
of the N-grams that appear in the notes. We felt this
was useful because, though these probabilities are
not reliably estimated, it is likely that people will use
many of the same N-grams in the notes as in their
meeting speech, particularly in the case of numbers
and acronyms. The results of interpolation and N-
gram closure are shown in Table 2 in terms of test-
set perplexity, and in Table 3 in terms of word error
rate. Using a paired t-test over the 10 meetings, the
improvements in perplexity and accuracy are highly
significant (p < 0.01).
5 Topic Clustering and Dimensionality
Reduction
In examining the interpolation component of the
adaptation method described above, we noticed that
the in-house meetings and the ICSI meetings consis-
tently took on the largest interpolation weights. This
is not surprising since both of these corpora are sim-
ilar to the test meetings. However, all of the source
corpora cover potentially relevant topics, and by in-
terpolating the corpora as single units, we have no
way to control the weights given to individual top-
Meeting Baseline Interpolated Closure
04/17 45.22 44.37 43.34
04/24 47.35 46.43 45.25
05/02 47.20 47.20 46.28
05/12 49.74 48.02 46.07
05/18 45.29 44.63 43.44
05/23 43.68 43.00 42.80
06/02 48.66 48.29 47.85
06/12 45.68 45.90 45.28
06/16 45.98 45.45 44.29
06/20 47.03 46.73 46.68
Mean 46.59 46.0 (-0.58) 45.13 (-1.46)
S.D. 1.78 1.68 (0.54) 1.64 (1.0)
p n/a < 0.01 < 0.01
Table 3: Adaptation Results: Word Error
ics within them. Also, people may use different, but
related, words in writing and speaking to describe
the same topic, but we are unable to capture these
semantic associations between the notes and speech.
To investigate these issues, we conducted sev-
eral brief experiments using a reduced training cor-
pus consisting of 69 ICSI meetings. We converted
these to a vector-space representation using tf.idf
scores and used a deterministic annealing algorithm
(Rose, 1998) to create hard clusters of meetings,
from each of which we trained a source model for
linear interpolation. We compared these clusters to
random uniform partitions of the corpus. The in-
terpolation weights were trained on the notes, and
the models were tested on the meeting transcripts.
Out-of-vocabulary words were not removed from
the perplexity calculation. The results (mean and
standard deviation over 10 meetings) are shown in
Table 4. For numbers of clusters between 2 and
42, the annealing-based clusters significantly out-
perform the random partition. The perplexity with
42 clusters is also significantly lower (p < 0.01)
than the perplexity (256.5? 21.5) obtained by train-
ing a separate source model for each meeting.
To address the second issue of vocabulary mis-
matches between notes and speech, we applied prob-
abilistic latent semantic analysis (Hofmann, 1999)
to the corpus, and used this to ?expand? the vocab-
ulary of the notes. We trained a 32-factor PLSA
model on the content words (we used a simple
75
# of Clusters Random Annealed
2 546.5 ? 107.4 514.1 ? 97.9
4 462.2 ? 86.3 426.2 ? 73.9
8 397.7 ? 67.1 356.1 ? 54.9
42 281.6 ? 31.5 253.7 ? 22.9
Table 4: Topic Clustering Results: Perplexity
Meeting Baseline PLSA ?Boosted?
04/17 105.49 104.59 104.87
04/24 98.97 97.58 97.80
05/02 105.61 104.15 104.48
05/12 122.37 116.73 118.04
05/18 98.55 94.92 95.18
05/23 111.28 107.84 108.03
06/02 125.31 121.49 121.64
06/12 109.31 106.38 106.55
06/16 106.86 103.27 104.28
06/20 117.46 113.76 114.18
Mean 110.12 107.07 107.50
S.D. 8.64 7.84 7.93
p n/a < 0.01 < 0.01
Table 5: PLSA Results: Perplexity
entropy-based pruning to identify these ?content
words?) from the ICSI meeting vocabulary. To adapt
the language model, we used the ?folding-in? proce-
dure described in (Hofmann, 1999), running an iter-
ation of EM over the notes to obtain an adapted un-
igram distribution. We then simply updated the uni-
gram probabilities in the language model with these
new values and renormalized. While the results,
shown in Table 5, show a statistically significant im-
provement in perplexity, this adaptation method is
is problematic, as it increases the probability mass
given to all the words in the PLSA model. In subse-
quent results, also shown in Table 5, we found that
simply extracting these words from the original un-
igram distribution and boosting their probabilities
by the equivalent amount also reduces perplexity
by nearly as much (though the difference from the
PLSA model is statistically significant, p = 0.004).
6 Conclusions
We have shown that notes collected automatically
from participants in a structured meeting situation
can be effectively used to improve language mod-
eling for automatic meeting transcription. Further-
more, we have obtained some encouraging results
in applying source clustering and dimensionality re-
duction to make more effective use of this data. In
future work, we plan to exploit other sources of
metadata such as e-mails, as well as the structure of
the meetings themselves.
7 Acknowledgements
This research was supported by DARPA grant NG
CH-D-03-0010. The content of the information in
this publication does not necessarily reflect the po-
sition or the policy of the US Government, and no
official endorsement should be inferred.
References
S. Banerjee and A. I. Rudnicky. 2007. Segmenting meet-
ings into agenda items by extracting implicit supervi-
sion from human note-taking. In Proceedings of the
2007 International Conference on Intelligent User In-
terfaces, January.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of UAI?99, Stockholm.
G. Ji and J. Bilmes. 2004. Multi-speaker language mod-
eling. In Proceedings of HLT-NAACL.
S. M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
J. Kominek and A. Black. 2004. The CMU Arctic speech
databases. In 5th ISCA Speech Synthesis Workshop,
Pittsburgh.
R. Kuhn and R. De Mori. 1990. A cache-based natural
language model for speech recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
pages 570?583.
N. Morgan, D. Baron, S. Bhagat, R. Dhillon H. Carvey,
J. Edwards, D. Gelbart, A. Janin, A. Krupski, B. Pe-
skin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. Meetings about meetings: research at ICSI on
speech in multiparty conversation. In Proceedings of
ICASSP, Hong Kong, April.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related op-
timization problems. In Proceedings of the IEEE,
pages 2210?2239.
76
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 17?19,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Interactive ASR Error Correction for Touchscreen Devices
David Huggins-Daines
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dhuggins@cs.cmu.edu
Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
air@cs.cmu.edu
Abstract
We will demonstrate a novel graphical inter-
face for correcting search errors in the out-
put of a speech recognizer. This interface
allows the user to visualize the word lattice
by ?pulling apart? regions of the hypothesis
to reveal a cloud of words simlar to the ?tag
clouds? popular in many Web applications.
This interface is potentially useful for dicta-
tion on portable touchscreen devices such as
the Nokia N800 and other mobile Internet de-
vices.
1 Introduction
For most people, dictating continuous speech is con-
siderably faster than entering text using a keyboard
or other manual input device. This is particularly
true on mobile devices which typically have no hard-
ware keyboard whatsoever, a 12-digit keypad, or at
best a miniaturized keyboard unsuitable for touch
typing.
However, the effective speed of text input using
speech is significantly reduced by the fact that even
the best speech recognition systems make errors.
After accounting for error correction, the effective
number of words per minute attainable with speech
recognition drops to within the range attainable by
an average typist (Moore, 2004). Moreover, on a
mobile phone with predictive text entry, it has been
shown that isolated word dictation is actually slower
than using a 12-digit keypad for typing SMS mes-
sages (Karpov et al, 2006).
2 Description
It has been shown that multimodal error correction
methods are much more effective than using speech
alone (Lewis, 1999). Mobile devices are increas-
ingly being equipped with touchscreens which lend
themselves to gesture-based interaction methods.
Therefore, we propose an interactive method of
visualizing and browsing the word lattice using ges-
tures in order to correct speech recognition errors.
The user is presented with the decoding result in a
large font, either in a window on the desktop, or in a
full-screen presentation on a touchscreen device. If
the utterance is too long to fit on the screen, the user
can scroll left and right using touch gestures. The
initial interface is shown in Figure 1.
Figure 1: Initial hypothesis view
Where there is an error, the user can ?pull apart?
the result using a touch stroke (or a multitouch ges-
ture where supported), revealing a ?cloud? of hy-
pothesis words at that point in the utterance, as
shown in Figure 2.
It is also possible to expand the time interval over
which the cloud is calculated by dragging sideways,
resulting in a view like that in Figure 3. The user
can then select zero or more words to add to the hy-
pothesis string in place of the errorful text which was
?exploded?, as shown in Figure 4.
17
Figure 2: Expanded word view
Figure 3: Word cloud expanded in time
The word cloud is constructed by finding all
words active within a time interval whose log poste-
rior probability falls within range of the most prob-
able word. Word posterior probabilities are cal-
culated using the forward-backward algorithm de-
scribed in (Wessel et al, 1998). Specifically, given a
word lattice in the form of a directed acyclic graph,
whose nodes represent unique starting points t in
time, and whose edges represent the acoustic likeli-
hoods of word hypotheses wts spanning a given time
interval (s, t), we can calculate the forward variable
?t(w), which represents the joint probability of all
word sequences ending in wts and the acoustic ob-
servations up to time t, as:
?t(w) = P (O
s
1, w
t
s) =
?
vts?prev(w)
P (w|v)P (wts)?s(v)
Here, P (w|v) is the bigram probability of (v, w)
obtained from the language model and P (wts) is the
acoustic likelihood of the word model w given the
observed speech from time s to t, as approximated
by the Viterbi path score.
Figure 4: Selecting replacement words
Likewise, we can compute the backward variable
?t(w), which represents the conditional probabil-
ity of all word sequences beginning in wts and the
acoustic observations from time t + 1 to the end of
the utterance, given wts:
?t(w) = P (O
T
t |w
t
s) =
?
vet?succ(w)
P (v|w)P (vet )?e(v)
The posterior probability P (wts|O
T
1 ) can then be
obtained by multiplication and normalization:
P (wts|O
T
1 ) =
P (wts, O
T
1 )
P (OT1 )
=
?t(w)?t(w)
P (OT1 )
This algorithm has a straightforward extension to
trigram language models which has been omitted
here for simplicity.
This interface is inspired by the web browser
zooming interface used on the Apple iPhone (Ap-
ple, Inc., 2008), as well as the Speech Dasher
lattice correction tool (Vertanen, 2004). We feel
that it is potentially useful not only for auto-
matic speech recognition, but also for machine
translation and any other situation in which
a lattice representation of a possibly errorful
hypothesis is available. A video of this in-
terface in Ogg Theora format1 can be viewed at
http://www.cs.cmu.edu/?dhuggins/touchcorrect.ogg.
1For Mac OS X: http://xiph.org/quicktime/download.html
For Windows: http://www.illiminable.com/ogg/downloads.html
18
3 Script Outline
For our demonstration, we will have available
a poster describing the interaction method being
demonstrated. We will begin by describing the mo-
tivation for this work, followed by a ?silent? demo
of the correction method itself, using pre-recorded
audio. We will then demonstrate live speech input
and correction using our own voices. The audience
will then be invited to test the interaction method on
a touchscreen device (either a handheld computer or
a tablet PC).
4 Requirements
To present this demo, we will be bringing two Nokia
Internet Tablets as well as a laptop and possibly a
Tablet PC. We have no requirements from the con-
ference organizers aside from a suitable number of
power outlets, a table, and a poster board.
Acknowledgements
We wish to thank Nokia for donating an N800 Inter-
net Tablet used to develop this software.
References
E. Karpov, I. Kiss, J. Leppa?nen, J. Olsen, D. Oria, S.
Sivadas and J. Tian 2006. Short Message Sys-
tem dictation on Series 60 mobile phones. Work-
shop on Speech in Mobile and Pervasive Environ-
ments (SiMPE) in Conjunction with MobileHCI 2006.
Helsinki, Finland.
Keith Vertanen 2004. Efficient Computer Interfaces
Using Continuous Gestures, Language Models, and
Speech. M.Phil Thesis, University of Cambridge,
Cambridge, UK.
Apple, Inc. 2008. iPhone: Zoom-
ing In to Enlarge Part of a Webpage.
http://docs.info.apple.com/article.html?artnum=305899
Roger K. Moore 2004. Modelling Data Entry Rates for
ASR and Alternative Input Methods. Proceedings of
Interspeech 2004. Jeju, Korea.
James R. Lewis 1999. Effect of Error Correction Strat-
egy on Speech Dictation Throughput Proceedings of
the Human Factors and Ergonomics Society 43rd An-
nual Meeting.
Frank Wessel, Klaus Macherey, Ralf Schlu?ter 1998. Us-
ing Word Probabilities as Confidence Measures. Pro-
ceedings of ICASSP 1998.
19
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 21?24,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mixture Pruning and Roughening for Scalable Acoustic Models
David Huggins-Daines
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dhuggins@cs.cmu.edu
Alexander I. Rudnicky
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
air@cs.cmu.edu
Abstract
In an automatic speech recognition system us-
ing a tied-mixture acoustic model, the main
cost in CPU time and memory lies not in
the evaluation and storage of Gaussians them-
selves but rather in evaluating the mixture
likelihoods for each state output distribution.
Using a simple entropy-based technique for
pruning the mixture weight distributions, we
can achieve a significant speedup in recogni-
tion for a 5000-word vocabulary with a negli-
gible increase in word error rate. This allows
us to achieve real-time connected-word dicta-
tion on an ARM-based mobile device.
1 Introduction
As transistors shrink and CPUs become faster and
more power-efficient, we find ourselves entering a
new age of intelligent mobile devices. We believe
that not only will these devices provide access to rich
sources of on-line information and entertainment,
but they themselves will find new applications as
personal knowledge management agents. Given the
constraints of the mobile form factor, natural speech
input is crucial to these applications. However, de-
spite the advances in processor technology, mobile
devices are still highly constrained by their memory
and storage subsystems.
2 Semi-Continuous Acoustic Models
Recent research into acoustic model compression
and optimization of acoustic scoring has focused
on ?Fully Continuous? acoustic models, where each
Hidden Markov Model state?s output probability dis-
tribution is modeled by a mixture of multivariate
Gaussian densities. This type of model allows large
amounts of training data to be efficiently exploited to
produce detailed models. However, due to the large
number of parameters in these models, approximate
computation techniques (Woszczyna, 1998) are re-
quired in order to achieve real-time recognition even
on workstation-class hardware.
Another historically popular type of acoustic
model is the so-called ?Semi-Continuous? or tied-
mixture model, where a single codebook of Gaus-
sians is shared by all HMM states (Huang, 1989).
The parameters of this codebook are updated using
the usual Baum-Welch equations during training, us-
ing sufficient statistics from all states. The mix-
ture weight distributions therefore become the main
source of information used to distinguish between
different speech sounds.
There are several benefits to semi-continuous
models for efficient speech recognition. The most
obvious is the greatly reduced number of Gaussian
densities which need to be computed. With fully
continuous models, we must compute one codebook
of 16 or more Gaussians for each HMM state, of
which there may be several thousand active for any
given frame of speech input. For semi-continuous
models, there is a single codebook with a small num-
ber of Gaussians, typically 256. In addition, since
only a few Gaussians will have non-negligible den-
sities for each frame of speech, and this set of Gaus-
sians tends to change slowly, partial computation of
each density is possible.
Another useful property of semi-continuous mod-
els is that the mixture weights for each state have
the form of a multinomial distribution, and are thus
amenable to various smoothing and adaptation tech-
niques. In particular, Bayesian and quasi-Bayes
21
adaptation (Huo and Chan, 1995) are effective and
computationally efficient.
3 Experimental Setup
All experiments in this paper were performed using
PocketSphinx (Huggins-Daines et al, 2006). The
baseline acoustic model was trained from the com-
bined WSJ0 and WSJ1 ?long? training sets (Paul and
Baker, 1992), for a total of 192 hours of speech.
This speech was converted to MFCC features us-
ing a bank of 20 mel-scale filters spaced from 0
to 4000Hz, allowing the model to work with au-
dio sampled at 8kHz, as is typical on mobile de-
vices. We used 5-state Hidden Markov Models
for all phones. Output distributions were modeled
by a codebook of 256 Gaussians, shared between
5000 tied states and 220 context-independent states.
Only the first pass of recognition (static lexicon tree
search) was performed.
Our test platform is the Nokia N800, a hand-
held Internet Tablet. It uses a Texas Instruments
OMAPTM 2420 processor, which combines an
ARM11 RISC core and a C55x DSP core on a single
chip. The RISC core is clocked at 400MHz while the
DSP is clocked at 220MHz. In these experiments,
we used the ARM core for all processing, although
we have also ported the MFCC extraction code to the
DSP. The decoder binaries, models and audio files
were stored on a high-speed SD flash card format-
ted with the ext3 journaling filesystem. Using the
standard bcb05cnp bigram language model, we
obtained a baseline word error rate of 9.46% on the
si_et_05 test set. The baseline performance of
this platform on the test set is 1.40 times real-time,
that is, for every second of speech, 1.40 seconds of
CPU time are required for recognition.
We used the oprofile utility1 on the Nokia
N800 to collect statistical profiling information for
a subset of the test corpus. The results are shown in
Table 1. We can see that three operations occupy the
vast majority of CPU time used in decoding: man-
aging the list of active HMM states, computing the
codebook of Gaussians, and computing mixture den-
sities.
The size of the files in the acoustic model is shown
in Table 2. The amount of CPU time required to
1http://oprofile.sourceforge.net/
Function %CPU
HMM evaluation 22.41
hmm vit eval 5st lr 13.36
hmm vit eval 5st lr mpx 3.71
Mixture Evaluation 21.66
get scores4 8b 14.94
fast logmath add 6.72
Lexicon Tree Search 19.89
last phone transition 5.25
prune nonroot chan 4.15
Active List Management 15.57
hmm sen active 13.75
compute sen active 1.19
Language Model Evaluation 7.80
find bg 2.55
ngram ng score 2.13
Gaussian Evaluation 5.87
eval cb 5.59
eval topn 0.28
Acoustic Feature Extraction 3.60
fe fft real 1.59
fixlog2 0.77
Table 1: CPU profiling, OMAP platform
File Size (bytes)
sendump (mixture weights) 5345920
mdef (triphone mappings) 1693280
means (Gaussians) 52304
variances (Gaussians) 52304
transition_matrices 5344
Table 2: File sizes, WSJ1 acoustic model
calculate densities is related to the size of the mix-
ture weight distribution by the fact that the N800
has a single-level 32Kb data cache, while a typical
desktop processor has two levels of cache totalling
at least 1Mb. We used cachegrind2 to simulate
the memory hierarchy of an OMAP versus an AMD
K8 desktop processor with 64Kb of L1 cache and
512Kb of L2 cache, with results shown in Table 3.
While other work on efficient recognition has fo-
cused on quantization of the Gaussian parameters
(Leppa?nen and Kiss, 2005), in a semi-continuous
model, the number of these parameters is small
2http://valgrind.org/
22
Function ARM K8
hmm vit eval 5st lr 4.71 3.95
hmm sen active 3.55 3.76
get scores4 8b 2.87 1.92
prune root chan 2.07 2.29
prune nonroot chan 1.99 1.73
eval cb 1.73 1.77
hmm vit eval 5st lr mpx 1.30 0.80
Table 3: Data cache misses (units of 107)
enough that little cost is incurred by storing and cal-
culating them as 32-bit fixed-point numbers. There-
fore, we focus here on ways to reduce the amount of
storage and computation used by the mixture weight
distributions.
4 Mixture Roughening
Our method for speeding up mixture computation is
based on the observation that mixture weight distri-
butions are typically fairly ?spiky?, with most of the
probability mass concentrated in a small number of
mixture weights. One can quantify this by calculat-
ing the perplexity of the mixture distributions:
pplx(wi) = exp
N?
k=0
wik log
1
wik
A histogram of perplexities is shown in Figure
1. The perplexity can be interpreted as the average
number of Gaussians which were used to generate
an observation drawn from a particular distribution.
Therefore, on average, the vast majority of the 256
Gaussians contribute minimally to the likelihood of
the data given a particular mixture model.
When evaluating mixture densities with pruned
models, one can either treat these mixture weights
as having a small but non-negligible value, or set
them to zero3. Note that the mixture weights are
renormalized in both cases, and thus the former is
more or less equivalent to add-one smoothing. The
latter can be thought of as exactly the opposite of
smoothing - ?roughening? the distribution. To in-
vestigate this, we set al but the top 16 values in each
mixture weight distribution to zero and ran a num-
ber of trials on a K8-based workstation, varing the
3Meaning a very small number, since they are stored in log
domain.
0 50 100 150 200Perplexity(w)0
200
400
600
800
1000
# o
f mi
xtur
e w
eigh
ts mode = 10
Figure 1: Perplexity distribution of mixture weights
3 4 5 6 7 8-log10(mixw_floor)0.00
0.05
0.10
0.15
0.20
0.25
0.30
Perf
orm
anc
e (x
RT)
5
10
15
20
25
30
Erro
r Ra
te (
%W
ER)
xRT (16 mixtures)xRT (baseline)WER (16 mixtures)WER (baseline)
Figure 2: Smoothing vs. Roughening, 16 mixtures
mixture weight floor to produce either a smoothing
or roughening effect. We discovered something ini-
tially surprising: ?roughening? the mixture weights
speeds up decoding significantly, while smoothing
them slows it down. A plot of speed and error rate
versus mixture weight floor is shown in Figure 2.
However, there is a simple explanation for this.
At each frame, only the top N Gaussian densities
are actually used to calculate the likelihood of the
data:
p(x|?i) =
?
k?topN
wikN(x; ~?ik, ~?
2
ik)
When we remove mixture weights, we increase
the probability that these top N densities will be
matched with pruned-out weights. If we smooth the
weights, we may raise some of these weights above
their maximum-likelihood estimate, thus increasing
23
3 4 5 6 7 8-log10(mixw_floor)0.05
0.10
0.15
0.20
0.25
0.30
Perf
orm
anc
e (x
RT)
0.10 xRT, 9.68 %WER
9.0
9.5
10.0
10.5
11.0
11.5
Erro
r Ra
te (
%W
ER)
xRT (64 mixtures)xRT (96 mixtures)xRT (baseline)WER (64 mixtures)WER (96 mixtures)WER (baseline)
Figure 3: Speed-accuracy tradeoff with pruned mixtures
the likelihood for models whose top mixture weights
do not overlap with the top N densities. This may
prevent HMM states whose output distributions are
modeled by said models from being pruned by beam
search, therefore slowing down the decoder. By
?roughening? the weights, we decrease the likeli-
hood of the data for these models, and hence make
them more likely to be pruned, speeding up the de-
coder and increasing the error rate. This is a kind
of ?soft? GMM selection, where instead of exclud-
ing some models, we simply make some more likely
and others less likely.
As we increase the number of retained mixture
weights, we can achieve an optimal tradeoff between
speed and accuracy, as shown in Figure 3. Addition-
ally, the perplexity calculation suggests a principled
way to vary the number of retained mixture weights
for each model. We propose setting a target number
of mixture weights, then calculating a scaling factor
based on the ratio of this target to the average per-
plexity of all models:
topKi =
target
1
N
?N
i=0 pplx(wi)
pplx(wi)
One problem is that many models have very low
perplexity, such that we end up retaining only a few
mixture weights. When the mixture weights are
?roughened?, this guarantees that these models will
score poorly, regardless of the data. We compensate
for this by keeping a minimum number of mixture
weights regardless of the perplexity. Using a tar-
get of 96 mixtures, a minimum of 16, and a mixture
weight floor of 10?8, we achieve 9.90% word error
rate in 0.09 times real-time, a 21% speedup with a
2.7% relative increase in error (baseline error rate is
9.64% on the desktop).
Using the same entropy-pruned mixture weights
on the N800, we achieve an error rate of 9.79%, run-
ning in 1.19 times real-time, a 15% speedup with a
3.4% relative increase in error. After applying ab-
solute pruning thresholds of 800 HMMs per frame
and 5 words per frame, we obtained a 10.01% word
error rate in 1.01 times real-time.
5 Conclusion
We have shown that a simple pruning technique al-
lows acoustic models trained for large-vocabulary
continuous speech recognition to be ?scaled down?
to run in real-time on a mobile device without major
increases in error. In related work, we are exper-
imenting with bottom-up clustering techniques on
the mixture weights to produce truly scalable acous-
tic models, and subvector clustering to derive semi-
continuous models automatically from well-trained
fully-continuous models.
Acknowledgments
We wish to thank Nokia for donating the N800 tablet
used in these experiments.
References
X. D. Huang. 1989. Semi-continuous Hidden Markov
Models for Speech Recognition. Ph.D. thesis, Univer-
sity of Edinburgh.
D. Huggins-Daines, M. Kumar, A. Chan, A. Black,
M. Ravishankar, and A. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recogni-
tion system for hand-held devices. In Proceedings of
ICASSP 2006, Toulouse, France.
Q. Huo and C. Chan. 1995. On-line Bayes adaptation of
SCHMM parameters for speech recognition. In Pro-
ceedings of ICASSP 1995, Detroit, USA.
J. Leppa?nen and I. Kiss. 2005. Comparison of low foot-
print acoustic modeling techniques for embedded ASR
systems. In Proceedings of Interspeech 2005, Lisbon,
Portugal.
D. Paul and J. Baker. 1992. The design for the Wall
Street Journal based CSR corpus. In Proceedings of
the ACL workshop on Speech and Natural Language.
M. Woszczyna. 1998. Fast Speaker Independent Large
Vocabulary Continuous Speech Recognition. Ph.D.
thesis, University of Karlsruhe.
24
