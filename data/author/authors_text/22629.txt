Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 62?72,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Overview for the First Shared Task on
Language Identification in Code-Switched Data
Thamar Solorio
Dept. of Computer Science
University of Houston
Houston, TX, 77004
solorio@cs.uh.edu
Elizabeth Blair, Suraj Maharjan, Steven Bethard
Dept. of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, AL, 35294
{eablair,suraj,bethard}@uab.edu
Mona Diab, Mahmoud Gohneim, Abdelati Hawwari, Fahad AlGhamdi
Dept. of Computer Science
George Washington University
Washington, DC 20052
{mtdiab,mghoneim,abhawwari,fghamdi}@gwu.edu
Julia Hirschberg and Alison Chang
Dept. of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
ayc2135@columbia.edu
Pascale Fung
Dept. of Electronic & Computer Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
pascale@ece.ust.hk
Abstract
We present an overview of the first shared
task on language identification on code-
switched data. The shared task in-
cluded code-switched data from four lan-
guage pairs: Modern Standard Arabic-
Dialectal Arabic (MSA-DA), Mandarin-
English (MAN-EN), Nepali-English (NEP-
EN), and Spanish-English (SPA-EN). A to-
tal of seven teams participated in the task
and submitted 42 system runs. The evalua-
tion showed that language identification at
the token level is more difficult when the
languages present are closely related, as in
the case of MSA-DA, where the prediction
performance was the lowest among all lan-
guage pairs. In contrast, the language pairs
with the higest F-measure where SPA-EN
and NEP-EN. The task made evident that
language identification in code-switched
data is still far from solved and warrants
further research.
1 Introduction
The main goal of this language identification shared
task is to increase awareness of the outstanding
challenges in the automated processing of Code-
Switched (CS) data and motivate more research in
this direction. We define CS broadly as a commu-
nication act, whether spoken or written, where two
or more languages are being used interchangeably.
In its spoken form, CS has probably been around
ever since different languages first came in contact.
Linguists have studied this phenomenon since the
mid 1900s. In contrast, the Natural Language Pro-
cessing (NLP) community has only recently started
to pay attention to CS, with the earliest work in
this area dating back to Joshi?s theoretical work
proposing an approach to parsing CS data (Joshi,
1982) based on the Matrix and Embedded language
framework. With the wide-spread use of social me-
dia, CS is now being used more and more in written
language and thus we are seeing an increase in pub-
lished papers dealing with CS. We are specifically
interested in intrasentential code switched phenom-
ena. As a result of this task, we have successfully
created the first set of annotated data for several
language pairs with a coherent set of labels across
the languages. As the shared task results show,
CS poses new research questions that warrant new
NLP approaches, and thus we expect to see a sig-
nificant increase in NLP work in the coming years
addressing CS phenomena in data.
The shared task covers four language pairs and
is focused on social media data. We provided par-
ticipants with annotated data from Twitter for the
62
language pairs: Modern Standard Arabic-Arabic
dialects (MSA-DA), Mandarin-English (MAN-
EN), NEP-EN (NEP-EN), and SPA-EN (SPA-EN).
These language pairs represent a good variety in
terms of language typology and relatedness among
pairs. They also cover languages with different rep-
resentation in terms of number of speakers world
wide. Participants were asked to make predictions
on unseen Twitter data for each language pair. We
also provided participants with test data from a
?surprise genre? with the objective of assessing the
robustness of language identification systems to
genre variation.
2 Task Description
The task consists of labeling each token/word in
the input file with one of six labels: lang1, lang2,
other, ambiguous, mixed, and named entities NE.
The lang1, lang2 labels refer to the two languages
addressed in the subtask, for example for the lan-
guage pair MSA-DA, lang1 would be an MSA and
lang2 is DA. The other category is a label used to
tag all punctuation marks, emoticons, numbers, and
similar tokens that do not represent actual words in
any of the given languages. The ambiguous label
is for instances where it is not possible to assign
a language with certainty, for example, a lexical
form that belongs to both languages, appearing in a
context that does not indicate one language over the
other. The mixed category is for words composed
of CS morphemes, such as the word snapchateando
?to chat? from SPA-EN, the word overai from NEP-
EN, or the word hayqwlwn
1
?they will say?, from
MSA-DA, where the ?ha? is a DA future morpheme
and the stem ?yqwlwn? is MSA.The NE label is
included in this task in an effort to allow for a more
focused analysis of CS data with the exclusion of
proper nouns. NEs have a very different behavior
than most other words in a language vocabulary
and thus from our perspective they need to be iden-
tified to be handled properly.
Table 1 shows Twitter examples taken from the
training data. The annotation guidelines are posted
on the workshop website
2
. We post the ones used
for SPA-EN as for the other language pairs the only
differences are the examples provided.
1
We use Buckwalter transliteration scheme http://
www.qamus.org/transliteration.htm
2
http://emnlp2014.org/workshops/
CodeSwitch/call.html
Language Pair Example
MSA-DA AlnhArdp AlsAEp 11 hAkwn Dyf >.
HAfZ AlmyrAzy ElY qnAp drym llHdyv
En >wlwyAt Alvwrp fy AlmrHlp Al-
HAlyp wqDyp tSHyH msAr Alvwrp
Al<ElAmy
(Today O?Clock 11 I will be
[a ]guest[ of] Mr. Hafez
AlMirazi on Channel Dream
to talk about [the ]priorities[ of]
the revolution in the stage the current
and [the ]issue[ of] correcting
[the ]path[ of] the revolution Media)
NEP-EN My car at the workshop for a much
needed repairs... ABA pocket khali
hune bho
(My car at the workshop for a much
needed repairs. . . now my pocket will
be empty)
SPA-EN Por primera vez veo a @username ac-
tually being hateful! it was beautiful:)
(For the first time I get to see @user-
name actually being hateful! it was
beautiful:)
Table 1: Examples of Twitter data used in the
shared task.
3 Related Work
In the past, most language identification research
has been done at the document level. Some re-
searchers, however, have developed methods to
identify languages within multilingual documents
(Singh and Gorla, 2007; Nguyen and Do?gru?oz,
2013; King and Abney, 2013). Their test data
comes from a variety of sources, including web
pages, bilingual forum posts, and jumbled data
from monolingual sources, but none of them are
trained on code-switched data, opting instead for a
monolingual training set per language. This could
prove to be a problem when working on code-
switched data, particularly in shorter samples such
as social media data, as the code-switching context
is not present in training material.
One system tackled both the problems of code-
switching and social media in language and code-
switched status identification (Lignos and Marcus,
2013). Lignos and Marcus gathered millions of
monolingual tweets in both English and Spanish in
order to model the two languages, and used crowd-
sourcing to annotate tens of thousands of Span-
ish tweets, approximately 11% of which contained
code-switched content. This system was able to
achieve 96.9% word-level accuracy and a 0.936
F-measure in identifying code-switched tweets.
The issue still stands that relatively little code-
switching data, such as that used in Lignos and
63
Marcus? research, is readily available. Even in
their data, the percentage of code-switched tweets
was barely over a tenth of the total test data. There
have been other corpora built, particularly for other
language pairs such as Mandarin-English (Li et
al., 2012; Lyu et al., 2010), but the amount of data
available and the percentage of code-switching data
within that data are not up to the standards of other
areas of the natural language processing field. With
this in mind, we sought to provide corpora for mul-
tiple language pairs, each with a better distribution
of code-switching phenomena.
4 Data Sets
Most of the data for the shared task comes form
Twitter. However, we also collected and annotated
data from other social media sources, including
Facebook, web forums, and blogs. These additional
sources of data were used as the surprise data. In
this section we describe briefly the corpora curated
for the shared task.
Language-pair Training Test Surprise
MAN-EN 1000 313 n/a
MSA-DA 5,838 2332, 1,777 12,017
NEP-EN 9,993 3,018 (2,874) 1,087
SPA-EN 11,400 3,060 (1,626) 1,102
Table 2: Statistics of the shared task data sets
per language pairs. The numbers are according to
what was actually annotated, numbers in parenthe-
sis show what the participating systems were able
to crawl from Twitter. The Surprise genre comes
from various sources, other than Twitter.
Table 2 shows some statistics about the differ-
ent datasets used in this task. We strive to provide
dataset sizes that would allow a robust analysis of
results. However, an unexpected challenge was
the rate at which tweets became unavailable. Dif-
ferent language pairs had different attrition rates
with SPA-EN being the most affected language and
MSA-DA and NEP-EN the least affected. Note
that we provided two test datasets for MSA-DA.
Since we separated the data on a per user basis, the
first test set had a highly skewed distribution. The
second test set was distributed to participants to
allow a comparison with a data set having a class
distribution more similar to the training set.
4.1 SPA-EN data
Developing the corpus involved two primary steps:
locating code-switching tweets and using crowd-
sourcing to annotate their tokens with language
tags. A small portion of the tweets were annotated
in-lab and this was used as the gold data for quality
control in the crowdsourcing annotation.
To avoid biasing the data used in this task, we
used a two step process to select the tweets: first we
identified CS tweets by doing a keyword search on
Twitter?s API. We selected a few frequently used
English words and restricted the search to tweets
identified by Twitter as Spanish from users in Cali-
fornia and Texas. An additional set of tweets was
then collected by using frequent Spanish words in
an all English tweet, from users in the same loca-
tions. We filtered these tweets to remove tweets
containing URLs, duplicates, spam tweets and
retweets.
In-lab annotators labeled the filtered tweets using
the guidelines referenced above. From this set of
labeled data we then ranked the users in this set by
the percentage of CS tweets. We selected the 12
most prolific CS users and then pulled all of their
available tweets. These 12 users contributed the
tweets used in the shared task. The tweets were
labeled using CrowdFlower
3
. After analyzing the
number and content distribution of the tweets, the
SPA-EN data was split into a 11,400 tweet training
set and a 3,014 tweet test set.
The SPA-EN Surprise Genre (SPA-EN-SG) in-
cluded Facebook comments from the Veteranas
community
4
and the Chicanas community
5
and
blog data from the Albino Bean
6
. Data was col-
lected using Python scripts that implemented the
Beautiful Soup library and the third-party Python
Facebook SDK (for Blogger and Facebook respec-
tively). Post and comment IDs were used to iden-
tify Facebook posts, and URLs were used to iden-
tify Blogger posts. The collected posts were format-
ted to match those collected from Twitter. In-lab
annotators were used to annotate approximately 1K
tokens. All the data we collected in this manner
was released as surprise data to all participants.
4.2 NEP-EN data
The collection of NEP-EN data followed a simi-
lar approach to that of SPA-EN. We first focused
on finding users that switched frequently between
3
http://www.crowdflower.com/
4
https://www.facebook.com/
VeteranaPinup
5
https://www.facebook.com/pages/
Chicanas/444483772293893
6
http://thealbinobean.blogspot.com/
64
Nepali and English. In addition, the users must
not be using Devnagari script as done by Nepalese
to write Nepali, but must have used its Roman-
ized form. We started by manually reading tweets
from some of our Nepali friends. We then crawled
their followers who corresponded with them using
code-switched tweets or replies. We found that
a lot of these users were regular code-switchers
themselves. We repeated the same process with the
followers and collected nearly 30 such users. We
then collected about 2,000 tweets each from these
users using the Twitter API. We filtered out all the
retweets and the tweets with URLs, following the
same process that was used for SPA-EN.
For the surprise test data, we crawled code-
switched data from Facebook comments and posts.
We found that most Nepalese comments had a rich
amount of code-switched data. However, we could
not crawl their data because of privacy issues. Nev-
ertheless, we could crawl data from public Face-
book pages. We identified some public Nepali Face-
book pages where anyone could comment. These
pages include FM, news and public figures? public
Facebook pages. We crawled the latest 10 feeds
from these public pages using the Facebook API
and gathered about 12,000 comments and posts for
the shared task.
Initially, we sought out help from Nepali gradu-
ate students at the University of Alabama at Birm-
ingham to annotate 100 tweets (1739 tokens). We
gave the same annotation file to two annotators to
do the annotation. We found that they agreed with
an accuracy of 95.34%. These tweets were then re-
viewed and used as initial gold data in Crowdflower
to annotate the first 1000 tweets. The annotation
job was enabled only in Nepal and Bhutan. We
disabled India, even though people living in some
regions of India (Darjeeling, Sikkim) also speak
and write in Nepali, as most spammers were com-
ing from India. We then ran two batches of 5000
tweets and one batch of 3000 tweets along with the
initial 1,000 tweets as the gold data. This NEP-EN
data was then split into a 9,993 tweet training set
and a 2,874 tweet test set. No Twitter user appeared
in both sets.
4.3 MAN-EN data
The MAN-EN tweets were collected from Twitter
with the Twitter API. Users were selected from
lists of most followed Twitter accounts in Taiwan
(where Mandarin Chinese is the official language).
These users? tweets were checked for Mandarin En-
glish bilingualism and added to our data collection
if they contained both languages.
The next round of usernames came from the
lists of users that our original top accounts were
following. The tweets written by this new set of
users were then examined for Mandarin English
code switching and stored as data if they matched
the criteria.
The jieba tokenizer
7
was used to segment the
Mandarin sections of the tweets and compute off-
sets of each segment. We format the code switch-
ing tweets into columns including language type,
labels, and offsets. Named entities were labeled
manually by a single annotator.
The data was split by user into 1000 tweets for
training and 313 for testing. No MAN-EN surprise
data for the current shared task.
4.4 MSA-DA data
For the MSA-DA language pair, we selected Egyp-
tian Arabic (EGY) as the Arabic dialect. We har-
vested data from two social media sources: Twitter
[TWT] and Blog commentaries [COM]. The TWT
data served as the main gold standard data for the
task where we provided fully annotated data for
Training/Tuning and Test. We provided two TWT
data sets for the test data that exemplified different
tag distributions. The COM data set comprised
only test data and it served as the Arabic surprise
data set.
To reduce the potential of TWT data attrition
from users deleting their accounts or tweets, we
selected tweets that are less prone to deletion and/or
change. Thereby we harvested tweets by a select
set of Egyptian Public Figures. The percentage
of deleted tweets and deactivated accounts among
those users is significantly lower if we compare it
to the tweets crawled from random Egyptian users.
We used the ?Tweepy? library to crawl the time-
lines of 12 Public Figures. Similar to other lan-
guage pairs, we excluded all re-tweets, tweets with
URLs, tweets mentioning other users, and tweets
containing Latin characters. We accepted 9,947
tweets, for each we extracted the tweet-id and user-
id. Using these IDs, we retrieved the tweets text,
tokenized it and assigned character offsets. To guar-
antee consistency and avoid any misalignment is-
sues, we compiled the full pipeline into the ?Arabic
Tweets Token Assigner? package which is made
7
https://github.com/fxsjy/jieba
65
available through the workshop website
8
.
For COM, we selected 6723 commentaries (half
MSA and half DA) from ?youm7?
9
commen-
taries provided by the Arabic Online Commentary
Dataset (Zaidan and Callison-Burch, 2011). The
COM data set was processed (12017 total tokens)
using the same pipeline created for the task. We
also provided the participants with the data format-
ted with character offsets to maintain consistency
across data sets in the Arabic subtask.
The annotation of MSA-DA language pair data
is based on two sets of guidelines. The first set
is a generic set of guidelines for code switching
in general across different language pairs. These
guidelines provide the overarching framework for
annotating code switched data on the morpholog-
ical, lexical, syntactic, and pragmatic levels. The
second set of guidelines is language pair specific.
We created the guidelines for the Arabic language
specifically. We enlisted the help of 3 annotators
in addition to a super annotator, hence resulting
in 4 annotators overall for the whole collection of
the data. All the annotators are native speakers
of Egyptian Arabic with excellent proficiency in
MSA. The super annotator only annotated 10% of
the overall data and served as the adjudicator. The
annotation process was iterative with several repe-
titions of the cycle of training, annotation, revision,
adjudication until we approached a stable Inter An-
notator Agreement (IAA) of over 90% pairwise
agreement.
5 Survey of Shared Task Systems
We received submissions from seven different
teams. Each participating system had the freedom
to submit responses to any of the language pairs
covered in the shared task. All seven participants
submitted system responses for SPA-EN, making
this language pair the most popular in this shared
task and MAN-EN the least popular.
All but one participating system used a machine
learning algorithm or language models, or even a
combination of both, as part of their configuration.
A couple of the participating systems used hand-
crafted rules of some sort, either at the intermediate
steps or as the final post-processing step. We also
observed a good number of systems using exter-
nal resources, in the form of labeled monolingual
8
http://emnlp2014.org/workshops/
CodeSwitch/call.html
9
An Egyptian newspaper, www.youm7.com
corpora, language specific gazetteers, off the shelf
tools (NE recognizers, language id systems, or mor-
phological analyzers) and even unsupervised data
crawled from the same users present in the data
sets provided. Affixes were also used in some form
by different systems.
The architecture of the different systems ranged
from a simple approach based on frequencies of
character n-grams combined in a rule-based system,
to more complex approaches using word embed-
dings, extended Markov Models, and CRF autoen-
coders. The majority of the systems that partici-
pated in more than one language pair did little to no
customization to account for the morphological dif-
ferences of the specific language pairs beyond lan-
guage specific parameter-tuning, which probably
reflects participants? goal to develop a multilingual
id system.
Due to the presence of the NE label, several
systems included a component for NE recognition
where there was one available for the specific lan-
guage. In addition, many systems also included
case information. One unexpected finding from
the shared task was that no participating system
tried to embed in their models some form of lin-
guistic theory or framework about CS. Only one
system made an explicit reference to CS theories
(Chittaranjan et al., 2014) in their motivation to use
contextual information, which can be considered
as a loose embedding of CS theory. While system
performance was competitive (see next section),
there is still room for improvement and perhaps
some of that improvement can come out of adding
this kind of knowledge into the models. Lastly, we
were surprised to see that not all systems made use
of character encoding information, even though for
Mandarin-English that would have been a strong
indicator. In Table 3 we present a summary high-
lighting some of the design choices of participating
systems.
6 Results
We used the following evaluation metrics: Accu-
racy, Precision, Recall, and F-measure. We use
F-measure to provide a ranking of systems. In the
evaluation at the tweet level we use the standard
f-measure. For the evaluation at the token level
we use instead the average weighted f-measure to
account for the highly imbalanced distribution of
classes.
To provide a fair evaluation, we only scored pre-
66
System
Machine
Learning
Rules Case
Character
Encoding
External Resources LM Affixes Context
(Chittaranjan et al., 2014)
CRF
4
4 dbpedia dumps, online sources
? 3
(Shrestha, 2014) 4
4 spell checker
(Jain and Bhat, 2014)
CRF
4
4 English dictionary
4 4 ? 2
(King et al., 2014)
eMM
ANERgazet, TwitterNLP, Stan-
ford NER
4 4 4
(Bar and Dershowitz, 2014)
SVM
4
Illocution Twitter Lexicon,
monolingual corpora (NE lists)
4 4 ? 2
(Lin et al., 2014)
CRF
4
4
Hindi-Nepali Wikipedia, JRC,
CoNLL 2003 shared task, lang
id predictors: cld2 and ldig
4 4
(Barman et al., 2014)
kNN, SVM
4 4
BNC, LexNorm
4 ? 1
Table 3: Comparison of shared task participating system algorithm choices. CRF stands for Conditional
Random Fields, SVM for Support Vector Machines and LM for Language Models.
dictions on tweets submitted by all teams. All
systems were compared to a simple lexicon-based
baseline. The lexicon was gathered from the train-
ing data for classes lang1 and lang2 only. Emoti-
cons, punctuation marks, usernames and URLs are
by default tagged as other. In the case of a tie or a
new token, the baseline system assigns the majority
class for that language pair.
Figure 1 shows prediction performance on the
Twitter test data for each language pair at the tweet
level. The system predictions for this task are taken
directly from the individual token predictions in
the following manner: if the system predictions for
the same tweet contain at least one tag from each
language (lang1 and lang2), the tweet is labeled
as code-switched, otherwise it is labeled as mono-
lingual. As illustrated, each language pair shows
different patterns. Comparing the systems that par-
ticipated in all language pairs, there is no clear
winner across the board. However, (Chittaranjan et
al., 2014) was in the top three places in at least one
test file for each language pair. Table 4 shows the
results at the token level by label. Here again the
figures show F-measure per class label and the last
column is the weighted average f-measure (Avg-F).
One of the few general trends on these results is
that most participating systems were not able to
correctly identify the minority classes ?ambiguous?
and ?other?. There are only few instances of these
labels in the training set and some test sets did not
have one of these classes present. The impact on
final system performance from these classes is not
significant. However, to study CS patterns we will
need to have these labels identified properly.
The MAN-EN pair received four system re-
sponses and all four of them reached an F-measure
>80% and outperformed the simple baseline by a
considerable margin. We expected this language
pair to be the easiest one for the shared task since
each language uses a different encoding script. A
very rough but accurate distinction between Man-
darin and English could be achieved by looking
at the character encoding. However, according to
the system descriptions provided, not all systems
used encoding information. The best performing
systems for MAN-EN are (King et al., 2014) and
(Chittaranjan et al., 2014). The former slightly
outperformed the latter at the Tweet level (see Fig-
ure 1a) task while the opposite was true at the token
level (see Table 4 rows 4 and 5).
In the case of SPA-EN, all seven systems out-
performed the simple baseline. The best perform-
ing system in all SPA-EN tasks was (Bar and
Dershowitz, 2014). This system achieved an F-
measure of 82.2%, 2.9 percentage points above the
second best system (Lin et al., 2014) on the tweet
level task (see Figure 1(d)). In the token level
evaluation, (Bar and Dershowitz, 2014) reached
an Avg. F-measure of 94%. This top performing
system uses a sequential classification approach
where the labels from the preceding words are used
as features in the model. Another design choice
that might have given the edge to this system is the
fact that their model combines character- and word-
based language models in what the authors call
?intra- and inter-word level? features. Both types
of language models are trained on large amounts
of monolingual data and NE lists, which again pro-
vides additional knowledge that other systems are
not exploiting. For instance, the NE lexicons might
account for the best results in the NE class in both
the Twitter data and the Surprise genre (see Table 4
last row for SPA-EN and second to last for SPA-
EN Surprise). Most systems showed considerable
67
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Chittaranjan
et al., 2014)
(King et
al., 2014)
0.6
0.7
0.8
0.9
1
0.838
0.888
0.892
0.894
F
-
m
e
a
s
u
r
e
Baseline
(a) MAN-EN
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Elfardy et
al., 2014)
(Lin et
al., 2014)
0.1
0.2
0.3
0.4
0.152
0.118
0.048
0.044
0.095
0.196
0.260
0.338
0.360
0.417
F
-
m
e
a
s
u
r
e
Baseline Test1
Baseline Test2
(b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2
(King et
al., 2014)
(Lin et
al., 2014)
(Jain and
Bhat, 2014)
(Shrestha,
2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
0.8
0.9
1
0.952
0.962
0.972
0.974
0.975
0.977
F
-
m
e
a
s
u
r
e
Baseline
(c) NEP-EN
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.6
0.7
0.8
0.9
1
0.634
0.703
0.753
0.754
0.783
0.793
0.822
F
-
m
e
a
s
u
r
e
Baseline
(d) SPA-EN
Figure 1: Prediction results on language identification at the tweet level. This is a binary task to distinguish
between a monolingual and a CS tweet. We show performance of participating systems using F-measure
as the evaluation metric. The solid line shows the lexicon baseline performance.
differences in prediction performance in both gen-
res. In all cases the Avg. F-measure was higher
on the Twitter test data than on the surprise genre.
Although the surprise genre is too small to draw
strong conclusions, all language pairs with surprise
genre test data showed a decrease in performance
of around 10%.
We analyzed system outputs and found some
consistent sources of error. Lexical forms that exist
in both languages were frequently mislabeled by
68
most systems. For example the word for ?he? was
frequently mislabeled by at least one system. In
most of the cases systems were predicting EN as
label when the target language was SPA. Cases like
this were even more prone to errors when these
words fell in the CS point, as in this tweet: ni el
header he hecho (I haven?t even done the header).
Tweets like this one, with just one token from the
other language, were difficult for most systems.
Named entities were also frequent sources of error,
especially when they were spelled with lower cases
letters.
By far the hardest language pair in this shared
task was MSA-DA, as anticipated. Especially when
considering the typological similarities between
MSA and DA. This is mainly due to the fact that
DA and MSA are close variants of one another and
hence they share considerable amount of lexical
items. The shared lexical items could be simple
cognates of one another, or faux amis where they
are homographs or homophones, but have com-
pletely different meaning. Both categories con-
stitute a significant challenge. Accordingly, the
baseline system had the lowest performance from
all language pairs in both test sets. We note chal-
lenges in this language pair on each linguistic level
where CS occurs especially for the shared lexical
items.
On the phonological level, DA writers tend to
mimic the MSA script for DA words even if they
are pronounced differently. For example: ?heart? is
pronounced in DA Alob and in MSA as qalob but
commonly written in MSA as ?qalob? in DA data.
Also many phonological differences are in short
vowels that are underspecified in written Arabic,
adding another layer of ambiguity.
On the morphological level, there is no avail-
able morphological analyzer able to recognize such
shared words and hence they are mostly misclassi-
fied. Language identification for MSA-DA CS text
highly depends on the context. Typically some Ara-
bic variety word serves as a marker for a context
switch such as mElh$ for DA or mn? for MSA. But
if shared lexical items are used, it is challenging
to identify the Arabic variant. An example from
the training data is qlb meaning either heart as a
noun or change as a verb in the phrase lw qlb mjrm,
corresponding to ?If the heart of a criminal? or ?if
he changes into a criminal?. These challenges ren-
der language identification for CS MSA-DA data
far from solved as evident by the fact that the high-
est scoring system reached an F-measure of only
41.7% in Test2 for CS identification. Moreover,
this is the only language pair where at least one
system was not able to outperform the baseline and
in the case of Test2 only one system (Lin et al.,
2014) outperformed the baseline.
Most teams did well for the NEP-EN shared task,
and all teams outperformed the baseline. The rea-
son for the high performance might be the high
number of codeswitched tweets in the training and
test data for NEP-EN (much higher than other lan-
guage pairs). This allowed systems to have more
samples of CS instances. The other reason for good
performance by most participants in both evalua-
tions might be that Nepali and English are two very
different languages. The structure of the words and
syntax of word formation are very different. We
suspect, for instance, that there is a much lower
overlap of character n-grams in this language pair
than in SPA-EN, which makes for an easier task. At
the Tweet level, system performance ranged over
a small set of values, the lowest F-measure was
95.2% while the highest was 97.7%. Looking at
the numbers in Table 4, we can see that even NE
recognition seemed to be a much easier task for this
language pair than for SPA-EN (compare results
for the NE category in both SPA-EN sets to those
of both NEP-EN data sets). The best performing
system for the Twitter test data is (Barman et al.,
2014) with an F-measure of 97.7%. The results
trend in the surprise genre is not consistent with
what we observed for the Twitter test data. The
top ranked system for Twitter sunk to the 4th place
with an F-measure or 59.6%, a considerable drop
of almost 40 percentage points. In this case, the
overall numbers indicate a much wider difference
in the genres than what we observed for other lan-
guages, such as SPA-EN, for example. We should
note that the class distribution in the surprise data is
considerably different from what the models used
for training, and from that of the test data as well.
In the Twitter data there was a larger number of CS
tweets than monolingual ones, while in the surprise
genre the majority class was monolingual. This
will account for a good portion of the differences
in performance. But here as well, the small number
of labeled instances makes it hard to draw strong
conclusions.
69
Test Set System lang1 lang2 NE other ambiguous mixed Avg-F
MAN-EN
Baseline 0.9 0.47 0 0.29 - 0 0.761
(Jain and Bhat, 2014) 0.97 0.66 0.52 0.33 - 0 0.871
(Lin et al., 2014) 0.98 0.73 0.62 0.34 - 0 0.886
(King et al., 2014) 0.98 0.74 0.58 0.30 - 0 0.884
(Chittaranjan et al., 2014) 0.98 0.76 0.66 0.34 - 0 0.892
MSA-DA Test 1
(King et al., 2014) 0.88 0.14 0.05 0 0 - 0.720
Baseline 0.92 0.06 0 0.89 0 - 0.819
(Chittaranjan et al., 2014) 0.94 0.15 0.57 0.91 0 - 0.898
(Jain and Bhat, 2014) 0.93 0.05 0.73 0.87 0 - 0.909
(Lin et al., 2014) 0.94 0.09 0.74 0.98 0 - 0.922
(Elfardy et al., 2014)* 0.94 0.05 0.85 0.99 0 - 0.936
MSA-DA Test 2
Baseline 0.54 0.27 0 0.94 0 0 0.385
(King et al., 2014) 0.59 0.59 0.13 0.01 0 0 0.477
(Chittaranjan et al., 2014) 0.58 0.50 0.42 0.43 0.01 0 0.513
(Jain and Bhat, 2014) 0.62 0.49 0.67 0.75 0 0 0.580
(Elfardy et al., 2014)* 0.73 0.73 0.91 0.98 0 0.01 0.777
(Lin et al., 2014) 0.76 0.81 0.73 0.98 0 0 0.799
MSA-DA Surprise
(King et al., 2014) 0.48 0.60 0.05 0.02 0 0 0.467
(Jain and Bhat, 2014) 0.53 0.61 0.62 0.96 0 0 0.626
(Chittaranjan et al., 2014) 0.56 0.69 0.33 0.96 0 0 0.654
(Lin et al., 2014) 0.68 0.82 0.61 0.97 0 0 0.778
(Elfardy et al., 2014)* 0.66 0.81 0.87 0.99 0 0 0.801
NEP-EN
Baseline 0.67 0.76 0 0.61 - 0 0.678
(King et al., 2014) 0.87 0.80 0.51 0.34 - 0.03 0.707
(Lin et al., 2014) 0.93 0.91 0.49 0.95 - 0.02 0.917
(Jain and Bhat, 2014) 0.94 0.96 0.52 0.94 - 0 0.942
(Shrestha, 2014) 0.94 0.96 0.57 0.95 - 0 0.944
(Chittaranjan et al., 2014) 0.94 0.96 0.45 0.97 - 0 0.948
(Barman et al., 2014) 0.96 0.97 0.58 0.97 - 0.06 0.959
NEP-EN Surprise
(Lin et al., 2014) 0.83 0.73 0.46 0.65 - - 0.712
(King et al., 2014) 0.82 0.88 0.43 0.12 - - 0.761
(Chittaranjan et al., 2014) 0.78 0.87 0.37 0.80 - - 0.796
(Jain and Bhat, 2014) 0.83 0.91 0.50 0.87 - - 0.850
(Barman et al., 2014) 0.87 0.90 0.61 0.74 - - 0.853
(Shrestha, 2014) 0.85 0.92 0.53 0.78 - - 0.855
SPA-EN
Baseline 0.72 0.56 0 0.75 0 0 0.704
(Shrestha, 2014) 0.88 0.85 0.35 0.92 0 0 0.873
(Jain and Bhat, 2014) 0.92 0.92 0.36 0.90 0 0 0.905
(Lin et al., 2014) 0.93 0.93 0.32 0.91 0.03 0 0.913
(Barman et al., 2014) 0.93 0.92 0.47 0.93 0.03 0 0.921
(King et al., 2014) 0.94 0.93 0.54 0.92 0 0 0.923
(Chittaranjan et al., 2014) 0.94 0.93 0.28 0.95 0 0 0.926
(Bar and Dershowitz, 2014) 0.95 0.95 0.56 0.94 0.04 0 0.940
SPA-EN Surprise
(Shrestha, 2014) 0.80 0.78 0.23 0.81 0 0 0.778
(Jain and Bhat, 2014) 0.83 0.84 0.22 0.79 0 0 0.811
(Lin et al., 2014) 0.83 0.86 0.19 0.80 0.03 0 0.816
(Barman et al., 2014) 0.84 0.85 0.31 0.82 0.03 0 0.823
(Chittaranjan et al., 2014) 0.94 0.86 0.14 0.83 0 0 0.824
(King et al., 2014) 0.84 0.85 0.35 0.81 0 0 0.828
(Bar and Dershowitz, 2014) 0.85 0.87 0.37 0.83 0.03 0 0.839
Table 4: Performance results on language identification at the token level. A ?-? indicates there were no
tokens of this class in the test set. We ranked systems using weighted averaged f-measure (Avg-F). The ?*?
marks the system by (Elfardy et al., 2014). This system was not considered in the ranking for the shared
task as it was developed by co-organizers of the task.
7 Lessons Learned
Among the things we want to improve for future
shared tasks is the issue of data loss due to re-
moval of tweets or users deleting their accounts.
We decided to use Twitter data to have a relevant
corpus. However, the trade-off is the lack of rights
to distribute the data ourselves. This is not just a
burden for the participants. It is an awful waste of
resources as the data that was expensive to gather
and label is not being used beyond the small group
of researchers involved in the creation of the cor-
pus. This will deter us from using Twitter data for
future shared tasks, at least until a better solution
is identified.
70
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Elfardy et
al., 2014)
0
0.1
0.2
0.3
0.170
0.194
0.222
0.276
0.277
F
-
m
e
a
s
u
r
e
(a) MSA-DA Surprise Genre Results
(Chittaranjan
et al., 2014)
(Jain and
Bhat, 2014)
(Barman et
al., 2014)
(King et
al., 2014)
(Shrestha,
2014)
(Lin et
al., 2014)
0.4
0.5
0.6
0.7
0.554
0.571
0.596
0.604
0.632
0.702
F
-
m
e
a
s
u
r
e
(b) NEP-EN Surprise Genre Results
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.4
0.5
0.6
0.7
0.8
0.633
0.640
0.704
0.710
0.725
0.727
0.753
F
-
m
e
a
s
u
r
e
(c) SPA-EN Surprise Genre Results
Figure 2: Prediction results on language identification at the document level for the surprise genre. This
is a binary task to distinguish between a monolingual and a code-switched text. We show performance of
participating systems using F-measure as the evaluation metric.
Using crowdsourcing for annotating the data is a
cheap and easy way for generating resources. But
we found out that even when following best prac-
tices for quality control, there was a substantial
amount of noise in the gold data. We plan to con-
tinue working on refining the annotation guidelines
and quality control processes to reduce the amount
of noise in gold annotations.
8 Conclusion
This is the first shared task on language identifica-
tion in CS data. Yet, the response was quite positive
as we received 42 system runs from seven different
teams, plus submissions for MSA-AD from a sub
group of the task organizers (Elfardy et al., 2014).
The systems presented are overall robust and with
interesting differences from one another. Although
we did not see a single system ranking in the top
places across all language pairs and tasks, we did
see systems showing robust performance indicat-
ing some level of language independence. But the
results are not consistent at the tweet/document
level. The language pair that proved to be the most
difficult for the task was MSA-DA, where the lexi-
con baseline system was hard to beat even with an
F-measure of 47.1%.
This shared task showed that language identifica-
tion in code-switched data is still an open problem
that warrants further investigation. Perhaps in the
near future we will see systems that embed some
form of linguistic theory about CS and maybe that
would result in more accurate predictions.
Our goal is to support new research addressing
CS data. Discussions about the challenge for the
next shared task are already underway. One pos-
sibility might be parsing. We plan to investigate
the challenges in parsing CS data and we will start
by exploring the hardships in manually annotating
CS with syntactic information. We would also like
to explore the possibility of classifying CS points
according to their socio-pragmatic role.
71
Acknowledgments
We would like to thank all shared task partici-
pants. We also thank Brian Hester and Mohamed
Elbadrashiny for their invaluable support in the
development of the gold standard data and analy-
sis of results. We also thank the in-lab annotators
and the CrowdFlower contributors. This work was
partly funded by NSF under awards 1205475 and
1205556.
References
Kfir Bar and Nachum Dershowitz. 2014. Tel Aviv Uni-
versity system description for the code-switching
workshop shared task. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Utsab Barman, Joachim Wagner, Grzegorz Chrupala,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. A framework to label
code-mixed sentences in social media. In Proceed-
ings of the First Workshop on Computational Ap-
proaches to Code-Switching, Doha, Qatar, October.
ACL.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying code switching in
informal Arabic text. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Naman Jain and Riyaz Ahmad Bhat. 2014. Language
identification in codeswitching scenario. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
A. Joshi. 1982. Processing of sentences with in-
trasentential code-switching. In J?an Horeck?y, editor,
COLING-82, pages 145?150, Prague, July.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Levi King, Eric Baucom, Tim Gilmanov, Sandra
K?ubler, Dan Whyatt, Wolfgang Maier, and Paul Ro-
drigues. 2014. The IUCL+ system: Word-level
language identification via extended Markov models.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
Mandarin-English code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL An-
thology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. Toward
web-scale analysis of codeswitching. In 87th An-
nual Meeting of the Linguistic Society of America.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2014. The CMU submission for the shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching, Doha,
Qatar, October. ACL.
D.C. Lyu, T.P. Tan, E. Chng, and H. Li. 2010. SEAME:
a Mandarin-English code-switching speech corpus
in South-East Asia. In INTERSPEECH, volume 10,
pages 1986?1989.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, pages 857?862, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Prajwol Shrestha. 2014. An incremental approach for
language identification in codeswitched text. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identifi-
cation of languages and encodings in a multilingual
document. In Proceedings of ACL-SIGWAC?s Web
As Corpus3, Belgium.
Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: An annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 37?41, Stroudsburg, PA, USA.
Association for Computational Linguistics.
72
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 99?108,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Handling OOV Words in Dialectal Arabic to English Machine Translation
Maryam Aminian, Mahmoud Ghoneim, Mona Diab
Department of Computer Science
The George Washington University
Washington, DC
{aminian,mghoneim,mtdiab}@gwu.edu
Abstract
Dialects and standard forms of a language
typically share a set of cognates that could
bear the same meaning in both varieties or
only be shared homographs but serve as
faux amis. Moreover, there are words that
are used exclusively in the dialect or the
standard variety. Both phenomena, faux
amis and exclusive vocabulary, are consid-
ered out of vocabulary (OOV) phenomena.
In this paper, we present this problem of
OOV in the context of machine translation.
We present a new approach for dialect
to English Statistical Machine Translation
(SMT) enhancement based on normaliz-
ing dialectal language into standard form
to provide equivalents to address both as-
pects of the OOV problem posited by di-
alectal language use. We specifically fo-
cus on Arabic to English SMT. We use
two publicly available dialect identifica-
tion tools: AIDA and MADAMIRA, to
identify and replace dialectal Arabic OOV
words with their modern standard Arabic
(MSA) equivalents. The results of evalua-
tion on two blind test sets show that using
AIDA to identify and replace MSA equiv-
alents enhances translation results by 0.4%
absolute BLEU (1.6% relative BLEU) and
using MADAMIRA achieves 0.3% ab-
solute BLEU (1.2% relative BLEU) en-
hancement over the baseline. We show
our replacement scheme reaches a notice-
able enhancement in SMT performance
for faux amis words.
1 Introduction
In this day of hyper connectivity, spoken vernacu-
lars are ubiquitously ever more present in textual
social media and informal communication chan-
nels. Written (very close to the spoken) informal
language as represented by dialect poses a signifi-
cant challenge to current natural language process-
ing (NLP) technology in general due to the lack
of standards for writing in these vernaculars. The
problem is exacerbated when the vernacular con-
stitutes a dialect of the language that is quite dis-
tinct and divergent from a language standard and
people code switch within utterance between the
standard and the dialect. This is the case for Ara-
bic. Modern Standard Arabic (MSA), as the name
indicates, is the official standard for the Arabic
language usually used in formal settings, while its
vernaculars vary from it significantly forming di-
alects known as dialectal Arabic (DA), commonly
used in informal settings such as the web and so-
cial media. Contemporary Arabic is a collection of
these varieties. Unlike MSA, DA has no standard
orthography (Salloum and Habash, 2013). Most
of the studies in Arabic NLP have been conducted
on MSA. NLP research on DA, the unstandard-
ized spoken variety of Arabic, is still at its in-
fancy. This constitutes a problem for Arabic pro-
cessing in general due to the ubiquity of DA usage
in written social media. Moreover, linguistic code
switching between MSA and DA always happens
either in the course of a single sentence or across
different sentences. However this intrasentential
code switching is quite pervasive (Elfardy et al.,
2013). For instance 98.13% of sentences crawled
from Egyptian DA (EGY) discussion forums for
the COLABA project (Diab et al., 2010) contains
intrasentential code switching.
MSA has a wealth of NLP tools and resources
compared to a stark deficiency in such resources
for DA. The mix of MSA and DA in utterances
constitutes a significant problem of Out of Vocab-
ulary (OOV) words in the input to NLP applica-
tions. The OOV problem is two fold: completely
unseen words in training data, and homograph
OOVs where the word appears in the training data
but with a different sense. Given these issues, DA
99
NLP and especially DA statistical machine trans-
lation (SMT) can be seen as highly challenging
tasks and this illustrates the need for conducting
more research on DA.
MSA has a wealth of resources such as parallel
corpora and tools like morphological analyzers,
disambiguation systems, etc. On the other hand,
DA still lacks such tools and resources. As an ex-
ample, parallel DA to English (EN) corpora are
still very few and there are almost no MSA-DA
parallel corpora. Similar to MSA, DA has the
problem of writing with optional diacritics. It also
lacks orthographic standards. Hence, translating
from DA to EN is challenging as there are imped-
iments posed by the nature of the language cou-
pled with the lack of resources and tools to process
DA (Salloum and Habash, 2013).
MSA and DA are significantly different on all lev-
els of linguistic representation: phonologically,
morphologically, lexically, syntactically, semanti-
cally and pragmatically. The morphological dif-
ferences between MSA and DA are most notice-
ably expressed by using some clitics and affixes
that do not exist in MSA. For instance, the DA
(Egyptian and Levantine) future marker clitic H
1
is expressed as the clitic s in MSA (Salloum and
Habash, 2013). On a lexical level, MSA and DA
share a considerable number of faux amis where
the lexical tokens are homographs but have dif-
ferent meanings. For instance the word yEny in
MSA means ?to mean?, but in DA, it is a prag-
matic marker meaning ?to some extent?. We refer
to this phenomenon as sense OOV (SOOV). This
phenomenon is in addition to the complete OOV
(COOV) that exist in DA but don?t exist in MSA.
These issues constitute a significant problem for
processing DA using MSA trained tools. This
problem is very pronounced in machine transla-
tion.
In this paper, we present a new approach to build a
DA-to-EN MT system by normalizing DA words
into MSA. We focus our investigation on the
Egyptian variety of DA (EGY). We leverage MSA
resources with robust DA identification tools to
improve SMT performance for DA-to-EN SMT.
We focus our efforts on replacing identified DA
words by MSA counterparts. We investigate the
replacement specifically in the decoding phase of
the SMT pipeline. We explore two state of the
1
We use the Buckwalter Transliteration as represented
in www.qamus.com for Romanized Arabic representation
throughout the paper.
art DA identification tools for the purposes of our
study. We demonstrate the effects of our replace-
ment scheme on each OOV type and show that
normalizing DA words into their equivalent MSA
considerably enhances SMT performance in trans-
lating SOOVs.
The remainder of this paper is organized as fol-
lows: Section 2 overviews related work; Section 3
details our approach; Section 4 presents the results
obtained on standard data sets; in Section 5, we
discuss the results and perform error analysis; fi-
nally we conclude with some further observations
in Section 6.
2 Related Work
Leveraging MSA resources and tools to enrich DA
for NLP purposes has been explored in several
studies. Chiang, et. al. (2006) exploit the rela-
tion between Levantine Arabic (LEV) and MSA
to build a syntactic parser on transcribed spoken
LEV without using any annotated LEV corpora.
Since there are no DA-to-MSA parallel corpora,
rule-based methods have been predominantly em-
ployed to translate DA-to-MSA. For instance,
Abo Bakr et al. (2008) introduces a hybrid ap-
proach to transfer a sentence from EGY into a
diacritized MSA form. They use a statistical ap-
proach for tokenizing and tagging in addition to
a rule-based system for constructing diacritized
MSA sentences. Moreover, Al-Sabbagh and Girju
(2010) introduce an approach to build a DA-to-
MSA lexicon through mining the web.
In the context of DA translation, Sawaf (2010) in-
troduced a hybrid MT system that uses statistical
and rule-based approaches for DA-to-EN MT. In
his study, DA words are normalized to the equiv-
alent MSA using a dialectal morphological ana-
lyzer. This approach achieves 2% absolute BLEU
enhancement for Web texts and about 1% absolute
BLEU improvement over the broadcast transmis-
sions. Furthermore, Salloum and Habash (2012)
use a DA morphological analyzer (ADAM) and a
list of hand-written morphosyntactic transfer rules
(from DA to MSA) to improve DA-to-EN MT.
This approach improves BLEU score on a blind
test set by 0.56% absolute BLEU (1.5% rela-
tive) on the broadcast conversational and broad-
cast news data. Test sets used in their study con-
tain a mix of Arabic dialects but Levantine Arabic
constitutes the majority variety.
Zbib et al. (2012) demonstrate an approach to ac-
100
Figure 1: Block diagram of the proposed system for enhancing DA-to-EN SMT via normalizing DA
quire more DA-to-EN data to improve DA SMT
performance by enriching translation models with
more DA data. They use Amazon Mechanical
Turk to create a DA-to-EN parallel corpus. This
parallel data is augmented to the available large
MSA-to-EN data and is used to train the SMT sys-
tem. They showed that their trained SMT model
on this DA-to-EN data, can achieve 6.3% and 7%
absolute BLEU enhancement over an SMT system
trained on MSA-to-EN data when translating EGY
and LEV test sets respectively. Habash (2008)
demonstrates four techniques for handling OOV
words through modifying phrase tables for MSA.
He also introduces a tool which employs these
four techniques for online handling of OOV in
SMT (Habash, 2009).
Habash et al. (2013) introduces MADA-ARZ, a
new system for morphological analysis and dis-
ambiguation of EGY based on an MSA morpho-
logical analyzer MADA (Habash and Rambow,
2005). They evaluate MADA-ARZ extrinsically
in the context of DA-to-EN MT and show that us-
ing MADA-ARZ for tokenization leads to 0.8%
absolute BLEU improvement over the baseline
which is simply tokenized with MADA. In this
paper, we use MADAMIRA (Pasha et al., 2014),
a system for morphological analysis and disam-
biguation for both MSA and DA (EGY), to iden-
tify DA words and replace MSA equivalents. Our
approach achieves 0.6% absolute BLEU improve-
ment over the scores reported in (Habash et al.,
2013).
3 Approach
In the context of SMT for DA-to-EN, we en-
counter a significant OOV rate between test and
training data since the size of the training data is
relatively small. On the other hand, we have sig-
nificant amounts of MSA-to-EN parallel data to
construct rich phrase tables. MSA and DA, though
divergent, they share many phenomena that can be
leveraged for the purposes of MT. Hence, if we
combine training data from MSA with that from
DA, and then at the decode time normalize OOV
DA words into their equivalent MSA counterparts
we should be able to overcome the resource chal-
lenges in the DA-to-EN SMT context, yielding
better overall translation performance. The OOV
problem is two fold: complete OOV (COOV) and
sense OOV (SOOV). The COOV problem is the
standard OOV problem where an OOV in the in-
put data is not attested at all in the training data.
The SOOV problem is where a word is observed
in the training data but with a different usage or
sense, different from that of the test data occur-
rence. To our knowledge, our research is the first
to address the SOOV directly in the context of
SMT. To that end, we employ two DA identifica-
tion tools: a morphological tagger, as well as a
full-fledged DA identification tool to identify and
replace DA words with their equivalent MSA lem-
mas in the test data at decoding time.
Accordingly, the ultimate goal of this work is to
assess the impact of different DA identification
and replacement schemes on SMT overall perfor-
101
mance and overall OOV (both types) reduction. It
is worth noting that we focus our experiments on
the decoding phase of the SMT system. Figure1
shows the block diagram of the proposed system.
We exploit the following tools and resources:
? MADAMIRA: A system for morphologi-
cal analysis and disambiguation for both
MSA and DA (EGY). MADAMIRA indi-
cates whether a word is EGY or MSA based
on its underlying lexicon which is used to
generate an equivalent EN gloss. However,
for EGY words, MADAMIRA does not gen-
erate the equivalent MSA lemma (Pasha et
al., 2014);
? AIDA: A full-fledged DA identification tool
which is able to identify and classify DA
words on the token and sentence levels.
AIDA exploits MADAMIRA internally in
addition to more information from context to
identify DA words (Elfardy and Diab, 2013).
AIDA provides both the MSA equivalent
lemma(s) and corresponding EN gloss(es) for
the identified DA words;
? THARWA: A three-way lexicon between
EGY, MSA and EN (Diab et al., 2014).
To evaluate effectiveness of using each of these re-
sources in OOV reduction, we have exploited the
following replacement schemes:
? AIDA identifies DA words in the context and
replaces them with the most probable equiv-
alent MSA lemma;
? MADAMIRA determines whether a word is
DA or not. If the word is DA, then EN
gloss(es) from MADAMIRA are used to find
the most probable equivalent MSA lemma(s)
from THARWA.
As all of these DA identification resources
(MADAMIRA, AIDA and THARWA) return
MSA equivalents in the lemma form, we adopt a
factored translation model to introduce the extra
information in the form of lemma factors. There-
fore, DA replacement affects only the lemma fac-
tor in the factored input. We consider the fol-
lowing setups to properly translate replaced MSA
lemma to the the corresponding inflected form
(lexeme):
2
2
We use the term lexeme to indicate an inflected tokenized
uncliticized form of the lemma. A lemma in principle is a
lexeme but it is also a citation form in a dictionary.
? Generated lexeme-to-lexeme translation
(Glex-to-lex): To derive inflected MSA
lexeme from MSA replaced lemma and
POS, we construct a generation table on the
factored data to map lemma and POS factors
into lexeme. This table is generated using
Moses toolkit (Koehn et al., 2007) genera-
tion scripts and provides a list of generated
lexemes for each lemma-POS pair. An MSA
lexeme language model (LM) is then used to
decode the most probable sequence of MSA
lexemes given these generated lexemes for
each word in the sentence.
? lemma+POS-to-lexeme translation
(lem+POS-to-lex): In this path source
lemma and POS are translated into the
appropriate target lexeme. We expect this
path provides plausible translations for DA
words that are not observed in the phrase
tables.
? lexeme-to-lexeme;lemma+POS-to-lexeme
translation (lex-to-lex;lem+POS-to-lex): The
first path translates directly from a source
lexeme to the target lexeme. So it provides
appropriate lexeme translations for the words
(MSA or DA) which have been observed
in the trained model. It is worth noting
that lex-to-lex translation path does not
contain any replacement or normalization.
Therefore, it is different from the first path
(Glex-to-lex). The second path is similar
to the lem+POS-to-lex path and is used to
translate DA words that do not exist in the
trained model.
3.1 Data Sets
For training translation models we use a collec-
tion of MSA and EGY texts created from mul-
tiple LDC catalogs
3
comprising multiple genres
(newswire, broadcast news, broadcast conversa-
tions, newsgroups and weblogs).The train data
contains 29M MSA and 5M DA tokenized words.
We use two test sets to evaluate our method on
both highly DA and MSA texts: For DA test data,
we selected 1065 sentences from LDC2012E30,
which comprises 16177 tokenized words (BOLT-
arz-test); For MSA, we use the NIST MTE-
val 2009 test set (LDC2010T23), which contains
3
41 LDC catalogs including data prepared for GALE and
BOLT projects. Please contact the authors for more details.
102
1445 sentences corresponding to 40858 tokenized
words (MT09-test). As development set (dev set),
we randomly select 1547 sentences from multi-
ple LDC catalogs (LDC2012E15, LDC2012E19,
LDC2012E55) which comprises 20780 tokens.
The following preprocessing steps are performed
on the train, test and dev sets: The Arabic
side of the parallel data is Alef/Ya normal-
ized and tokenized using MADAMIRA v1. ac-
cording to Arabic Treebank (ATB) tokenization
scheme (Maamouri et al., 2004); Tokenization on
the EN side of the parallel data is performed using
Tree Tagger (Schmid, 1994).
3.2 Language Modeling
We create a 5-gram language model (LM)
from three corpora sets: a) The English Giga-
word 5 (Graff and Cieri, 2003); b) The En-
glish side of the BOLT Phase1 parallel data;
and, c) different LDC English corpora col-
lected from discussion forums (LDC2012E04,
LDC2012E16, LDC2012E21, LDC2012E54). We
use SRILM (Stolcke., 2002) to build 5-gram lan-
guage models with modified Kneser-Ney smooth-
ing.
3.3 SMT System
We use the open-source Moses toolkit (Koehn et
al., 2007) to build a standard phrase-based SMT
system which extracts up to 8 words phrases in the
Moses phrase table. The parallel corpus is word-
aligned using GIZA++ (Och and Ney, 2003). Fea-
ture weights are tuned to maximize BLEU on
the dev set using Minimum Error Rate Training
(MERT) (Och, 2003). To account for the in-
stability of MERT, we run the tuning step three
times per condition with different random seeds
and use the optimized weights that give the me-
dian score on the development set. As all our
DA identification resources (MADAMIRA, AIDA
and THARWA) are lemma-based, we adopt a fac-
tored translation model setup to introduce the ex-
tra information in the form of a lemma factor. As
lemma only is not enough to generate appropriate
inflected surface (lexeme) forms, we add a POS
factor with two main translation paths: (i) direct
translation from a source lexeme to the target lex-
eme; and (ii) translation from source lemma and
POS to the appropriate target lexeme. Therefore,
the first path should provide plausible translations
for the words that have been seen before in the
phrase tables while we expect that the second path
provides feasible translations for DA words that
are not seen in the trained model.
4 Experimental Results
4.1 Baseline Results
For each experimental condition mentioned in
Section 3, we define a separate baseline with sim-
ilar setup. These baselines use the SMT setup de-
scribed in Section 3.3 and are evaluated on the two
test sets mentioned in Section 3.1. To assess ef-
fectiveness of normalizing DA into MSA on the
overall performance of MT system, the dev and
test sets are processed through the similar steps
to generate factored data but without any replace-
ment of the DA words with MSA correspondents.
We believe this to be a rigorous and high base-
line as data contains some morphological informa-
tion useful for DA-to-EN translation in the form
of lemma and POS factors. We started with a
baseline trained on the 29M words tokenized MSA
training set and 5M words tokenized DA set sepa-
rately. We created the baseline trained on the 34M
words MSA+DA train data. Our objective of split-
ting train data based on its dialectal variety is to
assess the role of DA words existing in the train
set in the performance of our approach.
Table 1 illustrates baseline BLEU scores on
BOLT-arz and MT09-test sets with three differ-
ent training conditions: MSA+DA, MSA only, and
DA only.
4.2 Replacement Experimental Results
We run the SMT pipeline using the feature weights
that performed best during the tuning session on
our dev set. Then the SMT pipeline with these
tuned weights is run on two blind test sets. To
account for statistical significance tests we used
bootstrapping methods as detailed in (Zhang and
Vogel, 2010). Table 2 shows BLEU scores of dif-
ferent DA identification and replacement schemes
exploited in different setups on the test sets.
As we can see in Table 2, both AIDA and
MADAMIRA replacement schemes outperform
the baseline scores using MSA+DA trained mod-
els and lem+POS-to-lex;lex-to-lex setup. AIDA
reaches 0.4% absolute BLEU (1.6% relative
BLEU) improvement and MADAMIRA achieves
0.3% absolute BLEU (1.2% relative BLEU) en-
hancement over the corresponding baselines. This
is while the same enhancement in BLEU scores
can not be captured when we exploit the model
103
Test Set Train Set lex-to-lex lem+POS-to-lex lex-to-lex:lem+POS-to-lex
BOLT-arz-test
MSA+DA 26.2 25.4 25.5
MSA 21.8 21.2 21.8
DA 24.3 24.6 24.8
MT09-test
MSA+DA 48.2 46.9 47.3
MSA 44.4 45.4 44.6
DA 35.6 36.1 34.2
Table 1: Baseline BLUE scores for each setup on two test sets: BOLT-arz-test and MT09-test. Results
are reported for each training input language variety separately.
Test Set Train Set Glex-to-lex lem+POS-to-lex lex-to-lex:lem+POS-to-lex
AIDA MADAMIRA AIDA MADAMIRA AIDA MADAMIRA
BOLT-arz-test
MSA+DA 24.4 25.1 22.6 24.1 25.9 25.8
MSA 20.6 21.0 20.1 20.3 21.7 22.0
DA 24.3 23.7 21.3 23.1 24.5 24.8
MT09-test
MSA+DA 45.9 45.8 45.4 44.6 47.1 47.3
MSA 42.7 42.4 45.2 43.7 44.5 44.6
DA 35.6 34.0 36.1 34.5 34.1 34.3
Table 2: BLEU scores of AIDA and MADAMIRA replacement for the different setups on
BOLT-arz-test and MT09-test. Results are reported for each training language variety separately.
which is trained on MSA or DA parallel data
solely. This indicates that normalizing DA into
MSA can reach its best performance only when
we enrich the training model with DA words at
the same time. Therefore, we note that acquir-
ing more DA data to enrich phrase tables at the
training phase and normalizing DA at the decod-
ing step of SMT system would yield the best DA-
to-EN translation accuracy.
Regardless of the replacement scheme we use to
reduce the OOV rate (AIDA or MADAMIRA),
BLEU scores on the MT09 are much higher than
those on the BOLT-arz because the amount of
MSA words in the training data is much more than
DA words. Therefore, SMT system encounters
less OOVs at the decode time on MSA texts such
as MT09. Overall we note that adding AIDA or
MADAMIRA to the setup at best has no impact
on performance on the MT09 data set since it is
mostly MSA. However, we note a small impact
for using the tools in the lex-to-lex:lem+POS-to-
lex path in the MSA+DA experimental setting.
Comparing results of different setups indi-
cates that adding lex-to-lex translation path to
the lem+POS-to-lex increases both AIDA and
MADAMIRA performance on two test sets sig-
nificantly. As Table 2 demonstrates adding lex-
to-lex path to the lem+POS-to-lex translation us-
ing the model trained on MSA+DA data leads to
3.3% and 1.7% BLEU improvement using AIDA
and MADAMIRA, respectively on the BOLT-arz
set. Similar conditions on the MT09-test gives
us 1.7% and 0.7% absolute improvement in the
BLEU scores using AIDA and MADAMIRA re-
spectively. This happens because lex-to-lex path
can provide better translations for the words (MSA
or DA) which have been seen in the phrase tables
and having both these paths enables the SMT sys-
tem to generate more accurate translations. Our
least results are obtained when we use lem+POS-
to-lex translation path solely either using AIDA or
MADAMIRA which mainly occurs due to some
errors existing in the output of morphological an-
alyzer that yields to the erroneous lemma or POS.
BOLT-arz MT09
Sent. 1065 1445
Types 4038 8740
Tokens 16177 40858
COOV (type) 126 (3%) 169 (2%)
COOV (token) 134 (0.82%) 187 (0.45%)
Table 3: Number of sentences, types, tokens and
COOV percentages in each test set
104
Reference not private , i mean like buses and the metro and trains ... etc .
Baseline mc mlkyp xASp yEny AqSd zy AlAtwbys w+ Almtrw w+ AlqTAr . . . Alx
Baseline translation privately , i mean , i mean , i do not like the bus and metro and train , etc .
Replacement mc mlkyp xASp yEny AqSd mvl AlAtwbys w+ Almtrw w+ AlqTAr . . . Alx
Replacement translation not a private property , i mean , i mean , like the bus and metro and train , etc .
Table 4: Example of translation enhancement by SOOV replacement
5 Error Analysis
To assess the rate of OOV reduction using
different replacement methodologies, we first
identify OOV words in the test sets. Then, out
of these words, cases that our approach has led
to an improvement in the sentence BLEU score
over the baseline is reported. Table 3 shows
the number of sentences, types and tokens for
each test set as well as the corresponding type
and token OOV counts. As we can see in this
table, 0.82% of tokens in BOLT-arz and 0.45% of
tokens in MT09-test sets are OOV. These cover
the complete OOV cases (COOV).
In addition to these cases of COOV that are caused
by lack of enough training data coverage, there
are sense OOV (SOOV). SOOV happens when
a particular word appears in both DA and MSA
data but have different senses as faux amis. For
instance the Arabic word qlb occurs in both MSA
and DA contexts but with a different set of senses
due to the lack of diacritics. In the specific MSA
context it means ?heart? while in DA it means
either ?heart? or ?change?. Therefore, in addition
to the cases that word sense is triggered by DA
context, other levels of word sense ambiguity
such as homonymy and polysemy are involved in
defining an SOOV word. Hence, SOOV identifi-
cation in the test set needs additional information
such as word equivalent EN gloss.
We determine SOOV as the words that (i) are
observed as MSA word in the training data and
considered a DA word in the test set once pro-
cessed by AIDA and MADAMIRA; and, (ii) MSA
and DA renderings have different non-overlapped
equivalent EN glosses as returned by our AIDA
and MADAMIRA. We assume that words with
different dialectal usages in the train and test
will have completely different EN equivalents,
and thereby will be considered as SOOV. One
of the words that this constraint has recognized
as SOOV is the word zy with English equivalent
?uniform? or ?clothing? in MSA and ?such as? or
?like? in DA. Replacement of this SOOV by the
MSA equivalent ?mvl? yields better translation as
shown in Table 4.
Among all COOV words, our approach only tar-
gets COOV which are identified as DA. Table 5
and 6 report the number of COOV words (type and
token) which have been identified as DA by AIDA
or MADAMIRA in BOLT-arz and MT09 test sets,
respectively. Second column in these tables repre-
sent number of SOOV (type and token) in each set.
Last columns show percentage of sentences which
have had at least one COOV or SOOV word and
our replacement methodology has improved the
sentence BLEU score over the baseline for each
setup, respectively. Percentages in these columns
demonstrate the ratio of enhanced sentences to the
total number of sentences which have been deter-
mined to have at least one COOV or SOOV word.
These percentages are reported on the MSA+DA
data to train the SMT system condition.
While Table 5 and 6 show enhancements through
DA COOV replacements, our manual assessment
finds that most of these enhancements are actu-
ally coming from SOOVs present in the same sen-
tences. For example, when we examined the 21
types identified by AIDA as DA COOV in BOLT-
arz we found 9 typos, 5 MSAs, one foreign word
and only 6 valid DA types. Moreover, none of the
replacements over these 6 DA types yield an en-
hancement.
Although Table 5 shows that MADAMIRA
achieves more success enhancing BLEU score of
sentences which contain SOOV words on BOLT-
arz test set, results of our investigation show that
AIDA deteriorated performance on SOOV hap-
pens due to the noise that its MSA replacements
add to the non-SOOV proportion of data. To as-
sess this hypothesis we ran the best experimental
setup (decoding:lex-to-lex:lem+POS-to-lex, train-
ing: MSA+DA) on the proportion of sentences
in BOLT-arz which contain at least one SOOV
105
Replacement Scheme
DA COOV SOOV setup Enhanced Sentences
DA COOV SOOV
AIDA Replacement
type 21 712
lex-to-lex 40% 58%
lem+POS-to-lex 60% 35%
token 26 1481
lex-to-lex:lem+POS-to-lex 55% 57%
MADAMIRA Replacement
type 9 194
lex-to-lex 34% 55%
lem+POS-to-lex 34% 47%
token 9 281
lex-to-lex:lem+POS-to-lex 45% 62%
Table 5: Columns from left to right: number of DA COOV, SOOV and percentages of enhanced
sentences for BOLT-arz set.
Replacement Scheme
DA COOV SOOV setup Enhanced Sentences
DA COOV SOOV
AIDA Replacement
type 6 376
lex-to-lex 67% 44%
lem+POS-to-lex 84% 35%
token 6 499
lex-to-lex:lem+POS-to-lex 50% 61%
MADAMIRA Replacement
type 7 559
lex-to-lex 29% 40%
lem+POS-to-lex 27% 34%
token 7 852
lex-to-lex:lem+POS-to-lex 43% 48%
Table 6: Similar to Table 5 for MT09 set.
word as processed using AIDA and MADAMIRA
(the intersection subset). It is worth noting that
compared to the baseline BLEU score of 23.8
on this subset, AIDA achieves a BLEU score of
24.4 while MADAMIRA only achieves a lower
BLEU score of 24.0. This implicitly demon-
strates that AIDA provides better MSA equiva-
lents even for DA words which have MSA homo-
graphs with different meanings (faux amis cases).
Overall, we note that the same results can be cap-
tured from Table 2 that shows AIDA outperform-
ing MADAMIRA in identifying and replacing DA
words.
6 Conclusion and Future Work
We presented a new approach to enhance DA-
to-EN machine translation by reducing the rate
of DA OOV words. We employed AIDA and
MADAMIRA to identify DA words and replace
them with the corresponding MSA equivalent.
We showed our replacement scheme reaches a
noticeable enhancement in SMT performance for
SOOVs. This can be considered one of the con-
tributions of this work which was not addressed in
the previous studies before. The results of evalua-
tion on two blind test sets showed that using AIDA
to identify and replace MSA equivalents enhances
translation results by 0.4% absolute BLEU (1.6%
relative BLEU) and using MADAMIRA achieves
0.3% absolute BLEU (1.2% relative BLEU) en-
hancement over the baseline on two blind test sets.
One of the interesting ideas to extend this
project in the future is to combine AIDA and
MADAMIRA top choices in a confusion network
and feeding this confusion network to the SMT
system. Acquiring more DA-to-EN parallel data
to enrich translation models is another work which
we intend to pursue later. Moreover, evaluating
possible effects of different genres and domains
on the framework efficiency provides another path
to extend this work in future.
Acknowledgments
This work was supported by the Defense Ad-
vanced Research Projects Agency (DARPA) Con-
tract No. HR0011-12-C-0014, the BOLT program
with subcontract from Raytheon BBN. We would
like to acknowledge the useful comments by three
anonymous reviewers who helped in making this
publication more concise and better presented.
References
Abhaya Agarwal, and Alon Lavie. 2008. Meteor,
m-bleu and m-ter: Evaluation metrics for high-
106
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pp. 115-118,
Rania Al-Sabbagh and Roxana Girju. 2010. Mining
the Web for the Induction of a dialectal Arabic Lexi-
con. In Proceedings of the Language Resources and
Evaluation Conference (LREC),
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
Dialects. In Proceedings of EACL 2006,
Mona Diab, Mohamed Al-Badrashiny, Maryam
Aminian, Mohammed Attia, Pradeep Dasigi, Heba
Elfardy, Ramy Eskander, Nizar Habash, Abdelati
Hawwari and Wael Salloum. 2014. Tharwa: A
Large Scale Dialectal Arabic - Standard Arabic -
English Lexicon. In Proceedings of LREC 2014,
Reykjavik, Iceland.
Mona Diab, Nizar Habash, Owen Rambow, Mohamed
Altantawy and Yassin Benajiba. 2010. Colaba: Ara-
bic dialect annotation and processing. In Proceed-
ings of LREC Workshop on Semitic Language Pro-
cessing, pp. 6674.
Heba Elfardy and Mona Diab. 2013. Sentence level
dialect identification in Arabic. In Proceedings of
ACL 2013, Sofia, Bulgaria.
Heba Elfardy, Mohamed Al-Badrashiny and Mona
Diab. 2013. Code Switch Point Detection in Ara-
bic. In Natural Language Processing and Informa-
tion Systems, Springer Berlin Heidelberg, pp. 412-
416.
David Graff and Christopher Cieri. 2003. English Gi-
gaword, LDC Catalog No.: LDC2003T05 Linguis-
tic Data Consortium, University of Pennsylvania.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL 2005,
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine transla-
tion.. In Proceedings of the 2nd International Con-
ference on Arabic Language Resources and Tools
(MEDAR), Cairo, Egypt.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL 2008: HLT, Short Papers, Colum-
bus, Ohio.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander and Nadi Tomeh. 2013. Morphological anal-
ysis and disambiguation for dialectal Arabic. In
Proceedings of NAACL 2013:HLT, pp. 426-432.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of ACL 2007, Demo and
Poster Sessions. Prague, Czech Republic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, Cairo, Egypt.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of ACL 2003, pages 160-167 Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics. Vol. 29. pp. 19-
51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL 2002, pages 311318, Philadelphia, PA.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow and Ryan M. Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of LREC 2014, Reykjavik,
Iceland.
Wael Salloum and Nizar Habash. 2013. Dialectal
Arabic to English Machine Translation: Pivoting
through Modern Standard Arabic. In Proceedings
of NAACL 2013:HLT, Atlanta, Georgia.
Wael Salloum and Nizar Habash. 2012. Elissa: A Di-
alectal to Standard Arabic Machine Translation Sys-
tem. In Proceedings of COLING 2012, Denver,
Colorado.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of AMTA 2010,
Denver, Colorado.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of in-
ternational conference on new methods in language
processing, pp. 44-49.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA 2006, pp. 223-231.
Andreas Stolcke. 2002. SRILM an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing,
Ying Zhang and Stephan Vogel. 2010. Significance
Tests of Automatic Machine Translation Evaluation
Metrics. In Machine Translation, Vol. 24, Issue 1,
pages 51-65.
107
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In Proceedings of NAACL 2012:HLT,
108
