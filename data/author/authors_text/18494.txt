Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 833?838,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Learning and Inference in a Large Knowledge-base
using Latent Syntactic Cues
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{mg1,ppt,bkisiel,tom.mitchell}@cs.cmu.edu
Abstract
Automatically constructed Knowledge Bases
(KBs) are often incomplete and there is a gen-
uine need to improve their coverage. Path
Ranking Algorithm (PRA) is a recently pro-
posed method which aims to improve KB cov-
erage by performing inference directly over
the KB graph. For the first time, we demon-
strate that addition of edges labeled with la-
tent features mined from a large dependency
parsed corpus of 500 million Web documents
can significantly outperform previous PRA-
based approaches on the KB inference task.
We present extensive experimental results val-
idating this finding. The resources presented
in this paper are publicly available.
1 Introduction
Over the last few years, several large scale Knowl-
edge Bases (KBs) such as Freebase (Bollacker et
al., 2008), NELL (Carlson et al, 2010), and YAGO
(Suchanek et al, 2007) have been developed. Each
such KB consists of millions of facts (e.g., (Tiger
Woods, playsSport, Golf )) spanning over multiple
relations. Unfortunately, these KBs are often incom-
plete and there is a need to increase their coverage of
facts to make them useful in practical applications.
A strategy to increase coverage might be to per-
form inference directly over the KB represented as a
graph. For example, if the KB contained the follow-
ing facts, (Tiger Woods, participatesIn, PGA Tour))
and (Golf, sportOfTournament, PGA Tour), then by
putting these two facts together, we could potentially
infer that (Tiger Woods, playsSport, Golf ). The
Figure 1: Example demonstrating how lexicalized syn-
tactic edges can improve connectivity in the KB enabling
PRA (Lao and Cohen, 2010) to discover relationships be-
tween Alex Rodriguez and World Series. Edges with la-
tent labels can improve inference performance by reduc-
ing data sparsity. See Section 1.1 for details.
recently proposed Path Ranking Algorithm (PRA)
(Lao and Cohen, 2010) performs such inference by
automatically learning semantic inference rules over
the KB (Lao et al, 2011). PRA uses features based
off of sequences of edge types, e.g., ?playsSport,
sportOfTournament?, to predict missing facts in the
KB.
PRA was extended by (Lao et al, 2012) to per-
form inference over a KB augmented with depen-
dency parsed sentences. While this opens up the
possibility of learning syntactic-semantic inference
rules, the set of syntactic edge labels used are
just the unlexicalized dependency role labels (e.g.,
nobj, dobj, etc., without the corresponding words),
thereby limiting overall expressitivity of the learned
inference rules. To overcome this limitation, in this
paper we augment the KB graph by adding edges
with more expressive lexicalized syntactic labels
(where the labels are words instead of dependen-
833
cies). These additional edges, e.g., (Alex Rodriguez,
?plays for?, NY Yankees), are mined by extracting
600 million Subject-Verb-Object (SVO) triples from
a large corpus of 500m dependency parsed docu-
ments, which would have been prohibitively expen-
sive to add directly as in (Lao et al, 2012). In order
to overcome the explosion of path features and data
sparsity, we derive edge labels by learning latent em-
beddings of the lexicalized edges. Through exten-
sive experiments on real world datasets, we demon-
strate effectiveness of the proposed approach.
1.1 Motivating Example
In Figure 1, the KB graph (only solid edges) is dis-
connected, thereby making it impossible for PRA to
discover any relationship between Alex Rodriguez
and World Series. However, addition of the two
edges with SVO-based lexicalized syntactic edges
(e.g., (Alex Rodriguez, plays for, NY Yankees)) re-
stores this inference possibility. For example, PRA
might use the edge sequence ??plays for?, team-
PlaysIn? as evidence for predicting the relation in-
stance (Alex Rodriguez, athleteWonChampionship,
World Series). Unfortunately, such na??ve addition
of lexicalized edges may result in significant data
sparsity, which can be overcome by mapping lexi-
calized edge labels to some latent embedding (e.g.,
(Alex Rodriguez, LatentFeat#5, NY Yankees) and
running PRA over this augmented graph. Using la-
tent embeddings, PRA could then use the following
edge sequence as a feature in its prediction models:
?LatentFeat#5, teamPlaysIn?. We find this strategy
to be very effective as described in Section 4.
2 Related Work
There is a long history of methods using suface-level
lexical patterns for extracting relational facts from
text corpora (Hearst, 1992; Brin, 1999; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002;
Etzioni et al, 2004). Syntactic information in the
form of dependency paths have been explored in
(Snow et al, 2006; Suchanek et al, 2006). A
method of latent embedding of relation instances
for sentence-level relation extraction was shown in
(Wang et al, 2011). However, none of this prior
work makes explicit use of the background KBs as
we explore in this paper.
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010) has been used previously to perform inference
over graph-structured KBs (Lao et al, 2011), and to
learn formation of online communities (Settles and
Dow, 2013). In (Lao et al, 2012), PRA is extended
to perform inference over a KB using syntactic in-
formation from parsed text. In contrast to these pre-
vious PRA-based approaches where all edge labels
are either KB labels or at surface-level, in this pa-
per we explore using latent edge labels in addition
to surface-level labels in the graph over which PRA
is applied. In particular, we focus on the problem of
performing inference over a large KB and learn la-
tent edge labels by mining dependency syntax statis-
tics from a large text corpus.
Though we use Principal Components Analysis
(PCA) for dimensionality reduction for the experi-
ments in this paper, this is by no means the only
choice. Various other dimensionality reduction tech-
niques, and in particular, other verb clustering tech-
niques (Korhonen et al, 2003), may also be used.
OpenIE systems such as Reverb (Etzioni et al,
2011) also extract verb-anchored dependency triples
from large text corpus. In contrast to such ap-
proaches, we focus on how latent embedding of
verbs in such triples can be combined with explicit
background knowledge to improve coverage of ex-
isting KBs. This has the added capability of infer-
ring facts which are not explicitly mentioned in text.
The recently proposed Universal Schema (Riedel
et al, 2013) also demonstrates the benefit of us-
ing latent features for increasing coverage of KBs.
Key differences between that approach and ours in-
clude our use of syntactic information as opposed to
surface-level patterns in theirs, and also the ability
of the proposed PRA-based method to generate use-
ful inference rules which is beyond the capability of
the matrix factorization approach in (Riedel et al,
2013).
3 Method
3.1 Path Ranking Algorithm (PRA)
In this section, we present a brief overview of the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010), building on the notations in (Lao et al, 2012).
Let G = (V,E, T ) be the graph, where V is the set
of vertices, E is the set of edges, and T is the set of
edge types. For each edge (v1, t, v2) ? E, we have
834
v1, v2 ? V and t ? T . LetR ? T be the set of types
predicted by PRA. R could in principal equal T , but
in this paper we restrict prediction to KB relations,
while T also includes types derived from surface text
and latent embeddings. Let pi = ?t1, t2, . . . , tw? be
a path type of length w over graph G, where ti ? T
is the type of the ith edge in the path. Each such
path type is also a feature in the PRA model. For
a given source and target node pair s, t ? V , let
P (s ? t;pi) be the value of the feature pi specify-
ing the probability of reaching node t starting from
node s and following a path constrained by path type
pi. We approximate these probabilities using random
walks. A value of 0 indicates unreachability from s
to t using path type pi.
Let B = {pi1, . . . , pim} be the set of all features
(path types). The score that relation r holds between
node s and node t is given by the following function:
ScorePRA(s, t, r) =
?
pi?B
P (s? t;pi) ?rpi
where ?rpi is the weight of feature pi in class r ? R.
Feature Selection: The set B of possible path
types grows exponentially in the length of the paths
that are considered. In order to have a manageable
set of features to compute, we first perform a feature
selection step. The goal of this step is to select for
computation only those path types that commonly
connect sources and targets of relation r. We per-
form this feature selection by doing length-bounded
random walks from a given list of source and tar-
get nodes, keeping track of how frequently each path
type leads from a source node to a target node. The
most common m path types are selected for the set
B.
Training: We perform standard logistic regres-
sion with L2 regularization to learn the weights ?rpi.
We follow the strategy in (Lao and Cohen, 2010) to
generate positive and negative training instances.
3.2 PRAsyntactic
In this section, we shall extend the knowledge graph
G = (V,E, T ) from the previous section with an
augmented graph G
?
= (V,E
?
, T
?
), where E ? E
?
and T ? T
?
, with the set of vertices unchanged.
In order to get the edges in E
?
? E, we first
collect a set of Subject-Verb-Object (SVO) triples
D = {(s, v, o, c)} from a large dependency parsed
text corpus, with c ? R+ denoting the frequency
of this triple in the corpus. The additional edge
set is then defined as Esyntactic = E
?
? E =
{(s, v, o) | ?(s, v, o, c) ? D, s, o ? V }. We de-
fine S = {v | ?(s, v, o) ? Esyntactic} and set
T
?
= T ? S. In other words, for each pair of
directly connected nodes in the KB graph G, we add
an additional edge between those two nodes for each
verb which takes the NPs represented by two nodes
as subjects and objects (or vice versa) as observed in
a text corpus. In Figure 1, (Alex Rodriguez, ?plays
for?, NY Yankees) is an example of such an edge.
PRA is then applied over this augmented graph
G
?
, over the same set of prediction types R as be-
fore. We shall refer to this version of PRA as
PRAsyntactic. For the experiments in this paper, we
collected |D| = 600 million SVO triples1 from the
entire ClueWeb corpus (Callan et al, 2009), parsed
using the Malt parser (Nivre et al, 2007) by the
Hazy project (Kumar et al, 2013).
3.3 PRAlatent
In this section we construct G
??
= (V,E??, T
??
),
another syntactic-information-induced extension of
the knowledge graph G, but instead of using the sur-
face forms of verbs in S (see previous section) as
edge types, we derive those edges types T
??
based
on latent embeddings of those verbs. We note that
E ? E
??
, and T ? T
??
.
In order to learn the latent or low dimensional em-
beddings of the verbs in S, we first define QS =
{(s, o) | ?(s, v, o, c) ? D, v ? S}, the set of
subject-object tuples in D which are connected by
at least one verb in S. We now construct a matrix
X|S|?|QS | whose entry Xv,q = c, where v ? S, q =
(s, o) ? QS , and (s, v, o, c) ? D. After row normal-
izing and centering matrix X , we apply PCA on this
matrix. Let A|S|?d with d << |QS | be the low di-
mensional embeddings of the verbs in S as induced
by PCA. We use two strategies to derive mappings
for verbs from matrix A.
? PRAlatentc : The verb is mapped to concatena-
tion of the k2 most positive columns in the row
in A that corresponds to the verb. Similarly, for
the most negative k2 columns.
1This data and other resources from the paper are publicly
available at http://rtw.ml.cmu.edu/emnlp2013 pra/.
835
Precision Recall F1
PRA 0.800 0.331 0.468
PRAsyntactic 0.804 0.271 0.405
PRAlatentc 0.885 0.334 0.485
PRAlatentd 0.868 0.424 0.570
Table 1: Comparison of performance of different variants
of PRA micro averaged across 15 NELL relations. We
find that use of latent edge labels, in particular the pro-
posed approach PRAlatentd , significantly outperforms
other approaches. This is our main result. (See Section 4)
? PRAlatentd : The verb is mapped to disjunction
of top-k most positive and negative columns in
the row in A that corresponds to the verb.
4 Experiments
We compared the various methods using 15 NELL
relations. For each relation, we split NELL?s known
relation instances into 90% training and 10% testing.
For each method, we then selected 750 path features
and trained the model, as described in Section 3, us-
ing GraphChi (Kyrola et al, 2012) to perform the
random walk graph computations. To evaluate the
model, we took all source nodes in the testing data
and used the model to predict target nodes. We re-
port the precision and recall (on the set of known tar-
get nodes) of the set of predictions for each model
that are above a certain confidence threshold. Be-
cause we used strong regularization, we picked for
our threshold a model score of 0.405, correspond-
ing to 60% probability of the relation instance being
true; values higher than this left many relations with-
out any predictions. Table 1 contains the results.
As can be seen in the table, PRAsyntactic on av-
erage performs slightly worse than PRA. While
the extra syntactic features are very informative for
some relations, they also introduce a lot of spar-
sity, which makes the model perform worse on other
relations. When using latent factorization meth-
ods to reduce the sparsity of the syntactic features,
we see a significant improvement in performance.
PRAlatentc has a 45% reduction in precision er-
rors vs. PRA while maintaining the same recall,
and PRAlatentd reduces precision errors by 35%
while improving recall by 27%. Section 4.1 con-
tains some qualitative analysis of how sparsity is re-
duced with the latent methods. As a piece quanti-
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
PRAPRAsyntacticPRAlatentcPRAlatentd
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
PRAPRAsyntacticPRAlatentcPRAlatentd
Figure 2: Precision (y axis) - Recall (x axis) plots for the
relations cityLiesOnRiver (top) and athletePlaysForTeam
(bottom). PRAlatentd (rightmost plot), the proposed ap-
proach which exploits latent edge labels, outperforms
other alternatives.
tative analysis, there were 908 possible path types
found in the feature selection step with PRA on the
relation cityLiesOnRiver (of which we then selected
750). For PRAsyntactic, there were 73,820, while
PRAlatentc had 47,554 and PRAlatentd had 58,414.
Table 2 shows F1 scores for each model on
each relation, and Figure 2 shows representative
Precision-Recall plots for two NELL relations. In
both cases, we find that PRAlatentd significantly
outperforms other baselines.
4.1 Discussion
While examining the model weights for each of
the methods, we saw a few occasions where sur-
face relations and NELL relations combined to form
interpretable path types. For example, in ath-
letePlaysForTeam, some highly weighted features
took the form of ?athletePlaysSport, ?(sport) played
by (team)??. A high weight on this feature would
bias the prediction towards teams that are known to
play the same sport as the athlete.
For PRA, the top features for the best performing
relations are path types that contain a single edge
836
PRA PRAsyntactic PRAlatentc PRAlatentd
animalIsTypeOfAnimal 0.52 0.50 0.47 0.53
athletePlaysForTeam 0.22 0.21 0.56 0.64
athletePlaysInLeague 0.81 0.75 0.73 0.74
cityLiesOnRiver 0.05 0 0.07 0.31
cityLocatedInCountry 0.15 0.20 0.45 0.55
companyCeo 0.29 0.18 0.25 0.35
countryHasCompanyOffice 0 0 0 0
drugHasSideEffect 0.96 0.95 0.94 0.94
headquarteredIn 0.31 0.11 0.41 0.64
locationLocatedWithinLocation 0.40 0.38 0.38 0.41
publicationJournalist 0.10 0.06 0.10 0.16
roomCanContainFurniture 0.72 0.70 0.71 0.73
stadiumLocatedInCity 0.53 0 0.13 0.67
teamPlaysAgainstTeam 0.47 0.24 0.26 0.21
writerWroteBook 0.59 0.62 0.73 0.80
Table 2: F1 performance of different variants of PRA for all 15 relations tested.
which is a supertype or subtype of the relation be-
ing predicted. For instance, for the relation ath-
letePlaysForTeam (shown in Figure 2), the highest-
weighted features in PRA are athleteLedSport-
sTeam (more specific than athletePlaysForTeam)
and personBelongsToOrganization (more general
than athletePlaysForTeam). For the same rela-
tion, PRAsyntactic has features like ?scored for?,
?signed?, ?have?, and ?led?. When using a latent
embedding of these verb phrases, ?signed?, ?have?,
and ?led? all have the same representation in the la-
tent space, and so it seems clear that PRAlatent gains
a lot by reducing the sparsity inherent in using sur-
face verb forms.
For cityLiesOnRiver, where PRA does not per-
form as well, there is no NELL relation that is an im-
mediate supertype or subtype, and so PRA does not
have as much evidence to use. It finds features that,
e.g., are analogous to the statement ?cities in the
same state probably lie on the same river?. Adding
lexical labels gives the model edges to use like ?lies
on?, ?runs through?, ?flows through?, ?starts in?
and ?reaches?, and these features give a significant
boost in performance to PRAsyntactic. Once again,
almost all of those verb phrases share the same latent
embedding, and so PRAlatent gains another signifi-
cant boost in performance by combining them into a
single feature.
5 Conclusion
In this paper, we introduced the use of latent lexi-
cal edge labels for PRA-based inference over knowl-
edge bases. We obtained such latent edge labels
by mining a large dependency parsed corpus of
500 million web documents and performing PCA
on the result. Through extensive experiments on
real datasets, we demonstrated that the proposed ap-
proach significantly outperforms previous state-of-
the-art baselines.
Acknowledgments
We thank William Cohen (CMU) for enlightening
conversations on topics discussed in this paper. We
thank the ClueWeb project (CMU) and the Hazy
Research Group (http://hazy.cs.wisc.edu/hazy/) for
their generous help with data sets; and to the anony-
mous reviewers for their constructive comments.
This research has been supported in part by DARPA
(under contract number FA8750-13-2-0005), and
Google. Any opinions, findings, conclusions and
recommendations expressed in this paper are the au-
thors? and do not necessarily reflect those of the
sponsors.
837
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM conference on Digital
libraries.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web.
J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009.
Clueweb09 data set. boston.lti.cs.cmu.edu.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka Jr, and Tom M Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In AAAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
itall:(preliminary results). In Proceedings of WWW.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second generation.
In Proceedings of IJCAI.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational Linguistics.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of ACL.
Arun Kumar, Feng Niu, and Christopher Re?. 2013.
Hazy: making it easier to build and maintain big-data
analytics. Communications of the ACM, 56(3):40?49.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin. 2012.
Graphchi: Large-scale graph computation on just a pc.
In Proceedings of the 10th USENIX Symposium on Op-
erating Systems Design and Implementation (OSDI),
pages 31?46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53?67.
Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Burr Settles and Steven Dow. 2013. Let?s get together:
the formation and success of online creative collabora-
tions. In Proceedings of CHI.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL.
Fabian M Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of KDD.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of WWW.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation top-
ics. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1426?1436. Association for Computational Linguis-
tics.
838
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 397?406,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incorporating Vector Space Similarity in Random Walk Inference over
Knowledge Bases
Matt Gardner
Carnegie Mellon University
mg1@cs.cmu.edu
Partha Talukdar
?
Indian Institute of Science
ppt@serc.iisc.in
Jayant Krishnamurthy
Carnegie Mellon University
jayantk@cs.cmu.edu
Tom Mitchell
Carnegie Mellon University
tom@cs.cmu.edu
Abstract
Much work in recent years has gone into
the construction of large knowledge bases
(KBs), such as Freebase, DBPedia, NELL,
and YAGO. While these KBs are very
large, they are still very incomplete, ne-
cessitating the use of inference to fill in
gaps. Prior work has shown how to make
use of a large text corpus to augment ran-
dom walk inference over KBs. We present
two improvements to the use of such large
corpora to augment KB inference. First,
we present a new technique for combin-
ing KB relations and surface text into a
single graph representation that is much
more compact than graphs used in prior
work. Second, we describe how to incor-
porate vector space similarity into random
walk inference over KBs, reducing the fea-
ture sparsity inherent in using surface text.
This allows us to combine distributional
similarity with symbolic logical inference
in novel and effective ways. With exper-
iments on many relations from two sepa-
rate KBs, we show that our methods sig-
nificantly outperform prior work on KB
inference, both in the size of problem our
methods can handle and in the quality of
predictions made.
1 Introduction
Much work in recent years has gone into the
construction of large knowledge bases, either
by collecting contributions from many users,
as with Freebase (Bollacker et al., 2008) and
?
Research carried out while at the Machine Learning
Department, Carnegie Mellon University.
DBPedia (Mendes et al., 2012), or automat-
ically from web text or other resources, as
done by NELL (Carlson et al., 2010) and
YAGO (Suchanek et al., 2007). These knowl-
edge bases contain millions of real-world enti-
ties and relationships between them. However,
even though they are very large, they are still
very incomplete, missing large fractions of possi-
ble relationships between common entities (West
et al., 2014). Thus the task of inference over
these knowledge bases, predicting new relation-
ships simply by examining the knowledge base it-
self, has become increasingly important.
A promising technique for inferring new re-
lation instances in a knowledge base is random
walk inference, first proposed by Lao and Cohen
(2010). In this method, called the Path Ranking
Algorithm (PRA), the knowledge base is encoded
as a graph, and random walks are used to find
paths that connect the source and target nodes of
relation instances. These paths are used as features
in a logistic regression classifier that predicts new
instances of the given relation. Each path can be
viewed as a horn clause using knowledge base re-
lations as predicates, and so PRA can be thought
of as a kind of discriminatively trained logical in-
ference.
One major deficiency of random walk inference
is the connectivity of the knowledge base graph?
if there is no path connecting two nodes in the
graph, PRA cannot predict any relation instance
between them. Thus prior work has introduced the
use of a text corpus to increase the connectivity of
the graph used as input to PRA (Lao et al., 2012;
Gardner et al., 2013). This approach is not without
its own problems, however. Whereas knowledge
base relations are semantically coherent and dif-
ferent relations have distinct meanings, this is not
397
true of surface text. For example, ?The Nile flows
through Cairo? and ?The Nile runs through Cairo?
have very similar if not identical meaning. Adding
a text corpus to the inference graph increases con-
nectivity, but it also dramatically increases feature
sparsity.
We introduce two new techniques for making
better use of a text corpus for knowledge base in-
ference. First, we describe a new way of incor-
porating the text corpus into the knowledge base
graph that enables much more efficient process-
ing than prior techniques, allowing us to approach
problems that prior work could not feasibly solve.
Second, we introduce the use of vector space sim-
ilarity in random walk inference in order to reduce
the sparsity of surface forms. That is, when fol-
lowing a sequence of edge types in a random walk
on a graph, we allow the walk to follow edges that
are semantically similar to the given edge types,
as defined by some vector space embedding of the
edge types. If a path calls for an edge of type
?flows through?, for example, we accept other
edge types (such as ?runs through?) with probabil-
ity proportional to the vector space similarity be-
tween the two edge types. This lets us combine
notions of distributional similarity with symbolic
logical inference, with the result of decreasing the
sparsity of the feature space considered by PRA.
We show with experiments using both the NELL
and Freebase knowledge bases that this method
gives significantly better performance than prior
approaches to incorporating text data into random
walk inference.
2 Graph Construction
Our method for knowledge base inference, de-
scribed in Section 3, performs random walks over
a graph to obtain features for a logistic regression
classifier. Prior to detailing that technique, we first
describe how we produce a graph G = (N , E ,R)
from a set of knowledge base (KB) relation in-
stances and a set of surface relation instances ex-
tracted from a corpus. Producing a graph from
a knowledge base is straightforward: the set of
nodes N is made up of the entities in the KB; the
set of edge types R is the set of relation types in
the KB, and the typed edges E correspond to re-
lation instances from the KB, with one edge of
type r connecting entity nodes for each (n
1
, r, n
2
)
triple in the KB. Less straightforward is how to
construct a graph from a corpus, and how to con-
nect that graph to the KB graph. We describe our
methods for each of those below.
To create a graph from a corpus, we first prepro-
cess the corpus to obtain a collection of surface
relations, such as those extracted by open infor-
mation extraction systems like OLLIE (Mausam et
al., 2012). These surface relations consist of a pair
of noun phrases in the corpus, and the verb-like
connection between them (either an actual verb,
as done by Talukdar et al. (2012), a dependency
path, as done by Riedel et al. (2013), or OpenIE
relations (Mausam et al., 2012)). The verb-like
connections are naturally represented as edges in
the graph, as they have a similar semantics to the
knowledge base relations that are already repre-
sented as edges. We thus create a graph from these
triples exactly as we do from a KB, with nodes cor-
responding to noun phrase types and edges corre-
sponding to surface relation triples.
So far these two subgraphs we have created
are entirely disconnected, with the KB graph con-
taining nodes representing entities, and the sur-
face relation graph containing nodes representing
noun phrases, with no edges between these noun
phrases and entities. We connect these two graphs
by making use of the ALIAS relation in the KB,
which links entities to potential noun phrase ref-
erents. Each noun phrase in the surface relation
graph is connected to those entity nodes which the
noun phrase can possibly refer to according to the
KB. These edges are not the output of an entity
linking system, as done by Lao et al. (2012), but
express instead the notion that the noun phrase can
refer to the KB entity. The use of an entity linking
system would certainly allow a stronger connec-
tion between noun phrase nodes and entity nodes,
but it would require much more preprocessing and
a much larger graph representation, as each men-
tion of each noun phrase would need its own node,
as opposed to letting every mention of the same
noun phrase share the same node. This graph rep-
resentation allows us to add tens of millions of sur-
face relations to a graph of tens of millions of KB
relations, and perform all of the processing on a
single machine.
As will be discussed in more detail in Section 4,
we also allow edge types to optionally have an as-
sociated vector that ideally captures something of
the semantics of the edge type.
Figure 1 shows the graph constructions used in
our experiments on a subset of KB and surface re-
398
KB Relations:
(Monongahela, RIVERFLOWSTHROUGHCITY, Pittsburgh)
(Pittsburgh, ALIAS, ?Pittsburgh?)
(Pittsburgh, ALIAS, ?Steel City?)
(Monongahela, ALIAS, ?Monongahela River?)
(Monongahela, ALIAS, ?The Mon?)
Surface Relations:
(?The Mon?, ?flows through?, ?Steel City?)
(?Monongahela River?, ?runs through?, ?Pittsburgh?)
Embeddings:
?flows through?: [.2, -.1, .9]
?runs through?: [.1, -.3, .8]
(a) An example data set.
(c) An example graph that replaces surface relations with a
cluster label, as done by Gardner et al. (2013). Note, how-
ever, that the graph structure differs from that prior work;
see Section 5.
(b) An example graph that combines a KB and surface rela-
tions.
(d) An example graph that uses vector space representations
of surface edges, as introduced in this paper.
Figure 1: Example graph construction as used in the experiments in this paper. A graph using only KB
edges is simply a subset of these graphs containing only the RIVERFLOWSTHROUGHCITY edge, and is
not shown.
lations. Note that Figures 1b and 1c are shown as
rough analogues of graphs used in prior work (de-
scribed in more detail in Section 5), and we use
them for comparison in our experiments.
3 The Path Ranking Algorithm
We perform knowledge base inference using the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010). We begin this section with a brief overview
of PRA, then we present our modification to the
PRA algorithm that allows us to incorporate vector
space similarity into random walk inference.
PRA can be thought of as a method for exploit-
ing local graph structure to generate non-linear
feature combinations for a prediction model. PRA
generates a feature matrix over pairs of nodes in
a graph, then uses logistic regression to classify
those node pairs as belonging to a particular rela-
tion.
More formally, given a graph G with nodes N ,
edges E , and edge labelsR, and a set of node pairs
(s
i
, t
i
) ? D, one can create a connectivity matrix
where rows correspond to node pairs and columns
correspond to edge lables. PRA augments this
matrix with additional columns corresponding to
sequences of edge labels, called path types, and
changes the cell values from representing the pres-
ence of an edge to representing the specificity of
the connection that the path type makes between
the node pair.
Because the feature space considered by PRA
is so large (the set of all possible edge label se-
quences, with cardinality
?
l
i=1
|R|
i
, assuming a
bound l on the maximum path length), the first
step PRA must perform is feature selection, which
is done using random walks over the graph. The
second step of PRA is feature computation, where
each cell in the feature matrix is computed using
a constrained random walk that follows the path
type corresponding to the feature. We now explain
each of these steps in more detail.
Feature selection finds path types pi that are
likely to be useful in predicting new instances of
the relation represented by the input node pairs .
These path types are found by performing random
walks on the graph G starting at the source and
target nodes in D, recording which paths connect
some source node with its target. The edge se-
quences are ranked by frequency of connecting a
source node to a corresponding target node, and
the top k are kept.
Feature computation. Once a set of path types
399
is selected as features, the next step of the PRA
algorithm is to compute a value for each cell in the
feature matrix, corresponding to a node pair and a
path type. The value computed is the probability
of arriving at the target node of a node pair, given
that a random walk began at the source node and
was constrained to follow the path type: p(t|s, pi).
Once these steps have been completed, the re-
sulting feature matrix can be used with whatever
model or learning algorithm is desired; in this and
prior work, simple logistic regression has been
used as the prediction algorithm.
4 Vector space random walks
Our modifications to PRA are confined entirely to
the feature computation step described above; fea-
ture selection (finding potentially useful sequences
of edge types) proceeds as normal, using the sym-
bolic edge types. When computing feature val-
ues, however, we allow a walk to follow an edge
that is semantically similar to the edge type in the
path, as defined by Euclidean distance in the vec-
tor space.
More formally, consider a path type pi. Re-
call that pi is a sequence of edge types <
e
1
, e
2
, . . . , e
l
>, where l is the length of the path;
we will use pi
i
to denote the i
th
edge type in the
sequence. To compute feature values, PRA begins
at some node and follows edges of type pi
i
until
the sequence is finished and a target node has been
reached. Specifically, if a random walk is at a node
n with m outgoing edge types {e
1
, e
2
, . . . , e
m
},
PRA selects the edge type from that set which
matches pi
i
, then selects uniformally at random
from all outgoing edges of that type. If there is
no match in the set, the random walk restarts from
the original start node.
We modify the selection of which edge type to
follow. When a random walk is at a node n with
m outgoing edge types {e
1
, e
2
, . . . , e
m
}, instead
of selecting only the edge type that matches pi
i
,
we allow the walk to select instead an edge that
is close to pi
i
in vector space. For each edge type
at node n, we select the edge with the following
probability:
p(e
j
|pi
i
) ? exp(??v(e
j
) ?v(pi
i
)), ?j, 1 ? j ? m
where v(?) is a function that returns the vector
representation of an edge type, and ? is a spiki-
ness parameter that determines how much weight
to give to the vector space similarity. As ? ap-
proaches infinity, the normalized exponential ap-
proximates a delta function on the closest edge
type to pi
i
, in {e
1
, e
2
, . . . , e
m
}. If pi
i
is in the set
of outgoing edges, this algorithm converges to the
original PRA.
However, if pi
i
is not in the set of outgoing edge
types at a node and all of the edge types are very
dissimilar to pi
i
, this algorithm (with ? not close to
infinity) will lead to a largely uniform distribution
over edge types at that node, and no way for the
random walk to restart. To recover the restart be-
havior of the original PRA, we introduce an addi-
tional restart parameter?, and add another value to
the categorical distribution before normalization:
p(restart|pi
i
) ? exp(? ? ?)
When this restart type is selected, the random
walk begins again, following pi
1
starting at the
source node. With ? set to a value greater than the
maximum similarity between (non-identical) edge
type vectors, and ? set to infinity, this algorithm
exactly replicates the original PRA.
Not all edge types have vector space representa-
tions: the ALIAS relation cannot have a meaning-
ful vector representation, and we do not use vec-
tors to represent KB relations, finding that doing
so was not useful in practice (which makes intu-
itive sense: KB relations are already latent repre-
sentations themselves). While performing random
walks, if pi
i
has no vector representation, we fall
back to the original PRA algorithm for selecting
the next edge.
We note here that when working with vector
spaces it is natural to try clustering the vectors to
reduce the parameter space. Each path type pi is
a feature in our model, and if two path types dif-
fer only in one edge type, and the differing edge
types have very similar vectors, the resultant fea-
ture values will be essentially identical for both
path types. It seems reasonable that running a
simple clustering algorithm over these path types,
to reduce the number of near-duplicate features,
would improve performance. We did not find this
to be the case, however; all attempts we made to
use clustering over these vectors gave performance
indistinguishable from not using clustering. From
this we conclude that the main issue hindering per-
formance when using PRA over these kinds of
graphs is one of limited connectivity, not one of
too many parameters in the model. Though the
400
feature space considered by PRA is very large, the
number of attested features in a real graph is much
smaller, and it is this sparsity which our vector
space methods address.
5 Related Work
Knowledge base inference. Random walk infer-
ence over knowledge bases was first introduced by
Lao and Cohen (2010). This work was improved
upon shortly afterward to also make use of a large
corpus, by representing the corpus as a graph and
connecting it to the knowledge base (Lao et al.,
2012). Gardner et al. (2013) further showed that
replacing surface relation labels with a represen-
tation of a latent embedding of the relation led
to improved prediction performance. This result
is intuitive: the feature space considered by PRA
is exponentially large, and surface relations are
sparse. The relations ?[river] flows through [city]?
and ?[river] runs through [city]? have near iden-
tical meaning, and both should be very predic-
tive for the knowledge base relation RIVERFLOW-
STHROUGHCITY. However, if one of these rela-
tions only appears in the training data and the other
only appears in the test data, neither will be useful
for prediction. Gardner et al. (2013) attempted to
solve this issue by finding a latent symbolic repre-
sentation of the surface relations (such as a cluster-
ing) and replacing the edge labels in the graph with
these latent representations. This makes it more
likely for surface relations seen in training data to
also be seen at test time, and naturally improved
performance.
This representation, however, is still brittle, as
it is still a symbolic representation that is prone to
mismatches between training and test data. If the
clustering algorithm used is too coarse, the fea-
tures will not be useful, and if it is too fine, there
will be more mismatches. Also, verbs that are on
the boundaries of several clusters are problematic
to represent in this manner. We solve these prob-
lems by modifying the PRA algorithm to directly
use vector representations of edge types during the
random walk inference.
These two prior techniques are the most directly
related work to what we present in this paper, and
we compare our work to theirs.
Graph construction. In addition to the incor-
poration of vector space similarity into the PRA
algorithm, the major difference between our work
and the prior approaches mentioned above is in the
construction of the graph used by PRA. We con-
trast our method of graph construction with these
prior approaches in more detail below.
Lao et al. (2012) represent every word of ev-
ery sentence in the corpus as a node in the graph,
with edges between the nodes representing depen-
dency relationships between the words. They then
connect this graph to the KB graph using a simple
entity linking system (combined with coreference
resolution). The resultant graph is enormous, such
that they needed to do complex indexing on the
graph and use a cluster of 500 machines to per-
form the PRA computations. Also, as the edges
represent dependency labels, not words, with this
graph representation the PRA algorithm does not
have access to the verbs or other predicative words
that appear in the corpus, which frequently express
relations. PRA only uses edge types as feature
components, not node types, and so the rich infor-
mation contained in the words is lost. This graph
construction also would not allow the incorpora-
tion of vector space similarity that we introduced,
as dependency labels do not lend themselves well
to vector space representations.
Gardner et al. (2013) take an approach very sim-
ilar to the one presented in Section 2, preprocess-
ing the corpus to obtain surface relations. How-
ever, instead of creating a graph with nodes rep-
resenting noun phrases, they added edges from
the surface relations directly to the entity nodes
in the graph. Using the ALIAS relation, as we do,
they added an edge between every possible con-
cept pair that could be represented by the noun
phrases in a surface relation instance. This leads
to some nonsensical edges added to the graph,
and if the ALIAS relation has high degree (as it
does for many common noun phrases in Freebase),
it quickly becomes unscalable?this method of
graph construction runs out of disk space when
attempting to run on the Freebase experiments in
Section 6. Also, in conflating entity nodes in the
graph with noun phrases, they lose an important
distinction that turns out to be useful for predic-
tion, as we discuss in Section 6.4.
1
1
Recent notions of ?universal schema? (Riedel et al.,
2013) also put KB entities and noun phrases into the same
conceptual space, though they opt for using noun phrases in-
stead of the KB entities used by Gardner et al. In general
this is problematic, as it relies on some kind of entity linking
system as preprocessing, and cannot handle common noun
references of proper entities without losing information. Our
method, and that of Lao et al., skirts this issue entirely by not
trying to merge KB entities with noun phrases.
401
Other related work. Also related to the present
work is recent research on programming lan-
guages for probabilistic logic (Wang et al., 2013).
This work, called ProPPR, uses random walks to
locally ground a query in a small graph before per-
forming propositional inference over the grounded
representation. In some sense this technique is
like a recursive version of PRA, allowing for more
complex inferences than a single iteration of PRA
can make. However, this technique has not yet
been extended to work with large text corpora, and
it does not yet appear to be scalable enough to han-
dle the large graphs that we use in this work. How
best to incorporate the work presented in this pa-
per with ProPPR is an open, and very interesting,
question.
Examples of other systems aimed at reason-
ing over common-sense knowledge are the CYC
project (Lenat, 1995) and ConceptNet (Liu and
Singh, 2004). These common-sense resources
could easily be incorporated into the graphs we
use for performing random walk inference.
Lines of research that seek to incorporate dis-
tributional semantics into traditional natural lan-
guage processing tasks, such as parsing (Socher
et al., 2013a), named entity recognition (Passos et
al., 2014), and sentiment analysis (Socher et al.,
2013b), are also related to what we present in this
paper. While our task is quite different from these
prior works, we also aim to combine distributional
semantics with more traditional methods (in our
case, symbolic logical inference), and we take in-
spiration from these methods.
6 Experiments
We perform both the feature selection step and the
feature computation step of PRA using GraphChi,
an efficient single-machine graph processing li-
brary (Kyrola et al., 2012). We use MAL-
LET?s implementation of logistic regression, with
both L1 and L2 regularization (McCallum, 2002).
To obtain negative evidence, we used a closed
world assumption, treating any (source, target)
pair found during the feature computation step as
a negative example if it was not given as a positive
example. We tuned the parameters to our methods
using a coarse, manual grid search with cross vali-
dation on the training data described below. The
parameters we tuned were the L1 and L2 regu-
larization parameters, how many random walks to
perform in the feature selection and computation
NELL Freebase
Entities 1.2M 20M
Relation instances 3.4M 67M
Total relation types 520 4215
Relation types tested 10 24
Avg. instances/relation 810 200
SVO triples used 404k 28M
Table 1: Statistics of the data used in our experi-
ments.
steps of PRA, and spikiness and restart parameters
for vector space walks. The results presented were
not very sensitive to changes in these parameters.
6.1 Data
We ran experiments on both the NELL and Free-
base knowledge bases. The characteristics of these
knowledge bases are shown in Table 1. The Free-
base KB is very large; to make it slightly more
manageable we filtered out relations that did not
seem applicable to relation extraction, as well as a
few of the largest relations.
2
This still left a very
large, mostly intact KB, as can be seen in the ta-
ble. For our text corpus, we make use of a set of
subject-verb-object triples extracted from depen-
dency parses of ClueWeb documents (Talukdar et
al., 2012). There are 670M such triples in the
data set, most of which are completely irrelevant to
the knowledge base relations we are trying to pre-
dict. For each KB, we filter the SVO triples, keep-
ing only those which can possibly connect training
and test instances of the relations we used in our
experiments. The number of SVO triples kept for
each KB is also shown in Table 1. We obtained
vector space representations of these surface rela-
tions by running PCA on the SVO matrix.
We selected 10 NELL relations and 24 Free-
base relations for testing our methods. The NELL
relations were hand-selected as the relations with
the largest number of known instances that had a
reasonable precision (the NELL KB is automati-
cally created, and some relations have low preci-
sion). We split the known instances of these rela-
tions into 75% training and 25% testing, giving on
average about 650 training instances and 160 test
2
We removed anything under /user, /common, /type (ex-
cept for the relation /type/object/type), /base, and /freebase,
as not applicable to our task. We also removed relations deal-
ing with individual music tracks, book editions, and TV epid-
sodes, as they are very large, very specific, and unlikely to be
useful for predicting the relations in our test set.
402
instances for each relation.
The 24 Freebase relations were semi-randomly
selected. We first filtered the 4215 relations based
on two criteria: the number of relation instances
must be between 1000 and 10000, and there must
be no mediator in the relation.
3
Once we selected
the relations, we kept all instances of each rela-
tion that had some possible connection in the SVO
data.
4
This left on average 200 instances per rela-
tion, which we again split 75%-25% into training
and test sets.
6.2 Methods
The methods we compare correspond to the graphs
shown in Figure 1. The KB method uses the orig-
inal PRA algorithm on just the KB relations, as
presented by Lao and Cohen (2010). KB + SVO
adds surface relations to the graph (Figure 1b). We
present this as roughly analogous to the methods
introduced by Lao et al. (2012), though with some
significant differences in graph representation, as
described in Section 5. KB + Clustered SVO fol-
lows the methods of Gardner et al. (2013), but us-
ing the graph construction introduced in this pa-
per (Figure 1c; their graph construction techniques
would have made graphs too large to be feasible
for the Freebase experiments). KB + Vector SVO
is our method (Figure 1d).
6.3 Evaluation
As evaluation metrics, we use mean average pre-
cision (MAP) and mean reciprocal rank (MRR),
following recent work evaluating relation extrac-
tion performance (West et al., 2014). We test sig-
nificance using a paired permutation test.
The results of these experiments are shown in
Table 2 and Table 3. In Table 4 we show average
precision for every relation tested on the NELL
KB, and we show the same for Freebase in Table 5.
6.4 Discussion
We can see from the tables that KB + Vector SVO
(the method presented in this paper) significantly
outperforms prior approaches in both MAP and
3
A mediator in Freebase is a reified relation in-
stance meant to handle n-ary relations, for instance
/film/performance. PRA in general, and our implementation
of it in particular, needs some modification to be well-suited
to predicting relations with mediators.
4
We first tried randomly selecting instances from these re-
lations, but found that the probability of selecting an instance
that benefited from an SVO connection was negligible. In or-
der to make use of the methods we present, we thus restricted
ourselves to only those that had a possible SVO connection.
Method MAP MRR
KB 0.193 0.635
KB + SVO 0.218 0.763
KB + Clustered SVO 0.276 0.900
KB + Vector SVO 0.301 0.900
Table 2: Results on the NELL knowledge base.
The bolded line is significantly better than all other
results with p < 0.025.
Method MAP MRR
KB 0.278 0.614
KB + SVO 0.294 0.639
KB + Clustered SVO 0.326 0.651
KB + Vector SVO 0.350 0.670
Table 3: Results on the Freebase knowledge base.
The bolded line is significantly better than all other
results with p < 0.0002.
MRR. We believe that this is due to the reduction
in feature sparsity enabled by using vector space
instead of symbolic representations (as that is the
only real difference between KB + Clustered SVO
and KB + Vector SVO), allowing PRA to make
better use of path types found in the training data.
When looking at the results for individual relations
in Table 4 and Table 5, we see that KB + Vector
SVO outperforms other methods on the majority
of relations, and it is a close second when it does
not.
We can also see from the results that mean av-
erage precision seems a little low for all meth-
ods tested. This is because MAP is computed as
the precision of all possible correct predictions in
a ranked list, where precision is counted as 0 if
the correct prediction is not included in the list.
In other words, there are many relation instances
in our randomly selected test set that are not in-
ferrable from the knowledge base, and the low re-
call hurts the MAP metric. MRR, which judges the
precision of the top prediction for each relation,
gives us some confidence that the main issue here
is one of recall, as MRR is reasonably high, es-
pecially on the NELL KB. As further evidence, if
we compute average precision for each query node
(instead of for each relation), excluding queries for
which the system did not return any predictions,
MAP ranges from .29 (KB) to .45 (KB + Vector
SVO) on NELL (with around 30% of queries hav-
ing no prediction), and from .40 (KB) to .49 (KB +
403
Relation KB KB + SVO KB + Clustered SVO KB + Vector SVO
ActorStarredInMovie 0.000 0.032 0.032 0.037
AthletePlaysForTeam 0.200 0.239 0.531 0.589
CityLocatedInCountry 0.126 0.169 0.255 0.347
JournalistWritesForPublication 0.218 0.254 0.291 0.319
RiverFlowsThroughCity 0.000 0.001 0.052 0.076
SportsTeamPositionForSport 0.217 0.217 0.178 0.180
StadiumLocatedInCity 0.090 0.156 0.275 0.321
StateHasLake 0.000 0.000 0.000 0.000
TeamPlaysInLeague 0.934 0.936 0.947 0.939
WriterWroteBook 0.144 0.179 0.195 0.202
Table 4: Average precision for each relation tested on the NELL KB. The best performing method on
each relation is bolded.
Relation KB KB + SVO KB + C-SVO KB + V-SVO
/amusement parks/park/rides 0.000 0.009 0.004 0.013
/architecture/architect/structures designed 0.072 0.199 0.257 0.376
/astronomy/constellation/contains 0.004 0.017 0.000 0.008
/automotive/automotive class/examples 0.003 0.001 0.002 0.006
/automotive/model/automotive class 0.737 0.727 0.742 0.768
/aviation/airline/hubs 0.322 0.286 0.298 0.336
/book/literary series/author s 0.798 0.812 0.818 0.830
/computer/software genre/software in genre 0.000 0.001 0.001 0.001
/education/field of study/journals in this discipline 0.001 0.003 0.003 0.001
/film/film/rating 0.914 0.905 0.914 0.905
/geography/island/body of water 0.569 0.556 0.580 0.602
/geography/lake/basin countries 0.420 0.361 0.409 0.437
/geography/lake/cities 0.111 0.134 0.177 0.175
/geography/river/cities 0.030 0.038 0.045 0.066
/ice hockey/hockey player/hockey position 0.307 0.243 0.222 0.364
/location/administrative division/country 0.989 0.988 0.991 0.989
/medicine/disease/symptoms 0.061 0.078 0.068 0.067
/medicine/drug/drug class 0.169 0.164 0.135 0.157
/people/ethnicity/languages spoken 0.134 0.226 0.188 0.223
/spaceflight/astronaut/missions 0.010 0.186 0.796 0.848
/transportation/bridge/body of water spanned 0.534 0.615 0.681 0.727
/tv/tv program creator/programs created 0.164 0.179 0.163 0.181
/visual art/art period movement/associated artists 0.044 0.040 0.046 0.037
/visual art/visual artist/associated periods or movements 0.276 0.295 0.282 0.290
Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on
each relation is bolded. For space considerations, ?Clustered SVO? is shortened to ?C-SVO? and ?Vector
SVO? is shortened to ?V-SVO? in the table header.
404
Vector SVO) on Freebase, (where 21% of queries
gave no prediction). Our methods thus also im-
prove MAP when calculated in this manner, but it
is not an entirely fair metric,
5
so we use standard
MAP to present our main results.
One interesting phenomenon to note is
a novel use of the ALIAS relation in some
of the relation models. The best exam-
ple of this was found with the relation
/people/ethnicity/languages spoken. A
high-weighted feature when adding surface
relations was the edge sequence <ALIAS, ALIAS
INVERSE>. This edge sequence reflects the
fact that languages frequently share a name
with the group of people that speaks them (e.g.,
Maori, French). And because PRA can gen-
erate compositional features, we also find the
following edge sequence for the same relation:
</people/ethnicity/included in group,
ALIAS, ALIAS INVERSE>. This feature captures
the same notion that languages get their names
from groups of people, but applies it to subgroups
within an ethnicity. These features would be
very difficult, perhaps impossible, to include in
systems that do not distinguish between noun
phrases and knowledge base entities, such as
the graphs constructed by Gardner et al. (2013),
or typical relation extraction systems, which
generally only work with noun phrases after
performing a heuristic entity linking.
7 Conclusion
We have offered two main contributions to the task
of knowledge base inference. First, we have pre-
sented a new technique for combining knowledge
base relations and surface text into a single graph
representation that is much more compact than
graphs used in prior work. This allowed us to ap-
ply methods introduced previously to much larger
problems, running inference on a single machine
over the entire Freebase KB combined with tens of
millions of surface relations. Second, we have de-
scribed how to incorporate vector space similarity
into random walk inference over knowledge bases,
reducing the feature sparsity inherent in using sur-
face text. This allows us to combine distributional
similarity with symbolic logical inference in novel
and effective ways. With experiments on many
5
MAP is intended to include some sense of recall, but ex-
cluding queries with no predictions removes that and opens
the metric to opportunistic behavior.
relations from two separate knowledge bases, we
have shown that our methods significantly outper-
form prior work on knowledge base inference.
The code and data used in the ex-
periments in this paper are available at
http://rtw.ml.cmu.edu/emnlp2014 vector space pra/.
Acknowledgments
This research has been supported in part by
DARPA under contract number FA8750-13-2-
0005, by NSF under grant 31043,18,1121946, and
by generous support from Yahoo! and Google.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of SIGMOD.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
inference in a large knowledge-base using latent
syntactic cues. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin.
2012. Graphchi: Large-scale graph computation
on just a pc. In Proceedings of the 10th USENIX
Symposium on Operating Systems Design and Im-
plementation (OSDI), pages 31?46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53?67.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
Douglas B Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
Hugo Liu and Push Singh. 2004. Conceptnet: a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211?226.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523?534. Asso-
ciation for Computational Linguistics.
405
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Pablo N. Mendes, Max Jakob, and Christian Bizer.
2012. Dbpedia for nlp: A multilingual cross-domain
knowledge base. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC?12).
Alexandre Passos, Vineet Kumar, and Andrew Mc-
Callum. 2014. Lexicon infused phrase embed-
dings for named entity resolution. arXiv preprint
arXiv:1404.5367.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Acquiring temporal constraints be-
tween relations. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 992?1001. ACM.
William Yang Wang, Kathryn Mazaitis, and
William W. Cohen. 2013. Programming with
personalized pagerank: A locally groundable
first-order probabilistic logic. In Proceedings of the
22Nd ACM International Conference on Conference
on Information &#38; Knowledge Management,
CIKM ?13, pages 2129?2138, New York, NY, USA.
ACM.
Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In WWW.
406
