A Client/Server Architecture for Word Sense Disambiguation 
Caroline Brun 
Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240 Meylan France 
Caroline.Brun @ xrce.xerox.com 
Abstract 
This paper presents a robust client/server implemen- 
tation of a word sense disambiguator for English. 
This system associates a word with its meaning in 
a given context using dictionaries as tagged corpora 
in order to extract semantic disambiguation rules. 
Semantic rules are used as input of a semantic appli- 
cation program which encodes a linguistic strategy 
in order to select he best disambiguation rule for 
the word to be disambiguated. The semantic dis- 
ambiguation rule application program is part of the 
client/server a chitecture enabling the processing of 
large corpora. 
1 Introduction 
This paper describes the implementation f an on- 
line lexical semantic disambiguation system for En- 
glish within a client/server linguistic application. 
This system allows to select he meaning of a word 
given its context of appearance in a text segment, 
and addresses the general problem of Word Sense 
Disambiguation (WSD), (Ide et a190), (Gale et al 
92), (Gale et al 93), (Leacock et al93), (Yarowsky 
95), (Ng et al 96), (Resnik et al 97), (Vdronis et al 
98) and (Wilks et al 98). 
The basic idea of the semantic disambiguation 
system described here is to use a dictionary, in 
our case, the Oxford-Hachette bilingual dictionary 
(OHFD), (Oxford 94), a bilingual English/French 
French/English dictionary designed initially for hu- 
mans but stored in SGML format, in order to extract 
a semantic disambiguation rule database. The dic- 
tionary is in effect used as a semantically tagged 
corpus. 
Once the semantic disambiguation database is avail- 
able, it becomes, as well as a dictionary and an on- 
tology, a resoume used by the server to perform 
WSD on new input. A linguistic strategy was im- 
plemented in order to select he best matching dis- 
ambiguation rule in a given context. 
This implementation is a follow-up of the Seman- 
tic Dictionary Lookup (SDL) aheady implemented 
in this client/server system (Aimelet 98) and o1' the 
methods proposed in (Dini et al 98) and (Dini 
et al 99). The originality of our implementation 
lies in the rule selection strategy for application as 
well as in the use of the client/server characteristics 
to perform WSD. After a brief presentation of the 
client/server characteristics, we examine the imple- 
mentation of the WSD system. Then we describe 
the results obtained after evaluation of the system, 
and finally we conclude with the description of its 
applications and perspectives. 
2 Architecture of the system 
2.1 XeLDa" a linguistic client/server 
application 
XeLDa addresses the problem of a generic de- 
veloplnent l'ramework for linguistic-based and 
linguistics-enriched applications, based on avail- 
able, as well as future research results. Potential 
applications include: translation aids over a net- 
work, on a desktop or a portable PC, syntax check- 
ing, terminology extraction, and authoring tools in 
general. This system provides developers and re- 
searchers with a common development architecture 
for the open and seamless integration of linguis- 
tic services. XeLDa offers different services uch 
as dictionary lookup, tokenization, tagging, shallow 
parsing, etc. 
Dictionary lookup and shallow parsing are ex- 
tensively used in the semantic rule extrac- 
tion/application processes described in this paper. 
2.2 Dictionary Lookup 
The OHFD dictionary is accessible via the XeLDa 
server, which allows a fast and easy lookup of 
words. Each entry in the OHFD dictionary (cf. 
Figl, entry of seize in SGML format) is organized in 
different levels (Akroyd 92), corresponding to syn- 
tactic ategories ( <S 1 >... </S 1 >, S 1 =part of speech 
132 
distinction J), which are themselves divided into se- 
nmntic categories (<$2> ... </$2>, the senses we are 
interested in), themselves divided into several trans- 
lations (<TR> ... </TR>). 
<SE> 
<HW>seize</HW> 
<HG><PR><PH>si:z</PH></PR></HG> 
<SI><OI><PS>vtr</PS></OI> 
<S2><02><LA>Iit</LA> 
<IC>take hold of</IC></02> 
<TR>saisir<CO>person, 
object</CO></TR> 
<TR><LE>to seize sb around the 
waist</LE>saisir qn par 
la taille</TR> 
<TR><LI>to seize hold of</Ll> 
se saisir de<CO>person</CO></TR> 
<TR>s'emparer de<CO>object</CO> 
</TR> 
<TR>sauter  sur<CO>idea</CO></TR> 
</$2> 
<$2><02> 
<LA>f ig</LA><lC>grasp</ IC></02> 
<TR>sa is i r<CO>oppor tun i ty ,  
moment</CO></TR> 
<TR>prendre<CO>in i t ia t ive</CO> 
</TR> 
<TR><LI>to be seized by</LI> 
@tre pris de<CO>emotion, 
pain, fit</CO></TR> 
</$2> 
<S2><02><LA>Mil</LA><LA>Pol</LA> 
<IC>capture</ iC></O2> 
<TR>s 'emparer  de<CO>power,  
territory, hostage, prisoner, 
installation</CO></TR> 
<TR>prendre<CO>control</CO></TR> 
</$2> 
<S2><O2<LA>Jur</LA></02> 
<TR>saisir<CO>arms, drugs, 
property</CO></TR> 
<TR>apprdhender<CO>person</CO> 
</TR> 
</$2> 
</SI> 
<SI><OI> 
<PS>vi</PS></OI> 
<TR><CO>engine, mechanism</CO> 
se gripper</TR> 
</SI> 
</SE> 
Figl : SGML entry of seiee 
\]S l are a bit l'llOl*~ informative tlmn simple part of speech 
since they distinguish also t,ansilivc, imransilivc rellexive 
verbs, past participles, as well as some plural/singular nouns. 
Fine-grained SGML tags mark up different kinds of 
infornmtion related to semantic categories (<$2>) 
and translations, in particular: 
? <C()> ... </CO> mark collocates (typical sub- 
jeers, objects, modiliers,...); 
? <LC> ... </LC> mark compound exalnples as- 
sociated with the headword ; 
? <LI~,> ... </LE> mark general examples used for 
illustration of a word or a phrase; 
? <I,I> ... </I,I> mark idiomatic examples; 
? <I~O> ... </LO> mark examples illustrating an 
obligatory syntactic structure of an entry; 
? <I.U> ... </LU> mark examples of usage; 
? <I,V> ... </IN> mark examples of phrasal verb 
l)attem. 
The recta-semantic information encoded into these 
(\]iffClCtlt SGMI. tags is used to acquire semantic dis- 
,mfl~\]guation rules from the dictionary and guides 
the semantic rule application process, kS explained 
later. 
2.3 Shallow Parser  
The "shanow parsing" technology is based on a 
cascade of finite state transducers which allows 
us to extracl from a sentence its shallow syntactic 
structure (chunks) and its ftmctional relationships 
(AYt ct al. 97). 
The following example illustrates the kind of 
analysis provided by the shallow parser: 
A i~voh,er attd two shotguns wefw sei~ed at 
thepart3~ 
\[SC \[NP A revolver  NP \ ] /SUBJ  and 
\[NP two shotguns NP\ ] /SUBJ  :v were 
se ized SC\] \[PP at the par ty  PP\]. 
SUBJPASS(revolver ,  seize) 
SUBJPASS(shotgun,  seize) 
VMODOBJ (se ize ,a t ,par ty )  
Shallow parser transducers al'C accessible via the 
XcLI)a server enabling fast and robust execution 
(Roux98). 
The syntactic relations used in the disambiguation 
system arc subject-verb, verb-object and modifier. 
Subject-verb relations include cases such as pas- 
sives, rellexive and relative constructions. Modilier 
133 
relations includes nominal, prepositional, adjecti- 
val, and adverbial phrases as well as relative clauses. 
2.4 Rule extractor 
To perform semantic tag assignment using the 
OHFD dictionary, a sense number (Si) is assigned 
to each semantic category (<$2>) of each entry. 
These sense numbers act as semantic tags in the 
process of disambiguation rule application, because 
they directly point to a particular meaning of an 
entry. 
In the context of our OHFD-based implementation, 
sense numbering consists in concatenating the 
homograph number (which is 0 if there are no 
homographs of the entry, or 1, 2, 3 ..... for each 
homograph otherwise), the S1 number, and the $2 
number. For example, the entry seize is composed 
of five distinct senses, respectively numbered 0.I.1, 
0.I.2, 0.I.3, 0.I.4 (for the transitive verb), 0.II.l (for 
the intransitive verb). Such sense numbers allow a 
deterministic retrieval of the semantic ategories of 
a word. 
As in GINGER I (Dini et al 98) and GINGER II 
(Dini et al 99) the acquired rules are of two types: 
word level and/or ambiguity class level. 
The database is built according to the following 
strategy: for each sense number Si of the entry, 
examples are parsed with the shallow parseh and 
functional dependencies are extracted from these 
examples: if a dependency involves the entry 
lemma (headword), a semantic disambiguation rule 
is built. It can be paraphrased as: 
If the lemma X, which is ambiguous between $1, 
$2, ..., S~, appears in the dependency DEP(X,Y) 
or DEp(Y,X) then it can be disambiguated by 
assigning the sense Si. 
Such roles ate word level roles, because they 
match the lexical context. 
For each sense number again, collocates am used 
to build semantic rules. The type of dependency 
illustrated by a collocate of an entry is SGML-tagged 
in the OHFD 2, and is directly exploited to build 
rules in the same way. 
Then, for each rnle already built, semantic lasses 
from an ontology (in our case, WordNet 3, (Fell- 
2For example, a collocate in a verb entry describes either a 
SUBJ or an OBJ dependency depending on its SGML tag 
3Since WordNet classes are relatively poor for adjectives 
and adverbs, additional infommtion about adjectival and adver- 
bial classes is extracted fi'om a general thesaurus, the Roget. 
baum 98)) are used to generalize the scope of the 
rules: the non-headword argument of functional 
dependencies is replaced in the rule by its selnan- 
tic classes. The resulting rule can be paraphrased as: 
If the lemma X, which is ambiguous between 
St, $2, ..., Sn, appears in the dependency DEP(X, 
ambiguityclass(Y)) or DEl'(mnbiguity_class(Y), 
X) then it can be disambiguated by assigning the 
sense Si. 
Such rules are class level rules, because they 
match the semantic context rather than lexical 
items. In both cases, the type of the role (<LC>, 
<LE>, <LI>, <LO>, <LU>, <LV>, <CO>) is kept and 
encoded into the rules. 
Fox" example, from the last semantic category of 
seize, 0.I.l, the system built the following word 
level rules: 
SUBJ(engine,seize) ~ 0.l.1 <CO>; 
SUBJ(mechanism,seize) =~ 0.I.1 <CO>; 
Since engine belongs to the classes number 
6 (noun.artifact) and 19 (noun.phenomenon), 
whereas mechanism belongs to the classes num- 
ber 6, 4 (noun.act), 17 (noun.object), and 22 
(noun.process), corresponding class level rules are: 
SUB J(6/19,seize) =~ 0.I. 1 <CO>; 
SUB.I(4/6/17/22,seize) ~ 0.l.l <CO>; 
All dictionary entries are processed, which allow 
to automatically build a semantic disambiguation 
rule database available to be used by the semantic 
application program to disambiguate unseen texts. 
2.5 Rule application program 
The rule application program matches rules of the 
semantic database against new unseen input text us- 
ing a preference strategy in order to disambiguate 
words on the fly. In cases where the system is not 
able to find any matching rules, it gives as fall back 
result he first meaning corresponding tothe syntac- 
tic part of speech of the word in the sentence. Since 
the OHFD has been built using corpora frequencies, 
the most frequent senses of a word appear first in 
the entry. Therefore, even if there are no matching 
rules, the system gives as result the most probable 
meaning of the word to disambiguate. 
The linguistic strategy used in the application pro- 
gram is shown on several examples. 
134 
2.5.1 Simple rule matching 
Sttppose one wants to disambiguate ile woM seize 
in the sentence: 
Only cg'ter oranges had been served did ,led 
seize the initiative, a scrmmnage pick-zq) e/fort by 
Ronme Kirkl)atriek cancelling out Moore's score. 
The rule application l)rograul lirst extracts the 
functional dependencies by means of the shallow 
parser. The word to be disambiguated has to be 
member of one or more dependencies, ill this case: 
DOBJ(seize,initiative) 
The next step tries to match these dependen- 
cies with one or more rules in the semantic 
disambiguation database. 
If one aud only one role matches the lexical context 
of the dependencies directly, the system uses it to 
disambiguate he word, i.e. to assign the sense 
number Si 4 to it; otherwise, if several rules match 
directly at word level, the selection process uses 
the meta-semantic information encoded in SGMI, 
tags within the dictionary (and kept in the rules oil 
purpose) with the following preference strategy: 
rule built fl'om collocate (<C()>), from compounds 
examples (<LC>), from idiomatic examples (<IA>), 
t'rom structure xamples (<L()>), from phrasal verb 
pattern examples (<IN>), t'rom usage examples 
(<LU>), and finally from general examples (<I,E>). 
As far its implementation is concerned, rules are 
weighted flom 1 to 7 according to their types. 
This strategy relies on the linguistic choices lexi- 
cographers made to build the dictionary and takes 
into account the accuracy of the linguistic type 
el' the examples: it ranges from collocates, which 
encode very typical arguments of prcdicatcs, to 
w'~ry general examples, as such the resulting rules 
are linguistically-based. 
In these particular exmnple, only one lexical rule 
matches the dependency extracted: 
seize: l)OBJ(scize,iuitiative) => 0.I.2 <C()> 
meaning that the sense nmnber alTected to 
seize is 0.1.2. This rule has been built using the 
typical collocate of seize in its 0.I.2 sense, namely 
initiative. The translation associated to this sense 
nmnber of seize in the dictionary is prendre, which 
4possibly translation, depending on the application 
is the desired one in this context. 
2.5.2 Rule competition 
In some casts, many rules may apply to a given 
woM in the same context, therefore we need a rule 
selection strategy. 
Suppose one wants now to disambiguate lhe word 
seize, in the sentence: 
77w police seized a man employed by the Krttgetw- 
dorp branch of the United Building Society on 
approximately 18 May 1985. 
The dependencies extracted by the shallow 
parser which might lead to a disambiguation, i.e. 
which involve seize, are: 
SUBJ(police,seize) 
DO13J(seize,man) 
VMODOBJ(seize,about, 1985) 
VMODOBJ(seize,of, Society) 
VMODOBJ(seize,by,branch) 
In the case of our example, none of Ihe rules 
of the database lnatch directly the lexical context 
of the dependencies. Therefore, the system tries 
to match the selnantic ontext of the dependency. 
To perform this task, the distance between the 
list of semantic lasses o1' a potential rule (El) 
and the list o1' semantic lasses associaled with tile 
non-headword o1' the dependency (L2) is calculated: 
d = (UAI? I ) (UNION(L1,L2) ) - ( ;A IU) ( INT ' I ,g I , : ( I , I , L2) ) )  
U AI~I)( U N IO N ( I A ,L2) ) 
"lb enable fast execution in terms of distance 
calculation, a transducer which associate:~ a word 
with its WoMNet top classes has been built and 
is loaded on the server. The distance calculated 
here ranges from 0 to 1, 0 meaning a fttll match 
of classes, 1 no match at all, the "best" rules 
being the ones with the smallest distance. Ill this 
particular example, the list of classes atlached to 
man in WordNet is used to calculate the distance 
with the potential matching rules. Several rules 
now match the semantic ontext of the dependency 
DOBJ(seize,man). 
After removing rules matching with a distance 
above some threshold, it appears that two potential 
matching rules still compete: 
? one is built using the collocate \[prise,let\]: 
DOBJ(seizc,prisoner) => 0.I.3 <CO>; 
135 
at class level DOBJ(seize, l 8) => 0.I.3 <CO>; 
? the other is built using the example to seize 
somebody around the waist: 
DOBJ(seize,somebody) :=>0.I.l <LE>; 
at class level DOBJ(seize,l 8) => 0.I.1 <LE>; 
Indeed, prisoner and somebody sham the same se- 
mantic WordNet class (18, noun.animate) which is 
a member of the list of classes attached to man as 
well. The following preference strategy is appliedS: 
first, prefer ules from collocate (<CO>), then from 
compounds examples (<LC>), then from structure 
examples (<LO>), then from phrasal verb pattern ex- 
amples (<LV>), then from usage examples (<LU>), 
and then fiom general examples (<LE>). This strat- 
egy allows the selection of the role to apply, here the 
one built with the collocate \[prisoner\]. The sense 
number attached by the system to seize is 0.i.3, 
the general meaning being capture, and the French 
translation s'emparer de. 
In cases where two competing rules are exactly of 
the same type, the system chooses the first one (first 
sense appearing in the entry), relying on the fact that 
the OHFD was built using corpora: by default, se- 
mantic ategories of the entries are ordered accord- 
ing to frequency in corpora. 
2.5.3 Rule cooperation 
The previous example showed how rules can 
compete between each other. But in some cases 
they can cooperate as well. Let's disambiguate 
seize in the following example sentence: 
United Slates federal agents seized a smface- 
to-air rocket launche~; a rocket motto; range-finders 
and a variety of milim O, manuals. 
Since the sentence contains a coordinated irect 
object of seize, one gets the following dependencies 
fiom the shallow parse,: 
DOBJ(seize,launcher) 
DOBJ(seize,motor) 
DOBJ(seize,range-finder) 
DOBJ(seize,manuals) 
Many roles are matching at class level, with a 
given distance d, namely: 
DOBJ(seize,4/6/1 l) =;, 0.I.3 <CO>; d=0.75 
DOBJ(seize,7/24/4/9/26/6/18/10) =5 0.I.3 <CO>; d=0.9 
DOBJ(seize,8/6/14) => 0.I.4 <CO> ;d=0.75 
DOBJ(seize,21/7/15/9/6) :::>0.I.4 <CO> ;d=0.83 
Two rules point out the sense number 0.I.3, 
the two others, the sense number 0.1.4. The strategy 
of role selection takes tiffs fact into account, giving 
inore importance to sense numbers matching many 
times. As far as implementation is concerned, 
the distances associated with roles pointing on 
the same sense number are multiplied together. 
Since distances range from 0 to 1, multiplying 
them decreases the resulting value of the distance. 
Since the lowest one is chosen, the system put the 
emphasis on semantic redundancy. In the example, 
the distance finally associated with sense nmnber 
0.I.4 is 0.6625, which is smaller than the one 
associated with sense number 0.I.3 (0.675). The 
sense number selected by the system is therefore 
0.1.4, the translation being saisir, which is the 
desired one. The stone strategy is implemented 
for word level rules cooperation, in this case, rule 
weights are added. 
2.6 hnplementation 
The different modules of the system presented here 
are ilnplemented in C++ in the XeLDa client/server 
architecture: 
- As aheady mentioned, the rule learner is a silnple 
XeLDa client that performs rule extraction once ; 
- The rule application program is implemented asa 
specific dictionary lookup service: when a word is 
semantically disalnbiguated with a rule, the applica- 
tion program reorders the dictionary entry accord- 
lug to the semantic ategory assigned to the word. 
The best matching part of the entry is then presented 
first. This application is built on top of Locolex 
(Bauer et al 95), an intelligent dictionary lookup 
which achieves ome word sense disambiguation 
using word context (part-of speech and nmltiword 
expressions (MWEs) 6 recognition). However, Lo- 
colex choices remain purely syntactic. Using the 
OHFD information about examples, collocates and 
subcategorization as well as semantic lasses from 
an ontology, the system presented here goes fnrther 
towm'ds emantic disambiguation. 
5At class level, idiomatic examples are not used, because 
the idiomatic expressions given in the dictionary are fully lexi- 
calized 
e'Multiword expressions range flom compounds (salle de 
bain) and fixed phrases (a priori) to idiomatic expressions (to 
sweep something t, nder the rug). 
136 
3 Evaluation 
We ewfluated the system for English on the 34 
words used in the SENSEVAL competition (Kilgar- 
tiff 98; Kilgarriff 99), as well as on the SENSE- 
VAL corpus (HECTOR). This provkled a test set of 
around 8500 sentences. The SENSEVAL words arc 
all polysemous which means that the results given 
below reflect real polysemy. 
We use the SENSEVAL test set for this in vitro ewfl- 
uation in order to give us a mean of comparison, es- 
pecially with the results obtained in tiffs competition 
with GINGER i1 (Dini el al. 99). Still, it is impel 
tant to keep in mind that this comparison is difficult 
since the dictionaries used are different. We used 
the OHF1) bilingual dictionary while in SENSE- 
VAL the Oxford monolingual dictionary fl'om HEC- 
TOR was used. 
The evaluation given below is l)efformed it' and only 
if the semantic disambiguator has found a matching 
rule, which means that tim results focus only on our 
methodology: recall and precision would have been 
better if we had ewduated all outputs (even when 
the resul! is just the first meaning corresponding to
the syntactic part el' speech of the word in the sen- 
tence) because the OHFI) gives by default he most 
frequent meaning of a word. 
The results obtained with the system arc given on 
the following table: 
POS Precision Rccall 
N 83.7 % 27.4 % 
A 81.3 % 55.8 % 
v 75 % 37.6 % 
Global 79.5 % 37.4 % 
Polysemy 
5.4 
5.7 
6.2 
5.8 
Numbers show that the recall is equivalent to the 
one we obtained with GINGER 1I (37.6 %) in SEN- 
SF, VAL (tiffs just means that dictionaries content is 
about the same) but precision is dramatically im- 
proved (46% for GINGER 1I for 79.5% with this 
system). Increase in precision is due to the fact that 
we used more fine-grained ictionary information. 
Moreover, the evaluation shows that the distribu- 
tion of the precision results follows the preference 
strategy employed to select rtfles: collocate rules 
am more precise than examples rules, compounds 
or idiom rules am themselves more precise than us- 
agle exalnples, etc. 
Another ewfluation of smaller coverage has been 
performed on "all polysemous words" of about 400 
sentences extracted flom the T/me,s' newspaper; and 
shows similar results according to part of speech 
distribution. 
POS Precision Recall Polyscm) 
N 81% 28.3 % 5.5 
A 79 % 64 % 5.8 
V 74% 34.5 % 9.8 
Global 78% 36.1% 6.2 
These results confirm that dictionary information is 
very reliable for senmntic disambiguation tasks. 
4 Conclusion and Future  expectat ions 
This paper describes a client/server implementation 
el' a word sense disambiguator. The method uses a 
dictionary as a tagged corpus in order to extract a 
semantic disalnbiguation rule database. Si rice there 
is no need for a tagged training corpus, tim method 
we describe, which performs "all words" semantic 
disambiguation, is unsupervised and avoide; the data 
acquisition bottleneck observed in WS1). Rules are 
available to be used by a semantic application pro- 
gram which uses a specilic linguistic strategy to se- 
lect the best matching rule to apply: the rule selec- 
l ion is based on an SGML typed-based preference 
strategy and takes into account rules competition 
and rule cooperation. 
l~mphasis put on the advantage of the client/server 
implementation i  tin'ms o1' robustness as well kS on 
the good results provided by the strategy in terms of 
recall and precision. The client/server implementa- 
lion provides robustness, modularity and l'a~t execu- 
tion. 
The disambiguation strategy provides hig, h preci- 
sion results, because senses and examples have been 
delined by lexicographers and therel'ore provide a 
reliable linguistic source for constructing a database 
of semantic disambiguation rules. Recall re.suits are 
good as well, meaning that the coverage of the dic- 
tionary is iml)ortant. 
These results could be improved by learning more 
disambiguation rules, for example using the co l  
respondences between functional dependencies: 
when a dependency DOBJ(X,Y) is extracted, a rule 
for SUBJPASS(Y,X) can be built (and vice-wzrsa). 
They could be improved as well by integrating more 
line-grained semantic inl'ormation l'or adverbs and 
ac!iective, WordNet being relatively poor \['or these 
parts of speech. 
Since the architecture is modular, the sy~;tem ini- 
tially provided for F, nglish can be quickly adapted 
for any other language as soon as the requi:red com- 
ponents are available. We already started to build a 
137 
semantic disambiguator for French, but we need to 
integrate a French semantic ontology into the sys- 
tem. At the moment, it is planned to extract such 
an ontology from the dictionary itself, using the se- 
mantic labels which am associated with semantic 
categories. The expectation is to obtain more con- 
sistency between semantic tags (dictionary) and se- 
mantic lasses (ontology). 
Because we used a bilingual dictionary we inte- 
grated the disambiguation module into a general 
system architecture d dicated to the comprehension 
of electronic texts written in a foreign language. 
This technique coupled with other natural language 
processing teclmiques such as shallow parsing can 
also be used to extract general semantic networks 
from dictionaries or encyclopedia. 
Acknowledgments: Many thanks to Frdddrique 
Segond for help, support and advices. Thanks to 
E. Aimelet, S. Aft-Mokhtal; J.R Chanod, M.H. Cor- 
rdard, G. Grefenstette, C. Roux, and N. Tarbouriech 
for helpful discussions. 
References 
Elisabeth Aimelet. 1998. XeLDa Dictiona~T 
Lookup hnprovement XRCE ATS XeLDa Techni- 
cal Report. 
S. Ait-Mokhtar, J-R Chanod. 1997. Subject and 
Object Dependency Extraction Using Finite-State 
Transducers. In Proceedings of the Workshop on 
atttomatic Information E:mztctiotz arid the Build- 
ing of Lexical Semantic Resourees,, ACL, p71-77, 
Madrid, Spain. 
R. Akroyd. September 1992. Markup for the 
Oxford-Hachette French Dictionary, English to 
French. Technical report Oxford University Press. 
D. Bauel; E Segond, A. Zaenen. 1995. LOCOLEX: 
the translation rolls off your tongue. In Proceedings 
of ACH-ALLC, Santa-Barbara, USA. 
L. Dini, V. Di Tomaso, E Segond. 1998. Error 
Driven Word Sense I)isambiguation I Proceedings 
of COLING/ACL, p320-324, Montreal, Canada. 
L. Dini, V. Di Tomaso, E Segond. 1999. GINGER 
II: an example-driven word sense disambiguator. In 
Computer and the Humanities, to appear. 
C. Fellbaum. 1998. WordNet: An Electronic Lexi- 
cal Database, MIT Press, Cambridge (MA). 
W.A. Gale, K.W. Church, D. Yarowsky. 1992. 
Work on statistical methods for word sense disam- 
biguation in Probabilistic Approaches to Natural 
Language: Papers from the 1992 AAAI Fall Sym- 
posium, p54-60, Cambridge, MA, October. 
W.A. Gale, K.W. Church, I3. Yarowsky. 1993. A 
method for dismbiguating word senses in a large 
corpus, in C()ml)uter and the Humanities, 26:415- 
439. 
N. Ide. Vdronis. 1990. Very lalge neural networks 
for word sense disambiguation, in Proceedil~gs of 
the 9th european conference oIz artificial intelli- 
gence, ECAI'90, p. 366-368, Stockhohn. 
A. Kilganiff. 1998. SENSEVAL: An Exercise in 
Evaluating Word Sense Disambiguation Programs. 
In Proceeding of the First International Col~\['erence 
on lxmguage Ressources and Evaluation, Granada, 
Spain. 
A. Kilgarriff. 1999. Gold standard atasets for 
evaluating word sense disambiguation programs. In 
Computer and the Humanities, to appear. 
C. Leaeock, G. Towell. 1993. Corpus-based statis- 
tical sense resolution, in Proceedings of the ARPA 
Human Langttage technology workshop, San Fran- 
cisco, Morgan Kaufman. 
H.T. Ng, H.B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
an Examplar-based Approach. In Proceedings of 
the ACL, p.40-47. 
Oxford-Hachette. 1994. The Oxford Hachette 
French Dictionao~. Edited by M.-H. Corrdard and 
V. Grundy, Oxford University Press-Hachette. 
R Resnik and D. Yarowsky. 1997. A perspective 
on word sense disambiguation methods and their 
evaluation. In Proceedings ().\['ACL SIGLEX Work- 
sho 1) on Tagging Text with Lexical Semantics: Why, 
What, and How?, Washington D.C., USA. 
Claude Roux. 1998. XeLDa Shallow Parser XRCE 
ATS XeLDa Technical Report. 
J. Vdronis, N. Ide. 1998. Introduction tothe Special 
Issue on Word Sense Disambiguation: The State of 
the Art. in Computational Liltguistics 24/1. 
J. Vdronis, N. lde. 1990. Word sense disambigua- 
tion with very large neural networks extracted fiom 
very large corpora In Proceedings of the 13th inter- 
national cotzference on computatimzal linguistics, 
COLING'90, volmne 2, p.389-394, Helsinki, Fin- 
land. 
D. Yarowsky. 1995. Unsupervised word sense dis- 
ambiguation method rivalizing supervised methods. 
In Proceedings o\['the ACL, p189-196. 
Y. Wilks, M. Stevenson. 1998. Word Sense Disam- 
biguation using Optimised Combinations of Knowl- 
edge Sources. In Proceedings o\[ COLING/ACL, 
Montreal, Canada. 
138 
195
196
197
198
Document Structure and Multilingual Authoring 
Carol ine Brun Marc Dymetman Veronika Lux  
Xerox  Research  Cent re  Europe  
6 chemin  de Mauper tu i s  
38240 Mey lan ,  F rance  
{brun ,  dymetman,  lux}?xrce ,  xerox ,  com 
Abst rac t  
The use of XML-based authoring tools is swiftly be- 
coming a standard in the world of technical docu- 
mentation. An XML document is a mixture of struc- 
ture (the tags) and surface (text between the tags). 
The structure reflects the choices made by the au- 
thor during the top-down stepwise refinement of the 
document under control of a DTD grammar. These 
choices are typically choices of meaning which are 
independent of the language in which the document 
is rendered, and can be seen as a kind of interlin- 
gua for the class of documents which is modeled by 
the DTD. Based on this remark, we advocate a rad- 
icalization of XML authoring, where the semantic 
content of the document is accounted for exclusively 
in terms of choice structures, and where appropri- 
ate rendering/realization mechanisms are responsi- 
ble for producing the surface, possibly in several lan- 
guages imultaneously. In this view, XML authoring 
has strong connections to natural language genera- 
tion and text authoring. We describe the IG (In- 
teraction Grammar) formalism, an extension of DT- 
D's which permits powerful inguistic manipulations, 
and show its application to the production of multi- 
lingual versions of a certain class of pharmaceutical 
documents. 
1 In t roduct ion  
The world of technical documentation is forcefully 
moving towards the use of authoring tools based 
on the XML markup language (W3C, 1998; Pardi, 
1999). This language is based on grammatical spec- 
ifications, called DTD's, which are roughly similar 
to context-free grammars 1 with an arbitrary num- 
ber of non-terminals and exactly one predefined ter- 
minal called pcdata. The pcdata  terminal has a 
special status: it can dominate any character st, ring 
(subject to certain restrictions on the characters al- 
lowed). Authoring is seen as a. top-down interactive 
process of step-wise refinement of the root nonter- 
minal (corresponding to the whole document) where 
the author iteratively selects a rule for expanding a
lBut see (Wood, 1995: Prescod, 1998) for discussions of 
the differences. 
nonterminal already present in the tree and where 
in addition s/he can choose an arbitrary sequence 
of characters (roughly) for expanding tile pcdata  
node. The resulting document is a mixture of tree- 
like structure (the context-free derivation tree cor- 
responding to the author's selections), represented 
through tags, and of surface, represented as free-text 
(PCDATA) between the tags. 
We see however a tension between the structure 
and surface aspects of an XML document: 
? While structural choices are under system con- 
trol (they have to be compatible with the DTD), 
surface choices are not. 2 
? Surface strings are treated as unanalysable 
chunks for the styling mechanisms that render 
the XML document o the reader. They can 
be displayed in a given font or moved around, 
but they lack the internal structure that would 
permit to "re-purpose" them for different ren- 
dering situations, such as displaying on mobile 
telephone screens, wording differently for a spe- 
cific audience, or producing prosodically ade- 
quate phonetic output. This situation stands 
in contrast with the underlying philosophy of 
XML, which emphasizes the separation between 
content specification and the multiple situations 
in which this content can be exploited. 
. Structural decisions tend t,o be associated wit, h 
choices of meaning which are independent of the 
language in which the document is rendered. 
Thus for instance the DTD for an aircraft main- 
tenance manual might distinguish between two 
kinds of risks: caut ion  (material damage risk) 
and warning (risk to the operator). By select- 
ing one of these options (a choice that will lead 
t,o further-t_owerdevel choices,), the::author takes 
a decision of a semantic nature, which is quite 
independent of the language in which the docu- 
ment is to be rendered, and which could be ex- 
ploited to produce multilingual versions of the 
2With  the emergenceof  schemas (W3C, 1999a), which per- 
mit some typing of the surface (float, boolean, string, etc.), 
some degree of control is becoming more feasible. 
24 
document. By contrast, a PCDATA string is 
language-specific.and ill-suited for multilingual 
applications. 
These remarks point to a possible radical view of 
XML authoring that advocates that surface strings 
be altogether eliminated from the document content, 
and that author choices be all under the explicit con- 
trol of the DTD and reflected in the document struc- 
ture. Such a view, which is argued for in a related 
paper (Dymetman et el., 2000), emphasizes the link 
application of MDA to a certain domain of pharma- 
ceutical documents. 
2 Our approach to Multilingual 
Document Authoring 
Our Multilingual Document Authoring system has 
the following main features: 
First, the authoring process is monolingual, but 
the results are multilingual. At each point of the pro- 
cess the author can view in his/her own language the 
..... . . . . . . . . . . .  between ~ML`d~cumeqt~a~a9ring`~aad;mu~ti~nguaL;~,~.~te~t:~s/h~hasa~u~h~rex~:~.~aa~a~d~rea~?where~he ..: 
text authoring/generation (Power and Scott, 1998; text still needs refinement are highlighted. Menus 
Hartley and Paris, 1997; Coch, 1996): the choices for selecting a refinement are also presented to the 
made by the author are treated as a kind of in- author is his/her own language. Thus, the author is 
terlingua (specific to the class of documents being always overtly working in the language s/he nows, 
modelled), and it is the responsibility of appropri- but is implicitly building a language-independent 
ate "rendering" mechanisms to produce actual text representation of the document content. From this 
from these choices ill tile different languages 3 under representation, the system builds multilingual texts 
consideration, in any of several anguages simultaneously. This ap- 
For such a program, existing XML tools suffer proach characterizes our system as belonging to an 
however from serious limitations. First, DTD's are emerging paradigm of"natural anguage authoring" 
too  poor in expressive power (they are close to (Power and Scott, 1998; Hartley and Paris, 1997), 
context-free grammars) for expressing dependencies which is distinguished from natural anguage gener- 
between different parts of the document, an aspect ation by the fact that the semantic input is provided 
which becomes central as soon as the document interactively by a person rather than by a program 
micro-structure (its fine-grained semantic structure) accessing digital knowledge representations. 
starts to play a prominent role, as opposed to simply Second, the system maintains strong control both 
its macro-structure (its organization i  large seman- over the semantics and the realizations of the docu- 
tic units, typically larger than a paragraph). Second, ment. At the semantic level, dependencies between 
current rendering mechanisms such as CSS (Cascad- different parts of the representation f the document 
ing Style Sheets) or XSLT (XLS transformation lan- content can be imposed: for instance the choice of 
guage) (W3C, 1999b) are ill-adapted for handling a certain chemical at a certain point in a mainte- 
even simple linguistic phenomena such as morpho- nance manual may lead to an obligatory warning 
logical variation or subject-verb agreement, at another point in the manual. At the realization 
In order to overcome these limitations, we are level, which is not directly manipulated by the au- 
using a formalism, Interaction Grammars (IG), a thor, the system can impose terminological choices 
specialization of Definite Clause Grammars (Pereira (e.g. company-specific nomenclature for a given con- 
and Warren, 1980) which originates in A. Ranta's cept) or stylistic choices (such as choosing between 
Grammatical Framework (GF) (Ranta; M~enp~igt using the infinitive or the imperative mode in French 
and Ranta, 1999; Dynaetman et el., 2000), a gram- to express an instruction to an operator). 
matical formalism based on Martin-LSf's Type The- Finally, and possibly most distinctively, the st- 
ory (Martin-L6f, 1984) and building on previous ex- mantle representation underlying the authoring pro- 
perience with interactive mathematical proof editors cess is strongly document-centric and geared towards 
(Magnusson and Nordstr6m, 1994). In this formal- directly expressing the choices which uniquely char- 
ism, the carrier of meaning is a choice tree (called aeterize a given document in an homoge~cous class 
"abstract ree" in GF), a strongly typed object in of documents belonging to the same domain. Our 
which dependencies between substructures can be view is document-centric in the sense that it takes 
easily stated using the notion of dependent types, as its point of departure the widespread practice of 
The remainder of this paper is organized as fol- using XML tools for authoring the macro-structure 
lows. In section 2,,,we give a'~,high.teveloverview .of ..... of doeuments,-oand--extends this-practice towards an 
the Multilingual Document Authoring (MDA) sys- account of their m.icro-structure. But the analysis 
tern that we have developed at XRCE. In section of the micro-structure is only pushed as far as is 
3, we present in some detail the formalism of In- necessary in order to account for the variability in- 
teraction Grammars. In section 4. we describe an side the class of documents considered, and not in 
terms of the ultimate meaning constituents of lan- 3The word "language" should be understood here in an 
extended sense tha! not only covers English. French. etc., but guage. This  nlicro-structure can in general be de- 
also different styles or modes of communication, ler ln iued by s tudy ing  a corpus of  documents  and by 
25 
exposing the structure of choices that distinguish a 
given document from other documents in this class. 
This structure of choices is represented in a choice 
tree, which is viewed as the semantic representation 
for the document. 4 One single choice may be asso- 
ciated with text realizations of drastically different 
granularities: while in a pharmaceutical document 
the choice of an ingredient may result in the produc- 
tion of a single word, the choice of a "responsability- 
waiver" may result in a long stereotypical paragraph 
of text, the further analysis of which would be totally 
.counter-productive. 
3 In teract ion  Grammars  
Let us now give some details about the formalism 
of Interaction Grammars. We start by explaining 
the notion of choice tree on the basis of a simple 
context-free grammar, analogous to a DTD. 
Context - f ree  grammars  and  choice trees 
Let's consider the following context-free grammar 
for describing simple "addresses" in English such as 
"Paris, France": s 
address --> city, " , " ,  
country. 
country --> "France". 
country --> "Germany". 
city --> "Paris". 
city --> "Hamburg". 
city --> "the capital of", 
country. 
What does it mean, remembering the XML anal- 
ogy, to author a "document" with such a CFG? It 
means that the author is iteratively presented with 
partial derivation trees relative to the grammar (par- 
tial in the sense that leaves can be terminals or non- 
terminals), and at each given authoring step both 
selects a certain nonterminal to "refine", and also a 
given rule to extend this non-terminal one step fur- 
ther: this action is repeated until the derivation tree 
is complete. 
If one conventionally uses the identifier 
nonterminal~ to name the i-th rule expanding 
the nonterminal nontermina l ,  then the collection 
of choices made by the author during a session can 
be represented by a choice tree labelled with rule 
identifiers, also called combinators. An example 
of such a tree is address l (c i ty2 ,count ry2)  
4This kind of semantic representation stands i-n contrast 
to some representations commonly used in NLP, which tend 
to emphasize the fine-grained predicate-argument structure of 
sentences independently of the productivity of such analyses 
.\[or a given class of documents. 
5For compatibil ity with the notacionsCo follow, we use low- 
ercase to denote nonlerminals, aml quoted strings to denote 
terminals,  ra ther  than  tile inore usna\[  ul)pot'case lowercase 
convent  ions. 
which corresponds to choices leading to the output 
"Hamburg, Germany". 6 In.practice, rather than 
using combinator names which strictly adhere to 
this numbering scheme, we prefer to use mnemonic 
names directly relating to the meaning of the 
choices. In the sequel we will use the names adr;  
f ra ,  ger ,  par ,  ham, cap for the six rules in the 
example grammar. The choice tree just described is 
thus written adr (ham,ger ) .  
Mak ing  choice t rees  exp l ic i t  As we have ar- 
gued previously, choices trees are in our view the cen- . 
tral repositoi-y of documentc0ntent and we Want to 
manipulate them explicitely. Definite Clause Gram- 
mars represent possibly the simplest extension of 
context-free grammars permitting such manipula- 
tion. Our context-free grammar can be extended 
straightforwardly into the DCG: 7 
address (adr (Co ,C) )  - -> c i ty (C) ,  " , "  
country(Co) .  
count ry ( f ra )  - -> "France" .  
count ry (ger )  - -> "Germany". 
city(par) --> "Paris". 
city(ham) --> "Hamburg". 
city(cap(Co)) --> "the capital of", 
country(Co). 
What these rules do is simply to construct choice 
trees recursively. Thus, the first rule says that if the 
author has described a city through the choice tree 
C and a country through the choice tree Co, then the 
choice tree adr(Co,C) represents the description of 
an address. 
If now, in this DCG, we "forget" all the terminals, 
which are language-specific, by replacing them with 
the empty string, we obtain the following "abstract 
gram mar' l :  
address(adr(Co,C)) --> city(C), country(Co). 
country(fra) --> \[\]. 
country(ger) --> \[\]. 
city(par) --> \[\]. 
city(ham) --> \[\]. 
city(cap(Co)) --> country(Co). 
which is in fact equivalent o the definite clause 
program: s
SSuch a choice tree can be projected into a derivation 
tree in a straightforward way, by mapping a combinator 
nonterminali into the monterminal name nontermin,:.l, and 
by 'introducing terminal material as required by the specific 
rules. 
7According to the usual logic programming conventions, 
lowercase letters denote predicates and functors, whereas up- 
percase letters denote metavariables that can be instauciated 
with terms. 
Sin the sense that rewriting the nonterminal goal 
address  (adr (Co ,C)) to the empty string in the DCG is equiv- 
alent to proving the goal address(adr (Co ,C) )  in the program, 
26 
address (adr (Co ,C) )  : -  c i ty (C) ,  count ry (Co) .  
count ry  ( f  ra ) .  
count ry  (ger ) .  
c i ty (par ) .  
city(ham). 
city(cap(Co)) :- country(Co). 
This abstract  g rammar  (or, equivalently, this logic 
program),  is language independent and recursively 
defines a set of well-formed choice trees of different 
categories, or types. Thus, the tree adr (ham,ger )  
is .well-formed "in".. the. :typ~/add.~:r~s, ,End the .lice 
cap( f ra )  well-formed in the type c i ty .  
Dependent  Types  In order to stress the type- 
related aspects of the previous tree specifications, 
we are actual ly using in our current implementa-  
tion the following notat ion for the previous abstract  
grammar :  
adr (Co ,C) : :address  - ->  C : :c i ty ,  
Co : : count ry .  
f ra :  : count ry  - -> \[\] . 
ger :  : count ry  - -> \[\] . 
par :  : c i ty  --> \[3 . 
ham: :city --> \[\]. 
cap(Co) : :c i ty  --> Co::country. 
The first rule is then read: "if C is a tree of 
type c i ty ,  and Co a tree of type count ry ,  then 
adr (Co ,C)  is a tree of type address" ,  and simi lar ly 
for the remaining rules. 
The grammars  we have given so far are deficient 
in one important  respect: there is no dependency 
between the city and the country in the same ad- 
dress, so that  the tree adr (ham, f ra )  is well-formed 
in the type address .  In order to remedy this prob- 
lena, dependent types (Ranta; Martin-L6f, 1984)can 
be used. From our point  of view, a dependent ype 
is s imply a type that can be parametr ized by objects 
of other types. We write: 
adr (Co ,C) : :address  - ->  C : :c i ty (Co) ,  
Co: : count ry .  
f ra :  : count ry  - -> \[\] . 
get :  : count ry  - -> \[\] .
par : : c i ty ( f ra )  - ->  \ [ \ ] .  
ham: :c i ty (ger )  - ->  \ [ \ ] .  
cap(Co) : : c i ty (Co)  - ->  Co: :count ry .  
in which the type c i ty  is now parametr ized by 
objects of type count ry ,  and where the notat ion 
par  : : c i ty ( f ra )  is read as " 'par is  at ree of the type: 
city of f ra ' .  9 
which is another way of stating the well-known duality be- 
tween the rewriting and the goal-proving approaches to the 
interpretation f Prolog. 
9In terms of the underlying Prolog implementation. "::" is 
simply an infix operator for a predicate ofarity 2 which relates 
an object and its type, and both simple and dependent types 
are handled st raighforwardly. 
Para l le l  Grammars  and  Semant ics -dr iven  
? Compos i t iona l i ty . fo r  . ;Text . ;Rea l izat6 ion We 
have just  explained how abstract  grammars  can be 
used for specifying well-formed typed trees repre- 
senting the content of a document.  
In order to produce actual  mult i l ingual documents  
from such specifications, a s imple approach is to al- 
low for parallel real ization English, French . . . . .  gram- 
mars, which all have the same underlying abstract. 
g rammar  (program),  but which introduce terminals  
specific, to ~the_ language -at. hand. Thus. the (ollow- 
ing French andEng l i sh  gi-annmkrs a/'e pai~allel to the ' : "  
previous abstract  g rammar : l ?  
adr(Co,C) : :address --> C::city(Co), ",", 
Co: :country. 
fra: :country --> "France". 
ger : : country --> "Germany". 
par: :c i ty(fra)  --> "Paris". 
ham: : city(ger) --> "Hamburg". 
cap(Co): :c i ty(Co) --> "the capital of", 
Co : : country. 
adr(Co,C): :address --> C::city(Co), ",", 
Co : : country. 
fra: : country --> "In France". 
ger : : country --> "i' Al lemagne". 
par: : city(fra) --> "Paris". 
ham: : city (get) -- > "Hambourg". 
cap(Co): :city(Co) --> "In capitale de", 
Co: :country. 
This view of real ization is essentially the one we 
have adopted in the prototype at the t ime of writ- 
ing, with some straighforward addit ions permit t ing  
the handl ing of agreement constraints and morpho- 
logical variants. This s imple approach has proven 
quite adequate for the class of documents we have 
been interested in. 
However, such an approach sees the activity of 
generat ing text from an abstract  structure as ba- 
sically a composit ional  process on strings, that  is, 
a process where strings are recursively associated 
with subtrees and concatenated to produce strings 
at the next subtree level. But such a direct proce- 
dure has well-known l imitat ions when the seinantic 
and syntact ic levels do not have a direct correspon- 
dence (simple example: ordering a list of modifiers 
around a noun). We are currently experimenting 
with.a, powerful extension~of.stri.ng compqsihonal i ty  - 
where tim objects  composit ional ly  associated with 
abstract  subtrees are not strings, but syntactic rep- 
resentations with rich internal structure. The text 
10Because the order of goals in the right-hand side of an ab- 
stract grammar rule is irrelevant, he goals on the right-hand 
sides of rule in two parallel realization grammars can appear 
in a different order, which permits certain reorganizations of 
the linguistic material (situation ot shown in the example). 
27 
itself is obtained from the syntactic representation 
associated with the .total tree .by simply enumerat- 
ing its leaves. 
In this extended view, realization grammars have 
rules of the following form: 
a l (B ,C  . . . .  ) : :a (D  . . . .  ) -Syn  - ->  
B: :b(E  . . . .  ) -SynB,  
C : :c (F , . . . ) -SynC,  
general public. Le VIDAL ? includes a collection of 
notices ,for .around? 5 5.00. dmgs..a~ailable .in France. 
As the publisher, OVP-t~ditions du Vidal has taken 
care of homogeneity across the notices, reformatting 
and reformulating source information. The main 
source are the New Drug Authorizations (Autori- 
sation de Mise sur le March~), regulatory docu- 
ments written by pharmaceutical laboratories and 
approved by legal authorities. 
Relative to multilingual document authoring, this 
{const ra in ts  (B, C . . . . .  D, E, F . . . .  ) }, corpus has three features whicli,~e, considered highly 
? ' {compose=engt.ish(~synB ;~.SynC, " :-;-.Syn.)~}-~.--:-desi-r~ble:;(l)-it-dea\[s.with ,a.res\[rlcted-~em~:tit d~2 
The rule shown is a rule for English: the syn- 
tactic representations are language dependent; par- 
allel rules for the other languages are obtained by 
replacing the compose_engl ish constraint (which is 
unique to this rule) by constraints appropriate to the 
other languages under consideration. 
Heterogeneous  Trees and  In teract iv i ty  Natu- 
ral language authoring is different from natural lan- 
guage generation i one crucial respect. Whenever 
the abstract ree to be generated is incomplete (for 
instance the tree cap(Co)), that is, has some leaves 
which are yet uninstanciated variables, the genera- 
tion process hould not proceed with nondeterminis- 
tically enumerating texts for all the possible instan- 
elations of the initial incomplete structure. Instead 
it should display to the author as much of the text as 
it can in its present "knowledge state", and enter into 
an interaction with the author to allow her to fur- 
thor refine the incomplete structure, that is, to fur- 
ther instanciate some of the uninstanciated leaves. 
To this purpose, it is useful to introduce along with 
the usual combinators (adr, fra, cap, etc.) new 
combinators of arity 0 called typenames, which are 
notated type,  and are of type "type. These combi- 
nators are allowed to stand as leaves (e.g. in the tree 
cap(count ry ) )  and the trees thus obtained are said 
to be heterogeneous. The typenames are treated by 
the text generation process as if they were standard 
semantic units, that is, they are associated with text 
units which are generated "at their proper place" in 
the generated output. These text units are specially 
phrased and highlighted to indicate to the author 
that some choice has to be made to refine the un- 
derlying type (e.g. obtaining the text "la capitale de 
PAYS"). This choice has the effect of further instan- 
elating the incomplete tree with "true" combinators, 
main (for which various terminological resources are 
available), (2) it is a homogeneous collection of docu- 
ments all complying to the same division in sections 
and sub-sections, (3) there is a strong trend in in- 
ternational bodies such as the EEC towards making 
drug package notices (which are similar to VIDAL 
notices) available in multilingual versions strictly 
aligned on a common model. 11 
4.2 Corpus  analys is  
An analysis of a large collection of notices from Le 
VIDAL ? de la famille, describing different drugs, 
from different laboratories was conducted in order 
to identify: 
* the structure of a notice, 
? the semantic dependencies between elements in 
the structure. 
For this task, all the recta-information available is 
useful, in particular: explanations provided by Le 
VIDAL ? de la famille and help of a domain expert. 
Corpus study was a necessary preliminary task be- 
fore modeling the notices in the IG formalism pre- 
sented in section 2. 
4.2.1 S t ructure  
Notices from Le VIDAL ? are all built on the same 
model, including a title (the name of the drug, plus 
some general information about it). followed by sec- 
tions describing the main characteristics of the cirug: 
general description, composition, indications, con- 
traindications, warnings, drug interactions, preg- 
nancy and breast-feeding, dosage and administra- 
tion, possible side effects. This initial knowledge 
? about the semantic ontent of the document is cap- 
tured with a first., simple context free rule, such as: 
and the generation process is iterated. 
4 An  App l i ca t ion  to  Pharmaceut ica l  
Documents  
4.1 Corpus  select ion 
Our corpus consists in drug notices extracted froln 
"'Le VIDAL?de la Famille" (Editions du Vidal. 
1998). a practical book about heahh made for the 
........ vidalNot.ice(T,D,C, I ,CI.~W,DI ~ PaBF,D~i-A,PSI) : :notice 
- ->  
T: :title, 
D: :description, 
C: :composition, 
I lA  similar but less extended corpus was previously built 
by the third author as the basis for a prototype ofmuhilingual 
ctocument authoring using G F. 
28 
I : : ind icat ions ,  
Cl::contraindications, 
W::warn ings ,  
D I : :d rugs In teract ion ,  
PaBF: :p regnancyAndBreastFeed ing ,  
DaA::dosageAndAdmin, 
PSI::possibleSideEffects. 
Each section is associated with context-bee rules 
that describe its internal structure: 
'vidalTitle(N,APi . . . ,  .~;>)~:-.:~d?1e-=:n ....... 
- ->  
N::name0fDrug, 
AP::activePrinciples . . . . .  
vidalDescription(N,PF,P...)::description 
- ->  
\['DESCRIPTION'\], 
N::nameOfDrug, 
PF::pharmaceutForm, 
P::package . . . . .  
vidalDosageAndAdmin(D,A)::dosageAndAdmin 
- ->  
\['DOSAGE AND ADMINISTRATION'\], 
D::dosage, 
A::administration. 
tablet::pharmaceutForm --> \['tablet'\]. 
eyeDrops:::pharmaceutForm --> \['eye drops'\]. 
At this point, we allow parallel realizations for 
French and English. So, in addition to the English 
grammar given above, we have the French grammar: 
vidalTitle(N, AP . . . . . . . .  )::title 
- ->  
N::name0fDrug, 
AP::activePrinciples, ... . 
vidalDescr(N,PF,P...)::description 
- ->  
\['PRESENTATION'\], 
N::nameOfDrug, 
PF::pharmaceutForm, 
P::package . . . . .  
vidalDosageAndAdmin(D,A)::dosageAndAdmin 
- ->  
\['MODE D'EMPLOI ET POSOLOGIE'\], 
D::dosage, 
A::administration. 
tab le t : :pharmaceutForm - -> \ [ ' compr im~' \ ] .  
eyeDrops : : :pharmaceutForm --> \ [ ' co l l y re ' \ ] .  
This first grammar is fully eq.ivalent o a XML 
I)TD that describes the structure of a notice, though 
it distinguishes finer-grained units 1hart traditional 
l)TI)s tends to do. 
4.2.2 Modeling dependencies 
, ,~ButHG :~ goes ?urt, her,:than XM-L DTDs ~it~h'regard 
to the semantic ontrol of documents: it enables us 
to express dependencies which may arise in differ- 
ent parts of a document, including tong-distance de- 
pendencies, through the use of dependent types pre: " 
sented in section 2. 
Identification of the dependencies to be modeled was 
done in a second stage of the corpus study. For ex- 
ample, we identified dependencies between: 
, ........ ,:.-.: ~-~ "the:--ghamaaeoa~tieal ,:forrrr;0t~ a :gi,#ed~dtfug :(.cbn:.- 
cept pharmaceutForm) and its packaging (con- 
cept package), 
? particular ingredients given in the section com- 
position and warning instructions given ill the 
section warnings, 
? categories of patients the drug is intended for in 
the section description and posology indicated 
for each category in the section indications. 
To illustrate the modeling task, we now give more 
details about one particular dependency identified. 
Intuitively, it appears that there is a strong link be- 
tween the pharmaceutical form of a given drug and 
the way it should be administered: tablets are swal- 
lowed, eye drops are put in the eyes, powder is di- 
luted in water etc. In our first grammar, the phar- 
maceutical form concept appears in the description 
section, since the administration way is described in 
the dosage and administration section. The use of 
dependent ypes permits to link these sections to- 
gether according to the pharmaceutical form. Tile 
parts of the (English) grammar involved become: 
vidalNotice(T,D,C,I,CI,W,DI,PaBF,DaA,PSI)::notice 
- ->  
T::title, 
D::description(PF), 
C::composition, 
I::indications, 
CI::contraindications, 
W::warnings, 
DI::drugslnteraction, 
PaBF::pregnancyAndBreastFeeding, 
DaA::dosageAndAdmin(PF), 
PSI::possibleSideEffects. 
vidalDescription(N,PF,P,...)::description(PF) 
- ->  
\['D~SCRIPTION'\], " ? 
N::nameOfDrug, 
PF::pharmaceutForm, 
P::package . . . . .  
vidalDosageAndAdmin(D,A)::dosageAndAdmin(PF) 
- ->  
\['DOSAGE AND ADMINISTRATION'\], 
D::dosage, 
29 
A : : administration (PF). 
The administration section should now be de-- .... 
scribed according to the pharmaceutical form it pre- 
supposes, several administration ways being compat- 
ible with each form: 
t ab le tsAdmin l  : : administrat  ion (Tablet) 
?O~I'~?-INDICAT%(~mS: ce ~id l?~ent  rm do|t  p~s ~tre ut~l~sb dlns les C~S sutvancs: 
----> a l le r~ le  au~ /~1SS nocu~ent t 'aset r lne  i 
\ [ 'Swal low the tab le ts  w i thout  "- l 
crunch ing  them. '\] . ar~n~: 
"'... -" . . . . .  _ . ~w=' ~ ' : "  : ~ ' , ' ~ . ~ ' % ' - ~ ~  -.-" . . . . . . . . . .  
? \[KTERACTZORS HI~DICAHENTEIJSES: Ce |~atc~ent  aeut tn ter lq t r  avec a'autres ~ed~ca~ents. tablet  sAdmin2 : :administrat ion (Tablet) ~,o~ .... ~ - ~-,~,. ,, .... t,,~ ,nt ,~n..,~to ,.~ .... t.,.,~ ,,~ 
augmentation des effets ~a~Is~r~bles. - le l t th tu~:  ~9uentat lon ~u taux de Hth iu |  
__> dam le sanq. 
\[ 'Let the tab le ts  mel t  under  c.oss~ss( ?TT AttAI~M~,T: 
the tongue. '\] . 
eyeDropsAdmin : :admin is t ra t ion(EyeDrops)  
- ->  
\ [~Pul l  the lower  eye l id  down wh i le  
look ing  up and squeeze  the eye drops,  
so that they fa l l  between the eye l id  
and the eyeba l l . ' \ ] .  
emacs: "prolo@ ? : 
I 
llOaOF?1t IbuDrofane 
P'R~\[NTATION: RUROFEN : ?ot~r|m~ C blanc ) : bQIte de Z? - ~ah &~ - 15.s F - 
? t@orat01 res Boots Healt.care 
?o,tposrrzoq: p cD 
Ibugrofene . . . . . . . . . . . . . .  20fl ig 
INDICATIONS: Ce |~d lcuent  est u,  gnc l - |n f lu la tO l?o  non stero~cHen {PISS). I \ ]  osc 
u t | l i s6  e, cas de aouIeurs diverses. 
.~OOE D'EHPtOI ET POSOLOCZE: i \ [~ l l l l l lm l lmlml  ~ . P~ologta 
Usuel t e: : ~ ?o.pr i mes . . . ,~ ;441 . i~   g/l 
The consequence of such a modeling is a better 
control of the semantic ontent of the document in 
the process of being authored: once the user chooses 
tablet as pharmaceutical form in the section descrip- 
tion, his choice is restricted between the two con- 
cepts tabletsAdminl and tabletsAdmin~ in the ad- 
ministration section. If he chooses eye drops as the 
pharmaceutical form, there is no choice left if the ad- 
ministration section: the text fragment correspond- 
ing to the concept eyeDropsAdmin will be generated 
automatically in the document. 
This example illustrates how dependencies are 
propagated into the macro-structure, but they can 
be propagated into the micro-structure as well: for 
example, in the description section, we can express 
that the packaging of the drugs is also dependent of 
their form: tablets are packaged in boxes, eye drops 
in flasks, powder in packets, etc.: 
v ida lDescr ip t ion(N ,P  . . . .  ) : :descr ipt ion(PF)  
- ->  
\ [ 'DESCRIPTIDN' \ ] ,  
N : :name0fDrug,  
PF : :pharmaceutForm,  
P : :package(PF)  . . . . .  
box:  :package(Tab le t )  . - ->  \ [ 'Box ' \ ] .  
f l ask : :package(EyeDrops)  - ->  \ [ ' F lask ' \ ] .  
This example shows that tile granularity degree of 
the linguistic realization cat\] vary from full text seg- 
ment (administration ways) to sing\[e words (forms 
like tablet, eye drops, powder, etc.). This is highly 
related to the reusability of the concept: references 
to specific forrns may appear it\] many parts of the 
Figure 1: A stage in the authoring of a notice, with 
French text shown. 
document, while the administration ways are more 
or less frozen segments. 12
The level of generality of dependencies encoded in 
the grammar needs to be paid attention to: one has 
to be sure that a given dependency is viable over a 
large collection of documents in the domain. If a 
choice made by the grammar writer is too specific, 
the risk is that it may be not relevant for other docu- 
ments. For this reason, an accurate knowledge of the 
corpus is necessary to ensure an adequate coverage 
of documents in the domain. 
4.3 An  Example  
Screen copies of the IG interface during an authoring 
process of a VIDAL notice are given on figures 1 and 
2. Figure 1 represents the notice authored in French 
at a given stage. The fields still to be refined by 
tile user appear ill dark. When the author wants to 
refine a given field, a pulldown menu presenting tile 
choices for this field appears on the screen. Here, the 
author chooses to refine the field avaler in the admin- 
istration (mode d'emploi et posologie ) section: the 
corresponding menu.proposes the list of.administra- 
tion ways corresponding to the pharmaceutical form 
tablet he has chosen before. Figure 2 shows the par- 
allel notice in English but one step further, i.e. once 
he has selected the administration way. 
12 For a discussion of some of the issues regarding the use of 
templates in nature\[ language generation systems, see (\[-leit er, 
1995). 
30 
I . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . .  ~ : ? - . . . .  aaa- . ;~o~}=:  . . . . . . . . . . . . . . . . . . .  . . 7~.  7 -~ i.. 
RUnOFE# I bupro fen  
OESERIPT|ON: HUROfEH : tab let  ( vh i te  ) ; box of 20 - G~ Rezab - X5.8 F - . Boots 
Real thcare Laborator ies 
?~?SFr IOH:  0 tb  
~buDrot en . . . . . . . . . . . . . .  200  i~ 
INDICATZC~S: This dru9 Is a ,on -~tero ld / I  anct - ln f lu la tc ry  (NSAIPS). I t  IS used to 
treat various pal~s 
COliTRA.\[KOIC&Tl~44S: This drug should not be used in  the fo l low ing  cases: a l l~rt ly  to 
NSAtOS l in  par t i cu la r  t~_~p_trtn i 
WA~I~INCS: . . . . . .  . ? 
~RU? I~TER~'I'ZONS: This clru9 can In teract  ~ l tb  other  drugs. In  ~art~cular:  - asp l r , ,  
aria the other non s tero ida l  ~t~- tn f l~ la tory  drugs: ~ncrea.se of side ef fec ts .  - 
Lithium: ~?reas l  of blood hth~ul  rate.  
I 
PRECNN(CV MD 8REAST-rE?DINC: 
VeDm~ 
DOSAGE AnD .~DMINISTRATI(~4: ~ tab le t  swallowed v i th  a lass of 
aye .  ~ . 
t 
PC~SIeLE SlO? EFFECTS: 
Figure 2: The parallel English notice one authoring 
step later. 
5 Conc lus ion  
XML-based authoring tools are more and more 
widely used in the business community for sup- 
porting the production of technical documentation, 
controlling their quality and improving their re- 
usability. In this paper, we have stressed the connec- 
tions between these practices and current research in 
natural anguage generation and authoring. We have 
described a formalism which removes ome of the 
limitations of DTD's when used for the production 
of multilingual texts and presented its application to 
a certain domain of pharmaceutical documents. 
Acknowledgements  Thanks to Jean-Pierre 
Chanod, Marie-H~_lb.ne Corr/mrd, Sylvain Pogodalla 
and Aarne Ranta for important contributions, 
discussions and comments. 
References  
a. Coch. 1996. Evaluating and comparing three text 
production techniques. In Proceedings of the 16th 
International Confe~vnce on Computational kin- 
guistics. 
OVP l~ditions du Vidal, editor. 1998. Le VIDAL de 
la famille. HACHETTE. 
M. Dymetman. V. Lux, and A. Ranta. 2000. XML 
and multilingual document authoring: Conver- 
gent trends. In Pro,'eedings Coling 2000, Saar- 
brficken. 
A. Hartley and ('. Paris. 1997. Muhilingual docu- 
ment production-: from supporl for translating to 
support for authoring. In Machine Translation, 
Special Issue. on New Tools for Huma n TranslaT,.. 
tots, pages 109-128. 
L. Magnusson and B. Nordstr6m. 1994. The ALF 
proofeditor and its proof engine. In Lecture Notes 
in Computer Science 806: Springer. 
P. Martin-L6f. 1984. Intuitionistic Type Theory. 
Bibliopolis, Naples. 
P. M/ienp/ii and A. Ranta. 1999. The type theory 
and type checker of GF. In Colloquium on Prin- 
ziples, .Logics, ..and Implementations .ofHigh-Level 
Progrdmm.ihg L~inTJages, Worl~shop: On-Logical 
Frameworks and Meta-languages, Paris, Septem- 
ber. Available at h t tp  : / /www. cs .  chalmers, se /  
~aarne/papers/Ifm 1999. ps. gz. 
W. Pardi. 1999. XML in Action. Microsoft Press. 
Fernando C. N. Pereira and David H. D. Warren. 
1980. Definite clause grammars for language anal- 
ysis. Artificial Intelligence, 13:231-278. 
R. Power and D. Scott. 1998. Multilingual au- 
thoring using feedback texts. In Proceedings of 
the 17th International Conference on Computa- 
tional Linguistics and 36th Annual Meeting of the 
Association for Computational Linguistics, pages 
1053-1059. 
P. Prescod. 1998. Formalizing SGML 
and XML instances and schemata 
with forest automata theory. 
http ://www. prescod, net/forest/shorttut/. 
A. Ranta. Grammatical Framework work 
page. h t tp  ://www. cs .  chalmers, se /  
aarne/GF/pub/work -  index/ index,  html. 
E. Reiter. 1995. NLG vs. templates. In Proceedings 
of the 5th European Workshop on Natural Lan- 
guage Generation (EWNLG '95), pages 95-106, 
Leiden. 
W3C, 1998. Extensible Markup Language (XML) 
1.0, February. W3C reconunendation. 
W3C, 1999a. XML Schema - Part 1: Structu~vs, 
Part 2 : Datatypes -, December. W3C Working 
draft. 
W3C, 1999b. XSL Transformations (XSLT), 
November. W3C recommendation. 
D. Wood. 1995. Standard Generalized Markup Lan- 
guage: Mathematical and philosophical issues. 
Lecture Notes in Computer Science. 1000:344-- 
365. 
31 
Normalization and Paraphrasing Using Symbolic Methods
Caroline Brun
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan France
Caroline.Brun@xrce.xerox.com
Caroline Hage`ge
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan France
Caroline.Hagege@xrce.xerox.com
Abstract
We describe an ongoing work in informa-
tion extraction which is seen as a text nor-
malization task. The normalized represen-
tation can be used to detect paraphrases
in texts. Normalization and paraphrase
detection tasks are built on top of a ro-
bust analyzer for English and are exclu-
sively achieved using symbolic methods.
Both grammar development rules and in-
formation extraction rules are expressed
within the same formalism and are devel-
oped in an integrated way. The experiment
we describe in the paper is evaluated and
presents encouraging results.
1 Introduction
Work on paraphrase can be seen in two main per-
spectives: From the analysis point of view, i.e. how
to recognize expressions found in texts that convey
similar information (we call it normalization), and
from the generation point of view, i.e. how to pro-
duce a natural language output semantically equiva-
lent to the original phrase.
In this paper, we address the analysis point of
view in an experiment we made in the processing of
a corpus consisting of a collection of texts from the
Agency for Toxic Substances and Disease Registry
(ATSDR) describing different toxic products1. In
these texts, multiple ways of describing toxic prod-
ucts are present (see 2.1 below), which makes this
1see http://www.atsdr.cdc.gov.
text collection particularly interesting for the task
of paraphrase detection. We build a system were
documents are processed and give as output a nor-
malized representation of some selected knowledge.
The analysis phase can thus be seen as a paraphrase
detection phase, as it unifies in a same representa-
tion different ways of expressing similar information
about toxic products.
We will first describe the corpus on which we
work and then the semantic focus of our paraphrase
system. The following section is dedicated to the
information extraction task which is seen as a para-
phrase detection task in the continuity of the task of
parsing. Finally we describe the evaluation of the
Information Extraction task performed by our sys-
tem. Future work and improvements are finally dis-
cussed.
2 Corpus Analysis and Expected Output
2.1 Corpus study
The corpus on which we work consists of a collec-
tion of texts presenting toxic products from ATSDR
that are meant to be read by general public. We have
concentrated on the first paragraphs containing in
average between 6-7 sentences and consisting in the
general presentation of a toxic product. They give
information about the name, the appearance (colour,
smell), some physical properties and possible syn-
onyms of a toxic product. They also explain where
the product comes from and for what purposes it is
used. Because of the uniformity of the information
conveyed in these different texts, the corpus is rich
in paraphrases.
For instance, in the text concerning acetone we
read:
It evaporates easily, is flammable, and dissolves in
water.
And in the text concerning acrolein we can read:
It dissolves in water very easily and quickly, changes
to a vapor when heated. It also burns easily.
Even in the same text, they are some redundancies
and a similar idea can be expressed more than once
in different ways. For instance, in the text describing
2-Butanone we can read:
it is also present in the environment from natural
sources.
And later:
2-Butanone occurs as a natural product
These few examples illustrates that the kind of
texts we work with deal with a restricted semantic
domain and contain a large number of reformula-
tions.
2.2 Semantic focus of our paraphrase system
Our goal is to detect and represent some selected in-
formation in the corpus presented above. To achieve
this, we want to associate a uniform representation
with the different wordings of the same information
that appears in the texts. We focus on the different
ways of expressing the information relative to the
appearance, physical properties, synonyms, use and
origin of toxic products. Our representation consists
of a list of predicates which are detailed below.
? PHYS FORM/2. This predicate is the result
of the normalization of strings expressing the
physical form of the toxic product. For in-
stance PHYS FORM(ammonia,gas) expresses that
the product ammonia is a gas.
? DESCRIPTION COLOUR/2. This predicate is the
result of the normalization of strings describ-
ing the colour of the toxic product. For in-
stance DESCRIPTION COLOUR(antimony,silvery-
white) expresses that antimony is a silvery-white
product.
? DESCRIPTION SMELL/2. This predicate is the
result of the normalization of strings describ-
ing the smell of toxic product. For instance
DESCRIPTION SMELL(1.3-butadiene,gasoline-like)
expresses that the product 1.3-butadiene has a
gasoline-like odor.
? SYNONYM/2. This predicate expresses that the
second argument is a synonym of the first,
which is the name of the toxic product. For
instance SYNONYM(acetone,dimethyl ketone) ex-
presses that dimethyl ketone is another name for
acetone.
? PROPERTY/5. The PROPERTY predicate
is the result of the normalization of strings
expressing physical or chemical properties
of the toxic product. For instance, PROP-
ERTY(acrolein,dissolve,water,in,NONE) expresses
that the product acrolein is soluble in water
(instantiation of the four first arguments of
the predicate), and that we do have precisions
about the way this dissolution occurs (last
argument NONE is not instantiated by a
value). For the same product we have PROP-
ERTY(acrolein,burn,NONE,NONE,easily) which
expresses that the product is flammable and
that the localization of the flammability is
unspecified.
? ORIGIN/4 contains the normalized informa-
tion whether the product is natural or not
and where it can be found. For instance,
ORIGIN(ammonia,manufactured,NONE,NONE) ex-
presses that the product ammonia is man-
made, and ORIGIN(amonnia,natural,soil,in) ex-
pressed that the same product can also be found
naturally in soil.
? USE/6 is the result of the normalization of
the uses of the described product. In this
first stage we only concentrate in uses where
the product is used alone2. For instance
USE(benzidine,NONE,NONE,produce,dye,past)
expresses that in the past (last argument
is past) the product benzidine was used to
produce dyes (4th and 5th arguments) while
USE(ammonia,smelling salts,in,NONE,NONE,present)
expresses that ammonia is now (last argument
is present) used in smelling salts (the purpose
of the use is not specified here).
2In the texts, uses of a product when it is mixed with another
can also be described but we decided to ignore this information.
To each of the above-mentioned predicates a suf-
fix NEG can be added if there is a negation.
3 Paraphrase detection
Paraphrasing means to be able, from some input text
that convey a certain meaning, to express the same
meaning in a different way. This subject has recently
been receiving an increasing interest. For instance,
Takahashi et. al. (Takahashi et al, 2000) developed
a lexico-structural paraphrasing system. Kaji et al
developed a system which is able to produce verbal
paraphrase using dictionary definitions (Kaji et al,
2000) and Barzilay and McKeown showed how, us-
ing parallel corpora of English literary translations,
they extract paraphrases (Barzilay and McKeown,
2001). Paraphrase detection is a useful step in many
NLP applications. For instance, in multi-document
summarization, paraphrase detection helps to iden-
tify similar text segments in order that the summary
become more concise (McKeown et al, 1999). Para-
phrase detection can also be used to augment recall
in different IE systems.
In our experiment, paraphrase detection is a
step in normalization, as we want to instantiate the
same way the predicates presented above when the
informative content is similar. For instance, we want
to obtain the same normalized predicate for the two
utterances ProductX is a colorless, nonflammable
liquid and ProductX is a liquid that has no colour
and that does not burn easily namely:
DESCRIPTION COLOUR(ProductX,colorless)
PHYS FORM(ProductX,liquid)
PROPERTY NEG(ProductX,burn,NONE,NONE,NONE).
The input to our paraphrase detection system is the
whole paragraph that describes the toxic product.
The analysis of the paragraph produces as output the
set of normalized predicates. This output can be pro-
duced either in simple text format or in an XML for-
mat that can feed directly some database.
The paraphrase detection system is based on three
different modules that are described in the follow-
ing subsections. As claimed in (Takahashi et al,
2000) and for the purpose of re-usability, we dis-
tinguish what is of general linguistic interest in the
paraphrasing task from what is clearly domain de-
pendent, so these three modules are:
? A general English dependency parser;
? A general morpho-syntactic normalizer;
? A specific- and application-oriented normal-
izer.
3.1 General English dependency parser
This component is a robust parser for English (XIP)
(A??t-Mokhtar et al, 2002) that extract syntactic
functionally labeled dependencies between lexical
nodes in the text.
Parsing includes tokenization, morpho-syntactic
analysis, tagging which is performed via a combina-
tion of hand-written rules and HMM, chunking and
finally, extraction of dependencies between lexical
nodes.
Dependencies are binary relations linking two
lexical nodes of a sentence. They are established
through what we call deduction rules.
Deduction rules
Deduction rules apply on a chunk tree and consist
in three parts:
? Context
? Condition
? Extraction
Context is a regular expression on chunk tree
nodes that has to be matched with the rule to apply.
Condition is a boolean condition on dependen-
cies, on linear order between nodes of the chunk
tree, or on a comparison of features associated with
nodes.
Extraction corresponds to a list of dependencies
if the contextual description and the conditions are
verified.
For instance, the following rule establishes a
SUBJ dependency between the head of a nominal
chunk and a finite verb:
| SN{?*,#1[last:+]},
?*[verb:?],
SV{?*, #2[last:+]}|
if (?SUBJ(#2,#1))
SUBJ(#2,#1).
The first three lines of the rule corresponds to con-
text and describe a nominal chunk in which the last
element is marked with the variable #1, followed by
anything but a verb, followed by a verbal chunk in
which the last element is marked with the variable
#2. The fourth line (negative condition: ?) veri-
fies if a SUBJ dependency exists between the lexical
nodes corresponding to the variable #2 (the verb)
and #1 (the head of the nominal chunk). The test
is true if the SUBJ dependency does not exist. If
both context and condition are verified, then a de-
pendency SUBJ is created between the verb and the
noun (last line).
An important feature is that our parser always pro-
vides a unique analysis (determinism), this analysis
being potentially underspecified.
3.2 General morpho-syntactic normalization
The morpho-syntactic normalizer is a general mod-
ule that is neither corpus- nor application-dedicated.
It consists of hand-made rules that apply to the syn-
tactic representation produced by our parser. It uses
well known syntactic equivalences such as passive-
active transformation and verb alternations proposed
in Levin. It also exploits the classification given by
the COMLEX lexicon (Grishman et al, 1994) in or-
der to calculate the deep-subject of infinitive verbs.
For instance the utterance Antimony ores are
mixed with other metals is finally represented with a
set of normalized syntactic relations expressing that
the normalized subject (SUBJ-N) of the verb mix
is unknown, and that mix has two second actants
(OBJ-N) ore and metal :
SUBJ-N(mix,SOMEONE)
OBJ-N(mix,ore)
OBJ-N(mix,metal)
For this example, both passive transformation and
reciprocal alternation transformation have been ap-
plied on the set of dependencies produced by the
general parser.
Deep syntactic rules are expressed using the same
formalism than general syntactic rules presented in
the previous section. For instance the following
rule construct an OBJ-N (Normalized object) depen-
dency between the surface syntactic subject and a
verb in a passive form3.
if ( SUBJ(#1,#2)
& VDOMAIN[passive](#1,#3)
)
OBJ-N(#3,#2)
Unlike Rose??s approach (Rose?, 2000) which also
developed a deep syntactic analyzer, this is done ex-
clusively by hand-made rules based on the previous
calculated dependencies on the one hand and syn-
tactic and morphological properties of the nodes in-
volved in the dependencies on the other hand.
Together with the exploration of syntactic prop-
erties, we also take advantage of morphological
properties in order enrich our deep syntactic anal-
ysis. This is done using the CELEX database (Celex
Database, 2000) by pairing nouns and verbs that be-
long to the same morphological family, which al-
lows us to obtain for the expression John?s creation
of the painting, the same deep syntactic representa-
tion as for John creates the painting.
As a result of the second stage, we obtain new
deep syntactic relations, together with the superficial
syntactic relations calculated by the general parser:
? SUBJ-N (Normalized subject) that links the
first actant of a verb (finite or non-finite) or of
a predicative noun to this verb or noun.
? OBJ-N (Normalized object) that links the sec-
ond actant of a verb (finite or non-finite) or of a
predicative noun to this verb or noun.
? ATTRIB (General attribute) that links two
nodes when the second one denotes a property
of the first one.
? PURPOSE that links a verb to its actant ex-
pressing the purpose of the action.
It is important to note that predicative nouns are
represented by their underlying verbs. e.g. The
invention of the process is represented by OBJ-
N(invent,process).
3VDOMAIN links the first element of a verbal chain to the
last element of a verbal chain and passive is a feature that is
added to this relation.
3.3 Application and corpus specific
normalization
Application and corpus specific normalization is a
follow-up of the previous module. But while general
normalization is purely based on syntactic transfor-
mations and some derivational morphology proper-
ties, synonymy relations and all further possibilities
of morphological derivations are not exploited. This
extension uses the results obtained at the previous
analysis level.
The application- and corpus-oriented analysis is
organized in two axes that are detailed below.
? corpus oriented linguistic processing;
? corpus oriented paraphrasing rules.
3.3.1 Corpus oriented linguistic processing
We exploit the corpus specific properties at dif-
ferent stages of the processing chain in order to im-
prove the results of the general syntactic analysis.
Below are the additions we made:
? Specific tokenization rules.
Since toxic products can have names like 2,3-
Benzofuran, which the general tokenizer does not
consider as one unique token, we add a local gram-
mar layer dedicated to the detection of these kinds of
names. In other words, this layer composes together
tokens that have been separated by the general tok-
enizer.
? Specific disambiguation rules valid for this
kind of corpus but not necessarily valid for all
kinds of texts.
For instance, the word sharp has a priori two possi-
ble part-of-speech analyzes, noun and adjective, and
we want to keep these two analyzes for the general
parser. But, since the noun sharp belongs to a cer-
tain domain (music) that has no intersection with the
domain handled by the corpus, we add specific dis-
ambiguation rules to remove the noun analysis for
this word.
? Improved treatment of coordination for this
kind of text.
The corpus contains long chains of coordinated ele-
ments and especially coordination in which the last
coordinated element is preceded by both a comma
and the coordinator. Since some elements have been
typed semantically, we can be more precise in the
coordination treatment exploiting this semantic in-
formation.
? Adding some lexical semantics information
For the purpose of the application, we have semanti-
cally typed some lexical entries that are useful for
paraphrase detection. For instance, colour names
have the features colour : + added.
? Automatic contextual typing
Some of the manually semantic typing (previous
point) allows us to indirectly type new lexical units.
For instance, as formulations like synonyms, call,
name, designate are marked as possible synonymy
introducers, we are able to infer that complements
of these lexical units are synonyms. In a similar
way, syntactic modifiers of lexical units that have
been marked in the application lexicon like smell
and odor are odor descriptions. In these cases, di-
rect typing cannot be achieved. For example, the
huge number of potential smellings (almond-like,
unpleasant, etc.) cannot be code by hand. How-
ever, the inference mechanism enable us to extract
the required information.
? Ad-hoc anaphora resolution.
In our corpus, the pronoun it and the possessive its
always refer to the toxic product that is described in
the text. As we do not have any anaphora resolution
device integrated to our parser, we take advantage of
this specificity to resolve anaphora for it and its.
3.3.2 Corpus oriented paraphrases
Paraphrases are detected by hand-made rules us-
ing lexical and structural information.
Lexical relations for paraphrasing
As mentioned before, in our general normalizer
some nouns and verbs belonging to the same mor-
phological family are related. We extend these re-
lations to other classes of words that appear in the
corpus. For instance, we want to link the adjec-
tive flammable and the verb burn, and we want the
same kind of relation between the adjectives soluble,
volatile, mixable and the verbs dissolve, evaporate
and mix respectively. We declaratively create a re-
lation (ISAJ relation) between these pairs of words,
and this relation can then be handled by our parser
exactly like a dependency relation which has been
previously calculated. Other lexical relations be-
tween synonyms (e.g. call and name) or non-related
morphological nouns and verbs (as for instance the
noun flammability and burn) are created.
The lexical relations we created are the following
? ISAJ links an adjective and a verb when the verb
can be paraphrased by BE+adjective
? TURNTO links a noun and a verb when the verb
can be paraphrased by TURN TO+noun
? HASN links a noun and a verb when the verb
can be paraphrased by HAVE+noun
? SYNO links two words belonging to the same
morpho-syntactic class when the first is a syn-
onym of the second4.
Normalization rules
Once these relations are created, we can then ex-
ploit them in rules.
For instance, the following rule5 (see below) al-
lows for the creation of the predicate
PROPERTY(aniline,dissolve,NONE,NONE,NONE)
for the utterance aniline is soluble.
if (
SUBSTANCE(#1) &
ATTRIB(#1,#8[adj_property]) &
ISAJ(#9,#10) &
#8[lemme]:#9[lemme]
)
PROPERTY(#1,#10,##Pron[lemme=NONE],
##Pron[lemme=NONE],
##Pron[lemme=NONE]
)
The rule formalism is the one used for the general
syntactic grammar and the deep syntax grammar. In
this case, we only have two parts in the rule (Condi-
tion and Extraction, Context being omitted). In the
4Since we work in a very specific domain, we have no prob-
lem of word-sense ambiguity here.
5Variables in a rule are represented by #n.
present example, since we have detected that aniline
is the described toxic product (SUBSTANCE(aniline)),
since an ISAJ relation exists between soluble and
dissolve (ISAJ(soluble,dissolve)) and finally since the
deep syntactic analysis of the sentence has given to
us the dependency ATTRIB(aniline,soluble), the final
predicate is created.
3.4 Example of output
When applied on an input text describing a toxic
substance, such as the following one :
Acetone is a manufactured chemical that is also
found naturally in the environment. It is a colorless
liquid with a distinct smell and taste. It evaporates
easily, is flammable, and dissolves in water. It
is also called dimethyl ketone, 2-propanone, and
beta-ketopropane. Acetone is used to make plastic,
fibers, drugs, and other chemicals. It is also used
to dissolve other substances. It occurs naturally in
plants, trees, volcanic gases, forest fires, and as a
product of the breakdown of body fat. It is present
in vehicle exhaust, tobacco smoke, and landfill sites.
Industrial processes contribute more acetone to the
environment than natural processes.
the system is able to extract the following list
of predicates:
SUBSTANCE(acetone)
PHYS_FORM(acetone,chemical)
PHYS_FORM(acetone,liquid)
DESCRIPTION_COLOUR(acetone,colorless)
DESCRIPTION_SMELL(acetone,distinct)
PROPERTY(acetone,burn,NONE,NONE,easily)
PROPERTY(acetone,evaporate,NONE,NONE,easily)
PROPERTY(acetone,dissolve,water,in,NONE)
ORIGIN(acetone,natural,vehicle exhaust,in)
ORIGIN(acetone,natural,tobacco smoke,in)
ORIGIN(acetone,natural,landfill site,in)
ORIGIN(acetone,natural,plant,in)
ORIGIN(acetone,natural,the environment,in)
ORIGIN(acetone,man-made,NONE,NONE)
ORIGIN(acetone,natural,tree,in)
ORIGIN(acetone,natural,volcanic gas,in)
ORIGIN(acetone,natural,forest fire,in)
ORIGIN(acetone,natural,a product,in)
SYNONYM(acetone,dimethyl ketone)
SYNONYM(acetone,beta-ketopropane)
SYNONYM(acetone,2-propanone)
USE(acetone,NONE,NONE,make,plastic,present)
USE(acetone,NONE,NONE,make,fiber,present)
USE(acetone,NONE,NONE,make,drug,present)
USE(acetone,NONE,NONE,make,other chemical,
present)
USE(acetone,NONE,NONE,dissolve,
other substance,present)
Most of the information present in the orig-
inal text has been extracted and normalized:
for example, flammable is normalized as PROP-
ERTY(acetone,burn,NONE,NONE,easily). However,
form the input ... as a product of the breakdown
of body fat, the system extract the partial analysis
ORIGIN(acetone,natural,a product,in). Such cases are
discussed in section 4.
In this section, we have shown how, extending a
general parser with limited information (morpholog-
ical and transformational) and adding specific do-
main knowledge for the corpora we consider, we
were able to obtain a normalization of some knowl-
edge enclosed in the texts. The next section is ded-
icated to the evaluation of the performances of this
system.
4 Evaluation
We decided to perform two kinds of evaluation
? First, we wanted to check if our system per-
forms correctly the extraction of the selected
information.
? Second, we wanted to verify the impact of
the normalization and the corpus oriented para-
phrase modules in the obtained results.
4.1 Performance of the whole system for
information extraction
In order to evaluate the results of the information
extraction system, we apply the full chain of infor-
mation extraction on an unseen collection of 30 texts
describing toxic substances. Then we associate the
output predicates to the corresponding texts and ask
each of the five evaluators to compare six pairs of
texts/predicates. We ask them to read carefully the
texts and to fill a table which covers the different
types of information in scope, i.e substance, physi-
cal form, colour, odor, synonyms, physical proper-
ties, and use. For each topic, they have to express
what is missing, superfluous or wrong in the list of
predicates, compared to the original texts. We con-
sider one missing answer for each missing informa-
tion detected by the evaluators. And we consider
an incorrect response for each information that had
been extracted by the system and that did not corre-
spond to any realization in text. We then compute
precision and recall, obtaining the following results:
Precision Recall F-score
.96 .65 .77
We obtain a high precision result which could be
expected considering our IE methodology. In most
of the cases, when the information has been ex-
tracted, it is correct. However, most of the prob-
lems are a consequence of insufficient coverage of
both the extraction grammar (problems with struc-
tural ambiguity) and domain-knowledge. The main
sources of errors which have been identified during
the evaluation comes from :
? Coordination detection problems. For exam-
ple, from the sentence Hexachlorobutadiene
is also used as a solvent, and to make lu-
bricants, in gyroscopes, as a heat transfer
liquid, and as an hydraulic fluid. the sys-
tem detects only one ?use? of the element:
USE(Hexachlorobutadiene,solvent,as,NONE,NONE),
because the complex coordination has not been
solved.
? Scope of the extraction: from the sen-
tence Nitrobenzene is used in the man-
ufacture of dyes, the system extracts
USE(Nitrobenzene,manufacture,in,NONE,NONE),
because the PP of dyes was not expected in the
structure of the USE predicate.
? Domain-knowledge coverage: form the sen-
tence Acetone completely miscible in water and
soluble in organics., the system extract PROP-
ERTY(Acetone,dissolve,in,organic,NONE), because
soluble is encoded as a property equivalent to
dissolve in the lexical relations for paraphras-
ing. However, it should also extract PROP-
ERTY(Acetone,mix,in,water,NONE), but miscible
was not coded as a possible chemical property
adjective.
From the evaluation results, it appears that further
developments need to focus on recall improvement.
This could be achieved by:
? extending our paraphrase detection module:
Some equivalences have not been yet consid-
ered. For instance, take fire which did not ap-
pear in the working corpus, appeared in the test
corpus. This expression had not been coded
as a possible equivalent of burn, therefore ex-
pected information about the physical property
of burning for a given element is missing when
this property is expressed in the text by take
fire;
? enriching the ontological knowledge of the do-
main;
? Improving structural ambiguity resolution:
Coordination and PP attachment resolution
could be improved by the development of
more fine-grained semantic and ontological re-
sources.
4.2 Impact of the normalization and corpus
oriented paraphrase modules
This second experiment was intended to verify in
what extent the normalization and paraphrase detec-
tion module affect the results obtained in the previ-
ous evaluation. This test was performed taking away
from the complete processing chain, the modules de-
scribed in sections 3.2 and 3.3.2. The results show
that we only obtained about 60% of the predicates
found in the first version. In other words, without
these processing steps, recall decreases in a dramatic
way. All predicates found in this second experiment
were also found in the first. Missing predicates in
the second experiment were the most complex to ex-
tract (i.e. USE, PROPERTY, ORIGIN), since they
intensively involve reformulations and lexical equi-
valences.
5 Conclusion
In this paper, we have presented a methodology for
extracting information using symbolic methods. In-
formation extraction consists here in normalization
of syntactic processing using both deep syntactic
and morphological information as well as corpus
specific knowledge. As the kind of corpus under
consideration is very rich in reformulations, we were
able to verify that our system could be used to de-
tect paraphrases in the domain of the corpus. In fact,
paraphrase detection can be seen as a side effect of
normalization, as utterances conveying similar infor-
mation are represented the same way. This is an on-
going work but the first results we obtained for infor-
mation extraction are really encouraging, although
many improvements seem to be necessary. We fore-
see to continue our experiment applying our system
on a different collection of texts from the same do-
main. We also plan to improve the current coverage
of our system having in mind the results of the first
evaluation.
Acknowledgments
We would like to thank our colleagues Jean-Pierre
Chanod, Marc Dymetman, Aaron Kaplan and ?Agnes
Sa?ndor for their careful reading and helpful com-
ments on this paper.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod and Claude
Roux. 2002. Robustness beyond shallowness: incre-
mental dependency parsing. Special issue of the NLE
Journal.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus Proceed-
ings of the ACL 2001 Toulouse, France.
Ralf Grishman, Catherine Macleod, and Adam Meyers.
1994 COMLEX: building a computational lexicon In
Proceedings of the 15th International Conference on
Computational Linguistics (COLING 1994), Comlex.
Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi,
and Satoshi Sato. 2001. Verb Paraphrase based on
Case Frame Alignment Proceedings of the Workshop
on Automatic Paraphrasing. NLPRS 2001, Tokyo,
Japan.
Beth Levin. 1993. English Verb Classes and Alterna-
tions - A Preliminary Investigation. The University of
Chicago Press.
Kathleen R. McKeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards Multidocument Summarization by Re-
formulation: Progress and Prospects. AAAI/IAAA.
Carolyn P. Rose?. 2000. A syntactic framework for Se-
mantic Interpretation. Proceedings of the 1st meeting
of the North American Chapter of the Association for
Computational Linguistics. Seattle, Washington.
Celex 2000. http://www.kun.nl/celex/index.html.
Tetsuro Takahashi, Tomoyam Iwakura, Ryu Iida, Atsushi
Fujita and Kentaro Inui. 2001. KURA: A Transfer-
Based Lexico-Structural Paraphrasing Engine. Pro-
ceedings of the Workshop on Automatic Paraphrasing.
NLPRS 2001, Tokyo, Japan.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 488?491,
Prague, June 2007. c?2007 Association for Computational Linguistics
XRCE-M: A Hybrid System for Named Entity Metonymy Resolution 
*Caroline Brun 
 
*Maud Ehrmann 
 
*Guillaume Jacquet 
 
 
* Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240 Meylan France 
*{Caroline.Brun, Maud.Ehrmann, Guillaume.Jacquet}@xrce.xerox.com 
 
Abstract 
This paper describes our participation to the 
Metonymy resolution at SemEval 2007 (task 
#8). In order to perform named entity me-
tonymy resolution, we developed a hybrid 
system based on a robust parser that extracts 
deep syntactic relations combined with a 
non-supervised distributional approach, also 
relying on the relations extracted by the 
parser.  
1 Description of our System 
SemEval 2007 introduces a task aiming at resolving 
metonymy for named entities, for location and or-
ganization names (Markert and Nissim 2007). Our 
system addresses this task by combining a symbolic 
approach based on robust deep parsing and lexical 
semantic information, with a distributional method 
using syntactic context similarities calculated on 
large corpora. Our system is completely unsuper-
vised, as opposed to state-of-the-art systems (see  
(Market and Nissim, 2005)).  
1.1 Robust and Deep Parsing Using XIP 
We use the Xerox Incremental Parser (XIP, (A?t et 
al., 2002)) to perform robust and deep syntactic 
analysis. Deep syntactic analysis consists here in the 
construction of a set of syntactic relations1 from an 
input text.  These relations, labeled with deep syn-
tactic functions, link lexical units of the input text 
and/or more complex syntactic domains that are 
constructed during the processing (mainly chunks, 
see (Abney, 1991)).  
                                                 
                                                
1 inspired from dependency grammars, see (Mel??uk, 
1998), and (Tesni?re, 1959). 
Moreover, together with surface syntactic relations, 
the parser calculates more sophisticated relations 
using derivational morphologic properties, deep 
syntactic properties2, and some limited lexical se-
mantic coding (Levin's verb class alternations, see 
(Levin, 1993)), and some elements of the Framenet3 
classification, (Ruppenhofer et al, 2006)). These 
deep syntactic relations correspond roughly to the 
agent-experiencer roles that is subsumed by the 
SUBJ-N relation and to the patient-theme role sub-
sumed by the OBJ-N relation, see (Brun and  Ha-
g?ge, 2003). Not only verbs bear these relations but 
also deverbal nouns with their corresponding argu-
ments.  
Here is an example of an output (chunks and 
deep syntactic relations): 
Lebanon still wanted to see the implementation of a UN 
resolution 
 
TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the 
implementation} PP{of NP{a UN resolution}} .} 
MOD_PRE(wanted,still) 
MOD_PRE(resolution,UN) 
MOD_POST(implementation,resolution) 
COUNTRY(Lebanon) 
ORGANISATION(UN) 
EXPERIENCER_PRE(wanted,Lebanon) 
EXPERIENCER(see,Lebanon) 
CONTENT(see,implementation) 
EMBED_INFINIT(see,wanted) 
OBJ-N(implement,resolution) 
1.2 Adaptation to the Task 
Our parser includes a module for ?standard? 
named entity recognition, but needs to be adapted to 
handle named entity metonymy. Following the 
guidelines of the SemEval task #8, we performed a 
 
2 Subject and object of infinitives in the context of con-
trol verbs. 
3 http://framenet.icsi.berkeley.edu/ 
488
corpus study on the trial data in order to detect lexi-
cal and syntactic regularities triggering a metonymy, 
for both location names and organization names. 
For example, we examined the subject relation be-
tween organizations or locations and verbs and we 
then classify the verbs accordingly: we draw hy-
pothesis like ?if a location name is the subject of a 
verb referring to an economic action, like import, 
provide, refund, repay, etc., then it is a place-for-
people?. We adapted our parser by adding dedicated 
lexicons that encode the information collected from 
the corpus and develop rules modifying the interpre-
tation of the entity, for example:  
 
 If (LOCATION(#1) & SUBJ-N(#2[v_econ],#1))4
 ? PLACE-FOR-PEOPLE(#1) 
 
We focus our study on relations like subject, object, 
experiencer, content, modifiers (nominal and prepo-
sitional) and attributes.  We also capitalize on the 
already-encoded lexical information attached to 
verbs by the parser, like communication verbs like 
say, deny, comment, or categories of the FrameNet 
Experiencer subject frame, i.e. verbs like feel, sense, 
see. This information was very useful since experi-
encers denote persons, therefore all organizations or 
locations having an experiencer role can be consid-
ered as organization-for-members or place-for-
people. Here is an example of output5, when apply-
ing the modified parser on the following sentence: 
?It was the largest Fiat everyone had ever seen?. 
ORG-FOR-PRODUCT(Fiat) 
MOD_PRE(seen,ever) 
SUBJ-N_PRE(was,It) 
EXPERIENCER_PRE(seen,everyone) 
SUBJATTR(It,Fiat) 
    QUALIF(Fiat,largest)  
 
Here, the relation QUALIF(Fiat, largest) triggers 
the metonymical interpretation of ?Fiat? as org-for-
product. 
This first development step is the starting point of 
our methodology, which is completed by a non-
supervised distributional approach described in the 
next section.  
                                                 
4 Which read as ?if the parser has detected a location 
name (#1), which is the subject of a verb (#2) bearing the 
feature ?v-econ?, then create a PLACE-FOR-PEOPLE 
unary predicate on #1.  
5 Only dependencies are shown. 
1.3 Hybridizing with a Distributional Approach 
The distributional approach proposes to establish a 
distance between words depending on there syntac-
tic distribution. 
The distributional hypothesis is that words that ap-
pear in similar contexts are semantically similar 
(Harris, 1951): the more two words have the same 
distribution, i.e. are found in the same syntactic con-
texts, the more they are semantically close. 
We propose to apply this principle for metonymy 
resolution. Traditionally, the distributional approach 
groups words like USA, Britain, France, Germany 
because there are in the same syntactical contexts:  
 
 (1) Someone live in Germany. 
(2) Someone works in Germany. 
(3) Germany declares something. 
(4) Germany signs something. 
 
The metonymy resolution task implies to distin-
guish the literal cases, (1) & (2), from the meto-
nymic ones, (3) & (4). Our method establishes these 
distinctions using the syntactic context distribution. 
We group contexts occurring with the same words: 
the syntactic contexts live in and work in are occur-
ring with Germany, France, country, city, place, 
when syntactic contexts subject-of-declare and sub-
ject-of-sign are occurring with Germany, France, 
someone, government, president. 
For each Named Entity annotation, the hybrid 
method consists in using symbolic annotation if 
there is (?1.2), else using distributional annotation 
(?1.3) as presented below. 
Method: We constructed a distributional space with 
the 100M-word BNC. We prepared the corpus by 
lemmatizing and then parsing with the same robust 
parser than for the symbolic approach (XIP, see sec-
tion 3.1). It allows us to identify triple instances. 
Each triple have the form w1.R.w2 where w1 and 
w2 are lexical units and R is a syntactic relation 
(Lin, 1998; Kilgarriff & al. 2004).  
Our approach can be distinguished from classical 
distributional approach by different points. 
First, we use triple occurrences to build a distribu-
tional space (one triple implies two contexts and 
two lexical units), but we use the transpose of the 
classical space: each point xi of this space is a syn-
tactical context (with the form R.w.), each dimen-
sion j is a lexical units, and each value xi(j) is the 
frequency of corresponding triple occurrences. Sec-
489
ond, our lexical units are words but also complex 
nominal groups or verbal groups. Third, contexts 
can be simple contexts or composed contexts6. 
We illustrate these three points on the phrase pro-
vide Albania with food aid. The XIP parser gives 
the following triples where for example, food aid is 
considered as a lexical unit: 
OBJ-N('VERB:provide','NOUN: Albania'). 
PREP_WITH('VERB: provide ','NOUN:aid'). 
PREP_WITH('VERB: provide ','NP:food aid'). 
From these triples, we create the following lexical 
units and contexts (in the context 1.VERB: provide. 
OBJ-N, ?1? mean that the verb provide is the gov-
ernor of the relation OBJ-N): 
Words: Contexts: 
VERB:provide 1.VERB: provide. OBJ-N 
NOUN:Albania 1.VERB: provide.PREP_WITH 
NOUN:aid 2.NOUN: Albania.OBJ-N 
NP:food aid 2.NOUN: aid. PREP_WITH 
 2.NP: food aid. PREP_WITH 
 1.VERB:provide.OBJ-N+2.NOUN:aid. PREP_WITH 
 1.VERB:provide.OBJ-N+2.NP:food aid. PREP_WITH 
 1.VERB:provide.PREP_WITH +2.NO:Albania.OBJ-N 
 
We use a heuristic to control the high productivity 
of these lexical units and contexts. Each lexical unit 
and each context should appear more than 100 times 
in the corpus. From the 100M-word BNC we ob-
tained 60,849 lexical units and 140,634 contexts. 
Then, our distributional space has 140,634 units and 
60,849 dimensions. 
Using the global space to compute distances be-
tween each context is too consuming and would 
induce artificial ambiguity (Jacquet, Venant, 2005). 
If any named entity can be used in a metonymic 
reading, in a given corpus each named entity has not 
the same distribution of metonymic readings. The 
country Vietnam is more frequently used as an event 
than France or Germany, so, knowing that a context 
is employed with Vietnam allow to reduce the meto-
nymic ambiguity. 
For this, we construct a singular sub-space de-
pending to the context and to the lexical unit (the 
ambiguous named entity): 
For a given couple context i + lexical unit j we 
construct a subspace as follows:  
Sub_contexts = list of contexts which are occur-
ring with the word i. If there are more than k con-
texts, we take only the k more frequents. 
Sub_dimension = list of lexical units which are 
occurring with at least one of the contexts from the 
                                                 
6 For our application, one context can be composed by 
two simple contexts. 
Sub_contexts list. If there are more than n words, 
we take only the n more frequents (relative fre-
quency) with the Sub_contexts list (for this applica-
tion, k = 100 and n = 1,000). 
We reduce dimensions of this sub-space to 10 
dimensions with a PCA (Principal Components 
Analysis). 
In this new reduced space (k*10), we compute 
the closest context of the context j with the Euclid-
ian distance. 
At this point, we use the results of the symbolic 
approach described before as starting point. We at-
tribute to each context of the Sub_contexts list, the 
annotation, if there is, attributed by symbolic rules. 
Each kind of annotation (literal, place-for-people, 
place-for-event, etc) is attributed a score corre-
sponding to the sum of the scores obtained by each 
context annotated with this category. The score of a 
context i  decreases in inverse proportion to its dis-
tance from the context j: score(context i) = 
1/d(context i, context j) where d(i,j) is the Euclidian 
distance between i and j. 
We illustrate this process with the sentence pro-
vide Albania with food aid. The unit Albania is 
found in 384 different contexts (|Sub_contexts| = 
384) and 54,183 lexical units are occurring with at 
least one of the contexts from the Sub_contexts list 
(|Sub_dimension| = 54,183). 
After reducing dimension with PCA, we obtain 
the context list below ordered by closeness with the 
given context (1.VERB:provide.OBJ-N):  
Contexts   d symb. annot. 
1.VERB:provide.OBJ-N  0.00  
1.VERB:allow.OBJ-N  0.76         place-for-people 
1.VERB:include.OBJ-N  0.96  
2.ADJ:new.MOD_PRE  1.02  
1.VERB:be.SUBJ-N  1.43  
1.VERB:supply.SUBJ-N_PRE 1.47 literal 
1.VERB:become.SUBJ-N_PRE 1.64  
1.VERB:come.SUBJ-N_PRE  1.69  
1.VERB:support.SUBJ-N_PRE 1.70          place-for-people 
etc. 
 
Score for each metonymic annotation of Albania: 
? place-for-people 3.11 
 literal  1.23 
place-for-event  0.00 
?  0.00 
The score obtained by each annotation type al-
lows annotating this occurrence of Albania as a 
place-for-people metonymic reading. If we can?t 
choose only one annotation (all score = 0 or equal-
ity between two annotations) we do not annotate.  
490
2 Evaluation and Results 
The following tables show the results on the test 
corpus: 
type Nb. 
samp 
accuracy coverage Baseline 
accuracy 
Baseline 
coverage 
Loc/coarse 908 0.851 1 0.794 1 
Loc/medium 908 0.848 1 0.794 1 
Loc /fine 908 0.841 1 0.794 1 
Org/coarse 842 0.732 1 0.618 1 
Org/medium 842 0.711 1 0.618 1 
Org/fine 842 0.700 1 0.618 1 
Table 1: Global Results 
 
 Nb 
occ. 
Prec. Recall F-score
Literal 721 0.867 0.960 0.911 
Place-for-people 141 0.651 0.490 0.559 
Place-for-event 10 0.5 0.1 0.166 
Place-for-product 1 _ 0 0 
Object-for-name 4 1 0.5 0.666 
Object-for-representation 0 _ _ _ 
Othermet 11 _ 0 0 
mixed 20 _ 0 0 
Table 2: Detailed Results for Locations 
 
 Nb 
occ. 
Prec. Recall F-score
Literal 520 0.730 0.906 0.808 
Organization-for-members 161 0.622 0.522 0.568 
Organization-for-event 1 _ 0 0 
Organization-for-product 67 0.550 0.418 0.475 
Organization-for-facility 16 0.5 0.125 0.2 
Organization-for-index 3 _ 0 0 
Object-for-name 6 1 0.666 0.8 
Othermet 8 _ 0 0 
Mixed  60 _ 0 0 
Table 3: Detailed Results for Organizations 
 
The results obtained on the test corpora are above 
the baseline for both location and organization 
names and therefore are very encouraging for the 
method we developed. However, our results on the 
test corpora are below the ones we get on the train 
corpora, which indicates that there is room for im-
provement for our methodology.  
Identified errors are of different nature: 
Parsing errors: For example in the sentence ?Many 
galleries in the States, England and France de-
clined the invitation.?, because the analysis of the 
coordination is not correct, France is calculated as 
subject of declined, a context triggering a place-for-
people interpretation, which is wrong here.  
Mixed cases: These phenomena, while relatively 
frequent in the corpora, are not properly treated. 
Uncovered contexts: some of the syntactico-
semantic contexts triggering a metonymy are not 
covered by the system at the moment.  
3 Conclusion 
This paper describes a system combining a sym-
bolic and a non-supervised distributional approach, 
developed for resolving location and organization 
names metonymy. We plan to pursue this work in 
order to improve the system on the already-covered 
phenomenon as well as on different names entities.  
References 
Abney S. 1991. Parsing by Chunks.  In Robert Berwick, Steven 
Abney and Carol Teny (eds.). Principle-based Parsing, Klu-
wer Academics Publishers.  
A?t-Mokhtar S., Chanod, J.P., Roux, C. 2002. Robustness be-
yond Shallowness: Incremental Dependency Parsing. Spe-
cial issue of NLE journal.  
Brun, C., Hag?ge C., 2003. Normalization and Paraphrasing 
Using Symbolic Methods, Proceeding of the Second Interna-
tional Workshop on Paraphrasing. ACL 2003, Vol. 16, Sap-
poro, Japan.  
Harris Z. 1951. Structural Linguistics, University of Chicago 
Press. 
Jacquet G.,Venant F. 2003. Construction automatique de clas-
ses de s?lection distributionnelle, In Proc. TALN 2003, 
Dourdan. 
Kilgarriff A., Rychly P., Smrz P., Tugwell D.  2004. The sketch 
engine. In Proc. EURALEX, pages 105-116. 
Levin, B. 1993. English Verb Classes and Alternations ? A 
preliminary Investigation. The University of Chicago Press.  
Nissim, M. and Markert, K. 2005. Learning to buy a Renault 
and to talk to a BMW: A supervised approach to conven-
tional metonymy. Proceedings of the 6th International Work-
shop on Computational Semantics, Tilburg. 
Nissim, M. and Markert, K. 2007. SemEval-2007 Task 08: Me-
tonymy Resolution at SemEval-2007. In Proceedings of Se-
mEval-2007.  
Lin D. 1998. Automatic retrieval and clustering of similar 
words. In COLING-ACL, pages 768-774. 
Mel??uk I. 1988. Dependency Syntax. State University of New 
York, Albany.  
Ruppenhofer, J. Michael Ellsworth, Miriam R. L. Petruck, 
Christopher R Johnson and Jan Scheffczyk. 2006. Framenet 
II: Extended Theory and Practice.  
Tesni?re L. 1959. El?ments de Syntaxe Structurale. Klincksiek 
Eds. (Corrected edition Paris 1969).  
491
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1764?1772, Dublin, Ireland, August 23-29 2014.
Part of Speech Tagging for French Social Media Data
Farhad Nooralahzadeh
?
Ecole Polytechnique
de Montr?eal
Montr?eal, PQ, Canada
nooralahzadeh
@gmail.com
Caroline Brun
Xerox Research Centre
Europe
Meylan, France
caroline.brun
@xrce.xerox.com
Claude Roux
Xerox Research Centre
Europe
Meylan, France
claude.roux
@xrce.xerox.com
Abstract
In the context of Social Media Analytics, Natural Language Processing tools face new chal-
lenges on on-line conversational text, such as microblogs, chat, or text messages, because of the
specificity of the language used in these channels. This work addresses the problem of Part-
Of-Speech tagging (initially for French but also for English) on noisy language usage from the
popular social media services like Twitter, Facebook and forums. We employ a linear-chain con-
ditional random fields (CRFs) model, enriched with several morphological, orthographic, lexical
and large-scale word clustering features. Our experiments used different feature configurations
to train the model. We achieved a higher tagging performance with these features, compared to
baseline results on French social media bank. Moreover, experiments on English social media
content show that our model improves over previous works on these data.
1 Introduction
There are many challenges inherent to applying standard natural language analysis techniques to social
media. On-line conversational texts, such as tweets are quite challenging for text mining tools, and in
particular for opinion mining, as they contain very little contextual information and assume too much
implicit knowledge. They expose much more language variation and tend to be less grammatical than
regular texts such as news articles or books. Furthermore, they contain unusual capitalization, and make
frequent use of emoticons, abbreviations and hash-tags, which can form an important part of their in-
ner meaning (Maynard et al., 2012). Conventional natural language processing tools for regular texts
have achieved reasonably high accuracy thanks to machine learning techniques on large annotated data
set. However, ?off the shelf? language processing systems fail to work on social media data and their
performance on this domain degrade very fast. For example, in English Part-Of-Speech tagging, the
accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to
85% accuracy on Twitter (Gimpel et al., 2011), similarly the MElt POS tagger (Denis and Sagot, 2012)
drops from 97.7% on the French Treebank (called the FTB-UC by (Candito and Crabb?e, 2009)) to 85.2%
on on-line conversational texts (Seddah et al., 2012). In Named Entity Recognition, the CoNLL-trained
Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test
set (Finkel et al., 2005); regarding parsing, see for example (Foster et al., 2011; Seddah et al., 2012),
poor performances have been reported for different state-of-the-art parsers applied to English and French
social media content.
The main objective of this work is to implement a dedicated Part-Of-Speech (POS) tagger for French
social media content such as Twitter, Facebook, blogs, forums and customer reviews. We used the
first user-generated content resource for French presented by Seddah et al. (2012), which contains a
fine-grained tag set and has been extracted from various social media contents. We have designed and
implemented a POS tagger considering one of the well-known discriminative type of sequence-based
methods; Conditional Random Fields (CRF) (Lafferty et al., 2001). To deal with sparsity and unknown
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1764
words, we have applied unsupervised techniques to enrich the feature set. Finally, we have evaluated our
tagger performance with different configurations on annotated corpora from French social media.
We will first present related work in Part-Of-Speech tagging (Section 2) on noisy data like social media
content. In Section 3, the annotated dataset and its characteristics (e.g., tag set) are described. Section
4 presents the result of applying the MElt POS tagger to user generated text as our baseline (Seddah et
al., 2012). In Section 5, we explain how we design and implement our POS tagger. Section 6 is devoted
to experiments and performance of our tagger. Section 7 describes the evaluation of the new tagger on
English social media texts. Conclusion and future work are given in Section 8.
2 Related work
Online conversational texts, typified by micro-blogs, chat, and text messages, are a challenge for natural
language processing. Unlike the highly edited genres for which conventional NLP tools have been de-
veloped, conversational texts contain many non-standard lexical items and syntactic patterns. These are
the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative
use of language and orthography (Eisenstein, 2013)
The language technology research community proposes two approaches to deal with noisy texts,
namely normalization and domain adaptation, which are briefly described here.
2.1 Normalization
One way to deal with ill-formed language is to turn it into a well-formed language as a pre-processing
task: ?normalizing? social media or SMS messages to better conform to the language that the technology
expects. For example, (Han and Baldwin, 2011) propose the lexical normalization of short text messages,
such as tweets, based on string and distributional similarity. They describe a method to identify and
normalize ill-formed words. Word similarity and context are exploited to select the best candidate for
noisy tokens.
2.2 Domain adaptation
The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold
of ?NLP for Twitter,? including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity
recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foster et al., 2011),
dialog modeling (Ritter et al., 2010) and summarization (Hutton and Kalita, 2010). These works adapt
various parts of the natural language processing pipeline for social media text, and make use of a range
of techniques (Preprocessing, New labeled data, New annotation schemes, Self training, Distributional
features, Distance supervision) (Eisenstein, 2013).
Recently, Seddah et al. (2012) followed the second approach on French social media content and
provided new labeled data and annotation schemes. They applied the MElt POS tagger (Denis and Sagot,
2012) embedded within text normalization and correction to noisy user generated texts and presented
baseline POS tagging and statistical constituency parsing results.
3 Annotated Dataset
A set of 1,700 sentences (38k tokens) has been extracted from various types of French Web 2.0 user
generated content (Facebook, Twitter, Video games and medical web forums) by Seddah et al. (2012).
They selected these corpora through direct examination of various search queries and ranked the texts
according to their distance from the French Treebank style, by measuring noisiness using the kullback-
Leibler divergence between the distribution of trigrams of characters in given corpus and the distribution
of trigrams of characters in the French Treebank reference. Some properties of this corpora are shown in
Table 1.
They targeted the annotation scheme of the FTB-UC in order to annotate the French social media
bank. The tagset includes 28 POS tags from FTB-UC and compound tags with additional categories
specific to social media, including HT for Twitter hashtags and META for meta-textual tokens, such as
1765
Twitter?s ?RT?. Twitter at-mention as well as URLs and e-mail addresses have been tagged NPP which
is the main difference with other works on on-line conversational texts. The inter-annotator agreement
rate in this corpora range between 93.4% for FACEBOOK data and 97.44% for JEUXVIDEOS.COM
(Table 1) which indicates an almost perfect agreement on the corpus (Landis and Koch, 1977).
Corpus Name # sent. # tokens Inter Annotator Agreement %
TWITTER 216 2465 95.40
FACEBOOK 452 4200 93.40
JEUXVIDEOS.COM 199 3058 97.44
DOCTISSIMO 771 10834 95.05
Table 1: Annotated datasets
4 Baseline
This section presents the performance of a state-of-the-art POS tagger for French, conducted by Seddah
et al. (2012). They used FTB-UC as training, development and test data. First, they applied several
correction processes in order to wrap the POS tagger to tag a sequence of tokens as close as possible to
standard French and training corpus. Then, the MElt tagger has been used with a set of 15 language-
independent rules, that aim at assigning the correct POS to tokens that belong to categories not found
in training corpus (e.g., URLs, e-mail addresses, emoticons). The preliminary evaluation experiments
with normalization and correction wrapper showed 84.72% and 85.28% token accuracy over annotated
development and test set respectively.
5 New POS Tagger Development
Conversational style context and 140-character limitation in micro-blogs require users to express their
thought or reply to others? messages within a short text. Therefore, without being ambiguous, some
words are usually abbreviated with a special spelling. For example, c t usually means c??etait (it was); qil
denotes qu? il (that it/he).
Our tagger is based on sequence labeling models (CRF), enabling arbitrary local features to be inte-
grated into a log-linear model. We employed three categories of feature templates to deal with syntactic
variations on social media contents and alleviating the data sparseness problem.
5.1 Basic Feature Templates
The feature templates we use here are a superset of the largely language independent features used by
(Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). These features fall into
two main categories. A first set of features tries to capture the lexical form of the word being tagged:
it includes prefixes and suffixes (of at most 10 characters) from the current word, together with binary
features based on the presence of special characters such as numbers, hyphens, and uppercase letters,
within w
i
. A second set of features directly models the context of the current word and tag: it includes
the previous tag, surrounding word forms in a 5 tokens window. The detailed list of feature templates we
used in this category is shown in Table 2.
1
Context
w
i
= X ,i ? [?2,?1, 0, 1, 2] & t
0
= T
w
i
w
j
= XY , (i, j) ? {(?1, 0), (0, 1), (?2, 0), (0, 2)} & t
0
= T
w
i
w
j
w
k
= XY Z , (i, j, k) ? {(?2,?1, 0), (0, 1, 2), (?1, 0, 1)} & t
0
= T
w
i
w
j
w
k
w
l
w
m
=XY ZPQ , (i, j, k, l,m) =(?2,?1, 0, 1, 2) & t
0
= T
t
?1
& t
0
= T
Lexical and Orthographic
f(w
i
),i ? [?1, 0, 1] ,f ? F & t
0
= T
m(w
i
), i ? [?1, 0, 1], m ?M & t
0
= T
Table 2: Basic Feature Templates
1
w
0
means the token at the current position while w
?1
means the previous token.
1766
The model generates the feature space by scanning each pair in the training data with the feature
templates given in Table 2. For example, if we consider the following tweet from the training set, the
generated features based on the first template can be seen in Table 3, in which the current word is ?vous?
(position 6) .
Sample tweet : ?@Marie Je vais tener De vous produire la vid?eo *-* ?
word: @Marie Je vais tener De vous produire la vid?eo *-*
Tag: NPP CLS-SUJ V VINF P CLO-A OBJ VINF DET NC I
Position: 1 2 3 4 5 6 7 8 9 10
w
0
=vous &t
0
=O
w
?1
=De &t
0
=O
w
?2
=tener &t
0
=O
w
+1
=produire &t
0
=O
w
+2
=la &t
0
=O
Table 3: Generated features with template :
w
i
= X ,i ? [?2,?1, 0, 1, 2] &t
0
= T
We defined two sets of operations, F and M . Each operation maps tokens to equivalence classes.
F is a set of regular expression rules that detect specific patterns on w
i
and return binary values. The
functions f(w
i
) ? F include the rules as detailed in the following list (List 1):
List 1: Set of regular expression rules (F )
. Return ?True? if the w
i
contains Punctuation marks otherwise return ?False?
. Return ?True? if the w
i
is list of Punctuation marks otherwise return ?False?
. Return ?True? if the w
i
contains digits otherwise return ?False?
. Return ?True? if the w
i
number otherwise return ?False?
. Return ?True? if all letters of w
i
are capitalized otherwise return ?False? allNumber
. Return ?True? if the w
i
starts with capital letter otherwise return ?False?
. Return ?True? if the w
i
has?URL? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Email? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Abbreviation? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Arrow? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Time ? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?NumberWithCommas? pattern otherwise return ?False?
. Return ?True? if the w
i
has symbol representing ?RT:retweeting? form otherwise return ?False?
. Return ?True? if the w
i
has symbol representing ?At-Mention? form otherwise return ?False?
. Return ?True? if the w
i
has symbol representing ?hash-tagh? form otherwise return ?False?
M is a set of orthographic transformations that maps a string to another string via a simple surface
level transformation. The functions m(w
i
) ?M are given in List 2 :
List 2: Set of orthographic transformation (M )
. Return capitalized type of w
i
,These types are (allCap, shortCap, longCap, noCap, initCap, mixCap)
(e.g.,?Plus-tard?? ?initCap? ,?RT???allCap,longCap? )
. Return the type of w
i
, obtained by replacing [a? z] with x, [A? Z] with X , and [0? 9] with 9
(e.g.,., ?@DJRyan1der?? ?@XXXxxx9xxx?)
. Return a vector of Unicode matching of the string w
i
(e.g., ?@DJRyan1der?? ?[64? 68? 74? 82? 121? 97? 110? 49? 100? 101? 114]?)
. Return the first n character of x (n-gram prefix), where 1 ? n ? 10
. Return the last n character of x (n-gram suffix), where 1 ? n ? 10
5.2 Word Clustering Feature Templates
To bridge the gap between high and low frequency words, we employed word clustering to acquire
knowledge about paradigmatic lexical relations from large-scale texts. Our work is inspired by the suc-
1767
cessful application of word clustering in supervised NLP models (Miller et al., 2004; Turian et al., 2010;
Ritter et al., 2011; Owoputi et al., 2013).
Various clustering techniques have been proposed, some of which, for example, perform automatic
word clustering optimizing a maximum likelihood criterion with iterative clustering algorithms. In this
work, we focus on distributional word clustering, based on the assumption that the words that appear in
similar contexts (especially surrounding words) tend to have similar meanings.
5.2.1 Brown Clustering
We used our unlabeled Twitter corpus (4M tweets) to improve our tagger performance. This corpus
has been extracted in the framework of a French government funded ANR project called Imagiweb,
whose goal is to develop tools to analyse the brand image of entities (persons or companies) on social
media. More specifically, one of the focus of the project is to analyse the brand image of politicians on
Twitter. Therefore, data about the two main candidates (F. Hollande and N. Sarkozy) in the last French
presidential election in May 2012 have been crawled from Twitter, using Twitter API, from 6 months
before to 6 months after the elections. Our unlabeled Twitter data is a sub-set of this corpus.
We obtained hierarchical word clusters via Brown Clustering (Brown et al., 1992) on a large set of
unlabeled tweets. This algorithm generates a hard clustering, each word belongs to exactly one cluster.
The input to the algorithm is a sequence of words w
i
, . . . , w
n
. Initially, the algorithm starts with each
word in its own cluster. As long as there are at least two clusters left, the algorithm merges the two
clusters that maximize the resulting cluster quality. The quality is defined on the class-based bigram
language model as follows, where C maps a word w to its class C(w).
p(w
i
|w
1
, . . . , w
i?1
) = p(C(w
i
)|C(w
i?1
))p(w
i
|C(w
i
))
We ended up with 500 clusters (the optimal number of clusters according to the performance of the
tagger among different number of clusters) with 222,788 word types by keeping the words appearing 10
or more times. Since Brown clustering creates hierarchical clusters in a binary tree, we used the feature
template which maps the word w
i
to the cluster at depths 2, 4, . . . , 16 containing w
i
. If w
i
was not seen
while constructing the clusters and thus does not belong to any cluster we tried to find similar words
by computing Jaro-Winkler distance (Philips, 1990; Winkler, 2006) and mapped the best match to the
cluster depths. Nevertheless, if we couldn?t find the best match (the threshold of the similarity score is
0.9), we mapped it to a special NULL cluster. The detailed list of feature templates we used in this
category is shown in Table 5.
2
5.2.2 MKCLS Clustering
We also did some experiments, using another popular clustering method based on the exchange algorithm
(Kneser and Ney, 1993). The objective function maximizes the likelihood
?
n
i=1
P (w
i
|w
1
, . . . , w
i?1
) of
the training data given a partially class-based bigram model of the form as follows:
p(w
i
|w
1
, . . . , w
i?1
) ? p(C(w
i
)|w
i?1
)p(w
i
|C(w
i
))
We use the publicly available implementation MKCLS
3
to train this model on our French Twitter data
(4M tweets). This algorithm provides us with 500 word clusters with 2,768,297 different words.
Word Cluster
c(w
i
) = X ,i ? [?2,?1, 0, 1, 2] and c ? C & t
0
= T
c(w
i
)c(w
j
) = XY , (i, j) ? {(?1, 0), (0, 1)} and c ? C & t
0
= T
c(w
i
)C(w
j
)c(w
k
) = XY Z , (i, j, k) ? {(?2,?1, 0), (0, 1, 2), (?1, 0, 1)}
and c ? C
& t
0
= T
c(w
i
)c(w
j
)c(w
k
)c(w
l
)c(w
m
)=XY ZPQ , (i, j, k, l,m) =(?2,?1, 0, 1, 2) and
c ? C
& t
0
= T
Table 5: Word Clustering Feature Templates
2
c(w
i
) ? C map the word w
i
to the clusters at depths 2, 4, . . . , 16
3
https://code.google.com/p/giza-pp/
1768
6 Experiments
For the implementation of discriminative sequential model, we chose the Wapiti
4
toolkit (Lavergne et al.,
2010). Wapiti is a very fast toolkit for segmenting and labeling sequences with discriminative models.
It is based on maxent models, maximum entropy Markov models and linear-chain CRF and proposes
various optimization and regularization methods to improve both the computational complexity and the
prediction performance of standard models. Wapiti has been ranked first on the sequence tagging task
for more than a year on MLcomp
5
web site.
6.1 Training and parameter regularization
In the training of log-linear models, regularization is normally required to prevent the model from over
fitting on the training data. The two most common regularization methods are called L1 and L2 regular-
ization (Tsuruoka et al., 2009). Wapiti uses the elastic-net penalty of the form:
?
1
? |?|
1
+
?
2
2
? ||?||
2
2
and it is implemented with 3 different algorithms: Orthant-Wise Limited-memory Quasi-Newton (OWL-
QN: L-BFGS), Stochastic Gradient Descent (SGD) and Block Coordinate Descent. We trained with
L-BFGS, a classical Quasi-Newton optimization algorithm with limited memory which minimizes the
regularized objective and uses elastic net regularization. Using even a very small L1 penalty excludes
many irrelevant or highly noisy features. We carried out a grid search for the regularization values,
assessing with F-measure and accuracy. We conducted a first order linear chain CRF model on the
French corpora with classical setting (training set: 80%, development set: 10% and test set: 10%) for
L1 ? {0, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16} and L2 ? {0, 0.0325, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16} (Owoputi
et al., 2013). In any experiment, the result of the regularization values were close to each other, there-
fore we selected L1,L2=(0.25, 0.5) achieving 80.4% and 90.6% F-measure and accuracy on the corpora
respectively.
6.2 Performance
In order to assess how the results of our tagger based on the current limited corpora could be general-
ized to an independent data set, a set of 10-fold cross validation experiments has been performed. We
investigated the effect of each feature template on the tagging. We used ?c: compact? option in Wapiti
which enables model compaction at the end of the training. This removes all inactive observations from
the model, leading to a much smaller model when an L1-penalty is used.
Table 6 shows the result of each experiment, measured by token and sentence accuracy. It shows that
word clustering is a very strong source of lexical knowledge and significantly increases the performance
of our tagger.
Feature Templates Token Accuracy % Sentence Accuracy %
B 88.2 45.8
B+C1 90.8 49.9
B+C2 90.3 50.3
B+C1+C2 91.9 51.1
B: Basic Feature Templates
C1: Brown word-Clustering Feature Templates
C2: MKCLS word-Clustering Feature Templates
Table 6: Performance of new tagger based on CRF with different configurations
The CRF model with all set of features (B+C1+C2) is the best model with 91.9% and 51.1% token
and sentence accuracy on 10-fold cross validation. All of these tagging accuracies are significantly above
previous results on the French social bank (baseline).
4
http://wapiti.limsi.fr/
5
http://mlcomp.org/
1769
7 Evaluation on English social media Content
In order to implement a tagger for English dedicated to social media content, we used the publicly avail-
able clusters data set (Owoputi et al., 2013) to build Brown clustering features. Moreover we performed
the same process as in Section 5.2.2 in order to provide MKCLS clustering features with English Twitter
data (1 million tweets obtained from
6
).
We applied our tagger with the best configuration to the annotated dataset provided by Ritter et al.
(2011). This dataset contains 800 tweets that have been annotated with the Penn Treebank (PTB) tagset
(Marcus et al., 1993). We trained and test our system with 10-fold cross validation. Table 7 shows our
tagger performance compared to other state-of-art taggers on this data set.
Tagger Accuracy%
Our new tagger, CRF with B+C1+C2 configuration 90.1
Ritter et al. (Ritter et al., 2011), CRF tagger 88.3
Owoputi et al. (Owoputi et al., 2013), MEMM tagger 90? 0.5
Table 7: Evaluation on Twitter data with PTB tags
In addition, we evaluated the tagger performance on another English social media data: NPS chat
(?Chat with PTB tags? (Forsythand and Martell, 2007)). Due to the large number of tokens (50 K), we
trained and tested our tagger with a 5-fold cross validation setup. Our new tagger performance as well
as the other taggers results are given in Table 8.
Tagger Accuracy%
Our new tagger, CRF with B+C1+C2 configuration 92.7
Forsythand and Martell (Forsythand and Martell, 2007), HMM tagger 90.8
Owoputi et al. (Owoputi et al., 2013), MEMM tagger 93.4? 0.3
Table 8: Evaluation on Chat data with PTB tags
8 Conclusion and Future Work
In this paper, we have presented an innovative work on POS tagging for French social media noisy
input. Because of the specific phenomena encountered in such data and also because of the lack of large
training corpus, we proposed a discriminative sequence labeling model (CRF) enhanced with several type
of features. After experimenting different configurations of features, we achieved 91.9% token accuracy
on target corpus. Moreover, experiments on English social media contents show that our model obtains
further improvement over previous works on these data and could be reproduced for other languages. In
the future, we plan to pursue this work in two main directions: (a) Integrate the new tagger with a robust
syntactic parser and investigate its impact on dependency parsing applied to social media and (b) evaluate
the impact of POS tagging on opinion mining on micro-blogs, since this parser is the core component of
an opinion mining system applied in different social-media analytics projects.
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Comput. Linguist., 18:467?479.
Marie Candito and Beno??t Crabb?e. 2009. Improving generative statistical parsing with semi-supervised word
clustering. In Proceedings of the 11th International Conference on Parsing Technologies, IWPT ?09, pages
138?141, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pascal Denis and Beno??t Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging.
Language Resources and Evaluation, 46:721?736.
6
http://illocutioninc.com/site/products-data.html
1770
Jacob Eisenstein. 2013. What to do about bad language on the internet. In proc. of NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Anno-
tating named entities in twitter data with crowdsourcing. Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 80?88.
Jenny R. Finkel, Trond Grenager, and Manning Christopher. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proceedings of ACL, pages 363?370.
Eric N. Forsythand and Craig H. Martell. 2007. Lexical and discourse analysis of online chat dialog. In Proceed-
ings of the International Conference on Semantic Computing, ICSC ?07, pages 19?26, Washington, DC, USA.
IEEE Computer Society.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and Joseph
Van Genabith. 2011. From news to comment: Resources and benchmarks for parsing the language of web 2.0.
In Proceedings of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter:
Annotation, features, and experiments. Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies: short papers, 2:42?47.
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: makn sens a #twitter. Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies, 1:365?378.
Beaux Sharifi Mark-Anthony Hutton and Jugal Kalita. 2010. Summarizing microblogs automatically. In Proceed-
ings of NAACL.
Reinhard Kneser and Hermann Ney. 1993. Improved clustering techniques for class-based statistical language
modeling. In In Proceedings of the European Conference on Speech Communication and Technology (Eu-
rospeech).
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. Proceedings of the Eighteenth International Conference on
Machine Learning, pages 282?289.
J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics,
33(1):159?174, March.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513. Association for
Computational Linguistics, July.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Comput. Linguist., 19(2):313?330, June.
Diana Maynard, Kalina Bontcheva, and Dominic Rout. 2012. Challenges in developing opinion mining tools
for social media. In Proceedings of @NLP can u tag #usergeneratedcontent?! Workshop at International
Conference on Language Resources and Evaluation, LREC 2012, 26 May 2012, Istanbul, Turkey.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In Proceedings of HLT, pages 337?342.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel, N. Schneider, and N.A. Smith. 2013. Improved part-of-speech
tagging for online conversational text with word clusters. In Proceedings of NAACL.
Lawrence Philips. 1990. Hanging on the metaphone. Computer Language, 7:12.
Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings
of NAACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental
study. ACL.
1771
Djam?e Seddah, Beno??t Sagot, Marie Candito, Virginie Mouilleron, and Vanessa Combet. 2012. The French Social
Media Bank: a Treebank of Noisy User Generated Content. In COLING 2012 - 24th International Conference
on Computational Linguistics, Mumbai, India, Dec. Kay, Martin and Boitet, Christian.
Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP/VLC 2000, pages 63?70.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. IN PROCEEDINGS OF HLT-NAACL, pages 252?259.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for
l1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ?09, pages 477?485, Stroudsburg, PA, USA. Association for Computational
Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semi-
supervised learning. In ACL.
William E Winkler. 2006. Overview of record linkage and current research directions. Technical report, BUREAU
OF THE CENSUS.
Liu Xiaohua, Zhang Shaodian, Wei Furu, and Zhou Ming. 2011. Recognizing named entities in tweets. In
Proceedings of ACL.
1772
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 5?8,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Graphical User Interface for Feature-Based Opinion Mining 
Pedro Balage Filho  
University of Wolverhampton 
pedrobalage@gmail.com 
Caroline Brun 
Xerox Research Centre Europe 
Caroline.Brun@xrce.xerox.com 
Gilbert Rondeau 
 Xerox Research Centre Europe 
Gilbert.Rondeau@xrce.xerox.com 
 
 
Abstract 
In this paper, we present XOpin, a graphical 
user interface that have been developed to 
provide a smart access to the results of a 
feature-based opinion detection system, build 
on top of a parser. 
1 Introduction 
Opinion mining (or sentiment analysis) arouses 
great interest in recent years both in academia and 
industry. Very broadly, sentiment analysis aims to 
detect the attitude of a person toward a specific 
topic expressed in natural language and to evaluate 
the polarity of what is been expressed, i.e., whether 
it is positive or negative. With the emergence of 
the Web 2.0, i.e., forums, blogs, web sites 
compiling consumer reviews on various subjects, 
there is a huge amount of documents containing 
information expressing opinions: the ?user 
generated content?. This constitutes a very 
important data source for monitoring various 
applications (business intelligence, product and 
service benchmarking, technology watch). 
Numerous research works at the crossroads of NLP 
and data mining are focusing on the problem of 
opinion detection and mining. In this paper, we 
present the advanced research prototype we have 
designed: it consists in an integration of a feature-
based opinion detection system together with a 
graphical user interface providing to the end-user a 
smart access to the results of the opinion detection.  
We first present an overview of sentiment 
analysis. Then, we detail the system we have 
developed, in particular the graphical user 
interface, and conclude. 
2 Analyzing Sentiment in Texts 
Sentiment Analysis plays a very important role to 
help people to find better products or to compare 
product characteristics. For the consumer, a good 
interface allows to navigate, compare and identify 
the main characteristics of the products or 
companies. For the company, it is interesting to 
know the customer preferences. It is an essential 
step to optimize marketing campaigns and to 
develop new features in products.  
Despite the increase of interest in sentiment 
analysis, many tools do not pay much attention to 
the user interface aspects. These aspects are very 
important in order to satisfy the user needs. 
In the literature, we find some different ways to 
aggregate and represent the summary information 
from a collection of texts annotated with sentiment. 
For instance, Gamon et al (2005) use colors to 
display the general assessment of product features. 
The system shows the reviews as boxes, where the 
box size indicates the number of mentions of that 
topic and the color indicates the average sentiment 
it contains. This interface allows having a quick 
glance about the most important topics and the 
sentiment expressed. 
Another display idea is presented in the 
Opinion Observer (Liu et al, 2005). In this system, 
a bar shows the polarity related with each product 
and each feature. The portions of the bar above and 
below a horizontal line represent the amount of 
positive and negative reviews. For example, in a 
cell phone domain, the sentiment associated with 
features like LCD, battery, reception and speaker 
are used to compare the relevance of one product 
in opposite to another. 
Morinaga et al (2002) present an interface 
where the sentiment information is represented by 
the degrees of association between products and 
opinion-indicative terms. The author uses principal 
component analysis to produce a two-dimensional 
visualization where the terms and products are 
plotted indicating the relatedness among the points. 
In the internet, we can find many systems and 
companies related with sentiment analysis. For 
example, the company Lexalytics has in its website 
5
an available demo1  for sentiment detection. This 
demo shows an interface which highlights positive 
and negative words in the text. The interface also 
shows entities, categories associated, a summary 
and the top terms. 
The RankSpeed 2  is a website for product 
comparison. The website includes in the search the 
sentiment associated with each product. In the 
interface, the user can input a list of sentiment 
words, like ?excellent?, ?cool?, ?easy? or 
?powerful? that the system will organize the results 
according the frequency of those words in reviews 
related to the products.  
The Stock Sonar3  has a timeline chart as the 
main interface. In this timeline, both positive and 
negative sentiments are displayed throughout time. 
The sentiments are retrieved from real-time news 
associated with a particular company. In the same 
timeline, it is possible to follow-up the increase or 
decrease of the stock prices for that company in 
that period of time. In this application, the 
sentiment is used to forecast market actions such as 
buy and sell stocks. 
All those systems presented relevant 
components for a powerful opinion mining 
interface, but none of them deliver a full interface 
to explore the multi-aspects in opinion mining. For 
us, a complete system should provide both single 
and multi-document visualization, work on the 
feature level classification, and produce an 
integrated interface to browse, navigate, filter and 
visualize files, features and sentiment tendencies. 
In the following section, we present XOpin, a 
graphical user interface that have been developed 
to provide the characteristics described.  
3 The System and its Interface 
To detect opinions in texts, our system relies on a 
robust incremental parser, XIP, (Ait-Mokhtar and 
Chanod 2002), specifically adapted for opinion 
detection. The system extracts opinions related to 
the main concepts commented in reviews (e.g. 
products, movies, books...), but also on features 
associated to these products (such as certain 
characteristics of the products, their price, 
associated services, etc...). More precisely, we 
adopt the formal representation of an opinion 
                                                           
1http://www.lexalytics.com/webdemo 
2http://www.rankspeed.com/ 
3http://www.thestocksonar.com/ 
proposed by Liu (2010): an opinion is represented 
as a five place predicate of the form 
?o?,f??, so????, h?, t?? , where:o?  is the target of the 
opinion (the main concept), f??  is a feature 
associated to the object o?, 	so????  is the value 
(positive or negative) of the opinion expressed by 
the opinion holder h? about the feature f??, h? is the 
opinion holder, t? is the time when the opinion is 
expressed.  
We use the robust parser to extract, using syntactic 
relations already extracted by a general 
dependency grammar, semantic relations 
instantiating this model. Other systems use 
syntactic dependencies to link source and target of 
the opinion, for example in Kim and Hovy (2006). 
Our system belongs to this family, as we believe 
that syntactic processing of complex phenomena 
(negation, comparison and anaphora) is a 
necessary step to perform feature-based opinion 
mining. Another specificity of our system is a two 
level architecture based on a generic level, 
applicable to any domain, and on a domain-
dependent level, adapted for each sub-domain of 
application. Regarding evaluation, the relations of 
opinion extracted by the system have been used to 
train a SVM classifier in order to assess the 
system?s ability to correctly classify user?s reviews 
as positive or negative. Results are quite satisfying, 
as they show 93% of accuracy to classify reviews 
about printers and 89% of accuracy to classify 
reviews about movies (Brun, 2011). 
The XOpin Interface was developed to provide 
an easy way to allow the user to explore the results 
of this sentiment analysis system. The interface 
provides a graphical environment that allows the 
user to browse, navigate, filter and visualize the 
necessary information in a collection of texts.  
The tool accepts as input pure text files or xml 
files. The xml files follow a specific format which 
allows the system to retrieve metadata information. 
It is also possible to retrieve web pages from the 
web. The tool offers the possibility to retrieve a 
single webpage, given the URL, or a collection of 
pages by crawling. To crawl, for example, reviews 
webpages, the user need to setup some crawling 
and information extraction rules defined by a 
template in the configuration file. The files 
retrieved from the web are converted in xml 
format, which allows preserving the metadata 
information. As an example, Figure 1 shows the 
6
organization of this xml file from a review 
retrieved from the website epinions.com 
(http://www.epinions.com). 
 
 
Figure 1. Organization of the XML file 
 
The tag source keeps the URL from where the 
review was extracted. The tags domain, brand and 
product keep the specific data about to the product. 
The tag opinion_holder keeps the name of the user 
who wrote the review. The tag review_date keeps 
the date when the review was written. The tag 
opinion keeps the user general assessment about 
the product. In the website epinions.com, the user 
can assess the product as recommended (Yes) or 
not recommended (No). The tag review_stars 
contains the number of stars the user attributed to 
the product. The tag review_popularity keeps the 
number of positive evaluations (thumbsUp) of this 
particular review by the other users. In the reviews 
from the website epinions.com we don?t have this 
assessment, so this number represents how many 
users assigned to trust in this reviewer. The tags 
textblock contain the text for the sections title, 
summary and review. 
After loading a file or a corpus into the tool, the 
texts are showed in a tree structure in the left 
panel. A hierarchical structure allows the user to 
have the corpus organized as a conventional folder 
structure. In this way, it is possible to analyze the 
texts inside a specific folder and also to include the 
texts in the subfolders inside. 
To analyze this data, the tool presents three 
main views: text, timeline and comparison. In the 
text view, negative terms, positive terms and 
entities present in the text are highlighted. The 
purpose of this view is to provide a visual 
assessment about the sentiment expressed in the 
text. If the text was loaded by crawling or by an 
xml file, the metadata is also displayed. Figure 2 
shows an example of reviews collected from the 
website epinions.com, in the category printers.  
As said before, XOpin is able to identify the 
predicates associated with each sentiment and the 
category it belongs. For example, in the sentence 
?This printer gives excellent quality color?, the 
tool highlights the positive sentiment ?excellent?, 
the predicate associated ?color? and organize this 
predicate into the category color. This predicate 
categorization depends of the sub-domain 
architecture level. 
 This classification is very important to present 
an organized summary about which category is 
most positive and with is most negative in the text. 
The right panel shows this information. 
 
Figure 2. Text visualization in XOpin 
 
The timeline screen (Figure 3) offers the user 
the option to analyze a corpus of texts organized 
by time, for example, reviews crawled from the 
web. In this way, the user can create flexible and 
interesting views about the products and features 
found in the corpus.  
The timeline shows the total of positive and 
negative words in the texts for a given date. With 
this information and a larger enough corpus of 
reviews it is possible to have a big picture about 
the user preferences and dissatisfactions. 
The timeline also offers the possibility to show 
the positive and negative lines for specific brands, 
<review> 
<source value="http://..." /> 
<domain value="Printers"/> 
<brand value="Hewlett Packard"/> 
<product value=" Hewlett Packard 6500A"/> 
<opinion_holder value="user_name"/> 
<review_date value="01/Dec/2011"/> 
<opinion value="Yes"/> 
<review_stars value="5"/> 
<review_popularity value="10"/> 
<textblock layout="title"> 
 Review Title 
</textblock> 
<textblock layout="summary"> 
 Review Summary 
</textblock> 
<textblock layout="text"> 
 Review Free Comment 
</textblock> 
</review> 
7
products and features in a determined timespan. 
Filters can remove anything that it is not useful and 
create a pure visualization about what the user 
need to see. The left and bottom panels offer 
options to create those views. 
These views can show an evolution in the 
user?s perspective in respect to some new 
improvement in the product. For example, in a 
marketing campaign, the company can evaluate the 
user behavior about the product price. 
 
Figure 3. Timeline visualization in XOpin 
 
The comparison view (Figure 4) allows the user 
to compare side by side different product features 
in a collection of texts. In this view, the user has 
the main predicate associated with each feature and 
the number of positive or negative occurrences. 
This is interesting in order to have a big picture 
about what the users are commenting in positive or 
negative aspects for each feature. 
 
Figure 4. Feature Comparison in XOpin 
4 Conclusion 
This paper presents an NLP-based opinion 
mining advanced prototype integrating a dedicated 
graphical user interface which provides a smart 
access to the results of the opinion detection. The 
interface has been build in order to ensure 
advanced functionalities such as opinion 
highlighting on text and features, timeline 
visualization and feature comparison. The system 
has been demonstrated to potential customers and 
it received a good feedback. In our assessment, the 
integrated features provided by the system 
increased the usability in the data exploration for a 
reviews corpus compared against other products. 
References 
Salah Ait-Mokthar, Jean-Pierre Chanod. Robustness  
beyond Shallowness: Incremental Dependency 
Parsing. Special Issue of NLE Journal, 2002. 
Caroline Brun. Detecting Opinions Using Deep 
Syntactic Analysis. In Proceedings of the Recent 
Advances in Natural Language Processing (RANLP), 
Hissar, Bulgaria, September 12-14, 2011. 
Michael Gamon, Anthony Aue, Simon Corston-Oliver, 
and Eric Ringger. Pulse: Mining customer opinions 
from free text. In Proceedings of the International 
Symposium on Intelligent Data Analysis (IDA), 
number 3646 in Lecture Notes in Computer Science, 
pages 121?132, 2005. 
Kim, S.M. and E.H. Hovy. Identifying and Analyzing 
Judgment Opinions. Proceedings of the Human 
Language Technology/HLT-NAACL. New York, 
2006. 
Bing Liu, Minqing Hu, and Junsheng Cheng. Opinion 
observer: Analyzing and comparing opinions on the 
web. In Proceedings of WWW, 2005. 
Bing Liu. Sentiment Analysis and Subjectivity, Chapter 
of Handbook of Natural Language Processing, 2nd 
edition, 2010. 
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and 
Toshikazu Fukushima.Mining product reputations on 
the web. In Proceedings of the ACM SIGKDD 
Conference on Knowledge Discovery and Data 
Mining (KDD), pages 341?349, 2002. 
8
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 838?842,
Dublin, Ireland, August 23-24, 2014.
XRCE: Hybrid Classification for Aspect-based Sentiment Analysis
Caroline Brun, Diana Nicoleta Popa, Claude Roux
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan, France
{caroline.brun, diana.popa, claude.roux}@xrce.xerox.com
Abstract
In this paper, we present the system we
have developed for the SemEval-2014
Task 4 dedicated to Aspect-Based Senti-
ment Analysis. The system is based on
a robust parser that provides information
to feed different classifiers with linguis-
tic features dedicated to aspect categories
and aspect categories polarity classifica-
tion. We mainly present the work which
has been done on the restaurant domain
1
for the four subtasks, aspect term and cat-
egory detection and aspect term and cate-
gory polarity.
1 Introduction
Aspect Based Sentiment Analysis aims at discov-
ering the opinions or sentiments expressed by a
user on the different aspects of a given entity ((Hu
and Liu, 2004); (Liu, 2012)). A wide range of
methods and techniques have been proposed to ad-
dress this task, among which systems that use syn-
tactic dependencies to link source and target of the
opinion, such as in (Kim and Hovy, 2004), (Bloom
et al., 2007), or (Wu et al., 2009). We have devel-
oped a system that belongs to this family, (Brun,
2011), as we believe that syntactic processing of
complex phenomena (negation, comparison, ...)
is a crucial step to perform aspect-based opinion
mining. In this paper, we describe the adaptations
we have made to this system for SemEval, and the
way it is applied to category and polarity classifi-
cation.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
We have not performed any domain adapation for the
laptop corpus and only submitted a run for the subtask 1, term
detection.
2 Description of the System
In this section, we describe the different compo-
nents of the system.
2.1 Existing System
In order to tackle the Semeval?14 Task 4, (Pon-
tiki et al., 2014), we used our existing aspect-
based opinion detection system. The opinion de-
tection system we built relies on a robust deep
syntactic parser, (Ait-Mokhtar et al., 2001), as a
fundamental component, from which semantic re-
lations of opinion are calculated. Parsing here
includes tokenization, morpho-syntactic analysis,
tagging which is performed via a combination of
hand-written rules and HMM, Named Entity De-
tection, chunking and finally, extraction of depen-
dency relations between lexical nodes. These re-
lations are labeled with deep syntactic functions.
More precisely, a predicate (verbal or nominal) is
linked with what we call its deep subject (SUBJ-
N), its deep object (OBJ-N), and modifiers. In
addition, the parser calculates more sophisticated
and complex relations using derivational morpho-
logic properties, deep syntactic properties (subject
and object of infinitives in the context of control
verbs), and some limited lexical semantic coding.
Syntactic relations already extracted by a
general dependency grammar, lexical information
about word polarities, sub categorization informa-
tion and syntactic dependencies are all combined
within our robust parser to extract the semantic
relations. The polarity lexicon has been built
using existing resources and also by applying
classification techniques over large corpora, while
the semantic extraction rules are handcrafted, see
(Brun, 2011) and (Brun, 2012) for the complete
description of these different components. The
system outputs a semantic dependency called
SENTIMENT which can be binary, i.e. linking
opinionated terms and their targets, or unary,
i.e. just the polar term in case the target of the
838
opinion hasn?t been detected. For example, when
parsing I was highly disappointed by their service
and food., the systems outputs the following
dependencies:
SUBJ N(disappointed,food)
SUBJ N(disappointed,service)
OBJ N(disappointed,I)
MANNER PRE(disappointed,highly)
SENTIMENT NEGATIVE(disappointed,service)
SENTIMENT NEGATIVE(disappointed,food)
In this system, aspects terms are not explic-
itly extracted, however all non-polar arguments of
the SENTIMENT dependency are potential aspect
terms. Moreover, this system considers only posi-
tive and negative opinions, but does not cover the
neutral and conflict polarities.
2.2 System Adaptation
The opinion detection system described in the
previous section has been adapted for the Se-
mEval2014 Task4, in two ways: some lexical ac-
quisition has been performed in order to detect the
terms of the domain, and some rules have been de-
veloped to detect multi-word terms and to output
semantic dependencies associating their polarity
to terms and categories.
2.2.1 Lexical Enrichment and Term
Detection
As said before, the existing system encodes a rea-
sonable amount of polar vocabulary. However, as
the task implies domain knowledge to detect the
terms, we have first extracted the terms from the
training corpus and encoded their words into our
lexicons, assigning to them the semantic features
food, service, ambiance and price. We have then
extended the list with Wordnet synonyms. To im-
prove coverage, we have also extracted and fil-
tered food term lists from Wikipedia pages and en-
coded them. More precisely, the list of food terms
has been extracted from the Wikipedia ?Food Por-
tal?, from the category ?Lists of foods?
2
. At the
end of this process, our lexicon has the following
coverage: Polar words: 1265 negative, 1082 posi-
tive and Domain words: 761 food words, 31 price
words, 105 ambiance words, 42 service words.
In order to detect the terms, some local grammar
rules (based on regular expressions) have been de-
veloped taking into account the lexical semantic
2
http://en.wikipedia.org/wiki/Category:Lists of foods
information encoded in the previous step. These
rules detect the multi-words terms, e.g. pas-
trami sandwiches, group them under the appropri-
ate syntactic category (noun, verb) and associate
them with the corresponding lexical semantic fea-
ture, food, service, ambiance, price. In addition to
this, in order to prepare the aspect category clas-
sification (c.f. section 2.3.3), a layer of semantic
dependencies has been added to the grammar: If
a domain term is detected in a sentence, a unary
dependency corresponding to its category (FOOD,
SERVICE, PRICE, AMBIANCE) is built.
2.2.2 Grammar Adaptation for Polarity
Detection
The English grammar, which had been previously
developed to detect sentiments, has also been
adapted in order to extract the opinions associated
to the terms and categories detected at the previous
step.
If an aspect term is the second argument of a
SENTIMENT relation, 2 dependencies, one for the
term (OPINION ON TERM) and one for the corre-
sponding category (OPINION ON CATEGORY) are
built. They inherit the polarity (positive or nega-
tive) of the SENTIMENT dependency. If these de-
pendencies target the same term and category and
if they have opposite polarity, they are modified in
order to bear the feature ?conflict?.
Then, if a sentence contains a term and
if no SENTIMENT dependency has been de-
tected, the OPINION ON TERM and OPIN-
ION ON CATEGORY are created with the polarity
?neutral?. Finally, if no terms have been de-
tected in a sentence, there are two cases: (1)
a SENTIMENT dependency has been detected
somewhere in the sentence, the dependency
OPINION ON CATEGORY(anecdote/misc), is
created with the corresponding polarity (positive
or negative); (2) no SENTIMENT dependency
has been detected, the dependency OPIN-
ION ON CATEGORY(anecdote/misc), is created
with polarity ?neutral?.
The dependency OPINION ON TERM links the
terms to their polarities in the sentences and serves
as input for the subtasks 1 and 3.
2.3 Classification
2.3.1 KiF (Knowledge in Frame)
The whole system, training and prediction, has
been implemented in KiF (Knowledge in Frame),
a script language that has been implemented into
839
the very fabric of the rule-based Xerox Incremen-
tal Parser (XIP). KiF offers a very simple way to
hybridize a rule-based parser with machine learn-
ing technique. For instance, a KiF function, which
evaluates a set a features to predict a class, can be
called from a rule, which could then be fired along
the output of that function. KiF is a multi-threaded
programming language, which is available for all
platforms (Windows, Mac OS, Linux). It pro-
vides all the necessary objects (strings, containers
or classes) and many encapsulations of dynamic
libraries from different C programs such as classi-
fiers (liblinear and libsvm), database (SQLite), or
XML (libxml2), which can be loaded on the fly.
All internal XIP linguistic structures are wrapped
up into KiF objects. For example, linguistic fea-
tures are available as maps, which can be modi-
fied and re-injected into their own syntactic nodes.
The language syntax is a mix between Java (types
are static) and Python (in the way containers are
handled), but provides many implicit conversions
to avoid code overloading with too many func-
tions. KiF allows for an efficient integration of
all aspects of linguistic analysis into a very sim-
ple framework, where XML documents can be an-
alyzed and modified both with linguistic parsing
and classifiers into a few hundred lines of code.
2.3.2 General Methodology
We focus on four main tasks: detecting the as-
pect terms and aspect categories and their corre-
sponding polarities. While the detection of aspect
terms and their corresponding polarities occurs at
the grammar level, for the detection of aspect cate-
gories and their corresponding polarities we make
use of the liblinear library (Fan et al., 2008) to
train our models. We train one classifier for detect-
ing the categories and further, for each category
we train a separate classifier for detecting the po-
larities corresponding to that particular category.
For both settings, we use 10-fold cross-validation.
The two modules for aspect category classification
and aspect category polarity classification are de-
scribed in details further.
2.3.3 Aspect Category Classification
The sentence classification module is used to as-
sign aspect categories to sentences. For each sen-
tence, the module takes as input features the bag
of words in the sentence as well as the information
provided by the syntactic parser. The output con-
sists of a list of categories corresponding to each
sentence.
In the pre-processing stage stop words are re-
moved (determinants, conjunctions). Further, we
use the L2-regularized logistic regression solver
from the liblinear library to train a model. The
features considered are the word lemmas from the
sentence along with their frequencies (term fre-
quency). Apart from this, the information pro-
vided by the rule based component is also taken
into account to increase the term frequency for
terms belonging to the detected categories.
Such information can consist of: dependencies
denoting the category to which a detected aspect
term belongs (Food, Service, Price, Ambiance)
and dependencies denoting the opinions on the
detected aspect terms and categories (OPIN-
ION ON CATEGORY, OPINION ON TERM). For
example for the following sentence: ?Fab-
ulous service, fantastic food, and a chilled
out atmosphere and environment?, the salient
dependencies produced by the syntactic parser are:
FOOD(food), AMBIANCE(atmosphere),
SERVICE(service), AMBIANCE(environment),
OPINION ON CATEGORY POSITIVE(food),
OPINION ON CATEGORY POSITIVE(service),
OPINION ON CATEGORY POSITIVE(ambiance),
OPINION ON TERM POSITIVE(food),
OPINION ON TERM POSITIVE(service),
OPINION ON TERM POSITIVE(atmosphere).
This yields the following features having an
increase in their frequencies: food (+3), service
(+3), atmosphere (+2), environment (+1), am-
biance (+1).
Once the logistic regression is performed, each
category is predicted with a certain probability.
Since in one sentence there may be entities that re-
fer to different categories, we set a threshold with
respect to the probability values to be taken into
account. We have tried different approaches to set
this threshold. The best results on the training and
trial data were obtained with a threshold of 0.25,
(i.e. we kept only the categories with a probability
over 0.25).
2.3.4 Aspect Category Polarity Classification
The approach to predict the polarity for each cate-
gory is similar to the one predicting the categories
for each sentence, with some differences as will
be further detailed. The classification uses for fea-
tures, the bag of words (term frequency), but also
840
the polarity provided by XIP by the following de-
pendencies: OPINION ON CATEGORY and SEN-
TIMENT. Whenever these dependencies are de-
tected, a feature is added to the classification of
the form polarity category. Thus for the previ-
ous example sentence: Fabulous service, fantastic
food, and a chilled out atmosphere and environ-
ment, the additional dependencies considered are
SENTIMENT POSITIVE(atmosphere, chilled out),
SENTIMENT POSITIVE(food, fantastic), SENTI-
MENT POSITIVE(service, Fabulous). After map-
ping back the terms to their corresponding cate-
gories, the added features are: positive ambiance,
positive food and positive service. Since the de-
pendency OPINION ON CATEGORY is also de-
tected by the parser for these categories, each
of the above mentioned features will have a fre-
quency of 2 in this case. Moreover, the polarity
alone is also added as a feature. The training is
performed using the L2-regularized L2-loss sup-
port vector classification solver from the same li-
brary (liblinear) and a model is generated for each
category. Thus, depending on the categories de-
tected within a certain sentence, the correspond-
ing model is used to make the prediction regarding
their polarities. The classifier?s output represents
the predicted polarity for one given category.
3 Evaluation
The corpus used for evaluating the system con-
tains 800 sentences, 1134 aspect term occurrences,
1025 aspect category occurrences, 5 different as-
pect categories and 555 distinct aspect terms. All
these belong to the restaurant domain.
3.1 Terms and Category Detection
When evaluating aspect terms and aspect cate-
gories detection, three measures were taken into
account: precision, recall and the f1-measure.
For both aspect term extraction and aspect cat-
egory detection, the baseline methodologies are
presented in (Pontiki et al., 2014). Table 1 shows
the results obtained using our approach as com-
pared to the baseline for aspect term detection,
whereas Table 2 outlines the results regarding as-
pect category detection in terms of the previously
mentioned measures.
Furthermore, it is interesting to notice the in-
crease in performance obtained by combining the
bag-of-words features with the output of the parser
as opposed to just using the bag-of words. These
Method Precision Recall F-Measure
Baseline 0.627329 0.376866 0.470862
XRCE 0.862453 0.818342 0.839818
Table 1: Aspect term detection.
Method Precision Recall F-Measure
Baseline 0.637500 0.483412 0.549865
BOW 0.77337 0.799024 0.785988
XRCE 0.832335 0.813658 0.822890
Table 2: Aspect category detection.
differences are outlined for aspect category detec-
tion in Table 2, where BOW denotes the system
using the same settings, but just the bag-of-words
features and XRCE denotes the submitted system
where the bag-of-words features are augmented
with parser output features.
For both tasks of aspect term and aspect cate-
gory detection, our system clearly outperforms the
baseline, resulting in being ranked among the first
3 in the competition for the restaurant corpus.
3.2 Terms and Category Polarity Detection
Similarly, Table 3 shows the results in terms of
accuracy on aspect term polarity detection and on
aspect category polarity detection. Here, baseline
methodologies are similar to the ones used for as-
pect category detection and also described in (Pon-
tiki et al., 2014). Again, our system ranks high in
the competition, achieving an overall accuracy of
0.77 for aspect term polarity detection and 0.78 for
aspect category polarity detection. Furthermore, a
comparison is also made between the current sys-
tem and one that, using the same settings, would
not take into account the features provided by the
parser (BOW). The results emphasize the impor-
tance of using the merged version.
Method Task Accuracy
Baseline Term polarity 0.552239
XRCE Term polarity 0.776895
Baseline Category polarity 0.563981
BOW Category polarity 0.681951
XRCE Category polarity 0.781463
Table 3: Aspect term and aspect category polarity.
841
Label Precision Recall F-measure
conflict NaN 0 NaN
negative 0.7857 0.7296 0.7566
neutral 0.5833 0.3214 0.4145
positive 0.7998 0.9272 0.8588
Table 4: Aspect term polarity (2).
Label Precision Recall F-measure
conflict 0.5333 0.1538 0.2388
negative 0.726 0.6802 0.7023
neutral 0.5119 0.4574 0.4831
positive 0.8343 0.9117 0.8713
Table 5: Aspect category polarity (2).
3.3 Error Analysis
The results obtained with our system are unar-
guably competitive, but some remarks can be
made regarding the most frequent causes of er-
rors. In the task of aspect category classification,
the choice of the threshold (0.25) may have con-
stituted a factor impacting the performance. In
the task of aspect term detection, the lexical cov-
erage is one of the factors to explain the difference
in performance between training/trial data and test
data.
Table 4 contains the results obtained in terms
of precision, recall and F-measure for each of the
possible polarities for terms (positive, negative,
neutral and conflict) and similarly does Table 5
for category polarities. In both cases we notice a
clear decrease for these measures when predicting
the conflict and neutral classes, with a higher de-
crease in the case of aspect term polarity detection.
This can be explained by the fact that the syntactic
parser was primarily customized to detect the neg-
ative and positive labels. This obviously had an
impact on the final results as the information from
the parser constituted some of the input features
for the classification.
4 Conclusion
The combination of a symbolic parser, customized
with specialized lexicons, with SVM classifiers
proved to be an interesting platform to implement
a category/polarity detection system. The sym-
bolic parser on the one hand provides a versatile
architecture to add lexical and multi-words infor-
mation, augmented with specific rules, in order to
feed classifiers with high quality features. How-
ever, some work will be needed to improve per-
formances on the neutral and conflict polarities,
which rely less on specific words, than on a more
global interpretation of the content.
Acknowledgements
We would like to thank the Semeval task 4
organizers, as well as our colleague, Vassilina
Nikoulina, for her help on this project.
References
Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
IWPT.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In In HLT-
NAACL 2007, pages 308?315.
Caroline Brun. 2011. Detecting opinions using deep
syntactic analysis. In RANLP, pages 392?398.
Caroline Brun. 2012. Learning opinionated patterns
for contextual opinion detection. In COLING, pages
165?174.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD, pages 168?177.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In International
Workshop on Semantic Evaluation (SemEval).
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
842
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 61?69,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Opinion and Suggestion Analysis for Expert Recommendations
Anna Stavrianou and Caroline Brun
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan, France
{anna.stavrianou,caroline.brun}@xrce.xerox.com
Abstract
In this paper, we propose the use of fine-
grained information such as opinions and
suggestions extracted from users? reviews
about products, in order to improve a rec-
ommendation system. While typical rec-
ommender systems compare a user profile
with some reference characteristics to rate
unseen items, they rarely make use of the
content of reviews users have done on a
given product. In this paper, we show how
we applied an opinion extraction system to
extract opinions but also suggestions from
the content of the reviews, use the results to
compare other products with the reviewed
one, and eventually recommend a better
product to the user.
1 Introduction
Social media has enabled web users to inter-
act through social platforms, express their opin-
ions, comment and review various products/items.
Such user-generated content has been analysed
from a social as well as content-oriented point
of view. For instance, social network analysis
techniques have been used to identify user roles
(Agarwal et al, 2008; Domingos and Richard-
son, 2001; Fisher et al, 2006; Zhang et al,
2007) and text or opinion mining techniques have
been applied to identify positive/negative tenden-
cies within user online review comments (Ding
and Liu, 2007; Ghose et al, 2007; Hu and Liu,
2004; Leskovec et al, 2010). In the applicative
context, recommender systems (Adomavicius and
Tuzhilin, 2005) make use of the opinion informa-
tion (such as in star-rating systems) and recom-
mend items (movies, products, news articles, etc.)
or social elements (i.e. propositions to connect
with other people or communities), that are likely
to be of interest to a specific user.
Typically, a recommender system compares a
user profile with some reference characteristics,
and seeks to predict the ?preference? or ?rating?
that a user would give to an item not yet consid-
ered. These characteristics may be part of the in-
formation item (the content-based approach) or
the user?s social environment (the collaborative
filtering approach). Comments published on so-
cial networking or review web sites are sometimes
used by recommender systems (Aciar et al, 2007;
Jakob et al, 2009) in order to find out similarities
between users that comment on the same items
in the same way. However, extracting explicit se-
mantic information carried out in these comments
(e.g. ?this printer is slow?) is of great interest in
order to detect what a user has liked or disliked
about a given topic (e.g. the speed of the printer)
and consequently take it into account to make rec-
ommendations.
In this paper, we propose the extraction of opin-
ions and suggestions from user reviews or free
text and their use as input information to improve
recommender systems. This technique could be
used on top of standard recommender techniques
in order to further fine-grain the recommendation
according to the user comments.
To the best of our knowledge, no existing ap-
proach takes advantage of the fine-grained opin-
ions or suggestions the user explicitly expresses
using natural language within a review or a free
text. As aforementioned, some works consider
the product reviews as a means to get user opin-
ions on certain products and use this information
for recommendation purposes. Nevertheless, they
all assign a polarity (?negative? or ?positive?) to
61
the review or they update the rating (e.g. giv-
ing a value from 1 to 5) without going further
down exploiting the exact phrases. More partic-
ularly they do not detect what aspects of the prod-
uct have been appreciated or not. For example, no
approach considers using the user-stated phrase ?I
would prefer a lighter camera? in order to recom-
mend to a user a camera that satisfies all the de-
sired features and on top of this being lighter than
the reviewed one.
The paper continues with a state-of-the art dis-
cussion. Section 3 is divided into two parts; a
description of the methodology followed in or-
der to extract opinion information from reviews
through NLP techniques and a description of how
this information is used for recommending prod-
uct items. Section 4 shows an example and Sec-
tion 5 presents a first attempt of an evaluation.
Section 6 concludes and discusses future work.
2 Related Work
Although there are no works that use the explicit
semantics extracted from reviews for recommen-
dation purposes, our approach has some similari-
ties with the analysis of reviews state-of-the-art.
Identifying the opinion of customer reviews has
concerned different research communities. Some
significant works infer opinion polarities based on
comparisons with a pre-defined seed-list of adjec-
tives (Ding and Liu, 2007; Hu and Liu, 2004) or
implicitly through observing the changes in the
respective product prices of reputation systems
(Ghose et al, 2007). An attempt of extracting
suggestions (and not just opinions) from customer
reviews has also been presented in (Vishwanath
and Aishwarya, 2011), in which ontologies and
feedback rules are used for this purpose.
Combining knowledge of opinions extracted
from reviews and recommender systems has also
some applications. For example, (Jakob et al,
2009), have analysed opinions of movie reviews.
They use pre-defined categories of movie features
(acting, production, soundtrack, cinematography
and storyline), and they assign polarities (nega-
tive or positive) to each category according to the
per-feature opinion words expressed for each re-
view. For example, if a movie review contains the
sentence ?the acting is flat?, they assign a neg-
ative polarity to the category ?acting? and they
just avoid recommending the specific movie to the
users. They do not explicitly use the opinion in-
formation in order to make comparisons with sim-
ilar movies and propose one ?less flat? to the user.
Similarly to (Jakob et al, 2009), most research
works that use opinion information for recom-
mendation purposes consider only the polarity
and not the explicit semantics of the opinions.
For instance, in (Aciar et al, 2007) or (Poirier,
2011) they assign a kind of ?rating? on each re-
view regarding the product. Comparisons are not
included.
(Sun et al, 2009) include opinion-based and
feature-based comparisons in order to recommend
products to users. Their approach takes into ac-
count a whole set of reviews (as opposed to indi-
vidual ones) and it involves no NLP parsing. The
opinions are aggregated into a sentiment value
and this value points out mainly whether a product
feature is better or not when it comes to compar-
ing different models of the same product.
NLP techniques have, in some cases, been used
for recommendation. As an example, in the pa-
per of (Chai et al, 2002) the user can ?chat? with
the system in order to describe what type of prod-
uct she desires, receiving in return a list of recom-
mended products. Although, in this case, compar-
isons between products take place in the database,
opinion identification is not included. The user
neither expresses a complaint nor she suggests
an improvement, thus, no opinion detection takes
place.
3 Opinion mining for expert
recommendations
In this section we describe the approach followed
in order to initially parse the user reviews regard-
ing manufactured products, extract opinion infor-
mation from them and, then, use this information
for the purpose of providing expert recommenda-
tions.
Each product review concerns one specific
product whose brand and model are clearly men-
tioned each time. In web sites such as ?epin-
ions.com? this information appears in the title of
the review and it is straightforward to extract.
In order to make use of the content of the re-
views, we apply a system relying on a deep se-
mantic analysis that detects opinions and sugges-
tions within the customer reviews. Natural lan-
guage techniques allow the detection of the weak-
nesses of the product (focusing on specific fea-
tures) or the potential improvements, according to
62
the user?s point of view.
The information extracted from the reviews is
then confronted to a database of products contain-
ing information such as product characteristics,
usage details, average price, etc. For the purposes
of this paper, we consider only product charac-
teristics whose values can be boolean or numeric
and as such they can be compared with the tra-
ditional methods. The system selects, within this
database, one or more similar products that com-
pensate for the problems or improvement needs
identified within the review. Then, pointers to
these products can be explicitly associated with
the specific review as ?expert recommendations?,
and constitute an automatic enrichment of the re-
view.
The advantage for readers of these enriched re-
views is to benefit from a contextualized recom-
mendation that takes into account the semantic
information conveyed in reviews of people who
have used a given product. Moreover, the re-
view?s reader may be helped in her product search
and may have a recommendation on a product
she did not even know it exists. Figure 1 shows
a schema of the process followed which is ex-
plained in more detail in the next sections.
3.1 Semantic Extraction
Our approach begins with the extraction of se-
mantic information from each review and more
specifically the identification of the user?s sugges-
tion(s) and/or opinion(s) together with the product
features and respective comparison words.
For the purpose of identifying the weaknesses
or the possible improvements mentioned in the
text, we need to extract the opinion of a user about
a given characteristic of a product. Thus, we ap-
ply an opinion detection system that is able to per-
form feature-based opinion mining, relating the
main concept (e.g. a printer) to several features
(e.g. quality, print speed and resolution), that can
be evaluated separately.
Formally, our system adopts the representation
of a given opinion as proposed by (Liu, 2010),
where an opinion is a five place predicate of the
form (oj , fjk, sijkl, hi, tl), where:
? oj is the target object of the opinion (the
main concept)
? fjk is a feature associated to the object
? sijkl is the value (positive or negative) of
the opinion expressed by the opinion holder
about the feature
? hi is the opinion holder
? tl is the time when the opinion is expressed.
The opinion extraction system is designed on
top of the XIP robust syntactic parser (A??t-
Mokhtar et al, 2002), which is used as a funda-
mental component, in order to extract deep syn-
tactic dependencies, from which semantic rela-
tions of opinion are calculated. These semantic
relations are intermediary steps to instantiate the
five place predicates which are compliant with
the aforementioned model. Having syntactic re-
lations already extracted by a general dependency
grammar, we use the robust parser by combining
lexical information about word polarities, subcat-
egorization information and syntactic dependen-
cies to extract the semantic relations that will then
instantiate this model.
There exist other systems, such as the one de-
scribed in (Kim and Hovy, 2006), that use syntac-
tic dependencies to link the source and target of
the opinions. Our system (Brun, 2011) belongs to
this family, since we believe that the syntactic pro-
cessing of complex phenomena (negation, com-
parison and anaphora) is a necessary step in or-
der to perform feature-based opinion mining. An-
other characteristic of our system is that it respects
a two-level architecture; it relies on a generic
level, applicable to all domains and corpora, and
on a domain-dependent level, adapted for each
sub-domain of application.
Moreover, our system includes a semantic map-
ping between polar vocabulary and the features
it corresponds to. For instance, the opinion
word ?fast? is mapped to the feature ?speed?, the
word ?expensive? to the feature ?price?, the word
?clunk? to ?noise? and so on. This mapping en-
ables us to further exploit the comments of the
user by referring to specific product characteris-
tics.
When analyzing an example like ?The photo
quality of my prints is astonishing. This printer
is really not that expensive.?, our system extracts
two relations of opinion :
? OPINION POSITIVE(astonishing,photo
quality): the dependency parser extracts an
63
 
User Review 
???????????? 
???????????? 
???????????? 
???????????? 
 
 
 
 
User Review 
???????????? 
???????????? 
???????????? 
???????????? 
 
 
 
 
User Review 
???????????? 
???????????? 
???????????? 
???????????? 
 
 
 
 
Semantic Extraction 
- Opinion detection 
- Suggestion detection 
Product identified issues 
and  
improvement needs 
Product  
Description 
Database 
 
Mapping 
?better than? 
Selected products 
Review enrichment  
with  
?Expert 
Recommendations? 
Figure 1: Extracting opinion semantic information from product reviews and provide expert recommendations.
attributive syntactic relation between the
subject ?photo quality? and the positive
adjectival attribute ?astonishing? from
which this relation of opinion is inferred
about the feature ?photo quality?
? OPINION POSITIVE(expensive,printer):
the dependency parser also extracts an
attributive syntactic relation between the
subject ?printer? and the negative adjective
attribute ?expensive?, but it also extracts a
negation on the main verb: the polarity of
the final relation is inverted, i.e. is finally
positive. As we have also encoded that the
adjective ?expensive? is semantically linked
to ?price?, this opinion is linked to the
feature ?price?.
In addition, the system includes a specific de-
tection of suggestions of improvements, which
goes beyond the scope of traditional opinion
detection. Suggestions of improvements are
expressed with two discursive figures denoting
?wishes? or ?regrets?. To detect these specific
discurse patterns, we use again information ex-
tracted by the parser, i.e. syntactic relations such
as SUBJECT, OBJECT, MODIFIER, but also in-
formation about verbal tenses, modality and ver-
bal aspect, combined with terminological infor-
mation about the domain, in our case, the domain
of printers.
Some examples follow that show what the sys-
tem would output considering certain input sen-
tences extracted from customer reviews about
printers:
1. Input: ?I think they should have put a faster
scanner on the machine, one at least as fast
as the printer.?
Output:
SUGGESTION IMPROVE(scanner, speed)
In this example, the system identifies
from the input sentence that the user is not
satisfied with the speed of the scanner and
would have liked it to be quicker.
2. Input: ?I like this printer, but I think it is too
expensive.?
Output: OPINION POSITIVE(printer, ),
OPINION NEGATIVE(printer, price).
In this example, the system identifies
that the user is not happy with the price
of the printer although the rest of its
characteristics satisfy him.
3. Input: ?The problem of this printer is the
fuser.?
Output:
OPINION NEGATIVE(printer, fuser).
In this example, the system identifies
that the problem lies in the fuser of the
printer.
The first two examples can be further exploited
by the approach we propose. For instance, for the
64
second example, the reader of this review could
benefit from a recommendation of a similar but
cheaper printer. The third example contains infor-
mation that is not measured (it has neither boolean
nor numeric values) and as such it is out of the
scope of this paper.
3.2 Review enrichment
Following the detection of the opinions or sug-
gestions regarding specific product features, we
identify products that match the non-mentioned
or positive characteristics of the reviewed product
while at the same time satisfying the user sugges-
tions.
We consider a database that stores products to-
gether with their features. Same type of prod-
ucts are stored similarly for evident reasons. The
database can be populated either manually or au-
tomatically through the web sites that hold prod-
uct information and it needs to be updated so
that new products appear and old ones are never
recommended. Access to the database is done
through standard SQL queries.
The system retrieves products of the same us-
age (e.g. a user that is reading a review for a PC
laptop will not need a recommendation for a PC
desktop), while selecting those ones whose fea-
tures are within the same or ?better? range. The
features that should definitely be in ?better? range
are the ones retrieved with the help of the opinion
detection system described previously. These fea-
tures would be suggestions or negative opinions
the user has expressed about a product.
The ranges can be defined in many ways and
they can be subject to change. For example, the
prices may be considered to change ranges every
50 Euro or 500 Euro depending on the average
price of the product. The feature requested by the
user (e.g. ?cheaper?) should have a value in a dif-
ferent range in order to really satisfy her this time
(e.g. a computer that costs 5 Euro less than the re-
viewed one is not really considered as ?cheaper?).
Defining what ?better? range refers to, depends
on the feature. For instance, the lower the price,
the better it is, whereas, the higher the speed the
better. In order to avoid this confusion we keep
the descending (e.g. in the case of price) or as-
cending (e.g. in the case of speed) semantics of
the feature within the database.
Once the system has identified the products that
seem to be closer to the user requirements, it high-
lights these products by presenting them as ?ex-
pert recommendations?. These recommendations
may appear on each review as enrichments assum-
ing that the characteristics not mentioned as nega-
tive by the user have satisfied her, so she would be
happy with a similar product having basically the
mentioned features improved. The recommenda-
tion is mainly useful to the reader of the review
that is in the decision process before buying a
product.
Some special - sometimes often appearing -
matching cases worth mentioning:
Multiple features: If more than one feature
needs to be improved, priorities can be de-
fined dependent on the order in which the
features are mentioned in the review.
No comparable features: for this paper features
are taken into account only if they are nu-
meric or boolean (presence/absence) and can
be subjectively compared.
Many matching products: more than one prod-
uct can be recommended. The limit of the
number of products can be pre-defined and
the products may appear to the user in the
order of less-to-more expensive.
No better answer: if no product is found that
may satisfy the user then the search can go
on in products of a different brand. The sys-
tem has also the choice to remain ?silent?
and give no recommendation.
A non-demanded feature changes: in the case
that a requested product is found but it is
more expensive than the reviewed product,
the recommendation would include some in-
formation regarding this feature (e.g. ?A pro-
posed product is ?...? whose price, though, is
higher?).
4 Example
Before evaluating our approach we present an ex-
ample that shows the semantic extraction and rec-
ommendation process. We consider a small set
of printers together with their characteristics and
prices. These data are taken from epinions.com
at a date just before the submission of this paper.
The data appear in Table 1 in descending order of
price.
65
Brand Model Usage Technology Black speed Capacity Price($)
X 8560 Laser Workgroup Color 30 1675 930
X 6360V Laser Workgroup Color 42 1250 754
X 6180 Laser Workgroup Color 26 300 750
X 4118 All-in-One Laser All-in-One Monochrome 18 650 747
HP Laserjet Cp2025n Workgroup Color 20 300 349
HP Laserjet M1212nf All-in-One Monochrome 19 150 139
Table 1: Printer information used for the purposes of the example(source: www.epinions.com).
In the examples that follow, the input is a sen-
tence that is assumed to be in the review of a given
product.
1. Review about the ?6180 Laser? printer.
Input:?I think they should have allowed for
a higher capacity.?
Semantic Extraction step:
SUGGESTION IMPROVE(printer, capac-
ity)
Identify similar products step:
? identify reviewed characteristics: work-
group, laser, color, 26 ppm black speed,
300 sheets capacity, $750 price
? identify similar printers where capacity
is higher (next range) than 300 sheets
Expert recommendation: A proposed printer
with a higher capacity is the ?6360V Laser
Printer?.
2. Review about the ?6180 Laser? printer.
Input:?I like it but it is expensive!?
Semantic Extraction step:
OPINION NEGATIVE(printer, price)
Identify similar products step:
? identify reviewed characteristics: work-
group, laser, color, 26 ppm black speed,
300 sheets capacity, $750 price
? identify similar printers where price is
lower than $750.
Expert recommendation: A proposed
cheaper printer of the same type is ?HP,
LaserJet Cp2025n?.
5 Evaluation
The evaluation of the proposed system concerns
two modules; the semantic extraction and the re-
view enrichment.
The first module has already been evaluated
previously showing encouraging results. The sys-
tem has been evaluated as to whether it correctly
classifies the reviews according to the overall
opinion. The structure of the ?epinions.com? web
site has been used for the evaluation since each
author has tagged the respective review with a tag
?recommended? or ?not recommended?, the cor-
pus can be thus considered as annotated for clas-
sification. The SVM classifier (Joachims, 1998)
has been used with a training set of opinions ex-
tracted by our system from 313 reviews and a test
set of 2735 reviews, giving a 93% accuracy.
The review enrichment module evaluation, pre-
sented in this paper, focuses on whether the rec-
ommended products enrich the specific review
and may satisfy the user by improving at least one
of the negative features mentioned or following a
specified suggestion without worsen the range of
the rest of the features. The experiments are run
against a database of 5,772 printers whose details
are extracted from the ?epinions.com? site.
For the purposes of this evaluation, we have de-
veloped a product comparison module that takes
as input, for our case, the reviewed printer model
together with the opinion and suggestion relations
as extracted by the opinion mining system. The
output of the comparison module is a set of rec-
ommended printers which are similar to the re-
viewed one while improving the negative features
(based on a comparison of the feature values).
The comparison module deals with features
that are numeric or boolean (presence/absence).
Printers are queried against their type (color-
laser/inkjet, personal/workgroup, etc.), their func-
tions (copier, scanner etc.) and their features
66
(speed, resolution, etc.). Ranges have been de-
fined according to the average per-feature-ranges
that are in the database. These ranges can be ex-
tended according to the number of recommenda-
tions we would like to have (the larger the range
the more the recommendations).
Certain assumptions have been made in order to
provide the recommendations. One such assump-
tion is that the author of the review knows how
to best make use of the printer she has bought.
For example, if the user is complaining about the
printer?s resolution or print quality, we assume
that she makes her printing decisions (paper size,
landscape/portrait) based on her knowledge of the
printer?s resolution. Thus, the specific review can
indeed be enriched with a recommendation of a
printer with a better resolution rather than an ad-
vice on how to use the specific printer (e.g. by
using a different media size).
Furthermore, certain issues had to be taken care
of such as missing data and different measure-
ment units that are not necessarily comparable.
When the values of the features that are to be im-
proved are missing, the respective products are
not taken into account. The missing data case is
also applied when the same feature is measured in
different units between two similar products. At
a later stage we may include such products in the
recommendations and inform the user about the
differences.
The experiments were run over 129 printer
reviews from the ?epinions.com? site contain-
ing negative opinions and/or suggestions. The
reviews concerned 6 different brands while the
database from which the recommended products
are extracted contains printers from 14 different
brands. Once the need-to-be improved features
were extracted from the reviews, the comparison
module was run in order to identify the recom-
mended products.
The recommendation output is manually eval-
uated by looking at the technical features on the
one side and by looking at the reviews of the rec-
ommended model on the other. It has to be noted
that this is a first evaluation of the system hav-
ing the usual problems that recommender systems
evaluations have e.g. recall calculations, finding
the right experts etc. Since we have used a printer
dataset, the ideal experts to validate whether we
propose better or not printers would be experts
from the field of printers. Not having found such
experts at the moment, we limit our evaluations to
the following two-faceted one:
Feature-based evaluation: Based on the feature
values, our system has a 100% precision,
meaning that the recommended products are
indeed similar to the reviewed ones while
improving at least one of the required fea-
tures. As a result, in all cases the recom-
mended products are technically better than
the reviewed one and they can help in the re-
view enrichment.
Rating-based evaluation: In order to see
whether an average user could benefit from
such a recommendation, we have also
evaluated our approach by looking at the
reviews of the recommended products. This
evaluation is quite limited, though, because
not all recommended products have had
reviews.
Thus, we took into account only the rec-
ommended products that have had a review.
We used the average rating values of the
?epinions.com? site which is a rating that
considers the number of reviews together
with the star-system ratings. These average
ratings range from ?disappointing?, ?ok?,
?very good? and ?excellent?. For each prod-
uct we accept the recommended products
that have a rating other than ?disappointing?
which is at least as good as the product?s
rating.
Only 32 products out of the 129 reviewed
were used because those were the ones
which had an average rating value on the
web site. The accuracy we have achieved
is 80.34%. In Figure 2 the percentage of
accepted versus rejected recommendations
is shown per brand. The brand names are
replaced by numbers.
Finally, we would like to point out that in
printer reviews people complain mostly about is-
sues that do not involve comparable features (e.g.
paper jams, toner problems) or that are not given
as part of the detailed characteristics (e.g. car-
tridge prices). As such, in the future, we would
like to use a different product dataset/review-set
to run the experiment over.
67
Figure 2: Rating-based evaluation results: rejected versus accepted recommendations over a number of different
brands.
6 Conclusion
In this paper, we propose using written opinions
and suggestions that are automatically extracted
from user web reviews as input to a recommender
system. This kind of opinions is analysed from a
syntactic and semantic point of view and is used
as a means to recommend items ?better than? the
reviewed one.
The novelty of our proposal lies in the fact that
the semantics of opinions hidden in social media
such as user reviews have not been explicitly used
in order to generate recommendations. To the best
of our knowledge, using the explicit comments of
a user in order to enrich the reviews in a contex-
tual manner has not yet appeared in literature.
In the future, our system could also consider
the user?s role knowledge (e.g. expert or novice)
in order to consider her suggestion from a differ-
ent weighted-point-of-view. An expert may have
already looked at certain existing products before
buying something so she may need a more origi-
nal or diverse recommendation provided. The role
of the user could potentially be identified through
the social network he is in (if there is one).
We realise that some reviews may be spam or
they may be written by non-trustworthy users.
However, our approach aims at providing expert
recommendations as a response to a single review
by considering only what is mentioned in this spe-
cific review. This means that the content of a re-
view, even if it is spam, will not be used in order
to provide recommendations for another review.
References
Silvana Aciar, Debbie Zhang, Simeon Simoff, and
John Debenham. 2007. Informed recommender:
Basing recommendations on consumer product re-
views. IEEE Intelligent Systems, 22(3).
Gediminas Adomavicius and Alexander Tuzhilin.
2005. Towards the next generation of recommender
systems: a survey of the state-of-the-art and possi-
ble extensions. IEEE Transactions on Knowledge
and Data Engineering, 17(6):734?749.
Nitin Agarwal, Huan Liu, Lei Tang, and Philip S. Yu.
2008. Identifying the influential bloggers in a com-
munity. In WSDM ?08: Proceedings of the interna-
tional conference on Web search and web data min-
ing, pages 207?218, New York, NY, USA. ACM.
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
cremental deep parsing. Nat. Lang. Eng., 8:121?
144, June.
Caroline Brun. 2011. Detecting opinions using deep
syntactic analysis. In Recent Advances in Natural
Language Processing (RANLP).
Joyce Chai, Veronika Horvath, Nicolas Nicolov, Stys
Margo, Nanda Kambhatla, Wlodek Zadrozny, and
Prem Melville. 2002. Natural language assistant:
A dialog system for online product recommenda-
tion. AI Magazine, 23(2).
Xiaowen Ding and Bing Liu. 2007. The utility of
linguistic rules in opinion mining. In SIGIR-07.
Pedro Domingos and Matt Richardson. 2001. Mining
the network value of customers. In SIGKDD, pages
57?66.
Danyel Fisher, Marc Smith, and Howard T. Welser.
2006. You are who you talk to: Detecting roles
in usenet newsgroups. In Proceedings of the 39th
Annual Hawaii International Conference on System
Sciences, pages 59b?59b.
68
Anindya Ghose, Panagiotis G. Ipeirotis, and Arun
Sundararajan. 2007. Opinion mining using econo-
metrics: a case study on reputation systems. In
ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD 04, pages 168?
177. ACM.
Niklas Jakob, Stefan Hagen Weber, Mark Christoph
Mu?ller, and Iryna Gurevych. 2009. Beyond the
stars: Exploiting free-text user reviews to improve
the accuracy of movie recommendations. In CIKM
Workshop on Topic-Sentiment Analysis for Mass
Opinion.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In 10th European Conference on Ma-
chine Learning (ECML), page 137142.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguis-
tics, HLT-NAACL ?06, pages 200?207, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jure Leskovec, Daniel P. Huttenlocher, and Jon M.
Kleinberg. 2010. Predicting positive and negative
links in online social networks. In WWW, pages
641?650.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, 2nd ed.
Damien Poirier. 2011. From text to recommendation
(des textes communautaires a la recommandation).
PhD Dissertation.
Jianshu Sun, Chong Long, Xiaoyan Zhu, and Minlie
Huang. 2009. Mining reviews for product compar-
ison and recommendation. Polibits, 39:33?40.
J. Vishwanath and S. Aishwarya. 2011. User sug-
gestions extraction from customer reviews. Inter-
national Journal on Computer Science and Engi-
neering, 3(3).
Jun Zhang, Mark S. Ackerman, and Lada Adamic.
2007. Expertise networks in online communities:
Structure and algorithms. In Proceedings of the
16th International conference on World Wide Web,
pages 221?230.
69
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Linguistically-Adapted Structural Query Annotation for Digital Li-
braries in the Social Sciences 
 Carol ine Brun Vassilina Nikoulina Nikolaos Lagos 
Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240, Meylan France 
{firstname.lastname}@xrce.xerox.com 
 
Abstrac t 
Query processing is an essential part of a  
range of applications in the social sciences 
and cultural heritage domain. However, out-
of-the-box natural language processing tools 
originally developed for full phrase analysis 
are inappropriate for query analysis. In this  
paper, we propose an approach to solving 
this problem by adapting a complete and in-
tegrated chain of NLP tools, to make it  suit-
able for queries analysis. Using as a case 
study the automatic translation of queries 
posed to the Europeana library, we demon-
strate that adapted linguistic processing can 
lead to improvements in translation quality. 
1 Introduction 
Query processing tools are essential components 
of digital libraries and content aggregators. Their 
operation varies from simple stop word removal 
and stemming to advanced parsing, that treats 
queries as a collection of phrases rather than sin-
gle terms (Mothe and Tanguy, 2007). They are 
used in a range of applications, from information 
retrieval (via search engines that provide access 
to the digital collections) to query analysis.  
Current query processing solutions tend to 
use out-of-the-box Natural Language Processing 
(NLP) tools that were originally developed for 
full phrase analysis, being inappropriate for 
query analysis. 
Correct query annotation and interpretation is 
even more important in the cultural heritage or 
social sciences domain, as a lot of the content 
can be in multimedia form and only metadata 
(most of the times in the form of tags) is exploit-
able by traditional text-oriented information re-
trieval and analysis techniques. 
Furthermore, as recent studies of user query-
ing behavior mention, queries in these domains 
are not only very short but are also quite specific 
in terms of content: they refer to artist names, 
titles, dates, and objects (Koolen and Kamps, 
2010; Ireson and Oomen, 2007). Take the exam-
ple of a query like ?coupe apollon? (?bowl apol-
lon?). While in standard analysis ?coupe? would 
be identified as a verb (?couper?, i.e. ?to cut?), in 
the context of a query it should be actually 
tagged as a noun, which refers to an object. Such 
a difference may lead to different preprocessing 
and worse retrieval. 
In this paper, we propose an approach to solv-
ing this problem by adapting a complete and in-
tegrated chain of NLP tools, based on the Xerox 
Incremental Parser (XIP), to make it suitable for 
queries? analysis. The adaptation includes recapi-
talization, adapted Part of Speech (PoS) tagging, 
adapted chunking and Named Entities (NE) rec-
ognition. We claim that several heuristics espe-
cially important for queries? analysis, such as 
favoring nominal interpretations, result in im-
proved linguistic structures, which can have an 
impact in a wide range of further applications 
(e.g. information retrieval, query translation, in-
formation extraction, query reformulation etc.). 
2 Prior art 
The problem of adapted query processing, often 
referred to as structural query annotation, in-
cludes capitalization, NEs detection, PoS tagging 
and query segmentation. Most of the existing 
works treat each of these steps independently and 
address only one of the above issues. 
Many works address the problem of query 
segmentation. According to Tan and Peng 
(2008), query segmentation is a problem which is 
close to the chunking problem, but the chunking 
problem is directly linked to the PoS tagging re-
sults, which are often noisy for the queries. Thus, 
most of the works on query segmentation are 
based on the statistical interaction between a pair 
of query words to identify the border between the 
segments in the query (Jones et al, 2006; Guo et 
55
al., 2008). Tan and Peng (2008) propose a gen-
erative language model enriched with Wikipedia 
to identify ?concepts? rather than simply ?fre-
quency-based? patterns. The segmentation pro-
posed by Bergsma and Wang (2007) is closer to 
the notion of NP chunking. They propose a ma-
chine-learned query segmentation system trained 
on manually annotated set of 500 AOL queries. 
However, in this work PoS tagging is used as one 
of the features in query segmentation and is done 
with a generic PoS tagger, non adapted for que-
ries. 
PoS tagging is an important part of query 
processing and used in many information ana-
lytics tasks (query reformulation, query segmen-
tation, etc.). However very few works address 
query-oriented PoS tagging. Allan and Raghavan 
(2002) consider that PoS tagging might be am-
biguous for short queries and propose to interact 
with the user for disambiguation. Barr et al 
(2008) produce a set of manually annotated que-
ries, and then train a Brill tagger on this set in 
order to create an adapted PoS tagger for search 
queries. 
A notable work is the one by Bendersky et al 
(2010), which addresses the capitalization, PoS 
tagging and query segmentation in the same pa-
per. However, this approach proposes for each of 
the above steps a probabilistic model that relies 
on the document corpus rather on the query it-
self. Such an approach is not applicable for most 
digital content providers who would reluctantly 
give access to their document collection. More-
over, the query expansion, which is the central 
idea of the described approach, is not possible for 
most of digital libraries that are organized in a 
database. Secondly, Bendersky et al (2010) pro-
poses adapting each processing step independ-
ently. Although this is not mentioned in the 
paper, these three steps can be applied in a se-
quence, where PoS tagging can profit from the 
recapitalization, and chunking from the PoS tag-
ging step. However, once the recapitalization is 
done, it can not be changed in the following 
steps. This work doesn?t address the adaptation 
of the NE recognition component, as we do, and 
which might change the final chunking and PoS 
tagging in certain cases. 
In our approach, part of the recapitalization is 
done during the PoS tagging, in interaction with 
the NE recognition, which allows us to consider 
these two steps as interleaved. Moreover, the 
linguistic processing we propose is generic: cor-
pus-independent (at least most of its parts except 
for NE recognition) and doesn?t require access to 
the document collection. 
3 Data 
This work is based on search logs from Euro-
peana 1
4 Motivation 
. These are real users? queries, where 
Named Entities are often lowercased and the 
structures are very different from normal phrase 
structure. Thus, this data is well adapted to dem-
onstrate the impact of adapted linguistic process-
ing. 
We show the importance of the adapted linguistic 
query processing using as example the task of 
query translation, a real need for today?s digital 
content providers operating in a multilingual en-
vironment. We took a sample of Europeana que-
ries and translated them with different MT 
systems: in-house (purely statistical) or available 
online (rule-based). Some examples of problem-
atic translations are shown in the Table 1. 
 
 Input query Automatic 
Translation 
Human 
translation 
 French-English 
1 
 
journal pano-
rama paris  
newspaper 
panorama bets 
newspaper 
panorama 
paris   
2 saint jean de 
luz 
saint jean of 
luz 
saint jean 
de luz 
3 vie et mort 
de l?image 
life and died of 
the image 
life and 
death of 
image 
4 langue et 
r?alit? 
and the reality 
of language 
language 
and reality 
 English-French 
5 maps europe trace l?Europe cartes de 
l?Europe 
6 17th century 
saw 
Du 17?me 
si?cle a vu 
scie du 
17?me 
si?cle 
7 chopin 
george sand 
george sable 
chopin soit 
chopin 
george 
sand 
Table 1: Examples of the problematic query 
translations 
 
                                                                 
1 A portal that acts as an interface to millions of digitized 
records, allowing users to explore Europe?s cultural heri-
tage. For more information please visit 
http://www.europeana.eu/portal/ 
56
Although in general, the errors done by statis-
tical and rule-based models are pretty different, 
there are some common errors done in the case 
of the query translation. Both models, being de-
signed for full-sentence translation, find the 
query structure very unnatural and tend to repro-
duce the full sentence in the output (ex. 1, 3, 4, 5, 
6). The errors may come either from a wrong 
PoS tagging (for rule-based systems), or from the 
wrong word order (statistical-based systems), or 
from the choice of the wrong translation (both 
types of systems). 
One might think that the word order problem 
is not crucial for queries, because most of the IR 
models use the bag of words models, which ig-
nore the order of words. However, it might mat-
ter in some cases: for example, if and/or are 
interpreted as a logical operator, it is important to 
place them correctly in the sentence (examples3, 
4).  
Errors also may happen when translating NEs 
(ex.1, 2, 7). The case information, which is often 
missing in the real-life queries, helps to deal with 
the NEs translation. 
The examples mentioned above illustrate that 
adapted query processing is important for a task 
such as query translation, both in the case of 
rule-based and empirical models. Although the 
empirical models can be adapted if an appropri-
ately sized corpus exists, such a corpus is not 
always available.  
Thus we propose adapting the linguistic proc-
essing prior to query translation (which is further 
integrated in the SMT model). We demonstrate 
the feasibility and impact of our approach based 
on the difference in translation quality but the 
adaptations can be useful in a number of other 
tasks involving query processing (e.g. question 
answering, query logs analysis, etc.). 
5 Linguistic Processing Adaptation 
As said before, queries have specific linguistic 
properties that make their analysis difficult for 
standard NLP tools. This section describes the 
approach we have designed to improve query 
chunking. Following a study of the corpus of 
query logs, we rely on the specific linguistic 
properties of the queries to adapt different steps 
of linguistic analysis, from preprocessing to 
chunking.  
These adaptations consist in the following 
very general processes, for both English and 
French: 
Recapitalization: we recapitalize, in a preproc-
essing step, some uncapitalized words in queries 
that can be proper nouns when they start with a 
capital letter. 
Part of Speech disambiguation
? the part of speech tagging favors nominal 
interpretation (whereas standard part of 
speech taggers are designed to find a verb in 
the input, as PoS tagging generally applies 
on complete sentences); 
:  
? the recapitalization information transmitted 
from the previous step is used to change the 
PoS interpretation in some contexts. 
Chunking
? considering that a full NE is a chunk, which 
is not the case in standard text processing, 
where a NE can perfectly be just a part of a 
chunk; 
:  the chunking is improved by: 
? grouping coordinated NEs of the same type; 
? performing PP and AP attachment with the 
closest antecedent that is morphologically 
compatible 
These processes are very general and may ap-
ply to queries in different application domains, 
with maybe some domain-dependent adaptations 
(for example, NEs may change across domains). 
These adaptations have been implemented 
within the XIP engine, for the French and Eng-
lish grammars. The XIP framework allows inte-
grating the adaptations of different steps of query 
processing into a unified framework, where the 
changes from one step can influence the result of 
the next step: the information performed at a 
given step is transmitted to the next step by XIP 
through linguistic features. 
5.1 Preprocessing 
Queries are often written with misspelling errors, 
in particular for accents and capital letters of 
NEs. See the following query examples extracted 
from our query log corpus: 
 
lafont Robert (French query) 
henry de forge et jean maucl?re 
(French query) 
muse prado madrid (French query) 
carpaccio queen cornaro (English 
query) 
man ray (English query) 
 
This might be quite a problem for linguistic 
treatments, like PoS tagging and of course NE 
57
recognition, which often use capital letter infor-
mation as a triggering feature.  
Recapitalizing these words at the preprocess-
ing step of a linguistic analysis, i.e. during the 
morphological analysis, is technically relatively 
easy, however it would be an important generator 
of spurious ambiguities in the context of full sen-
tence parsing (standard context of linguistic pars-
ing). Indeed, considering that all lower case 
words that can be proper nouns with a capital 
letter should also have capitalized interpretation, 
such as price, jean, read, us, bush, lay, etc., in 
English or pierre, m?decin, ? in French) would 
be problematic for a PoS tagger as well as for a 
NE recognizer. That?s why it is not performed in 
a standard analysis context, considering also that 
misspelling errors are not frequent in ?standard? 
texts. In the case of queries however, they are 
frequent, and since queries are far shorter in av-
erage than full sentences the tagging can be 
adapted to this context (see next section), we can 
afford to perform recapitalization using the fol-
lowing methodology, combining lexical informa-
tion and contextual rules: 
1. The preprocessing lexicon integrates all 
words starting with a lower case letter 
which can be first name (henry, jean, 
isaac ?), family and celebrity name 
(chirac, picasso...) and  place names 
(paris, saint p?tersbourg, ?) when capi-
talized. 
2. When an unknown word starting with a 
lower case letter is preceded by a first 
name and eventually by a particle (de, 
van, von ?), it is analyzed as a last 
name, in order to be able to trigger stan-
dard NE recognition. This is one exam-
ple of interleaving of the processes: here 
part-of-speech interpretation is condi-
tioned by the recapitalization steps which 
transmits information about recapitaliza-
tion (via features within XIP) that trig-
gers query-specific pos disambiguation 
rules. 
The recapitalization (1) has been imple-
mented within the preprocessing components of 
XIP within finite state transducers (see (Kart-
tunen, 2000)). The second point (2) is done di-
rectly within XIP in the part-of-speech tagging 
process, with a contextual rule.  For example, the 
analysis of the input query ?jean maucl?re? gets 
the following structure and dependency output 
with the standard French grammar. 
 
Query: jean maucl?re 
NMOD(jean, maucl?re) 
0>GROUP[NP[jean] AP[maucl?re]] 
 
Because jean is a common noun and maucl?re 
is an unknown word which has been guessed as 
an adjective by the lexical guesser. 
It gets the following analysis with the pre-
processing adaptations described above: 
 
NMOD(jean,maucl?re) 
PERSON_HUM(jean maucl?re) 
FIRSTNAME(jean,jean maucl?re) 
LASTNAME(maucl?re,jean maucl?re) 
0>GROUP[NP[NOUN[jean maucl?re]]] 
 
Because jean has been recognized as a first 
name and consequently the unknown word after 
has been inferred has a proper noun (last name) 
by the pos tagging contextual rule; the recapitali-
zation process and part-of-speech interpretation 
are therefore interleaved. 
5.2 Part of speech disambiguation 
In the context of query analysis, part-of-speech 
tagging has to be adapted also, since standard 
part-of-speech disambiguation strategies aim 
generally at disambiguating in the context of full 
sentences. But queries are very different from 
full sentences: they are mostly nominal with 
sometimes infinitive, past participial, or gerun-
dive insertions, e.g.: 
 
statuettes hommes jouant avec un 
chien (French query) 
coupe apollon (French query) 
architecture musique (French 
query) 
statue haut relief grecque du 5 
siecle (French query)  
david playing harp fpr saul (Eng-
lish query) 
stained glass angel (English 
query) 
 
Standard techniques for part-of-speech tag-
ging include rule based methods and statistical 
methods, mainly based on hidden Markov mod-
els (see for example (Chanod and Tapanainen, 
1995)). In this case, it would be possible to re-
compute the probabilities on a corpus of queries 
manually annotated. However, the correction of 
part-of-speech tags in the context of queries is 
easy to develop with a small set of rules. We fo-
cus on English and French, and in queries, the 
main problems come from the ambiguity be-
58
tween noun and verbs, which has to be solved 
differently than in the context of a standard sen-
tence.   
The approach we adopt to correct the tagging 
with the main following contextual rules: 
? If there is a noun/verb ambiguity: 
? If the ambiguity is on the first word of 
the query (e.g. ?coupe apollon?, ?oil
? If the ambiguity is on the second word of 
the query, prefer the noun interpretation 
if the query starts with an adjective or a 
noun (e.g. in ?young 
 
flask?), select the noun interpretation; 
people
? Select noun interpretation if there is no 
person agreement with one of the previ-
ous nouns (e.g. ?les fr?res 
 social com-
petences?, select the noun interpretation 
for people, instead of verb) 
bissons
? For a verb which is neither at the past 
participle form nor the infinitive form, 
select the noun interpretation if it is not 
followed by a determiner (e.g. ?tremble-
ment 
?, 
fr?res belongs to the 3rd person but bis-
sons to the 1st one of the verb ?bisser?) 
terre
? Choose the noun interpretation if the 
word is followed by a conjunction and a 
noun or preceded by a noun and a con-
junction (e.g. in ?gauguin 
 lisbonne?, terre is disambigu-
ated as a noun?)) 
moon and 
earth?, choose the noun interpretation 
for moon, instead of verb2
? In case of ambiguity between adjective and 
past participle verb, select the adjective in-
terpretation if the word is followed by a 
noun (e.g.  ?stained glass angel?, stained is 
disambiguated as an adjective instead of a 
past participle verb) 
). 
5.3 Chunking 
The goal of chunking is to assign a partial 
structure to a sentence and focuses on easy to 
parse pieces in order to avoid ambiguity and re-
cursion. In the very specific context of query 
analysis, and once again since queries have spe-
cific linguistic properties (they are not sentences 
but mostly nominal sequences), chunking can be 
improved along several heuristics. We propose 
here some adaptations to improve query chunk-
                                                                 
2To moon about 
ing to deal with AP and PP attachment, and co-
ordination, using also NE information to guide 
the chunking strategy. 
AP and PP attachment 
In standard cases of chunking, AP and PP at-
tachment is not considered, because of attach-
ment ambiguity problems that cannot be solved 
at this stage of linguistic analysis.  
Considering the shortness of queries and the 
fact that they are mostly nominal, some of these 
attachments can be solved however in this con-
text. 
For the adjectival attachment in French, we 
attach the post modifier adjectival phrases to the 
first previous noun with which there is agreement 
in number and gender. For example, the chunk-
ing structure for the query ?Biblioth?que eu-
ropeenne numerique? is: 
 
NP[ [Biblioth?que AP[europeenne] 
AP[numerique] ] 
 
while it is  
 
NP[Biblioth?que] AP[europeenne] 
AP[numerique] 
 
with our standard French grammar.  
For PP attachment, we simply consider that 
the PP attaches systematically to the previous 
noun. For example, the chunking structure for 
?The history of the University of Oxford? is: 
 
NP[the history PP[of the University 
PP[of Oxford] ] ]  
 
instead of: 
 
NP[The history] PP[of the Univer-
sity] PP[of  Oxford ] 
Coordination 
Some cases of coordination, usually very com-
plex, can be solved in the query context, in par-
ticular when NEs are involved. For both English 
and French, we attach coordinates when they 
belong to the same entity type (person conj per-
son, date conj date, place conj place, etc.), for 
example, ?vase achilles et priam? :  
 
NP[vase] NP[Achille et Priam]  
 
instead of: 
 
NP[vase] NP[Achille] et NP[Priam] 
59
 
We also attach coordinates when the second 
is introduced by a reflexive pronoun, such as in: 
?[Le laboureur et ses enfants] La Fontaine? and 
attach coordinates within a PP when they are in-
troduced by the preposition ?entre? in French 
and ?between? in English. 
Use of NE information to guide the chunking 
strategy 
We also use information about NEs present in 
the queries to guide the query chunking strategy. 
In standard analysis, NEs are generally part of 
larger chunking units. In queries, however, be-
cause of their strong semantic, they can be iso-
lated as separate chunking units. We have 
adapted our chunking strategy using this infor-
mation: when the parser detects a NE (including 
a date), it chunks it as a separate NP. The follow-
ing examples show the chunking results for this 
adapted strategy versus the analysis of standard 
grammar: 
 
? ?Anglo Saxon 11th century? (English) 
Adapted chunking:  
NP[Anglo Saxon] NP[ 11th century] 
 
Standard chunking:  
NP[Anglo Saxon 11th century ] 
 
? ?Alexandre le Grand Persepolis?  (French) 
Adapted chunking:  
NP[Alexandre le Grand] NP[Perspolis] 
 
Standard chunking: 
NP[Alexandre le Grand Perspolis] 
 
The whole process is illustrated in Figure 1.  
 
When applying the full chain on an example 
query like ?gauguin moon and earth?, we have 
the following steps and result: 
Preprocessing: gauguin is recognized as Gauguin 
(proper noun of celebrity); 
Part of speech tagging:  moon is disambiguated 
as a noun instead of a verb); 
Chunking:
So we get the following structure:  
 moon and earth are grouped together 
in a coordination chunk, gauguin is a NE 
chunked separately.  
 
NP[Gauguin] NP[moon and earth] 
 
and gauguin is recognized as a person name, 
instead of  
 
SC 3 [NP[gauguin] FV 4
 
Input query 
Query 
Preprocessing 
Query POS 
Disambiguation 
Query  
Chunking 
Adapted lexical 
Resources combined 
with contextual rules 
Improved query structure 
Adapted strategy for 
POS tagging 
(Contextual rules) 
Adapted Chunking 
strategy: contextual 
rules + named entities  
[moon]] and 
NP[earth],  
 
gauguin remaining unknown,  with the standard 
English grammar. 
 
Fig 1: Linguistic processing adaptation for que-
ries 
 
5.4 Examples of query structures 
The following table shows the differences of 
query structures obtained with the standard lin-
guistic processing and with the adapted linguistic 
processing.  
 
1. Albert Camus la peste 
Standard LP: NP {albert}  AP {camus}  
NP {la peste}   
Adapted LP: NP {albert camus}  NP {la 
peste}   
2. dieux ou h?ros grec 
Standard LP: NP {dieux}  COORD {ou}  
NP {h?ros}  AP {grec}   
                                                                 
3 SC: chunk tag for sentential clause 
4 FV: finite verb chunk 
60
Adapted LP: NP {dieux}  COORD {ou}  
NP {h?ros grec}   
3. pierre berg? 
Standard LP: NP {pierre}  VERB {berg?}   
Adapted LP: NP {pierre berg?} 
Table 2:  Some examples of query structure produced 
by standard and adapted linguistic processing. 
 
The evaluation of this customization is done 
indirectly through query translation, and is de-
scribed in the next section 
6 Experiments 
6.1 Experimental settings 
In our experiments we tried to enrich our base-
line SMT system with an adapted linguistic proc-
essing in order to improve the query translation. 
These experiments have double goal. First, to 
show that the adapted linguistic processing al-
lows to improve query translation compared to a 
standard linguistic processing, and second, to 
show that enriching an SMT model with a lin-
guistic processing (adapted) is helpful for the 
translation.  
We use an open source toolkit Moses (trained 
on Europarl) as a baseline model for query trans-
lations. Based on the examples from the section 
5, we choose to integrate the chunking and NE 
information in the translation. We integrate this 
knowledge in the following way: 
? Chunking: We check whether the query 
matches one of the following patterns: ?NP1 
and NP2?, ?NP1 or NP2?, ?NP1 NP2?, 
?NP1, NP2?, etc. If it is the case, the NPs are 
translated independently.  Thus, we make 
sure that the output query will preserve the 
logical structure, if ?and/or? are treated as 
logical operators. Also, translating NPs inde-
pendently might result at different (hopefully 
better) lexical choices.  
? Named entities: We introduce XML tags for 
person names where we propose a possible 
translation. During the translation process the 
proposed translation competes with the pos-
sible translations from a bi-phrase library. 
The translation maximizing internal transla-
tion score is chosen. In these experiments we 
propose not to translate an NE at all, how-
ever in more general case we could imagine 
having an adapted NE dictionary.  
6.2 Evaluation 
We have translated the totality of available 
Europeana French logs to English (8870 distinct 
queries), with the following translation models:  
? Moses trained on Europarl (Baseline 
MT) 
? Baseline MT model enriched with lin-
guistic processing (as defined in 6.1) 
based on basic grammar (Baseline MT + 
basic grammar) 
? Baseline MT enriched with linguistic 
processing based on adapted grammar 
(Baseline MT + adapted grammar) 
Our approach brings two new aspects com-
pared to simple SMT system. First, an SMT sys-
tem is enriched with linguistic processing as 
opposed to system without linguistic processing 
(baseline system), second: usage of an adapted 
linguistic processing as opposed to standard lin-
guistic processing. Thus, we evaluate: 
1. The impact of linguistic processing on 
the final query translations; 
2. The impact of grammar adaptation 
(adapted linguistic processing) in the 
context of query translation. 
First, we measure the overall impact of each 
of the two aspects mentioned above. Table 3 re-
flects the general impact of linguistic enrichment 
and grammar adaptation on query structure and 
translation. 
First, we note that the linguistic processing as 
defined in 6.1 won?t be applied to all queries. 
Thus, we count an amount of queries out of our 
test set to which this processing can actually be 
applied. This corresponds to the first line of the 
Table 3 (26% of all queries). 
Second, we compare the queries translation 
with and without linguistic processing. This is 
shown in the second line of the Table 3: the 
amount of queries for which the linguistic proc-
essing lead to different translation (25% of que-
ries for which the linguistic processing was 
applied). 
The second part of the table shows the differ-
ence between the standard linguistic processing 
and an adapted linguistic processing. First, we 
check how many queries get different structure 
after grammar adaptation (Section 5) (~42%) and 
second, we check how many of these queries 
61
actually get different translation (~16% queries 
with new structure obtained after adaptation get 
different translations).  
These numbers show that the linguistic 
knowledge that we integrated into the SMT 
framework may impact a limited portion of que-
ries? translations. However, we believe that this 
is due, to some extent, to the way the linguistic 
knowledge was integrated in SMT, which ex-
plores only a small portion of the actual linguis-
tic information that is available. We carried out 
these experiments as a proof of concept for the 
adapted linguistic processing, but we believe that 
a deeper integration of the linguistic knowledge 
into the SMT framework will lead to more sig-
nificant results. For example, integrating such an 
adapted linguistic processing in a rule-based MT 
system will be straightforward and beneficial, 
since the linguistic information is explored di-
rectly by a translation model (e.g. in the example 
6 in Table 1 tagging "saw" as a noun will defi-
nitely lead to a better translation). 
Next, we define 2 evaluation tasks, where the 
goal of each task is to compare 2 translation 
models. We compare:  
1. Baseline MT versus linguistically en-
riched translation model (Baseline MT+adapted 
adapted linguistic processing). This task evalu-
ates the impact of linguistic enrichment in the 
query translation task with SMT.  
2. Translation model using standard lin-
guistic processing versus translation model using 
adapted linguistic processing. This task evalu-
ates the impact of the adapted linguistic process-
ing in the query translation task.  
For each evaluation task we have randomly 
selected a sample of 200 translations (excluding 
previously the identical translations for the 2 
models compared) and we perform a pairwise 
evaluation for each evaluation task. Thus, for the 
first evaluation task, a baseline translation (per-
formed by standard Moses without linguistic 
processing) is compared to the translation done 
by Moses + adapted linguistic processing. In the 
second evaluation task, the translation performed 
by Moses + standard linguistic processing is 
compared to the translation performed by Moses 
+ adapted linguistic processing. 
The evaluation has been performed by 3 
evaluators. However, no overlapping evaluations 
have been performed to calculate intra-evaluators 
agreement. We could observe, however, the simi-
lar tendency for improvement in each on the 
evaluated sample (similar to the one shown in the 
Table 2).  
We evaluate the overall translation perform-
ance, independently of the task in which the 
translations are going to be used afterwards (text  
Table 3: Impact of linguistic processing and 
grammar adaptation for query translation 
 
understanding, text analytics, cross-lingual in-
formation retrieval etc.) 
The difference between slight improvements 
and important improvements as in the examples 
below has been done during the evaluation. 
 
src1: max weber 
t1:max mr weber 
t2:max weber (slight improvement)
   
src2: albert camus la peste 
t1:albert camus fever 
t2:albert camus the plague (impor-
tant improvement) 
 
Thus, each pair of translations (t1, t2) re-
ceives a score from the scale [-2, 2] which can 
be: 
? 2, if t2 is much better than t1,  
? 1, if t2 is better than t1,  
? 0, if t2 is equivalent to t1,  
? -1, if t1 is better than t2,  
? -2, if t1 is much better than t2,  
Linguistic enrichment 
Nb of  queries to which the adapted 
linguistic processing was applied 
before translation.  
2311 
(26% of 
8870) 
Nb of translations which differ 
between baseline Moses and Moses 
with adapted linguistic processing.  
582  
(25% of 
2311) 
Grammar adaptation 
Nb of queries which get different 
structures between standard linguistic 
processing and adapted linguistic 
processing. 
3756  
(42% of 
8870) 
Nb of translations which differ 
between Moses+standard linguistic 
processing and Moses+adapted 
linguistic processing 
638   
(16 %  of 
3756) 
62
Table 4 presents the results of translation 
evaluation. 
 
Note, that a part of slight decreases can be 
corrected by introducing an adapted named enti-
ties dictionary to the translation system. For ex-
ample, for the source query ?romeo et juliette?, 
keeping NEs untranslated results at the following 
translation: ?romeo and juliette?, which is con-
sidered as a slight decrease in comparison to a 
baseline translation: ?romeo and juliet?. Creating 
an adapted NEs dictionary, either by crawling 
Wikipedia, or other parallel resources, might be 
helpful for such cases. 
Often, the cases of significantly better transla-
tions could potentially lead to the better retrieval. 
For example, a better lexical choice (don juan 
moliere vs. donation juan moliere, the plague vs. 
fever) often judged as significant improvement 
may lead to a better retrieval. 
Based on this observation one may hope that 
the adapted linguistic processing can indeed be 
useful in the query translation task in CLIR con-
text, but also in general query analysis context. 
7 Conclusion 
Queries posed to digital library search engines in 
the cultural heritage and social sciences domain 
tend to be very short, referring mostly to artist 
names, objects, titles, and dates. As we have il-
lustrated with several examples, taken from the 
logs of the Europeana portal, standard NLP 
analysis is not well adapted to treat that domain. 
In this work we have proposed adapting a com-
plete chain of linguistic processing tools for 
query processing, instead of using out-of-the-box 
tools designed to analyze full sentences. 
Focusing on the cultural heritage domain, we 
translated queries from the Europeana portal us-
ing a state-of-the-art machine translation system 
and evaluated translation quality before and after 
applying the adaptations. The impact of the lin-
guistic adaptations is quite significant, as in 42% 
of the queries the resulting structure changes. 
Subsequently, 16% of the query translations are 
also different. The positive impact of the adapted 
linguistic processing on the translation quality is 
evident, as for 99 queries the translation (out of 
200 sample evaluated) is improved when com-
pared to having no linguistic processing. We ob-
serve also that 78 queries are better translated 
after adapting the linguistic processing compo-
nents. 
Our results show that customizing the linguis-
tic processing of queries can lead to important  
 
improvements in translation (and eventually to 
multilingual information retrieval and data min-
ing). A lot of the differences are related to the 
ability of properly identifying and treating do-
main-specific named entities. We plan to further 
research this aspect in future works. 
 
Acknowledge ments  
This research was supported by the European 
Union?s ICT Policy Support Programme as part 
of the Competitiveness and Innovation Frame-
work Programme, CIP ICT-PSP under grant 
agreement nr 250430 (Project GALATEAS). 
References 
Bin Tan and Fuchun Peng. 2008. Unsupervised query 
segmentation using generative language models 
and wikipedia. In Proceedings of the 17th interna-
tional conference on World Wide Web (WWW 
'08). ACM, New York, NY, USA, 347-356. 
Cory Barr, Rosie Jones, Moira Regelson. 2008. The 
Linguistic Structure of EnglishWeb-Search Que-
ries, Proceedings of ENMLP'08, pp 1021?1030, 
Octobre 2008, Honolulu. 
James Allan and Hema Raghavan. 2002. Using part-
of-speech patterns to reduce query ambiguity. In  
Proceedings of the 25th annual international ACM 
SIGIR conference on Research and development in  
informat ion retrieval (SIGIR '02). ACM, New 
York, NY, USA, 307-314. 
Jeann-Pierre Chanod, Pasi Tapanainen. 1995.  Tag-
ging French - comparing a statistical and a con-
straint-based method. Proc. From Texts To Tags: 
 Impor
tant 
++ 
 Total 
nb+ 
Impo
rtant 
- - 
Total  
nb - 
Overall 
impact   
Moses<
Moses+
adapted  
35 87 4 19 99 
Moses+
basic<
Moses+
adapted  
28 66 2 12 80 
Table 4: Translation evaluation. Total nb+ (-): total 
number of improvements (decreases), not distinguish-
ing whether it is slight or important; important ++ (--): 
the number of important improvements (decreases). 
Overall impact = (Total nb+) + (Importan++ ) ? (Total 
nb-) ? (Important --) 
63
Issues In Multilingual Language Analysis, EACL 
SIGDAT workshop. Dublin, 1995. 
Jiafeng Guo, Gu Xu, Hang Li, Xueqi Cheng. 2008. A 
Unified and Discriminative Model for Query Re-
finement. Proc. SIGIR?08, July 20?24, 2008, Sin-
gapore. 
Josiane Mothe and Ludovic Tanguy. 2007. Linguistic 
Analysis of Users' Queries: towards an adaptive In-
formation Retrieval System. International Confer-
ence on Signal-Image Technology & Internet?
Based Systems, Shangai, China, 2007. 
http://halshs.archives-ouvertes.fr/halshs-
00287776/fr/ [Last accessed March 3, 2011]  
Lauri Karttunen. 2000. Applications of Finite-State 
Transducers in Natural Language Processing. Pro-
ceedings of CIAA-2000. Lecture Notes in Com-
puter Science. Springer Verlag. 
Marijn Koolen and Jaap Kamps. 2010. Searching cul-
tural heritage data: does structure help expert  
searchers?. In Adaptivity, Personalizat ion and Fu-
sion of Heterogeneous Information (RIAO '10). Le 
centre des hautes etudes internationals 
d?informat ique documentaire, Paris, France, 152-
155. 
Michael Bendersky, W. Bruce Croft and David A. 
Smith. 2010. Structural Annotation of Search Que-
ries Using Pseudo-Relevance Feedback. Proceed-
ings of CIKM'10, October 26-29, 2010, Toronto, 
Ontario, Canada 
Neil Ireson and Johan Oomen. 2007. Capturing e-
Culture: Metadata in MultiMatch., J. In Proc. 
DELOS-MultiMatch workshop, February 2007, 
Tirrenia, Italy. 
Rosie Jones, Ben jamin Rey, Omid Madani, and Wiley 
Greiner. 2006. Generating query substitutions. In 
Proceedings of the 15th international conference on 
World Wide Web (WWW '06). ACM, New York, 
NY, USA, 387-396. 
Shane Bergsma and Qin Iris Wang. 2007. Learning 
Noun Phrase Query Segmentation, Proceedings of 
the 2007 Jo int Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pp. 819?826, Prague, 
June 2007. 
64
