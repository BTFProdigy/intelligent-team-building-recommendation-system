	

		
	
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474?1480,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Leveraging Effective Query Modeling Techniques  
for Speech Recognition and Summarization 
 
 Kuan-Yu Chen*?,  Shih-Hung Liu*, Berlin Chen#, Ea-Ee Jan+,  
Hsin-Min Wang*, Wen-Lian Hsu*, and Hsin-Hsi Chen? 
*Institute of Information Science, Academia Sinica, Taiwan 
?National Taiwan University, Taiwan 
#National Taiwan Normal University, Taiwan 
+IBM Thomas J. Watson Research Center, USA 
{kychen, journey, whm, hsu}@iis.sinica.edu.tw, 
berlin@ntnu.edu.tw, hhchen@csie.ntu.edu.tw, ejan@us.ibm.com 
 
Abstract 
Statistical language modeling (LM) that 
purports to quantify the acceptability of a 
given piece of text has long been an in-
teresting yet challenging research area. In 
particular, language modeling for infor-
mation retrieval (IR) has enjoyed re-
markable empirical success; one emerg-
ing stream of the LM approach for IR is 
to employ the pseudo-relevance feedback 
process to enhance the representation of 
an input query so as to improve retrieval 
effectiveness. This paper presents a con-
tinuation of such a general line of re-
search and the main contribution is three-
fold. First, we propose a principled 
framework which can unify the relation-
ships among several widely-used query 
modeling formulations. Second, on top of 
the successfully developed framework, 
we propose an extended query modeling 
formulation by incorporating critical que-
ry-specific information cues to guide the 
model estimation. Third, we further adopt 
and formalize such a framework to the 
speech recognition and summarization 
tasks. A series of empirical experiments 
reveal the feasibility of such an LM 
framework and the performance merits of 
the deduced models on these two tasks. 
1 Introduction 
Along with the rapidly growing popularity of the 
Internet and the ubiquity of social web commu-
nications, tremendous volumes of multimedia 
contents, such as broadcast radio and television 
programs, digital libraries and so on, are made 
available to the public. Research on multimedia 
content understanding and organization has wit-
nessed a booming interest over the past decade. 
By virtue of the developed techniques, a variety 
of functionalities were created to help distill im-
portant content from multimedia collections, or 
provide locations of important speech segments 
in a video accompanied with their corresponding 
transcripts, for users to listen to or to digest. Sta-
tistical language modeling (LM) (Jelinek, 1999; 
Jurafsky and Martin, 2008; Zhai, 2008), which 
manages to quantify the acceptability of a given 
word sequence in a natural language or capture 
the statistical characteristics of a given piece of 
text, has been proved to offer both efficient and 
effective modeling abilities in many practical 
applications of natural language processing and 
speech recognition (Ponte and Croft, 1998; Jelin-
ek, 1999; Huang, et al., 2001; Zhai and Lafferty, 
2001a; Jurafsky and Martin, 2008; Furui et al., 
2012; Liu and Hakkani-Tur, 2011). 
The LM approach was first introduced for the 
information retrieval (IR) problems in the late 
1990s, indicating very good potential, and was 
subsequently extended in a wide array of follow-
up studies. One typical realization of the LM ap-
proach for IR is to access the degree of relevance 
between a query and a document by computing 
the likelihood of the query generated by the doc-
ument (usually referred to as the query-
likelihood approach) (Zhai, 2008; Baeza-Yates 
and Ribeiro-Neto, 2011). A document is deemed 
to be relevant to a given query if the correspond-
ing document model is more likely to generate 
the query. On the other hand, the Kullback-
Leibler divergence measure (denoted by KLM 
for short hereafter), which quantifies the degree 
of relevance between a document and a query 
from a more rigorous information-theoretic per-
spective, has been proposed (Lafferty and Zhai, 
2001; Zhai and Lafferty, 2001b; Baeza-Yates and 
Ribeiro-Neto, 2011). KLM not only can be 
thought as a natural generalization of the query-
likelihood approach, but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the performance of doc-
ument ranking. For example, a main challenge 
facing such a measure is that since a given query 
usually consists of few words, the true infor-
mation need is hard to be inferred from the sur-
face statistics of a query. As such, one emerging 
stream of thought for KLM is to employ the 
1474
pseudo-relevance feedback process to construct 
an enhanced query model (or representation) so 
as to achieve better retrieval effectiveness (Hi-
emstra et al., 2004; Lv and Zhai, 2009; Carpineto 
and Romano, 2012; Lee and Croft, 2013). 
Following this line of research, the major con-
tribution of this paper is three-fold: 1) we ana-
lyze several widely-used query models and then 
propose a principled framework to unify the rela-
tionships among them; 2) on top of the success-
fully developed query models, we propose an 
extended modeling formulation by incorporating 
additional query-specific information cues to 
guide the model estimation; 3) we explore a nov-
el use of these query models by adapting them to 
the speech recognition and summarization tasks. 
As we will see, a series of experiments indeed 
demonstrate the effectiveness of the proposed 
models on these two tasks. 
2 Language Modeling Framework 
2.1 Kullback-Leibler Divergence Measure 
A promising realization of the LM approach to 
IR is the Kullback-Leibler divergence measure 
(KLM), which determines the degree of rele-
vance between a document and a query from a 
rigorous information-theoretic perspective. Two 
different language models are involved in KLM: 
one for the document and the other for the query. 
The divergence of the document model with re-
spect to the query model is defined by  
.)|( )|(log)|()||(KL ? ?? Vw DwP QPQwPDQ
  (1)  
KLM not only can be thought as a natural gener-
alization of the traditional query-likelihood ap-
proach (Yi and Allan, 2009; Baeza-Yates and 
Ribeiro-Neto, 2011), but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the estimation of its 
component models in a systematic way for better 
document ranking (Zhai, 2008).  
Due to that a query usually consists of only a 
few words, the true query model P(w|Q)
 
might 
not be accurately estimated by the simple ML 
estimator (Jelinek, 1991). There are several stud-
ies devoted to estimating a more accurate query 
modeling, saying that it can be approached with 
the pseudo-relevance feedback process (Lavren-
ko and Croft, 2001; Zhai and Lafferty, 2001b). 
However, the success depends largely on the as-
sumption that the set of top-ranked documents, 
DTop={D1,D2,...,Dr,...}, obtained from an initial 
round of retrieval, are relevant and can be used to 
estimate a more accurate query language model. 
2.2 Relevance Modeling  
Under the notion of relevance modeling (RM, 
often referred to as RM-1), each query Q is as-
sumed to be associated with an unknown rele-
vance class RQ, and documents that are relevant 
to the semantic content expressed in query are 
samples drawn from the relevance class RQ. 
Since there is no prior knowledge about RQ, we 
may use the top-ranked documents DTop to ap-
proximate the relevance class RQ. The corre-
sponding relevance model can be estimated using 
the following equation (Lavrenko and Croft, 
2001; Lavrenko, 2004): 
.)|()(
)|()|()(  )|(RM ? ? ??
? ? ??
???
?
?????
??
ToprD
ToprD
Qw rr
Qw rrr
DwPDP
DwPDwPDPQwP
D
D
(2) 
2.3 Simple Mixture Model 
Another perspective of estimating an accurate 
query model with the top-ranked documents is 
the simple mixture model (SMM), which as-
sumes that words in DTop are drawn from a two-
component mixture model: 1) One component is 
the query-specific topic model PSMM(w|Q), and 2) 
the other is a generic background model 
P(w|BG). By doing so, the SMM model 
PSMM(w|Q) can be estimated by maximizing the 
likelihood over all the top-ranked documents 
(Zhai and Lafferty, 2001b; Tao and Zhai, 2006): 
? ? ,)|()1()|( ),(SMM?? ?? ????? Topr rD Vw DwcBGwPQwPL D ??
(3) 
where ?  is a pre-defined weighting parameter 
used to control the degree of reliance between 
PSMM(w|Q) and P(w|BG). This estimation will 
enable more specific words to receive more 
probability mass, thereby leading to a more dis-
criminative query model PSMM(w|Q). 
Although the SMM modeling aims to extract 
extra word usage cues for enhanced query mod-
eling, it may confront two intrinsic problems. 
One is the extraction of word usage cues from 
DTop is not guided by the original query. The oth-
er is that the mixing coefficient ?  is fixed across 
all top-ranked documents albeit that different 
documents would potentially contribute different 
amounts of word usage cues to the enhanced 
query model. To mitigate these two problems, 
the regularized simple mixture model has been 
proposed and can be estimated by maximizing 
the likelihood function (Tao and Zhai, 2006; Dil-
lon and Collins-Thompson, 2010) 
? ? ,)|()1()|(    
)|(
),(
RSMM
)|(
RSMM
?
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
Dwc
DD
Vw
QwP
BGwPQwP
QwPL
D
??
?
(4) 
where   is a weighting factor indicating the con-
fidence on the prior information. 
3 The Proposed Modeling Framework 
3.1 Fundamentals 
It is obvious that the major difference among the 
1475
representative query models mentioned above is 
how to capitalize on the set of top-ranked docu-
ments and the original query. Several subtle rela-
tionships can be deduced through the following 
in-depth analysis. First, a direct inspiration of the 
LM-based query reformulation framework can 
be drawn from the celebrated Rocchio?s formula-
tion, while the former can be viewed as a proba-
bilistic counterpart of the latter (Robertson, 1990; 
Ponte and Croft, 1998; Baeza-Yates and Ribeiro-
Neto, 2011). Second, after some mathematical 
manipulation, the formulation of the RM model 
(c.f. Eq. (2)) can be rewritten as 
.)()|(
)()|()|(  )|(RM ? ? ???? ????? ToprD ToprD rr
rrr DPDQP
DPDQPDwPQwP D D
(5) 
It becomes evident that the RM model is com-
posed by mixing a set of document models 
P(w|Dr). As such, the RM model bears a close 
resemblance to the Rocchio?s formulation. Fur-
thermore, based on Eq. (5), we can recast the 
estimation of the RM model as an optimization 
problem, and the likelihood (or objective) func-
tion is formulated as 
1)|( ..
,)|()|(
),(
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Topr
Topr
D
r
Vw
Qwc
D
rr
QDPts
QDPDwPL
D
D
  (6) 
where the document models P(w|Dr) are known 
in advance; the conditional probability P(Dr|Q) 
of each document Dr is unknown and leave to be 
estimated. Finally, a principled framework can 
be obtained to unify all of these query models, 
including RM (c.f. Eq. (6)), SMM (c.f. Eq. (3)) 
and RSMM (c.f. Eq. (4))), by using a generalized 
objective likelihood function: 
1)( ..
,)()|(
),(
?
???
?
???
?
?
?
?
?
?
?
?
?
?
M
E M
r
i
i
r
M
r
Vw E
Ewc
M
rr
MPts
MPMwPL  (7) 
where E represents a set of observations which 
we want to maximize their likelihood, and M 
denotes a set of mixture components.  
3.2 Query-specific Mixture Modeling 
The SMM model and the RSMM model are in-
tended to extract useful word usage cues from 
DTop, which are not only relevant to the original 
query Q but also external to those already cap-
tured by the generic background model. Howev-
er, we argue in this paper that the ?generic in-
formation? should be carefully crafted for each 
query due mainly to the fact that users? infor-
mation needs may be very diverse from one an-
other. To crystallize the idea, a query-specific 
background model PQ(w|BG) for each query Q 
can be derived from DTop directly. Another con-
sideration is that since the original query model 
P(w|Q) cannot be accurately estimated, it thus 
may not necessarily be the best choice for use in 
defining a conjugate Dirichlet prior for the en-
hanced query model to be estimated. We propose 
to use the RM model as a prior to guide the esti-
mation of the enhanced query model. The en-
hanced query model is termed query-specific 
mixture model (QMM), and its corresponding 
training objective function can be expressed as 
? ??
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
DwcQDD
Vw
QwP
BGwPQwP
QwPL
D
.)|()1()|(    
)|(
),(QMM
)|(QMM RM
??
?
 (8) 
4 Applications 
4.1 Speech Recognition 
Language modeling is a critical and integral 
component in any large vocabulary continuous 
speech recognition (LVCSR) system (Huang et 
al., 2001; Jurafsky and Martin, 2008; Furui et al., 
2012). More concretely, the role of language 
modeling in LVCSR can be interpreted as calcu-
lating the conditional probability P(w|H), in 
which H is a search history, usually expressed as 
a sequence of words H=h1, h2,?, hL, and w is 
one of its possible immediately succeeding 
words. Once the various aforementioned query 
modeling methods are applied to speech recogni-
tion, for a search history H, we can conceptually 
regard it as a query and each of its immediately 
succeeding words w as a (single-word) document. 
Then, we may leverage an IR procedure that 
takes H as a query and poses it to a retrieval sys-
tem to obtain a set of top-ranked documents from 
a contemporaneous (or in-domain) corpus. Final-
ly, the enhanced query model (that is P(w|H) in 
speech recognition) can be estimated by RM, 
SMM, RSMM or QM , and further combined 
with the background n-gram (e.g., trigram) lan-
guage model to form an adaptive language model 
to guide the speech recognition process. 
4.2 Speech Summarization 
On the other hand, extractive speech summariza-
tion aims at producing a concise summary by 
selecting salient sentences or paragraphs from 
the original spoken document according to a pre-
defined target summarization ratio (Carbonell 
and Goldstein, 1998; Mani and Maybury, 1999; 
Nenkova and McKeown, 2011; Liu and 
Hakkani-Tur, 2011). Intuitively, this task could 
be framed as an ad-hoc IR problem, where the 
spoken document is treated as an information 
need and each sentence of the document is re-
garded as a candidate information unit to be re-
trieved according to its relevance to the infor-
mation need. Therefore, KLM can be used to 
quantify how close the document D and one of 
its sentences S are: the closer the sentence model 
P(w|S) to the document model P(w|D), the more 
1476
likely the sentence would be part of the summary. 
Due to that each sentence S of a spoken docu-
ment D to be summarized usually consists of 
only a few words, the corresponding sentence 
model P(w|S) might not be appropriately esti-
mated by the ML estimation. To alleviate the 
deficiency, we can leverage the merit of the 
above query modeling techniques to estimate an 
accurate sentence model for each sentence to 
enhance the summarization performance. 
5 Experimental Setup 
The speech corpus consists of about 196 hours of 
Mandarin broadcast news collected by the Aca-
demia Sinica and the Public Television Service 
Foundation of Taiwan between November 2001 
and April 2003 (Wang et al., 2005), which is 
publicly available and has been segmented into 
separate stories and transcribed manually. Each 
story contains the speech of one studio anchor, as 
well as several field reporters and interviewees. 
A subset of 25-hour speech data compiled during 
November 2001 to December 2002 was used to 
bootstrap the acoustic model training. The vo-
cabulary size is about 72 thousand words. The 
background language model was estimated from 
a background text corpus consisting of 170 mil-
lion Chinese characters collected from the Chi-
nese Gigaword Corpus released by LDC. 
The dataset for use in the speech recognition 
experiments is compiled by a subset of 3-hour 
speech data from the corpus within 2003 (1.5 
hours for development and 1.5 hours for test). 
The contemporaneous (in-domain) text corpus 
used for training the various LM adaptation 
methods was collected between 2001 and 2003 
from the corpus (excluding the test set), which 
consists of one million Chinese characters of the 
orthographic broadcast news transcripts. In this 
paper, all the LM adaptation experiments were 
performed in word graph rescoring. The associ-
ated word graphs of the speech data were built 
beforehand with a typical LVCSR system (Ort-
manns et al., 1997; Young et al., 2006). 
In addition, the summarization task also em-
ploys the same broadcast news corpus as well. A 
subset of 205 broadcast news documents com-
piled between November 2001 and August 2002 
was reserved for the summarization experiments 
(185 for development and 20 for test). A subset 
of about 100,000 text news documents, compiled 
during the same period as the documents to be 
summarized, was employed to estimate the relat-
ed summarization models compared in this paper. 
We adopted three variants of the widely-used 
ROUGE metric (i.e., ROUGE-1, ROGUE-2 and 
ROUGE-L) for the assessment of summarization 
performance (Lin, 2003). The summarization 
ratio, defined as the ratio of the number of words 
in the automatic (or manual) summary to that in 
the reference transcript of a spoken document, 
was set to 10% in this research. 
6 Experimental Results 
In the first part of experiments, we evaluate the 
effectiveness of the various query models applied 
to the speech recognition task. The correspond-
ing results with respect to different numbers of 
top-ranked documents being used for estimating 
their component models are shown in Table 1. 
Also worth mentioning is that the baseline sys-
tem with the background trigram language model, 
which was trained with the SRILM toolkit 
(Stolcke, 2005) and Good-Turing smoothing 
(Jelinek, 1999), results in a Chinese character 
error rate (CER) of 20.08% on the test set. Con-
sulting Table 1 we notice two particularities. One 
is that there is more fluctuation in the CER re-
sults of SMM than in those of RM. The reason 
might be that, for SMM, the extraction of rele-
vance information from the top-ranked docu-
ments is conducted with no involvement of the 
test utterance (i.e., the query; or its correspond-
ing search histories), as elaborated earlier in Sec-
tion 2. When too many feedback documents are 
being used, there would be a concern for SMM 
to be distracted from being able to appropriate 
model the test utterance, which is probably 
caused by some dominant distracting (or irrele-
vant) feedback documents. The other interesting 
observation is that RSMM only achieves a com-
parable (even worse) result when compared to 
SMM. A possible reason is that the prior con-
straint of the RSMM may contain too much 
noisy information so as to bias the model estima-
tion. Furthermore, it is evident that the proposed 
QMM is the best-performing method among all 
the query models compared in the paper. Alt-
hough the improvements made by QMM are not 
as pronounced as expected, we believe that 
QMM has demonstrated its potential to be ap-
plied to other related applications. On the other 
hand, we compare the various query models with 
two well-practiced language models, namely the 
cache model (Cache) (Kuhn and Mori, 1990; 
Jelinek et al., 1991) and the latent Dirichlet allo-
cation (LDA) (Liu and Liu, 2007; Tam and 
Schultz, 2005). The CER results of these two 
models are also shown in Table 1, respectively. 
For the cache model, bigram cache was used 
since it can yield better results than the unigram 
and trigram cache models in our experiments. It 
is worthy to notice that the LDA model was 
trained with the entire set of contemporaneous 
text document collection (c.f. Section 4), while 
all of the query models explored in the paper 
were estimated based on a subset of the corpus 
selected by an initial round of retrieval. The re-
sults reveal that most of these query models can 
achieve superior performance over the two con-
ventional language models. 
1477
In the second part of experiments, we evaluate 
the utilities of the various query models as ap-
plied to the speech summarization task. At the 
outset, we assess the performance level of the 
baseline KLM method by comparison with two 
well-practiced unsupervised methods, viz. the 
vector space model (VSM) (Gong and Liu, 2001), 
and its extension, maximal marginal relevance 
(MMR) (Carbonell and Goldstein, 1998). The 
corresponding results are shown in Table 2 and 
can be aligned with several related literature re-
views. By looking at the results, we find that 
KLM outperforms VSM by a large margin, con-
firming the applicability of the language model-
ing framework for speech summarization. Fur-
thermore, MMR that presents an extension of 
VSM performs on par with KLM for the text 
summarization task (TD) and exhibits superior 
performance over KLM for the speech summari-
zation task (SD). We now turn to evaluate the 
effectiveness of the various query models (viz. 
RM, SMM, RSMM and QMM) in conjunction 
with the pseudo-relevance feedback process for 
enhancing the sentence model involved in the 
KLM method. The corresponding results are also 
shown in Table 2. Two noteworthy observations 
can be drawn from Table 2. One is that all these 
query models can considerably improve the 
summarization performance of the KLM method, 
which corroborates the advantage of using them 
for enhanced sentence representations. The other 
is that QMM is the best-performing one among 
all the formulations studied in this paper for both 
the TD and SD cases.  
Going one step further, we explore to use extra 
prosodic features that are deemed complemen-
tary to the LM cue provided by QMM for speech 
summarization. To this end, a support vector ma-
chine (SVM) based summarization model is 
trained to integrate a set of 28 commonly-used 
prosodic features (Liu and Hakkani-Tur, 2011) 
for representing each spoken sentence, since 
SVM is arguably one of the state-of-the-art su-
pervised methods that can make use of a diversi-
ty of indicative features for text or speech sum-
marization (Xie and Liu, 2010; Chen et al., 
2013). The sentence ranking scores derived by 
QMM and SVM are in turn integrated through a 
simple log-linear combination. The correspond-
ing results are shown in Table 2, demonstrating 
consistent improvements with respect to all the 
three variants of the ROUGE metric as compared 
to that using either QMM or SVM in isolation. 
We also investigate using SVM to additionally 
integrate a richer set of lexical and relevance fea-
tures to complement QMM and further enhance 
the summarization effectiveness. However, due 
to space limitation, we omit the details here. As a 
side note, there is a sizable gap between the TD 
and SD cases, indicating room for further im-
provements. We may seek remedies, such as ro-
bust indexing schemes, to compensate for imper-
fect speech recognition. 
7 Conclusion and Outlook 
In this paper, we have presented a systematic and 
thorough analysis of a few well-practiced query 
models for IR and extended their novel applica-
bility to speech recognition and summarization in 
a principled way. Furthermore, we have pro-
posed an extension of this research line by intro-
ducing query-specific mixture modeling; the util-
ities of the deduced model have been extensively 
compared with several existing query models. As 
to future work, we would like to investigate 
jointly integrating proximity and other different 
kinds of relevance and lexical/semantic infor-
mation cues into the process of feedback docu-
ment selection so as to improve the empirical 
effectiveness of such query modeling.  
Acknowledgements 
This research is supported in part by the ?Aim 
for the Top University Project? of National Tai-
wan Normal University (NTNU), sponsored by 
the Ministry of Education, Taiwan, and by the 
Ministry of Science and Technology, Taiwan, 
under Grants MOST 103-2221-E-003-016-MY2, 
NSC 101-2221-E-003-024-MY3, NSC 102-
2221-E-003-014-, NSC 101-2511-S-003-057-
MY3, NSC 101-2511-S-003-047-MY3 and NSC 
103-2911-I-003-301. 
  
Table 1. The speech recognition results (in CER 
(%)) achieved by various language models along 
with different numbers of latent topics/pseudo-
relevance feedback documents. 
 16 32 64 128 
Baseline 20.08 
Cache 19.86 
LDA 19.29 19.30 19.28 19.15 
RM 19.26 19.26 19.26 19.26 
SMM 19.19 19.00 19.14 19.10 
RSMM 19.18 19.14 19.15 19.19 
QMM 19.05 18.97 19.00 18.99 
Table 2. The summarization results (in F-scores) 
achieved by various language models along with 
text and spoken documents. 
 
Text Documents (TD) Spoken Documents (SD) 
ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L 
VSM 0.347 0.228 0.290 0.342 0.189 0.287 
MMR 0.407 0.294 0.358 0.381 0.226 0.331 
KLM 0.411 0.298 0.361 0.364 0.210 0.307 
RM 0.453 0.335 0.403 0.382 0.239 0.331 
SMM 0.439 0.320 0.388 0.383 0.229 0.327 
RSMM 0.472 0.365 0.423 0.381 0.235 0.329 
QMM 0.486 0.382 0.435 0.395 0.256 0.349 
SVM 0.441 0.334 0.396 0.370 0.222 0.326 
QMM+
SVM 
0.492 0.395 0.448 0.398 0.261 0.358 
 
 
 
 
1478
References 
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 
2011. Modern information retrieval: the con-
cepts and technology behind search, ACM 
Press. 
David M. Blei, Andrew Y. Ng, and Michael I. 
Jordan. 2003. Latent dirichlet allocation. 
Journal of Machine Learning Research, 
pp.993?1022. 
David M. Blei and John Lafferty. 2009. Topic 
models. In A. Srivastava and M. Sahami, 
(eds.), Text Mining: Theory and Applications. 
Taylor and Francis.  
Jaime Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversitybased reranking for 
reordering documents and producing sum-
maries. In Proc. SIGIR, pp. 335?336. 
Claudio Carpineto and Giovanni Romano. 2012. 
A survey of automatic query expansion in in-
formation retrieval. ACM Computing Surveys, 
vol. 44, pp.1?56. 
Stephane Clinchant and Eric Gaussier. 2013. A 
theoretical analysis of pseudo-relevance 
feedback models. In Proc. ICTIR. 
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and 
Stephen Robertson. 2008. Selecting good 
expansion terms for pseudo-relevance feed-
back. In Proc. SIGIR, pp. 243?250. 
Berlin Chen, Shih-Hsiang Lin, Yu-Mei Chang, 
and Jia-Wen Liu. 2013. Extractive speech 
summarization using evaluation metric-
related training criteria. Information Pro-
cessing & Management, 49(1), pp. 1cess 
Arthur P. Dempster, Nan M. Laird, and Donald 
B. Rubin. 1977. Maximum likelihood from 
incomplete data via the EM algorithm. Jour-
nal of Royal Statistical Society B, 39(1), pp. 
1?38. 
Joshua V. Dillon and Kevyn Collins-Thompson. 
2010. A unified optimization framework for 
robust pseudo-relevance feedback algorithms. 
In Proc. CIKM, pp. 1069?1078. 
Sadaoki Furui, Li Deng, Mark Gales, Hermann 
Ney, and Keiichi Tokuda. 2012. Fundamen-
tal technologies in modern speech recogni-
tion. IEEE Signal Processing Magazine, 
29(6), pp. 16?17. 
Yihong Gong and Xin Liu. 2001. Generic text 
summarization using relevance measure and 
latent semantic analysis. In Proc. SIGIR, pp. 
19?25. 
Djoerd Hiemstra, Stephen Robertson, and Hugo 
Zaragoza. 2004. Parsimonious language 
models for information retrieval. In Proc. 
SIGIR, pp. 178?185. 
Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. In Proc. SIGIR, pp. 50?57.  
Thomas Hofmann. 2001. Unsupervised learning 
by probabilistic latent semantic analysis. 
Machine Learning, 42, pp. 177?196.  
Xuedong Huang, Alex Acero, and Hsiao-Wuen 
Hon. 2001. Spoken language processing: a 
guide to theory, algorithm, and system de-
velopment. Prentice Hall PTR, Upper Saddle 
River, NJ, USA. 
Frederick Jelinek, Bernard Merialdo, Salim Rou-
kos, and M. Strauss. 1991. A dynamic lan-
guage model for speech recognition. In Proc. 
the DARPA workshop on speech and natural 
language, pp. 293?295. 
Frederick Jelinek. 1999. Statistical methods for 
speech recognition. MIT Press. 
Daniel Jurafsky and James H. Martin. 2008. 
Speech and language processing. Prentice 
Hall PTR, Upper Saddle River, NJ, USA. 
Roland Kuhn and Renato D. Mori. 1990. A 
cache-based natural language model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
12(6), pp. 570?583. 
Solomon Kullback and Richard A. Leibler. 1951. 
On information and sufficiency. The Annals 
of Mathematical Statistics, 22(1), pp. 79?86. 
Chin-Yew Lin. 2003. ROUGE: Recall-oriented 
Understudy for Gisting Evaluation. Availa-
ble: http://haydn.isi.edu/ROUGE/. 
Feifan Liu and Yang Liu. 2007. Unsupervised 
language model adaptation incorporating 
named entity information. In Proc. ACL, pp. 
672?769. 
Yang Liu and Dilek Hakkani-Tur. 2011. Speech 
summarization. Chapter 13 in Spoken Lan-
guage Understanding: Systems for Extract-
ing Semantic Information from Speech, G. 
Tur and R. D. Mori (Eds), New York: Wiley. 
John Lafferty and Chengxiang Zhai. 2001. Doc-
ument language models, query models, and 
risk minimization for information retrieval. 
In Proc. SIGIR, pp. 111?119. 
Victor Lavrenko and W. Bruce Croft. 2001. Rel-
evance-based language models. In Proc. 
SIGIR, pp. 120?127. 
Victor Lavrenko. 2004. A Generative Theory of 
Relevance. PhD thesis, University of Massa-
chusetts, Amherst. 
1479
Shasha Xie and Yang Liu. 2010. Improving su-
pervised learning for meeting summarization 
using sampling and regression. Computer 
Speech & Language, 24(3), pp. 495?514. 
Yuanhua Lv and Chengxiang Zhai. 2009. A 
comparative study of methods for estimating 
query language models with pseudo feed-
back. In Proc. CIKM, pp. 1895?1898. 
Yuanhua Lv and Chengxiang Zhai. 2010. Posi-
tional relevance model for pseudo-relevance 
feedback. In Proc. SIGIR, pp. 579?586. 
Kyung Soon Lee, W. Bruce Croft, and James 
Allan. 2008. A cluster-based resampling 
method for pseudo-relevance feedback. In 
Proc. SIGIR, pp. 235?242. 
Kyung Soon Lee and W. Bruce Croft. 2013. A 
deterministic resampling method using over-
lapping document clusters for pseudo-
relevance feedback. Inf. Process. Manage. 
49(4), pp. 792?806. 
Inderjeet Mani and Mark T. Maybury (Eds.). 
1999. Advances in automatic text summari-
zation. Cambridge, MA: MIT Press. 
Ani Nenkova and Kathleen McKeown. 2011. 
Automatic summarization. Foundations and 
Trends in Information Retrieval, 5(2?3), pp. 
103?233. 
Stefan Ortmanns, Hermann Ney, and Xavier Au-
bert. 1997. A word graph algorithm for large 
vocabulary continuous speech recognition. 
Computer Speech and Language, pp. 43?72. 
Jay M. Ponte and W. Bruce Croft. 1998. A lan-
guage modeling approach to information re-
trieval. In Proc. SIGIR, pp. 275?281. 
Stephen E. Robertson. 1990. On term selection 
for query expansion. Journal of Documenta-
tion, 46(4), pp. 359?364. 
Andreas Stolcke. 2005. SRILM - An extensible 
language modeling toolkit. In Proc. INTER-
SPEECH, pp.901?904. 
Tao Tao and Chengxiang Zhai. 2006. Regular-
ized estimation of mixture models for robust 
pseudo-relevance feedback. In Proc. SIGIR, 
pp. 162?169. 
Yik-Cheung Tam and Tanja Schultz. 2005. Dy-
namic language model adaptation using vari-
ational Bayes inference. In Proc. INTER-
SPEECH, pp. 5?8. 
Xuanhui Wang, Hui Fang, and Chengxiang Zhai. 
2008. A study of methods for negative rele-
vance feedback. In Proc. SIGIR, pp. 219?226. 
Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo, and 
Shih-Sian Cheng. 2005. MATBN: A Manda-
rin Chinese broadcast news corpus. Interna-
tional Journal of Computational Linguistics 
& Chinese Language Processing, 10(2), pp. 
219?236. 
Xing Yi and James Allan. 2009. A comparative 
study of utilizing topic models for infor-
mation retrieval. In Proc. ECIR, pp. 29?41. 
Steve Young, Dan Kershaw, Julian Odell, Dave 
Ollason, Valtcho Valtchev, and Phil Wood-
land. 2006. The HTK book version 3.4. 
Cambridge University Press. 
Chengxiang Zhai and John Lafferty. 2001a. A 
study of smoothing methods for language 
models applied to ad hoc information re-
trieval. In Proc. SIGIR, pp. 334?342.  
Chengxiang Zhai and John Lafferty. 2001b. 
Model-based feedback in the language mod-
eling approach to information retrieval. In 
Proc. CIKM, pp. 403?410. 
Chengxiang Zhai. 2008. Statistical language 
models for information retrieval: a critical 
review. Foundations and Trends in Infor-
mation Retrieval, 2 (3), pp. 137?213. 
Yi Zhang, Jamie Callan, and Thomas Minka. 
2002. Novelty and redundancy detection in 
adaptive filtering. In Proc. SIGIR, pp. 81?88.  
1480
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 79?87,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Risk Minimization Framework for Extractive  
Speech Summarization 
 
 
Shih-Hsiang Lin and Berlin Chen 
National Taiwan Normal University 
Taipei, Taiwan 
{shlin, berlin}@csie.ntnu.edu.tw 
  
Abstract 
In this paper, we formulate extractive 
summarization as a risk minimization 
problem and propose a unified probabilis-
tic framework that naturally combines su-
pervised and unsupervised summarization 
models to inherit their individual merits as 
well as to overcome their inherent limita-
tions. In addition, the introduction of vari-
ous loss functions also provides the sum-
marization framework with a flexible but 
systematic way to render the redundancy 
and coherence relationships among sen-
tences and between sentences and the 
whole document, respectively. Experi-
ments on speech summarization show that 
the methods deduced from our framework 
are very competitive with existing summa-
rization approaches. 
1 Introduction 
Automated summarization systems which enable 
user to quickly digest the important information 
conveyed by either a single or a cluster of docu-
ments are indispensible for managing the rapidly 
growing amount of textual information and mul-
timedia content (Mani and Maybury, 1999). On 
the other hand, due to the maturity of text sum-
marization, the research paradigm has been ex-
tended to speech summarization over the years 
(Furui et al, 2004; McKeown et al, 2005). 
Speech summarization is expected to distill im-
portant information and remove redundant and 
incorrect information caused by recognition er-
rors from spoken documents, enabling user to 
efficiently review spoken documents and under-
stand the associated topics quickly. It would also 
be useful for improving the efficiency of a num-
ber of potential applications like retrieval and 
mining of large volumes of spoken documents. 
A summary can be either abstractive or extrac-
tive. In abstractive summarization, a fluent and 
concise abstract that reflects the key concepts of 
a document is generated, whereas in extractive 
summarization, the summary is usually formed 
by selecting salient sentences from the original 
document (Mani and Maybury, 1999). The for-
mer requires highly sophisticated natural lan-
guage processing techniques, including semantic 
representation and inference, as well as natural 
language generation, while this would make ab-
stractive approaches difficult to replicate or ex-
tend from constrained domains to more general 
domains.  In addition to being extractive or ab-
stractive, a summary may also be generated by 
considering several other aspects like being ge-
neric or query-oriented summarization, single-
document or multi-document summarization, and 
so forth. The readers may refer to (Mani and 
Maybury, 1999) for a comprehensive overview 
of automatic text summarization. In this paper, 
we focus exclusively on generic, single-
document extractive summarization which forms 
the building block for many other summarization 
tasks.  
Aside from traditional ad-hoc extractive sum-
marization methods (Mani and Maybury, 1999), 
machine-learning approaches with either super-
vised or unsupervised learning strategies have 
gained much attention and been applied with 
empirical success to many summarization tasks 
(Kupiec et al, 1999; Lin et al, 2009). For super-
vised learning strategies, the summarization task 
is usually cast as a two-class (summary and non-
summary) sentence-classification problem: A 
sentence with a set of indicative features is input 
to the classifier (or summarizer) and a decision is 
then returned from it on the basis of these fea-
tures. In general, they usually require a training 
set, comprised of several documents and their 
corresponding handcrafted summaries (or labeled 
data), to train the classifiers. However, manual 
labeling is expensive in terms of time and per-
sonnel. The other potential problem is the so-
called ?bag-of-sentences? assumption implicitly 
made by most of these summarizers. That is, sen-
tences are classified independently of each other, 
79
without leveraging the dependence relationships 
among the sentences or the global structure of 
the document (Shen et al, 2007). 
Another line of thought attempts to conduct 
document summarization using unsupervised 
machine-learning approaches, getting around the 
need for manually labeled training data. Most 
previous studies conducted along this line have 
their roots in the concept of sentence centrality 
(Gong and Liu, 2001; Erkan and Radev, 2004; 
Radev et al, 2004; Mihalcea and Tarau, 2005). 
Put simply, sentences more similar to others are 
deemed more salient to the main theme of the 
document; such sentences thus will be selected 
as part of the summary. Even though the perfor-
mance of unsupervised summarizers is usually 
worse than that of supervised summarizers, their 
domain-independent and easy-to-implement 
properties still make them attractive. 
Building on these observations, we expect that 
researches conducted along the above-mentioned 
two directions could complement each other, and 
it might be possible to inherit their individual 
merits to overcome their inherent limitations. In 
this paper, we present a probabilistic summariza-
tion framework stemming from Bayes decision 
theory (Berger, 1985) for speech summarization. 
This framework can not only naturally integrate 
the above-mentioned two modeling paradigms 
but also provide a flexible yet systematic way to 
render the redundancy and coherence relation-
ships among sentences and between sentences 
and the whole document, respectively. Moreover, 
we also illustrate how the proposed framework 
can unify several existing summarization models.  
The remainder of this paper is structured as 
follows. We start by reviewing related work on 
extractive summarization. In Section 3 we for-
mulate the extractive summarization task as a 
risk minimization problem, followed by a de-
tailed elucidation of the proposed methods in 
Section 4. Then, the experimental setup and a 
series of experiments and associated discussions 
are presented in Sections 5 and 6, respectively. 
Finally, Section 7 concludes our presentation and 
discusses avenues for future work. 
2 Background 
Speech summarization can be conducted using 
either supervised or unsupervised methods (Furui 
et al, 2004, McKeown et al, 2005, Lin et al, 
2008). In the following, we briefly review a few 
celebrated methods that have been applied to 
extractive speech summarization tasks with good 
success. 
2.1 Supervised summarizers 
Extractive speech summarization can be treated 
as a two-class (positive/negative) classification 
problem. A spoken sentence iS  is characterized by set of T  indicative features ? ?iTii xxX ,,1 ?? , and they may include lexical features (Koumpis 
and Renals, 2000), structural features (Maskey 
and Hirschberg, 2003), acoustic features (Inoue 
et al, 2004), discourse features (Zhang et al, 
2007) and relevance features (Lin et al, 2009). 
Then, the corresponding feature vector iX  of iS  is taken as the input to the classifier. If the output 
(classification) score belongs to the positive class, 
iS  will be selected as part of the summary; oth-erwise, it will be excluded (Kupiec et al, 1999). 
Specifically, the problem can be formulated as 
follows: Construct a sentence ranking model that 
assigns a classification score (or a posterior 
probability) of being in the summary class to 
each sentence of a spoken document to be sum-
marized; important sentences are subsequently 
ranked and selected according to these scores. To 
this end, several popular machine-learning me-
thods could be utilized, like Bayesian classifier 
(BC) (Kupiec et al, 1999),  Gaussian mixture 
model (GMM) (Fattah and Ren, 2009) , hidden 
Markov model (HMM) (Conroy and O'leary, 
2001), support vector machine (SVM) (Kolcz et 
al., 2001), maximum entropy (ME) (Ferrier, 
2001), conditional random field (CRF) (Galley, 
2006; Shen et al, 2007), to name a few.  
Although such supervised summarizers are ef-
fective, most of them (except CRF) usually im-
plicitly assume that sentences are independent of 
each other (the so-called ?bag-of-sentences? as-
sumption) and classify each sentence individual-
ly without leveraging the relationship among the 
sentences (Shen et al, 2007). Another major 
shortcoming of these summarizers is that a set of 
handcrafted document-reference summary ex-
emplars are required for training the summarizers; 
however, such summarizers tend to limit their 
generalization capability and might not be readi-
ly applicable for new tasks or domains. 
2.2 Unsupervised summarizers 
The related work conducted along this direction 
usually relies on some heuristic rules or statistic-
al evidences between each sentence and the doc-
ument, avoiding the need of manually labeled 
training data. For example, the vector space 
model (VSM) approach represents each sentence 
of a document and the document itself in vector 
space (Gong and Liu, 2001), and computes the 
relevance score between each sentence and the 
document (e.g., the cosine measure of the simi-
80
larity between two vectors). Then, the sentences 
with the highest relevance scores are included in 
the summary. A natural extension is to represent 
each document or each sentence vector in a latent 
semantic space (Gong and Liu, 2001), instead of 
simply using the literal term information as that 
done by VSM. 
On the other hand, the graph-based methods, 
such as TextRank (Mihalcea and Tarau, 2005) 
and LexRank (Erkan and Radev, 2004), concep-
tualize the document to be summarized as a net-
work of sentences, where each node represents a 
sentence and the associated weight of each link 
represents the lexical or topical similarity rela-
tionship between a pair of nodes. Document 
summarization thus relies on the global structural 
information conveyed by such conceptualized 
network, rather than merely considering the local 
features of each node (sentence).  
However, due to the lack of document-
summary reference pairs, the performance of the 
unsupervised summarizers is usually worse than 
that of the supervised summarizers. Moreover, 
most of the unsupervised summarizers are con-
structed solely on the basis of the lexical infor-
mation without considering other sources of in-
formation cues like discourse features, acoustic 
features, and so forth. 
3 A risk minimization framework for 
extractive summarization 
Extractive summarization can be viewed as a 
decision making process in which the summariz-
er attempts to select a representative subset of 
sentences or paragraphs from the original docu-
ments. Among the several analytical methods 
that can be employed for the decision process, 
the Bayes decision theory, which quantifies the 
tradeoff between various decisions and the po-
tential cost that accompanies each decision, is 
perhaps the most suited one that can be used to 
guide the summarizer in choosing a course of 
action in the face of some uncertainties underly-
ing the decision process (Berger, 1985). Stated 
formally, a decision problem may consist of four 
basic elements: 1) an observation O  from a ran-
dom variable O , 2) a set of possible decisions 
(or actions) ??a , 3) the state of nature ??? , 
and 4) a loss function ? ??,iaL  which specifies the cost associated with a chosen decision ia  given that ?  is the true state of nature. The expected 
risk (or conditional risk) associated with taking 
decision ia  is given by  
? ? ? ? ? ? ,| ?d?|Op,?aLOaR ? ii ??            (1) 
where ? ??|Op  is the posterior probability of the 
state of nature being ?  given the observation O . 
Bayes decision theory states that the optimum 
decision can be made by contemplating each ac-
tion ia , and then choosing the action for which the expected risk is minimum: 
? ?.|minarg* OaRa iai?             (2) 
The notion of minimizing the Bayes risk has 
gained much attention and been applied with 
success to many natural language processing 
(NLP) tasks, such as automatic speech recogni-
tion (Goel and Byrne, 2000), statistical machine 
translation (Kumar and Byrne, 2004) and statis-
tical information retrieval (Zhai and Lafferty, 
2006). Following the same spirit, we formulate 
the extractive summarization task as a Bayes risk 
minimization problem. Without loss of generality, 
let us denote ???  as one of possible selection 
strategies (or state of nature) which comprises a 
set of indicators used to address the importance 
of each sentence iS  in a document D  to be summarized. A feasible selection strategy can be 
fairly arbitrary according to the underlying prin-
ciple. For example, it could be a set of binary 
indicators denoting whether a sentence should be 
selected as part of summary or not. On the con-
trary, it may also be a ranked list used to address 
the significance of each individual sentence. 
Moreover, we refer to the k -th action ka  as choosing the k -th selection strategy k? , and the observation O  as the document D  to be summa-
rized. As a result, the expected risk of a certain 
selection strategy k?  is given by 
? ? ? ? ? ? .|,| ????? ? dDpLDR kk ??          (3) 
Consequently, the ultimate goal of extractive 
summarization could be stated as the search of 
the best selection strategy from the space of all 
possible selection strategies that minimizes the 
expected risk defined as follows: 
? ?
? ? ? ? .|,minarg      
|minarg*
????
??
??
?
dDpL
DR
k
k
k
k
??
?
           (4) 
Although we have described a general formu-
lation for the extractive summarization problem 
on the grounds of the Bayes decision theory, we 
consider hereafter a special case of it where the 
selection strategy is represented by a binary deci-
sion vector, of which each element corresponds 
to a specific sentence iS  in the document D  and designates whether it should be selected as part 
of the summary or not, as the first such attempt. 
More concretely, we assume that the summary 
81
sentences of a given document can be iteratively 
chosen (i.e., one at each iteration) from the doc-
ument until the aggregated summary reaches a 
predefined target summarization ratio. It turns 
out that the binary vector for each possible action 
will have just one element equal to 1 and all oth-
ers equal to zero (or the so-called ?one-of-n? 
coding). For ease of notation, we denote the bi-
nary vector by iS  when the i -th element has a value of 1. Therefore, the risk minimization 
framework can be reduced to 
? ?
? ? ? ?,~|,minarg      
~|minarg
~~
~
*
?
??
?
?
?
DS
jjiDS
iDS
ji
i
DSPSSL
DSRS
           (5) 
where D~  denotes the remaining sentences that 
have not been selected into the summary yet (i.e., 
the ?residual? document); ? ?DSP j ~|  is the post-erior probability of a sentence jS  given D~ . Ac-cording to the Bayes? rule, we can further ex-
press ? ?DSP j ~|  as (Chen et al, 2009) 
? ? ? ? ? ?? ? ,~|
~~| DP
SPSDPDSP jjj ?               (6) 
where ? ?jSDP |~  is the sentence generative prob-ability, i.e., the likelihood of D~  being generated 
by jS ; ? ?jSP  is the prior probability of jS  being important; and the evidence ? ?DP ~  is the marginal 
probability of D~ , which can be approximated by  
? ? ? ? ? ?.|~~ ~?? ?DS mmm SPSDPDP               (7) 
By substituting (6) and (7) into (5), we obtain 
the following final selection strategy for extrac-
tive summarization: 
? ? ? ? ? ?? ? ? ?.|~|
~
,minarg ~
~
~
* ? ??
?
?
?
DS
DS
mm
jj
jiDS j
m
i SPSDP
SPSDPSSLS  (8) 
A remarkable feature of this framework lies in 
that a sentence to be considered as part of the 
summary is actually evaluated by three different 
fundamental factors: (1) ? ?jSP  is the sentence prior probability that addresses the importance of 
sentence jS  itself; (2) ? ?jSDP |~  is the sentence generative probability that captures the degree of 
relevance of jS to the residual document D~ ; and (3) ? ?ji SSL ,  is the loss function that characteriz-es the relationship between sentence iS  and any other sentence jS . As we will soon see, such a framework can be regarded as a generalization of 
several existing summarization methods. A de-
tailed account on the construction of these three 
component models in the framework will be giv-
en in the following section. 
4 Proposed Methods 
There are many ways to construct the above 
mentioned three component models, i.e., the sen-
tence generative model ? ?jSDP |~ , the sentence prior model ? ?jSP , and the loss function ? ?ji SSL , . In what follows, we will shed light on one possi-
ble attempt that can accomplish this goal elegant-
ly. 
4.1 Sentence generative model 
In order to estimate the sentence generative 
probability, we explore the language modeling 
(LM) approach, which has been introduced to a 
wide spectrum of IR tasks and demonstrated with 
good empirical success, to predict the sentence 
generative probability. In the LM approach, each 
sentence in a document can be simply regarded 
as a probabilistic generative model consisting of 
a unigram distribution (the so-called ?bag-of-
words? assumption) for generating the document 
(Chen et al, 2009):   
? ? ? ? ? ?, ~ ~,~ DwcDw jj SwPSDP ?? ?           (9) 
where ? ?Dwc ~,  is the number of times that index 
term (or word) w  occurs in D~ , reflecting that w  
will contribute more in the calculation of ? ? ~ jSDP  if it occurs more frequently in D~ . Note that the sentence model ? ?jSwP  is simply esti-mated on the basis of the frequency of index 
term w  occurring in the sentence jS  with the maximum likelihood (ML) criterion. In a sense, 
(9) belongs to a kind of literal term matching 
strategy (Chen, 2009) and may suffer the prob-
lem of unreliable model estimation owing partic-
ularly to only a few sampled index terms present 
in the sentence (Zhai, 2008). To mitigate this 
potential defect, a unigram probability estimated 
from a general collection, which models the gen-
eral distribution of words in the target language, 
is often used to smooth the sentence model. In-
terested readers may refer to (Zhai, 2008; Chen 
et al, 2009) for a thorough discussion on various 
ways to construct the sentence generative model. 
4.2 Sentence prior model 
The sentence prior probability ? ?jSP  can be re-garded as the likelihood of a sentence being im-
portant without seeing the whole document. It 
could be assumed uniformly distributed over sen-
tences or estimated from a wide variety of factors, 
such as the lexical information, the structural 
information or the inherent prosodic properties of 
a spoken sentence. 
A straightforward way is to assume that the 
sentence prior probability ? ?jSP  is in proportion to the posterior probability of a sentence jS  be-
82
ing included in the summary class when observ-
ing a set of indicative features jX  of jS  derived from such factors or other sentence importance 
measures (Kupiec et al, 1999). These features 
can be integrated in a systematic way into the 
proposed framework by taking the advantage of 
the learning capability of the supervised ma-
chine-learning methods. Specifically, the prior 
probability ? ?jSP  can be approximated by: 
? ? ? ? ? ?? ? ? ? ? ? ? ? ,|| | SSSS SS PXPPXP PXpSP jj jj ??          (10) 
where ? ?S|jXP  and ? ?S|jXP  are the likelihoods that a sentence jS  with features jX  are generat-ed by the summary class S  and the non-
summary class S , respectively; the prior proba-
bility ? ?SP  and ? ?SP  are set to be equal in this 
research. To estimate ? ?S|jXP  and ? ?S|jXP , several popular supervised classifiers (or summa-
rizers), like BC or SVM, can be leveraged for 
this purpose. 
4.3 Loss function 
The loss function introduced in the proposed 
summarization framework is to measure the rela-
tionship between any pair of sentences. Intuitive-
ly, when a given sentence is more dissimilar 
from most of the other sentences, it may incur 
higher loss as it is taken as the representative 
sentence (or summary sentence) to represent the 
main theme embedded in the other ones. Conse-
quently, the loss function can be built on the no-
tion of the similarity measure. In this research, 
we adopt the cosine measure (Gong and Liu, 
2001) to fulfill this goal. We first represent each 
sentence iS  in vector form where each dimension specifies the weighted statistic itz , , e.g., the product of the term frequency (TF) and inverse 
document frequency (IDF) scores, associated 
with an index term tw  in sentence iS . Then, the cosine similarity between any given two sen-
tences ? ?ji SS ,  is 
? ? .,
1
2,1
2,
1 ,,
???
? ??
??
?
Tt jtTt it
Tt jtitji zz
zzSSSim          (10) 
The loss function is thus defined by 
? ? ? ?.,1, jiji SSSimSSL ??                 (11) 
Once the sentence generative model ? ?jSDP |~ , the sentence prior model ? ?jSP  and the loss func-tion ? ?ji SSL ,  have been properly estimated, the summary sentences can be selected iteratively by 
(8) according to a predefined target summariza-
tion ratio. However, as can be seen from (8), a 
new summary sentence is selected without con-
sidering the redundant information that is also 
contained in the already selected summary sen-
tences. To alleviate this problem, the concept of 
maximum marginal relevance (MMR) (Carbonell 
and Goldstein, 1998), which performs sentence 
selection iteratively by striking the balance be-
tween topic relevance and coverage, can be in-
corporated into the loss function:  
? ? ? ?? ? ? ? ,',max1
,
1,
' ?
?
?
?
???
?
???
?
??
?
SSSim
SSSim
SSL
iS
ji
ji
Summ
?
?   (12) 
where Summ  represents the set of sentences that 
have already been included into the summary 
and the novelty factor ?  is used to trade off be-
tween relevance and redundancy. 
4.4 Relation to other summarization models 
In this subsection, we briefly illustrate the rela-
tionship between our proposed summarization 
framework and a few existing summarization 
approaches. We start by considering a special 
case where a 0-1 loss function is used in (8), 
namely, the loss function will take value 0 if the 
two sentences are identical, and 1 otherwise. 
Then, (8) can be alternatively represented by 
? ? ? ?? ? ? ?
? ? ? ?? ? ? ? ,|~|
~
maxarg     
|~
|~minarg
~
~
,~ ~
~
*
??
? ??
?
?
??
?
?
DS mm
ii
DS
SSDS
DS mm
jj
DS
m
i
ijj
m
i
SPSDP
SPSDP
SPSDP
SPSDPS
   (13) 
which actually provides a natural integration of 
the supervised and unsupervised summarizers 
(Lin et al, 2009), as mentioned previously.  
If we further assume the prior probability ? ?jSP  is uniformly distributed, the important (or summary) sentence selection problem has now 
been reduced to the problem of measuring the 
document-likelihood ? ?jSDP |~ , or the relevance between the document and the sentence. Alone a 
similar vein, the important sentences of a docu-
ment can be selected (or ranked) solely based on 
the prior probability ? ?jSP  with the assumption of an equal document-likelihood ? ?jSDP |~ . 
5 Experimental setup 
5.1 Data 
The summarization dataset used in this research 
is a widely used broadcast news corpus collected 
by the Academia Sinica and the Public Televi-
sion Service Foundation of Taiwan between No-
vember 2001 and April 2003 (Wang et al, 2005). 
Each story contains the speech of one studio 
anchor, as well as several field reporters and in-
terviewees. A subset of 205 broadcast news doc-
83
uments compiled between November 2001 and 
August 2002 was reserved for the summarization 
experiments. 
Three subjects were asked to create summaries 
of the 205 spoken documents for the summariza-
tion experiments as references (the gold standard) 
for evaluation. The summaries were generated by 
ranking the sentences in the reference transcript 
of a spoken document by importance without 
assigning a score to each sentence. The average 
Chinese character error rate (CER) obtained for 
the 205 spoken documents was about 35%. 
Since broadcast news stories often follow a 
relatively regular structure as compared to other 
speech materials like conversations, the position-
al information would play an important (domi-
nant) role in extractive summarization of broad-
cast news stories; we, hence, chose 20 docu-
ments for which the generation of reference 
summaries is less correlated with the positional 
information (or the position of sentences) as the 
held-out test set to evaluate the general perfor-
mance of the proposed summarization frame-
work, and 100 documents as the development set.  
5.2 Performance evaluation 
For the assessment of summarization perfor-
mance, we adopted the widely used ROUGE 
measure (Lin, 2004) because of its higher corre-
lation with human judgments. It evaluates the 
quality of the summarization by counting the 
number of overlapping units, such as N-grams, 
longest common subsequences or skip-bigram, 
between the automatic summary and a set of ref-
erence summaries. Three variants of the ROGUE 
measure were used to quantify the utility of the 
proposed method. They are, respectively, the 
ROUGE-1 (unigram) measure, the ROUGE-2 
(bigram) measure and the ROUGE-L (longest 
common subsequence) measure (Lin, 2004). 
   The summarization ratio, defined as the ratio of 
the number of words in the automatic (or manual) 
summary to that in the reference transcript of a 
spoken document, was set to 10% in this re-
search. Since increasing the summary length 
tends to increase the chance of getting higher 
scores in the recall rate of the various ROUGE 
measures and might not always select the right 
number of informative words in the automatic 
summary as compared to the reference summary, 
all the experimental results reported hereafter are 
obtained by calculating the F-scores of these 
ROUGE measures, respectively (Lin, 2004). Ta-
ble 1 shows the levels of agreement (the Kappa 
statistic and ROUGE measures) between the 
three subjects for important sentence ranking. 
They seem to reflect the fact that people may not 
always agree with each other in selecting the im-
portant sentences for representing a given docu-
ment. 
5.3 Features for supervised summarizers 
We take BC as the representative supervised 
summarizer to study in this paper. The input to 
BC consists of a set of 28 indicative features 
used to characterize a spoken sentence, including 
the structural features, the lexical features, the 
acoustic features and the relevance feature. For 
each kind of acoustic features, the minimum, 
maximum, mean, difference value and mean dif-
ference value of a spoken sentence are extracted. 
The difference value is defined as the difference 
between the minimum and maximum values of 
the spoken sentence, while the mean difference 
value is defined as the mean difference between 
a sentence and its previous sentence. Finally, the 
relevance feature (VSM score) is use to measure 
the degree of relevance for a sentence to the 
whole document (Gong and Liu, 2001). These 
features are outlined in Table 2, where each of 
them was further normalized to zero mean and 
unit variance.  
6 Experimental results and discussions  
6.1 Baseline experiments 
In the first set of experiments, we evaluate the 
baseline performance of the LM and BC summa-
rizers (cf. Sections 4.1 and 4.2), respectively. 
The corresponding results are detailed in Table 3, 
Kappa ROGUE-1 ROUGE-2 ROUGE-L
0.400 0.600 0.532 0.527 
Table 1: The agreement among the subjects for impor-
tant sentence ranking for the evaluation set. 
Structural 
features 
1.Duration of the current sentence 
2.Position of the current sentence 
3.Length of the current sentence 
Lexical 
Features 
1.Number of named entities 
2.Number of stop words 
3.Bigram language model scores 
4.Normalized bigram scores 
Acoustic 
Features 
1.The 1st formant 
2.The 2nd formant 
3.The pitch value 
4.The peak normalized cross-
correlation of pitch 
Relevance 
Feature 1.VSM score 
Table 2: Basic sentence features used by BC. 
84
where the values in the parentheses are the asso-
ciated 95% confidence intervals. It is also worth 
mentioning that TD denotes the summarization 
results obtained based on manual transcripts of 
the spoken documents while SD denotes the re-
sults using the speech recognition transcripts 
which may contain speech recognition errors and 
sentence boundary detection errors. In this re-
search, sentence boundaries were determined by 
speech pauses. For the TD case, the acoustic fea-
tures were obtained by aligning the manual tran-
scripts to their spoken documents counterpart by 
performing word-level forced alignment.  
Furthermore, the ROGUE measures, in es-
sence, are evaluated by counting the number of 
overlapping units between the automatic sum-
mary and the reference summary; the corres-
ponding evaluation results, therefore, would be 
severely affected by speech recognition errors 
when applying the various ROUGE measures to 
quantify the performance of speech summariza-
tion. In order to get rid of the cofounding effect 
of this factor, it is assumed that the selected 
summary sentences can also be presented in 
speech form (besides text form) such that users 
can directly listen to the audio segments of the 
summary sentences to bypass the problem caused 
by speech recognition errors. Consequently, we 
can align the ASR transcripts of the summary 
sentences to their respective audio segments to 
obtain the correct (manual) transcripts for the 
summarization performance evaluation (i.e., for 
the SD case).  
Observing Table 3 we notice two particulari-
ties. First, there are significant performance gaps 
between summarization using the manual tran-
scripts and the erroneous speech recognition 
transcripts. The relative performance degrada-
tions are about 15%, 34% and 23%, respectively, 
for ROUGE-1, ROUGE2 and ROUGE-L meas-
ures. One possible explanation is that the errone-
ous speech recognition transcripts of spoken sen-
tences would probably carry wrong information 
and thus deviate somewhat from representing the 
true theme of the spoken document. Second, the 
supervised summarizer (i.e., BC) outperforms the 
unsupervised summarizer (i.e., LM). The better 
performance of BC can be further explained by 
two reasons. One is that BC is trained with the 
handcrafted document-summary sentence labels 
in the development set while LM is instead con-
ducted in a purely unsupervised manner. Another 
is that BC utilizes a rich set of features to charac-
terize a given spoken sentence while LM is con-
structed solely on the basis of the lexical (uni-
gram) information.  
6.2 Experiments on the proposed methods 
We then turn our attention to investigate the utili-
ty of several methods deduced from our pro-
posed summarization framework. We first con-
sider the case when a 0-1 loss function is used (cf. 
(13)), which just show a simple combination of 
BC and LM. As can be seen from the first row of 
Table 4, such a combination can give about 4% 
to 5% absolute improvements as compared to the 
results of BC illustrated in Table 3. It in some 
sense confirms the feasibility of combining the 
supervised and unsupervised summarizers. 
Moreover, we consider the use of the loss func-
tions defined in (11) (denoted by SIM) and (12) 
(denoted by MMR), and the corresponding re-
sults are shown in the second and the third rows 
of Table 4, respectively. It can be found that 
  Text Document (TD) Spoken Document (SD) 
  ROGUE-1 ROUGE-2 ROUGE-L ROGUE-1 ROUGE-2 ROUGE-L 
BC  0.445 (0.390 - 0.504) 0.346 (0.201 - 0.415) 0.404 (0.348 - 0.468) 0.369 (0.316 - 0.426) 0.241 (0.183 - 0.302) 0.321 (0.268 - 0.378) 
LM  0.387  (0.302 - 0.474) 0.264 (0.168 - 0.366) 0.334 (0.251 - 0.415) 0.319 (0.274 - 0.367) 0.164 (0.115 - 0.224) 0.253 (0.215 - 0.301) 
Table 3: The results achieved by the BC and LM summarizers, respectively. 
  Text Document (TD) Spoken Document (SD) 
Prior Loss ROGUE-1 ROUGE-2 ROUGE-L ROGUE-1 ROUGE-2 ROUGE-L 
BC 
0-1 0.501  0.401  0.459  0.417  0.281  0.356  
SIM 0.524  0.425  0.473  0.475  0.351  0.420  
MMR 0.529  0.426  0.479  0.475 0.351 0.420 
Uniform SIM 0.405 0.281 0.348 0.365 0.209 0.305 MMR 0.417 0.282 0.359 0.391 0.236 0.338 
Table 4: The results achieved by several methods derived from the proposed summarization framework. 
85
MMR delivers higher summarization perfor-
mance than SIM (especially for the SD case), 
which in turn verifies the merit of incorporating 
the MMR concept into the proposed framework 
for extractive summarization. If we further com-
pare the results achieved by MMR with those of 
BC and LM as shown in Table 3, we can find 
significant improvements both for the TD and 
SD cases. By and large, for the TD case, the pro-
posed summarization method offers relative per-
formance improvements of about 19%, 23% and 
19%, respectively, in the ROUGE-1, ROUGE-2 
and ROUGE-L measures as compared to the BC 
baseline; while the relative improvements are 
29%, 46% and 31%, respectively, in the same 
measurements for the SD case. On the other hand, 
the performance gap between the TD and SD 
cases are reduced to a good extent by using the 
proposed summarization framework. 
In the next set of experiments, we simply as-
sume the sentence prior probability ? ?jSP  de-fined in (8) is uniformly distributed, namely, we 
do not use any supervised information cue but 
use the lexical information only. The importance 
of a given sentence is thus considered from two 
angles: 1) the relationship between a sentence 
and the whole document, and 2) the relationship 
between the sentence and the other individual 
sentences. The corresponding results are illu-
strated in the lower part of Table 4 (denoted by 
Uniform). We can see that the additional consid-
eration of the sentence-sentence relationship ap-
pears to be beneficial as compared to that only 
considering the document-sentence relevance 
information (cf. the second row of Table 3). It 
also gives competitive results as compared to the 
performance of BC (cf. the first row of Table 3) 
for the SD case. 
6.3 Comparison with conventional summa-
rization methods 
In the final set of experiments, we compare our 
proposed summarization methods with a few 
existing summarization methods that have been 
widely used in various summarization tasks, in-
cluding LEAD, VSM, LexRank and CRF; the 
corresponding results are shown in Table 5. It 
should be noted that the LEAD-based method 
simply extracts the first few sentences in a doc-
ument as the summary. To our surprise, CRF 
does not provide superior results as compared to 
the other summarization methods. One possible 
explanation is that the structural evidence of the 
spoken documents in the test set is not strong 
enough for CRF to show its advantage of model-
ing the local structural information among sen-
tences. On the other hand, LexRank gives a very 
promising performance in spite that it only uti-
lizes lexical information in an unsupervised 
manner. This somewhat reflects the importance 
of capturing the global relationship for the sen-
tences in the spoken document to be summarized. 
As compared to the results shown in the ?BC? 
part of Table 4, we can see that our proposed 
methods significantly outperform all the conven-
tional summarization methods compared in this 
paper, especially for the SD case. 
7 Conclusions and future work 
We have proposed a risk minimization frame-
work for extractive speech summarization, which 
enjoys several advantages. We have also pre-
sented a simple yet effective implementation that 
selects the summary sentences in an iterative 
manner. Experimental results demonstrate that 
the methods deduced from such a framework can 
yield substantial improvements over several 
popular summarization methods compared in this 
paper. We list below some possible future exten-
sions: 1) integrating different selection strategies, 
e.g., the listwise strategy that defines the loss 
function on all the sentences associated with a 
document to be summarized, into this framework, 
2) exploring different modeling approaches for 
this framework, 3) investigating discriminative 
training criteria for training the component mod-
els in this framework, and 4) extending and ap-
plying the proposed framework to multi-
document summarization tasks. 
References  
James O. Berger Statistical decision theory and 
Bayesian analysis. Springer-Verlap, 1985. 
Berlin Chen. 2009. Word topic models for spoken 
document retrieval and transcription. ACM 
Transactions on Asian Language Information 
Processing, 8, (1): 2:1 - 2:27. 
Jaime Carbonell and Jade Goldstein. 1998. The use of 
mmr, diversity-based reranking for reordering 
documents and producing summaries. In Proc. of 
Annual International ACM SIGIR Conference on 
  ROGUE-1 ROUGE-2 ROUGE-L
LEAD TD 0.320 0.197 0.283 SD 0.312 0.168 0.251 
VSM TD 0.345  0.220 0.287 SD 0.337  0.189 0.277 
LexRank TD 0.435  0.314 0.377 SD 0.348   0.204 0.294 
CRF TD 0.431 0.315 0.383 SD 0.358 0.220 0.291 
Table 5: The results achieved by four conventional 
summarization methods. 
86
Research and Development in Information 
Retrieval: 335 - 336. 
Yi-Ting Chen, Berlin Chen and Hsin-Min Wang. 
2009. A probabilistic generative framework for 
extractive broadcast news speech summarization. 
IEEE Transactions on Audio, Speech and 
Language Processing, 17, (1): 95 - 106. 
John M. Conroy and Dianne P. O?Leary. 2001. Text 
summarization via hidden Markov models. In 
Proc. of Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval: 406 - 407. 
G?ne? Erkan and Dragomir R. Radev. 2004. LexRank: 
graph-based lexical centrality as salience in text 
summarization. Journal or Artificial Intelligence 
Research, 22: 457 - 479. 
Mohamed Abdel Fattah and Fuji Ren. 2009. GA, MR, 
FFNN, PNN and GMM based models for 
automatic text summarization. Computer Speech 
and Language, 23, (1): 126 - 144. 
Louisa Ferrier A maximum entropy approach to text 
summarization. School of Artificial Intelligence, 
University of Edinburgh, 2001. 
Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka 
and Chiori Hori. 2004. Speech-to-text and speech-
to-speech summarization of spontaneous speech. 
IEEE Transactions on Speech and Audio 
Processing, 12, (4): 401 - 408. 
Michel Galley. 2006. A skip-chain conditional 
random field for ranking meeting utterances by 
importance. In Proc. of Conference on Empirical 
Methods in Natural Language Processing: 364 - 
372. 
Vaibhava Goel and William Byrne. 2000. Minimum 
Bayes-risk automatic speech recognition. 
Computer Speech and Language, 14, (2): 115 - 
135. 
Yihong Gong and Xin Liu. 2001. Generic text 
summarization using relevance measure and latent 
semantic analysis. In Proc. of Annual 
International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval: 19 - 25. 
Akira Inoue, Takayoshi Mikami and Yoichi 
Yamashita. 2004. Improvement of speech 
summarization using prosodic information, In 
Proc. of Speech Prosody: 599 - 602. 
Shankar Kumar and William Byrne. 2004. Minimum 
Bayes-risk decoding for statistical machine 
translation. In Proc. of Human Language 
Technology conference / North American chapter 
of the Association for Computational Linguistics 
annual meeting: 169 - 176. 
Aleksander Kolcz, Vidya Prabakarmurthi and Jugal 
Kalita. 2001. Summarization as feature selection 
for text categorization. In Proc. of Conference on 
Information and Knowledge Management: 365 - 
370. 
Julian Kupiec, Jan Pedersen and Francine Chen. 1999. 
A trainable document summarizer. In Proc. of 
Annual International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval: 68 - 73. 
Konstantinos Koumpis and Steve Renals. 2000. 
Transcription And Summarization Of Voicemail 
Speech. In Proc. of International Conference on 
Spoken Language Processing: 688 - 691. 
Chin-Yew Lin. 2004. ROUGE: a Package for 
Automatic Evaluation of Summaries. In Proc. of 
Workshop on Text Summarization Branches Out. 
Shih-Hsiang Lin, Berlin Chen and Hsin-Min Wang. 
2009. A comparative study of probabilistic 
ranking models for Chinese spoken document 
summarization. ACM Transactions on Asian 
Language Information Processing, 8, (1): 3:1 - 
3:23. 
Shih-Hsiang Lin, Yueng-Tien Lo, Yao-Ming Yeh and 
Berlin Chen. 2009. Hybrids of supervised and 
unsupervised models for extractive speech 
summarization. In Proc. of Annual Conference of 
the International Speech Communication 
Association: 1507 - 1510. 
Inderjeet Mani and Mark T. Maybury Advances in 
automatic text summarization. MIT Press, 
Cambridge, 1999. 
Sameer R. Maskey and Julia Hirschberg. 2003. 
Automatic Summarization of Broadcast News 
using Structural Features. In Proc. of the Euro-
pean Conf. Speech Communication and Technolo-
gy: 1173 - 1176. 
Kathleen McKeown, Julia Hirschberg, Michel Galley 
and Sameer Maskey. 2005. From text to speech 
summarization. In Proc. of IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing: 997 - 1000. 
Rada Mihalcea and Paul Tarau. 2005. TextRank: 
bringing order into texts. In Proc. of Conference 
on Empirical Methods in Natural Language 
Processing: 404 - 411. 
Dragomir R. Radev, Hongyan Jing, Ma?gorzata Stys 
and Daniel Tam. 2004. Centroid-based 
summarization of multiple documents. 
Information Processing and Management, 40: 919 
- 938. 
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang and 
Zheng Chen. 2007. Document summarization 
using conditional random fields. In Proc. of 
International Joint Conference on Artificial 
Intelligence: 2862 - 2867. 
Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo and Shih-
Sian Cheng. 2005. MATBN: A Mandarin Chinese 
broadcast news corpus. International Journal of 
Computational Linguistics and Chinese Language 
Processing, 10, (2): 219 - 236. 
ChengXiang Zhai and John Lafferty. 2006. A risk 
minimization framework for information retrieval. 
Information Processing & Management, 42, (1): 
31 - 55. 
ChengXiang Zhai. Statistical language models for 
information retrieval. Morgan & Claypool 
Publishers, 2008. 
Justin Jian Zhang, Ho Yin Chan and Pascale Fung. 
2007. Improving Lecture Speech Summarization 
Using Rhetorical Information. In Proc. of Workshop 
of Automatic Speech Recognition Understanding: 
195 - 200. 
87
