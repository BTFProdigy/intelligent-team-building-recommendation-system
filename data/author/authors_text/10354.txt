Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1089?1096
Manchester, August 2008
    Sentence Type Based Reordering Model for Statistical Machine 
Translation 
Jiajun Zhang, Chengqing Zong, Shoushan Li 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{jjzhang, cqzong, sshanli}@nlpr.ia.ac.cn 
 
 
Abstract 
Many reordering approaches have been 
proposed for the statistical machine 
translation (SMT) system. However, the 
information about the type of source 
sentence is ignored in the previous 
works. In this paper, we propose a group 
of novel reordering models based on the 
source sentence type for Chinese-to-
English translation. In our approach, an 
SVM-based classifier is employed to 
classify the given Chinese sentences into 
three types: special interrogative sen-
tences, other interrogative sentences, and 
non-question sentences. The different 
reordering models are developed ori-
ented to the different sentence types. 
Our experiments show that the novel re-
ordering models have obtained an im-
provement of more than 2.65% in BLEU 
for a phrase-based spoken language 
translation system.  
1 Introduction 
The phrase-based translation approach has been 
the popular and widely used strategy to the sta-
tistical machine translation (SMT) since Och, et 
al. (2002) proposed the log-linear model. How-
ever, reordering is always a key issue in the de-
coding process. A number of models have been 
developed to deal with the problem of reorder-
ing. The existing reordering approaches could 
be divided into two categories: one is integrated 
into the decoder and the other is employed as a 
preprocessing module.   
                                                 
  ? 2008. Licensed under the Creative Commons Attribu-
tion-Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
Many reordering methods belong to the for-
mer category. Distortion model was first em-
ployed by Koehn et al (2003); a lexicalized re-
ordering model was proposed by Och et al 
(2004) and Koehn et al (2005); and the formal 
syntax-based reordering models were proposed 
by Chiang (2005) and Xiong et al (2006). It is 
worthy to note that little syntactic knowledge is 
used in the models mentioned above. 
Compared to the reordering models that are 
integrated into the decoder, the reordering at the 
source side can utilize more syntactic knowl-
edge, with the goal of  adjusting the source lan-
guage sentence to make its word order closer to 
that of the target language. The most notable 
models are given by Xia and McCord (2004), 
Collins et al (2005), Li et al (2007) and Wang 
et al (2007). Xia and McCord (2004) parsed the 
source and target sides of the training data and 
then automatically extracted the rewriting pat-
terns. The rewriting patterns are employed on 
the input source sentence to make the word or-
der more accordant to target language. Collins et 
al. (2005) described an approach to reorder Ger-
man in German-to-English translation. The 
method concentrates on the German clauses and 
six types of transforming rules are applied to the 
parsed source sentence. However, all the rules 
are manually built. Li et al (2007) used a parser 
to get the syntactic tree of the source language 
sentence. In this method, a maximum entropy 
model is developed to determine how probable 
the children of a node are to be reordered. Obvi-
ously, there is also disadvantage in this method 
because the parsing tree is obtained by a full 
parser and contains too many nodes that are not 
involved in desired reorderings. Wang et al 
(2007) discussed three categories which are con-
sidered to be the most prominent candidates for 
reordering in Chinese-to-English translation, 
including verb phrases (VPs), noun phrases 
(NPs), and localizer phrases (LCPs). The 
1089
method deals with some special modifiers of 
VPs and NPs because they have the property 
that some specific modifiers appear before VPs 
or NPs in Chinese but occur after VPs or NPs in 
its English translation. We observe that all the 
transformation rules in this method are hard 
crafted. Furthermore, there are some other re-
lated works, such as Costa-jussa and Fonollosa?s 
work (2006) and Zhang et al?s work (2007). 
Costa-jussa and Fonollosa (2006) considered the 
source reordering as a translation task which 
translates the source sentence into reordered 
source sentence. A chunk-level reordering 
model was first proposed by Zhang et al (2007). 
However, all the existing models make no 
distinction between the different types of the 
source sentence. Intuitively, we have different 
reordering information in different sentence type. 
Taking Chinese special interrogative sentence as 
an example, there is a fixed phrase that usually 
occurs at the end of Chinese sentence but ap-
pears at the beginning part of its English transla-
tion. See the following Chinese to English trans-
lation: 
Chinese: ? ? ? ??? ? ?? ? 
English: What kind of seats do you like ? 
Obviously, the Chinese question phrase ??
?? ? ?? (What kind of seats)? should be 
put at the beginning of its English translation. 
However, many phrase-based systems fail to do 
this. 
In this paper, we are interested in investigat-
ing the value of Chinese sentence types in reor-
dering for Chinese-to-English spoken language 
translation. Due to the syntactic difference be-
tween Chinese and English, different sentence 
type provides different reordering information. 
A phrase-ahead model is developed to exploit 
and utilize the reordering information of special 
interrogative sentences. A phrase-back model is 
employed to catch and make use of the reorder-
ing information of other sentence types. How-
ever, the sentence type should be first identified 
by an SVM-based classifier before reordering 
the source sentence. The method overall is used 
as a preprocessing module for translation. We 
will introduce our method in detail later. 
 The remainder of this paper is organized as 
follows: Section 2 introduces our motivations; 
Section 3 gives the details on the implementa-
tion of our approach; the experiments are shown 
in Section 4; and the final concluding remarks 
are given in Section 5. 
2 Our Motivations 
In this section, before we analyze the Chinese-
to-English spoken language translation corpus,  
some definitions are given first. 
2.1 Definitions 
z Special interrogative sentence / other inter-
rogative sentence / non-question sentence 
Chinese sentence can be divided into question 
sentence and non-question sentence. If a Chi-
nese question sentence is translated into the 
English sentence of wh-questions, the sentence 
is named as a Chinese special interrogative sen-
tence; otherwise, it is called the Chinese other 
interrogative sentence. Figure 1-3 show some 
examples for the three sentence types respec-
tively. 
z SQP / TP / SP 
In Chinese special interrogative sentence, the 
question phrase is always moved ahead while it 
is translated into English. Correspondingly, the 
question phrase is named as the special question 
phrase (SQP). For example, the question  phrase  
???? ? ?? (What kind of seats)? in the 
example mentioned above is an SQP.  
A few quantifier phrases (QPs) like ?? ? 
(many times)?, ??? ? (many years)? in Chi-
nese and some LCPs like ??? ?? ? (after 
the accident happened)?, ??? ?? ? (before 
the meeting ends)? together with some NPs like 
temporal phrases are named temporal phrase 
(TP) in our model. Some LCPs like ??? ? (at 
the front of the hotel)?, ??? ? (near the ta-
ble)? and a few NPs like spatial phrases are 
called spatial phrase (SP) in our model. As PPs1, 
TPs and SPs are the most prominent candidates 
for reordering in Chinese other interrogative 
sentences and non-question sentences, they will 
be handled in the phrase-back reordering model.  
 
Figure 1.  An example of Chinese special inter-
rogative sentence with its English translation. 
 
Figure 2.  An example of Chinese other inter-
rogative sentence with its English translation. 
                                                 
1 PPs here mean prepositional phrases 
?  ?  ?  ??  ?  ? 
Can you speak Japanese ? 
?  ?  ?  ???  ?  ??  ? 
What kind of seats do you like ? 
1090
My wallet was stolen in the subway . 
 
Figure 3.  An example of Chinese non-question 
sentence with its English translation. 
2.2 Analysis of Corpus and  Our Motivations 
In order to have an overview of the distribution 
of the Chinese sentence types, we have made a 
survey based on our training set for translation, 
which contains about 277k Chinese and English 
sentence pairs. We found that about 17.2% of 
the sentences are special interrogative sentences, 
about 25.5% of sentences are other interrogative 
sentences and the remainders are all non-
question sentences. 
Each sentence type has its own reordering 
strategy, as demonstrated in Figures 1-3. There 
is a settled phrase (SQP) in Chinese special in-
terrogative sentence which usually appears at 
the end but will be translated first in English, 
just as Figure 1 illustrates. For other interroga-
tive sentences, some specific Chinese words like 
???????? will just be translated into 
?Can? or ?Do?  and come first in English. At 
present, this information is not used in our ap-
proach. Figure 2 gives an example. For non-
questions, the reordering candidates usually 
need to be moved back during translation. An 
example is shown in Figure 3. 
According to the analysis above, it is mean-
ingful to develop reordering models based on 
the source sentence types. 
2.3 Framework 
As we mentioned above, our framework is illus-
trated as follows: 
 
Figure 4.  Architecture of the framework, where 
C1 means the special interrogative sentence, C2 
is other interrogative sentence and C3 is non-
question sentence. 
 
Conventional preprocessing approaches di-
vide the translation into two phases: 
                                       (1) 'S S T? ?
'
'
'cS S S T? ? ?
c
cS
'S
                                                
Reordering is first done in the source side 
which changes the source sentence S into reor-
dered one S , and then a standard phrase-based 
translation engine is used to translate the reor-
dered source sentence S  into target language 
sentence T. 
? ??  ? ?? ? ?? ? ? 
In our method, to utilize the information of 
sentence types, a new approach is proposed to 
improve the translation performance by devel-
oping a hybrid model as follows: 
                      (2) 
Before the source sentence is reordered, an 
SVM-based classifier is first employed to de-
termine its sentence type S , then, different re-
ordering model is used to reorder the source 
sentence with the specific sentence type . Af-
ter getting the reordered source sentence , we 
use our phrase-based SMT to obtain the optimal 
target language sentence.  
The contribution of this paper is embodied in 
the first two steps of our method. 
In the first step, an SVM classifier is used to 
identify the type of source sentence2.  
In the second step, two reordering models are 
built according to the different sentence types. A 
phrase-ahead reordering model is developed for 
the special interrogative sentences which uses 
shallow parsing technology to recognize the 
most prominent candidates for reordering (spe-
cial question phrase) and extracts reordering 
templates from bilingual corpus. For other sen-
tence types, we build a phrase-back reordering 
model which uses shallow parsing technology to 
identify the phrases that are almost always 
moved back during translation and applies 
maximum entropy algorithm to determine 
whether we should reorder them. 
Source text 
sentence 3 Models and Algorithms 
In this section, we first introduce the sentence 
type classifier model, and then we describe in 
detail the two reordering models, phrase-ahead 
reordering model and phrase-back reordering 
model. 
3.1 Sentence Type Identification 
Many models are used for classification such as 
Na?ve Bayes, decision tree and maximum en-
tropy. In our approach, we use an SVM-based 
classifier to classify the sentence types. SVM 
 
2 There are three sentence types: special interrogative sen-
tence, other interrogative sentence and non-question sen-
tence, which are defined in sub-section 2.1. 
 
Target 
sentence 
C1 
C3 
C2 
Phrase-ahead 
model 
Phrase-back 
model 
Phrase-
based 
decoder 
SVM 
classifier 
Phrase-back 
model 
1091
has been shown to be highly effective at tradi-
tional text categorization. For our problem, we 
regard a sentence as a text. The decision bound-
ary in SVM is a hyperplane, represented by vec-
tor , which separates the two classes, leaving 
the largest margin between the vectors of the 
two classes (Vapnik, 1998). The search of mar-
gin corresponds to a constrained optimization 
problem. Suppose 
w
G
{1, 1}jc ? ? (positive and 
negative) be the correct class of sentence js , the 
solution can be formalized as: 
: j j j
j
w c?=?G Gs 0j   ? ?              (3) 
Where the js
G
 is feature vector of our sen-
tence js .  We get j? s through solving a dual 
optimization problem. Identifying the type of a 
sentence is just to determine which side of w
G
?s 
hyperplane it will fall in. 
Feature selection is an important issue. We 
directly use all the words occurring in the sen-
tence as features. 
Some readers may argue that the features to 
distinguish the sentence types are very obvious 
in Chinese. For example, ??? can easily sepa-
rate the interrogative sentences from non-
question sentences. In this case, a simple classi-
fier like decision tree will work. It is true when 
the punctuation always appears in the sentence. 
However, sometimes there is no punctuation in 
the spoken language text. Under this situation, 
the decision tree will lose the most powerful 
features, but the performance of SVM is not af-
fected by the punctuations. The experimental 
results verifying this will be given in Section4. 
3.2 Phrase-ahead Reordering Model 
As we mentioned above, about 17.2% of the 
spoken language sentences are special interroga-
tive sentences. Furthermore, we note that each 
Chinese special interrogative sentence has one 
or more special question phrases (SQP) that we 
defined in section 2.1. Due to the difference be-
tween Chinese and English word order, the SQP 
needs to be moved ahead3 when it is translated 
into English. 
    Let S be a Chinese special interrogative sen-
tence, our first problem is to recognize the SQPs 
in S. If we have known the SQP, namely S be-
comes  (  is the left part of  the 0    S SQP S
                                                
1 0S
 
1S
0S
3 There is a specific situation that the SQP don?t have to be 
moved. In this case, we suppose it needs to be moved, but 
the distance is 0. 
sentence before SQP, and  is the right part of 
the sentence after SQP), our second problem is 
to find the correct position in where SQP will 
be moved to. 
 For the first problem, because each syntactic 
component is possible a SQP, for example, ??
?? ? ??? in Figure 1 is NP, ?? ??
(Where)? in Chinese sentence ?? ? ?? ? 
? ? ? ?(Where can I buy the ticket?)? is 
PP (also a VP modifier), ???  ?  (How to 
go)? in ?? ?? ?? ? ?(How to go to the 
beach?)? is VP, it is very difficult to find the 
SQP by syntax. In our model, we first find out 
all the key words, which we list below, in the 
special interrogative sentences through mutual 
information. Then, we define the syntactic com-
ponent containing the key word as an SQP. In-
stead of full syntactic parser, we utilize a CRF 
toolkit named FlexCrfs4 to train, test and predict 
the SQPs chunking. 
 
?? What 
? (?? / ???) Where 
? (?? / ???) How much/many/old?
? (??/??? ?) What about/How 
? (?? / ???) Who/whose/whom 
? (?? / ???) How many/old When?
??? Why 
?(?? / ???) When/where 
Table 1.  The special key words set 
 
For the second problem, we note that there 
are only three positions where the SQP will be 
moved to:  (1) the beginning of the sentence; (2) 
just after the rightmost punctuation (?,?, ?;? or 
?:?) before the SQP; (3) or after a regular phrase 
such as ???  (May I ask)? and ???  ? 
(Please tell me)?. Therefore, we can learn the 
reordering templates from bilingual corpus 5 . 
The simple algorithm is illustrated in Figure 5, 
and some reordering templates are shown in Ta-
bl
                                                
e 2.  
On the whole, When we reorder the special 
interrogative sentence, we first identify the SQP, 
then we find out whether there are punctuations 
(?,? , ?;? or ?:?) before SQP; if any, we keep the 
rightmost punctuation index, otherwise we keep 
the index 0 (beginning of sentence). In the third 
 
4 See http://flexCRF.sourceforge.net 
5 The bilingual corpus is the corpus combined by the train-
ing corpus for chunking SQPs and its corresponding Eng-
lish translation. 
1092
step, if we find that a reordering template like 
some one given in Table 2 can match the sen-
tence, we just apply the template, otherwise we 
just move the SQP after the index that we kept 
efore (0 or punctuation index). 
 
 empirical value N is 10 in our ex-
eriment. 
 
b
 
Figure 5. Reordering template extraction algo-
rithm. The
p
X1?? X2 SQP X1 ?? SQP X2 
X1 ?? ? X2 SQP X1 ?? ? SQP X2 
X1 P X1 ? ? ?? X2 SQ ? ? ?? SQP X2
X1 ? SQP X1 ? P X2? X2 ? SQ  
?? ?? 
 Table 2.  Some reordering templates 
3.3 Phrase-back Reordering Model 
In this paper, we employ the phrase-back reor-
dering model for Chinese other interrogative 
 posi-
tio
makes our model suitable for 
m e
 
??? (sign your name)? 
 identified as a NP. 
 
 
z The form of phrase-back reordering rules: 
sentences and non-question sentences. 
   Inspired by the work of Wang et al (2007), 
we only consider the most prominent candidates 
for reordering. The VP modifiers like PP, TP, 
and SP which we defined in sub-section 2.1 are 
typically in pre-verb position in Chinese but al-
most always appear after the verb in its corre-
sponding English translation. Wang et al (2007) 
concentrate on VP, NP, then determine whether 
their modifiers should be moved back. Instead, 
our interests are focused on the modifiers: PP, 
TP and SP; namely, we consider the modifiers 
PP, TP and SP as triggers, and the first VP oc-
curring after triggers will be the candidate
n where the triggers may be moved to. 
Changing the focus gives us the ability to 
handle a specific situation that there is no VP 
after the triggers for recognition error or other 
reasons. As the example in Figure 6, there is no 
VP after PP (?? ???) because the phrase ??
?? next to PP is wrongly recognized to be a NP. 
To deal with the case, we will further define a 
fake verb phrase (FVP): the phrase after PP (TP 
or SP) until the punctuation (?,?, ?;? or ?.?). The 
phrase ??? (sign your name)? in Figure 6 is 
an FVP. Here, FVP is given the same function 
with VP, thus it 
or  situations. 
 
Figure 6.  An example of FVP. In our model the 
whole sentence is recognized as a VP, ?? ?? 
(here)? is a PP, and 
is
Unlike hard reordering rules of Wang et al 
(2007), we develop a probabilistic reordering 
model to alleviate the impact of the errors 
caused by the parser when recognizing PPs, TPs, 
SPs and VPs. We believe that no reordering is 
better than bad reordering. The rule forms and 
1:  Input: special interrogative sentence pair (s, t) in which 
se which aligns to 
ndex-1] 
NONE then 
; 
_Phrase if Count(C_Phrase)<N 
SQP is labeled and their alignment M is given 
2:  R={} 
3: Find the rightmost punctuation index c_punc_index before 
SQP and English index e_punc_index aligned to 
c_punc_index 
4: Find the smallest index e_smallest_index of English which 
align to the SQP  
 C_Phra5: Get the Chinese phrase
[e_punc_index+1, e_smallest_i
6:  if C_Phrase is 
7:       Continue ; 
8:  end if 
Phrase in R then 8:  if C_
9:       Count(C_Phrase)++; 
10: else 
11:     Insert C_Phrase into R
12:     Count(C_Phrase)=1; 
13: end if 
14: remove C
? ? ??   ?? ? 
the probabilistic model will be given as follows:
A : 1 22
2 1
A XA straight
A XA1 XA A inver
?? ??
 
Where, 1 { , , }A PP TP SP
ted
?   { , }VP FVP?   2A
1 2{ }X phrases between A  and A?  
z We use the Maximum Entropy Model  
which is implemented by Zhang6.  The model is 
trained from bilingual spoken language corpus 
determine whether 1A  should be moved after 
2A . The features that we investigated include 
the leftmost, rightmost, and their POSs 
to 
of 1A  
and 2A . It leads to the following formula: 
exp( ( , ))
( | )
exp( ( , ))
i ii
i iO i
h O A
P O A
h O A
?
?=
?
? ?           (4) 
sWhere, { , }O traignt inverted? , ( , )ih O A  is a 
feature, and i? is the weight of the feature. 
When app  the rules, we first identify 
pairs like ( 1 2A XA ) in the sentence, and then 
m beginning t  
1A  behind 2A  if ( | ) ( | )P inverted A P straight A> . 
After all the pairs are pr
lying
fro o end of the sentence, we move
ocessed, we will get the 
reordered source result. 
                                                 
6http://homepages.inf.ed.ac.uk/s0450736/maxent_too
lkit.html 
15: output R 
1093
4 Experiments 
We have conducted several experiments to 
evaluate the models.  In this section, we first 
introduce the corpora, and then we discuss the 
performance of the SVM-based classifier, 
chunking and reordering models respectively. 
4.1 Corpora 
We perform our experiments on Chinese-to-
English speech translation task. The statistics of 
the corpus is given in Table 3 where CE_train 
means the Chinese-to-English training data re-
leased by IWSLT 2007; CE_sent_filtered means 
the bilingual sentence pairs filtered from the 
open resources of the bilingual sentences on the 
website; CE_dict_filtered means the bilingual 
dictionary filtered from the open resources of 
the bilingual dictionaries on the website; 
CE_dev123 denotes the bilingual sentence pairs 
obtained by the combination of the development 
data IWSLT07_CE_devset1, IWSLT07_CE_devset2 
and IWSLT07_CE_devset3 which are released 
by the IWSLT 2007; CE_dev4 and CE_dev5 are 
the remainder of development data released by 
IWSLT 2007; CE_test means the final test set 
released by IWSLT 2007. 
We combine the data from the top four rows 
as our training set. We use CE_dev4 as our de-
velopment set. CE_dev5 and CE_test are our 
two test data. The test data released by IWSLT 
2007 is based on the clean text with punctuation 
information, so we add the punctuation informa-
tion on the Chinese sentences of CE_dev4 and 
CE_dev5 by our SVM sentence type classifier to 
form the final development set. The detailed 
statistics are given in Table 4. 
4.2 Classification Result 
To evaluate the performance of SVM-based 
classifier on classifying the sentence types, we 
first use a simple decision tree to divide the 
Chinese sentences of our training data for trans-
lation into three sentence types. Then we clean 
them by hand in order to remove the errors. At 
last, 10k sentences for each sentence type are 
randomly selected as the experiment data. For 
each sentence type, 80% of the data are used as 
training data, 20% as test data. Table 5 gives the 
classification results. 
Punctuation in Table 5 means the punctuation 
which occurs at the end of the sentence such as  
??? and ???. We can see from the table that 
SVM classifier performs very well even if we 
remove the punctuations at the end of every sen-
tence. Therefore, almost no errors will be passed 
to the reordering stage. 
 
Data Chinese English 
CE_train 39,953 39,953 
CE_sent_filtered 188,282 188,282 
CE_dict_filtered 31,132 31,132 
CE_dev123 24,192 24,192 
CE_dev4 489 3,423 
CE_dev5 500 3,500 
CE_test 489 2,934 
Table 3.  Statistics of training data, development 
data and test data 
 
 Chinese English 
sentences 276,633 
Train set
words 1,665,073 1,198,984
sentences 489 489*7 Dev set  
CE_dev4 words 6241 47609 
sentences 500 500*7 Test set  
CE_dev5 words 6596 52567 
sentences 489 489*6 Test set   
CE_test words 3166 22574 
Table 4.  Detailed statistics of training data on 
development set 
 
 Accuracy (%)
With punctuation 99.80 
Without punctuation 98.00 
Table 5.  The accuracy of SVM classifier 
4.3 Chunking Results 
In our experiment, except that VPs are obtained 
by a syntactic parser (Klein and Manning, 2003),  
SQPs, PPs, TPs, SPs are all chunked by the 
FlexCrfs. 
The chunking data used for training and test 
in Table 6 are annotated by ourselves. Every 
chunk is  annotated according to the definition 
that we define in sub-section 2.1. The raw train-
ing and test data are all extracted from our train-
ing set for translation. TPs, SPs are annotated 
together; SQPs, PPs are annotated respectively. 
The statistics of the training and test data are 
shown in Table 6. Table 7 gives the chunking 
results. 
The precision, recall and F-Measure are met-
rics for the chunking results. F-Measure follows 
the criteria of CoNLL-20007.  
2*( * )precision recall
F Measure  
precision recall
? = +
                                                 
7 See  http://www.cnts.ua.ac.be/conll2000/chunking/ 
1094
Because the SQPs have the regularity that 
each one contains a key word listed in Table 1, 
the result of SQPs chunking is quite good. 
Moreover, the chunking of PPs, TPs and SPs 
also performs well. 
 
 Train Test 
sentences 10,000 500 SQP 
chunks 10030 501 
sentences 10,000 500 PP 
chunks 10106 512 
sentences 11,000 500 SP and TP 
chunks 10342 523 
Table 6.  Statistics of train and test data 
 
 Precision (%) 
Recall 
(%) 
F-Measure 
(%) 
SQP 95.52 95.52 95.52 
PP 94.65 93.31 93.98 
SP and TP 93.92 92.68 93.25 
Table 7.  Chunking results on test set 
4.4 Translation Results 
For the translation experiments, BLEU-4 and 
NIST are used as the evaluation metric. The 
baseline SMT uses the standard phrase-based 
decoder that applies the log-linear model (Och 
and Ney, 2002).  
  In the preprocessing module, all the Chinese 
words are segmented by the free software toolkit 
ICTCLAS3.08, and the POS tags are obtained 
by using the Stanford parser with its POS pars-
ing function. For the decoder, the phrase table is 
obtained as described in (Koehn et al, 2005), 
and our 4-gram language model is trained by the 
open SRILM9 toolkit. It should be noted that we 
use monotone decoding in translation. 
We have done three groups of experiments 
for translation. The first one is to test the effect 
of phrase-ahead reordering model, the result of 
which is shown in Table 8. Compared to the 
baseline system, phrase-ahead reordering model 
improves the results of the two test sets by 
0.41% and 1.87% in BLEU respectively. The 
difference in the performance gains can be at-
tributed to the fact that there are 100 Chinese 
special interrogative sentences in Test 2, while 
only 30 are found in Test 1. Accordingly, the 
reordering candidates of Test 1 are much fewer 
than that of Test 2. Thus, we can conclude that 
the more special interrogative sentences the bet-
ter performance of the translation. Furthermore, 
                                                 
8 See http://www.nlp.org.cn 
9 See http://www.speech.sri.com/projects/srilm 
the results show that the reordering on special 
interrogative sentences is a good try. 
The second experiment is conducted to test 
the effect of phrase-back reordering model. Ta-
ble 8 gives the results. For the two test sets, the 
model brings an improvement to the baseline by 
2.24% and 0.93% in BLEU respectively. How-
ever, the difference between them is still very 
big. We think there are two reasons: firstly, 
there are much more special interrogative sen-
tences in Test 2 than in Test 1, so the sentences 
of other sentence types in Test 2 are much fewer 
than that in Test 1. Thus, fewer candidates are 
found in Test 2 than in Test 1. Secondly, the 
average sentence length of Test 2 (6.5 words) is 
much shorter than that of Test 1 (13.2 words). 
We know that if the sentence is very short, the 
PP, TP, and SP will seldom occur. Naturally, 
only 89 candidates are found in Test 2 but 366 
in Test 1. Regardless of the difference, the 
phrase-back reordering model indeed improves 
the translation quality significantly. 
The last experiment merges the two reorder-
ing model together. The results in Table 8 show 
that the overall reordering model has done very 
well in both test sets: it improves the two test 
sets by 2.65% and 2.78% in BLEU score respec-
tively. It demonstrates that every reordering 
model has a positive effect on translation. 
Therefore, our reordering model based on the 
sentence type is quite successful. 
5 Conclusions and Future Work 
In this paper, we have investigated the effect of 
the Chinese sentence types on reordering prob-
lem for Chinese-to-English statistical machine 
translation. We have succeeded in applying a 
phrase-ahead reordering model to process the 
special interrogative sentences and a phrase-
back reordering model to deal with other sen-
tence types. Experiments show that our reorder-
ing model obtains a significant improvement in 
BLEU score on the IWSLT-07 task. 
With the encouraging experimental results, 
we believe that we can mine more reordering 
information from the Chinese sentence types. In 
this paper, we only apply a phrase-back model 
to reorder Chinese other interrogative sentences. 
In the next step, we will try to develop a special 
reordering model for this sentence type. Fur-
thermore, we plan to integrate the phrase-back 
model into phrase-ahead model for special inter-
rogative sentences and investigate the value of 
this integration. 
1095
 Table 8.  Statistics of translation results 
Notes: candidates here mean how many candidate reordering phrases are recognized for each model. Sentences 
mean the number of sentences belonging to the specific sentence type, i.e. for phrase-ahead reordering in Test 1, 
31 special question phrases (SQP) are recognized in 30 Chinese special interrogative sentences. 
 
Acknowledgments 
The research work described in this paper has 
been partially supported by the Natural Science 
Foundation of China under Grant No. 60575043 
and 60736014, the National High-Tech Research 
and Development Program (863 Program) of 
China under Grant No. 2006AA01Z194 and 
2006AA010108, the National Key Technologies 
R&D Program of China under Grant No. 
2006BAH03B02, and Nokia (China) Co. Ltd as 
well. 
 
References 
Cao Wang, Michael Collins and Philipp Koehn. 2007. 
Chinese syntactic reordering for statistical machine 
translation. In Proceedings of joint Conference on 
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, 2007.  
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou 
Minghui Li and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical 
machine translation. In Proceedings of 45th Meet-
ing of the Association for Computational Linguis-
tics . 
Dan Klein and Christopher D. manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of  41st  
Meeting of the Association for Computational Lin-
guistics.  
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Pro-
ceedings of 43rd Meeting of Association for Com-
putational Linguistics.  
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. maxi-
mum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the joint conference of the International Committee 
on Computational Linguistics and the Association 
for Computational Linguistics 2006.  
Fei Xia and Michael McCord. 2004. Improving a Sta-
tistical MT system with automatically learned re-
write patterns. In Proceedings of 20th International 
Conference on Computational Linguistics. 
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for 
statistical machine translation. In Proceedings of 
40th Meeting of Association for Computational 
Linguistics.  
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine 
translation. Computational Linguistics, 30:417-449 
Marta R. Costa-jussa and Jose A.R. Fonollosa. 2006. 
Statistical machine reordering. In proceedings of 
Conference on Empirical Methods in Natural Lan-
guage Processing 2006.  
Michael Collins, Philipp Koehn, and Ivona Kucerova. 
2005. Clause restructuring for statistical machine 
translation. In proceedings of 43rd Meeting of the 
Association for Computational Linguistics.  
Philipp Koehn, Franz J. Och. and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In proceed-
ings of HLT-NAACL 2003. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne and 
David Talbot. 2005. Edinburgn System Description 
for the 2005 IWSLT Speech Translation Evalua-
tion. In International Workshp on Spoken Lan-
guage Translation. 
Yuqi Zhang, Richard Zens and Hermann Ney. 2007. 
Chunk-level reordering of source language sen-
tence with automatically learned rules for statistical 
machine translation. In Proceedings of SSST, 
NAACL-HLT 2007/AMTA Workshop on Syntax and 
Structure in Statistical Translation. 
Vladimir Naumovich Vapnik. 1998. Statistical Learn-
ing Theory. John Wiley and Sons, Inc.  
 BLEU (%) NIST Sentences Candidates
Baseline 32.16 6.4844 500  
Phrase-ahead reordering 32.57 6.5579 30 31 
Phrase-back reordering 34.40 6.6857 470 366 
Test 1 
CE_dev5 
Phrase-ahead+phrase-back 34.81 6.7584 500 397 
Baseline 34.04 5.8340 489  
Phrase-ahead reordering 35.91 6.0693 100 97 
Phrase-back reordering 34.97 5.9172 389 89 
Test 2 
CE_test 
Phrase-ahead+phrase-back 36.82 6.1535 489 186 
1096
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 257?260,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multi-domain Sentiment Classification 
 
Shoushan Li and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{sshanli,cqzong}@nlpr.ia.ac.cn 
 
 
 
 
 
Abstract 
This paper addresses a new task in sentiment 
classification, called multi-domain sentiment 
classification, that aims to improve perform-
ance through fusing training data from multi-
ple domains. To achieve this, we propose two 
approaches of fusion, feature-level and classi-
fier-level, to use training data from multiple 
domains simultaneously. Experimental stud-
ies show that multi-domain sentiment classi-
fication using the classifier-level approach 
performs much better than single domain 
classification (using the training data indi-
vidually). 
1 Introduction 
Sentiment classification is a special task of text 
categorization that aims to classify documents 
according to their opinion of, or sentiment toward 
a given subject (e.g., if an opinion is supported or 
not) (Pang et al, 2002). This task has created a 
considerable interest due to its wide applications. 
Sentiment classification is a very domain-
specific problem; training a classifier using the 
data from one domain may fail when testing 
against data from another. As a result, real 
application systems usually require some labeled 
data from multiple domains, guaranteeing an 
acceptable performance for different domains. 
However, each domain has a very limited amount 
of training data due to the fact that creating large-
scale high-quality labeled corpora is difficult and 
time-consuming.  Given the limited multi-domain 
training data, an interesting task arises, how to 
best make full use of all training data to improve 
sentiment classification performance. We name 
this new task, ?multi-domain sentiment 
classification?. 
In this paper, we propose two approaches to 
multi-domain sentiment classification. In the first, 
called feature-level fusion, we combine the feature 
sets from all the domains into one feature set. 
Using the unified feature set, we train a classifier 
using all the training data regardless of domain. In 
the second approach, classifier-level fusion, we 
train a base classifier using the training data from 
each domain and then apply combination methods 
to combine the base classifiers. 
2 Related Work 
Sentiment classification has become a hot topic 
since the publication work that discusses classifi-
cation of movie reviews by Pang et al (2002). 
This was followed by a great many studies into 
sentiment classification focusing on many do-
mains besides that of movie. 
Research into sentiment classification over 
multiple domains remains sparse. It is worth not-
ing that Blitzer et al (2007) deal with the domain 
adaptation problem for sentiment classification 
where labeled data from one domain is used to 
train a classifier for classifying data from a differ-
ent domain. Our work focuses on the problem of 
how to make multiple domains ?help each other? 
when all contain some labeled samples. These two 
problems are both important for real applications 
of sentiment classification. 
3 Our Approaches 
3.1 Problem Statement 
In a standard supervised classification problem, 
we seek a predictor f (also called a classifier) that 
257
maps an input vector x to the corresponding class 
label y. The predictor is trained on a finite set of 
labeled examples { ( , )i iX Y } (i=1,?,n) and its 
objective is to minimize expected error, i.e., 
l arg min ( ( ), )
n
i i
f i
f L f X Y
?
= ?
?
 
Where L is a prescribed loss function and H is a 
set of functions called the hypothesis space, which 
consists of functions from x to y. In sentiment 
classification, the input vector of one document is 
constructed from weights of terms. The terms 
1( ,..., )Nt t  are possibly words, word n-grams, or 
even phrases extracted from the training data, with 
N being the number of terms. The output label y 
has a value of 1 or -1 representing a positive or 
negative sentiment classification. 
  In multi-domain classification, m different 
domains are indexed by k={1,?,m}, each with 
kn training samples ( , )k ki iX Y {1,..., }k ki n= . A 
straightforward approach is to train a predictor kf  
for the k-th domain only using the training 
data {( , )}
k ki i
X Y . We call this approach single 
domain classification and show its architecture in 
Figure 1. 
 
Figure 1: The architecture of single domain classifica-
tion. 
3.2 Feature-level Fusion Approach 
Although terms are extracted from multiple do-
mains, some occur in all domains and convey the 
same sentiment (this can be called global senti-
ment information). For example, some terms like 
?excellent? and ?perfect? express positive senti-
ment information independent of domain. To learn 
the global sentiment information more correctly, 
we can pool the training data from all domains for 
training. Our first approach is using a common set 
of terms 1( ' ,..., ' )allNt t  to construct a uniform fea-
ture vector 'x  and then train a predictor using all 
training data: 
m
1 1
arg min ( ( ' ), )
k
k k
all all k
nm
all i i
f k i
f L f X Y
? = =
= ??
?
 
We call this approach feature-level fusion and 
show its architecture in Figure 2. The common set 
of terms is the union of the term sets from 
multiple domains.  
 
Figure 2: The architecture of the feature-level fusion 
approach  
 
Feature-level fusion approach is simple to 
implement and needs no extra labeled data. Note 
that training data from different domains 
contribute differently to the learning process for a 
specific domain. For example, given data from 
three domains, books, DVDs and kitchen, we 
decide to train a classifier for classifying reviews 
from books. As the training data from DVDs is 
much more similar to books than that from 
kitchen (Blitzer et al, 2007), we should give the 
data from DVDs a higher weight. Unfortunately, 
the feature-level fusion approach lacks the 
capacity to do this. A more qualified approach is 
required to deal with the differences among the 
classification abilities of training data from 
different domains. 
3.3 Classifier-level Fusion Approach 
As mentioned in sub-Section 2.1, single domain 
classification is used to train a single classifier for 
each domain using the training data in the corre-
sponding domain. As all these single classifiers 
aim to determine the sentiment orientation of a 
document, a single classifier can certainly be used 
to classify documents from other domains. Given 
multiple single classifiers, our second approach is 
to combine them to be a multiple classifier system 
for sentiment classification. We call this approach 
classifier-level fusion and show its architecture in 
Figure 3. This approach consists of two main steps: 
Training Data 
from Domain 1 
Training Data 
from Domain 2 
Training Data 
from Domain m
Classifier  
1 
Classifier 
2 
Classifier 
m 
Testing Data 
from Domain 1 
Testing Data 
from Domain 2 
Testing Data 
from Domain m
   . . . 
   . . . 
   . . . 
Training Data 
from Domain 1
Training Data 
from Domain 2 
Training Data 
from Domain m
Classifier 
Testing Data 
from Domain 1
Testing Data 
from Domain 2
Testing Data 
from Domain m
   . . . 
   . . . 
Training Data from all Domains 
using a Uniform Feature Vector 
258
(1) train multiple base classifiers (2) combine the 
base classifiers. In the first step, the base classifi-
ers are multiple single classifiers kf  (k=1,?,m) 
from all domains. In the second step, many com-
bination methods can be applied to combine the 
base classifiers. A well-known method called 
meta-learning (ML) has been shown to be very 
effective (Vilalta and Drissi, 2002). The key idea 
behind this method is to train a meta-classifier 
with input attributes that are the output of the base 
classifiers. 
 
Figure 3: The architecture of the classifier-level fusion 
approach 
 
Formally, let 'kX denote a feature vector of a 
sample from the development data of the 
'-thk domain ( ' 1,..., )k m= . The output of the 
-thk base classifier kf on this sample is the 
probability distribution over the set of classes 
1 2{ , ,..., }nc c c , i.e., 
' 1 ' '( )  ( | ),..., ( | )k k k k k n kp X p c X p c X= < >  
For the '-thk domain, we train a meta-classifier 
'  ( ' 1,..., )kf k m= using the development data from 
the '-thk domain with the meta-level feature 
vector '
meta m n
kX R
??  
' 1 ' ' ' ( ),..., ( ),..., ( )
meta
k k k k m kX p X p X p X= < >  
Each meta-classifier is then used to test the testing 
data from the same domain.  
Different from the feature-level approach, the 
classifier-level approach treats the training data 
from different domains individually and thus has 
the ability to take the differences in classification 
abilities into account. 
4 Experiments 
Data Set:  We carry out our experiments on the 
labeled product reviews from four domains: books, 
DVDs, electronics, and kitchen appliances1. Each 
domain contains 1,000 positive and 1,000 
negative reviews.  
Experiment Implementation: We apply SVM 
algorithm to construct our classifiers which has 
been shown to perform better than many other 
classification algorithms (Pang et al, 2002). Here, 
we use LIBSVM2 with a linear kernel function for 
training and testing. In our experiments, the data 
in each domain are partitioned randomly into 
training data, development data and testing data 
with the proportion of 70%, 20% and 10% 
respectively. The development data are used to 
train the meta-classifier. 
Baseline: The baseline uses the single domain 
classification approach mentioned in sub-Section 
2.1. We test four different feature sets to construct 
our feature vector. First, we use unigrams (e.g., 
?happy?) as features and perform the standard fea-
ture selection process to find the optimal feature 
set of unigrams (1Gram). The selection method is 
Bi-Normal Separation (BNS) that is reported to be 
excellent in many text categorization tasks (For-
man, 2003). The criterion of the optimization is to 
find the set of unigrams with the best performance 
on the development data through selecting the 
features with high BNS scores.  Then, we get the 
optimal word bi-gram (e.g., ?very happy?) (2Gram) 
and mixed feature set (1+2Gram) in the same way. 
The fourth feature set (1Gram+2Gram) also con-
sists of unigrams and bi-grams just like the third 
one. The difference between them lies in their se-
lection strategy. The third feature set is obtained 
through selecting the unigrams and bi-grams with 
high BNS scores while the fourth one is obtained 
through simply uniting the two optimal sets of 
1Gram and 2Gram.  
From Table 1, we see that 1Gram+2Gram fea-
tures perform much better than other types of fea-
tures, which implies that we need to select good 
unigram and bi-gram features separately before 
combine them. Although the size of our training 
data are smaller than that reported in Blitzer et al 
                                                          
1 This data set is collected by Blitzer et al (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
2 LIBSVM is an integrated software for SVM: 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
Training Data 
from Domain 1 
Training Data 
from Domain 2 
Training Data 
from Domain m
Multiple Classifier 
System 1 
Testing Data 
from Domain 1 
Testing Data 
from Domain 2 
Testing Data 
from Domain m
   . . . 
   . . . 
Base Classifier 
1 
Base Classifier 
2 
Base Classifier
m 
   . . . 
Multiple Classifier 
System 2 
Multiple Classifier 
System m 
Development Data 
from Domain 1 
Development Data 
from Domain 2 
Development Data 
from Domain m 
   . . . 
   . . . 
259
(2007) (70% vs. 80%), the classification perform-
ance is comparative to theirs. 
 
We implement the fusion using 1+2Gram and 
1Gram+2Gram respectively. From Figure 4, we 
see that both the two fusion approaches generally 
outperform single domain classification when us-
ing 1+2Gram features. They increase the average 
accuracy from 0.8 to 0.82375 and 0.83875, a sig-
nificant relative error reduction of 11.87% and 
19.38% over baseline.  
1+2Gram Features
76.5
81 80
8382.5 82.5 82.581
83 84
86
83
72
74
76
78
80
82
84
86
88
Books DVDs Electronics Kitchen
Ac
cu
ra
cy
(%
)
  
1Gram+2Gram Features
79
84.584
82
84.5 85 838283.5
86
8889
74
76
78
80
82
84
86
88
90
Books DVDs Electronics Kitchen
Ac
cu
ra
cy
(%
)
Single domain classification
Feature-level fusion
Classifier-level fusion with ML
 Figure 4: Accuracy results on the testing data using 
multi-domain classification with different approaches. 
 
However, when the performance of baseline in-
creases, the feature level approach fails to help the 
performance improvement in three domains. This 
is mainly because the base classifiers perform ex-
tremely unbalanced on the testing data of these 
domains. For example, the four base classifiers 
from Books, DVDs, Electronics, and Kitchen 
achieve the accuracies of 0.675, 0.62, 0.85, and 
0.79 on the testing data from Electronics respec-
tively. Dealing with such an unbalanced perform-
ance, we definitely need to put enough high 
weight on the training data from Electronics. 
However, the feature-level fusion approach sim-
ply pools all training data from different domains 
and treats them equally. Thus it can not capture 
the unbalanced information. In contrast, meta-
learning is able to learn the unbalance automati-
cally through training the meta-classifier using the 
development data. Therefore, it can still increase 
the average accuracy from 0.8325 to 0.8625, an 
impressive relative error reduction of 17.91% over 
baseline. 
5 Conclusion 
In this paper, we propose two approaches to multi-
domain classification task on sentiment classifica-
tion. Empirical studies show that the classifier-
level approach generally outperforms the feature 
approach.  Compared to single domain classifica-
tion, multi-domain classification with the classi-
fier-level approach can consistently achieve much 
better results. 
Acknowledgments 
The research work described in this paper has 
been partially supported by the Natural Science 
Foundation of China under Grant No. 60575043, 
and 60121302, National High-Tech Research and 
Development Program of China under Grant No. 
2006AA01Z194, National Key Technologies 
R&D Program of China under Grant No. 
2006BAH03B02, and Nokia (China) Co. Ltd as 
well. 
References 
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, 
Bollywood, Boom-boxes and Blenders: Domain ad-
aptation for sentiment classification. In Proceedings 
of ACL.  
G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. Journal 
of Machine Learning Research, 3: 1533-7928. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine learning 
techniques. In Proceedings of EMNLP. 
R. Vilalta and Y. Drissi. 2002. A perspective view and 
survey of meta-learning. Artificial Intelligence Re-
view, 18(2): 77?95. 
Features Books DVDs Elec-
tronic 
Kitchen
1Gram 0.75 0.84 0.8 0.825 
2Gram 0.75 0.73 0.815 0.785 
1+2Gram 0.765 0.81 0.825 0.80 
1Gram+2Gram 0.79 0.845 0.85 0.845 
 
Table 1: Accuracy results on the testing data of single 
domain classification using different feature sets. 
260
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692?700,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Framework of Feature Selection Methods for Text Categorization 
 
 
Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1 
 
1
 Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang} 
@gmail.com 
 
2
 National Laboratory of Pattern 
Recognition 
 Institute of Automation 
 Chinese Academy of Sciences  
{rxia,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
In text categorization, feature selection (FS) is 
a strategy that aims at making text classifiers 
more efficient and accurate. However, when 
dealing with a new task, it is still difficult to 
quickly select a suitable one from various FS 
methods provided by many previous studies. 
In this paper, we propose a theoretic 
framework of FS methods based on two basic 
measurements: frequency measurement and 
ratio measurement. Then six popular FS 
methods are in detail discussed under this 
framework. Moreover, with the guidance of 
our theoretical analysis, we propose a novel 
method called weighed frequency and odds 
(WFO) that combines the two measurements 
with trained weights. The experimental results 
on data sets from both topic-based and 
sentiment classification tasks show that this 
new method is robust across different tasks 
and numbers of selected features.  
1 Introduction 
With the rapid growth of online information, text 
classification, the task of assigning text 
documents to one or more predefined categories, 
has become one of the key tools for 
automatically handling and organizing text 
information. 
The problems of text classification normally 
involve the difficulty of extremely high 
dimensional feature space which sometimes 
makes learning algorithms intractable. A 
standard procedure to reduce the feature 
dimensionality is called feature selection (FS). 
Various FS methods, such as document 
frequency (DF), information gain (IG), mutual 
information (MI), 2? -test (CHI), Bi-Normal 
Separation (BNS), and weighted log-likelihood 
ratio (WLLR), have been proposed for the tasks 
(Yang and Pedersen, 1997; Nigam et al, 2000; 
Forman, 2003) and make text classification more 
efficient and accurate. 
However, comparing these FS methods 
appears to be difficult because they are usually 
based on different theories or measurements. For 
example, MI and IG are based on information 
theory, while CHI is mainly based on the 
measurements of statistic independence. 
Previous comparisons of these methods have 
mainly depended on empirical studies that are 
heavily affected by the experimental sets. As a 
result, conclusions from those studies are 
sometimes inconsistent. In order to better 
understand the relationship between these 
methods, building a general theoretical 
framework provides a fascinating perspective. 
Furthermore, in real applications, selecting an 
appropriate FS method remains hard for a new 
task because too many FS methods are available 
due to the long history of FS studies. For 
example, merely in an early survey paper 
(Sebastiani, 2002), eight methods are mentioned. 
These methods are provided by previous work 
for dealing with different text classification tasks 
but none of them is shown to be robust across 
different classification applications. 
In this paper, we propose a framework with 
two basic measurements for theoretical 
comparison of six FS methods which are widely 
used in text classification. Moreover, a novel 
method is set forth that combines the two 
measurements and tunes their influences 
considering different application domains and 
numbers of selected features. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work on 
692
feature selection for text classification. Section 3 
theoretically analyzes six FS methods and 
proposes a new FS approach. Experimental 
results are presented and analyzed in Section 4. 
Finally, Section 5 draws our conclusions and 
outlines the future work. 
2 Related Work 
FS is a basic problem in pattern recognition and 
has been a fertile field of research and 
development since the 1970s. It has been proven 
to be effective on removing irrelevant and 
redundant features, increasing efficiency in 
learning tasks, and improving learning 
performance. 
FS methods fall into two broad categories, the 
filter model and the wrapper model (John et al, 
1994). The wrapper model requires one 
predetermined learning algorithm in feature 
selection and uses its performance to evaluate 
and determine which features are selected. And 
the filter model relies on general characteristics 
of the training data to select some features 
without involving any specific learning 
algorithm. There is evidence that wrapper 
methods often perform better on small scale 
problems (John et al 1994), but on large scale 
problems, such as text classification, wrapper 
methods are shown to be impractical because of 
its high computational cost. Therefore, in text 
classification, filter methods using feature 
scoring metrics are popularly used. Below we 
review some recent studies of feature selection 
on both topic-based and sentiment classification. 
In the past decade, FS studies mainly focus on 
topic-based classification where the classification 
categories are related to the subject content, e.g., 
sport or education. Yang and Pedersen (1997) 
investigate five FS metrics and report that good 
FS methods improve the categorization accuracy 
with an aggressive feature removal using DF, IG, 
and CHI. More recently, Forman (2003) 
empirically compares twelve FS methods on 229 
text classification problem instances and 
proposes a new method called 'Bi-Normal 
Separation' (BNS). Their experimental results 
show that BNS can perform very well in the 
evaluation metrics of recall rate and F-measure. 
But for the metric of precision, it often loses to 
IG. Besides these two comparison studies, many 
others contribute to this topic (Yang and Liu, 
1999; Brank et al, 2002; Gabrilovich and 
Markovitch, 2004) and more and more new FS 
methods are generated, such as, Gini index 
(Shang et al, 2007), Distance to Transition Point 
(DTP) (Moyotl-Hernandez and Jimenez-Salazar, 
2005), Strong Class Information Words (SCIW) 
(Li and Zong, 2005) and parameter tuning based 
FS for Rocchio classifier (Moschitti, 2003). 
Recently, sentiment classification has become 
popular because of its wide applications (Pang et 
al., 2002). Its criterion of classification is the 
attitude expressed in the text (e.g., recommended 
or not recommended, positive or negative) rather 
than some facts (e.g., sport or education). To our 
best knowledge, yet no related work has focused 
on comparison studies of FS methods on this 
special task. There are only some scattered 
reports in their experimental studies. Riloff et al 
(2006) report that the traditional FS method 
(only using IG method) performs worse than the 
baseline in some cases. However, Cui et al 
(2006) present the experiments on the sentiment 
classification for large-scale online product 
reviews to show that using the FS method of CHI 
does not degrade the performance but can 
significantly reduce the dimension of the feature 
vector. 
Moreover, Ng et al (2006) examine the FS of 
the weighted log-likelihood ratio (WLLR) on the 
movie review dataset and achieves an accuracy 
of 87.1%, which is higher than the result reported 
by Pang and Lee (2004) with the same dataset. 
From the analysis above, we believe that the 
performance of the sentiment classification 
system is also dramatically affected by FS. 
3 Our Framework 
In the selection process, each feature (term, or 
single word) is assigned with a score according 
to a score-computing function. Then those with 
higher scores are selected. These mathematical 
definitions of the score-computing functions are 
often defined by some probabilities which are 
estimated by some statistic information in the 
documents across different categories. For the 
convenience of description, we give some 
notations of these probabilities below. 
( )P t : the probability that a document x  contains 
term t ; 
( )iP c : the probability that a document x  does 
not belong to category ic ; 
( , )iP t c : the joint probability that a document x  
contains term t  and also belongs to category ic ; 
( | )iP c t : the probability that a document x belongs 
to category ic ?under the condition that it contains  
term t. 
693
( | )iP t c : the probability that, a document x does 
not contain term t with the condition that x belongs to 
category ic ; 
Some other probabilities, such as ( )P t , ( )iP c , 
( | )iP t c , ( | )iP t c , ( | )iP c t ,  and ( | )iP c t , are 
similarly defined. 
In order to estimate these probabilities, 
statistical information from the training data is 
needed, and notations about the training data are 
given as follows: 
1{ }mi ic = : the set of categories; 
iA : the number of the documents that contain the 
term t  and also belong to category ic ; 
iB : the number of the documents that contain the 
term t  but do not belong to category ic ; 
iN : the total number of the documents that belong 
to category ic ; 
allN : the total number of all documents from the 
training data. 
iC : the number of the documents that do not 
contain the term t  but belong to category ic , i.e., 
i iN A?  
iD : the number of the documents that neither 
contain the term t  nor belong to category ic , i.e., 
all i iN N B? ? ; 
In this section, we would analyze theoretically 
six popular methods, namely DF, MI, IG, CHI, 
BNS, and WLLR. Although these six FS 
methods are defined differently with different 
scoring measurements, we believe that they are 
strongly related. In order to connect them, we 
define two basic measurements which are 
discussed as follows. 
The first measurement is to compute the 
document frequency in one category, i.e., iA .  
The second measurement is the ratio between 
the document frequencies in one category and 
the other categories, i.e., /i iA B . The terms with 
a high ratio are often referred to as the terms with 
high category information. 
These two measurements form the basis for all 
the measurements that are used by the FS 
methods throughout this paper. In particular, we 
show that DF and MI are using the first and 
second measurement respectively. Other 
complicated FS methods are combinations of 
these two measurements. Thus, we regard the 
two measurements as basic, which are referred to 
as the frequency measurement and ratio 
measurement. 
3.1 Document Frequency (DF) 
DF is the number of documents in which a term 
occurs. It is defined as 
1
( )m iiDF A==?  
The terms with low or high document 
frequency are often referred to as rare or 
common terms, respectively. It is easy to see that 
this FS method is based on the first basic 
measurement. It assumes that the terms with 
higher document frequency are more informative 
for classification. But sometimes this assumption 
does not make any sense, for example, the stop 
words (e.g., the, a, an) hold very high DF scores, 
but they seldom contribute to classification. In 
general, this simple method performs very well 
in some topic-based classification tasks (Yang 
and Pedersen, 1997). 
3.2 Mutual Information (MI) 
The mutual information between term t  and 
class ic  is defined as 
( | )( , ) log ( )
i
i
P t cI t c
P t
=  
And it is estimated as 
log ( )( )
i all
i i i i
A NMI
A C A B
?
=
+ +
 
Let us consider the following formula (using 
Bayes theorem) 
( | ) ( | )( , ) log log( ) ( )
i i
i
i
P t c P c tI t c
P t P c
= =  
Therefore, 
( , )= log ( | ) log ( )i i iI t c P c t P c?  
And it is estimated as 
log log
      log log
1
      log(1 ) log
/
i i
i i all
i i i
i all
i
i i all
A NMI
A B N
A B N
A N
N
A B N
= ?
+
+
= ? ?
= ? + ?
 
From this formula, we can see that the MI score 
is based on the second basic measurement. This 
method assumes that the term with higher 
category ratio is more effective for classification. 
It is reported that this method is biased 
towards low frequency terms and the bias 
becomes extreme when ( )P t  is near zero. It can 
be seen in the following formula (Yang and 
Pedersen, 1997)  
( , ) log( ( | )) log( ( ))i iI t c P t c P t= ?  
694
Therefore, this method might perform badly 
when common terms are informative for 
classification. 
Taking into account mutual information of all 
categories, two types of MI score are commonly 
used: the maximum score ( )
max
I t  and the 
average score ( )avgI t , i.e.,  
1( ) max { ( , )}mmax i iI t I t c== , 
1
( ) ( ) ( , )mavg i iiI t P c I t c== ?? .  
We choose the maximum score since it performs 
better than the average score (Yang and Pedersen, 
1997). It is worth noting that the same choice is 
made for other methods, including CHI, BNS, 
and WLLR in this paper. 
3.3 Information Gain (IG) 
IG measures the number of bits of information 
obtained for category prediction by recognizing 
the presence or absence of a term in a document 
(Yang and Pedersen, 1997). The function is 
1
1
1
( ) { ( ) log ( )}
            +{ ( )[ ( | ) log ( | )]
           ( )[ ( | ) log ( | )]}
m
i ii
m
i ii
m
i ii
G t P c P c
P t P c t P c t
P t P c t P c t
=
=
=
= ?
+
?
?
?
 
And it is estimated as 
1
1 1
1 1
{ log }
    +( / )[ log ]
  ( / )[ log ]
m i i
i
all all
m m i i
i alli i
i i i i
m m i i
i alli i
i i i i
N NIG
N N
A AA N
A B A B
C CC N
C D C D
=
= =
= =
= ?
+ +
+
+ +
?
? ?
? ?
From the definition, we know that the 
information gain is the weighted average of the 
mutual information ( , )iI t c and ( , )iI t c  where 
the weights are the joint probabilities ( , )iP t c and 
( , )iP t c : 
1 1
( ) ( , ) ( , ) ( , ) ( , )m mi i i ii iG t P t c I t c P t c I t c= == +? ?  
Since ( , )iP t c is closely related to the 
document frequency iA  and the mutual 
information ( , )iI t c  is shown to be based on the 
second measurement, we can say that the IG 
score is influenced by the two basic 
measurements. 
3.4 2?  Statistic (CHI) 
The CHI measurement (Yang and Pedersen, 
1997) is defined as 
2( )
( ) ( ) ( ) ( )
all i i i i
i i i i i i i i
N A D C BCHI
A C B D A B C D
? ?
=
+ ? + ? + ? +
 
In order to get the relationship between CHI 
and the two measurements, the above formula is 
rewritten as follows 
2[ ( ) ( ) ]
( ) ( ) [ ( )]
all i all i i i i i
i all i i i all i i
N A N N B N A BCHI
N N N A B N A B
? ? ? ? ?
=
? ? ? + ? ? +
  
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same 
( 2
all iN N= ). The CHI score then can be written 
as
 
2
2
2 ( )
( ) [2 ( )]
2 ( / 1)
      2( / 1) [ / ( / 1)]
i i i
i i i i i
i i i
i
i i i i i i
i
N A BCHI
A B N A B
N A B
NA B A B A B
A
?
=
+ ? ? +
?
=
+ ? ? ? +
 
From the above formula, we see that the CHI 
score is related to both the frequency 
measurement iA
 
and ratio measurement 
/i iA B . Also, when keeping the same ratio value, 
the terms with higher document frequencies will 
yield higher CHI scores. 
3.5 Bi-Normal Separation (BNS) 
BNS method is originally proposed by Forman 
(2003) and it is defined as 
1 1( , ) ( ( | )) ( ( | )i i iBNS t c F P t c F P t c? ?= ?  
It is calculated using the following formula 
1 1( ) ( )i i
i all i
A B
BNS F F
N N N
? ?
= ?
?
 
where ( )F x  is the cumulative probability 
function of standard normal distribution. 
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same, 
i.e., 2
all iN N=  and we also assume that i iA B> . 
It should be noted that this assumption is only to 
allow easier analysis but will not be applied in 
our experiment implementation. In addition, we 
only consider the case when / 0.5i iA N ? . In 
fact, most terms take the document frequency 
iA which is less than half of iN .  
Under these conditions, the BNS score can be 
shown in Figure 1 where the area of the shadow 
part represents ( / / )i i i iA N B N?  and the length 
of the projection to the x  axis represents the 
BNS score. 
695
From Figure 1, we can easily draw the two 
following conclusions: 
1) Given the same value of iA , the BNS score 
increases with the increase of i iA B? . 
2) Given the same value of i iA B? , BNS score 
increase with the decrease of iA . 
 
Figure 1. View of BNS using the normal probability 
distribution. Both the left and right graphs have 
shadowed areas of the same size. 
 
And the value of i iA B?  can be rewritten as 
the following 
1(1 )
/
i i
i i i i
i i i
A BA B A A
A A B
?
? = ? = ? ?  
The above analysis gives the following 
conclusions regarding the relationship between 
BNS and the two basic measurements: 
1) Given the same iA , the BNS score increases 
with the increase of /i iA B . 
2) Given the same /i iA B , when iA  increases, 
i iA B?  also increase. It seems that the BNS 
score does not show a clear relationship with 
iA . 
In summary, the BNS FS method is biased 
towards the terms with the high category ratio 
but cannot be said to be sensitive to document 
frequency. 
3.6 Weighted Log Likelihood Ratio 
(WLLR) 
WLLR method (Nigam et al, 2000) is defined as 
( | )( , ) ( | ) log ( | )
i
i i
i
P t cWLLR t c P t c
P t c
=  
And it is estimated as 
( )logi i all i
i i i
A A N NWLLR
N B N
? ?
=
?
 
The formula shows WLLR is proportional to 
the frequency measurement and the logarithm of 
the ratio measurement. Clearly, WLLR is biased 
towards the terms with both high category ratio 
and high document frequency and the frequency 
measurement seems to take a more important 
place than the ratio measurement. 
3.7 Weighed Frequency and Odds (WFO)  
So far in this section, we have shown that the 
two basic measurements constitute the six FS 
methods. The class prior probabilities, 
( ),  1,2,...,iP c i m= , are also related to the 
selection methods except for the two basic 
measurements. Since they are often estimated 
according to the distribution of the documents in 
the training data and are identical for all the 
terms in a class, we ignore the discussion of their 
influence on the selection measurements. In the 
experiment, we consider the case when training 
data have equal class prior probabilities. When 
training data are unbalanced, we need to change 
the forms of the two basic measurements to 
/i iA N  and ( ) / ( )i all i i iA N N B N? ? ? . 
Because some methods are expressed in 
complex forms, it is difficult to explain their 
relationship with the two basic measurements, 
for example, which one prefers the category ratio 
most. Instead, we will give the preference 
analysis in the experiment by analyzing the 
features in real applications. But the following 
two conclusions are drawn without doubt 
according to the theoretical analysis given above. 
1) Good features are features with high 
document frequency; 
2) Good features are features with high 
category ratio. 
These two conclusions are consistent with the 
original intuition. However, using any single one 
does not provide competence in selecting the 
best set of features. For example, stop words, 
such as ?a?, ?the? and ?as?, have very high 
document frequency but are useless for the 
classification. In real applications, we need to 
mix these two measurements to select good 
features. Because of different distribution of 
features in different domains, the importance of 
each measurement may differ a lot in different 
applications. Moreover, even in a given domain, 
when different numbers of features are to be 
selected, different combinations of the two 
measurements are required to provide the best 
performance. 
Although a great number of FS methods is 
available, none of them can appropriately change 
the preference of the two measurements. A better 
way is to tune the importance according to the 
application rather than to use a predetermined 
combination. Therefore, we propose a new FS 
method called Weighed Frequency and Odds 
(WFO), which is defined as 
696
 ( | ) / ( | ) 1i iwhen P t c P t c >  
1( | )( , ) ( | ) [log ]( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
                 ( , ) 0i
else
WFO t c =
 
And it is estimated as 
1( )( ) (log )i i all i
i i i
A A N NWFO
N B N
? ??? ?
=
?
 
where ?
 
is the parameter for tuning the weight 
between frequency and odds. The value of ?
 
varies from 0 to 1. By assigning different value 
to ?  we can adjust the preference of each 
measurement. Specially, when 0? = , the 
algorithm prefers the category ratio that is 
equivalent to the MI method; when 1? = , the 
algorithm is similar to DF; when 0.5? = , the 
algorithm is exactly the WLLR method. In real 
applications, a suitable parameter ?  needs to be 
learned by using training data. 
4 Experimental Studies  
4.1 Experimental Setup 
Data Set:  The experiments are carried out on 
both topic-based and sentiment text classification 
datasets. In topic-based text classification, we 
use two popular data sets: one subset of 
Reuters-21578 referred to as R2 and the 20 
Newsgroup dataset referred to as 20NG. In detail, 
R2 consist of about 2,000 2-category documents 
from standard corpus of Reuters-21578. And 
20NG is a collection of approximately 20,000 
20-category documents 1 . In sentiment text 
classification, we also use two data sets: one is 
the widely used Cornell movie-review dataset2 
(Pang and Lee, 2004) and one dataset from 
product reviews of domain DVD3 (Blitzer et al, 
2007). Both of them are 2-category tasks and 
each consists of 2,000 reviews. In our 
experiments, the document numbers of all data 
sets are (nearly) equally distributed cross all 
categories. 
Classification Algorithm: Many 
classification algorithms are available for text 
classification, such as Na?ve Bayes, Maximum 
Entropy, k-NN, and SVM. Among these methods, 
SVM is shown to perform better than other 
methods (Yang and Pedersen, 1997; Pang et al, 
                                                      
1
 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 
2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 
3
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
 
2002). Hence we apply SVM algorithm with the 
help of the LIBSVM 4  tool. Almost all 
parameters are set to their default values except 
the kernel function which is changed from a 
polynomial kernel function to a linear one 
because the linear one usually performs better for 
text classification tasks. 
Experiment Implementation: In the 
experiments, each dataset is randomly and 
evenly split into two subsets: 90% documents as 
the training data and the remaining 10% as 
testing data. The training data are used for 
training SVM classifiers, learning parameters in 
WFO method and selecting "good" features for 
each FS method. The features are single words 
with a bool weight (0 or 1), representing the 
presence or absence of a feature. In addition to 
the ?principled? FS methods, terms occurring in 
less than three documents ( 3DF ? ) in the 
training set are removed. 
4.2 Relationship between FS Methods and 
the Two Basic Measurements 
To help understand the relationship between FS 
methods and the two basic measurements, the 
empirical study is presented as follows. 
Since the methods of DF and MI only utilize 
the document frequency and category 
information respectively, we use the DF scores 
and MI scores to represent the information of the 
two basic measurements. Thus we would select 
the top-2% terms with each method and then 
investigate the distribution of their DF and MI 
scores.  
First of all, for clear comparison, we 
normalize the scores coming from all the 
methods using Min-Max normalization method 
which is designed to map a score s  to 's  in 
the range [0, 1] by computing 
'
s Min
s
Max Min
?
=
?
 
where
 
Min
 
and Max
 
denote the minimum 
and maximum values respectively in all terms? 
scores using one FS method. 
Table 1 shows the mean values of all top-2% 
terms? MI scores and DF scores of all the six FS 
methods in each domain. From this table, we can 
apparently see the relationship between each 
method and the two basic measurements. For 
instance, BNS most distinctly prefers the terms 
with high MI scores and low DF scores. 
According to the degree of this preference, we 
                                                      
4
 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
697
FS 
Methods 
Domain 
20NG R2 Movie DVD 
DF score MI score DF score MI score DF score MI score DF score MI score 
MI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881 
BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880 
CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676 
IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669 
WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481 
DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111 
 
Table 1. The mean values of all top-2% terms? MI and DF scores using six FS methods in each domain 
 
can rank these six methods as 
MI, BNS IG, CHI, WLLR DFf f , where x yf
 
means method x
 
prefers the terms with  
higher MI scores (higher category information) 
and lower DF scores (lower document frequency) 
than method y. This empirical discovery is in 
agreement with the finding that WLLR is biased 
towards the high frequency terms and also with 
the finding that BNS is biased towards high 
category information (cf. Section 3 theoretical 
analysis). Also, we can find that CHI and IG 
share a similar preference of these two 
measurements in 2-category domains, i.e., R2, 
movie, and DVD. This gives a good explanation 
that CHI and IG are two similar-performed 
methods for 2-category tasks, which have been 
found by Forman (2003) in their experimental 
studies. 
According to the preference, we roughly 
cluster FS methods into three groups. The first 
group includes the methods which dramatically 
prefer the category information, e.g., MI and 
BNS; the second one includes those which prefer 
both kinds of information, e.g., CHI, IG, and 
WLLR; and the third one includes those which 
strongly prefer frequency information, e.g., DF. 
4.3 Performances of Different FS Methods 
It is worth noting that learning parameters in 
WFO is very important for its good performance. 
We use 9-fold cross validation to help learning 
the parameter ?  so as to avoid over-fitting. 
Specifically, we run nine times by using every 8 
fold documents as a new training data set and the 
remaining one fold documents as a development 
data set. In each running with one fixed feature 
number m, we get the best 
,i m best? ? (i=1,..., 9) 
value through varying 
,i m?  from 0 to 1 with the 
step of 0.1 to get the best performance in the 
development data set. The average value 
m best? ? , 
i.e., 
9
,1
( ) / 9m best i m besti? ?? ?== ?  
is used for further testing. 
Figure 2 shows the experimental results when 
using all FS methods with different selected 
feature numbers. The red line with star tags 
represents the results of WFO. At the first glance, 
in R2 domain, the differences of performances 
across all are very noisy when the feature 
number is larger than 1,000, which makes the 
comparison meaningless. We think that this is 
because the performances themselves in this task 
are very high (nearly 98%) and the differences 
between two FS methods cannot be very large 
(less than one percent). Even this, WFO method 
do never get the worst performance and can also 
achieve the top performance in about half times, 
e.g., when feature numbers are 20, 50, 100, 500, 
3000. 
Let us pay more attention to the other three 
domains and discuss the results in the following 
two cases. 
In the first case when the feature number is 
low (about less than 1,000), the FS methods in 
the second group including IG, CHI, WLLR,  
always perform better than those in the other two 
groups. WFO can also perform well because its 
parameters 
m best? ?  are successfully learned to be 
around 0.5, which makes it consistently belong 
to the second group. Take 500 feature number 
for instance, the parameters 500 best? ?  are 0.42, 
0.50, and 0.34 in these three domains 
respectively. 
In the second case when the feature number is 
large, among the six traditional methods, MI and 
BNS take the leads in the domains of 20NG and 
Movie while IG and CHI seem to be better and 
more stable than others in the domain of DVD. 
As for WFO, its performances are excellent cross 
all these three domains and different feature 
numbers. In each domain, it performs similarly 
as or better than the top methods due to its 
well-learned parameters. For example, in 20NG, 
the parameters 
m best? ?  are 0.28, 0.20, 0.08, and 
0.01 when feature numbers are 10,000, 15,000, 
20,000, and 30,000. These values are close to 0 
698
(WFO equals MI when 0? = ) while MI is the 
top one in this domain. 
10 20 50 100 200 500 1000 2000 3000 4227
0.88
0.9
0.92
0.94
0.96
0.98
1
feature number
a
cc
u
ra
cy
Topic - R2
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
200 500 1000 2000 5000 10000 15000 20000 30000 32091
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
feature number
ac
cu
ra
cy
Topic - 20NG
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
50 200 500 1000 4000 7000 10000 13000 15176
0.55
0.6
0.65
0.7
0.75
0.8
0.85
feature number
ac
cu
ra
cy
Sentiment - Movie
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
20 50 100 500 1000 1500 2000 3000 4000 5824
0.5
0.55
0.6
0.65
0.7
0.75
0.8
feature number
ac
cu
ra
cy
Sentiment - DVD
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
Figure 2. The classification accuracies of the four domains 
using seven different FS methods while increasing the 
number of selected features. 
 
From Figure 2, we can also find that FS does 
help sentiment classification. At least, it can 
dramatically decrease the feature numbers 
without losing classification accuracies (see 
Movie domain, using only 500-4000 features is 
as good as using all 15176 features). 
5 Conclusion and Future Work 
In this paper, we propose a framework with two 
basic measurements and use it to theoretically 
analyze six FS methods. The differences among 
them mainly lie in how they use these two 
measurements. Moreover, with the guidance of 
the analysis, a novel method called WFO is 
proposed, which combine these two 
measurements with trained weights. The 
experimental results show that our framework 
helps us to better understand and compare 
different FS methods. Furthermore, the novel 
method WFO generated from the framework, can 
perform robustly across different domains and 
feature numbers. 
In our study, we use four data sets to test our 
new method. There are much more data sets on 
text categorization which can be used. In 
additional, we only focus on using balanced 
samples in each category to do the experiments. 
It is also necessary to compare the FS methods 
on some unbalanced data sets, which are 
common in real-life applications (Forman, 2003; 
Mladeni and Marko, 1999). These matters will 
be dealt with in the future work. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in the 
Hong Kong Polytechnic University. 
References  
J. Blitzer, M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain adaptation for sentiment 
classification. In Proceedings of ACL-07, the 45th 
Meeting of the Association for Computational 
Linguistics. 
J. Brank, M. Grobelnik, N. Milic-Frayling, and D. 
Mladenic. 2002. Interaction of feature selection 
methods and linear classification models. In 
Workshop on Text Learning held at ICML. 
H. Cui, V. Mittal, and M. Datar. 2006. Comparative 
experiments on sentiment classification for online 
product reviews. In Proceedings of AAAI-06, the 
21st National Conference on Artificial Intelligence. 
G. Forman. 2003. An extensive empirical study of 
feature selection metrics for text classification. The 
Journal of Machine Learning Research, 3(1): 
1289-1305. 
699
E. Gabrilovich and S. Markovitch. 2004. Text 
categorization with many redundant features: using 
aggressive feature selection to make SVMs 
competitive with C4.5. In Proceedings of the ICML, 
the 21st International Conference on Machine 
Learning. 
G. John, K. Ron, and K. Pfleger. 1994. Irrelevant 
features and the subset selection problem. In 
Proceedings of ICML-94, the 11st International 
Conference on Machine Learning.  
S. Li and C. Zong. 2005. A new approach to feature 
selection for text categorization. In Proceedings of 
the IEEE International Conference on Natural 
Language Processing and Knowledge Engineering 
(NLP-KE). 
D. Mladeni and G. Marko. 1999. Feature selection for 
unbalanced class distribution and naive bayes. In 
Proceedings of ICML-99, the 16th International 
Conference on Machine Learning. 
A. Moschitti. 2003. A study on optimal parameter 
tuning for Rocchio text classifier. In Proceedings 
of ECIR, Lecture Notes in Computer Science, 
vol. 2633, pp. 420-435. 
E. Moyotl-Hernandez and H. Jimenez-Salazar. 2005. 
Enhancement of DTP feature selection method for 
text categorization. In Proceedings of CICLing, 
Lecture Notes in Computer Science, vol.3406, 
pp.719-722.  
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. 
Examining the role of linguistic knowledge sources 
in the automatic identification and classification of 
reviews. In Proceedings of the COLING/ACL Main 
Conference Poster Sessions. 
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 
2000. Text classification from labeled and 
unlabeled documents using EM. Machine Learning, 
39(2/3): 103-134. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine 
learning techniques. In Proceedings of EMNLP-02, 
the Conference on Empirical Methods in Natural 
Language Processing. 
B. Pang and L. Lee. 2004. A sentimental education: 
Sentiment analysis using subjectivity 
summarization based on minimum cuts. In 
Proceedings of ACL-04, the 42nd Meeting of the 
Association for Computational Linguistics. 
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature 
subsumption for opinion analysis. In Proceedings 
of EMNLP-06, the Conference on Empirical 
Methods in Natural Language Processing,. 
F. Sebastiani. 2002. Machine learning in automated 
text categorization. ACM Computing Surveys, 
34(1): 1-47. 
W. Shang, H. Huang, H. Zhu, Y. Lin, Y. Qu, and Z. 
Wang. 2007. A novel feature selection algorithm 
for text categorization. The Journal of Expert 
System with Applications, 33:1-5. 
Y. Yang and J. Pedersen. 1997. A comparative study 
on feature selection in text categorization. In 
Proceedings of ICML-97, the 14th International 
Conference on Machine Learning. 
Y. Yang and X. Liu. 1999. A re-examination of text 
categorization methods. In Proceedings of 
SIGIR-99, the 22nd annual international ACM 
Conference on Research and Development in 
Information Retrieval.  
700
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179?187,
Beijing, August 2010
Emotion Cause Detection with Linguistic Constructions 
 Ying Chen*?, Sophia Yat Mei Lee?, Shoushan Li?, Chu-Ren Huang?  
*
 Dep. of Computer Engineering 
China Agricultural University 
?Dep. of Chinese and Bilingual Studies 
The Hong Kong Polytechnic University 
{chenying3176,sophiaym,shoushan.li,churenhuang}@gmail.com 
 
 
Abstract 
This paper proposes a multi-label ap-
proach to detect emotion causes. The 
multi-label model not only detects mul-
ti-clause causes, but also captures the 
long-distance information to facilitate 
emotion cause detection. In addition, 
based on the linguistic analysis, we cre-
ate two sets of linguistic patterns during 
feature extraction. Both manually gener-
alized patterns and automatically gener-
alized patterns are designed to extract 
general cause expressions or specific 
constructions for emotion causes. Ex-
periments show that our system 
achieves a performance much higher 
than a baseline model.   
1 Introduction 
Text-based emotion processing has been a cen-
ter of attention in the NLP field in the past few 
years. Most previous researches have focused 
on detecting the surface information of emo-
tions, especially emotion classes, e.g., ?happi-
ness? and ?anger? (Mihalcea and Liu 2006, 
Strapparava and Mihalcea 2008, Abbasi et al 
2008, Tokuhisa et al 2008). Although most 
emotion theories recognize the important role of 
causes in emotion analysis (Descartes, 1649; 
James, 1884; Plutchik 1980, Wierzbicka 1999), 
very few studies explore the interactions be-
tween emotion and causes. Emotion-cause in-
teraction is the eventive relation which poten-
tially yields the most crucial information in 
terms of information extraction. For instance, 
knowing the existence of an emotion is often 
insufficient to predict future events or decide on 
the best reaction. However, if the emotion cause 
is known in addition to the type of emotion, 
prediction of future events or assessment of po-
tential implications can be done more reliably. 
In other words, when emotion is treated as an 
event, causal relation is the pivotal relation to 
discover. In this paper, we explore one of the 
crucial deep level types of information of emo-
tion, i.e. cause events.  
Our study focuses on explicit emotions in 
which emotions are often presented by emotion 
keywords such as ?shocked? in ?He was 
shocked after hearing the news?. Emotion caus-
es are the explicitly expressed propositions that 
evoke the presence of the corresponding emo-
tions. They can be expressed by verbs, nomi-
nalizations, and nominals. Lee et al (2010a) 
explore the causes of explicit emotions by con-
structing a Chinese emotion cause corpus. 
Based on this corpus, we formalize the emotion 
cause detection problem through extensive data 
analysis. We find that ~14% emotion causes are 
complicated events containing multi-clauses, to 
which previous cause detection systems can 
hardly be applied directly. Most previous cause 
detection systems focus on the causal relation 
between a pair of small-size text units, such as 
clauses or phrases. They are thus not able to 
detect emotion causes that are multi-clauses. In 
this paper, we formalize emotion cause detec-
tion as a multi-label classification task (i.e. each 
instance may contain more than one label), 
which allows us to capture long-distance infor-
mation for emotion cause detection. 
In term of feature extraction, as emotion 
cause detection is a case of cause detection, 
some typical patterns used in existing cause de-
tection systems, e.g., ?because? and ?thus?, can 
be adopted. In addition, various linguistic cues 
are examined which potentially indicate emo-
tion causes, such as causative verbs and epis-
temic markers (Lee at al. 2010a). Then some 
linguistic patterns of emotion causes are manu-
179
ally generalized by examining the linguistic 
context of the empirical data (Lee et al, 2010b). 
It is expected that these manually generalized 
patterns often yield a low-coverage problem. 
Thus, we extracted features which enable us to 
automatically capture more emotion-specific 
constructions. Experiments show that such an 
integrated system with various linguistic fea-
tures performs promisingly well. We believe 
that the present study should provide the foun-
dation for future research on emotion analysis, 
such as the detection of implicit emotion or 
cause.  
The paper is organized as follows. Section 2 
discusses the related work on cause-effect de-
tection. Section 3 briefly describes the emotion 
cause corpus, and then presents our data analy-
sis. Section 4 introduces the multi-label classifi-
cation system for emotion cause detection. Sec-
tion 5 describes the two kinds of features for our 
system, one is based on hand-coded patterns and 
the other is the generalized features. Section 6 
presents the evaluation and performance of our 
system. Section 7 highlights our main contribu-
tions and the possible future work. 
2 Related Work 
Most previous studies on textual emotion proc-
essing focus on emotion recognition or classifi-
cation given a known emotion context (Mihal-
cea and Liu 2006, Strapparava and Mihalcea 
2008, Abbasi et al 2008, Tokuhisa et al 2008). 
However, the performance is far from satisfac-
tory. One crucial problem in these works is that 
they limit the emotion analysis to a simple clas-
sification and do not explore the underlying in-
formation regarding emotions. Most theories 
conclude that emotions are often invoked by the 
perception of external events. An effective emo-
tion recognition model should thus take this into 
account.  
To the best of our knowledge, little research 
has been done with respect to emotion cause 
detection. Lee et al (2010a) first investigate the 
interactions between emotions and the corre-
sponding causes from a linguistic perspective. 
They annotate a small-scale emotion cause cor-
pus, and identify six groups of linguistic cues 
facilitating emotion cause detection. Based on 
these findings, they develop a rule-based system 
for automatic emotion cause detection (Lee et 
al., 2010b).  
Emotion cause detection can be considered as 
a kind of causal relation detection, which has 
been intensively studied for years. Most previ-
ous cause detection studies focus on a specific 
domain, such as aviation (Persing and Ng, 2009) 
and finance (Low, et al, 2001). Few works 
(Marcu and Echihabi, 2002; Girju, 2003; Chang 
and Choi, 2005) examine causal relation for 
open domains. 
In recognizing causal relations, most existing 
systems involve two steps: 1) cause candidate 
identification; 2) causal relation detection. To 
simplify the task, most systems omit the step of 
identifying cause candidates. Instead, they often 
predefine or filter out possible causes based on 
domain knowledge, e.g., 14 kinds of cause types 
are identified for aviation incidents (Persing and 
Ng, 2009). For events without specific domain 
information, open-domain systems choose to 
limit their cause candidate. For example, the 
cause-effect pairs are limited to two noun 
phrases (Chang and Choi, 2005; Girju, 2003), or 
two clauses connected with fixed conjunction 
words (Marcu and Echihabi, 2002). 
Given pairs of cause-effect candidates, causal 
relation detection is considered as a binary clas-
sification problem, i.e. ?causal? vs. ?non-
causal?. In general, there are two kinds of in-
formation extracted to identify the causal rela-
tion. One is patterns or constructions expressing 
a cause-effect relation (Chang and Choi, 2005; 
Girju, 2003), and the other is semantic informa-
tion underlying in a text (Marcu and Echihabi, 
2002; Persing and Ng, 2009), such as word pair 
probability. Undoubtedly, the two kinds of in-
formation usually interact with each other in a 
real cause detection system. 
In the literature, the three common classifica-
tion methods, i.e. unsupervised, semi-supervised, 
and supervised, have all been used for cause 
detection systems. Marcu and Echihabi (2002) 
first collected a cause corpus using an unsuper-
vised approach with the help of several conjunc-
tion words, such as ?because? and ?thus?, and 
determined the causal relation for a clause pair 
using the word pair probability. Chang and Choi 
(2005) used a semi-supervised method to recur-
sively learn lexical patterns for cause recogni-
tion based on syntactic trees. Bethard and Mar-
tin (2008) put various causal information in a 
180
supervised classifier, such as the temporal in-
formation and syntactic information.  
For our emotion cause detection, several 
practical issues need to be investigated and re-
solved. First, for the identification of cause can-
didates, we need to define a reasonable span of 
a cause. Based on our data analysis, we find that 
emotion causes often appear across phrases or 
even clauses. Second, although in emotion 
cause detection the effect is fixed, the cause is 
open-domain. We also notice that besides the 
common patterns, emotion causes have their 
own expression patterns. An effective emotion 
cause detection system should take them into 
account. 
3 Corpus Analysis  
In this section, we briefly introduce the Chinese 
emotion cause corpus (Lee et al, 2010a), and 
discuss emotion cause distribution. 
3.1 Emotion Cause corpus 
Lee at al. (2010a) made the first attempt to ex-
plore the correlation between emotions and 
causes, and annotate a Chinese emotion cause 
corpus. The emotion cause corpus focuses on 
five primary emotions, namely ?happiness?, 
?sadness?, ?fear?, ?anger?, and ?surprise?. The 
emotions are explicitly expressed by emotion 
keywords, e.g., gao1xing4 ?happy?, shang1xin1 
?sad?, etc. The corpus is created as follows. 
1. 6,058 entries of Chinese sentences are ex-
tracted from the Academia Sinica Balanced 
Corpus of Mandarin Chinese (Sinica Cor-
pus) with the pattern-match method as well 
as the list of 91 Chinese primary emotion 
keywords (Chen et al, 2009). Each entry 
contains the focus sentence with the emo-
tion keyword ?<FocusSentence>? plus the 
sentence before ?<PrefixSentence>? and 
after ?<SuffixSentence>? it. For each entry, 
the emotion keywords are indexed since 
more than one emotion may be presented in 
an entry;  
2. Some preprocessing, such as balancing the 
number of entry among emotions, is done 
to remove some entries. Finally, 5,629 en-
tries remain; 
3. Each emotion keyword is annotated with 
its corresponding causes if existing. An 
emotion keyword can sometimes be associ-
ated with more than one cause, in such a 
case, both causes are marked. Moreover, 
the cause type is also identified, which is 
either a nominal event or a verbal event (a 
verb or a nominalization).  
Lee at al. (2010a) notice that 72% of the ex-
tracted entries express emotions, and 80% of the 
emotional entries have a cause. 
3.2 The Analysis of Emotion Causes 
To have a deeper understanding of emotion 
cause detection, we take a closer look at the 
emotion cause distribution, including the distri-
bution of emotion cause occurrence and the dis-
tribution of emotion cause text. 
 
The occurrence of emotion causes: According 
to most emotion theories, an emotion is gener-
ally invoked by an external event. The corpus 
shows that, however, 20% of the emotional en-
tries have no cause. Entries without causes ex-
plicitly expressed are mainly due to the follow-
ing reasons: 
i) There is not enough contextual information, 
for instance the previous or the suffix sentence 
is interjections, e.g., en heng ?aha?;  
ii) When the focus sentence is the beginning 
or the ending of a paragraph, no prefix sentence 
or suffix sentence can be extracted as the con-
text. In this case, the cause may be beyond the 
context;  
iii) The cause is obscure, which can be very 
abstract or even unknown reasons.  
 
The emotion cause text: A cause is considered 
as a proposition. It is generally assumed that a 
proposition has a verb which optionally takes a 
noun occurring before it as the subject and a 
noun after it as the object. However, a cause can 
also be expressed as a nominal. In other words, 
both the predicate and the two arguments are 
optional provided that at least one of them is 
present. Thus, the fundamental issue in design-
ing a cause detection system is the definition of 
the span of a cause text. As mentioned, most 
previous studies on causal relations choose to 
ignore the identification of cause candidates. In 
this paper, we first analyze the distribution of 
cause text and then determine the cause candi-
dates for an emotion. 
Based on the emotion cause corpus, we find 
that emotion causes are more likely to be ex-
181
pressed by verbal events than nominal events 
(85% vs. 15%). Although a nominalization (a 
kind of verbal events) is usually a noun phrase, 
a proposition containing a verb plays a salient 
role in the expressions of emotion causes, and 
thus a cause candidate are more likely to be a 
clause-based unit. 
In addition, the actual cause can sometimes 
be too long and complicated, which involves 
several events. In order to explore the span of a 
cause text, we do the following analysis. 
 
Table 1: The clause distribution of cause texts 
Position Cause (%) Position Cause (%) 
Left_0 12.90 Right _0 15.54 
Left_1 31.37 Right _1  9.55 
Left_2 13.31 Right_n  
(n>1) 
9.18 
Left_n 
(n>2) 
10.15   
Total  67.73  32.27 
 
Table 2: The multi-clause distribution of cause 
text 
Same clause % Cross-clauses % 
Left_0 16.80 Left_2_1_0 0.25 
Left_1 31.82 Left_2_1 10.84 
Left_2 7.33 Left_1_0 0.62 
Right _0 18.97 Right_0_1 2.55 
Right _1  10.59   
Total 85.75  14.25 
 
Firstly, for each emotion keyword, an entry is 
segmented into clauses with four punctuations 
(i.e. commas, periods, question marks and ex-
clamation marks), and thus an entry becomes a 
list of cause candidates. For example, when an 
entry has four clauses, its corresponding list of 
cause candidates contains five text units, i.e. 
<left_2, left_1, left_0, right_0, right_1>. If we 
assume the clause where emotion keyword lo-
cates is a focus clause, ?left_2? and ?left_1? are 
previous two clauses, and ?right_1? is the fol-
lowing one. ?left_0? and ?right_0? are the partial 
texts of the focus clause, which locate in the left 
side of and the right side of the emotion key-
word, respectively. Moreover, a cause candidate 
must contain either a noun or a verb because a 
cause is either a verbal event or a nominal event; 
otherwise, it will be removed from the list. 
Secondly, we calculate whether a cause can-
didate overlaps with the real cause, as shown in 
Table 1. We find that emotion causes are more 
likely to occur in the left of emotion keyword. 
This observation is consistent with the fact that 
an emotion is often trigged by an external hap-
pened event. Thirdly, for all causes occurring 
between ?left_2? and ?right_1?, we calculate 
whether a cause occurs across clauses, as in Ta-
ble 2. We observe that most causes locate 
within the same clause of the representation of 
the emotion (85.57%). This suggests that a 
clause may be the most appropriate unit to de-
tect a cause. 
 
4 Emotion Cause Detection Based on 
Multi-label Classification 
A cause detection system is to identify the caus-
al relation between a pair of two text units. For 
emotion cause detection, one of the two text 
units is fixed (i.e. the emotion keyword), and 
therefore the remaining two unresolved issues 
are the identification of the other text unit and 
the causal relation. 
From the above data analysis, there are two 
observations. First, most emotion causes are 
verbal events, which are often expressed by a 
proposition (or a clause). Thus, we define an-
other text unit as a clause, namely a cause can-
didate. Second, as most emotion causes occur 
between ?left_2? and ?right_1? (~80%), we de-
fine the cause candidates for an emotion as 
<left_2, left_1, left_0, right_0, right_1>.  
Differing from the existing cause systems, we 
formalize emotion cause detection as a multi-
label problem. In other words, given an emotion 
keyword and its context, its label is the loca-
tions of its causes, such as ?left_1, left_0?. This 
multi-label-based formalization of the cause 
detection task has two advantages. First, it is an 
integrated system detecting causes for an emo-
tion from the contextual information. In most 
previous cause detection systems, a causal rela-
tion is identified based on the information be-
tween two small text units, i.e. a pair of clauses 
or noun phrases, and therefore it is often the 
case that long-distance information is missed. 
Second, the multi-label-based tagging is able to 
182
capture the relationship between two cause can-
didates. For example, ?left_2? and ?left_1? are 
often combined as a complicated event as a 
cause.   
As a multi-label classification task, every 
multi-label classifier is applicable. In this study, 
we use a simple strategy: we treat each possible 
combination of labels appearing in the training 
data as a unique label. Note that an emotion 
without causes is labeled as ?None?. This con-
verts multi-label classification to single-label 
classification, which is suitable for any multi-
class classification technologies. In particular, 
we choose a Max Entropy tool, Mallet1, to per-
form the classification.  
5 Linguistic Features  
As explained, there are basically two kinds of 
features for cause detection, namely pattern-
based features and semantic-based features. In 
this study, we develop two sets of patterns 
based on linguistic analysis: one is a set of ma-
nually generalized patterns, and the other con-
tains automatically generalized patterns. All of 
these patterns explore causal constructions ei-
ther for general causal relations or for specific 
emotion cause relations. 
5.1 Linguistic Cues  
Based on the linguistic analysis, Lee et al 
(2010a) identify six groups of linguistic cue 
words that are highly collocated with emotion 
causes, as shown in Table 3. Each group of the 
linguistic cues serves as an indicator marking 
the causes in different emotional constructions. 
In this paper, these groups of linguistic cues are 
reinterpreted from the computational perspec-
tive, and are used to develop pattern-based fea-
tures for the emotion cause detection system.  
 
Table 3:  Linguistic cue words for emotion 
cause detection (Lee et al 2010a) 
Group Cue Words 
I: 
Prepositions 
?for? as in ?I will do this for you?: wei4, 
wei4le 
?for? as in ?He is too old for the job?: 
dui4, dui4yu2 
?as?: yi3 
                                                 
1
 http://mallet.cs.umass.edu/ 
II: 
Conjunctions 
?because?: yin1, yin1wei4, you2yu2 
?so?: yu1shi4, suo3yi3, yin1er2 
?but?: ke3shi4 
III:  
Light Verbs ?to make?: rang4, ling4, shi3 
IV: 
Reported 
Verbs 
?to think about?: xiang3dao4, 
xiang3qi3, yi1xiang3, xiang3 lai2 
?to talk about?: shuo1dao4, shuo1qi3, 
yi1shuo1, jiang3dao4, jiang3qi3, 
yi1jiang3, tan2dao4, tan2qi3, yi1tan2, 
ti2dao4, ti2qi3, yi1ti2 
V: 
Epistemic 
Markers 
?to hear?: ting1, ting1dao4, ting1shuo1 
?to see?: kan4, kan4dao4, kan4jian4, 
jian4dao4, jian4, yan3kan4, qiao2jian4 
?to know?: zhi1dao4, de2zhi1, de2xi1, 
huo4zhi1, huo4xi1, fa1xian4, fa1jue2 
?to exist?: you3 
VI: 
Others 
?is?: deshi4 
?say?: deshuo1 
?at?: yu2 
?can?: neng2  
 
For emotion cause processing, Group I and II 
contain cues which are for general cause detec-
tion, and while Group III, IV and V include 
cues specifically for emotion cause detection. 
Group VI includes other linguistic cues that do 
not fall into any of the five groups.  
Group I covers some prepositions which all 
roughly mean ?for?, and Group II contains the 
conjunctions that explicitly mark the emotion 
cause. Group I is expected to capture the prepo-
sitions constructions in the focus clause where 
the emotion keyword locates. Group II tends to 
capture the rhetorical relation expressed by con-
junction words so as to infer causal relation 
among multi-clauses. These two groups are typ-
ical features for general cause detection. 
Group III includes three common light verbs 
which correspond to the English equivalents ?to 
make? or ?to cause?. Although these light verbs 
themselves do not convey any concrete meaning, 
they are often associated with several construc-
tions to express emotions and at the same time 
indicate the position of emotion causes. For ex-
ample, ?The birthday party made her happy?.  
One apparent difference between emotion 
causes and general causes is that emotions are 
often triggered by human activities or the per-
ception of such activities, e.g., ?glad to say? or 
?glad to hear?. Those human activities are often 
strong indicators for the location of emotion 
183
causes. Group IV and V are used to capture this 
kind of information. Group IV is a list of verbs 
of thinking and talking, and Group V includes 
four types of epistemic markers which are usu-
ally verbs marking the cognitive awareness of 
emotions in the complement position. The epis-
temic markers include verbs of seeing, hearing, 
knowing, and existing. 
  
5.2 Linguistic Patterns  
With the six groups of linguistic cues, we gen-
eralize 14 rules used in Lee et al (2010b) to 
locate the clause positions of an emotion cause, 
as shown in Table 4. The abbreviations used in 
the rules are given as follows:  
 
C = Cause 
K = Emotion keyword 
B = Clauses before the focus clause 
F = Focus clause/the clause containing the emotion 
verb 
A = Clauses after the focus clause 
 
Table 4: Linguistic rules for emotion cause de-
tection (Lee et al 2010b) 
No. Rules 
1 i) C(B/F) + III(F)  + K(F)  
ii) C = the nearest N/V before I in F/B 
2 i)  IV/V/I/II(B/F) + C(B/F) + K(F)  
ii) C = the nearest N/V before K in F 
3 i) I/II/IV/V (B) + C(B)  + K(F)  
ii) C = the nearest N/V after I/II/IV/V in B 
4 i) K(F) + V/VI(F) + C(F/A)  
ii) C = the nearest N/V after V/VI in F/A 
5 i) K(F)+II(A)+C(A)  
ii) C = the nearest N/V after II in A 
6 i) III(F) + K(F) + C(F/A)  
ii) C = the nearest N/V after K in F or A 
7 i) yue4 C yue4 K ?the more C the more K? (F)   
ii) C = the V in between the two yue4?s in F 
8 i) K(F) + C(F)  
ii) C = the nearest N/V after K in F 
9 i) V(F) + K(F)  
ii) C = V+(an aspectual marker) in F 
10 i) K(F)  + de ?possession?(F) + C(F)  
ii) C = the nearest N/V +?+N after de in F 
12 i) K(B) + IV (B) + C(F)   
ii) C = the nearest N/V after IV in F 
13 i) IV(B) + C(B) + K(F)  
ii) C = the nearest N/V after IV in B 
14 i) C(B) +  K(F)  
ii) C = the nearest N/V before K in B  
 
For illustration, an example of the rule descrip-
tion is given in Rule 1. 
Rule 1: 
i) C(B/F) + III(F) + K(F)  
ii) C = the nearest N/V before III in F/B  
 
Rule 1 indicates that the cause (C) comes before 
Group III cue words. Theoretically, in identify-
ing C, we look for the nearest verb/noun occur-
ring before Group III cue words in the focus 
clause (F) or the clauses before the focus clause 
(B), and consider the clause containing this 
verb/noun as a cause. Practically, for each cause 
candidate, i.e. ?left_1?, if it contains this 
verb/noun, we create a feature with 
?left_1_rule_1=1?. 
5.3 Generalized Patterns  
Rule-based patterns usually achieve a rather 
high accuracy, but suffer from low coverage. To 
avoid this shortcoming, we extract a generalized 
feature automatically according to the rules in 
Table 4. The features are able to detect two 
kinds of constructions, namely functional con-
structions, i.e. rhetorical constructions, and spe-
cific constructions for emotion causes.  
Local functional constructions: a cause occur-
ring in the focus clause is often expressed with 
certain functional words, such as ?because of?, 
?due to?. In order to capture the various expres-
sions of these functional constructions, we iden-
tify all functional words around the given emo-
tion keyword. For an emotion keyword, we 
search ?left_0? from the right until a noun or a 
verb is found. Next, all unigrams and bigrams 
between the noun or the verb and the emotion 
keyword are extracted. The same applies to 
?right_0?. 
Long-distance conjunction constructions: 
Group II enumerates only some typical conjunc-
tion words. To capture more general rhetorical 
relations, according to the given POS tags, the 
conjunction word is extracted for each cause 
candidate, if it occurs at the beginning of the 
candidate. 
Generalized action and epistemic verbs: 
Group IV and V cover only partial action and 
epistemic verbs. To capture possible related ex-
pressions, we take the advantage of Chinese 
characters. In Chinese, each character itself usu-
ally has a meaning and some characters have a 
strong capability to create words with extended 
meaning. For example, the character ?ting1-
listen? combines with other characters to create 
184
words expressing ?listening?, such as ting1jian4, 
ting1wen5. With the selected characters regard-
ing reported verbs and epistemic markers, each 
cause candidate is checked to see whether it 
contains the predefined characters.  
6 Experiments 
For the emotion cause corpus, we reserve 80% 
as the training data, 10% as the development 
data, and 10% as the test data. During evalua-
tion, we first convert the multi-label tag output-
ted from our system into a binary tag (?Y? 
means the presence of a causal relation; ?N? in-
dicates the absence of a causal relation) between 
the emotion keyword and each candidate in its 
corresponding cause candidates. Thus, the 
evaluation scores for binary classification based 
on three common measures, i.e. precision, recall 
and F-score, are chosen. 
6.1 Linguistic Feature Analysis 
According to the distribution in Table 1, we de-
sign a naive baseline to allow feature analysis. 
The baseline searches for the cause candidates 
in the order of <left_1, right_0, left_2, left_0, 
right_1>. If the candidate contains a noun or 
verb, consider this clause as a cause and stop. 
We run the multi-label system with different 
groups of features and the performances are 
shown in Table 5. The feature set begins with 
linguistic patterns (LP), and is then incorporated 
with local functional constructions (LFC), long-
distance conjunction constructions (LCC), and 
generalized action and epistemic verbs (GAE), 
one by one. Since the ?N? tag is overwhelming, 
we report only the Mac average scores for both 
?Y? and ?N? tags.  
In Table 5, we first notice that the perform-
ances achieve significant improvement from the 
baseline to the final system (~17%). This indi-
cates that our linguistic features are effective for 
emotion cause detection. In addition, we ob-
serve that LP and LFC are the best two effective 
features, whereas LCC and GAE have slight 
contributions. This shows that our feature ex-
traction has a strong capability to detect local 
causal constructions, and is yet unable to detect 
the long-distance or semantic causal informa-
tion. Here, ?local? refers to the information in 
the focus clause. We also find that incorporating 
LFC, which is a pure local feature, generally 
improves the performances of all cause candi-
dates, i.e. ~5% improvement for ?left_1?. This 
indicates that our multi-label integrated system 
is able to convey information among cause can-
didates.  
 
Table 5: The overall performance with different 
feature sets of the multi-label system 
 Precision Recall F-score 
Baseline 56.64 57.70 56.96 
LP 74.92 66.70 69.21 
+ LFC 72.80 71.94 72.35 
+ LCC 73.60 72.50 73.02 
+ GAE 73.90 72.70 73.26 
 
Table 6: The separate performances for ?Y? and 
?N? tags of the multi-label system 
 ?Y? ?N? 
Baseline 33.06 80.85 
LP 48.32 90.11 
+ LFC 55.45 89.24 
+ LCC 56.48 89.57 
+ GPE 56.84 89.68 
 
Table 6 shows the performances (F-scores) 
for ?Y? and ?N? tags separately. First, we notice 
that the performances of the ?N? tag are much 
better than the ones of ?Y? tag. Second, it is sur-
prising that incorporating the linguistic features 
significantly improves only the ?Y? tag (from 
33% to 56%), but does not affect ?N? tag. This 
suggests that our linguistic features are effective 
to detect the presence of causal relation, and yet 
do not hurt the detection of ?non_causal? rela-
tion. For the ?Y? tag, the features LP and LFC 
achieve ~15% and ~7% improvements respec-
tively. LCC and GPE, on the other hand, show 
slight improvements only. 
Finally, Table 7 shows the detailed perform-
ances of our multi-label system with all features. 
The last row shows the overall performances of 
?Y? and ?N? tags. For the ?Y? tag, the closer the 
cause candidates are to the emotion keyword, 
the better performances the system achieves. 
This proves that the features we propose effec-
tively detect local emotion causes, more effort, 
185
Table 7: The detailed performance for the multi-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 68.92 68.92 68.92 Left_0 93.72 93.72 93.72 
Left_1 57.63 63.35 60.36 Left_1 82.90 79.22 81.02 
Left_2 29.27 20.69 24.24 Left_2 89.23 92.93 91.04 
Right_0 67.78 64.89 66.30 Right_0 82.63 84.41 83.51 
Right_1 54.84 30.91 39.54 Right_1 92.00 96.90 94.38 
Total 58.84 54.98 56.84 Total 88.96 90.42 89.68 
 
Table 8: The detailed performance for the single-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 65.39  68.92 67.11 Left_0 93.65  92.62 93.13 
Left_1 61.19  50.93 55.59 Left_1 79.64   85.60 82.51 
Left_2 28.57   20.69 24.00 Left_2 89.20   92.68 90.91 
Right_0 70.13   57.45 63.16 Right_0 80.30  87.63 83.81 
Right_1 33.33   40.00 36.36 Right_1 92.50   90.24 91.36 
Total 55.67   50.00 52.68 Total 87.85  90.08 88.95 
 
however, should be put on the detection of 
long-distance causes. In addition, we find that 
the detection of long-distance causes usually 
relies on two kinds of information for inference: 
rhetorical relation and deep semantic informa-
tion. 
6.2 Modeling Analysis 
To compare our multi-label model with single-
label models, we create a single-label system as 
follows. The single-label model is a binary 
classification for a pair comprising the emotion 
keyword and a candidate in its corresponding 
cause candidates. For each pair, all linguistic 
features are extracted only from the focus 
clause and its corresponding cause candidate. 
Note that we only use the features in the focus 
clause for ?left_0? and ?right_0?. The perform-
ances are shown in Table 8. 
Comparing Tables 7 and 8, all F-scores of 
the ?Y? tag increase and the performances of 
the ?N? tag remain almost the same for both the 
single-label model and our multi-label model. 
We also find that the multi-label model takes 
more advantage of local information, and im-
proves the performances, particularly for 
?left_1?.  
To take an in-depth analysis of the cause de-
tection capability of the multi-label model, an 
evaluation is designed that the label is treated 
as a tag from the multi-label classifier. Due to 
the tag sparseness problem (as in Table 2), only 
the ?left_2, left_1? tag is detected in the test 
data, and its performance is 21% precision, 
26% recall and 23% F-score. Furthermore, we 
notice that ~18% of the ?left_1? tags are de-
tected through this combination tag. This 
shows that some causes need to take into ac-
count the mutual information between clauses. 
Although the scores are low, it still shows that 
our multi-label model provides an effective 
way of detecting some of the multi-clauses 
causes. 
7 Conclusion 
We treat emotion cause detection as a multi-
label task, and develop two sets of linguistic 
features for emotion cause detection based on 
linguistic cues. The experiments on the small-
scale corpus show that both the multi-label 
model and the linguistic features are able to 
effectively detect emotion causes. The auto-
matic detection of emotion cause will in turn 
allow us to extract directly relevant information 
for public opinion mining and event prediction. 
It can also be used to improve emotion detec-
tion and classification. In the future, we will 
attempt to improve our system from two as-
pects. On the one hand, we will explore more 
powerful multi-label classification models for 
our system. On the other hand, we will investi-
gate more linguistic patterns or semantic in-
formation to further help emotion cause detec-
tion. 
186
References 
Abbasi, A., H. Chen, S. Thoms, and T. Fu. 2008. 
Affect Analysis of Web Forums and Blogs using 
Correlation Ensembles?. In IEEE Tran. Knowl-
edge and Data Engineering, vol. 20(9), pp. 1168-
1180. 
Bethard, S. and J. Martin. 2008. Learning Semantic 
Links from a Corpus of Parallel Temporal and 
Causal Relations. In Proceedings of ACL. 
Descartes, R. 1649. The Passions of the Soul. In J. 
Cottingham et al (Eds), The Philosophical Writ-
ings of Descartes. Vol. 1: 325-404. 
Chang, D.-S. and K.-S. Choi. 2006. Incremental cue 
phrase learning and bootstrapping method for 
causality extraction using cue phrase and word 
pair probabilities. Information Processing and 
Management. 42(3): 662-678. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. Are 
Emotions Enumerable or Decomposable? And 
Its Implications for Emotion Processing. In Pro-
ceedings of the 23rd Pacific Asia Conference on 
Language, Information and Computation. 
Girju, R. 2003. Automatic Detection of Causal Re-
lations for Question Answering. In the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics, Workshop on Multilingual 
Summarization and Question Answering - Ma-
chine Learning and Beyond, Sapporo, Japan. 
James, W. 1884. What is an Emotion? Mind, 
9(34):188?205. 
Lee, S. Y. M., Y. Chen and C.-R. Huang. 2010a. A 
Text-driven Rule-based System for Emotion 
Cause Detection. In Proceedings of NAACL-HLT 
2010 Workshop on Computational Approaches to 
Analysis and Generation of Emotion in Text. 
Lee, S. Y. M., Y. Chen, S. Li and C.-R. Huang. 
2010b. Emotion Cause Events: Corpus Construc-
tion and Analysis. In Proceedings of LREC 2010. 
Low, B. T., K. Chan , L. L. Choi , M. Y. Chin , S. L. 
Lay. 2001. Semantic Expectation-Based Causa-
tion Knowledge Extraction: A Study on Hong 
Kong Stock Movement Analysis, In Proceedings 
of the 5th Pacific-Asia Conference on Knowledge 
Discovery and Data Mining, p.114-123, April 
16-18.  
Marcu, D., and A. Echihabi. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of ACL. 
Mihalcea, R. and H. Liu. 2006. A Corpus-based 
Approach to Finding Happiness. In Proceedings 
of the AAAI Spring Symposium on Computational 
Approaches to Weblogs.  
Persing, I. and V. Ng. 2009. Semi-Supervised Cause 
Identification from Aviation Safety Reports. In 
Proceedings of ACL. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Strapparava, C. and R. Mihalcea. 2008. Learning to 
Identify Emotions in Text. In Proceedings of the 
ACM Conference on Applied Computing ACM-
SAC. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. 
Emotion recognition Using Massive Examples 
Extracted from the Web. In Proceedings of COL-
ING. 
Wierzbicka, A. 1999. Emotions across Languages 
and Cultures: Diversity and Universals. Cam-
bridge: Cambridge University Press. 
 
 
 
187
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 635?643,
Beijing, August 2010
Sentiment Classification and Polarity Shifting 
 
Shoushan Li??  Sophia Yat Mei Lee?  Ying Chen?  Chu-Ren Huang?  Guodong Zhou?  
 
?Department of CBS 
The Hong Kong Polytechnic University 
{shoushan.li, sophiaym, 
chenying3176, churenhuang} 
@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and 
Technology 
      Soochow University
gdzhou@suda.edu.cn 
 
  
Abstract 
Polarity shifting marked by various 
linguistic structures has been a challenge 
to automatic sentiment classification. In 
this paper, we propose a machine learning 
approach to incorporate polarity shifting 
information into a document-level 
sentiment classification system. First, a 
feature selection method is adopted to 
automatically generate the training data 
for a binary classifier on polarity shifting 
detection of sentences. Then, by using the 
obtained binary classifier, each document 
in the original polarity classification 
training data is split into two partitions, 
polarity-shifted and polarity-unshifted, 
which are used to train two base 
classifiers respectively for further 
classifier combination. The experimental 
results across four different domains 
demonstrate the effectiveness of our 
approach. 
1 Introduction 
Sentiment classification is a special task of text 
classification whose objective is to classify a text 
according to the sentimental polarities of 
opinions it contains (Pang et al, 2002), e.g., 
favorable or unfavorable, positive or negative. 
This task has received considerable interests in 
the computational linguistic community due to its 
potential applications.  
In the literature, machine learning approaches 
have dominated the research in sentiment 
classification and achieved the state-of-the-art 
performance (e.g., Kennedy and Inkpen, 2006; 
Pang et al, 2002). In a typical machine learning 
approach, a document (text) is modeled as a 
bag-of-words, i.e. a set of content words without 
any word order or syntactic relation information. 
In other words, the underlying assumption is that 
the sentimental orientation of the whole text 
depends on the sum of the sentimental polarities 
of content words. Although this assumption is 
reasonable and has led to initial success, it is 
linguistically unsound since many function 
words and constructions can shift the 
sentimental polarities of a text. For example, in 
the sentence ?The chair is not comfortable?, the 
polarity of the word ?comfortable? is positive 
while the polarity of the whole sentence is 
reversed because of the negation word ?not?. 
Therefore, the overall sentiment of a document is 
not necessarily the sum of the content parts 
(Turney, 2002). This phenomenon is one main 
reason why machine learning approaches fail 
under some circumstances. 
As a typical case of polarity shifting, negation 
has been paid close attention and widely studied 
in the literature (Na et al, 2004; Wilson et al, 
2009; Kennedy and Inkpen, 2006). Generally, 
there are two steps to incorporate negation 
information into a system: negation detection 
and negation classification. For negation 
detection, some negation trigger words, such as 
?no?, ?not?, and ?never?, are usually applied to 
recognize negation phrases or sentences. As for 
negation classification, one way to import 
negation information is to directly reverse the 
polarity of the words which contain negation 
trigger words as far as term-counting approaches 
are considered (Kennedy and Inkpen, 2006). An 
alternative way is to add some negation features 
(e.g., negation bigrams or negation phrases) into 
635
machine learning approaches (Na et al, 2004). 
Such approaches have achieved certain success.  
There are, however, some shortcomings with 
current approaches in incorporating negation 
information. In terms of negation detection, 
firstly, the negation trigger word dictionary is 
either manually constructed or relies on existing 
resources. This leads to certain limitations 
concerning the quality and coverage of the 
dictionary. Secondly, it is difficult to adapt 
negation detection to other languages due to its 
language dependence nature of negation 
constructions and words. Thirdly, apart from 
negation, many other phenomena, e.g., contrast 
transition with trigger words like ?but?, 
?however?, and ?nevertheless?, can shift the 
sentimental polarity of a phrase or sentence. 
Therefore, considering negation alone is 
inadequate to deal with the polarity shifting 
problem, especially for document-level 
sentiment classification. 
In terms of negation classification, although it 
is easy for term-counting approaches to integrate 
negation information, they rarely outperform a 
machine learning baseline (Kennedy and Inkpen, 
2006). Even for machine learning approaches, 
although negation information is sometimes 
effective for local cases (e.g., not good), it fails 
on long-distance cases (e.g., I don?t think it is 
good). 
In this paper, we first propose a feature 
selection method to automatically generate a 
large scale polarity shifting training data for 
polarity shifting detection of sentences. Then, a 
classifier combination method is presented for 
incorporating polarity shifting information. 
Compared with previous ones, our approach 
highlights the following advantages?First of all, 
we apply a binary classifier to detect polarity 
shifting rather than merely relying on trigger 
words or phrases. This enables our approach to 
handle different kinds of polarity shifting 
phenomena. More importantly, a feature 
selection method is presented to automatically 
generate the labeled training data for polarity 
shifting detection of sentences. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
approach in details. Experimental results are 
presented and analyzed in Section 4. Finally, 
Section 5 draws the conclusion and outlines the 
future work. 
2 Related Work 
Generally, sentiment classification can be 
performed at four different levels: word level 
(Wiebe, 2000), phrase level (Wilson et al, 2009), 
sentence level (Kim and Hovy, 2004; Liu et al, 
2005), and document level (Turney, 2002; Pang 
et al, 2002; Pang and Lee, 2004; Riloff et al, 
2006). This paper focuses on document-level 
sentiment classification. 
In the literature, there are mainly two kinds of 
approaches on document-level sentiment 
classification: term-counting approaches 
(lexicon-based) and machine learning 
approaches (corpus-based). Term-counting 
approaches usually involve deriving a sentiment 
measure by calculating the total number of 
negative and positive terms (Turney, 2002; Kim 
and Hovy, 2004; Kennedy and Inkpen, 2006). 
Machine learning approaches recast the 
sentiment classification problem as a statistical 
classification task (Pang and Lee, 2004). 
Compared to term-counting approaches, 
machine learning approaches usually achieve 
much better performance (Pang et al, 2002; 
Kennedy and Inkpen, 2006), and have been 
adopted to more complicated scenarios, such as 
domain adaptation (Blitzer et al, 2007), 
multi-domain learning (Li and Zong, 2008) and 
semi-supervised learning (Wan, 2009; Dasgupta 
and Ng, 2009) for sentiment classification. 
Polarity shifting plays a crucial role in 
phrase-level, sentence-level, and document-level 
sentiment classification. However, most of 
previous studies merely focus on negation 
shifting (polarity shifting caused by the negation 
structure). As one pioneer research on sentiment 
classification, Pang et al (2002) propose a 
machine learning approach to tackle negation 
shifting by adding the tag ?not? to every word 
between a negation trigger word/phrase (e.g., not, 
isn't, didn't, etc.) and the first punctuation mark 
following the negation trigger word/phrase. To 
their disappointment, considering negation 
shifting has a negligible effect and even slightly 
harms the overall performance. Kennedy and 
Inkpen (2006) explore negation shifting by 
incorporating negation bigrams as additional 
features into machine learning approaches. The 
636
experimental results show that considering 
sentiment shifting greatly improves the 
performance of term-counting approaches but 
only slightly improves the performance of 
machine learning approaches. Other studies such 
as Na et al (2004), Ding et al (2008), and Wilson 
et al (2009) also explore negation shifting and 
achieve some improvements1. Nonetheless, as far 
as machine learning approaches are concerned, 
the improvement is rather insignificant (normally 
less than 1%). More recently, Ikeda et al (2008) 
first propose a machine learning approach to 
detect polarity shifting for sentence-level 
sentiment classification, based on a 
manually-constructed dictionary containing 
thousands of positive and negative sentimental 
words, and then adopt a term-counting approach 
to incorporate polarity shifting information. 
3 Sentiment Classification with Polarity 
Shifting Detection 
 
 
Figure 1: General framework of our approach 
 
The motivation of our approach is to improve the 
performance of sentiment classification by robust 
treatment of sentiment polarity shifting between 
sentences. With the help of a binary classifier, the 
sentences in a document are divided into two 
parts: sentences which contain polarity shifting 
structures and sentences without any polarity 
shifting structure. Figure 1 illustrates the general 
framework of our approach. Note that this 
framework is a general one, that is, different 
polarity shifting detection methods can be applied 
to differentiate polarity-shifted sentences from 
those polarity-unshifted sentences and different 
                                                      
1
 Note that Ding et al (2006) also consider but-clause, another 
important structure for sentiment shifting. Wilson et al (2009) use 
conjunctive and dependency relations among polarity words. 
polarity classification methods can be adopted to 
incorporate sentiment shifting information. For 
clarification, the training data used for polarity 
shifting detection and polarity classification are 
referred to as the polarity shifting training data 
and the polarity classification training data, 
respectively. 
3.1 Polarity Shifting Detection 
In this paper, polarity shifting means that the 
polarity of a sentence is different from the 
polarity expressed by the sum of the content 
words in the sentence. For example, in the 
sentence ?I am not disappointed?, the negation 
structure makes the polarity of the word 
'disappointed' different from that of the whole 
sentence (negative vs. positive). Apart from the 
negation structure, many other linguistic 
structures allow polarity shifting, such as 
contrast transition, modals, and 
pre-suppositional items (Polanyi and Zaenen, 
2006). We refer these structures as polarity 
shifting structures. 
One of the great challenges in building a 
polarity shifting detector lies on the lack of 
relevant training data since manually creating a 
large scale corpus of polarity shifting sentences 
is time-consuming and labor-intensive. Ikeda et 
al. (2008) propose an automatic way for 
collecting the polarity shifting training data 
based on a manually-constructed large-scale 
dictionary. Instead, we adopt a feature selection 
method to build a large scale training corpus of 
polarity shifting sentences, given only the 
already available document-level polarity 
classification training data. With the help of the 
feature selection method, the top-ranked word 
features with strong sentimental polarity 
orientation, e.g., ?great?, ?love?, ?worst? are first 
chosen as the polarity trigger words. Then, those 
sentences with the top-ranked polarity trigger 
words in both categories of positive and negative 
documents are selected. Finally, those candidate 
sentences taking opposite-polarity compared to 
the containing trigger word are deemed as 
polarity-shifted. 
The basic idea of automatically generating the 
polarity shifting training data is based on the 
assumption that the real polarity of a word or 
phrase is decided by the major polarity category 
where the word or phrase appears more often. As 
a result, the sentences in the 
Polarity Shifting 
Detector 
Documents 
 
Polarity-shifted 
Sentences 
Polarity-unshifted 
Sentences 
Polarity Classifier Positive/Negative 
637
frequently-occurring category would be seen as 
polarity-unshifted while the sentences in the 
infrequently-occurring category would be seen 
as polarity-shifted. 
In the literature, various feature selection 
methods, such as Mutual Information (MI), 
Information Gain (IG) and Bi-Normal Separation 
(BNS) (Yang and Pedersen, 1997; Forman 2003), 
have been employed to cope with the problem of 
the high-dimensional feature space which is 
normal in sentiment classification.  
In this paper, we employ the theoretical 
framework, proposed by Li et al (2009), 
including two basic measurements, i.e. frequency 
measurement and ratio measurement, where the 
first measures, the document frequency of a term 
in one category, and the second measures, the 
ratio between the document frequency in one 
category and other categories. In particular, a 
novel method called Weighed Frequency and 
Odds (WFO) is proposed to incorporate both 
basic measurements: 
1( | )( , ) ( | ) {max(0, log )}( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
where ( | )iP t c  denotes the probability that a 
document x contains the term t with the 
condition that x belongs to category ic ; 
( | )iP t c  denotes the probability that a document 
x contains the term t with the condition that x 
does not belong to category ic . The left part of 
the formula ( | )iP t c  implies the first basic 
measurement and the right part 
log( ( | ) / ( | ))i iP t c P t c  implies the second one. 
The parameter ?  0 1?? ?? ?is thus to tune the 
weight between the two basic measurements. 
Especially, when ?  equals 0, the WFO method 
fades to the MI method which fully prefers the 
second basic measurement. 
Figure 2 illustrates our algorithm for 
automatically generating the polarity shifting 
training data where 1c and 2c denote the two 
sentimental orientation categories, i.e. negative 
and positive. Step A segments a document into 
sentences with punctuations. Besides, two 
special words, ?but? and ?and?, are used to 
further segment some contrast transition 
structures and compound sentences. Step B 
employs the WFO method to rank all features 
including the words. Step D extracts those 
polarity-shifted and polarity-unshifted sentences 
containing top it ?  where maxN denotes the 
upper-limit number of sentences in each 
category of the polarity shifting training data and 
#(x) denotes the total number of the elements in 
x. Apart from that, the first word in the following 
sentence is also included to capture a common 
kind of long-distance polarity shifting structure: 
contrast transition. Thus, important trigger words 
like ?however? and ?but? may be considered. 
Finally, Step E guarantees the balance between 
the two categories of the polarity shifting 
training data. 
Given the polarity shifting training data, we 
apply SVM classification algorithm to train a 
polarity-shifting detector with word unigram 
features. 
Input: 
The polarity classification training data: the negative 
sentimental document set 
1c
D and the positive sentimental 
document set
 2c
D . 
Output: 
    The polarity shifting training data: the 
polarity-unshifted sentence set unshiftS  and the polarity- 
shifted sentence set
 
shiftS . 
Procedure: 
A. Segment documents 
1c
D  and  
2c
D  to single 
sentences  
1c
S  and  
2c
S . 
B. Apply feature selection on the polarity classification  
training data and get the ranked features, 
1( ,..., ,..., )top top i top Nt t t? ? ?  
C. shiftS  = {}, unshiftS  = {} 
D. For  top it ?  in  1( ,..., ,..., )top top i top Nt t t? ? ? : 
D1) if #( shiftS )> maxN : break 
D2) Collect all sentences  
1,top i c
S
?
 and  
2,top i c
S
?
 
which contain  top it ?  from  1cS  and  2cS  
respectively 
D3)  if #(
1,top i c
S
?
)>#(
2,top i c
S
?
): 
put  
2,top i c
S
?
 into  shiftS  
put  
1,top i c
S
?
 into  unshiftS  
else: 
put  
1,top i c
S
?
 into  shiftS  
put  
2,top i c
S
?
 into  unshiftS  
E. Randomly select 
maxN sentences from unshiftS as the 
output of 
unshiftS  
 
Figure 2: The algorithm for automatically 
generating the polarity shifting training data 
 
638
3.2 Polarity Classification with Classifier 
Combination  
After polarity shifting detection, each document 
in the polarity classification training data is 
divided into two parts, one containing 
polarity-shifted sentences and the other 
containing polarity-unshifted sentences, which 
are used to form the polarity-shifted training data 
and the polarity-unshifted training data. In this 
way, two different polarity classifiers, If  and 
2f , can be trained on the polarity-shifted 
training data and the polarity-unshifted training 
data respectively. Along with classifier 3f , 
trained on all original polarity classification 
training data, we now have three base classifiers 
in hand for possible classifier combination via a 
multiple classifier system. 
The key issue in constructing a multiple 
classifier system (MCS) is to find a suitable way 
to combine the outputs of the base classifiers. In 
MCS literature, various methods are available 
for combining the outputs, such as fixed rules 
including the voting rule, the product rule and 
the sum rule (Kittler et al, 1998) and trained 
rules including the weighted sum rule (Fumera 
and Roli, 2005) and the meta-learning 
approaches (Vilalta and Drissi, 2002). In this 
study, we employ the product rule, a popular 
fixed rule, and stacking (D?eroski and ?enko, 
2004), a well-known trained rule, to combine the 
outputs. 
Formally, each base classifier provides some 
kind of confidence measurements, e.g., posterior 
probabilities of the test sample belonging to each 
class. Formally, each base classifier 
 ( 1,2,3)lf l =  assigns a test sample (denoted as 
lx ) a posterior probability vector ( )lP x

:  
1 2( ) ( | ), ( | ))tl l lP x p c x p c x= (

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging 1c . 
The product rule combines the base classifiers 
by multiplying the posterior possibilities and 
using the multiplied possibility for decision, i.e. 
3
1
      arg max ( | )j i l
i l
assign y c when j p c x
=
? = ?  
Stacking belongs to well-known 
meta-learning (Vilalta and Drissi, 2002). The 
key idea behind meta-learning is to train a 
meta-classifier with input attributes that are the 
outputs of the base classifiers. Hence, 
meta-learning usually needs some development 
data for generating the meta-training data. Let 
'x  denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( ( | ' ), ( | ' ))l l l lP x p c x p c x=

 
A meta-classifier can be trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ( ' ), ( ' ), ( ' ))meta l l lx P x P x P x= = ==
  
 
Stacking is a specific meta-learning rule, in 
which a leave-one-out or a cross-validation 
procedure on the training data is applied to 
generate the meta-training data instead of using 
extra development data. In our experiments, we 
perform stacking with 10-fold cross-validation to 
generate the meta-training data. 
4 Experimentation 
4.1 Experimental Setting 
The experiments are carried out on product 
reviews from four domains: books, DVDs, 
electronics, and kitchen appliances (Blitzer et al, 
2007)2. Each domain contains 1000 positive and 
1000 negative reviews. 
For sentiment classification, all classifiers 
including the polarity shifting detector, three 
base classifiers and the meta-classifier in 
stacking are trained by SVM using the 
SVM-light tool 3  with Logistic Regression 
method for probability measuring (Platt, 1999). 
In all the experiments, each dataset is 
randomly and evenly split into two subsets: 50% 
documents as the training data and the remaining 
50% as the test data. The features include word 
unigrams and bigrams with Boolean weights. 
4.2 Experimental Results on Polarity 
Shifting Data 
To better understand the polarity shifting 
phenomena in document-level sentiment 
classification, we randomly investigate 200 
                                                      
2
 This data set is collected by Blitzer et al (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 It is available at: http://svmlight.joachims.org/ 
639
polarity-shifted sentences, together with their 
contexts (i.e. the sentences before and after it), 
automatically generated by the WFO ( 0? = ) 
feature selection method. We find that nearly 
half of the automatically generated polarity- 
shifted sentences are actually polarity-unshifted 
sentences or difficult to decide. That is to say, 
the polarity shifting training data is noisy to 
some extent. One main reason is that some 
automatically selected trigger words do not 
really contain sentiment information, e.g., ?hear?, 
?information? etc. Another reason is that some 
reversed opinion is given in a review without 
any explicit polarity shifting structures.  
To gain more insights, we manually checked 
100 sentences which are explicitly 
polarity-shifted and can also be judged by 
human according to their contexts. Table 1 
presents some typical structures causing polarity 
shifting. It shows that the most common polarity 
shifting type is Explicit Negation (37%), usually 
expressed by trigger words such as ?not?, ?no?, or 
?without?, e.g., in the sentence ?I am not happy 
with this flashcard at all?. Another common type 
of polarity shifting is Contrast Transition (20%), 
expressed by trigger words such as ?however?, 
e.g., in the sentence ?It is large and stylish, 
however, I cannot recommend it because of the 
lid?. Other less common yet productive polarity 
shifting types include Exception and Until. 
Exception structure is usually expressed by the 
trigger phrase ?the only? to indicate the one and 
only advantage of the product, e.g., in the 
sentence ?The only thing that I like about it is 
that bamboo is a renewable resource?. Until 
structure is often expressed by the trigger word 
?until? to show the reversed polarity, e.g. in the 
sentence ?This unit was a great addition until the 
probe went bad after only a few months?. 
 
Polarity Shifting 
Structures 
Trigger 
Words/Phrases 
Distribution 
(%) 
Explicit Negation not, no, without 37 
Contrast Transition but, however, 
unfortunately 
20 
Implicit Negation avoid, hardly,  7 
False Impression look, seem 6 
Likelihood probably, perhaps 5 
Counter-factual should, would 5 
Exception the only 5 
Until until 3 
Table 1: Statistics on various polarity shifting 
structures 
4.3 Experimental Results on Polarity 
Classification 
For comparison, several classifiers with different 
classification methods are developed.  
1) Baseline classifier, which applies SVM with 
all unigrams and bigrams. Note that it also 
serves as a base classifier in the following 
combined classifiers. 
2) Base classifier 1, a base classifier for the 
classifier combination method. It works on the 
polarity-unshifted data.  
3) Base classifier 2, another base classifier for 
the classifier combination method. It works on 
the polarity-shifted data. 
4) Negation classifier, which applies SVM with 
all unigrams and bigrams plus negation bigrams. 
It is a natural extension of the baseline classifier 
with the consideration of negation bigrams. In 
this study, the negation bigrams are collected 
using some negation trigger words, such as ?not? 
and ?never?. If a negation trigger word is found 
in a sentence, each word in the sentence is 
attached with the word ?_not? to form a negation 
bigram. 
5) Product classifier, which combines the 
baseline classifier, the base classifier 1 and the 
base classifier 2 using the product rule. 
6) Stacking classifier, a combined classifier 
similar to the Product classifier. It uses the 
stacking classifier combination method instead 
of the product rule.  
Please note that we do not compare our approach 
with the one as proposed in Ikeda et al (2008) 
due to the absence of a manually-collected 
sentiment dictionary. Besides, it is well known 
that a combination strategy itself is capable of 
improving the classification performance. To 
justify whether the improvement is due to the 
combination strategy or our polarity shifting 
detection or both, we first randomly split the 
training data into two portions and train two base 
classifiers on each portion, then apply the 
stacking method to combine them along with the 
baseline classifier. The corresponding results are 
shown as ?Random+Stacking? in Table 2. Finally, 
in our experiments, t-test is performed to 
evaluate the significance of the performance 
improvement between two systems employing 
different methods (Yang and Liu, 1999). 
 
640
Domain Baseline Base  
Classifier 
1 
Base  
Classifier 
2 
Negation 
Classifier 
Random 
+ 
Stacking 
Shifting 
+ 
Product 
Shifting 
+ 
Stacking 
Book 0.755 0.756 0.670 0.759 0.764 0.772 0.785 
DVD 0.750 0.743 0.667 0.748 0.759 0.768 0.770 
Electronic 0.779 0.786 0.711 0.785 0.789 0.820 0.830 
Kitchen 0.818 0.814 0.683 0.826 0.835 0.840 0.849 
Table 2: Performance comparison of different classifiers with equally-splitting between training and test data 
 
Performance comparison of different 
classifiers 
Table 2 shows the accuracy results of different 
methods using 2000 polarity shifted sentences 
and 2000 polarity-unshifted sentences to train the 
polarity shifting detector (Nmax=2000). Compared 
to the baseline classifier, it shows that: 1) The 
base classifier 1, which only uses the 
polarity-unshifted sentences as the training data, 
achieves similar performance. 2)  The base 
classifier 2 achieves much lower performance 
due to much fewer sentences involved. 3) 
Including negation bigrams usually allows 
insignificant improvements (p-value>0.1), which 
is consistent with most of previous works (Pang 
et al, 2002; Kennedy and Inkpen, 2006). 4) Both 
the product and stacking classifiers with polarity 
shifting detection significantly improve the 
performance (p-value<0.05). Compared to the 
product rule, the stacking classifier is preferable, 
probably due to the performance unbalance 
among the individual classifiers, e.g., the 
performance of the base classifier 2 is much 
lower than the other two. Although stacking with 
two randomly generated base classifiers, i.e. 
?Random + Stacking?, also consistently 
outperforms the baseline classifier, the 
improvements are much lower than what has 
been achieved by our approach. This suggests 
that both the classifier combination strategy and 
polarity shifting detection contribute to the 
overall performance improvement. 
Effect of WFO feature selection method 
Figure 3 presents the accuracy curve of the 
stacking classifier when using different Lambda 
( ? ) values in the WFO feature selection method. 
It shows that those feature selection methods 
which prefer frequency information, e.g., MI and 
BNS, are better in automatically generating the 
polarity shifting training data. This is reasonable 
since high frequency terms, e.g., ?is?, ?it?, ?a?, 
etc., tend to obey our assumption that the real 
polarity of one top term should belong to the 
polarity category where the term appears 
frequently. 
Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Lambda=0 0.25 0.5 0.75 1
Ac
cu
ra
cy
Book DVD Electronic Kitchen
Figure 3: Performance of the stacking classifier using 
WFO with different Lambda ( ? ) values 
 Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
200 500 1000 1500 2000 3000 4000 6000 8000
Ac
cu
ra
cy
Book DVD Electronic Kitchen
 Figure 4: Performance of the stacking classifier over 
different sizes of the polarity shifting training data 
(with Nmax sentences in each category) 
Effect of a classifier over different sizes of the 
polarity shifting training data 
Another factor which might influence the 
overall performance is the size of the polarity 
shifting training data. Figure 4 presents the 
overall performance on different numbers of the 
polarity shifting sentences when using the 
stacking classifier. It shows that 1000 to 4000 
sentences are enough for the performance 
improvement. When the number is too large, the 
noisy training data may harm polarity shifting 
detection. When the number is too small, it is not 
enough for the automatically generated polarity 
shifting training data to capture various polarity 
shifting structures. 
641
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: Book
The traning data sizes
Ac
c
ur
ac
y
 
 
Baseline BaseClassifier 1 BaseClassifier 2 Stacking
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: DVD
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Electronic
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Kitchen
The traning data sizes
Ac
c
ur
ac
y
 
 
Figure 5: Performance of different classifiers over different sizes of the polarity classification training data 
 
Effect of different classifiers over different 
sizes of the polarity classification training data 
Figure 5 shows the classification results of 
different classifiers with varying sizes of the 
polarity classification training data. It shows that 
our approach is able to improve the overall 
performance robustly. We also notice the big 
difference between the performance of the 
baseline classifier and that of the base classifier 
1 when using 30% training data in Book domain 
and 90% training data in DVD domain. Detailed 
exploration of the polarity shifting sentences in 
the training data shows that this difference is 
mainly attributed to the poor performance of the 
polarity shifting detector. Even so, the stacking 
classifier guarantees no worse performance than 
the baseline classifier. 
5 Conclusion and Future Work 
In this paper, we propose a novel approach to 
incorporate polarity shifting information into 
document-level sentiment classification. In our 
approach, we first propose a 
machine-learning-based classifier to detect 
polarity shifting and then apply two classifier 
combination methods to perform polarity 
classification. Particularly, the polarity shifting 
training data is automatically generated through 
a feature selection method. As shown in our 
experimental results, our approach is able to 
consistently improve the overall performance 
across different domains and training data sizes, 
although the automatically generated polarity 
shifting training data is prone to noise. 
Furthermore, we conclude that those feature 
selection methods, which prefer frequency 
information, e.g., MI and BNS, are good choices 
for generating the polarity shifting training data. 
In our future work, we will explore better 
ways in generating less-noisy polarity shifting 
training data. In addition, since our approach is 
language-independent, it is readily applicable to 
sentiment classification tasks in other languages. 
For availability of the automatically generated 
polarity shifting training data, please contact the 
first author (for research purpose only). 
Acknowledgments 
This research work has been partially supported 
by Start-up Grant for Newly Appointed 
Professors, No. 1-BBZM in the Hong Kong 
Polytechnic University and two NSFC grants, 
No. 60873150 and No. 90920004. We also thank 
the three anonymous reviewers for their helpful 
comments. 
642
References 
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Ding X., B. Liu, and P. Yu. 2008. A Holistic 
Lexicon-based Approach to Opinion Mining. In 
Proceedings of the International Conference on 
Web Search and Web Data Mining, WSDM-08. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Forman G. 2003. An Extensive Empirical Study of 
Feature Selection Metrics for Text Classification. 
The Journal of Machine Learning Research, 3(1), 
pp.1289-1305. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Ikeda D., H. Takamura, L. Ratinov, and M. Okumura. 
2008. Learning to Shift the Polarity of Words for 
Sentiment Classification. In Proceedings of 
IJCNLP-08. 
Kennedy, A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Li S., R. Xia, C. Zong, and C. Huang. 2009. A 
Framework of Feature Selection Methods for Text 
Categorization. In Proceedings of 
ACL-IJCNLP-09. 
Li S. and C. Zong. 2008. Multi-domain Sentiment 
Classification. In Proceedings of ACL-08: HLT, 
short paper. 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
Na J., H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004. 
Effectiveness of Simple Linguistic Processing in 
Automatic Sentiment Classification of Product 
Reviews. In Conference of the International 
Society for Knowledge Organization (ISKO-04). 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and Comparisons to Regularized 
Likelihood Methods. In: A. Smola, P. Bartlett, B. 
Schoelkopf and D. Schuurmans (Eds.): Advances 
in Large Margin Classiers. MIT Press, Cambridge, 
61?74. 
Polanyi L. and A. Zaenen. 2006. Contextual Valence 
Shifters. Computing attitude and affect in text: 
Theory and application. Springer Verlag. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In 
Proceedings of EMNLP-06. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial 
Intelligence Review, 18(2), pp. 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wiebe J. 2000. Learning Subjective Adjectives from 
Corpora. In Proceedings of AAAI-2000. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu, X. 1999. A Re-Examination of 
Text Categorization methods. In Proceedings of 
SIGIR-99. 
Yang Y. and J. Pedersen. 1997. A Comparative Study 
on Feature Selection in Text Categorization. In 
Proceedings of ICML-97. 
643
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 520?529, Dublin, Ireland, August 23-29 2014.
Skill Inference with Personal and Skill Connections
Zhongqing Wang
?
, Shoushan Li
??
, Hanxiao Shi
?
, and Guodong Zhou
?
?
Natural Language Processing Lab, School of Computer Science and Technology,
Soochow University, China
?
School of Computer Science and Information Engineering,
Zhejiang Gongshang University, China
{wangzq.antony, shoushan.li}@gmail.com
hxshi@mail.zjgsu.edu.cn, gdzhou@suda.edu.cn
Abstract
Personal skill information on social media is at the core of many interesting applications. In
this paper, we propose a factor graph based approach to automatically infer skills from per-
sonal profile incorporated with both personal and skill connections. We first extract personal
connections with similar academic and business background (e.g. co-major, co-university, and
co-corporation). We then extract skill connections between skills from the same person. To well
integrate various kinds of connections, we propose a joint prediction factor graph (JPFG) model
to collectively infer personal skills with help of personal connection factor, skill connection fac-
tor, besides the normal textual attributes. Evaluation on a large-scale dataset from LinkedIn.com
validates the effectiveness of our approach.
1 Introduction
With the large amount of user-generated content (UGC) published online every day in the context of
social networks (Tan et al., 2011; Luo et al., 2013), such online social networks (e.g., Twitter, Facebook,
and LinkedIn) have significantly enlarged our social circles and much affected our everyday life. One
popular and important type of UGC is the personal profile, where people post their detailed information,
such as education, experience and other personal information, on online portals. Social websites like
Facebook.com and LinkedIn.com have created a viable business as profile portals, with the popularity
and success largely attributed to their comprehensive personal profiles.
Obviously, online personal profiles can help people connect with others of similar backgrounds and
provide valuable resources for businesses, especially for personnel resource managers to find talents
(Yang et al., 2011a; Guy et al., 2010). In the profiles, the personal skill information is the most impor-
tant aspect to reflect the expertise of a person. However, few social platforms allow users to manually
attach such personal skill information into their personal profiles. For example, in our collected dataset,
91.8% skills appear less than 10 times. Even the distribution of the top 10 frequently occurring skills is
asymmetric, and only 43.1% people attach skills on their profiles. For this regard, it is highly desirable
to develop reliable methods to automatically infer personal skills for personal profiles.
Although it is straightforward to recast skill inference as a standard text classification problem, i.e.,
predicting the skills with the profile text alone, personal profiles usually are poorly organized, even with
critical information missing. Thus, it is challenging to infer skills given the limited information from
the profile texts. We propose two assumptions to address above challenges by incorporating additional
connection information between persons and skills:
? People are always connected to others with similar academic and business backgrounds (e.g. co-
major, co-corporation). For example if there is co-major, co-university, or co-corporation rela-
tionship between two persons, it is very likely that they may share similar skills. Therefore, it is
reasonable to resort to personal connection information to improve the performance of skill infer-
ence.
*corresponding author
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
520
? One person tends to have some related skills. For example, it is very likely that C++, C, and Python
programming languages may co-occur in the one?s profile, i.e., if a person has skill C++, it is highly
possible that he would have the skills such as C or Python. Thus, it is useful to integrate skill
connection information when inferring personal skills.
Based on these assumptions, we propose a Joint Prediction Factor Graph (JPFG) model, which collec-
tively predicts personal skills with help of both personal and skill connections. In particular, the JPFG
model provides a general framework to integrate three kinds of knowledge, i.e. local textual attribute
functions of an individual person, personal connection factors between persons, and skill connection fac-
tors between skills, in collectively inferring personal skills. Specially, we extract personal connections
with similar academic and business background (e.g. co-major, co-corporation). We then extract skill
connections between skills from same person. Evaluation on a large-scale data set from LinkedIn.com
indicates that our JPFG model can significantly improve the performance of personal skill inference.
The remainder of this paper is structured as follows. We review the related work in Section 2. In Sec-
tion 3, we introduce the data collection. In Section 4, we give the problem definition and some analysis
on the task of personal skill reference. In Section 5, we propose the JPFG model and corresponding
algorithms for parameter estimation and prediction. In Section 6, we present our experimental results. In
Section 7, we summarize our work and discuss future directions.
2 Related Works
In this section, we briefly review related studies in expert finding, social tag suggestion and factor graph
model.
2.1 Expert Finding
Expert finding aims to find right persons with appropriate skills or knowledge, i.e. ?Who are the experts
on topic X?? TREC-2005 and TREC-2006 have provided a common platform for researchers to empiri-
cally evaluate methods and techniques on expert finding (Soboroff et al, 2006; Zhang et al., 2007a).
In the literature, expert finding tends to consider each skill individually and seeks the most authority
experts for each skill. Thus, expert finding is always considered as a ranking process, i.e., ranking the
experts from the candidates who are most suitable for the skill (Balog and Rijke, 2007). For example,
Campbell et al. (2003) investigated the issue of expert finding in an email network. They utilized the
link between email authors and receivers to improve the expert finding performance.
Besides that link structure-based algorithms, such as PageRank and HITS, are employed to analyze
the relationship of the link-relationship graph, social networks are utilized to improve the performance
of expert finding. Zhang et al. (2007a) proposed a unified propagation-based approach to address the
issue of expert finding in a social network, considering both personal local and network information (e.g.
the relationship between persons).
Expert finding is in nature different from skill inference. Our study predicts various skills attachable to
a person collectively with both personal and skill connections among people. One distinguishing charac-
teristics of our study is that several skills from a person are simultaneously modeled and the relationship
among these skills is fully leveraged in the inference.
2.2 Social Tag Suggestion
Social tag suggestion aims to extract proper tags from social media and can thus help people organize
their information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006)
created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached
to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags
from Twitter.com and Flickr.com where various kinds of information in recommendations and comments
are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words
and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose
to model context-aware relations of tags for suggestion by regarding resource content as context of tags.
521
Different from above researches, our study is forced on skill inference instead of traditional tag sugges-
tion. Basically, the social connections in skill inference are much different from those in social tagging.
In our study, we use co-major, co-title and other academic and business relationships to build the social
connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both
personal and skill connections.
2.3 Factor Graph Model
Among various approaches investigated in social networks in the last several years (Leskovec et al.,
2010; Lu et al., 2010; Lampos et al., 2013; Guo et al., 2013), Factor Graph Model (FGM) becomes an
effective way to represent and optimize the relationship in social networks (Dong et al., 2012; Yang et
al., 2012b) via a graph structure. Tang et al. (2011a) and Zhuang et al. (2012) formalized the problem
of social relationship learning as a semi-supervised framework, and proposed Partially-labeled Pairwise
Factor Graph Model (PLP-FGM) for inferring the types of social ties. Tang et al. (2013) further proposed
a factor graph based distributed learning method to construct a conformity influence model and formalize
the effects of social conformity in a probabilistic way.
Different from previous studies, this paper proposes a pairwise factor graph model to collectively infer
personal skills with both social connection factor and skill connection factor.
3 Data Construction
We collect our data set from LinkedIn.com. It contains a large number of personal profiles generated by
users, containing various kinds of information, such as personal Summary, Experience, Education, and
Skills & Expertise. We do not collect personal names in public profiles to protect people?s privacy.
The dataset contains 7,381 personal profiles, among which only 3,182 profiles (43.1% of all the pro-
files) show the Skills & Expertise field. In this study, we adopt only these profiles in all our experiments.
As a result, we get 6,863 skills in total, among which 6,299 skills (91.8% of them) appear less than 10
times. Among the remaining 564 skills, we select top 10 frequently occurring skills as the candidate
personal skills in this study (Since the remaining 554 skills only appear less than 250 times in total, it is
difficult to build an effective classifier for them). Table 1 illustrates the statistics.
Skill Number Ratio
Semiconductors 948 0.298
IC 369 0.116
Thin Films 328 0.103
Characterization 326 0.102
CMOS 311 0.098
Matlab 287 0.090
Microsoft Office 283 0.089
Manufacturing 278 0.087
Design of Experiments 262 0.082
Semiconductor Industry 250 0.079
Table 1: The distribution of the candidate personal skills
From Table 1, we can see that the skill distribution in the personal profiles is asymmetric. For example,
the Semiconductor skill occurs about 1,000 times, taking 29.8%, while the Semiconductor Industry skill
occurs 250 times only, taking 7.9%.
4 Problem Definition and Analysis
Before presenting our approach for skill inference, we first give the definition of the problem, and convey
a series of discoveries we observed from the data.
522
4.1 Problem Definition
We first introduce some necessary definitions and then formulate of the problem.
Definition 1: Skill inference. In principle, we cast skill inference as a skill prediction problem. Since
one person might have several skills, we build several vectors for a person and each vector is designed to
determine whether the corresponding skill is appropriate for the person or not (?Positive? means that the
person has the target skill, whereas ?Negative? stands for the opposite). Note that the number of vectors
for a person is equal to the number of candidate skills. For example, suppose we have m persons and
n candidate skills in the dataset, we totally build vectors to represent if these skills are attached in these
persons? profiles.
Definition 2: Textual information. We use texts of Summary and Experience as the textual information
for our research. Texts of Summary and Experience are unstructured information, while texts of Skills
& Expertise are structured information. However, some skills in the Skill & Expertise fields may not be
mentioned in the Summary and Experience fields.
Definition 3: Personal connections. We can explicitly extract four kinds of personal relationships
between two persons from the Education and Experience fields, as follows:
? co major, which denotes that two persons have the same major at school
? co univ, which denotes that two persons graduated from the same university
? co title, which denotes that two persons have the same title in a corporation.
? co corp, which denotes that two persons work in the same corporation.
Definition 4: Skill connections. We extract skill connections from same person. That is, if two vectors
are from the same person with different skills, we consider these two vectors share skill connections (e.g.
John has IC and Thin Films skills).
Learn task: Given the textual information of each profile, the personal connections between pro-
files, and skill connections of skill from same persons, the goal is to infer the skill through the above
information.
To learn the skill inference model, there are several requirements. First, the skills of persons are related
to multiple factors, e.g., network structure, personal connections, and skill connections, it is important to
find a unified model which is able to incorporate all the information together. Second, the algorithm to
learn the inference model should be efficient. In practice, the scale of the social network might be very
large.
4.2 Statistics and Observations
In the following, we give some statistics and observations on personal and skill connections.
Figure 1: The statistic of personal connection edges in our dataset
Statistics of personal connections: Figure 1 gives the statistics of personal connection edges. It
shows that with 3,182 profiles, there exist 332,390 personal connection edges. Besides, among all the
523
four relations, co major, co unvi, co title, and co corp occupy 11.7%, 40.0%, 17.7% and 30.6% respec-
tively.
Observations of skills connections: To validate the tendency of a person sharing similar skills, we
use PMI (Point-wise Mutual Information) to measure the co-occurrence between two skills. As a popular
way to measure the co-occurrence between a pair (Turney, 2002), PMI is calculated as follows:
PMI(i, j) = log
(
N
P (i&j)
P (i)P (j)
)
(1)
N is the number of profiles, P (i&j) denotes the probability of the skills (i.e., i and j) co-occurrence in
a person?s profile, while P (i) denotes the probability of the skill i appearing in a person?s profile.
Skill i Skill j PMI
C COMS 1.711
Thin Films Characterization 1.624
Thin Films Design of Experiments 1.543
Semiconductor Industry IC 1.345
Semiconductor Industry Design of Experiments 1.345
IC Microsoft Office -2.390
CMOS Microsoft Office -2.627
Semiconductor Industry Matlab -3.112
Average PMI score 0.190
Table 2: The top-5 and bottom-3 co-occurred skill pairs with their PMI scores
Table 2 lists the top-5 and bottom-3 co-occurred skill pairs with their PMI scores, together with the
average PMI score. From this table, we can see that if two skills are related, e.g., ?IC? and ?CMOS?,
these two skills tend to co-occur in one person?s profile, vice versa.
5 Joint Prediction Factor Graph Model
In this section, we propose a Joint Prediction Factor Graph (JPFG) model for learning and predicting the
skills with personal and skill connection information besides local textual information.
5.1 Model
We formalize the problem of skill prediction using a pairwise factor graph model, and our basic idea of
defining the correlations is to use different types of factor functions (i.e., personal connection factor, and
skill connection factor). Here, the objective function P
?
(Y |X,G) is defined based on the joint probability
of the factor functions, and the problem of collective skill inference model learning is cast as learning
model parameters ? that maximizes the joint probability of skills based on the input continuous dynamic
network.
Since directly maximizing the conditional probability P
?
(Y |X,G) is often intractable, we factorize
the ?global? probability as a product of ?local? factor functions, each of which depends on a subset of
the variables in the graph (Tang et al., 2013). In particular, we use three kinds of functions to represent
the local textual information of the vector (local textual attribute function), personal connection informa-
tion between vectors (personal connection factor) and skill connection information between skills (skill
connection factor), respectively. We now briefly introduce the ways to define the above three functions.
Local textual attribute functions f(x
ij
, y
i
)
j
: It denotes the attribute value associated with each
person i. Here, we define the local textual attribute as a feature (Lafferty et al., 2001) and accumulate all
the attribute functions to obtain local entropy for a person:
1
Z
1
exp
(
?
i
?
k
?
k
f
k
(x
ik
, y
i
)
)
(2)
524
Where ?
k
is the function weight, representing the influence degree of the attribute k. For simplicity, we
use word unigrams of a text as the basic textual attributes.
Personal connection factor function g(y
i
, y
j
) : For the personal correlation factor function, we
define it through the pairwise network structure. That is, if a person i and another person j have a
personal relationship, we define a personal connection factor function as follows:
g(y
i
, y
j
) = exp
{
?
ij
(y
i
? y
j
)
2
}
(3)
The personal connections are defined Section 4, i.e., co major, co univ, co title, and co corp. We define
that if two persons have at least one personal connection edge, they have a personal relationship. In
addition, ?
ij
is the weight of the function, representing the influence degree of i on j.
Skill connection factor function h(y
i
, y
j
): For the skill connection factor function, we define it
through the pairwise network structure. That is, if vector i and vector j are from the same person with
different skills, we define their skill connection influence factor function as follows:
h(y
i
, y
j
) = exp
{
?
ij
(y
i
? y
j
)
2
}
(4)
Where ?
ij
is the function weight, representing the influence degree of i on j.
By the above defined correlations, we can construct the graphical structure in the factor model. Ac-
cording to the Hammersley-Clifford theorem (Hammersley and Clifford, 1971), we integrate all the factor
functions and obtain the following log-likelihood objective function:
L(?) = log
?
P (Y |X,G)
=
1
Z
1
?
i
?
k
?
k
f
k
(x
ik
, y
i
)
+
1
Z
2
?
i
?
j?NB(i)
exp
{
?
ij
(y
i
? y
j
)
2
}
+
1
Z
3
?
i
?
k?SAME(i)
exp
{
?
ik
(y
i
? y
k
)
2
}
? logZ
(5)
Where (i, j) is a pair derived from the input network, Z = Z
1
Z
2
Z
3
is a normalization factor and
? = ({?}, {?}, {?}) indicates a parameter configuration, NB(i) denotes the set of social relationship
neighbors nodes of i (personal connection), and SAME(i) denotes the set of the node with the same
person of i (skill connection).
5.2 Learning and Prediction
Model Learning: Learning of the factor model is to find the best configuration for free parameters
? = ({?}, {?}, {?}) that maximizes the log likelihood objective function L(?).
?
?
= argmaxL(?) (6)
As the network structure in a social network can be arbitrary (e.g. possible of containing cycles), we
use the Loopy Belief Propagation (LBP) algorithm (Tang et al., 2011a) to approximate the marginal
distribution. To explain how we learn the parameters, we can get the gradient of each ?
k
with regard to
the objective function (Eq. 5), taking ? (the weight of the personal connection factor function g(y
i
, y
j
))
as an example:
L(?)
?
k
= E[g(i, j)] + E
?k
P (Y |X,G)
[g(i, j)] (7)
Where E[g(i, j)] is the expectation of factor function g(i, j) given the data distribution in the input
network and E
?k
P (Y |X,G)
[g(i, j)] represents the expectation under the distribution learned by the model,
i.e., P (y
i
|X,G) .
With the marginal probabilities, the gradient is obtained by summing up all triads (similar gradients
can be derived for parameter ?
k
and ?
ij
). It is worth noting that we need to perform the LBP process
525
twice in each iteration. The first run to estimate the marginal distribution of unknown variables y
i
=? and
the second one is to estimate the marginal distribution over all pairs. Finally, with the obtained gradient,
we update each parameter with a learning rate ?.
Skill Prediction: We can see that in the learning process, additional loopy belief propagation is used
to infer the label of unknown relationships. After learning, all unknown skills are assigned with labels
that maximize the marginal probabilities (Tang et al., 2011b), i.e.,
Y
?
= argmaxL(Y |X,G, ?) (8)
6 Experimentation
In this section, we first introduce the experimental setting, and then evaluate the performance of our
proposed JPFG model with both personal and skill connection information.
6.1 Experimental Setting
As described in Section 3, the experimental data are collected from LinkedIn.com. With top 10 frequently
used skills as candidate skills in all our experiments, we randomly select 2,000 profiles as training data
and 1,000 profiles as testing data.
Though positive and negative samples of each skill are imbalanced (In this paper, the number of the
negative samples is much larger than that of the positive samples), we select balanced testing and training
samples for each skill. Following models are implemented and compared.
? Keyword, for each profile, we consider the profile attached with the skill, only if the text of the skill
appears on the profile article with textual information.
? MaxEnt, which first uses local textual information as features to train a maximum entropy (ME)
classification model, and then employs the classification model to predict the skills in the testing
data set. The ME algorithm is implemented with the mallet toolkit
1
.
? JPFG, exactly our proposed model, which jointly predicts personal skills with local textual infor-
mation, personal connection and skill connection.
For performance evaluation, we adopt Precision (P.), Recall (R.) and F1-Measure (F1.).
6.2 Comparison with Baselines
Our first group of experiments is to investigate whether the JPFG model is able to improve skill inference
and whether the personal and skill connections are useful. The experimental results are shown in Table
3. From the table we can find that as some skills may not be mentioned on the Summary and Experience
fields directly, the performance of the Keyword approach is far from satisfaction. As incorporating
personal and skill connections, the JPFG model yields a much higher F1-measure, which improves the
performance with about 6.8% gain than the MaxEnt model.
6.3 Performance of JPFG with Different Training Data Sizes
After we evaluate the effective of the JPFG model with the large-scale training data, we carry out ex-
periments to test the effect of the JPFG model with different training data sizes. Experiment results are
shown in Figure 3. It shows that the JPFG model with both personal and skill connections always out-
perform the two baseline models. Impressively, our JPFG model using 20% training data outperforms
MaxEnt using 100% training data.
1
http://mallet.cs.umass.edu/
526
Figure 2: The performance of different methods for skill inference
Figure 3: The performance of JPFG with different training data sizes
6.4 Connections Contribution Analysis
Personal connections and skill connections can be also used to build the factor graph models to infer the
skills. We therefore want to compare our JPFG model with the factor graph model with only consider
the personal connections or skill connections, and analysis the contribution of each kinds of connection.
Specifically, MaxEnt-Personal employs the personal connections as additional features incorporated with
textual features to build the maximum entropy classification. FGM-Personal is a simplified version of
the JPFG model, which only employs textual attribute functions and personal connection factor functions
to build the factor graph model. Likewise, FGM-Skill only employs textual attribute functions and skill
connection factor functions to build the factor graph model. Table 3 shows the experiment results.
System P. R. F1.
MaxEnt 0.744 0.797 0.769
MaxEnt-Personal 0.758 0.812 0.783
FGM-Personal 0.765 0.817 0.790
FGM-Skill 0.704 0.967 0.815
JPFG 0.780 0.905 0.837
Table 3: The contribution of connections
From Table 3, we can observe that, 1) Both FGM-Personal and FGM-Skill outperform the baseline
527
MaxEnt approach. It shows that both personal connections and skill connections are helpful for skill
inference; 2) MaxEnt-Personal and FGM-Personal outperform the baseline MaxEnt approach, it show
that personal connections are helpful for inferring skills, and as considering the global optimization,
FGM-Personal is more effective; 3) FGM-Skill built on the skill connections is more effective than
MaxEnt-Personal and FGM-Personal, it show that skill connections are more useful than personal con-
nections; 4) JPFG model outperforms both FGM-Personal and FGM-Skill, it suggests that we should
incorporate both personal and skill connections to the factor graph model when we infer the skills from
profile.
7 Conclusion
In this study, we propose a novel task named personal skill inference, which aims to determine whether a
person takes a specific skill or not. To address this task, we propose a joint prediction factor graph model
with help of both personal and skill connections besides local textual information. Evaluation on a large-
scale dataset shows that our joint model performs much better than several baselines. In particular, it
shows that the performance on personal skill inference can be greatly improved by incorporating skill
connection information.
The general idea of exploring personal and skill connections to help predict people?s skills represents
an interesting research direction in social networking, which has many potential applications. Besides,
as skill information of a person is normally incomplete and fuzzy, how to better infer personal skills with
weakly labeled information is challenging.
Acknowledgements
This research work is supported by the National Natural Science Foundation of China (No. 61273320,
No. 61331011, and No. 61375073), National High-tech Research and Development Program of China
(No. 2012AA011102), Zhejiang Provincial Natural Science Foundation of China (No. LY13F020007),
the Humanity and Social Science on Young Fund of the Ministry of Education (No. 12YJC630170).
We thank Dr. Jie Tang and Honglei Zhuang for providing their software and useful suggestions about
PGM. We thank Prof. Deyi Xiong for helpful discussions, and we acknowledge Dr. Xinfang Liu, and
Yunxia Xue for corpus construction and insightful comments. We also thank anonymous reviewers for
their valuable suggestions and comments.
References
Balog K and M. Rijke. 2007. Determining Expert Profiles (With an Application to Expert Finding). In Proceedings
of IJCAI-07.
Campbell C, P. Maglio, A. Cozzi, and B. Dom. 2003. Expertise Identification Using Email Communications. In
Proceedings of CIKM-03.
Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao, and H. Cao. 2012. Link Prediction and Recommendation
across Heterogeneous Social Networks. In Proceedings of ICDM-12.
Guo W., H. Li, H. Ji, and M. Diab. 2013. Linking Tweets to News: A Framework to Enrich Short Text Data in
Social Media. In Proceedings of ACL-13 .
Guy I., N. Zwerdling, I. Ronen, D. Carmel, and E. Uziel. 2010. Social Media Recommendation based on People
and Tags. In Proceedings of SIGIR-10 .
Hammersley J. and P. Clifford. 1971. Markov Field on Finite Graphs and Lattices, Unpublished manuscript.
Helic D. and M. Strohmaier. 2011. Building Directories for Social Tagging Systems. In Proceedings of CIKM-
2011.
Lafferty J, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting
and Labeling Sequence Data. In Proceedings of ICML-01.
528
Lampos V., D. Preo?iuc-Pietro, and T. Cohn. 2013. A User-centric Model of Voting Intention from Social Media.
In Proceedings of ACL-13.
Lappas T., K. Punera, and T. Sarlos. 2011. Mining Tags Using Social Endorsement Networks. In Proceedings of
SIGIR-11.
Li H., Z. Liu, and M. Sun. 2012. Random Walks on Context-Aware Relation Graphs for Ranking Social Tags. In
Proceedings of COLING-12.
Liu Z., X. Chen, and M. Sun. 2011. A Simple Word Trigger Method for Social Tag Suggestion. In Proceedings of
EMNLP-2011.
Liu Z., C. Tu, and M. Sun. 2012. Tag Dispatch Model with Social Network Regularization for Microblog User Tag
Suggestion. In Proceedings of COLING-12.
Lu Y., and P. Tsaparas, A. 2010. Ntoulas and L. Polanyi. 2010. Exploiting Social Context for Review Quality
Prediction. In Proceedings of WWW-10.
Luo T., J. Tang, J. Hopcroft, Z. Fang, and X. Ding. 2013. Learning to Predict Reciprocity and Triadic Closure in
Social Networks. ACM Transactions on Knowledge Discovery from Data. vol.7(2), Article No. 5.
Murphy K., Y. Weiss, and M. Jordan. 1999. Loopy Belief Propagation for Approximate Inference: An Empirical
Study. In Proceedings of UAI-99 .
Ohkura T., Y. Kiyota and H. Nakagawa. 2006. Browsing System for Weblog Articles based on Automated Folk-
sonomy. In Proceedings of WWW-06.
Si X., Z. Liu, and M. Sun. 2010. Explore the Structure of Social Tags by Subsumption Relations. In Proceedings
of COLING-10.
Soboroff I., A. Vries and N. Craswell. 2006. Overview of the TREC 2006 Enterprise Track In Proceedings of
TREC-06.
Turney P. 2002. Thumbs up or Thumbs down? Semantic Orientation Applied to Unsupervised Classification of
reviews. In Proceedings of ACL-02.
Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou, and P. Li. 2011. User-Level Sentiment Analysis Incorporating Social
Networks. In Proceedings of KDD-11.
Tang W., H. Zhuang, and J. Tang. 2011a. Learning to Infer Social Ties in Large Networks. In Proceedings of
ECML/PKDD-11.
Tang J., Y. Zhang, J. Sun, J. Rao, W. Yu, Y. Chen, and A. Fong. 2011b. Quantitative Study of Individual Emotional
States in Social Networks. IEEE Transactions on Affective Computing. vol.3(2), Pages 132-144.
Tang J., S. Wu, J. Sun, and H. Su. 2012. Cross-domain Collaboration Recommendation. In Proceedings of KDD-
12.
Tang J., S. Wu, and J. Sun. 2013. Confluence: Conformity Influence in Large Social Networks. In Proceedings of
KDD-13.
Xing E, M. Jordan, and S. Russell. 2003. A Generalized Mean Field Algorithm for Variational Inference in Expo-
nential Families. In Proceedings of UAI-03.
Yang S., B. Long, A. Smola, N. Sadagopan, Z. Zheng, and H. Zha. 2011a. Like like alike - Joint Friendship and
Interest Propagation in Social Networks. In Proceedings of WWW-11.
Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su, and J. Li. 2011b. Social Context Summarization. In Proceedings of
SIGIR-11.
Zhang J., J. Tang, and J. Li. 2007a. Expert Finding in A Social Network. In Proceedings of the Twelfth Database
Systems for Advanced Applications (DASFAA-2007).
Zhang J., M. Ackerman, and L. Adamic. 2007b. Expertise Networks in Online Communities: Structure and Algo-
rithms. In Proceedings of TREC-07.
Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X. Wang. 2012. Actively Learning to Infer Social Ties. In
Proceedings of Data Mining and Knowledge Discovery (DMKD-12), vol.25 (2), pages 270-297.
529
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 139?148, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Active Learning for Imbalanced Sentiment Classification 
 
Shoushan Li?, Shengfeng Ju?, Guodong Zhou?? Xiaojun Li? 
?Natural Language Processing Lab 
School of Computer Science and Technology 
?College of Computer and 
Information Engineering 
Soochow University, Suzhou, 215006, China Zhejiang Gongshang University
{shoushan.li, shengfeng.ju}@gmail.com, Hangzhou, 310035, China 
gdzhou@suda.edu.cn lixj@mail.zjgsu.edu.cn
 
 
 
 
 
Abstract 
Active learning is a promising way for 
sentiment classification to reduce the 
annotation cost. In this paper, we focus on 
the imbalanced class distribution scenario 
for sentiment classification, wherein the 
number of positive samples is quite 
different from that of negative samples. 
This scenario posits new challenges to 
active learning. To address these 
challenges, we propose a novel active 
learning approach, named co-selecting, by 
taking both the imbalanced class 
distribution issue and uncertainty into 
account. Specifically, our co-selecting 
approach employs two feature subspace 
classifiers to collectively select most 
informative minority-class samples for 
manual annotation by leveraging a 
certainty measurement and an uncertainty 
measurement, and in the meanwhile, 
automatically label most informative 
majority-class samples, to reduce human-
annotation efforts. Extensive experiments 
across four domains demonstrate great 
potential and effectiveness of our proposed 
co-selecting approach to active learning for 
imbalanced sentiment classification. 1 
1 Introduction 
Sentiment classification is the task of identifying 
the sentiment polarity (e.g., positive or negative) of 
                                                          
*1 Corresponding author 
a natural language text towards a given topic (Pang 
et al2002; Turney, 2002) and has become the 
core component of many important applications in 
opinion analysis (Cui et al2006; Li et al2009; 
Lloret et al2009; Zhang and Ye, 2008). 
Most of previous studies in sentiment 
classification focus on learning models from a 
large number of labeled data. However, in many 
real-world applications, manual annotation is 
expensive and time-consuming. In these situations, 
active learning approaches could be helpful by 
actively selecting most informative samples for 
manual annotation. Compared to traditional active 
learning for sentiment classification, active 
learning for imbalanced sentiment classification 
faces some unique challenges.  
As a specific type of sentiment classification, 
imbalanced sentiment classification deals with the 
situation in which there are many more samples of 
one class (called majority class) than the other 
class (called minority class), and has attracted 
much attention due to its high realistic value in 
real-world applications (Li et al2011a). In 
imbalanced sentiment classification, since the 
minority-class samples (denoted as MI samples) 
are normally much sparse and thus more precious 
and informative for learning compared to the 
majority-class ones (denoted as MA samples), it is 
worthwhile to spend more on manually annotating 
MI samples to  guarantee both the quality and 
quantity of MI samples. Traditionally, uncertainty 
has been popularly used as a basic measurement in 
active learning (Lewis and Gale, 2004). Therefore, 
how to select most informative MI samples for 
manual annotation without violating the basic 
139
uncertainty requirement in active learning is 
challenging in imbalanced sentiment classification. 
In this paper, we address above challenges in 
active learning for imbalanced sentiment 
classification. The basic idea of our active learning 
approach is to use two complementary classifiers 
for collectively selecting most informative MI 
samples: one to adopt a certainty measurement for 
selecting most possible MI samples and the other 
to adopt an uncertainty measurement for selecting 
most uncertain MI samples from the most possible 
MI samples returned from the first classifier. 
Specifically, the two classifiers are trained with 
two disjoint feature subspaces to guarantee their 
complementariness. This also applies to selecting 
most informative MA samples. We call our novel 
active learning approach co-selecting due to its 
collectively selecting informative samples through 
two disjoint feature subspace classifiers. To further 
reduce the annotation efforts, we only manually 
annotate those most informative MI samples while 
those most informative MA samples are 
automatically labeled using the predicted labels 
provided by the first classifier.  
In principle, our active learning approach differs 
from existing ones in two main aspects. First, a 
certainty measurement and an uncertainty 
measurement are employed in two complementary 
subspace classifiers respectively to collectively 
select most informative MI samples for manual 
annotation. Second, most informative MA samples 
are automatically labeled to further reduce the 
annotation cost. Evaluation across four domains 
shows that our active learning approach is effective 
for imbalanced sentiment classification and 
significantly outperforms the state-of-the-art active 
learning alternatives, such as uncertainty sampling 
(Lewis and Gale, 2004) and co-testing (Muslea et 
al., 2006). 
The remainder of this paper is organized as 
follows. Section 2 overviews the related work on 
sentiment classification and active learning. 
Section 3 proposes our active learning approach 
for imbalanced sentiment classification. Section 4 
reports the experimental results. Finally, Section 5 
draws the conclusion and outlines the future work. 
2 Related Work 
In this section, we give a brief overview on 
sentiment classification and active learning. 
2.1 Sentiment Classification 
Sentiment classification has become a hot research 
topic in NLP community and various kinds of 
classification methods have been proposed, such as 
unsupervised learning methods (Turney, 2002), 
supervised learning methods (Pang et al2002), 
semi-supervised learning methods (Wan, 2009; Li 
et al2010), and cross-domain classification 
methods (Blitzer et al2007; Li and Zong, 2008; 
He et al2011). However, imbalanced sentiment 
classification is relatively new and there are only a 
few studies in the literature. 
Li et al2011a) pioneer the research in 
imbalanced sentiment classification and propose a 
co-training algorithm to perform semi-supervised 
learning for imbalanced sentiment classification 
with the help of a great amount of unlabeled 
samples. However, their semi-supervised approach 
to imbalanced sentiment classification suffers from 
the problem that their balanced selection strategy 
in co-training would generate many errors in late 
iterations due to the imbalanced nature of the 
unbalanced data. In comparison, our proposed 
active learning approach can effectively avoid this 
problem. By the way, it is worth to note that the 
experiments therein show the superiority of under-
sampling over other alternatives such as cost-
sensitive and one-class classification for 
imbalanced sentiment classification. 
Li et al2011b) focus on supervised learning 
for imbalanced sentiment classification and 
propose a clustering-based approach to improve 
traditional under-sampling approaches. However, 
the improvement of the proposed clustering-based 
approach over under-sampling is very limited. 
Unlike all the studies mentioned above, our 
study pioneers active learning on imbalanced 
sentiment classification. 
2.2 Active Learning 
Active leaning, as a standard machine learning 
problem, has been extensively studied in many 
research communities and several approaches have 
been proposed to address this problem (Settles, 
2009). Based on different sample selection 
strategies, they can be grouped into two main 
categories: (1) uncertainty sampling (Lewis and 
Gale, 2004) where the active learner iteratively 
select most uncertain unlabeled samples for 
manual annotation; and (2) committee-based 
140
sampling where the active learner selects those 
unlabeled samples which have the largest 
disagreement among several committee classifiers. 
Besides query by committee (QBC) as the first of 
such type (Freund et al1997), co-testing learns a 
committee of member classifiers from different 
views and selects those contention points (i.e., 
unlabeled examples on which the views predict 
different labels) for manual annotation (Muslea et 
al., 2006). 
However, most previous studies focus on the 
scenario of balanced class distribution and only a 
few recent studies address the active learning issue 
on imbalanced classification problems including 
Yang and Ma (2010), Zhu and Hovy (2007), 
Ertekin et al2007a) and Ertekin et al2007b)2. 
Unfortunately, they straightly adopt the uncertainty 
sampling as the active selection strategy to address 
active learning in imbalanced classification, which 
completely ignores the class imbalance problem in 
the selected samples.  
Attenberg and Provost (2010) highlights the 
importance of selecting samples by considering the 
proportion of the classes. Their simulation 
experiment on text categorization confirms that 
selecting class-balanced samples is more important 
than traditional active selection strategies like 
uncertainty. However, the proposed experiment is 
simulated and non real strategy is proposed to 
balance the class distribution of the selected 
samples. 
Doyle et al2011) propose a real strategy to 
select balanced samples. They first select a set of 
uncertainty samples and then randomly select 
balanced samples from the uncertainty-sample set. 
However, the classifier used for selecting balanced 
samples is the same as the one for supervising 
uncertainty, which makes the balance control 
unreliable (the selected uncertainty samples take 
very low confidences which are unreliable to 
correctly predict the class label for controlling the 
balance). Different from their study, our approach 
possesses two merits: First, two feature subspace 
classifiers are trained to finely integrate the 
certainty and uncertainty measurements. Second, 
the MA samples are automatically annotated, 
                                                          
2  Ertekin et al2007a) and Ertekin et al2007b) select 
samples closest to the hyperplane provided by the SVM 
classifier (within the margin). Their strategy can be seen as a 
special case of uncertainty sampling. 
which reduces the annotation cost in a further 
effort.  
3 Active Learning for Imbalanced 
Sentiment Classification 
Generally, active learning can be either stream-
based or pool-based (Sassano, 2002). The main 
difference between the two is that the former scans 
through the data sequentially and selects 
informative samples individually, whereas the 
latter evaluates and ranks the entire collection 
before selecting most informative samples at batch. 
As a large collection of samples can easily 
gathered once in sentiment classification, pool-
based active learning is adopted in this study. 
Figure 1 illustrates a standard pool-based active 
learning approach, where the most important issue 
is the sampling strategy, which evaluates the 
informativeness of one sample. 
 
Input: 
       Labeled data L; 
       Unlabeled pool U; 
Output: 
    New Labeled data L 
Procedure: 
Loop for N iterations: 
(1). Learn a classifier using current L  
(2). Use current classifier to label all unlabeled 
samples 
(3). Use the sampling strategy to select n most 
informative samples for manual annotation 
(4). Move newly-labeled samples from U to L 
 
 
Figure 1: Pool-based active learning 
3.1 Sampling Strategy: Uncertainty vs. 
Certainty 
As one of the most popular selection strategies in 
active learning, uncertainty sampling depends on 
an uncertainty measurement to select informative 
samples. Since sentiment classification is a binary 
classification problem, the uncertainty 
measurement of a document d can be simply 
defined as follows: 
{ , }
( ) min ( | )
y pos neg
Uncer d P y d
?
?  
Where ( | )P y d denotes the posterior probability of 
the document d belonging to the class y and {pos, 
141
neg} denotes the class labels of positive and 
negative. 
In imbalanced sentiment classification, MI 
samples are much sparse yet precious for learning 
and thus are believed to be more valuable for 
manual annotation. The key in active learning for 
imbalanced sentiment classification is to guarantee 
both the quality and quantity of newly-added MI 
samples. To guarantee the selection of MI samples, 
a certainty measurement is necessary. In this study, 
the certainty measurement is defined as follows: 
{ , }
( ) max ( | )
y pos neg
Cer d P y d
?
?  
Meanwhile, in order to balance the samples in 
the two classes, once an informative MI sample is 
manually annotated, an informative MA sample is 
automatically labeled. In this way, the annotated 
data become more balanced than a random 
selection strategy.  
However, the two sampling strategies discussed 
above are apparently contradicted: while the 
uncertainty measurement is prone to selecting the 
samples whose posterior probabilities are nearest 
to 0.5, the certainty measurement is prone to 
selecting the samples whose posterior probabilities 
are nearest to 1. Therefore, it is essential to find a 
solution to balance uncertainty sampling and 
certainty sampling in imbalanced sentiment 
classification,  
3.2 Co-selecting with Feature Subspace 
Classifiers 
In sentiment classification, a document is 
represented as a feature vector generated from the 
feature set ? ?1,..., mF f f? . When a feature subset, 
i.e., ? ?1 ,...,S S SrF f f?  ( r m? ), is used, the 
original m-dimensional feature space becomes an 
r-dimensional feature subspace. In this study, we 
call a classifier trained with a feature subspace a 
feature subspace classifier. 
Our basic idea of balancing both the uncertainty 
measurement and the certainty measurement is to 
train two subspace classifiers to adopt them 
respectively. In our implementation, we randomly 
select two disjoint feature subspaces, each of 
which is used to train a subspace classifier. On one 
side, one subspace classifier is employed to select 
some certain samples; on the other side, the other 
classifier is employed to select the most uncertain 
sample from those certain samples for manual 
annotation. In this way, the selected samples are 
certain in terms of one feature subspace for 
selecting more possible MI samples. Meanwhile, 
the selected sample remains uncertain in terms of 
the other feature subspace to introduce uncertain 
knowledge into current learning model. We name 
this approach as co-selecting because it 
collectively selects informative samples by two 
separate classifiers. Figure 2 illustrates the co-
selecting algorithm. In our algorithm, we strictly 
constrain the balance of the samples between the 
two classes, i.e., positive and negative. Therefore, 
once two samples are annotated with the same 
class label, they will not be added to the labeled 
data, as shown in step (7) in Figure 2. 
 
Input: 
Labeled data L with balanced samples over the two classes 
Unlabeled pool U  Output: 
    New Labeled data L 
Procedure: 
Loop for N iterations: 
(1). Randomly select a feature subset SF  with 
size r (with the proportion /r m? ? ) from F  
(2). Generate a feature subspace from SF  and 
train a corresponding feature subspace 
classifier CerC  with L 
(3). Generate another feature subspace from the 
complement set of SF , i.e., SF F?  and train 
a corresponding feature subspace classifier 
UncerC  with L 
(4). Use CerC  to select top certain k positive and k 
negative samples, denoted as a sample set 
1CER  
(5). Use UncerC  to select the most uncertain 
positive sample and negative sample from 
1CER   
(6). Manually annotate the two selected samples 
(7). If the annotated labels of the two selected 
samples are different from each other: 
      Add the two newly-annotated samples into L 
 
Figure 2: The co-selecting algorithm 
 
There are two parameters in the algorithm: the 
size of the feature subspace for training the first 
subspace classifier, i.e., ?  and the number of 
142
selected certain samples, i.e., k. Both of the two 
parameters will be empirically studied in our 
experiments. 
3.3 Co-selecting with Selected MA Samples 
Automatically Labeled 
 Input: 
Labeled data L with balanced samples over the two classes 
Unlabeled pool U MA and MI Label (positive or negative) 
Output: 
    New Labeled data L 
Procedure: 
Loop for N iterations: 
(1). Randomly select a proportion of features 
(with the proportion ? ) from F to get a 
feature subset SF  
(2). Generate a feature subspace from SF  and 
train a corresponding subspace classifier CerC  
with L 
(3). Generate another feature subspace from the 
complement set of SF , i.e., SF F?  and train 
a corresponding subspace classifier UncerC  
with L 
(4). Use CerC  to select top certain k positive and k 
negative samples, denoted as a sample set 
1CER  
(5). Use UncerC  to select the most uncertain 
positive sample and negative sample from 
1CER  
(6). Manually annotate the sample that is predicted 
as a MI sample by CerC  and automatically 
annotate the sample that is predicted as 
majority class 
(7). If the annotated labels of the two selected 
samples are different from each other: 
          Add the two newly-annotated samples into L 
Figure 3: The co-selecting algorithm with selected 
MA samples automatically labeled 
 
To minimize manual annotation, it is a good choice 
to automatically label those selected MA samples. 
In our co-selecting approach, automatically 
labeling those selected MA samples is easy and 
straightforward: the subspace classifier for 
monitoring the certainty measurement provides an 
ideal solution to annotate the samples that have 
been predicted as majority class. Figure 3 shows 
the co-selecting algorithm with those selected MA 
samples automatically labeled. The main 
difference from the original co-selecting is shown 
in Step (6) in Figure 3. Another difference is the 
input where a prior knowledge of which class is 
majority class or minority class should be known. 
In real applications, it is not difficult to know this. 
We first use a classifier trained with the initial 
labeled data to test all unlabeled data. If the 
predicted labels in the classification results are 
greatly imbalanced, we can assume that the 
unlabeled data is imbalanced, and consider the 
dominated class as majority class.  
4 Experimentation 
In this section, we will systematically evaluate our 
active learning approach for imbalanced sentiment 
classification and compare it with the state-of-the-
art active learning alternatives. 
4.1 Experimental Setting 
Dataset 
We use the same data as used by Li et al2011a). 
The data collection consists of four domains: Book, 
DVD, Electronic, and Kitchen ?Blitzer et al
2007). For each domain, we randomly select an 
initial balanced labeled data with 50 negative 
samples and 50 positive samples. For the unlabeled 
data, we randomly select 2000 negative samples, 
and 14580/12160/7140/7560 positive samples from 
the four domains respectively, keeping the same 
imbalanced ratio as the whole data. For the test 
data in each domain, we randomly extract 800 
negative samples and 800 positive samples.  
 
Classification algorithm 
The Maximum Entropy (ME) classifier 
implemented with the Mallet 3  tool is mainly 
adopted, except that in the margin-based active 
learning approach (Ertekin et al2007a) where 
SVM is implemented with light-SVM 4 . The 
features for classification are unigram words with 
Boolean weights. 
                                                          
3 http://mallet.cs.umass.edu/  
4 http://www.cs.cornell.edu/people/tj/svm_light/ 
143
 0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
Book DVD Electronic Kitchen
G-
me
an
Random SVM-based Uncertainty Certainty
Co-testing Self-selecting Co-selecting-basic Co-selecting-plus
 Figure 4: Performance comparison of different active learning approaches on imbalanced sentiment 
classification  
Evaluation metrics 
The popular geometric mean 
= rate rateG - mean TP TN?  is adopted, where rateTP  
is the true positive rate (also called positive recall 
or sensitivity) and rateTN  is the true negative rate 
(also called negative recall or specificity) (Kubat 
and Matwin, 1997). 
4.2 Experimental Results 
For thorough comparison, various kinds of active 
learning approaches are implemented including: 
? Random: randomly select the samples from the 
unlabeled data for manual annotation; 
? Margin-based: iteratively select samples 
closest to the hyperplane provided by the SVM 
classifier, which is suggested by Ertekin et al
(2007a) and Ertekin et al2007b). One sample 
is selected in each iteration; 
? Uncertainty: iteratively select samples using 
the uncertainty measurement according to the 
output of ME classifier. One sample is selected 
in each iteration; 
? Certainty: iteratively select class-balanced 
samples using the certainty measurement 
according to the output of ME classifier. One 
positive and negative sample (the positive and 
negative label is provided by the ME classifier) 
are selected in each iteration; 
? Co-testing: first get contention samples (i.e., 
unlabeled examples on which the member 
classifiers predict different labels) and then 
select the least confidence one among the 
hypotheses of different member classifiers, i.e., 
the aggressive strategy as described Muslea et 
al. (2006). Specifically, the member classifiers 
are two subspace classifiers trained by splitting 
the whole feature space into two disjoint 
subspaces of same size; 
? Self-selecting: first select k uncertainty samples 
and then randomly select a positive and 
negative sample from the uncertainty-sample 
set, which is suggested by Doyle et al2011). 
We call it self-selecting since only one 
classifier is involved to measure uncertainty 
and predict class labels. 
 
For those approaches involving random 
selection of features, we run 5 times for them and 
report the average results. Note that the samples 
selected by these approaches are imbalanced. To 
address the problem of classification on 
imbalanced data, we adopt the under-sampling 
strategy which has been shown effective for 
supervised imbalanced sentiment classification (Li 
et al2011a). Our active learning approach 
includes two versions: the co-selecting algorithm 
as described in Section 3.2 and the co-selecting 
with selected MA samples automatically labeled as 
described in Section 3.3. For clarity, we refer the 
former as co-selecting-basic and the latter as co-
selecting-plus in the following. 
144
 
Comparison with other active learning 
approaches 
Figure 4 compares different active learning 
approaches to imbalanced sentiment classification 
when 600 unlabeled samples are selected for 
annotation. Specifically, the parameters ? and k is 
set to be 1/16 and 50 respectively. Figure 4 
justifies that it is challenging to perform active 
learning in imbalanced sentiment classification: the 
approaches of margin-based, uncertainty-based 
and self-selecting perform no better than random 
selection while co-testing only outperforms 
random selection in two domains: DVD and 
Electronic with only a small improvement (about 
1%). In comparison, our approaches, both co-
selecting-basic and co-selecting-plus significantly 
outperform the random selection approach on all 
the four domains. It also shows that co-selecting-
plus is preferable over co-selecting-basic. This 
verifies the effectiveness of automatically labeling 
those selected MA samples in imbalanced 
sentiment classification.  
Specifically, we notice that only using the 
certainty measurement (i.e., certainty) performs 
worst, which reflects that only considering sample 
balance factor in imbalanced sentiment 
classification is not helpful. 
Figure 5 compares our approach to other active 
learning approaches by varying the number of the 
selected samples for manually annotation. For 
clarity, we only include random selection and co-
testing in comparison and do not show the 
performances of the other active learning 
approaches due to their similar behavior to random 
selection. From this figure, we can see that co-
testing is effective on Book and Electronic when 
less than 1500 samples are selected for manual 
annotation but it fails to outperform random 
selection in the other two domains. In contract, our 
co-selecting-plus approach is apparently more 
advantageous and significantly outperforms 
random selection across all domains (p-value<0.05) 
when less than 4800 samples are selected for 
manual annotation. 
 
Sensitiveness of the parameters ?   
The size of the feature subspace is an important 
parameter in our approach. Figure 6 shows the 
performance of co-selecting-plus with varying 
sizes of the feature subspaces for the first subspace 
Electronic
0.68
0.7
0.72
0.74
0.76
0.78
0.8
300 600 900 1200 1500 2400 4800 7000
Number of the manually annoated samples
Random Co-testing Co-selecting-plus
DVD
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
300 600 900 1200 1500 2400 4800 9600
Nubmer of the manually annotated samples
Random Co-testing Co-selecting-plus
Book
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
300 600 900 1200 1500 2400 4800 9600
Nubmer of the manually annotated samples
Random Co-testing Co-selecting-plus
Kitchen
0.7
0.72
0.74
0.76
0.78
0.8
0.82
300 600 900 1200 1500 2400 4800 7000
Number of the manually annoated samples
Random Co-testing Co-selecting-plus
Figure 5:  Performance comparison of three active learning approaches:  random selection, co-testing and co-selecting-plus, by varying the number of the selected samples for manually annotation 
145
classifier CerC . From Figure 6, we can see that a 
choice of the proportion ?  between 1/8 and 1/32 is 
recommended. This result also shows that the size 
of the feature subspace for selecting certain 
samples should be much less than that for selecting 
uncertain samples, which indicates the more 
important role of the uncertainty measurement in 
active learning. 
 
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
 1/2  1/4  1/8 1/16 1/32 1/64 1/128
Proportion of the Selected Features for Subspace
(r /m )
G-
me
an
Book DVD Electornic Kitchen   
Figure 6: Performance of co-selecting-plus over 
varying sizes of feature subspaces (? ) 
  
   
   
   
   
 
 Figure 7: Performance of co-selecting-plus over 
varying numbers of the selected certain samples (k) 
 
Sensitiveness of parameter k 
Figure 7 presents the performance of co-selecting-
plus with different numbers of the selected certain 
samples in each iteration, i.e., parameter k. 
Empirical studies suggest that setting k between 20 
and 100 could get a stable performance. Also, this 
figure demonstrates that using certainty as the only 
query strategy is much less effective (see the result 
when k=1). This once again verifies the importance 
of the uncertainty strategy in active learning. 
Number of MI samples selected for manual 
annotation 
In Table 1, we investigate the number of the MI 
samples selected for manual annotation using 
different active learning approaches when a total of 
600 unlabeled samples are selected for annotation. 
From this table, we can see that almost all the 
existing active learning approaches can only select 
a small amount of MI samples, taking similar 
imbalanced ratios as the whole unlabeled data. 
Although the certainty approach could select 
many MI samples for annotation, this approach 
performs worst due to its totally ignoring the 
uncertainty factor. When our approach is applied, 
especially co-selecting-plus, more MI samples are 
selected for manual annotation and finally included 
to learn the models. This greatly improves the 
effectiveness of our active learning approach.  
 
Table 1: The number of MI samples selected for 
manual annotation when 600 samples are annotated on the whole. 
 
 Book DVD Electronic Kitchen
Random 71 82 131 123 
SVM-based 65 72 135 106 
Uncertainty 78 93 137 136 
Certainty 160 200 236 227 
Co-testing 89 84 136 109 
Self-selecting 87 95 141 126 
Co-selecting-
basic 
101 112 179 174 
Co-selecting-
plus 
161 156 250 272 
 
Precision of automatically labeled MA samples 
In co-selecting-plus, all the added MA samples are 
automatically labeled by the first subspace 
classifier. It is encouraging to observe that 92.5%, 
91.25%, 92%, and 93.5% of automatically labeled 
MA samples are correctly annotated in Book, DVD, 
Electronic, and Kitchen respectively. This suggests 
that the subspace classifiers are able to predict the 
MA samples with a high precision. This indicates 
the rationality of automatically annotating MA 
samples. 
5 Conclusion  
In this paper, we propose a novel active learning 
approach, named co-selecting, to reduce the 
annotation cost for imbalanced sentiment 
classification. It first trains two complementary 
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
1 5 20 50 100 150
Number of the selected certainty samples
G-
me
an
Book DVD Electornic Kitchen
146
classifiers with two disjoint feature subspaces and 
then uses them to collectively select most 
informative MI samples for manual annotation, 
leaving most informative MA samples for 
automatic annotation. Empirical studies show that 
our co-selecting approach is capable of greatly 
reducing the annotation cost and in the meanwhile, 
significantly outperforms several active learning 
alternatives 
For the future work, we are interested in 
applying our co-selecting approach to active 
learning for other imbalanced classification tasks, 
especially those with much higher imbalanced ratio. 
 
Acknowledgments 
The research work described in this paper has been 
partially supported by three NSFC grants, 
No.61003155, No.60873150 and No.90920004, 
one National High-tech Research and 
Development Program of China 
No.2012AA011102, Open Projects Program of 
National Laboratory of Pattern Recognition, and 
the NSF grant of Zhejiang Province No.Z1110551. 
We also thank the three anonymous reviewers for 
their helpful comments. 
 
References  
Attenberg J. and F. Provost. 2010. Why Label when you 
can Search? Alternatives to Active Learning for 
Applying Human Resources to Build Classification 
Models Under Extreme Class Imbalance. In 
Proceeding of KDD-10, 423-432. 
Blitzer J., M. Dredze and F. Pereira. 2007. Biographies, 
Bollywood, Boom-boxes and Blenders: Domain 
Adaptation for Sentiment Classification. In 
Proceedings of ACL-07, 440-447. 
Cui H., V. Mittal, and M. Datar. 2006. Comparative 
Experiments on Sentiment Classification for Online 
Product Reviews. In Proceedings of AAAI-06, 
pp.1265-1270. 
Doyle S., J. Monaco, M. Feldman, J. Tomaszewski and 
A. Madabhushi. 2011. An Active Learning based 
Classification Strategy for the Minority Class 
Problem: Application to Histopathology Annotation. 
BMC Bioinformatics, 12: 424, 1471-2105. 
Ertekin S., J. Huang, L. Bottou and C. Giles. 2007a. 
Learning on the Border: Active Learning in 
Imbalanced Data Classification. In Proceedings of 
CIKM-07, 127-136. 
Ertekin S., J. Huang, L. Bottou and C. Giles. 2007b. 
Active Learning in Class Imbalanced Problem. In 
Proceedings of SIGIR-07, 823-824. 
Freund Y., H. Seung, E. Shamir and N. Tishby. 1997. 
Selective Sampling using the Query by Committee 
algorithm. Machine Learning, 28(2-3), 133-168. 
He Y., C. Lin and H. Alani. 2011. Automatically 
Extracting Polarity-Bearing Topics for Cross-
Domain Sentiment Classification. In Proceeding of 
ACL-11, 123-131. 
Lewis D. and W. Gale. 1994. Training Text Classifiers 
by Uncertainty Sampling. In Proceedings of SIGIR-
94, 3-12. 
Li F., Y. Tang, M. Huang and X. Zhu. 2009. Answering 
Opinion Questions with Random Walks on Graphs. 
In Proceedings of ACL-IJCNLP-09, 737-745. 
Li S. and C. Zong. 2008. Multi-domain Sentiment 
Classification. In Proceedings of ACL-08, short paper, 
pp.257-260. 
Li S., C. Huang, G. Zhou and S. Lee.  2010. Employing 
Personal/Impersonal Views in Supervised and Semi-
supervised Sentiment Classification.  In Proceedings 
of ACL-10,  pp.414-423. 
Li S., Z. Wang, G. Zhou and S. Lee. 2011a. Semi-
supervised Learning for Imbalanced Sentiment 
Classification. In Proceeding of IJCAI-11, 826-1831. 
Li S., G. Zhou, Z. Wang, S. Lee and R. Wang. 2011b. 
Imbalanced Sentiment Classification. In Proceedings 
of CIKM-11,  poster paper, 2469-2472. 
Lloret E., A. Balahur, M. Palomar, and A. Montoyo. 
2009. Towards Building a Competitive Opinion 
Summarization System. In Proceedings of NAACL-
09 Student Research Workshop and Doctoral 
Consortium, 72-77. 
Kubat M. and S. Matwin. 1997. Addressing the Curse of 
Imbalanced Training Sets: One-Sided Selection. In 
Proceedings of ICML-97, 179?186. 
Muslea I., S. Minton and C. Knoblock . 2006. Active 
Learning with Multiple Views. Journal of Artificial 
Intelligence Research, vol.27, 203-233. 
Pang B. and L. Lee. 2008. Opinion Mining and 
Sentiment Analysis: Foundations and Trends. 
Information Retrieval, vol.2(12), 1-135. 
Pang B., L. Lee and S. Vaithyanathan. 2002.Thumbs up? 
Sentiment Classification using Machine Learning 
Techniques. In Proceedings of EMNLP-02, 79-86.  
147
Settles B. 2009. Active Learning Literature Survey. 
Computer Sciences Technical Report 1648, 
University of Wisconsin, Madison, 2009. 
Turney P. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of reviews. In Proceedings of ACL-02, 
417-424.  
Wan X. 2009. Co-Training for Cross-Lingual Sentiment 
Classification. In Proceedings of ACL-IJCNLP-09, 
235?243. 
Yang Y. and G. Ma. 2010. Ensemble-based Active 
Learning for Class Imbalance Problem. J. Biomedical 
Science and Engineering,  vol.3,1021-1028. 
Zhang M. and X. Ye. 2008. A Generation Model to 
Unify Topic Relevance and Lexicon-based Sentiment 
for Opinion Retrieval. In Proceedings of SIGIR-08, 
411-418. 
Zhu J. and E. Hovy. 2007. Active Learning for Word 
Sense Disambiguation with Methods for Addressing 
the Class Imbalance Problem. In Proceedings of 
ACL-07, 783-793. 
148
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 715?725,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Collective Personal Profile Summarization with Social Networks 
 
 
Zhongqing Wang, Shoushan Li*, Kong Fang, and Guodong Zhou 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China  
{wangzq.antony, shoushan.li}@gmail.com,  
{kongfang, gdzhou}@suda.edu.cn 
 
  
 
Abstract 
Personal profile information on social media 
like LinkedIn.com and Facebook.com is at the 
core of many interesting applications, such as 
talent recommendation and contextual advertis-
ing. However, personal profiles usually lack or-
ganization confronted with the large amount of 
available information. Therefore, it is always a 
challenge for people to find desired information 
from them. In this paper, we address the task of 
personal profile summarization by leveraging 
both personal profile textual information and so-
cial networks. Here, using social networks is 
motivated by the intuition that, people with 
similar academic, business or social connections 
(e.g. co-major, co-university, and co-
corporation) tend to have similar experience and 
summaries. To achieve the learning process, we 
propose a collective factor graph (CoFG) model 
to incorporate all these resources of knowledge 
to summarize personal profiles with local textual 
attribute functions and social connection factors. 
Extensive evaluation on a large-scale dataset 
from LinkedIn.com demonstrates the effective-
ness of the proposed approach.* 
1 Introduction 
Web 2.0 has empowered people to actively interact 
with each other, forming social networks around 
mutually interesting information and publishing a 
large amount of useful user-generated content 
(UGC) online (Lappas et al, 2011; Tan et al, 
2011). One popular and important type of UGC is 
the personal profile, where people post detailed 
                                                 
* Corresponding author 
information on online portals about their education, 
experiences and other personal information. Social 
websites like Facebook.com and LinkedIn.com 
have created a viable business as profile portals, 
with the popularity and success partially attributed 
to their comprehensive personal profiles. 
Generally, online personal profiles provide val-
uable resources for businesses, especially for hu-
man resource managers to find talents, and help 
people connect with others of similar backgrounds 
(Yang et al, 2011a; Guy et al, 2010). However, as 
there is always large-scale information of experi-
ence and education fields, it is hardly for us to find 
useful information from the profile. Therefore, it is 
always a challenge for people to find desired in-
formation from them. For this regard, it is highly 
desirable to develop reliable methods to generate a 
summary of a person through his profile automati-
cally.  
To the best of our knowledge, this is the first re-
search that explores automatic summarization of 
personal profiles in social media. A straightfor-
ward approach is to consider personal profile 
summarization as a traditional document summari-
zation problem, which treating each personal pro-
file independently and generate a summary for 
each personal profile individually. For example, 
the well-known extraction and ranking approaches 
(e.g. PageRank, HITS) extract a certain amount of 
important sentences from a document according to 
some ranking measurements to form a summary 
(Wan and Yang, 2008; Wan, 2011).  
However, such straightforward approaches are 
not sufficient to benefit from the carrier of person-
al profiles. As the centroid of social networking, 
people are usually connected to others with similar 
715
background in social media (e.g. co-major, co-
corporation). Therefore, it is reasonable to lever-
age social connection to improve the performance 
of profile summarizing. For example if there are 
co-major, co-university, co-corporation or other 
academic and business relationships between two 
persons, we consider them sharing similar experi-
ence and having similar summaries. 
The remaining challenge is how to incorporate 
both the profile textual information and the con-
nection knowledge in the social networks. In this 
study, we propose a collective factor graph model 
(CoFG) to summarize the text of personal profile 
in social networks with local textual information 
and social connection information. The CoFG 
framework utilizes both the local textual attribute 
functions of an individual person and the social 
connection factor between different persons to col-
lectively summarize personal profile on one person. 
In this study, we treat the profile summarization 
as a supervised learning task. Specifically, we 
model each sentence of the profile as a vector. In 
the training phase, we use the vectors with the so-
cial connection between each person to build the 
CoFG model; while in the testing phase, we per-
form collective inference for the importance of 
each sentence and select a subset of sentences as 
the summary according to the trained model. Eval-
uation on a large-scale data from LinkedIn.com 
indicates that our proposed joint model and social 
connection information improve the performance 
of profile summarization. 
The remainder of our paper is structured as fol-
lows. We go over the related work in Section 2. In 
Section 3, we introduce the data we collected from 
LinkedIn.com and the annotated corpus we con-
structed. In Section 4, we present some motiva-
tional analysis. In Section 5, we explain our pro-
posed model and describe algorithms for parame-
ter estimation and prediction. In Section 6, we pre-
sent our experimental results. We sum up our work 
and discuss future directions in Section 7. 
2 Related Work 
In this section, we will introduce the related work 
on the traditional topic-based summarization, so-
cial-based summarization and factor graph model 
respectively. 
2.1 Topic-based Summarization 
Generally, traditional topic-based summarization 
can be categorized into two categories: extractive 
(Radev et al, 2004) and abstractive (Radev and 
McKeown, 1998) summarization. The former se-
lects a subset of sentences from original docu-
ment(s) to form a summary; the latter reorganizes 
some sentences to form a summary where several 
complex technologies, such as information fusion, 
sentence compression and reformulation are nec-
essarily employed (Wan and Yang, 2008; Celiky-
ilmaz and Hakkani-Tur, 2011; Wang and Zhou, 
2012). This study focuses on extractive summari-
zation.  
Radev et al (2004) proposed a centroid-based 
method to rank the sentences in a document set, 
using various kinds of features, such as the cluster 
centroid, position and TF-IDF features. Ryang and 
Abekawa (2012) proposed a reinforcement learn-
ing approach on text summarization, which models 
the summarization within a reinforcement learn-
ing-based framework.  
Compared to unsupervised approaches, super-
vised learning for summarization is relatively rare. 
A typical work is Shen et al, (2007) which present 
a Conditional Random Fields (CRF) based frame-
work to treat the summarization task as a sequence 
labeling problem. However, different from all ex-
isting studies, our work is the first attempt to con-
sider both textual information and social relation-
ship information for supervised summarization. 
2.2 Social-based Summarization 
As web 2.0 has empowered people to actively in-
teract with each other, studies focusing on social 
media have attracted much attention recently 
(Meeder et al, 2011; Rosenthal and McKeown, 
2011; Yang et al, 2011a). Social-based summari-
zation is exactly a special case of summarization 
where the social connection is employed to help 
obtaining the summarization. Although topic-
based summarization has been extensively studied, 
studies on social-based summarization are relative 
new and rare.  
Hu et al, (2011) proposed an unsupervised Pag-
eRank-based social summarization approach by 
incorporating both document context and user con-
text in the sentence evaluation process. Meng et al, 
(2012) proposed a unified optimization framework 
to produce opinion summaries of tweets through 
716
integrating information from dimensions of topic, 
opinion and insight, as well as other factors (e.g. 
topic relevancy, redundancy and language styles). 
Unlike all the above studies, this paper focuses 
on a novel task, profile summarization. Further-
more, we employ many other kinds of social in-
formation in profiles, such as co-major, and co-
corporation between two people. They are shown 
to be very effective for profile summarization.  
2.3 Factor Graph Model 
As social network has been investigated for sever-
al years (Leskovec et al, 2010; Tan et al, 2011; 
Lu et al, 2010; Guy et al, 2010) and Factor Graph 
Model (FGM) is a popular approach to describe 
the relationship of social network (Tang et al, 
2011a; Zhuang et al, 2012). Factor Graph Model 
builds a graph to represent the relationship of 
nodes on the social networks, and the factor func-
tions are always considered to represent the rela-
tionship of the nodes. 
Tang et al (2011a) and Zhuang et al (2012) 
formalized the problem of social relationship 
learning into a semi-supervised framework, and 
proposed Partially-labeled Pairwise Factor Graph 
Model (PLP-FGM) for learning to infer the type of 
social ties. Dong et al (2012) gave a formal defini-
tion of link recommendation across heterogeneous 
networks, and proposed a ranking factor graph 
model (RFG) for predicting links in social net-
works, which effectively improves the predictive 
performance. Yang et al, (2011b) generated sum-
maries by modeling tweets and social contexts into 
a dual wing factor graph (DWFG), which utilized 
the mutual reinforcement between Web documents 
and their associated social contexts.  
Different from all above researches, this paper 
proposes a pair-wise factor graph model to collec-
tively utilize both textual information and social 
connection factor to generate summary of profile. 
3 Data Collection and Statistics   
The personal profile summarization is a novel task 
and there exists no related data for accessing this 
issue. Therefore, in this study, we collect a data set 
containing personal summaries with the corre-
sponding knowledge, such as the self-introduction 
and personal profiles. In this section, we will in-
troduce this data set in detail. 
3.1 Data Collection  
We collect our data set from LinkedIn.com1 . It 
contains a large number of personal profiles gen-
erated by users, containing various kinds of infor-
mation, such as personal overview, summary, edu-
cation, experience, projects and skills.  
 
John Smith2  
Overview 
Current Applied Researcher at Apple Inc. 
Previous 
Senior Research Scientist at IBM 
? 
Education 
MIT, 
Georgia Institute of Technology,   
? 
Summary 
Machine learning researcher and engineer on 
many fields: 
Query understanding. Automatic Information 
extraction? 
Experience 
Applied Researcher 
Apple Inc., September 2012 ~  
Query recognition and relevance 
? 
Education 
MIT 
Ph.D., Electrical Engineering, 2002 ? 2008 
? 
Figure 1: An example of a profile webpage from 
LinkedIn.com 
 
In this study, the data set is crawled in the fol-
lowing ways. To begin with, 10 random people?s 
public profiles are selected as seed profiles, and 
then the profiles from their ?People Also Viewed? 
field were collected. The data is composed of 
3,182 public profiles3 in total. We do not collect 
personal names in public profiles to protect peo-
ple?s privacy. Figure 1 shows an example of a per-
son?s profile from LinkedIn.com. The profile in-
cludes following fields: 
? Overview: It gives a structure description of a 
person?s general information, such as cur-
rent/previous position and workplace, brief 
                                                 
1 http://www.linkedin.com 
2 The information of the example is a pseudo one. 
3 We collect all the data from LinkedIn.com at Dec 17, 
2012.  
717
education background and general technical 
background.  
? Summary: It summarizes a person?s work, 
experience and education.  
? Experience: It details a person?s work experi-
ence.  
? Education: It details a person?s education 
background.  
Among these fields, the Overview is required 
and the others are optional, such as Project, 
Course and Interest groups. However, compared 
with Overview, Summary, Experience, Education 
fields, they seem to be less important for summari-
zation of personal profiles. Thus, we ignore them 
in our study. 
3.2 Data Statistics of Major Fields 
We collected 3,182 personal profiles from 
LinkedIn.com. Table 1 shows the statistics of ma-
jor fields in our data collection. 
 
Field 
#Non-empty 
fields 
Average 
field 
length 
Overview 3,182 45.1 
Summary 921 25.8 
Experience 3,148 192.1 
Education 2,932 33.6 
Table 1: Statistics of major fields in our data set, i.e. the 
number of non-empty fields and the average length for 
each field 
 
From Table 1, we can see that, 
? The information of each profile is incom-
plete and inconsistent, That is, not all kinds 
of fields are available in each personal?s 
profile.  
? Most people provide their experience and 
education information. However, the Sum-
mary fields are popularly missing (Only 
about 30% of people provide it). This is 
mainly because writing summary is nor-
mally more difficult than other fields. 
Therefore, it is highly desirable to develop 
reliable automatic methods to generate a 
summary of a person through his/her pro-
file. 
? The length of the Experience field is the 
longest one, and work experience always 
could represent general information of 
people.  
3.3 Corpus Construction and Annotation  
Among the 921 profiles that contain the summary, 
we manually select 497 profiles with high quality 
summary to construct the corpus for our research. 
These high-quality summaries are all written by 
the authors themselves. Here, the quality is meas-
ured by manually checking that whether they are 
well capable of summarizing their profiles. That is, 
they are written carefully, and could give an over-
view of a person and represent the education and 
experience information of a person. 
After carefully seeing the profiles, we observe 
that the Experience field contains the most abun-
dant information of a person. Thus, we treat the 
text of Experience field as the source of summary 
for each profile. Besides, we collect social context 
information from Education and Experience field, 
and these social contexts are including by 
LinkedIn explicitly. Table 2 shows the average 
length of summary and experience fields we used 
for evaluating our summarization approach.  
 
Field 
Average 
length 
Summary 
(the summary of the 
profile) 
37.2 
Experience 
(the source text for the 
summarizing) 
372.0 
Table 2: Average length of the high-quality summary  
and corresponding experience fields 
 
From Table 2, we can see that,  
? Compared with the average length of 25.8 
in Table 1, summaries of high quality have 
longer length because they contain more in-
formation of the profiles.  
? The compression ratio of our proposed cor-
pus is 0.1 (37.2/372.0).  
4 Motivation and Analysis 
In this section, we propose the motivation of social 
connection to address the task of personal profile 
summarization. To preliminarily support the moti-
vation, some statistics of the social connection are 
provided. 
718
 Figure 2: An example of personal profile network.  
Red is for female, blue is for male, and the dotted line 
means the social connection between two persons. 
 
We first describe the social connections which 
we used. Figure 2 shows an example of social 
connection between people from the profiles of 
LinkedIn. We find that people are sometimes con-
nected by several social connections. For example, 
John and Lucy are connected by co_unvi relation-
ship, while Lily and Linda are connected by 
co_corp relationship. From LinkedIn, four kinds of 
social relationship between people are extracted 
from the Education field and Experience field. 
They are: 
? co_major denotes that two persons have the 
same major at school 
? co_univ denotes that two persons are graduat-
ed from the same university 
? co_title denotes that two persons have the 
same title at corporation. 
? co_corp denotes that two persons work at the 
same corporation. 
Our basic motivation of using social connection 
lies in the fact that ?connected? people will tend to 
hold related experience and similar summaries.  
We then give the statistics of edges of social 
connection. Table 3 shows basic statistics across 
these edges. From Table 3, we can see that the 
number of users is 497 while the number of social 
connection edges is 14,307. The latter is much 
larger than the former. The number of the edges 
from Education field is similar with the number of 
the edges from Experience filed. Among all the 
relationships, co_unvi is the most common one.  
 
 Numbers 
# users 497 
co_major 1,288 
co_unvi 6,015 
# education field 7,303 
co_title 3,228 
co_corp 3,776 
# experience field 7,004 
# total edges 14,307 
Table 3: The statistic of edges for our main datasets 
5 Collective Factor Graph Model 
In this section, we propose a collective factor 
graph (CoFG) model for learning and summarizing 
the text of personal profile with local textual in-
formation and social connection. 
5.1 Overview of Our Framework 
To generate summaries for profiles, a straightfor-
ward approach is to treat each personal profile in-
dependently and generating a summary for each 
personal profile individually. As we mentioned on 
Section 3.3, we use the sentences of Experience 
field as a text document and consider it as the 
source of summary for each profile. 
Instead, we formalize the problem of personal 
profile summarization in a pair-wise factor graph 
model and propose an approach referred to as 
Loopy Belief Propagation algorithm to learn the 
model for generating the summary of the profile. 
Our basic idea is to define the correlations using 
different types of factor functions. An objective 
function is defined based on the joint probability 
of the factor functions. Thus, the problem of col-
lective personal profile summarization model 
learning is cast as learning model parameters that 
maximizes the joint probability of the input con-
tinuous dynamic network. 
The overview of the proposed method is a su-
pervised framework (as shown in Figure 3).  First, 
we treat each sentence of the training data and test-
ing data as vectors with textual information (local 
textual attribute functions); Second, all the vectors 
are connected by social connection relationships 
(social connection factors) and we model these 
vectors and their relationships into the collective 
factor graph; third, we propose Loopy Belief Prop-
 
John 
Antony 
   Bill 
Lily  
Lucy  
       Linda 
 
 
 
 
 
co_major 
co_univ 
co_corp 
co_corp 
co_title 
co_title 
co_major 
co_univ 
719
agation algorithm to learn the model and predict 
the sentences of testing data; finally, we select a 
subset of sentences of each testing profile as the 
summary according to the models with top-n pre-
diction score. Thus, the core issues of our frame-
work are 1) how to define the collective factor 
graph model to connection profiles with social 
connection; 2) how to learn and predict the pro-
posed CoFG model; 3) how to predict the sentenc-
es from the testing data with the proposed CoFG 
model, and generate the summary by the predict 
scores. We will discuss these issues on the follow-
ing subsections. 
 
 
Figure 3: The overview of our proposed framework 
 
5.2 Model Definition 
Formally, given a network ( , , , )L UG V S S X? , 
each sentence 
is  is associated with an attribute 
vector 
ix  of the profile and a label iy  indicating 
whether the sentence is selected as a summary of 
the profile (The value of 
iy  is binary. 1 means that 
the sentence is selected as a summary sentence, 
whereas 0 stands for the opposite). V denotes the 
authors of the profiles, LS  denotes the labeled 
training data, and US denotes the unlabeled testing 
data. Let { }iX x? and { }iY y? . Then, we have the 
following formulation 
         
? ? ? ? ? ?? ?
, || , ,
P X G Y P YP Y X G P X G?
             (1) 
Here, G denotes all forms of network infor-
mation. This probabilistic formulation indicates 
that labels of skills depend on not only local at-
tributes X, but also the structure of the network G. 
According to Bayes? rule, we have 
         ? ? ? ? ? ?? ?
? ? ? ?
, |
| ,
,
                  | |
P X G Y P Y
P Y X G
P X G
P X Y P Y G
?
?
             (2) 
Where ( | )P Y G represents the probability of labels 
given the structure of the network and ( | )P X Y  
denotes the probability of generating attributes X
associated to their labels Y . We assume that the 
generative probability of attributes given the label 
of each edge is conditionally independent, thus we 
have 
? ? ? ? ? ?| , | |i iiP Y X G P Y G P x y? ?
    (3) 
Where ( | )i iP x y  is the probability of generating 
attributes 
ix given the label iy . Now, the problem 
becomes how to instantiate the probability 
( | )P Y G and ( | )i iP x y . We model them in a Mar-
kov random field, and thus according to the Ham-
mersley-Clifford theorem (Hammersley and 
Clifford, 1971), the two probabilities can be in-
stantiated as follows: 
? ? ? ?
11
1| exp ,
d
i i j j ij i
j
P x y f x yZ ??
? ?? ? ?? ??
       (4) 
? ? ? ?
( )2
1| exp ,
i j NB i
P Y G g i jZ ?
? ?? ? ?? ?? ?
       (5) 
                       
Where 
1 2 and Z Z  are normalization factors. Eq. 4 
indicates that we define an attribute function 
( , )i if x y  for each attribute ijx
 associated with 
sentence
is . j?  is the weight of the j
th attribute. Eq. 
5 represents that we define a set of correlation fac-
tor functions ( , )g i j  over each pair ( , )i j in the 
network. ( )NB i  denotes the set of social relation-
ship neighbors nodes of i.  
 
 
Training  
Set 
 
  
  
Social  
Connection 
Social  
Connection 
Testing  
Set 
  Sentence Scoring 
  Sentence Selection 
 Summarized Profile 
Profiles 
Profiles 
Collective Factor Graph 
Modeling 
  
720
 1 
3 
2 
  
  
 
 
 
  
 
 
 
 
 
 
 
f (v1,y1) 
y
2
 
y
1
 y3 
y
4
 
y
5
 
y
6
 
 S
1
 
 S
2
 
S
3
 
S
4
 
S
5
 
S
6
 
f (v
1
,y
2
) 
f (v
6
,y
6
) 
 
CoFG model 
Nodes of sentences 
with different people 
y1=0 
y
2
=1 
y
3
=1 
y
4
=0 
y
6
=? 
y
5
=? g (y
3
,y
5
) 
Figure 4: Graph representation of CoFG 
The left figure shows the personal profile network. Each dotted line indicates a social connection. Each dotted 
square denotes a person, and the grey square denotes the sentence selected in the summary, and the white square 
denotes a sentence that is not selected as the summary.. 
The right figure shows the CoFG model derived from left figure. Each eclipse denotes a sentence vector of a 
person, and each circle indicates the hidden variable yi. f(vi,yi) indicates the attribute factor function. g(yi,yj) indi-
cates the social connection factor function. 
 
4 
5 
6 
  
  
co_major 
co_corp 
  
Person A 
Person B 
Person C 
We now briefly introduce possible ways to de-
fine the attribute functions{ ( , )}ij i jf x y
, and factor 
function ( , )g i j  .  
Local textual attribute functions{ ( , )}ij i jf x y
: 
It denotes the attribute value associated with each 
sentence i. We define the local textual attribute as 
a feature (Lafferty et al, 2001). We can accumu-
late all the attribute functions and obtain local en-
tropy for a person: 
? ?
1
1 exp ,k k ik i
i k
f x yZ ?
? ?? ?? ???
              (6) 
The textual attributes include following features 
(Shen et al, 2007; Yang et al, 2011b):  
1) BOW: the bag-of-words of each sentence, we 
use unigram features as the basic textual fea-
tures for each sentence.  
2) Length: the number of terms of each sentence. 
3) Topic_words: these are the most frequent 
words in the sentence after the stop words are 
removed. 
4) PageRank_scores: as shown in the related 
work section, a document can be treated as a 
graph and applying a graph-based ranking al-
gorithm (Wan and Yang., 2008). We thus use 
the PageRank score to reflect the importance 
of each sentence. 
Social connection factor function ( , )i jg y y
: 
For the social correlation factor function, we de-
fine it through the pairwise network structure. That 
is, if the person of sentence i and the person of 
sentence j have a social relationship, a factor func-
tion for this social connection is defined (Tang et 
al., 2011a; Tang et al, 2011b), i.e., 
? ? ? ?? ?2, expi j ij i jg y y y y?? ?         (7) 
The person-person social relationships are de-
fined on Section 4, e.g. co_major, co_univ, co_title, 
and co_corp. We define that if two persons have at 
least one social connection edge, they have a so-
cial relationship. In addition, 
ij?  is the weight of 
the function, representing the influence degree of i 
on j. 
To better understand our model, one example of 
factor decomposition is given in Figure 4. In this 
example, there are six sentences from three pro-
files. Among them, four sentences are labeled (two 
are labeled with the category of ?1?, i.e,  1y ?  and 
the other two are labeled with the category of ?0?, 
i.e., 0y ? ) and two sentences are unlabeled (they 
are represented by y=?). We have six attribute 
functions. For example, 
1( , )if v y  denotes the set 
721
of local textual attribute functions of 
iy . We also 
have five pairwise relationships (e.g.,
2 4( , )y y ,
3 5( , )y y ) based on the structure of the input per-
sonal profile social network. For example, 
3 5( , )g y y  denotes social connection between 3y  
and 
5y , while they share the co_major relationship 
on the left figure. 
5.3 Model Learning 
We now address the problem of estimating the free 
parameters. The objective of learning the CoFG 
model is to estimate a parameter configuration 
({ },{ })? ? ??  to maximize the log-likelihood ob-
jective function ( ) log ( | , )L P Y X G?? ? , i.e., 
? ?* argmax L? ??                     (9) 
To solve the objective function, we adopt a gra-
dient descent method. We use ?  (the weight of 
the social connection factor function ( , )i jg y y
) as 
the example to explain how we learn the parame-
ters (the algorithm also applies to tune ?  by simp-
ly replacing ? with? ). Specifically, we first write 
the gradient of each 
k? with regard to the objective 
function (Eq. 9) :  
  ? ? ? ? ? ?( | , ), ,kP Y X G
k
L E g i j E g i j?
?
? ? ? ? ? ? ?? ? ? ?
   (10) 
Where [ ( , )]E g i j is the expectation of factor 
function ( , )g i j  given the data distribution (essen-
tially it can be considered as the average value of 
the factor function ( , )g i j over all pair in the train-
ing data); and 
( | , ) [ ( , )]k Y X GPE g i j?
is the expectation of 
factor function ( , )g i j under the distribution 
( | , )kP Y X G?
given by the estimated model. A 
similar gradient can be derived for parameter
ja
. 
We approximate the marginal distribution
( | , ) [ ( , )]k Y X GPE g i j?
 using LBP (Tang et al, 2011; 
Zhuang et al, 2012). With the marginal probabili-
ties, the gradient can be obtained by summing over 
all triads. It is worth noting that we need to per-
form the LBP process twice for each iteration: one 
is to estimate the marginal distribution of unknown 
variables ?iy ?  and the other is to estimate the 
marginal distribution over all pairs. In this way, 
the algorithm essentially performs a transfer learn-
ing over the complete network. Finally, with the 
obtained gradient, we update each parameter with 
a learning rate? . The learning algorithm is sum-
marized in Figure 5. 
 
Input: Network G , Learning rate ?   
Output: Estimated parameters ?   
Initialize 0? ?   
Repreat 
1) Perform LBP to calculate the 
marginal distribution of unknown 
variables, i.e., ? ?| ,i iP y x G   
2) Perform LBP to calculate the 
marginal distribution of each  
variables, i.e., ? ?( , ), | ,i j i jP y y X G  
3) Calculate the gradient of 
k? ac-
cording to Eq. 10 (for a  with a 
similar formula) 
4) Update parameter ?  with the 
learning rate ?  
               
? ?
new old
L ?? ? ? ?? ?  
Until Convergence 
Figure 5: The Learning Algorithm for CoFG model 
 
5.4 Model Prediction and Summary Gener-
ated 
We can see that in the learning process, the learn-
ing algorithm uses an additional loopy belief prop-
agation to infer the label of unknown relationships. 
With the estimated parameter ? , the summariza-
tion process is to find the most likely configuration 
of Y  for a given profile. This can be obtained by  
? ?* argmax | , ,Y L Y X G ??              (11) 
Finally, we select a subset of sentences of each 
testing profile as the summary according to the 
trained models with top-n prediction scores by *Y   
(Tang et al, 2011b; Dong et al 2012).  
6 Experimentation 
In this section, we describe the settings of our ex-
periment and present the experimental results of 
our proposed CoFG model. 
722
6.1 Experiment Settings 
In the experiment, we use the corpus collected 
from LinkedIn.com that contains 497 profiles (see 
more details in Section 3). The existing summaries 
in these profiles are served as the reference sum-
mary (the standard answers). As discussed in sub-
section 3.3, the average length of summary is 
about 40 words. Thus, we extract 40 words to con-
struct the summary for each profile. We use 200 
personal profiles as the testing data, and the re-
maining ones as the training data. 
We use the ROUGE-1.5.5 (Lin and Hovy, 2004) 
toolkit for evaluation, a popular tool that has been 
widely adopted by several evaluations such as 
DUC and TAC (Wan and Yang, 2008; Wan, 2011). 
We provide four of the ROUGE F-measure scores 
in the experimental results: ROUGE-2 (bigram-
based), ROUGE-L (based on longest common 
subsequences), ROUGE-W (based on weighted 
longest common subsequence, weight=1.2), and 
ROUGE-SU4 (based on skip bigram with a maxi-
mum skip distance of 4).  
6.2 Experimental Results 
We compare the proposed CoFG approach with 
three baselines illustrated as follows: 
? Random: we randomly select sentences of 
each profile to generate the summary for the 
profile. 
? HITS: we employ the HITS algorithm to per-
form profile summarization (Wan and Yang, 
2008). In detail, we first consider the words as 
hubs the sentences as authorities; Then, we 
rank the sentences with the authorities? scores 
for each profile individually; Finally, the 
highest ranked sentences are chosen to consti-
tute the summary. 
? PageRank: we employ the PageRank algo-
rithm to perform profile summarization (Wan 
and Yang, 2008). In detail, we first connect 
the sentences of the profile with cosine text-
based similar measure to construct a graph; 
Then, we apply PageRank algorithm to rank 
the sentence through the graph for each pro-
file individually; Finally, the highest ranked 
sentences are chosen to constitute the sum-
mary.  
?  MaxEnt: as a supervised learning approach, 
maximum entropy uses textual attribute as 
features to train a classification model. Then, 
the classification model is employed to pre-
dict which sentences can be selected to gener-
ate the summary. For the implementation of 
MaxEnt, we employ the tool of mallent 
toolkits4. 
Table 4 shows the comparison results of our ap-
proach (CoFG) and the baseline approaches. From 
Table 4, we can see that 1) either HITS or Pag-
eRank outperforms the approach of  random selec-
tion; 2) The supervised approach i.e. MaxEnt, out-
performs both the HITS algorithm and the Pag-
eRank approach; 3) CoFG model performs best 
and it greatly outperforms both the unsupervised 
and supervised learning baseline approaches in 
terms of the ROUGE-2 F-measure score. This re-
sult verifies the effectiveness of considering the 
social connection between the sentences in differ-
ent profiles, 
Figure 6 shows the performance of our proposed 
CoFG model with different sizes of training data. 
From Figure 6, we can see that CoFG model with 
social connection always performs better than 
MaxEnt, and the performance of our approach de-
scends slowly when the training dataset becomes 
small. Specifically, the performance of CoFG us-
ing only 10% training data achieves better perfor-
mance than MaxEnt using 100% training data. 
 
                                                 
4 http://mallet.cs.umass.edu/ 
 ROUGE-2 ROUGE-L ROUGE-W ROUGE-SU4 
Random 0.0219 0.1363 0.0831 0.0288 
HITS 0.0295 0.1499 0.0905 0.0355 
PageRank 0.0307 0.1574 0.0944 0.0383 
MaxEnt 0.0349 0.1659 0.0995 0.0377 
CoFG 0.0383 0.1696 0.1015 0.0415 
Table 4: Performances of different approaches to profile summarization in terms of different measurements 
723
 
Figure 6:  The performance of CoFG with different 
training data size 
 
Table 5 shows the contribution of the social 
edges with CoFG. Specifically, CoFG is our pro-
posed approach with both education and experi-
ence information, CoFG-edu means that the CoFG 
model considers the social edges of education field 
(co_major, co_univ) only, and CoFG-exp means 
that the CoFG model considers the social edges of 
work experience field (co_title, co_corp) only. 
MaxEnt can be considered as using textual infor-
mation only. 
 
 ROUGE-2 
MaxEnt 0.0349 
CoFG 0.0383 
CoFG-edu 0.0382 
CoFG-exp 0.0381 
Table 5: ROUGE-2 F-Measure score of the contribu-
tion of social edges 
 
From Table 5, we can see that all of our pro-
posed approaches, i.e., CoFG-edu, CoFG-exp, and 
CoFG, outperform the baseline approach, i.e., 
MaxEnt. However, the performance of CoFG-edu, 
CoFG-exp and CoFG are similar. This result is 
mainly due to the fact that the information of so-
cial connection is redundant. For example, two 
persons who are connected by co_major (educa-
tion field) might also be connected by co_corp 
(experience field).  
7 Conclusion and Future Work 
In this paper, we present a novel task named pro-
file summarization and propose a novel approach 
called collective factor graph model to address this 
task. One distinguishing feature of the proposed 
approach lies in its incorporating the social con-
nection. Empirical studies demonstrate that the 
social connection is effective for profile summari-
zation, which enables our approach outperform 
some competitive supervised and unsupervised 
baselines. 
The main contribution of this paper is to explore 
social context information to help generate the 
summary of the profiles, which represents an in-
teresting research direction in social network min-
ing. In the future work, we will explore more kinds 
of social context information and investigate better 
ways of incorporating them into profile summari-
zation and a wider range of social network mining. 
 
Acknowledgments 
 
This research work is supported by the National 
Natural Science Foundation of China 
(No.61273320, No.61272257, No.61331011 and 
No.61375073), and National High-tech Research 
and Development Program of China 
(No.2012AA011102). 
We thank Dr. Jie Tang and Honglei Zhuang for 
providing their software and useful suggestions 
about PGM. We acknowledge Dr. Xinfang Liu, 
Yunxia Xue and Yulai Shen for corpus construc-
tion and insightful comments. We also thank 
anonymous reviewers for their valuable sugges-
tions and comments.  
References  
Baeza-Yates R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. ACM Press and Addison Wes-
ley, 1999 
Celikyilmaz A. and D. Hakkani-Tur. 2011. Discovery 
of Topically Coherent Sentences for Extractive 
Summarization. In Proceeding of ACL-11. 
Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao, 
and H. Cao. 2012. Link Prediction and Recommen-
dation across Heterogeneous Social Networks. In 
Proceedings of ICDM-12. 
Elson D., N. Dames and K. McKeown. 2010. Extracting 
Social Networks from Literary Fiction. In Proceed-
ing of ACL-10. 
Erkan G. and D. Radev. 2004. LexPageRank: Prestige 
in Multi-document Text Summarization. In Proceed-
ings of EMNLP-04. 
Guy I., N. Zwerdling, I.  Ronen, D. Carmel, E. Uziel. 
2010. Social Media Recommendation based on Peo-
ple and Tags. In Proceeding of SIGIR-10. 
0.030
0.032
0.034
0.036
0.038
0.040
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R
O
U
G
E
-2
 
size of training data 
PageRank MaxEnt CFG
724
Hammersley J. and P. Clifford. 1971. Markov Field on 
Finite Graphs and Lattices, Unpublished manuscript. 
1971. 
Hu P., C. Sun, L. Wu, D. Ji and C. Teng. 1011. Social 
Summarization via Automatically Discovered Social 
Context. In Proceeding of IJCNLP-11. 
Lafferty J, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of ICML-01. 
Lappas T., K. Punera and T. Sarlos. 2011. Mining Tags 
Using Social Endorsement Networks. In Proceeding 
of SIGIR-11. 
Leskovec J., D. Huttenlocher and J. Kleinberg. 2010. 
Predicting Positive and Negative Links in Online So-
cial Networks. In Proceedings of WWW-10. 
Lin, C. 2004. ROUGE: a Package for Automatic Evalu-
ation of Summaries. In Proceedings of ACL-04 
Workshop on Text Summarization Branches Out. 
Lu Y., P. Tsaparas, A. Ntoulas and L. Polanyi. 2010. 
Exploiting Social Context for Review Quality Pre-
diction. In Proceeding of WWW-10. 
Meng X?F. Wei? X. Liu? M. Zhou? S. Li and H. 
Wang. 2012. Entity-Centric Topic-Oriented Opinion 
Summarization in Twitter. In Proceeding of KDD-12.  
Murphy K., Y. Weiss, and M. Jordan. 1999. Loopy Be-
lief Propagation for Approximate Inference: An Em-
pirical Study. In Proceedings of UAI-99. 
Radev D. and K. McKeown. 1998. Generating Natural 
Language Summaries from Multiple On-line Sources. 
Computational Linguistics, 24(3):469?500. 
Radev D., H. Jing, M. Stys, and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management. 40 (2004), 
919-938. 
Rosenthal S. and K. McKeown. 2011. Age Prediction in 
Blogs: A Study of Style, Content, and OnlineBehav-
ior in Pre- and Post-Social Media Generations. In 
Proceeding of ACL-11. 
Ryang S. and T. Abekawa. 2012. Framework of Auto-
matic Text Summarization Using Reinforcement 
Learning. In Proceeding of EMNLP-2012. 
Shen D., J. Sun, H. Li, Q. Yang and Zheng Chen. 2007. 
Document Summarization using Conditional Ran-
dom Fields. In Proceeding of IJCAI-07. 
Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou and P. Li. 
2011. User-Level Sentiment Analysis Incorporating 
Social Networks. In Proceedings of KDD-11. 
Tang W., H. Zhuang, and J. Tang. 2011a. Learning to 
Infer Social Ties in Large Networks. In Proceedings 
of ECML/PKDD-11. 
Tang J., Y. Zhang, J. Sun, J. Rao, W. Yu, Y. Chen, and 
A. Fong. 2011b. Quantitative Study of Individual 
Emotional States in Social Networks. IEEE Transac-
tions on Affective Computing. vol.3(2), Pages 132-
144. 
Wan X. and J. Yang. 2008. Multi-document Summari-
zation using Cluster-based Link Analysis. In Pro-
ceedings of SIGIR-08. 
Wan X. 2011. Using Bilingual Information for Cross-
Language Document Summarization. In Proceedings 
of ACL-11. 
Wang H. and G. Zhou. 2012. Toward a Unified Frame-
work for Standard and Update Multi-Document 
Summarization. ACM Transactions on Asian Lan-
guage Information Processing. vol.11(2). 
Xing E, M. Jordan, and S. Russell. 2003. A Generalized 
Mean Field Algorithm for Variational Inference in 
Exponential Families. In Proceedings of UAI-03. 
Yang S., B. Long, A. Smola, N. Sadagopan, Z. Zheng 
and H. Zha. 2011a. Like like alike ? Joint Friend-
ship and Interest Propagation in Social Networks. In 
Proceeding of WWW-11. 
Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su and J. Li. 
2011b. Social Context Summarization. In Proceed-
ing of SIGIR-11. 
Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X. 
Wang. 2012. Actively Learning to Infer Social Ties. 
In Proceedings of Data Mining and Knowledge Dis-
covery (DMKD-12), vol.25 (2), pages 270-297. 
725
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 414?423,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Employing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification 
 
Shoushan Li??  Chu-Ren Huang?  Guodong Zhou?  Sophia Yat Mei Lee? 
 
?Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang, 
sophiaym}@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, China 
gdzhou@suda.edu.cn 
 
 
Abstract 
In this paper, we adopt two views, personal 
and impersonal views, and systematically 
employ them in both supervised and 
semi-supervised sentiment classification. Here, 
personal views consist of those sentences 
which directly express speaker?s feeling and 
preference towards a target object while 
impersonal views focus on statements towards 
a target object for evaluation. To obtain them, 
an unsupervised mining approach is proposed. 
On this basis, an ensemble method and a 
co-training algorithm are explored to employ 
the two views in supervised and 
semi-supervised sentiment classification 
respectively. Experimental results across eight 
domains demonstrate the effectiveness of our 
proposed approach. 
1 Introduction 
As a special task of text classification, sentiment 
classification aims to classify a text according to 
the expressed sentimental polarities of opinions 
such as ?thumb up? or ?thumb down? on the 
movies (Pang et al, 2002). This task has recently 
received considerable interests in the Natural 
Language Processing (NLP) community due to its 
wide applications. 
In general, the objective of sentiment 
classification can be represented as a kind of 
binary relation R, defined as an ordered triple (X, 
Y, G), where X is an object set including different 
kinds of people (e.g. writers, reviewers, or users), 
Y is another object set including the target 
objects (e.g. products, events, or even some 
people), and G is a subset of the Cartesian 
product X Y? . The concerned relation in 
sentiment classification is X ?s evaluation on Y, 
such as ?thumb up?, ?thumb down?, ?favorable?, 
and ?unfavorable?. Such relation is usually 
expressed in text by stating the information 
involving either a person (one element in X ) or a 
target object itself (one element in Y ). The first 
type of statement called personal view, e.g. ?I am 
so happy with this book?, contains X ?s 
?subjective? feeling and preference towards a 
target object, which directly expresses 
sentimental evaluation. This kind of information 
is normally domain-independent and serves as 
highly relevant clues to sentiment classification. 
The latter type of statement called impersonal 
view, e.g. ?it is too small?, contains Y ?s 
?objective? (i.e. or at least criteria-based) 
evaluation of the target object. This kind of 
information tends to contain much 
domain-specific classification knowledge. 
Although such information is sometimes not as 
explicit as personal views in classifying the 
sentiment of a text, speaker?s sentiment is 
usually implied by the evaluation result.  
It is well-known that sentiment classification 
is very domain-specific (Blitzer et al, 2007), so 
it is critical to eliminate its dependence on a 
large-scale labeled data for its wide applications. 
Since the unlabeled data is ample and easy to 
collect, a successful semi-supervised sentiment 
classification system would significantly 
minimize the involvement of labor and time. 
Therefore, given the two different views 
mentioned above, one promising application is to 
adopt them in co-training algorithms, which has 
been proven to be an effective semi-supervised 
learning strategy of incorporating unlabeled data 
to further improve the classification performance 
(Zhu, 2005). In addition, we would show that 
personal/impersonal views are linguistically 
marked and mining them in text can be easily 
performed without special annotation.  
414
In this paper, we systematically employ 
personal/impersonal views in supervised and 
semi-supervised sentiment classification. First, 
an unsupervised bootstrapping method is adopted 
to automatically separate one document into 
personal and impersonal views. Then, both views 
are employed in supervised sentiment 
classification via an ensemble of individual 
classifiers generated by each view. Finally, a 
co-training algorithm is proposed to incorporate 
unlabeled data for semi-supervised sentiment 
classification. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
unsupervised approach for mining personal and 
impersonal views. Section 4 and Section 5 
propose our supervised and semi-supervised 
methods on sentiment classification respectively. 
Experimental results are presented and analyzed 
in Section 6. Section 7 discusses on the 
differences between personal/impersonal and 
subjective/objective. Finally, Section 8 draws our 
conclusions and outlines the future work. 
2 Related Work 
Recently, a variety of studies have been reported 
on sentiment classification at different levels: 
word level (Esuli and Sebastiani, 2005), phrase 
level (Wilson et al, 2009), sentence level (Kim 
and Hovy, 2004; Liu et al, 2005), and document 
level (Turney, 2002; Pang et al, 2002). This 
paper focuses on the document-level sentiment 
classification. Generally, document-level 
sentiment classification methods can be 
categorized into three types: unsupervised, 
supervised, and semi-supervised. 
Unsupervised methods involve deriving a 
sentiment classifier without any labeled 
documents. Most of previous work use a set of 
labeled sentiment words called seed words to 
perform unsupervised classification. Turney 
(2002) determines the sentiment orientation of a 
document by calculating point-wise mutual 
information between the words in the document 
and the seed words of ?excellent? and ?poor?. 
Kennedy and Inkpen (2006) use a term-counting 
method with a set of seed words to determine the 
sentiment. Zagibalov and Carroll (2008) first 
propose a seed word selection approach and then 
apply the same term-counting method for Chinese 
sentiment classifications. These unsupervised 
approaches are believed to be 
domain-independent for sentiment classification. 
Supervised methods consider sentiment 
classification as a standard classification problem 
in which labeled data in a domain are used to 
train a domain-specific classifier. Pang et al 
(2002) are the first to apply supervised machine 
learning methods to sentiment classification. 
Subsequently, many other studies make efforts to 
improve the performance of machine 
learning-based classifiers by various means, such 
as using subjectivity summarization (Pang and 
Lee, 2004), seeking new superior textual features 
(Riloff et al, 2006), and employing document 
subcomponent information (McDonald et al, 
2007). As far as the challenge of 
domain-dependency is concerned, Blitzer et al 
(2007) present a domain adaptation approach for 
sentiment classification. 
Semi-supervised methods combine unlabeled 
data with labeled training data (often 
small-scaled) to improve the models. Compared 
to the supervised and unsupervised methods, 
semi-supervised methods for sentiment 
classification are relatively new and have much 
less related studies. Dasgupta and Ng (2009) 
integrate various methods in semi-supervised 
sentiment classification including spectral 
clustering, active learning, transductive learning, 
and ensemble learning. They achieve a very 
impressive improvement across five domains. 
Wan (2009) applies a co-training method to 
semi-supervised learning with labeled English 
corpus and unlabeled Chinese corpus for Chinese 
sentiment classification. 
3 Unsupervised Mining of Personal and 
Impersonal Views 
As mentioned in Section 1, the objective of 
sentiment classification is to classify a specific 
binary relation: X ?s evaluation on Y, where X is 
an object set including different kinds of persons 
and Y is another object set including the target 
objects to be evaluated. First of all, we focus on 
an analysis on sentences in product reviews 
regarding the two views: personal and 
impersonal views.  
The personal view consists of personal 
sentences (i.e. X ?s sentences) exemplified 
below: 
I. Personal preference: 
E1: I love this breadmaker! 
E2: I disliked it from the beginning. 
II. Personal emotion description: 
E3: Very disappointed! 
E4: I am happy with the product. 
III. Personal actions: 
415
E5: Do not waste your money. 
E6: I have recommended this machine to all my 
friends. 
The impersonal view consists of impersonal 
sentences (i.e.Y ?s sentences) exemplified below: 
I. Impersonal feature description: 
E7: They are too thin to start with. 
E8: This product is extremely quiet. 
II. Impersonal evaluation: 
E9: It's great. 
E10: The product is a waste of time and money. 
III. Impersonal actions: 
E11: This product not even worth a penny. 
E12: It broke down again and again. 
We find that the subject of a sentence presents 
important cues for personal/impersonal views, 
even though a formal and computable definition 
of this contrast cannot be found. Here, subject 
refers to one of the two main constituents in the 
traditional English grammar (the other 
constituent being the predicate) (Crystal, 2003)1. 
For example, the subjects in the above examples 
of E1, E7 and E11 are ?I?, ?they?, and ?this 
product? respectively. For automatic mining the 
two views, personal/impersonal sentences can be 
defined according to their subjects: 
Personal sentence: the sentence whose 
subject is (or represents) a person. 
Impersonal sentence: the sentence whose 
subject is not (does not represent) a person. 
In this study, we mainly focus on product 
review classification where the target object in 
the set Y  is not a person. The definitions need 
to be adjusted when the evaluation target itself is 
a person, e.g. the political sentiment 
classification by Durant and Smith (2007). 
Our unsupervised mining approach for mining 
personal and impersonal sentences consists of 
two main steps. First, we extract an initial set of 
personal and impersonal sentences with some 
heuristic rules: If the first word of one sentence 
is (or implies) a personal pronoun including ?I?, 
?we?, and ?do?, then the sentence is extracted as a 
personal sentence; If the first word of one 
sentence is an impersonal pronoun including 'it', 
'they', 'this', and 'these', then the sentence is 
extracted as an impersonal sentence. Second, we 
apply the classifier which is trained with the 
initial set of personal and impersonal sentences 
to classify the remaining sentences. This step 
aims to classify the sentences without pronouns 
                                                      
1
 The subject has the grammatical function in a sentence of 
relating its constituent (a noun phrase) by means of the verb to any 
other elements present in the sentence, i.e. objects, complements, 
and adverbials. 
(e.g. E3). Figure 1 shows the unsupervised 
mining algorithm. 
Input: 
The training data D
 
 
Output: 
    All personal and impersonal sentences, i.e. 
sentence sets personalS  and impersonalS . 
Procedure: 
(1). Segment all documents in D to sentences 
S using punctuations (such as periods and 
interrogation marks) 
(2). Apply the heuristic rules to classify the 
sentences S  with proper pronouns into, 1pS  
and  1iS  
(3). Train a binary classifier p if ?  with  1pS  and  
1iS  
(4). Use  p if ?  to classify the remaining sentences 
into  2pS  and  2iS  
(5). 1 2personal p pS S S= ? ,  1 2impersonal i iS S S= ?  
 
Figure 1: The algorithm for unsupervised mining 
personal and impersonal sentences from a training 
data 
4 Employing Personal/Impersonal 
Views in Supervised Sentiment 
Classification 
After unsupervised mining of personal and 
impersonal sentences, the training data is divided 
into two views: the personal view, which 
contains personal sentences, and the impersonal 
view, which contains impersonal sentences. 
Obviously, these two views can be used to train 
two different classifiers, 1f  and 2f , for 
sentiment classification respectively.  
Since our mining approach is unsupervised, 
there inevitably exist some noises. In addition, 
the sentences of different views may share the 
same information for sentiment classification. 
For example, consider the following two 
sentences: ?It is a waste of money.? and ?Do not 
waste your money.? Apparently, the first one 
belongs to the impersonal view while the second 
one belongs to personal view, according to our 
heuristic rules. However, these two sentences 
share the same word, ?waste?, which conveys 
strong negative sentiment information. This 
suggests that training a single-view classifier 3f  
with all sentences should help. Therefore, three 
base classifiers, 1f , 2f , and 3f , are eventually 
derived from the personal view, the impersonal 
416
view and the single view, respectively. Each base 
classifier provides not only the class label 
outputs but also some kinds of confidence 
measurements, e.g. posterior probabilities of the 
testing sample belonging to each class.  
Formally, each base classifier  ( 1,2,3)lf l =  
assigns a test sample (denoted as lx ) a posterior 
probability vector ( )lP x

:  
1 2( ) ( | ), ( | ) tl l lP x p c x p c x= < >

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging to 1c . 
In the ensemble learning literature, various 
methods have been presented for combining base 
classifiers. The combining methods are 
categorized into two groups (Duin, 2002): fixed 
rules such as voting rule, product rule, and sum 
rule (Kittler et al, 1998), and trained rules such 
as weighted sum rule (Fumera and Roli, 2005) 
and meta-learning approaches (Vilalta and Drissi, 
2002). In this study, we choose a fixed rule and a 
trained rule to combine the three base classifiers 
1f , 2f , and 3f .  
The chosen fixed rule is product rule which 
combine base classifiers by multiplying the 
posterior possibilities and using the multiplied 
possibility for decision, i.e. 
3
1
                 
  arg max ( | )
j
i l
i l
assign y c
where j p c x
=
?
= ?  
The chosen trained rule is stacking (Vilalta and 
Drissi, 2002; D?eroski and ?enko, 2004) where a 
meta-classifier is trained with the output of the 
base classifiers as the input. Formally, let 'x  
denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( | ' ), ( | ' )l l l lP x p c x p c x=< >

 
Then, a meta-classifier is trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ' ), ( ' ), ( ' )meta l l lx P x P x P x= = ==< >
  
 
In our experiments, we perform stacking with 
4-fold cross validation to generate meta-training 
data where each fold is used as the development 
data and the other three folds are used to train the 
base classifiers in the training phase. 
5 Employing Personal/Impersonal 
Views in Semi-Supervised Sentiment 
Classification 
Semi-supervised learning is a strategy which 
combines unlabeled data with labeled training 
data to improve the models. Given the two-view 
classifiers 1f  and 2f  along with the single-view 
classifier 3f , we perform a co-training algorithm 
for semi-supervised sentiment classification. The 
co-training algorithm is a specific 
semi-supervised learning approach which starts 
with a set of labeled data and increases the 
amount of labeled data using the unlabeled data 
by bootstrapping (Blum and Mitchell, 1998). 
Figure 2 shows the co-training algorithm in our 
semi-supervised sentiment classification. 
Input: 
The labeled data L
 
containing personal 
sentence set L personalS ?  and impersonal sentence set 
L impersonalS ?  
The unlabeled data U  containing personal 
sentence set
 
U personalS ?  and impersonal sentence set 
U impersonalS ?  
Output: 
    New labeled data L  
Procedure: 
Loop for N iterations untilU ?=  
(1). Learn the first classifier 1f  with L personalS ?  
(2). Use 1f  to label samples from U with 
U personalS ?  
(3). Choose 1n  positive and 1n negative most 
confidently predicted samples 1A  
(4). Learn the second classifier 2f  with L impersonalS ?  
(5). Use 2f to label samples from U with 
U impersonalS ?   
(6). Choose 2n  positive and 2n negative most 
confidently predicted samples 2A   
(7). Learn the third classifier 3f  with L  
(8). Use 3f  to label samples from U  
(9). Choose 3n  positive and 3n  negative most 
confidently predicted samples 3A  
(10). Add samples 1 2 3A A A? ?  with the 
corresponding labels into L  
(11). Update L personalS ?  and L impersonalS ?  
 
Figure 2: Our co-training algorithm for 
semi-supervised sentiment classification 
417
After obtaining the new labeled data, we can 
either adopt one classifier (i.e. 3f ) or a 
combined classifier (i.e. 1 2 3f f f+ + ) in further 
training and testing. In our experimentation, we 
explore both of them with the former referred to 
as co-training and single classifier and the latter 
referred to as co-training and combined 
classifier. 
6 Experimental Studies 
We have systematically explored our method on 
product reviews from eight domains: book, DVD, 
electronic appliances, kitchen appliances, health, 
network, pet and software. 
6.1 Experimental Setting 
The product reviews on the first four domains 
(book, DVD, electronic, and kitchen appliances) 
come from the multi-domain sentiment 
classification corpus, collected from 
http://www.amazon.com/ by Blitzer et al (2007)2. 
Besides, we also collect the product views from 
http://www.amazon.com/ on other four domains 
(health, network, pet and software)3. Each of the 
eight domains contains 1000 positive and 1000 
negative reviews. Figure 3 gives the distribution 
of personal and impersonal sentences in the 
training data (75% labeled data of all data). It 
shows that there are more impersonal sentences 
than personal ones in each domain, in particular 
in the DVD domain, where the number of 
impersonal sentences is at least twice as many as 
that of personal sentences. This unusual 
phenomenon is mainly attributed to the fact that 
many objective descriptions, e.g. the movie plot 
introductions, are expressed in the DVD domain 
which makes the extracted personal and 
impersonal sentences rather unbalanced. 
We apply both support vector machine (SVM) 
and Maximum Entropy (ME) algorithms with the 
help of the SVM-light4 and Mallet5 tools. All 
parameters are set to their default values. We 
find that ME performs slightly better than SVM 
on the average. Furthermore, ME offers posterior 
probability information which is required for 
                                                      
2
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 Note that the second version of multi-domain sentiment 
classification corpus does contain data from many other domains. 
However, we find that the reviews in the other domains contain 
many duplicated samples. Therefore, we re-collect the reviews from 
http://www.amazon.com/ and filter those duplicated ones. The new 
collection is here:  
http://llt.cbs.polyu.edu.hk/~lss/ACL2010_Data_SSLi.zip 
4
 http://svmlight.joachims.org/  
5
 http://mallet.cs.umass.edu/  
combination methods. Thus we apply the ME 
classification algorithm for further combination 
and co-training. In particular, we only employ 
Boolean features, representing the presence or 
absence of a word in a document. Finally, we 
perform t-test to evaluate the significance of the 
performance difference between two systems 
with different methods (Yang and Liu, 1999). 
Sentence Number in the Training Data
16134
8477 8337 8843
13097
29290
1485214414
12691 11941
1381814265 16441
1475315573
27714
0
10000
20000
30000
40000
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Nu
mb
er
Number of personal sentences
Number of impersonal sentences
Figure 3: Distribution of personal and impersonal 
sentences in the training data of each domain 
6.2 Experimental Results on Supervised 
Sentiment Classification 
4-fold cross validation is performed for 
supervised sentiment classification. For 
comparison, we generate two random views by 
randomly splitting the whole feature space into 
two parts. Each part is seen as a view and used to 
train a classifier. The combination (two random 
view classifiers along with the single-view 
classifier 3f ) results are shown in the last column 
of Table 1. The comparison between random two 
views and our proposed two views will clarify 
whether the performance gain comes truly from 
our proposed two-view mining, or simply from 
using the classifier combination strategy. 
Table 1 shows the performances of different 
classifiers, where the single-view classifier 3f  
which uses all sentences for training and testing, 
is considered as our baseline. Note that the 
baseline performances of the first four domains 
are worse than the ones reported in Blitzer et al 
(2007). But their experiment is performed with 
only one split on the data with 80% as the 
training data and 20% as the testing data, which 
means the size of their training data is larger than 
ours. Also, we find that our performances are 
similar to the ones (described as fully supervised 
results) reported in Dasgupta and Ng (2009) 
where the same data in the four domains are used 
and 10-fold cross validation is performed.  
418
Domain Personal 
View 
Classifier 
1f  
Impersonal 
View 
Classifier 
2f  
Single View 
Classifier 
(baseline) 
3f
 
Combination  
(Stacking) 
1 2 3f f f+ +  
Combination 
(Product rule) 
1 2 3f f f+ +  
Combination 
with two 
random views 
(Product rule) 
Book 0.7004 0.7474 0.7654 0.7919 0.7949 0.7546 
DVD 0.6931 0.7663 0.7884 0.8079 0.8165 0.8054 
Electronic 0.7414 0.7844 0.8074 0.8304 0.8364 0.8210 
Kitchen 0.7430 0.8030 0.8290 0.8555 0.8565 0.8152 
Health 0.7000 0.7370 0.7559 0.7780 0.7815 0.7548 
Network 0.7655 0.7710 0.8265 0.8360 0.8435 0.8312 
Pet 0.6940 0.7145 0.7390 0.7565 0.7665 0.7423 
Software 0.7035 0.7205 0.7470 0.7730 0.7715 0.7615 
AVERAGE 0.7176 0.7555 0.7823 0.8037 0.8084 0.7858 
 
Table 1: Performance of supervised sentiment classification 
 
From Table 1, we can see that impersonal view 
classifier 1f  consistently performs better than 
personal view classifier 2f . Similar to the 
sentence distributions, the difference in the 
classification performances between these two 
views in the DVD domain is the largest (0.6931 
vs. 0.7663). 
Both the combination methods (stacking and 
product rule) significantly outperform the 
baseline in each domain (p-value<0.01) with a 
decent average performance improvement of 
2.61%. Although the performance difference 
between the product rule and stacking is not 
significant, the product rule is obviously a better 
choice as it involves much easier implementation. 
Therefore, in the semi-supervised learning 
process, we only use the product rule to combine 
the individual classifiers. Finally, it shows that 
random generation of two views with the 
combination method of the product rule only 
slightly outperforms the baseline on the average 
(0.7858 vs. 0.7823) but performs much worse 
than our unsupervised mining of personal and 
impersonal views.  
6.3 Experimental Results on 
Semi-supervised Sentiment 
Classification 
We systematically evaluate and compare our 
two-view learning method with various 
semi-supervised ones as follows: 
Self-training, which uses the unlabeled data 
in a bootstrapping way like co-training yet limits 
the number of classifiers and the number of 
views to one. Only the baseline classifier 3f  is 
used to select most confident unlabeled samples 
in each iteration. 
Transductive SVM, which seeks the largest 
separation between labeled and unlabeled data 
through regularization (Joachims, 1999). We 
implement it with the help of the SVM-light tool. 
Co-training with random two-view 
generation (briefly called co-training with 
random views), where two views are generated 
by randomly splitting the whole feature space 
into two parts.  
In semi-supervised sentiment classification, 
the data are randomly partitioned into labeled 
training data, unlabeled data, and testing data 
with the proportion of 10%, 70% and 20% 
respectively. Figure 4 reports the classification 
accuracies in all iterations, where baseline 
indicates the supervised classifier 3f  trained on 
the 10% data; both co-training and single 
classifier and co-training and combined 
classifier refer to co-training using our proposed 
personal and impersonal views. But the former 
merely applies the baseline classifier 3f  trained 
the new labeled data to test on the testing data 
while the latter applies the combined classifier 
1 2 3f f f+ + . In each iteration, two top-confident 
samples in each category are chosen, i.e. 
1 2 3 2n n n= = = . For clarity, results of other 
methods (e.g. self-training, transductive SVM) 
are not shown in Figure 4 but will be reported in 
Figure 5 later.  
Figure 4 shows that co-training and 
combined classifier always outperforms 
co-training and single classifier. This again 
justifies the effectiveness of our two-view 
learning on supervised sentiment classification.
419
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
Domain: Book
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
0.7
Domain: DVD
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.7
0.72
0.74
0.76
0.78
0.8
Domain: Electronic
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
Domain: Kitchen
Iteration Number
Ac
cu
ra
cy
 
 
 
25 50 75 100 125
0.54
0.56
0.58
0.6
0.62
0.64
0.66
Domain: Health
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Domain: Network
Iteration Number
Ac
cu
ra
cy
 
 
Baseline
Co-traning and single classifier
Co-traning and combined classifier
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
Domain: Pet
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
Domain: Software
Iteration Number
Ac
cu
ra
cy
 
 
 
 
Figure 4: Classification performance vs. iteration numbers (using 10% labeled data as training data) 
 
One open question is whether the unlabeled 
data improve the performance. Let us set aside 
the influence of the combination strategy and 
focus on the effectiveness of semi-supervised 
learning by comparing the baseline and 
co-training and single classifier. Figure 4 
shows different results on different domains. 
Semi-supervised learning fails on the DVD 
domain while on the three domains of book, 
electronic, and software, semi-supervised 
learning benefits slightly (p-value>0.05). In 
contrast, semi-supervised learning benefits much 
on the other four domains (health, kitchen, 
network, and pet) from using unlabeled data and 
the performance improvements are statistically 
significant (p-value<0.01). Overall speaking, we 
think that the unlabeled data are very helpful as 
they lead to about 4% accuracy improvement on 
the average except for the DVD domain. Along 
with the supervised combination strategy, our 
approach can significantly improve the 
performance more than 7% on the average 
compared to the baseline. 
Figure 5 shows the classification results of 
different methods with different sizes of the 
labeled data: 5%, 10%, and 15% of all data, 
where the testing data are kept the same (20% of 
all data). Specifically, the results of other 
methods including self-training, transductive 
SVM, and random views are presented when 
10% labeled data are used in training. It shows 
that self-training performs much worse than our 
approach and fails to improve the performance of 
five of the eight domains. Transductive SVM 
performs even worse and can only improve the 
performance of the ?software? domain. Although 
co-training with random views outperforms the 
baseline on four of the eight domains, it performs 
worse than co-training and single classifier. 
This suggests that the impressive improvements 
are mainly due to our unsupervised two-view 
mining rather than the combination strategy.
420
Using 10% labeled data as training data
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Book DVD Electronic Kitchen Health Network Pet Software
Ac
cu
rac
y
Baseline Transductive SVM Self-training
Co-training with random views Co-training and single classifier Co-training and combined classifier
 
Using 5% labeled data as training data
0.69
0.747
0.584
0.525
0.67 0.6530.626
0.55
0.564
0.683
0.495
0.615
0.8675
0.7855
0.7
0.601
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
Using 15% labeled data as training data
0.763
0.6925
0.765
0.5925
0.679
0.564
0.677
0.7375
0.6625
0.735
0.655
0.615
0.8625
0.8325
0.782
0.716
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
 
Figure 5: Performance of semi-supervised sentiment classification when 5%, 10%, and 15% labeled data are used 
 
Figure 5 also shows that our approach is rather 
robust and achieves excellent performances in 
different training data sizes, although our 
approach fails on two domains, i.e. book and 
DVD, when only 5% of the labeled data are used. 
This failure may be due to that some of the 
samples in these two domains are too ambiguous 
and hard to classify. Manual checking shows that 
quite a lot of samples on these two domains are 
even too difficult for professionals to give a 
high-confident label. Another possible reason is 
that there exist too many objective descriptions 
in these two domains, thus introducing too much 
noisy information for semi-supervised learning. 
The effectiveness of different sizes of chosen 
samples in each iteration is also evaluated like 
1 2 3 6n n n= = = and 1 2 33, 6n n n= = = (This 
assignment is considered because the personal 
view classifier performs worse than the other two 
classifiers). Our experimental results are still 
unsuccessful in the DVD domain and do not 
show much difference on other domains. We also 
test the co-training approach without the 
single-view classifier 3f . Experimental results 
show that the inclusion of the single-view 
classifier 3f  slightly helps the co-training 
approach. The detailed discussion of the results 
is omitted due to space limit. 
6.4 Why our approach is effective? 
One main reason for the effectiveness of our 
approach on supervised learning is the way how 
personal and impersonal views are dealt with. As 
personal and impersonal views have different 
ways of expressing opinions, splitting them into 
two separations can filter some classification 
noises. For example, in the sentence of ?I have 
seen amazing dancing, and good dancing. This 
was TERRIBLE dancing!?. The first sentence is 
classified as a personal sentence and the second 
one is an impersonal sentence. Although the 
words ?amazing? and ?good? convey strong 
positive sentiment information, the whole text is 
negative. If we get the bag-of-words from the 
whole text, the classification result will be wrong. 
Rather, splitting the text into two parts based on 
different views allows correct classification as 
the personal view rarely contains impersonal 
words such as ?amazing? and ?good?. The 
classification result will thus be influenced by 
the impersonal view.  
In addition, a document may contain both 
personal and impersonal sentences, and each of 
them, to a certain extent, , provides classification 
evidence. In fact, we randomly select 50 
documents in the domain of kitchen appliances 
and find that 80% of the documents take both 
personal and impersonal sentences in which both 
of them express explicit opinions. That is to say, 
the two views provide different, complementary 
information for classification. This qualifies the 
success requirement of co-training algorithm to 
some extend. This might be the reason for the 
effectiveness of our approach on semi-supervised 
learning. 
421
7 Discussion on Personal/Impersonal vs. 
Subjective/Objective 
As mentioned in Section 1, personal view 
contains X ?s ?subjective? feeling, and 
impersonal view containsY ?s ?objective? (i.e. or 
at least criteria-based) evaluation of the target 
object. However, our technically-defined 
concepts of personal/impersonal are definitely 
different from subjective/objective: Personal 
view can certainly contain many objective 
expressions, e.g. ?I bought this electric kettle? and 
impersonal view can contain many subjective 
expressions, e.g. ?It is disappointing?.  
Our technically-defined personal/impersonal 
views are two different ways to describe 
opinions. Personal sentences are often used to 
express opinions in a direct way and their target 
object should be one of X. Impersonal ones are 
often used to express opinions in an indirect way 
and their target object should be one of Y. The 
ideal definition of personal (or impersonal) view 
given in Section 1 is believed to be a subset of 
our technical definition of personal (or 
impersonal) view. Thus impersonal view may 
contain both Y ?s objective evaluation (more 
likely to be domain independent) and subjective 
Y?s description. 
In addition, simply splitting text into 
subjective/objective views is not particularly 
helpful. Since a piece of objective text provides 
rather limited implicit classification information, 
the classification abilities of the two views are 
very unbalanced. This makes the co-training 
process unfeasible. Therefore, we believe that 
our technically-defined personal/impersonal 
views are more suitable for two-view learning 
compared to subjective/objective views. 
8 Conclusion and Future Work 
In this paper, we propose a robust and effective 
two-view model for sentiment classification 
based on personal/impersonal views. Here, the 
personal view consists of subjective sentences 
whose subject is a person, whereas the 
impersonal view consists of objective sentences 
whose subject is not a person. Such views are 
lexically cued and can be obtained without 
pre-labeled data and thus we explore an 
unsupervised learning approach to mine them.  
Combination methods and a co-training 
algorithm are proposed to deal with supervised 
and semi-supervised sentiment classification 
respectively. Evaluation on product reviews from 
eight domains shows that our approach 
significantly improves the performance across all 
eight domains on supervised sentiment 
classification and greatly outperforms the 
baseline with more than 7% accuracy 
improvement on the average across seven of 
eight domains (except the DVD domain) on 
semi-supervised sentiment classification. 
In the future work, we will integrate the 
subjectivity summarization strategy (Pang and 
Lee, 2004) to help discard noisy objective 
sentences. Moreover, we need to consider the 
cases when both X and Y appear in a sentence. 
For example, the sentence ?I think they're poor? 
should be an impersonal view but wrongly 
classified as a personal one according to our 
technical rules. We believe that these will help 
improve our approach and hopefully are 
applicable to the DVD domain. Another 
interesting and practical idea is to integrate 
active learning (Settles, 2009), another popular 
but principally different kind of semi-supervised 
learning approach, with our two-view learning 
approach to build high-performance systems 
with the least labeled data. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in 
the Hong Kong Polytechnic University and two 
NSFC grants, No. 60873150 and No. 90920004. 
We also thank the three anonymous reviewers 
for their invaluable comments. 
References  
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Blum A. and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In 
Proceedings of COLT-98. 
Crystal D. 2003. The Cambridge Encyclopedia of the 
English Language. Cambridge University Press. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Durant K. and M. Smith. 2007. Predicting the 
Political Sentiment of Web Log Posts using 
422
Supervised Machine Learning Techniques Coupled 
with Feature Selection. In Processing of Advances 
in Web Mining and Web Usage Analysis. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Esuli A. and F. Sebastiani. 2005. Determining the 
Semantic Orientation of Terms through Gloss 
Classification. In Proceedings of CIKM-05. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Joachims, T. 1999. Transductive Inference for Text 
Classification using Support Vector Machines. 
ICML1999. 
Kennedy A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
McDonald R., K. Hannan, T. Neylon, M. Wells, and J. 
Reynar. 2007. Structured Models for 
Fine-to-coarse Sentiment Analysis. In Proceedings 
of ACL-07. 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06. 
Settles B. 2009. Active Learning Literature Survey. 
Technical Report 1648, Department of Computer 
Sciences, University of Wisconsin at Madison, 
Wisconsin. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu. 1999. A Re-Examination of Text 
Categorization methods. In Proceedings of 
SIGIR-99. 
Zagibalov T. and J. Carroll. 2008. Automatic Seed 
Word Selection for Unsupervised Sentiment 
Classification of Chinese Test. In Proceedings of 
COLING-08.  
Zhu X. 2005. Semi-supervised Learning Literature 
Survey. Technical Report Computer Sciences 1530, 
University of Wisconsin ? Madison. 
 
423
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 511?515,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Modeling of News Reader?s and Comment Writer?s Emotions?
 
 
Huanhuan Liu?  Shoushan Li??*  Guodong Zhou?  Chu-Ren Huang?  Peifeng Li? 
 
?Natural Language Processing Lab 
Soochow University, China 
{huanhuanliu.suda,shoushan.li, 
churenhuang}@gmail.com 
 
?Department of CBS 
the Hong Kong Polytechnic University 
{gdzhou,pfli}@suda.edu.cn 
 
 
Abstract 
Emotion classification can be generally done 
from both the writer?s and reader?s 
perspectives. In this study, we find that two 
foundational tasks in emotion classification, 
i.e., reader?s emotion classification on the 
news and writer?s emotion classification on 
the comments, are strongly related to each 
other in terms of coarse-grained emotion 
categories, i.e., negative and positive. On the 
basis, we propose a respective way to jointly 
model these two tasks. In particular, a co-
training algorithm is proposed to improve 
semi-supervised learning of the two tasks. 
Experimental evaluation shows the 
effectiveness of our joint modeling 
approach.* 
1 Introduction 
Emotion classification aims to predict the emo-
tion categories (e.g., happy, angry, or sad) of a 
given text (Quan and Ren, 2009; Das and Ban-
dyopadhyay, 2009). With the rapid growth of 
computer mediated communication applications, 
such as social websites and miro-blogs, the re-
search on emotion classification has been attract-
ing more and more attentions recently from the 
natural language processing (NLP) community 
(Chen et al, 2010; Purver and Battersby, 2012). 
In general, a single text may possess two kinds 
of emotions, writer?s emotion and reader?s emo-
tion, where the former concerns the emotion ex-
pressed by the writer when writing the text and 
the latter concerns the emotion expressed by a 
reader after reading the text. For example, con-
sider two short texts drawn from a news and cor-
responding comments, as shown in Figure 1. On 
                                                 
* *  Corresponding author 
one hand, for the news text, while its writer just 
objectively reports the news and thus does not 
express his emotion in the text, a reader could 
yield sad or worried emotion. On the other hand, 
for the comment text, its writer clearly expresses 
his sad emotion while the emotion of a reader 
after reading the comments is not clear (Some 
may feel sorry but others might feel careless). 
 
News:  
Today's Japan earthquake could be 
     2011 quake aftershock. ?? 
News Writer?s emotion: None 
News Reader?s emotion: sad, worried 
Comments: 
(1) I hope everything is ok, so sad. I still can 
not forget last year. 
(2) My father-in-law got to experience this 
quake... what a suffering. 
Comment Writer?s emotion: sad 
Comment Reader?s emotion: Unknown 
Figure 1: An example of writer?s and reader?s 
emotions on a news and its comments 
 
Accordingly, emotion classification can be 
grouped into two categories: reader?s emotion 
and writer?s emotion classifications. Although 
both emotion classification tasks have been 
widely studied in recent years, they are always 
considered independently and treated separately.  
However, news and their corresponding com-
ments often appear simultaneously. For example, 
in many news websites, it is popular to see a 
news followed by many comments. In this case, 
because the writers of the comments are a part of 
the readers of the news, the writer?s emotions on 
the comments are exactly certain reflection of the 
reader?s emotions on the news. That is, the 
comment writer?s emotions and the news read-
er?s emotions are strongly related. For example, 
511
in Figure 1, the comment writer?s emotion ?sad? 
is among the news reader?s emotions. 
Above observation motivates joint modeling 
of news reader?s and comment writer?s emotions. 
In this study, we systematically investigate the 
relationship between the news reader?s emotions 
and the comment writer?s emotions. Specifically, 
we manually analyze their agreement in a corpus 
collected from a news website. It is interesting to 
find that such agreement only applies to coarse-
grained emotion categories (i.e., positive and 
negative) with a high probability and does not 
apply to fine-grained emotion categories (e.g., 
happy, angry, and sad). This motivates our joint 
modeling in terms of the coarse-grained emotion 
categories. Specifically, we consider the news 
text and the comment text as two different views 
of expressing either the news reader?s or com-
ment writer?s emotions. Given the two views, a 
co-training algorithm is proposed to perform 
semi-supervised emotion classification so that 
the information in the unlabeled data can be ex-
ploited to improve the classification performance. 
2 Related Work  
2.1 Comment Writer?s Emotion Classifica-
tion 
Comment writer?s emotion classification has 
been a hot research topic in NLP during the last 
decade (Pang et al, 2002; Turney, 2002; Alm et 
al., 2005; Wilson et al, 2009) and previous stud-
ies can be mainly grouped into two categories: 
coarse-grained and fine-grained emotion classifi-
cation. 
Coarse-grained emotion classification, also 
called sentiment classification, concerns only 
two emotion categories, such as like or dislike 
and positive or negative (Pang and Lee, 2008; 
Liu, 2012). This kind of emotion classification 
has attracted much attention since the pioneer 
work by Pang et al (2002) in the NLP communi-
ty due to its wide applications (Cui et al, 2006; 
Riloff et al, 2006; Dasgupta and Ng, 2009; Li et 
al., 2010; Li et al, 2011). 
In comparison, fine-grained emotion classifi-
cation aims to classify a text into multiple emo-
tion categories, such as happy, angry, and sad. 
One main group of related studies on this task is 
about emotion resource construction, such as 
emotion lexicon building (Xu et al, 2010; 
Volkova et al, 2012) and sentence-level or doc-
ument-level corpus construction (Quan and Ren, 
2009; Das and Bandyopadhyay, 2009). Besides, 
all the related studies focus on supervised learn-
ing (Alm et al, 2005; Aman and Szpakowicz, 
2008; Chen et al, 2010; Purver and Battersby, 
2012; Moshfeghi et al, 2011), and so far, we 
have not seen any studies on semi-supervised 
learning on fine-grained emotion classification.  
2.2 News Reader?s Emotion Classification 
While comment writer?s emotion classification 
has been extensively studied, there are only a 
few studies on news reader?s emotion classifica-
tion from the NLP and related communities.  
Lin et al (2007) first describe the task of read-
er?s emotion classification on the news articles 
and then employ some standard machine learning 
approaches to train a classifier for determining 
the reader?s emotion towards a news. Their fur-
ther study, Lin et al (2008) exploit more features 
and achieve a higher performance. 
Unlike all the studies mentioned above, our 
study is the first attempt on exploring the rela-
tionship between comment writer?s emotion 
classification and news reader?s emotion classifi-
cation.  
3 Relationship between News Reader?s 
and Comment Writer?s Emotions 
To investigate the relationship between news 
reader?s and comment writer?s emotions, we col-
lect a corpus of Chinese news articles and their 
corresponding comments from Yahoo! Kimo 
News (http://tw.news.yahoo.com), where each 
news article is voted with emotion tags from 
eight categories: happy, sad, angry, meaningless, 
boring, heartwarming, worried, and useful. 
These emotion tags on each news are selected by 
the readers of the news. Note that because the 
categories of ?useful? and ?meaningless? are not 
real emotion categories, we ignore them in our 
study. Same as previous studies of Lin et al 
(2007) and Lin et al (2008), we consider the 
voted emotions as reader?s emotions on the news, 
i.e., the news reader?s emotions. We only select 
the news articles with a dominant emotion (pos-
sessing more than 50% votes) in our data. Be-
sides, as we attempt to consider the comment 
writer?s emotions, the news articles without any 
comments are filtered. 
As a result, we obtain a corpus of 3495 news 
articles together with their comments and the 
numbers of the articles of happy, sad, angry, 
boring, heartwarming, and worried are 1405, 
230, 1673, 75, 92 and 20 respectively. For 
coarse-grained categories, happy and heartwarm-
ing are merged into the positive category while 
512
sad, angry, boring and worried are merged into 
the negative category. 
Besides the tags of the reader?s emotions, each 
news article is followed by some comments, 
which can be seen as a reflection of the writer?s 
emotions (Averagely, each news is followed by 
15 comments). In order to know the exact rela-
tionship between these two kinds of emotions, 
we select 20 news from each category and ask 
two human annotators, named A and B, to manu-
ally annotate the writer?s emotion (single-label) 
according to the comments of each news. Table 1 
reports the agreement on annotators and emo-
tions, measured with Cohen?s kappa (?) value 
(Cohen, 1960). 
 ?  Value 
(Fine-grained 
emotions) 
? Value 
(Coarse-grained 
emotions) 
Annotators 0.566 0.742 
Emotions 0.504 0.756 
Table 1: Agreement on annotators and emotions 
 
Agreement between two annotators: The 
annotation agreement between the two annota-
tors is 0.566 on the fine-grained emotion catego-
ries and 0.742 on the coarse-grained emotion 
categories.  
Agreement between news reader?s and 
comment writer?s emotions: We compare the 
news reader?s emotion (automatically extracted 
from the web page) and the comment writer?s 
emotion (manually annotated by annotator A). 
The annotation agreement between the two kinds 
of emotions is 0.504 on the fine-grained emotion 
categories and 0.756 on the coarse-grained emo-
tion categories. From the results, we can see that 
the agreement on the fine-grained emotions is a 
bit low while the agreement between the coarse-
grained emotions, i.e., positive and negative, is 
very high. We find that although some fine-
grained emotions of the comments are not con-
sistent with the dominant emotion of the news, 
they belong to the same coarse-grained category.  
In a word, the agreement between news read-
er?s and comment writer?s emotions on the 
coarse-grained emotions is very high, even high-
er than the agreement between the two annota-
tors (0.754 vs. 0.742).  
In the following, we focus on the coarse-
grained emotions in emotion classification. 
4 Joint Modeling of News Reader?s and 
Comment Writer?s Emotions 
Given the importance of both news reader?s and 
comment writer?s emotion classification as de-
scribed in Introduction and the close relationship 
between news reader?s and comment writer?s 
emotions as described in last section, we system-
atically explore their joint modeling on the two 
kinds of emotion classification. 
In semi-supervised learning, the unlabeled da-
ta is exploited to improve the models with a 
small amount of the labeled data. In our ap-
proach, we consider the news text and the com-
ment text as two different views to express the 
news or comment emotion and build the two 
classifiers 
NC  and CC . Given the two-view clas-
sifiers, we perform co-training for semi-
supervised emotion classification, as shown in 
Figure 2, on both news reader?s and comment 
writer?s emotion classification. 
 
 
Input:   
NewsL  the labeled data on the news 
CommentL the labeled data  on the comments 
NewsU the unlabeled data  on the news  
CommentU  the labeled data  on the comments 
Output: 
NewsL New labeled data on the news 
CommentL  New labeled data on the comments 
 
Procedure: 
 
Loop for N iterations until
NewsU ??  or CommentU ??  
(1). Learn classifier 
NC  with NewsL  
(2). Use 
NC  to label the samples from NewsU   
(3). Choose 
1n  positive and 1n negative news 1N  
most confidently predicted by 
NC  
(4). Choose corresponding comments 
1M (the 
comments of the news in 
1N ) 
(5). Learn classifier 
CC  with CommentL  
(6). Use 
CC  to label the samples from CommentU   
(7). Choose 
2n  positive and 2n negative comments 
2M  most confidently predicted by CC  
(8). Choose corresponding comments 
2N (the news 
of the comments in 
2M ) 
(9). 
1 2News NewsL L N N? ? ?  
1 2Comment CommentL L M M? ? ? 
(10). 
1 2News NewsU U N N? ? ?
1 2Comment CommentU U M M? ? ? 
 
Figure 2: Co-training algorithm for semi-
supervised emotion classification 
513
5 Experimentation 
5.1 Experimental Settings 
Data Setting: The data set includes 3495 news 
articles (1572 positive and 1923 negative) and 
their comments as described in Section 3. Alt-
hough the emotions of the comments are not giv-
en in the website, we just set their coarse-grained 
emotion categories the same as the emotions of 
their source news due to their close relationship, 
as described in Section 3. To make the data bal-
anced, we randomly select 1500 positive and 
1500 negative news with their comments for the 
empirical study. Among them, we randomly se-
lect 400 news with their comments as the test 
data. 
Features: Each news or comment text is treat-
ed as a bag-of-words and transformed into a bi-
nary vector encoding the presence or absence of 
word unigrams. 
Classification algorithm: the maximum en-
tropy (ME) classifier implemented with the pub-
lic tool, Mallet Toolkits*. 
5.2 Experimental Results 
News reader?s emotion classifier: The classifier 
trained with the news text. 
Comment writer?s emotion classifier: The 
classifier trained with the comment text. 
Figure 3 demonstrates the performances of the 
news reader?s and comment writer?s emotion 
classifiers trained with the 10 and 50 initial la-
beled samples plus automatically labeled data 
from co-training. Here, in each iteration, we pick 
2 positive and 2 negative most confident samples, 
i.e, 
1 2 2n n? ? . From this figure, we can see that 
our co-training algorithm is very effective: using 
only 10 labeled samples in each category 
achieves a very promising performance on either 
news reader?s or comment writer?s emotion clas-
sification. Especially, the performance when us-
ing only 10 labeled samples is comparable to that 
when using more than 1200 labeled samples on 
supervised learning of comment writer?s emotion 
classification. 
   For comparison, we also implement a self-
training algorithm for the news reader?s and 
comment writer?s emotion classifiers, each of 
which automatically labels the samples from the 
unlabeled data independently. For news reader?s 
emotion classification, the performances of self-
training are 0.783 and 0.79 when 10 and 50 ini-
                                                 
* http://mallet.cs.umass.edu/ 
tial labeled samples are used. For comment writ-
er?s emotion classification, the performances of 
self-training are 0.505 and 0.508. These results 
are much lower than the performances of our co-
training approach, especially on the comment 
writer?s emotion classification i.e., 0.505 and 
0.508 vs. 0.783 and 0.805. 
 
10 Initial Labeled Samples
0.5
0.6
0.7
0.8
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data
A
c
c
u
r
a
c
y
 
50 Initial Labeled Samples
0.65
0.7
0.75
0.8
0.85
0.9
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data data
A
c
c
u
r
a
c
y
The news reader's emotion
classifier (Co-training)
The comment writer's emotion
classifier (Co-training)
 Figure 3: Performances of the news reader?s and 
comment writer?s emotion classifiers using the 
co-training algorithm 
6 Conclusion 
In this paper, we focus on two popular emotion 
classification tasks, i.e., reader?s emotion classi-
fication on the news and writer?s emotion classi-
fication on the comments. From the data analysis, 
we find that the news reader?s and comment 
writer?s emotions are highly consistent to each 
other in terms of the coarse-grained emotion cat-
egories, positive and negative. On the basis, we 
propose a co-training approach to perform semi-
supervised learning on the two tasks. Evaluation 
shows that the co-training approach is so effec-
tive that using only 10 labeled samples achieves 
nice performances on both news reader?s and 
comment writer?s emotion classification.  
514
Acknowledgments 
This research work has been partially supported 
by two NSFC grants, No.61003155, and 
No.61273320, one National High-tech Research 
and Development Program of China 
No.2012AA011102, one General Research Fund 
(GRF) sponsored by the Research Grants Coun-
cil of Hong Kong No.543810, the NSF grant of 
Zhejiang Province No.Z1110551, and one pro-
ject supported by Zhejiang Provin-cial Natural 
Science Foundation of China, No.Y13F020030.  
References  
Alm C., D. Roth and R. Sproat. 2005. Emotions from 
Text: Machine Learning for Text-based Emotion 
Prediction. In Proceedings of EMNLP-05, pp.579-
586. 
Aman S. and S. Szpakowicz. 2008. Using Roget?s 
Thesaurus for Fine-grained Emotion Recognition. 
In Proceedings of IJCNLP-08, pp.312-318. 
Chen Y., S. Lee, S. Li and C. Huang. 2010. Emotion 
Cause Detection with Linguistic Constructions. In 
Proceeding of COLING-10, pp.179-187. 
Cohen J. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Meas-
urement, 20(1):37?46. 
 Cui H., V. Mittal and M. Datar. 2006. Comparative 
Experiments on Sentiment Classification for 
Online Product Comments. In Proceedings of 
AAAI-06, pp.1265-1270. 
Das D. and S. Bandyopadhyay. 2009. Word to Sen-
tence Level Emotion Tagging for Bengali Blogs. In 
Proceedings of ACL-09, pp.149-152. 
Dasgupta S. and V. Ng. 2009. Mine the Easy, Classify 
the Hard: A Semi-Supervised Approach to Auto-
matic Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09,  pp.701-709, 2009. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Fumera G. and F. Roli. 2005. A Theoretical and Ex-
perimental Analysis of Linear Combiners for Mul-
tiple Classifier Systems. IEEE Trans. PAMI, vol.27, 
pp.942?956, 2005. 
Li S., Z. Wang, G. Zhou and S. Lee. 2011. Semi-
supervised Learning for Imbalanced Sentiment 
Classification. In Proceeding of IJCAI-11,  pp.826-
1831. 
Li S., C. Huang, G. Zhou and S. Lee.  2010. Employ-
ing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification. In Pro-
ceedings of ACL-10,  pp.414-423. 
Lin K., C. Yang and H. Chen. 2007. What Emotions 
do News Articles Trigger in Their Readers? In 
Proceeding of SIGIR-07, poster, pp.733-734. 
Lin K., C. Yang and H. Chen. 2008. Emotion Classi-
fication of Online News Articles from the Reader?s 
Perspective. In Proceeding of the International 
Conference on Web Intelligence and Intelligent 
Agent Technology, pp.220-226. 
 Liu B. 2012. Sentiment Analysis and Opinion Mining 
(Introduction and Survey). Morgan & Claypool 
Publishers, May 2012. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Moshfeghi Y., B. Piwowarski and J. Jose. 2011. Han-
dling Data Sparsity in Collaborative Filtering using 
Emotion and Semantic Based Features. In Proceed-
ings of SIGIR-11, pp.625-634. 
Pang B. and L. Lee. 2008. Opinion Mining and 
Sentiment Analysis: Foundations and Trends. 
Information Retrieval, vol.2(12), 1-135. 
Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of EMNLP-
02, pp.79-86. 
Purver M. and S. Battersby. 2012. Experimenting 
with Distant Supervision for Emotion Classifica-
tion. In Proceedings of EACL-12, pp.482-491. 
Quan C. and F. Ren. 2009. Construction of a Blog 
Emotion Corpus for Chinese Emotional Expression 
Analysis. In Proceedings of EMNLP-09, pp.1446-
1454. 
Riloff E., S. Patwardhan and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06, pp.440-448. 
Turney P. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of comments. In Proceedings of 
ACL-02, pp.417-424.  
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Volkova S., W. Dolan and T. Wilson. 2012. CLex: A 
Lexicon for Exploring Color, Concept and Emo-
tion Associations in Language. In Proceedings of 
EACL-12, pp.306-314. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433. 
Xu G., X. Meng and H. Wang. 2010. Build Chinese 
Emotion Lexicons Using A Graph-based 
Algorithm and Multiple Resources. In Proceeding 
of COLING-10, pp.1209-1217. 
515
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521?525,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
 
Dual Training and Dual Prediction for Polarity Classification 
 
Rui Xia, Tao Wang, Xuelei Hu 
Department of Computer Science 
Nanjing University of  
Science and Technology 
rxia@njust.edu.cn,  
linclonwang@163.com, 
xlhu@njust.edu.cn 
Shoushan Li 
NLP Lab 
Department of  
Computer Science 
Soochow University 
shoushan.li 
@gmail.com 
Chengqing Zong 
National Lab of  
Pattern Recognition 
Institute of Automation 
CAS 
cqzong 
@nlpr.ia.ac.cn 
 
 
Abstract 
Bag-of-words (BOW) is now the most popular 
way to model text in machine learning based 
sentiment classification. However, the perfor-
mance of such approach sometimes remains 
rather limited due to some fundamental defi-
ciencies of the BOW model. In this paper, we 
focus on the polarity shift problem, and pro-
pose a novel approach, called dual training and 
dual prediction (DTDP), to address it. The 
basic idea of DTDP is to first generate artifi-
cial samples that are polarity-opposite to the 
original samples by polarity reversion, and 
then leverage both the original and opposite 
samples for (dual) training and (dual) predic-
tion. Experimental results on four datasets 
demonstrate the effectiveness of the proposed 
approach for polarity classification.  
1 Introduction 
The most popular text representation model in 
machine learning based sentiment classification 
is known as the bag-of-words (BOW) model, 
where a piece of text is represented by an unor-
dered collection of words, based on which stand-
ard machine learning algorithms are employed as 
classifiers. Although the BOW model is simple 
and has achieved great successes in topic-based 
text classification, it disrupts word order, breaks 
the syntactic structures and discards some kinds 
of semantic information that are possibly very 
important for sentiment classification. Such dis-
advantages sometimes limit the performance of 
sentiment classification systems.  
A lot of subsequent work focused on feature 
engineering that aims to find a set of effective 
features based on the BOW representation. How-
ever, there still remain some problems that are 
not well addressed. Out of them, the polarity 
shift problem is the biggest one. 
We refer to ?polarity shift? as a linguistic phe-
nomenon that the sentiment orientation of a text 
is reversed (from positive to negative or vice ver-
sa) because of some particular expressions called 
polarity shifters. Negation words (e.g., ?no?, ?not? 
and ?don?t?) are the most important type of po-
larity shifter. For example, by adding a negation 
word ?don?t? to a positive text ?I like this book? 
in front of ?like?, the orientation of the text is 
reversed from positive to negative.  
Naturally, handling polarity shift is very im-
portant for sentiment classification. However, the 
BOW representations of two polarity-opposite 
texts, e.g., ?I like this book? and ?I don?t like this 
book?, are considered to be very similar by most 
of machine learning algorithms. Although some 
methods have been proposed in the literature to 
address the polarity shift problem (Das and Chen, 
2001; Pang et al, 2002; Na et al, 2004; Kenndey 
and Inkpen, 2006; Ikeda et al, 2008; Li and 
Huang, 2009; Li et al, 2010), the state-of-the-art 
results are still far from satisfactory. For example, 
the improvements are less than 2% after consid-
ering polarity shift in Li et al (2010). 
In this work, we propose a novel approach, 
called dual training and dual prediction (DTDP), 
to address the polarity shift problem. By taking 
advantage of the unique nature of polarity classi-
fication, DTDP is motivated by first generating 
artificial samples that are polarity-opposite to the 
original ones. For example, given the original 
sample ?I don?t like this book. It is boring,? its 
polarity-opposite version, ?I like this book. It is 
interesting?, is artificially generated. Second, the 
original and opposite training samples are used 
together for training a sentiment classifier (called 
dual training), and the original and opposite test 
samples are used together for prediction (called 
dual prediction). Experimental results prove that 
the procedure of DTDP is very effective at cor-
recting the training and prediction errors caused 
521
 by polarity shift, and it beats other alternative 
methods of considering polarity shift. 
2 Related Work 
The lexicon-based sentiment classification sys-
tems can be easily modified to include polarity 
shift. One common way is to directly reverse the 
sentiment orientation of polarity-shifted words, 
and then sum up the orientations word by word 
(Hu and Liu, 2004; Kim and Hovy, 2004; Po-
lanyi and Zaenen, 2004; Kennedy and Inkpen, 
2006). Wilson et al (2005) discussed other com-
plex negation effects by using conjunctive and 
dependency relations among polarity words. Alt-
hough handling polarity shift is easy and effec-
tive in term-counting systems, they rarely outper-
form the baselines of machine learning methods 
(Kennedy, 2006). 
The machine learning methods are generally 
more effective for sentiment classification. How-
ever, it is difficult to handle polarity shift based 
on the BOW model. Das and Chen (2001) pro-
posed a method by simply attaching ?NOT? to 
words in the scope of negation, so that in the text 
?I don?t like book?, the word ?like? is changed to 
a new word ?like-NOT?. There were also some 
attempts to model polarity shift by using more 
complex linguistic features (Na et al, 2004; 
Kennedy and Inkpen, 2006). But the improve-
ments upon the baselines of machine learning 
systems are very slight (less than 1%). 
Ikeda et al (2008) proposed a machine learn-
ing method, to model polarity-shifters for both 
word-wise and sentence-wise sentiment classifi-
cation, based on a dictionary extracted from 
General Inquirer. Li and Huang (2009) proposed 
a method first to classify each sentence in a text 
into a polarity-unshifted part and a polarity-
shifted part according to certain rules, then to 
represent them as two bag-of-words for senti-
ment classification. Li et al (2010) further pro-
posed a method to separate the shifted and un-
shifted text based on training a binary detector. 
Classification models are then trained based on 
each of the two parts. An ensemble of two com-
ponent parts is used at last to get the final polari-
ty of the whole text. 
3 The Proposed Approach 
We first present the method for generating artifi-
cial polarity-opposite samples, and then intro-
duce the algorithm of dual training and dual pre-
diction (DTDP). 
3.1 Generating Artificial Polarity-Opposite 
Samples 
Given an original sample and an antonym dic-
tionary (e.g., WordNet 1 ), a polarity-opposite 
sample is generated artificially according to the 
following rules: 
1) Sentiment word reversion: All sentiment 
words out of the scope of negation are re-
versed to their antonyms; 
2) Handling negation: If there is a negation 
expression, we first detect the scope of nega-
tion, and then remove the negation words 
(e.g., ?no?, ?not?, and ?don?t?). The senti-
ment words in the scope of negation are not 
reversed; 
3) Label reversion: The class label of the la-
beled sample is also reversed to its opposite 
(i.e., Positive to Negative, or vice versa) as 
the class label of newly generated samples 
(called polarity-opposite samples). 
Let us use a simple example to explain the 
generation process. Given the original sample: 
The original sample 
Text:   I don?t like this book. It is boring. 
Label: Negative 
According to Rule 1, ?boring? is reversed to 
its antonym ?interesting?; According to Rule 2, 
the negation word ?don?t? is removed, and ?like? 
is not reversed; According to Rule 3, the class 
label Negative is reversed to Positive. Finally, an 
artificial polarity-opposite sample is generated: 
The generated opposite sample 
Text:   I like this book. It is interesting. 
Label: Positive 
All samples in the training and test set are re-
versed to their polarity-opposite versions. We 
refer to them as ?opposite training set? and ?op-
posite test set?, respectively. 
3.2 Dual Training and Dual Prediction 
In this part, we introduce how to make use of the 
original and opposite training/test data together 
for dual training and dual prediction (DTDP). 
Dual Training: Let D = f(xi; yi)gNi=1 and 
~D = f(~xi; ~yi)gNi=1 be the original and opposite 
training set respectively, where x  denotes the 
feature vector, y  denotes the class label, and N  
denotes the size of training set. In dual training, 
D [ ~D  are used together as training data to learn 
                                                 
1 http://wordnet.princeton.edu/ 
522
 a classification model. The size of training data 
is doubled in dual training. 
Suppose the example in Section 3.1 is used as 
one training sample. As far as only the original 
sample (?I don?t like this book. It is boring.?) is 
considered, the feature ?like? will be improperly 
recognized as a negative indicator (since the 
class label is Negative), ignoring the expression 
of negation. Nevertheless, if the generated oppo-
site sample (?I like this book. It is interesting.?) 
is also used for training, ?like? will be learned 
correctly, due to the removal of negation in sam-
ple reversion. Therefore, the procedure of dual 
training can correct some learning errors caused 
by polarity shift. 
Dual Prediction: Given an already-trained 
classification model, in dual prediction, the orig-
inal and opposite test samples are used together 
for prediction. In dual prediction, when we pre-
dict the positive degree of a test sample, we 
measure not only how positive the original test 
sample is, but also how negative the opposite 
sample is.  
Let x  and ~x  denote the feature vector of the 
original and opposite test samples respectively; 
let pd(cjx)  and pd(cj~x)  denote the predictions of 
the original and opposite test sample, based on 
the dual training model. The dual predicting 
function is defined as: 
pd(+jx; ~x) = (1?a)pd(+jx)+apd(?j~x), 
pd(?jx; ~x) = (1?a)pd(?jx)+apd(+j~x), 
where a  (06 a6 1 ) is the weight of the oppo-
site prediction.  
Now suppose the example in Section 3.1 is a 
test sample. As far as only the original test sam-
ple (?I don?t like this book. It is boring.?) is used 
for prediction, it is very likely that it is falsely 
predicted as Positive, since ?like? is a strong pos-
itive feature, despite that it is in the scope of ne-
gation. While in dual prediction, we still measure 
the ?sentiment-opposite? degree of the opposite 
test sample (?I like this book. It is interesting.?). 
Since negation is removed, it is very likely that 
the opposite test sample is assigned with a high 
positive score, which could compensate the pre-
diction errors of the original test sample. 
Final Output: It should be noted that alt-
hough the artificially generated training and test-
ing data are helpful in most cases, they still pro-
duce some noises (e.g., some poorly generated 
samples may violate the quality of the original 
data set). Therefore, instead of using all dual 
predictions as the final output, we use the origi-
nal prediction po(cjx)  as an alternate, in case that 
the dual prediction pd(cjx; ~x)  is not enough con-
fident, according to a confidence threshold t . The 
final output is defined as: 
pf(cjx) =
? pd(cjx; ~x); if?p > tpo(cjx); if?p < t
 
where ?p= pd(cjx; ~x)?po(cjx). 
4 Experimental Study 
4.1 Datasets 
The Multi-Domain Sentiment Datasets2 are used 
for evaluations. They consist of product reviews 
collected from four different domains: Book, 
DVD, Electronics and Kitchen. Each of them 
contains 1,000 positive and 1,000 negative re-
views. Each of the datasets is randomly spit into 
5 folds, with four folds serving as training data, 
and the remaining one fold serving as test data. 
All of the following results are reported in terms 
of an average of 5-fold cross validation. 
4.2 Evaluated Systems 
We evaluate four machine learning systems that 
are proposed to address polarity shift in docu-
ment-level polarity classification: 
1) Baseline: standard machine learning meth-
ods based on the BOW model, without han-
dling polarity shift;  
2) Das-2001: the method proposed by Das and 
Chen (2001), where ?NOT? is attached to the 
words in the scope of negation as a prepro-
cessing step; 
3) Li-2010: the approach proposed by Li et al 
(2010). The details of the algorithm is intro-
duced in related work; 
4) DTDP: our approach proposed in Section 3. 
The WordNet dictionary is used for sample 
reversion. The empirical value of the param-
eter a  and t  are used in the evaluation.  
4.3 Comparison of the Evaluated Systems 
In table 1, we report the classification accuracy 
of four evaluated systems using unigram features. 
We consider two widely-used classification algo-
rithms: SVM and Na?ve Bayes. For SVM, the 
LibSVM toolkit3 is used with a linear kernel and 
the default penalty parameter. For Na?ve Bayes, 
the OpenPR-NB toolkit4 is used. 
                                                 
2 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/ 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/  
4 http://www.openpr.org.cn  
523
 Dataset 
SVM Na?ve Bayes 
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP 
Book 0.745 0.763 0.760 0.800 0.779 0.783 0.792 0.814 
DVD 0.764 0.771 0.795 0.823 0.795 0.793 0.810 0.820 
Electronics 0.796 0.813 0.812 0.828 0.815 0.827 0.824 0.841 
Kitchen 0.822 0.820 0.844 0.849 0.830 0.847 0.840 0.859 
Avg. 0.782 0.792 0.803 0.825 0.804 0.813 0.817 0.834 
Table 1: Classification accuracy of different systems using unigram features 
Dataset 
SVM Na?ve Bayes 
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP 
Book 0.775 0.777 0.788 0.818 0.811 0.815 0.822 0.840 
DVD 0.790 0.793 0.809 0.828 0.824 0.826 0.837 0.868 
Electronics 0.818 0.834 0.841 0.848 0.841 0.857 0.852 0.866 
Kitchen 0.847 0.844 0.870 0.878 0.878 0.879 0.883 0.896 
Avg. 0.808 0.812 0.827 0.843 0.839 0.844 0.849 0.868 
Table 2: Classification accuracy of different systems using both unigram and bigram features 
Compared to the Baseline system, the Das-
2001 approach achieves very slight improve-
ments (less than 1%). The performance of Li-
2010 is relatively effective: it improves the aver-
age score by 0.21% and 0.13% on SVM and Na-
?ve Bayes, respectively. Yet, the improvements 
are still not satisfactory. 
As for our approach (DTDP), the improve-
ments are remarkable. Compared to the Baseline 
system, the average improvements are 4.3% and 
3.0% on SVM and Na?ve Bayes, respectively. In 
comparison with the state-of-the-art (Li-2010), 
the average improvement is 2.2% and 1.7% on 
SVM and Na?ve Bayes, respectively. 
We also report the classification accuracy of 
four systems using both unigrams and bigrams 
features for classification in Table 2. From this 
table, we can see that the performance of each 
system is improved compared to that using uni-
grams. It is now relatively difficult to show im-
provements by incorporating polarity shift, be-
cause using bigrams already captured a part of 
negations (e.g., ?don?t like?).  
The Das-2001 approach still shows very lim-
ited improvements (less than 0.5%), which 
agrees with the reports in Pang et al (2002). The 
improvements of Li-2010 are also reduced: 1.9% 
and 1% on SVM and Na?ve Bayes, respectively.  
Although the improvements of the previous 
two systems are both limited, the performance of 
our approach (DTDP) is still sound. It improves 
the Baseline system by 3.7% and 2.9% on SVM 
and Na?ve Bayes, respectively, and outperforms 
the state-of-the-art (Li-2010) by 1.6% and 1.9% 
on SVM and Na?ve Bayes, respectively. 
5 Conclusions 
In this work, we propose a method, called dual 
training and dual prediction (DTDP), to address 
the polarity shift problem in sentiment classifica-
tion. The basic idea of DTDP is to generate arti-
ficial samples that are polarity-opposite to the 
original samples, and to make use of both the 
original and opposite samples for dual training 
and dual prediction. Experimental studies show 
that our DTDP algorithm is very effective for 
sentiment classification and it beats other alterna-
tive methods of considering polarity shift.  
One limitation of current work is that the tun-
ing of parameters in DTDP (such as a  and t ) is 
not well discussed. We will leave this issue to an 
extended version. 
Acknowledgments 
The research work is supported by the Jiangsu 
Provincial Natural Science Foundation of China 
(BK2012396), the Research Fund for the Doc-
toral Program of Higher Education of China 
(20123219120025), and the Open Project Pro-
gram of the National Laboratory of Pattern 
Recognition (NLPR). This work is also partly 
supported by the Hi-Tech Research and Devel-
opment Program of China (2012AA011102 and 
2012AA011101), the Program of Introducing 
Talents of Discipline to Universities (B13022), 
and the Open Project Program of the Jiangsu Key 
Laboratory of Image and Video Understanding 
for Social Safety (30920130122006).  
524
 References  
S. Das and M. Chen. 2001. Yahoo! for Amazon: 
Extracting market sentiment from stock mes-
sage boards. In Proceedings of the Asia Pacif-
ic Finance Association Annual Conference. 
M. Hu and B. Liu. 2004. Mining opinion features 
in customer reviews. In Proceedings of the 
National Conference on Artificial Intelligence 
(AAAI). 
D. Ikeda, H. Takamura L. Ratinov M. Okumura. 
2008. Learning to Shift the Polarity of Words 
for Sentiment Classification. In Proceedings 
of the International Joint Conference on Natu-
ral Language Processing (IJCNLP).  
S. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceeding of the Inter-
national Conference on Computational Lin-
guistics (COLING). 
A. Kennedy and D. Inkpen. 2006. Sentiment 
classification of movie reviews using contex-
tual valence shifters. Computational Intelli-
gence, 22:110?125. 
S. Li and C. Huang. 2009. Sentiment classifica-
tion considering negation and contrast transi-
tion. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Com-
putation (PACLIC). 
S. Li, S. Lee, Y. Chen, C. Huang and G. Zhou. 
2010. Sentiment Classification and Polarity 
Shifting. In Proceeding of the International 
Conference on Computational Linguistics 
(COLING). 
J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. 
2004. Effectiveness of simple linguistic pro-
cessing in automatic sentiment classification 
of product reviews. In Proceeding of the Con-
ference of the International Society for 
Knowledge Organization. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. 
Thumbs up?: sentiment classification using 
machine learning techniques. In Proceedings 
of the Conference on Empirical Methods in 
Natural Language Processing (EMNLP). 
L. Polanyi and A. Zaenen. 2004. Contextual lex-
ical valence shifters. In Proceedings of the 
AAAI Spring Symposium on Exploring Attitude 
and Affect in Text, AAAI technical report. 
P. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceeding of the 
Annual Meeting of the Association for Compu-
tational Linguistics (ACL). 
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. 
Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of 
the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). 
 
525
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 842?847,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bilingual Event Extraction: a Case Study on Trigger Type Determina-
tion 
Zhu Zhu?  Shoushan Li?*  Guodong Zhou?  Rui Xia? 
 
?Natural Language Processing Lab 
Soochow University, China 
{zhuzhu0020, 
shoushan.li}@gmail.com, 
gdzhou@suda.edu.cn 
 
?Department of Computer Science 
 Nanjing University of Science and 
Technology 
rxia@njust.edu.cn 
 
 
Abstract 
Event extraction generally suffers from the 
data sparseness problem. In this paper, we 
address this problem by utilizing the labeled 
data from two different languages. As a pre-
liminary study, we mainly focus on the sub-
task of trigger type determination in event 
extraction. To make the training data in dif-
ferent languages help each other, we pro-
pose a uniform text representation with bi-
lingual features to represent the samples and 
handle the difficulty of locating the triggers 
in the translated text from both monolingual 
and bilingual perspectives. Empirical studies 
demonstrate the effectiveness of the pro-
posed approach to bilingual classification on 
trigger type determination. ? 
1 Introduction 
Event extraction is an increasingly hot and chal-
lenging research topic in the natural language 
processing (NLP) community (Ahn, 2006; Saun 
et al 2006; Zhao et al 2008). It aims to automat-
ically extract certain types of events with the ar-
guments to present the texts under a structured 
form. In event extraction, there are four primary 
subtasks, named trigger identification, trigger 
type determination, argument identification, and 
argument role determination (Chen and NG, 
2012). As an important technology in infor-
mation extraction, event extraction could be ap-
plied to many fields such as information retrieval, 
summarization, text mining, and question an-
swering. 
Recently, the dominative approach to event 
extraction is based on supervised learning where 
a set of labeled samples are exploited to train a 
model to extract the events. However, the availa-
                                                 
? *  Corresponding author 
ble labeled data are rather sparse due to various 
kinds of event categories. For example, the event 
taxonomy in ACE 2005 1  (Automatic Content 
Extraction) includes 8 types of events, with 33 
subtypes, such as ?Marry/Life? (subtype/type), 
and ?Transport/Movement?. Moreover, some 
subtypes such as ?Nominate/Personnel? and 
?Convict/Justice? contain less than 10 labeled 
samples in the English and Chinese corpus re-
spectively. Apparently, such a small scale of 
training data is difficult to yield a satisfying per-
formance. 
One possible way to alleviate the data sparse-
ness problem in event extraction is to conduct 
bilingual event extraction with training data from 
two different languages. This is motivated by the 
fact that labeled data from a language is highly 
possible to convey similar information in another 
language. For example, E1 is an event sample 
from the English corpus and E2 is another one in 
the Chinese corpus. Apparently, E1 and the Eng-
lish translation text of E2, share some important 
clues such as meet and Iraq which highly indi-
cates the event type of ?Meet/Contact?.  
 
E1: Bush arrived in Saint Petersburg on Sat-
urday, when he also briefly met German chancel-
lor Gerhard Schroeder, whose opposition to the 
Iraq war had soured his relationship with Wash-
ington, at a dinner hosted by Putin. 
E2: ?????????????????
???? ???????????????
????(U.S. president George W. Bush   will 
visit Germany in February and meet with   
Schroeder, Iran and Iraq will be the focus of the   
talks the two sides.) 
 
In this paper, we address the data sparseness 
problem in event extraction with a bilingual pro-
                                                 
1http://www.nist.gov/speech/tests/ace/2005 
842
cessing approach which aims to exploit bilingual 
training data to enhance the extraction perfor-
mance in each language. As a preliminary work, 
we mainly focus on the subtask of trigger type 
determination. Accordingly, our goal is to design 
a classifier which is trained with labeled data 
from two different languages and is capable of 
classifying the test data from both languages. 
Generally, this task possesses two main chal-
lenges.  
The first challenge is text representation, 
namely, how to eliminate the language gap be-
tween the two languages. To tackle this, we first 
employ Google Translate2, a state-of-the-art ma-
chine translation system, to gain the translation 
of an event instance, similar to what has been 
widely done by previous studies in bilingual 
classification tasks e.g., Wan (2008); Then, we 
uniformly represent each text with bilingual 
word features. That is, we augment each original 
feature vector into a novel one which contains 
the translated features.  
The second challenge is the translation for 
some specific features. It is well-known that 
some specific features, such as the triggers and 
their context features, are extremely important 
for determining the event types. For example, in 
E3, both trigger ?left? and named entity ?Sad-
dam? are important features to tell the event type, 
i.e., "Transport/Movement". When it is translated 
to Chinese, it is also required to know trigger ??
??(left) and named entity ????? (Saddam) 
in E4, the Chinese translation of E3.  
 
E3: Saddam's clan is said to have left for a 
small village in the desert. 
E4: Chinese translation: ? ?  ? ? ?
(Saddam) ?? ?? ??(left) ?? ? ? ?? 
? ??? 
 
However, it is normally difficult to know 
which words are the triggers and surrounding 
entities in the translated sentence. To tackle this 
issue, we propose to locate the trigger from both 
monolingual and bilingual perspectives in the 
translation text. Empirical studies demonstrate 
that adding the translation of these specific fea-
tures substantially improves the classification 
performance.  
The remainder of this paper is organized as 
follows. Section 2 overviews the related work on 
event extraction. Section 3 proposes our ap-
                                                 
2 www.google.com 
proach to bilingual event extraction. Section 4 
gives the experimental studies. In Section 5, we 
conclude our work and give some future work. 
2 Related Work  
In the NLP community, event extraction has 
been mainly studied in both English and Chinese. 
In English, various supervised learning ap-
proaches have been explored recently. Bethard 
and Martin (2006) formulate the event identifica-
tion as a classification problem in a word-
chunking paradigm, introducing a variety of lin-
guistically motivated features. Ahn (2006) pro-
poses a trigger-based method. It first identifies 
the trigger in an event, and then uses a multi-
classifier to implement trigger type determina-
tion. Ji and Grishman (2008) employ an ap-
proach to propagate consistent event arguments 
across sentences and documents. Liao and 
Grishman (2010) apply document level infor-
mation to improve the performance of event ex-
traction. Hong et al (2011) leverage cross-entity 
information to improve traditional event extrac-
tion, regarding entity type consistency as a key 
feature. More recently, Li et al (2013) propose a 
joint framework based on structured prediction 
which extracts triggers and arguments together. 
In Chinese, relevant studies in event extraction 
are in a relatively primary stage with focus on 
more special characteristics and challenges. Tan 
et al (2008) employ local feature selection and 
explicit discrimination of positive and negative 
features to ensure the performance of trigger type 
determination. Chen and Ji (2009) apply lexical, 
syntactic and semantic features in trigger label-
ing and argument labeling to improve the per-
formance. More recently, Li et al (2012) and Li 
et al (2013) introduce two inference mechanisms 
to infer unknown triggers and recover trigger 
mentions respectively with morphological struc-
tures.  
In comparison with above studies, we focus on 
bilingual event extraction. Although bilingual 
classification has been paid lots of attention in 
other fields (Wan 2008; Haghighi et al, 2008; 
Ismail et al, 2010; Lu et al, 2011?Li et al, 
2013), there is few related work in event extrac-
tion. The only one related work we find is Ji 
(2009) which proposes an inductive learning ap-
proach to exploit cross-lingual predicate clusters 
to improve the event extraction task with the 
main goal to get the event taggers from extra re-
sources, i.e., an English and Chinese parallel 
corpus. Differently, our goal is to make the la-
843
beled data from two languages help each other 
without any other extra resources, which is origi-
nal in the study of event extraction. 
3 The Proposed Approach 
Trigger type determination aims to determine the 
event type of a trigger given the trigger and its 
context (e.g., a sentence). Existing approaches to 
trigger type determination mainly focus on mon-
olingual classification. Figure 1 illustrates the 
framework for Chinese and English. 
In comparison, our approach exploits the cor-
pora from two different languages. Figure 2 illus-
trates the framework. As shown in the figure, we 
first get the translated corpora of Chinese and 
English origin corpora through machine transla-
tion. Then, we represent each text with bilingual 
features, which enables us to merge the training 
data from both languages so as to make them 
help each other. 
 
Figure 1: The framework of monolingual classifi-
cation for trigger type determination 
 
Figure 2: The framework of bilingual classification 
for trigger type determination 
3.1 Text Representation  
In a supervised learning approach, labeled data is 
trained to obtain a classifier. In this approach, the 
extracted features are the key components to 
make a successful classifier. Table 1 shows some 
typical kinds of features in a monolingual classi-
fication task for trigger type determination. To 
better understand these features, the real feature 
examples in E3 are given in the table. 
Given the feature definition, a monolingual 
sample x  is represented as the combination of all 
the features, i.e.,  
1 2, , , , _ , _ ,
_ , , _ , _
ne e e Tri POS Tri Tri conx POS con Ent Ent type Ent subtype
? ?? ? ?? ?
  (1) 
Features Feature examples in E3 
All words 
(
1 2, , ne e e ) 
Saddam, clan, is, ... , 
desert 
Trigger (Tri) left 
POS of the trigger 
(POS_Tri) 
VBN 
Trigger's context 
words (Tri_con) 
...,have, for,... 
POS of trigger's 
context words 
(POS_con) 
...,VB,IN,? 
Entities around trig-
ger (Ent) 
Saddam 
Entity type 
(Ent_type) 
PER 
Entity subtype 
(Ent_subtype) 
individual 
Table 1: The features and some feature examples for 
trigger type determination 
 
In bilingual classification, we represent a sam-
ple with bilingual features, which makes it possi-
ble to train with the data from two languages. To 
achieve this goal, we employ a single feature 
augmentation strategy to augment the monolin-
gual features into bilingual features, i.e.,  
,Chinese Englishx x x?
                      (2) 
Specifically, a sample x  is represented as fol-
lows: 
1 2
1 2
, , , , _ , _ ,
_ , , _ , _
, , , , , _ , _ ,
_ , , _ , _
m c c c
c c c
n e e e
e e e
c c c Tri POS Tri Tri con
POS con Ent Ent type Ent subtype
x
e e e Tri POS Tri Tri con
POS con Ent Ent type Ent subtype
? ?? ?
? ?? ?
? ?? ?? ? ?? ?? ?? ?? ?? ?? ?
  (3) 
Where the tokens with the ?c?/?e? subscript mean 
the features generated from the Chinese/English 
text. From the features, we can see that some 
Classifier Results 
Chinese event 
corpus 
Machine trans-
lation 
Translated 
samples 
Text representation 
Translated 
samples 
English event 
corpus 
Machine trans-
lation 
Text representation 
Samples with 
bilingual features 
Samples with 
bilingual features 
Trigger type determination 
for Chinese 
Trigger type determination 
for English 
Chinese event 
corpus 
Classifier 
English 
event corpus 
Classifier 
Results Results 
844
features, such as Tri_con and Ent, depend on the 
location of the trigger word. Therefore, locating 
the trigger in the translated text becomes crucial.  
3.2 Locating Translated Trigger 
Without loss of generality, we consider the case 
of translating a Chinese event sample into an 
English one. Formally, the word sequence of a 
Chinese event sample is denoted as 
1 2( , , , )c ns c c c? , while the sequence of the 
translated one is denoted as
1 2( , , )e ms e e e? . 
Then, the objective is to get the English trigger 
eTri  in es , given the Chinese trigger word  
cTri in cs . The objective function is given as fol-
lows:  
? ?_1 ,argmax k l ek l m P e Tri? ? ?
                 (4) 
Where 
_k le
 denotes the substring 
1( , , )k k le e e?  
in 
es  and 1 ,k l m? ? . 
In this paper, the above function could be 
solved in two perspectives: monolingual and bi-
lingual ones. The former uses the English train-
ing data alone to locate the trigger while the lat-
ter exploit the bilingual information to get the 
translated counterpart of the Chinese trigger. 
The monolingual perspective: The objective 
is to locate the trigger with the monolingual in-
formation. That is,  
? ?_1 ,argmax | ,k l e e ek l m P e Tri s R? ? ?
           (5) 
Where 
eR  denotes the training resource in Eng-
lish. In fact, this task is exactly the first subtask 
in event extraction named trigger identification, 
as mentioned in Introduction. For a simplified 
implementation, we first estimate the probabili-
ties of ? ?_k l eP e Tri?  in eR  with maximum like-
lihood estimation when 
_k l ee s?
.  
The bilingual perspective: The objective is to 
locate the trigger with the bilingual information. 
That is, 
? ?_1 ,argmax | , ,k l e e c ck l m P e Tri s s Tri? ? ?
        (6) 
Where 
cTri  is the trigger word in Chinese and es  
is the translated text towards 
cs . More generally, 
this can be solved from a standard word align-
ment model in machine translation (Och et al 
1999; Koehn et al 2003). However, training a 
word alignment requires a huge parallel corpus 
which is not available here.  
 For a simplified implementation, we first get 
the 
cTri ?s translation? denoted as cTritrans
?
with Google Translate. Then, we estimate 
? ?_k l eP e Tri?  as follows:  
? ? __ 0.9 ck l Trik l e if e transP e Tri others?
???? ? ???
    (7) 
Where 0.9 is an empirical value which makes the 
translation probability become a dominative fac-
tor when the translation of the trigger is found in 
the translated sentence. ?  is a small value which 
makes the sum of all probabilities equals 1.   
The final decision is made according to both 
the monolingual and bilingual perspectives, i.e., 
? ?
? ?
_
1 ,
_
arg max  | ,
              | , ,
k l e e e
k l m
k l e e c c
P e Tri s R
P e Tri s s Tri
? ?
?
? ?
        (8) 
Note that we reduce the computational cost by 
make the word length of the trigger less than 3, 
i.e., 3l k? ? . 
4 Experimentation 
4.1 Experimental Setting  
Data sets: The Chinese and English corpus for 
even extraction are from ACE2005, which in-
volves 8 types and 33 subtypes. All our experi-
ments are conducted on the subtype case. Due to 
the space limit, we only report the statistics for 
each type, as shown in Table 2. For each subtype, 
80% samples are used as training data while the 
rest are as test data. 
 
# Chinese English total 
Life 389 902 1291 
Movement 593 679 1272 
Transaction 147 379 526 
Business 144 137 281 
Conflict 514 1629 2143 
Contact 263 373 636 
Personnel 203 514 717 
Justice 457 672 1129 
total 2710 5285 7995 
Table 2: Statistics in each event type in both Chinese 
and English data sets 
 
Features: The features have been illustrated in 
Table 1 in Section 3.2.  
845
Classification algorithm: The maximum en-
tropy (ME) classifier is implemented with the 
public tool, Mallet Toolkits3 . 
Evaluation metric: The performance of event 
type recognition is evaluated with F-score. 
4.2 Experimental Results  
In this section, we evaluate the performance of 
our approach to bilingual classification on trigger 
type determination. For comparison, following 
approaches are implemented: 
? Monolingual: perform monolingual classi-
fication on the Chinese and English corpus 
individually, as shown in Figure 1. 
? Bilingual: perform bilingual classification 
with partial bilingual features, ignoring the 
context features (e.g., context words, con-
text entities) under the assumption that the 
trigger location task is not done. 
? Bilingual_location: perform bilingual clas-
sification by translating each sample into 
another language and using a uniform repre-
sentation with all bilingual features as 
shown in Section 3.2. This is exactly our 
approach. The number of the context words 
and entities before or after the trigger words 
is set as 3. 
0.658
0.706
0.677
0.679
0.678
0.734
0.62
0.64
0.66
0.68
0.7
0.72
0.74
Chinese Test Data English Test Data
F
-
s
c
o
r
e
Monolingual Bilingual Bilingual_location
 
Figure 3: Performance comparison of the three ap-
proaches on the Chinese and English test data 
 
Figure 3 shows the classification results of the 
three approaches on the Chinese and English test 
data. From this figure, we can see that Bilin-
gual_location apparently outperform Monolin-
gual, which verifies the effectiveness of using 
bilingual corpus. Specifically, the improvement 
by our approach in Chinese is impressive, reach-
ing 7.6%. The results also demonstrate the im-
portance of the operation of the trigger location, 
                                                 
3 http://mallet.cs.umass.edu/   
without which, bilingual classification can only 
slightly improve the performance, as shown in 
the English test data.  
The results demonstrate that our bilingual 
classification approaches are more effective for 
the Chinese data. This is understandable because 
the size of English data is much larger than that 
of Chinese data, 5285 vs. 2710, as shown in Ta-
ble 2. Specifically, after checking the results in 
each subtype, we find that some subtypes in Chi-
nese have very few samples while corresponding 
subtypes in English have a certain number sam-
ples. For example, the subtype of 
?Elect/Personnel? only contains 30 samples in 
the Chinese data while 161 samples can be found 
in the English data, which leads a very high im-
provement (15.4%) for the Chinese test data. In 
summary, our bilingual classification approach 
provides an effective way to handle the data 
sparseness problem in even extraction. 
5 Conclusion and Future Work 
This paper addresses the data sparseness problem 
in event extraction by proposing a bilingual clas-
sification approach. In this approach, we use a 
uniform text representation with bilingual fea-
tures and merge the training samples from both 
languages to enlarge the size of the labeled data. 
Furthermore, we handle the difficulty of locating 
the trigger from both the monolingual and bilin-
gual perspectives. Empirical studies show that 
our approach is effective in using bilingual cor-
pus to improve monolingual classification in 
trigger type determination.  
Bilingual event extraction is still in its early 
stage and many related research issues need to be 
investigated in the future work. For example, it is 
required to propose novel approaches to the bi-
lingual processing tasks in other subtasks of 
event extraction. Moreover, it is rather challeng-
ing to consider a whole bilingual processing 
framework when all these subtasks are involved 
together.  
Acknowledgments 
This research work has been partially supported 
by two NSFC grants, No.61375073, and 
No.61273320, one National High-tech Research 
and Development Program of China 
No.2012AA011102, one General Research Fund 
(GRF) project No.543810 and one Early Career 
Scheme (ECS) project No.559313 sponsored by 
the Research Grants Council of Hong Kong, the 
NSF grant of Zhejiang Province No.Z1110551. 
846
References  
Ahn D. 2006. The Stages of Event Extraction. In Pro-
ceedings of the Workshop on Annotating and Rea-
soning about Time and Events, pp.1~8. 
Bethard S. and J. Martin. 2006. Identification of 
Event Mentions and Their Semantic Class. In Pro-
ceedings of EMNLP-2006, pp.146-154. 
Chen C. and V. NG. 2012. Joint Modeling for Chi-
nese Event Extraction with Rich Linguistic Fea-
tures. In Proceedings of COLING-2012, pp. 529-
544. 
Chen Z. and H. Ji. 2009. Language Specific Issue and 
Feature Exploration in Chinese Event Extraction. 
In Proceedings of NAACL-2009, pp. 209-212. 
Haghighi A., P. Liang, T. Berg-Kirkpatrick and D. 
Klein. 2008. Learning Bilingual Lexicons from 
Monolingual Corpora. In Proceedings of ACL-
2008, pp. 771-779. 
Hong Y., J. Zhang., B. Ma., J. Yao., and G. Zhou. 
2011. Using Cross-Entity Inference to Improve 
Event Extraction. In Proceedings of ACL-2011, pp. 
1127?1136. 
Ismail A., and S. Manandhar. 2010. Bilingual Lexicon 
Extraction from Comparable Corpora Using In-
domain Terms. In Proceedings of COLING-2010, 
pp.481-489. 
Ji H. 2009. Cross-lingual Predicate Cluster Acquisi-
tion to Improve Bilingual Event Extraction by In-
ductive Learning. In Proceedings of the Workshop 
on Unsupervised and Minimally Supervised Learn-
ing of Lexical Semantics, pp. 27-35. 
Ji H, and R. Grishman. 2008. Refining Event Extrac-
tion through Cross-Document Inference. In Pro-
ceedings of ACL-2008, pp. 254-262. 
Koehn P., F. Och, and D. Marcu. 2003. Statistical 
Phrase-based Translation. In Proceedings of HTL-
NAACL-2003, pp. 127-133. 
Li P., and G. Zhou. 2012. Employing Morphological 
Structures and Sememes for Chinese Event Extrac-
tion. In Proceedings of COLING-2012, pp. 1619-
1634. 
Li P., Q. Zhu and G. Zhou. 2013. Using Composition-
al Semantics and Discourse Consistency to Im-
prove Chinese Trigger Identification. In Proceed-
ings of COLING-2013, pp. 399-415. 
Li Q, H Ji, and H. Liang. 2013. Joint Event Extraction 
via Structured Prediction with Global Features. In 
Proceedings of ACL-2013, pp. 73-82. 
Li S, R Wang, H Liu, and CR Huang. 2013. Active 
Learning for Cross-Lingual Sentiment Classifica-
tion. In Proceedings of Natural Language Pro-
cessing and Chinese Computing, pp. 236-246. 
Liao S and R. Grishman. 2010. Using Document Lev-
el Cross-event Inference to Improve Event Extrac-
tion. In Proceedings of ACL-2010, pp. 789-797. 
Lu B., C. Tan, C. Cardie and B. K. Tsou. 2011. Joint 
Bilingual Sentiment Classification with Unlabeled 
Parallel Corpora. In Proceedings of ACL-2011, pp. 
320-330.  
Och F., C. Tillmann, and H. Ney. 1999. Improved 
Alignment Models for Statistical Machine Transla-
tion. In Proceedings of EMNLP-1999, pp.20-28. 
Tan H., T. Zhao, and J. Zheng. 2008. Identification of 
Chinese Event and Their Argument Roles. In Pro-
ceedings of  CITWORKSHOPS-2008,  pp. 14-19. 
Wan X. 2008. Using Bilingual Knowledge and En-
semble Techniques for Unsupervised Chinese Sen-
timent Analysis. In  Proceedings of EMNLP-2008, 
pp. 553-561. 
Zhao Y., Y. Wang, B. Qin, et al 2008. Research on 
Chinese Event Extraction. In Proceedings of Jour-
nal of  Chinese Information, 22(01), pp. 3-8. 
847
