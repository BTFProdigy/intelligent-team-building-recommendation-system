Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 81?84,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Realistic Grammar Error Simulation using Markov Logic 
 
Sungjin Lee 
Pohang University of Science and 
Technology 
Pohang, Korea 
junion@postech.ac.kr  
Gary Geunbae Lee 
Pohang University of Science and 
Technology 
Pohang, Korea 
gblee@postech.ac.kr 
 
  
 
Abstract 
The development of Dialog-Based Computer-
Assisted Language Learning (DB-CALL) sys-
tems requires research on the simulation of 
language learners. This paper presents a new 
method for generation of grammar errors, an 
important part of the language learner simula-
tor. Realistic errors are generated via Markov 
Logic, which provides an effective way to 
merge a statistical approach with expert know-
ledge about the grammar error characteristics 
of language learners. Results suggest that the 
distribution of simulated grammar errors gen-
erated by the proposed model is similar to that 
of real learners. Human judges also gave con-
sistently close judgments on the quality of the 
real and simulated grammar errors. 
1 Introduction 
Second Language Acquisition (SLA) researchers 
have claimed that feedback provided during con-
versational interaction facilitates the acquisition 
process. Thus, interest in developing Dialog-
Based Computer Assisted Language Learning 
(DB-CALL) systems is rapidly increasing. How-
ever, developing DB-CALL systems takes a long 
time and entails a high cost in collecting learnerV? 
data. Also, evaluating the systems is not a trivial 
task because it requires numerous language 
learners with a wide range of proficiency levels 
as subjects.  
While previous studies have considered user 
simulation in the development and evaluation of 
spoken dialog systems (Schatzmann et al, 2006), 
they have not yet simulated grammar errors be-
cause those systems were assumed to be used by 
native speakers, who normally produce few 
grammar errors in utterances. However, as tele-
phone-based information access systems become 
more commonly available to the general public, 
the inability to deal with non-native speakers is 
becoming a serious limitation since, at least for 
some applications, (e.g. tourist information, le-
gal/social advice) non-native speakers represent 
a significant portion of the everyday user popula-
tion. Thus, (Raux and Eskenazi, 2004) conducted 
a study on adaptation of spoken dialog systems 
to non-native users. In particular, DB-CALL sys-
tems should obviously deal with grammar errors 
because language learners naturally commit nu-
merous grammar errors. Thus grammar error si-
mulation should be embedded in the user simula-
tion for the development and evaluation of such 
systems. 
In Foster?s (2007) pioneering work, she de-
scribed a procedure which automatically intro-
duces frequently occurring grammatical errors 
into sentences to make ungrammatical training 
data for a robust parser. However the algorithm 
cannot be directly applied to grammar error gen-
eration for language learner simulation for sever-
al reasons. First, it either introduces one error per 
sentence or none, regardless of how many words 
of the sentence are likely to generate errors. 
Second, it determines which type of error it will 
create only by relying on the relative frequencies 
of error types and their relevant parts of speech. 
This, however, can result in unrealistic errors. As 
exemplified in Table 1, when the algorithm tries 
to create an error by deleting a word, it would 
probably omit the word ?go? because verb is one 
of the most frequent parts of speech omitted re-
sulting in an unrealistic error like the first simu-
lated output. However, Korean/Japanese lan-
guage learners of English tend to make subject-
verb agreement errors, omission errors of the 
preposition of prepositional verbs, and omission 
errors of articles because their first language 
does not have similar grammar rules so that they 
may be slow on the uptake of such constructs. 
Thus, they often commit errors like the second 
simulated output.  
81
This paper develops an approach to statistical 
grammar error simulation that can incorporate 
this type of knowledge about language learners? 
error characteristics and shows that it does in-
deed result in realistic grammar errors. The ap-
proach is based on Markov logic, a representa-
tion language that combines probabilistic graphi-
cal models and first-order logic (Richardson and 
Domingos, 2006). Markov logic enables concise 
specification of very complex models. Efficient 
open-source Markov logic learning and inference 
algorithms were used to implement our solution.  
We begin by describing the overall process of 
grammar error simulation and then briefly re-
viewing the necessary background in Markov 
logic. We then describe our Markov Logic Net-
work (MLN) for grammar error simulation. Fi-
nally, we present our experiments and results. 
2 Overall process of grammar error si-
mulation 
The task of grammar error simulation is to gen-
erate an ill-formed sentence when given a well-
formed input sentence. The generation procedure 
involves three steps: 1) Generating probability 
over error types for each word of the well-
formed input sentence through MLN inference 2) 
Determining an error type by sampling the gen-
erated probability for each word 3) Creating an 
ill-formed output sentence by realizing the cho-
sen error types (Figure 1).  
3 Markov Logic 
Markov logic is a probabilistic extension of finite 
first-order logic (Richardson and Domingos, 
2006). An MLN is a set of weighted first-order 
clauses. Together with a set of constants, it de-
fines a Markov network with one node per 
ground atom and one feature per ground clause. 
The weight of a feature is the weight of the first-
order clause that originated it. The probability of 
a state x in such a network is given by ?(?)  =
 (1/?) ??? (? ?? ?? (?)? ), where ?  is a normali-
zation constant, ??  is the weight of the ?th clause, ??  =  1 if the ?th clause is true, and ??  =  0 oth-
erwise.  
Markov logic makes it possible to compactly 
specify probability distributions over complex 
relational domains. We used the learning and 
inference algorithms provided in the open-source 
Alchemy package (Kok et al, 2006).  In particu-
lar, we performed inference using the belief 
propagation algorithm (Pearl, 1988), and genera-
tive weight learning. 
4 An MLN for Grammar Error Simula-
tion 
This section presents our MLN implementation 
which consists of three components: 1) Basic 
formulas based on parts of speech, which are 
comparable to Foster?s method 2) Analytic for-
mulas drawn from expert knowledge obtained by 
error analysis on a learner corpus 3) Error limit-
ing formulas that penalize statistical model?s 
over-generation of nonsense errors.  
4.1 Basic formulas 
Error patterns obtained by error analysis, which 
might capture a lack or an over-generalization of 
knowledge of a particular construction, cannot 
explain every error that learners commit. Be-
cause an error can take the form of a perfor-
mance slip which can randomly occur due to 
carelessness or tiredness, more general formulas 
are needed as a default case. The basic formulas 
are represented by the simple rule: 
y ????????, ?, +??? ? ?)?????????, ?, +??) 
where all free variables are implicitly universally 
quantified. The ?+??, +??? notation signifies 
that the MLN contains an instance of this rule for 
each (part of speech, error type) pair. The evi-
Input sentence 
He wants to go to a movie theater 
Unrealistic simulated output 
He wants to to a movie theater 
Realistic simulated output 
He want go to movie theater 
Table 1: Examples of simulated outputs  
Figure 1: An example process of grammar error simulation 
82
dence predicate in this case is ??)?????, ?, ??), 
which is true iff the ?th position of the sentence ? 
has the part of speech ??. The query predicate is ?)?????????, ?, ??). It is true iff the ?th position 
of the sentence ? has the error type ??, and infer-
ring it returns the probability that the word at 
position ? would commit an error of type ??.  
4.2 Analytic formulas 
On top of the basic formulas, analytic formulas 
add concrete knowledge of realistic error charac-
teristics of language learners. Error analysis and 
linguistic differences between the first language 
and the second language can identify various 
error sources for each error type. We roughly 
categorize the error sources into three groups for 
explanation: 1) Over-generalization of the rules 
of the second language 2) Lack of knowledge of 
some rules of the second language 3) Applying 
rules and forms of the first language into the 
second language. 
Often, English learners commit pluralization 
error with irregular nouns. This is because they 
over-generalize the pluralization rule, i.e. attach-
ing ?s/es?, so that they apply the rule even to ir-
regular  nouns such  as ?fish? and ?feet? etc. This 
characteristic is captured by the simple formula: 
y ?????????????????????, ?? ? ????????, ?, ???? 
? ?)?????????, ?, ?_?????_?) 
where ?????????????????????, ?? is true iff the ?th word of the sentence ? is an irregular plural 
and N_NUM_SUB is the abbreviation for substi-
tution by noun number error.  
One trivial error caused by a lack of know-
ledge of the second language is using the singu-
lar noun form for weekly events: 
y ??????, ? ? 1, ???  ? ?????????, ??? ????????, ?, ????? ?)?????????, ?, ?_?????_?) 
where ??????, ? ? 1, ??? is true iff the ? ? 1th 
word is ?on? and ?????????, ?? is true iff the ?th word of the sentence ? is a noun describing 
day like Sunday(s). Another example is use of 
plurals behind ?every? due to the ignorance that a 
noun modified by ?every? should be singular: 
y ??????, ??, ?????? ? ???????????????, ??, ??? 
? ?)?????????, ??, ?_?????_?) 
where ???????????????, ??, ???  is true iff the ??th word is the determiner of the ??th word. 
An example of errors by applying the rules of 
the first language is that Korean/Japanese often 
allows omission of the subject of a sentence; thus, 
they easily commit the subject omission error. 
The following formula is for the case: 
y ?????????, ??? ?)?????????, ?,?_???_???) 
where ?????????, ?? is true iff the ?th word is the 
subject and N_LXC_DEL is the abbreviation for 
deletion by noun lexis error.1 
4.3 Error limiting formulas 
A number of elementary formulas explicitly 
stated as hard formulas prevent the MLN from 
generating improbable errors that might result 
from over-generations of the statistical model. 
For example, a verb complement error should not 
have a probability at the words that are not com-
plements of a verb: 
y ! ????????????????, ??, ??? 
? ! ?)?????????, ??, ?_???_???). 
where ?!? denotes logically ?not? and ?.? at the 
end signifies that it is a hard formula. Hard formu-
las are given maximum weight during inference. ????????????????, ??, ???  is true iff the ?? th 
word is a complement of the verb at the ??th po-
sition and V_CMP_SUB is the abbreviation for 
substitution by verb complement error. 
5 Experiments  
Experiments used the NICT JLE Corpus, which 
is speech samples from an English oral profi-
ciency interview test, the ACTFL-ALC Standard 
Speaking Test (SST). 167 of the files are error 
annotated. The error tagset consists of 47 tags 
that are described in Izumi (2005). We appended 
structural type of errors (substitution, addition, 
deletion) to the original error types because 
structural type should be determined when creat-
ing an error. For example, V_TNS_SUB consists 
of the original error type V_TNS (verb tense) and 
structural type SUB (substitution).  Level-
specific language learner simulation was accom-
plished by dividing the 167 error annotated files 
into 3 level groups: Beginner(level1-4), Interme-
diate(level5-6), Advanced(level7-9).  
The grammar error simulation was compared 
with real learnerV? errors and the baseline model 
using only basic formulas comparable to Foster?s 
algorithm, with 10-fold cross validations per-
formed for each group. The validation results 
were added together across the rounds to com-
pare the number of simulated errors with the 
number of real errors. Error types that occurred 
less than 20 times were excluded to improve re-
liability. Result graphs suggest that the distribu-
tion of simulated grammar errors generated by 
the proposed model using all formulas is similar 
to that of real learners for all level groups and the 
                                                 
1
 Because space is limited, all formulas can be found at 
http://isoft.postech.ac.kr/ges/grm_err_sim.mln 
83
proposed model outperforms the baseline model 
using only the basic formulas. The Kullback-
Leibler divergences, a measure of the difference 
between two probability distributions, were also 
measured for quantitative comparison. For all 
level groups, the Kullback-Leibler divergence of 
the proposed model from the real is less than that 
of the baseline model (Figure 2). 
Two human judges verified the overall realism 
of the simulated errors. They evaluated 100 ran-
domly chosen sentences consisting of 50 sen-
tences each from the real and simulated data. The 
sequence of the test sentences was mixed so that 
the human judges did not know whether the 
source of the sentence was real or simulated. 
They evaluated sentences with a two-level scale 
(0: Unrealistic, 1: Realistic). The result shows 
that the inter evaluator agreement (kappa) is 
moderate and that both judges gave relatively 
close judgments on the quality of the real and 
simulated data (Table 2). 
6 Summary and Future Work  
This paper introduced a somewhat new research 
topic, grammar error simulation. Expert know-
ledge of error characteristics was imported to 
statistical modeling using Markov logic, which 
provides a theoretically sound way of encoding 
knowledge into probabilistic first order logic. 
Results indicate that our method can make an 
error distribution more similar to the real error 
distribution than the baseline and that the quality 
of simulated sentences is relatively close to that 
of real sentences in the judgment of human eva-
luators. Our future work includes adding more 
expert knowledge through error analysis to in-
crementally improve the performance. Further-
more, actual development and evaluation of a 
DB-CALL system will be arranged so that we 
may investigate how much the cost of collecting 
data and evaluation would be reduced by using 
language learner simulation. 
Acknowledgement  
This research was supported by the MKE (Ministry of 
Knowledge Economy), Korea, under the ITRC (In-
formation Technology Research Center) support pro-
gram supervised by the IITA (Institute for Informa-
tion Technology Advancement) (IITA-2009-C1090-
0902-0045). 
References  
Foster, J. 2007. Treebanks Gone Bad: Parser evalua-
tion and retraining using a treebank of ungrammat-
ical sentences. IJDAR, 10(3-4), 129-145. 
Izumi, E et al 2005. Error Annotation for Corpus of 
Japanese Learner English. In Proc. International 
Workshop on Linguistically Interpreted Corpora  
Kok, S. et al 2006. The Alchemy system for statistic-
al relational AI. http://alchemy.cs.washington.edu/. 
Pearl, J. 1988. Probabilistic Reasoning in Intelligent 
Systems Morgan Kaufmann. 
Raux, A. and Eskenazi, M. 2004. Non-Native Users in 
the Let's Go!! Spoken Dialogue System: Dealing 
with Linguistic Mismatch, HLT/NAACL. 
Richardson, M. and Domingos, P. 2006. Markov logic 
networks. Machine Learning, 62(1):107-136. 
Schatzmann, J. et al 2006. A survey of statistical user 
simulation techniques for reinforcement-learning 
of dialogue management strategies, The Know-
ledge Engineering ReviewVolProceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 344?346,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
POMY: A Conversational Virtual Environment for Language Learning 
in POSTECH 
 
Hyungjong 
 Noh 
Kyusong  
Lee 
Sungjin  
Lee 
 
Gary Geunbae 
Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology, Pohang, South Korea 
{nohhj, kyusonglee, junion, gblee}@postech.ac.kr 
  
 
 
Abstract 
This demonstration will illustrate an inter-
active immersive computer game, POMY, 
designed to help Korean speakers learn 
English. This system allows learners to ex-
ercise their visual and aural senses, receiv-
ing a full immersion experience to increase 
their memory and concentration abilities to 
a greatest extent. In POMY, learners can 
have free conversations with game charac-
ters and receive corrective feedback to their 
errors. Game characters show various emo-
tional expressions based on learners? input 
to keep learners motivated. Through this 
system, learners can repeatedly practice 
conversations in everyday life setting in a 
foreign language with no embarrassment. 
1 Introduction 
The needs for computer-based methods for learn-
ing language skills and components are increasing. 
One of the ultimate goals of computer-assisted 
language learning is to provide learners with an 
immersive environment that facilitates acquiring 
communicative competence. According to Second 
Language Acquisition (SLA) theories, there are 
some essential factors for improving learners? con-
versational skills: 1) comprehensible inputs and 
outputs, 2) corrective feedback, and 3) motivation 
and attitude. SLA theories imply that providing 
learners with the opportunity to have free conver-
sations with someone who can correct their errors 
is very important for successful acquisition of for-
eign languages. Moreover, motivation is another 
crucial factor; therefore a good CALL system 
should have elements which can interest learners 
[1]. 
Considering these requirements, we have devel-
oped a conversational English education frame-
work, POMY (POstech iMmersive English studY). 
The program allows users to exercise their visual 
and aural senses to receive a full immersion expe-
rience to develop into independent English as a 
Foreign Language (EFL) learners and increase 
their memory and concentration abilities to a 
greatest extent [2].  
 
 
Figure 1: Example screenshots of POMY: path-finding, post office, and market 
344
2 Demonstrated System 
In order to provide learners with immersive world, 
we have developed a virtual reality environment 
using the Unity 3D game engine1. For the domains 
that learners are exposed to, we select such do-
mains as path-finding, market, post office, library, 
and movie theater (Figure 1) to ensure having 
learners practice conversations in everyday life 
setting. To keep learners motivated and interested 
during learning sessions, learners are encouraged 
to accomplish several missions. For example, the 
first mission in the post office is to send a camera 
to one?s uncle in England. The package must be 
insured and delivered by the next week. In order to 
send the package, a learner must talk to Non-
Player Characters (NPCs) to fill in the zip-code 
properly.  
All NPCs can perceive the utterances of learners, 
especially Korean learners of English. Korean 
learners? production of the sound is different from 
those of native speakers, resulting in numerous 
pronunciation errors. Therefore, we have collected 
a Korean-English corpus to train acoustic models. 
In addition, since language learners commit nu-
merous grammatical errors, we should consider 
this to understand their utterances. Thus, we statis-
tically infer the actual learners' intention by taking 
not only the utterance itself but also the dialog con-
text into consideration, as human tutors do [1]. 
While free conversation is invaluable to the 
acquisition process, it is not sufficient for learners 
to fully develop their L2 proficiency. Corrective 
feedback to learners? grammatical errors is 
necessary for improving accuracy in their 
interlanguage. For this purpose, we designed a 
                                                          
1 http://unity3d.com/ 
special character, Ghost Tutor, which plays the 
role of English tutor and helps learners to use more 
appropriate words and expressions during the game. 
When a learner produces ungrammatical utterances, 
the Ghost Tutor provides both implicit and explicit 
negative and positive feedback in a form of 
elicitation or recast, which was manifested as 
effective ways in the second language acquisition 
processes [3].  To provide corrective feedback on 
grammatical errors, we use a method which con-
sists of two sub-models: the grammaticality check-
ing model and the error type classification model 
[4]. Firstly, we automatically generate grammatical 
errors that learners usually commit [5-6], and con-
struct error patterns based on the articulated errors. 
Then the grammaticality checking model classifies 
the recognized user speech based on the similarity 
between the error patterns and the recognition re-
sult using confidence scores. After that, the error 
type classification model chooses the error type 
based on the most similar error pattern and the er-
ror frequency extracted from a learner corpus. 
Finally, the human perception of NPC?s emo-
tional expressions plays a crucial role in human 
computer interaction. Thus, all NPCs are provided 
with a number of communicative animations such 
as talking, laughing, waving, crying, thinking, and 
getting angry (Figure 2).The total number of ani-
mations is over thirty from which the system can  
select one based on the response of a learner. The 
system generates positive expressions such as 
clapping and laughing when the learner answers 
correctly, and negative expressions such as crying 
and getting angry for incorrect answers.  
 
 
 
 
 
Figure 2: Various character animations 
345
Acknowledgments 
This work was supported by the Industrial Strate-
gic technology development program, 10035252, 
development of dialog-based spontaneous speech 
interface technology on mobile platform, funded 
by the Ministry of Knowledge Economy (MKE, 
Korea), and by Basic Science Research Program 
through the National Research Foundation of Ko-
rea (NRF) funded by the Ministry of Education, 
Science and Technology (2010-0019523). 
References  
Lee, S., Noh, H., Lee, J., Lee, K., Lee, G. G., Sagong, S., 
Kim, M. 2011. On the Effectiveness of Robot-
Assisted Language Learning, ReCALL Journal, 
Vol.23(1). 
Lee, S., Noh, H., Lee, J., Lee, K., Lee, G. G. 2010. 
POSTECH Approaches for Dialog-based English 
Conversation Tutoring. Proceedings of the APSIPA 
annual summit and conference, Singapore. 
Long, M. H., Inagaki, S., Ortega, L. 1998. The Role of 
Input and Interaction in Second Language Acquisi-
tion. The Modern Language Journal, 82, 357-371.  
Lee, S., Noh, H., Lee, K., Lee, G. G. 2011. Grammatical 
error detection for corrective feedback provision in 
oral conversations. Proceedings of the 25th AAAI 
conference on artificial intelligence (AAAI-11), San 
Francisco. 
Lee, S., Lee J., Noh, H., Lee, K., Lee, G. G, 2011. 
Grammatical Error Simulation for Computer-
Assisted Language Learning, Knowledge-Based Sys-
tems (to be published). 
Lee, S. and Lee, G. G. 2009. Realistic grammar error 
simulation using markov logic. Proceedings of the 
ACL, Singapore. 
 
346
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 50?59,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
An Unsupervised Approach to User Simulation: toward Self-Improving
Dialog Systems
Sungjin Lee1,2 and Maxine Eskenazi1
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania
2Computer Science and Engineering, Pohang University of Science and Technology, South Korea
{sungjin.lee, max}@cs.cmu.edu1, junion@postech.ac.kr2
Abstract
This paper proposes an unsupervised ap-
proach to user simulation in order to automati-
cally furnish updates and assessments of a de-
ployed spoken dialog system. The proposed
method adopts a dynamic Bayesian network
to infer the unobservable true user action from
which the parameters of other components are
naturally derived. To verify the quality of the
simulation, the proposed method was applied
to the Let?s Go domain (Raux et al, 2005)
and a set of measures was used to analyze the
simulated data at several levels. The results
showed a very close correspondence between
the real and simulated data, implying that it is
possible to create a realistic user simulator that
does not necessitate human intervention.
1 Introduction
For the past decade statistical approaches to dialog
modeling have shown positive results for optimizing
a dialog strategy with real data by applying well-
understood machine learning methods such as rein-
forcement learning (Henderson et al, 2008; Thom-
son and Young, 2010; Williams and Young, 2007b).
User simulation is becoming an essential component
in developing and evaluating such systems. In this
paper we describe an unsupervised process to au-
tomatically develop user simulators. The motiva-
tion for this comes from the fact that many systems
are presently moving from being simple lab simu-
lations to actual deployed systems with real users.
These systems furnish a constant flow of new data
that needs to be processed in some way. Our goal is
to minimize human intervention in processing this
data. Previously, data had to be hand-annotated, a
slow and costly process. Recently crowdsourcing
has made annotation faster and less expensive, but
all of the data still has to be processed and time
must be spent in creating the annotation interface
and tasks, and in quality control. Our goal is to pro-
cess the metadata (e.g. user actions, goals, error ty-
pology) in an unsupervised manner. And our method
eliminates the need for human transcription and an-
notation by inferring the user goal from grounding
information. We also consider user actions as la-
tent variables which are inferred based on observa-
tions from Automatic Speech Recognition (ASR).
We used the above inferred user actions paired with
the observed actions to build an error model. Since
the focus of this work is placed on improving and
evaluating the dialog strategy, error simulation can
be carried out at the semantic level. This eliminates
the need for transcription, which would have neces-
sitated an error simulation at the surface level. The
end result here will be a system that has as little hu-
man intervention as possible.
This paper is structured as follows. Section 2 de-
scribes previous research and the novelty of our ap-
proach. Section 3 elaborates on our proposed un-
supervised approach to user simulation. Section 4
explains the experimental setup. Section 5 presents
and discusses the results. Finally, Section 6 con-
cludes with a brief summary and suggestions for fu-
ture research.
2 Related Work
Previous user simulation studies can be roughly cat-
egorized into rule-based methods (Chung, 2005;
50
Lopez-Cozar et al, 2006; Schatzmann et al, 2007a)
and data-driven methods (Cuayahuitl et al, 2005;
Eckert et al, 1997; Jung et al, 2009; Levin et al,
2000; Georgila et al, 2006; Pietquin, 2004). Rule-
based methods generally allow for more control over
their designs for the target domain while data-driven
methods afford more portability from one domain to
another and are attractive for modeling user behav-
ior based on real data. Although development costs
for data-driven methods are typically lower than
those of rule-based methods, previous data-driven
approaches have still required a certain amount of
human effort. Most intention-level models take a
semantically annotated corpus to produce user in-
tention without introducing errors (Cuayahuitl et al,
2005; Jung et al, 2009). Surface-level approaches
need transcribed data to train their surface form and
error generating models (Jung et al, 2009; Schatz-
mann et al, 2007b). A few studies have attempted to
directly simulate the intention, surface, and error by
applying their statistical methods on the recognized
data rather than on the transcribed data (Georgila et
al., 2006; Schatzmann et al, 2005). Although such
approaches can avoid human intervention, the sole
incorporation of erroneous user action can propa-
gate those errors to the higher-level discourse fea-
tures which are computed from them, and thus could
result in less realistic user behavior. In this work, the
true user action is treated as a hidden variable and,
further, its associated dialog history is also viewed as
latent so that the uncertainty of the true user action
is properly controlled in a principled manner. Syed
and Williams (2008) adopted the Expectation Max-
imization algorithm for parameter learning for a la-
tent variable model. But their method still requires a
small amount of transcribed data to learn the obser-
vation confusability, and it suffers from overfitting
as a general property of maximum likelihood. To
address this problem, we propose a Bayesian learn-
ing method, which requires no transcribed data.
3 Unsupervised Approach to User
Simulation
Before describing each component in detail, we
present the overall process of user simulation with
an example in the Let?s Go domain in Figure 1. To
begin a dialog, the user simulator first sets the user
Figure 1: The overall process of user simulation in the
Let?s Go domain, where users call the spoken dialog sys-
tem to get bus schedule information for Pittsburgh
goal by sampling the goal model. Then the user sim-
ulator engages in a conversation with the dialog sys-
tem until the termination model ends it. At each
turn, the termination model randomly determines
whether the dialog will continue or not. If the dia-
log continues, the user model generates user actions
at the predicate level with respect to the given user
goal and system action. Having the user actions, the
error template model transforms some user actions
into other actions if necessary and determines which
action will receive an incorrect value. After that, the
error value model substantiates the values by draw-
ing a confusable value if specified to be incorrect or
by using the goal value. Finally, a confidence score
will be attached to the user action by sampling the
confidence score model which conditions on the cor-
rectness of the final user action.
3.1 Goal Model
The goal model is the first component to be de-
fined in terms of the working flow of the user sim-
ulator. In order to generate a plausible user goal
in accordance with the frequency at which it ap-
pears in a real situation, the dialog logs are parsed
to look for the grounding information1 that the users
have provided. Since the representation of a user
goal in this study is a vector of constraints required
by a user, for example [Route:61C, Source:CMU,
1Specifically, we used explicitly confirmed information by
the system for this study
51
Destination:AIRPORT, Time:6 PM], each time we
encounter grounding information that includes the
constraints used in the backend queries, this is added
to the user goal. If two actions contradict each other,
the later action overwrites the earlier one. Once all
of the user goals in the data have been gathered,
a discrete distribution over the user goal is learned
using a maximum likelihood estimation. Because
many variables later in this paper are discrete, a gen-
eral notation of a conditional discrete distribution is
expressed as follows:
p(xi|xpa(i),?) =
?
k,k?
??(pa(i),k)?(xi,k
?)
k,k? (1)
where k represents the joint configuration of all the
parents of i and ?(?, ?) denotes Kronecker delta. Note
that ?k? ?k,k? = 1. Given this notation, the goal
model ? can be written in the following form:
g ? p(g|?) =
?
k
??(g,k)k (2)
3.2 User Model
Having generated a user goal, the next task is to infer
an appropriate user action for the given goal and sys-
tem action. This is what the user model does. Since
one of key properties of our unsupervised approach
is that the true user actions are not observable, the
user model should maintain a belief over the dia-
log state by taking into consideration the observed
user actions. Inspired by (Williams et al, 2005),
to keep the complexity of the user model tractable,
a dynamic Bayesian network is adopted with sev-
eral conditional independence assumptions, giving
rise to the graphical structure which is shown in Fig-
ure 2. Unlike belief tracking in a dialog system, the
user goal in a user simulation is pre-determined be-
fore the beginning of the dialog. As with most pre-
vious studies, this property allows the user model
to deal with a predicate-level action consisting of a
speech act and a concept (e.g. [Inform(Source), In-
form(Time)]) and is only concerned about whether a
given field is specified or not in the user goal (e.g.
Bus:Unspecified, Source:Specified). This abstract-
level handling enables the user model to employ ex-
act inference algorithms such as the junction tree
algorithm (Lauritzen and Spiegelhalter, 1988) for
more efficient reasoning over the graphical structure.
Figure 2: The graphical structure of the dynamic
Bayesian network for the user model. g denotes the user
goal and st,ut,ht,ot represents the system action, the
user action, the dialog history, and the observed user ac-
tion for each time slice, respectively. The shaded items
are observable and the transparent ones are latent.
The joint distribution for this model is given by
p(g,S,H,U,O|?)
= p(h0|pi)
?
t
p(ut|g, st,ht?1,?)
? p(ht|ht?1,ut,?)p(ot|ut, ?)
(3)
where a capital letter stands for the set of
corresponding random variables, e.g., U =
{u1, . . . ,uN}, and ? = {pi,?,?, ?} denotes the
set of parameters governing the model2.
For a given user goal, the user model basically
performs an inference to obtain a marginal distribu-
tion over ut for each time step from which it can
sample the probability of a user action in a given
context:
ut ? p(ut|g, st1,ut?11 ,?) (4)
where st1 denotes the set of system actions from time
1 to time t and ut?11 is the set of previously sampled
user actions from time 1 to time t? 1.
3.2.1 Parameter Estimation
As far as parameters are concerned, ? is a determin-
istic function that yields a fraction of an observed
confidence score in accordance with the degree of
agreement between ut and ot:
p(ot|ut) = CS(ot) ?
( |ot ? ut|
|ot ? ut|
)p
+  (5)
2Here, uniform prior distributions are assigned on g and S
52
where CS(?) returns the confidence score of the as-
sociated observation and p is a control variable over
the strength of disagreement penalty3. In addition, pi
and ? are deterministically set by simple discourse
rules, for example:
p(ht = Informed|ht?1,ut) ={
1 if ht?1 = Informed or ut = Inform(?),
0 otherwise.
(6)
The only parameter that needs to be learned in the
user model, therefore, is ? and it can be estimated
by maximizing the likelihood function (Equation 7).
The likelihood function is obtained from the joint
distribution (Equation 3) by marginalizing over the
latent variables.
p(g,S,O|?) =
?
H,U
p(g,S,H,U,O|?) (7)
Since direct maximization of the likelihood func-
tion will lead to complex expressions with no
closed-form solutions due to the latent variables, the
Expectation-Maximization (EM) algorithm is an ef-
ficient framework for finding maximum likelihood
estimates.
As it is well acknowledged, however, that over-
fitting can arise as a general property of maximum
likelihood, especially when only a small amount of
data is available, a Bayesian approach needs to be
adopted. In a Bayesian model, any unknown pa-
rameter is given a prior distribution and is absorbed
into the set of latent variables, thus it is infeasible
to directly evaluate the posterior distribution of the
latent variables and the expectations with respect to
this distribution. Therefore a deterministic approx-
imation, called mean field theory (Parisi, 1988), is
applied.
In mean field theory, the family of posterior distri-
butions of the latent variables is assumed to be par-
titioned into disjoint groups:
q(Z) =
M?
i=1
qi(Zi) (8)
where Z = {z1, . . . , zN} denotes all latent variables
including parameters and Zi is a disjoint group.
3For this study, p was set to 1.0
Amongst all distributions q(Z) having the form of
Equation 8, we then seek the member of this family
for which the divergence from the true posterior dis-
tribution is minimized. To achieve this, the follow-
ing optimization with respect to each of the qi(Zi)
factors is to be performed in turn (Bishop, 2006):
ln q?j (Zj) = Ei 6=j
[
ln p(X,Z)
]
+ const (9)
where X = {x1, . . . ,xN} denotes all observed vari-
ables and Ei 6=j means an expectation with respect to
the q distributions over all groups Zi for i 6= j.
Now, we apply the mean field theory to the user
model. Before doing so, we need to introduce the
prior over the parameter ? which is a product of
Dirichlet distributions4.
p(?) =
?
k
Dir(?k|?0k)
=
?
k
C(?0k)
?
l
??
0
k?1
k,l
(10)
where k represents the joint configuration of all of
the parents and C(?0k) is the normalization constant
for the Dirichlet distribution. Note that for symme-
try we have chosen the same parameter ?0k for each
of the components.
Next we approximate the posterior distribution,
q(H,U,?) using a factorized form, q(H,U)q(?).
Then we first apply Equation 9 to find an expression
for the optimal factor q?(?):
ln q?(?) = EH,U
[
ln p(g,S,H,U,O,?)
]
+ const
= EH,U
[?
t
ln p(ut|g, st,ht?1,?)
]
+ ln p(?) + const
=
?
t
?
i,j,k,l
(
EH,U
[
?i,j,k,l
]
ln?i,j,k,l
)
+
?
i,j,k,l
(?oi,j,k,l ? 1) ln?i,j,k,l + const
=
?
i,j,k,l
((
EH,U[ni,j,k,l] + (?oi,j,k,l ? 1)
)
? ln?i,j,k,l
)
+ const
(11)
4Note that priors over parameters for deterministic distribu-
tions (e.i., pi,?,and ?) are not necessary.
53
where ?i,j,k,l denotes ?(g, i)?(st, j)?(ht?1, k)
?(ut, l) and ni,j,k,l is the number of times where
g = i, st = j,ht?1 = k, and ut = l. This leads
to a product of Dirichlet distributions by taking the
exponential of both sides of the equation:
q?(?) =
?
i,j,k
Dir(?i,j,k|?i,j,k),
?i,j,k,l = ?0i,j,k,l + EH,U[ni,j,k,l]
(12)
To evaluate the quantity EH,U[ni,j,k,l], Equation 9
needs to be applied once again to obtain an op-
timal approximation of the posterior distribution
q?(H,U).
ln q?(H,U) = E?
[
ln p(g,S,H,U,O,?)
]
+ const
= E?
[?
t
ln p(ut|g, st,ht?1,?)
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
]
+ const
=
?
t
(
E?
[
ln p(ut|g, st,ht?1,?)
]
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
)
+ const
(13)
where E?
[
ln p(ut|g, st,ht?1,?)
] can be obtained
using Equation 12 and properties of the Dirichlet
distribution:
E?
[
ln p(ut|g, st,ht?1,?)
]
=
?
i,j,k,l
?i,j,k,lE?
[
ln?i,j,k,l
]
=
?
i,j,k,l
?i,j,k,l(?(?i,j,k,l)? ?(??i,j,k))
(14)
where ?(?) is the digamma function with ??i,j,k =?
l ?i,j,k,l. Because computing EH,U[ni,j,k,l] is
equivalent to summing each of the marginal poste-
rior probabilities q?(ht?1,ut) with the same con-
figuration of conditioning variables, this can be
done efficiently by using the junction tree algorithm.
Note that the expression on the right-hand side for
both q?(?) and q?(H,U) depends on expectations
computed with respect to the other factors. We
will therefore seek a consistent solution by cycling
through the factors and replacing each in turn with a
revised estimate.
3.3 Error Model
The purpose of the error model is to alter the user
action to reflect the prevalent speech recognition and
understanding errors. The error generation process
consists of three steps: the error model first gen-
erates an error template then fills it with erroneous
values, and finally attaches a confidence score.
Given a user action, the error model maps it into a
distorted form according to the probability distribu-
tion of the error template model ?:
T (u) ? p(T (u)|u) =
?
k,k?
??(u,k)?(T (u),k
?)
k,k? (15)
where T (?) is a random function that maps a pred-
icate of the user action to an error template, e.g.
T (Inform(Time)) ? Inform(Route:incorrect). To
learn the parameters, the hidden variable ut is sam-
pled using Equation 4 for each observation ot in the
training data and the value part of each observation
is replaced with a binary value representing its cor-
rectness with respect to the user goal. This results in
a set of complete data on which the maximum like-
lihood estimates of ? are learned.
With the error template provided, next, the error
model fills it with incorrect values if necessary fol-
lowing the distribution of the error value model ?
which is separately defined for each concept, other-
wise it will keep the correct value:
C(v) ? p(C(v)|v) =
?
k,k?
??(v,k)?(C(v),k?) (16)
where C(?) is a random function which maps a cor-
rect value to a confusable value, e.g. C(Forbes) ?
Forward. As with the error template model, the pa-
rameters of the error value model are also easily
trained on the dataset of all pairs of a user goal value
and the associated observed value. Because no er-
ror values can be observed for a given goal value, an
unconditional probability distribution is also trained
as a backoff.
Finally, the error model assigns a confidence
score by sampling the confidence score model ?
54
which is separately defined for each concept:
s ? p(s|c) =
?
k,k?
??(c,k)?(s,k?) (17)
where s denotes the confidence score and c repre-
sents the correctness of the value of the user action
which is previously determined by the error tem-
plate model. Since two decimal places are used to
describe the confidence score, the confidence score
model is represented with a discrete distribution.
This lends itself to trivial parameter learning similar
to other models by computing maximum likelihood
estimates on the set of observed confidence scores
conditioned on the correctness of the relevant val-
ues.
In sum, for example, having a user action
[Inform(Source:Forbes), Inform(Time:6 PM)] go
through the sequence of aforementioned models
possibly leads to [Inform(Source:Forward), In-
form(Route:6C)].
3.4 Termination Model
Few studies have been conducted to estimate the
probability that a dialog will terminate at a certain
turn in the user simulation. Most existing work
attempts to treat a termination initiated by a user
as one of the dialog actions in their user models.
These models usually have a limited dialog history
that they can use to determine the next user action.
This Markov assumption is well-suited to ordinary
dialog actions, each generally showing a correspon-
dence with previous dialog actions. It is not diffi-
cult, however, to see that more global contexts (e.g.,
cumulative number of incorrect confirmations) will
help lead a user to terminate a failed dialog. In ad-
dition, the termination action occurs only once at
the end of a dialog unlike the other actions. Thus,
we do not need to put the termination action into
the user model. In order to easily incorporate many
global features involving an entire dialog (Table 1)
into the termination model, the logistic regression
model is adapted. At every turn, before getting into
the user model, we randomly determine whether a
dialog will stop according to the posterior probabil-
ity of the termination model given the current dialog
context.
Feature Description
NT Number of turns
RIC Ratio of incorrect confirmations
RICW Ratio of incorrect confirmationswithin a window
RNONU Ratio of non-understanding
RNONUW Ratio of non-understandingwithin a window
ACS Averaged confidence score
ACSW Averaged confidence scorewithin a window
RCOP Ratio of cooperative turns
RCOPW Ratio of cooperative turnswithin a window
RRT C Ratio of relevant system turnsfor each concept
RRTW C Ratio of relevant system turnsfor each concept within a window
NV C Number of values appeared foreach concept
Table 1: A description of features used for a logistic
regression model to capture the termination probability.
The window size was set to 5 for this study.
4 Experimental Setup
4.1 Data
To verify the proposed method, three months of data
from the Let?s Go domain were split into two months
of training data and one month of test data. Also,
to take the error level into consideration, we classi-
fied the data into four groups according to the aver-
aged confidence score and used each group of data
to build a different error model for each error level.
For comparison purposes, simulated data was gen-
erated for both training and test data by feeding the
same context of each piece of data to the proposed
method. Due to the characteristics of the bus sched-
ule information domain, there are a number of cases
where no bus schedule is available, such as requests
for uncovered routes and places. Such cases were
excluded for clearer interpretation of the result, giv-
ing us the data sets described in Table 2.
4.2 Measures
To date, a variety of evaluation methods have been
proposed in the literature (Cuayahuitl et al, 2005;
Jung et al, 2009; Georgila et al, 2006; Pietquin and
55
Training data Test data
Number of dialogs 1,275 669
Number of turns 9,645 5,103
Table 2: A description of experimental data sets.
Hastie, 2011; Schatzmann et al, 2005; Williams,
2007a). Nevertheless, it remains difficult to find
a suitable set of evaluation measures to assess the
quality of the user simulation. We have chosen
to adopt a set of the most commonly used mea-
sures. Firstly, expected precision (EP), expected re-
call (ER) and F-Score offer a reliable method for
comparing real and simulated data even though it
is not possible to specify the levels that need to be
satisfied to conclude that the simulation is realistic.
These are computed by comparison of the simulated
and real user action for each turn in the corpus:
EP = 100 ? Number of identical actionsNumber of simulated actions (18)
ER = 100 ? Number of identical actionsNumber of real actions (19)
F-Score = 100 ? 2 ? EP ? EREP + ER (20)
Next, several descriptive statistics are employed to
show the closeness of the real and simulated data
in a statistical sense. The distribution of different
user action types, turn length and confidence score
can show constitutional similarity. It is still possible,
however, to be greatly different in their interdepen-
dence and cause quite different behavior at the dia-
log level even though there is a constitutional sim-
ilarity. Therefore, the dialog-level statistics such as
dialog completion rate and averaged dialog length
were also computed by running the user simulator
with the Let?s Go dialog system.
5 Results
As mentioned in Section 4.2, expected precision and
recall were measured. Whereas previous studies
only reported the scores computed in the predicate
level, i.e. speech act and concept, we also measured
the scores based on the output of the error template
model which is the predicate-level action with an
indicator of the correctness of the associated value
(Figure 1). The result (Table 3) shows a moderate
Training data Test data
Error Mark w/o w/ w/o w/
EP 58.13 45.12 54.44 41.86
ER 58.40 45.33 54.61 41.99
F-Score 58.27 45.22 54.52 41.93
Table 3: Expected precision, expected recall and F-Score
balance between agreement and variation which is
a very desirable characteristic of a user simulator
since a simulated user is expected not only to resem-
ble real data but also to cover diverse unseen behav-
ior to a reasonable extent. As a natural consequence
of the increased degree of freedom, the scores con-
sidering error marking are consistently lower. In ad-
dition, the results of test data are slightly lower than
those of training data, as expected, yet a suitable bal-
ance remains.
Next, the comparative distributions of different
actions between real and simulated data are pre-
sented for both training and test data (Figure 3).
The results are also based on the output of the er-
ror template model to further show how errors are
distributed over different actions. The distributions
of simulated data either from training or test data
show a close match to the corresponding real dis-
tributions. Interestingly, even though the error ratio
of the test data is noticeably different from that of
the training data, the proposed method is still able
to generate similar results. This means the vari-
ables and their conditional probabilities of the pro-
posed method were designed and estimated properly
enough to capture the tendency of user behavior with
respect to various dialog contexts. Moreover, the
comparison of the turn length distribution (Figure 4)
indicates that the simulated data successfully repli-
cated the real data for both training and test data.
The results of confidence score simulation are pre-
sented in Figure 55. For both training and test data,
the simulated confidence score displays forms that
are very similar to the real ones.
Finally, to confirm the resemblance on the dialog
level, the comparative results of dialog completion
rate and averaged dialog length are summarized in
Table 4. As shown in the dialog completion result,
the simulated user is a little harder than the real user
5Due to the space limitation, the detailed illustrations for
each action type are put in Appendix A.
56
Figure 3: A comparison of the distribution of different
actions between real and simulated data for both training
and test data
Figure 4: A comparison of the distribution of turn length
between real and simulated data for both training and test
data
to accomplish the purpose. Also, the variation of the
simulated data as far as turn length is concerned was
greater than that of the real data, although the aver-
aged lengths were similar to each other. This might
indicate the need to improve the termination model.
The proposed method for the termination model is
confined to incorporating only semantic-level fea-
tures but a variety of different features would, of
course, cause the end of a dialog, e.g. system de-
lay, acoustic features, spatial and temporal context,
weather and user groups.
6 Conclusion
In this paper, we presented a novel unsupervised ap-
proach for user simulation which is especially de-
sirable for real deployed systems. The proposed
Figure 5: A comparison of the distribution of confidence
score between real and simulated data for both training
and test data
Real Simulated
DCR (%) 59.68 55.04
ADL mean std. mean std.
Success 10.62 4.59 11.08 5.10
Fail 7.75 6.20 7.75 8.64
Total 9.46 5.48 9.50 7.12
Table 4: A comparison of dialog completion rate (DCR)
and averaged dialog length (ADL) which is presented ac-
cording to the dialog result.
method can cover the whole pipeline of user sim-
ulation on the semantic level without human inter-
vention. Also the quality of simulated data has been
demonstrated to be similar to the real data over a
number of commonly employed metrics. Although
the proposed method does not deal with simulat-
ing N-best ASR results, the extension to support
N-best results will be one of our future efforts, as
soon as the Let?s Go system uses N-best results.
Our future work also includes evaluation on improv-
ing and evaluating dialog strategies. Furthermore, it
would be scientifically more interesting to compare
the proposed method with a supervised approach us-
ing a corpus with semantic transcriptions. On the
other hand, as an interesting application, the pro-
posed user model could be exploited as a part of be-
lief tracking in a spoken dialog system since it also
considers a user action to be hidden.
57
Acknowledgments
We would like to thank Alan Black for helpful com-
ments and discussion. This work was supported by
the second Brain Korea 21 project.
References
C. Bishop, 2006. Pattern Recognition and Machine
Learning. Springer.
G. Chung, 2004. Developing a Flexible Spoken Dialog
System Using Simulation. In Proceedings of ACL.
H. Cuayahuitl, S. Renals, O. Lemon, H. Shimodaira,
2005. Humancomputer dialogue simulation using hid-
den Markov models. In Proceedings of ASRU.
W. Eckert, E. Levin, R. Pieraccini, 1997. User modeling
for spoken dialogue system evaluation. In Proceed-
ings of ASRU.
K. Georgila, J. Henderson, O. Lemon, 2006. User simu-
lation for spoken dialogue systems: Learning and eval-
uation. In Proceedings of Interspeech.
J. Henderson, O. Lemon, K. Georgila, 2008. Hybrid Re-
inforcement / Supervised Learning of Dialogue Poli-
cies from Fixed Datasets. Computational Linguistics,
34(4):487-511
S. Jung, C. Lee, K. Kim, M. Jeong, G. Lee, 2009.
Data-driven user simulation for automated evaluation
of spoken dialog systems. Computer Speech and Lan-
guage, 23(4):479?509.
S. Lauritzen and D. J. Spiegelhalter, 1988. Local Com-
putation and Probabilities on Graphical Structures and
their Applications to Expert Systems. Journal of
Royal Statistical Society, 50(2):157?224.
E. Levin, R. Pieraccini, W. Eckert, 2000. A stochastic
model of humanmachine interaction for learning di-
alogstrategies. IEEE Transactions on Speech and Au-
dio Processing, 8(1):11-23.
R. Lopez-Cozar, Z. Callejas, and M. McTear, 2006. Test-
ing the performance of spoken dialogue systems by
means of an articially simulated user. Articial Intel-
ligence Review, 26(4):291-323.
G. Parisi, 1988. Statistical Field Theory. Addison-
Wesley.
O. Pietquin, 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Faculty
of Engineering.
O. Pietquin and H. Hastie, 2011. A survey on metrics
for the evaluation of user simulations. The Knowledge
Engineering Review.
A. Raux, B. Langner, D. Bohus, A. W Black, and M.
Eskenazi, 2005. Let?s Go Public! Taking a Spoken
Dialog System to the Real World. In Proceedings of
Interspeech.
J. Schatzmann, K. Georgila, S. Young, 2005. Quantita-
tive evaluation of user simulation techniques for spo-
ken dialogue systems. In Proceedings of SIGdial.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, S.
Young, 2007. Agenda-based user simulation for boot-
strapping a POMDP dialogue system. In Proceedings
of HLT/NAACL.
J. Schatzmann, B. Thomson, S. Young, 2007. Error
simulation for training statistical dialogue systems. In
Proceedings of ASRU.
U. Syed and J. Williams, 2008. Using automatically
transcribed dialogs to learn user models in a spoken
dialog system. In Proceedings of ACL.
B. Thomson and S. Young, 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech & Language,
24(4):562-588.
J. Williams, P. Poupart, and S. Young, 2005. Factored
Partially Observable Markov Decision Processes for
Dialogue Management. In Proceedings of Knowledge
and Reasoning in Practical Dialogue Systems.
J. Williams, 2007. A Method for Evaluating and Com-
paring User Simulations: The Cramer-von Mises Di-
vergence. In Proceedings of ASRU.
J. Williams and S. Young, 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393-422.
58
Appendices
Appendix A. Distribution of confidence score for each concept
Figure 6: A comparison of the distribution of confidence score between real and simulated data for the training data
Figure 7: A comparison of the distribution of confidence score between real and simulated data for the test data
59
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 189?196,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Exploiting Machine-Transcribed Dialog Corpus to Improve Multiple Dialog
States Tracking Methods
Sungjin Lee1,2 and Maxine Eskenazi1
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania
2Computer Science and Engineering, Pohang University of Science and Technology, South Korea
{sungjin.lee, max}@cs.cmu.edu1, junion@postech.ac.kr2
Abstract
This paper proposes the use of unsuper-
vised approaches to improve components of
partition-based belief tracking systems. The
proposed method adopts a dynamic Bayesian
network to learn the user action model directly
from a machine-transcribed dialog corpus. It
also addresses confidence score calibration to
improve the observation model in a unsuper-
vised manner using dialog-level grounding in-
formation. To verify the effectiveness of the
proposed method, we applied it to the Let?s Go
domain (Raux et al, 2005). Overall system
performance for several comparative models
were measured. The results show that the pro-
posed method can learn an effective user ac-
tion model without human intervention. In
addition, the calibrated confidence score was
verified by demonstrating the positive influ-
ence on the user action model learning process
and on overall system performance.
1 Introduction
With present Automatic Speech Recognition (ASR)
and Spoken Language Understanding (SLU) errors,
it is impossible to directly observe the true user goal
and action. It is crucial, therefore, to efficiently infer
this true state from erroneous observations over mul-
tiple dialog turns. The Partially Observable Markov
Decision Process (POMDP) framework has offered
a well-founded theory for this purpose (Henderson
et al, 2008; Thomson and Young, 2010a; Williams
and Young, 2007; Young et al, 2010). Several
approximate methods have also emerged to tackle
the vast complexity of representing and maintaining
belief states, e.g., partition-based approaches (Ga-
sic and Young, 2011; Williams, 2010; Young et
al., 2010) and Bayesian network (BN)-based meth-
ods (Raux and Ma, 2011; Thomson and Young,
2010a). The partition-based approaches attempt to
group user goals into a small number of partitions
and split a partition only when a distinction is re-
quired by observations. This property endows it
with the high scalability that is suitable for fairly
complex domains. However, the parameter learn-
ing procedures for the partition-based methods is
still limited to hand-crafting or the use of a sim-
ple maximum likelihood estimation (Keizer et al,
2008; Roy et al, 2000; Thomson and Young, 2010a;
Williams, 2008). In contrast, several unsupervised
methods which do not require human transcription
and annotation have been recently proposed to learn
BN-based models (Jurcicek et al, 2010; Syed and
Williams, 2008; Thomson et al, 2010b). In this pa-
per we describe an unsupervised process that can be
applied to the partition-based methods. We adopt a
dynamic Bayesian network to learn the user action
model which defines the likelihood of user actions
for a given context. In addition, we propose a simple
confidence score calibration method to improve the
observation model which represents the probability
of an observation given the true user action.
This paper is structured as follows. Section 2 de-
scribes previous research and the novelty of our ap-
proach. Section 3 and Section 4 elaborate on our
proposed unsupervised approach. Section 5 explains
the experimental setup. Section 6 presents and dis-
cusses the results. Finally, Section 7 concludes with
a brief summary and suggestions for future research.
189
2 Background and Related Work
In order to reduce the complexity of the belief states
over the POMDP states, the following factorization
of the belief state has been commonly applied to the
belief update procedure (Williams et al, 2005):
b(gt,ut,ht)
? p(ot|ut)? ?? ?
observation model
?
ht?1
p(ht|ht?1,ut, st)? ?? ?
dialog history model
p(ut|gt, st,ht?1)? ?? ?
user action model
?
gt?1
p(gt|gt?1, st?1)? ?? ?
user goal model?
ut?1
b(gt?1,ut?1,ht?1)
(1)
where gt, st,ut,ht,ot represents the user goal, the
system action, the user action, the dialog history,
and the observed user action for each time slice, re-
spectively. The user goal model describes how the
user goal evolves. In the partition-based approaches,
this model is further approximated by assuming that
the user does not change their mind during the dia-
log (Young et al, 2010):
?
gt?1
p(gt|gt?1, st?1) = p(pt|pt?1) (2)
where pt is a partition from the current turn. The di-
alog history model indicates how the dialog history
changes and can be set deterministically by simple
discourse rules, for example:
p(ht = Informed|ht?1,ut, st) ={
1 if ht?1 = Informed or ut = Inform(?),
0 otherwise.
(3)
The user action model defines how likely user ac-
tions are. By employing partitions, this can be ap-
proximated by the bigram model of system and user
action at the predicate level, and the matching func-
tion (Keizer et al, 2008):
p(ut|gt, st,ht?1)
? p(T (ut)|T (st)) ? M(ut,pt, st)
(4)
where T (?) denotes the predicate of the action
and M(?) indicates whether or not the user action
matches the partition and system action. However,
it turned out that the bigram user action model did
not provide an additional gain over the improve-
ment achieved by the matching function according
to (Keizer et al, 2008). This might indicate that
it is necessary to incorporate more historical infor-
mation. To make use of historical information in
an unsupervised manner, the Expectation Maximiza-
tion algorithm was adopted to obtain maximum like-
lihood estimates (Syed and Williams, 2008). But
these methods still require a small amount of tran-
scribed data to learn the observation confusability,
and they suffer from overfitting as a general prop-
erty of maximum likelihood. To address this prob-
lem, we propose a Bayesian learning method, which
requires no transcribed data.
The observation model represents the probability
of an observation given the true user action. The
observation model is usually approximated with the
confidence score computed from the ASR and SLU
results:
p(ot|ut) ? p(ut|ot) (5)
It is therefore of vital importance that we obtain the
most accurate confidence score as possible. We pro-
pose an efficient method that can improve the confi-
dence score by calibrating it using grounding infor-
mation.
3 User Action Model
To learn the user action model, a dynamic Bayesian
network is adopted with several conditional inde-
pendence assumptions similar to Equation 1. This
gives rise to the graphical structure shown in Fig-
ure 1. As mentioned in Section 2, the user ac-
tion model deals with actions at the predicate level1.
This abstract-level handling enables the user action
model to employ exact inference algorithms such as
the junction tree algorithm (Lauritzen and Spiegel-
halter, 1988) for more efficient reasoning over the
graphical structure.
1To keep the notation uncluttered, we will omit T (?).
190
Figure 1: The graphical structure of the dynamic
Bayesian network for the user action model. The shaded
items are observable and the transparent ones are latent.
The joint distribution for this model is given by
p(S,H,U,O|?)
= p(h0|pi)
?
t
p(ut|st,ht?1,?)
? p(ht|ht?1,ut,?)p(ot|ut, ?)
(6)
where a capital letter stands for the set of
corresponding random variables, e.g., U =
{u1, . . . ,uN}, and ? = {pi,?,?, ?} denotes the
set of parameters governing the model2.
Unlike previous research which learns ? using
maximum likelihood estimation, we use a determin-
istic function that yields a fraction of an observed
confidence score in accordance with the degree of
agreement between ut and ot:
p(ot|ut) = CS(ot) ?
( |ot ? ut|
|ot ? ut|
)
+  (7)
where CS(?) returns the confidence score of the as-
sociated observation. As mentioned above, pi and
? are deterministically set by simple discourse rules
(Equation 3). This only leaves the user action model
? to be learned. In a Bayesian model, any unknown
parameter is given a prior distribution and is ab-
sorbed into the set of latent variables, thus it is not
feasible to directly evaluate the posterior distribution
of the latent variables and the expectations with re-
spect to this distribution. Therefore a determinis-
tic approximation, called mean field theory (Parisi,
1988), is applied.
In mean field theory, the family of posterior distri-
butions of the latent variables is assumed to be par-
titioned into disjoint groups:
q(Z) =
M?
i=1
qi(Zi) (8)
2Here, a uniform prior distribution is assigned on S
where Z = {z1, . . . , zN} denotes all latent variables
including parameters and Zi is a disjoint group.
Amongst all distributions q(Z) having the form of
Equation 8, we then seek the member of this family
for which the divergence from the true posterior dis-
tribution is minimized. To achieve this, the follow-
ing optimization with respect to each of the qi(Zi)
factors is to be performed in turn (Bishop, 2006):
ln q?j (Zj) = Ei 6=j
[
ln(X,Z)
]
+ const (9)
where X = {x1, . . . ,xN} denotes all observed vari-
ables and Ei 6=j means an expectation with respect to
the q distributions over all groups Zi for i 6= j.
Now we apply the mean field theory to the user
model. Before doing so, we need to introduce the
prior over the parameter ? which is a product of
Dirichlet distributions3.
p(?) =
?
k
Dir(?k|?0k)
=
?
k
C(?0k)
?
l
??
0
k?1
k,l
(10)
where k represents the joint configuration of all of
the parents and C(?0k) is the normalization constant
for the Dirichlet distribution. Note that for symme-
try we have chosen the same parameter ?0k for each
of the components.
Next we approximate the posterior distribution,
q(H,U,?) using a factorized form, q(H,U)q(?).
Then we first apply Equation 9 to find an expression
for the optimal factor q?(?):
3Note that priors over parameters for deterministic distribu-
tions (e.i., pi,?,and ?) are not necessary.
191
ln q?(?) = EH,U
[
ln p(S,H,U,O,?)
]
+ const
= EH,U
[?
t
ln p(ut|st,ht?1,?)
]
+ ln p(?) + const
=
?
t
?
i,j,k
(
EH,U
[
?i,j,k
]
ln?i,j,k
)
+
?
i,j,k
(?oi,j,k ? 1) ln?i,j,k + const
=
?
i,j,k
((
EH,U[ni,j,k] + (?oi,j,k ? 1)
)
? ln?i,j,k
)
+ const
(11)
where ?(?, ?) denotes Kronecker delta and ?i,j,k de-
notes ?(st, i)?(ht?1, j) ?(ut, k). ni,j,k is the num-
ber of times where , st = i,ht?1 = j, and ut = k.
This leads to a product of Dirichlet distributions by
taking the exponential of both sides of the equation:
q?(?) =
?
i,j
Dir(?i,j |?i,j),
?i,j,k = ?0i,j,k + EH,U[ni,j,k]
(12)
To evaluate the quantity EH,U[ni,j,k], Equation 9
needs to be applied once again to obtain an op-
timal approximation of the posterior distribution
q?(H,U).
ln q?(H,U) = E?
[
ln p(S,H,U,O,?)
]
+ const
= E?
[?
t
ln p(ut|st,ht?1,?)
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
]
+ const
=
?
t
(
E?
[
ln p(ut|st,ht?1,?)
]
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
)
+ const
(13)
where E?
[
ln p(ut|st,ht?1,?)
] can be obtained us-
ing Equation 12 and properties of the Dirichlet dis-
tribution:
E?
[
ln p(ut|st,ht?1,?)
]
=
?
i,j,k
?i,j,kE?
[
ln?i,j,k
]
=
?
i,j,k
?i,j,k(?(?i,j,k)? ?(??i,j))
(14)
where ?(?) is the digamma function with ??i,j =?
k ?i,j,k. Because computing EH,U[ni,j,k] is
equivalent to summing each of the marginal poste-
rior probabilities q?(ht?1,ut) with the same con-
figuration of conditioning variables, this can be
done efficiently by using the junction tree algorithm.
Note that the expression on the right-hand side for
both q?(?) and q?(H,U) depends on expectations
computed with respect to the other factors. We
will therefore seek a consistent solution by cycling
through the factors and replacing each in turn with a
revised estimate.
4 Confidence Score Calibration
As shown in Section 2, we can obtain a better obser-
vation model by improving confidence score accu-
racy. Since the confidence score is usually computed
using the ASR and SLU results, it can be enhanced
by adding dialog-level information. Basically, the
confidence score represents how likely it is that the
recognized input is correct. This means that a well-
calibrated confidence score should satisfy that prop-
erty such that:
p(ut = a|ot = a) '
?
k ?(uk, a)?(ok, a)?
k ?(ok, a)
(15)
However, the empirical distribution on the right side
of this equation often does not well match the con-
fidence score measure on the left side. If a large
corpus with highly accurate annotation was used, a
straightforward remedy for this problem would be to
construct a mapping function from the given confi-
dence score measure to the empirical distribution.
This leads us to propose an unsupervised method
that estimates the empirical distribution and con-
structs the mapping function which is fast enough
to run in real time. Note that we will not construct
192
Figure 2: Illustrations of confidence score calibration for the representative concepts in the Let?s Go domain
a mapping function for each instance, but rather
for each concept, since the former could cause se-
vere data sparseness. In order to estimate the em-
pirical distribution in an unsupervised manner, we
exploit grounding information4 as true labels. We
first parse dialog logs to look for the grounding in-
formation that the users have provided. Each time
we encounter grounding information that includes
the constraints used in the backend queries, this is
added to the list. If two actions contradict each other,
the later action overwrites the earlier one. Then,
for each observation in the data, we determine its
correctness by comparing it with the grounding in-
formation. Next, we gather two sets of confidence
scores with respect to correctness, on which we ap-
ply a Gaussian kernel-based density estimation. Af-
4Specifically, we used explicitly confirmed information by
the system for this study
ter that, we scale the two estimated densities by their
total number of elements to see how the ratio of cor-
rect ones over the sum of correct and incorrect ones
varies according to the confidence score. The ratio
computed above will be the calibrated score:
c? = dc(c)dc(c) + dinc(c)
(16)
where c? indicates the calibrated confidence score
and c is the input confidence score. dc(?) denotes
the scaled density for the correct set and dinc(?) is
the scaled density for the incorrect set.
Note that this approach tends to yield a more
conservative confidence score since correct user ac-
tions can exist, even though they may not match
the grounding information. Finally, in order to effi-
ciently obtain the calibrated score for a given confi-
dence score, we employ the sparse Bayesian regres-
sion (Tipping, 2001) with the Gaussian kernel. By
193
virtue of the sparse representation, we only need to
consider a few so-called relevance vectors to com-
pute the score:
y(x) =
?
xn?RV
wnk(x,xn) + b (17)
where RV denotes the set of relevance vectors,
|RV |  |{xn}|. k(?, ?) represents a kernel function
and b is a bias parameter. Figure 2 shows the afore-
mentioned process for several representative con-
cepts in the Let?s Go domain.
5 Experimental Setup
To verify the proposed method, three months of data
from the Let?s Go domain were used to train the
user action model and the observation model. The
training data consists of 2,718 dialogs and 23,044
turns in total. To evaluate the user action model,
we compared overall system performance with three
different configurations: 1) the uniform distribution,
2) the user action model without historical infor-
mation5 which is comparable to the bigram model
of (Keizer et al, 2008), 3) the user action model with
historical information included. For system perfor-
mance evaluation, we used a user simulator (Lee and
Eskenazi, 2012) which provides a large number of
dialogs with statistically similar conditions. Also,
the simulated user enables us to examine how per-
formance changes over a variety of error levels. This
simulated user supports four error levels and each
model was evaluated by generating 2,000 dialogs at
each error level. System performance was measured
in terms of average dialog success rate. A dialog is
considered to be successful if the system provides
the bus schedule information that satisfies the user
goal.
To measure the effectiveness of the calibration
method, we conducted two experiments. First, we
applied the calibration method to parameter learn-
ing for the user action model by using the calibrated
confidence score in Equation 7. We compared the
log-likelihood of two models, one with calibration
and the other without calibration. Second, we com-
pared overall system performance with four differ-
ent settings: 1) the user action model with histori-
5This model was constructed by marginalizing out the his-
torical variables.
cal information and the observation model with cal-
ibration, 2) the user action model with historical in-
formation and the observation model without cali-
bration, 3) the user action model without historical
information and the observation model with calibra-
tion, 4) the user action model without historical in-
formation and the observation model without cali-
bration.
6 Results
The effect of parameter learning of the user action
model on average dialog success rate is shown in
Figure 3. While, in the previous study, the bigram
model unexpectedly did not show a significant ef-
fect, our result here indicates that our comparable
model, i.e. the model with historical information ex-
cluded, significantly outperformed the baseline uni-
form model. The difference could be attributed to
the fact that the previous study did not take tran-
scription errors into consideration, whereas our ap-
proach handles the problem by treating the true user
action as hidden. However, we cannot directly com-
pare this result with the previous study since the tar-
get domains are different. The model with historical
information included also consistently surpassed the
uniform model. Interestingly, there is a noticeable
trend: the model without historical information per-
forms better as the error level increases. This result
may indicate that the simpler model is more robust
Figure 3: The effect of parameter learning of each user
action model on overall system performance. The error
bar represents standard error.
194
Figure 4: The effect of confidence score calibration on
the log-likelihood of the user action model during the
training process.
Figure 5: The effect of confidence score calibration for
the observation model on overall system performance.
The error bar shows standard error.
to error. Although average dialog success rates be-
came almost zero at error level four, this result is a
natural consequence of the fact that the majority of
the dialogs in this corpus are failed dialogs.
Figure 4 shows the effect of confidence score
calibration on the log-likelihood of the user action
model during the training process. To take into ac-
count the fact that different confidence scores result
in different log-likelihoods regardless of the qual-
ity of the confidence score, we shifted both log-
likelihoods to zero at the beginning. This modifica-
tion more clearly shows how the quality of the confi-
dence score influences the log-likelihood maximiza-
tion process. The result shows that the calibrated
confidence score gives greater log-likelihood gains,
which implies that the user action model can better
describe the distribution of the data.
The effect of confidence score calibration for the
observation model on average dialog success rate is
presented in Figure 5. For both the user action model
with historical information included and excluded,
the application of the confidence score calibration
consistently improved overall system performance.
This result implies the possibility of automatically
improving confidence scores in a modularized man-
ner without introducing a dependence on the under-
lying methods of ASR and SLU.
7 Conclusion
In this paper, we have presented novel unsupervised
approaches for learning the user action model and
improving the observation model that constitute the
partition-based belief tracking method. Our pro-
posed method can learn a user action model directly
from a machine-transcribed spoken dialog corpus.
The enhanced system performance shows the effec-
tiveness of the learned model in spite of the lack of
human intervention. Also, we have addressed con-
fidence score calibration in a unsupervised fashion
using dialog-level grounding information. The pro-
posed method was verified by showing the positive
influence on the user action model learning process
and the overall system performance evaluation. This
method may take us a step closer to being able to
automatically update our models while the system is
live. Although the proposed method does not deal
with N-best ASR results, the extension to support
N-best results will be one of our future directions,
as soon as the Let?s Go system uses N-best ASR re-
sults.
Acknowledgments
This work was supported by the second Brain Korea
21 project.
References
C. Bishop, 2006. Pattern Recognition and Machine
Learning. Springer.
195
M. Gasic and S. Young, 2011. Effective handling
of dialogue state in the hidden information state
POMDP-based dialogue manager. ACM Transactions
on Speech and Language Processing, 7(3).
J. Henderson, O. Lemon, K. Georgila, 2008. Hybrid Re-
inforcement / Supervised Learning of Dialogue Poli-
cies from Fixed Datasets. Computational Linguistics,
34(4):487-511.
F. Jurcicek, B. Thomson and S. Young, 2011. Natu-
ral Actor and Belief Critic: Reinforcement algorithm
for learning parameters of dialogue systems modelled
as POMDPs. ACM Transactions on Speech and Lan-
guage Processing, 7(3).
S. Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu, S.
Young, 2008. Modelling User Behaviour in the HIS-
POMDP Dialogue Manager. In Proceedings of SLT.
S. Lauritzen and D. J. Spiegelhalter, 1988. Local Com-
putation and Probabilities on Graphical Structures and
their Applications to Expert Systems. Journal of
Royal Statistical Society, 50(2):157?224.
S. Lee and M. Eskenazi, 2012. An Unsuper-
vised Approach to User Simulation: toward Self-
Improving Dialog Systems. In Proceedings of SIG-
DIAL. http://infinitive.lti.cs.cmu.edu:9090.
G. Parisi, 1988. Statistical Field Theory. Addison-
Wesley.
A. Raux, B. Langner, D. Bohus, A. W Black, and M.
Eskenazi, 2005. Let?s Go Public! Taking a Spoken
Dialog System to the Real World. In Proceedings of
Interspeech.
A. Raux and Y. Ma, 2011. Efficient Probabilistic Track-
ing of User Goal and Dialog History for Spoken Dia-
log Systems. In Proceedings of Interspeech.
N. Roy, J. Pineau, and S. Thrun, 2000. Spoken dia-
logue management using probabilistic reasoning. In
Proceedings of ACL.
U. Syed and J. Williams, 2008. Using automatically
transcribed dialogs to learn user models in a spoken
dialog system. In Proceedings of ACL.
B. Thomson and S. Young, 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech & Language,
24(4):562-588.
B. Thomson, F. Jurccek, M. Gasic, S. Keizer, F. Mairesse,
K. Yu, S. Young, 2010. Parameter learning for
POMDP spoken dialogue models. In Proceedings of
SLT.
M. Tipping, 2001. Sparse Bayesian Learning and
the Relevance Vector Machine. Journal of Machine
Learning Research, 1:211?244.
J. Williams, P. Poupart, and S. Young, 2005. Factored
Partially Observable Markov Decision Processes for
Dialogue Management. In Proceedings of Knowledge
and Reasoning in Practical Dialogue Systems.
J. Williams and S. Young, 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393-422.
J. Williams, 2008. Exploiting the ASR N-best by track-
ing multiple dialog state hypotheses. In Proceedings
of Interspeech.
J. Williams, 2010. Incremental partition recombination
for efficient tracking of multiple dialog states. In Pro-
ceedings of ICASSP.
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson and K. Yu, 2010. The Hidden
Information State Model: a practical framework for
POMDP-based spoken dialogue management. Com-
puter Speech and Language, 24(2):150?174.
196
Proceedings of the SIGDIAL 2013 Conference, pages 414?422,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Recipe For Building Robust Spoken Dialog State Trackers:                         
Dialog State Tracking Challenge System Description 
 
 
Sungjin Lee 
Language Technologies Institute,  
Carnegie Mellon University,        
Pittsburgh, Pennsylvania, USA 
sungjin.lee@cs.cmu.edu  
Maxine Eskenazi 
Language Technologies Institute,   
Carnegie Mellon University,        
Pittsburgh, Pennsylvania, USA 
max@cs.cmu.edu 
 
  
 
Abstract 
For robust spoken conversational interaction, 
many dialog state tracking algorithms have 
been developed. Few studies, however, have 
reported the strengths and weaknesses of each 
method. The Dialog State Tracking Challenge 
(DSTC) is designed to address this issue by 
comparing various methods on the same 
domain. In this paper, we present a set of 
techniques that build a robust dialog state 
tracker with high performance: wide-coverage 
and well-calibrated data selection, feature-rich 
discriminative model design, generalization 
improvement techniques and unsupervised 
prior adaptation. The DSTC results show that 
the proposed method is superior to other 
systems on average on both the development 
and test datasets.  
1 Introduction 
Even though we have recently seen an explosive 
growth of interest in speech-enabled applications, 
there are still many problems to overcome in 
order to provide users with practical and 
profitable services. One of the long-standing 
problems which may often frustrate users is 
Automatic Speech Recognition (ASR) error. Due 
to ASR error, it is barely possible to directly 
observe what the user said and finally figure out 
the true user goal. The aim of dialog state 
tracking is, therefore, to accurately estimate the 
true dialog state from erroneous observations as 
a dialog unfolds. 
In order to achieve this goal, many dialog state 
tracking algorithms have been developed. Few 
studies, however, have reported the strengths and 
weaknesses of each method. The Dialog State 
Tracking Challenge1  (DSTC) was organized to 
advance state-of-the-art technologies for dialog 
state tracking by allowing for reliable 
comparisons between different approaches using 
the same datasets. Unlike other machine 
learning-based empirical tasks, DSTC is also 
carefully designed to take into consideration 
diverse realistic mismatches. For instance, there 
are test datasets that were collected by systems 
using different speech recognizers, spoken 
language understanding (SLU) modules, and 
dialog managers. Also there are test datasets that 
were produced by similar systems but deployed 
at a different time (1 year later) with extended 
coverage. Since such mismatches between 
training and test data may often happen in real 
deployment, it is important to build a tracker 
which constantly shows high performance across 
all test datasets despite various mismatches. 
The aim of this paper is to describe a set of 
techniques used to build a robust tracker with 
high performance: wide-coverage and well-
calibrated data selection, feature-rich 
discriminative model design, generalization 
improvement techniques and unsupervised prior 
adaptation. Our challenge systems are basically 
various combinations of those techniques. The 
DSTC results demonstrate the effectiveness of 
each technique.  
This paper is structured as follows. Section 2 
describes the challenge setup. Section 3 
elaborates on our proposed approaches. Section 4 
briefly describes previous research and other 
systems that participated in DSTC. Section 5 
presents and discusses the results. Finally, 
Section 6 concludes with a brief summary and 
suggestions for future research.  
                                                 
1 http://research.microsoft.com/en-us/events/dstc/ 
414
2 Dialog State Tracking Challenge 
This section describes the task for DSTC and 
datasets provided for training and test. Most part 
of this section is borrowed from the DSTC 
manual2.  
2.1 Task Description 
DSTC data is taken from several different 
spoken dialog systems which all provided bus 
schedule information for Pittsburgh, 
Pennsylvania, USA as part of the Spoken Dialog 
Challenge (Black et al, 2011). There are 9 slots 
which are evaluated: route, from.desc, 
from.neighborhood, from.monument, to.desc, 
to.neighborhood, to.monument, date, and time. 
Since both marginal and joint representations of 
dialog states are important for deciding dialog 
actions, the challenge takes into consideration 
both. Each joint representation is an assignment 
of values to all slots.  Thus there are 9 marginal 
outputs and 1 joint output in total, which are all 
evaluated separately. 
The dialog tracker receives SLU N-best 
hypotheses for each user turn, each with a 
confidence score. In general, there are a large 
number of values for each slot, and the coverage 
of N-best hypotheses is good, thus the challenge 
confines consideration of goals to slots and 
values that have been observed in an SLU output. 
By exploiting this aspect, the task of a dialog 
state tracker is to generate a set of observed slot 
and value pairs, with a score between 0 and 1. 
The sum of all scores is restricted to sum to 1.0. 
Thus 1.0 ? total score is defined as the score of a 
special value None that indicates the user?s goal 
has not yet been appeared on any SLU output. 
2.2 Datasets 
The data is divided into 2 training sets and 4 test 
sets (Table 1). For standardized development sets, 
each training set is split in half. Participants were 
asked to report results on the second half of each 
set. The data from group A in train2, and test1 
was collected using essentially the same dialog 
system. Only a few updates were made to reflect 
changes to the bus schedule. The data in test2 
was collected using a different version of group 
A?s dialog manager. The data from group B in 
train3 and test3 were collected using essentially 
the same dialog system; the main difference is 
that test3 covers more bus routes. Test4 tests the 
condition when training and testing using totally 
                                                 
2 http://research.microsoft.com/apps/pubs/?id=169024 
different dialog systems, and when there is no 
same-system training data available. 
2.3 Metrics 
There are a variety of aspects of tracker 
performance that were measured: accuracy, mean 
reciprocal rank (MRR), ROC curves, Average 
score 3 , and Brier score 4 . There are three 
schedules for determining which turns to include 
in each evaluation. 
? Schedule 1: Include all turns. 
? Schedule 2: Include a turn for a given 
concept only if that concept either appears on 
the SLU N-Best list in that turn, or if the 
system?s action references that concept in 
that turn. 
? Schedule 3: Include only the turn before the 
system starts over from the beginning, and 
the last turn of the dialog. 
3 Recipe for Building a Robust Tracker  
In this section, we present several ingredients for 
building a robust state tracker that come into play 
at various levels of the development process: 
from data selection to model adaptation. 
3.1 Wide-Coverage and Well-Calibrated 
Data Selection 
The first step to create a robust dialog state 
tracker is the use of data which covers diverse 
system dialog actions and user inputs with well-
calibrated confidence scores. Since dialog 
policies can be varying according to how a 
dialog proceeds, it is crucial to arrange a training 
dialog corpus with well-balanced dialog actions. 
For example, group A datasets barely have 
implicit confirmation and heavily rely on explicit 
confirmation, while group B datasets have both 
types of confirmation. Thus a model trained on 
group A datasets cannot exploit implicit 
                                                 
3 the average score assigned to the correct item 
4 the L2 norm between the vector of scores output by 
dialog state tracker and a vector with 1 in the position 
of the correct item, and 0 elsewhere 
Dataset Source Calls Time period 
train2 Group A 678 Summer 2010 
train3 Group B 779 Summer 2010 
test1 Group A 765 Winter 2011-12 
test2 Group A 983 Winter 2011-12 
test3 Group B 1037 Winter 2011-12 
test4 Group C 451 Summer 2010 
 
Table 1: Dataset description. 
 
415
confirmation when applied to group B datasets, 
whereas a model trained on group B datasets can 
be applied to group A datasets without much 
loss.  
Another important aspect of the data is how 
well user inputs are calibrated. If the confidence 
score is well-calibrated, confirmation can be 
skipped in the case of a hypothesis with a high 
confidence.  On the contrary, if the quality of the 
confidence score is very poor, a successful dialog 
will only be possible via heavy use of 
confirmation. Thus a model trained on a well-
calibrated dataset is likely to perform well on the 
poorly-calibrated dataset because of backup 
confirmation. Whereas, a model trained on the 
poorly-calibrated dataset will not perform well 
on the well-calibrated dataset due to the 
mismatch of the confidence score as well as the 
scarceness of confirmation information. The 
group A datasets have been shown to be poorly 
calibrated (Lee and Eskenazi, 2012); this is also 
shown in Fig. 2. Group B datasets are relatively 
well-calibrated, however. 
The importance of wide coverage and well-
calibrated data can be observed by examining the 
results of entry1 and entry2 (Fig. 1) which are 
trained on group A and B datasets, respectively. 
3.2 Feature-Rich Discriminative Model Design 
Most previous approaches are based on 
generative temporal modeling where the current 
dialog state is estimated using a few features 
such as the current system action and N-best 
hypotheses with corresponding confidence scores 
given the estimated dialog state at the previous 
turn (Gasic and Young, 2011; Lee and Eskenazi, 
2012; Raux and Ma, 2011; Thomson and Young, 
2010; Williams, 2010; Young et al, 2010). 
However, several fundamental questions have 
been raised recently about the formulation of the 
dialog state update as a generative temporal 
model: limitation in modeling correlations 
between observations in different time slices; and 
the insensitive discrimination between true and 
false dialog states (Williams, 2012).  
 
Figure 2: Estimated empirical accuracy of confidence 
score for from slot. Ideally calibrated confidence score 
should be directly proportional to empirical accuracy. 
 
 
 
Figure 1: Diagram showing the relation between datasets and models. Each team could have up to five systems 
entered. Our challenge entries are tagged by their entry numbers. More detailed descriptions about each model 
are provided in Section 3. 
416
In fact, such limitations can be improved by 
adopting a discriminative approach, which 
enables the incorporation of a rich set of features 
without worrying about their interdependence 
(Sutton and McCallum, 2006). For example, a 
hypothesis that repeats with low confidence 
scores is likely to be a manifestation of ASR 
error correlations between observations in 
different time slices. Thus, the highest 
confidence score that a hypothesis has attained 
so far could be a useful feature in preventing 
repeated incorrect hypotheses from defeating the 
correct hypothesis (which had a higher score but 
was only seen once). Another useful feature 
could be the distribution of confidence scores 
that a hypothesis has attained thus far, since it 
may not have the same effect as having a single 
observation with the total score due to the 
potential nonlinearity of confidence scores. 
There are many other potentially useful features. 
The entire list of features used for the challenge 
system is found in Appendix A. 
In addition to the role of rich features in 
performance enhancement, the incorporation of 
rich features is also important for robust state 
tracking. If the tracker estimates the true state by 
considering various aspects of observations and 
prior knowledge, then the influence of 
differences in certain factors between datasets 
can be mitigated by many other factors that are 
retained relatively unchanged between datasets.  
For the challenge system, we employed a 
Maximum Entropy (MaxEnt) model which is one 
of most powerful undirected graphical models. 
Unlike previous work using MaxEnt (Bohus and 
Rudnicky, 2006) where the model is limited to 
maintain only the top K-best hypotheses, we 
amended MaxEnt to allow for the entire set of 
observed hypotheses to be incorporated; Several 
feature functions which differ only by output 
labels were aggregated into one common feature 
function so that they can share common 
parameters and gather their statistics together 
(Appendix A). This modification is also crucial 
for robust estimation of the model parameters 
since some slots such as from and to can have 
about 104 values but most of them are not seen in 
the training corpus. 
The effectiveness of feature-rich 
discriminative modeling can be observed by 
comparing the results of DMALL and PBM (Fig. 
1) which are discriminative and generative 
models, respectively. 
Note that interesting relational constraints, e.g. 
whether or not departure and arrival places are 
valid on a route, can be incorporated by adopting 
a structured model such as Conditional Random 
Field (CRF). But CRF was not used for the 
challenge since the bus information that was 
provided is not compatible with every dataset. 
The effectiveness of a structured model has been 
investigated in a separate publication (Lee, 2013). 
3.3 Generalization Improvement Techniques 
Even though the incorporation of a set of rich 
features helps overcome the weaknesses of 
previous approaches, it also implies a risk of 
overfitting training datasets due to its increased 
capacity of function class. Overfitting is a serious 
hazard especially for test datasets that are 
severely dissimilar to training datasets. As noted 
above, since the test datasets of the challenge are 
intentionally arranged to have various 
mismatches, it is crucial that we prevent a model 
from overfitting training datasets. In the rest of 
this section, we describe various ways of 
controlling the capacity of a model.  
 The most obvious method to control the 
capacity is to penalize larger weights 
proportional to the squared values of the weights 
or the absolute values of the weights. We employ 
the Orthant-wise Limited-memory Quasi Newton 
optimizer (Andrew and Gao, 2007) for L1 
regularization. The weights for L1 regularization 
were set to be 10 and 3 for the prior features and 
the other features, respectively. These values 
were chosen through cross-validation over 
several values rather than doing a thorough 
search. 
A second method, which is often convenient, 
is to start with small weights and then stop the 
learning before it has time to overfit provided 
that it finds the true regularities before it finds 
the spurious regularities that are related to 
specific training datasets. It could be hard, 
however, to decide when to stop. A typical 
technique is to keep learning until the 
performance on the validation set gets worse and 
then stop training and go back to the best point. 
For the challenge systems, we applied a simpler 
method that is to stop the training if the average 
objective function change over the course of 10 
previous iterations is less than 0.1, which is 
usually set to a much smaller number such as 10-4. 
In general, prediction errors can be 
decomposed into two main subcomponents, i.e., 
error due to bias and variance (Hastie et. al, 
2009). It is also known that there is a tradeoff 
between bias and variance. If a model is flexible 
enough to fit the given data, errors due to bias 
417
will decrease while errors due to variance will 
increase. The methods stated above try to 
achieve less error by decreasing errors due to 
variance. However we cannot avoid increasing 
errors due to bias in this way. Thus we need a 
method to alleviate the tradeoff between bias and 
variance.  
System combination is one powerful way to 
reduce variance without raising bias. If we 
average models that have different forms and 
make different mistakes, the average will do 
better than the individual models. This effect is 
largest when the models make very different 
predictions from one another. We could make the 
models different by simply employing different 
machine learning algorithms as well as by 
training them on different subsets of the training 
data.  
The challenge system, entry3, consists of three 
discriminative models and one generative model 
(Fig. 1). Entry1 and entry2 were trained on 
different training datasets to make them produce 
different predictions. DMCOND is a discriminative 
model trained on both train2 and train3. Also, 
DMCOND differs from other discriminative 
models in the way that it was trained: the 
parameters associated with the features which are 
computable without grounding action 
information (features (1), (5), (8), (9) and (10) in 
Appendix A) are trained first and then the other 
features are learned given the former parameters. 
The idea behind this training method is to 
encourage the model to put more weight on 
dialog policy invariant features. The final 
component PBM is the AT&T Statistical Dialog 
Toolkit 5  which is one of the state-of-the-art 
generative model-based systems. We modified it 
to process implicit confirmation and incorporate 
the prior distribution which was estimated on the 
training corpus. The prior distribution was 
smoothed by an approximate Good-Turing 
estimation on the fly when the system encounters 
an unseen value at run time. The improvement 
from system combination is verified by the 
results of entry3. 
3.4 Unsupervised Prior Adaptation 
While a prior is a highly effective type of 
information for dialog state tracking, it is also 
able to hamper the performance when incorrectly 
estimated. Thus it is worthwhile to investigate 
adapting the prior to the test datasets. Since a 
dialog state tracker is meant to estimate the 
                                                 
5 http://www2.research.att.com/sw/tools/asdt/ 
posterior probabilities over hypotheses, we can 
extract estimated labels from test datasets by 
setting an appropriate threshold, taking the 
hypotheses with a greater probability than the 
threshold as labels. By combining the predictive 
prior from test datasets and the prior from 
training datasets, we adapted entry2 and entry3 
in an unsupervised way to produce entry5 and 
entry4, respectively (Fig. 1). For each test dataset, 
we used different thresholds: 0.95 for test1, test2 
and test3, and 0.85 for test4. 
4 Related Work 
Since the Partially Observable Markov Decision 
Process (POMDP) framework has offered a 
well-founded theory for both state tracking and 
decision making, most earlier studies adopted 
generative temporal models, the typical way to 
formulate belief state updates for POMDP-based 
systems (Williams and Young, 2007). Several 
approximate methods have also emerged to 
tackle the vast complexity of representing and 
maintaining belief states, e.g., partition-based 
approaches (Gasic and Young, 2011; Lee and 
Eskenazi, 2012; Williams, 2010; Young et al, 
2010) and Bayesian network (BN)-based 
methods (Raux and Ma, 2011; Thomson and 
Young, 2010). A drawback of the previous 
generative models is that it is hard to incorporate 
a rich set of observation features, which are often 
partly dependent on one another. Moreover, the 
quality of the confidence score will be critical to 
all generative models proposed so far, since they 
do not usually try to handle potential nonlinearity 
in confidence scores.  
As far as discriminative models are concerned, 
the MaxEnt model has been applied (Bohus and 
Rudnicky, 2006). But the model is restricted to 
maintaining only the top K-best hypotheses, 
where K is a predefined parameter, resulting in 
potential degradation of performance and 
difficulties in extending it to structured models. 
Finally, there is a wide range of systems that 
participated in Dialog State Tracking Challenge 
2013: from rule-based systems to fairly complex 
statistical methods such as Deep Neural 
Networks. Since we have not only traditional 
generative models such as Dynamic Bayesian 
Network and partition-based approaches, but also 
newly-proposed discriminative approaches such 
as log-linear models, Support Vector Machines 
and Deep Neural Networks, the analysis of the 
challenge results is expected to reveal valuable 
lessons and future research directions. 
418
5 Results and Discussion  
The official results of the challenge are publicly 
available and our team is team6. As mentioned in 
Section 2.3, there are a variety of aspects of 
tracker performance that were measured on 
different schedules. Since prediction accuracy at 
the end of a dialog directly translates to the 
success of the entire task, we first show the 
average accuracy across all test datasets 
measured at schedule 3 in Fig. 3. The average 
accuracy at schedule 3 also well represents how 
robust a state tracker is since the test datasets are 
widely distributed in the dimensions of dialog 
policies, dialog length and the quality of user 
input and confidence score.  
First of all, we note that our 4 entries 
(entries2-5) took the top positions in both the All 
and Joint categories. Entry4, which showed the 
best performance, outperformed the best entry 
from other teams by 4.59% (entry2 of team9) 
and 10.1% (entry2 of team2). Specificially, the 
large improvement in Joint implies that our 
model performs evenly well for all slots and is 
more robust to the traits of each slot. 
Furthermore, from the results we can verify 
the effectiveness of each technique for achieving 
robustness. Given the large gap between the 
performance of entry1 and of entry2, it is clearly 
shown that a model trained on a wide-coverage 
and well-calibrated dialog corpus can be 
applicable to a broad range of test datasets 
without much loss. Even though entry2 was 
trained on only 344 dialogs (the first half of 
train3), it already surpasses most of competing 
models.  
The utility of a feature-rich discriminative 
model is demonstrated by the fact that DMALL 
greatly outperformed PBM. We also note that 
just using a discriminative model does not 
 
 
 
(a) All slot: a weighted average accuracy across all slots 
 
(b) Joint slot 
 
Figure 3: Accuracy measured at schedule 3 averaged over the test and development datasets. Models which do 
not appear in Fig. 1 are the best system of each team except for us. Rule denotes a rule-based system, Hybrid a 
hybrid system of discriminative and generative approaches, DiscTemp a discriminative temporal model, RForest 
a random forest model, DNN a deep neural network model, DiscJoint a discriminative model which deals with 
slots jointly, SVM a support vector machine model, and DBN a dynamic Bayesian network mode. 
419
guarantee improved performance since many 
discriminative systems that participated in the 
challenge underperformed some of the entries 
that were based on generative modeling or rules. 
This result implies that devising effective 
features is central to performance.  
In addition, this result also points to the 
necessity of controlling the capacity of a model. 
While our models constantly show good 
performance both on development sets and test 
sets, the performance of the other models 
significantly dropped off. In fact, this explains 
why Hybrid and Rule systems switch their 
positions in the Joint slot. Moreover, many other 
systems in the graph tail seem to be severely 
overfitted, resulting in poor performance on test 
datasets despite relatively good performance on 
development datasets. As expected, system 
combination gives rise to better accuracy without 
loss of robustness; entry3 clearly outperforms 
each of its components, i.e. entry1, entry2, 
DMCOND and PBM, on both development and test 
datasets. 
Finally, the improvement observed when 
using unsupervised prior adaptation is also 
shown to be positive but its effect size is not 
significant: entry5 vs. entry2 and entry4 vs. 
entry3. Given that the way in which we have 
adapted the model is fairly primitive, we believe 
that there is much room to refine the 
unsupervised adaptation method.  
MRR measures the average of 1/R, where R is 
the rank of the first correct hypothesis. MRR at 
schedule 3 measures the quality of the final 
ranking which may be most important to a multi-
modal interface that can display results to the 
user. Even though the results are not displayed 
due to space limitations, the results for MRR are 
very similar to those for accuracy. Our 4 entries 
(entries2-5) still take the top positions. 
The ROC curves assess the discrimination of 
the top hypothesis? score. The better 
discrimination at schedule 2 may be helpful for 
reducing unnecessary confirmations for values 
with sufficiently high belief. Also, the better 
discrimination at schedule 3 may enable a model 
to adapt to test data in an unsupervised manner 
by allowing us to set a proper threshold to 
produce predictive labels. The ROC curves of 
our systems again showed the highest levels of 
discrimination. 
6 Conclusion 
In this paper, we presented a set of techniques to 
build a robust dialog state tracker without losing 
performance: wide-coverage and well-calibrated 
data selection, feature-rich discriminative model 
design, generalization improvement techniques 
and unsupervised prior adaptation. The results in 
terms of various metrics show that the proposed 
method is truly useful for building a tracker 
prominently robust not only to mismatches 
between training and test datasets but also to the 
traits of different slots. Since we used relatively 
simple features for this work, there is much room 
to boost performance through feature 
engineering. Also, more thorough search for 
regularization weights can give additional 
performance gain. Moreover, one can extend the 
present discriminative model presented here to a 
structured version which can improve 
performance further by allowing  relational 
constraints to be incorporated (Lee, 2013). 
Finally, we believe that once a more detailed and 
thorough investigation of the challenge results 
has been carried out, we will be able to take the 
best of each system and combine them to 
generate a much better dialog state tracker.   
Acknowledgments 
This work was funded by NSF grant IIS0914927. 
The opinions expressed in this paper do not 
necessarily reflect those of NSF. 
References  
G. Andrew and J. Gao, 2007. Scalable training of L1-
regularized log-linear models. In Proceedings of 
ICML. 
A. Black et al, 2011. Spoken dialog challenge 2010: 
Comparison of live and control test results. In 
Proceedings of SIGDIAL. 
D. Bohus and A. Rudnicky, 2006. A K hypotheses + 
other belief updating model. In Proceedings of 
AAAI Workshop on Statistical and Empirical 
Approaches for Spoken Dialogue Systems. 
M. Gasic and S. Young, 2011. Effective handling of 
dialogue state in the hidden information state 
POMDP-based dialogue manager. ACM 
Transactions on Speech and Language Processing, 
7(3). 
T. Hastie, R. Tibshirani, and J. Friedman, 2009. The 
Elements of Statistical Learning: Data Mining, 
Inference, and Prediction (2nd edition). Springer. 
420
S. Lee and M. Eskenazi, 2012. Exploiting Machine-
Transcribed Dialog Corpus to Improve Multiple 
Dialog  States Tracking Methods. In Proceedings 
of SIGDIAL, 2012. 
S. Lee, 2013. Structured Discriminative Model For 
Dialog State Tracking. Submitted to SIGDIAL, 
2013. 
A. Raux, B. Langner, D. Bohus, A. W Black, and M. 
Eskenazi, 2005. Let?s Go Public! Taking a Spoken 
Dialog System to the Real World. In Proceedings 
of Interspeech. 
A. Raux and Y. Ma, 2011. Efficient Probabilistic 
Tracking of User Goal and Dialog History for 
Spoken Dialog Systems. In Proceedings of 
Interspeech. 
C. Sutton and A. McCallum, 2006. An Introduction to 
Conditional Random Fields for Relational 
Learning. Introduction to Statistical Relational 
Learning. Cambridge: MIT Press. 
B. Thomson and S. Young, 2010. Bayesian update of 
dialogue state: A POMDP framework for spoken 
dialogue systems. Computer Speech & Language, 
24(4):562-588. 
B. Thomson, F. Jurccek, M. Gasic, S. Keizer, F. 
Mairesse, K. Yu, S. Young, 2010a. Parameter 
learning for POMDP spoken dialogue models. In 
Proceedings of SLT.  
J. Williams and S. Young, 2007. Partially observable 
Markov decision processes for spoken dialog 
systems. Computer Speech & Language, 
21(2):393-422. 
J. Williams, 2010. Incremental partition 
recombination for efficient tracking of multiple 
dialog states. In Proceedings of ICASSP. 
J. Williams, 2011. An Empirical Evaluation of a 
Statistical Dialog System in Public Use, In 
Proceedings of SIGDIAL. 
J. Williams, 2012. A Critical Analysis of Two 
Statistical Spoken Dialog Systems in Public Use. 
In Proceedings of SLT. 
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson and K. Yu, 2010. The Hidden 
Information State Model: a practical framework for 
POMDP-based spoken dialogue management. 
Computer Speech and Language, 24(2):150?174. 
 
Appendix A. Feature Functions 
Feature functions are playing a central role to the 
performance of discriminative models. We 
describe the feature functions that we used for 
the challenge system in the following. To 
facilitate readers? understanding an example of 
feature extraction is illustrated in Fig. 4. 
One of the most fundamental features for 
dialog state tracking should exploit the 
confidence scores assigned to an informed 
hypothesis. The simplest form could be direct 
use of confidence scores. But often pre-trained 
confidence measures fail to match the empirical 
distribution of a given dialog domain (Lee and 
Eskenazi, 2012; Thomson et al 2010). Also the 
distribution of confidence scores that a 
hypothesis has attained so far may not have the 
same effect as the total score of the confidence 
scores (e.g., in Fig. 4, two observations for 61C 
with confidence score 0.3 vs. 0.6 which is the 
sum of the scores). Thus we create a feature 
function that divides the range of confidence 
scores into bins and returns the frequency of 
observations that fall into the corresponding bin: 
 
       (    
 )  
        {
                 (       (    
 ))
           
   
(1) 
 
where      ( )  returns the set of confidence 
scores whose action informs   in the sequence of 
observations   
 .         (   )  computes the 
frequency of observations that fall into the     
bin. 
There are two types of grounding actions 
which are popular in spoken dialog systems, i.e., 
implicit and explicit confirmation. To leverage 
affirmative or negative responses, the following 
feature functions are introduced in a similar 
fashion as the        feature function: 
 
       (    
 )  
       {
                 (       (    
 ))
           
   
(2) 
 
       (    
 )  
       {
                 (       (    
 ))
           
   
(3) 
 
where      ( )  /      ( )  returns the set of 
confidence scores whose associated action 
affirms / negates   in the sequence of 
observations   
 . 
 
           (    
 )  
                         {
                  (    
 )
           
   
(4) 
 
421
where          ( ) indicates whether or not the 
user has negated the system?s implicit 
confirmation in the sequence of observations   
 . 
One of interesting feature functions is the so-
called baseline feature which exploits the output 
of a baseline system. The following feature 
function emulates the output of the baseline 
system which always selects the top ASR 
hypothesis for the entire dialog: 
 
          (    
 )  
      {
            (           (    
 ))
           
   
(5) 
 
where          ( )  returns the maximum 
confidence score whose action informs   in the 
sequence of observations   
 .    (   )  indicates 
whether or not the maximum score falls into the 
    bin. 
Yet another feature function of this kind is the 
accumulated score which adds up all confidence 
scores associated with inform and affirm and 
subtracts the ones with negation: 
 
         (    
 )  
                  
{
 
 
 
                    (    
 )
                                (    
 )
                                (    
 )
                 
   
(6) 
 
Since we have a partition-based tracker, it is also 
possible to take advantage of its output: 
 
         (    
 )  
                          {
            (    
 ))
           
   
(7) 
 
where    ( )  returns the posterior probability 
of a hypothesis estimated by the partition-based 
tracker. Note that such feature functions as 
         ( ) ,         ( )  and    ( )  are not 
independent of the others defined previously, 
which may cause generative models to produce 
deficient probability distributions. 
It is known that prior information can boost 
the performance (Williams, 2012) if the prior is 
well-estimated. One of advantages of generative 
models is that they provide a natural mechanism 
to incorporate a prior. Discriminative models 
also can exploit a prior by introducing additional 
feature functions: 
 
      (    
 )  
            {
            (            ( ))
           
   
(8) 
 
where           ( ) returns the fraction of 
occurrences of   in the set of true labels. 
If the system cannot process a certain user 
request, it is highly likely that the user change 
his/her goal. The following feature function is 
designed to take care of such cases: 
 
        (    
 )  {
             ( )
           
   (9) 
 
where     ( ) indicates whether or not   is out-
of-coverage. 
As with other log-linear models, we also have 
feature functions for bias: 
 
    (    
 )    
        (    
 )   {
          
            
 
(10) 
 
Note that we have an additional bias term for 
None to estimate an appropriate weight for it. 
Here, None is a special value to indicate that the 
true hypothesis has not yet appeared in the ASR 
N-best lists. Since there are generally a large 
number of values for each concept, the 
probability of the true hypothesis will be very 
small unless the true hypothesis appears on the 
N-best lists. Thus we can make inferences on the 
model very quickly by focusing only on the 
observed hypotheses at the cost of little 
performance degradation. 
 
 
Figure 4: A simplified example of feature extraction for the route concept. It shows the values that each feature 
will have when three consecutive user inputs are given. 
 
422
Proceedings of the SIGDIAL 2013 Conference, pages 442?451,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Structured Discriminative Model For Dialog State Tracking 
 
 
Sungjin Lee 
Language Technologies Institute,  
Carnegie Mellon University,        
Pittsburgh, Pennsylvania, USA 
sungjin.lee@cs.cmu.edu  
 
  
 
Abstract 
Many dialog state tracking algorithms have 
been limited to generative modeling due to the 
influence of the Partially Observable Markov 
Decision Process framework. Recent analyses, 
however, raised fundamental questions on the 
effectiveness of the generative formulation. In 
this paper, we present a structured 
discriminative model for dialog state tracking 
as an alternative. Unlike generative models, 
the proposed method affords the incorporation 
of features without having to consider 
dependencies between observations. It also 
provides a flexible mechanism for imposing 
relational constraints. To verify the 
effectiveness of the proposed method, we 
applied it to the Let?s Go domain (Raux et al, 
2005). The results show that the proposed 
model is superior to the baseline and 
generative model-based systems in accuracy, 
discrimination, and robustness to mismatches 
between training and test datasets.  
 
1 Introduction 
With the recent remarkable growth of speech-
enabled applications, dialog state tracking has 
become a critical component not only for typical 
telephone-based spoken dialog systems but also 
for multi-modal dialog systems on mobile 
devices and in automobiles. With present 
Automatic Speech Recognition (ASR) and 
Spoken Language Understanding errors, it is 
impossible to directly observe the true user goal 
and action. It is crucial, therefore, to accurately 
estimate the true dialog state from erroneous 
observations as a dialog unfolds.  
Since the Partially Observable Markov 
Decision Process (POMDP) framework has 
offered a well-founded theory for both state 
tracking and decision making, most earlier 
studies adopted generative temporal models, the 
typical way to formulate belief state updates for 
POMDP-based systems (Williams and Young, 
2007). Several approximate methods have also 
emerged to tackle the vast complexity of 
representing and maintaining belief states, e.g., 
partition-based approaches (Gasic and Young, 
2011; Lee and Eskenazi, 2012a; Williams, 2010; 
Young et al, 2010) and Bayesian network (BN)-
based methods (Raux and Ma, 2011; Thomson 
and Young, 2010). 
To verify the effectiveness of these techniques, 
some were deployed in a real user system for the 
Spoken Dialog Challenge (Black et al, 2010). 
The results demonstrated that the use of 
statistical approaches helps estimate the true 
dialog state and achieves increased robustness to 
ASR errors (Thomson et al, 2010b; Lee and 
Eskenazi 2012b; Williams, 2011; Williams, 
2012). However, further analysis also raised 
several fundamental questions about the 
formulation of the belief update as a generative 
temporal model: limitation in modeling 
correlations between observations in different 
time slices; and the insensitive discrimination 
between true and false dialog states (Williams, 
2012). There are more potential downsides of 
generative models, which will be discussed in 
detail in Section 2. 
On the other hand, natural language 
processing, computer vision and other machine 
learning research areas have increasingly 
profited from discriminative approaches. 
Discriminative approaches directly model the 
class posteriors, allowing them to incorporate a 
rich set of features without worrying about their 
dependencies on one another. This could result in 
a deficient probability distribution with 
generative models (Sutton and McCallum, 2006). 
442
The aim of this paper is to describe a first 
attempt to adopt a structured discriminative 
model for dialog state tracking. To handle 
nonlinearity of confidence score and variable 
cardinality of the possible values of output 
variables, the traditional approaches applied to 
other tasks have been modified.  
To verify the effectiveness of the proposed 
method, we applied it to the Let?s Go1 domain 
(Raux et al, 2005). The proposed model was 
compared with its unstructured version without 
relational constraints, the baseline system which 
always takes the top ASR hypothesis in the entire 
dialog, and finally the AT&T Statistical Dialog 
Toolkit2 (ASDT) which is one of the state-of-the-
art generative model-based systems. 
This paper is structured as follows. Section 2 
describes previous research and the novelty of 
our approach. Section 3 elaborates on our 
proposed structured discriminative approach. 
Section 4 explains the experimental setup. 
Section 5 presents and discusses the results. 
Finally, Section 6 concludes with a brief 
summary and suggestions for future research.  
2 Background and Related Work  
A statistical dialog system needs to update its 
dialog state when taking the action    and 
observing  . Since the POMDP framework 
assumes the Markovian property between states, 
updating a belief state involves only the previous 
belief state, the system action, and the current 
observation: 
 
  (  )     (  |  )? (  |    ) ( )
   
 (1) 
 
where  ( )  denotes the probability distribution 
over states  ,  ( | )  the likelihood of   given 
the state  ,  (  |    )  the state transition 
probability, and   is a normalizing constant. 
In practice, however, belief state updates 
(Equation 1) in many domains are often 
computationally intractable due to the 
tremendously large size of the belief state space. 
In order to reduce the complexity of the belief 
states, the following belief state factorization has 
been commonly applied to the belief update 
procedure (Williams et al, 2005): 
 
  (     
    )    (2) 
                                                 
1 In this task, users call the spoken dialog system to 
2 http://www2.research.att.com/sw/tools/asdt/ 
 
 (  |  
 )? 
                
  (  
 |     
 )?   
                
  
             
      ?  (  |    
    
 )?   
            
  ?  ( 
 |    
 )?  
         
   
          ?  (      )   
 
where  ,  ,   , represents the user goal, the 
dialog history, and the user action, respectively. 
Partition-based approaches (Gasic and Young, 
2011; Lee and Eskenazi, 2012; Williams, 2010; 
Young et al, 2010) attempt to group user goals 
into a small number of partitions and split a 
partition only when this distinction is required by 
observations. This property endows it with the 
high scalability that is suitable for fairly complex 
domains. In partition-based approaches, the goal 
model in Equation 2 is further approximated as 
follows: 
 
? (  |    
 )  ? (  | )
  
 (3) 
  
where   is a partition from the current turn. One 
of the flaws of the partition-based approaches is 
that when one defines a partition to be a 
Cartesian product of subsets of possible values of 
multiple concepts, it will be difficult to adopt 
sophisticated prior distributions over partitions. 
That may lead to either employing very simple 
priors such as uniform distribution or 
maintaining partition structures separately for 
each concept. This is one of the main reasons 
that the previous partition-based approaches 
could not incorporate probabilistic or soft 
relational constraints into the models.  
To allow for relational constraints and 
alleviate the complexity problem at the same 
time, Dynamic Bayesian Networks (DBN) with 
more detailed structures for the user goal have 
also been developed (Thomson and Young, 
2010). Nevertheless, there is still a limitation on 
the types of constraints they can afford. Since 
DBN is a directed network, it is not quite suitable 
for specifying undirected constraints. For 
example, in the Let?s Go domain, users can say 
the same name for the arrival place as the 
departure place if they are distracted, missing the 
prompt for the arrival place and so repeating 
themselves with the departure place. It is also 
possible for some place names with similar 
pronunciations to be recognized as the same (e.g. 
Forbes and Forward). The system can, in this 
443
case, use the constraint that the departure and 
arrival places may not be identical. 
Another drawback of both approaches is that it 
is hard to incorporate a rich set of observation 
features, which are often partly dependent on 
each other. One can create a feature which 
reflects ASR error correlations between 
observations in different time slices. For example, 
a hypothesis that repeats with low confidence 
scores is likely to be a manifestation of ASR 
error correlations. Thus, the highest confidence 
score that a hypothesis has attained so far could 
be a useful feature in preventing repeated 
incorrect hypotheses from defeating the correct 
hypothesis (which had a higher score but was 
only seen once). Another useful feature could be 
the distribution of confidence scores that a 
hypothesis has attained thus far, since it may not 
have the same effect as having a single 
observation with the total score due to the 
potential nonlinearity of confidence scores. 
There are many other potentially useful features. 
The entire list of features is found in Section 3.2. 
Dynamic Probabilistic Ontology Trees (Raux 
and Ma, 2011) is another method based upon 
DBN which does not impose explicit temporal 
structures. Since it does not impose temporal 
structures, it is more flexible in considering 
multiple observations together. However, it is 
still difficult to capture co-dependent features, 
which are exemplified above, without 
introducing probabilistic deficiency due to its 
generative foundation (Appendix E). Moreover, 
the quality of the confidence score will be critical 
to all generative models up to that point since 
they do not usually try to handle potential 
nonlinearity in confidence scores. 
As far as discriminative models are concerned, 
the Maximum Entropy (MaxEnt) model has been 
applied (Bohus and Rudnicky, 2006). But the 
model is limited to a set of separate models for 
each concept, not incorporating relational 
dependencies. Also, it is restricted to maintain 
only top K-best hypotheses where K is a 
predefined parameter, resulting in potential 
degradation of performance and difficulties in 
extending it to structured models. In Section 3, 
our structured discriminative model is described. 
It is designed to take into consideration the 
aforementioned limitations of generative models 
and the previous discriminative approach. 
3 Structured Discriminative Model 
Unlike generative models, discriminative models 
directly model the class posterior given the 
observations. Maximum Entropy is one of most 
powerful undirected graphical models (Appendix 
A). But for some tasks that predict structured 
outputs, e.g. a dialog state, MaxEnt becomes 
impractical as the number of possible outputs 
astronomically grows. For example, in the Lets 
Go domain, the size of possible joint output 
configurations is around 1017. To address this 
problem, Conditional Random Field (CRF) was 
introduced which allows dependencies between 
output variables to be incorporated into the 
statistical model (Appendix B).  
3.1 Model Structure for Dialog State 
Tracking 
We now describe our model structure for dialog 
state tracking in detail using the Let?s Go domain 
as a running example. The graphical 
representation of the model is shown in Fig. 1. 
The global output nodes for each concept (clear 
nodes in Fig. 1) are unlike other temporal 
models, where a set of output nodes are newly 
introduced for each time slice. Instead, as a 
dialog proceeds, a set of new observations   
  
(shaded nodes in Fig. 1) are continuously 
attached to the model structure and the feature 
 
 
Figure 1: Factor graph representing the structured discriminative model in the Let?s Go domain. The shaded 
nodes show observed random variables. The smaller solid node is the deterministic parameters and explicitly 
represents parameter sharing between two associated factors.  
 
444
functions are responsible for producing fixed 
length feature vectors. The sequence of 
observations includes not only ASR N-best lists 
but also system actions from the beginning of the 
dialog to the current time slice  . Any output 
node can be freely connected to any other to 
impose desirable constraints between them 
whether or not the connections form a loop (solid 
lines in Fig. 1).  
In practice, models rely extensively on 
parameter tying, e.g., transition parameters in a 
Hidden Markov Model. One specific example of 
relational constraints and parameter tying 
naturally arises in the Let?s Go domain: the 
feature function which indicates whether a place 
is valid on a given route could use the same 
weights for both departure and arrival places (the 
solid node and the associated factor nodes in Fig. 
1). Parameter tying is also implicitly taking 
place. This is crucial for robust estimation of the 
model parameters in spite of data sparseness. 
Some concepts such as from and to can have 
about 104 values but most of them are not seen in 
the training corpus. Thus we aggregate several 
feature functions which differ only by output 
labels into one common feature function so that 
they can gather their statistics together. For 
example, we can aggregate the observation 
feature functions (dotted lines in Fig. 1) 
associated with each output label except for 
None (Section 3.2). Here, None is a special value 
to indicate that the true hypothesis has not yet 
appeared in the ASR N-best lists. Since there are 
generally a large number of values for each 
concept, the probability of the true hypothesis 
will be very small unless the true hypothesis 
appears on the N-best lists. Thus we can make 
inferences on the model very quickly by focusing 
only on the observed hypotheses at the cost of 
little performance degradation. Additionally, the 
feature function aggregation allows for the entire 
observed hypotheses to be incorporated without 
being limited to only the pre-defined number of  
hypotheses.  
3.2 Model Features 
In this section, we describe the model features 
which are central to the performance of 
discriminative models. Features can be broadly 
split into observation features and relational 
features. To facilitate readers? understanding an 
example of feature extraction is illustrated in Fig. 
2. 
One of the most fundamental features for 
dialog state tracking should exploit the 
confidence scores assigned to an informed 
hypothesis. The simplest form could be direct 
use of confidence scores. But often pre-trained 
confidence measures fail to match the empirical 
distribution of a given dialog domain (Lee and 
Eskenazi, 2012; Thomson et al 2010a). Also the 
distribution of confidence scores that a 
hypothesis has attained so far may not have the 
same effect as the total score of the confidence 
scores (e.g., in Fig. 2, two observations for 61C 
with confidence score 0.3 vs. 0.6 which is the 
sum of the scores). Thus we create a feature 
function that divides the range of confidence 
scores into bins and returns the frequency of 
observations that fall into the corresponding bin: 
 
       (    
 )  
        {
                 (       (    
 ))
           
   
(4) 
 
where      ( )  returns the set of confidence 
scores whose action informs   in the sequence of 
observations   
 .         (   )  computes the 
frequency of observations that fall into the     
bin. 
There are two types of grounding actions 
which are popular in spoken dialog systems, i.e., 
implicit and explicit confirmation. To leverage 
affirmative or negative responses to such system 
acts, the following feature functions are 
introduced in a similar fashion as the        
feature function: 
 
 
 
Figure 2: A simplified example of feature extraction for the route concept. It shows the values that each feature 
will have when three consecutive user inputs are given. 
 
445
       (    
 )  
       {
                 (       (    
 ))
           
   
(5) 
 
       (    
 )  
       {
                 (       (    
 ))
           
   
(6) 
 
where      ( )  /      ( )  returns the set of 
confidence scores whose associated action 
affirms / negates   in the sequence of 
observations   
 . 
 
           (    
 )  
                         {
                  (    
 )
           
   
(7) 
 
where          ( ) indicates whether or not the 
user has negated the system?s implicit 
confirmation in the sequence of observations   
 . 
Another interesting feature function is the so-
called baseline feature which exploits the output 
of a baseline system. The following feature 
function emulates the output of the baseline 
system which always selects the top ASR 
hypothesis for the entire dialog: 
 
          (    
 )  
      {
            (           (    
 ))
           
   
(8) 
 
where          ( )  returns the maximum 
confidence score whose action informs   in the 
sequence of observations   
 .    (   )  indicates 
whether or not the maximum score falls into the 
    bin. 
Yet another feature function of this kind is the 
accumulated score which adds up all confidence 
scores associated with inform and affirm and 
subtracts the ones with negation: 
 
         (    
 )  
                  
{
 
 
 
              ?     (    
 )
                          ?     (    
 )
                          ?     (    
 )
                 
   
(9) 
 
Note that such feature functions as 
          ( )  and          ( )  are not 
independent of the others defined previously, 
which may cause generative models to produce 
deficient probability distributions (Appendix E). 
It is known that prior information can boost 
the performance (Williams, 2012) if the prior is 
well-estimated. One of advantages of generative 
models is that they provide a natural mechanism 
to incorporate a prior. Discriminative models 
also can exploit a prior by introducing additional 
feature functions: 
 
      (    
 )  
            {
            (            ( ))
           
   
(10) 
 
where           ( ) returns the fraction of 
occurrences of   in the set of true labels. 
If the system cannot process a certain user 
request, it is highly likely that the user change 
his/her goal. The following feature function is 
designed to take care of such cases: 
 
        (    
 )  {
             ( )
           
   (11) 
 
where     ( ) indicates whether or not   is out-
of-coverage. 
As with other log-linear models, we also have 
feature functions for bias: 
 
    (    
 )    
        (    
 )   {
          
            
 
(12) 
 
Note that we have an additional bias term for 
None to estimate an appropriate weight for it. 
Regarding relational constraints, we have 
created two feature functions. To reflect the 
presumption that it is likely for the true 
hypothesis for the place concepts (i.e. from and 
to) to be valid on the true hypothesis for the 
route concept, we have: 
 
              (   )  
                             {
              (   )
                            
   
(13) 
 
where      (   )  indicates whether or not the 
place   is valid on the route  . Another feature 
function considers the situation where the same 
place name for both departure and arrival places 
is given: 
 
               (     )  
        {
           
                  
                                                       
   
(14) 
 
3.3 Inference & Parameter Estimation 
One of the common grounding actions of spoken 
dialog systems is to ask a confirmation question 
about hypotheses which do not have sufficient 
marginal beliefs. This makes marginal inference 
446
to be one of the fundamental reasoning tools for 
dialog state tracking. In treelike graphs, exact 
marginal probabilities are efficiently computable 
by using the Junction Tree algorithm (Lauritzen 
and Spiegelhalter, 1988) but in general it is 
intractable on structured models with loops.  
Since it is highly likely to have loopy 
structures in various domains (e.g. Fig. 1), we 
need to adopt approximate inference algorithms 
instead. Note that CRF (Equation 16) is an 
instance of the exponential family. For the 
exponential family, it is known that the exact 
inference can be formulated as an optimization 
problem (Wainwright and Jordan, 2008). The 
variational formulation opens the door to various 
approximate inference methods. Among many 
possible approximations, we adopt the Tree 
Reweighted Belief Propagation (TRBP) method 
which convexifies the optimization problem that 
it guarantees finding the global solution 
(Appendix C).  
On the other hand, joint inference also 
becomes important for either selecting a 
hypothesis to confirm or determining the final 
joint configuration when there exist strong 
relational dependencies between concepts. 
Moreover, we would like to find not just the best 
configuration but rather the top  configurations. 
Since the number of concept nodes is generally 
moderate, we approximate the inference by 
searching for the top   configurations only 
within the Cartesian product of the top   
hypotheses of each concept. For domains with a 
large number of concepts, one can use more 
advanced methods, e.g., Best Max-Marginal 
First (Yanover and Weiss, 2004) and Spanning 
Tree Inequalities and Partitioning for 
Enumerating Solutions (Fromer and Globerson, 
2009). 
The goal of parameter estimation is to 
minimize the empirical risk. In this paper, we 
adopt the negative of the conditional log 
likelihood (Appendix D). Given the partial 
derivative (Equation 26), we employ the 
Orthant-wise Limited-memory Quasi Newton 
optimizer (Andrew and Gao, 2007) for L1 
regularization to avoid model overfitting.  
4 Experimental Setup 
In order to evaluate the proposed method, two 
variants of the proposed method (discriminative 
model (DM) and structured discriminative model 
(SDM)) were compared with the baseline system, 
which always takes the top ASR hypothesis for 
the entire dialog and outputs the joint 
configuration using the highest average score, 
and the ASDT system as being the state-of-the-
art partition-based model (PBM). To train and 
evaluate the models, two datasets from the 
Spoken Dialog Challenge 2010 are used: a) 
AT&T system (Williams, 2011), b) Cambridge 
system (Thomson et. al, 2010b).  
For discriminative models, we used 10 bins 
for the feature functions that need to discretize 
their inputs (Section 3.2). Parameter tying for 
relational constraints was applied to dataset A 
but not to dataset B. To make sure that TRBP 
produces an upper bound on the original entropy, 
the constants    were set to be     for SDM and 
1 for DM (Appendix C). Also the weights for L1 
regularization were set to be 10 and 2.5 for the 
prior features and the other features, respectively. 
These values were chosen through cross-
validation over several values rather than doing a 
thorough search. For the ASDT system, we 
modified it to process implicit confirmation and 
incorporate the prior distribution which was 
estimated on the training corpus. The prior 
distribution was smoothed by approximate 
Good-Turing estimation on the fly when the 
system encounters an unseen value at run time. 
Two aspects of tracker performance were 
measured at the end of each dialog, i.e. Accuracy 
and Receiver Operating Characteristic (ROC). 
Accuracy measures the percent of dialogs where 
the tracker?s top hypothesis is correct. ROC 
assesses the discrimination of the top 
hypothesis?s score. Note that we considered 
None as being correct if there is no ASR 
hypothesis corresponding to the transcription. If 
all turns are evaluated regardless of context, 
concepts which appear earlier in the dialog will 
be measured more times than concepts later in 
the dialog. In order to make comparisons across 
concepts fair, concepts are only measured when 
 Route From To Date Time Joint 
Training 378 334 309 33 30 378 
Test 379 331 305 54 50 379 
 
(a) Dataset A 
 
 Route From To Date Time Joint 
Training 94 403 353 18 217 227 
Test 99 425 376 18 214 229 
 
(b) Dataset B 
 
Table 1: Counts for each concept represent the 
number of dialogs which have non-empty utterances 
for that concept. From and To concepts add up the 
counts for their sub-concepts. Joint denotes the joint 
configuration of all concepts. 
 
447
they are in focus. It does not, however, allow for 
a tracker to receive score for new estimations 
about concepts that are not in focus. In addition, 
dialogs with more turns will have a greater effect 
than dialogs with fewer turns. Therefore we only 
measure concepts which appear in the dialog at 
the last turn of the dialog before restart. The 
statistics of the training and test datasets are 
summarized in Table 1. 
5 Results and Discussion  
The results indicate that discriminative methods 
outperform the baseline and generative method 
by a large performance gap for both dataset A 
and B (Table 2). Also, SDM exceeds DM, 
demonstrating the effectiveness of using 
relational constraints. Furthermore, the 
performance of SDM surpasses that of the best 
system in the Dialog State Tracking Challenge3 
(Lee and Eskenazi, 2013). Even though the 
generative model underperforms discriminative 
models, it is also shown that dialog state tracking 
methods in general are effective in improving 
robustness to ASR errors. Another noteworthy 
result is that the gains for Joint by using 
discriminative models are much larger than those 
for All. Estimating joint configurations correctly 
is crucial to eventually satisfy the user?s request. 
This result implies that the proposed model 
performs evenly well for all concepts and is more 
robust to the traits of each concept. For example, 
PBM works relatively poorly for To on dataset A. 
What makes To different is that the quality of the 
                                                 
3 http://research.microsoft.com/en-us/events/dstc/ 
ASR hypotheses of the training data is much 
better than that of test data: the baseline accuracy 
on the training data is 84.79% while 77.05% on 
the test data. Even though PBM suffers this 
mismatch, the discriminative models are doing 
well without significant differences, implying 
that the discriminative models achieve 
robustness by considering not just the confidence 
score but also several features together. 
Since there has been no clear evidence that the 
use of N-best ASR hypotheses is helpful for 
dialog state tracking (Williams, 2012), we also 
report accuracies while varying the number of N-
best hypotheses. The results show that the use of 
N-bests helps boost accuracy across all models 
on dataset A. However, interestingly it hampers 
the performance in the case of dataset B. It 
demonstrates that the utility of N-bests depends 
on various factors, e.g., the quality of N-bests 
and dialog policies. The system which yielded 
dataset A employs implicit and explicit 
confirmation much more frequently than the 
system which produced dataset B does. The 
proposed model trained on dataset A without 
confirmation features incorporated actually 
showed a slight degradation in accuracy when 
using more than 3-bests. This result indicates that 
we need to take into consideration the type of 
dialog strategy to determine how many 
hypotheses to use. Thus, it can be conceivable to 
dynamically change the range of N-bests 
according to how a dialog proceeds. That allows 
the system to reduce processing time when a 
dialog goes well. 
 All (%)  Joint 
N-best Baseline PBM DM SDM  Baseline PBM DM SDM 
1-best 74.80 77.93 83.65 83.74  53.56 54.62 60.16 60.69 
3-best 74.80 84.00 88.83 89.10  53.56 64.38 70.18 70.98 
5-best 74.80 84.54 89.54 89.81  53.56 65.70 72.30 73.09 
All 74.80 84.81 89.81 90.26  53.56 65.96 73.09 74.67 
 
(a) Dataset A 
 
 All  Joint 
N-best Baseline PBM DM SDM  Baseline PBM DM SDM 
1-best 65.46 68.73 78.00 80.12  11.35 12.23 26.20 30.13 
3-best 65.46 68.02 78.00 79.51  11.35 11.35 27.51 28.82 
5-best 65.46 67.40 77.92 79.15  11.35 11.79 24.89 25.76 
All 65.46 66.61 78.00 79.24  11.35 11.79 24.89 25.76 
 
(b) Dataset B 
 
Table 2: Accuracy of the comparative models. The best performaces across the models are marked in bold. All 
means a weighted average accuracy across all concepts. 
448
The ROC curves assess the discrimination of 
the top hypothesis? score (Fig. 3). Note that the 
discriminative models are far better than PBM on 
both dataset A and B. In fact, PBM turns out to 
be even worse than the baseline. The better 
discrimination can give rise to additional values 
of a tracker. For example, it can reduce 
unnecessary confirmations for values with 
sufficiently high belief. Also, it enables a model 
to adapt to test data in an unsupervised manner 
by allowing us to set a proper threshold to 
produce predictive labels.  
6 Conclusion 
In this paper, we presented the first attempt, to 
our knowledge, to create a structured 
discriminative model for dialog state tracking. 
Unlike generative models, the proposed method 
allows for the incorporation of various features 
without worrying about dependencies between 
observations. It also provides a flexible 
mechanism to impose relational constraints. The 
results show that the discriminative models are 
superior to the generative model in accuracy, 
discrimination, and robustness to mismatches 
between training and test datasets. Since we used 
relatively simple features for this work, there is 
much room to boost performance through feature 
engineering. Also, more thorough search for 
regularization weights can give additional 
performance gain. Moreover, one can apply 
different loss functions, e.g., hinge loss to obtain 
structured support vector machine. In order to 
further confirm if the performance improvement 
by the proposed method can be translated to the 
enhancement of the overall spoken dialog 
system, we need to deploy and assess it with real 
users. 
Acknowledgments 
This work was funded by NSF grant IIS0914927. 
The opinions expressed in this paper do not 
necessarily reflect those of NSF. The author 
would like to thank Maxine Eskenazi for helpful 
comments and discussion. 
References  
G. Andrew and J. Gao, 2007. Scalable training of L1-
regularized log-linear models. In Proceedings of 
ICML. 
A. Black et al, 2011. Spoken dialog challenge 2010: 
Comparison of live and control test results. In 
Proceedings of SIGDIAL. 
D. Bohus and A. Rudnicky, 2006. A K hypotheses + 
other belief updating model. In Proceedings of 
AAAI Workshop on Statistical and Empirical 
Approaches for Spoken Dialogue Systems. 
M. Fromer and A. Globerson, 2009. An LP View of 
the M-best MAP problem. Advances in Neural 
Information Processing Systems, 22:567-575. 
M. Gasic and S. Young, 2011. Effective handling of 
dialogue state in the hidden information state 
POMDP-based dialogue manager. ACM 
Transactions on Speech and Language Processing, 
7(3). 
S. Lauritzen and D. J. Spiegelhalter, 1988. Local 
Computation and Probabilities on Graphical 
Structures and their Applications to Expert 
Systems. Journal of Royal Statistical Society, 
50(2):157?224. 
S. Lee and M. Eskenazi, 2012a. Exploiting Machine-
Transcribed Dialog Corpus to Improve Multiple 
Dialog  States Tracking Methods. In Proceedings 
of SIGDIAL, 2012. 
 
 
Figure 3: Weighted average ROC curves across all concepts 
449
S. Lee and M. Eskenazi, 2012b. POMDP-based Let?s 
Go System for Spoken Dialog Challenge. In 
Proceedings of SLT. 
S. Lee and M. Eskenazi, 2013. Recipe For Building 
Robust Spoken Dialog State Trackers: Dialog State 
Tracking Challenge System Description. Submitted 
to SIGDIAL, 2013. 
A. Raux, B. Langner, D. Bohus, A. W Black, and M. 
Eskenazi, 2005. Let?s Go Public! Taking a Spoken 
Dialog System to the Real World. In Proceedings 
of Interspeech. 
A. Raux and Y. Ma, 2011. Efficient Probabilistic 
Tracking of User Goal and Dialog History for 
Spoken Dialog Systems. In Proceedings of 
Interspeech. 
C. Sutton and A. McCallum, 2006. An Introduction to 
Conditional Random Fields for Relational 
Learning. Introduction to Statistical Relational 
Learning. Cambridge: MIT Press. 
B. Thomson and S. Young, 2010. Bayesian update of 
dialogue state: A POMDP framework for spoken 
dialogue systems. Computer Speech & Language, 
24(4):562-588. 
B. Thomson, F. Jurccek, M. Gasic, S. Keizer, F. 
Mairesse, K. Yu, S. Young, 2010a. Parameter 
learning for POMDP spoken dialogue models. In 
Proceedings of SLT.  
B. Thomson, K. Yu, S. Keizer, M. Gasic, F. Jurcicek, 
F. Mairesse, S. Young, 2010b. Bayesian dialogue 
system for the Let's Go spoken dialogue challenge. 
In Proceedings of SLT. 
M. Wainwright and M. Jordan, 2008. Graphical 
Models, Exponential Families, and Variational 
Inference. Foundations and Trends in Machine 
Learning, 1(1-2):1?305. 
J. Williams and S. Young, 2007. Partially observable 
Markov decision processes for spoken dialog 
systems. Computer Speech & Language, 
21(2):393-422. 
J. Williams, 2010. Incremental partition 
recombination for efficient tracking of multiple 
dialog states. In Proceedings of ICASSP. 
J. Williams, 2011. An Empirical Evaluation of a 
Statistical Dialog System in Public Use, In 
Proceedings of SIGDIAL. 
J. Williams, 2012. A Critical Analysis of Two 
Statistical Spoken Dialog Systems in Public Use. 
In Proceedings of SLT. 
C. Yanover and Y. Weiss, 2004. Finding the M Most 
Probable Configurations Using Loopy Belief 
Propagation. In Advances in Neural Information 
Processing Systems 16. MIT Press.  
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson and K. Yu, 2010. The Hidden 
Information State Model: a practical framework for 
POMDP-based spoken dialogue management. 
Computer Speech and Language, 24(2):150?174. 
Appendix A. Maximum Entropy 
Maximum Entropy directly models the class 
posterior given the observations:  
 
 ( | )  
 
 ( )
   (   (   ))  (15) 
 
where  ( ) is a normalization function,   the model 
parameters, and  (   ) the vector of feature functions 
which are key to performance. 
Appendix B. Conditional Random Field 
Let   be a factor graph over outputs  . Then, if 
the distribution  ( | ) factorizes according to   
and   *  +  is the set of factors in  , the 
conditional distribution can be written as: 
 
 ( | )  
 
 ( )
?    (  
  (     ))
    
 (16) 
 
In practice, models rely extensively on parameter 
tying. To formalize this, let the factors of   be 
partitioned to   *          +, where each    
is a clique template whose parameters are tied. 
Each clique template is a set of factors which has 
an associated vector of feature functions 
  (     )  and parameters   . From these it 
follows (Sutton and McCallum, 2006):  
 
 ( | )  
 
 ( )
? ?    (  
  (     ))
         
 (17) 
 
where the normalizing function is: 
 
 ( )  ?? ?    (  
  (     ))
          
 (18) 
 
Appendix C. Tree-reweighted Belief Propagation 
Unlike treelike graphs, computing exact marginal 
probabilities is in general intractable on 
structured models with loops. Therefore, we need 
to adopt approximate inference algorithms 
instead. Note that CRF (Equation 16) is an 
instance of exponential family: 
 
 (   )      (   ( )   ( )) (19) 
 
where   is a function of the observations   and 
the parameters   above,  ( ) a vector of 
sufficient statistics consisting of indicator 
functions for each configuration of each clique 
and each variable, and  ( ) is the log-partition 
450
function    ?     (   ( ) ) . For exponential 
family, it is known that the exact inference can 
be formulated as an optimization problem 
(Wainwright and Jordan, 2008): 
 
 ( )     
   
     ( ) (20) 
 
where  *  |       ( )+ is the marginal 
polytope,  ( ) is the mapping from parameters 
to marginals, and  ( ) is the entropy. Applying 
Danskin?s theorem to Equation 20 yields: 
 
 ( )  
  
  
       
   
     ( ) (21) 
 
Thus both the partition function (Equation 20) 
and marginals (Equation 21) can be computed at 
once. The variational formulation opens the door 
to various approximate inference methods: to 
derive a tractable algorithm, one approximates 
the log-partition function  ?( )  by using a 
simpler feasible region of   and a tractable  ( ). 
Then the approximate marginals are taken as the 
exact gradient of  ? . Among many possible 
approximations, we adopt the Tree Reweighted 
Belief Propagation (TRBP) method which 
convexifies the optimization problem that it 
guarantees finding the global solution. TRBP 
takes the local polytope as a relaxation of the 
marginal polytope: 
 
  * |? (  )
    
  (   ) ? (  )
  
    + (22) 
 
where  and   index each clique and output 
variable, respectively. TRBP approximates the 
entropy as follows: 
 
 ( )  ? (  )  ?    (  )
  
 (23) 
 
where  ( )  denotes the mutual information and 
the constants    need to be selected so that they 
generate an upper bound on the original entropy. 
 
Appendix D. Parameter Estimation For 
Conditional Random Field 
The goal of parameter estimation is to minimize 
the empirical risk: 
 
 ( )  ? (       )
 
 (24) 
 
where there is summation over all training 
examples. The loss function  (       ) 
quantifies the difference between the true and 
estimated outputs. In this paper, we adopt the 
negative of the conditional log likelihood: 
 ( )  ? ?   
  (     )
         
     ( ) (25) 
 
The partial derivative of the log likelihood with 
respect to a vector of parameters    associated 
with a clique template    is: 
  
  
   
 ?   (     )
     
 
             ? ?  (  
    )
  
      
 (  
 |  ) 
(26) 
 
Appendix E. Probabilistic Deficiency 
To include interdependent features in a 
generative model, we have two choices: enhance 
the model to represent dependencies among the 
inputs, or make independence assumptions. The 
first approach is often difficult to do while 
retaining tractability. For example, it is hard to 
model the dependence between        , 
       ,        ,            , and 
         . On the other hand, the second 
approach can hurt performance by resulting in 
poor probability estimates. Let?s consider the 
joint probability  (         )  which the 
generative approach is based on. Because of the 
independence assumption, the joint probability 
can be written as  ( ) (  | )  (  | ) . For 
example, let?s assume that we observe two 
hypotheses 61D and 61B with confidence score 
0.6 and 0.2, respectively. Then the conditional 
probabilities can be written as: 
 
 (                       |   )  
               (         |   )   
                         (             |   )  
 
 (                       |   ) 
         (         |   )   
              (             |   ) 
 
Since           and                have 
a strong correlation, their probability estimates 
should also be positively correlated. To simplify 
the discussion, now suppose 61B and 61D are 
equiprobable,  (   )    (   )  and have 
similar conditional probabilities: 
 
 (         |   )   (         |   ) 
 (             |   )   
                                          (             |   ) 
 
Then, multiplying those conditional probabilities, 
 (         | )   (             | ) , 
will increase or decrease the confidence of the 
classifier too much, even though no new 
evidence has been added. 
451
Proceedings of the SIGDIAL 2014 Conference, pages 310?317,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Extrinsic Evaluation of Dialog State Tracking and Predictive Metrics 
for Dialog Policy Optimization 
 
 
Sungjin Lee 
Language Technologies Institute,  
Carnegie Mellon University,        
Pittsburgh, Pennsylvania, USA 
sungjin.lee@cs.cmu.edu  
 
  
 
Abstract 
During the recent Dialog State Tracking 
Challenge (DSTC), a fundamental question 
was raised: ?Would better performance in 
dialog state tracking translate to better 
performance of the optimized policy by 
reinforcement learning?? Also, during the 
challenge system evaluation, another non-
trivial question arose: ?Which evaluation 
metric and schedule would best predict 
improvement in overall dialog performance?? 
This paper aims to answer these questions by 
applying an off-policy reinforcement learning 
method to the output of each challenge system. 
The results give a positive answer to the first 
question. Thus the effort to separately improve 
the performance of dialog state tracking as 
carried out in the DSTC may be justified. The 
answer to the second question also draws 
several insightful conclusions on the 
characteristics of different evaluation metrics 
and schedules. 
1 Introduction 
Statistical approaches to spoken dialog 
management have proven very effective in 
gracefully dealing with noisy input due to 
Automatic Speech Recognition (ASR) and 
Spoken Language Understanding (SLU) error 
(Lee, 2013; Williams et al., 2013). Most recent 
advances in statistical dialog modeling have been 
based on the Partially Observable Markov 
Decision Processes (POMDP) framework which 
provides a principled way for sequential action 
planning under uncertainty (Young et al., 2013). 
In this approach, the task of dialog management 
is generally decomposed into two subtasks, i.e., 
dialog state tracking and dialog policy learning. 
The aim of dialog state tracking is to accurately 
estimate the true dialog state from noisy 
observations by incorporating patterns between 
turns and external knowledge as a dialog unfolds 
(Fig. 1). The dialog policy learning process then 
strives to select an optimal system action given 
the estimated dialog state.  
Many dialog state tracking algorithms have 
been developed. Few studies, however, have 
reported the strengths and weaknesses of each 
method. Thus the Dialog State Tracking 
Challenge (DSTC) was organized to advance 
state-of-the-art technologies for dialog state 
tracking by allowing for reliable comparisons 
between different approaches using the same 
datasets (Williams et al., 2013). Thanks to the 
DSTC, we now have a better understanding of 
effective models, features and training methods 
we can use to create a dialog state tracker that is 
not only of superior performance but also very 
robust to realistic mismatches between 
development and deployment environments (Lee 
and Eskenazi, 2013). 
Despite the fruitful results, it was largely 
limited to intrinsic evaluation, thus leaving an 
important question unanswered: ?Would the 
improved performance in dialog state tracking 
carry over to dialog policy optimization?? 
Furthermore, there was no consensus on what 
and when to measure, resulting in a large set of 
metrics being evaluated with three different 
schedules. With this variety of metrics, it is not 
clear what the evaluation result means. Thus it is 
important to answer the question: ?Which metric 
best serves as a predictor to the improvement in 
dialog policy optimization? since this is the 
ultimate goal, in terms of end-to-end dialog 
performance. The aim of this paper is to answer 
these two questions via corpus-based 
experiments. Similar to the rationale behind the 
DSTC, the corpus-based design allows us to 
310
compare different trackers on the same data. We 
applied a sample efficient off-policy 
reinforcement learning (RL) method to the 
outputs of each tracker so that we may examine 
the relationship between the performance of 
dialog state tracking and that of the optimized 
policy as well as which metric shows the highest 
correlation with the performance of the 
optimized policy. 
This paper is structured as follows. Section 2 
briefly describes the DSTC and the metrics 
adopted in the challenge. Section 3 elaborates on 
the extrinsic evaluation method based on off-
policy RL. Section 4 presents the extrinsic 
evaluation results and discusses its implication 
on metrics for dialog state tracking evaluation. 
Finally, Section 5 concludes with a brief 
summary and suggestions for future research.  
2 DSTC Task and Evaluation Metrics 
This section briefly describes the task for the 
DSTC and evaluation metrics. For more details, 
please refer to the DSTC manual1.  
                                                 
1 http://research.microsoft.com/apps/pubs/?id=169024 
2.1 Task Description 
DSTC data is taken from several different 
spoken dialog systems which all provided bus 
schedule information for Pittsburgh, 
Pennsylvania, USA (Raux et al., 2005) as part of 
the Spoken Dialog Challenge (Black et al., 2011). 
There are 9 slots which are evaluated: route, 
from.desc, from.neighborhood, from.monument, 
to.desc, to.neighborhood, to.monument, date, and 
time. Since both marginal and joint 
representations of dialog states are important for 
deciding dialog actions, the challenge takes both 
into consideration. Each joint representation is an 
assignment of values to all slots.  Thus there are 
9 marginal outputs and 1 joint output in total, 
which are all evaluated separately. 
The dialog tracker receives the SLU N-best 
hypotheses for each user turn, each with a 
confidence score. In general, there are a large 
number of values for each slot, and the coverage 
of N-best hypotheses is good, thus the challenge 
confines its determination of whether a goal has 
been reached to slots and values that have been 
observed in an SLU output. By exploiting this 
aspect, the task of a dialog state tracker is to 
generate a set of observed slot and value pairs, 
with a score between 0 and 1. The sum of all 
 
 
Figure 1: An example of dialog state tracking for the Route slot. At each turn the system asks for user?s 
goal or attempts to confirm one of hypotheses. The user?s utterance is recognized to output an N-best 
list. The SLU module generates semantic inputs to the dialog manager by parsing the N-best 
hypotheses. Each SLU hypothesis receives a confidence score. From the current turn?s SLU 
hypotheses and all previous ones thus far, the dialog state tracker computes a probability distribution 
over a set of dialog state hypotheses. Note that the number of hypotheses in a dialog state can be 
different from the number of SLU hypotheses, e.g., at turn t+1, 3 and 5 respectively.  
311
scores is restricted to sum to 1.0. Thus 1.0 ? total 
score is defined as the score of a special value 
None that indicates the user?s goal has not yet 
appeared on any SLU output. 
2.2 Evaluation Metrics 
To evaluate tracker output, the correctness of 
each hypothesis is labeled at each turn. Then 
hypothesis scores and labels over the entire 
dialogs are collected to compute 11 metrics:  
 
? Accuracy measures the ratio of states under 
evaluation where the top hypothesis is 
correct. 
? ROC.V1 computes the following quantity:  
 
     ( )  
  ( )
 
 
 
where   is the total number of top 
hypotheses over the entire data and   ( ) 
denotes the number of correctly accepted top 
hypotheses with the threshold being set to  . 
Similarly FA denotes false-accepts and FR 
false-rejects. From these quantities, several 
metrics are derived. ROC.V1.EER 
computes FA.V1(s) where FA.V1(s) = 
FR.V1(s). The metrics ROC.V1.CA05, 
ROC.V1.CA10, and ROC.V1.CA20 
compute CA.V1(s) when FA.V1(s) = 0.05, 
0.10, and 0.20 respectively. These metrics 
measure the quality of score via plotting 
accuracy with respect to false-accepts so that 
they may reflect not only accuracy but also 
discrimination.  
? ROC.V2 computes the conventional ROC 
quantity:  
 
     ( )  
  ( )
  ( )
 
 
ROC.V2.CA05, ROC.V2.CA10, and 
ROC.V2.CA20 do the same as the V1 
versions. These metrics measure the 
discrimination of the score for the top 
hypothesis independently of accuracy. 
 
Note that Accuracy and ROC curves do not take 
into consideration non-top hypotheses while the 
following measures do. 
 
? L2 calculates the Euclidean distance 
between the vector consisting of the scores 
of all hypotheses and a zero vector with 1 in 
the position of the correct one. This 
measures the quality of tracker?s output 
score as probability. 
? AvgP indicates the averaged score of the 
correct hypothesis. Note that this measures 
the quality of the score of the correct 
hypothesis, ignoring the scores assigned to 
incorrect hypotheses.  
? MRR denotes the mean reciprocal rank of 
the correct hypothesis. This measures the 
quality of rank instead of score. 
 
As far as evaluation schedule is concerned, there 
are three schedules for determining which turns 
to include in each evaluation. 
 
? Schedule 1: Include all turns. This schedule 
allows us to account for changes in concepts 
that are not in focus. But this makes across-
concept comparison invalid since different 
concepts appear at different times in a dialog. 
? Schedule 2: Include a turn for a given 
concept only if that concept either appears on 
the SLU N-Best list in that turn, or if the 
system?s action references that concept in 
that turn. Unlike schedule 1, this schedule 
makes comparisons across concepts valid but 
cannot account for changes in concepts 
which are not in focus. 
? Schedule 3: Include only the turn before the 
system starts over from the beginning, and 
the last turn of the dialog. This schedule does 
not consider what happens during a dialog.  
3 Extrinsic Evaluation Using Off-Policy 
Reinforcement Learning  
In this section, we present a corpus-based 
method for extrinsic evaluation of dialog state 
tracking. Thanks to the corpus-based design 
where outputs of various trackers with different 
characteristics are involved, it is possible to 
examine how the differences between trackers 
affect the performance of learned policies. The 
performance of a learned policy is measured by 
the expected return at the initial state of a dialog 
which is one of the common performance 
measures for episodic tasks.  
3.1 Off-Policy RL on Fixed Data 
To learn an optimal policy from fixed data, we 
applied a state-of-the-art kernelized off-policy 
RL method. Off-policy RL methods allows for 
optimization of a policy by observing how other 
policies behave. The policy used to control the 
312
system?s behavior is called Behavior policy. As 
far as a specific algorithm is concerned, we have 
adopted Least-Squares Temporal Difference 
(LSTD) (Bradtke and Barto, 1996) for policy 
evaluation and Least-Squares Policy Iteration 
(LSPI) (Lagoudakis and Parr, 2003) for policy 
learning. LSTD and LSPI have been well known 
to be sample efficient, thus easily lending 
themselves to the application of RL to fixed data 
(Pietquin et al., 2011). LSPI is an instance of 
Approximate Policy Iteration where an 
approximated action-state value function (a.k.a Q 
function) is established for a current policy and 
an improved policy is formed by taking greedy 
actions with respect to the estimated Q function. 
The process of policy evaluation and 
improvement iterates until convergence. For 
value function approximation, in this work, we 
adopted the following linear approximation 
architecture: 
 
 ? (   )   
  (   ) 
 
where   is the set of parameters,  (   )  an 
activation vector of basis functions,   a state and 
  an action. Given a policy    and a set of state 
transitions  (             )      , where    is the 
reward that the system would get from the 
environment by executing action    at state   , 
the approximated state-action value function  ?  
is estimated by LSTD. The most important part 
of LSTD lies in the computation of the gradient 
of temporal difference: 
 
 (   )    (    (  )) 
 
In LSPI,  (  ) takes the form of greedy policy:  
 
 (  )        
  
 ? ( 
    ) 
 
It is however critical to take into consideration 
the inherent problem of insufficient exploration 
in fixed data to avoid overfitting (Henderson et 
al., 2008). Thus we confined the set of available 
actions at a given state to the ones that have an 
occurrence probability greater than some 
threshold  : 
 
 (  )        
       (  |  )  
 ? ( 
    ) 
 
The conditional probability  (  |  )  can be 
easily estimated by any conventional 
classification methods which provide posterior 
probability. In this study, we set   to 0.1. 
3.2 State Representation and Basis Function 
In order to make the process of policy 
optimization tractable, the belief state is 
normally mapped to an abstract space by only 
taking crucial information for dialog action 
selection, e.g., the beliefs of the top and second 
hypotheses for a concept. Similarly, the action 
space is also mapped into a smaller space by 
only taking the predicate of an action. In this 
work, the simplified state includes the following 
elements: 
 
? The scores of the top hypothesis for each 
concept with None excluded 
? The scores of the second hypothesis for each 
concept with None excluded 
? The scores assigned to None for each 
concept 
? Binary indicators for a concept if there are 
hypotheses except None 
? The values of the top hypothesis for each 
concept 
? A binary indicator if the user affirms when 
the system asks a yes-no question for next 
bus 
 
It has been shown that the rapid learning speed 
of recent approaches is partly attributed to the 
use of kernels as basis functions (Gasic et al., 
2010; Lee and Eskenazi, 2012; Pietquin et al., 
2011). Thus to make the best of the limited 
amount of data, we adopted a kernelized 
approach. Similar to previous studies, we used a 
product of kernel functions: 
 
 (    )    (   
 )?  (   
 )
   
 
 
where   (   )  is responsible for a vector of 
continuous elements of a state and   (   )  for 
each discrete element. For the continuous 
elements, we adopted Gaussian kernels: 
 
  (   
 )       ( 
?     
 ?
   
)  
 
where   governs the value at center,   controls 
the width of the kernel and   represents the 
vector of continuous elements of a state. In the 
experiments,  and   were set to 4 and 3, 
313
respectively. For a discrete element, we adopted 
delta kernel: 
 
  (   
 )     (  
 ) 
 
where   ( 
 )  returns one if     , zero 
otherwise and    represents an element of a state. 
As the number of data points increases, 
kernelized approaches commonly encounter 
severe computational problems. To address this 
issue, it is necessary to limit the active kernel 
functions being used for value function 
approximation. This sparsification process has to 
find out the sufficient number of kernels which 
keeps a good balance between computational 
tractability and approximation quality. We 
adopted a simple sparsification method which 
was commonly used in previous studies (Engel et 
al., 2004). The key intuition behind of the 
sparsification method is that there is a mapping 
 ( )  to a Hilbert space in which the kernel 
function  (    )  is represented as the inner 
product of  ( )  and  (  )  by the Mercer?s 
theorem. Thus the kernel-based representation of 
Q function can be restated as a plain linear 
equation in the Hilbert space: 
 
 ? ( )  ?  
 
 (    
 )  ? ( ) ?   (  
 )
 
? 
 
where   denotes the pair of state and action. The 
term ?    (  
 )  plays the role of the weight 
vector in the Hilbert space. Since this term takes 
the form of linear combination, we can safely 
remove any linearly dependent  (  
 )  without 
changing the weighted sum by tuning  . It is 
known that the linear dependence of  ( ) from 
the rest can be tested based on kernel functions 
as follows:  
 
     (     )      (  )
     (1) 
 
where          (       )  (       )    
and   is a sparsification threshold. When 
equation 1 is satisfied,    can be safely removed 
from the set of basis functions. Thus the sparsity 
can be controlled by changing  . It can be shown 
that equation 1 is minimized when   
    
      (  ) , where     
   is the Gram matrix 
excluding   . In the experiments,   was set to 3. 
3.3 Reward Function 
The reward function is defined following a  
common approach to form-filling, task-oriented 
systems: 
 
? Every correct concept filled is rewarded 100 
? Every incorrect concept filled is assigned     
-200 
? Every empty concept is assigned -300 if the 
system terminated the session, -50 otherwise. 
? At every turn, -20 is assigned 
 
The reward structure is carefully designed such 
that the RL algorithm cannot find a way to 
maximize the expected return without achieving 
the user goal. 
4 Experimental Setup 
In order to see the relationship between the 
performance of dialog state tracking and that of 
the optimized policy, we applied the off-policy 
RL method presented in Section 3 to the outputs 
of each tracker for all four DSTC test datasets2. 
The summary statistics of the datasets are 
presented in Table 1. In addition, to quantify the 
impact of dialog state tracking on an end-to-end 
dialog, the performance of policies optimized by 
RL was compared with Behavior policies and 
another set of learned policies using supervised 
learning (SL). Note that Behavior policies were 
developed by experts in spoken dialog research. 
The use of a learned policy using supervised 
                                                 
2 We took the entry from each team that achieved the 
highest ranks of that team in the largest number of 
evaluation metrics: entry2 for team3 and team6, 
entry3 for team8, entry4 for team9, and entry1 for the 
rest of the teams. We were not, however, able to 
process the tracker output of team2 due to its large 
size. This does not negatively impact the general 
results of this paper. 
 # Dialogs # Turns 
Training Test Training Test 
DS1 274 312 2594 2168 
DS2 321 339 3394 2579 
DS3 277 286 2221 1988 
DS4 141 165 1060 979 
Table 1: The DSTC test datasets (DS1-4) 
were evenly divided into two groups of 
datasets for off-policy RL training and test. To 
simplify the analysis, the dialogs that include 
startover and canthelp were excluded. 
314
learning (Hurtado et al., 2005) is also one of the 
common methods of spoken dialog system 
development. We exploited the SVM method 
with the same kernel functions as defined in 
Section 3.2 except that the action element is not 
included. The posterior probability of the SVM 
model was also used for handling the insufficient 
exploration problem (in Section 3.1).  
5 Results and Discussion  
The comparative results between RL, SL and 
Behavior policies are plotted in Fig. 2. Despite 
the relatively superior performance of SL 
policies over Behavior policies, the performance 
improvement is neither large nor constant. This 
confirms that Behavior policies are very strong 
baselines which were designed by expert 
researchers. RL policies, however, consistently 
outperformed Behavior as well as SL policies, 
with a large performance gap. This result 
indicates that the policies learned by the 
proposed off-policy RL method are a lot closer to 
optimal ones than the hand-crafted policies 
created by human experts. Given that many state 
features are derived from the belief state, the 
large improvement in performance implies that 
the estimated belief state is indeed a good 
summary representation of a state, maintaining 
the Markov property between states. The Markov 
property is a crucial property for RL methods to 
approach to the optimal policy. On the other 
hand, most of the dialog state trackers surpassed 
the baseline tracker (team0) in the performance 
of RL policies. This result assures that the better 
the performance in dialog state tracking, the 
better a policy we can learn in the policy 
optimization stage. Given these two results, we 
can strongly assert that dialog state tracking 
plays a key role in enhancing end-to-end dialog 
performance. 
Another interesting result worth noticing is 
that the performance of RL policies does not 
exactly align with the accuracy measured at the 
end of a dialog (Schedule 3) which would have 
been the best metric if the task were a one-time 
classification (Fig. 2). This misalignment 
therefore supports the speculation that accuracy-
schedule3 might not be the most appropriate 
metric for predicting the effect of dialog state 
tracking on end-to-end dialog performance. In 
order to better understand What To Measure and 
When To Measure to predict end-to-end dialog 
performance, a correlation analysis was carried 
out between the performance of RL policies and 
that of the dialog state tracking measured by 
different metrics and schedules. The correlations 
are listed in descending order in Fig. 3. This 
result reveals several interesting insights for 
different metrics.  
First, metrics which are intended to measure 
the quality of a tracker?s score (e.g., L2 and 
AvgP) are more correlated than other metrics. 
This tendency can be understood as a 
consequence of the sequential decision-making 
nature of a dialog task. A dialog system can 
always initiate an additional turn, unless the user 
 
 
Figure 2: The left vertical axis is associated with the performance plots of RL, SL and Behavior 
policies for each team. The right vertical axis measures the accuracies of each team?s tracker at the end 
of a dialog (schedule 3). 
315
terminates the session, to refine its belief state 
when there is no dominant hypothesis. Thus 
accurate estimation of the beliefs of all observed 
hypotheses is essential. This is why the 
evaluation of only the top hypothesis does not 
provide sufficient information.  
Second, schedule1 and schedule3 showed a 
stronger correlation than schedule2. In fact 
schedule2 was more preferred in previous studies 
since it allows for a valid comparison of different 
concepts (Williams, 2013; Williams et al., 2013). 
This result can be explained by the fact that the 
best system action is selected by considering all 
of the concepts together. For example, when the 
system moves the conversation focus from one 
concept to another, the beliefs of the concepts 
that are not in focus are as important as the 
concept in focus. Thus evaluating all concepts at 
the same time is more suitable for predicting the 
performance of a sequential decision-making 
task involving multiple concepts in its state.  
Finally, metrics for evaluating discrimination 
quality (measured by ROC.V2) have little 
correlation with end-to-end dialog performance. 
In order to understand this relatively unexpected 
result, we need to give deep thought to how the 
scores of a hypothesis are distributed during the 
session. For example, the score of a true 
hypothesis usually starts from a small value due 
to the uncertainty of ASR output and gets bigger 
every time positive evidence is observed. The 
score of a false hypothesis usually stays small or 
medium. This leads to a situation where both true 
and false hypotheses are pretty much mixed in 
the zone of small and medium scores without 
significant discrimination. It is, however, very 
important for a metric to reveal a difference 
between true and false hypotheses before their 
scores fully arrive at sufficient certainty since 
most additional turns are planned for hypotheses 
with a small or medium score. Thus general 
metrics evaluating discrimination alone are 
hardly appropriate for a tracking problem where 
the score develops gradually. Furthermore, the 
choice of threshold (i.e. FA = 0.05, 0.10, 0.20) 
was made to consider relatively unimportant 
regions where the true hypothesis is likely to 
have a higher score, meaning that no further 
turns need to be planned. 
6 Conclusion 
In this paper, we have presented a corpus-based 
study that attempts to answer two fundamental 
questions which, so far,  have not been 
rigorously addressed: ?Would better 
performance in dialog state tracking translate to 
better performance of the optimized policy by 
RL??  and ?Which evaluation metric and 
schedule would best predict improvement in 
overall dialog performance?? The result 
supports a positive answer to the first question. 
Thus the effort to separately improve the 
performance of dialog state tracking as carried 
out in the recent held DSTC may be justified. As 
a way to address the second question, the 
correlations of different metrics and schedules 
 
 
Figure 3: The correlations of each combination of metric and schedule with the performance of 
optimized polices.  
316
with the performance of optimized policies were 
computed. The results revealed several insightful 
conclusions: 1) Metrics measuring score quality 
are more suitable for predicting the performance 
of an optimized policy. 2) Evaluation of all 
concepts at the same time is more appropriate for 
predicting the performance of a sequential 
decision making task involving multiple 
concepts in its state. 3) Metrics evaluating only 
discrimination (e.g., ROC.V2) are inappropriate 
for a tracking problem where the score gradually 
develops. Interesting extensions of this work 
include finding a composite measure of 
conventional metrics to obtain a better predictor. 
A data-driven composition may tell us the 
relative empirical importance of each metric. In 
spite of several factors which generalize our 
conclusions such as handling insufficient 
exploration, the use of separate test sets and 
various mismatches between test sets, it is still 
desirable to run different policies for live tests in 
the future. Also, since the use of an approximate 
policy evaluation method (e.g. LSTD) can 
introduce systemic errors, more deliberate 
experimental setups will be designed for a future 
study: 1) the application of different RL 
algorithms for training and test datasets 2) 
further experiments on different datasets, e.g., 
the datasets for DSTC2 (Henderson et al., 2014). 
Although the state representation adopted in this 
work is quite common for most systems that use 
a POMDP model, different state representations 
could possibly reveal new insights. 
 
References  
A. Black et al., 2011. Spoken dialog challenge 2010: 
Comparison of live and control test results. In 
Proceedings of SIGDIAL. 
S. Bradtke and A. Barto, 1996. Linear Least-Squares 
algorithms for temporal difference learning. 
Machine Learning, 22, 1-3, 33-57.  
Y. Engel, S. Mannor and R. Meir, 2004. The Kernel 
Recursive Least Squares Algorithm. IEEE 
Transactions on Signal Processing, 52:2275-2285. 
M. Gasic and S. Young, 2011. Effective handling of 
dialogue state in the hidden information state 
POMDP-based dialogue manager. ACM 
Transactions on Speech and Language Processing, 
7(3). 
M. Gasic, F. Jurcicek, S. Keizer, F. Mairesse, B. 
Thomson, K. Yu and S. Young, 2010. Gaussian 
Processes for Fast Policy Optimisation of POMDP-
based Dialogue Managers, In Proceedings of 
SIGDIAL, 2010. 
J. Henderson, O. Lemon and K. Georgila, 2008. 
Hybrid reinforcement/supervised learning of 
dialogue policies from fixed data sets. 
Computational Linguistics, 34(4):487-511. 
M. Henderson, B. Thomson and J. Williams, 2014. 
The Second Dialog State Tracking Challenge. In 
Proceedings of SIGDIAL, 2014. 
L. Hurtado, D. Grial, E. Sanchis and E. Segarra, 2005. 
A Stochastic Approach to Dialog Management. In 
Proceedings of ASRU, 2005. 
M. Lagoudakis and R. Parr, 2003. Least-squares 
policy iteration. Journal of Machine Learning 
Research 4, 1107-1149. 
S. Lee, 2013. Structured Discriminative Model For 
Dialog State Tracking. In Proceedings of SIGDIAL, 
2013. 
S. Lee and M. Eskenazi, 2012. Incremental Sparse 
Bayesian Method for Online Dialog Strategy 
Learning. IEEE Journal of Selected Topics in 
Signal Processing, 6(8). 
S. Lee and M. Eskenazi, 2013. Recipe For Building 
Robust Spoken Dialog State Trackers: Dialog State 
Tracking Challenge System Description. In 
Proceedings of SIGDIAL, 2013. 
O. Pietquin, M. Geist, S. Chandramohan and H. 
Frezza-buet, 2011. Sample Efficient Batch 
Reinforcement Learning for Dialogue Management 
Optimization. ACM Transactions on Speech and 
Language Processing, 7(3).  
O. Pietquin, M. Geist, and S. Chandramohan, 2011. 
Sample Efficient On-Line Learning of Optimal 
Dialogue Policies with Kalman Temporal 
Differences.  In Proceedings of IJCAI, 2011. 
A. Raux, B. Langner, D. Bohus, A. W Black, and M. 
Eskenazi, 2005. Let?s Go Public! Taking a Spoken 
Dialog System to the Real World. In Proceedings 
of Interspeech. 
J. Williams, 2013. Multi-domain learning and 
generalization in dialog state tracking. In 
Proceedings of SIGDIAL, 2013. 
J. Williams, A. Raux, D. Ramachandran and A. Black, 
2013. The Dialog State Tracking Challenge. In 
Proceedings of SIGDIAL, 2013. 
S. Young, M. Gasic, B. Thomson and J. Williams 
2013. POMDP-based Statistical Spoken Dialogue 
Systems: a Review. IEEE, 101(5):1160-1179. 
 
 
317
