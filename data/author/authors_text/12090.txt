Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 113?116,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
An Integrated Multi-document Summarization Approach based on 
Word Hierarchical Representation 
You Ouyang, Wenji Li, Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University  
{csyouyang,cswjli,csluqin}@comp.polyu.edu.hk 
 
Abstract 
This paper introduces a novel hierarchical 
summarization approach for automatic multi-
document summarization. By creating a 
hierarchical representation of the words in the 
input document set, the proposed approach is 
able to incorporate various objectives of multi-
document summarization through an 
integrated framework. The evaluation is 
conducted on the DUC 2007 data set. 
1 Introduction and Background 
Multi-document summarization requires creating 
a short summary from a set of documents which 
concentrate on the same topic. Sometimes an 
additional query is also given to specify the 
information need of the summary. Generally, an 
effective summary should be relevant, concise 
and fluent. It means that the summary should 
cover the most important concepts in the original 
document set, contain less redundant information 
and should be well-organized.  
Currently, most successful multi-document 
summarization systems follow the extractive 
summarization framework. These systems first 
rank all the sentences in the original document 
set and then select the most salient sentences to 
compose summaries for a good coverage of the 
concepts. For the purpose of creating more 
concise and fluent summaries, some intensive 
post-processing approaches are also appended on 
the extracted sentences. For example, 
redundancy removal (Carbonell and Goldstein, 
1998) and sentence compression (Knight and 
Marcu, 2000) approaches are used to make the 
summary more concise. Sentence re-ordering 
approaches (Barzilay et al, 2002) are used to 
make the summary more fluent. In most systems, 
these approaches are treated as independent steps. 
A sequential process is usually adopted in their 
implementation, applying the various approaches 
one after another. 
In this paper, we suggest a new summarization 
framework aiming at integrating multiple 
objectives of multi-document summarization. 
The main idea of the approach is to employ a 
hierarchical summarization process which is 
motivated by the behavior of a human 
summarizer. While the document set may be 
very large in multi-document summarization, the 
length of the summary to be generated is usually 
limited. So there are always some concepts that 
can not be included in the summary. A natural 
thought is that more general concepts should be 
considered first. So, when a human summarizer 
faces a set of many documents, he may follow a 
general-specific principle to write the summary. 
The human summarizer may start with finding 
the core topic in a document set and write some 
sentences to describe this core topic. Next he 
may go to find the important sub-topics and 
cover the subtopics one by one in the summary, 
then the sub-sub-topics, sub-sub-sub-topics and 
so on. By this process, the written summary can 
convey the most salient concepts. Also, the 
general-specific relation can be used to serve 
other objectives, i.e. diversity, coherence and etc.  
Motivated by this experience, we propose a 
hierarchical summarization approach which 
attempts to mimic the behavior of a human 
summarizer. The approach includes two phases. 
In the first phase, a hierarchical tree is 
constructed to organize the important concepts in 
a document set following the general-to-specific 
order. In the second phase, an iterative algorithm 
is proposed to select the sentences based on the 
constructed hierarchical tree with consideration 
of the various objectives of multi-document 
summarization. 
2 Word Hierarchical  Representation 
2.1 Candidate Word Identification 
As a matter of fact, the concepts in the original 
document set are not all necessary to be included 
in the summary. Therefore, before constructing 
the hierarchical representation, we first conduct a 
113
filtering process to remove the unnecessary 
concepts in the document set in order to improve 
the accuracy of the hierarchical representation. In 
this study, concepts are represented in terms of 
words. Two types of unnecessary words are 
considered. One is irrelevant words that are not 
related to the given query. The other is general 
words that are not significant for the specified 
document set. The two types of words are 
filtered through two features, i.e. query-
relevance and topic-specificity.  
The query-relevance of a word is defined as 
the proportion of the number of sentences that 
contains both the word and at least one query 
word to the number of sentences that contains the 
word. If a feature value is large, it means that the 
co-occurrence rate of the word and the query is 
high, thus it is more related to the query. The 
topic-specificity of a word is defined as the 
entropy of its frequencies in different document 
sets. If the feature value is large, it means that the 
word appears uniformly in document sets, so its 
significance to a specified document set is low. 
Thus, the words with very low query-relevance 
or with very high topic-specificity are filtered 
out1. 
2.2 Word Relation Identification and 
Hierarchical Representation 
To construct a hierarchical representation for the 
words in a given document set, we follow the 
idea introduced by Lawrie et al (2001) who use 
the subsuming relation to express the general-to-
specific structure of a document set. A 
subsumption is defined as an association of two 
words if one word can be regarded as a sub-
concept of the other one. In our approach, the 
pointwise mutual information (PMI) is used to 
identify the subsumption between words. 
Generally, two words with a high PMI is 
regarded as related. Using the identified relations, 
the word hierarchical tree is constructed in a top-
bottom manner. Two constraints are used in the 
tree construction process: 
(1) For two words related by a subsumption 
relation, the one which appears more frequently 
in the document set serves as the parent node in 
the tree and the other one serves as the child 
node. 
(2) For a word, its parent node in the hierarchical 
tree is defined as the most related word, which is 
identified by PMI.  
                                                 
1 Experimental thresholds are used on the evaluated data.  
2 http://duc.nist.gov/ 
The construction algorithm is detailed below. 
Algorithm 1: Hierarchical Tree Construction 
1: Sort the identified key words by their 
frequency in the document set in descending 
order, denoted as T = {t1, t2 ,?, tn} 
2: For each ti, i from 1 to n, find the most 
relevant word tj from all the words before ti in T, 
as Ti = {t1, t2 ,?, ti-1}. Here the relevance of two 
words is calculated by their PMI, i.e. 
)()(
*),(
log),(
ji
ji
ji tfreqtfreq
Nttfreq
ttPMI   
If the coverage rate of word ti by word tj 
2.0
)(
),(
)|( t 
i
ji
ji tfreq
ttfreq
ttP , ti is regarded as 
being subsumed by tj. Here freq(ti) is the 
frequency of ti in the document set and  freq(ti, 
ti) is the co-occurrence of ti and tj in the same 
sentences of the document set. N is the total 
number of tokens in the document set. 
4: After all the subsumption relations are found, 
the tree is constructed by connecting the related 
words from the first word t1. 
An example of a tree fragment is demonstrated 
below. The tree is constructed on the document 
set D0701A from DUC 20072, the query of this 
document set is ?Describe the activities of 
Morris Dees and the Southern Poverty Law 
Center?. 
 
3 Summarization based on Word 
Hierarchical Representation 
3.1 Word Significance Estimation 
In order to include the most significant concepts 
into the summary, before using the hierarchical 
tree to create an extract, we need to estimate the 
significance of the words on the tree first. 
Initially, a rough estimation of the significance of 
a word is given by its frequency in the document 
set. However, this simple frequency-based 
measure is obviously not accurate. One thing we 
observe from the constructed hierarchical tree is 
that a word which subsumes many other words is 
usually very important, though it may not appear 
Center 
Dee Law group 
Morris hatePoverty Southern
lawyer organizationcivil Klan
114
frequently in the document set. The reason is that 
the word covers many key concepts so it is 
dominant in the document set. Motivated by this, 
we develop a bottom-up algorithm which 
propagates the significance of the child nodes in 
the hierarchical tree backward to their parent 
nodes to boost the significance of nodes with 
many descendants. 
Algorithm 2: Word Scoring Theme 
1: Set the initial score of each word in T as its 
log-frequency, i.e. score(ti) =log freq(ti). 
2: For ti from n to 1, propagate an importance 
score to its parent node par(ti) (if exists) 
according to their relevance, i.e. score(par(ti)) = 
score(par(ti)) + log freq(ti, par(ti)).  
3.2 Sentence Selection  
Based on the word hierarchical tree and the 
estimated word significance, we propose an 
iterative algorithm to select sentences which is 
able to integrate the multiple objectives for 
composing a relevant, concise and fluent 
summary. The algorithm follows a general-to-
specific order to select sentences into the 
summary. In the implementation, the idea is 
carried out by following a top-down order to 
cover the words in the hierarchical tree. In the 
beginning, we consider several ?seed? words 
which are in the top-level of the tree (these 
words are regarded as the core concepts in the 
document set). Once some sentences have been 
extracted according to these ?seed? words, the 
algorithm moves to down-level words through 
the subsumption relations between the words. 
Then new sentences are added according to the 
down-level words and the algorithm continues 
moving to lower levels of the tree until the whole 
summary is generated. For the purpose of 
reducing redundancy, the words already covered 
by the extracted sentences will be ignored while 
selecting new sentences. To improve the fluency 
of the generated summary, after a sentence is 
selected, it is inserted to the position according to 
the subsumption relation between the words of 
this sentence and the sentences which are already 
in the summary. The detailed process of the 
sentence selection algorithm is described below. 
Algorithm 3: Summary Generation  
1: For the words in the hierarchical tree, set the 
initial states of the top n words3 as ?activated? 
and the states of other words as ?inactivated?. 
2: For all the sentences in the document set, 
                                                 
3 n is set to 3 experimentally on the evaluation data set. 
select the sentence with the largest score 
according to the ?activated? word set. The 
score of a sentence s is defined as 
? )(|| 1)( itscoressscore  where ti is a word 
belongs to s and the state of ti should be 
?activated?. | s | is the number of words in s. 
3: For the selected sentence sk, the subsumption 
relations between it and the existing sentences 
in the current summary are calculated and the 
most related sentence sl is selected. sk is then 
inserted to the position right behind sl. 
4: For each word ti belongs to the selected 
sentence sk, set its state to ?inactivated?; for 
each word tj which is subsumed by ti, set its 
state to ?activated?. 
5: Repeat step 2-4 until the length limit of the 
summary is exceeded. 
4 Experiment  
Experiments are conducted on the DUC 2007 
data set which contains 45 document sets. Each 
document set consists of 25 documents and a 
topic description as the query. In the task 
definition, the length of the summary is limited 
to 250 words. In our summarization system, pre-
processing includes stop-word removal and word 
stemming (conducted by GATE4). 
One of the DUC evaluation methods, ROUGE 
(Lin and Hovy, 2003), is used to evaluate the 
content of the generated summaries. ROUGE is a 
state-of-the-art automatic evaluation method 
based on N-gram matching between system 
summaries and human summaries. In the 
experiment, our system is compared to the top 
systems in DUC 2007. Moreover, a baseline 
system which considers only the frequencies of 
words but ignores the relations between words is 
included for comparison. Table 1 below shows 
the average recalls of ROUGE-1, ROUGE-2 and 
ROUGE-SU4 over the 45 DUC 2007document 
sets. In the experiment, the proposed 
summarization system outperforms the baseline 
system, which proves the benefit of considering 
the relations between words. Also, the system 
ranks the 6th among the 32 submitted systems in 
DUC 2007. This shows that the proposed 
approach is competitive. 
  ROUGE-1 ROUGE-2 ROUGE-SU4
S15 0.4451 0.1245 0.1771 
S29 0.4325 0.1203 0.1707 
S4 0.4342 0.1189 0.1699 
S24 0.4526 0.1179 0.1759 
                                                 
4 http://gate.ac.uk/ 
115
S13 0.4218 0.1117 0.1644 
Ours 0.4257 0.1110 0.1608 
Baseline 0.4088 0.1040 0.1542 
Table 1. ROUGE Evaluation Results 
To demonstrate the advantage of the proposed 
approach, i.e. its ability to incorporate multiple 
summarization objectives, the fragments of the 
generated summaries on the data set D0701A are 
also provided below as a case study. 
The summary produced by our system 
The Southern Poverty Law Center tracks hate 
groups, and Intelligence Report covers right-wing 
extremists. 
Morris Dees, co-founder of the Southern Poverty 
Law Center in Montgomery, Ala. 
Dees, founder of the Southern Poverty Law 
Center, has won a series of civil right suits against 
the Ku Klux Klan and other racist organizations in 
a campaign to drive them out of business. 
In 1987, Dees won a $7 million verdict against a 
Ku Klux Klan organization over the slaying of a 
19-year-old black man in Mobile, Ala. 
The summary produced by the baseline system
Morris Dees, co-founder of the Southern Poverty 
Law Center in Montgomery, Ala.  
The Southern Poverty Law Center tracks hate 
groups, and Intelligence Report covers right-wing 
extremists.  
The Southern Poverty Law Center previously 
recorded a 20-percent increase in hate groups 
from 1996 to 1997.  
The verdict was obtained by lawyers for the 
Southern Poverty Law Center, a nonprofit 
organization in Birmingham, Ala. 
Comparing the generated summaries of the 
two systems, we can see that the summary 
generated by the proposed approach is better in 
coherence and fluency since these factors are 
considered in the integrated summarization 
framework. Various summarization approaches, 
i.e. sentence ranking, redundancy removal and 
sentence re-ordering, are all implemented in the 
sentence selection algorithm based on the word 
hierarchical tree. However, we also observe that 
the proposed approach fails to generate better 
summaries on some document sets. The main 
problem is that the quality of the constructed 
hierarchical tree is not always satisfied. In the 
proposed summarization approach, we mainly 
rely on the PMI between the words to construct 
the hierarchical tree. However, a single PMI-
based measure is not enough to characterize the 
word relation. Consequently the constructed tree 
can not always well represent the concepts for 
some document sets. Another problem is that the 
two constraints used in the tree construction 
algorithm are not always right in real data. So we 
regard developing better tree construction 
approaches as of primary importance. Also, there 
are other places which can be improved in the 
future, such as the word significance estimation 
and sentence inserting algorithms. Nevertheless, 
we believe that the idea of incorporating the 
multiple summarization objectives into one 
integrated framework is meaningful and worth 
further study. 
5 Conclusion  
We introduced a summarization framework 
which aims at integrating various summarization 
objectives. By constructing a hierarchical tree 
representation for the words in the original 
document set, we proposed a summarization 
approach for the purpose of generating a relevant, 
concise and fluent summary. Experiments on 
DUC 2007 showed the advantages of the 
integrated framework.  
Acknowledgments 
The work described in this paper was partially 
supported by Hong Kong RGC Projects (No. 
PolyU 5217/07E) and partially supported by The 
Hong Kong Polytechnic University internal 
grants (A-PA6L and G-YG80). 
 
References  
R. Barzilay, N. Elhadad, and K. R. McKeown. 2002. 
Inferring strategies for sentence ordering in 
multidocument news summarization. Journal of 
Artificial Intelligence Research, 17:35-55, 2002. 
J. Carbonell and J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering 
Documents and Producing Summaries. In 
Proceedings of ACM SIGIR 1998, pp 335-336. 
K. Knight and D. Marcu. 2000. Statistics-based 
summarization --- step one: Sentence compression. 
In Proceeding of The American Association for 
Artificial Intelligence Conference (AAAI-2000), 
pp 703-710. 
D. Lawrie, W. B. Croft and A. Rosenberg. 2001. 
Finding topic words for hierarchical 
summarization. In Proceedings of ACM SIGIR 
2001, pp 349-357. 
C. Lin and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurance statistics. 
In Proc. of HLT-NAACL 2003, pp 71-78. 
116
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 134?142,
Beijing, August 2010
Simultaneous Ranking and Clustering of Sentences: A Reinforcement 
Approach to Multi-Document Summarization 
1Xiaoyan Cai, 1Wenjie Li, 1You Ouyang, 2Hong Yan 
1Department of Computing, The Hong Kong Polytechnic University 
{csxcai,cswjli,csyouyang}@comp.polyu.edu.hk 
2Department of Logistics and Maritime Studies, The Hong Kong Polytechnic University 
lgthyan@polyu.edu.hk 
 
 
Abstract 
Multi-document summarization aims to 
produce a concise summary that contains 
salient information from a set of source 
documents. In this field, sentence ranking 
has hitherto been the issue of most concern. 
Since documents often cover a number of 
topic themes with each theme represented 
by a cluster of highly related sentences, 
sentence clustering was recently explored in 
the literature in order to provide more 
informative summaries. Existing cluster-
based ranking approaches applied clustering 
and ranking in isolation. As a result, the 
ranking performance will be inevitably 
influenced by the clustering result. In this 
paper, we propose a reinforcement approach 
that tightly integrates ranking and clustering 
by mutually and simultaneously updating 
each other so that the performance of both 
can be improved. Experimental results on 
the DUC datasets demonstrate its 
effectiveness and robustness. 
1 Introduction 
Automatic multi-document summarization has 
drawn increasing attention in the past with the 
rapid growth of the Internet and information 
explosion. It aims to condense the original text 
into its essential content and to assist in 
filtering and selection of necessary information. 
So far extractive summarization that directly 
extracts sentences from documents to compose 
summaries is still the mainstream in this field. 
Under this framework, sentence ranking is the 
issue of most concern. 
Though traditional feature-based ranking 
approaches and graph-based approaches 
employed quite different techniques to rank 
sentences, they have at least one point in 
common, i.e., all of them focused on sentences 
only, but ignored the information beyond the 
sentence level (referring to Figure 1(a)). 
Actually, in a given document set, there 
usually exist a number of themes (or topics) 
with each theme represented by a cluster of 
highly related sentences (Harabagiu and 
Lacatusu, 2005; Hardy et al, 2002). These 
theme clusters are of different size and 
especially different importance to assist users 
in understanding the content in the whole 
document set. The cluster level information is 
supposed to have foreseeable influence on 
sentence ranking.  
 
Figure 1. Ranking vs. Clustering 
In order to enhance the performance of 
summarization, recently cluster-based ranking 
approaches were explored in the literature 
(Wan and Yang, 2006; Sun et al 2007; Wang 
et al 2008a,b; Qazvinian and Radev, 2008). 
Normally these approaches applied a clustering 
algorithm to obtain the theme clusters first and 
then ranked the sentences within each cluster 
or by exploring the interaction between 
sentences and obtained clusters (referring to 
Figure 1(b)). In other words, clustering and 
ranking are regarded as two independent 
processes in these approaches although the 
cluster-level information has been incorporated 
into the sentence ranking process. As a result, 
Ranking Ranking 
Clustering 
Ranking 
Clustering
(a)                           (b)                           (c) 
134
the ranking performance is inevitably 
influenced by the clustering result.  
To help alleviate this problem, we argue in 
this paper that the quality of ranking and 
clustering can be both improved when the two 
processes are mutually enhanced (referring to 
Figure 1(c)). Based on it, we propose a 
reinforcement approach that updates ranking 
and clustering interactively and iteratively to 
multi-document summarization. The main 
contributions of the paper are three-fold: (1) 
Three different ranking functions are defined 
in a bi-type document graph constructed from 
the given document set, namely global, within-
cluster and conditional rankings, respectively. 
(2) A reinforcement approach is proposed to 
tightly integrate ranking and clustering of 
sentences by exploring term rank distributions 
over the clusters. (3) Thorough experimental 
studies are conducted to verify the 
effectiveness and robustness of the proposed 
approach. 
The rest of this paper is organized as follows. 
Section 2 reviews related work in cluster-based 
ranking. Section 3 defines ranking functions 
and explains reinforced ranking and clustering 
process and its application in multi-document 
summarization. Section 4 presents experiments 
and evaluations. Section 5 concludes the paper.  
2 Related Work 
Clustering has become an increasingly 
important topic with the explosion of 
information available via the Internet. It is an 
important tool in text mining and knowledge 
discovery. Its ability to automatically group 
similar textual objects together enables one to 
discover hidden similarity and key concepts, as 
well as to summarize a large amount of text 
into a small number of groups (Karypis et al, 
2000).  
To summarize a scientific paper, Qazvinian 
and Radev (2008) presented two sentence 
selection strategies based on the clusters which 
were generated by a hierarchical 
agglomeration algorithm applied in the citation 
summary network. One was called C-RR, 
which started with the largest cluster and 
extracted the first sentence from each cluster in 
the order they appeared until the summary 
length limit was reached. The other was called 
C-LexRank, which was similar to C-RR but 
adopted LexRank to rank the sentences within 
each cluster and chose the most salient one. 
Meanwhile, Wan and Yang (2008) proposed 
two models to incorporate the cluster-level 
information into the process of sentence 
ranking for generic summarization. While the 
Cluster-based Conditional Markov Random 
Walk model (ClusterCMRW) incorporated the 
cluster-level information into the text graph 
and manipulated clusters and sentences equally, 
the Cluster-based HITS model (ClusterHITS) 
treated clusters and sentences as hubs and 
authorities in the HITS algorithm.  
Besides, Wang et al (2008) proposed a 
language model to simultaneously cluster and 
summarize documents. Nonnegative 
factorization was performed on the term-
document matrix using the term-sentence 
matrix as the base so that the document-topic 
and sentence-topic matrices could be 
constructed, from which the document clusters 
and the corresponding summary sentences 
were generated simultaneously. 
3 A Reinforcement Approach to 
Multi-document Summarization 
3.1 Document Bi-type Graph 
First of all, let?s introduce the sentence-term 
bi-type graph model for a set of given 
documents D, based on which the algorithm of 
reinforced ranking and clustering is developed. 
Let >=< WEVG ,, , where V is the set of 
vertices that consists of the sentence set 
},,,{ 21 nsssS ?=  and the term set 
},,{ 21 mtttT ,?= , i.e., TSV ?= , E is the set of 
edges that connect the vertices, i.e., 
},|,{ VvvvvE jiji ?><= . W is the adjacency 
matrix in which the element ijw  represents the 
weight of the edge connecting iv  and jv . 
Formally, W can be decomposed into four 
blocks, i.e., SSW , STW , TSW  and TTW , each 
representing a sub-graph of the textual objects 
indicated by the subscripts. W can be written as 
???
?
???
?=
TTTS
STSS
WW
WW
W ,       
where ),( jiWST  is the number of times the 
term jt  appears in the sentence is . )(i,jWSS  is 
135
the number of common terms in the sentences 
is  and js . TSW  is equal to 
T
STW  as the 
relationships between terms and sentences are 
symmetric. For simplification, in this study we 
assume there is no direct relationships between 
terms, i.e., 0=TTW . In the future, we will 
explore effective ways to integrate term 
semantic relationships into the model.  
3.2 Basic Ranking Functions 
Recall that our ultimate goal is sentence 
ranking. As an indispensable part of the 
approach, the basic ranking functions need to 
be defined first.  
3.2.1 Global Ranking (without Clustering) 
Let )( isr  (i=1, 2, ?, n) and )( jtr  (j=1, 2, ?, 
m) denote the ranking scores of the sentence is  
and the term jt  in the whole document set, 
respectively. Based on the assumptions that 
?Highly ranked terms appear in highly ranked 
sentences, while highly ranked sentences 
contain highly ranked terms. Moreover, a 
sentence is ranked higher if it contains many 
terms that appear in many other highly ranked 
sentences.? 
we define  
)(),()1()(),()(
11
j
n
j
SS
m
j
jSTi srjiWtrjiWsr ??
==
???+??= ?? (1) 
and  
)(),()(
1
i
n
i
TSj srijWtr ?
=
?= .      (2) 
For calculation purpose, )( isr  and )( jtr  are 
normalized by  
?
=
?
n
i
i
i
i
sr
sr
sr
1'
' )(
)(
)(  and 
?
=
?
m
j
j
j
j
tr
tr
tr
1'
' )(
)(
)( . 
Equations (1) and (2) can be rewritten using 
the matrix form, i.e.,  
???
???
?
?
?=
?
???+?
??=
||)(||
)(
)(
||)(||
)(
)1(
||)(||
)(
)(
SrW
SrW
Tr
SrW
SrW
TrW
TrW
Sr
TS
TS
SS
SS
ST
ST ??
. (3) 
We call )(Sr  and )(Tr  the ?global ranking 
functions?, because at this moment sentence 
clustering is not yet involved and all the 
sentences/terms in the whole document set are 
ranked together. 
Theorem: The solution to )(Sr  and )(Tr  
given by Equation (3) is the primary 
eigenvector of SSTSST WWW ??+?? )1( ??  and 
STSSTS WWIW ????? ?1))1(( ?? , respectively. 
Proof: Combine Equations (1) and (2), we get 
||)(||
)(
)1(
||)(||
)(
||)(||
)(
)1(
||
||)(||
)(
||
||)(||
)(
SrW
SrW
SrWW
SrWW
SrW
SrW
SrW
SrW
W
SrW
SrW
W
Sr
SS
SS
TSST
TSST
SS
SS
TS
TS
ST
TS
TS
ST
?
???+??
???=
?
???+
?
??
?
??
?=
??
??)(
 
As the iterative process is a power method, 
it is guaranteed that )(Sr  converges to the 
primary eigenvector of +?? TSST WW?  
SSW?? )1( ? . Similarly,  )(Tr  is guaranteed to 
converge to the primary eigenvector of 
STSSTS WWIW ????? ?1))1(( ?? .                      ? 
3.2.2 Local Ranking (within Clusters) 
Assume now K theme clusters have been 
generated by certain clustering algorithm, 
denoted as },,,{ 21 KCCCC ?=  where kC  (k=1, 
2, ?, K) represents a cluster of highly related 
sentences )( kC CS k ?  which contain the terms 
)( kC CT k ? . The sentences and terms within 
the cluster kC  form a cluster bi-type graph 
with the adjacency matrix 
kCW . Let )( kk CC Sr  
and )(
kk CC Tr  denote the ranking scores of kCS  
and 
kCT  within kC . They are calculated by an 
equation similar to Equation (3) by replacing 
the document level adjacency matrix W  with 
the cluster level adjacency matrix 
kCW . We 
call )(
kk CC Sr  and )( kk CC Tr  the ?within-
cluster ranking functions? with respect to the 
cluster kC . They are the local ranking 
functions, in contrast to )(Sr  and )(Tr  that 
rank all the sentences and terms in the whole 
document set D. We believe that it will benefit 
sentence overall ranking when knowing more 
details about the ranking results at the finer 
granularity of theme clusters, instead of at the 
coarse granularity of the whole document set. 
136
3.2.3 Conditional Ranking (across Clusters) 
To facilitate the discovery of rank distributions 
of terms and sentences over all the theme 
clusters, we further define two ?conditional 
ranking functions? )|( kCSr  and )|( kCTr . 
These rank distributions are necessary for the 
parameter estimation during the reinforcement 
process introduced later. The conditional 
ranking score of the term jt  on the cluster kC , 
i.e., )|( kCTr  is directly derived from kCT , i.e., 
=)|( kj Ctr )( jC tr k  if kj Ct ? , and 0)|( =kj Ctr  
otherwise. It is further normalized as  
? =
=
m
j kj
kj
kj
Ctr
Ctr
Ctr
1
)|(
)|(
)|( .   (4) 
Then the conditional ranking score of the 
sentence is  on the cluster kC  is deduced from 
the terms that are included in is , i.e.,  
? ?
?
= =
=
?
?
=
n
i
m
j kjST
m
j kjST
ki
CtrjiW
CtrjiW
Csr
1 1
1
)|(),(
)|(),(
)|( . (5) 
Equation (5) can be interpreted as that the 
conditional rank of is  on kC  is higher if many 
terms in is  are ranked higher in kC . Now we 
have sentence and term conditional ranks over 
all the theme clusters and are ready to 
introduce the reinforcement process.  
3.3 Reinforcement between Within-
Cluster Ranking and Clustering  
The conditional ranks of the term jt  across the 
K theme clusters can be viewed as a rank 
distribution. Then the rank distribution of the 
sentence is  can be considered as a mixture 
model over K conditional rank distributions of 
the terms contained in the sentence is . And the 
sentence is  can be represented as a K-
dimensional vector in the new measure space, 
in which the vectors can be used to guide the 
sentence clustering update. Next, we will 
explain the mixture model of sentence and use 
EM algorithm (Bilmes, 1997) to get the 
component coefficients of the model. Then, we 
will present the similarity measure between 
sentence and cluster, which is used to adjust 
the clusters that the sentences belong to and in 
turn modify within-cluster ranking for the 
sentences in the updated clusters.  
3.3.1 Sentence Mixture Model  
For each sentence  is , we assume that it 
follows the distribution )|( isTr  to generate the 
relationship between the sentence is  and the 
term set T. This distribution can be considered 
as a mixture model over K component 
distributions, i.e. the term conditional rank 
distributions across K theme clusters. We use 
ki,?  to denote the probability that is  belongs 
to kC , then )|( isTr  can be modeled as: 
?
=
?=
K
k
kki CTrsTr
1
i, )|()|( ?  and ?
=
=
K
k
k
1
i, 1? . (6) 
ki,?  can be explained as )|( ik sCp  and 
calculated by the Bayesian equation 
?? )|()|( kiik CspsCp )( kCp , where )|( ki Csp  
is assumed to be )|( ki Csr  obtained from the 
conditional rank of is  on kC  as introduced 
before and )( kCp  is the prior probability. 
3.3.2 Parameter Estimation 
We use EM algorithm to estimate the 
component coefficients ki,?  along with 
)}({ kCp . A hidden variable zC , },,2,1{ Kz ??  
is used to denote the cluster label that a 
sentence term pair ),( ji ts  are from. In addition, 
we make the independent assumption that the 
probability of is  belonging to kC  and the 
probability of jt  belonging to kC  are 
independent, i.e., ?= )|()|,( kikji CspCtsp  
)|( kj Ctp , where )|,( kji Ctsp is the probability 
of is  and jt  both belonging to kC . Similarly, 
)|( kj Ctp  is assumed to be )|( kj Ctr . 
Let ?  be the parameter matrix, which is a 
Kn?  matrix }{ ,kiKn ?=? ?  ;,,1( ni ?=  
),,1 Kk ?= . The best ?  is estimated from the 
relationships observed in the document bi-type 
graph, i.e., STW  and SSW . The likelihood of 
generating all the relationships under the 
parameter ?  can be calculated as:  
????
= == =
???=
???=?
n
i
n
j
jiW
ji
n
i
m
j
jiW
ji
SSSTSSST
SSST ssptsp
WpWpWWL
1 1
),(
1 1
),(
'
)|,()|,(
)|()|(),|(
 
137
where )|,( ?ji tsp  is the probability that is  
and jt  both belong to the same cluster, given 
the current parameter. As )|,( ?ji ssp  does not 
contain variables from ? , we only need to 
consider maximizing the first part of the 
likelihood in order to get the best estimation of 
? . Let )|( STWL ?  be the first part of 
likelihood.  
Taking into account the hidden variable zC , 
the complete log-likelihood can be written as  
( )
( )
( )??
??
??
= =
= =
= =
???=
???=
?=?
n
i
m
j
zjiZST
n
i
m
j
zzji
n
i
m
j
jiW
zjiZST
CptspjiW
CpCtsp
CtspCWL
jiSTW
ST
1 1
1 1
1 1
),(
)|(),(log),(
)|(),|,(log
)|,,(log),|(log
),( . 
In the E-step, given the initial parameter 0? , 
which is set to Kki
10
, =?  for all i and k, the 
expectation of log-likelihood under the current 
distribution of ZC  is: 
???
???
= = =
= = =
?
?=??=?
+?=??=
?=??
n
i
K
k
m
j
jikzkzST
K
k
n
i
m
j
jikzjikST
ZSTWCf
tsCCpCCpjiW
tsCCptspjiW
CWLEQ
STZ
1 1 1
0
1 1 1
0
),|(
0
),,|())|(log(),(
),,|()),(log(),(
),|((log),( 0
 
The conditional distribution in the above 
equation, i.e., ),,|( 0?= jikz tsCCp , can be 
calculated using the Bayesian rule as follows: 
)()|()|(
)|(),|,(
),,|(
000
00
0
kzkjki
kzkzji
jikz
CCpCtpCsp
CCpCCtsp
tsCCp
=?
?=?=?
?=
. (7) 
In the M-Step, we first get the estimation of 
)( kz CCp =  by maximizing the expectation 
),( 0??Q . By introducing a Lagrange 
multiplier ? , we get the equation below. 
?=?=+??=?
? ?
=
0)]1)((),([
)( 1
0
K
k
kz
kz
CCpQ
CCp
?
??
= =
=+?==
n
i
m
j
jikz
kz
ST tsCCpCCp
jiW
1 1
0 0),,|(
)(
1
),( ?  
Thus, the estimation of )( kz CCp =  given 
previous 0?  is  
??
??
= =
= =
?=
==
n
i
m
j
ST
n
i
m
j
jikzST
kz
jiW
tsCCpjiW
CCp
1 1
1 1
0
),(
),,|(),(
)( . (8) 
Then, the parameters ki,?  can be calculated 
with the Bayesian rule as 
?
=
=
==
K
l
lzli
kzki
ki
CCpCsp
CCpCsp
1
,
)()|(
)()|(? .  (9) 
By setting ?=?0 , the whole process can 
be repeated. The updating rules provided in 
Equations (7)-(9) are applied at each iteration. 
Finally ?  will converge to a local maximum. 
A similar estimation process has been adopted 
in (Sun et al, 2009), which was used to 
estimate the component coefficients for author-
conference networks.  
3.3.3 Similarity Measure 
After we get the estimations of the component 
coefficients ki,?  for is  , is  will be represented 
as a K dimensional vector ,,,( 2,1, ?iiis ??=  
),Ki? . The center of each cluster can thus be 
calculated accordingly, which is the mean of 
is  for all is  in the same cluster, i.e., 
|| k
Cs
i
C
C
s
Center kik
?
?= ,      
where || kC  is the size of kC .  
Then the similarity between each sentence 
and each cluster can be calculated as the cosine 
similarity between them, i.e.,  
??
?
==
==
K
l C
K
l i
K
l Ci
ki
lCenterls
lCenterls
Cssim
k
k
1
2
1
2
1
))())(
)()(
),( . (10) 
Finally, each sentence is re-assigned to a 
cluster that is the most similar to the sentence. 
Based on the updated clusters, within-cluster 
ranking is updated accordingly, which triggers 
the next round of clustering refinement. It is 
expected that the quality of clusters should be 
improved during this iterative update process 
since the similar sentences under new 
attributes will be grouped together, and 
meanwhile the quality of ranking will be 
improved along with the better clusters and 
138
thus offers better attributes for further 
clustering.  
3.4 Ensemble Ranking 
The overall sentence ranking function f is 
defined as the ensemble of all the sentence 
conditional ranking scores on the K clusters.  
?
=
?=
K
k
kiki Csrsf
1
)|()( ? ,  (11) 
where k?  is a coefficient evaluating the 
importance of kC . It can be formulated as the 
normalized cosine similarity between a theme 
cluster and the whole document set for generic 
summarization, or between a theme cluster and 
a given query for query-based summarization. 
]1,0[?k?  and ?
=
=
K
k
k
1
1? . 
Figure 2 below summarizes the whole 
process that determines the overall sentence 
ensemble ranking scores.  
Input: The bi-type document graph >=< WETSG ,,? , 
ranking functions, the cluster number K, 1=? , 
001.0=Tre , 10=IterNum . 
Output: sentence final ensemble ranking vector )(Sf . 
1. 0?t ; 
2. Get the initial partition for S, i.e. tkC , Kk ?,2,1= , 
calculate cluster centers t
kC
Center accordingly.  
3. For (t=1; t<IterNum && Tre>? ; t++) 
4.     Calculate the within-cluster ranking )(
kk CC Tr
, 
)(
kCkC
Sr  and the conditional ranking )|( ki Csr ; 
5.     Get new attribute is  for each sentence is , and 
new attribute t
kCCenter  for each cluster 
t
kC ; 
6.     For each sentence is in S 
7.          For k=1 to K 
8.               Calculate similarity value ),( tki Cssim  
9.          End For 
10.        Assign is to 10
+t
kC , ),(maxarg0
t
kik Cssimk =  
11.   End For 
12.   ||max 1 t
kC
t
kCk
CenterCenter ?= +?  
13.   1+? tt  
14. End For 
15. For each sentence is  in S 
16.        For k=1 to K 
17.             ?
=
?=
K
k
kiki Csrsf
1
)|()( ?  
18.        End For 
19. End For 
Figure 2. The Overall Sentence Ranking Algorithm  
3.5 Summary Generation 
In multi-document summarization, the number 
of documents to be summarized can be very 
large. This makes information redundancy 
appears to be more serious in multi-document 
summarization than in single-document 
summarization. Redundancy control is 
necessary. We apply a simple yet effective 
way to choose summary sentences. Each time, 
we compare the current candidate sentence to 
the sentences already included in the summary. 
Only the sentence that is not too similar to any 
sentence in the summary (i.e., the cosine 
similarity between them is lower than a 
threshold) is selected into the summary. The 
iteration is repeated until the length of the 
sentences in the summary reaches the length 
limitation. In this paper, the threshold is set to 
0.7 as always in our past work. 
4 Experiments and Evaluations 
We conduct the experiments on the DUC 2004 
generic multi-document summarization dataset 
and the DUC 2006 query-based multi-
document summarization dataset. According to 
task definitions, systems are required to 
produce a concise summary for each document 
set (without or with a given query description) 
and the length of summaries is limited to 665 
bytes in DUC 2004 and 250 words in DUC 
2006. 
A well-recognized automatic evaluation 
toolkit ROUGE (Lin and Hovy, 2003) is used 
in evaluation. It measures summary quality by 
counting overlapping units between system-
generated summaries and human-written 
reference summaries. We report two common 
ROUGE scores in this paper, namely ROUGE-
1 and ROUGE-2, which base on Uni-gram 
match and Bi-gram match, respectively. 
Documents and queries are pre-processed by 
segmenting sentences and splitting words. Stop 
words are removed and the remaining words 
are stemmed using Porter stemmer.  
4.1 Evaluation of Performance  
In order to evaluate the performance of 
reinforced clustering and ranking approach, we 
compare it with the other three ranking 
approaches: (1) Global-Rank, which does not 
apply clustering and simply relies on the 
139
sentence global ranking scores to select 
summary sentences; (2) Local-Rank, which 
clusters sentences first and then rank sentences 
within each cluster. A summary is generated in 
the same way as presented in (Qazvinian and 
Radev, 2008). The clusters are ordered by 
decreasing size; (3) Cluster-HITS, which also 
clusters sentences first, but then regards 
clusters as hubs and sentences as authorities in 
the HITS algorithm and uses the obtained 
authority scores to rank and select sentences. 
The classical clustering algorithm K-means is 
used where necessary. For query-based 
summarization, the additional query-relevance 
(i.e. the cosine similarity between sentences 
and query) is involved to re-rank the candidate 
sentences chosen by the ranking approaches 
for generic summarization. 
Note that K-means requires a predefined 
cluster number K. To avoid exhaustive search 
for a proper cluster number for each document 
set, we employ the spectra approach 
introduced in (Li et al, 2007) to predict the 
number of the expected clusters. Based on the 
sentence similarity matrix using the 
normalized 1-norm, for its eigenvalues i?  
(i=1,2, ?, n), the ratio )1(/ 21 ?= + ???? ii   is 
defined. If 05.01 >? +ii ??  and i?  is still close 
to 1, then set K=i+1. Tables 1 and 2 below 
compare the performance of the four 
approaches on DUC 2004 and 2006 according 
to the calculated K.  
DUC 2004 ROUGE-1 ROUGE-2 
Reinforced 0.37082 0.08351 
Cluster-HITS 0.36463 0.07632 
Local-Rank 0.36294 0.07351 
Global-Rank 0.35729 0.06893 
Table 1. Results on the DUC 2004 dataset 
DUC 2006 ROUGE-1 ROUGE-2 
Reinforced 0.39531 0.08957 
Cluster-HITS 0.38315 0.08632 
Local-Rank 0.38104 0.08841 
Global-Rank 0.37478 0.08531 
Table 2. Results on the DUC 2006 dataset 
It is not surprised to find that ?Global-Rank? 
shows the poorest performance, when it 
utilizes the sentence level information only 
whereas the other three approaches all 
integrate the additional cluster level 
information in various ways. In addition, as 
results illustrate, the performance of ?Cluster-
HITS? is better than the performance of 
?Local-Rank?. This can be mainly credited to 
the ability of ?Cluster-HITS? to consider not 
only the cluster-level information, but also the 
sentence-to-cluster relationships, which are 
ignored in ?Local-Rank?. It is happy to see that 
the proposed reinforcement approach, which 
simultaneously updates clustering and ranking 
of sentences, consistently outperforms the 
other three approaches. 
4.2 Analysis of Cluster Quality 
Our original intention to propose the 
reinforcement approach is to hope to generate 
more accurate clusters and ranking results by 
mutually refining within-cluster ranking and 
clustering. In order to check and monitor the 
variation trend of the cluster quality during the 
iterations, we define the following measure 
?
?=
?= ??
?=
K
k
K
kll
ji
CsCs
ki
Cs
sssim
Cssim
quan
ljki
ki
1
,1 ,
)
),(min
),(min
( , (12) 
where ),(min ki
Cs
Cssim
ki?
 denotes the distance 
between the cluster center and the border 
sentence in a cluster that is the farthest away 
from the center. The larger it is, the more 
compact the cluster is. ),(min
,
ji
CsCs
sssim
ljki ??
, on 
the other hand, denotes the distance between 
the most distant pair of sentences, one from 
each cluster. The smaller it is, the more 
separated the two clusters are. The distance is 
measured by cosine similarity. As a whole, the 
larger quan means the better cluster quality. 
Figure 3 below plots the values of quan in each 
iteration on the DUC 2004 and 2006 datasets. 
Note that the algorithm converges in less than 
6 rounds and 5 rounds on the DUC 2004 and 
2006 datasets, respectively. The curves clearly 
show the increasment of quan and thus the 
improved cluster quality. 
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
7
7.5
1 2 3 4 5 6IterNum
Q
ua
n
DUC2004 DUC2006
 
Figure 3. Cluster Quality on DUC 2004 and 2006  
140
While quan directly evaluate the quality of 
the generated clusters, we are also quite 
interested in whether the improved clusters 
quality can further enhance the quality of 
sentence ranking and thus consequently raise 
the performance of summarization. Therefore, 
we evaluate the ROUGEs in each iteration as 
well. Figure 4 below illustrates the changes of 
ROUGE-1 and ROUGE-2 result on the DUC 
2004 and 2006 datasets, respectively. Now, we 
have come to the positive conclusion. 
0.29
0.3
0.31
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.4
1 2 3 4 5 6
IterNum
R
O
U
G
E
-1
DUC2004 DUC2006
0.045
0.05
0.055
0.06
0.065
0.07
0.075
0.08
0.085
0.09
0.095
1 2 3 4 5 6
IterNum
R
O
U
G
E-
2
 
Figure 4. ROUGEs on DUC 2004 and 2006  
4.3 Impact of Cluster Numbers 
In previous experiments, the cluster number is 
predicted through the eigenvalues of 1-norm 
normalized sentence similarity matrix. This 
number is just the estimated number. The 
actual number is hard to predict accurately. To 
further examine how the cluster number 
influences summarization, we conduct the 
following additional experiments by varying 
the cluster number. Given a document set, we 
let S denote the sentence set in the document 
set, and set K in the following way: 
|| SK ?= ? ,   (13) 
where )1,0(??  is a ratio controlling the 
expected cluster number. The larger ?  is, the 
more clusters will be produced. ?  ranges from 
0.1 to 0.9 in the experiments. Due to page 
limitation, we only provide the ROUGE-1 and 
ROUGE-2 results of the proposed approach, 
?Cluster-HITS? and ?Local-Rank? on the DUC 
2004 dataset in Figure 5. The similar curves 
are also observed on the 2006 dataset. 
0.355
0.36
0.365
0.37
0.375
0.38
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E
-1
Cluster-HITS Local Rank Reinforced
?  
0.072
0.075
0.078
0.081
0.084
0.087
0.09
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E
-2
?  
Figure 5. ROUGEs vs.? on DUC 2004 
It is shown that (1) the proposed approach 
outperforms ?Cluster-HITS? and ?Local-
Rank? in almost all the cases no matter how 
the cluster number is set; (2) the performances 
of ?Cluster-HITS? and ?Local-Rank? are more 
sensitive to the cluster number and a large 
number of clusters appears to deteriorate the 
performances of both. This is reasonable. 
Actually when ?  getting close to 1, ?Local-
Rank? approaches to ?Global-Rank?. These 
results demonstrate the robustness of the 
proposed approach. 
5 Conclusion 
In this paper, we present a reinforcement 
approach that tightly integrates ranking and 
clustering together by mutually and 
simultaneously updating each other. 
Experimental results demonstrate the 
effectiveness and the robustness of the 
proposed approach. In the future, we will 
explore how to integrate term semantic 
relationships to further improve the 
performance of summarization. 
Acknowledgement 
The work described in this paper was 
supported by an internal grant from the Hong 
Kong Polytechnic University (G-YG80). 
 
141
References 
J. Bilmes. 1997. A Gentle Tutorial on the em 
Algorithm and Its Application to Parameter 
Wstimation for Gaussian Mixture and Hidden 
Markov Models. Technical Report ICSI-TR-97-
02, University of Berkeley. 
Brin, S., and Page, L. 1998. The Anatomy of a 
Large-scale Hypertextual Web Search Engine. In 
Proceedings of WWW1998.. 
Harabagiu S. and Lacatusu F. 2005. Topic Themes 
for Multi-Document Summarization. In 
Proceedings of SIGIR2005. 
Hardy H., Shimizu N., Strzalkowski T., Ting L., 
Wise G. B., and Zhang X. 2002. Cross-
Document Summarization by Concept 
Classification. In Proceedings of SIGIR2002. 
Jon M. Kleinberg. 1999. Authoritative Sources in a 
Hyperlinked Environment. In Proceedings of the 
9th ACM-SIAM Symposium on Discrete 
Algorithms.  
Karypis, George, Vipin Kumar and Michael 
Steinbach. 2000. A Comparison of Document 
Clustering Techniques. KDD workshop on Text 
Mining. 
Lin, C. Y. and Hovy, E. 2000. The Automated 
Acquisition of Topic Signature for Text 
Summarization. In Proceedings of COLING2000.  
Li W.Y., Ng W.K., Liu Y.  and Ong K.L. 2007. 
Enhancing the Effectiveness of Clustering with 
Spectra Analysis. IEEE Transactions on 
Knowledge and Data Engineering (TKDE). 
19(7): 887-902.  
Li, F., Tang, Y., Huang, M., Zhu, X. 2009. 
Answering Opinion Questions with Random 
Walks on Graphs. In Proceedings of ACL2009. 
Otterbacher J., Erkan G. and Radev D. 2005. Using 
RandomWalks for Question-focused Sentence 
Retrieval. In Proceedings of HLT/EMNLP 2005. 
Qazvinian  V. and Radev D. R. 2008. Scientific 
paper summarization using citation summary 
networks. In Proceedings of COLING2008. 
Sun P., Lee J.H., Kim D.H., and Ahn C.M. 2007. 
Multi-Document Using Weighted Similarity 
Between Topic and Clustering-Based Non-
negative Semantic Feature. APWeb/WAIM 
2007. 
Sun Y., Han J., Zhao P., Yin Z., Cheng H., and Wu 
T. 2009. Rankclus: Integrating Clustering with  
 
Ranking for Heterogenous Information Network 
Analysis. In Proceedings of EDBT 2009. 
Wang D.D., Li T., Zhu S.H., Ding Chris. 2008a 
Multi-Document Summarization via Sentence-
Level Semantic Analysis and Symmetric Matrix 
Factorization. In Proceedings of SIGIR2008. 
Wang D.D., Zhu S.H., Li T., Chi Y., and Gong Y.H. 
2008b. Integrating Clustering and Multi-
Document Summarization to Improve Document 
Understanding. In Proceedings of CIKM 2008. 
Wan X. and Yang J. 2006. Improved Affinity Graph 
based Multi-Document Summarization. In 
Proceedings of HLT-NAACL2006. 
Zha H. 2002. Generic Summarization and Key 
Phrase Extraction using Mutual Reinforcement 
Principle and Sentence Clustering. In 
Proceedings of SIGIR2002. 
142
Coling 2010: Poster Volume, pages 919?927,
Beijing, August 2010
A Study on Position Information in Document Summarization 
You Ouyang       Wenjie Li       Qin Lu       Renxian Zhang 
Department of Computing, the Hong Kong Polytechnic University 
{csyouyang,cswjli,csluqin,csrzhang}@comp.polyu.edu.hk 
Abstract 
Position information has been proved to 
be very effective in document 
summarization, especially in generic 
summarization. Existing approaches 
mostly consider the information of 
sentence positions in a document, based 
on a sentence position hypothesis that 
the importance of a sentence decreases 
with its distance from the beginning of 
the document. In this paper, we consider 
another kind of position information, i.e., 
the word position information, which is 
based on the ordinal positions of word 
appearances instead of sentence 
positions. An extractive summarization 
model is proposed to provide an 
evaluation framework for the position 
information. The resulting systems are 
evaluated on various data sets to 
demonstrate the effectiveness of the 
position information in different 
summarization tasks. Experimental 
results show that word position 
information is more effective and 
adaptive than sentence position 
information. 
1 Introduction 
Position information has been frequently used in 
document summarization. It springs from 
human?s tendency of writing sentences of 
greater topic centrality at particular positions in 
a document. For example, in newswire 
documents, topic sentences are usually written 
earlier. A sentence position hypothesis is then 
given as: the first sentence in a document is the 
most important and the importance decreases as 
the sentence gets further away from the 
beginning. Based on this sentence position 
hypothesis, sentence position features are 
defined by the ordinal position of sentences. 
These position features have been proved to be 
very effective in generic document 
summarization. In more recent summarization 
tasks, such as query-focused and update 
summarization tasks, position features are also 
widely used.  
Although in these tasks position features may 
be used in different ways, they are all based on 
the sentence position hypothesis. So we regard 
them as providing the sentence position 
information. In this paper, we study a new kind 
of position information, i.e., the word position 
information. The motivation of word position 
information comes from the idea of assigning 
different importance to multiple appearances of 
one word in a document.  
As to many language models such as the bag-
of-words model, it is well acknowledged that a 
word which appears more frequently is usually 
more important. If we take a closer look at all 
the appearances of one word, we can view this 
as a process that the different appearances of the 
same word raise the importance of each other. 
Now let?s also take the order of the appearances 
into account. When reading a document, we can 
view it as a word token stream from the first 
token to the last. When a new token is read, we 
attach more importance to previous tokens that 
have the same lemma because they are just 
repeated by the new token. Inspired by this, we 
postulate a word position hypothesis here: for 
all the appearances of a fixed word, the 
importance of each appearance depends on all 
its following appearances. Therefore, the first 
appearance of a word is the most important and 
the importance decreases with the ordinal 
919
positions of the appearances. Then, a novel kind 
of position features can be defined for the word 
appearances based on their ordinal positions. 
We believe that these word position features 
have some advantages when compared to 
traditional sentence position features. According 
to the sentence position hypothesis, sentence 
position features generally prefer earlier 
sentences in a document. As to the word 
position features that attempt to differentiate 
word appearances instead of sentences, a 
sentence which is not the first one in the 
document may still not be penalized as long as 
its words do not appear in previous sentences. 
Therefore, word position features are able to 
discover topic sentences in deep positions of the 
document. On the other hand, the assertion that 
the first sentence is always the most important is 
not true in actual data. It depends on the writing 
style indeed. For example, some authors may 
like to write some background sentences before 
topic sentences. In conclusion, we can expect 
word position features  to be more adaptive to 
documents with different structures.  
In the study of this paper, we define several 
word position features based on the ordinal 
positions of word appearances. We also develop 
a word-based summarization system to evaluate 
the effectiveness of the proposed word position 
features on a series of summarization data sets. 
The main contributions of our work are: 
(1) representation of word position information, 
which is a new kind of position information in 
document summarization area. 
(2) empirical results on various data sets that 
demonstrate the impact of position information 
in different summarization tasks. 
2 Related Work 
The use of position information in document 
summarization has a long history. In the seminal 
work by (Luhn, 1958), position information was 
already considered as a good indicator of 
significant sentences. In (Edmundson, 1969), a 
location method was proposed that assigns 
positive weights to the sentences to their ordinal 
positions in the document. Position information 
has since been adopted by many successful 
summarization systems, usually in the form of 
sentence position features. For example, Radev 
et al (2004) developed a feature-based system 
MEAD based on word frequencies and sentence 
positions. The position feature was defined as a 
descending function of the sentence position. 
The MEAD system performed very well in the 
generic multi-document summarization task of 
the DUC 2004 competition. Later, position 
information is also applied to more 
summarization tasks. For example, in query-
focused task, sentence position features are 
widely used in learning-based summarization 
systems as a component feature for calculating 
the composite sentence score (Ouyang et al 
2007; Toutanova et al 2007). However, the 
effect of position features alone was not studied 
in these works.  
There were also studies aimed at analyzing 
and explaining the effectiveness of position 
information. Lin and Hovy (1997) provided an 
empirical validation on the sentence position 
hypothesis. For each position, the sentence 
position yield was defined as the average value 
of the significance of the sentences with the 
fixed position. It was observed that the average 
significance at earlier positions was indeed 
larger. Nenkova (2005) did a conclusive 
overview on the DUC 2001-2004 evaluation 
results. It was reported that position information 
is very effective in generic summarization. In 
generic single-document summarization, a lead-
based baseline that simply takes the leading 
sentences as the summary can outperform most 
submitted summarization system in DUC 2001 
and 2002. As in multi-document summarization, 
the position-based baseline system is 
competitive in generating short summaries but 
not in longer summaries. Schilder and 
Kondadadi (2008) analyzed the effectiveness of 
the features that are used in their learning-based 
sentence scoring model for query-focused 
summarization. By comparing the ROUGE-2 
results of each individual feature, it was 
reported that position-based features are less 
effective than frequency-based features. In 
(Gillick et al, 2009), the effect of position 
information in the update summarization task 
was studied. By using ROUGE to measure the 
density of valuable words at each sentence 
position, it was observed that the first sentence 
of newswire document was especially important 
for composing update summaries. They defined 
a binary sentence position feature based on the 
920
observation and the feature did improve the 
performance on the update summarization data. 
3 Methodology 
In the section, we first describe the word-based 
summarization model. The word position 
features are then defined and incorporated into 
the summarization model. 
3.1 Basic Summarization Model 
To test the effectiveness of position information 
in document summarization, we first propose a 
word-based summarization model for applying 
the position information. The system follows a 
typical extractive style that constructs the target 
summary by selecting the most salient sentences.  
Under the bag-of-words model, the 
probability of a word w in a document set D can 
be scaled by its frequency, i.e., p(w)=freq(w)/|D|, 
where freq(w) indicates the frequency of w in D 
and |D| indicates the total number of words in D. 
The probability of a sentence s={w1, ?, wN} is 
then calculated as the product of the word 
probabilities, i.e., p(s)=?i p(wi). Moreover, the 
probability of a summary consisting a set of 
sentences, denoted as S={s1, ?, sM}, can be 
calculated by the product of the sentence 
probabilities, i.e., p(S)=?j p(sj). To obtain the 
optimum summary, an intuitive idea is to select 
the sentences to maximize the overall summary 
probability p(S), equivalent to maximizing 
log(p(S)) = ?j?i log(p(wji)) = ?j?i (logfreq(wji)- 
log|D|) = ?j?i log freq(wji) - |S|?log |D|,  
where wji indicates the ith word in sj and |S| 
indicates the total number of words in S. As to 
practical summarization tasks, a maximum 
summary length is usually postulated. So here 
we just assume that the length of the summary 
is fixed. Then, the above optimization target is 
equivalent to maximizing ?j?i logfreq(wji). 
From the view of information theory, the sum 
can also be interpreted as a simple measure on 
the total information amount of the summary. In 
this interpretation, the information of a single 
word wji is measured by log freq(wji) and the 
summary information is the sum of the word 
information. So the optimization target can also 
be interpreted as including the most informative 
words to form the most informative summary 
given the length limit.  
In extractive summarization, summaries are 
composed by sentence selection. As to the 
above optimization target, the sentence scoring 
function for ranking the sentences should be 
calculated as the average word information, i.e., 
score(s) = ?i log freq(wi) / |s|. 
After ranking the sentences by their ranking 
scores, we can select the sentences into the 
summary by the descending order of their score 
until the length limit is reached. By this process, 
the summary with the largest  p(S) can be 
composed.  
3.2 Word Position Features 
With the above model, word position features 
are defined to represent the word position 
information and are then incorporated into the 
model. According to the motivation, the features 
are defined by the ordinal positions of word 
appearances, based on the position hypothesis 
that earlier appearances of a word are more 
informative. Formally, for the ith appearance 
among the total n appearances of a word w, four 
position features are defined based on i and n 
using different formulas as described below. 
(1) Direct proportion (DP) With the word 
position hypothesis, an intuitive idea is to regard 
the information degree of the first appearance as 
1 and the last one as 1/n, and then let the degree 
decrease linearly to the position i. So we can 
obtain the first position feature defined by the 
direct proportion function, i.e., f(i)=(n-i+1)/n. 
(2) Inverse proportion (IP). Besides the linear 
function, other functions can also be used to 
characterize the relationship between the 
position and the importance. The second 
position feature adopts another widely-used 
function, the inversed proportion function, i.e., 
f(i)=1/i. This measure is similar to the above 
one, but the information degree decreases by the 
inverse proportional function. Therefore, the 
degree decreases more quickly at smaller 
positions, which implies a stronger preference 
for leading sentences. 
(3) Geometric sequence (GS). For the third 
feature, we make an assumption that the degree 
of every appearance is the sum of the degree of 
all the following appearances, i.e., f(i) = f(i+1)+ 
f(i+2)+?+ f(n). It can be easily derived that the 
sequence also satisfies f(i) = 2?f(i-1). That is, the 
information degree of each new appearance is 
921
halved. Then the feature value of the ith 
appearance can be calculated as f(i) = (1/2)i-1.  
(4) Binary function (BF). The final feature is a 
binary position feature that regards the first 
appearance as much more informative than the 
all the other appearances, i.e., f(i)=1, if i=1; ? 
else, where ? is a small positive real number.  
3.3 Incorporating the Position Features  
To incorporate the position features into the 
word-based summarization model, we use them 
to adjust the importance of the word appearance. 
For the ith appearance of a word w, its original 
importance is multiplied by the position feature 
value, i.e., log freq(w)?pos(w, i), where pos(w, i) 
is calculated by one of the four position features 
introduced above. By this, the position feature is 
also incorporated into the sentence scores, i.e., 
score?(s) = ?i [log freq(wi) ? pos(wi)] / |s| 
3.4 Sentence Position Features 
In our study, another type of position features, 
which model sentence position information, is 
defined for comparison with the word position 
features. The sentence position features are also 
defined by the above four formulas. However, 
for each appearance, the definition of i and n in 
the formulas are changed to the ordinal position 
of the sentence that contains this appearance 
and the total number of sentences in the 
document respectively. In fact, the effects of the 
features defined in this way are equivalent to 
traditional sentence position features. Since i 
and n are now defined by sentence positions, the 
feature values of the word tokens in the same 
sentence s are all equal. Denote it by pos(s), and 
the sentence score with the position feature can 
be written as  
score?(s) = ( ?w in slogfreq(w) ? pos(s))/|s|  
= pos(s)?(? logw in s freq(w)/|s|), 
which can just be viewed as the product of the 
original score and a sentence position feature. 
3.5 Discussion 
By using the four functions to measure word or 
sentence position information, we can generate 
a total of eight position features. Among the 
four functions, the importance drops fastest 
under the binary function and the order is BF > 
GS > IP > DP. Therefore, the features based on 
the binary function are the most biased to the 
leading sentences in the document and the 
features based on the direct proportion function 
are the least. On the other hand, as mentioned in 
the introduction, sentence-based features have 
larger preferences for leading sentences than 
word-based position features.  
An example is given below to illustrate the 
difference between word and sentence position 
features. This is a document from DUC 2001. 
1. GENERAL ACCIDENT, the leading British 
insurer, said yesterday that insurance claims 
arising from Hurricane Andrew could 'cost it as 
much as Dollars 40m.'  
2. Lord Airlie, the chairman who was 
addressing an extraordinary shareholders' 
meeting, said: 'On the basis of emerging 
information, General Accident advise that the 
losses to their US operations arising from 
Hurricane Andrew, which struck Florida and 
Louisiana, might in total reach the level at 
which external catastrophe reinsurance covers 
would become exposed'.  
3. What this means is that GA is able to pass on 
its losses to external reinsurers once a certain 
claims threshold has been breached.  
4. It believes this threshold may be breached in 
respect of Hurricane Andrew claims.  
5. However, if this happens, it would suffer a 
post-tax loss of Dollars 40m (Pounds 20m).  
6. Mr Nelson Robertson, GA's chief general 
manager, explained later that the company has a  
1/2 per cent share of the Florida market.  
7. It has a branch in Orlando.  
8. The company's loss adjusters are in the area 
trying to estimate the losses.  
9. Their guess is that losses to be faced by all 
insurers may total more than Dollars 8bn.  
10. Not all damaged property in the area is 
insured and there have been estimates that the 
storm caused more than Dollars 20bn of 
damage.  
11. However, other insurers have estimated that 
losses could be as low as Dollars 1bn in total. 
12 Mr Robertson said: 'No one knows at this 
time what the exact loss is'. 
For the word ?threshold? which appears 
twice in the document, its original importance is 
log(2), for the appearance of ?threshold? in the 
4th sentence, the modified score based on word 
position feature with the direct proportion 
function is 1/2?log(2). In contrast, the score 
based on sentence position feature with the 
922
same function is 9/12?log(2), which is larger. 
For the appearance of the word ?estimate? in the 
8th sentence, its original importance is log(3) 
(the three boldfaced tokens are regarded as one 
word with stemming). The word-based and 
sentence-based scores are log(3) and 5/12?log(3) 
respectively. So its importance is larger under 
word position feature. Therefore, the system 
with word position features may prefer the 8th 
sentence that is in deeper positions but the 
system with sentence position feature may 
prefer the 4th sentence. As for this document, the 
top 5 sentences selected by sentence position 
feature are {1, 4, 3, 5, 2} and the those selected 
by the word position features are {1, 8, 3, 6, 9}. 
This clearly demonstrates the difference 
between the position features. 
4 Experimental Results 
4.1 Experiment Settings 
We conduct the experiments on the data sets 
from the Document Understanding Conference 
(DUC) run by NIST. The DUC competition 
started at year 2001 and has successfully 
evaluated various summarization tasks up to 
now. In the experiments, we evaluate the 
effectiveness of position information on several 
DUC data sets that involve various 
summarization tasks. One of the evaluation 
criteria used in DUC, the automatic 
summarization evaluation package ROUGE, is 
used to evaluate the effectiveness of the 
proposed word position features in the context 
of document summarization1. The recall scores 
of ROUGE-1 and ROUGE-2, which are based 
on unigram and bigram matching between 
system summaries and reference summaries, are 
adopted as the evaluation criteria.  
In the data sets used in the experiments, the 
original documents are all pre-processed by 
sentence segmentation, stop-word removal and 
word stemming. Based on the word-based 
summarization model, a total of nine systems 
are evaluated in the experiments, including the 
system with the original ranking model (denoted 
as None), four systems with each word position 
feature (denoted as WP) and four systems with 
each sentence position feature (denoted as SP). 
                                                 
1 We run ROUGE-1.5.5 with the parameters ?-x -m -
n 2 -2 4 -u -c 95 -p 0.5 -t 0? 
For reference, the average ROUGE scores of all 
the human summarizers and all the submitted 
systems from the official results of NIST are 
also given (denoted as Hum and NIST 
respectively).  
4.2 Redundancy Removal 
To reduce the redundancy in the generated 
summaries, we use an approach similar to the 
maximum marginal relevance (MMR) approach 
in the sentence selection process (Carbonell and 
Goldstein, 1998). In each round of the sentence 
selection, the candidate sentence is compared 
against the already-selected sentences. The 
sentence is added to the summary only if it is 
not significantly similar to any already-selected 
sentence, which is judged by the condition that 
the cosine similarity between the two sentences 
is less than 0.7. 
4.3 Generic Summarization 
In the first experiment, we use the DUC 2001 
data set for generic single-document 
summarization and the DUC 2004 data set for 
generic multi-document summarization. The 
DUC 2001 data set contains 303 document-
summary pairs; the DUC 2004 data set contains 
45 document sets, with each set consisting of 10 
documents. A summary is required for each 
document set. Here we need to adjust the 
ranking model for the multi-document task, i.e., 
the importance of a word is calculated as its 
total frequency in the whole document set 
instead of a single document. For both tasks, the 
summary length limit is 100 words. 
Table 1 and 2 below provide the average 
ROUGE-1 and ROUGE-2 scores (denoted as R-
1 and R-2) of all the systems. Moreover, we 
used paired two sample t-test to calculate the 
significance of the differences between a pair of 
word and sentence position features. The bolded 
score in the tables indicates that that score is 
significantly better than the corresponding 
paired one. For example, in Table 1, the bolded 
R-1 score of system WP DP means that it is 
significantly better than the R-1 score of system 
SP DP. Besides the ROUGE scores, two 
statistics, the number of ?first sentences 2 ? 
among the selected sentences (FS-N) and the 
                                                 
2 A ?first sentence? is the sentence at the fist position 
of a document.  
923
average position of the selected sentences (A-
SP), are also reported in the tables for analysis.  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.4473 0.1942 301 4.00 
SP DP 0.4396 0.1844 300 3.69 
WP IP 0.4543 0.2023 290 4.30 
SP IP 0.4502 0.1964 303 3.08 
WP GS 0.4544 0.2041 278 4.50 
SP GS 0.4509 0.1974 303 2.93 
WP BF 0.4544 0.2036 253 5.57 
SP BF 0.4239 0.1668 303 9.64 
None 0.4193 0.1626 265 10.06
NIST 0.4445 0.1865 - - 
Hum 0.4568 0.1740 - - 
Table 1. Results on the DUC 2001 data set  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.3728 0.0911 89 4.16 
SP DP 0.3724 0.0908 112 2.68 
WP IP 0.3756 0.0912 108 3.77 
SP IP 0.3690 0.0905 201 1.01 
WP GS 0.3751 0.0916 110 3.67 
SP GS 0.3690 0.0905 201 1.01 
WP BF 0.3740 0.0926 127 3.14 
SP BF 0.3685 0.0903 203 1 
None 0.3550 0.0745 36 10.98
NIST 0.3340 0.0686 - - 
Hum 0.4002 0.0962 - - 
Table 2. Results on the DUC 2004 data set 
 
From Table 1 and Table 2, it is observed that 
position information is indeed very effective in 
generic summarization so that all the systems 
with position features performed better than the 
system None which does not use any position 
information. Moreover, it is also clear that the 
proposed word position features consistently 
outperform the corresponding sentence position 
features. Though the gaps between the ROUGE 
scores are not large, the t-tests proved that word 
position features are significantly better on the 
DUC 2001 data set. On the other hand, the 
advantages of word position features over 
sentence position features are less significant on 
the DUC 2004 data set. One reason may be that 
the multiple documents have provided more 
candidate sentences for composing the summary. 
Thus it is possible to generate a good summary 
only from the leading sentences in the 
documents. According to Table 2, the average-
sentence-position of system SP BF is 1, which 
means that all the selected sentences are ?first 
sentences?. Even under this extreme condition, 
the performance is not much worse. 
The two statistics also show the different 
preferences of the features. Compared to word 
position features, sentence position features are 
likely to select more ?first sentences? and also 
have smaller average-sentence-positions. The 
abnormally large average-sentence-position of 
SP BF in DUC 2001 is because it does not 
differentiate all the other sentences except the 
first one. The corresponding word-position-
based system WP BF can differentiate the 
sentences since it is based on word positions, so 
its average-sentence-position is not that large. 
4.4 Query-focused Summarization 
Since year 2005, DUC has adopted query-
focused multi-document summarization tasks 
that require creating a summary from a set of 
documents to a given query. This task has been 
specified as the main evaluation task over three 
years (2005-2007). The data set of each year 
contains about 50 DUC topics, with each topic 
including 25-50 documents and a query. In this 
experiment, we adjust the calculation of the 
word importance again for the query-focused 
issue. It is changed to the total number of the 
appearances that fall into the sentences with at 
least one word in the query. Formally, given the 
query which is viewed as a set of words 
Q={w1, ?, wT}, a sentence set SQ is defined as 
the set of sentences that contain at least one wi 
in Q. Then the importance of a word w is 
calculated by its frequency in SQ. For the query-
focused task, the summary length limit is 250 
words. 
Table 3 below provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
DUC 2005-2007 data sets. The boldfaced terms 
in the tables indicate the best results in each 
column. According to the results, on query-
focused summarization, position information 
seems to be not as effective as on generic 
summarization. The systems with position 
features can not outperform the system None. In 
fact, this is reasonable due to the requirement 
specified by the pre-defined query. Given the 
query, the content of interest may be in any 
924
position of the document and thus the position 
information becomes less meaningful.  
On the other hand, we find that though the 
systems with word position features cannot 
outperform the system None, it does 
significantly outperform the systems with 
sentence position features. This is also due to 
the role of the query. Since it may refer to the 
specified content in any position of the 
documents, sentence position features are more 
likely to fail in discovering the desired 
sentences since they always prefer leading 
sentences. In contrast, word position features 
are less sensitive to this problem and thus 
perform better. Similarly, we can see that the 
direct proportion (DP), which has the least bias 
for leading sentences, has the best performance 
among the four functions. 
System 2005 2006 2007 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3791 0.0805 0.3909 0.0917 0.4158 0.1135 
SP DP 0.3727 0.0776 0.3832 0.0869 0.4118 0.1103 
WP IP 0.3772 0.0791 0.3830 0.0886 0.4106 0.1121 
SP IP 0.3618 0.0715 0.3590 0.0739 0.3909 0.1027 
WP GS 0.3767 0.0794 0.3836 0.0879 0.4109 0.1119 
SP GS 0.3616 0.0716 0.3590 0.0739 0.3909 0.1027 
WP BF 0.3740 0.0741 0.3642 0.0796 0.3962 0.1037 
SP BF 0.3647 0.0686 0.3547 0.0742 0.3852 0.1013 
NONE 0.3788 0.0791 0.3936 0.0924 0.4193 0.1140 
NIST 0.3353 0.0592 0.3707 0.0741 0.0962 0.3978 
Hum 0.4392 0.1022 0.4532 0.1101 0.4757 0.1402 
Table 3. Results on the DUC 2005 - 2007 data sets 
 
System 2008 A 2008 B 2009 A 2009 B R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3687 0.0978 0.3758 0.1036 0.3759 0.1015 0.3693 0.0922 
SP DP 0.3687 0.0971 0.3723 0.1011 0.3763 0.1031 0.3704 0.0946 
WP IP 0.3709 0.1014 0.3741 0.1058 0.3758 0.1030 0.3723 0.0906 
SP IP 0.3619 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP GS 0.3705 0.1004 0.3732 0.1048 0.3770 0.1051 0.3731 0.0917 
SP GS 0.3625 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP BF 0.3661 0.0975 0.3678 0.0992 0.3720 0.1069 0.3650 0.0936 
SP BF 0.3658 0.0965 0.3674 0.0980 0.3683 0.1043 0.3654 0.0945 
NONE 0.3697 0.0978 0.3656 0.0915 0.3653 0.0934 0.3595 0.0834 
NIST 0.3389 0.0799 0.3192 0.0676 0.3468 0.0890 0.3315 0.0761 
Hum 0.4105 0.1156 0.3948 0.1134 0.4235 0.1249 0.3901 0.1059 
Table 4. Results on the TAC 2008 - 2009 data sets 
 
4.5 Update Summarization 
Since year 2008, the DUC summarization track 
has become a part of the Text Analysis 
Conference (TAC). In the update summarization 
task, each document set is divided into two 
ordered sets A and B. The summarization target 
on set A is the same as the query-focused task in 
DUC 2005-2007. As to the set B, the target is to 
write an update summary of the documents in 
set B, under the assumption that the reader has 
already read the documents in set A. The data 
set of each year contains about 50 topics, and 
each topic includes 10 documents for set A, 10 
documents for set B and an additional query. 
For set A, we follow exactly the same method 
used in section 4.4; for set B, we make an 
additional novelty check for the sentences in B 
with the MMR approach. Each candidate 
sentence for set B is now compared to both the 
selected sentences in set B and in set A to 
925
ensure its novelty. In the update task, the 
summary length limit is 100 words.  
Table 4 above provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
TAC 2008-2009 data sets. The results on set A 
and set B are shown individually. For the task 
on set A which is almost the same as the DUC 
2005-2007 tasks, the results are also very 
similar. A small difference is that the systems 
with position features perform slightly better 
than the system None on these two data sets. 
Also, the difference between word position 
features and sentence position features becomes 
smaller. One reason may be that the shorter 
summary length increases the chance of 
generating good summaries only from the 
leading sentences. This is somewhat similar to 
the results reported in (Nenkova, 2005) that 
position information is more effective for short 
summaries. 
For the update set B, the results show that 
position information is indeed very effective. In 
the results, all the systems with position features 
significantly outperform the system None. We 
attribute the reason to the fact that we are more 
concerned with novel information when 
summarizing update set B. Therefore, the effect 
of the query is less on set B, which means that 
the effect of position information may be more 
pronounced in contrast. On the other hand, 
when comparing the position features, we can 
see that though the difference of the position 
features is quite small, word position features 
are still better in most cases.  
4.6 Discussion 
Based on the experiments, we briefly conclude 
the effectiveness of position information in 
document summarization. In different tasks, the 
effectiveness varies indeed. It depends on 
whether the given task has a preference for the 
sentences at particular positions. Generally, in 
generic summarization, the position hypothesis 
works well and thus the ordinal position 
information is effective. In this case, those 
position features that are more distinctive, such 
as GS and BF, can achieve better performances. 
In contrast, in the query-focused task that relates 
to specified content in the documents, ordinal 
position information is not so useful. Therefore, 
the more distinctive a position feature is, the 
worse performance it leads to. However, in the 
update summarization task that also involves 
queries, position information becomes effective 
again since the role of the query is less 
dominant on the update document set.   
On the other hand, by comparing the sentence 
position features and word position features on 
all the data sets, we can draw an overall 
conclusion that word position features are 
consistently more appreciated. For both generic 
tasks in which position information is effective 
and query-focused tasks in which it is not so 
effective, word position features show their 
advantages over sentence position features. This 
is because of the looser position hypothesis 
postulated by them. By avoiding arbitrarily 
regarding the leading sentences as more 
important, they are more adaptive to different 
tasks and data sets. 
5 Conclusion and Future Work 
In this paper, we proposed a novel kind of word 
position features which consider the positions of 
word appearances instead of sentence positions. 
The word position features were compared to 
sentence position features under the proposed 
sentence ranking model. From the results on a 
series of DUC data sets, we drew the conclusion 
that the word position features are more 
effective and adaptive than traditional sentence 
position features. Moreover, we also discussed 
the effectiveness of position information in 
different summarization tasks. 
In our future work, we?d like to conduct more 
detailed analysis on position information. 
Besides the ordinal positions, more kinds of 
position information can be considered to better 
model the document structures. Moreover, since 
position hypothesis is not always correct in all 
documents, we?d also like to consider a pre-
classification method, aiming at identifying the 
documents for which position information is 
more suitable. 
 
Acknowledgement The work described in 
this paper was supported by Hong Kong RGC 
Projects (PolyU5217/07E). We are grateful to 
professor Chu-Ren Huang for his insightful 
suggestions and discussions with us. 
926
References
Edmundson, H. P.. 1969. New methods in automatic 
Extracting. Journal of the ACM, volume 16, issue 
2, pp 264-285. 
Gillick, D., Favre, B., Hakkani-Tur, D., Bohnet, B., 
Liu, Y., Xie, S.. 2009. The ICSI/UTD 
Summarization System at TAC 2009. Proceedings 
of Text Analysis Conference 2009.  
Jaime G. Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversity-based reranking for 
reordering documents and producing summaries. 
Proceedings of the 21st annual international ACM 
SIGIR conference on Research and development 
in information retrieval, pp 335-336. 
Lin, C. and Hovy, E.. 1997. Identifying Topics by 
Position. Proceedings of the fifth conference on 
Applied natural language processing 1997, pp 
283-290. 
Luhn, H. P.. 1958. The automatic creation of 
literature abstracts. IBM J. Res. Develop. 2, 2, pp 
159-165. 
Nenkova. 2005. Automatic text summarization of 
newswire: lessons learned from the document 
understanding conference. Proceedings of the 
20th National Conference on Artificial 
Intelligence, pp 1436-1441. 
Ouyang, Y., Li, S., Li, W.. 2007. Developing 
learning strategies for topic-based summarization. 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp 79-86. 
Radev, D., Jing, H., Sty?s, M. and Tam, D.. 2004. 
Centroid-based summarization of multiple 
documents. Information Processing and 
Management, volume 40, pp 919?938. 
Schilder, F., Kondadadi, R.. 2008. FastSum: fast and 
accurate query-based multi-document 
summarization. Proceedings of the 46th Annual 
Meeting of the Association for Computational 
Linguistics on Human Language Technologies, 
short paper session, pp 205-208. 
Toutanova, K. et al 2007. The PYTHY 
summarization system: Microsoft research at 
DUC 2007. Proceedings of Document 
Understanding Conference 2007.  
 
927
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 142?145,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
273. Task 5. Keyphrase Extraction Based on Core Word                 
Identification and Word Expansion 
You Ouyang        Wenjie Li        Renxian Zhang 
The Hong Kong Polytechnic University 
{csyouyang,cswjli,csrzhang}@comp.polyu.edu.hk 
Abstract 
This paper provides a description of the Hong 
Kong Polytechnic University (PolyU) System 
that participated in the task #5 of SemEval-2, 
i.e., the Automatic Keyphrase Extraction from 
Scientific Articles task. We followed a novel 
framework to develop our keyphrase 
extraction system, motivated by differentiating 
the roles of the words in a keyphrase. We first 
identified the core words which are defined as 
the most essential words in the article, and 
then expanded the identified core words to the 
target keyphrases by a word expansion 
approach.  
1 Introduction 
The task #5 in SemEval-2 requires extracting the 
keyphrases for scientific articles. According to 
the task definition, keyphrases are the words that 
capture the main topic of the given document. 
Currently, keyphrase extraction is usually carried 
out by a two-stage process, including candidate 
phrase identification and key phrase selection. 
The first stage is to identify the candidate phrases 
that are potential keyphrases. Usually, it is 
implemented as a process that filters out the 
obviously unimportant phrases. After the 
candidate identification stage, the target 
keyphrases can then be selected from the 
candidates according to their importance scores, 
which are usually estimated by some features, 
such as word frequencies, phrase frequencies, 
POS-tags, etc.. The features can be combined 
either by heuristics or by learning models to 
obtain the final selection strategy. 
In most existing keyphrase extraction methods, 
the importance of a phrase is estimated by a 
composite score of the features. Different 
features indicate preferences to phrases with 
specific characteristics. As to the common 
features, the phrases that consist of important and 
correlated words are usually preferred. Moreover, 
it is indeed implied in these features that the 
words are uniform in the phrase, that is, their 
degrees of importance are evaluated by the same 
criteria. However, we think that this may not 
always be true. For example, in the phrase ?video 
encoding/decoding?, the word ?video? appears 
frequently in the article and thus can be easily 
identified by simple features, while the word 
?encoding/decoding? is very rare and thus is very 
hard to discover. Therefore, a uniform view on 
the words is not able to discover this kind of 
keyphrases. On the other hand, we observe that 
there is usually at least one word in a keyphrase 
which is very important to the article, such as the 
word ?video? in the above example. In this paper, 
we call this kind of words core words. For each 
phrase, there may be one or more core words in 
it, which serve as the core component of the 
phrase. Moreover, the phrase may contain some 
words that support the core words, such as 
?encoding/decoding? in the above example. 
These words may be less important to the article, 
but they are highly correlated with the core word 
and are able to form an integrated concept with 
the core words. Motivated by this, we consider a 
new keyphrase extraction framework, which 
includes two stages: identifying the core words 
and expanding the core words to keyphrases. The 
methodology of the proposed approaches and the 
performance of the resulting system are 
introduced below. We also provide further 
discussions and modifications.  
2 Methodology 
According to our motivation, our extraction 
framework consists of three processes, including 
(1) The pre-processing to obtain the necessary 
information for the following processes; 
(2) The core word identification process to 
discover the core words to be expanded; 
(3) The word expansion process to generate the 
final keyphrases.  
 In the pre-processing, we first identify the text 
fields for each scientific article, including its title, 
abstract and main text (defined as all the section 
titles and section contents). The texts are then 
processed by the language toolkit GATE 1  to 
carry out sentence segmentation, word stemming 
and POS (part-of-speech) tagging. Stop-words 
                                                 
1 Publicly available at http://gate.ac.uk/gate 
142
are not considered to be parts of the target 
keyphrases. 
2.1 Core Word Identification 
Core words are the words that represent the 
dominant concepts in the article. To identify the 
core words, we consider the features below.  
Frequencies: In a science article, the words with 
higher frequencies are usually more important. 
To differentiate the text fields, in our system we 
consider three frequency-based features, i.e., 
Title-Frequency (TF), Abstract-Frequency 
(AF) and MainText-Frequency (MF), to 
represent the frequencies of one word in different 
text fields. For a word w in an article t, the 
frequencies are denoted by 
TF(w) = Frequency of  w in the title of t;  
AF(w) = Frequency of w in the abstract of t;  
MF(w) = Frequency of w in the main text of t. 
POS tag: The part-of-speech tag of a word is a 
good indicator of core words. Here we adopt a 
simple constraint, i.e., only nouns or adjectives 
can be potential core words. 
In our system, we use a progressive algorithm 
to identify all the core words. The effects of 
different text fields are considered to improve the 
accuracy of the identification result. First of all, 
for each word w in the title, it is identified to be a 
core word when satisfying  
{ TF(w)> 0 ? AF(w) > 0 } 
Since the abstract is usually less indicative 
than the title, we use stricter conditions for the 
words in the abstract by considering their co-
occurrence with the already-identified core 
words in the title. For a word w in the abstract, a 
co-occurrence-based feature COT(w) is defined 
as |S(w)|, where S(w) is the set of sentences 
which contain both w and at least one title core 
word. For a word w in the abstract, it is identified 
as an abstract core word when satisfying 
{ AF(w)> 0 ? MF(w) > ?1 ? COT (w) > ?2} 
Similarly, for a word w in the main text, it is 
identified as a general core word when satisfying 
{ MF(w) > ?1 ? COTA (w) >?2} 
where COTA (w) = |S?(w)| and S?(w) is the set of 
sentences which contain both w and at least one 
identified title core word or abstract core word. 
With this progressive algorithm, new core 
words can be more accurately identified with the 
previously identified core words. In the above 
heuristics, the parameters ? and ? are pre-defined 
thresholds, which are manually assigned2.  
                                                 
2 (?1, ?2, ?1, ?2) = (10, 5, 20, 10) in the system 
As a matter of fact, this heuristic-based 
identification approach is simple and preliminary. 
More sophisticated approaches, such as training 
machine learning models to classify the words, 
can be applied for better performance. Moreover, 
more useful features can also be considered. 
Nevertheless, we adopted the heuristic-based 
implementation to test the applicability of the 
framework as an initial study.  
An example of the identified core words is 
illustrated in Table 1 below: 
Type Core Word 
Title grid, service, discovery, UDDI 
Abstract distributed, multiple, web, computing, 
registry, deployment, scalability, DHT, 
DUDE, architecture 
Main proxy, search, node, key, etc. 
Table 1: Different types of core words 
2.2 Core Word Expansion 
Given the identified core words, the keyphrases 
can then be generated by expanding the core 
words. An example of the expansion process is 
illustrated below as 
grid ? grid service ? grid service discovery ? 
scalable grid service discovery  
For a core word, each appearance of it can be 
viewed as a potential expanding point. For each 
expanding point of the word, we need to judge if 
the context words can form a keyphrase along 
with it. Formally, for a candidate word w and the 
current phrase e (here we assume that w is the 
previous word, the case for the next word is 
similar), we consider the following features to 
judge if e should be expanded to w+e. 
Frequencies: the frequency of w (denoted by 
Freq(w)) and the frequency of the combination 
of w and e (denoted by phraseFreq(w, e)) which 
reflects the degree of w and e forming an 
integrated phrase. 
POS pattern: The part-of-speech tag of the 
word w is also considered here, i.e., we only try 
to expand w to w+e when w is a noun, an 
adjective or the specific conjunction ?of?. 
A heuristic-based approach is adopted here 
again. We intend to define some loose heuristics, 
which prefer long keyphrases. The heuristics 
include (1) If w and e are in the title or abstract, 
expand e to e+w when w satisfies the POS 
constraint and Freq(w) > 1; (2) If w and e are in 
the main text, expand e to e+w when w satisfies 
the POS constraint and phraseFreq(w, e) >1.  
More examples are provided in Table 2 below. 
 
 
143
Core Word Expanded Key Phrase 
grid scalable grid service discovery, 
grid computing 
UDDI UDDI registry, UDDI key 
web web service,  
scalability Scalability issue 
DHT DHT node 
Table 2: Core words and corresponding key phrases 
3 Results 
3.1 The Initial PolyU System in SemEval-2 
In the Semeval-2 test set, a total of 100 articles 
are provided. Systems are required to generate 15 
keyphrases for each article. Also, 15 keyphrases 
are generated by human readers as standard 
answers. Precision, recall and F-value are used to 
evaluate the performance. 
To generate exactly 15 keyphrases with the 
framework, we expand the core words in the title, 
abstract and main text in turn. Moreover, the core 
words in one fixed field are expanded following 
the descending order of frequency. When 15 
keyphrases are obtained, the process is stopped.  
For each new phrase, a redundancy check is 
also conducted to make sure that the final 15 
keyphrases can best cover the core concepts of 
the article, i.e.,  
(1) the new keyphrase should contain at least one 
word that is not included in any of the selected 
keyphrases; 
(2) if a selected keyphrase is totally covered by 
the new keyphrase, the covered keyphrase will 
be substituted by the new keyphrase. 
    The resulting system based on the above 
method is the one we submitted to SemEval-2. 
3.2 Phrase Filtering and Ranking 
Initially, we intend to use just the proposed 
framework to develop our system, i.e., using the 
expanded phrases as the keyphrases. However, 
we find out later that it must be adjusted to suit 
the requirement of the SemEval-2 task. In our 
subsequent study, we consider two adjustments, 
i.e., phrase filtering and phrase ranking.  
In SemEval-2, the evaluation criteria require 
exact match between the phrases. A phrase that 
covers a reference keyphrase but is not equal to it 
will not be counted as a successful match. For 
example, the candidate phrase ?scalable grid 
service discovery? is not counted as a match 
when compared to the reference keyphrase ?grid 
service discovery?. We call this the ?partial 
matching problem?. In our original framework, 
we followed the idea of ?expanding the phrase as 
much as possible? and adopted loose conditions. 
Consequently, the partial matching problem is 
indeed very serious. This unavoidably affects its 
performance under the criteria in SemEval-2 that 
requires exact matches. Therefore, we consider a 
simple filtering strategy here, i.e., filtering any 
keyphrase which only appears once in the article.  
Another issue is that the given task requires a 
total of exactly 15 keyphrases. Naturally we need 
a selection process to handle this. As to our 
framework, a keyphrase ranking process is 
necessary for discovering the best 15 keyphrases, 
not the best 15 core words. For this reason, we 
also try a simple method that re-ranks the 
expanded phrases by their frequencies. The top 
15 phrases are then selected finally. 
3.3 Results 
Table 3 below shows the precision, recall and F-
value of our submitted system (PolyU), the best 
and worst systems submitted to SemEval-2 and 
the baseline system that uses simple TF-IDF 
statistics to select keyphrases. 
On the SemEval-2 test data, the performance 
of the PolyU system was not good, just a little 
better than the baseline. A reason is that we just 
developed the PolyU system with our past 
experiences but did not adjust it much for better 
performance (since we were focusing on 
designing the new framework). After the 
competition, we examined two refined systems 
with the methods introduced in section 3.2. 
First, the PolyU system is adapted with the 
phrase filtering method. The performance of the 
resulting system (denoted by PolyU+) is given in 
Table 4. As shown in Table 4, the performance is 
much better just with this simple refinement to 
meet the requirement on extract matches for the 
evaluation criteria. Then, the phrase ranking 
method is also incorporated into the system. The 
performance of the resulting system (denoted by 
PolyU++) is also provided in Table 4. The 
performance is again much improved with the 
phrase ranking process. 
3.4 Discussion 
In our participation in SemEval-2, we submitted 
the PolyU system with the proposed extraction 
framework, which is based on expanding the 
core words to keyphrases. However, the PolyU 
system did not perform well in SemEval-2. 
However, we also showed later that the 
framework can be much improved after some
144
Simple but necessary refinements are made 
according to the given task. The final PolyU++ 
system with two simple refinements is much 
better. These refinements, including phrase 
filtering and ranking, are similar to traditional 
techniques. So it seems that our expansion-based 
framework is more applicable along with some 
traditional techniques. Though this conflicts our 
initial objective to develop a totally novel 
framework, the framework shows its ability of 
finding those keyphrases which contain different 
types of words. As to the PolyU++ system, when 
adapted with just two very simple post-
processing methods, the extracted candidate 
phrases can already perform quite well in 
SemEval-2. This may suggest that the framework 
can be considered as a new way for candidate 
keyphrase identification for the traditional 
extraction process. 
4 Conclusion and future work 
In this paper, we introduced our system in our 
participation in SemEval-2. We proposed a new 
framework for the keyphrase extraction task, 
which is based on expanding core words to 
keyphrases. Heuristic approaches are developed 
to implement the framework. We also analyzed 
the errors of the system in SemEval-2 and 
conducted some refinements. Finally, we 
concluded that the framework is indeed 
appropriate as a candidate phrase identification 
method. Another issue is that we just consider 
some simple information such as frequency or 
POS tag in this initial study. This indeed limits 
the power of the resulting systems. In future 
work, we?d like to develop more sophisticated 
implementations to testify the effectiveness of 
the framework. More syntactic and semantic 
features should be considered. Also, learning 
models can be applied to improve both the core 
word identification approach and the word 
expansion approach. 
 
Acknowledgments 
The work described in this paper is supported by 
Hong Kong RGC Projects (PolyU5217/07E and 
PolyU5230/08E). 
References  
Frank, E., Paynter, G.W., Witten, I., Gutwin, C. and 
Nevill-Manning, C.G.. 1999. Domain Specific 
Keyphrase Extraction. Proceedings of the IJCAI 
1999, pp.668--673. 
Medelyan, O. and Witten, I. H.. 2006. Thesaurus 
based automatic keyphrase indexing. Proceedings 
of the JCDL 2006, Chapel Hill, NC, USA. 
Medelyan, O. and Witten, I. H.. 2008. Domain 
independent automatic keyphrase indexing with 
small training sets. Journal of American Society for 
Information Science and Technology. Vol. 59 (7), 
pp. 1026-1040 
SemEval-2. Evaluation Exercises on Semantic 
Evaluation. http://semeval2.fbk.eu/ 
Turney, P.. 1999. Learning to Extract Keyphrases 
from Text. National Research Council, Institute for 
Information Technology, Technical Report ERB-
1057. (NRC \#41622), 1999. 
Wan, X. Xiao, J.. 2008. Single document keyphrase 
extraction using neighborhood knowledge. In 
Proceedings of AAAI 2008, pp 885-860. 
 
System 5 Keyphrases 10 Keyphrases 15 Keyphrases P R F P R F P R F 
Best 34.6% 14.4% 20.3% 26.1% 21.7% 23.7% 21.5% 26.7% 23.8%
Worst 8.2% 3.4% 4.8% 5.3% 4.4% 4.8% 4.7% 5.8% 5.2%
PolyU 13.6% 5.65% 7.98% 12.6% 10.5% 11.4% 12.0% 15.0% 13.3%
Baseline 17.8% 7.4% 10.4% 13.9% 11.5% 12.6% 11.6% 14.5% 12.9%
Table 3: Results from SemEval-2 
 
System 5 Keyphrases 10 Keyphrases 15 Keyphrases P R F P R F P R F 
PolyU 13.6% 5.65% 7.98% 12.6% 10.5% 11.4% 12.0% 15.0% 13.3%
PolyU+ 21.2% 8.8% 12.4% 16.9% 14.0% 15.3% 13.9% 17.3% 15.4%
PolyU++ 31.2% 13.0% 18.3% 22.1% 18.4% 20.1% 20.3% 20.6% 20.5%
Table 4: The performance of the refined systems 
 
145
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 102?109,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Using Deep Belief Nets for Chinese Named Entity Categorization 
Yu Chen1, You Ouyang2, Wenjie Li2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{csyouyang, cswjli}@comp.polyu.edu.hk
Abstract 
Identifying named entities is essential in 
understanding plain texts. Moreover, the 
categories of the named entities are indicative 
of their roles in the texts. In this paper, we 
propose a novel approach, Deep Belief Nets 
(DBN), for the Chinese entity mention 
categorization problem. DBN has very strong 
representation power and it is able to 
elaborately self-train for discovering 
complicated feature combinations. The 
experiments conducted on the Automatic 
Context Extraction (ACE) 2004 data set 
demonstrate the effectiveness of DBN. It 
outperforms the state-of-the-art learning 
models such as SVM or BP neural network. 
1 Introduction 
Named entities (NE) are defined as the names of 
existing objects, such as persons, organizations 
and etc. Identifying NEs in plain texts provides 
structured information for semantic analysis. 
Hence the named entity recognition (NER) task 
is a fundamental task for a wide variety of 
natural language processing applications, such as 
question answering, information retrieval and etc. 
In a text, an entity may either be referred to by a 
common noun, a noun phrase, or a pronoun. 
Each reference of the entity is called a mention. 
NER indeed requires the systems to identify 
these entity mentions from plain texts. The task 
can be decomposed into two sub-tasks, i.e., the 
identification of the entities in the text and the 
classification of the entities into a set of pre-
defined categories. In the study of this paper, we 
focus on the second sub-task and assume that the 
boundaries of all the entity mentions to be 
categorized are already correctly identified. 
In early times, NER systems are mainly based 
on handcrafted rule-based approaches. Although 
rule-based approaches achieved reasonably good 
results, they have some obvious flaws. First, they 
require exhausted handcraft work to construct a 
proper and complete rule set, which partially 
expressing the meaning of entity. Moreover, 
once the interest of task is transferred to a 
different domain or language, rules have to be 
revised or even rewritten. The discovered rules 
are indeed heavily dependent on the task 
interests and the particular corpus. Finally, the 
manually-formatted rules are usually incomplete 
and their qualities are not guaranteed. 
Recently, more attentions are switched to the 
applications of machine learning models with 
statistic information. In this camp, entity 
categorization is typically cast as a multi-class 
classification process, where the named entities 
are represented by feature vectors. Usually, the 
vectors are abstracted by some lexical and 
syntactic features instead of semantic feature. 
Many learning models, such as Support Vector 
Machine (SVM) and Neural Network (NN), are 
then used to classify the entities by their feature 
vectors. 
Entity categorization in Chinese attracted less 
attention when compared to English or other 
western languages. This is mainly because the 
unique characteristics of Chinese. One of the 
most common problems is the lack of boundary 
information in Chinese texts. For this problem, 
character-based methods are reported to be a 
possible substitution of word-based methods. As 
to character-based methods, it is important to 
study the implicit combination of characters.  
In our study, we explore the use of Deep 
Belief Net (DBN) in character-based entity 
categorization. DBN is a neural network model 
which is developed under the deep learning 
architecture. It is claimed to be able to 
automatically learn a deep hierarchy of the input 
features with increasing levels of abstraction for 
the complex problem. In our problem, DBN is 
used to automatically discover the complicated 
composite effects of the characters to the NE 
categories from the input data. With DBN, we 
need not to manually construct the character 
combination features for expressing the semantic 
relationship among characters in entities. 
Moreover, the deep structure of DBN enables the 
possibility of discovering very sophisticated 
102
combinations of the characters, which may even 
be hard to discover by human. 
The rest of this paper is organized as follow. 
Section 2 reviews the related work on name 
entity categorization. Section 3 introduces the 
methodology of the proposed approach. Section 
4 provides the experimental results. Finally, 
section 5 concludes the whole paper. 
2 Related work 
Over the past decades, NER has evolved from 
simple rule-based approaches to adapted self-
training machine learning approaches. 
As early rule-based approaches, MacDonald 
(1993) utilized local context, which implicate 
internal and external evidence, to aid on 
categorization. Wacholder (1997) employed an 
aggregation of classification method to capture 
internal rules. Both used hand-written rules and 
knowledge bases. Later, Collins (1999) adopted 
the AdaBoost algorithm to find a weighted 
combination of simple classifiers. They reported 
that the combination of simple classifiers can 
yield some powerful systems with much better 
performances. As a matter of fact, these methods 
all need manual studies on the construction of the 
rule set or the simple classifiers. 
Machine learning models attract more 
attentions recently. Usually, they train 
classification models based on context features. 
Various lexical and syntactic features are 
considered, such as N-grams, Part-Of-Speech 
(POS), and etc. Zhou and Su (2002) integrated 
four different kinds of features, which convey 
different semantic information, for a 
classification model based on the Hidden 
Markov Model (HMM). Koen (2006) built a 
classifier with the Conditional Random Field 
(CRF) model to classify noun phrases in a text 
with the WordNet SynSet. Isozaki and Kazawa 
(2002) studied the use of SVM instead. 
There were fewer studies in Chinese entity 
categorization. Guo and Jiang (2005) applied 
Robust Risk Minimization to classify the named 
entities. The features include seven traditional 
lexical features and two external-NE-hints based 
features. An important result they reported is that 
character-based features can be as good as word-
based features since they avoid the Chinese word 
segmentation errors. In (Jing et al, 2003), it was 
further reported that pure character-based models 
can even outperform word-based models with 
character combination features.  
Deep Belief Net is introduced in (Hinton et al, 
2006). According to their definition, DBN is a 
deep neural network that consists of one or more 
Restricted Boltzmann Machine (RBM) layers 
and a Back Propagation (BP) layer. This multi-
layer structure leads to a strong representation 
power of DBN. Moreover, DBN is quite efficient 
by using RBM to implement the middle layers, 
since RBM can be learned very quickly by the 
Contrastive Divergence (CD) approach. 
Therefore, we believe that DBN is very suitable 
for the character-level Chinese entity mention 
categorization approach. It can be used to solve 
the multi-class categorization problem with just 
simple binary features as the input. 
3 Deep Belief Network for Chinese 
Entity Categorization 
3.1 Problem Formalization 
An Entity mention categorization is a process of 
classifying the entity mentions into different 
categories. In this paper, we assume that the 
entity mentions are already correctly detected 
from the texts. Moreover, an entity mention 
should belong to one and only one predefined 
category. Formally, the categorization function 
of the name entities is 
( ( ))if V e C?            (1) 
where 
ie  is an entity mention from all the 
mention set E, ( )iV e  is the binary feature 
vector of 
ie , C={C1, C2, ?, CM} is the pre-
defined categories. Now the question is to find a 
classification function : Df R C?  which maps 
the feature vector V(ei) of an entity mention to its 
category. Generally, this classification function 
is learned from training data consisting of entity 
mentions with labeled categories. The learned 
function is then used to predict the category of 
new entity mentions by their feature vectors. 
3.2 Character-based Features 
As mentioned in the introduction, we intend to 
use character-level features for the purpose of 
avoiding the impact of the Chinese word 
segmentation errors. Denote the character 
dictionary as D={d1, d2, ?, dN}. To an e, it?s 
feature vector is V(e)={ v1, v2, ?, vN }. Each unit 
vi can be valued as Equation 2. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (2) 
103
For example, there is an entity mention ??
? ?Clinton?. So its feature vector is a vector 
with the same length as the character dictionary, 
in which all the dimensions are 0 except the three 
dimensions standing for ?, ?, and ?. The 
representation is clearly illustrated in Figure 1 
below. Since our objective is to test the 
effectiveness of DBN for this task. Therefore, we 
do not involve any other feature. 
 
Fig. 1. Generating the character-level features 
Characters compose the named entity and 
express its meaning. As a matter of fact, the 
composite effect of the characters to the 
mention category is quite complicated. For 
example, ?? ?Mr. Li? and ?? ?Laos? both 
have character ?, but ?? ?Mr. Li? indicates 
a person but ?? ?Laos? indicates a country. 
These are totally different NEs. Another 
example is ????? ?Capital of Paraguay? 
and ??? ?Asuncion?. They are two entity 
mentions point to the same entity despite that 
the two entities do not have any common 
characters. In such case, independent character 
features are not sufficient to determine the 
categories of the entity mentions. So we should 
also introduce some features which are able to 
represent the combinational effects of the 
characters. However, such kind of features is 
very hard to discover. Meanwhile, a complete 
set of combinations is nearly impossible to be 
found manually due to the exponential number 
of all the possible combinations. As in our 
study, we adopt DBN to automatically find the 
character combinations.  
3.3 Deep Belief Nets 
Deep Belief Network (DBN) is a complicated 
model which combines a set of simple models 
that are sequentially connected (Ackley, 1985). 
This deep architecture can be viewed as multiple 
layers. In DBN, upper layers are supposed to 
represent more ?abstract? concepts that explain 
the input data whereas lower layers extract ?low-
level features? from the data. DBN often consists 
of many layers, including multiple Restricted 
Boltzmann Machine (RBM) layers and a Back 
Propagation (BP) layer.  
 
Fig. 2.  The structure of a DBN. 
As illustrated in Figure 2, when DBN receives 
a feature vector, the feature vector is processed 
from the bottom to the top through several RBM 
layers in order to get the weights in each RBM 
layer, maintaining as many features as possible 
when they are transferred to the next layer. RBM 
deals with feature vectors only and omits the la-
bel information. It is unsupervised. In addition, 
each RBM layer learns its parameters indepen-
dently. This makes the parameters optimal for 
the relevant RBM layer but not optimal for the 
whole model. To solve this problem, there is a 
supervised BP layer on top of the model which 
fine-tunes the whole model in the learning 
process and generates the output in the inference 
process. After the processing of all these layers, 
the final feature vector consists of some sophisti-
cated features, which reflect the structured in-
formation among the original features. With this 
new feature vector, the classification perfor-
mance is better than directly using the original 
feature vector. 
None of the RBM is capable of guaranteeing 
that all the information conveyed to the output is 
accurate or important enough. However the 
learned information produced by preceding RBM 
layer will be continuously refined through the 
next RBM layer to weaken the wrong or insigni-
ficant information in the input. Each layer can 
detect feature in the relevant spaces. Multiple 
layers help to detect more features in different 
spaces. Lower layers could support object detec-
tion by spotting low-level features indicative of 
object parts. Conversely, information about ob-
jects in the higher layers could resolve lower-
level ambiguities. The units in the final layer 
share more information from the data. This in-
creases the representation power of the whole 
model. It is certain that more layers mean more 
computation time. 
104
DBN has some attractive features which make 
it very suitable for our problem. 
1) The unsupervised process can detect the 
structures in the input and automatically ob-
tain better feature vectors for classification. 
2) The supervised BP layer can modify the 
whole network by back-propagation to im-
prove both the feature vectors and the classi-
fication results. 
3) The generative model makes it easy to in-
terpret the distributed representations in the 
deep hidden layers. 
4) This is a fast learning algorithm that can 
find a fairly good set of parameters quickly 
and can ensure the efficiency of DBN. 
3.3.1 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which is 
the core component of DBN. RBM is Boltzmann 
Machine with no connection within the same 
layer. An RBM is constructed with one visible 
layer and one hidden layer. Each visible unit in 
the visible layer V  is an observed variable iv  
while each hidden unit in the hidden layer H  is 
a hidden variable 
jh
. Its joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (3) 
In RBM, the parameters that need to be esti-
mated are ( , , )W b c? ?  and 2( , ) {0,1}v h ? . 
To learn RBM, the optimum parameters are 
obtained by maximizing the above probability on 
the training data (Hinton, 1999). However, the 
probability is indeed very difficult in practical 
calculation. A traditional way is to find the gra-
dient between the initial parameters and the re-
spect parameters. By modifying the previous pa-
rameters with the gradient, the expected parame-
ters can gradually approximate the target para-
meters as 
0
( 1) ( ) ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (4) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging to 
the target. 
Traditionally, the Markov chain Monte Carlo 
method (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(5) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the av-
erage over the data states and its relevant sample 
in hidden unit. 
h v? ?
 denotes the multiplication 
of the average over the model states in visible 
unit and its relevant sample in hidden unit. 
However, MCMC requires estimating an ex-
ponential number of terms. Therefore, it typically 
takes a long time to converge to 
h v? ?
. Hinton 
(2002) introduced an alternative algorithm, i.e., 
the contrastive divergence (CD) algorithm, as a 
substitution. It is reported that CD can train the 
model much more efficiently than MCMC. To 
estimate the distribution ( )p x , CD considers a 
series of distributions { ( )np x } which indicate the 
distributions in n steps. It approximates the gap 
of two different Kullback-Leiler divergences 
(Kullback, 1987) as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (6) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution p?  
defined by the model. In each step, the gap is 
approximately minimized so that we can obtain 
the final distribution which has the smallest 
Kullback-Leiler divergence with the fantasy dis-
tribution.  
After n steps, the gradient can be estimated 
and used in Equation 4 to adjust the weights of 
RBM. In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust the 
weight of RBM. In this case, the estimate of the 
gradient is just the gap between the products of 
the visual layer and the hidden layer, i.e., 
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (7) 
Figure 3 below illustrates the process of learning 
RBM with CD-based gradient estimation. 
 
105
Fig. 3.  Learning RBM with CD-based gradient 
estimation 
3.3.2 Back-propagation (BP) 
The RBM layers provide an unsupervised analy-
sis on the structures of data set. They automati-
cally detect sophisticated feature vectors. The 
last layer in DBN is the BP layer. It takes the 
output from the last RBM layer and applies it in 
the final supervised learning process. In DBN, 
not only is the supervised BP layer used to gen-
erate the final categories, but it is also used to 
fine-tune the whole network. Specifically speak-
ing, when the parameters in BP layer are 
changed during its iterating process, the changes 
are passed to the other RBM layers in a top-to-
bottom sequence. 
The BP algorithm has a feed-forward step and 
a back-propagation step. In the feed-forward step, 
the input values are propagated to obtain the out-
put values. In the back-propagation step, the out-
put values are compared to the real category la-
bels and used them to modify the parameters of 
the model. We consider the weight
ijw  
which 
indicates the edge pointing from the i-th node in 
one RBM layer to the j-th node in its upper layer. 
The computation in feed-forward is 
i ijo w , 
where 
io  is the stored output for the unit i. In 
the back-propagation step, we compute the error 
E in the upper layers and also the gradient with 
respect to this error, i.e., 
i ijE o w? ?
. Then the 
weight
ijw  
will be adjusted by the gradient des-
cent. 
ij i i j
i ij
Ew o oo w? ? ?
?? ? ? ? ??
 (8) 
where ??  is used to control the length of the 
moving step. 
3.3.3 DBN-based Entity Mention Categori-
zation 
For each entity mention, it is represented by the 
character feature vector as introduced in section 
3.2 and then fed to DBN. The training procedure 
can be divided into two phases. The first phase is 
the parameter estimation process of the RBMs on 
all the inputted feature vectors. When a feature 
vector is fed to DBN, the first RBM layer is 
adjusted automatically according to this vector. 
After the first RBM layer is ready, its output 
becomes the input of the second RBM layer. The 
weights of the second RBM layer are also 
adjusted. The similar procedure is carried out on 
all the RBM layers. Then DBN will operates in 
the second phase, the back-propagation 
algorithm. The labeled categories of the entity 
mention are used to tune the parameters of the 
BP layer. Moreover, the changes of the BP layer 
are also fed back to the RBM layers. The 
procedure will iterate until the terminating 
condition is met. It can be a fixed number of 
iterations or a pre-given precision threshold. 
Once the weights of all the layers in DBN are 
obtained, the estimated model could be used to 
prediction. 
 
Fig. 4.  The mention categorization process 
of DBN 
Figure 4 illustrates the classification process of 
DBN. In prediction, for an entity mention e, we 
first calculate its feature vector V(e) and used as 
the input of DBN. V(e) is passed through all the 
layers to get the outputs for all RBM layers and 
last back-propagation layer. In the ith RBM layer, 
the dimensions in the input vector Vinput_i(e) are 
combined to yield the dimensions of the next 
feature vector Voutput_i(e) as input of the next layer. 
After the feature vector V(e) goes through all the 
RBM layers, it is indeed transformed to another 
feature vector V?(e) which consists of 
complicated combinations of the original 
character features and contains rich structured 
information between the characters. This feature 
vector is then fed into the BP layer to get the 
final category c(e). 
4 Experiments 
4.1 Experiment Setup 
In our experiment, we use the ACE 2004 corpus 
to evaluate our approach. The objective of this 
study is that the correctly detected Chinese entity 
mentions categorization using DBN from the text 
and figure out the suitability of DBN on this task. 
Moreover, an entity mention should belong to 
one and only one category. 
106
According to the guideline of the ACE04 task, 
there are five categories for consideration in total, 
i.e., Person, Organization, Geo-political entity, 
Location, and Facility. Moreover, each entity 
mention is expressed in two forms, i.e., the head 
and the extent. For example, ??????? 
?President Clinton of USA? is the extent of an 
entity mention and ???  ?Clinton? is the 
corresponding head. The two phrases both point 
to a named entity whose name is Clinton and he 
is the president of USA.  Here we make the 
?breakdown? strategy mentioned in Li et al 
(2007) that only the entity head is considered to 
generate the feature vector, considering that the 
information from the entity head refines the 
name entity. Although the entity extent includes 
more information, it also brings many noises 
which may make the learning process much 
more difficult. 
   In our experiments, we test the machine 
learning models under a 4-flod cross-validation. 
All entity mentions are divided into four parts 
randomly where three parts are used for training 
and one for test. In total, 7746 mentions are used 
for training and 2482 mentions are used for 
testing at each round. Precision is chosen as the 
evaluation criterion, calculated by the proportion 
of the number of correctly categorized instances 
and the number of total instances. Since all the 
instances should be classified, the recall value is 
equal to the precision value. 
4.2 Evaluation on Named Entity categoriza-
tion 
First of all, we provide some statistics of the data 
set. The distribution of entity mentions in each 
category is given in table 1. The size of the 
character dictionary in the corpus is 1185, so 
does the dimension of each feature vector. 
Type Quantity 
Person 4197 
Organization 1783 
Geo-political entity 287 
Location 3263 
Facility 399 
Table 1.  Number of entity mentions in each 
category 
In the first experiment, we compare the 
performance of DBN with some popular 
classification algorithms, including Support 
Vector Machine (labeled by SVM) and a 
traditional BP neutral network (labeled by NN 
(BP)). To implement the models, we use the 
LibSVM toolkit1 for SVM and the neural neutral 
network toolbox in Matlab2 for BP. The DBN in 
this experiment includes two RBM layers and 
one BP layer. Results of the first experiment are 
given in Table 2.  
Learning Model Precision 
DBN 91.45% 
SVM 90.29% 
NN(BP) 87.23% 
Table 2.  Performances of the systems with 
different classification models 
In this experiment, the DBN has three RBM 
layers and one BP layer. And the numbers of 
units in each RBM layer are 900, 600 and 300 
respectively. NN (BP) has the same structure as 
DBN. As for SVM, we choose the linear kernel 
with the penalty parameter C=1 and set the other 
parameters as default after comparing different 
kernels and parameters. 
In the results, DBN achieved better 
performance than both SVM and BP neural 
network. This clearly proved the advantages of 
DBN. The deep architecture of DBN yields 
stronger representation power which makes it 
able to detect more complicated and efficient 
features, thus better performance is achieved.  
In the second experiment, we intend to 
examine the performance of DBN with different 
number of RBM layers, from one RBM layer 
plus one BP layer to three RBM layers plus one 
BP layer. The amount of the units in the first 
RBM layer is set 900 and the amount in the 
second RBM layer is 600, if the second layer 
exists. As for the third RBM layers, the amount 
of units is set to 300. 
Construction of Neural Network Precision 
Three RBMs and One BP 91.45% 
Two RBMs and One BP 91.42% 
One RBM and one BP 91.05% 
Table 3.  Performance of DBNs with different 
number s of RBM layers 
Results in Table 3 show that the performance 
tends to be better when more RBM layers are 
incorporated. More RBM layers do enhance the 
representation power of DBN. However, it is 
also noted that the improvement is not significant 
from two layers to three layers. The reason may 
                                                 
1 available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
2 available at 
http://www.mathworks.com/access/helpdesk/help/toolbox
/nnet/backprop.html 
107
be that two-RBM DBN already has enough 
representation power for modeling this data set 
and thus one more RBM layer brings 
insignificant improvement. It is also mentioned 
in Hinton (2006) that more than three RBM 
layers are indeed not necessary. Another 
important result in Table 3 is that the DBN with 
One RBM and one BP performs much better than 
the neutral network with only BP in Table 1. 
This clearly showed the effectiveness of feature 
combination by the RBM layer again. 
As to the amount of units in each RBM layer, 
it is manually fixed in upper experiments. This 
number certainly affects the representation 
power of an RBM layer, consequently the 
representation power of the whole DBN. In this 
set of experiment, we intend to study the 
effectiveness of the unit size to the performance 
of DBN. A series of DBNs with only one RBM 
layer and different unit numbers for this RBM 
layer is evaluated. The results are provided in 
Table 4 below. 
Construction of Neural Network Precision 
one RBM(300 units) + one BP 90.61% 
one RBM(600 units) + one BP 90.69% 
one RBM(900 units) + one BP 91.05% 
one RBM(1200 units) + one BP 90.98% 
one RBM(1500 units) + one BP 90.61% 
one RBM(1800 units) + one BP 90.57% 
Table 4.  Performance of One-RBM DBNs 
with different number of units 
Based on the results, we can see that the 
performance is quite stable with different unit 
numbers. But the numbers that are closer to the 
original feature size seem to be some better. This 
could suggest that we should not decrease or 
increase the dimension of the vector feature too 
much when casting the vector transformation by 
RBM layers. 
Finally, we show the results of the individual 
categories. For each category, the Precision-
Recall-F values are provided in table 5, in which 
the F-measure is calculated by 
2*Precision*Recall-measure= Precision+RecallF
    (9) 
Type P R F 
Person 91.26% 96.26% 93.70% 
Organization 89.86% 89.04% 89.45% 
Location 77.58% 59.21% 76.17% 
Geo-political 
entity 
93.60% 91.89% 92.74% 
Facility 77.43% 63.72% 69.91% 
Table 5.  Performances of the system on each 
category 
5 Conclusions 
In this paper we presented our recent work on 
applying a novel machine learning model, the 
Deep Belief Nets, on Chinese entity mention 
categorization. It is demonstrated that DBN is 
very suitable for character-level mention 
categorization approaches due to its strong 
representation power and the ability on 
discovering complicated feature combinations. 
We conducted a series of experiments to prove 
the benefits of DBN. Experimental results 
clearly showed the advantages of DBN that it 
obtained better performance than existing 
approaches such as SVM and traditional BP 
neutral network. 
References  
David Ackley, Geoffrey Hinton, and Terrence 
Sejnowski. 1985. A learning algorithm for 
Boltzmann machines. Cognitive Science. 9. 
David MacDonald. 1993. Internal and external 
evidence in the identification and semantic 
categorization of proper names. Corpus 
Processing for Lexical Acquisition, MIT Press, 61-
76. 
Geoffrey Hinton. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Geoffrey Hinton. 2002. Training products of experts 
by minimizing contrastive divergence. Neural 
Computation, 14, 1771?1800. 
Geoffrey Hinton, Simon Osindero, and Yee-Whey 
Teh. 2006. A fast learning algorithm for deep 
belief nets. Neural Computation. 18, 1527?1554 . 
GuoDong Zhou and Jian Su. 2002. Named entity 
recognition using an hmm-based chunk tagger. In 
proceedings of ACL. 473-480. 
Hideki Isozaki and Hideto Kazawa. 2002. Efficient 
support vector classifiers for named entity 
recognition. In proceedings of IJCNLP. 1-7. 
Honglei Guo, Jianmin Jiang, Guang Hu and Tong 
Zhang. 2005. Chinese named entity recognition 
based on multilevel linguistics features. In 
pr ceedings of IJCNLP. 90-99. 
Jing, Hongyan, Radu Florian, Xiaoqiang Luo, Tong 
Zhang and Abraham Ittycheriah. 2003. How to get 
a Chinese name (entity): Segmentation and 
combination issues. In proceedings of EMNLP. 
200-207. 
Koen Deschacht and Marie-Francine Moens. 2006, 
Efficient Hierarchical Entity Classifier Using 
Conditional Random Field. In Proceedings of the 
108
2nd Workshop on Ontology Learning and 
Population. 33-40. 
Michael Collins and Yoram Singer. 1999. 
Unsupervised models for named entity 
classification. In Proceedings of EMNLP'99. 
Nina Wacholder, Yael Ravin and Misook Choi. 1997. 
Disambiguation of Proper Names in Text. In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing. 
Solomon Kullback. 1987. Letter to the Editor: The 
Kullback-Leibler distance. The American 
Statistician 41 (4): 340?341. 
Wenjie Li and Donglei Qian. 2007. Detecting, 
Categorizing and Clustering Entity Mentions in 
Chinese Text, in Proceedings of the 30th Annual 
International ACM SIGIR Conference (SIGIR?07). 
647-654. 
Yoshua Bengio and Yann LeCun. 2007. Scaling 
learning algorithms towards ai. Large-Scale Ker-
nel Machines. MIT Press. 
 
109
