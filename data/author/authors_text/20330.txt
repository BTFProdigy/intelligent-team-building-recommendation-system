Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 103?108, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level
Semantic Similarity Estimation
Nikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,
Shrikanth Narayanan1
1Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,
shri@sipi.usc.edu
Abstract
This paper describes our submission for the
*SEM shared task of Semantic Textual Sim-
ilarity. We estimate the semantic similarity
between two sentences using regression mod-
els with features: 1) n-gram hit rates (lexical
matches) between sentences, 2) lexical seman-
tic similarity between non-matching words, 3)
string similarity metrics, 4) affective content
similarity and 5) sentence length. Domain
adaptation is applied in the form of indepen-
dent models and a model selection strategy
achieving a mean correlation of 0.47.
1 Introduction
Text semantic similarity estimation has been an ac-
tive research area, thanks to a variety of potential ap-
plications and the wide availability of data afforded
by the world wide web. Semantic textual similar-
ity (STS) estimates can be used for information ex-
traction (Szpektor and Dagan, 2008), question an-
swering (Harabagiu and Hickl, 2006) and machine
translation (Mirkin et al, 2009). Term-level simi-
larity has been successfully applied to problems like
grammar induction (Meng and Siu, 2002) and affec-
tive text categorization (Malandrakis et al, 2011). In
this work, we built on previous research and our sub-
mission to SemEval?2012 (Malandrakis et al, 2012)
to create a sentence-level STS model for the shared
task of *SEM 2013 (Agirre et al, 2013).
Semantic similarity between words has been
well researched, with a variety of knowledge-based
(Miller, 1990; Budanitsky and Hirst, 2006) and
corpus-based (Baroni and Lenci, 2010; Iosif and
Potamianos, 2010) metrics proposed. Moving to
sentences increases the complexity exponentially
and as a result has led to measurements of simi-
larity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al, 2009), and semantic (Rinaldi
et al, 2003; Bos and Markert, 2005). Machine trans-
lation evaluation metrics can be used to estimate lex-
ical level similarity (Finch et al, 2005; Perez and
Alfonseca, 2005), including BLEU (Papineni et al,
2002), a metric using word n-gram hit rates. The pi-
lot task of sentence STS in SemEval 2012 (Agirre et
al., 2012) showed a similar trend towards multi-level
similarity, with the top performing systems utilizing
large amounts of partial similarity metrics and do-
main adaptation (the use of separate models for each
input domain) (Ba?r et al, 2012; S?aric? et al, 2012).
Our approach is originally motivated by BLEU
and primarily utilizes ?hard? and ?soft? n-gram hit
rates to estimate similarity. Compared to last year,
we utilize different alignment strategies (to decide
which n-grams should be compared with which).
We also include string similarities (at the token and
character level) and similarity of affective content,
expressed through the difference in sentence arousal
and valence ratings. Finally we added domain adap-
tation: the creation of separate models per domain
and a strategy to select the most appropriate model.
2 Model
Our model is based upon that submitted for the same
task in 2012 (Malandrakis et al, 2012). To esti-
mate semantic similarity metrics we use a super-
vised model with features extracted using corpus-
103
based word-level similarity metrics. To combine
these metrics into a sentence-level similarity score
we use a modification of BLEU (Papineni et al,
2002) that utilizes word-level semantic similarities,
string level comparisons and comparisons of affec-
tive content, detailed below.
2.1 Word level semantic similarity
Co-occurrence-based. The semantic similarity be-
tween two words, wi and wj , is estimated as their
pointwise mutual information (Church and Hanks,
1990): I(i, j) = log p?(i,j)p?(i)p?(j) , where p?(i) and p?(j) are
the occurrence probabilities of wi and wj , respec-
tively, while the probability of their co-occurrence
is denoted by p?(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al, 2012)
we employed a modification of the pointwise mutual
information based on the maximum sense similar-
ity assumption (Resnik, 1995) and the minimization
of the respective error in similarity estimation. In
particular, exponential weights ? were introduced in
order to reduce the overestimation of denominator
probabilities. The modified metric Ia(i, j), is de-
fined as:
Ia(i, j)=
1
2
[
log
p?(i, j)
p??(i)p?(j) + log
p?(i, j)
p?(i)p??(j)
]
. (1)
The weight ? was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of ? = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets
(Rubenstein and Goodenough, 1965; Miller and
Charles, 1998; Finkelstein et al, 2002).
Context-based: The fundamental assumption
behind context-based metrics is that similarity
of context implies similarity of meaning (Harris,
1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj , is computed as the cosine
of their feature vectors: QH(i, j) = vi.vj||vi|| ||vj || .
The elements of feature vectors can be weighted
according various schemes [(Iosif and Potamianos,
2010)], while, here we use a binary scheme.
Network-based: The aforementioned similarity
metrics were used for the definition of a semantic
network (Iosif and Potamianos, 2013; Iosif et al,
2013). A number of similarity metrics were pro-
posed under either the attributional similarity (Tur-
ney, 2006) or the maximum sense similarity (Resnik,
1995) assumptions of lexical semantics1.
2.2 Sentence level similarities
To utilize word-level semantic similarities in the
sentence-level task we use a modified version of
BLEU (Papineni et al, 2002). The model works in
two passes: the first pass identifies exact matches
(similar to baseline BLEU), the second pass com-
pares non-matched terms using semantic similarity.
Non-matched terms from the hypothesis sentence
are compared with all terms of the reference sen-
tence (regardless of whether they were matched dur-
ing the first pass). In the case of bigram and higher
order terms, the process is applied recursively: the
bigrams are decomposed into two words and the
similarity between them is estimated by applying the
same method to the words. All word similarity met-
rics used are peak-to-peak normalized in the [0,1]
range, so they serve as a ?degree-of-match?. The se-
mantic similarity scores from term pairs are summed
(just like n-gram hits) to obtain a BLEU-like hit-rate.
Alignment is performed via maximum similarity:
we iterate on the hypothesis n-grams, left-to-right,
and compare each with the most similar n-gram in
the reference. The features produced by this process
are ?soft? hit-rates (for 1-, 2-, 3-, 4-grams)2. We also
use the ?hard? hit rates produced by baseline BLEU
as features of the final model.
2.3 String similarities
We use the following string-based similarity fea-
tures: 1) Longest Common Subsequence Similarity
(LCSS) (Lin and Och, 2004) based on the Longest
Common Subsequence (LCS) character-based dy-
1The network-based metrics were applied only during the
training phase of the shared task, due to time limitations. They
exhibited almost identical performance as the metric defined by
(1), which was used in the test runs.
2Note that the features are computed twice on each sentence
pair and then averaged.
104
namic programming algorithm. LCSS represents the
length of the longest string (or strings) that is a sub-
string (or are substrings) of two or more strings. 2)
Skip bigram co-occurrence measures the overlap of
skip-bigrams between two sentences or phrases. A
skip-bigram is defined as any pair of words in the
sentence order, allowing for arbitrary gaps between
words (Lin and Och, 2004). 3) Containment is de-
fined as the percentage of a sentence that is con-
tained in another sentence. It is a number between
0 and 1, where 1 means the hypothesis sentence is
fully contained in the reference sentence (Broder,
1997). We express containment as the amount of n-
grams of a sentence contained in another. The con-
tainment metric is not symmetric and is calculated
as: c(X,Y ) = |S(X) ? S(Y )|/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.
2.4 Affective similarity
We used the method proposed in (Malandrakis et al,
2011) to estimate affective features. Continuous (va-
lence and arousal) ratings in [?1, 1] of any term are
represented as a linear combination of a function of
its semantic similarities to a set of seed words and
the affective ratings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (2)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
andwj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English
Words (ANEW) (Bradley and Lang, 1999) corpus.
Using this model we generate affective ratings for
every content word (noun, verb, adjective or adverb)
of every sentence. We assume that these can ad-
equately describe the affective content of the sen-
tences. To create an ?affective similarity metric? we
use the difference of means of the word affective rat-
ings between two sentences.
d?affect = 2? |?(v?(s1))? ?(v?(s2))| (3)
where ?(v?(si)) the mean of content word ratings in-
cluded in sentence i.
2.5 Fusion
The aforementioned features are combined using
one of two possible models. The first model is a
Multiple Linear Regression (MLR) model
D?L = a0 +
k
?
n=1
an fk, (4)
where D?L is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.
The second model is motivated by an assumption
of cognitive scaling of similarity scores: we expect
that the perception of hit rates is non-linearly af-
fected by the length of the sentences. We call this the
hierarchical fusion scheme. It is a combination of
(overlapping) MLR models, each matching a range
of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l ? l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l ? [1, l1] are decoded
with DL1, sentences with length l ? (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,
we use four models with l1 = 10, l2 = 20, l3 = 30,
l4 = ?.
Domain adaptation is employed, by creating sep-
arate models per domain (training data source). Be-
yond that, we also create a unified model, trained
on all data to be used as a fallback if an appropriate
model can not be decided upon during evaluation.
3 Experimental Procedure and Results
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al, 2005; Toutanova et al,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. We evaluated
multiple types of preprocessing per unsupervised
metric and chose different ones depending on the
metric. Word-level semantic similarities, used for
soft comparisons and affective feature extraction,
were computed over a corpus of 116 million web
snippets collected by posing one query for every
word in the Aspell spellchecker (asp, ) vocabulary to
the Yahoo! search engine. Word-level emotional rat-
ings in continuous valence and arousal scales were
produced by a model trained on the ANEW dataset
105
and using contextual similarities. Finally, string sim-
ilarities were calculated over the original unmodified
sentences.
Next, results are reported in terms of correla-
tion between the generated scores and the ground
truth, for each corpus in the shared task, as well as
their weighted mean. Feature selection is applied
to the large candidate feature set using a wrapper-
based backward selection approach on the train-
ing data.The final feature set contains 15 features:
soft hit rates calculated over content word 1- to 4-
grams (4 features), soft hit rates calculated over un-
igrams per part-of-speech, for adjectives, nouns, ad-
verbs, verbs (4 features), BLEU unigram hit rates
for all words and content words (2 features), skip
and containment similarities, containment normal-
ized by sum of sentence lengths or product of sen-
tence lengths (3 features) and affective similarities
for arousal and valence (2 features).
Domain adaptation methods are the only dif-
ference between the three submitted runs. For all
three runs we train one linear model per training set
and a fallback model. For the first run, dubbed lin-
ear, the fallback model is linear and model selection
during evaluation is performed by file name, there-
fore results for the OnWN set are produced by a
model trained with OnWN data, while the rest are
produced by the fallback model. The second run,
dubbed length, uses a hierarchical fallback model
and model selection is performed by file name. The
third run, dubbed adapt, uses the same models as
the first run and each test set is assigned to a model
(i.e., the fallback model is never used). The test set -
model (training) mapping for this run is: OnWN ?
OnWN, headlines ? SMTnews, SMT ? Europarl
and FNWN? OnWN.
Table 1: Correlation performance for the linear model us-
ing lexical (L), string (S) and affect (A) features
Feature headl. OnWN FNWN SMT mean
L 0.68 0.51 0.23 0.25 0.46
L+S 0.69 0.49 0.23 0.26 0.46
L+S+A 0.69 0.51 0.27 0.28 0.47
Results are shown in Tables 1 and 2. Results for
the linear run using subsets of the final feature set
are shown in Table 1. Lexical features (hit rates) are
obviously the most valuable features. String similar-
ities provided us with an improvement in the train-
Table 2: Correlation performance on the evaluation set.
Run headl. OnWN FNWN SMT mean
linear 0.69 0.51 0.27 0.28 0.47
length 0.65 0.51 0.25 0.28 0.46
adapt 0.62 0.51 0.33 0.30 0.46
ing set which is not reflected in the test set. Af-
fect proved valuable, particularly in the most diffi-
cult sets of FNWN and SMT.
Results for the three submission runs are shown
in Table 2. Our best run was the simplest one, using
a purely linear model and effectively no adaptation.
Adding a more aggressive adaptation strategy im-
proved results in the FNWN and SMT sets, so there
is definitely some potential, however the improve-
ment observed is nowhere near that observed in the
training data or the same task of SemEval 2012. We
have to question whether this improvement is an ar-
tifact of the rating distributions of these two sets
(SMT contains virtually only high ratings, FNWN
contains virtually only low ratings): such wild mis-
matches in priors among training and test sets can
be mitigated using more elaborate machine learning
algorithms (rather than employing better semantic
similarity features or algorithms). Overall the sys-
tem performs well in the two sets containing large
similarity rating ranges.
4 Conclusions
We have improved over our previous model of sen-
tence semantic similarity. The inclusion of string-
based similarities and more so of affective content
measures proved significant, but domain adaptation
provided mixed results. While expanding the model
to include more layers of similarity estimates is
clearly a step in the right direction, further work is
required to include even more layers. Using syntac-
tic information and more levels of abstraction (e.g.
concepts) are obvious next steps.
5 Acknowledgements
The first four authors have been partially funded
by the PortDial project (Language Resources for
Portable Multilingual Spoken Dialog Systems) sup-
ported by the EU Seventh Framework Programme
(FP7), grant number 296170.
106
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. SemEval, pages 385?393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In Proc. *SEM.
Gnu aspell. http://www.aspell.net.
D. Ba?r, C. Biemann, I. Gurevych, and T. Zesch. 2012.
Ukp: Computing semantic textual similarity by com-
bining multiple content similarity measures. In Proc.
SemEval, pages 435?440.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
M. Bradley and P. Lang. 1999. Affective norms for En-
glish words (ANEW): Stimuli, instruction manual and
affective ratings. Technical report C-1. The Center for
Research in Psychophysiology, University of Florida.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In In Compression and Com-
plexity of Sequences (SEQUENCES97, pages 21?29.
IEEE Computer Society.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13?47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363?370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st InternationalCon-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2012. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. In Proc. Eighth International Con-
ference on Language Resources and Evaluation, pages
3499?3504.
Elias Iosif and Alexandros Potamianos. 2013. Similarity
Computation Using Semantic Networks Created From
Web-Harvested Data. Natural Language Engineering,
(submitted).
E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-
vanou. 2013. Semantic similarity computation for ab-
stract and concrete nouns using network-based distri-
butional semantic models. In 10th International Con-
ference on Computational Semantics (IWCS), pages
328?334.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42?47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
N. Malandrakis, E. Iosif, and A. Potamianos. 2012.
DeepPurple: Estimating sentence semantic similarity
using n-gram regression models and web snippets. In
Proc. Sixth International Workshop on Semantic Eval-
uation (SemEval) ? The First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
565?570.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
107
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172?181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th AnnualMeeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791?799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448?453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25?32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849?856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173?180.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379?416.
F. S?aric?, G. Glavas?, M. Karan, J. S?najder, and B. Dal-
belo Bas?ic?. 2012. Takelab: Systems for measuring
semantic text similarity. In Proc. SemEval, pages 441?
448.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
108
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 438?442, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SAIL: A hybrid approach to sentiment analysis
Nikolaos Malandrakis1, Abe Kazemzadeh2, Alexandros Potamianos3, Shrikanth Narayanan1
1 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2 Annenberg Innovation Laboratory (AIL), USC, Los Angeles, CA 90089, USA
3Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, kazemzad@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.edu
Abstract
This paper describes our submission for Se-
mEval2013 Task 2: Sentiment Analysis in
Twitter. For the limited data condition we use
a lexicon-based model. The model uses an af-
fective lexicon automatically generated from a
very large corpus of raw web data. Statistics
are calculated over the word and bigram af-
fective ratings and used as features of a Naive
Bayes tree model. For the unconstrained data
scenario we combine the lexicon-based model
with a classifier built on maximum entropy
language models and trained on a large exter-
nal dataset. The two models are fused at the
posterior level to produce a final output. The
approach proved successful, reaching rank-
ings of 9th and 4th in the twitter sentiment
analysis constrained and unconstrained sce-
nario respectively, despite using only lexical
features.
1 Introduction
The analysis of the emotional content of text, is
relevant to numerous natural language processing
(NLP), web and multi-modal dialogue applications.
To that end there has been a significant scientific
effort towards tasks like product review analysis
(Wiebe and Mihalcea, 2006; Hu and Liu, 2004),
speech emotion extraction (Lee and Narayanan,
2005; Lee et al, 2002; Ang et al, 2002) and pure
text word (Esuli and Sebastiani, 2006; Strappar-
ava and Valitutti, 2004) and sentence (Turney and
Littman, 2002; Turney and Littman, 2003) level
emotion extraction.
The rise of social media in recent years has seen
a shift in research focus towards them, particularly
twitter. The large volume of text data available is
particularly useful, since it allows the use of com-
plex machine learning methods. Also important is
the interest on the part of companies that are actively
looking for ways to mine social media for opinions
and attitudes towards them and their products. Sim-
ilarly, in journalism there is interest in sentiment
analysis for a way to process and report on the public
opinion about current events (Petulla, 2013).
Analyzing emotion expressed in twitter borrows
from other tasks related to affective analysis, but
also presents unique challenges. One common is-
sue is the breadth of content available in twitter: a
more limited domain would make the task easier,
however there are no such bounds. There is also a
significant difference in the form of language used
in tweets. The tone is informal and typographical
and grammatical errors are very common, making
even simple tasks, like Part-of-Speech tagging much
harder. Features like hashtags and emoticons can
also be helpful (Davidov et al, 2010).
This paper describes our submissions for Se-
mEval 2013 task 2, subtask B, which deals pri-
marily with sentiment analysis in twitter. For the
constrained condition (using only the organizer-
provided twitter sentences) we implemented a sys-
tem based on the use of an affective lexicon and part-
of-speech tag information, which has been shown
relevant to the task (Pak and Paroubek, 2010).
For the unconstrained condition (including external
sources of twitter sentences) we combine the con-
strained model with a maximum entropy language
438
model trained on external data.
2 Experimental procedure
We use two separate models, one for the constrained
condition and a combination for the unconstrained
condition. Following are short descriptions.
2.1 Lexicon-based model
The method used for the constrained condition is
based on an affective lexicon containing out-of-
context affective ratings for all terms contained in
each sentence. We use an automated algorithm of
affective lexicon expansion based on the one pre-
sented in (Malandrakis et al, 2011), which in turn
is an expansion of (Turney and Littman, 2002).
We assume that the continuous (in [?1, 1]) va-
lence and arousal ratings of any term can be repre-
sented as a linear combination of its semantic simi-
larities to a set of seed words and the affective rat-
ings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (1)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
and wj . For the purposes of this work, the seman-
tic similarity metric is the cosine similarity between
context vectors computed over a corpus of 116 mil-
lion web snippets collected by posing one query for
every word in the Aspell spellchecker?s vocabulary
to the Yahoo! search engine and collecting up to 500
of the top results.
Given a starting, manually annotated, lexicon we
can select part of it to serve as seed words and then
use 1 to create a system of linear equations where
the only unknowns are the weights ai. The system
is solved using Least Squares Estimation. That pro-
vides us with an equation that can generate affective
ratings for every term (not limited to words), as long
as we can estimate the semantic similarity between
it and the seed words.
Seed word selection is performed by a simple
heuristic (though validated through experiments):
we want seed words to have extreme affective rat-
ings (maximum absolute value) and we want the set
to be as closed to balanced as possible (sum of seed
ratings equal to zero).
Given these term ratings, the next step is combin-
ing them through statistics. To do that we use sim-
ple statistics (mean, min, max) and group by part
of speech tags. The results are statistics like ?max-
imum valence among adjectives?, ?mean arousal
among proper nouns? and ?number of verbs and
nouns?. The dimensions used are: valence, absolute
valence and arousal. The grouping factors are the 39
Penn treebank pos tags plus higher order tags (adjec-
tives, verbs, nouns, adverbs and combinations of 2,3
and 4 of them). The statistics extracted are: mean,
min, max, most extreme, sum, number, percentage
of sentence coverage. In the case of bigram terms no
part-of-speech filtering/grouping is applied. These
statistics form the feature vectors.
Finally we perform feature selection on the mas-
sive set of candidates and use them to train a model.
The model selected is a Naive Bayes tree, a tree with
Naive Bayes classifiers on each leaf. The motivation
comes by considering this a two stage problem: sub-
jectivity detection and polarity classification, mak-
ing a hierarchical model a natural choice. NB trees
proved superior to other types of trees during our
testing, presumably due to the smoothing of obser-
vation distributions.
2.2 N-gram language model
The method used for the unconstrained condition
is based on a combination of the automatically ex-
panded affective lexicon described in the previ-
ous section together with a bigram language model
based on the work of (Wang et al, 2012), which
uses a large set of twitter data from the U.S. 2012
Presidential election. As a part of the unconstrained
system, we were able to leverage external annotated
data apart from those provided by the SEMEVAL
2013 sentiment task dataset. Of the 315 million
tweets we collected about the election, we anno-
tated a subset of 40 thousand tweets using Ama-
zon Mechanical Turk. The annotation labels that
we used were ?positive?, ?negative?, ?neutral?, and
?unsure?, and additionally raters could mark tweets
for sarcasm and humor. We excluded tweets marked
as ?unsure? as well as tweets that had disagree-
439
ment in labels if they were annotated by more than
one annotator. To extract the bigram features, we
used a twitter-specific tokenizer (Potts, 2011), which
marked uniform resource locators (URLs), emoti-
cons, and repeated characters, and which lowercased
words that began with capital letters followed by
lowercase letters (but left words in all capitals). The
bigram features were computed as presence or ab-
sense in the tweet rather than counts due to the small
number of words in tweets. The machine learning
model used to classify the tweets was the Megam
maximum entropy classifier (Daume? III, 2004) in
the Natural Language Toolkit (NLTK) (Bird et al,
2009).
2.3 Fusion
The submitted system for the unconstrained condi-
tion leverages both the lexicon-based and bigram
language models. Due to the very different nature
of the models we opt to not fuse them at the feature
level, using a late fusion scheme instead. Both par-
tial models are probabilistic, therefore we can use
their per-class posterior probabilities as features of
a fusion model. The fusion model is a linear kernel
SVM using six features, the three posteriors from
each partial model, and trained on held out data.
3 Results
Following are results from our method, evaluated
on the testing sets (of sms and twitter posts) of
SemEval2013 task 2. We evaluate in terms of 3-
class classification, polarity classification (positive
vs. negative) and subjectivity detection (neutral vs.
other). Results shown in terms of per category f-
measure.
3.1 Constrained
The preprocessing required for the lexicon-based
model is just part-of-speech tagging using Treetag-
ger (Schmid, 1994). The lexicon expansion method
is used to generate valence and arousal ratings for
all words and ngrams in all datasets and the part of
speech tags are used as grouping criteria to gener-
ate statistics. Finally, feature selection is performed
using a correlation criterion (Hall, 1999) and the re-
sulting feature set is used to train a Naive Bayes
tree model. The feature selection and model train-
Table 1: F-measure results for the lexicon-based model,
using different machine learning methods, evaluated on
the 3-class twitter testing data.
model
per-class F-measure
neg neu pos
Nbayes 0.494 0.652 0.614
SVM 0.369 0.677 0.583
CART 0.430 0.676 0.593
NBTree 0.561 0.662 0.643
Table 2: F-measure results for the constrained condition,
evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.561 0.662 0.643
pos vs neg 0.679 0.858
neu vs other 0.685 0.699
sms
3-class 0.506 0.709 0.531
pos vs neg 0.688 0.755
neu vs other 0.730 0.628
ing/classification was conducted using Weka (Wit-
ten and Frank, 2000).
The final model uses a total of 72 features, which
can not be listed here due to space constraints. The
vast majority of these features are necessary to de-
tect the neutral category: positive-negative separa-
tion can be achieved with under 30 features.
One aspect of the model we felt worth investigat-
ing, was the type of model to be used. Using a multi-
stage model, performing subjectivity detection be-
fore positive-negative classification, has been shown
to provide an improvement, however single models
have also been used extensively. We compared some
popular models: Naive Bayes, linear kernel SVM,
CART-trained tree and Naive Bayes tree, all using
the same features, on the twitter part of the SemEval
testing data. The results are shown in Table 1. The
two Naive Bayes-based models proved significantly
better, with NBTree being clearly the best model for
these features.
Results from the submitted constrained model are
shown in Table 2. Looking at the twitter data re-
sults and comparing the positive-negative vs the
440
3-class results, it appears the main weakness of
this model is subjectivity detection, mostly on the
neutral-negative side. It is not entirely clear to us
whether that is an artifact of the model (the nega-
tive class has the lowest prior probability, thus may
suffer compared to neutral) or of the more complex
forms of negativity (sarcasm, irony) which we do not
directly address. There is a definite drop in perfor-
mance when using the same twitter-trained model on
sms data, which we would not expect, given that the
features used are not twitter-specific. We believe this
gap is caused by lower part-of-speech tagger perfor-
mance: visual inspection reveals the output on twit-
ter data is fairly bad.
Overall this model ranked 9th out of 35 in the
twitter set and 11th out of 28 in the sms set, among
all constrained submissions.
3.2 Unconstrained
Table 3: F-measure results for the maximum entropy
model with bigram features, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.403 0.661 0.623
pos vs neg 0.586 0.804
neu vs other 0.661 0.704
sms
3-class 0.390 0.587 0.542
pos vs neg 0.710 0.648
neu vs other 0.587 0.641
Table 4: F-measure results for the unconstrained condi-
tion, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.565 0.679 0.655
pos vs neg 0.672 0.881
neu vs other 0.667 0.732
sms
3-class 0.502 0.723 0.538
pos vs neg 0.625 0.772
neu vs other 0.710 0.637
In order to create the submitted unconstrained
model we train an SVM model using the lexicon-
based and bigram language model posterior proba-
bilities as features. This fusion model is trained on
held-out data (the development set of the SemEval
data). The results of classification using the bigram
language model alone are shown in Table 3 and the
results from the final fused model are shown in Ta-
ble 4. Looking at relative per-class performance, the
results follow a form most similar to the constrained
model, though there are gains in all cases. These
gains are less significant when evaluated on the sms
data, resulting in a fair drop in ranks: the bigram lan-
guage model (expectedly) suffers more when mov-
ing to a different domain, since it uses words as
features rather than the more abstract affective rat-
ings used by the lexicon-based model. Also, because
the external data used to train the bigram language
model was from discussions of politics on Twitter,
the subject matter also varied in terms of prior senti-
ment distribution in that the negative class was pre-
dominant in politics, which resulted in high recall
but low precision for the negative class.
This model ranked 4th out of 16 in the twitter set
and 7th out of 17 in the sms set, among all uncon-
strained submissions.
4 Conclusions
We presented a system of twitter sentiment analy-
sis combining two approaches: a hierarchical model
based on an affective lexicon and a language model-
ing approach, fused at the posterior level. The hier-
archical lexicon-based model proved very successful
despite using only n-gram affective ratings and part-
of-speech information. The language model was
not as good individually, but provided a noticeable
improvement to the lexicon-based model. Overall
the models achieved good performance, ranking 9th
of 35 and 4th of 16 in the constrained and uncon-
strained twitter experiments respectively, despite us-
ing only lexical information.
Future work will focus on incorporating im-
proved tokenization (including part-of-speech tag-
ging), making better use of twitter-specific features
like emoticons and hashtags, and performing affec-
tive lexicon generation on twitter data.
441
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP, pages 2037?2040.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
H. Daume? III. 2004. Notes on cg and lm-bfgs op-
timization of logistic regression. Paper available at
http://pub. hal3. name# daume04cg-bfgs, implementa-
tion available at http://hal3. name/megam.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced
sentiment learning using twitter hashtags and smileys.
In Proc. COLING, pages 241?249.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A pub-
licly available lexical resource for opinion mining. In
Proc. LREC, pages 417?422.
M. A. Hall. 1999. Correlation-based feature selection
for machine learning. Ph.D. thesis, The University of
Waikato.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proc. SIGKDD, KDD ?04, pages
168?177. ACM.
C. M. Lee and S. Narayanan. 2005. Toward detecting
emotions in spoken dialogs. IEEE Transactions on
Speech and Audio Processing, 13(2):293?303.
C. M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotion
recognition. In Proc. ICSLP, pages 873?876.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
A. Pak and P. Paroubek. 2010. Twitter as a corpus
for sentiment analysis and opinion mining. In Proc.
LREC, pages 1320?1326.
S. Petulla. 2013. Feelings, nothing more than feelings:
The measured rise of sentiment analysis in journalism.
Neiman Journalism Lab, January.
C. Potts. 2011. Sentiment symposium tutorial: Tokeniz-
ing. Technical report, Stanford Linguistics.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proc. International Confer-
ence on New Methods in Language Processing, vol-
ume 12, pages 44?49.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proc. LREC,
volume 4, pages 1083?1086.
P. Turney and M. L. Littman. 2002. Unsupervised
Learning of Semantic Orientation from a Hundred-
Billion-Word Corpus. Technical report ERC-1094
(NRC 44929). National Research Council of Canada.
P. Turney and M. L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems, 21:315?346.
H. Wang, D. Can, A. Kazemzadeh, F. Bar, and
S. Narayanan. 2012. A system for real-time twitter
sentiment analysis of 2012 u.s. presidential election
cycle. In Proc. ACL, pages 115?120.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proc. COLING/ACL, pages 1065?1072.
Ian H.Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
442
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 508?511,
Dublin, Ireland, August 23-24, 2014.
SAIL-GRS: Grammar Induction for Spoken Dialogue Systems using
CF-IRF Rule Similarity
Kalliopi Zervanou, Nikolaos Malandrakis and Shrikanth Narayanan
Signal Analysis and Interpretation Laboratory (SAIL),
University of Southern California, Los Angeles, CA 90089, USA
kzervanou@gmail.com, malandra@usc.edu , shri@sipi.usc.edu
Abstract
The SAIL-GRS system is based on a
widely used approach originating from in-
formation retrieval and document index-
ing, the TF -IDF measure. In this im-
plementation for spoken dialogue system
grammar induction, rule constituent fre-
quency and inverse rule frequency mea-
sures are used for estimating lexical and
semantic similarity of candidate grammar
rules to a seed set of rule pattern instances.
The performance of the system is evalu-
ated for the English language in three dif-
ferent domains, travel, tourism and finance
and in the travel domain, for Greek. The
simplicity of our approach makes it quite
easy and fast to implement irrespective of
language and domain. The results show
that the SAIL-GRS system performs quite
well in all three domains and in both lan-
guages.
1 Introduction
Spoken dialogue systems typically rely on gram-
mars which define the semantic frames and re-
spective fillers in dialogue scenarios (Chen et al.,
2013). Such systems are tailored for specific
domains for which the respective grammars are
mostly manually developed (Ward, 1990; Seneff,
1992). In order to address this issue, numerous
current approaches attempt to infer these grammar
rules automatically (Pargellis et al., 2001; Meng
and Siu, 2002; Yoshino et al., 2011; Chen et al.,
2013).
The acquisition of grammar rules for spoken
language systems is defined as a task comprising
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
of two subtasks (Meng and Siu, 2002; Iosif and
Potamianos, 2007), the acquisition of:
(i) Low-level rules These are rules defining
domain-specific entities, such as names of lo-
cations, hotels, airports, e.g. CountryName:
?USA?, Date: ?July 15th, 2014?, CardType:
?VISA? and other common domain multi-word ex-
pressions, e.g. DoYouKnowQ: ?do you know?.
(ii) High-level rules These are larger,
frame-like rule patterns which contain as
semantic slot fillers multi-word entities
identified by low-level rules. For exam-
ple: DirectionsQ: ?<DoYouKnowQ>
<where> the <MuseumName> is lo-
cated?, ExpressionCardProblem: ?my
<CardType> has expired?.
The shared task of Grammar Induction for Spo-
ken Dialogue Systems, where our system partic-
ipated, focused on the induction of high-level
grammar rules and in particular on the identifica-
tion and semantic classification of new rule pat-
terns based on their semantic similarity to known
rule instances.
Within this research framework, the work de-
scribed in this paper proposes a methodology for
estimating rule semantic similarity using a varia-
tion of the well-known measure of TF -IDF as
rule constituent frequency vs. inverse rule fre-
quency, henceforth CF -IRF .
In the remainder of this paper, we start in Sec-
tion 2 by a detailed description of our system. Sub-
sequently, in Section 3, we present the datasets
used and the evaluation process, and in Section 4
we discuss our results. We conclude in Section 5
with a summary of our observations and directions
for future work.
2 System Description
The SAIL-GRS system is based on a widely used
approach in information retrieval and document
indexing, the TF -IDF measure. TF -IDF is
508
an approach that has found numerous applications
in information management applications, such as
document keyword extraction, (e.g., Dillon and
Gray (1983)), document clustering, summarisa-
tion, (e.g., Gong and Liu (2001)), event cluster-
ing, (e.g., De Smet and Moens (2013)). In dia-
logue systems, TF -IDF has been used, among
other applications, for discovering local coher-
ence (Gandhe and Traum, 2007) and for acquir-
ing predicate-argument rule fragments in an open
domain, information extraction-based spoken dia-
logue system (Yoshino et al., 2011). In their ap-
proach, Yoshino et al. (2011) use the TF -IDF
measure to determine the importance of a given
word for a given domain or topic, so as to select
the most salient predicate-argument structure rule
patterns from their corpus.
In our implementation for spoken dialogue
system grammar induction, rule constituent fre-
quency (CF ) and inverse rule frequency (IRF )
measures are used for estimating lexical and se-
mantic similarity of candidate grammar rules to a
seed set of rule pattern instances. As illustrated in
Table 1, the SAIL-GRS algorithm has two main
steps, the training stage and the rule induction
stage.
Input: known rule pattern instances
Output: new candidate rule patterns
Training stage:
1. Known rule instance parsing
2. Rule constituent extraction (uni-/bigrams)
3. Rule constituent frequency count (CF )
4. Inverse rule frequency count (IRF )
5. CF -IRF rule instance vector creation
Rule induction stage:
1. Unknown text fragment parsing
2. Unigram & bigram extraction
3. Uni-/bigram CF -IRF value lookup
4. Creation of CF -IRF vector for
unknown text fragment
5. Estimation of cosine similarity of
unknown fragment to rule instances
6. New candidate rule selection & rule
semantic category classification using
maximum cosine similarity
Table 1: The SAIL-GRS system algorithm.
In the first, the Training stage, known rule in-
stances are parsed and, for each rule semantic cat-
egory, the respective high-level rule pattern in-
stances are acquired. These patterns are subse-
quently split into unigram and bigram constituents
and the respective constituent frequencies and in-
verse rule frequencies are estimated. Finally, for
each rule category, a vector representation is cre-
ated for the respective rule pattern instance, based
on the CF -IRF value of its unigram and bigram
constituents.
In the second step, the Rule induction stage, the
unknown text fragments are parsed and split into
unigrams and bigrams. Subsequently, we lookup
the known rule instance unigram and bigram rep-
resentations for potential lexical matches to these
new unigrams and bigrams. If these are found,
then the new n-grams acquire the respective CF -
IRF values found in the training instances and the
respective CF -IRF vector for the unknown text
fragments is created. Finally, we estimate the co-
sine similarity of this unknown text vector to each
known rule vector. The unknown text fragments
that are most similar to a given rule category are
selected as candidate rule patterns and are classi-
fied in the known rule semantic category. An un-
known text fragment that is selected as candidate
rule pattern is assigned only to one, the most sim-
ilar, rule category.
3 Experimental Setup
The overall objective in spoken dialogue system
grammar induction is the fast and efficient devel-
opment and portability of grammar resources. In
the Grammar Induction for Spoken Dialogue Sys-
tems task, this challenge was addressed by pro-
viding datasets in three different domains, travel,
tourism and finance, and by attempting to cover
more than one language for the travel domain,
namely English and Greek.
As illustrated in Table 2, the travel domain data
for the two languages are comparable, with 32 and
35 number of known rule categories, for English
and Greek, comprising of 982 and 956 high-level
rule pattern instances respectively. The smallest
dataset is the finance dataset, with 9 rule categories
and 136 rule pattern instances, while the tourism
dataset has a relatively low number of rule cate-
gories comprising of the highest number of rule
pattern instances. Interestingly, as indicated in the
column depicting the percent of unknown n-grams
in the test-set, i.e. the unigrams and the bigrams
without a CF -IRF value in the training data, the
tourism domain test-set appears also to be the one
509
with the greatest overlap with the training data,
with a mere 0.72% and 4.84% of unknown uni-
grams and bigrams respectively.
For the evaluation, the system performance is
estimated in terms of precision (P ), recall (R) and
F -score measures, for the correct classification of
an unknown text fragment to a given rule cate-
gory cluster of pattern instances. In addition to
these measures, the weighted average of the per
rule scores is computed as follows:
P
w
=
?
N?1
i=1
P
i
c
i
?
N?1
i=1
c
i
, R
w
=
?
N?1
i=1
R
i
n
i
?
N?1
i=1
n
i
(1)
F
w
=
2 ? P
w
?R
w
P
w
+ R
w
(2)
where N ? 1 is the total number of rule cate-
gories, P
i
and R
i
are the per rule i scores for preci-
sion and recall, c
i
the unknown patterns correctly
assigned to rule i, and n
i
the total number of cor-
rect rule instance patterns for rule i indicated in
the ground truth data.
4 Results
The results of the SAIL-GRS system outperform
the Baseline in all dataset categories, except the
Tourism domain, as illustrated in Table 3. In this
domain, both systems present the highest scores
compared to the other domains. The high results
in the travel domain are probably due to the high
data overlap between the train and the test data, as
discussed in the previous section and illustrated in
Table 2. However, this domain was also the one
with the highest average number of rule instances
per rule category, compared to the other domains,
thus presenting an additional challenge in the cor-
rect classification of unknown rule fragments.
We observe that the overall higher F measures
of the SAIL-GRS system in the travel and fi-
nance domains are due to higher precision scores,
whereas Baseline system displays higher recall but
lower precision scores and lower F-measure in
these domains.
The overall lowest scores for both systems are
reached in the Travel domain for Greek, which
is also the dataset with the lowest overlap with
the training data. However, the performance of
the SAIL-GRS system does not deteriorate to the
same extent as the Baseline, the precision of which
falls to a mere 0.16-0.17, compared to 0.49-0.46
for the SAIL-GRS system.
5 Conclusion
In this work, we have presented the SAIL-GRS
system used for the Grammar Induction for Spo-
ken Dialogue Systems task. Our approach uses
a fairly simple, language independent method for
measuring lexical and semantic similarity of rule
pattern instances. Our rule constituent frequency
vs. inverse rule frequency measure, CF -IRF is a
modification the TF -IDF measure for estimating
rule similarity in the induction process of new rule
instances.
The performance of our system in rule induc-
tion and rule pattern semantic classification was
tested in three different domains, travel, tourism
and finance in four datasets, three for English
and an additional dataset for the travel domain
in Greek. SAIL-GRS outperforms the Baseline
in all datasets, except the travel domain for En-
glish. Moreover, our results showed that our sys-
tem achieved an overall better score in precision
and respective F-measure, in the travel and finance
domains, even when applied to a language other
than English. Finally, in cases of a larger percent-
age of unknown data in the test set, as in the Greek
travel dataset, the smooth degradation of SAIL-
GRS results compared to the Baseline indicates
the robustness of our method.
A limitation of our system in its current version
lies in the requirement for absolute lexical match
with unknown rule unigrams and bigrams. Fu-
ture extensions of the system could include rule
constituent expansion using synonyms, variants or
semantically or lexically similar words, so as to
improve recall and the overall F-measure perfor-
mance.
References
Yun-Nung Chen, William Yang Wang, and Alexan-
der I. Rudnicky. 2013. Unsupervised induction and
filling of semantic slots for spoken dialogue systems
using frame-semantic parsing. In Proceedings of the
2013 IEEEWorkshop on Automatic Speech Recogni-
tion and Understanding, pages 120?125.
Wim De Smet and Marie-Francine Moens. 2013. Rep-
resentations for multi-document event clustering.
Data Mining and Knowledge Discovery, 26(3):533?
558.
Martin Dillon and Ann S. Gray. 1983. FASIT: A
fully automatic syntactically based indexing system.
Journal of the American Society for Information Sci-
ence, 34(2):99?108.
510
High-Level Rule Rule Patterns # Test-set: Unknown n-grams %
Domain Categories #
Training-set Test-set Unigrams Bigrams
Travel EN 32 982 284 5.13% 20.71%
Travel GR 35 956 324 17.26% 33.09%
Tourism EN 24 1004 285 0.72% 4.84%
Finance EN 9 136 37 12.35% 36.74%
Table 2: Characteristics of training and test datasets.
Domain SAIL-GRS Baseline
P P
w
R R
w
F F
w
P P
w
R R
w
F F
w
Travel EN 0.57 0.54 0.66 0.62 0.61 0.58 0.38 0.40 0.67 0.69 0.48 0.51
Travel GR 0.49 0.46 0.62 0.51 0.55 0.49 0.16 0.17 0.73 0.65 0.26 0.26
Tourism EN 0.75 0.75 0.90 0.90 0.82 0.82 0.82 0.80 0.94 0.94 0.87 0.87
Finance EN 0.67 0.78 0.62 0.78 0.65 0.78 0.40 0.48 0.63 0.78 0.49 0.60
Table 3: Evaluation results for SAIL-GRS system compared to the baseline in all four datasets in terms
of per rule Precision P , Recall R, and F-score F . In the grey column, P
w
, R
w
, and F
w
stand for the
weighted average of the per rule precision, recall and F-score respectively, as defined in Equ. 1 and 2.
Sudeep Gandhe and David Traum. 2007. First steps
towards dialogue modelling from an un-annotated
human-human corpus. In Proceedings of the Fifth
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, pages 22?27.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?01,
pages 19?25, New York, NY, USA. ACM.
Elias Iosif and Alexandros Potamianos. 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. In Proceedings of the 8th Annual
Conference of the International Speech Communi-
cation Association, pages 1609?1612. ISCA.
Helen M. Meng and Kai-Chung Siu. 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1):172?181.
Andrew N. Pargellis, Eric Fosler-Lussier, Alexandros
Potamianos, and Chin-Hui Lee. 2001. Metrics
for measuring domain independence of semantic
classes. In Proceedings of the 7th European Con-
ference on Speech Communication and Technology,
pages 447?450. ISCA.
Stephanie Seneff. 1992. TINA: A natural language
system for spoken language applications. Computa-
tional Linguistics, 18(1):61?86, March.
Wayne Ward. 1990. The CMU air travel informa-
tion service: Understanding spontaneous speech.
In Speech and Natural Language: Proceedings of
a Workshop Held at Hidden Valley, Pennsylvania,
pages 127?129.
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken dialogue system based on in-
formation extraction using similarity of predicate ar-
gument structures. In Proceedings of the SIGDIAL
2011 Conference, pages 59?66.
511
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512?516,
Dublin, Ireland, August 23-24, 2014.
SAIL: Sentiment Analysis using Semantic Similarity and Contrast
Features
Nikolaos Malandrakis, Michael Falcone, Colin Vaz, Jesse Bisogni,
Alexandros Potamianos, Shrikanth Narayanan
Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
{malandra,mfalcone,cvaz,jbisogni}@usc.edu,
potam@telecom.tuc.gr, shri@sipi.usc.edu
Abstract
This paper describes our submission to Se-
mEval2014 Task 9: Sentiment Analysis in
Twitter. Our model is primarily a lexi-
con based one, augmented by some pre-
processing, including detection of Multi-
Word Expressions, negation propagation
and hashtag expansion and by the use of
pairwise semantic similarity at the tweet
level. Feature extraction is repeated for
sub-strings and contrasting sub-string fea-
tures are used to better capture complex
phenomena like sarcasm. The resulting
supervised system, using a Naive Bayes
model, achieved high performance in clas-
sifying entire tweets, ranking 7th on the
main set and 2nd when applied to sarcastic
tweets.
1 Introduction
The analysis of the emotional content of text is
relevant to numerous natural language process-
ing (NLP), web and multi-modal dialogue appli-
cations. In recent years the increased popularity
of social media and increased availability of rele-
vant data has led to a focus of scientific efforts on
the emotion expressed through social media, with
Twitter being the most common subject.
Sentiment analysis in Twitter is usually per-
formed by combining techniques used for related
tasks, like word-level (Esuli and Sebastiani, 2006;
Strapparava and Valitutti, 2004) and sentence-
level (Turney and Littman, 2002; Turney and
Littman, 2003) emotion extraction. Twitter how-
ever does present specific challenges: the breadth
of possible content is virtually unlimited, the writ-
ing style is informal, the use of orthography and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
grammar can be ?unconventional? and there are
unique artifacts like hashtags. Computation sys-
tems, like those submitted to SemEval 2013 task
2 (Nakov et al., 2013) mostly use bag-of-words
models with specific features added to model emo-
tion indicators like hashtags and emoticons (Davi-
dov et al., 2010).
This paper describes our submissions to Se-
mEval 2014 task 9 (Rosenthal et al., 2014), which
deals with sentiment analysis in twitter. The sys-
tem is an expansion of our submission to the same
task in 2013 (Malandrakis et al., 2013a), which
used only token rating statistics as features. We
expanded the system by using multiple lexica and
more statistics, added steps to the pre-processing
stage (including negation and multi-word expres-
sion handling), incorporated pairwise tweet-level
semantic similarities as features and finally per-
formed feature extraction on substrings and used
the partial features as indicators of irony, sarcasm
or humor.
2 Model Description
2.1 Preprocessing
POS-tagging / Tokenization was performed
using the ARK NLP tweeter tagger (Owoputi et
al., 2013), a Twitter-specific tagger.
Negations were detected using the list from
Christopher Potts? tutorial. All tokens up to the
next punctuation were marked as negated.
Hashtag expansion into word strings was per-
formed using a combination of a word insertion
Finite State Machine and a language model. A
normalized perplexity threshold was used to
detect if the output was a ?proper? English string
and expansion was not performed if it was not.
Multi-word Expressions (MWEs) were detected
using the MIT jMWE library (Kulkarni and
Finlayson, 2011). MWEs are non-compositional
expressions (Sag et al., 2002), which should be
512
handled as a single token instead of attempting to
reconstruct their meaning from their parts.
2.2 Lexicon-based features
The core of the system was formed by the lexicon-
based features. We used a total of four lexica and
some derivatives.
2.2.1 Third party lexica
We used three third party affective lexica.
SentiWordNet (Esuli and Sebastiani, 2006) pro-
vides continuous positive, negative and neutral rat-
ings for each sense of every word in WordNet.
We created two versions of SentiWordNet: one
where ratings are averaged over all senses of a
word (e.g., one ratings for ?good?) and one where
ratings are averaged over lexeme-pos pairs (e.g.,
one rating for the adjective ?good? and one for the
noun ?good?).
NRC Hashtag (Mohammad et al., 2013) Senti-
ment Lexicon provides continuous polarity ratings
for tokens, generated from a collection of tweets
that had a positive or a negative word hashtag.
Sentiment140 (Mohammad et al., 2013) Lexi-
con provides continuous polarity ratings for to-
kens, generated from the sentiment140 corpus of
1.6 million tweets, with emoticons used as posi-
tive and negative labels.
2.2.2 Emotiword: expansion and adaptation
To create our own lexicon we used an automated
algorithm of affective lexicon expansion based on
the one presented in (Malandrakis et al., 2011;
Malandrakis et al., 2013b), which in turn is an ex-
pansion of (Turney and Littman, 2002).
We assume that the continuous (in [?1, 1]) va-
lence, arousal and dominance ratings of any term
t
j
can be represented as a linear combination of
its semantic similarities d
ij
to a set of seed words
w
i
and the known affective ratings of these words
v(w
i
), as follows:
v?(t
j
) = a
0
+
N
?
i=1
a
i
v(w
i
) d
ij
, (1)
where a
i
is the weight corresponding to seed word
w
i
(that is estimated as described next). For the
purposes of this work, d
ij
is the cosine similarity
between context vectors computed over a corpus
of 116 million web snippets (up to 1000 for each
word in the Aspell spellchecker) collected using
the Yahoo! search engine.
Given the starting, manually annotated, lexi-
con Affective Norms for English Words (Bradley
and Lang, 1999) we selected 600 out of the 1034
words contained in it to serve as seed words and
all 1034 words to act as the training set and used
Least Squares Estimation to estimate the weights
a
i
. Seed word selection was performed by a sim-
ple heuristic: we want seed words to have extreme
affective ratings (high absolute value) and the set
to be close to balanced (sum of seed ratings equal
to zero). The equation learned was used to gener-
ate ratings for any new terms.
The lexicon created by this method is task-
independent, since both the starting lexicon and
the raw text corpus are task-independent. To cre-
ate task-specific lexica we used corpus filtering on
the 116 million sentences to select ones that match
our domain, using either a normalized perplex-
ity threshold (using a maximum likelihood trigram
model created from the training set tweets) or a
combination of pragmatic constraints (keywords
with high mutual information with the task) and
perplexity threshold (Malandrakis et al., 2014).
Then we re-calculated semantic similarities on the
filtered corpora. In total we created three lexica: a
task-independent (base) version and two adapted
versions (filtered by perplexity alone and filtered
by combining pragmatics and perplexity), all con-
taining valence, arousal and dominance token rat-
ings.
2.2.3 Statistics extraction
The lexica provide up to 17 ratings for each to-
ken. To extract tweet-level features we used sim-
ple statistics and selection criteria. First, all token
unigrams and bigrams contained in a tweet were
collected. Some of these n-grams were selected
based on a criterion: POS tags, whether a token is
(part of) a MWE, is negated or was expanded from
a hashtag. The criteria were applied separately
to token unigrams and token bigrams (POS tags
only applied to unigrams). Then ratings statistics
were extracted from the selected n-grams: length
(cardinality), min, max, max amplitude, sum, av-
erage, range (max minus min), standard deviation
and variance. We also created normalized versions
by dividing by the same statistics calculated over
all tokens, e.g., the maximum of adjectives over
the maximum of all unigrams. The results of this
process are features like ?maximum of Emotiword
valence over unigram adjectives? and ?average of
SentiWordNet objectivity among MWE bigrams?.
513
2.3 Tweet-level similarity ratings
Our lexicon was formed under the assumption
that semantic similarity implies affective similar-
ity, which should apply to larger lexical units like
entire tweets. To estimate semantic similarity
scores between tweets we used the publicly avail-
able TakeLab semantic similarity toolkit (
?
Sari?c et
al., 2012) which is based on a submission to Se-
mEval 2012 task 6 (Agirre et al., 2012). We used
the data of SemEval 2012 task 6 to train three
semantic similarity models corresponding to the
three datasets of that task, plus an overall model.
Using these models we created four similarity rat-
ings between each tweet of interest and each tweet
in the training set. These similarity ratings were
used as features of the final model.
2.4 Character features
Capitalization features are frequencies and rela-
tive frequencies at the word and letter level, ex-
tracted from all words that either start with a capi-
tal letter, have a capital letter in them (but the first
letter is non-capital) or are in all capital letters.
Punctuation features are frequencies, relative fre-
quencies and punctuation unigrams.
Character repetition features are frequencies,
relative frequencies and longest string statistics of
words containing a repetition of the same letter.
Emoticon features are frequencies, relative fre-
quencies, and emoticon unigrams.
2.5 Contrast features
Cognitive Dissonance is an important phe-
nomenon associated with complex linguistic cases
like sarcasm, irony and humor (Reyes et al., 2012).
To estimate it we used a simple approach, inspired
by one-liner joke detection: we assumed that the
final few tokens of each tweet (the ?suffix?) con-
trast the rest of the tweet (the ?prefix?) and created
split versions of the tweet where the last N tokens
are the suffix and all other tokens are the prefix,
for N = 2 and N = 3. We repeated the fea-
ture extraction process for all features mentioned
above (except for the semantic similarity features)
for the prefix and suffix, nearly tripling the total
number of features.
2.6 Feature selection and Training
The extraction process lead to tens of thousands
of candidate features, so we performed forward
stepwise feature selection using a correlation crite-
Table 1: Performance and rank achieved by our
submission for all datasets of subtasks A and B.
task dataset avg. F1 rank
A
LJ2014 70.62 16
SMS2013 74.46 16
TW2013 78.47 14
TW2014 76.89 13
TW2014SC 65.56 15
B
LJ2014 69.34 15
SMS2013 56.98 24
TW2013 66.80 10
TW2014 67.77 7
TW2014SC 57.26 2
rion (Hall, 1999) and used the resulting set of 222
features to train a model. The model chosen is a
Naive Bayes tree, a tree with Naive Bayes clas-
sifiers on each leaf. The motivation comes from
considering this a two stage problem: subjectivity
detection and polarity classification, making a hi-
erarchical model a natural choice. The feature se-
lection and model training/classification was con-
ducted using Weka (Witten and Frank, 2000).
Table 2: Selected features for subtask B.
Features number
Lexicon-derived 178
By lexicon
Ewrd / S140 / SWNet / NRC 71 / 53 / 33 / 21
By POS tag
all (ignore tag) 103
adj / verb / proper noun 25 / 11 / 11
other tags 28
By function
avg / min / sum / max 45 / 40 / 38 / 26
other functions 29
Semantic similarity 29
Punctuation 7
Emoticon 5
Other features 3
Contrast 72
prefix / suffix 54 / 18
3 Results
We took part in subtasks A and B of SemEval
2014 task 9, submitting constrained runs trained
with the data the task organizers provided. Sub-
task B was the priority and the subtask A model
was created as an afterthought: it only uses the
lexicon-based and morphology features for the tar-
get string and the entire tweet as features of an NB
Tree.
The overall performance of our submission
on all datasets (LiveJournal, SMS, Twitter 2013,
Twitter 2014 and Twitter 2014 Sarcasm) can be
seen in Table 1. The subtask A system performed
514
Table 3: Performance on all data sets of subtask B after removing 1 set of features. Performance differ-
ence with the complete system listed if greater than 1%.
Features removed
LJ2014 SMS2013 TW2013 TW2014 TW2014SC
avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff
None (Submitted) 69.3 57.0 66.8 67.8 57.3
Lexicon-derived 43.6 -25.8 38.2 -18.8 49.5 -17.4 51.5 -16.3 43.5 -13.8
Emotiword 67.5 -1.9 56.4 63.5 -3.3 66.1 -1.7 54.8 -2.5
Base 68.4 56.3 65.0 -1.9 66.4 -1.4 59.6 2.3
Adapted 69.3 57.4 66.7 67.5 50.8 -6.5
Sentiment140 68.1 -1.3 54.5 -2.5 64.4 -2.4 64.2 -3.6 45.4 -11.9
NRC Tag 70.6 1.3 58.5 1.6 66.3 66.0 -1.7 55.3 -2.0
SentiWordNet 68.7 56.0 66.2 68.1 52.7 -4.6
per Lexeme 69.3 56.7 66.1 68.0 52.7 -4.5
per Lexeme-POS 68.8 57.1 66.7 67.4 55.0 -2.2
Semantic Similarity 69.0 58.2 1.2 64.9 -2.0 65.5 -2.2 52.2 -5.0
Punctuation 69.7 57.4 66.6 67.1 53.9 -3.4
Emoticon 69.3 57.0 66.8 67.8 57.3
Contrast 69.2 57.5 66.7 67.0 51.9 -5.4
Prefix 69.5 57.2 66.8 67.2 47.4 -9.9
Suffix 68.6 57.2 66.5 67.9 56.3
badly, ranking near the bottom (among 20 submis-
sions) on all datasets, a result perhaps expected
given the limited attention we gave to the model.
The subtask B system did very well on the three
Twitter datasets, ranking near the top (among 42
teams) on all three sets and placing second on the
sarcastic tweets set, but did notably worse on the
two non-Twitter sets.
A compact list of the features selected by the
subtask B system can be seen in Table 2. The ma-
jority of features (178 of 222) are lexicon-based,
29 are semantic similarities to known tweets and
the rest are mainly punctuation and emoticon fea-
tures. The lexicon-based features mostly come
from Emotiword, though that is probably because
Emotiword contains a rating for every unigram
and bigram in the tweets, unlike the other lexica.
The most important part-of-speech tags are adjec-
tives and verbs, as expected, with proper nouns
being also highly important, presumably as indi-
cators of attribution. Still, most features are cal-
culated over all tokens (including stop words). Fi-
nally it is worth noting the 72 contrast features se-
lected.
We also conducted a set of experiments using
partial feature sets: each time we use all features
minus one set, then apply feature selection and
classification. The results are presented in Ta-
ble 3. As expected, the lexicon-based features are
the most important ones by a wide margin though
the relative usefulness of the lexica changes de-
pending on the dataset: the twitter-specific NRC
lexicon actually hurts performance on non-tweets,
while the task-independent Emotiword hurts per-
formance on the sarcastic tweets set. Overall
though using all is the optimal choice. Among the
other features only semantic similarity provides a
relatively consistent improvement.
A lot of features provide very little benefit on
most sets, but virtually everything is important for
the sarcasm set. Lexica, particularly the twitter
specific ones like Sentiment 140 and the adapted
version of Emotiword make a big difference, per-
haps indicating some domain-specific aspects of
sarcasm expression (though such assumptions are
shaky at best due to the small size of the test
set). The contrast features perform their intended
function well, providing a large performance boost
when dealing with sarcastic tweets and perhaps
explaining our high ranking on that dataset.
Overall the subtask B system performed very
well and the semantic similarity features and con-
trast features provide potential for further growth.
4 Conclusions
We presented a system of twitter sentiment anal-
ysis combining lexicon-based features with se-
mantic similarity and contrast features. The sys-
tem proved very successful, achieving high ranks
among all competing systems in the tasks of senti-
ment analysis of generic and sarcastic tweets.
Future work will focus on the semantic similar-
ity and contrast features by attempting more accu-
rately estimate semantic similarity and using some
more systematic way of identifying the ?contrast-
ing? text areas.
515
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In proc. Se-
mEval, pages 385?393.
Margaret Bradley and Peter Lang. 1999. Affective
Norms for English Words (ANEW): Stimuli, in-
struction manual and affective ratings. technical re-
port C-1. The Center for Research in Psychophysi-
ology, University of Florida.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proc. COLING, pages 241?249.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. In Proc. LREC, pages 417?422.
Mark A. Hall. 1999. Correlation-based feature selec-
tion for machine learning. Ph.D. thesis, The Univer-
sity of Waikato.
Nidhi Kulkarni and Mark Alan Finlayson. 2011.
jMWE: A java toolkit for detecting multi-word ex-
pressions. In proc. Workshop on Multiword Expres-
sions, pages 122?124.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2011. Kernel mod-
els for affective lexicon creation. In Proc. Inter-
speech, pages 2977?2980.
Nikolaos Malandrakis, Abe Kazemzadeh, Alexandros
Potamianos, and Shrikanth Narayanan. 2013a.
SAIL: A hybrid approach to sentiment analysis. In
proc. SemEval, pages 438?442.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2013b. Distri-
butional semantic models for affective text analy-
sis. Audio, Speech, and Language Processing, IEEE
Transactions on, 21(11):2379?2392.
Nikolaos Malandrakis, Alexandros Potamianos,
Kean J. Hsu, Kalina N. Babeva, Michelle C. Feng,
Gerald C. Davison, and Shrikanth Narayanan. 2014.
Affective language model adaptation via corpus
selection. In proc. ICASSP, pages 4871?4874.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In proc. Se-
mEval, pages 321?327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment analysis in
Twitter. In Proc. SemEval, pages 312?320.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
proc. NAACL, pages 380?390.
Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &
Knowledge Engineering, 74(0):1 ? 12.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment analysis in Twitter. In Proc. SemEval.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Compu-
tational Linguistics and Intelligent Text Processing,
volume 2276 of Lecture Notes in Computer Science,
pages 189?206.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-Affect: an affective extension of WordNet.
In Proc. LREC, volume 4, pages 1083?1086.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. technical report ERC-
1094 (NRC 44929). National Research Council of
Canada.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21:315?346.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Systems
for measuring semantic text similarity. In proc. Se-
mEval, pages 441?448.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann.
516
