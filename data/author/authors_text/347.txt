Proceedings of NAACL HLT 2007, pages 364?371,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Log-linear Block Transliteration Model based on Bi-Stream HMMs
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vogel
{bzhao, nbach, ianlane, vogel}@cs.cmu.edu
Language Technologies Institute
School of Computer Science, Carnegie Mellon University
Abstract
We propose a novel HMM-based framework to
accurately transliterate unseen named entities.
The framework leverages features in letter-
alignment and letter n-gram pairs learned from
available bilingual dictionaries. Letter-classes,
such as vowels/non-vowels, are integrated to
further improve transliteration accuracy. The
proposed transliteration system is applied to
out-of-vocabulary named-entities in statistical
machine translation (SMT), and a significant
improvement over traditional transliteration ap-
proach is obtained. Furthermore, by incor-
porating an automatic spell-checker based on
statistics collected from web search engines,
transliteration accuracy is further improved.
The proposed system is implemented within
our SMT system and applied to a real transla-
tion scenario from Arabic to English.
1 Introduction
Cross-lingual natural language applications, such as in-
formation retrieval, question answering, and machine
translation for web-documents (e.g. Google translation),
are becoming increasingly important. However, current
state-of-the-art statistical machine translation (SMT) sys-
tems cannot yet translate named-entities which are not
seen during training. New named-entities, such as per-
son, organization, and location names are continually
emerging on the World-Wide-Web. To realize effective
cross-lingual natural language applications, handling out-
of-vocabulary named-entities is becoming more crucial.
Named entities (NEs) can be translated via transliter-
ation: mapping symbols from one writing system to an-
other. Letters of the source language are typically trans-
formed into the target language with similar pronunci-
ation. Transliteration between languages which share
similar alphabets and sound systems is usually not dif-
ficult, because the majority of letters remain the same.
However, the task is significantly more difficult when the
language pairs are considerably different, for example,
English-Arabic, English-Chinese, and English-Japanese.
In this paper, we focus on forward transliteration from
Arabic to English.
The work in (Arbabi et al, 1994), to our knowledge, is
the first work on machine transliteration of Arabic names
into English, French, and Spanish. The idea is to vow-
elize Arabic names by adding appropriate vowels and uti-
lizing a phonetic look-up table to provide the spelling in
the target language. Their framework is strictly applica-
ble within standard Arabic morphological rules. Knight
and Graehl (1997) introduced finite state transducers that
implement back-transliteration from Japanese to English,
which was then extended to Arabic-English in (Stalls and
Knight, 1998). Al-Onaizan and Knight (2002) translit-
erated named entities in Arabic text to English by com-
bining phonetic-based and spelling-based models, and re-
ranking candidates with full-name web counts, named en-
tities co-reference, and contextual web counts. Huang
(2005) proposed a specific model for Chinese-English
name transliteration with clusterings of names? origins,
and appropriate hypotheses are generated given the ori-
gins. All of these approaches, however, are not based
on a SMT-framework. Technologies developed for SMT
are borrowed in Virga and Khudanpur (2003) and Ab-
dulJaleel and Larkey (2003). Standard SMT alignment
models (Brown et al, 1993) are used to align letter-pairs
within named entity pairs for transliteration. Their ap-
proach are generative models for letter-to-letter transla-
tions, and the letter-alignment is augmented with heuris-
tics. Letter-level contextual information is shown to be
very helpful for transliteration. Oh and Choi (2002)
used conversion units for English-Korean Transliteration;
Goto et al (2003) used conversion units, mapping En-
glish letter-sequence into Japanese Katakana character
string. Li et al (2004) presented a framework allowing
direct orthographical mapping of transliteration units be-
tween English and Chinese, and an extended model is
presented in Ekbal et al (2006).
We propose a block-level transliteration framework, as
shown in Figure 1, to model letter-level context infor-
mation for transliteration at two levels. First, we pro-
pose a bi-stream HMM incorporating letter-clusters to
better model the vowel and non-vowel transliterations
with position-information, i.e., initial and final, to im-
prove the letter-level alignment accuracy. Second, based
on the letter-alignment, we propose letter n-gram (letter-
sequence) alignment models (block) to automatically
learn the mappings from source letter n-grams to target
letter n-grams. A few features specific for transliterations
are explored, and a log-linear model is used to combine
364
Figure 1: Transliteration System Structure. The upper-part is
the two-directional Bi-Stream HMM for letter-alignment; the
lower-part is a log-linear model for combining different feature
functions for block-level transliteration.
these features to learn block-level transliteration-pairs
from training data. The proposed transliteration frame-
work obtained significant improvements over a strong
baseline transliteration approach similar to AbdulJaleel
and Larkey (2003) and Virga and Khudanpur (2003).
The remainder of this paper is organized as follows.
In Section 2, we formulate the transliteration as a general
translation problem; in Section 4, we propose a log-linear
alignment model with a local search algorithm to model
the letter n-gram translation pairs; in Section 5, exper-
iments are presented. Conclusions and discussions are
given in Section 6.
2 Transliteration as Translation
Transliteration can be viewed as a special case of transla-
tion. In this approach, source and target NEs are split into
letter sequences, and each sequence is treated as a pseudo
sentence. The appealing reason of formulating transliter-
ation in this way is to utilize advanced alignment models,
which share ideas applied also within phrase-based sta-
tistical machine translation (Koehn, 2004).
To apply this approach to transliteration, however,
some unique aspects should be considered. First, letters
should be generated from left to right, without any re-
ordering. Thus, the transliteration models can only exe-
cute forward sequential jumps. Second, for unvowelized
languages such as Arabic, a single Arabic letter typically
maps to less than four English letters. Thus, the fertility
for each letter should be recognized to ensure reasonable
length relevance. Third, the position of the letter within
a NE is important. For example, in Arabic, letters such
as ?al? at the beginning of the NE can only be translated
into ?the? or ?al?. Therefore position information should
be considered within the alignment models.
Incorporating the above considerations, transliteration
can be formulated as a noisy channel model. Let fJ1 =
f1f2...fJ denote the source NE with J letters, eI1 =
e1e2...eI be an English transliteration candidate with I
letters. According to Bayesian decision rule:
e?I1=argmax
{eI1}
P (eI1|fJ1 )= argmax
{eI1}
P (fJ1 |eI1)P (eI1), (1)
where P (fJ1 |eI1) is the letter translation model and P (eI1)
is the English letter sequence model corresponding to
the monolingual language models in SMT. In this noisy-
channel scheme, P (fJ1 |eI1) is the key component for
transliteration, in which the transliteration between eI1
and fJ1 can be modeled at either letter-to-letter level, or
letter n-gram transliteration level (block-level).
Our transliteration models are illustrated in Figure 1.
We propose a Bi-Stream HMM of P (fJ1 |eI1) to infer
letter-to-letter alignments in two directions: Arabic-to-
English (F-to-E) and English-to-Arabic (E-to-F), shown
in the upper-part in Figure 1; refined alignment is then
obtained. We propose a log-linear model to extract block-
level transliterations with additional informative features,
as illustrated in the lower-part of Figure 1.
3 Bi-Stream HMMs for Transliteration
Standard IBM translation models (Brown et al, 1993)
can be used to obtain letter-to-letter translations. How-
ever, these models are not directly suitable, because
letter-alignment within NEs is strictly left-to-right. This
sequential property is well suited to HMMs (Vogel et al,
1996), in which the jumps from the current aligned posi-
tion can only be forward.
3.1 Bi-Stream HMMs
We propose a bi-stream HMM for letter-alignment within
NE pairs. For the source NE fJ1 and a target NE eI1, a bi-
stream HMM is defined as follows:
p(fJ1 |eI1)=
?
aJ1
J?
j=1
p(fj |eaj )p(cfj |ceaj )p(aj |aj?1), (2)
where aj maps fj to the English letter eaj at the position
aj in the English named entity. p(aj |aj?1) is the transi-
tion probability distribution assuming first-order Markov
dependency; p(fj |eaj ) is a letter-to-letter translation lex-
icon; cfj is the letter cluster of fj and p(cfj |ceaj ) is a
cluster level translation lexicon. As mentioned in the
above, the vowel/non-vowel linguistic features can be uti-
lized to cluster the letters. The letters from the same clus-
ter tend to share the similar letter transliteration forms.
p(cfj |ceaj ) enables to leverage such letter-correlation in
the transliteration process.
The HMM in Eqn. 2 generates two streams of observa-
tions: the letters together with the letters? classes follow-
ing the distribution of p(fj |eaj ) and p(cfj |ceaj ) at each
365
Figure 2: Block of letters for transliteration. A block is defined
by the left- and right- boundaries in the NE-pair.
state, respectively. To be in accordance with the mono-
tone nature of the NE?s alignment mentioned before, we
enforce the following constraints in Eqn. 3, so that the
transition can only jump forward or stay at the same state:
aj?aj?1?0 ?j ? [1, J ]. (3)
Since the two streams are conditionally independent
given the current state, the extended EM is straight-
forward, with only small modifications of the standard
forward-backward algorithm (Zhao et al, 2005), for pa-
rameter estimation.
3.2 Designing Letter-Classes
Pronunciation is typically highly structured. For in-
stance, in English the pronunciation structure of ?cvc?
(consonant-vowel-consonant) is common. By incorpo-
rating letter classes into the proposed two-stream HMM,
the models? expressiveness and robustness can be im-
proved. In this work, we focus on transliteration of Ara-
bic NEs into English. We define six non-overlapping
letter classes: vowel, consonant, initial, final, noclass,
and unknown. Initial and final classes represent semantic
markers at the beginning or end of NEs such as ?Al? and
?wAl? (in romanization form). Noclass signifies letters
which can be pronounced as both a vowel and a conso-
nant depending on context, for example, the English let-
ter ?y?. The unknown class is reserved for punctuations
and letters that we do not have enough linguistic clues for
mapping them to phonemes.
4 Transliteration Blocks
To further leverage the information from the letter-
context beyond the letter-classes incorporated in our bi-
stream HMM in Eqn. 2, we define letter n-grams, which
consist of n consecutive letters, as the basic transliter-
ation unit. A block is defined as a pair of such letter
n-grams which are transliterations of each other. Dur-
ing decoding of unseen NEs, transliteration is performed
block-by-block, rather than letter-by-letter. The goal of
transliteration model is to learn high-quality translitera-
tion blocks from the training data in a unsupervised fash-
ion.
Specifically, a block X can be represented by its left
and right boundaries in the source and target NEs shown
in Figure 2:
X = (f j+lj , ei+ki ), (4)
where f j+lj is the source letter-ngram with (l+1) letters
in source language, and its projection of ei+ki in the En-
glish NE with left boundary at the position of i, and right
boundary at (i+ k).
We formulate the block extraction as a local search
problem following the work in Zhao and Waibel (2005):
given a source letter n-gram f j+lj , search for the pro-
jected boundaries of candidate target letter n-gram ei+ki
according to a weighted combination of the diverse fea-
tures in a log-linear model detailed in ?4.3. The log-linear
model serves as a performance measure to guide the local
search, which, in our setup, is randomized hill-climbing,
to extract bilingual letter n-gram transliteration pairs.
4.1 Features for Block Transliteration
Three features: fertility, distortion, and lexical transla-
tion are investigated for inferring transliteration blocks
from the NE pairs. Each feature corresponds to one as-
pect of the block within the context of a given NE pair.
4.1.1 Letter n-gram Fertility
The fertility P (?|e) of a target letter e specifies the
probability of generating ? source letters for translitera-
tion. The fertilities can be easily read-off from the letter-
alignment, i.e., the output from the Bi-stream HMM.
Given letter fertility model P (?|ei), a target letter n-gram
eI1, and a source n-gram fJ1 of length J , we compute a
probability of letter n-gram length relevance: P (J |eI1)
via a dynamic programming.
The probability of generating J letters by the English
letter n-gram eI1 is defined:
P (J |eI1) = max{?I1,J=?Ii=1 ?i}
I?
i=1
P (?i|ei). (5)
The recursively updated cost ?[j, i] in dynamic program-
ming is defined as follows:
?[j, i] = max
?
???
???
?[j, i? 1] + logPNull(0|ei)
?[j ? 1, i? 1] + logP?(1|ei)
?[j ? 2, i? 1] + logP?(2|ei)
?[j ? 3, i? 1] + logP?(3|ei)
, (6)
where PNull(0|ei) is the probability of generating a Null
letter from ei; P?(k=1|ei) is the letter-fertility model of
generating one source letter from ei; ?[j, i] is the cost
366
so far for generating j letters from i consecutive English
letters (letter n-gram) ei1 : e1, ? ? ? , ei.
After computing the cost of ?[J, I], the probability
P (J |eI1) is computed for generating the length of the
source NE fJ1 from the English NE eI1 shown in Eqn. 5.
With this letter n-gram fertility model, for every block,
we can compute a fertility score to estimate how relevant
the lengths of the transliteration-pairs are.
4.1.2 Distortion of Centers
When aligning blocks of letters within transliteration
pairs, we expect most of them are close to the diagonal
due to the monotone alignment nature. Thus, a simple
position metric is proposed for each block considering
the relative positions within NE-pairs.
The center ?fj+lj of the source phrase f
j+l
j with a
length of (l + 1) is simply a normalized relative position
in the source entity defined as follows:
?fj+lj =
1
l + 1
j?=j+l?
j?=j
j?
l + 1 . (7)
For the center of English letter-phrase ei+ki , we first
define the expected corresponding relative center for ev-
ery source letter fj? using the lexicalized position score
as follows:
?ei+ki (fj?) =
1
k + 1 ?
?(i+k)
i?=i i? ? P (fj? |ei?)?(i+k)
i?=i P (fj? |ei?)
, (8)
where P (fj? |ei) is the letter translation lexicon estimated
in IBM Models 1?5. i is the position index, which
is weighted by the letter-level translation probabilities;
the term of
?i+k
i?=i P (fj? |ei?) provides a normalization so
that the expected center is within the range of the target
length. The expected center for ei+ki is simply the aver-
age of the ?ei+ki (fj?):
?ei+ki =
1
l + 1
j+l?
j?=j
?ei+ki (fj?) (9)
Given the estimated centers of ?fj+lj and ?ei+ki , we
can compute how close they are via the probability of
P (?fj+lj |?ei+ki ). In our case, because of the mono-
tone alignment nature of transliteration pairs, a simple
gaussian model is employed to enforce that the point
(?ei+ki ,?fj+lj ) is not far away from the diagonal.
4.1.3 Letter Lexical Transliteration
Similar to IBM Model-1 (Brown et al, 1993), we use
a ?bag-of-letter? generative model within a block to ap-
proximate the lexical transliteration equivalence:
P (f j+lj |ei+ki )=
j+l?
j?=j
i+k?
i?=i
P (fj? |ei?)P (ei? |ei+ki ), (10)
where P (ei? |ei+ki ) ' 1/(k+1) is approximated by a bag-
of-word unigram. Since named entities are usually rela-
tively short, this approximation works reasonably well in
practice.
4.2 Extended Feature Functions
Because of the underlying nature of the noisy-channel
model in our proposed transliteration approach in Section
2, the three base feature functions are extended to cover
the directions both from target-to-source and source-to-
target. Therefore, we have in total six feature functions
for inferring transliteration blocks from a named entity
pair.
Besides the above six feature functions, we also com-
pute the average letter-alignment links per block. We
count the number of letter-alignment links within the
block, and normalize the number by the length of the
source letter-ngram. Note that, we can refine the letter-
alignment by growing the intersections of the two di-
rection letter-alignments from Bi-stream HMM via ad-
ditional aligned letter-pairs seen in the union of the two.
In a way, this approach is similar to those of refining the
word-level alignment for SMT in (Och and Ney, 2003).
This step is shown in the upper-part in Figure 1.
Overall, our proposed feature functions cover rela-
tively different aspects for transliteration blocks: the
block level length relevance probability in Eqn. 5, lexical
translation equivalence, and positions? distortion from a
gaussian distribution in Eqn. 8, in both directions; and
the average number of letter-alignment links within the
block. Also, these feature functions are positive and
bounded within [0, 1]. Therefore, it is suitable to apply a
log-linear model (in ?4.3) to combine the weighted indi-
vidual strengths from the proposed feature functions for
better modeling the quality of the candidate translitera-
tion blocks. This log-linear model will serve as a per-
formance measure in a local-search in ?4.4 for inferring
transliteration blocks.
4.3 Log-Linear Transliteration Model
We propose a log-linear model to combine the seven fea-
ture functions in ?4.1 with proper weights as in Eqn. 11:
Pr(X|e, f)= exp(
?M
m=1 ?m?m(X, e, f))?
{X?} exp(
?M
m=1 ?m?m(X ?, e, f))
,
(11)
where ?m(X, e, f) are the real-valued bounded feature
functions corresponding to the seven models introduced
in ?4.1. The log-linear model?s parameters are the
weights {?m} associated with each feature function.
With hand-labeled data, {?m} can be learnt via gen-
eralized iterative scaling algorithm (GIS) (Darroch and
Ratcliff, 1972) or improved iterative scaling (IIS) (Berger
367
et al, 1996). However, as these algorithms are computa-
tionally expensive, we apply an alternative approach us-
ing a simplex down-hill algorithm to optimize the weights
toward better F-measure of block transliterations. Each
feature function corresponds to one dimension in the sim-
plex, and the local optimum only happens at a vertex of
the simplex. Simplex-downhill has several advantages:
it is an efficient approach for optimizing multi-variables
given some performance measure. We compute the F-
measure against a gold-standard block set extracted from
hand-labeled letter-alignment.
To build gold-standard blocks from hand-labeled
letter-alignment, we propose the block transliteration co-
herence in a two-stage fashion. First is the forward pro-
jection: for each candidate source letter-ngram f j+nj ,
search for its left-most el and right-most er projected
positions in the target NE according to the given letter-
alignment. Second is the backward projection: for the
target letter-gram erl , search for its left-most fl? and right-
most fr? projected positions in the source NE. Now if
l??j and r??j+n, i.e. frl is contained within the source
letter-ngram f j+nj , then this block X = (f j+nj , erl ) is de-
fined as coherent for the aligned pairs: (f j+nj , erl ) . We
accept coherent X as gold-standard blocks. This block
transliteration coherence is generally sound for extracting
the gold-blocks mostly because of the the monotone left-
to-right nature of the letter-alignment for transliteration.
A related coherence assumption can be found in (Fox,
2002), where their assumption on phrase-pairs for sta-
tistical machine translation is shown to be somewhat re-
strictive for SMT. This is mainly because the word align-
ment is often non-monotone, especially for langauge-
pairs from different families such as Arabic-English and
Chinese-English.
4.4 Aligning Letter-Blocks: a Local Search
Aligning the blocks within NE pairs can be formulated
as a local search given the heuristic function defined in
Eqn. 11. To be more specific: given a Arabic letter-ngram
f j+lj , our algorithm searches for the best translation can-
didate ei+ki in the target named entities. In our implemen-
tation, we use stochastic hill-climbing with Eqn. 11 as the
performance measure. Down-hill moves are accepted to
allow one or two left and right null letters to be attached
to ei+ki to expand the table of transliteration-blocks.
To make the local search more effective, we normal-
ize the letter translation lexicon p(f |e) within the parallel
entity pair as in:
P? (f |e) = P (f |e)?J
j?=1 P (fj? |e)
. (12)
In this way, the distribution of P? (f |e) is sharper and more
focused in the context of an entity pair.
Overall, given the parallel NE pairs, we can train the
letter level translation models in both directions via the
Bi-stream HMM in Eqn. 2. From the letter-alignment,
we can build the letter translation lexicons and fertility
tables. With these tables, the base feature functions are
then computed for each candidate block, and the features
are combined in the log-linear model in Eqn. 11. Given
a named-entity pair in the training data, we rank all the
transliteration blocks by the scores using the log-linear
model. This step is shown in the lower-part in Figure 1.
4.5 Decoding Unseen NEs
The decoding of NEs is an extension to the noisy-channel
scheme in Eqn. 1. In our configurations for NE translit-
eration, the extracted transliteration blocks are used. Our
letter ngram is a standard letter-ngram model trained us-
ing the SriLM toolkit (Stolcke, 2002). To transliterate the
unseen NEs, the decoder (Hewavitharana et al, 2005) is
configured for monotone decoding. It loads the transliter-
ation blocks and the letter-ngram LM, and it decodes the
unseen Arabic named entities with block-based translit-
eration from left to right.
5 Experiments
5.1 The Data
We have 74,887 bilingual geographic names from
LDC2005G01-NGA, 11,212 bilingual person names
from LDC2005G021, and about 6,000 bilingual names
extracted from the BAMA2 dictionary. In total, there are
92,099 NE pairs. We split them into three parts: 91,459
pairs as the training dataset, 100 pairs as the development
dataset, and 540 unique NE pairs as the held-out dataset.
An additional test set is collected from the TIDES 2003
Arabic-English machine translation evaluation test set.
The 663 sentences contain 286 unique words, which were
not covered by the available training data. From this set
of untranslated words, we manually labeled the entities of
persons, locations and organizations, giving a total of 97
unique un-translated NEs. The BAMA toolkit was used
to romanize the Arabic words. Some names from this test
set are shown in Figure 1.
These untranslated NEs make up only a very small
fraction of all words in the test set. Therefore, having
correct transliterations would give only small improve-
ments in terms of BLEU (Papineni et al, 2002) and NIST
scores. However, successfully translating these unknown
NEs is very crucial for cross-lingual distillation tasks or
question-answering based on the MT-output.
1The corpus is provided as FOUO (for official use only) in
the DARPA-GALE project
2LDC2004L02: Buckwalter Arabic Morphological Ana-
lyzer version 2.0
368
Table 1: Test Set Examples.
To evaluate the transliteration performance, we use
edit-distance between the hypothesis against a reference
set. This is to count the number of insertions, dele-
tions, and substitutions required to correct the hypoth-
esis to match the given reference. An edit-distance of
zero is a perfect match. However, NEs typically have
more than one correct variant. For example, the Arabic
name ?mHmd? (in romanized form) can be transliterated
as Muhammad or Mohammed; both are considered as
correct transliterations. Ideally, we want to have all vari-
ants as reference transliterations. To enable our translit-
eration evaluation to be more informative given only one
reference, edit-distance of one between hypothesis and
reference is considered to be an acceptable match.
5.2 Comparison of Transliteration Models
We compare the performance of three systems within our
proposed framework in Figure.1: the baseline Block sys-
tem, a system in which we use a log-linear combination
of alignment features as described in ?4.3, we call the the
L-Block system, and finally a system, which also uses
the bi-stream HMM alignment model as described in ?3.
This last system will be denoted LCBE system.
The baseline is based on the refined letter-alignment
from the two directions of IBM-Model-4, trained with a
scheme of 15h545 using GIZA++ (Och and Ney, 2004).
The final alignment was obtained by growing the inter-
sections between Arabic-to-English (AE) and English-
to-Arabic (EA) alignments with additional aligned letter-
pairs seen in the union. This is to compensate for the
inherent asymmetry in alignment models. Blocks (letter-
ngram pairs) were collected directly from the refined
letter-alignment, using the same algorithm as described
in ?4.3 for extracting gold-standard letter blocks. There is
no length restrictions to the letter-ngram extracted in our
system. All the blocks were then scored using relative
frequencies and lexical scores in both directions, similar
to the scoring of phrase-pairs in SMT (Koehn, 2004).
In the L-Block system additional feature functions as
defined in ?4.1 were computed on top of the letter-level
alignment obtained from the baseline system. A log-
linear model combining these features was learned with
the gold-blocks described in ?4.3. Transliteration blocks
were extracted using the local-search ?4.4. The other
Table 2: Transliteration accuracy for different translitera-
tion models.
System Accuracy
Baseline 39.18%
L-Block 41.24%
LCBE 46.39%
components remained the same as in the baseline system.
The LCBE system is an extension to both the baseline
and the L-Block system. The key difference in LCBE
is that our proposed bi-stream HMM in Eqn. 2 was ap-
plied in both directions with extended letter-classes. The
resulting combined alignment was used together with all
features of the L-Block system to guide the local-search
for extracting the blocks. The same procedure of decod-
ing was then carried out for the unseen NEs using the
extracted blocks.
To build the letter language model for the decoding
process, we first split the English entities into charac-
ters; additional position indicators ? begin? and ? end?
were added to the begin and end position of the named-
entity; ? middle? was added between the first name and
last name. A letter-trigram language model with SRI LM
toolkit (Stolcke, 2002) was then built using the target side
(English) of NE pairs tagged with the above position in-
formation.
Table 2 shows that the baseline system gives an accu-
racy of 39.18%, while the extended systems L-Block and
LCBE give 41.24% and 46.39%, respectively. These re-
sults show that the additional features besides the letter-
alignment are helpful. The L-Block system, which uses
these features, outperforms the baseline system signifi-
cantly by 2.1% absolute in accuracy. The results also
show that the bi-stream HMM alignment, which uses not
only the letters but also the letter-classes, leads to signif-
icant improvement. It outperforms the L-Block system,
which does not leverage the letter-classes and monotone
alignment, by 4.15% absolute.
5.3 Incorporation of Spell Checking
Our spelling-checker is based on the suggested word-
forms from web search engines for ambiguous candi-
dates. We collected web statistics frequency for both the
proposed transliteration candidates from our system, and
also the suggested candidates from web-search engines.
All the candidates were re-ranked by their frequencies.
Figure 3 shows the performances on the held-out set,
using system LCBE augmented with a spell-checker
(LCBE+Spell), with varying sizes of N-best hypotheses
lists. The held-out set contains 540 unique named entity
pairs. We show accuracy when exact match is requested
and when an edit distances of one is allowed.
369
Figure 3: Transliteration accuracy of LCBE and LCBE+Spell
models for 540 named entity pairs in the held-out set.
Figure 4: Transliteration accuracy of N-best hypotheses for
LCBE and LCBE+Spell models it the MT-03 test set.
Figure 4 shows the performances in the unseen test set
of LCBE and LCBE+Spell, with varying sizes of N-best
hypotheses lists. LCBE+Spell reaches 52% accuracy in
1-best hypothesis. In the 5-best and 10-best cases, the ac-
curacies of LCBE+Spell system archive the highest per-
formances with 66% and 72.16% respectively. The spell-
checker increases the 1-best accuracy by 11.12% and the
10-best accuracy by 7.69%. All these improvements are
statistically significant. These results are also comparable
to other state-of-the-art statistical Arabic name transliter-
ation systems such as (Al-Onaizan and Knight, 2002).
5.4 Comparison with the Google Web Translation
We finally compared our best system with the
state-of-the-art Arabic-English Google Web Translation
(Google). Table 3 shows transliteration examples from
our best system in comparison with Google (as in June
20, 2006)3. The Google system achieved 45.36% accu-
racy for the 1-best hypothesis, which is comparable to
the results when using the LCBE transliteration system,
while LCBE+Spell archived 52%.
3http://www.google.com/translate t
Table 3: Transliteration examples between LCBE+Spell
and Google web translation.
6 Conclusions and Discussions
In this paper we proposed a novel transliteration model.
Viewing transliteration as a translation task we adopt
alignment and decoding techniques used in a phrase-
based statistical machine translation system to work on
letter sequences instead of word sequences. To improve
the performance we extended the HMM alignment model
into a bi-stream HMM alignment by incorporating letter-
classes into the alignment process. We also showed that a
block-extraction approach, which uses a log-linear com-
bination of multiple alignment features, can give signif-
icant improvements in transliteration accuracy. Finally,
spell-checking based on work occurrence statistics ob-
tained from the web gave an additional boost in translit-
eration accuracy.
The goal for this work is to improve the quality of ma-
chine translation, esp. when used in cross-lingual infor-
mation retrieval and distillation tasks, by incorporating
the proposed framework to handle unknown words. Fig-
ure 5 gives an example of the difference named entity
transliteration can make. Shown are the original SMT
system output, the translation when the proposed translit-
eration models are used to translate the unknown named-
entities, and the reference translation. A comparison of
the two SMT outputs indicates that integrating the pro-
posed transliteration model into our machine translation
system can significantly improve translation utility.
Acknowledgment
This work was partially supported by grants from
DARPA (GALE project) and NFS (Str-Dust project).
References
Nasreen AbdulJaleel and Leah Larkey. 2003. Statistical
transliteration for English-Arabic cross language informa-
tion retrieval. In Proceedings of the 12th International
Conference on Information and Knowledge Management,
New Orleans, LA, USA, November.
370
Figure 5: Incorporation of the transliteration model to our
SMT System.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of ACL
Workshop on Computational Approaches to Semitic Lan-
guages, Philadelphia, PA, USA.
Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng, and
Elizabeth Bart. 1994. Algorithms for Arabic name translit-
eration. In IBM Journal of Research and Development,
volume 38(2), pages 183?193.
Adam L. Berger, Vincent Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Linguistics,
volume 22 of 1, pages 39?71, March.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263?331.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. In Annals of Mathematical
Statistics, volume 43, pages 1470?1480.
Asif Ekbal, S. Naskar, and S. Bandyopadhyay. 2006. A modi-
fied joint source channel model for machine transliteration.
In Proceedings of COLING/ACL, pages 191?198, Australia.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 304?311,
Philadelphia, PA, July 6-7.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context informa-
tion based on the maximum entropy method. In Proceedings
of MT-Summit IX, New Orleans, Louisiana, USA.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand,
Matthias Eck, Chiori Hori, Stephan Vogel, and Alex Waibel.
2005. The CMU statistical machine translation system
for IWSLT2005. In The 2005 International Workshop on
Spoken Language Translation.
Fei Huang. 2005. Cluster-specific name transliteration. In
Proceedings of the HLT-EMNLP 2005, Vancouver, BC,
Canada, October.
Kevin Knight and Jonathan Graehl. 1997. Machine transliter-
ation. In Proceedings of the Conference of the Association
for Computational Linguistics (ACL), Madrid, Spain.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based smt. In Proceedings of the Conference of
the Association for Machine Translation in the Americans
(AMTA), Washington DC, USA.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-
channel model for machine transliteration. In Proceedings
of 42nd ACL, pages 159?166, Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 1:29, pages 19?51.
Franz J. Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. In Computa-
tional Linguistics, volume 30, pages 417?449.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-Korean
transliteration model using pronunciation and contextual
rules. In Proceedings of COLING-2002, pages 1?7, Taipei,
Taiwan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Bonnie Stalls and Kevin Knight. 1998. Translating names
and technical terms in Arabic text. In Proceedings of the
COLING/ACL Workshop on Computational Approaches to
Semitic Languages, Montreal, Quebec, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration
of proper names in cross-lingual information retrieval. In
Proceedings of the ACL Workshop on Multi-lingual Named
Entity Recognition, Edmonton, Canada.
Stephan. Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Computational
Lingustics, (COLING-1996), pages 836?841, Copenhagen,
Denmark.
Bing Zhao and Alex Waibel. 2005. Learning a log-linear
model with bilingual phrase-pair features for statistical
machine translation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, Jeju Island,
Korean, October.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual
word spectral clustering for statistical machine translation.
In Proceedings of the ACL Workshop on Building and Using
Parallel Texts, pages 25?32, Ann Arbor, Michigan, June.
371
Proceedings of NAACL HLT 2009: Short Papers, pages 1?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Cohesive Constraints in A Beam Search Phrase-based Decoder
Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, stephan.vogel}@cs.cmu.edu
Colin Cherry
Microsoft Research
One Microsoft Way
Redmond, WA, 98052, USA
collinc@microsoft.com
Abstract
Cohesive constraints allow the phrase-based decoder
to employ arbitrary, non-syntactic phrases, and en-
courage it to translate those phrases in an order that
respects the source dependency tree structure. We
present extensions of the cohesive constraints, such
as exhaustive interruption count and rich interrup-
tion check. We show that the cohesion-enhanced de-
coder significantly outperforms the standard phrase-
based decoder on English?Spanish. Improvements
between 0.5 and 1.2 BLEU point are obtained on
English?Iraqi system.
1 Introduction
Phrase-based machine translation is driven by a phrasal
translation model, which relates phrases (contiguous seg-
ments of words) in the source to phrases in the tar-
get. This translation model can be derived from a word-
aligned bitext. Translation candidates are scored accord-
ing to a linear model combining several informative fea-
ture functions. Crucially, this model incorporates trans-
lation model scores and n-gram language model scores.
The component features are weighted to minimize a
translation error criterion on a development set (Och,
2003). Decoding the source sentence takes the form of
a beam search through the translation space, with inter-
mediate states corresponding to partial translations. The
decoding process advances by extending a state with the
translation of a source phrase, until each source word has
been translated exactly once. Re-ordering occurs when
the source phrase to be translated does not immediately
follow the previously translated phrase. This is penalized
with a discriminatively-trained distortion penalty. In or-
der to calculate the current translation score, each state
can be represented by a triple:
? A coverage vector HC indicates which source words
have already been translated.
? A span f? indicates the last source phrase translated
to create this state.
? A target word sequence stores context needed by the
target language model.
As cohesion concerns only movement in the source, we
can completely ignore the language model context, mak-
ing state effectively an (f? ,HC ) tuple.
To enforce cohesion during the state expansion pro-
cess, cohesive phrasal decoding has been proposed in
(Cherry, 2008; Yamamoto et al, 2008). The cohesion-
enhanced decoder enforces the following constraint: once
the decoder begins translating any part of a source sub-
tree, it must cover all the words under that subtree before
it can translate anything outside of it. This notion can be
applied to any projective tree structure, but we use de-
pendency trees, which have been shown to demonstrate
greater cross-lingual cohesion than other structures (Fox,
2002). We use a tree data structure to store the depen-
dency tree. Each node in the tree contains surface word
form, word position, parent position, dependency type
and POS tag. We use T to stand for our dependency tree,
and T (n) to stand for the subtree rooted at node n. Each
subtree T (n) covers a span of contiguous source words;
for subspan f? covered by T (n), we say f? ? T (n).
Cohesion is checked as we extend a state (f?h,HC h)
with the translation of f?h+1, creating a new state
(f?h+1,HC h+1). Algorithm 1 presents the cohesion
check described by Cherry (2008). Line 2 selects focal
points, based on the last translated phrase. Line 4 climbs
from each focal point to find the largest subtree that needs
to be completed before the translation process can move
elsewhere in the tree. Line 5 checks each such subtree
for completion. Since there are a constant number of fo-
cal points (always 2) and the tree climb and completion
checks are both linear in the size of the source, the entire
check can be shown to take linear time.
The selection of only two focal points is motivated by
a ?violation free? assumption. If one assumes that the
1
Algorithm 1 Interruption Check (Coh1) (Cherry, 2008)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: Interruption ? False
2: F ? the left and right-most tokens of f?h
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists and T (n) is not covered in HCh+1
then
6: Interruption ? True
7: end if
8: end for
9: Return Interruption
Figure 1: A candidate translation where Coh1 does not fire
translation represented by (f?h,HC h) contains no cohe-
sion violations, then checking only the end-points of f?h
is sufficient to maintain cohesion. However, once a soft
cohesion constraint has been implemented, this assump-
tion no longer holds.
2 Extensions of Cohesive Constraints
2.1 Exhaustive Interruption Check (Coh2)
Because of the ?violation free? assumption, Algorithm 1
implements the design decision to only suffer a violation
penalty once, when cohesion is initially broken. How-
ever, this is not necessarily the best approach, as the de-
coder does not receive any further incentive to return to
the partially translated subtree and complete it.
For example, Figure 1 illustrates a translation candi-
date of the English sentence ?the presidential election
of the united states begins tomorrow? into French. We
consider f?4 = ?begins?, f?5 = ?tomorrow?. The decoder
already translated ?the presidential election? making the
coverage vector HC 5 = ?1 1 1 0 0 0 0 1 1?. Algorithm 1
tells the decoder that no violation has been made by trans-
lating ?tomorrow? while the decoder should be informed
that there exists an outstanding violation. Algorithm 1
found the violation when the decoder previously jumped
from ?presidential? to ?begins?, and will not find another
violation when it jumps from ?begins? to ?tomorrow?.
Algorithm 2 is a modification of Algorithm 1, chang-
ing only line 2. The resulting system checks all previ-
Algorithm 2 Exhaustive Interruption Check (Coh2)
Input: Source tree T , previous phrase fh, current
phrase fh+1, coverage vector HC
1: Interruption ? False
2: F ? {f |HCh(f) = 1}
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists and T (n) is not covered in HC h+1
then
6: Interruption ? True
7: end if
8: end for
9: Return Interruption
Algorithm 3 Interruption Count (Coh3)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: ICount ? 0
2: F ? the left and right-most tokens of f?h
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists then
6: for each of e ? T (n) and HCh+1(e) = 0 do
7: ICount = ICount+ 1
8: end for
9: end if
10: end for
11: Return ICount
ously covered tokens, instead of only the left and right-
most tokens of f?h+1, and therefore makes no violation-
free assumption. For the example above, Algorithm 2
will inform the decoder that translating ?tomorrow? also
incurs a violation. Because |F | is no longer constant,
the time complexity of Coh2 is worse than Coh1. How-
ever, we can speed up the interruption check algorithm
by hashing cohesion checks, so we only need to run Al-
gorithm 2 once per (f?h+1,HC h+1) .
2.2 Interruption Count (Coh3) and Exhaustive
Interruption Count (Coh4)
Algorithm 1 and 2 described above interpret an inter-
ruption as a binary event. As it is possible to leave several
words untranslated with a single jump, some interrup-
tions may be worse than others. To implement this obser-
vation, an interruption count is used to assign a penalty
to cohesion violations, based on the number of words left
uncovered in the interrupted subtree. We initialize the in-
terruption count with zero. At any search state when the
cohesion violation is detected the count is incremented by
2
Algorithm 4 Exhaustive Interruption Count (Coh4)
Input: Source tree T , previous phrase fh, current
phrase fh+1, coverage vector HC
1: ICount ? 0
2: F ? {f |HCh(f) = 1}
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists then
6: for each of e ? T (n) and HCh+1(e) = 0 do
7: ICount = ICount+ 1
8: end for
9: end if
10: end for
11: Return ICount
one. The modification of Algorithm 1 and 2 lead to Inter-
ruption Count (Coh3) and Exhaustive Interruption Count
(Coh4) algorithms, respectively. The changes only hap-
pen in lines 1, 5 and 6. We use an additional bit vector
to make sure that if a node has been reached once during
an interruption check, it should not be counted again. For
the example in Section 2.1, Algorithm 4 will return 4 for
ICount (?of?; ?the?; ?united?; ?states?).
2.3 Rich Interruption Constraints (Coh5)
The cohesion constraints in Sections 2.1 and 2.2 do not
leverage node information in the dependency tree struc-
tures. We propose the rich interruption constraints (Coh5)
algorithm to combine four constraints which are Interrup-
tion, Interruption Count, Verb Count and Noun Count.
The first two constraints are identical to what was de-
scribed above. Verb and Noun count constraints are en-
forcing the following rule: a cohesion violation will be
penalized more in terms of the number of verb and noun
words that have not been covered. For example, we want
to translate the English sentence ?the presidential elec-
tion of the united states begins tomorrow? to French with
the dependency structure as in Figure 1. We consider f?h
= ?the united states?, f?h+1 = ?begins?. The coverage bit
vector HC h+1 is ?0 0 0 0 1 1 1 1 0?. Algorithm 5 will re-
turn true for Interruption, 4 for ICount (?the?; ?pres-
idential?; ?election?; ?of?), 0 for V erbCount and 1 for
NounCount (?election?).
3 Experiments
We built baseline systems using GIZA++ (Och and Ney,
2003), Moses? phrase extraction with grow-diag-final-
end heuristic (Koehn et al, 2007), a standard phrase-
based decoder (Vogel, 2003), the SRI LM toolkit (Stol-
cke, 2002), the suffix-array language model (Zhang and
Vogel, 2005), a distance-based word reordering model
Algorithm 5 Rich Interruption Constraints (Coh5)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: Interruption ? False
2: ICount, V erbCount,NounCount ? 0
3: F ? the left and right-most tokens of f?h
4: for each of f ? F do
5: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
6: if n exists then
7: for each of e ? T (n) and HCh+1(e) = 0 do
8: Interruption ? True
9: ICount = ICount+ 1
10: if POS of e is ?VB? then
11: V erbCount ? V erbCount+ 1
12: else if POS of e is ?NN? then
13: NounCount ? NounCount+ 1
14: end if
15: end for
16: end if
17: end for
18: Return Interruption, ICount, V erbCount,
NounCount
with a window of 3, and the maximum number of target
phrases restricted to 10. Results are reported using low-
ercase BLEU (Papineni et al, 2002). All model weights
were trained on development sets via minimum-error rate
training (MERT) (Och, 2003) with 200 unique n-best lists
and optimizing toward BLEU. We used the MALT parser
(Nivre et al, 2006) to obtain source English dependency
trees and the Stanford parser for Arabic (Marneffe et al,
2006). In order to decide whether the translation output
of one MT engine is significantly better than another one,
we used the bootstrap method (Zhang et al, 2004) with
1000 samples (p < 0.05). We perform experiments on
English?Iraqi and English?Spanish. Detailed corpus
statistics are shown in Table 1. Table 2 shows results in
lowercase BLEU and bold type is used to indicate high-
est scores. An italic text indicates the score is statistically
significant better than the baseline.
English?Iraqi English?Spanish
English Iraqi English Spanish
sentence pairs 654,556 1,310,127
unique sent. pairs 510,314 1,287,016
avg. sentence length 8.4 5.9 27.4 28.6
# words 5.5 M 3.8 M 35.8 M 37.4 M
vocabulary 34 K 109 K 117 K 173 K
Table 1: Corpus statistics
Our English-Iraqi data come from the DARPA
TransTac program. We used TransTac T2T July 2007
3
English?Iraqi English?Spanish
july07 june08 ncd07 nct07
Baseline 31.58 23.58 33.18 32.04
+Coh1 32.63 24.45 33.49 32.72
+Coh2 32.51 24.73 33.52 32.81
+Coh3 32.43 24.19 33.37 32.87
+Coh4 32.32 24.66 33.47 33.20
+Coh5 31.98 24.42 33.54 33.27
Table 2: Scores of baseline and cohesion-enhanced systems on
English?Iraqi and English?Spanish systems
(july07) as the development set and TransTac T2T June
2008 (june08) as the held-out evaluation set. Each test set
has 4 reference translation. We applied the suffix-array
LM up to 6-gram with Good-Turing smoothing. Our co-
hesion constraints produced improvements ranging be-
tween 0.5 and 1.2 BLEU point on the held-out evaluation
set.
We used the Europarl and News-Commentary parallel
corpora for English?Spanish as provided in the ACL-
WMT 2008 shared task evaluation. The baseline sys-
tem used the parallel corpus restricting sentence length
to 100 words for word alignment and a 4-gram SRI
LM with modified Kneyser-Ney smoothing. We used
nc-devtest2007(ncd07) as the development set and nc-
test2007(nct07) as the held-out evaluation set. Each test
set has 1 translation reference. We obtained improve-
ments ranging between 0.7 and 1.2 BLEU. All cohesion
constraints perform statistically significant better than the
baseline on the held-out evaluation set.
4 Conclusions
In this paper, we explored cohesive phrasal decoding, fo-
cusing on variants of cohesive constraints. We proposed
four novel cohesive constraints namely exhaustive inter-
ruption check (Coh2), interruption count (Coh3), exhaus-
tive interruption count (Coh4) and rich interruption con-
straints (Coh5). Our experimental results show that with
cohesive constraints the system generates better transla-
tions in comparison with strong baselines. To ensure the
robustness and effectiveness of the proposed approaches,
we conducted experiments on 2 different language pairs,
namely English?Iraqi and English?Spanish. These ex-
periments also covered a wide range of training corpus
sizes, ranging from 600K sentence pairs up to 1.3 mil-
lion sentence pairs. All five proposed approaches give
positive results. The improvements on English?Spanish
are statistically significant at the 95% level. We observe
a consistent pattern indicating that the improvements are
stable in both language pairs.
Acknowledgments
This work is in part supported by the US DARPA TransTac pro-
grams. Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the views of DARPA. We would like
to thank Qin Gao and Matthias Eck for helpful conversations,
Johan Hall and Joakim Nirve for helpful suggestions on train-
ing and using the English dependency model. We also thanks
the anonymous reviewers for helpful comments.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for statis-
tical machine translation. In Proceedings of ACL-08: HLT,
pages 72?80, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP?02, pages 304?311,
Philadelphia, PA, July 6-7.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL?07, pages 177?180, Prague,
Czech Republic, June.
Marie-Catherine Marneffe, Bill MacCartney, and Christopher
Manning. 2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC?06, Genoa,
Italy.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser:
A data-driven parser-generator for dependency parsing. In
Proceedings of LREC?06, Genoa, Italy.
Franz J. Och and Hermann Ney. 2003. A systematic compar-
ison of various statistical alignment models. In Computa-
tional Linguistics, volume 1:29, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL?03, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: A method for automatic evaluation of
machine translation. In Proceedings of ACL?02, pages 311?
318, Philadelphia, PA, July.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing, volume 2, pages 901?904, Denver.
Stephan Vogel. 2003. SMT decoder dissected: Word reorder-
ing. In Proceedings of NLP-KE?03, pages 561?566, Bejing,
China, Oct.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro Sumita.
2008. Imposing constraints from the source tree on ITG
constraints for SMT. In Proceedings of the ACL-08: HLT,
SSST-2, pages 1?9, Columbus, Ohio, June. Association for
Computational Linguistics.
Ying Zhang and Stephan Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and large
corpora. In Proceedings of EAMT?05, Budapest, Hungary,
May. The European Association for Machine Translation.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC?04,
pages 2051?2054.
4
Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incremental Adaptation of Speech-to-Speech Translation
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
Abstract
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
1 Introduction
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al, 2006;
Bach et al, 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
W i i c o n t r o l l e r
M i c & L i g h t
L o u d  s p e a k e r
Figure 1: The users interact with the system
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
2 Data Scenario
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
149
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
3 ASR LM Adaptation
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al, 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
Table 1: Iraqi ASR?s WER on day 2 using different adaptation
schemes for day 1 data
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
Table 2: Impact of ASR adaptation to SMT
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al, 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
4 SMT Adaptation
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al, 2007).
Related work including (Eck et al, 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
150
language models. This approach is similar to the work
in (Chen et al, 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
Table 3: Performance in BLEU of unsupervised adaptation.
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
Table 4: Performance in BLEU of supervised adaptation.
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
5 Joint Adaptation
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
Table 5: Performance in BLEU of joint adaptation.
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
6 Selective Adaptation
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
151
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
No. sents Day 1 Day 2
Baseline 29.39 27.41
? 0 864 30.27 28.29
? 10 797 31.15 28.27
? 20 747 30.81 28.24
? 30 585 30.04 27.71
? 40 416 29.72 27.65
? 50 296 30.06 27.04
Correct 98 29.18 27.19
Table 6: Performance in BLEU of selective adaptation
W i c o n t r o l e M & L g
h r n u
d
r c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  
h r n u c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 77?80,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recent Improvements in the
CMU Large Scale Chinese-English SMT System
Almut Silja Hildebrand, Kay Rottmann, Mohamed Noamany, Qin Gao,
Sanjika Hewavitharana, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
silja, kayrm, mfn, qing, sanjika, nbach, vogel+@cs.cmu.edu
Abstract
In this paper we describe recent improvements
to components and methods used in our statis-
tical machine translation system for Chinese-
English used in the January 2008 GALE eval-
uation. Main improvements are results of
consistent data processing, larger statistical
models and a POS-based word reordering ap-
proach.
1 Introduction
Building a full scale Statistical Machine Transla-
tion (SMT) system involves many preparation and
training steps and it consists of several components,
each of which contribute to the overall system per-
formance. Between 2007 and 2008 our system im-
proved by 5 points in BLEU from 26.60 to 31.85
for the unseen MT06 test set, which can be mainly
attributed to two major points.
The fast growth of computing resources over
the years make it possible to use larger and larger
amounts of data in training. In Section 3 we show
how parallelizing model training can reduce training
time by an order of magnitude and how using larger
training data as well as more extensive models im-
prove translation quality.
Word reordering is still a difficult problem in
SMT. In Section 4 we apply a Part Of Speech (POS)
based syntactic reordering model successfully to our
large Chinese system.
1.1 Decoder
Our translation system is based on the CMU
SMT decoder as described in (Hewavitharana et
al., 2005). Our decoder is a phrase-based beam
search decoder, which combines multiple models
e.g. phrase tables, several language models, a dis-
tortion model ect. in a log-linear fashion. In order
to find an optimal set of weights, we use MER train-
ing as described in (Venugopal et al, 2005), which
uses rescoring of the top n hypotheses to maximize
an evaluation metric like BLEU or TER.
1.2 Evaluation
In this paper we report results using the BLEU met-
ric (Papineni et al, 2002), however as the evaluation
criterion in GALE is HTER (Snover et al, 2006), we
also report in TER (Snover et al, 2005).
We used the test sets from the NIST MT evalua-
tions from the years 2003 and 2006 as development
and unseen test data.
1.3 Training Data
In translation model training we used the Chinese-
English bilingual corpora relevant to GALE avail-
able through the LDC1. After sentence alignment
these sources add up to 10.7 million sentences with
301 million running words on the English side. Our
preprocessing steps include tokenization on the En-
glish side and for Chinese: automatic word segmen-
tation using the revised version of the Stanford Chi-
nese Word Segmenter2 (Tseng et al, 2005) from
2007, replacement of traditional by simplified Chi-
nese characters and 2-byte to 1-byte ASCII charac-
ter normalization. After data cleaning steps like e.g.
removal of sentence pairs with very unbalanced sen-
1http://projects.ldc.upenn.edu/gale/data/catalog.html
2http://nlp.stanford.edu/software/segmenter.shtml
77
tence length etc., we used the remaining 10 million
sentences with 260 million words (English) in trans-
lation model training (260M system).
2 Number Tagging
Systematic tagging and pre-translation of numbers
had shown significant improvements for our Arabic-
English system, so we investigated this for Chinese-
English. The baseline for these experiments was a
smaller system with 67 million words (67M) bilin-
gual training data (English) and a 500 million word
3-gram LM with a BLEU score of 27.61 on MT06.
First we pre-translated all numbers in the testdata
only, thus forcing the decoder to treat the numbers as
unknown words. Probably because the system could
not match longer phrases across the pre-translated
numbers, the overall translation quality degraded by
1.6 BLEU to 26.05 (see Table 1).
We then tagged all numbers in the training corpus,
replaced them with a placeholder tag and re-trained
the translation model. This reduced the vocabu-
lary and enabled the decoder to generalize longer
phrases across numbers. This strategy did not lead to
the expected result, the BLEU score for MT06 only
reached 25.97 BLEU.
System MT03 MT06
67M baseline 31.45/60.93 27.61/62.18
test data tagged ? 26.06/63.36
training data tagged 29.07/62.52 25.97/63.39
Table 1: Number tagging experiments, BLEU/TER
Analysing this in more detail, we found, the rea-
son for this degradation in translation quality could
be the unbalanced occurrence of number tags in the
training data. From the bilingual sentence pairs,
which contain number tags, 66.52% do not contain
the same number of tags on the Chinese and the En-
glish side. As a consequence 52% of the phrase pairs
in the phrase table, which contain number tags had
to be removed, because the tags were unbalanced.
This hurts system performance considerably.
3 Scaling up to Large Data
3.1 Language Model
Due to the availability of more computing resources,
we were able to extend the language model history
from 4- to 5-gram, which improved translation qual-
ity from 29.49 BLEU to 30.22 BLEU for our large
scale 260M system (see Table 2). This shows, that
longer LM histories help if we are able to use enough
data in model training.
System MT03 MT06
260M, 4gram 31.20/61.00 29.49/61.00
260M, 5gram 32.20/60.59 30.22/60.81
Table 2: 4- and 5-gram LM,260M system, BLEU/TER
The language model was trained on the sources
from the English Gigaword Corpus V3, which con-
tains several newspapers for the years between 1994
to 2006. We also included the English side of the
bilingual training data, resulting in a total of 2.7 bil-
lion running words after tokenization.
We trained separate open vocabulary language
models for each source and interpolated them using
the SRI Language Modeling Toolkit (Stolcke, 2002).
Table 3 shows the interpolation weights for the dif-
ferent sources. Apart from the English part of the
bilingual data, the newswire data from the Chinese
Xinhua News Agency and the Agence France Press
have the largest weights. This reflects the makeup of
the test data, which comes in large parts from these
sources. Other sources, as for example the UN par-
lamentary speeches or the New York Times, differ
significantly in style and vocabulary from the test
data and therefore get small weights.
xin 0.30 cna 0.06 nyt 0.03
bil 0.26 un 0.07 ltw 0.01
afp 0.21 apw 0.05
Table 3: LM interpolation weights per source
3.2 Speeding up Model Training
To accelerate the training of word alignment
models we implemented a distributed version of
GIZA++ (Och and Ney, 2003), based on the latest
version of GIZA++ and a parallel version developed
at Peking University (Lin et al, 2006). We divide the
bilingual training data in equal parts and distribute it
over several processing nodes, which perform align-
ment independently. In each iteration the nodes read
the model from the previous step and output all nec-
essary counts from the data for the models, e.g. the
78
co-occurrence or fertility model. A master process
collects the counts from the nodes, normalizes them
and outputs the intermediate model for each itera-
tion.
This distributed GIZA++ version finished training
the word alignment up to IBM Model 4 for both lan-
guage directions on the full bilingual corpus (260
million words, English) in 39 hours. On average
about 11 CPUs were running concurrently. In com-
parison the standard GIZA++ implementation fin-
ished the same training in 169 hours running on 2
CPUs, one for each language direction.
We used the Pharaoh/Moses package (Koehn et
al., 2007) to extract and score phrase pairs using the
grow-diag-final extraction method.
3.3 Translation Model
We trained two systems, one on the full data and one
without the out-of-domain corpora: UN parlament,
HK hansard and HK law parallel texts. These parla-
mentary sessions and law texts are very different in
genre and style from the MT test data, which con-
sists mainly of newspaper texts and in recent years
also of weblogs, broadcast news and broadcast con-
versation. The in-domain training data had 3.8 mil-
lion sentences and 67 million words (English). The
67 million word system reached a BLEU score of
29.65 on the unseeen MT06 testset. Even though the
full 260M system was trained on almost four times
as many running words, the baseline score for MT06
only increased by 0.6 to 30.22 BLEU (see Table 4).
System MT03 MT06
67M in-domain 32.42/60.26 29.65/61.22
260M full 32.20/60.59 30.22/60.81
Table 4: In-domain only or all training data, BLEU/TER
The 67M system could not translate 752 Chinese
words out of 38937, the number of unknown words
decreased to 564 for the 260M system. To increase
the unigram coverage of the phrase table, we added
the lexicon entries that were not in the phrase table
as one-word translations. This lowered the number
of unknown words further to 410, but did not effect
the translation score.
4 POS-based Reordering
As Chinese and English have very different word
order, reordering over a rather limited distance dur-
ing decoding is not sufficient. Also using a simple
distance based distortion probability leaves it essen-
tially to the language model to select among dif-
ferent reorderings. An alternative is to apply auto-
matically learned reordering rules to the test sen-
tences before decoding (Crego and Marino, 2006).
We create a word lattice, which encodes many re-
orderings and allows long distance reordering. This
keeps the translation process in the decoder mono-
tone and makes it significantly faster compared to
allowing long distance reordering at decoding time.
4.1 Learning Reordering Rules
We tag both language sides of the bilingual corpus
with POS information using the Stanford Parser3
and extract POS based reordering patterns from
word alignment information. We use the context in
which a reordering pattern is seen in the training data
as an additional feature. Context refers to the words
or tags to the left or to the right of the sequence for
which a reordering pattern is extracted.
Relative frequencies are computed for every rule
that has been seen more than n times in the training
corpus (we observed good results for n > 5).
For the Chinese system we used only 350k bilin-
gual sentence pairs to extract rules with length of
up to 15. We did not reorder the training corpus
to retrain the translation model on modified Chinese
word order.
4.2 Applying Reordering Rules
To avoid hard decisions, we build a lattice struc-
ture for each source sentence as input for our de-
coder, which contains reordering alternatives consis-
tent with the previously extracted rules.
Longer reordering patterns are applied first.
Thereby shorter patterns can match along new paths,
creating short distance reordering on top of long dis-
tance reordering. Every outgoing edge of a node is
scored with the relative frequency of the pattern used
on the following sub path (For details see (Rottmann
and Vogel, 2007)). These model scores give this re-
3http://nlp.stanford.edu/software/lex-parser.shtml
79
ordering approach an advantage over a simple jump
model with a sliding window.
System MT03 MT06
260M, standard 32.20/60.59 30.22/60.81
260M, lattice 33.53/59.74 31.74/59.59
Table 5: Reordering lattice decoding in BLEU/TER
The system with reordering lattice input outper-
forms the system with a reordering window of 4
words by 1.5 BLEU (see Table 5).
5 Summary
The recent improvements to our Chinese-English
SMT system (see Fig. 1) can be mainly attributed to
a POS based word reordering method and the possi-
bility to work with larger statistical models.
We used the lattice translation functionality of our
decoder to translate reordering lattices. They are
built using reordering rules extracted from tagged
and aligned parallel data. There is further potential
for improvement in this approach, as we did not yet
reorder the training corpus and retrain the translation
model on modified Chinese word order.Improvements in BLEU
242526
272829
303132
33
2007 67M+3gr 260M+3gr 260M+4gr 260M+5gr 260M+RO
Figure 1: Improvements for MT06 in BLEU
We modified GIZA++ to run in parallel, which en-
abled us to include especially longer sentences into
translation model training. We also extended our de-
coder to use 5-gram language models and were able
to train an interpolated LM from all sources of the
English GigaWord Corpus.
Acknowledgments
This work was partly funded by DARPA under
the project GALE (Grant number #HR0011-06-2-
0001).
References
Josep M. Crego and Jose B. Marino. 2006. Reordering
Experiments for N-Gram-Based SMT. Spoken Lan-
guage Technology Workshop, Palm Beach, Aruba.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hilde-
brand, Matthias Eck, Chiori Hori, Stephan Vogel and
Alex Waibel. 2005. The CMU Statistical Machine
Translation System for IWSLT 2005. IWSLT 2005,
Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL 2007,
Demonstration Session, Prague, Czech Republic.
Xiaojun Lin, Xinhao Wang, and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 MT Evaluation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Poukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. ACL 2002, Philadel-
phia, USA.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
based Distortion Model. TMI-2007: 11th Interna-
tional Conference on Theoretical and Methodological
Issues in MT, Skvde, Sweden.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula and Ralph Weischedel.
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. LAMP-TR-126, University
of Maryland, College Park and BBN Technologies.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. 7th Conference of AMTA, Cambridge, Mas-
sachusetts, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. ICSLP, Denver, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky and Christopher Manning. 2005. A Con-
ditional Random Field Word Segmenter. Fourth
SIGHAN Workshop on Chinese Language Processing.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. ACL 2005,
WPT-05, Ann Arbor, MI
80
Proceedings of the Third Workshop on Statistical Machine Translation, pages 151?154,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Word Alignment with Language Model Based Confidence Scores
Nguyen Bach, Qin Gao, Stephan Vogel
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, qing, vogel+}@cs.cmu.edu
Abstract
This paper describes the statistical machine trans-
lation systems submitted to the ACL-WMT 2008
shared translation task. Systems were submitted for
two translation directions: English?Spanish and
Spanish?English. Using sentence pair confidence
scores estimated with source and target language
models, improvements are observed on the News-
Commentary test sets. Genre-dependent sentence
pair confidence score and integration of sentence
pair confidence score into phrase table are also in-
vestigated.
1 Introduction
Word alignment models are a crucial component in sta-
tistical machine translation systems. When estimating
the parameters of the word alignment models, the sen-
tence pair probability is an important factor in the objec-
tive function and is approximated by the empirical prob-
ability. The empirical probability for each sentence pair
is estimated by maximum likelihood estimation over the
training data (Brown et al, 1993). Due to the limitation of
training data, most sentence pairs occur only once, which
makes the empirical probability almost uniform. This is
a rather weak approximation of the true distribution.
In this paper, we investigate the methods of weighting
sentence pairs using language models, and extended the
general weighting method to genre-dependent weight. A
method of integrating the weight directly into the phrase
table is also explored.
2 The Baseline Phrase-Based MT System
The ACL-WMT08 organizers provided Europarl and
News-Commentary parallel corpora for English ? Span-
ish. Detailed corpus statistics is given in Table 1. Follow-
ing the guidelines of the workshop we built baseline sys-
tems, using the lower-cased Europarl parallel corpus (re-
stricting sentence length to 40 words), GIZA++ (Och and
Ney, 2003), Moses (Koehn et al, 2007), and the SRI LM
toolkit (Stolcke, 2002) to build 5-gram LMs. Since no
News development sets were available we chose News-
Commentary sets as replacements. We used test-2006
(E06) and nc-devtest2007 (NCd) as development sets for
Europarl and News-Commentary; test-2007 (E07) and
nc-test2007 (NCt) as held-out evaluation sets.
English Spanish
Europarl (E)
sentence pairs 1,258,778
unique sent. pairs 1,235,134
avg. sentence length 27.9 29.0
# words 35.14 M 36.54 M
vocabulary 108.7 K 164.8 K
News-Commentary (NC)
sentence pairs 64,308
unique sent. pairs 64,205
avg. sentence length 24.0 27.4
# words 1.54 M 1.76 M
vocabulary 44.2 K 56.9 K
Table 1: Statistics of English?Spanish Europarl and News-
Commentary corpora
To improve the baseline performance we trained sys-
tems on all true-cased training data with sentence length
up to 100. We used two language models, a 5-gram LM
build from the Europarl corpus and a 3-gram LM build
from the News-Commentary data. Instead of interpolat-
ing the two language models, we explicitly used them in
the decoder and optimized their weights via minimum-
error-rate (MER) training (Och, 2003). To shorten the
training time, a multi-threaded GIZA++ version was used
to utilize multi-processor servers (Gao and Vogel, 2008).
Other parameters were the same as the baseline sys-
tem. Table 2 shows results in lowercase BLEU (Pap-
ineni et al, 2002) for both the baseline (B) and the im-
proved baseline systems (B5) on development and held-
151
out evaluation sets. We observed significant gains for the
News-Commentary test sets. Our improved baseline sys-
tems obtained a comparable performance with the best
English?Spanish systems in 2007 (Callison-Burch et al,
2007).
Pairs Europarl NC
E06 E07 NCd NCt
En?Es B 33.00 32.21 31.84 30.56B5 33.33 32.25 35.10 34.08
Es?En B 33.08 33.23 31.18 31.34B5 33.26 33.23 36.06 35.56
Table 2: NIST-BLEU scores of baseline and improved baseline
systems experiments on English?Spanish
3 Weighting Sentence Pairs
3.1 Problem Definition
The quality of word alignment is crucial for the perfor-
mance of the machine translation system.
In the well-known so-called IBM word alignment
models (Brown et al, 1993), re-estimating the model pa-
rameters depends on the empirical probability P? (ek, fk)
for each sentence pair (ek, fk). During the EM train-
ing, all counts of events, e.g. word pair counts, distortion
model counts, etc., are weighted by P? (ek, fk). For ex-
ample, in IBM Model 1 the lexicon probability of source
word f given target word e is calculated as (Och and Ney,
2003):
p(f |e) =
?
k c(f |e; ek, fk)?
k,f c(f |e; ek, fk)
(1)
c(f |e; ek, fk) =
?
ek,fk
P? (ek, fk)
?
a
P (a|ek, fk) ? (2)
?
j
?(f , fkj )?(e, ekaj )
Therefore, the distribution of P? (ek, fk) will affect the
alignment results. In Eqn. 2, P? (ek, fk) determines
how much the alignments of sentence pair (ek, fk) con-
tribute to the model parameters. It will be helpful if
the P? (ek, fk) can approximate the true distribution of
P (ek, fk).
Consider that we are drawing sentence pairs from a
given data source, and each unique sentence pair (ek, fk)
has a probability P (ek, fk) to be observed. If the training
corpora size is infinite, the normalized frequency of each
unique sentence pair will converge to P (ek, fk). In that
case, equally assigning a number to each occurrence of
(ek, fk) and normalizing it will be valid. However, the
assumption is invalid if the data source is finite. As we
can observe in the training corpora, most sentences occur
only one time, and thus P? (ek, fk) will be uniform.
To get a more informative P? (ek, fk), we explored
methods of weighting sentence pairs. We investigated
three sets of features: sentence pair confidence (sc),
genre-dependent sentence pair confidence (gdsc) and
phrase alignment confidence (pc) scores. These features
were calculated over an entire training corpus and could
be easily integrated into the phrase-based machine trans-
lation system.
3.2 Sentence Pair Confidence
We can hardly compute the joint probability of P (ek, fk)
without knowing the conditional probability P (ek|fk)
which is estimated during the alignment process. There-
fore, to estimate P (ek, fk) before alignment, we make an
assumption that P? (ek, fk) = P (ek)P (fk), which means
the two sides of sentence pair are independent of each
other. P (ek) and P (fk) can be obtained by using lan-
guage models. P (ek) or P (fk), however, can be small
when the sentence is long. Consequently, long sentence
pairs will be assigned low scores and have negligible ef-
fect on the training process. Given limited training data,
ignoring these long sentences may hurt the alignment re-
sult. To compensate this, we normalize the probability by
the sentence length. We propose the following method
to weighting sentence pairs in the corpora. We trained
language models for source and target language, and the
average log likelihood (AVG-LL) of each sentence pair
was calculated by applying the corresponding language
model. For each sentence pair (ek, fk), the AVG-LL
L(ek, fk) is
L(ek) = 1|ek|
?
eki ?ek logP (e
k
i |h)
L(fk) = 1|fk|
?
fkj ?fk logP (f
k
j |h)
L(ek, fk) = [L(ek) + L(fk)]/2
(3)
where P (eki |h) and P (fkj |h) are ngram probabilities.
The sentence pair confidence score is then given by:
sc(ek, fk) = exp(L(ek, fk)). (4)
3.3 Genre-Dependent Sentence Pair Confidence
Genre adaptation is one of the major challenges in statis-
tical machine translation since translation models suffer
from data sparseness (Koehn and Schroeder, 2007). To
overcome these problems previous works have focused
on explicitly modeling topics and on using multiple lan-
guage and translation models. Using a mixture of topic-
dependent Viterbi alignments was proposed in (Civera
and Juan, 2007). Language and translation model adap-
tation to Europarl and News-Commentary have been ex-
plored in (Paulik et al, 2007).
Given the sentence pair weighting method, it is pos-
sible to adopt genre-specific language models into the
152
weighting process. The genre-dependent sentence pair
confidence gdsc simulates weighting the training sen-
tences again from different data sources, thus, given
genre g, it can be formulated as:
gdsc(ek, fk) = sc(ek, fk|g) (5)
where P (eki |h) and P (fkj |h) are estimated by genre-
specific language models.
The score generally represents the likelihood of the
sentence pair to be in a specific genre. Thus, if both sides
of the sentence pair show a high probability according
to the genre-specific language models, alignments in the
pair should be more possible to occur in that particular
domain, and put more weight may contribute to a better
alignment for that genre.
3.4 Phrase Alignment Confidence
So far the confidence scores are used only in the train-
ing of the word alignment models. Tracking from which
sentence pairs each phrase pair was extracted, we can use
the sentence level confidence scores to assign confidence
scores to the phrase pairs. Let S(e?, f?) denote the set of
sentences pairs from which the phrase pair (e?, f?) was ex-
tracted. We calculate then a phrase alignment confidence
score pc as:
pc(e?, f?) = exp
?
(ek,fk)?S(e?,f?) log sc(ek, fk)
|S(e?, f?)| (6)
This score is used as an additional feature of the phrase
pair. The feature weight is estimated in MER training.
4 Experimental Results
The first step in validating the proposed approach was
to check if the different language models do assign dif-
ferent weights to the sentence pairs in the training cor-
pora. Using the different language models NC (News-
Commentary), EP (Europarl), NC+EP (both NC and EP)
the genre-specific sentence pair confidence scores were
calculated. Figure 1 shows the distributions of the dif-
ferences in these scores across the two corpora. As ex-
pected, the language model build from the NC corpus as-
signs - on average - higher weights to sentence pairs in the
NC corpus and lower weights to sentence pairs in the EP
corpus (Figure 1a). The opposite is true for the EP LM.
When comparing the scores calculated from the NC LM
and the combined NC+EP LM we still see a clear sep-
aration (Figure 1b). No marked difference can be seen
between using the EP LM and the NC+EP LM (Figure
1c), which again is expected, as the NC corpus is very
small compared to the EP corpus.
The next step was to retrain the word alignment mod-
els using sentences weights according to the various con-
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
(a) Difference in Weight (NC?EP)
Pro
port
ion 
in C
orpo
ra
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
0.02
(b) Difference in Weight (NC?NE)
Pro
port
ion 
in C
orpo
ra
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
0.02
(c) Difference in Weight (NE?EP)
Pro
port
ion 
in C
orpo
ra
Europal DataNews Commentary Data
Europal DataNews Commentary Data
Europal DataNews Commentary Data
Figure 1: Histogram of weight differences genre specific con-
fidence scores on NC and EP training corpora
fidence scores. Table 3 shows training and test set per-
plexities for IBM model 4 for both training directions.
Not only do we see a drop in training set perplexities,
but also in test set perplexities. Using the genre specific
confidence scores leads to lower perplexities on the cor-
responding test set, which means that using the proposed
method does lead to small, but consistent adjustments in
the alignment models.
Uniform NC+EP NC EP
train En?Es 46.76 42.36 42.97 44.47Es?En 70.18 62.81 62.95 65.86
test
NC(En?Es) 53.04 53.44 51.09 55.94
EP(En?Es) 91.13 90.89 91.84 90.77
NC(Es?En) 81.39 81.28 78.23 80.33
EP(Es?En) 126.56 125.96 123.23 122.11
Table 3: IBM model 4 training and test set perplexities using
genre specific sentence pair confidence scores.
In the final step the specific alignment models were
used to generate various phrase tables, which were then
used in translation experiments. Results are shown in Ta-
ble 4. We report lower-cased Bleu scores. We used nc-
dev2007 (NCt1) as an additional held-out evaluation set.
Bold cells indicate highest scores.
As we can see from the results, improvements are ob-
tained by using sentence pair confidence scores. Us-
ing confidence scores calculated from the EP LM gave
overall the best performance. While we observe only a
small improvement on Europarl sets, improvements on
News-Commentary sets are more pronounced, especially
on held-out evaluation sets NCt and NCt1. The exper-
iments do not give evidence that genre-dependent con-
fidence can improve over using the general confidence
153
Test Set
E06 E07 NCd NCt NCt1
Es?En
B5 33.26 33.23 36.06 35.56 35.64
NC+EP 33.23 32.29 36.12 35.47 35.97
NC 33.43 33.39 36.14 35.27 35.68
EP 33.36 33.39 36.16 35.63 36.17
En?Es
B5 33.33 32.25 35.10 34.08 34.43
NC+EP 33.23 32.29 35.12 34.56 34.89
NC 33.30 32.27 34.91 34.07 34.29
EP 33.08 32.29 35.05 34.52 35.03
Table 4: Translation results (NIST-BLEU) using gdsc with dif-
ferent genre-specific language models for Es?En systems
score. As the News-Commentary language model was
trained on a very small amount of data further work is
required to study this in more detail.
Test Set
E06 E07 NCd NCt NCt1
Es?En
B5 33.26 33.23 36.06 35.56 35.64
NC+EP+pc 33.54 33.39 36.07 35.38 35.85
NC+pc 33.17 33.31 35.96 35.74 36.04
EP+pc 33.44 32.87 36.22 35.63 36.09
En?Es
B5 33.33 32.25 35.10 34.08 34.43
NC+EP+pc 33.28 32.45 34.82 33.68 33.86
NC+pc 33.13 32.47 34.01 34.34 34.98
EP+pc 32.97 32.20 34.26 33.99 34.34
Table 5: Translation results (NIST-BLEU) using pc with differ-
ent genre-specific language models for Es?En systems
Table 5 shows experiments results in NIST-BLEU us-
ing pc score as an additional feature on phrase tables
in Es?En systems. We observed that across develop-
ment and held-out sets the gains from pc are inconsistent,
therefore our submissions are selected from the B5+EP
system.
5 Conclusion
In the ACL-WMT 2008, our major innovations are meth-
ods to estimate sentence pair confidence via language
models. We proposed to use source and target language
models to weight the sentence pairs. We developed sen-
tence pair confidence (sc), genre-dependent sentence pair
confidence (gdsc) and phrase alignment confidence (pc)
scores. Our experimental results shown that we had a bet-
ter word alignment and translation performance by using
gdsc. We did not observe consistent improvements by
using phrase pair confidence scores in our systems.
Acknowledgments
This work is in part supported by the US DARPA under the
GALE program. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of DARPA.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statisti-
cal machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263?331.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-) evalua-
tion of machine translation. In Proc. of the ACL 2007 Second
Workshop on Statistical Machine Translation, Prague, Czech
Republic.
Jorge Civera and Alfons Juan. 2007. Domain adaptation in sta-
tistical translation with mixture modelling. In Proc. of the
ACL 2007 Second Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic.
Qin Gao and Stephan Vogel. 2008. Parallel implementations
of word alignment tool. In Proc. of the ACL 2008 Soft-
ware Engineering, Testing, and Quality Assurance Work-
shop, Columbus, Ohio, USA.
Philipp Koehn and Josh Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation. In Proc.
of the ACL 2007 Second Workshop on Statistical Machine
Translation, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics, demo sessions, pages 177?
180, Prague, Czech Republic, June.
Franz J. Och and Hermann Ney. 2003. A systematic compar-
ison of various statistical alignment models. In Computa-
tional Linguistics, volume 1:29, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Erhard Hinrichs and Dan Roth,
editors, Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja Hildebrand,
and Stephan Vogel. 2007. The ISL phrase-based mt system
for the 2007 ACL workshop on statistical machine transla-
tion. In In Proc. of the ACL 2007 Second Workshop on Sta-
tistical Machine Translation, Prague, Czech Republic.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing, volume 2, pages 901?904, Denver.
154
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211?219,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Goodness: A Method for Measuring Machine Translation Confidence
Nguyen Bach?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nbach@cs.cmu.edu
Fei Huang and Yaser Al-Onaizan
IBM T.J. Watson Research Center
1101 Kitchawan Rd
Yorktown Heights, NY 10567, USA
{huangfe, onaizan}@us.ibm.com
Abstract
State-of-the-art statistical machine translation
(MT) systems have made significant progress
towards producing user-acceptable translation
output. However, there is still no efficient
way for MT systems to inform users which
words are likely translated correctly and how
confident it is about the whole sentence. We
propose a novel framework to predict word-
level and sentence-level MT errors with a large
number of novel features. Experimental re-
sults show that the MT error prediction accu-
racy is increased from 69.1 to 72.2 in F-score.
The Pearson correlation between the proposed
confidence measure and the human-targeted
translation edit rate (HTER) is 0.6. Improve-
ments between 0.4 and 0.9 TER reduction are
obtained with the n-best list reranking task us-
ing the proposed confidence measure. Also,
we present a visualization prototype of MT er-
rors at the word and sentence levels with the
objective to improve post-editor productivity.
1 Introduction
State-of-the-art Machine Translation (MT) systems are
making progress to generate more usable translation
outputs. In particular, statistical machine translation
systems (Koehn et al, 2007; Bach et al, 2007; Shen
et al, 2008) have advanced to a state that the transla-
tion quality for certain language pairs (e.g. Spanish-
English, French-English, Iraqi-English) in certain do-
mains (e.g. broadcasting news, force-protection, travel)
is acceptable to users.
However, a remaining open question is how to pre-
dict confidence scores for machine translated words
and sentences. An MT system typically returns the
best translation candidate from its search space, but
still has no reliable way to inform users which word
is likely to be correctly translated and how confident it
is about the whole sentence. Such information is vital
? Work done during an internship at IBM T.J. Watson
Research Center
to realize the utility of machine translation in many ar-
eas. For example, a post-editor would like to quickly
identify which sentences might be incorrectly trans-
lated and in need of correction. Other areas, such as
cross-lingual question-answering, information extrac-
tion and retrieval, can also benefit from the confidence
scores of MT output. Finally, even MT systems can
leverage such information to do n-best list reranking,
discriminative phrase table and rule filtering, and con-
straint decoding (Hildebrand and Vogel, 2008).
Numerous attempts have been made to tackle the
confidence estimation problem. The work of Blatz et
al. (2004) is perhaps the best known study of sentence
and word level features and their impact on transla-
tion error prediction. Along this line of research, im-
provements can be obtained by incorporating more fea-
tures as shown in (Quirk, 2004; Sanchis et al, 2007;
Raybaud et al, 2009; Specia et al, 2009). Sori-
cut and Echihabi (2010) developed regression models
which are used to predict the expected BLEU score
of a given translation hypothesis. Improvement also
can be obtained by using target part-of-speech and null
dependency link in a MaxEnt classifier (Xiong et al,
2010). Ueffing and Ney (2007) introduced word pos-
terior probabilities (WPP) features and applied them in
the n-best list reranking. From the usability point of
view, back-translation is a tool to help users to assess
the accuracy level of MT output (Bach et al, 2007).
Literally, it translates backward the MT output into the
source language to see whether the output of backward
translation matches the original source sentence.
However, previous studies had a few shortcomings.
First, source-side features were not extensively inves-
tigated. Blatz et al(2004) only investigated source n-
gram frequency statistics and source language model
features, while other work mainly focused on target
side features. Second, previous work attempted to in-
corporate more features but faced scalability issues,
i.e., to train many features we need many training ex-
amples and to train discriminatively we need to search
through all possible translations of each training exam-
ple. Another issue of previous work was that they are
all trained with BLEU/TER score computing against211
the translation references which is different from pre-
dicting the human-targeted translation edit rate (HTER)
which is crucial in post-editing applications (Snover et
al., 2006; Papineni et al, 2002). Finally, the back-
translation approach faces a serious issue when forward
and backward translation models are symmetric. In this
case, back-translation will not be very informative to
indicate forward translation quality.
In this paper, we predict error types of each word
in the MT output with a confidence score, extend it to
the sentence level, then apply it to n-best list reranking
task to improve MT quality, and finally design a vi-
sualization prototype. We try to answer the following
questions:
? Can we use a rich feature set such as source-
side information, alignment context, and depen-
dency structures to improve error prediction per-
formance?
? Can we predict more translation error types i.e
substitution, insertion, deletion and shift?
? How good do our prediction methods correlate
with human correction?
? Do confidence measures help the MT system to
select a better translation?
? How confidence score can be presented to im-
prove end-user perception?
In Section 2, we describe the models and training
method for the classifier. We describe novel features
including source-side, alignment context, and depen-
dency structures in Section 3. Experimental results and
analysis are reported in Section 4. Section 5 and 6
present applications of confidence scores.
2 Confidence Measure Model
2.1 Problem setting
Confidence estimation can be viewed as a sequen-
tial labelling task in which the word sequence is
MT output and word labels can be Bad/Good or
Insertion/Substitution/Shift/Good. We first esti-
mate each individual word confidence and extend it to
the whole sentence. Arabic text is fed into an Arabic-
English SMT system and the English translation out-
puts are corrected by humans in two phases. In phase
one, a bilingual speaker corrects the MT system trans-
lation output. In phase two, another bilingual speaker
does quality checking for the correction done in phase
one. If bad corrections were spotted, they correct them
again. In this paper we use the final correction data
from phase two as the reference thus HTER can be
used as an evaluation metric. We have 75 thousand sen-
tences with 2.4 million words in total from the human
correction process described above.
We obtain training labels for each word by perform-
ing TER alignment between MT output and the phase-
two human correction. From TER alignments we ob-
served that out of total errors are 48% substitution, 28%
deletion, 13% shift, and 11% insertion errors. Based
on the alignment, each word produced by the MT sys-
tem has a label: good, insertion, substitution and shift.
Since a deletion error occurs when it only appears in the
reference translation, not in the MT output, our model
will not predict deletion errors in the MT output.
2.2 Word-level model
In our problem, a training instance is a word from MT
output, and its label when the MT sentence is aligned
with the human correction. Given a training instance x,
y is the true label of x; f stands for its feature vector
f(x, y); and w is feature weight vector. We define a
feature-rich classifier score(x, y) as follow
score(x, y) = w.f(x, y) (1)
To obtain the label, we choose the class with the high-
est score as the predicted label for that data instance.
To learn optimized weights, we use the Margin Infused
Relaxed Algorithm or MIRA (Crammer and Singer,
2003; McDonald et al, 2005) which is an online learner
closely related to both the support vector machine and
perceptron learning framework. MIRA has been shown
to provide state-of-the-art performance for sequential
labelling task (Rozenfeld et al, 2006), and is also able
to provide an efficient mechanism to train and opti-
mize MT systems with lots of features (Watanabe et
al., 2007; Chiang et al, 2009). In general, weights are
updated at each step time t according to the following
rule:
wt+1 = argminwt+1 ||wt+1 ? wt||
s.t. score(x, y) ? score(x, y?) + L(y, y?)
(2)
where L(y, y?) is a measure of the loss of using y? in-
stead of the true label y. In this problem L(y, y?) is 0-1
loss function. More specifically, for each instance xi in
the training data at a time t we find the label with the
highest score:
y? = argmax
y
score(xi, y) (3)
the weight vector is updated as follow
wt+1 = wt + ?(f(xi, y)? f(xi, y
?)) (4)
? can be interpreted as a step size; when ? is a large
number we want to update our weights aggressively,
otherwise weights are updated conservatively.
? = max(0, ?)
? = min
{
C, L(y,y
?)?(score(xi,y)?score(xi,y
?))
||f(xi,y)?f(xi,y?)||22
}
(5)
where C is a positive constant used to cap the maxi-
mum possible value of ? . In practice, a cut-off thresh-
old n is the parameter which decides the number of
features kept (whose occurrence is at least n) during212
training. Note that MIRA is sensitive to constant C,
the cut-off feature threshold n, and the number of iter-
ations. The final weight is typically normalized by the
number of training iterations and the number of train-
ing instances. These parameters are tuned on a devel-
opment set.
2.3 Sentence-level model
Given the feature sets and optimized weights, we use
the Viterbi algorithm to find the best label sequence.
To estimate the confidence of a sentence S we rely on
the information from the forward-backward inference.
One approach is to directly use the conditional prob-
abilities of the whole sequence. However, this quan-
tity is the confidence measure for the label sequence
predicted by the classifier and it does not represent the
goodness of the whole MT output. Another more ap-
propriated method is to use the marginal probability of
Good label which can be defined as follow:
p(yi = Good|S) =
?(yi|S)?(yi|S)
?
j ?(yj |S)?(yj |S)
(6)
p(yi = Good|S) is the marginal probability of label
Good at position i given the MT output sentence S.
?(yi|S) and ?(yi|S) are forward and backward values.
Our confidence estimation for a sentence S of k words
is defined as follow
goodness(S) =
?k
i=1 p(yi = Good|S)
k
(7)
goodness(S) is ranging between 0 and 1, where 0 is
equivalent to an absolutely wrong translation and 1
is a perfect translation. Essentially, goodness(S) is
the arithmetic mean which represents the goodness of
translation per word in the whole sentence.
3 Confidence Measure Features
Features are generated from feature types: abstract
templates from which specific features are instantiated.
Features sets are often parameterized in various ways.
In this section, we describe three new feature sets intro-
duced on top of our baseline classifier which has WPP
and target POS features (Ueffing and Ney, 2007; Xiong
et al, 2010).
3.1 Source-side features
From MT decoder log, we can track which source
phrases generate target phrases. Furthermore, one can
infer the alignment between source and target words
within the phrase pair using simple aligners such as
IBM Model-1 alignment.
Source phrase features: These features are designed
to capture the likelihood that source phrase and target
word co-occur with a given error label. The intuition
behind them is that if a large percentage of the source
phrase and target have often been seen together with the
Sour
ce P
OS a
nd P
hras
es
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
Targ
et PO
S: PR
P  V
BZ  
 
IN   
DT  
 
NN  
 
 
RB  
 
VBZ
 
 
 
TO  
 
DT  
 
NN  
IN   
DT  
 
JJ   J
JNN
S
VBP
 
 
 
IN   
 
 
DT  
 
 
 
DTN
N    
 
RB  
 
 
VBP
 
 
 
IN   
NN  
 
 
NN
DTJJ
 
 
 
 
 
 
DTJ
JD
TNN
S    D
TJJ
MT o
utpu
t
Sour
ce P
OS
Sour
ce
He  
 
adds
 
 
 
that 
 
this 
proc
ess
also
 
 
refer
s  to
 
 
the  
inab
ility  
of  th
e  m
ultin
ation
al  n
ava
l  for
ces
wydy
fan
 
 
 
hdhh
alam
lyt
ayda
tshyr
alya
dm
qdrt
almt
addt
aljnsy
tal
qwat
albh
ryt
(a) Source phrase
Sour
ce P
OS a
nd P
hras
es
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
Targ
et PO
S: PR
P  V
BZ  
 
IN   
DT  
 
NN  
 
 
RB  
 
VBZ
 
 
 
TO  
 
DT  
 
NN  
IN   
DT  
 
JJ   J
JNN
S
1 if  
sou
rce-
POS
-
sequ
enc
e =
 
?DT 
DTN
N?
f 125(ta
rget-
wor
d = ?
proc
ess
?) = 
0 oth
erw
ise
MT o
utpu
t
Sour
ce P
OS
Sour
ce
wydy
fan
 
 
 
hdhh
alam
lyt
ayda
tshyr
alya
dm
qdrt
almt
addt
aljnsy
tal
qwat
albh
ryt
He  
 
adds
 
 
 
that 
 
this 
proc
ess
also
 
 
refer
s  to
 
 
the  
inab
ility  
of  th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
IN   
 
 
DT  
 
 
 
DTN
N
RB  
 
 
VBP
 
 
 
IN   
NN  
 
 
NN
DTJJ
 
 
 
 
 
 
DTJ
JD
TNN
S    D
TJJ
(b) Source POS
Sour
ce P
OS a
nd P
hras
es
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
Targ
et PO
S: PR
P  V
BZ  
 
IN   
DT  
 
NN  
 
 
RB  
 
VBZ
 
 
 
TO  
 
DT  
 
NN  
IN   
DT  
 
JJ   J
JNN
S
MT o
utpu
t
Sour
ce P
OS
Sour
ce
He  
adds
 
 
that 
this 
 
 
proc
ess
also
 
 
 
 
refer
s  to
 
 
the  
inab
ility  
of  th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
IN   
 
 
DT  
 
 
 
DTN
N
RB  
 
 
 
 
VBP
IN   
NN  
 
 
NN
DTJJ
 
 
 
 
 
 
DTJJ
DTN
NS  
 
 
DTJJ
wydy
fan
 
 
 
hdhh
alam
lyt
ayda
tshy
ra
lyad
m
qdrt
almt
addt
aljnsy
tal
qwat
albh
ryt
(c) Source POS and phrase in right context
Figure 1: Source-side features.
same label, then the produced target word should have
this label in the future. Figure 1a illustrates this feature
template where the first line is source POS tags, the
second line is the Buckwalter romanized source Arabic
sequence, and the third line is MT output. The source
phrase feature is defined as follow
f102(process) =
{
1 if source-phrase=?hdhh alamlyt?
0 otherwise
Source POS: Source phrase features might be suscep-
tible to sparseness issues. We can generalize source
phrases based on their POS tags to reduce the number
of parameters. For example, the example in Figure 1a
is generalized as in Figure 1b and we have the follow-
ing feature:
f103(process) =
{
1 if source-POS=? DT DTNN ?
0 otherwise
Source POS and phrase context features: This fea-
ture set alows us to look at the surrounding context
of the source phrase. For example, in Figure 1c we
have ?hdhh alamlyt? generates ?process?. We also
have other information such as on the right hand side
the next two phrases are ?ayda? and ?tshyr? or the se-
quence of source target POS on the right hand side is
?RB VBP?. An example of this type of feature is
f104(process) =
{
1 if source-POS-context=? RB VBP ?
0 otherwise
3.2 Alignment context features
The IBM Model-1 feature performed relatively well in
comparison with the WPP feature as shown by Blatz et
al. (2004). In our work, we incorporate not only the213
Align
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
TO
DT 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
VBP
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
DTN
N    
 
 
 
 
 
RB 
 
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
wyd
yfa
n hd
hh
alam
lyt
ayd
a
tshy
ra
lya
dm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
He  
add
s  th
at  t
his  
 
proc
ess
 
 
 
also
 
 
refe
rs
to  t
he  
inab
ility 
 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  fo
rce
s
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
(a) Left sourceAlign
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
 
 
TO
DT 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT 
 
 
 
 
 
DTN
N    
 
 
RB 
 
 
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
wyd
yfa
n   
hdh
ha
lam
lyta
yda
tshy
ra
lya
dm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
He  
add
s  th
at  t
his  
 
 
proc
ess
 
also
 
 
 
refe
rs
to  t
he  
inab
ility 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  fo
rce
s
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
(b) Rig t source
Align
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
 
 
 
TO  
 
DT 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
VBP
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
DTN
N    
 
 
 
 
 
RB 
 
 
 
 
 
VBP
 
 
 
 
 
 
 
IN   
 
 
NN 
 
 
 
 
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
wyd
yfa
n hd
hh
alam
lyt
ayda
tshy
r
aly
adm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
He  
add
s  th
at th
is    
proc
ess
also
 
 
refe
rs  
 
to  t
he  
inab
ility 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  fo
rce
s
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
(c) Left targetAlign
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
 
 
TO
DT 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wyd
yfa
n  h
dhh
alam
lyt
ayda
tshy
r
aly
adm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
He  
add
s  th
at  t
his  
 
proc
ess
 
 
 
also
 
 
refe
rs
tot
he  
inab
ility 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
DT 
 
 
 
 
DTN
N    
 
 
 
 
 
RB 
 
 
 
 
VBP
IN   
 
 
 
NN 
 
 
 
 
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
(d) Source POS & right tar-
get
Figur 2: Alignment context features.
IBM Model-1 feature but also the surr unding align-
ment context. The key intuition is that collocation is a
reliable indicator for judging if a target word is gener-
ated by a particular source word (Huang, 2009). More-
over, the IBM Model-1 feature was already used in sev-
eral steps of a translation system such as word align-
ment, phrase extraction and scoring. Also the impact of
this feature alone might fade away when the MT sys-
tem is scaled up.
We obtain word-to-word alignments by applying
IBM Model-1 to bilingual phrase pairs that generated
the MT output. The IBM Model-1 assumes one
target word can only be aligned to one source word.
Therefore, given a target word we can always identify
which source word it is aligned to.
Source alignment context feature: We anchor the
target word and derive context features surround-
ing its source word. For example, in Figure 2a
and 2b we have an alignment between ?tshyr? and
?refers? The source contexts ?tshyr? with a window
of one word are ?ayda? to the left and ?aly? to the right.
Target algnment context feature: Similar to source
alignment context features, we anchor the source word
and derive context features surrounding the aligned
target word. Figure 2c shows a left target context
feature of word ?refers?. Our features are derived from
a window of four words.
Combining alignment context with POS tags: In-
stead of using lexical context we have features to look
at source and target POS alignment context. For in-
stance, the feature in Figure 2d is
f141(refers) =
{
1 if source-POS = ?VBP?
and target-context = ?to?
0 otherwise
Sour
ce &
 
Targ
et D
epen
denc
y 
Struc
ture
s
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
NN  
 
 
 
 
 
 
 
RB  
 
 
 
 
VBZ
 
 
 
 
 
TO
DT  
 
 
NN  
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
 
JJ    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wydy
fan
 
 
 
hdhh
alam
lyta
yda
tshyr
alya
dm
qdrt
almt
addt
aljnsy
talq
wat
albh
ryt
He  
adds
 
 
that 
 
this 
 
 
proc
ess
 
 
 
also
 
 
refer
s   to
 
 
the  
inab
ility  
 
of   t
he  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT  
 
 
DTN
N    
 
 
 
 
 
RB  
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S     
DTJ
J
null
(a) Source-Target dependency
Sour
ce &
 
Targ
et D
epen
denc
y 
Struc
ture
s
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
NN  
 
 
 
 
 
 
 
RB  
 
 
 
 
VBZ
 
 
 
 
 
TO
DT  
 
 
NN  
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
 
JJ    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wydy
fan
 
 
 
hdhh
alam
lyta
yda
tshy
ra
lyad
m
qdrt
almt
addt
aljnsy
talq
wat
albh
ryt
He  
adds
 
 
that 
 
this 
 
 
proc
ess
 
 
 
also
 
 
refe
rs
to  th
e  in
abilit
y   o
f   th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT  
 
 
DTN
N    
 
 
 
 
 
RB  
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S     
DTJ
J
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
(b) Child-Father agreement
Sour
ce &
 
Targ
et D
epen
denc
y 
Struc
ture
s
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
NN  
 
 
 
 
 
 
 
RB  
 
 
 
 
VBZ
 
 
 
 
 
TO
DT  
 
 
NN  
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
 
JJ    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wydy
fan
 
 
 
hdhh
alam
lyta
yda
tshy
ra
lyad
m
qdrt
almt
addt
aljnsy
talq
wat
albh
ryt
He  
adds
 
 
that 
 
this 
 
 
proc
ess
 
 
 
also
 
 
refe
rs
to  th
e  in
abilit
y   o
f   th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT  
 
 
DTN
N    
 
 
 
 
 
RB  
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S     
DTJ
J
Child
ren
 
Agre
em
ent: 
2
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
(c) Children agreement
Figure 3: Dependency structures features.
3.3 Source and target dependency structure
features
The contextual and source information in the previous
sections only take into account surface structures of
source and target sentences. Meanwhile, dependency
structures have been extensively used in various
translation systems (Shen et al, 2008; Ma et al,
2008; Bach et al, 2009). The adoption of dependency
structures might enable the classifier to utilize deep
structures to predict translation errors. Source and tar-
get structures are unlikely to be isomorphic as shown
in Figure 3a. However, we expect some high-level
linguistic structures are likely to transfer across certain
language pairs. For example, prepositional phrases
(PP) in Arabic and English are similar in a sense
that PPs generally appear at the end of the sentence
(after all the verbal arguments) and to a lesser extent
at its beginning (Habash and Hu, 2009). We use the
Stanford parser to obtain dependency trees and POS
tags (Marneffe et al, 2006).
Child-Father agreement: The motivation is to take
advantage of the long distance dependency relations
between source and target words. Given an alignment
between a source word si and a target word tj . A child-214
father agreement exists when sk is aligned to tl, where
sk and tl are father of si and tj in source and target
dependency trees, respectively. Figure 3b illustrates
that ?tshyr? and ?refers? have a child-father agreement.
To verify our intuition, we analysed 243K words of
manual aligned Arabic-English bitext. We observed
29.2% words having child-father agreements. In term
of structure types, we found 27.2% of copula verb
and 30.2% prepositional structures, including object
of a preposition, prepositional modifier, and preposi-
tional complement, are having child-father agreements.
Children agreement: In the child-father agreement
feature we look up in the dependency tree, however,
we also can look down to the dependency tree with a
similar motivation. Essentially, given an alignment be-
tween a source word si and a target word tj , how many
children of si and tj are aligned together? For exam-
ple, ?tshyr? and ?refers? have 2 aligned children which
are ?ayda-also? and ?aly-to? as shown in Figure 3c.
4 Experiments
4.1 Arabic-English translation system
The SMT engine is a phrase-based system similar to
the description in (Tillmann, 2006), where various
features are combined within a log-linear framework.
These features include source-to-target phrase transla-
tion score, source-to-target and target-to-source word-
to-word translation scores, language model score, dis-
tortion model scores and word count. The training
data for these features are 7M Arabic-English sentence
pairs, mostly newswire and UN corpora released by
LDC. The parallel sentences have word alignment au-
tomatically generated with HMM and MaxEnt word
aligner (Ge, 2004; Ittycheriah and Roukos, 2005).
Bilingual phrase translations are extracted from these
word-aligned parallel corpora. The language model is
a 5-gram model trained on roughly 3.5 billion English
words.
Our training data contains 72k sentences Arabic-
English machine translation with human corrections
which include of 2.2M words in newswire and weblog
domains. We have a development set of 2,707 sen-
tences, 80K words (dev); an unseen test set of 2,707
sentences, 79K words (test). Feature selection and pa-
rameter tuning has been done on the development set in
which we experimented values of C, n and iterations in
range of [0.5:10], [1:5], and [50:200] respectively. The
final MIRA classifier was trained by using pocket crf
toolkit1 with 100 iterations, hyper-parameter C was 5
and cut-off feature threshold n was 1.
We use precision (P ), recall (R) and F-score (F ) to
evaluate the classifier performance and they are com-
1http://pocket-crf-1.sourceforge.net/
puted as follow:
P = the number of correctly tagged labelsthe number of tagged labels
R = the number of correctly tagged labelsthe number of reference labels
F = 2*P*RP+R
(8)
4.2 Contribution of feature sets
We designed our experiments to show the impact
of each feature separately as well as their cumu-
lative impact. We trained two types of classifiers
to predict the error type of each word in MT out-
put, namely Good/Bad with a binary classifier and
Good/Insertion/Substitution/Shift with a 4-class classi-
fier. Each classifier is trained with different feature sets
as follow:
? WPP: we reimplemented WPP calculation based
on n-best lists as described in (Ueffing and Ney,
2007).
? WPP + target POS: only WPP and target POS fea-
tures are used. This is a similar feature set used by
Xiong et al (2010).
? Our features: the classifier has source side, align-
ment context, and dependency structure features;
WPP and target POS features are excluded.
? WPP + our features: adding our features on top of
WPP.
? WPP + target POS + our features: using all fea-
tures.
binary 4-class
dev test dev test
WPP 69.3 68.7 64.4 63.7
+ source side 72.1 71.6 66.2 65.7
+ alignment context 71.4 70.9 65.7 65.3
+ dependency structures 69.9 69.5 64.9 64.3
WPP+ target POS 69.6 69.1 64.4 63.9
+ source side 72.3 71.8 66.3 65.8
+ alignment context 71.9 71.2 66 65.6
+ dependency structures 70.4 70 65.1 64.4
Table 1: Contribution of different feature sets measure
in F-score.
To evaluate the effectiveness of each feature set, we
apply them on two different baseline systems: using
WPP and WPP+target POS, respectively. We augment
each baseline with our feature sets separately. Ta-
ble 1 shows the contribution in F-score of our proposed
feature sets. Improvements are consistently obtained
when combining the proposed features with baseline
features. Experimental results also indicate that source-
side information, alignment context and dependency215
Pred
ictin
g Go
od/B
ad w
ords
 
59.4
59.3
69.3
68.7
69.6
69.1
72.1
71.5
72.4
72
72.6
72.2
586062646668707274
dev
test
Test
 
sets
F-score
WPP
+targ
etPO
S+Ou
rfeat
ures
WPP
+Our
featu
res
Our f
eatur
es
WPP
+targ
etPO
S
WPP
All-G
ood
(a) Binary
Pred
icting
 
Good
/Inse
rtion
/Subs
titutio
n/Shi
ft wo
rds
59.4
59.3
64.4
63.7
64.4
63.9
66.2
65.6
66.6
65.9
66.8
66.1
5859606162636465666768
dev
test
Test 
sets
F-score
WPP
+targ
etPO
S+Ou
rfeat
ures
WPP
+Our
featu
res
Our f
eatur
es
WPP
+targ
etPO
S
WPP
All-Go
od
(b) 4-class
Figure 4: Performance of binary and 4-class classifiers trained with different feature sets on the development and
unseen test sets.
structures have unique and effective levers to improve
the classifier performance. Among the three proposed
feature sets, we observe the source side information
contributes the most gain, which is followed by the
alignment context and dependency structure features.
4.3 Performance of classifiers
We trained several classifiers with our proposed feature
sets as well as baseline features. We compare their per-
formances, including a naive baseline All-Good classi-
fier, in which all words in the MT output are labelled
as good translations. Figure 4 shows the performance
of different classifiers trained with different feature sets
on development and unseen test sets. On the unseen test
set our proposed features outperform WPP and target
POS features by 2.8 and 2.4 absolute F-score respec-
tively. Improvements of our features are consistent in
development and unseen sets as well as in binary and
4-class classifiers. We reach the best performance by
combining our proposed features with WPP and target
POS features. Experiments indicate that the gaps in F-
score between our best system with the naive All-Good
system is 12.9 and 6.8 in binary and 4-class cases, re-
spectively. Table 2 presents precision, recall, and F-
score of individual class of the best binary and 4-class
classifiers. It shows that Good label is better predicted
than other labels, meanwhile, Substitution is gener-
ally easier to predict than Insertion and Shift.
4.4 Correlation between Goodness and HTER
We estimate sentence level confidence score based
on Equation 7. Figure 5 illustrates the correla-
tion between our proposed goodness sentence level
confidence score and the human-targeted translation
edit rate (HTER). The Pearson correlation between
goodness and HTER is 0.6, while the correlation of
WPP and HTER is 0.52. This experiment shows that
goodness has a large correlation with HTER. The
black bar is the linear regression line. Blue and red
Label P R F
Binary
Good 74.7 80.6 77.5
Bad 68 60.1 63.8
4-class
Good 70.8 87 78.1
Insertion 37.5 16.9 23.3
Substitution 57.8 44.9 50.5
Shift 35.2 14.1 20.1
Table 2: Detailed performance in precision, recall
and F-score of binary and 4-class classifiers with
WPP+target POS+Our features on the unseen test set.
bars are thresholds used to visualize good and bad sen-
tences respectively. We also experimented goodness
computation in Equation 7 using geometric mean and
harmonic mean; their Pearson correlation values are 0.5
and 0.35 respectively.
5 Improving MT quality with N-best list
reranking
Experiments reporting in Section 4 indicate that the
proposed confidence measure has a high correlation
with HTER. However, it is not very clear if the core MT
system can benefit from confidence measure by provid-
ing better translations. To investigate this question we
present experimental results for the n-best list rerank-
ing task.
The MT system generates top n hypotheses and for
each hypothesis we compute sentence-level confidence
scores. The best candidate is the hypothesis with high-
est confidence score. Table 3 shows the performance of
reranking systems using goodness scores from our best
classifier in various n-best sizes. We obtained 0.7 TER
reduction and 0.4 BLEU point improvement on the de-
velopment set with a 5-best list. On the unseen test, we
obtained 0.6 TER reduction and 0.2 BLEU point im-
provement. Although, the improvement of BLEU score216
0.91
Goo
d?
Bad Line
ar?f
it
0.70.8 040.50.6
Goodness
0.20.30.4 00.10.2
0
20
40
60
80
100
HTE
R
Figure 5: Correlation between Goodness and HTER.
Dev Test
TER BLEU TER BLEU
Baseline 49.9 31.0 50.2 30.6
2-best 49.5 31.4 49.9 30.8
5-best 49.2 31.4 49.6 30.8
10-best 49.2 31.2 49.5 30.8
20-best 49.1 31.0 49.3 30.7
30-best 49.0 31.0 49.3 30.6
40-best 49.0 31.0 49.4 30.5
50-best 49.1 30.9 49.4 30.5
100-best 49.0 30.9 49.3 30.5
Table 3: Reranking performance with goodness score.
is not obvious, TER reductions are consistent in both
development and unseen sets. Figure 6 shows the im-
provement of reranking with goodness score. Besides,
the figure illustrates the upper and lower bound perfor-
mances with TER metric in which the lower bound is
our baseline system and the upper bound is the best hy-
pothesis in a given n-best list. Oracle scores of each n-
best list are computed by choosing the translation can-
didate with lowest TER score.
6 Visualizing translation errors
Besides the application of confidence score in the n-
best list reranking task, we propose a method to visual-
ize translation error using confidence scores. Our pur-
pose is to visualize word and sentence-level confidence
scores with the following objectives 1) easy for spotting
translations errors; 2) simple and intuitive; and 3) help-
ful for post-editing productivity. We define three cate-
gories of translation quality (good/bad/decent) on both
word and sentence level. On word level, the marginal
probability of good label is used to visualize translation
errors as follow:
Li =
?
?
?
good if p(yi = Good|S) ? 0.8
bad if p(yi = Good|S) ? 0.45
decent otherwise
42
43
44
45
46
47
48
49
50
51
1 2 5 10 20 30 40 50 100
TE
R
N-best size
Oracle
Our models
Baseline
Figure 6: A comparison between reranking and oracle
scores with different n-best size in TER metric on the
development set.
On sentence level, the goodness score is used as follow:
LS =
?
?
?
good if goodness(S) ? 0.7
bad if goodness(S) ? 0.5
decent otherwise
Choices Intention
Font size
big bad
small good
medium decent
Colors
red bad
black good
orange decent
Table 4: Choices of layout
Different font sizes and colors are used to catch the
attention of post-editors whenever translation errors are
likely to appear as shown in Table 4. Colors are ap-
plied on word level, while font size is applied on both
word and sentence level. The idea of using font size
and colour to visualize translation confidence is simi-
lar to the idea of using tag/word cloud to describe the
content of websites2. The reason we are using big font
size and red color is to attract post-editors? attention
and help them find translation errors quickly. Figure 7
shows an example of visualizing confidence scores by
font size and colours. It shows that ?not to deprive
yourself ?, displayed in big font and red color, is likely
to be bad translations. Meanwhile, other words, such
as ?you?, ?different?, ?from?, and ?assimilation?, dis-
played in small font and black color, are likely to be
good translation. Medium font and orange color words
are decent translations.
2http://en.wikipedia.org/wiki/Tag cloud217
you
 
tota
lly d
iffe
ren
t fro
m 
za
ida
mr
, 
an
d n
ot t
o d
epr
ive
 
you
rse
lf in
 
a b
as
em
en
t of
 
imi
tati
on
 
an
d a
ss
imi
lati
on
 
.
  ?? 
	

??? ?
 
 
 
 
 ?
? 
?Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 1?10,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-supervised Word Alignment Algorithm
with Partial Manual Alignments
Qin Gao, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA, 15213
{qing, nbach, stephan.vogel}@cs.cmu.edu
Abstract
We present a word alignment framework
that can incorporate partial manual align-
ments. The core of the approach is a
novel semi-supervised algorithm extend-
ing the widely used IBM Models with
a constrained EM algorithm. The par-
tial manual alignments can be obtained
by human labelling or automatically by
high-precision-low-recall heuristics. We
demonstrate the usages of both methods
by selecting alignment links from manu-
ally aligned corpus and apply links gen-
erated from bilingual dictionary on unla-
belled data. For the first method, we con-
duct controlled experiments on Chinese-
English and Arabic-English translation
tasks to compare the quality of word align-
ment, and to measure effects of two differ-
ent methods in selecting alignment links
from manually aligned corpus. For the
second method, we experimented with
moderate-scale Chinese-English transla-
tion task. The experiment results show an
average improvement of 0.33 BLEU point
across 8 test sets.
1 Introduction
Word alignment is used in various natural lan-
guage processing applications, and most statistical
machine translation systems rely on word align-
ment as a preprocessing step. Traditionally the
word alignment model is trained in an unsuper-
vised manner, e.g. the most widely used tool
GIZA++ (Och and Ney, 2003), which implements
the IBM Models (Brown et. al., 1993) and the
HMM model (Vogel et al, 1996). However, for
language pairs such as Chinese-English, the word
alignment quality is often unsatisfactory (Guzman
et al, 2009). There has been increasing interest on
using manual alignments in word alignment tasks.
Ittycheriah and Roukos (2005) proposed to use
only manual alignment links in a maximum en-
tropy model. A number of semi-supervised word
aligners are proposed (Blunsom and Cohn, 2006;
Niehues and Vogel, 2008; Taskar et al, 2005; Liu
et al, 2005; Moore, 2005). These approaches use
held-out manual alignments to tune the weights
for discriminative models, with the model param-
eters, model scores or alignment links from un-
supervised word aligners as features. Also, sev-
eral models are proposed to address the prob-
lem of improving generative models with small
amount of manual data, including Model 6 (Och
and Ney, 2003) and the model proposed by Fraser
and Marcu (2006) and its extension called LEAF
aligner (Fraser and Marcu, 2007). The approaches
use labelled data to tune parameters to combine
different components of the IBM Models.
2005? ? ??
2005nian     de      xiatian
The   summer   of    2005
Figure 1: Partial and full alignments
An interesting question is, if we only have par-
tial alignments of sentences, can we make use of
them? Figure 1 shows the comparison of par-
tial alignments (the bold link) and full alignments
(both of the dashed and the bold links). A partial
alignment of a sentence only provides a portion of
links of the full alignment. Although it seems to be
trivial, they actually convey different information.
In the example, if the full alignment is given, we
can assert 2005 is only aligned to 2005nian, not to
de or xiatian, but if only the partial alignment is
given we cannot make such assertion.
Partial alignments can be obtained from vari-
ous sources, for example, we can fetch them by
manually correcting unsupervised alignments, by
simple heuristics such as dictionaries of technical
1
terms, by rule-based alignment systems that have
high accuracy but low recall rate. The function-
ality is considered useful in many scenarios. For
example, the researchers can analyse the align-
ments generated by GIZA++ and fix common
error patterns, and perform training again. On
another way, an application can combine active
learning (Arora et al, 2009) and crowdsourcing,
asking non-expertise such as workers of Amazon
Mechanical Turk to label crucial alignment links
that can improve the system with low cost, which
is now a promising methodology in NLP areas
(Callison-Burch, 2009).
In this paper, we propose a semi-supervised ex-
tension of the IBM Models that can utilize partial
alignment links. More specifically, we are seeking
answers for the following questions:
? Given the partial alignment of a sentence,
how to find the most probable alignment that
is consistent with the partial alignment.
? Given a set of partially aligned sentences,
how to get the parameters that maximize the
likelihood of the sentence pairs with align-
ments consistent with the partial alignments
? Given a set of partially aligned sentences,
with conflicting partial alignments, how to
answer the two questions above.
In the proposed approach, the manual partial
alignment links are treated as ground truth, there-
fore, they will be fixed. However, for all other
links we make no additional assumption. When
using manual alignments, there can be links con-
flicting with each other. These conflicting evi-
dences are treated as options and the generative
model will choose the most probable alignment
from them. An efficient training algorithm for
fertility-based models is proposed. The algorithm
manipulates the Moving and Swapping matrices
used in the hill-climbing algorithm (Och and Ney,
2003) to rule out inconsistent alignments in both
E-step and M-step of the training.
A similar attempt has been made by Callison-
Burch et al (2004), where the authors interpo-
late the parameters estimated by sentence-aligned
and word-aligned corpus. Our approach is differ-
ent from their method that we do not require fully
aligned data and we do not need to interpolate two
parameter sets. All the training is done within a
unified framework. Our approach is also different
from LEAF (Fraser and Marcu, 2007) and Model
6 (Och and Ney, 2003) that we do not use these
additional links to tune additional parameters to
combine model components, as a result, it is not
limited to fully aligned corpus.
A question may raise why the proposed method
is superior over using the partial alignment links
as features in discriminative aligners? There are
three possible explanations. First, the method pre-
serves the power of the generative model in which
the algorithm utilizes large amount of unlabeled
data. More importantly, the additional information
can propagate over the whole corpus through bet-
ter estimation of model parameters. In contrast, if
we use the alignment links in discriminative align-
ers as a feature, one link can only affect the par-
ticular word, or at most the sentence. Second, al-
though the discriminative word alignment meth-
ods provide flexibility to utilize labeled data, most
of them still rely on generative aligners. Some
rely on the model parameters of the IBM Mod-
els (Liu et al, 2005; Blunsom and Cohn, 2006),
others rely on the alignment links from GIZA++
as features or as training data (Taskar et al, 2005),
or use both the model parameters and the align-
ment links (Niehues and Vogel, 2008). Therefore,
improving the generative aligner is still important
even when using discriminative aligners. Third,
these methods require full alignment of sentences
to provide positive (aligned) and negative (non-
aligned) information, which limits the availability
of data (Niehues and Vogel, 2008).
The proposed method has been successfully ap-
plied on various tasks, such as utilizing manual
alignments harvested from Amazon Mechanical
Turk (Gao and Vogel, 2010), and active learning
methods for improving word alignment (Ambati
et al, 2010). This paper provides the detailed al-
gorithm of the method and controlled experiments
to demonstrate its behavior.
The paper is organized as follows, in section
2 we describe the proposed model as well as the
modified training algorithm. Section 3 presents
two approaches of obtaining manual alignment
links, The experimental results will be shown in
section 4. We conclude the paper in section 5.
2 Semi-supervised word alignment
2.1 Problem Setup
The IBM Models (Brown et. al., 1993) are a
series of generative models for word alignment.
GIZA++ (Och and Ney, 2003) is the most widely
used implementation of the IBM Models and the
2
HMM model (Vogel et al, 1996). Given two
strings from target and source languages fJ1 =
f1, ? ? ? , fj , ? ? ? fJ and eI1 = e1, ? ? ? , ei, ? ? ? eI , an
alignment of the sentence pair is defined as aJ1 =
[a1, a2, ? ? ? , aJ ], aj ? [0, I]. The IBM Models
assume all the target words must be covered ex-
actly once (Brown et. al., 1993). We try to model
P (fJ1 |e
I
1), which is the probability of observing
source sentence given target sentence eI1. In sta-
tistical models a hidden alignment variable is in-
troduced, so that we can write the probability as
P (fJ1 |e
I
1) =
?
aJ1
Pr(fJ1 , a
J
1 |e
J
1 , ?), where Pr(?)
is the estimated probability given the parameter set
?. The IBM Models define several different set of
parameters, from Model 1 to Model 5. Starting
from Model 3, the fertility model is introduced.
EM algorithm is employed to estimate the
model parameters of the IBM Models. In E-step,
it is possible to obtain sufficient statistics from
all possible alignments with simplified formulas
for simple models such as Model 1 and Model 2.
Meanwhile for fertility-based models, enumerat-
ing all possibilities is NP-complete and hence it
cannot be carried out for long sentences. A solu-
tion is to explore only the ?neighbors? of Viterbi
alignments. However, obtaining Viterbi align-
ments itself is NP-complete for these models. In
practice, a greedy algorithm is employed to find
a local optimal alignments based on Viterbi align-
ments generated by simpler models.
First, we define the neighbor alignments of a as
the set of alignments that differ by one of the two
operators from the original ?center alignment?.
? Move operator m[i,j], that changes aj := i,
i.e. arbitrarily set word fj in source sentence
to align to word fi in target sentence.
? Swap operator s[j1,j2] that exchanges aj1 and
aj2 .
We denote the neighbor alignments set of
current center alignment a as nb(a). In
each step of hill-climbing algorithm, we find
the alignment b(a) in nb(a), s.t. b(a) =
argmaxa??nb(a) p(a
?|e, f), and update the current
center alignment. The algorithm iterates until
there is no update could be made. The statistics of
the neighbor alignments of the final center align-
ment will be collected for normalization step (M-
step). The algorithm is greedy, so a reasonable
start point is important. In practice GIZA++ uses
Model 2 or HMM to generate the seed alignment.
To improve the speed of hill climbing, GIZA++
caches the cost of all possible move and swap op-
erations in two matrices. In the so called Moving
Matrix M , the element Mij stores the likelihood
difference of a move operator aj = i:
Mij =
Pr(m[i,j](a)|e, f)
Pr(a|e, f)
? (1? ?(aj , i)) (1)
and in the Swapping Matrix S, the element Sjj?
stores the likelihood difference of a swap operator
between aj and aj? :
Sjj? =
{
Pr(S[j,j?](a)|e,f)
Pr(a|e,f) ? (1? ?(aj , aj?)) if j < j
?
0 otherwise
(2)
The matrices will be updated whenever an oper-
ator is made, but the update is limited to the rows
and columns involved in the operator.
We define a partial alignment of a sentence
pair (fJ1 , e
I
1) as ?
J
I = {(i, j), 0 ? i < I, 0 ?
j < J}, note that the partial alignment does not
assume 1-to-N restriction on either side, and the
word from neither source nor target side need to be
covered with links. If an index is missing, it does
not mean the word is aligned to the empty word.
Instead it just means no information is provided.
We use a link (0, j) or (i, 0) to explicitly represent
the information that word fj or ei is aligned to the
empty word.
In order to find the most probable align-
ment that is consistent the partial alignments,
we treat the partial alignment as constraints, i.e.
for an alignment aJ1 = [a1, a2, ? ? ? , aj ] on the
sentence pair fJ1 , e
I
1, the translation probability
Pr(fJ1 , a
J
1 |e
I
1, ?
J
I ) will be zero if the alignment is
inconsistent with the partial alignments.
Pr(fJ1 |e
I
1, a
J
1 , ?
J
I ) =
{
0, aJ1 is inconsistent with?
J
I
Pr(fJ1 |e
I
1, a
J
1 , ?), otherwise
(3)
Under the constraints of the IBM Models, there
are two situations that aJ1 is inconsistent with ?
J
I :
1. Target word misalignment: The IBM Models
assume one target word can only be aligned
to one source word. Therefore, if the target
word fj aligns to a source word ei, while the
constraint ?JI suggests fj should be aligned
to ei? , the alignment violates the constraint
and thus is considered inconsistent.
3
2005?
?
??
the
summer
of
2005Manual Alignment Link
(a)
2005?
?
??
the
summer
Of
2005Seed Alignment Consistent Alignment Center Alignment
(b)                    (c)
2005?
?
??
the
summer
of
2005
Figure 2: Illustration of Algorithm 1
2. Source word to empty word misalignment:
Since one source word can be aligned to mul-
tiple target words, it is hard to constrain the
alignments of source words. However, if a
source word is aligned to the empty word,
it cannot be aligned to any concrete target
word.
However, we are facing the problem of con-
flicting evidences. The problem is not necessar-
ily caused by errors in manual alignments, but
the assumption of the IBM Models that one tar-
get word can only be aligned to one source word.
This assumption causes multiple alignment links
from one target word conflict with each other. In
this case, we relax the constraints of situation 1
that if the alignment link aj? is consistent with any
target-to-source links (i, j) that j = j?, it will be
considered consistent. Also, we arbitrarily assign
the source word to empty word constraints higher
priorities than other constraints.
In EM algorithm, to ensure the final model be
marginalized on the fixed alignment links, and
the final Viterbi alignment is consistent with the
fixed alignment links, we need to guarantee that
no statistics from inconsistent alignments be col-
lected into the sufficient statistics. On fertility-
based models, we have to make sure:
1. The hill-climbing algorithm outputs align-
ment links consistent with the fixed align-
ment links.
2. The count collection algorithm rules out all
the inconsistent statistics.
With the constrained hill-climbing algorithm
and count collection algorithm which will be de-
scribed below, the above two criteria are satisfied.
2.2 Constrained hill-climbing algorithm
Algorithm 1 shows the algorithm outline of con-
strained hill-climbing. First, similar to the original
hill-climbing algorithm described above, HMM
(or Model 2) is used to obtain a seed alignment.
To ensure the resulting center alignment be con-
sistent with manual alignment, we need to split the
Algorithm 1 Constrained Hill-Climbing
1: Calculate the seed alignment a0 using HMM model
2: while ic(a0) > 0 do
3: if {a : ic(a) < ic(a0)} = ? then
4: break
5: end if
6: a0 := argmaxa?nb(a0),ic(a)<ic(a0) Pr(f |e, a)
7: end while
8: Mij := ?1 if (i, j) 6? ?JI or (i, 0) ? ?
J
I
9: loop
10: Sjj? := ?1 if (j, aj?) 6? ?
J
I or (j
?, aj) 6? ?JI
11: Mi1j1 = argmaxMij ; Sj1j?1 = argmaxSij
12: if Mi1j1 ? 1 and Sj1j?1 ? 1 then
13: Break
14: end if
15: if Mi1j1 > Sj1j?1 then
16: Update Mi1?,Mj1?,M?i1 ,M?j1
and Si1?, Sj1?, S?i1 , S?j1 , set a0 := Mi1j1(a0)
17: else
18: Update Mj1?,Mj?1?,M?j1 ,M?j?1
and Sj?1?, Sj1?, S?j?1 , S?j1 , set a0 := Sj1j?1(a0)
19: end if
20: end loop
21: Return a0
hill-climbing algorithm into two stages, i.e. opti-
mize towards the constraints and towards the opti-
mal alignment under the constraints.
From a seed alignment, we first try to move the
alignment towards the constraints by choosing a
move or swap operator that:
1. has highest likelihood among alignments
generated by other operators, excluding the
original alignment,
2. eliminates at least one inconsistent link.
The first step reflects in line 2 through 7 in the
algorithm, where we use ic(?) to denote the total
number of inconsistent links in the alignment, and
nb(?) to denote the neighbor alignments.
We iteratively update the alignment until no ad-
ditional inconsistent link can be removed. The al-
gorithm implies that we force the seed alignment
to become closer to the constraints while trying
to find the best consistent alignment. Figure 2
demonstrates the idea, given the manual alignment
link shown in (a), and the seed alignment shown as
solid links in (b), we move the inconsistent link to
the dashed link by a move operation.
After we find the consistent alignment, we pro-
ceed to optimize towards the optimal alignment
within the constraints. The algorithm sets the cells
to negative if the corresponding operations are not
allowed. The Moving matrix only need to be up-
dated once, as in line 8 of the algorithm. Whereas
the swapping matrix need to be updated every it-
4
eration, Since once the alignment is updated, the
possible violations will also change. This is done
in line 10.
If source words ik are aligned to the empty
word, we set Mik,j = ?1,?j, as shown in line 8.
The swapping matrix does not need to be modified
in this case because the swapping operator will not
introduce new links. Again, Figure 2 demonstrates
the optimization step in (c), two move operators
or one swap operator can move the link marked
with cross to the dashed line, which can be a bet-
ter alignment.
Because the cells that can lead to violations are
set to negative, the operators will never be picked
in line 11, therefore we effectively ensure the con-
sistency of the final center alignment.
The algorithm will end when no better update
can be made (line 12 through 14), otherwise, we
pick the new update with highest likelihood as new
center alignment and update the cells in the Mov-
ing and Swapping matrices that will be affected
by the update. Line 15 through line 19 perform
the operation.
2.3 Count Collection
After finding the center alignment, we collect
counts from the neighbor alignments so that the
M-step can normalize the counts to produce the
model parameters for the next step. All statis-
tics from inconsistent alignments are ruled out to
ensure the final sufficient statistics marginalized
on the fixed alignment links. Similar to the con-
strained hill climbing algorithm, we can manipu-
late the Moving/Swapping matrices to effectively
exclude inconsistent alignments. We just need to
bypass all the cells whose values are negative, i.e.
represent inconsistent alignments.
By combining the constrained EM algorithm
and the count collection, the Viterbi alignment is
guaranteed to be consistent with the fixed align-
ment links, and the sufficient statistics is guar-
anteed to contain no statistics from inconsistent
alignments.
2.4 Training scheme
We extend the multi-thread GIZA++ (Gao and
Vogel, 2008) to load the alignments from a mod-
ified corpus file. The links are appended to the
end of each sentence in the corpus file in the form
of indices pairs, which will be read by the aligner
during training. In practice, we first training un-
constrained models up to Model 4, and then switch
to constrained Model 4 and continue training for
several iterations, the actual number of training
order is: 5 iterations of Model 1, 5 iterations of
HMM, 3 iterations of Model 3, 3 iterations of
unconstrained Model 4 and 3 iterations of con-
strained Model 4. Because here we actually have
more Model 4 iterations, to make the comparison
fair, in all the experiments below we perform 6 it-
erations of Model 4 in the baseline systems.
3 Obtaining alignment links
Given the algorithm described in the Section 2,
we still face the problem of obtaining alignment
links to constrain the system. In this section, we
describe two approaches to obtain the links, the
first is to resort to human labels, while the second
applies high-precision-low-recall heuristic-based
aligner on large unsupervised corpus.
3.1 Using manual alignment links
Using manual alignment links is simple and
straight-forward, however the problem is how to
select links for human to label given that labelling
the whole corpus is impossible. We propose two
link selectors, the first is the random selector in
which every links in the manual alignment has
equal probability of being selected. Obviously,
the random selecting method is far from optimal
because it pays no attention on the quality of ex-
isting links. In order to demonstrate that by select-
ing links carefully we can achieve better alignment
quality with less manual alignment links, we pro-
pose the second selector based on disagreements
of alignments from two directions. We first clas-
sify the source and target words fj and ei into
three categories. Use fj as an example, the cat-
egories are:
? C1: fj aligns to ei, i > 0 in e ? f ,1 but in
reversed direction ei does not align to fj but
to another word.
? C2: fj aligns to ei, i > 0, in f ? e, but in
reversed direction (e ? f ), fj aligns to the
empty word.
? C3: no word aligns to fj , in f ? e, but in
reversed direction fj aligns to ei, i > 0.2
The criteria of ei are the same as fj after swap-
ping the definitions of ?source? and ?target?.
We prioritize the links ?JI = (i, j) by looking at
the classes of the source/target words. The order of
1Recall that fj can align to only one word.
2This class is different from C1 that whether ei aligns to
concrete words or the empty word.
5
Order Criterion Order Criterion
1 fj ? C1 5 ei ? C2
2 fj ? C2 4 ei ? C1
3 fj ? C3 6 ei ? C3
Table 1: The priorities of alignment links
priorities is shown in Table 1. All the links not in
the six classes will have the lowest priorities. The
links with higher priorities will be selected first,
but the order of two links in a same priority class
is not defined and they will be selected randomly.
3.2 Using heuristics on unlabelled data
Another possible way of getting alignment links
is to make use of heuristics to generate high-
precision-low-recall links and feed them into the
aligner. The heuristics can be number map-
ping, person name translator or more sophisticated
methods such as alignment confidence measure
(Huang, 2009). In this paper we propose to use
manual dictionaries to generate alignment links.
First we filter out from the dictionary the en-
tries with high frequency in the source side, and
then build an aligner based on it. The aligner out-
put links between words if them match an entry
in the dictionary. The method can be applied on
large unlabelled corpus and generate large num-
ber of links, after that we use the links as manual
alignment links in proposed method.
The readers may notice that GIZA++ supports
utilizing manual dictionary as well, however it is
different from our method. The dictionary is used
in GIZA++ only in the initialization step of Model
1, where only the statistics of the word pairs ap-
peared in the dictionary will be collected and nor-
malized. Given the fact that Model 1 converges to
global optimal, the effect will fade out after sev-
eral iterations. In contrast, our method impose
a hard constraint on the alignments. Also, our
method can be used side-by-side with the method
in GIZA++.
4 Experiments
4.1 Experiments on manual link selectors
We designed a set of controlled experiments to
show that the algorithm acts as desired. Particu-
larly, with a number of manual alignment links fed
into the aligner, we should be able to correct more
misaligned alignment links than the manual align-
ment links through better alignment models. Also,
carefully selected alignment links should outper-
form randomly selected alignment links.
We used Chinese-English and Arabic-English
manually aligned corpus in the experiments. Ta-
ble 2 shows the statistics of the corpora:
Number of Num. of Words Alignment
Sentences Source Target Links
Ch-En 21,863 424,683 524,882 687,247
Ar-En 29,876 630,101 821,938 830,349
Table 2: Corpus statistics of the corpora
First the corpora is trained as unlabelled data
to serve as baselines, and then we feed a portion
of alignment links into the proposed aligner. We
experimented with different methods of choosing
alignment links and adjust the number of links vis-
ible to the aligner. Because of the limitations of
the IBM Models, such as no N-to-1 alignments,
the manual alignment is not reachable from ei-
ther direction. We then define the best align-
ment that the IBM Models can express ?oracle
alignment?, which can be obtained by dropping
all N-to-1 links from manual alignment. Also, to
show the upper-bound performance, we feed all
the manual alignment links to our aligner, and call
the alignment ?force alignment?. Table 3 shows
the alignment qualities of oracle alignments and
force alignments of both systems. For force align-
ments, we show the scores with and without im-
plicit empty links derived from the manual align-
ment.3 The oracle alignments are the performance
upper-bounds of all aligners under IBM Model?s
1-to-N assumption. The result from Table 3 shows
that, if we include the derived empty links, the
force alignments are close to the oracle results.
Then the question is how fast we can approach the
upper-bound.
To answer the question, we gradually increase
the number of links being fed into the aligner. In
these experiments the seeds for random number
generator are fixed so that the links selected in
later experiments are always superset of that of
earlier experiments. The comparison of the align-
ment quality is shown in Figure 3 and 4. To show
the actual improvement brought in by the algo-
rithm instead of the manual alignment links them-
selves, we compare the alignment results of the
proposed method with directly fixing the align-
ments from original GIZA++ training. By fix-
ing alignments we mean that first the conventional
3We can derive empty links if one word has no alignment
link from the full alignment we have access to.
6
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
100
Number of Links
Pre
cisio
n
Precision?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
48
50
52
54
56
58
60
62
Number of Links
Rec
all
Recall?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
40
Number of Links
AER
AER?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
100
Number of Links
Pre
cisio
n
Precision?Number of links (English?Chinese)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
60
65
70
75
80
Number of Links
Rec
all
Recall?Number of links (English?Chinese)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
40
Number of Links
AER
AER?Number of links (English?Chinese)
 
 NNWNDFFRFD
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
Number of Links
Pre
cisio
n
Precision?Number of links (Combined Ch/En)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
60
65
70
75
80
85
90
Number of Links
Rec
all
Recall?Number of links (Combined Ch/En)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (Combined Ch/En)
 
 NNWNDFFRFD
Figure 3: Alignment qualities of Chinese-English word alignment, NN: Random selector without empty
links, WN: Random seletor with empty links, DF: Disagreement selector, FR: Directly fixing the align-
ments with random selector, FD: Directly fixing the alignments with disagreement selector. Each row
shows the precision, recall and AER when applying different number of manual alignment links. The
three rows are for Chinese-English, English-Chinese and heuristically symmetrized alignments (grow-
diag-final-and) accordingly.
GIZA++ training is performed and then we add the
manual alignment links to the resulting alignment.
In case that the 1-to-N restriction of the IBM Mod-
els is violated, we keep the manual alignment links
and remove the links from GIZA++.
We show the results as FR (dashed curves with
diamond markers) and FD (dashed curves with
square markers) in the plots, corresponding to
alignments selected from the random link selector
and the disagreement-based link selector. These
two curves serve as baseline, and the gaps between
the FR curves and the WN curves (dotted curves
with cross markers) and the gaps between the FD
curves and the DF curves (solid curves) show the
amount of improvement we achieved using the
method in addition to the manual alignment links.
Therefore, they represent the effectiveness of the
proposed alignment approach. Also the gaps be-
tween DF and WN curves indicate the differences
in the performance of two link selectors.
The plots illustrate that when the number of
links is small, the WN and DF curves are al-
ways higher than the FR/FD curves. It proves
that our system does not just fix the links pro-
vided by manual alignments, instead the informa-
tion propagates to other links. The largest gap
between FD and DF is 8% absolute in com-
bined alignment of Chinese-English system with
200,000 manual alignment links. Also, we can
see that the disagreement-based link selector (DF)
always outperform the random selector (WN). It
suggest that, if we want to harvest manual align-
ment links, it is possible to apply active learning
method to minimize the user labelling effort while
maximizing the improvement on word alignment
qualities. Especially, notice that in the lower parts
7
0 1 2 3 4 5 6 7 8
x 105
60
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
55
60
65
70
Number of Links
Rec
all
Recall?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
40
45
Number of Links
AER
AER?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
60
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (English?Arabic)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
70
75
80
85
90
Number of Links
Rec
all
Recall?Number of links (English?Arabic)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (English?Arabic)
 
 NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (Combined En/Ar)
 
 NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
70
75
80
85
90
95
100
Number of Links
Rec
all
Recall?Number of links (Combined En/Ar)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (Combined En/Ar)
 
 
NNWNDFFRFD
Figure 4: Alignment qualities of Arabic-English word alignment, NN: Random selector without empty
links, WN: Random selector with empty links, DF: Disagreement selector, FR: Directly fixing the align-
ments with random selector, FD: Directly fixing the alignments with disagreement selector. Each row
shows the precision, recall and AER when applying different number of manual alignment links. The
three rows are for Arabic-English, English-Arabic and heuristically symmetrized alignments (grow-diag-
final-and) accordingly.
of the curves, with a small number of manual
alignment links, we can already improve the align-
ment quality by a large gap. This observation can
benefit low-resource word alignment tasks.
4.2 Experiment on using heuristics
The previous experiment shows the potential of
using the method on manual aligned corpus, here
we demonstrate another possible usage of the pro-
posed method that uses heuristics to generate high-
precision-low-recall links. We use LDC Chinese-
English dictionary as an example. The entries with
single Chinese character and more than six En-
glish words are filtered out. The heuristic-based
aligner yields alignment that has 79.48% preci-
sion and 17.36% recall rate on the test set we used
in 4.1. By applying the links as manual links,
we run proposed method on the same Chinese-
English test data presented in 4.1, and the results
of alignment qualities are shown in 5. As we can
see, the AER reduced by 1.64 from 37.23 to 35.61
on symmetrized alignment.
We also experimented with translation tasks
with moderate-size corpus. We used the corpus
LDC2006G05 with 25 million words. The train-
ing scheme is the same as previous experiments,
where the filtered LDC dictionary is used. After
word alignment, standard Moses phrase extraction
tool (Och and Ney, 2004) is used to build the trans-
lation models and finally Moses (Koehn et. al.,
2007) is used to tune and decode.
We tune the system on the NIST MT06 test
set (1664 sentences), and test on the MT08 (1357
sentences) and the DEV075 (1211 sentences) test
sets, which are further divided into two sources
(newswire and web data). A trigram language
5It is a test set used by GALE Rosseta Team
8
MT02 MT03 MT04 MT05 MT08-NW MT08-WB Dev07NW Dev07WB
Baseline 28.87 27.82 30.08 26.77 25.09 17.72 24.88 21.76
Dict-Link 29.59 27.67 31.01 27.13 25.14 17.96 25.51 21.88
Table 4: Comparison of the performance of baseline and the alignment generated by new aligner with
dictionary links in BLEU scores
Precision Recall AER
ORL 100.00 62.61 23.00
Ch-En F/NE 89.25 62.47 26.50
F/WE 99.59 62.47 23.22
ORL 100.00 80.98 10.51
En-Ch F/NE 93.49 80.79 13.32
F/WE 99.82 80.79 10.70
F/NE 90.79 87.49 10.89Comb F/WE 99.78 87.23 6.92
ORL 100.00 72.07 16.23
Ar-En F/NE 82.46 72.00 23.13
F/WE 94.25 72.00 18.36
ORL 100.00 90.14 5.18
En-Ar F/NE 79.81 90.06 15.37
F/WE 93.27 90.10 8.34
F/NE 78.91 93.07 14.59Comb F/WE 94.64 93.21 6.08
Table 3: Alignment quality of oracle alignment
and force alignment, the rows with ?ORL? in the
second column are oracle alignments, ?F/NE? and
?F/WE? represent force alignments with empty
links and without empty links correspondingly.
For ?F/NE? and ?F/WE? we also listed the
scores of heuristically symmetrized alignment4.
(?Comb?)
model trained from GigaWord V1 and V2 cor-
pora is used. Table 4 shows the comparison of
the performances on BLEU metric (Papineni et
al., 2002). As we can observe from the results,
the proposed method outperforms the baseline on
all test sets except MT03, and has significant6
improvement on MT02 (+0.72), MT04 (+0.93),
and Dev07NW(+0.63). The average improvement
across all test sets is 0.35 BLEU points.
As a summary, the purpose of the this experi-
ment is to demonstrate an important characteris-
tic of the proposed method. Even with imperfect
manual alignment links, we can get better align-
ment by applying our method. This characteristic
opens a possibility to integrate other more sophis-
ticated aligners.
5 Conclusion
In this study, our major contribution is a novel
generative model extended from IBM Model 4 to
6We used the confidence measurement described in
(Zhang and Vogel, 2004)
Chinese-English
Precision Recall AER
Baseline 68.22 46.88 44.43
Dict-Link 69.93 48.28 42.88
English-Chinese
Precision Recall AER
Baseline 65.35 55.05 40.24
Dict-Link 66.70 56.45 38.85
grow-diag-final-and
Precision Recall AER
Baseline 69.15 57.47 37.23
Dict-Link 70.11 59.54 35.61
Table 5: Comparison on alignment error rate by
using alignment links generated by dictionaries
utilize partial manual alignments. The proposed
method enables us to efficiently enforce subtle
alignment constraints into the EM training. We
performed experiments on manually aligned cor-
pora to prove the validity. We also demonstrated
using the method with simple heuristics to boost
the translation quality on moderate size unlabelled
corpus. The results show that our method is ef-
fective in promoting the word alignment quali-
ties with small amounts of partial alignments and
with high-precision-low-recall heuristics. Also the
method of using dictionary to generate manual
alignment links showed an average improvement
of 0.35 BLEU points across 8 test sets.
The algorithm has small impact on the speed of
GIZA++, and can easily be added to current multi-
thread implementation of GIZA++. Therefore it is
suitable for large scale training.
Future work includes applying the proposed ap-
proach on low resource language pairs and in-
tegrating the algorithm with other rule-based or
discriminative aligners that can generate high-
precision-low-recall partial alignments.
Acknowledgement
This work is supported by DARPA GALE
project and NSF CluE project.
9
References
V. Ambati, S. Vogel, and J. Carbonell. 2010. Ac-
tive semi-supervised learning for improving word
alignment. In Proceedings of the NAACL HLT 2010
Workshop on Active Learning for Natural Language
Processing.
S. Arora, E. Nyberg, and C. P. Rose?. 2009. Estimat-
ing annotation cost for active learning in a multi-
annotator environment. In HLT ?09: Proceedings of
the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 18?26.
P. Blunsom and T. Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 65?72.
P. F. Brown et. al. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation.
In Computational Linguistics, volume 19(2), pages
263?331.
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-and
sentence-aligned parallel corpora. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, pages 175?183.
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazons me-
chanical turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 286?295.
A. Fraser and D. Marcu. 2006. Semi-supervised
training for statistical word alignment. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 769?776.
A. Fraser and D. Marcu. 2007. Getting the struc-
ture right for word alignment: LEAF. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 51?60.
Q. Gao and S. Vogel. 2008. Parallel implementations
of word alignment tool. In Proceedings of the ACL
2008 Software Engineering, Testing, and Quality As-
surance Workshop, pages 49?57.
Q. Gao and S. Vogel. 2010. Consensus versus exper-
tise : A case study of word alignment with mechan-
ical turk. In NAACL 2010 Workshop on Creating
Speech and Language Data With Mechanical Turk,
pages 30?34.
F. Guzman, Q. Gao, and S. Vogel. 2009. Reassessment
of the role of phrase extraction in pbsmt. In The
twelfth Machine Translation Summit.
F. Huang. 2009. Confidence measure for word align-
ment. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 932?940. Association
for Computational Linguistics.
A. Ittycheriah and S. Roukos. 2005. A maximum en-
tropy word aligner for arabic-english machine trans-
lation. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 89?96.
P. Koehn et. al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models
for word alignment. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 459?466.
R. C Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88.
J. Niehues and S. Vogel. 2008. Discriminative word
alignment via alignment matrix modeling. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 18?25.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. In Compu-
tational Linguistics, volume 1:29, pages 19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. In Com-
putational Linguistics, volume 30, pages 417?449.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL?02, pages
311?318, Philadelphia, PA, July.
B. Taskar, S. Lacoste-Julien, and Klein D. 2005. A dis-
criminative matching approach to word alignment.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 73?80.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM
based word alignment in statistical machine trans-
lation. In Proceedings of 16th International Confer-
ence on Computational Linguistics), pages 836?841.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. In Proceedings of The 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation, October.
10
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 386?392,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU Haitian Creole-English Translation System for WMT 2011
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi Ambati, Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sanjika,nbach,qing,vamshi,vogel+}@cs.cmu.edu
Abstract
This paper describes the statistical machine
translation system submitted to the WMT11
Featured Translation Task, which involves
translating Haitian Creole SMS messages into
English. In our experiments we try to ad-
dress the issue of noise in the training data,
as well as the lack of parallel training data.
Spelling normalization is applied to reduce
out-of-vocabulary words in the corpus. Us-
ing Semantic Role Labeling rules we expand
the available training corpus. Additionally we
investigate extracting parallel sentences from
comparable data to enhance the available par-
allel data.
1 Introduction
In this paper we describe the CMU-SMT Haitian
Creole-English translation system that was built as
part of the Featured Translation Task of the WMT11.
The task involved translating text (SMS) messages
that were collected during the humanitarian opera-
tions in the aftermath of the earthquake in Haiti in
2010.
Due to the circumstances of this situation, the
SMS messages were often noisy, and contained in-
complete information. Additionally they sometimes
contained text from other languages (e.g. French).
As is typical in SMS messages, abbreviated text (as
well as misspelled words) were present. Further,
since the Haitian Creole orthography is not fully
standardized (Allen, 1998), the text inherently con-
tained several different spelling variants.
These messages were translated into English by
a group of volunteers during the disaster response.
The background and the details of this crowdsourc-
ing translation effort is discussed in Munro (2010).
Some translations contain additional annotations
which are not part of the original SMS, possibly
added by the translators to clarify certain issues with
the original message. Along with the noise, spelling
variants, and fragmented nature of the SMS mes-
sages, the annotations contribute to the overall diffi-
culty in building a machine translation system with
this type of data. We aim to address some of these
issues in out effort.
Another challenge with building a Haitian Creole-
English translation system is the lack of parallel
data. As Haitian Creole is a less commonly spo-
ken language, the available resources are limited.
Other than the manually translated SMS messages,
the available Haitian Creole-English parallel data
is about 2 million tokens, which is considerably
smaller than the parallel data available for the Stan-
dard Translation Task of the WMT11.
Lewis (2010) details the effort quickly put
forth by the Microsoft Translator team in building
a Haitian Creole-English translation system from
scratch, as part of the relief effort in Haiti. We took
a similar approach to this shared task: rapidly build-
ing a translation system to a new language pair uti-
lizing available resources. Within a short span (of
about one week), we built a baseline translation sys-
tem, identified the problems with the system, and
exploited several approaches to rectify them and im-
prove its overall performance. We addressed the is-
sues above (namely: noise in the data and sparsity of
parallel data) when building our translation system
for Haitian Creole-English task. We also normalized
386
different spelling variations to reduce the number of
out-of-vocabulary (OOV) tokens in the corpus. We
used Semantic Role Labeling to expand the available
training corpus. Additionally we exploited other re-
sources, such as comparable corpora, to extract par-
allel data to enhance the limited amount of available
parallel data.
The paper is organized as follows: Section 2
presents the baseline system used, along with a de-
scription of training and testing data used. Section 3
explains different preprocessing schemes that were
tested for SMS data, and their effect on the trans-
lation performance. Corpus expansion approach is
given in Section 4. Parallel data extraction from
comparable corpora is presented in section 5. We
present our concluding remarks in Section 6.
2 System Architecture
The WMT11 has provided a collection of Haitian
Creole-English parallel data from a variety of
sources, including data from CMU1. A summary
of the data is given in Table 1. The primary in-
domain data comprises the translated (noisy) SMS
messages. The additional data contains newswire
text, medical dialogs, the Bible, several bilingual
dictionaries, and parallel sentences from Wikipedia.
Corpus Sentences Tokens (HT/EN)
SMS messages 16,676 351K / 324K
Newswire text 13,517 336K / 292K
Medical dialog 1,619 10K / 10K
Dictionaries 42,178 97K / 92K
Other 41,872 939K / 865K
Wikipedia 8,476 77K / 90K
Total 124,338 1.81M / 1.67M
Table 1: Haitian Creole (HT) and English (EN) parallel
data provide by WMT11
We preprocessed the data by separating the punc-
tuations, and converting both sides into lower case.
SMS data was further processed to normalize quo-
tations and other punctuation marks, and to remove
all markups.
To build a baseline translation system we fol-
lowed the recommended steps: generate word align-
1www.speech.cs.cmu.edu/haitian/
ments using GIZA++ (Och and Ney, 2003) and
phrase extraction using Moses (Koehn et al, 2007).
We built a 4-gram language model with the SRI
LM toolkit (Stolcke, 2002) using English side of
the training corpus. Model parameters for the lan-
guage model, phrase table, and lexicalized reorder-
ing model were optimized via minimum error-rate
(MER) training (Och, 2003).
The SMS test sets were provided in two formats:
raw (r) and cleaned (cl), where the latter had been
manually cleaned. We used the SMS dev clean to op-
timize the decoder parameters and the SMS devtest
clean and SMS devtest raw as held-out evaluation sets.
Each set contains 900 sentences. A separate SMS
test, with 1274 sentences, was used as the unseen
test set in the final evaluation. For each experiment
we report the case-insensitive BLEU (Papineni et
al., 2002) score.
Using the available training data we built several
baseline systems: The first system (Parallel-OOD),
uses all the out-of-domain parallel data except the
Wikipedia sentences. The second system, in addi-
tion, includes Wikipedia data. The third system uses
all available parallel training data (including both the
out-of-domain data as well as in-domain SMS data).
We used the third system as the baseline for later
experiments.
dev (cl) devtest (cl) devtest (r)
Parallel-OOD 23.84 22.28 17.32
+Wikipedia 23.89 22.42 17.37
+SMS 32.28 33.49 29.95
Table 2: Translation results in BLEU for different corpora
Translation results for different test sets using the
three systems are presented in Table 2. No signifi-
cant difference in BLEU was observed with the ad-
dition of Wikipedia data. However, a significant
improvement in performance can be seen when in-
domain SMS data is added, despite the fact that this
is noisy data. Because of this, we paid special atten-
tion to clean the noisy SMS data.
3 Preprocessing of SMS Data
In this section we explain two approaches that we
explored to reduce the noise in the SMS data.
387
3.1 Lexicon-based Collapsing of OOV Words
We observed that a number of words in the raw SMS
data consisted of asterisks or special character sym-
bols. This seems to occur because either users had
to type with a phone-based keyboard or simply due
to processing errors in the pipeline. Our aim, there-
fore, was to collapse these incorrectly spelled words
to their closest vocabulary entires from the rest of
the data.
We first built a lexicon of words using the entire
data provided for the Featured Task. We then built
a second probabilistic lexicon by cross-referencing
SMS dev raw with the cleaned-up SMS dev clean.
The first resource can be treated as a dictionary
while the second is a look-up table. We processed
incoming text by first selecting all the words with
special characters in the text, and then computing
an edit distance with each of the words in the first
lexicon. We return the most frequent word that is
the closest match as a substitute. For all words that
don?t have a closest match, we looked them up in the
probabilistic dictionary and return a potential substi-
tution if it exists. As the probabilistic dictionary is
constructed using a very small amount of data, the
two-level lookup helps to place less trust in it and
use it only as a back-off option for a missing match
in the larger lexicon.
This approach only collapses words with special
characters to their closest in-vocabulary words. It
does not make a significant difference to the OOV
ratios, but reduces the number of tokens in the
dataset. Using this approach we were able to col-
lapse about 80% of the words with special characters
to existing vocabulary entries.
3.2 Spelling Normalization
One of the most problematic issues in Haitian Cre-
ole SMS translation system is misspelled words.
When training data contains misspelled words, the
translation system performance will be affected at
several levels, such as word alignment, phrase/rule
extractions, and tuning parameters (Bertoldi et al,
2010). Therefore, it is desirable to perform spelling
correction on the data. Spelling correction based
on the noisy channel model has been explored in
(Kernighan et al, 1990; Brill and Moore, 2000;
Toutanova and Moore, 2002). The model is gener-
ally presented in the following form:
p(c?|h) = argmax
?c
p(h|c)p(c) (1)
where h is the Haitian Creole word, and c is a pos-
sible correction. p(c) is a source model which is a
prior of word probabilities. p(h|c) is an error model
or noisy channel model that accounts for spelling
transformations on letter sequences.
Unfortunately, in the case of Haitian Creole SMS
we do not have sufficient data to estimate p(h|c)
and p(c). However, we can assume p(c|h) ? p(c)
and c is in the French vocabulary and is not an En-
glish word. The rationale for this, from linguistic
point of view, is that Haitian Creole developed from
the 18th century French. As a result, an important
part of the Haitian Creole lexicon is directly derived
from French. Furthermore, SMS messages some-
times were mixed with English words. Therefore,
we ignore c if it appears in an English dictionary.
Given h, how do we get a list of possible normal-
ization c and estimate p(c)? We use edit distance
of 1 between h and c. An edit can be a deletion,
transposition, substitution, or insertion. If a word
has l characters, there will be 66l+31 possible cor-
rections2. It may result in a large list. However,
we only keep possible normalizations which appear
in a French dictionary and do not appear in an En-
glish dictionary3. To approximate p(c), we use the
French parallel Giga training data from the Shared
Task of the WMT11. p(c) is estimated by MLE. Fi-
nally, our system chooses the French word with the
highest probability.
dev (cl) devtest (cl) test (cl)
Before 2.6 ; 16 2.7 ; 16 2.6 ; 16
After 2.2 ; 13.63 2.3 ; 13.95 2.2 ; 14.3
Table 3: Percentage of OOV tokens and types in test sets
before and after performing spelling normalization.
Table 3 shows that spelling normalization helps
to bring down the percentage of OOV tokens and
types by 0.4% and 2% respectively on the three test
2l deletions, l-1 transpositions, 32l substitutions, and 32(l+1)
insertions; Haitian Creole orthography has 32 forms.
3The English dictionary was created from the English Gigaword
corpus.
388
sets. Some examples of Haitian Creole words and
their French normalization are (tropikal:tropical),
(economiques:economique), (irjan:iran), (idanti-
fie:identifie).
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
S1 32.18 30.22 25.45
S2 28.9 31.06 27.69
Table 4: Translation results in BLEU with/without
spelling correction
Given the encouraging OOV reductions, we ap-
plied the spelling normalization for the full corpus,
and built new translation systems. Our baseline sys-
tem has no spelling correction (for the training cor-
pus or the test sets); in S1, the spelling corrections
is applied to all words; in S2, the spelling correc-
tion is only applied to Haitian Creole words that oc-
cur only once or twice in the data. In S1, 11.5% of
Haitian Creole words had been mapped to French,
including high frequency words. Meanwhile, 4.5%
Haitian Creole words on training data were mapped
to French words in S2. Table 4 presents a compar-
ison of translation performance of the baseline, S1
and S2 for the SMS test sets. Unfortunately, none of
systems with spelling normalization outperformed
the system trained on the original data. Restricting
the spelling correction only to infrequent words (S2)
performed better for the devtest sets, but not for the
dev set, although all the test sets come from the same
domain.
4 Corpus Expansion using Semantic Role
Labeling
To address the problem of limited resources, we
tried to expand the training corpus by applying the
corpus expansion method described in (Gao and Vo-
gel, 2011). First, we parsed and labeled the semantic
roles of the English side of the corpus, using the AS-
SERT labeler (Pradhan et al, 2004). Next, using the
word alignment models of the parallel corpus, we
extracted Semantic Role Label (SRL) substitution
rules. SRL rules consist of source and target phrases
that cover whole constituents of semantic roles, the
verb frames they belong to, and the role labels of
the constituents. The source and target phrases must
comply with the restrictions detailed in (Gao and Vo-
gel, 2011). Third, for each sentence, we replaced
one of embedded SRL substitution rules with equiv-
alent rules that have the same verb frame and the
same role label.
The original method includes an additional but
crucial step of filtering out the grammatically incor-
rect sentences using an SVM classifier, trained with
labeled samples. However, we were unable to find
Haitian Creole speakers who could manually label
training data for the filtering step. Therefore, we
were forced to skip this filtering step. We expanded
the full training corpus which contained 124K sen-
tence pairs, resulting in an expanded corpus with
505K sentences. The expanded corpus was force-
aligned using the word alignment models trained
on the original unexpanded corpus. A new trans-
lation system was built using the original plus the
expanded corpus. As seen in Table 5, we observed
a small improvement with the expanded corpus for
the raw devtest. This method did not improve per-
formance for the other two test sets.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Expanded 31.79 32.98 30.1
Table 5: Translation results in BLEU with/without corpus
expansion
A possible explanation for this, in addition to
the missing component of filtering, is the low qual-
ity of SRL parsing on the SMS corpus. We ob-
served a very small ratio of expansions in the
Haitian Creole-English data, when compared to the
Chinese-English experiment shown in (Gao and Vo-
gel, 2011). The latter used a high quality corpus for
the expansion and the expanded corpus was 20 times
larger than the original one. Due to the noisy nature
of the available parallel data, only 61K of the 124K
sentences were successfully parsed and SRL-labeled
by the labeler.
389
5 Extracting Parallel Data from
Comparable Data
As we only have a limited amount of parallel data,
we focused on automatically extracting additional
parallel data from other available resources, such as
comparable corpora. We were not able to find com-
parable news articles in Haitian Creole and English.
However, we found several hundred Haitian Creole
medical articles on the Web which were linked to
comparable English articles4. Although some of the
medical articles seemed to be direct translations of
each other, converting the original pdf formats into
text did not produce sentence aligned parallel arti-
cles. Rather, it produced sentence fragments (some-
times in different orders) due to the structural dif-
ferences in the article pair. Hence a parallel sen-
tence detection technique was necessary to process
the data. Because the SMS messages are related to
the disaster relief effort, which may include many
words in the medical domain, we believe the newly
extracted data may help improve translation perfor-
mance.
Following Munteanu and Marcu (2005), we used
a Maximum Entropy classifier to identify compara-
ble sentence. To avoid the problem of having dif-
ferent sentence orderings in the article pair, we take
every source-target sentence pair in the two articles,
and apply the classifier to detect if they are paral-
lel. The classifier approach is appealing to a low-
resource language such as Haitian Creole, because
the features for the classifier can be generated with
minimal translation resources (i.e. a translation lex-
icon).
5.1 Maximum Entropy Classifier
The classifier probability can be defined as:
Pr(ci|S, T ) =
exp
(?n
j=1 ?jfij(ci, S, T )
)
Z(S, T )
(2)
where (S, T ) is a sentence pair, ci is the class, fij
are feature functions and Z(S) is a normalizing fac-
tor. The parameters ?i are the weights for the feature
functions and are estimated by optimizing on a train-
ing data set. For the task of classifying a sentence
pair, there are two classes, c0 = non ? parallel
4Two main sources were: www.rhin.org and www.nlm.nih.gov
and c1 = parallel . A value closer to one for
Pr(c1|S, T ) indicates that (S, T ) are parallel.
The features are defined primarily based on trans-
lation lexicon probabilities. Rather than computing
word alignment between the two sentences, we use
lexical probabilities to determine alignment points
as follows: a source word s is aligned to a tar-
get word t if p(s|t) > 0.5. Target word align-
ment is computed similarly. We defined a feature set
which includes: length ratio and length difference
between source and target sentences, lexical proba-
bility scores similar to IBM model 1 (Brown et al,
1993), number of aligned/unaligned words and the
length of the longest aligned word sequence. Lexi-
cal probability score, and alignment features gener-
ate two sets of features based on translation lexica
obtained by training in both directions. Features are
normalized with respect to the sentence length.
5.2 Training and Testing the Classifier
To train the model we need training examples that
belong to each of the two classes: parallel and non-
parallel. Initially we used a subset of the available
parallel data as training examples for the classifier.
This data was primarily sourced from medical con-
versations and newswire text, whereas the compa-
rable data was found in medical articles. This mis-
match in domain resulted in poor classification per-
formance. Therefore we manually aligned a set of
250 Haitian Creole-English sentence pairs from the
medical articles and divided them in to a training set
(175 sentences) and a test set (100 sentences).
The parallel sentence pairs were directly used as
positive examples. In selecting negative examples,
we followed the same approach as in (Munteanu
and Marcu, 2005): pairing all source phrases with
all target phrases, but filter out the parallel pairs and
those that have high length difference or a low lex-
ical overlap, and then randomly select a subset of
phrase pairs as the negative training set. The test
set was generated in a similar manner. The model
parameters were estimated using the GIS algorithm.
We used the trained ME model to classify the sen-
tences in the test set into the two classes, and notice
how many instances are classified correctly.
Classification results are as given in Table 6. We
notice that even with a smaller training set, the clas-
sifier produces results with high precision. Using
390
Precision Recall F-1 Score
Training Set 93.90 77.00 84.61
Test Set 85.53 74.29 79.52
Table 6: Performance of the Classifier
the trained classifier, we processed 220 article pairs
which contained a total of 20K source sentences
and 18K target sentences. The classifier selected
about 10K sentences as parallel. From these, we se-
lected sentences where pr(c1|S, T ) > 0.7 for trans-
lation experiments. The extracted data expanded the
source vocabulary by about 5%.
We built a second translation system by combin-
ing the baseline parallel corpus and the extracted
corpus. Table 7 shows the translation results for this
system.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Extracted 32.29 33.29 29.89
Table 7: Translation results in BLEU with/without ex-
tracted data
The results indicate that there is no significant per-
formance difference in using the extracted data. This
may be due to the relatively small size of the com-
parable corpus we used when extract the data.
6 Conclusion
Building an MT system to translate Haitian Creole
SMS messages involved several challenges. There
was only a limited amount of parallel data to train
the models. The SMS messages tend to be quite
noisy. After building a baseline MT system, we
investigated several approaches to improve its per-
formance. In particular, we tried collapsing OOV
words using a lexicon generated with clean data, and
normalize different variations in spelling. However,
these methods did not results in improved translation
performance.
We tried to address the data sparseness problem
with two approaches: expanding the corpus using
SRL rules, and extracting parallel sentences from
a collection of comparable documents. Corpus ex-
pansion showed a small improvement for the raw
devtest. Both corpus expansion and parallel data
extraction did not have a positive impact on other
test sets. Both these methods have shown significant
performance improvement in the past in large data
scenarios (for Chinese-English and Arabic-English),
but failed to show improvements in the current low-
data scenario. Thus, we need further investigations
in handling noisy data, especially in low-resource
scenarios.
Acknowledgment
We thank Julianne Mentzer for assisting with editing
and proofreading the final version of the paper. We
also thank the anonymous reviewers for their valu-
able comments.
References
Jeff Allen. 1998. Lexical variation in haitian cre-
ole and orthographic issues for machine translation
(MT) and optical character recognition (OCR) appli-
cations. In Proceedings of the First Workshop on Em-
bedded Machine Translation systems of AMTA confer-
ence, Philadelphia, Pennsylvania, USA, October.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico.
2010. Statistical machine translation of texts with mis-
spelled words. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, California, June.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics (ACL 2000), pages
286?293.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Qin Gao and Stephan Vogel. 2011. Corpus expansion
for statistical machine translation with semantic role
label substitution rules. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Portland,
Oregon, USA, June.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
conference on Computational linguistics - Volume 2,
COLING ?90, pages 205?210.
391
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
June.
William Lewis. 2010. Haitian Creole: How to build and
ship an mt engine from scratch in 4 days, 17 hours, &
30 minutes. In Proceedings of the 14th Annual confer-
ence of the European Association for Machine Trans-
lation (EAMT), Saint-Raphae?l, France, May.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collab-
orative Crowdsourcing for Translation, Denver, Col-
orado, USA, October-November.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL-2004).
Andreas Stolcke. 2002. An extensible language model-
ing toolkit. In Proc. of International Conference on
Spoken Language Processing, volume 2, pages 901?
904, Denver, CO, September.
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2002).
392
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 145?151,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The SDL Language Weaver Systems in the WMT12 Quality Estimation
Shared Task
Radu Soricut Nguyen Bach Ziyuan Wang
SDL Language Weaver
6060 Center Drive, Suite 101
Los Angeles, CA, USA
{rsoricut,nbach,zwang}@sdl.com
Abstract
We present in this paper the system sub-
missions of the SDL Language Weaver
team in the WMT 2012 Quality Estimation
shared-task. Our MT quality-prediction sys-
tems use machine learning techniques (M5P
regression-tree and SVM-regression models)
and a feature-selection algorithm that has been
designed to directly optimize towards the of-
ficial metrics used in this shared-task. The
resulting submissions placed 1st (the M5P
model) and 2nd (the SVM model), respec-
tively, on both the Ranking task and the Scor-
ing task, out of 11 participating teams.
1 Introduction
The WMT 2012 Quality Estimation shared-task fo-
cused on automatic methods for estimating machine
translation output quality at run-time (sentence-level
estimation). Different from MT evaluation met-
rics, quality prediction (QP) systems do not rely
on reference translations and are generally built us-
ing machine learning techniques to estimate quality
scores (Specia et al, 2009; Soricut and Echihabi,
2010; Bach et al, 2011; Specia, 2011).
Some interesting uses of sentence-level MT qual-
ity prediction are the following: decide whether a
given translation is good enough for publishing as-
is (Soricut and Echihabi, 2010), or inform monolin-
gual (target-language) readers whether or not they
can rely on a translation; filter out sentences that
are not good enough for post-editing by professional
translators (Specia, 2011); select the best translation
among options from multiple MT systems (Soricut
and Narsale, 2012), etc.
This shared-task focused on estimating the qual-
ity of English to Spanish automatic translations. The
training set distributed for the shared task comprised
of 1, 832 English sentences taken from the news do-
main and their Spanish translations. The translations
were produced by the Moses SMT system (Koehn et
al., 2007) trained on Europarl data. Translations also
had a quality score derived from an average of three
human judgements of Post-Editing effort using a 1-
5 scale (1 for worse-quality/most-effort, and 5 for
best-quality/least-effort). Submissions were evalu-
ated using a blind official test set of 422 sentences
produced in the same fashion as the training set.
Two sub-tasks were considered: (i) scoring transla-
tions using the 1-5 quality scores (Scoring), and (ii)
ranking translations from best to worse (Ranking).
The official metrics used for the Ranking task were
DeltaAvg (measuring how valuable a proposed rank-
ing is from the perspective of extrinsic values asso-
ciated with the test entries, in this case post-editing
effort on a 1-5 scale; for instance, a DeltaAvg of 0.5
means that the top-ranked quantiles have +0.5 bet-
ter quality on average compared to the entire set), as
well as the Spearman ranking correlation. For the
Scoring task the metrics were Mean-Absolute-Error
(MAE) and Root Mean Squared Error (RMSE). The
interested reader is referred to (Callison-Burch et al,
2012) for detailed descriptions of both the data and
the evaluation metrics used in the shared-task.
The SDL Language Weaver team participated
with two submissions based on M5P and SVM re-
gression models in both the Ranking and the Scoring
145
tasks. The models were trained and used to predict
Post-Editing?effort scores. These scores were used
as-such for the Scoring task, and also used to gener-
ate sentence rankings for the Ranking task by simply
(reverse) sorting the predicted scores. The submis-
sions of the SDL Language Weaver team placed 1st
(the M5P model) and 2nd (the SVM model) on both
the Ranking task (out of 17 entries) and the Scoring
task (out of 19 entries).
2 The Feature Set
Both SDLLW system submissions were created
starting from 3 distinct sets of features: the baseline
feature set (here called BFs), the internal features
available in the decoder logs of Moses (here called
MFs), and an additional set of features that we de-
veloped internally (called LFs). We are presenting
each of these sets in what follows.
2.1 The Baseline Features
The WMT Quality Estimation shared-task defined
a set of 17 features to be used as ?baseline? fea-
tures. In addition to that, all participants had access
to software that extracted the corresponding feature
values from the inputs and necessary resources (such
as the SMT-system?s training data, henceforth called
SMTsrc and SMTtrg). For completeness, we are
providing here a brief description of these 17 base-
line features (BFs):
BF1 number of tokens in the source sentence
BF2 number of tokens in the target sentence
BF3 average source token length
BF4 LM probability of source sentence
BF5 LM probability of the target sentence
BF6 average number of occurrences of the target
word within the target translation
BF7 average number of translations per source word
in the sentence (as given by IBM 1 table thresh-
olded so that Prob(t|s) > 0.2)
BF8 average number of translations per source word
in the sentence (as given by IBM 1 table thresh-
olded so that Prob(t|s) > 0.01) weighted
by the inverse frequency of each word in the
source corpus
BF9 percentage of unigrams in quartile 1 of fre-
quency (lower frequency words) in SMTsrc
BF10 percentage of unigrams in quartile 4 of fre-
quency (higher frequency words) in SMTsrc
BF11 percentage of bigrams in quartile 1 of fre-
quency of source words in SMTsrc
BF12 percentage of bigrams in quartile 4 of fre-
quency of source words in SMTsrc
BF13 percentage of trigrams in quartile 1 of fre-
quency of source words in SMTsrc
BF14 percentage of trigrams in quartile 4 of fre-
quency of source words in SMTsrc
BF15 percentage of unigrams in the source sentence
seen in SMTsrc
BF16 number of punctuation marks in source sen-
tence
BF17 number of punctuation marks in target sentence
These features, together with the other ones we
present here, are entered into a feature-selection
component that decides which feature set to use for
optimum performance (Section 3.2).
In Table 1, we are presenting the performance
on the official test set of M5P and SVM-regression
(SVR) models using only the BF features. The
M5P model is trained using the Weka package 1
and the default settings for M5P decision-trees
(weka.classifiers.trees.M5P). The SVR model is
trained using the LIBSVM toolkit 2. The follow-
ing options are used: ?-s 3? (-SVR) and ?-t 2? (ra-
dial basis function). The following parameters were
optimized via 5-fold cross-validation on the train-
ing data: ?-c cost?, the parameter C of -SVR; ?-g
gamma?, the ? parameter of the kernel function; ?-p
epsilon?, the  for the loss-function of -SVR.
1http://www.cs.waikato.ac.nz/ml/weka/
2http://www.csie.ntu.edu.tw/?cjlin/libsvm/
146
Systems Ranking Scoring
DeltaAvg Spearman MAE RMSE Predict. Interval
17 BFs with M5P 0.53 0.56 0.69 0.83 [2.3-4.9]
17 BFs with SVR 0.55 0.58 0.69 0.82 [2.0-5.0]
best-system 0.63 0.64 0.61 0.75 [1.7-5.0]
Table 1: Performance of the Baseline Features using M5P and SVR models on the test set.
The results in Table 1 are compared against the
?best-system? submission, in order to offer a com-
parison point. The ?17 BFs with SVM? system ac-
tually participated as an entry in the shared-task, rep-
resenting the current state-of-the-art in MT quality-
prediction. This system has been ranked 6th (out of
17 entries) in the Ranking task, and 8th (out of 19
entries) in the Scoring task.
2.2 The Decoder Features
The current Quality Estimation task has been de-
fined as a glass-box task. That is, the prediction
component has access to everything related to the
internal workings of the MT system for which the
quality prediction is made. As such, we have cho-
sen to use the internal scores of the Moses 3 decoder
(available to all the participants in the shared-task)
as a distinct set of features. These features are the
following:
MF1 Distortion cost
MF2 Word penalty cost
MF3 Language-model cost
MF4 Cost of the phrase-probability of source given
target ?(s|t)
MF5 Cost of the word-probability of source given
target ?lex(s|t)
MF6 Cost of the phrase-probability of target given
source ?(t|s)
MF7 Cost of the word-probability of target given
source ?lex(t|s)
MF8 Phrase penalty cost
3http://www.statmt.org/moses/
These features are then entered into a feature-
selection component that decides which feature set
to use for achieving optimal performance.
The results in Table 2 present the performance
on the test set of the Moses features (with an M5P
model), presented against the ?best-system? sub-
mission. These numbers indicate that the Moses-
internal features, by themselves, are fueling a QP
system that surpasses the performance of the strong
?baseline? system. We note here that the ?8 MFs
with M5P? system would have been ranked 4th (out
of 17 entries) in the Ranking task, and 5th (out of 19
entries) in the Scoring task.
2.3 Language Weaver Features
In addition to the features presented until this point,
we have created and tested additional features that
helped our systems achieve improved performance.
In addition to the SMT training corpus, these fea-
tures also use the SMT tuning dev set (henceforth
called Devsrc and Devtrg). These features are the
following:
LF1 number of out-of-vocabulary tokens in the
source sentence
LF2 LM perplexity for the source sentence
LF3 LM perplexity for the target sentence
LF4 geometric mean (?-smoothed) of 1-to-4?gram
precision scores (i.e., BLEU score without
brevity-penalty) of source sentence against the
sentences of SMTsrc used as ?references?
LF5 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against the
sentences of SMTtrg used as ?references?
147
Systems Ranking Scoring
DeltaAvg Spearman-Corr MAE RMSE Predict. Interval
8 MFs with M5P 0.58 0.58 0.65 0.81 [1.8-5.0]
best-system 0.63 0.64 0.61 0.75 [1.7-5.0]
Table 2: Performance of the Moses-based Features with an M5P model on the test set.
LF6 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of source sentence against the
top BLEU-scoring quartile of Devsrc
LF7 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against the
top BLEU-scoring quartile of Devtrg
LF8 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of source sentence against the
bottom BLEU-scoring quartile of Devsrc
LF9 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against the
bottom BLEU-scoring quartile Devtrg
LF10 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against a
pseudo-reference produced by a second MT
Eng-Spa system
LF11 count of one-to-one (O2O) word alignments
between source and target translation
LF12 ratio of O2O alignments over source sentence
LF13 ratio of O2O alignments over target translation
LF14 count of O2O alignments with Part-of-Speech?
agreement
LF15 ratio of O2O alignments with Part-of-Speech?
agreement over O2O alignments
LF16 ratio of O2O alignments with Part-of-Speech?
agreement over source
LF17 ratio of O2O alignments with Part-of-Speech?
agreement over target
Most of these features have been shown to help
Quality Prediction performance, see (Soricut and
Echihabi, 2010) and (Bach et al, 2011). Some of
them are inspired from word-based confidence esti-
mation, in which the alignment consensus between
the source words and target-translation words are
informative indicators for gauging the quality of a
translation hypothesis. The one-to-one (O2O) word
alignments are obtained from the decoding logs of
Moses. We use the TreeTagger to obtain Spanish
POS tags4 and a maximum-entropy POS tagger for
English. Since Spanish and English POS tag sets
are different, we normalize their fine-grained POS
tag sets into a coarser tag set by mapping the orig-
inal POS tags into more general linguistic concepts
such as noun, verb, adjective, adverb, preposition,
determiner, number, and punctuation.
3 The Models
3.1 The M5P Prediction Model
Regression-trees built using the M5P algo-
rithm (Wang and Witten, 1997) have been previ-
ously shown to give good QP performance (Soricut
and Echihabi, 2010). For these models, the num-
ber of linear equations used can provide a good
indication whether the model overfits the training
data. In Table 3, we compare the performance of
several M5P models: one trained on all 42 features
presented in Section 2, and two others trained on
only 15 and 14 features, respectively (selected using
the method described in Section 3.2). We also
present the number of linear equations (L.Eq.) used
by each model. Aside from the number of features
they employ, these models were trained under
identical conditions: default parameters of the Weka
implementation, and 1527 training instances (305
instances were held-out for the feature-selection
step, from the total 1832 labeled instances available
for the shared-task).
As the numbers in Table 3 clearly show, the set of
4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
148
Systems #L.Eq. Dev Set Test Set
DeltaAvg MAE DeltaAvg MAE
42 FFs with M5P 10 0.60 0.58 0.56 0.64
(best-system) 15 FFs with M5P 2 0.63 0.52 0.63 0.61
14 FFs with M5P 6 0.62 0.50 0.61 0.62
Table 3: M5P-model performance for different feature-function sets (15-FFs ? 42-FFs; 14-FFs ? 42-FFs).
feature-functions that an M5P model is trained with
matters considerably. On both our development set
and the official test set, the 15-FF M5P model out-
performs the 42-FF model (even if 15-FF ? 42-FF).
The 42-FF model would have been ranked 5th (out
of 17 entries) in the Ranking task, and also 5th (out
of 19 entries) in the Scoring task. In comparison, the
15-FF model (feature set optimized for best perfor-
mance under the DeltaAvg metric) was our official
M5P submission (SDLLW M5PBestDeltaAvg), and
ranked 1st in the Ranking task and also 1st in the
Scoring task. The 14-FF model (also a subset of the
42-FF set, optimized for best performance under the
MAE metric) was not part of our submission, but
would have been ranked 2nd on both the Ranking
and Scoring tasks.
The number of linear equations used (see #L.Eq.
in Table 3) is indicative for our results. When using
42 FFs, the M5P model seems to overfit the train-
ing data (10 linear equations). In contrast, the model
trained on a subset of 15 features has only 2 linear
equations. This latter model is less prone to overfit-
ting, and performs well given unseen test data. The
same number for the 14-FF model indicates slight
overfit on the training and dev data: with 6 equa-
tions, this model has the best MAE numbers on the
Dev set, but slightly worse MAE numbers on the
Test score compared to the 15-FF model.
3.2 Feature Selection
As we already pointed out, some of the features of
the entire 42-FF set are highly overlapping and cap-
ture roughly the same amount of information. To
achieve maximum performance given this feature-
set, we applied a computationally-intensive feature-
selection method. We have used the two official
metrics, DeltaAvg and MAE, and a development set
of 305 instances to perform an extensive feature-
selection procedure that directly optimizes the two
official metrics using M5P regression-trees.
The overall space that needs to be explored for 42
features is huge, on the order of 242 possible com-
binations. We performed the search in this space in
several steps. In a first step, we eliminated the obvi-
ously overlapping features (e.g., BF5 and MF3 are
both LM costs of the target translation), and also
excluded the POS-based features (LF14-LF17, see
Section 2.3). This step reduced the overall num-
ber of features to 24, and therefore left us with an
order of 224 possible combinations. Next, we ex-
haustively searched all these combinations by build-
ing and evaluating M5P models. This operation
is computationally-intensive and takes approxima-
tively 60 hours on a cluster of 800 machines. At
the conclusion of this step, we ranked the results
and considered the top 64 combinations. The perfor-
mance of these top combinations was very similar,
and a set of 15 features was selected as the superset
of active feature-functions present in most of the top
64 combinations.
DeltaAvg optim. BF1 BF3 BF4 BF6 BF12
BF13 BF14 MF3 MF4 MF6
LF1 LF10 LF14 LF15 LF16
MAE optim. BF1 BF3 BF4 BF6 BF12
BF14 BF16 MF3 MF4 MF6
LF1 LF10 LF14 LF17
Table 4: Feature selection results.
The second round of feature selection consid-
ers these 15 feature-functions plus the 4 POS-based
feature-functions, for a total of 19 features and there-
fore a space of 219 possible combinations (215 of
these already covered by the first search pass). A
second search procedure was executed exhaustively
149
Dev Set Test Set
SVR Model (C;?;) #S.V. DeltaAvg MAE DeltaAvg MAE
1.0 ; 0.00781; 0.50 695 0.62 0.52 0.60 0.66
1.74; 0.00258; 0.3299 952 0.63 0.51 0.61 0.64
8.0 ; 0.00195; 0.0078 1509 0.64 0.50 0.60 0.68
16.0; 0.00138; 0.0884 1359 0.63 0.51 0.59 0.70
Table 5: SVR-model performance for dev and test sets.
over the set of all the new possible combinations.
In the end, we selected the winning feature-function
combination as our final feature-function sets: 15
features for DeltaAvg optimization and 14 features
for MAE optimization. They are given in Table 4,
using the feature id-s given in Section 2. The perfor-
mance of these two feature-function sets using M5P
models can be found in Table 3.
3.3 The SVM Prediction Model
The second submission of our team consists of rank-
ings and scores produced by a system using an -
SVM regression model (-SVR) and a subset of 19
features. This model is trained on 1,527 training
examples by the LIBSVM package using radial ba-
sis function (RBF) kernel. We have found that the
feature-set obtained by the feature-selection opti-
mization for M5P models described in Section 3.2
does not achieve the same performance for SVR
models on our development set. Therefore, we
have performed our SVR experiments using a hand-
selected set of features: 9 features from the BF fam-
ily (BF1 BF3 BF4 BF6 BF10 BF11 BF12 BF14
BF16); all 8 features from the MF family; and 2 fea-
tures from the LF family (LF1 LF10).
We optimize the three hyper parameters C, ?, and
 of the SVR method using a grid-search method and
measure their performance on our development set
of 305 instances. The C parameter is a penalty fac-
tor: if C is too high, we have a high penalty for non-
separable points and may store many support vec-
tors and therefore overfit the training data; if C is
too low, we may end up with a model that is poorly
fit. The  parameter determines the level of accuracy
of the approximated function; however, getting too
close to zero may again overfit the training data. The
? parameter relates to the RBF kernel: large ? val-
ues give the model steeper and more flexible kernel
functions, while small gamma values give the model
smoother functions. In general, C, , and ? are all
sensitive parameters and instantiate -SVR models
that may behave very differently.
In order to cope with the overfitting issue given
a small amount of training data and grid search op-
timization, we train our models with 10-fold cross
validation and restart the tuning process several
times using different starting points and step sizes.
We select the best model parameters based on a cou-
ple of indicators: the performance on the develop-
ment set and the number of support vectors of the
model. In Table5 we present the performance of dif-
ferent model parameters on both the development
set and the official test set. Our second submis-
sion (SDLLW SVM), which placed 2nd in both the
Ranking and the Scoring tasks, is the entry in bold
font. It was chosen based on good performance on
the Dev set and also a setting of the (C, ?, ) pa-
rameters that provides a number of support vectors
that is neither too high nor too low. As a contrastive
point, the model on the row below it uses 1,509 sup-
port vectors extracted from 1,527 training vectors,
which represents a clear case of overfitting. Indeed,
the performance of this model is marginally better
on the Dev set, but ends up underperforming on the
Test data.
4 Conclusions
The WMT 2012 Quality Estimation shared-task pro-
vided the opportunity for the comparing different
QP systems using shared datasets and standardized
evaluation metrics. Our participation in this shared-
task revealed two important aspects of Quality Pre-
diction for MT that we regard as important for the
future. First, our experiments indicated that the
150
Moses-internal features, by themselves, can fuel a
QP-system that surpasses the performance of the
strong ?baseline? system used in this shared task to
represent state-of-the-art performance in MT qual-
ity prediction. This is a surprising finding, consid-
ering that these decoder-internal features have been
primarily designed to gauge differences in transla-
tion quality when starting from the same source sen-
tence. In contrast, for quality-prediction tasks like
ranking one needs to gauge differences in quality of
translations of different source sentences.
The second aspect relates to the importance of
feature selection. Given the availability and good
scalability of Machine Learning toolkits today, it
is tempting to throw as much features as possible
at this problem and let the built-in mechanisms of
these learning algorithms deal with issues relating
to feature overlapping, training-data overfitting, etc.
However, these learning algorithms have their own
limitations in these regards, and, in conjunction with
the limited availability of the labeled data, can easily
produce models that are underperforming on blind
tests. There is a need for careful engineering of
the models and evaluation of the resulting perfor-
mance in order to achieve optimal performance us-
ing the current state-of-the-art supervised learning
techniques.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: A method for measuring machine transla-
tion confidence. In Proceedings of the ACL/HLT, Port-
land, Oregon, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations via
ranking. In Proceedings of ACL.
Radu Soricut and Sushant Narsale. 2012. Combining
quality prediction and system selection for improved
automatic translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Mar-
cho Turchi, and Nello Cristianini. 2009. Estimating
the sentence-level quality of machine translation. In
Proceedings of EAMT.
Lucia Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proceed-
ings of EAMT.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Proceedings of
the 9th European Conference on Machine Learning.
151
