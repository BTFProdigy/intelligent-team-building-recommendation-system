Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Automatic Question Generation using Discourse Cues
Manish Agarwal?, Rakshit Shah? and Prashanth Mannem
Language Technologies Research Center
International Institute of Information Technology
Hyderabad, AP, India - 500032
{manish.agarwal, rakshit.shah, prashanth}@research.iiit.ac.in
Abstract
In this paper, we present a system that au-
tomatically generates questions from natural
language text using discourse connectives. We
explore the usefulness of the discourse con-
nectives for Question Generation (QG) that
looks at the problem beyond sentence level.
Our work divides the QG task into content se-
lection and question formation. Content se-
lection consists of finding the relevant part in
text to frame question from while question for-
mation involves sense disambiguation of the
discourse connectives, identification of ques-
tion type and applying syntactic transforma-
tions on the content. The system is evaluated
manually for syntactic and semantic correct-
ness.
1 Introduction
Automatic QG from sentences and paragraphs has
caught the attention of the NLP community in the
last few years through the question generation work-
shops and the shared task in 2010 (QGSTEC, 2010).
Previous work in this area has concentrated on gen-
erating questions from individual sentences (Varga
and Ha, 2010; Paland et al, 2010; Ali et al, 2010).
Sneiders and E. (2002) used question templates and
Heilman et al (2009) used general-purpose rules
to transform sentences into questions. A notable
exception is Mannem et al (2010) who generated
questions of various scopes (general, medium and
specific) ? 1 from paragraphs instead of individual
?First two authors contributed equally to this work
1General scope - entire or almost entire paragraph, Medium
scope - multiple clauses or sentences, and Specific scope - sen-
sentences. They boil down the QG from paragraphs
task into first identifying the sentences in the para-
graph with general, medium and specific scopes and
then generating the corresponding questions from
these sentences using semantic roles of predicates.
Discourse connectives play a vital role in mak-
ing the text coherent. They connect two clauses
or sentences exhibiting discourse relations such as
temporal, causal, elaboration, contrast, result,
etc. Discourse relations have been shown to be use-
ful to generate questions (Prasad and Joshi, 2008)
but identifying these relations in the text is a difficult
task (Pitler et al, 2009). So in this work, instead of
identifying discourse relations and generating ques-
tions using them, we explore the usefulness of dis-
course connectives for QG. We do this by analyzing
the senses of the connectives that help in QG and
propose a system that makes use of this analysis to
generate questions of the type why, when, give an
example and yes/no.
The two main problems in QG are identifying the
content to ask a question on and finding the corre-
sponding question type for that content. We ana-
lyze the connectives in terms of the content useful
for question generation based on the senses they ex-
hibit. We show that the senses of the connectives
further help in choosing the relevant question type
for the content.
In this paper, we present an end-to-end QG sys-
tem that takes a document as input and outputs all
the questions generated using the selected discourse
connectives. The system has been evaluated man-
ually by two evaluators for syntactic and semantic
tence or less
1
correctness of the generated questions. The over-
all system has been rated 6.3 out of 8 for QGSTEC
development dataset and 5.8 out of 8 for Wikipedia
dataset.
2 Overview
Question Generation involves two tasks, content
selection (the text selected for question generation)
and question formation (transformations on the con-
tent to get the question). Question formation further
has the subtasks of (i) finding suitable question type
(wh-word), (ii) auxiliary and main verb transforma-
tions and (iii) rearranging the phrases to get the final
question.
There are 100 distinct types of discourse connec-
tives listed in PDTB manual (PDTB, 2007). The
most frequent connectives in PDTB are and, or,
but, when, because, since, also, although, for
example, however and as a result. In this paper,
we provide analysis for four subordinating conjunc-
tions, since, when, because and although, and three
adverbials, for example, for instance and as a re-
sult. Connectives such as and, or and also show-
ing conjunction relation have not been found to
be good candidates for generating wh-type ques-
tions and hence have not been discussed in the pa-
per. Leaving aside and, or and also, the selected
connectives cover 52.05 per cent of the total number
of the connectives in QGSTEC-2010 2 dataset and
41.97 per cent in Wikipedia articles. Connective-
wise coverage in both the datasets is shown in Table
1. Though but and however denoting contrast re-
lation occur frequently in the data, it has not been
feasible to generate wh-questions using them.
QGSTEC-2010 Dev. Data Wikipedia Dataset
Connective count % count %
because 20 16.53 36 10.28
since 9 7.44 18 5.14
when 23 19.00 35 10.00
although 4 3.30 22 6.28
as a result 5 4.13 6 1.71
for example 2 1.65 30 8.28
for instance 0 0.00 1 0.28
Total 121 52.05 350 41.97
Table 1: Coverage of the selected discourse connec-
tives in the data
The system goes through the entire document and
2QGSTEC 2010 data set involves Wikipedia, Yahoo An-
swers and OpenLearn articles.
identifies the sentences containing at least one of the
seven discourse connectives. In our approach, suit-
able content for each discourse connective which is
referred to as target argument is decided based on
the properties of discourse connective. The system
finds the question type on the basis of discourse re-
lation shown by discourse connective.
3 Discourse connectives for QG
In this section, we provide an analysis of dis-
course connectives with respect to their target argu-
ments and the question types they take.
3.1 Question type identification
The sense of the discourse connective influences
the question-type (Q-type). Since few discourse
connectives such as when, since and although
among the selected ones can show multiple senses,
the task of sense disambiguation of the connectives
is essential for finding the question type.
Since: The connective can show temporal, causal
or temporal + causal relation in a sentence. Sen-
tence exhibits temporal relation in presence of key-
words like time(7 am), year (1989 or 1980s), start,
begin, end, date(9/11), month (January) etc. If the
relation is temporal then the question-type is when
whereas in case of causal relation it would be why.
1. Single wicket has rarely been played since lim-
ited overs cricket began.
Q-type: when
2. Half-court games require less cardiovascular
stamina , since players need not run back and
forth a full court.
Q-type: why
In examples 1 and 2, 1 is identified to show tem-
poral relation because it has the keyword began
whereas there is no keyword in the context of ex-
ample 2 that gives the hint of temporal relation and
so the relation here is identified as causal.
When: Consider the sentences with connec-
tive when in Figure 1. Although when shows
multiple senses (temporal, temporal+causal and
conditional), we can frame questions by a single
question type, when. Given a new instance of
the connective, finding the correct sense of when
2
Sentence:  The San?Francisco earthquake hit when resources in the field already were stetched. (Temporal)
Sentence:  Venice?s long decline started in the 15th century, when it first made an unsuccessful attempt to hold Thessalonica
Sentence:   Earthquake mainly occurs when the different blocks or plates that make up the Earth?s surface move relative to
Question:   When do earthquake mainly occur ?     
                    each other, causing distortion in the rock. ( Conditional ) 
Question:   When did San?Francisco earthquake hit ? 
against the Ottomans (1423?1430). ( Temporal + Causal ) 
Question:  When did Venice?s long decline start in the 15th century ?
Figure 1: Questions for discourse connective when
Discourse Sense Q-type
connectives
because causal why
since temporal when
causal why
when
causal + temporal
whentemporal
conditional
although contrast yes/ no
concession
as a result result why
for example instantiation give an example
where
for instance instantiation give an instance
where
Table 2: Question type for discourse connectives
becomes unnecessary as a result of using discourse
connectives.
Although: The connective can show concession
or contrast discourse relations. It is difficult to frame
a wh-question on contrast or concession relations.
So, system generates a yes/no type question for al-
though. Moreover, yes/no question-type adds to the
variety of questions generated by the system.
3. Greek colonies were not politically controlled
by their founding cities , although they often
retained religious and commercial links with
them .
Q-type: Yes/No
A yes/no question could have been asked for
connectives but and however denoting a contrast re-
lation but it was not done to preserve the question-
type variety in the final output of the QG system.
Y es/no questions have been asked for occurrences
of although since they occur less frequently than
but and however.
Identifying the question types for other selected
discourse connectives is straight forward because
they broadly show only one discourse relation
(Pitler and Nenkova, 2009). Based on the relations
exhibited by these connectives, Table 2 shows the
question types for each discourse connective.
3.2 Target arguments for discourse connectives
A discourse connective can realize its two argu-
ments, Arg1 and Arg2, structurally and anaphori-
cally. Arg2 is always realized structurally whereas
Arg1 can be either structural or anaphoric (PDTB,
2007; Prasad et al, 2010).
4. [Arg1 Organisms inherit the characteristics of
their parents] because [Arg2 the cells of the
offspring contain copies of the genes in their
parents? cells.](Intra-sentential connective be-
cause)
5. [Arg1 The scorers are directed by the hand sig-
nals of an umpire.] For example, [Arg2 the
umpire raises a forefinger to signal that the
batsman is out (has been dismissed); he raises
both arms above his head if the batsman has
hit the ball for six runs.](Inter-sentential con-
nective for example)
Consider examples 4 and 5. In 4, Arg1 and Arg2
are the structural arguments of the connective be-
cause whereas in 5, Arg2 is the structural argument
and Arg1 is realized anaphorically.
The task of content selection involves finding the
target argument (either Arg1 or Arg2) of the dis-
course connective. Since both the arguments are po-
tential candidates for QG, we analyze the data to
identify which argument makes better content for
each of the connectives. Our system selects one of
the two arguments based on the properties of the dis-
course connectives. Table 3 shows the target argu-
3
Discourse connective Target argument
because Arg1
since Arg1
when Arg1
although Arg1
as a result Arg2
for example Arg1
for instance Arg1
Table 3: Target argument for discourse connectives
ment i.e. either Arg1 or Arg2, which is used as con-
tent for QG.
4 Target Argument Identification
Target argument for a discourse connective can
be a clause(s) or a sentence(s). It could be one or
more sentences in case of inter-sentential3 discourse
connectives, whereas one or more clauses in case of
intra-sentential4 connectives.
Discourse connectives for example and for in-
stance can realize its Arg1 anywhere in the prior dis-
course (Elwell and Baldridge, 2008). So the system
considers only those sentences in which the connec-
tives occur at the beginning of the sentence and the
immediate previous sentence is assumed to be the
Arg1 of the connective (which is the target argument
for QG).
In case of intra-sentential connectives (because,
since, although and when) and as a result (target ar-
gument is Arg2 which would be a clause), identifi-
cation of target argument is done in two steps. The
system first locates the syntactic head or head verb
of the target argument and then extracts it from the
dependency tree of the sentence.
4.1 Locate syntactic head
Approach for locating the syntactic head of tar-
get argument is explained with the help of Figure 2
(generic dependency trees) and an example shown
in Figure 3. Syntactic head of Arg2 is the first fi-
nite verb while percolating up in the dependency tree
starting from the discourse connective. In case of
intra-sentential connectives where Arg1 is the target
argument, the system percolates up until it gets the
second finite verb which is assumed to be target head
3Connectives that realize its Arg1 anaphorically and Arg2
structurally
4Connectives that realize both of its arguments structurally
X          P         Z                   
DC        A
(a)                                            (b)
1   1
   
V                                           V 
2  X          V           Z                 
V2
DC         A                                     Q  
Figure 2: Head selection of the target argument
for intra-sentential connectives (V1,V2: finite verbs;
X,Z: subtrees of V1; A: subtree of V2; P,Q:Not verbs;
DC:discourse connective(child of V2))
of Arg1. Number of percolations entirely depend on
structure and complexity of the sentence. Figure 2
shows two dependency trees (a) and (b). Starting
from the discourse connective DC and percolating
up, the system identifies that the head of Arg2 is V2
and that of Arg1 is V1.
  
 
aux : "is"
played
competitive
badminton
is    indoors     
by     because   flight    is
   wind                shuttlecock
Why is competitive badminton played indoors ?
affected
Because 
(From section 2.1)                      (From section 2.2)
qtype : "Why"                        Target Arg Head : "played"
(section 2.3)
              [Arg2 shuttlecock flight is affected by wind],
[Arg1 competitive badminton is played indoors].(content)
Figure 3: Question Generation process
Since the discourse connective in the example of
Figure 3 is because, the target argument is Arg1
(from Table 2). By percolating up the tree starting
from because, the head of Arg2 is affected and that
of Arg1 is played. Once we locate the head of the
target argument, we find the auxiliary as Mannem
et al (2010) does. For the example in Figure 3, the
auxiliary for question generation is is.
4.2 Target Argument Extraction
The extraction of the target argument is done af-
ter identifying its syntactic head. For as a result,
the target argument, Arg2, is the subtree with head
4
Score Description Example
4 The question is grammatically correct and idiomatic/natural. In which type of animals are phagocytes highly developed?
3 The question is grammatically correct but does not read as In which type of animals are phagocytes, which are importantfluently as we would like. throughout the animal kingdom, highly developed?
2 There are some grammatical errors in the question. In which type of animals is phagocytes, which are important
throughout the animal kingdom, highly developed?
1 The question is grammatically unacceptable. On which type of animals is phagocytes, which are important
throughout the animal kingdom, developed?
Table 4: Evaluation guidelines for syntactic correctness measure
as the head of the connective. For intra-sentential
connectives, the target argument, Arg1, is the tree
remaining after removing the subtree that contains
Arg2.
In Figures 2 (a) and (b) both, a tree with head
V1 and its children, X and Z, is left after removing
Arg2 from dependency trees, which is the content
required for generating the question. Note that in the
tree of Figure 2(b), the child P of the head verb V1 is
removed with its entire subtree that contains Arg2.
Thus, subtree with head V2 is the unwanted part for
the tree in Figure 2(a) whereas subtree with head P
is the unwanted part for the tree in Figure 2(b) when
the target argument is Arg1.
In Figure 3, after removing the unwanted argu-
ment Arg2 (subtree with head affected), the system
gets competitive badminton is played indoors which
is the required clause (content) for question genera-
tion. The next section describes how the content is
transformed into a question.
5 Syntactic Transformations and Question
Generation
The syntactic transformations used in this work
are similar to those by Mannem et al (2010). At this
stage, the system has the question type, auxiliary and
the content. The following set of transformations
are applied on the content to get the final question.
(1) If the auxiliary is present in the sentence itself
then it is moved to the beginning of the sentence;
otherwise auxiliary is added at the beginning of the
sentence. (2) If a wh-question is to be formed, the
question word is added just before the auxiliary. In
case of Yes/No questions, the question starts with
the auxiliary itself as no question word is needed. (3)
A question-mark(?) is added at the end to complete
the question.
Consider the example in Figure 3. Here the con-
tent is competitive badminton is played indoors.
Applying the transformations, the auxiliary is first
moved at the start of the sentence to get is compet-
itive badminton played indoors. Then the question
type Why is added just before the auxiliary is, and
a question-mark is added at the end to get the final
question, Why is competitive badminton played in-
doors ?
Scope: In QGSTEC 2010 the question had to be
assigned a scope, specific, medium or general. The
scope is defined as: general - entire input paragraph,
medium - one or more clauses or sentences and spe-
cific - phrase or less. Questions generated using dis-
course connectives are usually of the scope specific
or medium. Mannem et al (2010) assigned medium
scope to the questions generated using the seman-
tic roles such as ARGM-DIS (result), ARGM-CAU
(causal) and ARGM-PNC (purpose) given by the
SRL. However, most of the times, the scope of the
answer to these questions is just a clause or a sen-
tence and should have been assigned specific scope
instead of medium.
6 Evaluation and Results
Automatic evaluation of any natural language
generated text is difficult. So, our system is eval-
uated manually. The evaluation was performed
by two graduate students with good English profi-
ciency. Evaluators were asked to rate the questions
on the scale of 1 to 4 (4 being the best score) on syn-
tactic and semantic correctness (Evalguide, 2010)
of the question and an overall rating on the scale of
8 (4+4) is assigned to each question.
The syntactic correctness is rated to ensure that
the system can generate grammatical output. In ad-
dition, those questions which read fluently are given
greater score. The syntactic correctness and fluency
is evaluated using the following scores: 4 - gram-
5
Discourse ExampleConnective
because
One-handed backhand players move to the net with greater ease than two-handed players
because the shot permits greater forward momentum and has greater similarities in muscle
memory to the preferred type of backhand volley (one-handed, for greater reach ).
Why do one-handed backhand players move to the net with greater ease than two-handed
players ? (Causal)
since
Half-court games require less cardiovascular stamina, since players need not run back and
forth a full court.
Why do half-court games require less cardiovascular stamina ? (Causal)
Single wicket has rarely been played since limited overs cricket began.
Since when has single wicket rarely been played ? (Temporal)
when A one-point shot can be earned when shooting from the foul line after a foul is made.When can a one-point shot be earned ? (Conditional)
although
A bowler cannot bowl two successive overs, although a bowler can bowl unchanged at
end for several overs.
Can a bowler bowl unchanged at the same end for several overs? (Contrast, concession)
as a result
In the United States sleep deprivation is common with students because almost all schools
begin early in the morning and many of these students either choose to stay up awake late into
the night or cannot do otherwise due to delayed sleep phase syndrome. As a result, students
that should be getting between 8.5 and 9.25 hours of sleep are getting only 7 hours.
Why are students that should be getting between 8.5 and 9.25 hours of sleep getting
only 7 hours? (Result)
As a result of studies showing the effects of sleep-deprivation on grades , and the different
sleep patterns for teenagers , a school in New Zealand , changed its start time to 10:30,
in 2006, to allow students to keep to a schedule that allowed more sleep.
Why did a school in New Zealand change its start time ? (Result)
for example
Slicing also causes the shuttlecock to travel much slower than the arm movement suggests.
For example, a good cross court sliced drop shot will use a hitting action that suggests a straight
clear or smash, deceiving the opponent about both the power and direction of the shuttlecock.
Give an example where slicing also causes the shuttlecock to travel much slower than
the arm movement suggests. (Instantiation)
for instance
If the team that bats last scores enough runs to win, it is said to have ?won by n wickets?,
where n is the number of wickets left to fall. For instance a team that passes its opponents?
score having only lost six wickets would have won ?by four wickets?.
Give an instance where if the team that bats last scores enough runs to win, it is said to have
?won by n wickets?,where n is the number of wickets left to fall. (Instantiation)
Table 5: Examples
matically correct and idiomatic/natural, 3 - gram-
matically correct, 2 - some grammar problems, 1 -
grammatically unacceptable. Table 4 shows syntac-
tic correctness measure with examples.
The semantic correctness is evaluated using the
following scores: 4 - semantically correct and id-
iomatic/natural, 3 - semantically correct and close to
the text or other questions, 2 - some semantic issues,
1 - semantically unacceptable.
Table 5 shows questions generated by the system
for each connective. The results of our system on
QGSTEC-2010 development dataset are shown in
Table 6. The overall system is rated 6.3 out of 8 on
this dataset and the total number of questions gen-
erated for this dataset is 61. The instances of the
connectives were less in the QGSTEC-2010 devel-
opment dataset. So, the system is further tested on
five Wikipedia articles (football, cricket, basketball,
badminton and tennis) for effective evaluation. Re-
sults on this dataset are presented in Table 7. Overall
rating of the system is 5.8 out of 8 for this dataset
and 150 are the total number of questions generated
for this dataset. The ratings presented in the Tables 6
and 7 are the average of the ratings given by both the
evaluators. The inter-evaluator agreement (Cohen?s
kappa coefficient) for the QGSTEC-2010 develop-
6
ment dataset for syntactic correctness measure is 0.6
and is 0.5 for semantic correctness measure, and in
case of Wikipedia articles the agreement is 0.7 and
0.6 for syntactic and semantic correctness measures
respectively.
Discourse No. of Syntactic Semantic Overall
connective questions Correctness(4) Correctness(4) Rating(8)
because 20 3.6 3.6 7.2
since 9 3.8 3.2 7
when 23 2.3 2.2 4.5
although 4 4 3.8 7.8
as a result 5 4 4 8
Overall 61 3.2 3.1 6.3
Table 6: Results on QGSTEC-2010 development
dataset
Discourse No. of Syntactic Semantic Overall
connective questions Correctness(4) Correctness(4) Rating(8)
because 36 3.3 3.2 6.5
since 18 3.1 3 6.1
when 35 2.4 2.0 4.4
although 22 3.1 2.8 5.9
as a result 6 3.6 3.2 6.8
for example 16 3.1 2.9 6.0
for instance 2 4 3 7
Overall 135 3.0 2.8 5.8
Table 7: Results on the Wikipedia data(cricket, foot-
ball, basketball, badminton, tennis)
On analyzing the data, we found that the
Wikipedia articles have more complex sentences
(with unusual structure as well as more number of
clauses) than QGSTEC-2010 development dataset.
As a result, the system?s performance consistently
drops for all the connectives in case of Wikipedia
dataset.
No comparable evaluation was done as none of
the earlier works in QG exploited the discourse con-
nectives in text to generate questions.
7 Error Analysis
An error analysis was carried out on the system?s
output and the four most frequent types of errors are
discussed in this section.
7.1 Coreference resolution
The system doesn?t handle coreference resolution
and as a result of this, many questions have been
rated low for semantic correctness by the evalua-
tors. Greater the number of pronouns in the ques-
tion, lesser is the semantic rating of the question.
6. They grow in height when they reach shallower
water, in a wave shoaling process.
Question: When do they grow in height?
Although the above example 6 is syntactically
correct, such questions are rated semantically low
because the context is not sufficient to answer the
question due to the pronouns in it. 13.54% of
the generated questions on the Wikipedia dataset
have pronouns without their antecedents, making the
questions semantically insufficient.
7.2 Parsing Errors
Sometimes the parser fails to give a correct parse
for the sentences with complex structure. In such
cases, the system generates a question that is unac-
ceptable. Consider the examples below.
7. In a family who know that both parents are car-
riers of CF , either because they already have a
CF child or as a result of carrier testing , PND
allows the conversion of a probable risk of the
disease affecting an unborn child to nearer a
certainty that it will or will not be affected.
Question: Why do in a family who know that
both parents are carriers of CF , either or will
not be affected ?
In example 7 above, the sentence has a com-
plex structure containing paired connective, either-
or, where the argument of either has because and
that of or has as a result in it. Here the question is
formed using because which is correct neither syn-
tactically nor semantically due to the complex nature
of the sentence. 9.38% sentences in the datasets are
complex with either three or more discourse connec-
tives.
7.3 Errors due to the inter-sentential
connectives
For inter-sentential connectives, system considers
only those sentences in which the connectives occur
at the beginning of the sentence and the immediate
previous sentence is assumed to be the Arg1 of the
connective (which is the target argument for QG).
But this assumption is not always true. Of the total
number of instances of these connectives, 52.94%
(for Wikipedia dataset) connectives occur at the be-
ginning of the sentences. Consider the paragraph be-
low.
8. A game point occurs in tennis whenever the
7
player who is in the lead in the game needs only
one more point to win the game. The termi-
nology is extended to sets (set point), matches
(match point), and even championships (cham-
pionship point). For example, if the player who
is serving has a score of 40-love, the player has
a triple game point (triple set point, etc.) as the
player has three consecutive chances to win the
game.
Here in example 8, the third sentence in which the
example is specified is related to the first sentence
but not the immediately previous sentence. For these
connectives, the assumption that immediate previ-
ous sentence is Arg1 is false 14.29% of the times.
7.4 Fluency issues
The system does not handle the removal of pred-
icative adjuncts. So the questions with optional
phrases in it are rated low for syntactic correctness
measure.
8 Conclusions and Future Work
Our QG system generates questions using dis-
course connectives for different question types. In
this work, we present an end-to-end system that
takes a document as input and outputs all the ques-
tions for selected discourse connectives. The system
has been evaluated for syntactic and semantic sound-
ness of the question by two evaluators. We have
shown that some specific discourse relations are im-
portant such as causal, temporal and result than
others from the QG point of view. This work also
shows that discourse connectives are good enough
for QG and that there is no need for full fledged dis-
course parsing. In the near future, we plan to im-
plement coreference resolution and sentences with
more than two connectives. We aim to improve the
system with respect to the sentence complexity and
also incorporate other discourse connectives.
Acknowledgements
We would like to thank Suman Yelati and Sudheer
Kolachina from LTRC, IIIT-Hyderabad for their
helpful discussions and pointers during the course of
this work. Thanks also to the anonymous reviewers
for useful feedback.
References
2010 Question generation shared task and evaluation
challenge, http://questiongeneration.org/QG2010
Andrea Varga and Le An Ha 2010 WLV: A Question
Generation System for the QGSTEC 2010 Task B,
Proceedings of QG2010: The Third Workshop on
Question Generation
Santanu Paland Tapabrata Mondal, Partha Pakray,
Dipankar Das and Sivaji Bandyopadhyay 2010
QGSTEC System Description JUQGG: A Rule based
approach , Proceedings of QG2010: The Third
Workshop on Question Generation
Husam Ali, Yllias Chali, and Sadid A. Hasan 2010
Automation of Question Generation From Sentences,
Proceedings of QG2010: The Third Workshop on
Question Generation
Eriks Sneiders 2002. Automated question answering
using question templates that cover the conceptual
model of the database. In Proceedings of the 6th
International Conference on Applications of Natural
Language to Information Systems (pp. 235-239).
Michael Heilman, Noah A. Smith. 2009 Question gener-
ation via overgenerating transformations and ranking
Technical Report CMU-LTI-09-013, Carnegie Mellon
University.
Prashanth Mannem, Rashmi Prasad and Aravind Joshi
2010 Question Generation from Paragraphs at
UPenn: QGSTEC System Discription, Proceedings of
QG2010: The Third Workshop on Question Genera-
tion
Rashmi Prasad and Aravind Joshi 2008 A Discourse-
based Approach to Generating why-Questions from
text, Proceedings of the Workshop on the Question
Generation Shared Task and Evaluation Challenge
Arlington, VA, September 2008
Emily Pitler, Annie Louis and Ani Nenkova 2009 Auto-
matic sense prediction for implicit discourse relations
in text , ACL ?09 Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2 - Volume 2
2007 PDTB 2.0 Annotation Manual,
http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtb-
annotation-manual.pdf
8
Emily Pitler and Ani Nenkova 2009 Using syntax to
Disambiguate Explicit Discourse Connectives in Text,
ACLShort ?09 Proceedings of the ACL-IJCNLP 2009
Conference Short Papers
Rashmi Prasad and Aravind Joshi and Bonnie Webber
2010 Exploiting Scope for Shallow discourse Parsing,
LREC2010
Robert Elwell and Jason Baldridge 2008 Discourse
connective argument identification with connective
specific rankers, In Proceedings of ICSC-2008
Evaluation guidelines 2010 In QGSTEC-2010 Task B
evaluation guidelines, http://www.question genera-
tion.org/QGSTEC2010/ uploads/QG-fromSentences-
v2.doc
9
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 56?64,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Automatic Gap-fill Question Generation from Text Books
Manish Agarwal and Prashanth Mannem
Language Technologies Research Center
International Institute of Information Technology
Hyderabad, AP, India - 500032
{manish.agarwal,prashanth}@research.iiit.ac.in
Abstract
In this paper, we present an automatic question
generation system that can generate gap-fill
questions for content in a document. Gap-fill
questions are fill-in-the-blank questions with
multiple choices (one correct answer and three
distractors) provided. The system finds the in-
formative sentences from the document and
generates gap-fill questions from them by first
blanking keys from the sentences and then de-
termining the distractors for these keys. Syn-
tactic and lexical features are used in this pro-
cess without relying on any external resource
apart from the information in the document.
We evaluated our system on two chapters of
a standard biology textbook and presented the
results.
1 Introduction
Gap-fill questions are fill-in-the-blank questions,
where one or more words are removed from a
sentence/paragraph and potential answers are listed.
These questions, being multiple choice ones, are
easy to evaluate. Preparing these questions manu-
ally will take a lot of time and effort. This is where
automatic gap-fill question generation (GFQG)
from a given text is useful.
1. A bond is the sharing of a pair of va-
lence electrons by two atoms.
(a) Hydrogen (b) Covalent (c) Ionic (d) Double
(correct answer: Covalent)
In a gap-fill question (GFQ) such as the one
above, we refer to the sentence with the gap as the
question sentence (QS) and the sentence in the text
that is used to generate the QS as the gap-fill sen-
tence (GFS). The word(s) which is removed from a
GFS to form the QS is referred to as the key while
the three alternatives in the question are called as
distractors, as they are used to distract the students
from the correct answer.
Previous works in GFQG (Sumita et al, 2005;
John Lee and Stephanie Seneff, 2007; Lin et al,
2007; Pino et al, 2009; Smith et al, 2010) have
mostly worked in the domain of English language
learning. Gap-fill questions have been generated to
test student?s knowledge of English in using the cor-
rect verbs (Sumita et al, 2005), prepositions (John
Lee and Stephanie Seneff, 2007) and adjectives (Lin
et al, 2007) in sentences. Pino et al (2009) and
Smith et al (2010) have generated GFQs to teach
and evaluate student?s vocabulary.
In this paper, we move away from the domain
of English language learning and work on generat-
ing gap-fill questions from the chapters of a biol-
ogy textbook used for Advanced Placement (AP) ex-
ams. The aim is to go through the textbook, identify
informative sentences1 and generate gap-fill ques-
tions from them to aid students? learning. The sys-
tem scans through the text in the chapter and iden-
tifies the informative sentences in it using features
inspired by summarization techniques. Questions
from these sentences (GFSs) are generated by first
choosing a key in each of these and then finding ap-
propriate distractors for them from the chapter.
Our GFQG system takes a document with its title
as an input and produces a list of gap-fill questions as
1A sentence is deemed informative if it has the relevant
course knowledge which can be questioned.
56
output. Unlike previous works (Brown et al, 2005;
Smith et al, 2010) it doesn?t use any external re-
source for distractor selection, making it adaptable
to text from any domain. Its simplicity makes it use-
ful not only as an aid for teachers to prepare gap-fill
questions but also for students who need an auto-
matic question generator to aid their learning from a
textbook.
2 Data Used
A Biology text book Campbell Biology, 6th Edi-
tion has been used for work in this paper. We have
reported results of our system on 2 chapters (the
structure and function of macromolecules and an
introduction to metabolism ) of unit 1. Each chapter
contains sections and subsections with their respec-
tive topic headings. Number of subsections, sen-
tences, words per sentence in each chapter are (25,
416, 18.3) and (32, 423, 19.5) respectively. Each
subsection is taken as a document. The chapters are
divided into documents and each document is used
for GFQG independently.
3 Approach
Given a document, the gap-fill questions are gen-
erated from it in three stages: sentence selection,
key selection and distractor selection. Sentence se-
lection involves identifying informative sentences in
the document which can be used to generate a gap-
fill question. These sentences are then processed in
the key selection stage to identify the key on which
to ask the question. In the final stage, the distrac-
tors for the selected key are identified from the given
chapter by searching for words with the same con-
text as that of the key.
In each stage, the system identifies a set of candi-
dates (i.e. all sentences in the document in stage I,
words in the previously selected sentence in stage II
and words in the chapter in stage III) and extracts a
set of features relevant to the task. Weighted sum of
extracted features (see equation 1) is used to score
these candidates, with the weights for the features
in each of the three steps assigned heuristically. A
small development data has been used to tune the
feature weights.
score =
n
?
i=0
wi ? fi (1)
In equation 1, fi denotes the feature and wi denotes
the weight of the feature fi. The overall architecture
of the system is shown in Figure 1.
Sentence
Selection
sentence (GFS)
Gap?fill
selection
      &
Distractors
 selection
GAP?FILL
Question
GFSs
Document
Chapter
  Key 
Figure 1: System architecture
In earlier approaches to generating gap-fill ques-
tions (for English language learning), the keys in a
text were gathered first (or given as input in some
cases) and all the sentences containing the key were
used to generate the question. In domains where
language learning is not the aim, a gap-fill question
needs an informative sentence and not just any sen-
tence with the desired key present in it. For this rea-
son, in our work, sentence selection is performed be-
fore key selection.
3.1 Sentence Selection
A good GFS should be (1) informative and (2)
gap-fill question-generatable. An informative sen-
tence in a document is one which has relevant
knowledge that is useful in the context of the docu-
ment. A sentence is gap-fill question-generatable if
there is sufficient context within the sentence to pre-
dict the key when it is blanked out. An informative
sentence might not have enough context to generate
a question from and vice versa.
The sentence selection module goes through all
the sentences in the documents and extracts a set of
features from each of them. These features are de-
fined in such a way that the two criterion defined
above are accounted for. Table 1 gives a summary
of the features used.
First sentence: f(si) is a binary feature to check
whether the sentence si is the first sentence of the
document or not. Upon analysing the documents in
the textbook, it was observed that the first sentence
in the document usually provides a summary of the
document. Hence, f(si) has been used to make use
of the summarized first sentence of the document.
57
Feature Symbol Description Criterion
f(si) Is si the first sentence of the document? I
sim(si) No. of tokens common in si and title / length(si) I, G
abb(si) Does si contain any abbreviation? I
super(si) Does si contain a word in its superlative degree? I
pos(si) si?s position in the document (= i) G
discon(si) Is si beginning with a discourse connective? G
l(si) Number of words in si G
nouns(si) No. of nouns in si / length(si) G
pronouns(si) No. of pronouns in si / length(si) G
Table 1: Feature set for Sentence Selection (si: ith sen-
tence of the document; I: to capture informative sen-
tences; G: to capture the potential candidate for gener-
ating a GFQs)
Common tokens: sim(si) is the count of words
(nouns and adjectives) that the sentence and the title
of the document have in common. A sentence with
words from the title in it is important and is a good
candidate to ask a question using the common words
as the key.
2. The different states of potential energy that
electrons have in an atom are called energy
levels, or electron shells. (Title: The Energy
Levels of Electrons)
For example sentence 2, value of the feature is
3/19 (common words:3, sentence length:19) and
generating gap-fill question using energy, levels or
electrons as the key will be useful.
Abbreviations and Superlatives: abb(si),
super(si) features capture those sentences which
contain abbreviations and words in superlative de-
gree respectively. The binary features determine the
degree of the importance of a sentence in terms of
the presence of abbreviations and superlatives.
3. In living organisms, most of the strongest
chemical bonds are covalent ones.
For example, in sentence 3, presence of strongest
makes sentence more informative and useful for
generating a gap-fill question.
Sentence position: pos(si) is position of the
sentence si, in the document (= i). Since topic of
the document is elaborated in the middle of the
document, the sentences occurring in the middle of
the document are less important for the GFSs than
those which occur either at the start or the end of the
document. In order to use the above observation,
the module uses this feature.
Discourse connective at the beginning:
discon(si)?s value is 1 if first word of si is a
discourse connective2 and 0 otherwise. Discourse
connective at the beginning of a sentence indicates
that the sentence might not have enough context for
a QS to be understood by the students.
4. Because of this, it is both an amine and a car-
boxylic acid.
In example 4, after selecting amine and car-
boxylic as a key, QS will be left with insufficient
context to answer. Thus binary feature, discon(si),
is used.
Length: l(si) is the number of words in the
sentence. It is important to note that a very short
sentence might generate an unanswerable question
because of short context and a very long sentence
might have enough context to make the question
generated from it trivial.
Number of nouns and pronouns: Features
nouns(si) and pronouns(si) represent the amount
of context present in a sentence. More number of
pronouns in a sentence reduces the contextual infor-
mation, instead more number of nouns increases the
number of potential keys to ask a gap-fill question
on.
Four sample GFSs are shown in Table 3 with their
document?s titles.
3.2 Key Selection
For each sentence selected in the previous stage,
the key selection stage identifies the most appropri-
ate key from the sentence to ask the question on.
Previous works in this area, Smith et al (2010)
take keys as an input and, Karamanis et al (2006)
and Mitkov et al (2006) select keys on the basis of
term frequency and regular expressions on nouns.
Then they search for sentences which contain that
particular key in it. Since their approaches generate
gap-fill questions only with one blank, they could
end up with a trivial GFQ, especially in case of con-
junctions.
2because, since, when, thus, however, although, for example
and for instance connectives have been included.
58
   
(A)
  DT       JJS       NNS       IN    NN         NNS      VBP       JJ         NNS   CC        JJ    NNS  
potential keys selection
     [The  strongest   kind]     of    [chemical  bonds]    are    [covalent  bond   and    ionic    bond].
    [The  strongest  kind]  of   [ chemical  bonds]  are  [ covalent  bond  and   ionic  bond] .(B)  
Figure 2: Generating potential key?s list, (key-list) of strongest, chemical and covalent + ionic.
5. Somewhere in the transition from molecules to
cells, we will cross the blurry boundary be-
tween nonlife and life.
For instance in example sentence 5, selecting only
one of non-life and life makes the question trivial.
This is an other reason for performing sentence se-
lection before key selection. Our system can gen-
erate GFQs with multiple blanks unlike previous
works described above.
Our approach of key selection from a GFS is two
step process. In the first step the module generates
a list of potential keys from the GFS (key-list) and
in the second step it selects the best key from this
key-list.
3.2.1 Key-list formation
A list of potential keys is created in this step using
the part of speech (POS) tags of words and chunks
of the sentence in the following manner:
1. Each sequence of words in all the noun chunks
is pushed into key-list. In figure 2(A), the three
noun chunks the strongest kind, chemical bond
and covalent bond and ionic bond are pushed
into the key-list.
2. For each sequence in the key-list, the most im-
portant word(s) is selected as the potential key
and the other words are removed. The most im-
portant word in a noun chunk in the context of
GFQG in biology domain is a cardinal, adjec-
tive and noun in that order. In case where there
are multiple nouns, the first noun is chosen as
the potential key. If the noun chunk is a NP
coordination, both the conjuncts are selected as
a single potential key making it a case of mul-
tiple gaps in QS. In Figure 2(B) potential keys
strongest, chemical and covalent + ionic are se-
lected from the noun chunks by taking the order
of importance into account.
An automatic POS tagger and a noun chunker has
been used to process the sentences selected in the
first stage. It was observed that if words of a key
are spread across a chunk then there might not be
enough context left in QS to answer the question.
The noun chunk boundaries ensure that the sequence
of words in the potential keys are not disconnected.
6. Hydrogen has 1 valence electron in the first
shell, but the shell?s capacity is 2 electrons.
Any element of the key-list which occurs more
than once in the GFS is discarded as a potential key
as it more often than not generates a trivial question.
For example, in sentence 6 selecting any one of the
two electron as a key generates an easy gap-fill ques-
tion.
7. In contrast , trypsin , a digestive enzyme resid-
ing in the alkaline environment of the intestine
, has an optimal pH of .
(a) 6 (b) 7 (c) 8 (d) 9 (correct answer: 8)
If cardinals are present in a GFS, the first one is cho-
sen as its key directly and a gap-fill question has been
generated (see example 7).
3.2.2 Best Key selection
In this step three features, term(keyp),
title(keyp) and height(keyp), described in Ta-
ble 2, are used to select the best key from the key-list.
Feature Symbol Description
term(keyp)
Number of occurrences of the
keyp in the document.
title(keyp)
Does title contain
keyp ?
height(keyp)
height of the keyp in the
syntactic tree of the sentence.
Table 2: Feature set for key selection (potential key,
keyp is an element of key-list)
Term frequency: term(keyp) is number of oc-
currences of the keyp in the document. term(keyp)
59
is considered as a feature to give preference to the
potential keys with high frequency.
In title: title(keyp) is a binary feature to check
whether keyp is present in the title of the document
or not. A common word of GFS and the title of the
document serves as a better key for gap-fill question
than the ones that are not present in both.
Height: height(keyp) denotes the height 3 of the
keyp in the syntactic tree of the sentence. Height
gives an indirect indication of the importance of the
word. It also denotes the amount of text in the sen-
tence that modifies the word under consideration.
                                  
                                
                                      
      F(0)                G(0)                  
D (1)                E (0)     
A(3)                                  
C(2)B(0)
Figure 3: Height feature: node (height)
An answerable question should have enough con-
text left after the key blanked out. A word with
greater height in dependency tree gets more score
since there is enough context from its dependent
words in the syntactic tree to predict the word. For
example in Figure 3, node C?s height is two and the
words in the dashed box in its subtree provide the
context to answer a question on C.
The score of each potential key is normalized by
the number of words present in it and the best key is
chosen based on the scores of potential keys in key-
list. Table 3 shows the selected keys (red colored)
for sample GFSs.
3.3 Distractor Selection
Karamanis et al (2006) defines a distractor as,
an appropriate distractor is a concept semantically
close to the key which, however, cannot serve as the
right answer itself.
For distractor selection, Brown et al (2005) and
Smith et al (2010) used WordNet, Kunichika et
3The height of a tree is the length of the path from the deep-
est node in the tree to the root.
No. Selected keys (red colored)
1
An electron having a certain discrete amount of energy is
something like a ball on a staircase.
(The Energy Levels of Electrons)
2
Lipids are the class of large biological molecules that does not
include polymer.
(Lipids?Diverse Hydrophobic Molecules)
3
A DNA molecule is very long and usually consists of hundreds
or thousands of genes.
(Nucleic acids store and transmit hereditary information)
4
The fatty acid will have a kink in its tail wherever a double bond
occurs.
(Fats store large amounts of energy)
Table 3: Selected keys for each sample GFS
al. (2002) used their in-house thesauri to retrieve
similar or related words (synonyms, hypernyms, hy-
ponyms, antonyms, etc.). However, their approaches
can?t be used for those domains which don?t have
ontologies. Moreover, Smith et al (2010) do not se-
lect distractors based on the context of the keys. For
example, in the sentences 8 and 9, the key book oc-
curs in two different senses but same set of distrac-
tors will be generated by them.
8. Book the flight.
9. I read a book.
Feature Symbol Description
context(distractorp , measure of contextual similarity
keys) of distractorp and the keys
in which they are present
sim(distractorp , Dice coefficient score between
keys) GFS and the sentence
containing the distractorp
diff(distractorp , difference in term frequencies
keys) of distractorp and keys
in the chapter
Table 4: Feature set for distractor selection (keys is the
selected key for a GFS, distractorp is the potential dis-
tractor for the keys)
So a distractor should come from the same con-
text and domain, and should be relevant. It is also
clear from the above discussion that only term fre-
quency formula alone will not work for selection
of distractors. Our module uses features, shown in
Table 4, to select three distractors from the set of
all potential distractors. Potential distractors are the
words in the chapter which have the same POS tag
as that of the key.
60
Contextual similarity: context(distractorp,
keys) gets the contextual similarity score of a
potential distractor and the keys on the basis
of context in which they occur in their respective
sentences. Value of the feature depends on how
similar are the key and the potential distractor
contextually. The previous two and next two words
along with their POS tags are compared to calculate
the score.
Sentence Similarity: sim(distractorp, keys)
feature value represents similarity of the sentences
in which the keys and the distractorp occur in.
Dice Coefficient (Dice, 1945) (equation 2) has been
used to assign weights to those potential distractors
which come from sentences similar to GFS because
a distractor coming from a similar sentence will be
more relevant.
dice coefficient(s1, s2) =
2? commontokens
l(s1) + l(s2)
(2)
Difference in term frequencies: Feature,
diff (distractorp, keys) is used to find distractors
with comparable importance to the key. Term fre-
quency of a word represents its importance in the
text and words with comparable importance might
be close in their semantic meanings. So, a smaller
difference in the term frequencies is preferable.
key Distractors
energy charge, mass, water
polymer acid, glucose, know
DNA RNA, branch, specific
kink available, start, method
Table 5: Selected distractors for selected keys, shown in
Table 3
10. Electrons have a negative charge, the unequal
sharing of electrons in water causes the oxy-
gen atom to have a partial negative charge and
each hydrogen atom a partial positive charge.
A word that is present in the GFS would not be
selected as a distractor. For example in sentence 10,
if system selects oxygen as a key then hydrogen will
not be considered as a distractor. Table 5 shows
selected three distractors for each selected keys.
4 Evaluation and Results
Two chapters of the biology book are selected for
testing and top 15% candidates are selected by three
modules (sentence selection, key selection and dis-
tractor selection). The modules were manually eval-
uated independently by two biology students with
good English proficiency. Since in current system
any kind of post editing or manual work is avoided,
comparison of efficiency in manual and automatic
generation is not needed unlike Mitkov and Ha et
al. (2003).
4.1 Sentence Selection
The output of the sentence selection module is
a list of sentences. The evaluators check if each
of these sentences are good GFSs (informative and
gap-fill question-generatable) or not and binary
scoring is done. Evaluators are asked to evaluate
selected sentences independently, whether they are
useful for learning and answerable, or not.The cov-
erage of the selected sentences w.r.t the document
has not been evaluated.
Chapter-5 Chapter-6 Total
No. of 390 423 813Sentences
No. of 55 65 120Selected Sentences
No. of Good 51 59 110GFSs (Eval-1)
No. of Good 44 51 95GFSs (Eval-2)
Table 6: Evaluation of Sentence Selection
Evaluator-1 and 2 rated 91.66% and 79.16% of
sentences as good potential candidates for gap-fill
question respectively with 0.7 inter evaluator agree-
ment (Cohen?s kappa coefficient). Table 6 shows
the results of sentence selection for individual
chapters. Upon analysing the bad GFSs, we found
two different sources of errors. The first source is
the feature first sentence and the second is lack of
used in sentence selection module.
First sentence: Few documents in the data had
either a general statement or a summary of the pre-
vious section as the first sentence and the first sen-
tence feature contributed to their selection as GFS
even though they aren?t good GFSs.
11. An understanding of energy is as important
for students of biology as it is for students of
physics, chemistry and engineering.
For example, the system generated a gap-fill
61
question on example 11 which isn?t a good GFS at
all even though it occurs as the first sentence in the
document.
Less no. of features: Features like common to-
kens, superlative and abbreviation, discourse con-
nective at the beginning and number of pronouns
was useful in selecting informative sentences from
the documents. However, in absence of these fea-
tures in the document, module has selected the GFSs
on the basis of only two features, length and position
of the sentence. In those cases Evaluators rated few
GFSs as bad.
12. Here is another example of how emergent prop-
erties result from a specific arrangement of
building components.
For example, sentence 12 rated as a bad GFS by
the evaluators. So more features are need to be to
used to avoid this kind of errors.
13. A molecule has a characteristic size and shape.
Apart from these we also found few cases where
the context present in the GFS wasn?t sufficient to
answer the question although those sentences were
informative. In the above example 13, size and
shape were selected as the key that makes gap-fill
question unanswerable because of short context.
4.2 Key Selection
Our evaluation characterizes a key into two cat-
egories namely good (G) and bad (B). Evaluator-1
and 2 found that 94.16% and 84.16% of the keys
are good respectively with inter evaluator agreement
0.75. Table 7 shows the results of keys selection for
individual chapters.
Chap-5 Chap-6 Total
G B G B G B
Eval-1 50 5 63 2 113 7
Eval-2 50 5 51 14 101 19
Table 7: Evaluation of Key(s) Selection: Chap: Chap-
ter, Eval: Evaluator, G and B are for good and bad key
respectively
14. Carbon has a total of 6 electrons , with 2 in the
first electron shell and 4 in the second shell.
We observed that selection of first cardinal as key
is not always correct. For example, in sentence 14
selection of 6 as the key generated trivial GFQ.
4.3 Distractors Selection
Our system generates four alternatives for each
gap-fill question, out of which three are distrac-
tors. To evaluate the distractors? quality, evaluators
are asked to substitute the distractor in the gap and
check the readability and semantic meaning of the
QS to classify the distractor as good or bad. Eval-
uators rate 0, 1, 2 or 3 depending on the number of
good distractors in the GFQ (for example, questions
that are rated 2 have two good distractors and one
bad distractor).
15. An electron having a certain discrete amount of
is something like a ball on a staircase.
(a) charge (b) energy (c) mass (d) water
(Class: 3)
16. Lipids are the class of large biological
molecules that does not include .
(a) acid (b)polymer (c) glucose (d) know
(Class: 2)
17. A molecule is very long and usually
consists of hundreds or thousands of genes.
(a) DNA (b) RNA (c) specific (d) branch
(Class: 1)
18. The fatty acid will have a in its tail
wherever a double bond occurs .
(a) available (b) method (c) kink (d) start
(Class: 0)
Examples of gap-fill questions generated by our
system are shown above (red colored alternatives are
good distractors, blue colored ones are the correct
answers for the questions and the black ones are bad
distractors).
Chap-5 Chap-6 Total
Class 0 1 2 3 0 1 2 3 0 1 2 3
Eval-1 21 19 12 3 8 31 21 5 29 50 33 8
Eval-2 20 19 13 3 9 25 28 3 29 44 41 6
Table 8: Evaluation of Distractor Selection (Before any
corrections)
Table 8 shows the human evaluated results for
individual chapter. According to both evaluator-
1 and evaluator-2, 75.83% of the cases the system
finds useful gap-fill questions with 0.67 inter evalu-
ator agreement. Useful gap-fill questions are those
which have at least one good distractor. 60.05% and
67.72% test items are answered correctly by Evalu-
ator 1 and 2 respectively.
62
We observed that when a key has more than one
word, distractors? quality reduces because every to-
ken in a distractor must be comparably relevant.
Small chapter size also effects the number of good
distractors because distractors are selected from the
chapter text.
In our work, as we only considered syntactic and
lexical features for distractor selection, the selected
distractors could be semantically conflicting with
themselves or with the key. For example, due to
the lack of semantic features in our method a hyper-
nym of the key could find way into the distractors
list thereby providing a confusing list of distractors
to the students. In the example question 1 in section
1, chemical which is the hypernym of covalent and
ionic could prove confusing if its one of the choices
for the answer. Semantic similarity measures need
to be used to solve this problem.
5 Related work
Given the distinct domains in which our system
and other systems were deployed, a direct com-
parison of evaluation scores could be misleading.
Hence, in this section we compare our approach with
previous approaches in this area.
Smith et al (2010) and Pino et al (2009) used
gap-fill questions for vocabulary learning. Smith et
al. (2010) present a system, TEDDCLOG, which au-
tomatically generates draft test items from a corpus.
TEDDCLOG takes the key as input. It finds dis-
tractors from a distributional thesaurus. They got
53.33% (40 out of 75) accuracy after post editing
(editing either in carrier sentence (GFS) or in dis-
tractors) in the generated gap-fill questions.
Pino et al (2009) describe a baseline technique to
generate cloze questions (gap-fill questions) which
uses sample sentences from WordNet. They then re-
fine this technique with linguistically motivated fea-
tures to generate better questions. They used the
Cambridge Advanced Learners Dictionary (CALD)
which has several sample sentences for each sense
of a word for stem selection (GFS). The new strat-
egy produced high quality cloze questions 66% of
the time.
Karamanis et al (2006) report the results of a pi-
lot study on generating Multiple-Choice Test Items
(MCTI) from medical text which builds on the work
of Mitkov et al (2006). Initially key set is enlarged
with NPs featuring potential key terms as their heads
and satisfying certain regular expressions. Then sen-
tences having at least one key are selected and the
terms with the same semantic type in UMLS are se-
lected as distractors. In their manual evaluation, the
domain experts regarded a MCTI as unusable if it
could not be used in a test or required too much revi-
sion to do so. The remaining items were considered
to be usable and could be post edited by the experts
to improve their content and readability or replace
inappropriate distractors. They have reported 19%
usable items generated from their system and after
post editing stems accuracy jumps to 54%.
However, our system takes a document and pro-
duces a list of GFQs by selecting informative sen-
tences from the document. It doesn?t use any exter-
nal resources for distractors selection and finds them
in the chapter only that makes it adaptable for those
domains which do not have ontologies.
6 Conclusions and Future Work
Our GFQG system, selects most informative sen-
tences of the chapters and generates gap-fill ques-
tions on them. Syntactic features helped in quality of
gap-fill questions. We look forward to experiment-
ing on larger data by combining the chapters. Eval-
uation of course coverage by our system and use of
semantic features will be part of our future work.
Acknowledgements
We would like to thank Avinesh Polisetty and
Sudheer Kolachina from LTRC, IIIT-Hyderabad for
their helpful discussions and pointers during the
course of this work. Thanks also to the anonymous
reviewers for useful feedback.
References
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Yamamoto
2005. Measuring Non-native Speakers Proficiency of
English by Using a Test with Automatically-Generated
Fill-in-the-Blank Questions, 2nd Wkshop on Building
Educational Applications using NLP, Ann Arbor.
John Lee and Stephanie Seneff. 2007. Automatic Gen-
eration of Cloze Items for Prepositions, CiteSeerX
- Scientific Literature Digital Library and Search
Engine [http://citeseerx.ist.psu.edu/oai2] (United
States).
Lin, Y. C., Sung, L. C., Chen and M. C. 2007. An
63
Automatic Multiple-Choice Question Generation
Scheme for English Adjective Understanding, CCE
2007 Workshop Proc. of Modeling, Management and
Generation of Problems / Questions in eLearning, pp.
137-142.
Juan Pino, Michael Heilman and Maxine Eskenazi.
2009. A Selection Strategy to Improve Cloze Question
Quality, Wkshop on Intelligent Tutoring Systems for
Ill-Defined Domains. 9th Int. Conf. on ITS.
Simon Smith, P.V.S Avinesh and Adam Kilgarriff. 2010.
Gap-fill Tests for Language Learners: Corpus-Driven
Item Generation .
Jonathan C. Brown, Gwen A. Frishkoff, Maxine Es-
kenazi. 2005. Automatic Question Generation for
Vocabulary Assessment, Proc. of HLT/EMNLP ?05,
pp. 819-826.
Nikiforos Karamanis, Le An Ha and Ruslan Mitkov.
2006 Generating Multiple-Choice Test Iterms from
Medical Text: A Pilot Study, In Proceedings of INLG
2006, Sydney, Australia.
Ruslan Mitkov, Le An Ha and Nikiforos Karamanis.
2006 A computer-aided environment for generating
multiple-choice test items, Natural Language Engi-
neering 12(2): 177-194
Hidenobu Kunichika, Minoru Urushima,Tsukasa Hi-
rashima and Akira Takeuchi. 2002. A Computational
Method of Complexity of Questions on Contents of
English Sentences and its Evaluation, In: Proc. of
ICCE 2002, Auckland, NZ, pp. 97101 (2002).
Lee Raymond Dice. 1945. Measures of the Amount of
Ecologic Association Between Species
Ruslan Mitkov and Le An Ha. 2003 Computer-aided
generation of multiple-choice tests, Proceedings
of the HLT/NAACL 2003 Workshop on Building
educational applications using Natural Language
Processing. Edmonton, Canada, 17-22.
64
